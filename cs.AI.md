# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Bifurcations and loss jumps in RNN training.](http://arxiv.org/abs/2310.17561) | 这篇论文研究了在RNN训练中的分歧现象和损失跳跃，并证明了在特定类型的RNN中存在着某些分歧。 |
| [^2] | [Instability of computer vision models is a necessary result of the task itself.](http://arxiv.org/abs/2310.17559) | 该论文研究了计算机视觉模型的不稳定性，并指出这是当前问题表述方式的必然结果。该问题无法完全消除，但可以通过提高图像分辨率、提供上下文信息、全面标注训练数据和防止攻击者访问计算机视觉系统来部分缓解该问题。 |
| [^3] | [Interactive Robot Learning from Verbal Correction.](http://arxiv.org/abs/2310.17555) | 本研究设计了一个新的基于大型语言模型（LLM）OLAF的交互式机器人学习系统，它允许日常用户通过口头纠正教授机器人，并能根据口头反馈更新机器人的视觉运动神经策略，从而避免重复错误。实验结果表明，在模拟和物理硬件上，该系统在长时间线的操纵任务中平均改善了20.0%的策略成功率。 |
| [^4] | [Model-Based Runtime Monitoring with Interactive Imitation Learning.](http://arxiv.org/abs/2310.17552) | 本文提出了一种基于模型的运行时监控算法，通过学习部署数据来检测系统异常并预测故障，旨在解决机器人学习中的泛化和鲁棒性挑战。 |
| [^5] | [Unpacking the Ethical Value Alignment in Big Models.](http://arxiv.org/abs/2310.17551) | 本文探讨了大型模型中的伦理价值对齐问题，调查了现有的人工智能伦理准则，并提出了建立统一和普遍的人工智能伦理框架的重要性。此外，对当前主流大型语言模型的伦理倾向进行了研究，并分析了在实现伦理价值对齐时的挑战。 |
| [^6] | [Human-Guided Complexity-Controlled Abstractions.](http://arxiv.org/abs/2310.17550) | 本研究通过训练神经模型生成一系列离散表示，并通过调整表示的复杂性来提高任务的泛化性能。在微调实验中，我们发现适当的复杂性水平支持最佳的微调性能，并且在人类参与者的研究中也得到验证。 |
| [^7] | [Neuro-Inspired Fragmentation and Recall to Overcome Catastrophic Forgetting in Curiosity.](http://arxiv.org/abs/2310.17537) | 提出了一种受神经启发的方法，通过分段和回溯来克服深度强化学习中的灾难性遗忘问题，并通过内在奖励激励代理探索新的状态。 |
| [^8] | [SoK: Pitfalls in Evaluating Black-Box Attacks.](http://arxiv.org/abs/2310.17534) | 提出了一个评估黑盒攻击的分类法，揭示了未开发的威胁空间，并展示了在某些设置上已有技术的局限性。 |
| [^9] | [Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages.](http://arxiv.org/abs/2310.17526) | 本研究评估了GPT-4在多种语言的同行评审文献和灰色文献筛选和提取数据方面的能力，结果显示GPT-4在大多数任务上准确度与人类表现相当，在调整了偶然一致性和数据集不平衡后，其在数据提取方面表现出中等水平的准确度。 |
| [^10] | [The Expressive Power of Low-Rank Adaptation.](http://arxiv.org/abs/2310.17513) | 本文分析了低秩适应（LoRA）的表达能力，证明了对于全连接神经网络，当LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度）时，LoRA可以使任何模型f准确表示任何较小的目标模型f。对于Transformer网络，通过rank-（嵌入大小/ 2）的LoRA适配器可以使任何模型适应于相同大小的目标模型。 |
| [^11] | [CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents.](http://arxiv.org/abs/2310.17512) | 本文研究了基于大型语言模型的智能体之间的竞争行为。通过建立一个竞争环境并进行实验，发现竞争可以促使智能体进行转变和采取新策略，从而在社会和经济发展中发挥重要作用。 |
| [^12] | [Orchestration of Emulator Assisted Mobile Edge Tuning for AI Foundation Models: A Multi-Agent Deep Reinforcement Learning Approach.](http://arxiv.org/abs/2310.17492) | 本研究提出了一种基于模拟器辅助的移动边缘调优的AI基础模型编排方法，该方法通过创新的模拟器适配器架构和混合多智能体深度强化学习策略，实现了高效部署和精调基础模型，从而提高了本地任务性能。 |
| [^13] | [Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering.](http://arxiv.org/abs/2310.17490) | 本研究提出了一种通过减少无关文档的干扰来改善开放领域问答中的零样本阅读器的方法。采用了干扰感知的答案选择(DAS)方法，以解决LLMs受到干扰和过度自信的问题。实验结果表明，该方法成功地改善了零样本阅读器的性能，并展现出了优越的可迁移性。 |
| [^14] | [Bias in Evaluation Processes: An Optimization-Based Model.](http://arxiv.org/abs/2310.17489) | 本研究提出了一个基于优化的模型，用于评估过程中的偏见分析。模型通过将真实效用分布转化为观察到的分布来考虑偏见，并对参数进行研究，以探究其对观察到的分布的影响。通过实证验证和数据拟合，我们证明了该模型的有效性。 |
| [^15] | [Towards Learning Monocular 3D Object Localization From 2D Labels using the Physical Laws of Motion.](http://arxiv.org/abs/2310.17462) | 该论文提出了一种新的方法，利用物体运动的物理知识和简单的2D标注，从单目图像中实现精确的3D物体定位，无需昂贵的3D标注。经过实验证明，在真实数据实验中平均距离误差仅为6厘米，表明该方法在无法收集3D数据进行训练的情况下具有潜力。 |
| [^16] | [Generating by Understanding: Neural Visual Generation with Logical Symbol Groundings.](http://arxiv.org/abs/2310.17451) | 这篇论文提出了一种神经符号学习方法，AbdGen，用于将知识推理系统与神经视觉生成模型集成。它解决了符号赋值和规则学习的问题，通过量化诱导方法实现可靠高效的符号赋值，通过对比元诱导方法实现精确的规则学习。 |
| [^17] | [LSA64: An Argentinian Sign Language Dataset.](http://arxiv.org/abs/2310.17429) | 这篇论文介绍了一个来自阿根廷手语的数据集LSA64。该数据集包含3200个视频，包括64个不同的阿根廷手语手势，为构建一个全面的阿根廷手势研究级数据集迈出了第一步。 |
| [^18] | [Handshape recognition for Argentinian Sign Language using ProbSom.](http://arxiv.org/abs/2310.17427) | 本文的两个主要贡献是：首次创建了一个针对阿根廷手语的手势数据库，并提出了一种使用ProbSom进行图像处理和手势分类的技术，该技术在目前的研究中与其他方法进行了对比。 |
| [^19] | [Distribution of Action Movements (DAM): A Descriptor for Human Action Recognition.](http://arxiv.org/abs/2310.17421) | 本文提出了一种新的动作描述器，动作运动的分布描述器（DAM），通过计算关节运动方向分布的归一化直方图来表示动作。该描述器在多个著名数据集上超过了几种最新技术。 |
| [^20] | [Goals are Enough: Inducing AdHoc cooperation among unseen Multi-Agent systems in IMFs.](http://arxiv.org/abs/2310.17416) | 该论文提出了一种在IMF中引导未知多智能体系统之间即时合作的方法，通过对意图驱动的管理进行层次协调，避免了重新训练整个系统的成本和时间开销。 |
| [^21] | [PETA: Evaluating the Impact of Protein Transfer Learning with Sub-word Tokenization on Downstream Applications.](http://arxiv.org/abs/2310.17415) | PETA通过评估亚词切分在蛋白质迁移学习中的影响，提出了关于蛋白质语言模型的综合评估方法，并发现词汇表大小在50以上能够获得最佳性能。 |
| [^22] | [Synthesizing Efficiently Monitorable Formulas in Metric Temporal Logic.](http://arxiv.org/abs/2310.17410) | 本论文提出了一种自动化合成从系统执行中得到形式规范的方法，通过将合成任务简化为线性实数算术问题的可满足性问题，并设计了一种学习算法，合成了具有有限先行查看的简洁公式。 |
| [^23] | [Invariance Measures for Neural Networks.](http://arxiv.org/abs/2310.17404) | 本文提出了一种用于量化神经网络不变性的测量方法，该方法敏感且可解释，并能应用于任何神经网络模型。在仿射变换领域和CIFAR10和MNIST数据集上的验证表明，神经网络的内部不变性对于随机权重初始化非常稳定，但对于数据集或变换的改变不稳定。这些测量方法将为不变性表示的新研究方向提供可能性。 |
| [^24] | [ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation.](http://arxiv.org/abs/2310.17389) | 本研究引入了ToxicChat，一个基于实际用户查询构建的新型毒性检测基准。该基准揭示了当前毒性检测模型在实际用户-AI对话中面临的困难，强调了实际对话中存在的独特挑战。 |
| [^25] | [YOLO-BEV: Generating Bird's-Eye View in the Same Way as 2D Object Detection.](http://arxiv.org/abs/2310.17379) | YOLO-BEV是一个高效的框架，利用独特的摄像头设置生成车辆环境的2D鸟瞰图。它采用了YOLO的检测机制，并通过自定义设计的检测头部将全景数据转化为统一的鸟瞰视图地图。这种方法在实时车辆感知任务中具有实际可行性。 |
| [^26] | [Optimization dependent generalization bound for ReLU networks based on sensitivity in the tangent bundle.](http://arxiv.org/abs/2310.17378) | 本文通过估计梯度下降得到的初始参数向量中可用的网络集合的Rademacher复杂度，提出了一个关于前向ReLU网络泛化误差的PAC类型界限，通过限制网络梯度对于输入数据在优化轨迹上的扰动的灵敏度，不显式地依赖网络的深度。 |
| [^27] | [Dialogue-based generation of self-driving simulation scenarios using Large Language Models.](http://arxiv.org/abs/2310.17372) | 本文介绍了一个系统，使用大型语言模型将用户的英文话语映射为领域特定代码，以支持对话式生成自动驾驶仿真场景，并探索了语言模型所能捕捉到的上下文敏感性。 |
| [^28] | [Exploring the Potential of Generative AI for the World Wide Web.](http://arxiv.org/abs/2310.17370) | 本文探索了生成型人工智能在万维网领域的潜力，特别关注图像生成。我们开发了WebDiffusion工具来模拟基于稳定扩散的万维网，并评估了生成图像质量。 |
| [^29] | [Cultural Adaptation of Recipes.](http://arxiv.org/abs/2310.17353) | 本研究介绍了一项涉及中餐和英语国家菜系之间食谱的翻译和文化适应的新任务，提供了一个独特的数据集并评估了多种方法的性能。 |
| [^30] | [CQM: Curriculum Reinforcement Learning with a Quantized World Model.](http://arxiv.org/abs/2310.17330) | CQM提出了一种新的课程增强学习方法，通过自动定义语义目标空间和提出课程目标，在解决复杂任务中取得了显著进展。该方法通过向量量化-变分自动编码器(VQ-VAE)将连续观察结果进行离散化，并通过图形恢复离散观察结果之间的时间关系，提供了不确定性和时间距离感知的课程目标。 |
| [^31] | [C-Disentanglement: Discovering Causally-Independent Generative Factors under an Inductive Bias of Confounder.](http://arxiv.org/abs/2310.17325) | 本文提出了一个名为C-Disentanglement的框架，旨在发现具有因果关系且受混淆因素影响的生成因子，以提高数据生成的可控性和鲁棒性。 |
| [^32] | [FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language.](http://arxiv.org/abs/2310.17306) | FormaT5是一个基于转换器的模型，可以根据目标表格和自然语言描述生成数据相关的条件格式规则。为了解决描述不足的问题，FormaT5通过放弃目标的方式学习预测占位符。 |
| [^33] | [Comparing Photorealistic and Animated Embodied Conversational Agents in Serious Games: An Empirical Study on User Experience.](http://arxiv.org/abs/2310.17300) | 本文通过比较逼真和动画形象的交互式对话代理在严肃游戏中的表现，提供了关于基于语音的ECAs设计的洞察和建议。结果显示逼真和动画形象版本都具有很高的可用性，但大多数参与者更喜欢逼真版本。 |
| [^34] | [Attribute Based Interpretable Evaluation Metrics for Generative Models.](http://arxiv.org/abs/2310.17261) | 本论文提出了一种基于属性的生成模型可解释性评估指标，通过度量生成图像集与训练集关于属性强度分布的差异，可以更好地衡量模型生成结果与训练数据的相似度。 |
| [^35] | [IDENAS: Internal Dependency Exploration for Neural Architecture Search.](http://arxiv.org/abs/2310.17250) | IDENAS是一种集成神经架构搜索和特征选择的方法，通过探索内部依赖性来提高分类任务的性能。 |
| [^36] | [CROP: Conservative Reward for Model-based Offline Policy Optimization.](http://arxiv.org/abs/2310.17245) | CROP提出了一种保守奖励的模型训练方法用于基于模型的离线策略优化，通过同时最小化估计误差和随机动作奖励来实现保守的奖励估计。 |
| [^37] | [Joint Entity and Relation Extraction with Span Pruning and Hypergraph Neural Networks.](http://arxiv.org/abs/2310.17238) | 本文提出了基于超图神经网络的联合实体和关系抽取方法，使用span剪枝机制减轻误差传播问题，通过构建超图进行高阶建模，实现多个实体和关系之间的交互。 |
| [^38] | [TST$^\mathrm{R}$: Target Similarity Tuning Meets the Real World.](http://arxiv.org/abs/2310.17228) | 本文提出了在现实世界中应用和改进目标相似度调整（TST）的不同方法，包括使用更大的模型嵌入、训练一个小模型转换嵌入以匹配代码相似度，并介绍了高效选择训练样例和基于排名的评估方法。 |
| [^39] | [Beyond MLE: Convex Learning for Text Generation.](http://arxiv.org/abs/2310.17217) | 本论文提出了一种基于凸函数的训练目标类，超越了传统的最大似然估计方法。该方法适用于闭合型文本生成任务，并能够使得模型生成更加合适的响应。 |
| [^40] | [Emotion Recognition by Video: A review.](http://arxiv.org/abs/2310.17212) | 本文是一篇关于视频情感识别的综述论文，总结了相关研究中的现有趋势、情感模型、数据库以及单模态和多模态的视频情感识别方法的结构、性能和优缺点。 |
| [^41] | [Efficient Data Fusion using the Tsetlin Machine.](http://arxiv.org/abs/2310.17207) | 这项研究提出了一种使用Tsetlin机器进行高效数据融合的新方法，通过监测学习到的逻辑子句在动态数据中的变化，识别和融合噪声，并在实验中展示出高性能。 |
| [^42] | [Taming Gradient Variance in Federated Learning with Networked Control Variates.](http://arxiv.org/abs/2310.17200) | 本论文提出了一种名为FedNCV的新型框架，用于解决联邦学习中梯度方差的问题。通过在客户端和服务器级别实现REINFORCE Leave-One-Out (RLOO)作为控制变量单元，优化了本地梯度更新并提供了无偏和低方差的聚合梯度，实现了稳健的全局更新。 |
| [^43] | [How do Language Models Bind Entities in Context?.](http://arxiv.org/abs/2310.17191) | 通过分析语言模型的表示，我们发现了绑定ID机制，它可以将实体与属性进行有效地绑定。我们通过因果干预实验进一步证明了语言模型内部激活表示绑定信息的方式。研究结果揭示了语言模型在上下文中如何表示符号知识，从而为理解大规模语言模型的一般上下文推理提供了指导。 |
| [^44] | [Understanding the Effects of Projectors in Knowledge Distillation.](http://arxiv.org/abs/2310.17183) | 投影仪在知识蒸馏中有助于提高性能，并且可以在维度匹配和逻辑蒸馏的情况下起到改善效果。学生使用投影仪可以在训练准确性和测试准确性之间取得更好的平衡。 |
| [^45] | [Graphical Object-Centric Actor-Critic.](http://arxiv.org/abs/2310.17178) | 这项研究提出了一种新颖的以对象为中心的强化学习算法，将演员-评论家和基于模型的方法结合起来，利用解耦的对象表示有效地学习策略。该方法填补了以对象为中心的强化学习环境中高效且适用于离散或连续动作空间的世界模型的研究空白。 |
| [^46] | [Bridging The Gaps Between Token Pruning and Full Pre-training via Masked Fine-tuning.](http://arxiv.org/abs/2310.17177) | 本文提出了一种遮蔽微调方法，用于弥合标记修剪和完整预训练之间的差距，该方法通过遮蔽图像块并预测图像类别标签来实现动态视觉transformers的初始化和加速推理。 |
| [^47] | [A Deep Learning Approach to Teeth Segmentation and Orientation from Panoramic X-rays.](http://arxiv.org/abs/2310.17176) | 本研究提出了一个利用深度学习技术从全景X射线图像中进行牙齿分割和定位的方法。我们通过修改已有模型并引入注意力机制，实现了高精度和高性能的牙齿分割和定位。在公开数据集上的评估结果表明，我们的方法在牙齿实例分割和牙齿定位方面取得了优异的性能。 |
| [^48] | [Improving Denoising Diffusion Models via Simultaneous Estimation of Image and Noise.](http://arxiv.org/abs/2310.17167) | 通过重新参数化扩散过程并直接估计图像和噪声，本文改进了去噪扩散模型，提高了图像生成的速度和质量。 |
| [^49] | [Content-based Controls For Music Large Language Modeling.](http://arxiv.org/abs/2310.17162) | 该论文提出了一种基于内容的控制方法，用于音乐大语言建模。通过对音高、和弦和鼓乐等固有音乐语言的直接控制，实现了高质量的音乐生成，并且使用了参数高效微调的方法，比原始模型的参数数量少于4%。 |
| [^50] | [CosmosDSR -- a methodology for automated detection and tracking of orbital debris using the Unscented Kalman Filter.](http://arxiv.org/abs/2310.17158) | CosmosDSR是一种新颖的方法，将YOLOv3与无味卡尔曼滤波器结合起来，用于自动检测和跟踪轨道碎片。在使用SPARK数据集进行训练和测试时，YOLOv3表现出很高的准确度，并且CosmosDSR和LKF都能准确地跟踪卫星。 |
| [^51] | [Technical Note: Feasibility of translating 3.0T-trained Deep-Learning Segmentation Models Out-of-the-Box on Low-Field MRI 0.55T Knee-MRI of Healthy Controls.](http://arxiv.org/abs/2310.17152) | 本研究评估了在0.55T低场MRI中对健康控制者膝关节进行标记物定量的深度学习技术的可行性，并表明这些技术在分割软骨区域方面与3.0T几乎相当。 |
| [^52] | [Explainable Spatio-Temporal Graph Neural Networks.](http://arxiv.org/abs/2310.17149) | 这篇论文提出了一种可解释的时空图神经网络（STExplainer）框架，通过增强时空图神经网络的可解释性，在城市资源分配和政策制定方面具有应用潜力。 |
| [^53] | [Counterfactual-Augmented Importance Sampling for Semi-Offline Policy Evaluation.](http://arxiv.org/abs/2310.17146) | 提出了一种半离线评估框架，用于强化学习中的定量和定性评估。通过人类用户提供未被观察到的反事实轨迹的注释，设计了一种基于重要性抽样和加权的新型OPE估计器系列。 |
| [^54] | [Symbolic Planning and Code Generation for Grounded Dialogue.](http://arxiv.org/abs/2310.17140) | 该论文介绍了一个模块化和可解释的基于实际对话系统，通过组合大型语言模型(LLMs)、符号化规划器和基于实际的代码执行来解决基于任务的对话中的挑战。实验证明该系统在协同参考解析任务上的性能显著优于先前的最先进技术。 |
| [^55] | [Core Challenge 2023: Solver and Graph Descriptions.](http://arxiv.org/abs/2310.17136) | 本文总结了 CoRe Challenge 2023 中提交的求解器和 ISR 实例的所有描述。 |
| [^56] | [Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs.](http://arxiv.org/abs/2310.17133) | 本文提出了一种通过视觉问答对的方式将探测信号融入多模态机器翻译中，以增强跨模态交互。实验证明了该方法的有效性。 |
| [^57] | [Unleashing the potential of GNNs via Bi-directional Knowledge Transfer.](http://arxiv.org/abs/2310.17132) | 本文通过研究发现GNN未充分利用内在的特征转换操作能力，提出了一种双向知识传递的插拔式方法，使GNN能够充分释放特征转换操作的潜力，而不需要修改原始架构。 |
| [^58] | [Topic Segmentation of Semi-Structured and Unstructured Conversational Datasets using Language Models.](http://arxiv.org/abs/2310.17120) | 本文通过对非结构化文本进行分析，揭示了目前主题分段模型在此类数据上的泛化能力不足，并提出了从头开始训练相对小规模的目标数据集来改善分段结果的方法。实证评估表明使用多种损失函数可以减轻非结构化对话数据集的不平衡效应。 |
| [^59] | [Detecting stealthy cyberattacks on adaptive cruise control vehicles: A machine learning approach.](http://arxiv.org/abs/2310.17091) | 本研究针对自适应巡航控制车辆可能遭受的网络攻击问题，提出了一个交通模型框架和基于GAN的异常检测模型，能够实时识别恶意操纵、虚假注入和拒绝服务攻击。 |
| [^60] | [Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models.](http://arxiv.org/abs/2310.17086) | Transformers学会了高阶优化方法，用于上下文学习，通过实现类似于迭代牛顿法的算法，而不是梯度下降。 |
| [^61] | [Isometric Motion Manifold Primitives.](http://arxiv.org/abs/2310.17072) | Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods. |
| [^62] | [math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories.](http://arxiv.org/abs/2310.17064) | 该研究调查了将大型语言模型应用于形式化高级数学概念的可行性，并提出了一个可以批判性地审查和检查研究论文中数学推理的框架。 |
| [^63] | [Learning Repeatable Speech Embeddings Using An Intra-class Correlation Regularizer.](http://arxiv.org/abs/2310.17049) | 本文介绍了一种使用类内相关正则化器来学习可重复的语音嵌入的方法，该方法通过引入ICC正则化器作为对比损失的补充组件，可以提高深度神经网络生成的嵌入的重复性。 |
| [^64] | [StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling.](http://arxiv.org/abs/2310.17042) | StochGradAdam是一种利用随机梯度抽样加速神经网络训练的优化器，通过选择性梯度考虑，能够稳定收敛，提升鲁棒训练。在图像分类和分割任务中表现优异。 |
| [^65] | [On Surgical Fine-tuning for Language Encoders.](http://arxiv.org/abs/2310.17041) | 本文表明，对于不同的下游语言任务，只对语言编码器的部分层进行细调即可获得接近甚至优于细调所有层的性能。通过提出一种高效度量方法，我们证明了该方法可以选择性微调导致强大下游性能的层。研究突出表明，任务特定信息通常局部化在少数层内，只调整这些层就足够了。 |
| [^66] | [netFound: Foundation Model for Network Security.](http://arxiv.org/abs/2310.17025) | netFound是一个基于自我监督算法的基础模型，用于网络安全领域。该模型通过预训练捕捉网络流量的层次化和多模态属性，并能够在质量低、有限和嘈杂的数据情况下进行微调。 |
| [^67] | [Controlled Decoding from Language Models.](http://arxiv.org/abs/2310.17022) | 本论文提出了一种名为受控解码（CD）的离策略强化学习方法，用于控制语言模型的生成，以达到高回报的结果。CD通过前缀评分器来引导生成，可以在推理时预测预期回报，并且具有模块化设计，可用于解决多目标强化学习问题，而不增加复杂性。 |
| [^68] | [An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives.](http://arxiv.org/abs/2310.17017) | 本研究使用PRISMA框架综述了计算机科学和医学领域发表的534篇论文，发现了136篇关键论文，涵盖了心理健康对话代理的多种建模和实验设计技术特征。 |
| [^69] | [Personalized Speech-driven Expressive 3D Facial Animation Synthesis with Style Control.](http://arxiv.org/abs/2310.17011) | 提出了一个个性化语音驱动的表情三维动画合成框架，通过建模身份特定的面部动作风格，以及语音相关内容，实现高度自然和可信度的动画合成。 |
| [^70] | [This Reads Like That: Deep Learning for Interpretable Natural Language Processing.](http://arxiv.org/abs/2310.17010) | 本研究将原型网络方法扩展到了自然语言处理领域，引入了一种学习的加权相似度度量和事后可解释性机制，通过聚焦于句子嵌入的重要维度来提高相似度计算，并改善了预测性能和解释准确性。 |
| [^71] | [STEER: Semantic Turn Extension-Expansion Recognition for Voice Assistants.](http://arxiv.org/abs/2310.16990) | STEER是一个用于语音助手的语义转向扩展识别模型，通过训练数据集和启发式规则进行转向意图预测，并在实验中展现出了良好的性能。 |
| [^72] | [The Significance of Machine Learning in Clinical Disease Diagnosis: A Review.](http://arxiv.org/abs/2310.16978) | 本综述研究了机器学习算法在临床疾病诊断中的应用，特别关注提高准确性和计算效率的优化。该研究发现，通过利用先进的ML和AI方法，可以增强医疗保健相关方的诊断和治疗能力。 |
| [^73] | [CL-MASR: A Continual Learning Benchmark for Multilingual ASR.](http://arxiv.org/abs/2310.16931) | CL-MASR是一个用于研究多语音ASR的持续学习基准，提供了一系列多样化的持续学习方法，并解决了学习新语言时遗忘问题。 |
| [^74] | [Wide Flat Minimum Watermarking for Robust Ownership Verification of GANs.](http://arxiv.org/abs/2310.16919) | 该论文提出了一种广泛平坦最低水印技术，用于对抗白盒攻击，保护GAN的知识产权。通过将水印嵌入到GAN的训练中，使生成的图片包含不可见的水印，并通过模型参数的随机噪声，使模型收敛到广泛平坦的水印损失最低点，提高水印的鲁棒性。 |
| [^75] | [An Explainable Deep Learning-Based Method For Schizophrenia Diagnosis Using Generative Data-Augmentation.](http://arxiv.org/abs/2310.16867) | 本研究提出了一种基于可解释性深度学习和生成式数据增强的精神分裂症诊断方法，通过使用卷积神经网络进行初步诊断，并利用WGAN-GP和VAE生成的合成数据集进行增强，显著提高了诊断准确性和模型可解释性。 |
| [^76] | [Graph-based multimodal multi-lesion DLBCL treatment response prediction from PET images.](http://arxiv.org/abs/2310.16863) | 本文提出了一种基于图神经网络的方法，利用临床和图像数据来预测DLBCL患者的治疗反应，并在实验证明这种方法优于传统的监督方法。 |
| [^77] | [Balancing Augmentation with Edge-Utility Filter for Signed GNNs.](http://arxiv.org/abs/2310.16862) | 本文提出一种用于有符号图神经网络的平衡增强方法，以解决语义和结构不平衡的问题，并通过边效用过滤器选择性地增强原始有符号图。 |
| [^78] | [CP-BCS: Binary Code Summarization Guided by Control Flow Graph and Pseudo Code.](http://arxiv.org/abs/2310.16853) | CP-BCS是一个基于控制流图和伪代码的二进制代码摘要框架，用于自动生成函数摘要，特别是对于剥离了符号表和调试信息的二进制。它利用专家知识学习二进制函数的执行行为和逻辑语义。 |
| [^79] | [Enhancing Energy-efficiency by Solving the Throughput Bottleneck of LSTM Cells for Embedded FPGAs.](http://arxiv.org/abs/2310.16842) | 本研究通过优化LSTM单元，提出了一种在终端设备上进行能效推断的新方法。以交通速度预测为例，优化后的LSTM单元在FPGA上实现了较快的推断速度和较低的能耗，相比现有方法提高了吞吐量和能效。 |
| [^80] | [Multi-scale Diffusion Denoised Smoothing.](http://arxiv.org/abs/2310.16779) | 本文研究了多尺度扩散去噪平滑的准确度和认证鲁棒性之间的权衡，并提出了一种在共享扩散模型上调整以实现平滑分类器鲁棒性的新方法。 |
| [^81] | [DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection.](http://arxiv.org/abs/2310.16776) | 这项研究介绍了一种名为DEFT的数据高效微调框架，通过无监督核心集选择来最小化微调大规模语言模型所需的数据量。研究结果表明，DEFT模型在准确性上与现有模型相当，并且仅使用了70%的数据量。 |
| [^82] | [SkyMath: Technical Report.](http://arxiv.org/abs/2310.16713) | SkyMath是一个13亿参数的大型语言模型，通过自比较微调，它在数学推理任务中表现优秀，超过了同等规模的所有开源模型，并取得了新的SOTA最佳性能。 |
| [^83] | [Knowledge Editing for Large Language Models: A Survey.](http://arxiv.org/abs/2310.16218) | 大型语言模型(LLMs)在学术和工业领域具有巨大潜力。本文综述了LLMs的知识编辑问题，强调了需要开发有效和高效的技术来更新预训练LLMs以纳入新知识的重要性。 |
| [^84] | [Digital Twin-Enabled Intelligent DDoS Detection Mechanism for Autonomous Core Networks.](http://arxiv.org/abs/2310.12924) | 本文提出了一种基于数字孪生的智能DDoS检测机制，适用于自治核心网络。通过使用在线学习方法，我们设计了一个DDoS检测架构，并实现了自动特征选择模块和Yet Another Next Generation模型来提高准确性和性能。这种解决方案成功地检测DDoS攻击，并具有97%的真实分类率。 |
| [^85] | [Network-Aware AutoML Framework for Software-Defined Sensor Networks.](http://arxiv.org/abs/2310.12914) | 这项研究提出了一个面向软件定义传感器网络的网络感知自动机器学习框架，能够在网络限制环境下选择合适的机器学习算法来检测DDoS攻击，并防止过拟合。 |
| [^86] | [TwinPot: Digital Twin-assisted Honeypot for Cyber-Secure Smart Seaports.](http://arxiv.org/abs/2310.12880) | 该论文介绍了一种数字孪生辅助的蜜罐，用于保护智能海港的网络安全。传统安全解决方案对于保护物联网和物理网络系统是有限的，而蜜罐则可以提供有关攻击者行为的宝贵信息。数字孪生技术可以增强虚拟蜜罐的逼真性，吸引更多的攻击者。 |
| [^87] | [Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding.](http://arxiv.org/abs/2310.11191) | 本研究探索了进一步提高医学领域文本简化可读性的方法，包括一种新的非典型损失和一种优化简单性的重新排序解码方法，取得了更好的性能。 |
| [^88] | [MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models.](http://arxiv.org/abs/2310.02255) | 本论文提出了MathVista，这是一个评估视觉场景中数学推理能力的基准测试。通过对12个著名的基础模型进行全面的定量评估，发现最好的GPT-4V模型相对于第二名的Bard模型在准确率上提升了15.1%。 |
| [^89] | [Harnessing the Power of Choices in Decision Tree Learning.](http://arxiv.org/abs/2310.01551) | 该研究提出了一种利用选择能力的决策树学习算法Top-$k$，相比于传统贪婪算法和最优决策树算法，在准确率和性能上取得更好的结果。 |
| [^90] | [Multilingual Natural Language ProcessingModel for Radiology Reports -- The Summary is all you need!.](http://arxiv.org/abs/2310.00100) | 本研究通过在多语言文本到文本变换器模型上微调，开发了一个能够自动在多语言中总结放射学报告的模型。该模型有助于提高未来深度学习模型的研究和发展，且能够应用于不同族裔背景的患者数据。 |
| [^91] | [CoFiI2P: Coarse-to-Fine Correspondences for Image-to-Point Cloud Registration.](http://arxiv.org/abs/2309.14660) | CoFiI2P是一种粗到精的图像到点云注册方法，通过利用全局信息和特征建立对应关系，实现全局最优解。 |
| [^92] | [Statistically Valid Variable Importance Assessment through Conditional Permutations.](http://arxiv.org/abs/2309.07593) | 本论文提出了一种通过条件置换进行统计有效的变量重要性评估的方法。实证结果表明，这种方法克服了标准置换重要性的局限性，并在深度神经网络中表现出最高的准确性。 |
| [^93] | [Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions.](http://arxiv.org/abs/2309.07510) | 本论文提出了一个环境感知的可供性框架，考虑了物体级的可行性先验和环境约束，以解决多个遮挡的复杂情况下的三维关节物体操作问题。 |
| [^94] | [COMEDIAN: Self-Supervised Learning and Knowledge Distillation for Action Spotting using Transformers.](http://arxiv.org/abs/2309.01270) | 使用自监督学习和知识蒸馏的COMEDIAN提出了一种初始化时空Transformer的流程，用于动作定位任务。在SoccerNet-v2数据集上实验证明了其最先进的性能和有效性。 |
| [^95] | [LLM Powered Sim-to-real Transfer for Traffic Signal Control.](http://arxiv.org/abs/2308.14284) | 本研究利用大型语言模型（LLMs）通过基于提示的行动转换，解决了交通信号控制任务中从仿真到真实的迁移问题。 |
| [^96] | [Adaptive whitening with fast gain modulation and slow synaptic plasticity.](http://arxiv.org/abs/2308.13633) | 本研究提出了一个多时间尺度的自适应白化机制模型，使用快速增益调制和慢速突触可塑性相结合的方式来适应变化的感觉统计信息。 |
| [^97] | [PID-Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks.](http://arxiv.org/abs/2307.05891) | 该论文受到PID控制器的启发，提出了两种用于编码历史记录的架构，以平衡提取相关信息的灵活性与对环境变化的鲁棒性。 |
| [^98] | [Large Language Models as General Pattern Machines.](http://arxiv.org/abs/2307.04721) | 预训练的大型语言模型（LLMs）展现出了强大的序列模式完成能力，即使在随机样本的情况下也能保持一定的准确性。这些模型可以用于机器人领域的问题，如动态状态序列预测和自主路径规划。 |
| [^99] | [A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks.](http://arxiv.org/abs/2307.01951) | 本文以节点分类为例，通过“神经塌陷”现象探索图神经网络中特征演化的机制，并发现即使在节点分类情况下，特征的类内变异性也会减少，但不及基于实例的情况那么显著。 |
| [^100] | [DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models.](http://arxiv.org/abs/2306.14685) | 本文介绍了DiffSketcher，一种通过隐式扩散模型实现文本引导的矢量素描合成的创新算法。DiffSketcher通过直接优化贝塞尔曲线和扩散模型损失来生成矢量化的手绘素描，并通过注意力图加快生成过程。实验结果表明DiffSketcher的素描质量高于之前方法。 |
| [^101] | [Can Differentiable Decision Trees Learn Interpretable Reward Functions?.](http://arxiv.org/abs/2306.13004) | 本文提出了使用可微分决策树从人类偏好中学习具有表达能力和可解释性的奖励函数，通过在多个环境上的评估，发现该方法能够学习到可解释的奖励函数，但在强化学习测试时表现受到树的离散性的影响。 |
| [^102] | [Tuning Legged Locomotion Controllers via Safe Bayesian Optimization.](http://arxiv.org/abs/2306.07092) | 本文提出了一种通过安全贝叶斯优化来调整四足行走控制器的策略，并通过模拟和硬件实验验证了该方法的有效性。 |
| [^103] | [Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL.](http://arxiv.org/abs/2306.04220) | 本文提出了一个新的离线强化学习算法TDM，利用系统动力学的基本对称性实现高效学习小数据集。 |
| [^104] | [SourceP: Smart Ponzi Schemes Detection on Ethereum Using Pre-training Model with Data Flow.](http://arxiv.org/abs/2306.01665) | 本文提出了一种使用预训练模型和数据流的方法，通过智能合约的源代码特征，实现检测以太坊上的智能庞兹骗局。该方法提高了模型的可解释性和降低了数据获取和特征提取的难度。 |
| [^105] | [Efficient Diffusion Policies for Offline Reinforcement Learning.](http://arxiv.org/abs/2305.20081) | 本论文提出了高效扩散策略（EDP）用于解决离线强化学习中的两个关键挑战，即计算效率低和难以与最大似然的强化学习算法兼容。EDP通过近似构建动作来避免运行采样链，并在实验中得到验证。 |
| [^106] | [Neural (Tangent Kernel) Collapse.](http://arxiv.org/abs/2305.16427) | 本文介绍了神经切向核（NTK）和神经崩溃（NC）之间的关系，证明了在具有块状NTK的DNN中会出现NC，并通过大规模实验支持理论的正确性。 |
| [^107] | [Scaling Data-Constrained Language Models.](http://arxiv.org/abs/2305.16264) | 研究人员研究了在数据受限制的情况下缩放语言模型，并提出了一个计算最优性的缩放定律，考虑到重复令牌和过量参数的价值递减。 |
| [^108] | [Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models.](http://arxiv.org/abs/2305.15080) | 本文介绍了一种名为对比阅读模型（Cream）的新型神经架构，旨在通过捕捉复杂细节，提升大型语言模型在语言-图像理解能力方面的性能。该方法通过视觉和辅助编码器及对比特征对齐技术实现了对视觉定位上下文中语言信息的更有效理解。 |
| [^109] | [Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers.](http://arxiv.org/abs/2305.14858) | Pre-RMSNorm和Pre-CRMSNorm Transformers是等效且高效的Pre-LN Transformers架构，可以统一使用两种主流归一化技术，LayerNorm和RMSNorm，从而加速和稳定Transformer模型的训练。 |
| [^110] | [What Else Do I Need to Know? The Effect of Background Information on Users' Reliance on QA Systems.](http://arxiv.org/abs/2305.14331) | 本研究调查了用户在评估模型预测时缺乏足够信息时与QA系统的交互方式，并发现即使缺乏这些信息，用户仍然过度依赖于模型的预测。然而，提供相关背景信息有助于减少对错误预测的依赖。 |
| [^111] | [Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation.](http://arxiv.org/abs/2305.14327) | Dynosaur提出了一种动态增长模式，用于自动整理指令调优数据。通过利用现有的注释数据集，Dynosaur能够以较低的API成本提供高质量的指令调优数据。 |
| [^112] | [Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document.](http://arxiv.org/abs/2305.13850) | 这篇论文提出了一种结合全局结构知识的连续迭代的方式去捕获实体之间的依赖关系，以提高视觉丰富文档中关系抽取的准确性。 |
| [^113] | [Detecting and Mitigating Hallucinations in Multilingual Summarisation.](http://arxiv.org/abs/2305.13632) | 本文提出一种新的度量方法mFACT，可以在非英语摘要中评估其忠实性。本文还提出了一种简单有效的加权方法，可以通过跨语言转移减少摘要的幻觉问题。 |
| [^114] | [Squared Neural Families: A New Class of Tractable Density Models.](http://arxiv.org/abs/2305.13552) | 提出一种新的可计算密度模型类——平方神经分布族，其通过对神经网络的2范数进行平方和基于某个基础度量进行归一化，严格推广了经典指数族，具有闭性条件推断和可计算的边际分布。 |
| [^115] | [Multimodal Automated Fact-Checking: A Survey.](http://arxiv.org/abs/2305.13507) | 本调查提出了一个多模态自动事实核查的框架，并包括了独特的子任务，重点关注了文本，图像，音频和视频这四种模态的现实应用。纪录了相关的基准模型，讨论了未来研究的局限性和前景。 |
| [^116] | [Multi-grained Hypergraph Interest Modeling for Conversational Recommendation.](http://arxiv.org/abs/2305.04798) | 本文提出了一种多粒度超图兴趣建模方法，通过利用历史对话数据丰富当前对话的上下文，从不同角度捕捉用户兴趣。采用超图结构表示复杂的语义关系，建模用户的历史对话会话，捕捉粗粒度的会话级关系。 |
| [^117] | [NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports.](http://arxiv.org/abs/2305.03598) | 本文提出了一种面向临床试验报告的自然语言推理模型，通过检索支持事实来确定自然语言陈述和CTR之间的推理关系。 |
| [^118] | [Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models.](http://arxiv.org/abs/2305.02531) | 本研究分析了大型语言模型在不同语言提示下的奖励时间偏好，并发现GPT在具有较弱未来时态的语言下表现出更大的耐心，这与使用该语言的人类的偏好相似。 |
| [^119] | [Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding.](http://arxiv.org/abs/2305.00633) | 本论文提出了一种通过自我评估引导解码提高推理的方法，使用经过校准的自动标准探索推理搜索空间，使搜索能够产生更高质量的最终预测结果；使用自我评估引导的随机束搜索在产生推理链的质量和多样性之间平衡权衡，适应多数投票，并且可以准确判断逻辑错误，提高一致性和鲁棒性。 |
| [^120] | [Detecting Spoilers in Movie Reviews with External Movie Knowledge and User Networks.](http://arxiv.org/abs/2304.11411) | 该研究提出了一种新型多视角剧透检测框架，MVSD，该框架将电影知识和用户活动纳入考虑，并且在LCS数据集上展示了其优于强基线的相关实验结果。 |
| [^121] | [Changes to Captions: An Attentive Network for Remote Sensing Change Captioning.](http://arxiv.org/abs/2304.01091) | 本研究提出了一个关注变化到描述网络，用于准确描述遥感图像中的变化。与自然图像的变化描述不同，遥感变化描述需要捕捉影响因素的最显著变化。 |
| [^122] | [A Survey on Causal Discovery Methods for Temporal and Non-Temporal Data.](http://arxiv.org/abs/2303.15027) | 本文综述了时序和非时序数据因果发现的方法，探讨了不同算法在不同情况下识别因果边缘的能力。同时还介绍了可用于评估因果发现方法性能的基准数据集和常见指标。 |
| [^123] | [TriPlaneNet: An Encoder for EG3D Inversion.](http://arxiv.org/abs/2303.13497) | 本研究介绍了一种实时方法TriPlaneNet，通过直接利用EG3D生成模型的三平面表示，建立在一个用于潜在编码的前馈卷积编码器上，并扩展了一个完全卷积的三平面数值偏移预测器，旨在弥合现有的GAN反演方法的差距。 |
| [^124] | [Penalty-Based Imitation Learning With Cross Semantics Generation Sensor Fusion for Autonomous Driving.](http://arxiv.org/abs/2303.11888) | 本文介绍了一种基于惩罚的模仿学习方法和特征级多传感器融合技术，应用于自动驾驶导航中。文中重点介绍了针对激光雷达和RGB信息的融合技术，旨在提高模型对交通规则的遵守能力。 |
| [^125] | [What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement.](http://arxiv.org/abs/2303.11249) | 本文通过采用量子物理学的理论工具，提出了一种判定数据适合于局部连接神经网络的必要且充分条件，并导出了一种相应的预处理方法。 |
| [^126] | [The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models.](http://arxiv.org/abs/2303.03284) | 本论文提出了一种“Wasserstein Belief Updater”算法来学习部分可观测环境下的信念更新，该算法通过学习POMDP​​的潜在模型和置信更新的近似值，实现了对历史观察和行动的有效压缩，提升了算法的性能表现。 |
| [^127] | [Open-World Object Manipulation using Pre-trained Vision-Language Models.](http://arxiv.org/abs/2303.00905) | 本文研究了使用预训练的视觉-语言模型来实现机器人的开放世界目标操作，从而解决了机器人在面对丰富语义信息时的困难挑战。 |
| [^128] | [DualStreamFoveaNet: A Dual Stream Fusion Architecture with Anatomical Awareness for Robust Fovea Localization.](http://arxiv.org/abs/2302.06961) | DualStreamFoveaNet是一种具有解剖意识的双流融合架构，通过利用视网膜和血管分布进行多线索融合，实现对鲁棒的中央凹点定位。实验证明该架构在中央凹点定位方面达到了最先进的性能。 |
| [^129] | [Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals.](http://arxiv.org/abs/2302.04449) | 本论文提出了阅读并奖励的框架，通过阅读Atari游戏开发者发布的指导手册，以提高强化学习算法在Atari游戏中的效率。该框架包含一个QA提取模块和一个推理模块，能够从指导手册中提取关键信息，并评估物体与智能体的交互效果。 |
| [^130] | [A Theory of Link Prediction via Relational Weisfeiler-Leman.](http://arxiv.org/abs/2302.02209) | 本文提出了一种基于关系Weisfeiler-Leman算法的理论，为知识图谱中的图神经网络提供了理论解释，并对各种模型的表达能力进行了表征，并解释了一些广泛采用的实际设计选择的优点。 |
| [^131] | [Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning.](http://arxiv.org/abs/2301.12593) | 该论文提出了一种分布鲁棒安全强化学习框架，通过使用失真风险度量来处理模型不确定性。该方法不需要极小极大优化，具有高效且不依赖模型的特点。实验证明该框架能够在具有安全约束的控制任务中实现稳健的性能和安全性。 |
| [^132] | [Increasing Fairness via Combination with Learning Guarantees.](http://arxiv.org/abs/2301.10813) | 该论文提出了一种公平质量度量方法，名为判别风险，旨在反映个体和群体公平性。此外，研究者还讨论了公平性是否可以在理论上得到保证。 |
| [^133] | [Event knowledge in large language models: the gap between the impossible and the unlikely.](http://arxiv.org/abs/2212.01488) | 大型语言模型拥有丰富的事件知识，几乎总是将可能事件的描述比不可能事件的描述赋予更高的可能性。 |
| [^134] | [Artificial intelligence in government: Concepts, standards, and a unified framework.](http://arxiv.org/abs/2210.17218) | 本论文研究了人工智能在政府中的应用，强调了标准化操作程序和符合社会规范期望的重要性，并指出了多学科研究者在概念上的碎片化问题。 |
| [^135] | [Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks.](http://arxiv.org/abs/2209.06589) | 本研究探讨了图神经网络在推广到更大的图和从未见过的数据方面的局限，并提出了多模块GNN框架，通过推广单个规范非线性变换来适应新图。结果表明，多模块GNN在合成和实际数据集上均显著提高了GNN的泛化能力，并在几项具有挑战性的任务上实现了最先进的性能。 |
| [^136] | [Semi-supervised Deep Multi-view Stereo.](http://arxiv.org/abs/2207.11699) | 本文针对学习式多视图立体问题，探讨了仅有一部分带有深度准确值的数据的半监督设置。通过引入新的半监督分布增强框架，提出了一种方法来解决MVS中的分布间隙多义性问题。 |
| [^137] | [Local Advantage Networks for Cooperative Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2112.12458) | 这项研究提出了一种局部优势网络（LAN）算法，该算法通过对决架构和中心化评论家来学习合作多智能体的最佳响应策略，并在StarCraft II多智能体挑战基准测试上达到了最先进的性能。 |
| [^138] | [Investigating the usefulness of Quantum Blur.](http://arxiv.org/abs/2112.01646) | “量子模糊”方法是一种简单的量子软件示例，在计算机游戏、音乐和艺术等领域中具有实用价值。这项研究通过与最重要用户的讨论，确定了该方法最有用的特性，并揭示了这些特性与量子现象的关系。 |
| [^139] | [Road Network Guided Fine-Grained Urban Traffic Flow Inference.](http://arxiv.org/abs/2109.14251) | 本文提出了一种基于道路网络的交通流量推断方法，利用道路网络的先验知识全面学习细粒度交通流的道路感知空间分布，普遍适用于城市交通流量监测和调控方案。 |
| [^140] | [Tuned Compositional Feature Replays for Efficient Stream Learning.](http://arxiv.org/abs/2104.02206) | 本文提出了一种名为CRUMB的新的持续学习算法，通过重放通过重新组合特征图来缓解遗忘问题。CRUMB通过存储内存块的索引来使得在后续任务中能够回放特定的记忆，这种重建机制还可以帮助神经网络最小化灾难性遗忘。 |
| [^141] | [Improving Few-Shot Learning through Multi-task Representation Learning Theory.](http://arxiv.org/abs/2010.01992) | 本文通过多任务表示学习理论的最新进展，提出了一个新的基于谱的正则化项来改进少样本学习，并在实验证明了其有效性。 |

# 详细

[^1]: RNN训练中的分歧和损失跳跃

    Bifurcations and loss jumps in RNN training. (arXiv:2310.17561v1 [cs.LG])

    [http://arxiv.org/abs/2310.17561](http://arxiv.org/abs/2310.17561)

    这篇论文研究了在RNN训练中的分歧现象和损失跳跃，并证明了在特定类型的RNN中存在着某些分歧。

    

    循环神经网络（RNN）是用于建模和预测序列数据以及从观测时间序列中推断动力系统（DS）的常用机器学习工具。DS理论的概念已被用于进一步理解经过训练的RNN如何解决复杂任务以及训练过程本身。分歧是DS中特别重要的现象，包括RNN，在系统的一个或多个参数变化时，指系统的动力行为的拓扑（定性）变化。了解RNN的分歧结构将有助于推断其许多计算和动力属性，例如对参数变化的敏感性或训练过程中的行为。特别是，分歧可能解释RNN训练中观察到的突然损失跳跃，这可能严重阻碍训练过程。在这里，我们首先数学地证明了针对一类基于ReLU的RNN，确实存在一些分歧。

    Recurrent neural networks (RNNs) are popular machine learning tools for modeling and forecasting sequential data and for inferring dynamical systems (DS) from observed time series. Concepts from DS theory (DST) have variously been used to further our understanding of both, how trained RNNs solve complex tasks, and the training process itself. Bifurcations are particularly important phenomena in DS, including RNNs, that refer to topological (qualitative) changes in a system's dynamical behavior as one or more of its parameters are varied. Knowing the bifurcation structure of an RNN will thus allow to deduce many of its computational and dynamical properties, like its sensitivity to parameter variations or its behavior during training. In particular, bifurcations may account for sudden loss jumps observed in RNN training that could severely impede the training process. Here we first mathematically prove for a particular class of ReLU-based RNNs that certain bifurcations are indeed associ
    
[^2]: 计算机视觉模型的不稳定性是任务本身的必然结果

    Instability of computer vision models is a necessary result of the task itself. (arXiv:2310.17559v1 [cs.CV])

    [http://arxiv.org/abs/2310.17559](http://arxiv.org/abs/2310.17559)

    该论文研究了计算机视觉模型的不稳定性，并指出这是当前问题表述方式的必然结果。该问题无法完全消除，但可以通过提高图像分辨率、提供上下文信息、全面标注训练数据和防止攻击者访问计算机视觉系统来部分缓解该问题。

    

    由于当前计算机视觉模型的不稳定性而产生的对抗性样本是一个非常重要的话题，因为它们有可能危及任何应用。在本文中，我们证明了不稳定性是不可避免的，原因包括：a) 数据的对称性（平移不变性），b) 分类任务的范畴性质，以及c) 将图像分类为对象本身的基本差异。不完全标注的训练数据进一步加剧了这个问题。因此我们得出结论，不稳定性是当前计算机视觉问题表述方式所必然的结果。虽然问题无法完全消除，但通过分析原因，我们找到了一些方式来部分缓解该问题，包括：i) 提高图像的分辨率，ii) 为图像提供上下文信息，iii) 对训练数据进行全面标注，和iv) 防止攻击者频繁访问计算机视觉系统。

    Adversarial examples resulting from instability of current computer vision models are an extremely important topic due to their potential to compromise any application. In this paper we demonstrate that instability is inevitable due to a) symmetries (translational invariance) of the data, b) the categorical nature of the classification task, and c) the fundamental discrepancy of classifying images as objects themselves. The issue is further exacerbated by non-exhaustive labelling of the training data. Therefore we conclude that instability is a necessary result of how the problem of computer vision is currently formulated. While the problem cannot be eliminated, through the analysis of the causes, we have arrived at ways how it can be partially alleviated. These include i) increasing the resolution of images, ii) providing contextual information for the image, iii) exhaustive labelling of training data, and iv) preventing attackers from frequent access to the computer vision system.
    
[^3]: 从口头纠正中进行交互式机器人学习

    Interactive Robot Learning from Verbal Correction. (arXiv:2310.17555v1 [cs.RO])

    [http://arxiv.org/abs/2310.17555](http://arxiv.org/abs/2310.17555)

    本研究设计了一个新的基于大型语言模型（LLM）OLAF的交互式机器人学习系统，它允许日常用户通过口头纠正教授机器人，并能根据口头反馈更新机器人的视觉运动神经策略，从而避免重复错误。实验结果表明，在模拟和物理硬件上，该系统在长时间线的操纵任务中平均改善了20.0%的策略成功率。

    

    随着我们将机器人设计成能在家庭等非结构化环境中运行，学习和改进行为的能力越来越重要。在这项工作中，我们设计了一个基于大型语言模型（LLM）OLAF的新学习系统，它允许日常用户通过口头纠正教授机器人，当机器人犯错误时，例如说“停下你正在做的事情。你应该靠近杯子。” OLAF的一个关键特点是它能够根据口头反馈更新机器人的视觉运动神经策略，以避免将来重复错误。这与现有的基于LLM的机器人系统形成对比，后者仅仅遵循口头命令或纠正，而不会从中学习。在模拟和物理硬件上的实验中，我们证明了我们设计的有效性，用户教机器人执行长时间线的操纵任务，在策略成功率方面平均改善了20.0%。视频和更多结果可在ht找到。

    The ability to learn and refine behavior after deployment has become ever more important for robots as we design them to operate in unstructured environments like households. In this work, we design a new learning system based on large language model (LLM), OLAF, that allows everyday users to teach a robot using verbal corrections when the robot makes mistakes, e.g., by saying "Stop what you're doing. You should move closer to the cup." A key feature of OLAF is its ability to update the robot's visuomotor neural policy based on the verbal feedback to avoid repeating mistakes in the future. This is in contrast to existing LLM-based robotic systems, which only follow verbal commands or corrections but not learn from them. We demonstrate the efficacy of our design in experiments where a user teaches a robot to perform long-horizon manipulation tasks both in simulation and on physical hardware, achieving on average 20.0% improvement in policy success rate. Videos and more results are at ht
    
[^4]: 基于模型的运行时监控与交互式模仿学习

    Model-Based Runtime Monitoring with Interactive Imitation Learning. (arXiv:2310.17552v1 [cs.RO])

    [http://arxiv.org/abs/2310.17552](http://arxiv.org/abs/2310.17552)

    本文提出了一种基于模型的运行时监控算法，通过学习部署数据来检测系统异常并预测故障，旨在解决机器人学习中的泛化和鲁棒性挑战。

    

    机器人学习方法近年来取得了巨大的进展，但是泛化和鲁棒性的挑战仍然制约着它们的广泛应用。无法检测和解决潜在故障使得最先进的学习系统无法在高风险任务中投入使用。最近在交互式模仿学习方面的进展提出了一个有希望的框架，可以实现人机团队合作，使机器人能够安全操作并在长期部署中不断提高性能。然而，现有方法通常需要持续的人工监督和预先的反馈，限制了它们在实际领域的实用性。本文旨在赋予机器人在任务执行过程中监控和检测错误的能力。我们引入了一种基于模型的运行时监控算法，通过学习部署数据来检测系统异常并预测故障。与以前的工作不同，我们的方法能够预见未来的故障，而且不需要故障经验来进行训练。

    Robot learning methods have recently made great strides, but generalization and robustness challenges still hinder their widespread deployment. Failing to detect and address potential failures renders state-of-the-art learning systems not combat-ready for high-stakes tasks. Recent advances in interactive imitation learning have presented a promising framework for human-robot teaming, enabling the robots to operate safely and continually improve their performances over long-term deployments. Nonetheless, existing methods typically require constant human supervision and preemptive feedback, limiting their practicality in realistic domains. This work aims to endow a robot with the ability to monitor and detect errors during task execution. We introduce a model-based runtime monitoring algorithm that learns from deployment data to detect system anomalies and anticipate failures. Unlike prior work that cannot foresee future failures or requires failure experiences for training, our method l
    
[^5]: 解读大模型中的伦理价值对齐问题

    Unpacking the Ethical Value Alignment in Big Models. (arXiv:2310.17551v1 [cs.CY])

    [http://arxiv.org/abs/2310.17551](http://arxiv.org/abs/2310.17551)

    本文探讨了大型模型中的伦理价值对齐问题，调查了现有的人工智能伦理准则，并提出了建立统一和普遍的人工智能伦理框架的重要性。此外，对当前主流大型语言模型的伦理倾向进行了研究，并分析了在实现伦理价值对齐时的挑战。

    

    大型模型极大地提升了人工智能理解、生成和操作信息与内容的能力，并被广泛应用于各个领域。然而，随着这些模型越来越多地融入日常生活，其固有的伦理价值观和潜在偏见给社会带来了未预料的风险。本文概述了与大型模型有关的风险和挑战，调查了现有的人工智能伦理准则，并分析了这些模型的局限性所引发的伦理影响。从规范伦理的角度出发，我们提出了对最近的规范准则进行重新评估的建议，强调了学术界在建立统一和普遍的人工智能伦理框架方面的合作努力的重要性。此外，我们使用道德基础理论调查了当前主流大型语言模型的道德倾向，分析了现有的价值观对齐算法，并概述了在实现伦理价值对齐时遇到的独特挑战。

    Big models have greatly advanced AI's ability to understand, generate, and manipulate information and content, enabling numerous applications. However, as these models become increasingly integrated into everyday life, their inherent ethical values and potential biases pose unforeseen risks to society. This paper provides an overview of the risks and challenges associated with big models, surveys existing AI ethics guidelines, and examines the ethical implications arising from the limitations of these models. Taking a normative ethics perspective, we propose a reassessment of recent normative guidelines, highlighting the importance of collaborative efforts in academia to establish a unified and universal AI ethics framework. Furthermore, we investigate the moral inclinations of current mainstream LLMs using the Moral Foundation theory, analyze existing alignment algorithms, and outline the unique challenges encountered in aligning ethical values within them. To address these challenges
    
[^6]: 人类引导的复杂度控制抽象化

    Human-Guided Complexity-Controlled Abstractions. (arXiv:2310.17550v1 [cs.LG])

    [http://arxiv.org/abs/2310.17550](http://arxiv.org/abs/2310.17550)

    本研究通过训练神经模型生成一系列离散表示，并通过调整表示的复杂性来提高任务的泛化性能。在微调实验中，我们发现适当的复杂性水平支持最佳的微调性能，并且在人类参与者的研究中也得到验证。

    

    神经网络通常学习任务特定的潜在表示，但这些表示无法推广到新的环境或任务。相反，人类在各种抽象级别（例如，“鸟”与“麻雀”）上学习离散表示（即概念或单词），并根据任务使用适当的抽象。受此启发，我们训练神经模型生成一系列离散表示，并通过调整表示分布的熵来控制表示的复杂性（大致上是为编码输入分配了多少位）。在微调实验中，仅使用少量带标签的示例用于新任务，我们展示了（1）调整表示以适当的复杂性水平支持最高的微调性能，以及（2）在一个人类参与者的研究中，用户能够根据离散表示的可视化来确定下游任务的适当复杂性水平。我们的结果表明一个有希望的方向。

    Neural networks often learn task-specific latent representations that fail to generalize to novel settings or tasks. Conversely, humans learn discrete representations (i.e., concepts or words) at a variety of abstraction levels (e.g., ``bird'' vs. ``sparrow'') and deploy the appropriate abstraction based on task. Inspired by this, we train neural models to generate a spectrum of discrete representations, and control the complexity of the representations (roughly, how many bits are allocated for encoding inputs) by tuning the entropy of the distribution over representations. In finetuning experiments, using only a small number of labeled examples for a new task, we show that (1) tuning the representation to a task-appropriate complexity level supports the highest finetuning performance, and (2) in a human-participant study, users were able to identify the appropriate complexity level for a downstream task using visualizations of discrete representations. Our results indicate a promising
    
[^7]: 神经启发的分段和回溯法克服好奇心中的灾难性遗忘

    Neuro-Inspired Fragmentation and Recall to Overcome Catastrophic Forgetting in Curiosity. (arXiv:2310.17537v1 [cs.AI])

    [http://arxiv.org/abs/2310.17537](http://arxiv.org/abs/2310.17537)

    提出了一种受神经启发的方法，通过分段和回溯来克服深度强化学习中的灾难性遗忘问题，并通过内在奖励激励代理探索新的状态。

    

    深度强化学习方法在一系列任务上表现出色，但在大型环境中稀疏奖励的困难探索任务中仍然存在困难。为了解决这个问题，可以使用通过前向模型预测误差生成的内在奖励，这些误差随着环境的熟悉程度而减少，并激励代理探索新的状态。虽然基于预测的内在奖励可以帮助代理解决困难的探索任务，但它们可能会遭受灾难性遗忘，并且实际上会在访问的状态上增加。我们首先研究了格状世界环境中灾难性遗忘的条件和原因。然后，我们提出了一种受人类和动物学习启发的新方法FARCuriosity。该方法依赖于分段和回溯：代理根据惊讶性对环境进行分段，并为每个分段使用不同的本地好奇模块（基于预测的内在奖励函数），以使模块不是在整个环境上进行训练。在每个分段中，代理可以同时存储并回溯先前的奖励值以应对灾难性遗忘。

    Deep reinforcement learning methods exhibit impressive performance on a range of tasks but still struggle on hard exploration tasks in large environments with sparse rewards. To address this, intrinsic rewards can be generated using forward model prediction errors that decrease as the environment becomes known, and incentivize an agent to explore novel states. While prediction-based intrinsic rewards can help agents solve hard exploration tasks, they can suffer from catastrophic forgetting and actually increase at visited states. We first examine the conditions and causes of catastrophic forgetting in grid world environments. We then propose a new method FARCuriosity, inspired by how humans and animals learn. The method depends on fragmentation and recall: an agent fragments an environment based on surprisal, and uses different local curiosity modules (prediction-based intrinsic reward functions) for each fragment so that modules are not trained on the entire environment. At each fragm
    
[^8]: SoK：评估黑盒攻击中的陷阱

    SoK: Pitfalls in Evaluating Black-Box Attacks. (arXiv:2310.17534v1 [cs.CR])

    [http://arxiv.org/abs/2310.17534](http://arxiv.org/abs/2310.17534)

    提出了一个评估黑盒攻击的分类法，揭示了未开发的威胁空间，并展示了在某些设置上已有技术的局限性。

    

    许多研究涉及对图像分类器的黑盒攻击。然而，这些研究对对手的知识假设不同，目前的文献缺乏围绕威胁模型进行组织的连贯性。为了系统化该领域的知识，我们提出了一个关于威胁空间的分类法，涵盖了反馈粒度、交互式查询的访问和攻击者可用的辅助数据的质量和数量三个维度。我们的新分类法提供了三个关键见解。1) 尽管有广泛文献，仍存在许多未开发的威胁空间，无法通过从已知领域的技术简单地改进来解决。我们通过将基于已知领域从完整置信向量访问的技术适应到访问前k个置信得分的较少研究的设置中来证明这一点，但同时也展示了它在仅获得预测标签的更严格设置中仍然不足。

    Numerous works study black-box attacks on image classifiers. However, these works make different assumptions on the adversary's knowledge and current literature lacks a cohesive organization centered around the threat model. To systematize knowledge in this area, we propose a taxonomy over the threat space spanning the axes of feedback granularity, the access of interactive queries, and the quality and quantity of the auxiliary data available to the attacker. Our new taxonomy provides three key insights. 1) Despite extensive literature, numerous under-explored threat spaces exist, which cannot be trivially solved by adapting techniques from well-explored settings. We demonstrate this by establishing a new state-of-the-art in the less-studied setting of access to top-k confidence scores by adapting techniques from well-explored settings of accessing the complete confidence vector, but show how it still falls short of the more restrictive setting that only obtains the prediction label, h
    
[^9]: 大型语言模型能否取代人类在系统评价过程中的角色？评估GPT-4在多种语言的同行评审文献和灰色文献筛选和提取数据方面的效果。

    Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages. (arXiv:2310.17526v1 [cs.CL])

    [http://arxiv.org/abs/2310.17526](http://arxiv.org/abs/2310.17526)

    本研究评估了GPT-4在多种语言的同行评审文献和灰色文献筛选和提取数据方面的能力，结果显示GPT-4在大多数任务上准确度与人类表现相当，在调整了偶然一致性和数据集不平衡后，其在数据提取方面表现出中等水平的准确度。

    

    系统评价对于指导实践、研究和政策至关重要，然而常常需要耗费大量时间和人力。大型语言模型（LLM）可能能够加快和自动化系统评价的过程，但是它们在这些任务中的表现尚未经过全面评估，而且还没有研究测试过迄今为止最大的LLM——GPT-4。本预注册研究采用“无人参与”的方法评估了GPT-4在标题/摘要筛选、全文审查和数据提取方面在不同文献类型和语言上的能力。尽管GPT-4在大多数任务中的准确度与人类表现相当，但结果受到偶然一致性和数据集不平衡的影响。在调整了这些因素后，数据提取方面表现出中等水平的准确度，在使用高可靠性提示进行筛选的研究中，筛选全文文献的表现水平在不同阶段和语言上均为无到中等。

    Systematic reviews are vital for guiding practice, research, and policy, yet they are often slow and labour-intensive. Large language models (LLMs) could offer a way to speed up and automate systematic reviews, but their performance in such tasks has not been comprehensively evaluated against humans, and no study has tested GPT-4, the biggest LLM so far. This pre-registered study evaluates GPT-4's capability in title/abstract screening, full-text review, and data extraction across various literature types and languages using a 'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human performance in most tasks, results were skewed by chance agreement and dataset imbalance. After adjusting for these, there was a moderate level of performance for data extraction, and - barring studies that used highly reliable prompts screening performance levelled at none to moderate for different stages and languages. When screening full-text literature using highly reliable prom
    
[^10]: 《低秩适应的表达能力》

    The Expressive Power of Low-Rank Adaptation. (arXiv:2310.17513v1 [cs.LG])

    [http://arxiv.org/abs/2310.17513](http://arxiv.org/abs/2310.17513)

    本文分析了低秩适应（LoRA）的表达能力，证明了对于全连接神经网络，当LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度）时，LoRA可以使任何模型f准确表示任何较小的目标模型f。对于Transformer网络，通过rank-（嵌入大小/ 2）的LoRA适配器可以使任何模型适应于相同大小的目标模型。

    

    低秩适应（LoRA）是一种参数高效的微调方法，利用矩阵的低秩适应性，在微调预训练模型（如大型语言模型和扩散模型）中得到了广泛应用。尽管在实践中取得了巨大成功，但是LoRA的理论基础在很大程度上尚未得到探索。本文通过从理论角度分析LoRA的表达能力，首次尝试弥合这一差距。我们证明了对于全连接神经网络，如果LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度），则LoRA可以使任何模型f准确表示任何较小的目标模型f。当LoRA-rank低于阈值时，我们还量化了逼近误差。对于Transformer网络，我们证明任何模型可以通过rank-（嵌入大小/ 2）的LoRA适配器适应于相同大小的目标模型。

    Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model $f$ to accurately represent any smaller target model $\overline{f}$ if LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of }\overline{f}}{\text{depth of }f}$. We also quantify the approximation error when LoRA-rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
    
[^11]: CompeteAI:理解基于大型语言模型的智能体竞争行为

    CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents. (arXiv:2310.17512v1 [cs.AI])

    [http://arxiv.org/abs/2310.17512](http://arxiv.org/abs/2310.17512)

    本文研究了基于大型语言模型的智能体之间的竞争行为。通过建立一个竞争环境并进行实验，发现竞争可以促使智能体进行转变和采取新策略，从而在社会和经济发展中发挥重要作用。

    

    大型语言模型（LLM）被广泛应用于完成不同任务，如个人助理或事件规划。虽然大多数工作都集中在智能体之间的合作与协作，但很少有研究探索另一个重要机制——竞争，它是社会和经济发展的推动力之一。本文旨在研究LLM智能体之间的竞争行为。我们首先提出一个通用框架来研究智能体之间的竞争。然后，我们使用GPT-4实现了一个实际的竞争环境，模拟了一个由餐馆智能体和顾客智能体组成的虚拟城镇。具体而言，餐馆智能体相互竞争以吸引更多顾客，这种竞争促使它们进行转变，比如培养新的运营策略。我们实验的结果揭示了从社会学习到马太效应等多个有趣发现，与现有的社会学和经济学理论相一致。

    Large language models (LLMs) have been widely used as agents to complete different tasks, such as personal assistance or event planning. While most work has focused on cooperation and collaboration between agents, little work explores competition, another important mechanism that fosters the development of society and economy. In this paper, we seek to examine the competition behaviors in LLM-based agents. We first propose a general framework to study the competition between agents. Then, we implement a practical competitive environment using GPT-4 to simulate a virtual town with two types of agents, including restaurant agents and customer agents. Specifically, restaurant agents compete with each other to attract more customers, where the competition fosters them to transform, such as cultivating new operating strategies. The results of our experiments reveal several interesting findings ranging from social learning to Matthew Effect, which aligns well with existing sociological and e
    
[^12]: 基于模拟器辅助的移动边缘调优的AI基础模型编排：一种多智能体深度强化学习方法

    Orchestration of Emulator Assisted Mobile Edge Tuning for AI Foundation Models: A Multi-Agent Deep Reinforcement Learning Approach. (arXiv:2310.17492v1 [cs.AI])

    [http://arxiv.org/abs/2310.17492](http://arxiv.org/abs/2310.17492)

    本研究提出了一种基于模拟器辅助的移动边缘调优的AI基础模型编排方法，该方法通过创新的模拟器适配器架构和混合多智能体深度强化学习策略，实现了高效部署和精调基础模型，从而提高了本地任务性能。

    

    在当代人工智能中，高效部署和精调基础模型至关重要。本研究提出一种突破性的范式，将移动边缘计算（MEC）与基础模型集成，旨在增强用户设备上的本地任务性能。我们的方法的核心是创新的模拟器适配器架构，将基础模型分割为两个协同模块。这种设计不仅节省了计算资源，还确保了适应性和下游任务的精调效率。此外，我们引入了一种先进的资源分配机制，针对去中心化环境中的模拟器适配器结构的需求进行精调。为应对该系统所面临的挑战，我们采用了一种混合多智能体深度强化学习（DRL）策略，擅长处理混合离散-连续动作空间，确保动态和最优资源分配。我们进行了全面的模拟研究来验证我们的方法。

    The efficient deployment and fine-tuning of foundation models are pivotal in contemporary artificial intelligence. In this study, we present a groundbreaking paradigm integrating Mobile Edge Computing (MEC) with foundation models, specifically designed to enhance local task performance on user equipment (UE). Central to our approach is the innovative Emulator-Adapter architecture, segmenting the foundation model into two cohesive modules. This design not only conserves computational resources but also ensures adaptability and fine-tuning efficiency for downstream tasks. Additionally, we introduce an advanced resource allocation mechanism that is fine-tuned to the needs of the Emulator-Adapter structure in decentralized settings. To address the challenges presented by this system, we employ a hybrid multi-agent Deep Reinforcement Learning (DRL) strategy, adept at handling mixed discrete-continuous action spaces, ensuring dynamic and optimal resource allocations. Our comprehensive simula
    
[^13]: 提高通过减少无关文档对开放领域问答中的零样本阅读器的干扰的方法

    Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering. (arXiv:2310.17490v1 [cs.CL])

    [http://arxiv.org/abs/2310.17490](http://arxiv.org/abs/2310.17490)

    本研究提出了一种通过减少无关文档的干扰来改善开放领域问答中的零样本阅读器的方法。采用了干扰感知的答案选择(DAS)方法，以解决LLMs受到干扰和过度自信的问题。实验结果表明，该方法成功地改善了零样本阅读器的性能，并展现出了优越的可迁移性。

    

    大型语言模型(LLMs)使得在开放领域问答(ODQA)中实现零样本方法成为可能，但是由于阅读器相对于检索器的进展有限。本研究旨在探讨一种零样本阅读器的可行性，以解决计算成本和标注数据需求等挑战。我们发现LLMs由于检索到的无关文档以及作为零样本阅读器时生成答案的过度自信而受到干扰。为了解决这些问题，我们采用了基于否定的指令和分数调整的干扰感知的答案选择(DAS)方法，以减轻这些文档的影响。实验结果表明，我们的方法成功地处理了不同场景下的干扰，提高了零样本阅读器的性能。此外，与面对未见过数据而困难重重的监督式阅读器不同，零样本阅读器展现出了优越的可迁移性，无需任何训练。

    Large language models (LLMs) enable zero-shot approaches in open-domain question answering (ODQA), yet with limited advancements as the reader is compared to the retriever. This study aims at the feasibility of a zero-shot reader that addresses the challenges of computational cost and the need for labeled data. We find that LLMs are distracted due to irrelevant documents in the retrieved set and the overconfidence of the generated answers when they are exploited as zero-shot readers. To tackle these problems, we mitigate the impact of such documents via Distraction-aware Answer Selection (DAS) with a negation-based instruction and score adjustment for proper answer selection. Experimental results show that our approach successfully handles distraction across diverse scenarios, enhancing the performance of zero-shot readers. Furthermore, unlike supervised readers struggling with unseen data, zero-shot readers demonstrate outstanding transferability without any training.
    
[^14]: 评估过程中的偏见：基于优化的模型

    Bias in Evaluation Processes: An Optimization-Based Model. (arXiv:2310.17489v1 [cs.CY])

    [http://arxiv.org/abs/2310.17489](http://arxiv.org/abs/2310.17489)

    本研究提出了一个基于优化的模型，用于评估过程中的偏见分析。模型通过将真实效用分布转化为观察到的分布来考虑偏见，并对参数进行研究，以探究其对观察到的分布的影响。通过实证验证和数据拟合，我们证明了该模型的有效性。

    

    在诸如招生和招聘等设置中，关于个人社会显著属性的偏见已被广泛记录在评估过程中。我们将这样的评估过程视为将个人对任务的真实效用分布转化为观察到的分布，并将其建模为在信息约束下的损失最小化问题的解。我们的模型具有两个参数，被视为导致偏见的因素：信息约束中的资源信息交换参数和损失函数中的风险厌恶参数。我们表征了由我们的模型产生的分布，并研究了参数对观察到的分布的影响。我们模型的输出丰富了可用于捕捉观察评估中群组间变化的分布类。我们通过拟合真实世界数据集来进行实证验证，并使用它来研究介入效果。

    Biases with respect to socially-salient attributes of individuals have been well documented in evaluation processes used in settings such as admissions and hiring. We view such an evaluation process as a transformation of a distribution of the true utility of an individual for a task to an observed distribution and model it as a solution to a loss minimization problem subject to an information constraint. Our model has two parameters that have been identified as factors leading to biases: the resource-information trade-off parameter in the information constraint and the risk-averseness parameter in the loss function. We characterize the distributions that arise from our model and study the effect of the parameters on the observed distribution. The outputs of our model enrich the class of distributions that can be used to capture variation across groups in the observed evaluations. We empirically validate our model by fitting real-world datasets and use it to study the effect of interve
    
[^15]: 使用物理运动定律从2D标注中学习单目3D物体定位

    Towards Learning Monocular 3D Object Localization From 2D Labels using the Physical Laws of Motion. (arXiv:2310.17462v1 [cs.CV])

    [http://arxiv.org/abs/2310.17462](http://arxiv.org/abs/2310.17462)

    该论文提出了一种新的方法，利用物体运动的物理知识和简单的2D标注，从单目图像中实现精确的3D物体定位，无需昂贵的3D标注。经过实验证明，在真实数据实验中平均距离误差仅为6厘米，表明该方法在无法收集3D数据进行训练的情况下具有潜力。

    

    我们提出了一种新的方法，利用物体的运动物理知识和简单的2D标注，从单个校准相机的单个图像中精确定位3D物体，无需昂贵的3D标注。在训练过程中，模型能够推断出隐含的第三个维度，即使在训练时从未见过此信息。我们在合成和真实世界数据集上评估了该方法，并在真实数据实验中实现了平均距离误差仅为6厘米。结果表明该方法在无法收集3D数据进行训练的情况下，作为学习3D物体定位估计的一步具有潜力。

    We present a novel method for precise 3D object localization in single images from a single calibrated camera using only 2D labels. No expensive 3D labels are needed. Thus, instead of using 3D labels, our model is trained with easy-to-annotate 2D labels along with the physical knowledge of the object's motion. Given this information, the model can infer the latent third dimension, even though it has never seen this information during training. Our method is evaluated on both synthetic and real-world datasets, and we are able to achieve a mean distance error of just 6 cm in our experiments on real data. The results indicate the method's potential as a step towards learning 3D object location estimation, where collecting 3D data for training is not feasible.
    
[^16]: 通过理解生成：具有逻辑符号基础的神经视觉生成

    Generating by Understanding: Neural Visual Generation with Logical Symbol Groundings. (arXiv:2310.17451v1 [cs.AI])

    [http://arxiv.org/abs/2310.17451](http://arxiv.org/abs/2310.17451)

    这篇论文提出了一种神经符号学习方法，AbdGen，用于将知识推理系统与神经视觉生成模型集成。它解决了符号赋值和规则学习的问题，通过量化诱导方法实现可靠高效的符号赋值，通过对比元诱导方法实现精确的规则学习。

    

    尽管近年来神经视觉生成模型取得了很大的成功，但将其与强大的符号知识推理系统集成仍然是一个具有挑战性的任务。主要挑战有两个方面：一个是符号赋值，即将神经视觉生成器的潜在因素与知识推理系统中的有意义的符号进行绑定。另一个是规则学习，即学习新的规则，这些规则控制数据的生成过程，以增强知识推理系统。为了解决这些符号基础问题，我们提出了一种神经符号学习方法，Abductive Visual Generation (AbdGen)，用于基于诱导学习框架将逻辑编程系统与神经视觉生成模型集成起来。为了实现可靠高效的符号赋值，引入了量化诱导方法，通过语义编码本中的最近邻查找生成诱导提案。为了实现精确的规则学习，引入了对比元诱导方法。

    Despite the great success of neural visual generative models in recent years, integrating them with strong symbolic knowledge reasoning systems remains a challenging task. The main challenges are two-fold: one is symbol assignment, i.e. bonding latent factors of neural visual generators with meaningful symbols from knowledge reasoning systems. Another is rule learning, i.e. learning new rules, which govern the generative process of the data, to augment the knowledge reasoning systems. To deal with these symbol grounding problems, we propose a neural-symbolic learning approach, Abductive Visual Generation (AbdGen), for integrating logic programming systems with neural visual generative models based on the abductive learning framework. To achieve reliable and efficient symbol assignment, the quantized abduction method is introduced for generating abduction proposals by the nearest-neighbor lookups within semantic codebooks. To achieve precise rule learning, the contrastive meta-abduction
    
[^17]: LSA64：阿根廷手语数据集

    LSA64: An Argentinian Sign Language Dataset. (arXiv:2310.17429v1 [cs.CV])

    [http://arxiv.org/abs/2310.17429](http://arxiv.org/abs/2310.17429)

    这篇论文介绍了一个来自阿根廷手语的数据集LSA64。该数据集包含3200个视频，包括64个不同的阿根廷手语手势，为构建一个全面的阿根廷手势研究级数据集迈出了第一步。

    

    自动手语识别是涵盖人机交互、计算机视觉和机器学习的研究领域。稳健的自动手语识别可以帮助翻译过程和融合听障人士，同时也可以教授听觉人群手语。不同国家甚至地区的手语差异显著,它们的语法和语义与书面语言也不同。尽管对于不同语言的自动手语识别技术大致相同，但要训练一个新语言的识别系统，就需要有这个语言的整个数据集。本文提出了一个来自阿根廷手语（LSA）的64个手势的数据集LSA64。这个数据集包含由10个受试者录制的64个不同LSA手势的3200个视频，是构建阿根廷手势全面研究级数据集的第一步。

    Automatic sign language recognition is a research area that encompasses human-computer interaction, computer vision and machine learning. Robust automatic recognition of sign language could assist in the translation process and the integration of hearing-impaired people, as well as the teaching of sign language to the hearing population. Sign languages differ significantly in different countries and even regions, and their syntax and semantics are different as well from those of written languages. While the techniques for automatic sign language recognition are mostly the same for different languages, training a recognition system for a new language requires having an entire dataset for that language. This paper presents a dataset of 64 signs from the Argentinian Sign Language (LSA). The dataset, called LSA64, contains 3200 videos of 64 different LSA signs recorded by 10 subjects, and is a first step towards building a comprehensive research-level dataset of Argentinian signs, specific
    
[^18]: 使用ProbSom进行阿根廷手语手势识别的研究

    Handshape recognition for Argentinian Sign Language using ProbSom. (arXiv:2310.17427v1 [cs.CV])

    [http://arxiv.org/abs/2310.17427](http://arxiv.org/abs/2310.17427)

    本文的两个主要贡献是：首次创建了一个针对阿根廷手语的手势数据库，并提出了一种使用ProbSom进行图像处理和手势分类的技术，该技术在目前的研究中与其他方法进行了对比。

    

    自动手语识别是人机交互和机器学习领域的重要课题。这既是一个复杂的挑战，需要涉及视频处理、图像处理、智能系统和语言学等多个知识领域的介入，又是一种强大的手语识别，可以辅助翻译过程和融入听障人士的整合。本文提出了两个主要贡献：首先，创建了一个针对阿根廷手语（LSA）的手势数据库，这是一个迄今为止几乎没有讨论过的话题。其次，利用一种名为ProbSom的自组织映射的监督自适应技术，进行图像处理、特征提取和后续手势分类。该技术与支持向量机（SVM）、随机森林和神经网络等现有技术进行了比较。

    Automatic sign language recognition is an important topic within the areas of human-computer interaction and machine learning. On the one hand, it poses a complex challenge that requires the intervention of various knowledge areas, such as video processing, image processing, intelligent systems and linguistics. On the other hand, robust recognition of sign language could assist in the translation process and the integration of hearing-impaired people.  This paper offers two main contributions: first, the creation of a database of handshapes for the Argentinian Sign Language (LSA), which is a topic that has barely been discussed so far. Secondly, a technique for image processing, descriptor extraction and subsequent handshape classification using a supervised adaptation of self-organizing maps that is called ProbSom. This technique is compared to others in the state of the art, such as Support Vector Machines (SVM), Random Forests, and Neural Networks.  The database that was built conta
    
[^19]: 动作运动的分布描述器（DAM）：用于人类动作识别的描述器

    Distribution of Action Movements (DAM): A Descriptor for Human Action Recognition. (arXiv:2310.17421v1 [cs.CV])

    [http://arxiv.org/abs/2310.17421](http://arxiv.org/abs/2310.17421)

    本文提出了一种新的动作描述器，动作运动的分布描述器（DAM），通过计算关节运动方向分布的归一化直方图来表示动作。该描述器在多个著名数据集上超过了几种最新技术。

    

    从骨骼数据中进行人类动作识别是一个重要且活跃的研究领域，在许多著名数据集上，目前的最新技术仍未能达到近乎完美的准确性。本文介绍了一种新颖的动作描述器，即动作运动的分布描述器（DAM），它基于关节在帧之间运动的方向分布，通过计算在数据集中所有可能运动的方向集合上的归一化直方图来表示。该描述器通过聚类获得一组代表性的关节运动方向，并通过应用窗口方案部分保留其时间结构。与标准分类器相结合，该描述器在许多著名数据集上的表现超过了几种最新技术。

    Human action recognition from skeletal data is an important and active area of research in which the state of the art has not yet achieved near-perfect accuracy on many well-known datasets. In this paper, we introduce the Distribution of Action Movements Descriptor, a novel action descriptor based on the distribution of the directions of the motions of the joints between frames, over the set of all possible motions in the dataset. The descriptor is computed as a normalized histogram over a set of representative directions of the joints, which are in turn obtained via clustering. While the descriptor is global in the sense that it represents the overall distribution of movement directions of an action, it is able to partially retain its temporal structure by applying a windowing scheme.  The descriptor, together with a standard classifier, outperforms several state-of-the-art techniques on many well-known datasets.
    
[^20]: 目标足够：在IMF中引导未知多智能体系统之间的即时合作

    Goals are Enough: Inducing AdHoc cooperation among unseen Multi-Agent systems in IMFs. (arXiv:2310.17416v1 [cs.AI])

    [http://arxiv.org/abs/2310.17416](http://arxiv.org/abs/2310.17416)

    该论文提出了一种在IMF中引导未知多智能体系统之间即时合作的方法，通过对意图驱动的管理进行层次协调，避免了重新训练整个系统的成本和时间开销。

    

    意图驱动的管理在下一代移动网络中将发挥关键作用，传统方法无法有效管理资源，因为它们倾向于独立处理每个期望。现有方法（如基于多智能体强化学习的方法）在网络切片上存在冲突的期望时可以有效地分配资源。然而，实际上，系统往往更加复杂，无法仅通过单独的多智能体强化学习来解决。通常存在着意图实现的层次结构，多个预训练的、自利的智能体可能需要由一个监控或控制智能体进一步协调。这些智能体可能以adhoc方式进入系统，需要与其他可用的智能体一起协调。基于整个系统的重新训练往往是不可行的，因为需要相关的时间和成本。鉴于这些挑战，需要协调预训练智能体的adhoc协调。

    Intent-based management will play a critical role in achieving customers' expectations in the next-generation mobile networks. Traditional methods cannot perform efficient resource management since they tend to handle each expectation independently. Existing approaches, e.g., based on multi-agent reinforcement learning (MARL) allocate resources in an efficient fashion when there are conflicting expectations on the network slice. However, in reality, systems are often far more complex to be addressed by a standalone MARL formulation. Often there exists a hierarchical structure of intent fulfilment where multiple pre-trained, self-interested agents may need to be further orchestrated by a supervisor or controller agent. Such agents may arrive in the system adhoc, which then needs to be orchestrated along with other available agents. Retraining the whole system every time is often infeasible given the associated time and cost. Given the challenges, such adhoc coordination of pre-trained s
    
[^21]: PETA: 评估亚词切分对蛋白质迁移学习在下游应用中的影响

    PETA: Evaluating the Impact of Protein Transfer Learning with Sub-word Tokenization on Downstream Applications. (arXiv:2310.17415v1 [cs.CL])

    [http://arxiv.org/abs/2310.17415](http://arxiv.org/abs/2310.17415)

    PETA通过评估亚词切分在蛋白质迁移学习中的影响，提出了关于蛋白质语言模型的综合评估方法，并发现词汇表大小在50以上能够获得最佳性能。

    

    大规模的蛋白质语言模型擅长捕捉原始结构中的进化信息，对蛋白质工程具有重要实用价值。与自然语言模型相比，蛋白质氨基酸序列的数据量较小，组合空间有限。选择合适的词汇表大小来优化预训练模型是一个关键问题。此外，尽管自然语言领域拥有大量的基准测试和研究，但目前还缺乏一个全面评估蛋白质语言模型质量的基准。鉴于这些挑战，PETA使用了三种标记化方法，在14种不同的词汇表大小下训练语言模型。它在33个不同的下游数据集上进行了数千次测试，评估了模型的迁移学习能力，并结合了两个分类头和三个随机种子以减轻潜在偏见。大量实验表明，词汇表大小在50以上大约能够获得最佳的性能。

    Large protein language models are adept at capturing the underlying evolutionary information in primary structures, offering significant practical value for protein engineering. Compared to natural language models, protein amino acid sequences have a smaller data volume and a limited combinatorial space. Choosing an appropriate vocabulary size to optimize the pre-trained model is a pivotal issue. Moreover, despite the wealth of benchmarks and studies in the natural language community, there remains a lack of a comprehensive benchmark for systematically evaluating protein language model quality. Given these challenges, PETA trained language models with 14 different vocabulary sizes under three tokenization methods. It conducted thousands of tests on 33 diverse downstream datasets to assess the models' transfer learning capabilities, incorporating two classification heads and three random seeds to mitigate potential biases. Extensive experiments indicate that vocabulary sizes between 50 
    
[^22]: 在度量时序逻辑中合成高效的可监控公式

    Synthesizing Efficiently Monitorable Formulas in Metric Temporal Logic. (arXiv:2310.17410v1 [cs.AI])

    [http://arxiv.org/abs/2310.17410](http://arxiv.org/abs/2310.17410)

    本论文提出了一种自动化合成从系统执行中得到形式规范的方法，通过将合成任务简化为线性实数算术问题的可满足性问题，并设计了一种学习算法，合成了具有有限先行查看的简洁公式。

    

    在运行时验证中，手动为监控系统执行规范化一个规范是一项繁琐且容易出错的过程。为了解决这个问题，我们考虑了从系统执行中自动合成形式化规范的问题。为了展示我们的方法，我们考虑了流行的规范语言度量时序逻辑（MTL），该语言特别适用于为网络物理系统（CPS）指定时间属性。大多数传统的合成时序逻辑公式的方法旨在最小化公式的大小。然而，对于监控的效率而言，除了大小，规范所需的“先行查看”量也变得相关，尤其对于安全关键应用来说。我们形式化了这个概念，并设计了一种学习算法，用于合成具有有限先行查看的简洁公式。为此，我们的算法将合成任务简化为一系列线性实数算术问题的可满足性问题。

    In runtime verification, manually formalizing a specification for monitoring system executions is a tedious and error-prone process. To address this issue, we consider the problem of automatically synthesizing formal specifications from system executions. To demonstrate our approach, we consider the popular specification language Metric Temporal Logic (MTL), which is particularly tailored towards specifying temporal properties for cyber-physical systems (CPS). Most of the classical approaches for synthesizing temporal logic formulas aim at minimizing the size of the formula. However, for efficiency in monitoring, along with the size, the amount of "lookahead" required for the specification becomes relevant, especially for safety-critical applications. We formalize this notion and devise a learning algorithm that synthesizes concise formulas having bounded lookahead. To do so, our algorithm reduces the synthesis task to a series of satisfiability problems in Linear Real Arithmetic (LRA)
    
[^23]: 神经网络的不变性测量

    Invariance Measures for Neural Networks. (arXiv:2310.17404v1 [cs.LG])

    [http://arxiv.org/abs/2310.17404](http://arxiv.org/abs/2310.17404)

    本文提出了一种用于量化神经网络不变性的测量方法，该方法敏感且可解释，并能应用于任何神经网络模型。在仿射变换领域和CIFAR10和MNIST数据集上的验证表明，神经网络的内部不变性对于随机权重初始化非常稳定，但对于数据集或变换的改变不稳定。这些测量方法将为不变性表示的新研究方向提供可能性。

    

    神经网络中的不变性对许多任务都是有用且必要的。然而，大多数神经网络模型中的不变性表示尚未被明确表征。我们提出了一种量化神经网络不变性的测量方法，该方法基于其内部表示。这些测量方法高效且可解释，并可应用于任何神经网络模型。与之前定义的测量方法相比，它们对不变性更为敏感。我们在仿射变换领域和CIFAR10和MNIST数据集上验证了这些测量方法及其属性，包括其稳定性和可解释性。利用这些测量方法，我们对CNN模型进行了首次分析，并展示了它们的内部不变性对于随机权重初始化非常稳定，但对于数据集或变换的改变不稳定。我们相信这些测量方法将为不变性表示的新研究方向提供可能性。

    Invariances in neural networks are useful and necessary for many tasks. However, the representation of the invariance of most neural network models has not been characterized. We propose measures to quantify the invariance of neural networks in terms of their internal representation. The measures are efficient and interpretable, and can be applied to any neural network model. They are also more sensitive to invariance than previously defined measures. We validate the measures and their properties in the domain of affine transformations and the CIFAR10 and MNIST datasets, including their stability and interpretability. Using the measures, we perform a first analysis of CNN models and show that their internal invariance is remarkably stable to random weight initializations, but not to changes in dataset or transformation. We believe the measures will enable new avenues of research in invariance representation.
    
[^24]: ToxicChat: 揭示实际用户-AI对话中的毒性检测隐藏挑战

    ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation. (arXiv:2310.17389v1 [cs.CL])

    [http://arxiv.org/abs/2310.17389](http://arxiv.org/abs/2310.17389)

    本研究引入了ToxicChat，一个基于实际用户查询构建的新型毒性检测基准。该基准揭示了当前毒性检测模型在实际用户-AI对话中面临的困难，强调了实际对话中存在的独特挑战。

    

    尽管大型语言模型在聊天机器人方面取得了显著的进展，但如今维持一个非毒性的用户-AI互动环境变得越来越重要。然而，先前的毒性检测工作大多基于社交媒体内容导出的基准，未充分探索实际用户-AI互动中固有的独特挑战。本研究引入了ToxicChat，这是一个基于开源聊天机器人的实际用户查询构建的新型基准。该基准包含了对当前毒性检测模型难以识别的丰富而微妙的现象，相对于社交媒体内容来说存在显著的领域差异。我们对在现有毒性数据集上训练的模型进行了系统评估，发现它们在应用于ToxicChat的这个独特领域时存在缺点。我们的研究揭示了实际用户-AI对话中毒性检测可能被忽视的挑战。将来，ToxicChat的研究可以推动更好的毒性检测方法和工具的发展。

    Despite remarkable advances that large language models have achieved in chatbots, maintaining a non-toxic user-AI interactive environment has become increasingly critical nowadays. However, previous efforts in toxicity detection have been mostly based on benchmarks derived from social media content, leaving the unique challenges inherent to real-world user-AI interactions insufficiently explored. In this work, we introduce ToxicChat, a novel benchmark based on real user queries from an open-source chatbot. This benchmark contains the rich, nuanced phenomena that can be tricky for current toxicity detection models to identify, revealing a significant domain difference compared to social media content. Our systematic evaluation of models trained on existing toxicity datasets has shown their shortcomings when applied to this unique domain of ToxicChat. Our work illuminates the potentially overlooked challenges of toxicity detection in real-world user-AI conversations. In the future, Toxic
    
[^25]: YOLO-BEV：以与2D物体检测相同的方式生成鸟瞰图

    YOLO-BEV: Generating Bird's-Eye View in the Same Way as 2D Object Detection. (arXiv:2310.17379v1 [cs.CV])

    [http://arxiv.org/abs/2310.17379](http://arxiv.org/abs/2310.17379)

    YOLO-BEV是一个高效的框架，利用独特的摄像头设置生成车辆环境的2D鸟瞰图。它采用了YOLO的检测机制，并通过自定义设计的检测头部将全景数据转化为统一的鸟瞰视图地图。这种方法在实时车辆感知任务中具有实际可行性。

    

    车辆感知系统旨在通过全面快速地解释其周围环境以改善安全性和导航能力。我们介绍了YOLO-BEV，这是一个高效的框架，利用独特的周围摄像头设置生成车辆环境的2D鸟瞰图。通过在45度间隔处策略性地放置八个摄像头，我们的系统捕获并整合图像到一个连贯的3x3网格格式，中心留空，提供了一个丰富的空间表示，便于高效处理。在我们的方法中，我们采用了YOLO的检测机制，借助其快速响应和紧凑的模型结构的固有优势。我们没有使用常规的YOLO检测头部，而是使用自定义设计的检测头部，将全景捕获的数据转化为统一的自车鸟瞰视图地图。初步结果验证了YOLO-BEV在实时车辆感知任务中的可行性。

    Vehicle perception systems strive to achieve comprehensive and rapid visual interpretation of their surroundings for improved safety and navigation. We introduce YOLO-BEV, an efficient framework that harnesses a unique surrounding cameras setup to generate a 2D bird's-eye view of the vehicular environment. By strategically positioning eight cameras, each at a 45-degree interval, our system captures and integrates imagery into a coherent 3x3 grid format, leaving the center blank, providing an enriched spatial representation that facilitates efficient processing. In our approach, we employ YOLO's detection mechanism, favoring its inherent advantages of swift response and compact model structure. Instead of leveraging the conventional YOLO detection head, we augment it with a custom-designed detection head, translating the panoramically captured data into a unified bird's-eye view map of ego car. Preliminary results validate the feasibility of YOLO-BEV in real-time vehicular perception ta
    
[^26]: 基于切向空间中的灵敏度的ReLU网络的优化相关泛化界限

    Optimization dependent generalization bound for ReLU networks based on sensitivity in the tangent bundle. (arXiv:2310.17378v1 [cs.LG])

    [http://arxiv.org/abs/2310.17378](http://arxiv.org/abs/2310.17378)

    本文通过估计梯度下降得到的初始参数向量中可用的网络集合的Rademacher复杂度，提出了一个关于前向ReLU网络泛化误差的PAC类型界限，通过限制网络梯度对于输入数据在优化轨迹上的扰动的灵敏度，不显式地依赖网络的深度。

    

    近年来，深度学习取得了一些非常有希望的结果，对于深度神经网络的泛化能力，然而文献仍然缺乏一个全面的理论解释为什么过度参数化的模型能够很好地泛化，同时拟合训练数据。在本文中，我们通过估计梯度下降得到的初始参数向量中可用的网络集合的Rademacher复杂度，提出了一个关于前向ReLU网络泛化误差的PAC类型界限。关键思想是将网络梯度对于输入数据在优化轨迹上的扰动的灵敏度限制在一个界限内。所得到的界限不显式地依赖网络的深度。我们的结果在MNIST和CIFAR-10数据集上得到实验证实。

    Recent advances in deep learning have given us some very promising results on the generalization ability of deep neural networks, however literature still lacks a comprehensive theory explaining why heavily over-parametrized models are able to generalize well while fitting the training data. In this paper we propose a PAC type bound on the generalization error of feedforward ReLU networks via estimating the Rademacher complexity of the set of networks available from an initial parameter vector via gradient descent. The key idea is to bound the sensitivity of the network's gradient to perturbation of the input data along the optimization trajectory. The obtained bound does not explicitly depend on the depth of the network. Our results are experimentally verified on the MNIST and CIFAR-10 datasets.
    
[^27]: 基于大型语言模型的对话式生成自动驾驶仿真场景

    Dialogue-based generation of self-driving simulation scenarios using Large Language Models. (arXiv:2310.17372v1 [cs.AI])

    [http://arxiv.org/abs/2310.17372](http://arxiv.org/abs/2310.17372)

    本文介绍了一个系统，使用大型语言模型将用户的英文话语映射为领域特定代码，以支持对话式生成自动驾驶仿真场景，并探索了语言模型所能捕捉到的上下文敏感性。

    

    仿真是开发和评估自动驾驶汽车控制器的无价工具。当前的仿真框架基于高度专业的领域特定语言，因此自然语言界面将极大地增加可用性。但是，英文简洁用语和捕捉用户意图的可执行代码之间经常存在一定的隐含假设差距。本文描述了一个系统来解决这个问题，通过支持扩展的多模态交互，用户可以用修正或修改来跟进之前的指令，以对迄今为止从他们的话语生成的仿真作出反应。我们使用大型语言模型（LLMs）将用户的英文话语在这种互动中映射到特定领域的代码，因此我们探索了LLMs能否捕捉到计算发言者在话语中的预期信息所需的上下文敏感性。

    Simulation is an invaluable tool for developing and evaluating controllers for self-driving cars. Current simulation frameworks are driven by highly-specialist domain specific languages, and so a natural language interface would greatly enhance usability. But there is often a gap, consisting of tacit assumptions the user is making, between a concise English utterance and the executable code that captures the user's intent. In this paper we describe a system that addresses this issue by supporting an extended multimodal interaction: the user can follow up prior instructions with refinements or revisions, in reaction to the simulations that have been generated from their utterances so far. We use Large Language Models (LLMs) to map the user's English utterances in this interaction into domain-specific code, and so we explore the extent to which LLMs capture the context sensitivity that's necessary for computing the speaker's intended message in discourse.
    
[^28]: 探索生成型人工智能在万维网领域的潜力

    Exploring the Potential of Generative AI for the World Wide Web. (arXiv:2310.17370v1 [cs.AI])

    [http://arxiv.org/abs/2310.17370](http://arxiv.org/abs/2310.17370)

    本文探索了生成型人工智能在万维网领域的潜力，特别关注图像生成。我们开发了WebDiffusion工具来模拟基于稳定扩散的万维网，并评估了生成图像质量。

    

    生成型人工智能（AI）是一种先进的技术，利用生成模型和用户提示能够生成文本、图像和各种媒体内容。在2022年到2023年期间，生成型人工智能在AI电影到聊天机器人等众多应用领域迅速增长。本文深入探讨了生成型人工智能在万维网领域的潜力，特别关注图像生成。网络开发人员已经利用生成型人工智能来辅助编写文本和图像，而网络浏览器未来可能会使用它来本地生成图像，以修复损坏的网页、节省带宽和增强隐私保护。为了探索这一研究领域，我们开发了一款名为WebDiffusion的工具，可以从客户端和服务器的角度模拟由稳定扩散（一种流行的文本到图像模型）驱动的万维网。WebDiffusion还支持用户意见的众包，我们利用这一功能评估了生成图像质量。

    Generative Artificial Intelligence (AI) is a cutting-edge technology capable of producing text, images, and various media content leveraging generative models and user prompts. Between 2022 and 2023, generative AI surged in popularity with a plethora of applications spanning from AI-powered movies to chatbots. In this paper, we delve into the potential of generative AI within the realm of the World Wide Web, specifically focusing on image generation. Web developers already harness generative AI to help crafting text and images, while Web browsers might use it in the future to locally generate images for tasks like repairing broken webpages, conserving bandwidth, and enhancing privacy. To explore this research area, we have developed WebDiffusion, a tool that allows to simulate a Web powered by stable diffusion, a popular text-to-image model, from both a client and server perspective. WebDiffusion further supports crowdsourcing of user opinions, which we use to evaluate the quality and 
    
[^29]: 食谱的文化适应

    Cultural Adaptation of Recipes. (arXiv:2310.17353v1 [cs.CL])

    [http://arxiv.org/abs/2310.17353](http://arxiv.org/abs/2310.17353)

    本研究介绍了一项涉及中餐和英语国家菜系之间食谱的翻译和文化适应的新任务，提供了一个独特的数据集并评估了多种方法的性能。

    

    在大型语言模型（LLM）取得显著进展的基础上，我们现在有能力解决更复杂的任务，需要对跨文化环境有一个细致的理解。一个关键的例子是食谱的适应，这超出了简单的翻译，还包括对某个特定文化的食材、烹饪技巧和膳食偏好的掌握。我们介绍了一个涉及中餐和英语国家菜系之间食谱的翻译和文化适应的新任务。为了支持这项调查，我们提供了CulturalRecipes，这是一个由自动配对的中文和英文食谱构成的独特数据集。该数据集还通过人工编写和精心策划的测试集进行了丰富。在这个复杂的跨文化食谱适应任务中，我们评估了各种方法的性能，包括GPT-4和其他LLM、传统机器翻译和信息检索技术。我们的综合分析包括自动化和人工评估方面的内容。

    Building upon the considerable advances in Large Language Models (LLMs), we are now equipped to address more sophisticated tasks demanding a nuanced understanding of cross-cultural contexts. A key example is recipe adaptation, which goes beyond simple translation to include a grasp of ingredients, culinary techniques, and dietary preferences specific to a given culture. We introduce a new task involving the translation and cultural adaptation of recipes between Chinese and English-speaking cuisines. To support this investigation, we present CulturalRecipes, a unique dataset comprised of automatically paired recipes written in Mandarin Chinese and English. This dataset is further enriched with a human-written and curated test set. In this intricate task of cross-cultural recipe adaptation, we evaluate the performance of various methods, including GPT-4 and other LLMs, traditional machine translation, and information retrieval techniques. Our comprehensive analysis includes both automati
    
[^30]: CQM：具有量化的世界模型的课程增强学习

    CQM: Curriculum Reinforcement Learning with a Quantized World Model. (arXiv:2310.17330v1 [cs.LG])

    [http://arxiv.org/abs/2310.17330](http://arxiv.org/abs/2310.17330)

    CQM提出了一种新的课程增强学习方法，通过自动定义语义目标空间和提出课程目标，在解决复杂任务中取得了显著进展。该方法通过向量量化-变分自动编码器(VQ-VAE)将连续观察结果进行离散化，并通过图形恢复离散观察结果之间的时间关系，提供了不确定性和时间距离感知的课程目标。

    

    最近的课程增强学习在解决复杂任务方面取得了显著进展，通过提出一系列代理任务的顺序来实现。然而，先前的方法在生成高维空间中的课程目标时经常面临挑战。因此，它们通常依赖于手动指定的目标空间。为了减轻这个限制并提高课程的可扩展性，我们提出了一种新的课程方法，它自动定义包含课程过程的关键信息的语义目标空间，并在其上提出课程目标。为了定义语义目标空间，我们的方法通过向量量化-变分自动编码器(VQ-VAE)将连续观察结果进行离散化，并通过图形恢复离散观察结果之间的时间关系。与此同时，我们的方法提出了不确定性和时间距离感知的课程目标，这些目标在自动组合的目标空间中收敛到最终目标。我们证明了所提出的方法

    Recent curriculum Reinforcement Learning (RL) has shown notable progress in solving complex tasks by proposing sequences of surrogate tasks. However, the previous approaches often face challenges when they generate curriculum goals in a high-dimensional space. Thus, they usually rely on manually specified goal spaces. To alleviate this limitation and improve the scalability of the curriculum, we propose a novel curriculum method that automatically defines the semantic goal space which contains vital information for the curriculum process, and suggests curriculum goals over it. To define the semantic goal space, our method discretizes continuous observations via vector quantized-variational autoencoders (VQ-VAE) and restores the temporal relations between the discretized observations by a graph. Concurrently, ours suggests uncertainty and temporal distance-aware curriculum goals that converges to the final goals over the automatically composed goal space. We demonstrate that the propose
    
[^31]: C-Disentanglement: 带有混淆因子归纳偏差的因果独立生成因子的发现

    C-Disentanglement: Discovering Causally-Independent Generative Factors under an Inductive Bias of Confounder. (arXiv:2310.17325v1 [cs.LG])

    [http://arxiv.org/abs/2310.17325](http://arxiv.org/abs/2310.17325)

    本文提出了一个名为C-Disentanglement的框架，旨在发现具有因果关系且受混淆因素影响的生成因子，以提高数据生成的可控性和鲁棒性。

    

    表征学习假设实际世界的数据是由几个语义上有意义的生成因子（即变异源）产生的，并旨在在潜在空间中发现它们。这些因子被期望是因果上解缠的，意味着不同的因子被编码为单独的潜在变量，并且一个因子的变化不会影响其他因子的值。与统计独立性相比，因果解缠允许更可控的数据生成，提高了鲁棒性和更好的泛化性能。然而，大多数现有工作在发现过程中假设没有混淆因素，即生成因子没有共同的原因，因此只获得统计独立性。在本文中，我们认识到在发现因果生成因子中建立混淆因素的模型的重要性。然而，如果没有适当的归纳偏差，这些因素是无法识别的。我们通过引入一个名为混淆-解缠（C-Di）的框架来填补这一空白。

    Representation learning assumes that real-world data is generated by a few semantically meaningful generative factors (i.e., sources of variation) and aims to discover them in the latent space. These factors are expected to be causally disentangled, meaning that distinct factors are encoded into separate latent variables, and changes in one factor will not affect the values of the others. Compared to statistical independence, causal disentanglement allows more controllable data generation, improved robustness, and better generalization. However, most existing work assumes unconfoundedness in the discovery process, that there are no common causes to the generative factors and thus obtain only statistical independence. In this paper, we recognize the importance of modeling confounders in discovering causal generative factors. Unfortunately, such factors are not identifiable without proper inductive bias. We fill the gap by introducing a framework entitled Confounded-Disentanglement (C-Di
    
[^32]: FormaT5: 以自然语言生成条件表格格式化的抽样和示例

    FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language. (arXiv:2310.17306v1 [cs.AI])

    [http://arxiv.org/abs/2310.17306](http://arxiv.org/abs/2310.17306)

    FormaT5是一个基于转换器的模型，可以根据目标表格和自然语言描述生成数据相关的条件格式规则。为了解决描述不足的问题，FormaT5通过放弃目标的方式学习预测占位符。

    

    表格的格式化是可视化、展示和分析中的重要属性。电子表格软件允许用户通过编写数据相关的条件格式规则来自动格式化表格。但对用户来说，编写这样的规则通常是具有挑战性的，因为它要求他们理解和实现底层逻辑。我们提出了一个基于转换器的模型FormaT5，可以根据目标表格和期望的格式逻辑的自然语言描述生成一个条件格式规则。我们发现，用户为这些任务提供的描述通常是不明确或含糊的，这使得代码生成系统难以在一步中准确学习到所需的规则。为了解决这个规范不足的问题并减少参数错误，FormaT5通过放弃目标的方式学习预测占位符。这些占位符可以由第二个模型或者当可用的行示例时，由一个基于示例的编程系统填充。

    Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires them to understand and implement the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system
    
[^33]: 比较逼真和动画形象的交互式对话代理在严肃游戏中的用户体验：一项关于用户体验的实证研究

    Comparing Photorealistic and Animated Embodied Conversational Agents in Serious Games: An Empirical Study on User Experience. (arXiv:2310.17300v1 [cs.HC])

    [http://arxiv.org/abs/2310.17300](http://arxiv.org/abs/2310.17300)

    本文通过比较逼真和动画形象的交互式对话代理在严肃游戏中的表现，提供了关于基于语音的ECAs设计的洞察和建议。结果显示逼真和动画形象版本都具有很高的可用性，但大多数参与者更喜欢逼真版本。

    

    交互式对话代理（ECAs）是以具象化角色形式呈现的对话用户界面的范例。本文重点研究了两种不同级别的表现逼真度，即逼真和动画形象。研究旨在提供关于基于语音的ECAs在严肃游戏环境中的洞察和设计建议。研究采用了一个完全组内、二因素设计，共有36名性别平衡的参与者。结果显示，逼真和动画形象的版本都被认为具有很高的可用性，其平均得分分别为5.76和5.71。然而，69.4％的参与者表示他们更喜欢逼真版本，25％的参与者表示他们更喜欢动画版本，还有5.6％的参与者没有表明偏好。逼真代理被认为更真实和类人。

    Embodied conversational agents (ECAs) are paradigms of conversational user interfaces in the form of embodied characters. While ECAs offer various manipulable features, this paper focuses on a study conducted to explore two distinct levels of presentation realism. The two agent versions are photorealistic and animated. The study aims to provide insights and design suggestions for speech-enabled ECAs within serious game environments. A within-subjects, two-by-two factorial design was employed for this research with a cohort of 36 participants balanced for gender. The results showed that both the photorealistic and the animated versions were perceived as highly usable, with overall mean scores of 5.76 and 5.71, respectively. However, 69.4 per cent of the participants stated they preferred the photorealistic version, 25 per cent stated they preferred the animated version and 5.6 per cent had no stated preference. The photorealistic agents were perceived as more realistic and human-like, w
    
[^34]: 基于属性的生成模型可解释性评估指标

    Attribute Based Interpretable Evaluation Metrics for Generative Models. (arXiv:2310.17261v1 [cs.CV])

    [http://arxiv.org/abs/2310.17261](http://arxiv.org/abs/2310.17261)

    本论文提出了一种基于属性的生成模型可解释性评估指标，通过度量生成图像集与训练集关于属性强度分布的差异，可以更好地衡量模型生成结果与训练数据的相似度。

    

    当训练数据集中狗和猫的比例为1:1时，生成模型生成的狗和猫也应更好地符合训练数据集的分布。然而，现有的评估指标只提供了“多样性”这个解释性之外的维度。在这个背景下，我们提出了一种新的评估协议，通过度量生成图像集与训练集关于属性强度分布的差异来捕捉这种现象。单属性差异（SaD）衡量了关于单个属性的概率密度函数的差异。双属性差异（PaD）衡量了关于一对属性的联合概率密度函数的差异。它们提供了模型所面临的困难属性。为了衡量图像的属性强度，我们提出了异构CLIP评分（HCS），它通过测量图像和文本向量之间的余弦相似度来实现。

    When the training dataset comprises a 1:1 proportion of dogs to cats, a generative model that produces 1:1 dogs and cats better resembles the training species distribution than another model with 3:1 dogs and cats. Can we capture this phenomenon using existing metrics? Unfortunately, we cannot, because these metrics do not provide any interpretability beyond "diversity". In this context, we propose a new evaluation protocol that measures the divergence of a set of generated images from the training set regarding the distribution of attribute strengths as follows. Single-attribute Divergence (SaD) measures the divergence regarding PDFs of a single attribute. Paired-attribute Divergence (PaD) measures the divergence regarding joint PDFs of a pair of attributes. They provide which attributes the models struggle. For measuring the attribute strengths of an image, we propose Heterogeneous CLIPScore (HCS) which measures the cosine similarity between image and text vectors with heterogeneous 
    
[^35]: IDENAS: 内部依赖性探索用于神经架构搜索

    IDENAS: Internal Dependency Exploration for Neural Architecture Search. (arXiv:2310.17250v1 [cs.LG])

    [http://arxiv.org/abs/2310.17250](http://arxiv.org/abs/2310.17250)

    IDENAS是一种集成神经架构搜索和特征选择的方法，通过探索内部依赖性来提高分类任务的性能。

    

    机器学习是从不同数据集中提取有价值信息和进行各种预测的强大工具。传统算法依赖于明确定义的输入和输出变量，然而，在某些情况下，输入和输出变量之间的区别以及模型的底层关联（输入和输出）层是未知的。神经架构搜索（NAS）和特征选择已成为这些场景中的有希望的解决方案。该研究提出了IDENAS，一种基于内部依赖性的神经架构搜索方法，将NAS与特征选择相结合。该方法在涉及1D传感器和2D图像数据的分类问题中探索了完整的参数空间的内部依赖性。IDENAS采用了修改的编码器-解码器模型和顺序前向搜索（SFS）算法，将输入-输出配置搜索与嵌入式特征选择相结合。实验结果证明了IDENAS的优越性能。

    Machine learning is a powerful tool for extracting valuable information and making various predictions from diverse datasets. Traditional algorithms rely on well-defined input and output variables however, there are scenarios where the distinction between the input and output variables and the underlying, associated (input and output) layers of the model, are unknown. Neural Architecture Search (NAS) and Feature Selection have emerged as promising solutions in such scenarios. This research proposes IDENAS, an Internal Dependency-based Exploration for Neural Architecture Search, integrating NAS with feature selection. The methodology explores internal dependencies in the complete parameter space for classification involving 1D sensor and 2D image data as well. IDENAS employs a modified encoder-decoder model and the Sequential Forward Search (SFS) algorithm, combining input-output configuration search with embedded feature selection. Experimental results demonstrate IDENASs superior perf
    
[^36]: CROP: 保守奖励用于基于模型的离线策略优化

    CROP: Conservative Reward for Model-based Offline Policy Optimization. (arXiv:2310.17245v1 [cs.LG])

    [http://arxiv.org/abs/2310.17245](http://arxiv.org/abs/2310.17245)

    CROP提出了一种保守奖励的模型训练方法用于基于模型的离线策略优化，通过同时最小化估计误差和随机动作奖励来实现保守的奖励估计。

    

    离线强化学习旨在使用收集到的数据进行策略优化，而无需进行在线交互。基于模型的方法在解决离线强化学习挑战方面特别有吸引力，因为它们能够通过使用模型生成数据来缓解离线数据的限制。之前的研究表明，在策略优化过程中将保守性引入模型或Q函数可以有效缓解离线强化学习中普遍存在的分布漂移问题。然而，关于奖励估计中保守性的影响的研究仍然不足。本文提出了一种新颖的基于模型的离线强化学习算法CROP，该算法在模型训练中保守地估计奖励。为了实现保守的奖励估计，CROP同时最小化估计误差和随机动作的奖励。理论分析表明，这种保守的奖励机制导致...（文章摘要未完，下同）

    Offline reinforcement learning (RL) aims to optimize policy using collected data without online interactions. Model-based approaches are particularly appealing for addressing offline RL challenges due to their capability to mitigate the limitations of offline data through data generation using models. Prior research has demonstrated that introducing conservatism into the model or Q-function during policy optimization can effectively alleviate the prevalent distribution drift problem in offline RL. However, the investigation into the impacts of conservatism in reward estimation is still lacking. This paper proposes a novel model-based offline RL algorithm, Conservative Reward for model-based Offline Policy optimization (CROP), which conservatively estimates the reward in model training. To achieve a conservative reward estimation, CROP simultaneously minimizes the estimation error and the reward of random actions. Theoretical analysis shows that this conservative reward mechanism leads 
    
[^37]: 通过span剪枝和超图神经网络实现联合实体和关系抽取

    Joint Entity and Relation Extraction with Span Pruning and Hypergraph Neural Networks. (arXiv:2310.17238v1 [cs.CL])

    [http://arxiv.org/abs/2310.17238](http://arxiv.org/abs/2310.17238)

    本文提出了基于超图神经网络的联合实体和关系抽取方法，使用span剪枝机制减轻误差传播问题，通过构建超图进行高阶建模，实现多个实体和关系之间的交互。

    

    实体和关系抽取（ERE）是信息提取中的重要任务，最近基于标记的流水线模型取得了最先进的性能，但仍然存在误差传播问题。此外，大多数当前的ERE模型在多个实体和关系之间不考虑高阶交互，而高阶建模可能会有益处。在这项工作中，我们提出了超图神经网络（HGNN）用于ERE，它是建立在PL-marker（最先进的基于标记的流水线模型）之上的。为了减轻误差传播，我们使用高召回剪枝器机制将实体的识别和标注负担从NER模块转移到我们模型的联合模块。对于高阶建模，我们构建了一个超图，其中节点是实体（由span剪枝器提供），以及其关系，并且超边编码了两个不同关系之间或关系与其相关的主体和宾语实体之间的交互。接着我们运行...

    Entity and Relation Extraction (ERE) is an important task in information extraction. Recent marker-based pipeline models achieve state-of-the-art performance, but still suffer from the error propagation issue. Also, most of current ERE models do not take into account higher-order interactions between multiple entities and relations, while higher-order modeling could be beneficial.In this work, we propose HyperGraph neural network for ERE ($\hgnn{}$), which is built upon the PL-marker (a state-of-the-art marker-based pipleline model). To alleviate error propagation,we use a high-recall pruner mechanism to transfer the burden of entity identification and labeling from the NER module to the joint module of our model. For higher-order modeling, we build a hypergraph, where nodes are entities (provided by the span pruner) and relations thereof, and hyperedges encode interactions between two different relations or between a relation and its associated subject and object entities. We then run
    
[^38]: TST$^\mathrm{R}$: 目标相似度调整遇见现实世界

    TST$^\mathrm{R}$: Target Similarity Tuning Meets the Real World. (arXiv:2310.17228v1 [cs.AI])

    [http://arxiv.org/abs/2310.17228](http://arxiv.org/abs/2310.17228)

    本文提出了在现实世界中应用和改进目标相似度调整（TST）的不同方法，包括使用更大的模型嵌入、训练一个小模型转换嵌入以匹配代码相似度，并介绍了高效选择训练样例和基于排名的评估方法。

    

    目标相似度调整（TST）是一种通过大型语言模型（LLM）在自然语言（NL）到代码生成中选择相关例子以提升性能的方法。其目标是使得句子嵌入模型适应两个NL输入的相似度与其相关代码输出的相似度匹配。本文提出了不同的方法来应用和改进TST在现实世界中的使用。首先，我们将句子转换器替换为更大模型的嵌入，从而降低对语言分布的敏感性，增加了合成示例的灵活性，并训练一个小模型将这些嵌入转换到一个空间中，其中嵌入相似度匹配代码相似度，使得模型保持黑箱状态，并在推断时只需进行少量矩阵乘法。其次，我们介绍了如何高效地选择较少数量的训练样例来训练TST模型。第三，我们引入了基于排名的评估方法。

    Target similarity tuning (TST) is a method of selecting relevant examples in natural language (NL) to code generation through large language models (LLMs) to improve performance. Its goal is to adapt a sentence embedding model to have the similarity between two NL inputs match the similarity between their associated code outputs. In this paper, we propose different methods to apply and improve TST in the real world. First, we replace the sentence transformer with embeddings from a larger model, which reduces sensitivity to the language distribution and thus provides more flexibility in synthetic generation of examples, and we train a tiny model that transforms these embeddings to a space where embedding similarity matches code similarity, which allows the model to remain a black box and only requires a few matrix multiplications at inference time. Second, we how to efficiently select a smaller number of training examples to train the TST model. Third, we introduce a ranking-based evalu
    
[^39]: 超越MLE: 用于文本生成的凸学习方法

    Beyond MLE: Convex Learning for Text Generation. (arXiv:2310.17217v1 [cs.CL])

    [http://arxiv.org/abs/2310.17217](http://arxiv.org/abs/2310.17217)

    本论文提出了一种基于凸函数的训练目标类，超越了传统的最大似然估计方法。该方法适用于闭合型文本生成任务，并能够使得模型生成更加合适的响应。

    

    最大似然估计（MLE）是一种统计方法，用于估计最能解释观测数据的概率分布参数。在文本生成的背景下，MLE经常用于训练生成型语言模型，从而生成新的文本。然而，我们认为MLE并不总是必要且最优的，尤其是对于闭合型文本生成任务，如机器翻译。在这些任务中，模型的目标是生成最合适的响应，这并不一定需要使用MLE来估计整个数据分布。为此，我们提出了一种基于凸函数的新型训练目标类，使得文本生成模型能够集中于高概率的输出，而不需要估计整个数据分布。我们研究了在将凸函数应用于损失函数时的最优预测分布的理论性质，证明了凸函数可以使得最优预测分布变得更加锐利。

    Maximum likelihood estimation (MLE) is a statistical method used to estimate the parameters of a probability distribution that best explain the observed data. In the context of text generation, MLE is often used to train generative language models, which can then be used to generate new text. However, we argue that MLE is not always necessary and optimal, especially for closed-ended text generation tasks like machine translation. In these tasks, the goal of model is to generate the most appropriate response, which does not necessarily require it to estimate the entire data distribution with MLE. To this end, we propose a novel class of training objectives based on convex functions, which enables text generation models to focus on highly probable outputs without having to estimate the entire data distribution. We investigate the theoretical properties of the optimal predicted distribution when applying convex functions to the loss, demonstrating that convex functions can sharpen the opt
    
[^40]: 视频情感识别综述

    Emotion Recognition by Video: A review. (arXiv:2310.17212v1 [cs.CV])

    [http://arxiv.org/abs/2310.17212](http://arxiv.org/abs/2310.17212)

    本文是一篇关于视频情感识别的综述论文，总结了相关研究中的现有趋势、情感模型、数据库以及单模态和多模态的视频情感识别方法的结构、性能和优缺点。

    

    视频情感识别是情感计算的重要分支，其解决方案可应用于人机交互（HCI）和智能医疗等领域。尽管情感识别领域发表的论文数量正在增加，但很少有全面的综述报道相关的视频情感识别研究。因此，本文选择了2015年至2023年发表的文章，系统总结了相关研究中视频情感识别的现有趋势。本文首先讨论了两种典型的情感模型，然后介绍了经常用于视频情感识别的单模态数据库和多模态数据库。接下来，我们研究和分类了现代单模态和多模态视频情感识别方法的具体结构和性能，并讨论了每种方法的优缺点，并详细比较了它们的表现。此外，

    Video emotion recognition is an important branch of affective computing, and its solutions can be applied in different fields such as human-computer interaction (HCI) and intelligent medical treatment. Although the number of papers published in the field of emotion recognition is increasing, there are few comprehensive literature reviews covering related research on video emotion recognition. Therefore, this paper selects articles published from 2015 to 2023 to systematize the existing trends in video emotion recognition in related studies. In this paper, we first talk about two typical emotion models, then we talk about databases that are frequently utilized for video emotion recognition, including unimodal databases and multimodal databases. Next, we look at and classify the specific structure and performance of modern unimodal and multimodal video emotion recognition methods, talk about the benefits and drawbacks of each, and then we compare them in detail in the tables. Further, we
    
[^41]: 使用Tsetlin机器进行高效数据融合

    Efficient Data Fusion using the Tsetlin Machine. (arXiv:2310.17207v1 [cs.AI])

    [http://arxiv.org/abs/2310.17207](http://arxiv.org/abs/2310.17207)

    这项研究提出了一种使用Tsetlin机器进行高效数据融合的新方法，通过监测学习到的逻辑子句在动态数据中的变化，识别和融合噪声，并在实验中展示出高性能。

    

    我们提出了一种使用Tsetlin Machine评估和融合嘈杂动态数据的新方法。我们的方法是通过监测Tsetlin Machine学习到的逻辑子句在动态数据中可能存在的噪声情况下的变化来识别和融合噪声。通过降低先前学习到的子句的权重或以新的子句形式反映噪声。我们还进行了一项全面的实验研究，使用了不同的数据集，结果显示出所提出方法的高性能。

    We propose a novel way of assessing and fusing noisy dynamic data using a Tsetlin Machine. Our approach consists in monitoring how explanations in form of logical clauses that a TM learns changes with possible noise in dynamic data. This way TM can recognize the noise by lowering weights of previously learned clauses, or reflect it in the form of new clauses. We also perform a comprehensive experimental study using notably different datasets that demonstrated high performance of the proposed approach.
    
[^42]: 用网络控制变量驯服联邦学习中的梯度方差

    Taming Gradient Variance in Federated Learning with Networked Control Variates. (arXiv:2310.17200v1 [cs.LG])

    [http://arxiv.org/abs/2310.17200](http://arxiv.org/abs/2310.17200)

    本论文提出了一种名为FedNCV的新型框架，用于解决联邦学习中梯度方差的问题。通过在客户端和服务器级别实现REINFORCE Leave-One-Out (RLOO)作为控制变量单元，优化了本地梯度更新并提供了无偏和低方差的聚合梯度，实现了稳健的全局更新。

    

    联邦学习是一种分散式的机器学习方法，面临着诸如沟通开销大、收敛速度慢和改进不稳定等重要挑战。这些挑战主要源于由异构客户端数据分布引起的梯度方差。为了解决这个问题，我们引入了一种新颖的网络控制变量（FedNCV）框架来进行联邦学习。我们在FedNCV框架的客户端和服务器级别都采用REINFORCE Leave-One-Out（RLOO）作为基本控制变量单元。在客户端级别，RLOO控制变量用于优化本地梯度更新，减轻数据样本引入的方差。一旦传输到服务器，基于RLOO的估计量进一步提供了无偏和低方差的聚合梯度，实现了稳健的全局更新。这种双侧应用被形式化为组合控制变量的线性组合。我们提供了一个数学公式来捕捉这个过程。

    Federated learning, a decentralized approach to machine learning, faces significant challenges such as extensive communication overheads, slow convergence, and unstable improvements. These challenges primarily stem from the gradient variance due to heterogeneous client data distributions. To address this, we introduce a novel Networked Control Variates (FedNCV) framework for Federated Learning. We adopt the REINFORCE Leave-One-Out (RLOO) as a fundamental control variate unit in the FedNCV framework, implemented at both client and server levels. At the client level, the RLOO control variate is employed to optimize local gradient updates, mitigating the variance introduced by data samples. Once relayed to the server, the RLOO-based estimator further provides an unbiased and low-variance aggregated gradient, leading to robust global updates. This dual-side application is formalized as a linear combination of composite control variates. We provide a mathematical expression capturing this i
    
[^43]: 语言模型如何将实体绑定到上下文中?

    How do Language Models Bind Entities in Context?. (arXiv:2310.17191v1 [cs.LG])

    [http://arxiv.org/abs/2310.17191](http://arxiv.org/abs/2310.17191)

    通过分析语言模型的表示，我们发现了绑定ID机制，它可以将实体与属性进行有效地绑定。我们通过因果干预实验进一步证明了语言模型内部激活表示绑定信息的方式。研究结果揭示了语言模型在上下文中如何表示符号知识，从而为理解大规模语言模型的一般上下文推理提供了指导。

    

    为了正确使用上下文信息，语言模型（LMs）必须将实体与其属性进行绑定。例如，给定描述“绿色方块”和“蓝色圆形”的上下文，LMs必须将形状与它们对应的颜色进行绑定。我们分析LM表示并确定绑定ID机制：这是一种解决绑定问题的通用机制，我们在Pythia和LLaMA家族的每个足够大的模型中观察到。通过因果干预，我们展示了LMs内部激活通过将绑定ID向量附加到相应的实体和属性上来表示绑定信息。我们进一步展示了绑定ID向量形成连续的子空间，在这个子空间中，绑定ID向量之间的距离反映了它们的区别。总体而言，我们的结果揭示了LMs在上下文中表示符号知识的可解释策略，为理解大规模LMs中的一般上下文推理迈出了一步。

    To correctly use in-context information, language models (LMs) must bind entities to their attributes. For example, given a context describing a "green square" and a "blue circle", LMs must bind the shapes to their respective colors. We analyze LM representations and identify the binding ID mechanism: a general mechanism for solving the binding problem, which we observe in every sufficiently large model from the Pythia and LLaMA families. Using causal interventions, we show that LMs' internal activations represent binding information by attaching binding ID vectors to corresponding entities and attributes. We further show that binding ID vectors form a continuous subspace, in which distances between binding ID vectors reflect their discernability. Overall, our results uncover interpretable strategies in LMs for representing symbolic knowledge in-context, providing a step towards understanding general in-context reasoning in large-scale LMs.
    
[^44]: 理解投影仪对知识蒸馏的影响

    Understanding the Effects of Projectors in Knowledge Distillation. (arXiv:2310.17183v1 [cs.CV])

    [http://arxiv.org/abs/2310.17183](http://arxiv.org/abs/2310.17183)

    投影仪在知识蒸馏中有助于提高性能，并且可以在维度匹配和逻辑蒸馏的情况下起到改善效果。学生使用投影仪可以在训练准确性和测试准确性之间取得更好的平衡。

    

    在传统的知识蒸馏过程中，由于教师和学生网络之间的维度不匹配，通常需要额外的投影仪来进行特征转换。有趣的是，我们发现即使学生和教师具有相同的特征维度，添加投影仪仍然有助于提高蒸馏性能。此外，如果我们在架构中添加投影仪，它们甚至可以改善逻辑蒸馏的效果。鉴于这些令人惊讶的发现以及现有文献对知识蒸馏过程中投影仪的理解的普遍缺乏，本文研究了投影仪在知识蒸馏中的隐含作用，迄今为止这个问题一直被忽视。我们的实证研究表明，当学生具有与教师相同的特征维度时，使用投影仪的学生在训练准确性和测试准确性之间取得了更好的平衡。

    Conventionally, during the knowledge distillation process (e.g. feature distillation), an additional projector is often required to perform feature transformation due to the dimension mismatch between the teacher and the student networks. Interestingly, we discovered that even if the student and the teacher have the same feature dimensions, adding a projector still helps to improve the distillation performance. In addition, projectors even improve logit distillation if we add them to the architecture too. Inspired by these surprising findings and the general lack of understanding of the projectors in the knowledge distillation process from existing literature, this paper investigates the implicit role that projectors play but so far have been overlooked. Our empirical study shows that the student with a projector (1) obtains a better trade-off between the training accuracy and the testing accuracy compared to the student without a projector when it has the same feature dimensions as th
    
[^45]: 图形化的以对象为中心的Actor-Critic算法

    Graphical Object-Centric Actor-Critic. (arXiv:2310.17178v1 [cs.AI])

    [http://arxiv.org/abs/2310.17178](http://arxiv.org/abs/2310.17178)

    这项研究提出了一种新颖的以对象为中心的强化学习算法，将演员-评论家和基于模型的方法结合起来，利用解耦的对象表示有效地学习策略。该方法填补了以对象为中心的强化学习环境中高效且适用于离散或连续动作空间的世界模型的研究空白。

    

    最近在无监督的以对象为中心的表示学习及其在下游任务中的应用方面取得了重要进展。最新的研究支持这样一个观点，即在基于图像的以对象为中心的强化学习任务中采用解耦的对象表示能够促进策略学习。我们提出了一种新颖的以对象为中心的强化学习算法，将演员-评论家算法和基于模型的方法结合起来，以有效利用这些表示。在我们的方法中，我们使用一个变换器编码器来提取对象表示，并使用图神经网络来近似环境的动力学。所提出的方法填补了开发强化学习环境中可以用于离散或连续动作空间的高效以对象为中心的世界模型的研究空白。我们的算法在一个具有复杂视觉3D机器人环境和一个具有组合结构的2D环境中表现更好。

    There have recently been significant advances in the problem of unsupervised object-centric representation learning and its application to downstream tasks. The latest works support the argument that employing disentangled object representations in image-based object-centric reinforcement learning tasks facilitates policy learning. We propose a novel object-centric reinforcement learning algorithm combining actor-critic and model-based approaches to utilize these representations effectively. In our approach, we use a transformer encoder to extract object representations and graph neural networks to approximate the dynamics of an environment. The proposed method fills a research gap in developing efficient object-centric world models for reinforcement learning settings that can be used for environments with discrete or continuous action spaces. Our algorithm performs better in a visually complex 3D robotic environment and a 2D environment with compositional structure than the state-of-t
    
[^46]: 通过遮蔽微调来弥合标记修剪和完整预训练之间的差距

    Bridging The Gaps Between Token Pruning and Full Pre-training via Masked Fine-tuning. (arXiv:2310.17177v1 [cs.CV])

    [http://arxiv.org/abs/2310.17177](http://arxiv.org/abs/2310.17177)

    本文提出了一种遮蔽微调方法，用于弥合标记修剪和完整预训练之间的差距，该方法通过遮蔽图像块并预测图像类别标签来实现动态视觉transformers的初始化和加速推理。

    

    尽管transformers在各种计算机视觉任务中取得了成功，但它们存在内存和计算成本过高的问题。一些工作提出了动态视觉transformers来通过修剪多余的标记来加速推理。改进标记修剪的关键是使用训练有素的模型作为初始化，以实现更快的收敛和更好的性能。然而，当前的基本模型通常采用完整的图像训练，即使用完整的图像作为输入，并在前向过程中保留整个特征图，这与逐渐减少标记的动态模型存在不一致，包括计算模式、信息量和标记选择策略上的不一致。受到执行遮蔽和重构自监督任务的MAE的启发，我们设计了遮蔽微调来弥合用于初始化的预训练基本模型与基于标记修剪的动态视觉transformers之间的差距，通过遮蔽图像块并预测图像类别标签来完成微调。

    Despite the success of transformers on various computer vision tasks, they suffer from excessive memory and computational cost. Some works present dynamic vision transformers to accelerate inference by pruning redundant tokens. A key to improving token pruning is using well-trained models as initialization for faster convergence and better performance. However, current base models usually adopt full image training, i.e., using full images as inputs and keeping the whole feature maps through the forward process, which causes inconsistencies with dynamic models that gradually reduce tokens, including calculation pattern, information amount and token selection strategy inconsistencies. Inspired by MAE which performs masking and reconstruction self-supervised task, we devise masked fine-tuning to bridge the gaps between pre-trained base models used for initialization and token pruning based dynamic vision transformers, by masking image patches and predicting the image class label based on 
    
[^47]: 从全景X射线中进行牙齿分割和定位的深度学习方法

    A Deep Learning Approach to Teeth Segmentation and Orientation from Panoramic X-rays. (arXiv:2310.17176v1 [cs.CV])

    [http://arxiv.org/abs/2310.17176](http://arxiv.org/abs/2310.17176)

    本研究提出了一个利用深度学习技术从全景X射线图像中进行牙齿分割和定位的方法。我们通过修改已有模型并引入注意力机制，实现了高精度和高性能的牙齿分割和定位。在公开数据集上的评估结果表明，我们的方法在牙齿实例分割和牙齿定位方面取得了优异的性能。

    

    准确的牙齿分割和定位在现代口腔保健中是基础，可实现精确诊断、治疗计划和牙齿种植设计。本研究提出了一种综合的方法，利用深度学习技术从全景X射线图像中进行牙齿分割和定位。我们根据FUSegNet构建了我们的模型，这是一种最初用于创面分割的流行模型，并通过将基于网格的注意力门引入跳跃连接进行了修改。我们通过主成分分析（PCA）引入定向边界框（OBB）生成，以实现精确的牙齿定位估计。在公开可获得的DNS数据集上评估我们的方法，该数据集包括543个全景X射线图像，我们在牙齿实例分割中得到了最高的交并比（IoU）得分82.43%，Dice相似系数（DSC）得分90.37%，在OBB分析中，我们获得了旋转的交并比（RIoU）得分82.82%。

    Accurate teeth segmentation and orientation are fundamental in modern oral healthcare, enabling precise diagnosis, treatment planning, and dental implant design. In this study, we present a comprehensive approach to teeth segmentation and orientation from panoramic X-ray images, leveraging deep learning techniques. We build our model based on FUSegNet, a popular model originally developed for wound segmentation, and introduce modifications by incorporating grid-based attention gates into the skip connections. We introduce oriented bounding box (OBB) generation through principal component analysis (PCA) for precise tooth orientation estimation. Evaluating our approach on the publicly available DNS dataset, comprising 543 panoramic X-ray images, we achieve the highest Intersection-over-Union (IoU) score of 82.43% and Dice Similarity Coefficient (DSC) score of 90.37% among compared models in teeth instance segmentation. In OBB analysis, we obtain the Rotated IoU (RIoU) score of 82.82%. We
    
[^48]: 通过同时估计图像和噪声改进去噪扩散模型

    Improving Denoising Diffusion Models via Simultaneous Estimation of Image and Noise. (arXiv:2310.17167v1 [cs.LG])

    [http://arxiv.org/abs/2310.17167](http://arxiv.org/abs/2310.17167)

    通过重新参数化扩散过程并直接估计图像和噪声，本文改进了去噪扩散模型，提高了图像生成的速度和质量。

    

    本文介绍了两个关键的贡献，旨在通过反向扩散过程生成的图像的速度和质量。第一个贡献是通过以图像和噪声之间的四分之一圆弧上的角度重新参数化扩散过程，特别是设置传统的 $\displaystyle \sqrt{\bar{\alpha}}=\cos(\eta)$。这种重新参数化消除了两个奇异点，并允许将扩散演化表达为一个良好行为的常微分方程（ODE）。从而，可以有效地使用更高阶的ODE求解器，如Runge-Kutta方法。第二个贡献是直接使用我们的网络估计图像（$\mathbf{x}_0$）和噪声（$\mathbf{\epsilon}$），这使得逆向扩散过程中的更新步骤计算更加稳定，因为在过程的不同阶段准确估计图像和噪声都是至关重要的。在这些变化的基础上，我们的模型实现了...

    This paper introduces two key contributions aimed at improving the speed and quality of images generated through inverse diffusion processes. The first contribution involves reparameterizing the diffusion process in terms of the angle on a quarter-circular arc between the image and noise, specifically setting the conventional $\displaystyle \sqrt{\bar{\alpha}}=\cos(\eta)$. This reparameterization eliminates two singularities and allows for the expression of diffusion evolution as a well-behaved ordinary differential equation (ODE). In turn, this allows higher order ODE solvers such as Runge-Kutta methods to be used effectively. The second contribution is to directly estimate both the image ($\mathbf{x}_0$) and noise ($\mathbf{\epsilon}$) using our network, which enables more stable calculations of the update step in the inverse diffusion steps, as accurate estimation of both the image and noise are crucial at different stages of the process. Together with these changes, our model achie
    
[^49]: 基于内容的音乐大语言建模的控制

    Content-based Controls For Music Large Language Modeling. (arXiv:2310.17162v1 [cs.AI])

    [http://arxiv.org/abs/2310.17162](http://arxiv.org/abs/2310.17162)

    该论文提出了一种基于内容的控制方法，用于音乐大语言建模。通过对音高、和弦和鼓乐等固有音乐语言的直接控制，实现了高质量的音乐生成，并且使用了参数高效微调的方法，比原始模型的参数数量少于4%。

    

    近年来，在音乐音频领域出现了大规模语言模型的迅速增长。这些模型使得能够进行高质量音乐的端到端生成，并且一些模型可以使用文本描述进行条件生成。然而，文本在音乐上的控制能力本质上是有限的，因为它们只能通过元数据（如歌手和乐器）或高级表示（如流派和情感）间接地描述音乐。我们的目标是进一步提供对音高、和弦和鼓乐等固有音乐语言的直接和基于内容的控制能力。为此，我们提出了Coco-Mulla，这是一种用于音乐大语言建模的基于内容的控制方法。它使用了针对基于Transformer的音频模型量身定制的参数高效微调（PEFT）方法。实验表明，我们的方法在低资源半监督学习中实现了高质量的音乐生成，相比原始模型，参数调优的比例不到4%。

    Recent years have witnessed a rapid growth of large-scale language models in the domain of music audio. Such models enable end-to-end generation of higher-quality music, and some allow conditioned generation using text descriptions. However, the control power of text controls on music is intrinsically limited, as they can only describe music indirectly through meta-data (such as singers and instruments) or high-level representations (such as genre and emotion). We aim to further equip the models with direct and content-based controls on innate music languages such as pitch, chords and drum track. To this end, we contribute Coco-Mulla, a content-based control method for music large language modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transformer-based audio models. Experiments show that our approach achieved high-quality music generation with low-resource semi-supervised learning, tuning with less than 4% parameters compared to the original model and t
    
[^50]: CosmosDSR -- 一种使用无味卡尔曼滤波器自动检测和跟踪轨道碎片的方法论

    CosmosDSR -- a methodology for automated detection and tracking of orbital debris using the Unscented Kalman Filter. (arXiv:2310.17158v1 [astro-ph.EP])

    [http://arxiv.org/abs/2310.17158](http://arxiv.org/abs/2310.17158)

    CosmosDSR是一种新颖的方法，将YOLOv3与无味卡尔曼滤波器结合起来，用于自动检测和跟踪轨道碎片。在使用SPARK数据集进行训练和测试时，YOLOv3表现出很高的准确度，并且CosmosDSR和LKF都能准确地跟踪卫星。

    

    Kessler综合征是指频繁的太空活动产生的升级的太空碎片，威胁到未来的太空探索。解决这个问题至关重要。过去的研究强调了YOLO目标检测器和线性卡尔曼滤波器的组合用于物体检测和跟踪。在此基础上，我们的项目引入了CosmosDSR，一种新颖的方法，将YOLOv3与无味卡尔曼滤波器结合起来，用于在序列图像中跟踪卫星，与线性卡尔曼滤波器相比。使用卢森堡大学的SPARK数据集进行训练和测试，YOLOv3准确地检测和分类了所有卫星类别（mAP=97.18％，F1=0.95），出现了少量错误（TP=4163，FP=209，FN=237）。CosmosDSR和LKF都准确跟踪卫星（UKF：MSE=2.83/RMSE=1.66，LKF：...

    The Kessler syndrome refers to the escalating space debris from frequent space activities, threatening future space exploration. Addressing this issue is vital. Several AI models, including Convolutional Neural Networks (CNN), Kernel Principal Component Analysis (KPCA), and Model-Agnostic Meta-Learning (MAML), have been assessed with various data types. Earlier studies highlighted the combination of the YOLO object detector and a linear Kalman filter for object detection and tracking. Building on this, our project introduces CosmosDSR, a novel methodology combining YOLOv3 with an Unscented Kalman Filter for tracking satellites in sequential images, compared to a linear Kalman filter. Using the SPARK dataset from the University of Luxembourg for training and testing, the YOLOv3 precisely detected and classified all satellite categories (mAP=97.18%, F1=0.95) with few errors (TP=4163, FP=209, FN=237). Both CosmosDSR and the LKF tracked satellites accurately (UKF: MSE=2.83/RMSE=1.66, LKF: 
    
[^51]: 技术注解：将在低场MRI 0.55T膝关节MRI gesunden 控制者进行训练的3.0T深度学习分割模型直接应用

    Technical Note: Feasibility of translating 3.0T-trained Deep-Learning Segmentation Models Out-of-the-Box on Low-Field MRI 0.55T Knee-MRI of Healthy Controls. (arXiv:2310.17152v1 [cs.CV])

    [http://arxiv.org/abs/2310.17152](http://arxiv.org/abs/2310.17152)

    本研究评估了在0.55T低场MRI中对健康控制者膝关节进行标记物定量的深度学习技术的可行性，并表明这些技术在分割软骨区域方面与3.0T几乎相当。

    

    在本研究中，我们的目的是评估应用深度学习技术来量化健康控制者0.55T膝关节生物标记物的可行性，并与3.0T进行比较。该研究定性和定量地评估了0.55T下标准的骨骼和软骨分割算法的性能，比较了分割性能的差异、改进空间的区域以及0.55T和3.0T之间的软骨厚度值。初步结果表明，现有的基于深度学习的图像分割技术能够在0.55T的膝关节MRI中实现可行的转译，尤其在分割软骨区域方面，模型在Likert排名上表现几乎等同于3.0T。因此，0.55T低场可持续和易于安装的MRI可以用于评估健康控制者的膝关节。

    In the current study, our purpose is to evaluate the feasibility of applying deep learning (DL) enabled algorithms to quantify bilateral knee biomarkers in healthy controls scanned at 0.55T and compared with 3.0T. The current study assesses the performance of standard in-practice bone, and cartilage segmentation algorithms at 0.55T, both qualitatively and quantitatively, in terms of comparing segmentation performance, areas of improvement, and compartment-wise cartilage thickness values between 0.55T vs. 3.0T. Initial results demonstrate a usable to good technical feasibility of translating existing quantitative deep-learning-based image segmentation techniques, trained on 3.0T, out of 0.55T for knee MRI, in a multi-vendor acquisition environment. Especially in terms of segmenting cartilage compartments, the models perform almost equivalent to 3.0T in terms of Likert ranking. The 0.55T low-field sustainable and easy-to-install MRI, as demonstrated, thus, can be utilized for evaluating 
    
[^52]: 可解释的时空图神经网络

    Explainable Spatio-Temporal Graph Neural Networks. (arXiv:2310.17149v1 [cs.LG])

    [http://arxiv.org/abs/2310.17149](http://arxiv.org/abs/2310.17149)

    这篇论文提出了一种可解释的时空图神经网络（STExplainer）框架，通过增强时空图神经网络的可解释性，在城市资源分配和政策制定方面具有应用潜力。

    

    时空图神经网络在各种现实世界的城市应用中，包括智能交通和公共安全等，因其有效建模时空依赖性的能力而受到广泛关注。然而，时空图神经网络的黑盒性质限制了其可解释性，阻碍了其在城市资源分配和政策制定相关场景中的应用。为了弥合这一差距，我们提出了一个可解释的时空图神经网络（STExplainer）框架，通过增强STGNNs的内在可解释性，使其能够同时提供准确的预测和忠实的解释。我们的框架将统一的时空图注意力网络与位置信息融合层作为STG编码器和解码器进行集成。此外，我们基于图信息瓶颈（GIB）原则提出了一种具有可解释目标的结构蒸馏方法，该方法通过实例化GIB原则来提高可解释性。

    Spatio-temporal graph neural networks (STGNNs) have gained popularity as a powerful tool for effectively modeling spatio-temporal dependencies in diverse real-world urban applications, including intelligent transportation and public safety. However, the black-box nature of STGNNs limits their interpretability, hindering their application in scenarios related to urban resource allocation and policy formulation. To bridge this gap, we propose an Explainable Spatio-Temporal Graph Neural Networks (STExplainer) framework that enhances STGNNs with inherent explainability, enabling them to provide accurate predictions and faithful explanations simultaneously. Our framework integrates a unified spatio-temporal graph attention network with a positional information fusion layer as the STG encoder and decoder, respectively. Furthermore, we propose a structure distillation approach based on the Graph Information Bottleneck (GIB) principle with an explainable objective, which is instantiated by the
    
[^53]: 半离线策略评估中的反事实增强重要性抽样

    Counterfactual-Augmented Importance Sampling for Semi-Offline Policy Evaluation. (arXiv:2310.17146v1 [cs.LG])

    [http://arxiv.org/abs/2310.17146](http://arxiv.org/abs/2310.17146)

    提出了一种半离线评估框架，用于强化学习中的定量和定性评估。通过人类用户提供未被观察到的反事实轨迹的注释，设计了一种基于重要性抽样和加权的新型OPE估计器系列。

    

    在将强化学习应用于高风险领域时，使用观测数据进行定量和定性评估可以帮助从业人员了解新策略的泛化性能。然而，这种离网策略评估（OPE）在本质上存在限制，因为离线数据可能不反映由于应用新策略而导致的分布偏移。另一方面，通过根据新策略收集轨迹进行在线评估通常是不可行的，因为在这些领域部署新策略可能是不安全的。在这项工作中，我们提出了一种半离线评估框架，作为离线和在线评估之间的中间步骤，其中人类用户提供未被观察到的反事实轨迹的注释。虽然诱人地简单地用这些注释来增加现有数据，但我们表明这种天真的方法可能导致有偏的结果。相反，我们设计了一种基于重要性抽样（IS）和新颖加权的OPE估计器系列。

    In applying reinforcement learning (RL) to high-stakes domains, quantitative and qualitative evaluation using observational data can help practitioners understand the generalization performance of new policies. However, this type of off-policy evaluation (OPE) is inherently limited since offline data may not reflect the distribution shifts resulting from the application of new policies. On the other hand, online evaluation by collecting rollouts according to the new policy is often infeasible, as deploying new policies in these domains can be unsafe. In this work, we propose a semi-offline evaluation framework as an intermediate step between offline and online evaluation, where human users provide annotations of unobserved counterfactual trajectories. While tempting to simply augment existing data with such annotations, we show that this naive approach can lead to biased results. Instead, we design a new family of OPE estimators based on importance sampling (IS) and a novel weighting s
    
[^54]: 符号化规划和代码生成在基于实际对话中的应用

    Symbolic Planning and Code Generation for Grounded Dialogue. (arXiv:2310.17140v1 [cs.CL])

    [http://arxiv.org/abs/2310.17140](http://arxiv.org/abs/2310.17140)

    该论文介绍了一个模块化和可解释的基于实际对话系统，通过组合大型语言模型(LLMs)、符号化规划器和基于实际的代码执行来解决基于任务的对话中的挑战。实验证明该系统在协同参考解析任务上的性能显著优于先前的最先进技术。

    

    大型语言模型(LLMs)在处理和生成文本和代码方面表现出色。然而，在基于任务的对话中，LLMs的适用性有限，因为很难引导其朝着任务目标前进，并且无法处理新颖的基于实际对话。我们提出了一个模块化和可解释的基于实际对话系统，通过组合LLMs和符号化规划器以及基于实际的代码执行，解决了这些缺点。我们的系统由阅读器和规划器组成：阅读器利用LLMs将合作伙伴的话语转换为可执行的代码，调用执行基于实际的函数。转换后的代码的输出被存储以跟踪对话状态，而符号化规划器确定下一个适当的响应。我们在要求高的OneCommon对话任务上评估了我们系统的性能，该任务涉及解决散点图像的协同参考问题。我们的系统在先前的最先进技术的基础上实现了显著的性能提升，包括在人工评估中改善了任务成功率。

    Large language models (LLMs) excel at processing and generating both text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and interpretable grounded dialogue system that addresses these shortcomings by composing LLMs with a symbolic planner and grounded code execution. Our system consists of a reader and planner: the reader leverages an LLM to convert partner utterances into executable code, calling functions that perform grounding. The translated code's output is stored to track dialogue state, while a symbolic planner determines the next appropriate response. We evaluate our system's performance on the demanding OneCommon dialogue task, involving collaborative reference resolution on abstract images of scattered dots. Our system substantially outperforms the previous state-of-the-art, including improving task success in human evaluat
    
[^55]: Core Challenge 2023: 求解器和图描述

    Core Challenge 2023: Solver and Graph Descriptions. (arXiv:2310.17136v1 [cs.PL])

    [http://arxiv.org/abs/2310.17136](http://arxiv.org/abs/2310.17136)

    本文总结了 CoRe Challenge 2023 中提交的求解器和 ISR 实例的所有描述。

    

    本文收集了提交给 CoRe Challenge 2023 的求解器和 ISR 实例的所有描述。

    This paper collects all descriptions of solvers and ISR instances submitted to CoRe Challenge 2023.
    
[^56]: 通过视觉问答对将探测信号融入多模态机器翻译

    Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs. (arXiv:2310.17133v1 [cs.CL])

    [http://arxiv.org/abs/2310.17133](http://arxiv.org/abs/2310.17133)

    本文提出了一种通过视觉问答对的方式将探测信号融入多模态机器翻译中，以增强跨模态交互。实验证明了该方法的有效性。

    

    本文通过对多模态机器翻译(MMT)的深入研究，检验了MMT系统在文本输入完整时对视觉信息的敏感性降低的认识，我们认为这种现象源于跨模态交互不足，而不是图像信息冗余。提出了一种新颖的方法，即从源文本生成并行的视觉问答(VQA)样式对，促进更强大的跨模态交互。使用大型语言模型(LLM)，我们明确地对MMT中的探测信号进行建模，将其转化为VQA样式数据，创建了Multi30K-VQA数据集。引入了MMT-VQA多任务学习框架，将数据集中的显式探测信号融入MMT训练过程。在两个广泛使用的基准测试上的实验结果表明了这种新颖方法的有效性。我们的代码和数据可在以下链接获取：\url{https://github.com/libeineu/MMT-VQA}。

    This paper presents an in-depth study of multimodal machine translation (MMT), examining the prevailing understanding that MMT systems exhibit decreased sensitivity to visual information when text inputs are complete. Instead, we attribute this phenomenon to insufficient cross-modal interaction, rather than image information redundancy. A novel approach is proposed to generate parallel Visual Question-Answering (VQA) style pairs from the source text, fostering more robust cross-modal interaction. Using Large Language Models (LLMs), we explicitly model the probing signal in MMT to convert it into VQA-style data to create the Multi30K-VQA dataset. An MMT-VQA multitask learning framework is introduced to incorporate explicit probing signals from the dataset into the MMT training process. Experimental results on two widely-used benchmarks demonstrate the effectiveness of this novel approach. Our code and data would be available at: \url{https://github.com/libeineu/MMT-VQA}.
    
[^57]: 通过双向知识传递释放GNN的潜力

    Unleashing the potential of GNNs via Bi-directional Knowledge Transfer. (arXiv:2310.17132v1 [cs.LG])

    [http://arxiv.org/abs/2310.17132](http://arxiv.org/abs/2310.17132)

    本文通过研究发现GNN未充分利用内在的特征转换操作能力，提出了一种双向知识传递的插拔式方法，使GNN能够充分释放特征转换操作的潜力，而不需要修改原始架构。

    

    基于消息传递范式，已经有大量研究提出了各种各样令人印象深刻的特征传播机制来提高GNN的性能。然而，对于特征转换，消息传递框架的另一个重要操作，关注较少。在本文中，我们首先经验性地研究了几种典型GNN中特征转换操作的性能。意外的是，我们注意到GNN没有完全释放出内在的特征转换操作的能力。基于这一观察，我们提出了双向知识传递（BiKT），一种插拔式方法，旨在释放特征转换操作的潜力，而不修改原始架构。将特征转换操作视为与原始GNN共享参数的派生表示学习模型，通过该模型的直接预测提供了一种与拓扑无关的知识反馈，进一步指导了GNN的学习过程。

    Based on the message-passing paradigm, there has been an amount of research proposing diverse and impressive feature propagation mechanisms to improve the performance of GNNs. However, less focus has been put on feature transformation, another major operation of the message-passing framework. In this paper, we first empirically investigate the performance of the feature transformation operation in several typical GNNs. Unexpectedly, we notice that GNNs do not completely free up the power of the inherent feature transformation operation. By this observation, we propose the Bi-directional Knowledge Transfer (BiKT), a plug-and-play approach to unleash the potential of the feature transformation operations without modifying the original architecture. Taking the feature transformation operation as a derived representation learning model that shares parameters with the original GNN, the direct prediction by this model provides a topological-agnostic knowledge feedback that can further instru
    
[^58]: 使用语言模型对半结构化和非结构化对话数据进行主题分段

    Topic Segmentation of Semi-Structured and Unstructured Conversational Datasets using Language Models. (arXiv:2310.17120v1 [cs.CL])

    [http://arxiv.org/abs/2310.17120](http://arxiv.org/abs/2310.17120)

    本文通过对非结构化文本进行分析，揭示了目前主题分段模型在此类数据上的泛化能力不足，并提出了从头开始训练相对小规模的目标数据集来改善分段结果的方法。实证评估表明使用多种损失函数可以减轻非结构化对话数据集的不平衡效应。

    

    在自然语言处理中，将文档或对话根据其语义结构分解为多个连续片段是一个重要且具有挑战性的问题，可以帮助许多下游任务。然而，当前关于主题分段的研究往往集中在结构化文本的分段上。在本文中，我们全面分析了最先进的主题分段模型在非结构化文本上的泛化能力。我们发现：（a）目前在大规模结构化文本语料库（如Wiki-727K）上进行预训练的策略对于在非结构化对话数据上的可传递性并不有帮助。（b）从头开始使用相对小规模的目标非结构化领域数据集训练能显著提高分段结果。我们通过尝试多种损失函数来进行我们的主题分段方法的强化测试，以减轻非结构化对话数据集的不平衡效应。我们的实证评估表明Fo

    Breaking down a document or a conversation into multiple contiguous segments based on its semantic structure is an important and challenging problem in NLP, which can assist many downstream tasks. However, current works on topic segmentation often focus on segmentation of structured texts. In this paper, we comprehensively analyze the generalization capabilities of state-of-the-art topic segmentation models on unstructured texts. We find that: (a) Current strategies of pre-training on a large corpus of structured text such as Wiki-727K do not help in transferability to unstructured conversational data. (b) Training from scratch with only a relatively small-sized dataset of the target unstructured domain improves the segmentation results by a significant margin. We stress-test our proposed Topic Segmentation approach by experimenting with multiple loss functions, in order to mitigate effects of imbalance in unstructured conversational datasets. Our empirical evaluation indicates that Fo
    
[^59]: 对自适应巡航控制车辆的隐蔽网络攻击的检测：一种机器学习方法

    Detecting stealthy cyberattacks on adaptive cruise control vehicles: A machine learning approach. (arXiv:2310.17091v1 [cs.MA])

    [http://arxiv.org/abs/2310.17091](http://arxiv.org/abs/2310.17091)

    本研究针对自适应巡航控制车辆可能遭受的网络攻击问题，提出了一个交通模型框架和基于GAN的异常检测模型，能够实时识别恶意操纵、虚假注入和拒绝服务攻击。

    

    随着配备了自适应巡航控制（ACC）和其他自动驾驶功能的先进驾驶辅助系统的出现，针对这些自动驾驶车辆（AVs）的网络攻击潜在风险也出现了。虽然强制车辆发生碰撞的明显攻击容易被识别，但更隐蔽的攻击，只略微改变行驶行为，可能会导致网络范围内拥堵、燃油消耗增加，甚至增加碰撞风险，但很难被检测到。为了解决这种攻击的检测问题，我们首先提出了一个交通模型框架，用于描述可能的三种网络攻击类型：恶意操纵车辆控制命令、对传感器测量数据进行虚假注入攻击和拒绝服务（DoS）攻击。然后，我们研究了这些攻击对个体车辆（微观）和交通流（宏观）水平的影响。我们提出了一种基于生成对抗网络（GAN）的异常检测模型，用于实时识别。

    With the advent of vehicles equipped with advanced driver-assistance systems, such as adaptive cruise control (ACC) and other automated driving features, the potential for cyberattacks on these automated vehicles (AVs) has emerged. While overt attacks that force vehicles to collide may be easily identified, more insidious attacks, which only slightly alter driving behavior, can result in network-wide increases in congestion, fuel consumption, and even crash risk without being easily detected. To address the detection of such attacks, we first present a traffic model framework for three types of potential cyberattacks: malicious manipulation of vehicle control commands, false data injection attacks on sensor measurements, and denial-of-service (DoS) attacks. We then investigate the impacts of these attacks at both the individual vehicle (micro) and traffic flow (macro) levels. A novel generative adversarial network (GAN)-based anomaly detection model is proposed for real-time identifica
    
[^60]: Transformers学会了高阶优化方法用于上下文学习：一项与线性模型的研究

    Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models. (arXiv:2310.17086v1 [cs.LG])

    [http://arxiv.org/abs/2310.17086](http://arxiv.org/abs/2310.17086)

    Transformers学会了高阶优化方法，用于上下文学习，通过实现类似于迭代牛顿法的算法，而不是梯度下降。

    

    Transformers在上下文学习中表现出色，但是它们是如何进行上下文学习仍然是一个谜。最近的研究表明，Transformers可能通过内部运行梯度下降，即一阶优化方法，来进行上下文学习。本文中，我们展示了Transformers学会了实现高阶优化方法来进行上下文学习。我们以上下文线性回归为重点，展示了Transformers学会了实现一个非常类似于迭代牛顿法的算法，而不是梯度下降。从实证上来看，我们展示了连续的Transformer层的预测与牛顿法的不同迭代非常接近，每个中间层大致计算了3次迭代。相比之下，需要指数级的梯度下降步骤才能匹配额外的Transformer层；这表明Transformers具有相当的收敛速率。

    Transformers are remarkably good at in-context learning (ICL) -- learning from demonstrations without parameter updates -- but how they perform ICL remains a mystery. Recent work suggests that Transformers may learn in-context by internally running Gradient Descent, a first-order optimization method. In this paper, we instead demonstrate that Transformers learn to implement higher-order optimization methods to perform ICL. Focusing on in-context linear regression, we show that Transformers learn to implement an algorithm very similar to Iterative Newton's Method, a higher-order optimization method, rather than Gradient Descent. Empirically, we show that predictions from successive Transformer layers closely match different iterations of Newton's Method linearly, with each middle layer roughly computing 3 iterations. In contrast, exponentially more Gradient Descent steps are needed to match an additional Transformers layer; this suggests that Transformers have an comparable rate of conv
    
[^61]: 等距运动流形基元

    Isometric Motion Manifold Primitives. (arXiv:2310.17072v1 [cs.AI])

    [http://arxiv.org/abs/2310.17072](http://arxiv.org/abs/2310.17072)

    Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods.

    

    运动流形基元（MMP）为给定任务生成一系列连续轨迹流形，每一个轨迹流形都能成功完成任务。它由对流形进行参数化的解码函数以及潜在坐标空间中的概率密度组成。本文首先展示了由于潜在空间中的几何扭曲，MMP的性能可能会显著降低--通过变形，我们指的是相似的运动在潜在空间中无法相邻。然后，我们提出了等距运动流形基元（IMMP），其潜在坐标空间保持了流形的几何结构。为此，我们建立和使用了一个Riemannian度量，用于运动空间（即，参数化曲线空间），我们称之为CurveGeom Riemannian度量。对于平面障碍避让运动和推动操纵任务的实验表明，IMMP明显优于现有的MMP方法。代码可在https://github.com/Gabe-YHLee/IMMP找到。

    The Motion Manifold Primitive (MMP) produces, for a given task, a continuous manifold of trajectories each of which can successfully complete the task. It consists of the decoder function that parametrizes the manifold and the probability density in the latent coordinate space. In this paper, we first show that the MMP performance can significantly degrade due to the geometric distortion in the latent space -- by distortion, we mean that similar motions are not located nearby in the latent space. We then propose {\it Isometric Motion Manifold Primitives (IMMP)} whose latent coordinate space preserves the geometry of the manifold. For this purpose, we formulate and use a Riemannian metric for the motion space (i.e., parametric curve space), which we call a {\it CurveGeom Riemannian metric}. Experiments with planar obstacle-avoiding motions and pushing manipulation tasks show that IMMP significantly outperforms existing MMP methods. Code is available at https://github.com/Gabe-YHLee/IMMP
    
[^62]: math-PVS:一个将科学出版物映射到PVS理论的大型语言模型框架

    math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories. (arXiv:2310.17064v1 [cs.AI])

    [http://arxiv.org/abs/2310.17064](http://arxiv.org/abs/2310.17064)

    该研究调查了将大型语言模型应用于形式化高级数学概念的可行性，并提出了一个可以批判性地审查和检查研究论文中数学推理的框架。

    

    随着人工智能在各种应用领域的广泛采用，它在数学发现方面有巨大潜力，可以引导猜想生成，构造反例，协助形式化数学，并发现不同数学领域之间的联系，等等。尽管之前的工作利用计算机进行详尽的数学证明搜索，但基于大型语言模型的最近努力致力于将计算平台定位为数学研究过程中的合作贡献者。尽管目前的语言模型在逻辑和数学任务方面存在局限性，但越来越多的人对将定理证明系统与基础模型结合起来的兴趣日益增长。本研究调查了LLMs在形式化高级数学概念方面的适用性，并提出了一个能够批判性地审查和检查研究论文中数学推理的框架。

    As artificial intelligence (AI) gains greater adoption in a wide variety of applications, it has immense potential to contribute to mathematical discovery, by guiding conjecture generation, constructing counterexamples, assisting in formalizing mathematics, and discovering connections between different mathematical areas, to name a few.  While prior work has leveraged computers for exhaustive mathematical proof search, recent efforts based on large language models (LLMs) aspire to position computing platforms as co-contributors in the mathematical research process. Despite their current limitations in logic and mathematical tasks, there is growing interest in melding theorem proving systems with foundation models. This work investigates the applicability of LLMs in formalizing advanced mathematical concepts and proposes a framework that can critically review and check mathematical reasoning in research papers. Given the noted reasoning shortcomings of LLMs, our approach synergizes the 
    
[^63]: 使用类内相关正则化器学习可重复的语音嵌入

    Learning Repeatable Speech Embeddings Using An Intra-class Correlation Regularizer. (arXiv:2310.17049v1 [cs.SD])

    [http://arxiv.org/abs/2310.17049](http://arxiv.org/abs/2310.17049)

    本文介绍了一种使用类内相关正则化器来学习可重复的语音嵌入的方法，该方法通过引入ICC正则化器作为对比损失的补充组件，可以提高深度神经网络生成的嵌入的重复性。

    

    一个好的监督嵌入用于特定的机器学习任务，仅对感兴趣的标签的变化敏感，并且对其他混淆因素不变。我们利用测量理论中的重复性概念来描述这个属性，并提出使用类内相关系数（ICC）来评估嵌入的重复性。然后，我们提出了一种新的正则化器，即ICC正则化器，作为对比损失的补充组件，引导深度神经网络生成具有更高重复性的嵌入。我们使用模拟数据来解释为什么ICC正则化器在减小类内变异性方面比仅使用对比损失更有效。我们实现了ICC正则化器，并将其应用于三个语音任务：说话人验证、语音风格转换以及用于检测声带功能紊乱的临床应用。实验结果表明，添加ICC正则化器可以提高学习到的嵌入的重复性。

    A good supervised embedding for a specific machine learning task is only sensitive to changes in the label of interest and is invariant to other confounding factors. We leverage the concept of repeatability from measurement theory to describe this property and propose to use the intra-class correlation coefficient (ICC) to evaluate the repeatability of embeddings. We then propose a novel regularizer, the ICC regularizer, as a complementary component for contrastive losses to guide deep neural networks to produce embeddings with higher repeatability. We use simulated data to explain why the ICC regularizer works better on minimizing the intra-class variance than the contrastive loss alone. We implement the ICC regularizer and apply it to three speech tasks: speaker verification, voice style conversion, and a clinical application for detecting dysphonic voice. The experimental results demonstrate that adding an ICC regularizer can improve the repeatability of learned embeddings compared 
    
[^64]: StochGradAdam: 利用随机梯度抽样加速神经网络训练

    StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling. (arXiv:2310.17042v1 [cs.LG])

    [http://arxiv.org/abs/2310.17042](http://arxiv.org/abs/2310.17042)

    StochGradAdam是一种利用随机梯度抽样加速神经网络训练的优化器，通过选择性梯度考虑，能够稳定收敛，提升鲁棒训练。在图像分类和分割任务中表现优异。

    

    在深度学习优化领域中，本文介绍了StochGradAdam优化器，这是对广受赞誉的Adam算法的新颖改进。StochGradAdam的核心是其梯度抽样技术。该方法不仅确保稳定收敛，而且利用选择性梯度考虑的优势，通过减轻噪声或异常数据的影响和增强损失函数空间的探索，提升了鲁棒训练。在图像分类和分割任务中，StochGradAdam表现出优于传统Adam优化器的性能。通过在每次迭代中精心选择一部分梯度进行抽样，该优化器能够有效应对复杂模型的管理。本文从数学基础到偏差校正策略全面探讨了StochGradAdam的方法，展示了深度学习训练技术的可期进展。

    In the rapidly advancing domain of deep learning optimization, this paper unveils the StochGradAdam optimizer, a novel adaptation of the well-regarded Adam algorithm. Central to StochGradAdam is its gradient sampling technique. This method not only ensures stable convergence but also leverages the advantages of selective gradient consideration, fostering robust training by potentially mitigating the effects of noisy or outlier data and enhancing the exploration of the loss landscape for more dependable convergence. In both image classification and segmentation tasks, StochGradAdam has demonstrated superior performance compared to the traditional Adam optimizer. By judiciously sampling a subset of gradients at each iteration, the optimizer is optimized for managing intricate models. The paper provides a comprehensive exploration of StochGradAdam's methodology, from its mathematical foundations to bias correction strategies, heralding a promising advancement in deep learning training tec
    
[^65]: 关于语言编码器的手术微调

    On Surgical Fine-tuning for Language Encoders. (arXiv:2310.17041v1 [cs.CL])

    [http://arxiv.org/abs/2310.17041](http://arxiv.org/abs/2310.17041)

    本文表明，对于不同的下游语言任务，只对语言编码器的部分层进行细调即可获得接近甚至优于细调所有层的性能。通过提出一种高效度量方法，我们证明了该方法可以选择性微调导致强大下游性能的层。研究突出表明，任务特定信息通常局部化在少数层内，只调整这些层就足够了。

    

    细调预训练的神经语言编码器的所有层（使用所有参数或使用参数高效的方法）往往是将其适应于新任务的默认方法。我们展示了证据，对于不同的下游语言任务，仅细调部分层即可获得接近甚至优于细调语言编码器的所有层的性能。我们提出了一种基于Fisher信息矩阵的对角线（FIM评分）的高效度量方法，用于选择用于选择性微调的候选层。我们在GLUE和SuperGLUE任务以及不同的语言编码器上经验性地展示了，这个度量可以有效选择导致强大下游性能的层。我们的工作突出了与给定的下游任务对应的任务特定信息通常局部化在少数层内，只调整这些层对于强大的性能就足够了。

    Fine-tuning all the layers of a pre-trained neural language encoder (either using all the parameters or using parameter-efficient methods) is often the de-facto way of adapting it to a new task. We show evidence that for different downstream language tasks, fine-tuning only a subset of layers is sufficient to obtain performance that is close to and often better than fine-tuning all the layers in the language encoder. We propose an efficient metric based on the diagonal of the Fisher information matrix (FIM score), to select the candidate layers for selective fine-tuning. We show, empirically on GLUE and SuperGLUE tasks and across distinct language encoders, that this metric can effectively select layers leading to a strong downstream performance. Our work highlights that task-specific information corresponding to a given downstream task is often localized within a few layers, and tuning only those is sufficient for strong performance. Additionally, we demonstrate the robustness of the 
    
[^66]: netFound: 网络安全的基础模型

    netFound: Foundation Model for Network Security. (arXiv:2310.17025v1 [cs.NI])

    [http://arxiv.org/abs/2310.17025](http://arxiv.org/abs/2310.17025)

    netFound是一个基于自我监督算法的基础模型，用于网络安全领域。该模型通过预训练捕捉网络流量的层次化和多模态属性，并能够在质量低、有限和嘈杂的数据情况下进行微调。

    

    在网络安全的机器学习领域，传统工作流依赖于高质量标记数据和手动特征工程，但有限的数据集和人类专业知识阻碍了特征选择，导致模型难以捕捉关键关系和有效泛化。受到GPT-4和Vision Transformers等机器学习应用领域的最新进展的启发，我们开发了netFound，一个网络安全的基础模型。该模型利用自我监督算法对现有的未标记网络数据包进行预训练。netFound的设计融合了网络流量的层次化和多模态属性，有效捕捉了隐藏的网络上下文，包括应用逻辑、通信协议和网络条件。有了这个预训练基础，即使处理质量低、有限和嘈杂的标记数据，我们也可以对netFound进行微调，适用于各种下游任务。我们的实验证明了netFound的效果。

    In ML for network security, traditional workflows rely on high-quality labeled data and manual feature engineering, but limited datasets and human expertise hinder feature selection, leading to models struggling to capture crucial relationships and generalize effectively. Inspired by recent advancements in ML application domains like GPT-4 and Vision Transformers, we have developed netFound, a foundational model for network security. This model undergoes pre-training using self-supervised algorithms applied to readily available unlabeled network packet traces. netFound's design incorporates hierarchical and multi-modal attributes of network traffic, effectively capturing hidden networking contexts, including application logic, communication protocols, and network conditions.  With this pre-trained foundation in place, we can fine-tune netFound for a wide array of downstream tasks, even when dealing with low-quality, limited, and noisy labeled data. Our experiments demonstrate netFound'
    
[^67]: 受控解码来自语言模型

    Controlled Decoding from Language Models. (arXiv:2310.17022v1 [cs.LG])

    [http://arxiv.org/abs/2310.17022](http://arxiv.org/abs/2310.17022)

    本论文提出了一种名为受控解码（CD）的离策略强化学习方法，用于控制语言模型的生成，以达到高回报的结果。CD通过前缀评分器来引导生成，可以在推理时预测预期回报，并且具有模块化设计，可用于解决多目标强化学习问题，而不增加复杂性。

    

    我们提出了一种新颖的离策略强化学习方法，称为受控解码（CD），用于控制自回归语言模型的生成，以获得高回报的结果。CD通过值函数来解决离策略强化学习问题，该值函数被称为前缀评分器。前缀评分器在推理时用于引导生成向更高回报的结果。我们展示了前缀评分器可以从（可能是）离策略数据中训练出来，用于预测从部分解码的响应继续解码时的预期回报。我们在Reddit对话语料库上经验证明，CD作为一种控制机制是有效的。我们还展示了CD设计的模块化使其能够有效解决多目标强化学习问题，而不会增加任何复杂性。最后，我们展示了CD可以以一种新颖的分块方式在推理时应用，同样无需任何额外的操作。

    We propose controlled decoding (CD), a novel off-policy reinforcement learning method to control the autoregressive generation from language models towards high reward outcomes. CD solves an off-policy reinforcement learning problem through a value function for the reward, which we call a prefix scorer. The prefix scorer is used at inference time to steer the generation towards higher reward outcomes. We show that the prefix scorer may be trained on (possibly) off-policy data to predict the expected reward when decoding is continued from a partially decoded response. We empirically demonstrate that CD is effective as a control mechanism on Reddit conversations corpus. We also show that the modularity of the design of CD makes it possible to control for multiple rewards, effectively solving a multi-objective reinforcement learning problem with no additional complexity. Finally, we show that CD can be applied in a novel blockwise fashion at inference-time, again without the need for any 
    
[^68]: 基于计算机科学和医学视角的心理健康对话代理综合调查

    An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives. (arXiv:2310.17017v1 [cs.CL])

    [http://arxiv.org/abs/2310.17017](http://arxiv.org/abs/2310.17017)

    本研究使用PRISMA框架综述了计算机科学和医学领域发表的534篇论文，发现了136篇关键论文，涵盖了心理健康对话代理的多种建模和实验设计技术特征。

    

    心理健康对话代理（也称为聊天机器人）被广泛研究，因其提供对那些面临心理健康挑战的人可及性支持的潜力。之前对该主题的调查主要考虑在计算机科学或医学领域发表的论文，导致理解上的分裂，阻碍了两个领域之间有益知识的共享。为了弥合这一鸿沟，我们使用PRISMA框架进行全面的文献综述，审查了534篇在计算机科学和医学领域发表的论文。我们的系统综述揭示了136篇关键论文，涵盖了建立与心理健康相关的对话代理的多种建模和实验设计技术特征。我们发现，计算机科学论文侧重于LLM技术和使用自动度量评估响应质量，对应用应用关注较少，而医学论文则使用基于规则的对话代理和结果度量来衡量参与者的健康结果。

    Mental health conversational agents (a.k.a. chatbots) are widely studied for their potential to offer accessible support to those experiencing mental health challenges. Previous surveys on the topic primarily consider papers published in either computer science or medicine, leading to a divide in understanding and hindering the sharing of beneficial knowledge between both domains. To bridge this gap, we conduct a comprehensive literature review using the PRISMA framework, reviewing 534 papers published in both computer science and medicine. Our systematic review reveals 136 key papers on building mental health-related conversational agents with diverse characteristics of modeling and experimental design techniques. We find that computer science papers focus on LLM techniques and evaluating response quality using automated metrics with little attention to the application while medical papers use rule-based conversational agents and outcome metrics to measure the health outcomes of parti
    
[^69]: 个性化语音驱动的具有风格控制的表情三维动画合成

    Personalized Speech-driven Expressive 3D Facial Animation Synthesis with Style Control. (arXiv:2310.17011v1 [cs.AI])

    [http://arxiv.org/abs/2310.17011](http://arxiv.org/abs/2310.17011)

    提出了一个个性化语音驱动的表情三维动画合成框架，通过建模身份特定的面部动作风格，以及语音相关内容，实现高度自然和可信度的动画合成。

    

    不同的人在情感表达时有不同的面部表情。一个真实的面部动画系统应该考虑到这种身份特定的表达风格和面部习性，以实现高度自然和可信度。现有的个性化语音驱动的三维面部动画方法要么使用独热标签，要么依赖于特定个体的模型，限制了其可扩展性。我们提出了一个个性化的语音驱动的表情三维动画合成框架，将身份特定的面部动作建模为潜在表示（称为风格），并根据情感类别，用目标风格合成新的动画。我们的框架以端到端的方式训练，并具有三个主要组成部分：表情编码器、语音编码器和表情解码器。由于富有表情的面部动作包括身份特定的风格和语音相关的内容。

    Different people have different facial expressions while speaking emotionally. A realistic facial animation system should consider such identity-specific speaking styles and facial idiosyncrasies to achieve high-degree of naturalness and plausibility. Existing approaches to personalized speech-driven 3D facial animation either use one-hot identity labels or rely-on person specific models which limit their scalability. We present a personalized speech-driven expressive 3D facial animation synthesis framework that models identity specific facial motion as latent representations (called as styles), and synthesizes novel animations given a speech input with the target style for various emotion categories. Our framework is trained in an end-to-end fashion and has a non-autoregressive encoder-decoder architecture with three main components: expression encoder, speech encoder and expression decoder. Since, expressive facial motion includes both identity-specific style and speech-related conte
    
[^70]: 这就像那样阅读：用于可解释性自然语言处理的深度学习

    This Reads Like That: Deep Learning for Interpretable Natural Language Processing. (arXiv:2310.17010v1 [cs.CL])

    [http://arxiv.org/abs/2310.17010](http://arxiv.org/abs/2310.17010)

    本研究将原型网络方法扩展到了自然语言处理领域，引入了一种学习的加权相似度度量和事后可解释性机制，通过聚焦于句子嵌入的重要维度来提高相似度计算，并改善了预测性能和解释准确性。

    

    原型学习是一种流行的机器学习方法，用于解释性决策，通过学习的原型的相似性来对新数据进行分类。尽管它主要应用于计算机视觉，但在这项工作中，我们建立在之前的研究基础上，进一步探索了原型网络在自然语言处理中的扩展。我们引入了一种学习的加权相似度度量，通过聚焦于预训练的句子嵌入的信息维度来增强相似度计算。此外，我们提出了一种事后可解释性机制，从原型和输入句子中提取与预测相关的单词。最后，我们通过实验证明，我们提出的方法不仅改善了在AG News和RT Polarity数据集上的预测性能，而且与基于合理性的递归卷积相比，还提高了解释的准确性。

    Prototype learning, a popular machine learning method designed for inherently interpretable decisions, leverages similarities to learned prototypes for classifying new data. While it is mainly applied in computer vision, in this work, we build upon prior research and further explore the extension of prototypical networks to natural language processing. We introduce a learned weighted similarity measure that enhances the similarity computation by focusing on informative dimensions of pre-trained sentence embeddings. Additionally, we propose a post-hoc explainability mechanism that extracts prediction-relevant words from both the prototype and input sentences. Finally, we empirically demonstrate that our proposed method not only improves predictive performance on the AG News and RT Polarity datasets over a previous prototype-based approach, but also improves the faithfulness of explanations compared to rationale-based recurrent convolutions.
    
[^71]: STEER: 语义转向扩展识别用于语音助手

    STEER: Semantic Turn Extension-Expansion Recognition for Voice Assistants. (arXiv:2310.16990v1 [cs.CL])

    [http://arxiv.org/abs/2310.16990](http://arxiv.org/abs/2310.16990)

    STEER是一个用于语音助手的语义转向扩展识别模型，通过训练数据集和启发式规则进行转向意图预测，并在实验中展现出了良好的性能。

    

    在语音助手系统的背景下，转向是指用户发出后续命令，试图引导或澄清之前的指令的现象。我们提出了STEER，一个转向检测模型，用于预测后续命令是否是用户企图转向之前指令的尝试。由于冷启动问题，构建用于转向案例的训练数据集带来了挑战。为了克服这个问题，我们开发了启发式规则来采样选择加入使用数据，近似正负样本而无需任何标注。我们的实验结果显示了识别转向意图的良好性能，在我们采样的数据上超过95%的准确率。此外，STEER结合我们的采样策略，在人工评估集上表现出了强大的零样本性能，有效地与真实的转向场景相匹配。除了仅依赖用户的转录作为输入，我们还引入了STEER+，这是模型的增强版本。

    In the context of a voice assistant system, steering refers to the phenomenon in which a user issues a follow-up command attempting to direct or clarify a previous turn. We propose STEER, a steering detection model that predicts whether a follow-up turn is a user's attempt to steer the previous command. Constructing a training dataset for steering use cases poses challenges due to the cold-start problem. To overcome this, we developed heuristic rules to sample opt-in usage data, approximating positive and negative samples without any annotation. Our experimental results show promising performance in identifying steering intent, with over 95% accuracy on our sampled data. Moreover, STEER, in conjunction with our sampling strategy, aligns effectively with real-world steering scenarios, as evidenced by its strong zero-shot performance on a human-graded evaluation set. In addition to relying solely on user transcripts as input, we introduce STEER+, an enhanced version of the model. STEER+ 
    
[^72]: 机器学习在临床疾病诊断中的意义：一项综述

    The Significance of Machine Learning in Clinical Disease Diagnosis: A Review. (arXiv:2310.16978v1 [cs.LG])

    [http://arxiv.org/abs/2310.16978](http://arxiv.org/abs/2310.16978)

    本综述研究了机器学习算法在临床疾病诊断中的应用，特别关注提高准确性和计算效率的优化。该研究发现，通过利用先进的ML和AI方法，可以增强医疗保健相关方的诊断和治疗能力。

    

    鉴于各种疾病机制的复杂性和患者症状的多样性，有效的疾病诊断在全球范围内仍然具有重要意义。为了解决这些挑战，研究人员、医生和患者正转向机器学习（ML），一门人工智能（AI）学科，来开发解决方案。通过利用先进的ML和AI方法，医疗保健相关方可以获得增强的诊断和治疗能力。然而，目前缺乏针对提高准确性和计算效率的机器学习算法的研究。本研究调查了机器学习算法在时间序列医疗指标中改善心率数据传输的能力，特别关注准确性和效率的优化。通过探索在医疗应用中使用的各种ML算法，综述介绍了基于ML的疾病诊断（MLBDD）的最新趋势和方法。考虑的因素包括算法的性能、数据处理和特征选择。

    The global need for effective disease diagnosis remains substantial, given the complexities of various disease mechanisms and diverse patient symptoms. To tackle these challenges, researchers, physicians, and patients are turning to machine learning (ML), an artificial intelligence (AI) discipline, to develop solutions. By leveraging sophisticated ML and AI methods, healthcare stakeholders gain enhanced diagnostic and treatment capabilities. However, there is a scarcity of research focused on ML algorithms for enhancing the accuracy and computational efficiency. This research investigates the capacity of machine learning algorithms to improve the transmission of heart rate data in time series healthcare metrics, concentrating particularly on optimizing accuracy and efficiency. By exploring various ML algorithms used in healthcare applications, the review presents the latest trends and approaches in ML-based disease diagnosis (MLBDD). The factors under consideration include the algorith
    
[^73]: CL-MASR：一个用于多语音自动语音识别（ASR）的持续学习基准

    CL-MASR: A Continual Learning Benchmark for Multilingual ASR. (arXiv:2310.16931v1 [cs.CL])

    [http://arxiv.org/abs/2310.16931](http://arxiv.org/abs/2310.16931)

    CL-MASR是一个用于研究多语音ASR的持续学习基准，提供了一系列多样化的持续学习方法，并解决了学习新语言时遗忘问题。

    

    现代多语音自动语音识别（ASR）系统，如Whisper，使得能够使用单个模型转录多种语言的音频成为可能。然而，当前最先进的ASR模型通常在单个语言或多任务设置下进行评估，忽视了持续学习新语言的挑战。关于如何添加新语言而不丢失先前数据中有价值信息的研究不足。此外，现有的持续学习基准主要集中在视觉和语言任务上，对于多语音ASR的持续学习还未深入探索。为了填补这一空白，我们提出CL-MASR，一个专为在持续学习环境中研究多语音ASR而设计的基准。CL-MASR提供了一系列多样化的持续学习方法，基于大规模预训练的ASR模型实现，并提供常见的指标来评估学习新语言的效果，同时解决了灾难性遗忘问题。

    Modern multilingual automatic speech recognition (ASR) systems like Whisper have made it possible to transcribe audio in multiple languages with a single model. However, current state-of-the-art ASR models are typically evaluated on individual languages or in a multi-task setting, overlooking the challenge of continually learning new languages. There is insufficient research on how to add new languages without losing valuable information from previous data. Furthermore, existing continual learning benchmarks focus mostly on vision and language tasks, leaving continual learning for multilingual ASR largely unexplored. To bridge this gap, we propose CL-MASR, a benchmark designed for studying multilingual ASR in a continual learning setting. CL-MASR provides a diverse set of continual learning methods implemented on top of large-scale pretrained ASR models, along with common metrics to assess the effectiveness of learning new languages while addressing the issue of catastrophic forgetting
    
[^74]: 广泛平坦最低水印技术用于GAN的鲁棒性归属验证

    Wide Flat Minimum Watermarking for Robust Ownership Verification of GANs. (arXiv:2310.16919v1 [cs.CV])

    [http://arxiv.org/abs/2310.16919](http://arxiv.org/abs/2310.16919)

    该论文提出了一种广泛平坦最低水印技术，用于对抗白盒攻击，保护GAN的知识产权。通过将水印嵌入到GAN的训练中，使生成的图片包含不可见的水印，并通过模型参数的随机噪声，使模型收敛到广泛平坦的水印损失最低点，提高水印的鲁棒性。

    

    我们提出了一种新颖的多位无盒水印方法，用于保护GAN的知识产权，改进了对白盒攻击，如微调、剪枝、量化和替代模型攻击的鲁棒性。水印是通过在GAN训练过程中增加额外的水印损失项进行嵌入的，确保GAN生成的图片包含一个不可见的水印，可以由预训练的水印解码器提取出来。为了提高对白盒模型级攻击的鲁棒性，我们确保模型收敛到水印损失项的广泛平坦最低点，这样任何模型参数的修改都不会擦除水印。为此，我们向生成器的参数添加随机噪声向量，并要求水印损失项在噪声存在时尽可能保持不变。这个过程强迫生成器收敛到水印技术的广泛平坦最低点。

    We propose a novel multi-bit box-free watermarking method for the protection of Intellectual Property Rights (IPR) of GANs with improved robustness against white-box attacks like fine-tuning, pruning, quantization, and surrogate model attacks. The watermark is embedded by adding an extra watermarking loss term during GAN training, ensuring that the images generated by the GAN contain an invisible watermark that can be retrieved by a pre-trained watermark decoder. In order to improve the robustness against white-box model-level attacks, we make sure that the model converges to a wide flat minimum of the watermarking loss term, in such a way that any modification of the model parameters does not erase the watermark. To do so, we add random noise vectors to the parameters of the generator and require that the watermarking loss term is as invariant as possible with respect to the presence of noise. This procedure forces the generator to converge to a wide flat minimum of the watermarking l
    
[^75]: 基于可解释性深度学习和生成式数据增强的精神分裂症诊断方法研究

    An Explainable Deep Learning-Based Method For Schizophrenia Diagnosis Using Generative Data-Augmentation. (arXiv:2310.16867v1 [cs.LG])

    [http://arxiv.org/abs/2310.16867](http://arxiv.org/abs/2310.16867)

    本研究提出了一种基于可解释性深度学习和生成式数据增强的精神分裂症诊断方法，通过使用卷积神经网络进行初步诊断，并利用WGAN-GP和VAE生成的合成数据集进行增强，显著提高了诊断准确性和模型可解释性。

    

    本研究利用深度学习的方法，基于脑电图脑部记录，进行精神分裂症的自动诊断。该方法利用生成式数据增强技术，提高了诊断的准确性。为了能够利用时频特征，从原始信号中提取了频谱图。在探索了多种神经网络结构设置后，选择了适合的卷积神经网络(CNN)进行初步诊断。随后，利用Wasserstein GAN with Gradient Penalty(WGAN-GP)和变分自动编码器(VAE)生成了两个不同的合成数据集，以增强初始数据集并解决过拟合问题。使用VAE生成的增强数据集在准确度上提高了3.0％，达到了99.0％，同时得到了更低的损失值和更快的收敛速度。最后，我们使用局部可解释的模型无关解释(LIME)方法解决了对于黑盒模型的信任缺乏问题。

    In this study, we leverage a deep learning-based method for the automatic diagnosis of schizophrenia using EEG brain recordings. This approach utilizes generative data augmentation, a powerful technique that enhances the accuracy of the diagnosis. To enable the utilization of time-frequency features, spectrograms were extracted from the raw signals. After exploring several neural network architectural setups, a proper convolutional neural network (CNN) was used for the initial diagnosis. Subsequently, using Wasserstein GAN with Gradient Penalty (WGAN-GP) and Variational Autoencoder (VAE), two different synthetic datasets were generated in order to augment the initial dataset and address the over-fitting issue. The augmented dataset using VAE achieved a 3.0\% improvement in accuracy reaching up to 99.0\% and yielded a lower loss value as well as a faster convergence. Finally, we addressed the lack of trust in black-box models using the Local Interpretable Model-agnostic Explanations (LI
    
[^76]: 基于图像的多模态多病变DLBCL治疗反应预测

    Graph-based multimodal multi-lesion DLBCL treatment response prediction from PET images. (arXiv:2310.16863v1 [eess.IV])

    [http://arxiv.org/abs/2310.16863](http://arxiv.org/abs/2310.16863)

    本文提出了一种基于图神经网络的方法，利用临床和图像数据来预测DLBCL患者的治疗反应，并在实验证明这种方法优于传统的监督方法。

    

    弥漫大B细胞淋巴瘤（DLBCL）是涉及一个或多个淋巴结和淋巴外部位点的淋巴癌症。其诊断和随访依赖于正电子发射断层扫描（PET）和计算机断层扫描（CT）。在诊断后，标准一线治疗非反应患者的数量仍然很大（30-40%）。本文旨在开发一种计算机辅助方法，有效利用每个患者的所有可用信息，包括临床和图像数据，以识别需要适应性治疗的高风险患者。我们提出了一种基于最近的图神经网络的方法，结合了来自多个病变的图像信息，以及交叉注意力模块来有效地整合不同的数据模态。该模型在一个私人前瞻性多中心的数据集上进行训练和评估，该数据集包含583名患者。实验结果表明，我们提出的方法胜过了基于临床、图像或临床和图像的经典监督方法。

    Diffuse Large B-cell Lymphoma (DLBCL) is a lymphatic cancer involving one or more lymph nodes and extranodal sites. Its diagnostic and follow-up rely on Positron Emission Tomography (PET) and Computed Tomography (CT). After diagnosis, the number of nonresponding patients to standard front-line therapy remains significant (30-40%). This work aims to develop a computer-aided approach to identify high-risk patients requiring adapted treatment by efficiently exploiting all the information available for each patient, including both clinical and image data. We propose a method based on recent graph neural networks that combine imaging information from multiple lesions, and a cross-attention module to integrate different data modalities efficiently. The model is trained and evaluated on a private prospective multicentric dataset of 583 patients. Experimental results show that our proposed method outperforms classical supervised methods based on either clinical, imaging or both clinical and im
    
[^77]: 平衡增强与边效用过滤器用于有符号图神经网络

    Balancing Augmentation with Edge-Utility Filter for Signed GNNs. (arXiv:2310.16862v1 [cs.SI])

    [http://arxiv.org/abs/2310.16862](http://arxiv.org/abs/2310.16862)

    本文提出一种用于有符号图神经网络的平衡增强方法，以解决语义和结构不平衡的问题，并通过边效用过滤器选择性地增强原始有符号图。

    

    最近有符号图神经网络（SGNNs）引起了更多关注，因为许多现实世界的网络是由正负两种类型的边组成的。负边的存在影响了SGNN在两个方面的稳健性。其一是语义不平衡，因为负边通常很难获得，尽管它们可能提供潜在有用的信息。其二是结构不平衡，例如不平衡的三角形，它们表示节点之间的不兼容关系。本文提出了一种平衡增强方法来解决SGNN的上述两个方面问题。首先，通过计算负边在不平衡结构中的出现次数来衡量每个负边的效用。其次，通过（1）边扰动调节器平衡正负边的数量，并确定扰动边与原始边的比率和（2）边效用过滤器选择性地增强原始有符号图，以去除负边。

    Signed graph neural networks (SGNNs) has recently drawn more attention as many real-world networks are signed networks containing two types of edges: positive and negative. The existence of negative edges affects the SGNN robustness on two aspects. One is the semantic imbalance as the negative edges are usually hard to obtain though they can provide potentially useful information. The other is the structural unbalance, e.g. unbalanced triangles, an indication of incompatible relationship among nodes. In this paper, we propose a balancing augmentation method to address the above two aspects for SGNNs. Firstly, the utility of each negative edge is measured by calculating its occurrence in unbalanced structures. Secondly, the original signed graph is selectively augmented with the use of (1) an edge perturbation regulator to balance the number of positive and negative edges and to determine the ratio of perturbed edges to original edges and (2) an edge utility filter to remove the negativ
    
[^78]: CP-BCS：由控制流图和伪代码引导的二进制代码摘要

    CP-BCS: Binary Code Summarization Guided by Control Flow Graph and Pseudo Code. (arXiv:2310.16853v1 [cs.PL])

    [http://arxiv.org/abs/2310.16853](http://arxiv.org/abs/2310.16853)

    CP-BCS是一个基于控制流图和伪代码的二进制代码摘要框架，用于自动生成函数摘要，特别是对于剥离了符号表和调试信息的二进制。它利用专家知识学习二进制函数的执行行为和逻辑语义。

    

    自动为二进制生成函数摘要是一项非常有价值但具有挑战性的任务，因为它涉及将低级语言（汇编代码）的执行行为和语义转化为可读的自然语言。然而，目前大部分关于理解汇编代码的工作都是为了生成函数名称，其中包含许多缩写术语，使得它们仍然令人困惑。为了填补这个空白，我们专注于为二进制函数生成完整的摘要，尤其是对于剥离了符号表和调试信息的二进制。为了充分利用汇编代码的语义，我们提出了一个名为CP-BCS的控制流图和伪代码引导的二进制代码摘要框架。CP-BCS利用双向指令级控制流图和伪代码，结合专家知识来学习全面的二进制函数执行行为和逻辑语义。我们在三种不同的二进制优化案例上评估了CP-BCS。

    Automatically generating function summaries for binaries is an extremely valuable but challenging task, since it involves translating the execution behavior and semantics of the low-level language (assembly code) into human-readable natural language. However, most current works on understanding assembly code are oriented towards generating function names, which involve numerous abbreviations that make them still confusing. To bridge this gap, we focus on generating complete summaries for binary functions, especially for stripped binary (no symbol table and debug information in reality). To fully exploit the semantics of assembly code, we present a control flow graph and pseudo code guided binary code summarization framework called CP-BCS. CP-BCS utilizes a bidirectional instruction-level control flow graph and pseudo code that incorporates expert knowledge to learn the comprehensive binary function execution behavior and logic semantics. We evaluate CP-BCS on 3 different binary optimiz
    
[^79]: 通过解决嵌入式FPGA中LSTM单元的吞吐量瓶颈，增强能效

    Enhancing Energy-efficiency by Solving the Throughput Bottleneck of LSTM Cells for Embedded FPGAs. (arXiv:2310.16842v1 [cs.AR])

    [http://arxiv.org/abs/2310.16842](http://arxiv.org/abs/2310.16842)

    本研究通过优化LSTM单元，提出了一种在终端设备上进行能效推断的新方法。以交通速度预测为例，优化后的LSTM单元在FPGA上实现了较快的推断速度和较低的能耗，相比现有方法提高了吞吐量和能效。

    

    在物联网中处理传感器数据，嵌入式深度学习对于一维数据非常重要。过去，经常使用CNN因为它们对于特殊的嵌入式硬件比如FPGA来说很容易优化。本研究提出了一种针对在终端设备上进行能效推断的新型LSTM单元优化方法。以交通速度预测为案例研究，优化后的LSTM单元的简单LSTM模型在FPGA XC7S15（来自Spartan-7系列）上每秒可实现17534个推断，仅消耗每个推断3.8微焦耳的能量。相比现有方法，它的吞吐量至少提高了5.4倍，能效提高了1.37倍。

    To process sensor data in the Internet of Things(IoTs), embedded deep learning for 1-dimensional data is an important technique. In the past, CNNs were frequently used because they are simple to optimise for special embedded hardware such as FPGAs. This work proposes a novel LSTM cell optimisation aimed at energy-efficient inference on end devices. Using the traffic speed prediction as a case study, a vanilla LSTM model with the optimised LSTM cell achieves 17534 inferences per second while consuming only 3.8 $\mu$J per inference on the FPGA \textit{XC7S15} from \textit{Spartan-7} family. It achieves at least 5.4$\times$ faster throughput and 1.37$\times$ more energy efficient than existing approaches.
    
[^80]: 多尺度扩散去噪平滑

    Multi-scale Diffusion Denoised Smoothing. (arXiv:2310.16779v1 [cs.LG])

    [http://arxiv.org/abs/2310.16779](http://arxiv.org/abs/2310.16779)

    本文研究了多尺度扩散去噪平滑的准确度和认证鲁棒性之间的权衡，并提出了一种在共享扩散模型上调整以实现平滑分类器鲁棒性的新方法。

    

    随着最近的扩散模型，随机平滑已成为少数几个切实可行的方法之一，为大规模预训练模型提供对抗鲁棒性。具体而言，可以通过简单的“去噪和分类”流程，即所谓的去噪平滑，在任何分类器上执行随机平滑，前提是有一个准确的去噪器可用，比如扩散模型。在本文中，我们研究了去噪平滑的准确度和认证鲁棒性之间的权衡：例如，我们质疑哪种扩散模型的表示形式能够最大化去噪平滑的认证鲁棒性。我们考虑了一个新的目标，旨在实现共同噪声水平下平滑分类器的鲁棒性，在共享扩散模型上进行精细调整，同时也为其认证鲁棒性补偿准确度的成本提供了一种新途径。

    Along with recent diffusion models, randomized smoothing has become one of a few tangible approaches that offers adversarial robustness to models at scale, e.g., those of large pre-trained models. Specifically, one can perform randomized smoothing on any classifier via a simple "denoise-and-classify" pipeline, so-called denoised smoothing, given that an accurate denoiser is available - such as diffusion model. In this paper, we investigate the trade-off between accuracy and certified robustness of denoised smoothing: for example, we question on which representation of diffusion model would maximize the certified robustness of denoised smoothing. We consider a new objective that aims collective robustness of smoothed classifiers across multiple noise levels at a shared diffusion model, which also suggests a new way to compensate the cost of accuracy in randomized smoothing for its certified robustness. This objective motivates us to fine-tune diffusion model (a) to perform consistent de
    
[^81]: DEFT：通过无监督核心集选择实现大规模语言模型数据高效微调

    DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection. (arXiv:2310.16776v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.16776](http://arxiv.org/abs/2310.16776)

    这项研究介绍了一种名为DEFT的数据高效微调框架，通过无监督核心集选择来最小化微调大规模语言模型所需的数据量。研究结果表明，DEFT模型在准确性上与现有模型相当，并且仅使用了70%的数据量。

    

    最近的进展使得许多预训练语言模型（PLMs）可以使用；然而，一个仍然存在的问题是微调PLMs以用于下游任务究竟需要多少数据？在这项工作中，我们介绍了DEFT，一种数据高效的微调框架，它利用无监督的核心集选择来最小化微调PLMs所需的数据量。我们在文本编辑LM的背景下展示了DEFT框架的有效性，并与最先进的文本编辑模型CoEDIT进行了比较。我们的定量和定性结果表明，DEFT模型在准确性上与CoEDIT一样，而使用的数据量要少约70%。

    Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to minimize the amount of data needed to fine-tune PLMs for downstream tasks. We demonstrate the efficacy of our DEFT framework in the context of text-editing LMs, and compare to the state-of-the art text-editing model, CoEDIT. Our quantitative and qualitative results demonstrate that DEFT models are just as accurate as CoEDIT while being finetuned on ~70% less data.
    
[^82]: SkyMath：技术报告

    SkyMath: Technical Report. (arXiv:2310.16713v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.16713](http://arxiv.org/abs/2310.16713)

    SkyMath是一个13亿参数的大型语言模型，通过自比较微调，它在数学推理任务中表现优秀，超过了同等规模的所有开源模型，并取得了新的SOTA最佳性能。

    

    大型语言模型（LLMs）展示了在各种自然语言处理（NLP）任务中解决问题的巨大潜力，包括数学推理。在这项工作中，我们提出了一个具有130亿参数的数学大型语言模型SkyMath。通过自比较微调，我们显著提升了Skywork-13B-Base的数学推理能力。在GSM8K上，SkyMath超过了所有已知的开源模型，并建立了新的SOTA性能。

    Large language models (LLMs) have shown great potential to solve varieties of natural language processing (NLP) tasks, including mathematical reasoning. In this work, we present SkyMath, a large language model for mathematics with 13 billion parameters. By applying self-compare fine-tuning, we have enhanced mathematical reasoning abilities of Skywork-13B-Base remarkably. On GSM8K, SkyMath outperforms all known open-source models of similar size and has established a new SOTA performance.
    
[^83]: 大型语言模型的知识编辑：一项综述

    Knowledge Editing for Large Language Models: A Survey. (arXiv:2310.16218v1 [cs.CL])

    [http://arxiv.org/abs/2310.16218](http://arxiv.org/abs/2310.16218)

    大型语言模型(LLMs)在学术和工业领域具有巨大潜力。本文综述了LLMs的知识编辑问题，强调了需要开发有效和高效的技术来更新预训练LLMs以纳入新知识的重要性。

    

    大型语言模型(LLMs)近期以其出色的理解、分析和生成文本的能力，根据其广博的知识和推理能力，改变了学术和工业领域的格局。然而，LLMs的一个主要缺点是它们在预训练时需要大量计算资源，因为其参数数量前所未有。当需要频繁引入新知识到预训练模型中时，这个缺点更加显著。因此，开发有效和高效的技术来更新预训练LLMs是必不可少的。传统方法是通过直接微调将新知识编码到预训练LLMs中。然而，简单地重新训练LLMs可能计算资源密集，并且存在将与模型更新无关的有价值的预训练知识退化的风险。最近，基于知识的模型编辑(KME)引起了越来越多的关注，旨在精确修改LLMs以纳入特定的知识。

    Large language models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently, Knowledge-based Model Editing (KME) has attracted increasing attention, which aims to precisely modify the LLMs to incorporate specific knowledge, wit
    
[^84]: 基于数字孪生的智能DDoS检测机制用于自治核心网络

    Digital Twin-Enabled Intelligent DDoS Detection Mechanism for Autonomous Core Networks. (arXiv:2310.12924v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2310.12924](http://arxiv.org/abs/2310.12924)

    本文提出了一种基于数字孪生的智能DDoS检测机制，适用于自治核心网络。通过使用在线学习方法，我们设计了一个DDoS检测架构，并实现了自动特征选择模块和Yet Another Next Generation模型来提高准确性和性能。这种解决方案成功地检测DDoS攻击，并具有97%的真实分类率。

    

    现有的分布式拒绝服务攻击（DDoS）解决方案无法处理高度聚合的数据速率，因此它们不适用于互联网服务提供商（ISP）核心网络。本文提出了一种基于数字孪生的智能DDoS检测机制，使用在线学习方法用于自治系统。我们的贡献有三个方面：首先，我们基于数字孪生为ISP核心网络设计了一个DDoS检测架构。我们实现了Yet Another Next Generation（YANG）模型和自动特征选择（AutoFS）模块来处理核心网络数据。我们采用在线学习方法实时和高效地更新模型，快速改进学习模型，并确保准确预测。最后，我们揭示了我们的解决方案成功检测DDoS攻击，并且将特征选择方法和学习模型更新的真实分类率达到了97％。我们的解决方案可以在大约

    Existing distributed denial of service attack (DDoS) solutions cannot handle highly aggregated data rates; thus, they are unsuitable for Internet service provider (ISP) core networks. This article proposes a digital twin-enabled intelligent DDoS detection mechanism using an online learning method for autonomous systems. Our contributions are three-fold: we first design a DDoS detection architecture based on the digital twin for ISP core networks. We implemented a Yet Another Next Generation (YANG) model and an automated feature selection (AutoFS) module to handle core network data. We used an online learning approach to update the model instantly and efficiently, improve the learning model quickly, and ensure accurate predictions. Finally, we reveal that our proposed solution successfully detects DDoS attacks and updates the feature selection method and learning model with a true classification rate of ninety-seven percent. Our proposed solution can estimate the attack within approxima
    
[^85]: 面向软件定义传感器网络的网络感知自动机器学习框架

    Network-Aware AutoML Framework for Software-Defined Sensor Networks. (arXiv:2310.12914v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2310.12914](http://arxiv.org/abs/2310.12914)

    这项研究提出了一个面向软件定义传感器网络的网络感知自动机器学习框架，能够在网络限制环境下选择合适的机器学习算法来检测DDoS攻击，并防止过拟合。

    

    鉴于当前分布式拒绝服务攻击（DDoS）的检测解决方案需要额外的基础设施来处理高聚合数据速率，因此它们不适用于传感器网络或物联网。此外，软件定义传感器网络的安全架构需要注意软件定义网络和传感器网络的漏洞。在本文中，我们提出了一个网络感知自动机器学习（AutoML）框架，用于检测软件定义传感器网络中的DDoS攻击。我们的框架根据变量流量负载、异构流量速率和检测时间等指标，在网络限制环境下选择理想的机器学习算法来检测DDoS攻击，同时防止过拟合。我们的贡献有两个方面：(i)我们首次在DDoS检测范围内探讨了机器学习算法的效率与网络/流量状态之间的权衡。(ii)我们设计并实现了一个软件架构

    As the current detection solutions of distributed denial of service attacks (DDoS) need additional infrastructures to handle high aggregate data rates, they are not suitable for sensor networks or the Internet of Things. Besides, the security architecture of software-defined sensor networks needs to pay attention to the vulnerabilities of both software-defined networks and sensor networks. In this paper, we propose a network-aware automated machine learning (AutoML) framework which detects DDoS attacks in software-defined sensor networks. Our framework selects an ideal machine learning algorithm to detect DDoS attacks in network-constrained environments, using metrics such as variable traffic load, heterogeneous traffic rate, and detection time while preventing over-fitting. Our contributions are two-fold: (i) we first investigate the trade-off between the efficiency of ML algorithms and network/traffic state in the scope of DDoS detection. (ii) we design and implement a software archi
    
[^86]: TwinPot: 数字孪生辅助的蜜罐用于网络安全的智能海港

    TwinPot: Digital Twin-assisted Honeypot for Cyber-Secure Smart Seaports. (arXiv:2310.12880v1 [cs.CR])

    [http://arxiv.org/abs/2310.12880](http://arxiv.org/abs/2310.12880)

    该论文介绍了一种数字孪生辅助的蜜罐，用于保护智能海港的网络安全。传统安全解决方案对于保护物联网和物理网络系统是有限的，而蜜罐则可以提供有关攻击者行为的宝贵信息。数字孪生技术可以增强虚拟蜜罐的逼真性，吸引更多的攻击者。

    

    最近十年，随着效率需求的上升和货物量的不断增加，下一代港口的概念变得更加明显。在这个智能基础设施和设施的新时代，很明显网络安全已经成为海港和海事当局近期最关注的问题，并且是大多数港口议程上的首要关注点。传统的安全解决方案可以应用于保护物联网和物理网络系统免受有害实体的侵害。然而，如果这些解决方案更透明地运行，安全研究人员只能观察、检查和了解攻击者的行为。因此，蜜罐是潜在的解决方案，因为它们提供有关攻击者的宝贵信息。蜜罐可以是虚拟的或物理的。虚拟蜜罐必须更加逼真以吸引攻击者，因此需要更好的高度逼真性。为此，可以采用数字孪生技术(DT)。

    The idea of next-generation ports has become more apparent in the last ten years in response to the challenge posed by the rising demand for efficiency and the ever-increasing volume of goods. In this new era of intelligent infrastructure and facilities, it is evident that cyber-security has recently received the most significant attention from the seaport and maritime authorities, and it is a primary concern on the agenda of most ports. Traditional security solutions can be applied to safeguard IoT and Cyber-Physical Systems (CPS) from harmful entities. Nevertheless, security researchers can only watch, examine, and learn about the behaviors of attackers if these solutions operate more transparently. Herein, honeypots are potential solutions since they offer valuable information about the attackers. It can be virtual or physical. Virtual honeypots must be more realistic to entice attackers, necessitating better high-fidelity. To this end, Digital Twin (DT) technology can be employed t
    
[^87]: 医学文本简化：通过非典型训练和重新排序的Beam Search解码优化可读性

    Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding. (arXiv:2310.11191v1 [cs.CL])

    [http://arxiv.org/abs/2310.11191](http://arxiv.org/abs/2310.11191)

    本研究探索了进一步提高医学领域文本简化可读性的方法，包括一种新的非典型损失和一种优化简单性的重新排序解码方法，取得了更好的性能。

    

    文本简化作为人工智能在专业领域（如医学）中弥合沟通差距的越来越有用的应用，已逐渐崭露头角。然而，医学简化方法有时会导致生成的文本质量和多样性下降。在本研究中，我们探索了进一步提高医学领域文本简化可读性的方法。我们提出了一种新的非典型吃亏损失干图片刺激生成更简单的术语，以及一种优化简单性的重新排序的Beam Search解码方法，在三个数据集上的可读性指标上取得了更好的性能。这项研究的发现为改进医学领域的文本简化提供了有希望的途径。

    Text simplification has emerged as an increasingly useful application of AI for bridging the communication gap in specialized fields such as medicine, where the lexicon is often dominated by technical jargon and complex constructs. Despite notable progress, methods in medical simplification sometimes result in the generated text having lower quality and diversity. In this work, we explore ways to further improve the readability of text simplification in the medical domain. We propose (1) a new unlikelihood loss that encourages generation of simpler terms and (2) a reranked beam search decoding method that optimizes for simplicity, which achieve better performance on readability metrics on three datasets. This study's findings offer promising avenues for improving text simplification in the medical field.
    
[^88]: MathVista: 用GPT-4V、Bard和其他大型多模态模型评估视觉场景中的数学推理能力

    MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models. (arXiv:2310.02255v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.02255](http://arxiv.org/abs/2310.02255)

    本论文提出了MathVista，这是一个评估视觉场景中数学推理能力的基准测试。通过对12个著名的基础模型进行全面的定量评估，发现最好的GPT-4V模型相对于第二名的Bard模型在准确率上提升了15.1%。

    

    大型语言模型（LLMs）和大型多模态模型（LMMs）在许多任务和领域中展示出令人印象深刻的问题解决能力，但它们在视觉环境中的数学推理能力尚未得到系统研究。为了弥补这一差距，我们提出了MathVista，这是一个综合了不同数学和视觉任务的挑战的基准测试。它包含了6141个例子，其中有28个现有的多模态数据集和3个新创建的数据集（即IQTest、FunctionQA和PaperQA）。完成这些任务需要精细的、深入的视觉理解和组合推理，这些都是当前最先进的基础模型所面临的困难。通过MathVista，我们对12个著名的基础模型进行了全面的定量评估。表现最好的GPT-4V模型的整体准确率为49.9%，明显优于第二名的Bard模型，相差15.1%。我们的深入分析揭示了

    Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of
    
[^89]: 利用选择的能力优化决策树学习

    Harnessing the Power of Choices in Decision Tree Learning. (arXiv:2310.01551v1 [cs.LG])

    [http://arxiv.org/abs/2310.01551](http://arxiv.org/abs/2310.01551)

    该研究提出了一种利用选择能力的决策树学习算法Top-$k$，相比于传统贪婪算法和最优决策树算法，在准确率和性能上取得更好的结果。

    

    我们提出了一种简单的对标准和经验成功的决策树学习算法（如ID3、C4.5和CART）进行推广的方法。这些算法凭借贪婪的特性，在机器学习领域发挥了关键作用：它们通过迭代地基于最佳属性进行划分来构建决策树。我们的算法Top-$k$则考虑$k$个最佳属性作为可能的划分，而不仅仅是单个最佳属性。我们通过理论和实证研究展示了这个简单推广方法的优势。首先，我们证明了一个“贪婪层次定理”，对于每个$k \in \mathbb{N}$，Top-$(k+1)$比Top-$k$更加强大：在某些数据分布下，前者可以达到$1-\varepsilon$的准确率，而后者只能达到$\frac1{2}+\varepsilon$的准确率。然后，我们通过大量实验表明，Top-$k$算法在决策树学习中优于两种主要方法：经典贪婪算法和较新的“最优决策树”算法。

    We propose a simple generalization of standard and empirically successful decision tree learning algorithms such as ID3, C4.5, and CART. These algorithms, which have been central to machine learning for decades, are greedy in nature: they grow a decision tree by iteratively splitting on the best attribute. Our algorithm, Top-$k$, considers the $k$ best attributes as possible splits instead of just the single best attribute. We demonstrate, theoretically and empirically, the power of this simple generalization. We first prove a {\sl greediness hierarchy theorem} showing that for every $k \in \mathbb{N}$, Top-$(k+1)$ can be dramatically more powerful than Top-$k$: there are data distributions for which the former achieves accuracy $1-\varepsilon$, whereas the latter only achieves accuracy $\frac1{2}+\varepsilon$. We then show, through extensive experiments, that Top-$k$ outperforms the two main approaches to decision tree learning: classic greedy algorithms and more recent "optimal decis
    
[^90]: 放射学报告的多语言自然语言处理模型--摘要是你需要的一切！

    Multilingual Natural Language ProcessingModel for Radiology Reports -- The Summary is all you need!. (arXiv:2310.00100v1 [cs.CL])

    [http://arxiv.org/abs/2310.00100](http://arxiv.org/abs/2310.00100)

    本研究通过在多语言文本到文本变换器模型上微调，开发了一个能够自动在多语言中总结放射学报告的模型。该模型有助于提高未来深度学习模型的研究和发展，且能够应用于不同族裔背景的患者数据。

    

    放射学报告的印象部分总结了重要的放射学发现，并在向医生传达这些发现时起到了关键作用。然而，对于放射科医生来说，准备这些摘要既耗时又容易出错。最近，已经开发了许多用于放射学报告摘要的模型。然而，目前还没有能够在多种语言中总结这些报告的模型。这样的模型可以极大地改进未来的研究和融合来自不同族裔背景的患者数据的深度学习模型的发展。本研究通过在公开可用的基于多语言文本到文本变换器的模型上微调，自动化地生成了不同语言的放射学印象，以总结英语、葡萄牙语和德语的放射学报告中的发现。在一项盲测中，两位有执业资格的放射科医生表示，对于至少70%的系统生成的摘要，其质量

    The impression section of a radiology report summarizes important radiology findings and plays a critical role in communicating these findings to physicians. However, the preparation of these summaries is time-consuming and error-prone for radiologists. Recently, numerous models for radiology report summarization have been developed. Nevertheless, there is currently no model that can summarize these reports in multiple languages. Such a model could greatly improve future research and the development of Deep Learning models that incorporate data from patients with different ethnic backgrounds. In this study, the generation of radiology impressions in different languages was automated by fine-tuning a model, publicly available, based on a multilingual text-to-text Transformer to summarize findings available in English, Portuguese, and German radiology reports. In a blind test, two board-certified radiologists indicated that for at least 70% of the system-generated summaries, the quality 
    
[^91]: CoFiI2P: 粗到精的图像到点云注册的对应关系

    CoFiI2P: Coarse-to-Fine Correspondences for Image-to-Point Cloud Registration. (arXiv:2309.14660v1 [cs.CV])

    [http://arxiv.org/abs/2309.14660](http://arxiv.org/abs/2309.14660)

    CoFiI2P是一种粗到精的图像到点云注册方法，通过利用全局信息和特征建立对应关系，实现全局最优解。

    

    图像到点云（I2P）注册是机器人导航和移动建图领域中的一项基础任务。现有的I2P注册方法在点到像素级别上估计对应关系，忽略了全局对齐。然而，没有来自全局约束的高级引导的I2P匹配容易收敛到局部最优解。为了解决这个问题，本文提出了一种新的I2P注册网络CoFiI2P，通过粗到精的方式提取对应关系，以得到全局最优解。首先，将图像和点云输入到一个共享编码-解码网络中进行层次化特征提取。然后，设计了一个粗到精的匹配模块，利用特征建立稳健的特征对应关系。具体来说，在粗匹配块中，采用了一种新型的I2P变换模块，从图像和点云中捕捉同质和异质的全局信息。通过判别描述子，完成粗-细特征匹配过程。最后，通过细化匹配模块进一步提升对应关系的准确性。

    Image-to-point cloud (I2P) registration is a fundamental task in the fields of robot navigation and mobile mapping. Existing I2P registration works estimate correspondences at the point-to-pixel level, neglecting the global alignment. However, I2P matching without high-level guidance from global constraints may converge to the local optimum easily. To solve the problem, this paper proposes CoFiI2P, a novel I2P registration network that extracts correspondences in a coarse-to-fine manner for the global optimal solution. First, the image and point cloud are fed into a Siamese encoder-decoder network for hierarchical feature extraction. Then, a coarse-to-fine matching module is designed to exploit features and establish resilient feature correspondences. Specifically, in the coarse matching block, a novel I2P transformer module is employed to capture the homogeneous and heterogeneous global information from image and point cloud. With the discriminate descriptors, coarse super-point-to-su
    
[^92]: 通过条件置换进行统计有效的变量重要性评估

    Statistically Valid Variable Importance Assessment through Conditional Permutations. (arXiv:2309.07593v1 [cs.LG])

    [http://arxiv.org/abs/2309.07593](http://arxiv.org/abs/2309.07593)

    本论文提出了一种通过条件置换进行统计有效的变量重要性评估的方法。实证结果表明，这种方法克服了标准置换重要性的局限性，并在深度神经网络中表现出最高的准确性。

    

    在使用复杂学习器（如深度神经网络）处理大规模数据时，变量重要性评估已成为机器学习应用中的关键步骤。目前，基于移除的重要性评估是参考方法，特别是在需要统计保证来验证变量包含性时。通常，它们使用变量置换方案来实现。然而，这些方法在存在协变量之间的相关性时容易将不重要的变量误识别为重要变量。本文提出了一种系统方法来研究条件置换重要性（Conditional Permutation Importance，CPI），它是模型无关且计算效率高的方法，并提供了基准测试，用于评估当前最先进的变量重要性估计器。理论和实证结果表明，CPI通过提供准确的I型错误控制，克服了标准置换重要性的局限性。当与深度神经网络一起使用时，CPI始终显示出最高的准确性。

    Variable importance assessment has become a crucial step in machine-learning applications when using complex learners, such as deep neural networks, on large-scale data. Removal-based importance assessment is currently the reference approach, particularly when statistical guarantees are sought to justify variable inclusion. It is often implemented with variable permutation schemes. On the flip side, these approaches risk misidentifying unimportant variables as important in the presence of correlations among covariates. Here we develop a systematic approach for studying Conditional Permutation Importance (CPI) that is model agnostic and computationally lean, as well as reusable benchmarks of state-of-the-art variable importance estimators. We show theoretically and empirically that $\textit{CPI}$ overcomes the limitations of standard permutation importance by providing accurate type-I error control. When used with a deep neural network, $\textit{CPI}$ consistently showed top accuracy ac
    
[^93]: 学习环境感知的遮挡下三维关节物体操作的可供性

    Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions. (arXiv:2309.07510v1 [cs.RO])

    [http://arxiv.org/abs/2309.07510](http://arxiv.org/abs/2309.07510)

    本论文提出了一个环境感知的可供性框架，考虑了物体级的可行性先验和环境约束，以解决多个遮挡的复杂情况下的三维关节物体操作问题。

    

    在多样的环境中感知和操作三维关节物体对于家庭助理机器人至关重要。最近的研究表明，点级可供性为下游操作任务提供了可行性先验。然而，现有工作主要集中在单个物体场景中的均质代理，忽视了环境和代理形态所施加的现实约束，如遮挡和物理限制。在本文中，我们提出了一个环境感知的可供性框架，结合了物体级可行性先验和环境约束。与以物体为中心的可供性方法不同，学习环境感知的可供性面临着由各种遮挡的复杂性引起的组合爆炸挑战，这些遮挡以其数量、几何形状、位置和姿势来刻画。为了解决这个问题并提高数据效率，我们引入了一种新颖的对比式可供性学习框架，能够在含有遮挡的场景中进行训练。

    Perceiving and manipulating 3D articulated objects in diverse environments is essential for home-assistant robots. Recent studies have shown that point-level affordance provides actionable priors for downstream manipulation tasks. However, existing works primarily focus on single-object scenarios with homogeneous agents, overlooking the realistic constraints imposed by the environment and the agent's morphology, e.g., occlusions and physical limitations. In this paper, we propose an environment-aware affordance framework that incorporates both object-level actionable priors and environment constraints. Unlike object-centric affordance approaches, learning environment-aware affordance faces the challenge of combinatorial explosion due to the complexity of various occlusions, characterized by their quantities, geometries, positions and poses. To address this and enhance data efficiency, we introduce a novel contrastive affordance learning framework capable of training on scenes containin
    
[^94]: COMEDIAN: 使用自监督学习和知识蒸馏的Transformer进行动作定位

    COMEDIAN: Self-Supervised Learning and Knowledge Distillation for Action Spotting using Transformers. (arXiv:2309.01270v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.01270](http://arxiv.org/abs/2309.01270)

    使用自监督学习和知识蒸馏的COMEDIAN提出了一种初始化时空Transformer的流程，用于动作定位任务。在SoccerNet-v2数据集上实验证明了其最先进的性能和有效性。

    

    我们提出了COMEDIAN，一种使用自监督学习和知识蒸馏初始化时空Transformer进行动作定位的新型流程。动作定位是一种基于时间戳级别的时间动作检测任务。我们的流程包括三个步骤，其中有两个初始化阶段。首先，我们使用短视频作为输入对空间Transformer进行自监督初始化。此外，我们还初始化一个时序Transformer，通过与每个短视频片段对齐的预计算特征库的知识蒸馏，增强空间Transformer的输出与全局上下文。最后一步，我们对Transformer进行微调以适应动作定位任务。在SoccerNet-v2数据集上进行的实验表明，COMEDIAN具有最先进的性能，并验证了其预训练模式的有效性。我们的结果突显了我们预训练流程的几个优点，包括改进的性能和更快的收敛速度。

    We present COMEDIAN, a novel pipeline to initialize spatiotemporal transformers for action spotting, which involves self-supervised learning and knowledge distillation. Action spotting is a timestamp-level temporal action detection task. Our pipeline consists of three steps, with two initialization stages. First, we perform self-supervised initialization of a spatial transformer using short videos as input. Additionally, we initialize a temporal transformer that enhances the spatial transformer's outputs with global context through knowledge distillation from a pre-computed feature bank aligned with each short video segment. In the final step, we fine-tune the transformers to the action spotting task. The experiments, conducted on the SoccerNet-v2 dataset, demonstrate state-of-the-art performance and validate the effectiveness of COMEDIAN's pretraining paradigm. Our results highlight several advantages of our pretraining pipeline, including improved performance and faster convergence c
    
[^95]: LLM强化了交通信号控制的从仿真到真实的迁移

    LLM Powered Sim-to-real Transfer for Traffic Signal Control. (arXiv:2308.14284v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.14284](http://arxiv.org/abs/2308.14284)

    本研究利用大型语言模型（LLMs）通过基于提示的行动转换，解决了交通信号控制任务中从仿真到真实的迁移问题。

    

    尽管已经有很多解决方案用于交通信号控制（TSC）任务，旨在提供高效的交通和减轻拥堵浪费，但仍然存在性能差距，当在仿真器中训练的策略部署到现实世界时。本研究利用大型语言模型（LLMs）通过基于提示的扎根行动转换，来理解和描述系统动态。通过接受填空提示模板，并根据可以访问的上下文填写答案，利用预训练的LLM的推理能力，应用于对系统动态的理解。

    Numerous solutions are proposed for the Traffic Signal Control (TSC) tasks aiming to provide efficient transportation and mitigate congestion waste. In recent, promising results have been attained by Reinforcement Learning (RL) methods through trial and error in simulators, bringing confidence in solving cities' congestion headaches. However, there still exist performance gaps when simulator-trained policies are deployed to the real world. This issue is mainly introduced by the system dynamic difference between the training simulator and the real-world environments. The Large Language Models (LLMs) are trained on mass knowledge and proved to be equipped with astonishing inference abilities. In this work, we leverage LLMs to understand and profile the system dynamics by a prompt-based grounded action transformation. Accepting the cloze prompt template, and then filling in the answer based on accessible context, the pre-trained LLM's inference ability is exploited and applied to understa
    
[^96]: 自适应白化：快速增益调制和慢速突触可塑性

    Adaptive whitening with fast gain modulation and slow synaptic plasticity. (arXiv:2308.13633v1 [q-bio.NC])

    [http://arxiv.org/abs/2308.13633](http://arxiv.org/abs/2308.13633)

    本研究提出了一个多时间尺度的自适应白化机制模型，使用快速增益调制和慢速突触可塑性相结合的方式来适应变化的感觉统计信息。

    

    早期感觉区的神经元能够迅速适应变化的感觉统计信息，通过对其个体响应的方差进行归一化以及减少响应之间的相关性。这些转换可以被视为一种自适应的白化过程。现有的自适应白化的机制模型只使用突触可塑性或增益调制作为适应的生物基质，然而，每个模型都有显著的局限性。在这项工作中，我们将这些方法统一起来，提出了一个规范性的多时间尺度机制模型，通过突触可塑性和增益调制的计算角色来自适应地进行白化。增益在快速时间尺度上根据当前的统计情况进行调整，而突触在慢速时间尺度上进行调整，学习输入统计中与情境无关的结构特性。我们的模型来自于一种新颖的多时间尺度

    Neurons in early sensory areas rapidly adapt to changing sensory statistics, both by normalizing the variance of their individual responses and by reducing correlations between their responses. Together, these transformations may be viewed as an adaptive form of statistical whitening. Existing mechanistic models of adaptive whitening exclusively use either synaptic plasticity or gain modulation as the biological substrate for adaptation; however, on their own, each of these models has significant limitations. In this work, we unify these approaches in a normative multi-timescale mechanistic model that adaptively whitens its responses with complementary computational roles for synaptic plasticity and gain modulation. Gains are modified on a fast timescale to adapt to the current statistical context, whereas synapses are modified on a slow timescale to learn structural properties of the input statistics that are invariant across contexts. Our model is derived from a novel multi-timescale
    
[^97]: 受PID控制器启发的偏差归纳法在部分可观测控制任务中的深度强化学习

    PID-Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks. (arXiv:2307.05891v1 [cs.LG])

    [http://arxiv.org/abs/2307.05891](http://arxiv.org/abs/2307.05891)

    该论文受到PID控制器的启发，提出了两种用于编码历史记录的架构，以平衡提取相关信息的灵活性与对环境变化的鲁棒性。

    

    深度强化学习（RL）已经展现出通过数据自己学习控制系统的巨大潜力。然而，深度RL面临的一个挑战是系统的完整状态通常不可观测。当出现这种情况时，策略需要利用观察历史来推断当前状态。同时，训练和测试环境之间的差异使得策略不会过度拟合训练时观察到的序列。因此，在历史记录编码器灵活提取相关信息的同时，要对环境变化具有鲁棒性，这是一个重要的平衡。为了达到这个平衡，我们寻求PID控制器的启发。我们断定PID控制器的成功表明，许多控制任务只需要求和和求差来累积信息。基于这个原则，我们提出了两种用于编码历史记录的架构：一种直接使用...

    Deep reinforcement learning (RL) has shown immense potential for learning to control systems through data alone. However, one challenge deep RL faces is that the full state of the system is often not observable. When this is the case, the policy needs to leverage the history of observations to infer the current state. At the same time, differences between the training and testing environments makes it critical for the policy not to overfit to the sequence of observations it sees at training time. As such, there is an important balancing act between having the history encoder be flexible enough to extract relevant information, yet be robust to changes in the environment. To strike this balance, we look to the PID controller for inspiration. We assert the PID controller's success shows that only summing and differencing are needed to accumulate information over time for many control tasks. Following this principle, we propose two architectures for encoding history: one that directly uses
    
[^98]: 大型语言模型作为通用模式机器

    Large Language Models as General Pattern Machines. (arXiv:2307.04721v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2307.04721](http://arxiv.org/abs/2307.04721)

    预训练的大型语言模型（LLMs）展现出了强大的序列模式完成能力，即使在随机样本的情况下也能保持一定的准确性。这些模型可以用于机器人领域的问题，如动态状态序列预测和自主路径规划。

    

    我们观察到预训练的大型语言模型（LLMs）能够自动完成复杂的令牌序列，从由概率上下文无关语法（PCFG）随机生成的任意序列，到在Abstraction and Reasoning Corpus（ARC）中发现的更丰富的空间模式，以ASCII艺术的形式提示。令人惊讶的是，即使使用从词汇表中随机抽样的令牌表示序列，模式完成能力也可以部分保留。这些结果表明，在没有任何额外训练的情况下，LLMs可以作为通用序列模型器，通过上下文学习驱动。在这项工作中，我们研究了这些零样本能力如何应用于机器人领域的问题，从对表示随时间变化的状态的数字序列进行外推，以完成简单的运动，到以奖励条件轨迹的最小到最大提示方式，能够发现和表示闭环策略（例如，稳定的控制系统）。

    We observe that pre-trained large language models (LLMs) are capable of autoregressively completing complex token sequences -- from arbitrary ones procedurally generated by probabilistic context-free grammars (PCFG), to more rich spatial patterns found in the Abstraction and Reasoning Corpus (ARC), a general AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern completion proficiency can be partially retained even when the sequences are expressed using tokens randomly sampled from the vocabulary. These results suggest that without any additional training, LLMs can serve as general sequence modelers, driven by in-context learning. In this work, we investigate how these zero-shot capabilities may be applied to problems in robotics -- from extrapolating sequences of numbers that represent states over time to complete simple motions, to least-to-most prompting of reward-conditioned trajectories that can discover and represent closed-loop policies (e.g., a stabilizing cont
    
[^99]: 图神经网络中特征演化的神经塌陷视角

    A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks. (arXiv:2307.01951v1 [cs.LG])

    [http://arxiv.org/abs/2307.01951](http://arxiv.org/abs/2307.01951)

    本文以节点分类为例，通过“神经塌陷”现象探索图神经网络中特征演化的机制，并发现即使在节点分类情况下，特征的类内变异性也会减少，但不及基于实例的情况那么显著。

    

    图神经网络（GNNs）在图结构数据的分类任务中越来越受欢迎。然而，GNNs中图拓扑和特征演化之间的相互作用尚不清楚。本文以基于节点的分类为主题，以随机块模型图上的社区检测为例，通过“神经塌陷”现象来探索特征演化。当训练基于实例的深度分类器（例如图像分类）超过零训练误差点时，神经塌陷表现为最深层特征的类内变异性减少，并且类均值与特定的对称结构更加对齐。我们先从实证研究开始，显示类内变异性的减少在基于节点的分类环境中也普遍存在，但不及基于实例的案例那么明显。然后，我们从理论上研究了这种区别。具体而言，我们证明了即使在不考虑激活，图拓扑信息也能导致特征崩溃。

    Graph neural networks (GNNs) have become increasingly popular for classification tasks on graph-structured data. Yet, the interplay between graph topology and feature evolution in GNNs is not well understood. In this paper, we focus on node-wise classification, illustrated with community detection on stochastic block model graphs, and explore the feature evolution through the lens of the "Neural Collapse" (NC) phenomenon. When training instance-wise deep classifiers (e.g. for image classification) beyond the zero training error point, NC demonstrates a reduction in the deepest features' within-class variability and an increased alignment of their class means to certain symmetric structures. We start with an empirical study that shows that a decrease in within-class variability is also prevalent in the node-wise classification setting, however, not to the extent observed in the instance-wise case. Then, we theoretically study this distinction. Specifically, we show that even an "optimis
    
[^100]: DiffSketcher: 通过隐式扩散模型实现文本引导的矢量素描合成

    DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models. (arXiv:2306.14685v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.14685](http://arxiv.org/abs/2306.14685)

    本文介绍了DiffSketcher，一种通过隐式扩散模型实现文本引导的矢量素描合成的创新算法。DiffSketcher通过直接优化贝塞尔曲线和扩散模型损失来生成矢量化的手绘素描，并通过注意力图加快生成过程。实验结果表明DiffSketcher的素描质量高于之前方法。

    

    尽管主要训练于图像，但我们发现预训练的扩散模型在引导素描合成方面表现出惊人的能力。本文提出了DiffSketcher，一种创新的算法，利用自然语言输入创建矢量化的手绘素描。DiffSketcher基于预训练的文本到图像扩散模型开发，通过使用扩展版本的得分蒸馏采样（SDS）损失直接优化一组贝塞尔曲线，使得我们可以将栅格级扩散模型作为先验来优化参数化的矢量素描生成器。此外，我们还探索了扩散模型中嵌入的注意力图，在生成过程中实现有效的笔画初始化以加快速度。生成的素描展示了多层次的抽象，同时保持了被绘制主题的可识别性、基本结构和重要的视觉细节。我们的实验证明，DiffSketcher的质量优于之前方法。

    Even though trained mainly on images, we discover that pretrained diffusion models show impressive power in guiding sketch synthesis. In this paper, we present DiffSketcher, an innovative algorithm that creates vectorized free-hand sketches using natural language input. DiffSketcher is developed based on a pre-trained text-to-image diffusion model. It performs the task by directly optimizing a set of Bezier curves with an extended version of the score distillation sampling (SDS) loss, which allows us to use a raster-level diffusion model as a prior for optimizing a parametric vectorized sketch generator. Furthermore, we explore attention maps embedded in the diffusion model for effective stroke initialization to speed up the generation process. The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual details of the subject drawn. Our experiments show that DiffSketcher achieves greater quality than pr
    
[^101]: 可微分决策树是否能够学习可解释的奖励函数?

    Can Differentiable Decision Trees Learn Interpretable Reward Functions?. (arXiv:2306.13004v1 [cs.LG])

    [http://arxiv.org/abs/2306.13004](http://arxiv.org/abs/2306.13004)

    本文提出了使用可微分决策树从人类偏好中学习具有表达能力和可解释性的奖励函数，通过在多个环境上的评估，发现该方法能够学习到可解释的奖励函数，但在强化学习测试时表现受到树的离散性的影响。

    

    学习人的意图和偏好的奖励函数越来越受到关注，但许多框架使用黑盒学习方法，难以解释。本文提出并评估了一种新颖方法，使用可微分决策树（DDT）从偏好中学习具有表达能力和可解释性的奖励函数，适用于低维和高维状态输入。我们在Cartpole、视觉网格世界环境和Atari游戏上评估了我们的算法，探讨了使用DDT学习可解释奖励函数的可行性。我们提供证据表明，学习到的奖励函数的树形结构有助于确定奖励函数与人类偏好的一致程度。我们可视化了学习到的奖励DDT，发现它们能够学习可解释的奖励函数，但树的离散性会影响强化学习在测试时的表现。

    There is an increasing interest in learning reward functions that model human intent and human preferences. However, many frameworks use blackbox learning methods that, while expressive, are difficult to interpret. We propose and evaluate a novel approach for learning expressive and interpretable reward functions from preferences using Differentiable Decision Trees (DDTs) for both low- and high-dimensional state inputs. We explore and discuss the viability of learning interpretable reward functions using DDTs by evaluating our algorithm on Cartpole, Visual Gridworld environments, and Atari games. We provide evidence that that the tree structure of our learned reward function is useful in determining the extent to which a reward function is aligned with human preferences. We visualize the learned reward DDTs and find that they are capable of learning interpretable reward functions but that the discrete nature of the trees hurts the performance of reinforcement learning at test time. How
    
[^102]: 通过安全贝叶斯优化调整四足行走控制器

    Tuning Legged Locomotion Controllers via Safe Bayesian Optimization. (arXiv:2306.07092v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2306.07092](http://arxiv.org/abs/2306.07092)

    本文提出了一种通过安全贝叶斯优化来调整四足行走控制器的策略，并通过模拟和硬件实验验证了该方法的有效性。

    

    本文提出了一种数据驱动的策略，以简化在四足机器人硬件平台中部署基于模型的控制器的过程。我们的方法利用一种无模型安全学习算法自动调整控制增益，解决了在控制制定中使用的简化模型与实际系统之间的不匹配问题。该方法通过在可能安全区域内高效优化参数，大大减小了与机器人的危险交互的风险。此外，我们扩展了我们的方法的适用范围，以将不同的步态参数作为背景，从而得到一种安全、高效的探索算法，能够安全地调整多样化步态模式的运动控制器。我们通过模拟和硬件实验验证了我们的方法，在多个步态中调整基于模型的运动控制器方面，该算法获得了更好的性能。

    This paper presents a data-driven strategy to streamline the deployment of model-based controllers in legged robotic hardware platforms. Our approach leverages a model-free safe learning algorithm to automate the tuning of control gains, addressing the mismatch between the simplified model used in the control formulation and the real system. This method substantially mitigates the risk of hazardous interactions with the robot by sample-efficiently optimizing parameters within a probably safe region. Additionally, we extend the applicability of our approach to incorporate the different gait parameters as contexts, leading to a safe, sample-efficient exploration algorithm capable of tuning a motion controller for diverse gait patterns. We validate our method through simulation and hardware experiments, where we demonstrate that the algorithm obtains superior performance on tuning a model-based motion controller for multiple gaits safely.
    
[^103]: 在表面之下寻找：利用基本对称性实现高效离线强化学习

    Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v1 [cs.LG])

    [http://arxiv.org/abs/2306.04220](http://arxiv.org/abs/2306.04220)

    本文提出了一个新的离线强化学习算法TDM，利用系统动力学的基本对称性实现高效学习小数据集。

    

    离线强化学习通过从预先收集的数据集中学习策略来解决与环境交互的实际问题。然而，现有的离线强化学习算法的性能严重依赖于数据集的规模和状态-动作空间覆盖范围。真实世界数据的收集通常是昂贵和难以控制的，导致数据集小且覆盖范围狭窄，从而对离线强化学习的实际部署提出了重大挑战。在本文中，我们提供了一个新的见解，即利用系统动力学的基本对称性可以在小数据集下显著提高离线强化学习的性能。具体来说，我们提出了一个时间反演对称(T-symmetry)强制的动力学模型(TDM)，建立了一对正向和反向潜在动力学之间的一致性。TDM为小数据集提供了良好的表示，并基于T-symmetry的符合性提供了一种新的OOD样本的可靠性度量。

    Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. 
    
[^104]: SourceP：使用预训练模型和数据流智能检测以太坊上的智能庞兹骗局

    SourceP: Smart Ponzi Schemes Detection on Ethereum Using Pre-training Model with Data Flow. (arXiv:2306.01665v1 [cs.SE])

    [http://arxiv.org/abs/2306.01665](http://arxiv.org/abs/2306.01665)

    本文提出了一种使用预训练模型和数据流的方法，通过智能合约的源代码特征，实现检测以太坊上的智能庞兹骗局。该方法提高了模型的可解释性和降低了数据获取和特征提取的难度。

    

    随着区块链技术越来越流行，典型的金融骗局庞兹骗局也在区块链平台以太坊上出现。通过智能合约部署的这种庞兹骗局，也称为智能庞兹骗局，已经造成了大量的经济损失和负面影响。现有的以太坊智能庞兹骗局检测方法主要依赖于智能合约的字节码特征、操作码特征、账户特征和交易行为特征，这些方法缺乏可解释性和可持续性。本文提出了SourceP，一种使用预训练模型和数据流在以太坊平台上检测智能庞兹骗局的方法。该方法只需要利用智能合约的源代码作为特征，从另一个角度探索检测智能庞兹骗局的可能性。SourceP降低了现有检测方法的数据获取和特征提取难度，同时增加了模型的可解释性。

    As blockchain technology becomes more and more popular, a typical financial scam, the Ponzi scheme, has also emerged in the blockchain platform Ethereum. This Ponzi scheme deployed through smart contracts, also known as the smart Ponzi scheme, has caused a lot of economic losses and negative impacts. Existing methods for detecting smart Ponzi schemes on Ethereum mainly rely on bytecode features, opcode features, account features, and transaction behavior features of smart contracts, and such methods lack interpretability and sustainability. In this paper, we propose SourceP, a method to detect smart Ponzi schemes on the Ethereum platform using pre-training models and data flow, which only requires using the source code of smart contracts as features to explore the possibility of detecting smart Ponzi schemes from another direction. SourceP reduces the difficulty of data acquisition and feature extraction of existing detection methods while increasing the interpretability of the model. 
    
[^105]: 离线强化学习的高效扩散策略

    Efficient Diffusion Policies for Offline Reinforcement Learning. (arXiv:2305.20081v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.20081](http://arxiv.org/abs/2305.20081)

    本论文提出了高效扩散策略（EDP）用于解决离线强化学习中的两个关键挑战，即计算效率低和难以与最大似然的强化学习算法兼容。EDP通过近似构建动作来避免运行采样链，并在实验中得到验证。

    

    离线强化学习旨在从离线数据集中学习最优策略，其中策略的参数化是关键但常常被忽视。最近，Diffusion-QL通过使用扩散模型表示策略，显著提高了离线强化学习的性能，该模型的成功依赖于参数化的马尔可夫链进行采样，但是Diffusion-QL存在两个关键限制。1）在训练过程中，通过整个马尔可夫链进行前向和反向传播计算效率低下。2）由于扩散模型的似然函数难以计算，与基于最大似然的强化学习算法（如策略梯度方法）不兼容。因此，我们提出了高效扩散策略（EDP）来克服这两个挑战。EDP在训练过程中通过从损坏的动作中近似构建动作，避免了运行采样链。我们在D4RL基准测试中进行了大量实验。结果表明，EDP能够减少扩散的潜在计算复杂性。

    Offline reinforcement learning (RL) aims to learn optimal policies from offline datasets, where the parameterization of policies is crucial but often overlooked. Recently, Diffsuion-QL significantly boosts the performance of offline RL by representing a policy with a diffusion model, whose success relies on a parametrized Markov Chain with hundreds of steps for sampling. However, Diffusion-QL suffers from two critical limitations. 1) It is computationally inefficient to forward and backward through the whole Markov chain during training. 2) It is incompatible with maximum likelihood-based RL algorithms (e.g., policy gradient methods) as the likelihood of diffusion models is intractable. Therefore, we propose efficient diffusion policy (EDP) to overcome these two challenges. EDP approximately constructs actions from corrupted ones at training to avoid running the sampling chain. We conduct extensive experiments on the D4RL benchmark. The results show that EDP can reduce the diffusion po
    
[^106]: 神经（切向核）崩溃

    Neural (Tangent Kernel) Collapse. (arXiv:2305.16427v1 [cs.LG])

    [http://arxiv.org/abs/2305.16427](http://arxiv.org/abs/2305.16427)

    本文介绍了神经切向核（NTK）和神经崩溃（NC）之间的关系，证明了在具有块状NTK的DNN中会出现NC，并通过大规模实验支持理论的正确性。

    

    本文介绍了两个重要的概念：神经切向核（NTK），它捕捉深度神经网络（DNN）训练期间的演化和神经崩溃（NC）现象，它指的是经过良好训练的分类DNN最后一层特征中对称性和结构的出现。我们假设经验NTK与类标签对齐并形成块状结构，即同一类别的样本之间的相关性比不同类别的样本更强，基于这个假设，我们推导了使用均方误差（MSE）训练的DNN动态，并将其分解为可解释的阶段。此外，我们确定了一种不变量，捕捉了动态的本质，并用它证明了在具有块状NTK的DNN中会出现NC。我们进行了三种常见DNN架构和三个基准数据集的大规模数值实验来支持我们的理论。

    This work bridges two important concepts: the Neural Tangent Kernel (NTK), which captures the evolution of deep neural networks (DNNs) during training, and the Neural Collapse (NC) phenomenon, which refers to the emergence of symmetry and structure in the last-layer features of well-trained classification DNNs. We adopt the natural assumption that the empirical NTK develops a block structure aligned with the class labels, i.e., samples within the same class have stronger correlations than samples from different classes. Under this assumption, we derive the dynamics of DNNs trained with mean squared (MSE) loss and break them into interpretable phases. Moreover, we identify an invariant that captures the essence of the dynamics, and use it to prove the emergence of NC in DNNs with block-structured NTK. We provide large-scale numerical experiments on three common DNN architectures and three benchmark datasets to support our theory.
    
[^107]: 缩放数据受限的语言模型

    Scaling Data-Constrained Language Models. (arXiv:2305.16264v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.16264](http://arxiv.org/abs/2305.16264)

    研究人员研究了在数据受限制的情况下缩放语言模型，并提出了一个计算最优性的缩放定律，考虑到重复令牌和过量参数的价值递减。

    

    现在扩展语言模型的趋势涉及增加参数计数和训练数据集大小。推断这个趋势表明，训练数据集大小可能很快就会受到互联网上可用文本数据的限制。出于此限制的动机，我们研究在数据受限制的情况下缩放语言模型。具体而言，我们运行了大量的实验，变化数据重复程度和计算预算，范围达到了9000亿个训练令牌和9亿参数模型。我们发现，在有限的数据的情况下，使用高达4次重复数据的训练与使用唯一数据相比对损失的贡献微不足道。然而，使用更多的重复数据，添加计算的价值最终会衰减为零。我们提出并经验证了一个计算最优性的缩放定律，考虑到重复令牌和过量参数的价值递减。最后，我们尝试了缓解数据稀缺的方法。

    The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity,
    
[^108]: 通过对比阅读模型和冻结大型语言模型进行视觉定位自然语言理解

    Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models. (arXiv:2305.15080v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15080](http://arxiv.org/abs/2305.15080)

    本文介绍了一种名为对比阅读模型（Cream）的新型神经架构，旨在通过捕捉复杂细节，提升大型语言模型在语言-图像理解能力方面的性能。该方法通过视觉和辅助编码器及对比特征对齐技术实现了对视觉定位上下文中语言信息的更有效理解。

    

    最近大型语言模型的进展刺激了对将其应用扩展到视觉领域的研究潮流。尽管这些模型在生成抽象图像标题和促进自然对话方面表现出了潜力，但它们在文本丰富的图像上的表现仍需要改进。本文介绍了一种名为对比阅读模型（Cream）的新型神经架构，旨在通过捕捉现有方法常常忽视的复杂细节，提升大型语言模型在语言-图像理解能力方面的性能。Cream结合了视觉和辅助编码器，并借助对比特征对齐技术来实现对图像中视觉定位上下文中语言信息的更有效理解。我们的方法弥合了视觉与语言理解之间的差距，为更复杂的文档智能助手的开发铺平了道路。通过对不同视觉定位上下文中的严格评估，我们验证了该方法的有效性。

    Recent advances in Large Language Models (LLMs) have stimulated a surge of research aimed at extending their applications to the visual domain. While these models exhibit promise in generating abstract image captions and facilitating natural conversations, their performance on text-rich images still requires improvement. In this paper, we introduce Contrastive Reading Model (Cream), a novel neural architecture designed to enhance the language-image understanding capability of LLMs by capturing intricate details that are often overlooked in existing methods. Cream combines vision and auxiliary encoders, fortified by a contrastive feature alignment technique, to achieve a more effective comprehension of language information in visually situated contexts within the images. Our approach bridges the gap between vision and language understanding, paving the way for the development of more sophisticated Document Intelligence Assistants. Through rigorous evaluations across diverse visually-sit
    
[^109]: Pre-RMSNorm和Pre-CRMSNorm Transformers: 等效和高效的Pre-LN Transformers

    Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers. (arXiv:2305.14858v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14858](http://arxiv.org/abs/2305.14858)

    Pre-RMSNorm和Pre-CRMSNorm Transformers是等效且高效的Pre-LN Transformers架构，可以统一使用两种主流归一化技术，LayerNorm和RMSNorm，从而加速和稳定Transformer模型的训练。

    

    Transformer模型在机器学习应用中取得了巨大成功。归一化技术，如Layer Normalization（LayerNorm，LN）和Root Mean Square Normalization（RMSNorm），在加速和稳定Transformer的训练中起着关键作用。虽然LayerNorm对输入向量进行重新中心化和重新缩放，而RMSNorm仅按其RMS值重新缩放向量。尽管RMSNorm在计算上更高效，但可能会损害Transformer的表示能力。目前关于首选归一化技术尚无共识，因为一些模型使用LayerNorm，而其他模型则使用RMSNorm，尤其是最近的大语言模型。将具有一种归一化的Transformer转换为另一种类型是一项具有挑战性的任务。虽然目前两种归一化类型之间存在争议，但我们提出了一种解决方案，即统一两种主流Transformer架构，Pre-LN和Pre-RMSNorm Transformers。通过去除固有的冗余均值信息来实现等效性和高效性。

    Transformers have achieved great success in machine learning applications. Normalization techniques, such as Layer Normalization (LayerNorm, LN) and Root Mean Square Normalization (RMSNorm), play a critical role in accelerating and stabilizing the training of Transformers. While LayerNorm recenters and rescales input vectors, RMSNorm only rescales the vectors by their RMS value. Despite being more computationally efficient, RMSNorm may compromise the representation ability of Transformers. There is currently no consensus regarding the preferred normalization technique, as some models employ LayerNorm while others utilize RMSNorm, especially in recent large language models. It is challenging to convert Transformers with one normalization to the other type. While there is an ongoing disagreement between the two normalization types, we propose a solution to unify two mainstream Transformer architectures, Pre-LN and Pre-RMSNorm Transformers. By removing the inherent redundant mean informat
    
[^110]: 用户对于QA系统的背景信息依赖的影响研究

    What Else Do I Need to Know? The Effect of Background Information on Users' Reliance on QA Systems. (arXiv:2305.14331v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14331](http://arxiv.org/abs/2305.14331)

    本研究调查了用户在评估模型预测时缺乏足够信息时与QA系统的交互方式，并发现即使缺乏这些信息，用户仍然过度依赖于模型的预测。然而，提供相关背景信息有助于减少对错误预测的依赖。

    

    NLP系统在通过检索相关上下文来回答问题方面表现出色。然而，随着模型越来越大，仅限于检索到的上下文来限制模型的知识或推理是不可能的且通常也不可取的。这导致了模型从中提取答案的信息与用户用来评估模型预测答案的信息之间的不匹配。在本研究中，我们研究了在缺乏足够信息来评估预测时用户如何与QA系统交互。此外，我们还询问是否添加必要的背景信息有助于减少用户对预测的过度依赖。我们的研究发现，即使在缺乏足够信息来评估模型正确性的情况下，用户仍然依赖于模型的预测。然而，提供相关背景信息有助于用户更好地发现模型错误，减少对不正确预测的依赖。而背景信息的添加也可能增加用户对模型的过度依赖。

    NLP systems have shown impressive performance at answering questions by retrieving relevant context. However, with the increasingly large models, it is impossible and often undesirable to constrain models' knowledge or reasoning to only the retrieved context. This leads to a mismatch between the information that the models access to derive the answer and the information that is available to the user to assess the model predicted answer. In this work, we study how users interact with QA systems in the absence of sufficient information to assess their predictions. Further, we ask whether adding the requisite background helps mitigate users' over-reliance on predictions. Our study reveals that users rely on model predictions even in the absence of sufficient information needed to assess the model's correctness. Providing the relevant background, however, helps users better catch model errors, reducing over-reliance on incorrect predictions. On the flip side, background information also in
    
[^111]: Dynosaur: 一种用于指令调优数据整理的动态增长模式

    Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation. (arXiv:2305.14327v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14327](http://arxiv.org/abs/2305.14327)

    Dynosaur提出了一种动态增长模式，用于自动整理指令调优数据。通过利用现有的注释数据集，Dynosaur能够以较低的API成本提供高质量的指令调优数据。

    

    指令调优已经成为增强大型语言模型（LLM）理解指令和生成适当回应能力的方法。现有方法要么手动注释，要么使用LLM（如GPT系列）生成指令调优数据。然而，它们经常忽视将指令与现有的注释数据集关联起来。在这篇论文中，我们提出了Dynosaur，一种用于自动整理指令调优数据的动态增长模式。基于现有数据集的元数据，我们使用LLM自动构建指令调优数据，通过识别相关的数据字段并生成适当的指令。通过利用现有的注释数据集，Dynosaur具有以下几个优点：1）减少了生成指令的API成本（例如，通过调用GPT-3.5-turbo生成80万个指令调优样本的成本低于12美元）；2）为指令调优提供高质量的数据（例如，表现优于Alpaca）。

    Instruction tuning has emerged to enhance the capabilities of large language models (LLMs) to comprehend instructions and generate appropriate responses. Existing methods either manually annotate or employ LLM (e.g., GPT-series) to generate data for instruction tuning. However, they often overlook associating instructions with existing annotated datasets. In this paper, we propose Dynosaur, a dynamic growth paradigm for the automatic curation of instruction-tuning data. Based on the metadata of existing datasets, we use LLMs to automatically construct instruction-tuning data by identifying relevant data fields and generating appropriate instructions.  By leveraging the existing annotated datasets, Dynosaur offers several advantages: 1) it reduces the API cost for generating instructions (e.g., it costs less than $12 USD by calling GPT-3.5-turbo for generating 800K instruction tuning samples; 2) it provides high-quality data for instruction tuning (e.g., it performs better than Alpaca a
    
[^112]: 结合全局结构知识的视觉丰富文档关系抽取方法

    Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document. (arXiv:2305.13850v1 [cs.CL])

    [http://arxiv.org/abs/2305.13850](http://arxiv.org/abs/2305.13850)

    这篇论文提出了一种结合全局结构知识的连续迭代的方式去捕获实体之间的依赖关系，以提高视觉丰富文档中关系抽取的准确性。

    

    视觉关系提取（VRE）旨在从视觉丰富的文档中提取实体之间的关系。现有方法通常基于实体特征单独预测每对实体之间的关系，但忽略了全局结构信息，即实体对之间的依赖关系。缺乏全局结构信息可能使模型难以学习长程关系，并容易产生冲突的预测结果。为了缓解这些限制，我们提出了一种GOSE框架，该框架以迭代的方式捕获实体对之间的依赖关系。给定文档的扫描图像，GOSE首先对实体对生成初步的关系预测。第二，在先前迭代的预测结果基础上，GOSE利用全局结构知识进一步整合实体表示。这种“生成-捕获-整合”模式被多次执行，以便实体之间的依赖关系能够被很好地捕获和利用。

    Visual relation extraction (VRE) aims to extract relations between entities from visuallyrich documents. Existing methods usually predict relations for each entity pair independently based on entity features but ignore the global structure information, i.e., dependencies between entity pairs. The absence of global structure information may make the model struggle to learn long-range relations and easily predict conflicted results. To alleviate such limitations, we propose a GlObal Structure knowledgeguided relation Extraction (GOSE) framework, which captures dependencies between entity pairs in an iterative manner. Given a scanned image of the document, GOSE firstly generates preliminary relation predictions on entity pairs. Secondly, it mines global structure knowledge based on prediction results of the previous iteration and further incorporates global structure knowledge into entity representations. This "generate-capture-incorporate" schema is performed multiple times so that entit
    
[^113]: 多语言摘要中的幻觉检测和缓解

    Detecting and Mitigating Hallucinations in Multilingual Summarisation. (arXiv:2305.13632v1 [cs.CL])

    [http://arxiv.org/abs/2305.13632](http://arxiv.org/abs/2305.13632)

    本文提出一种新的度量方法mFACT，可以在非英语摘要中评估其忠实性。本文还提出了一种简单有效的加权方法，可以通过跨语言转移减少摘要的幻觉问题。

    

    幻觉对于抽象摘要的神经模型的可靠性构成了重大挑战。虽然自动产生的摘要可能流畅，但通常缺乏对原始文档的忠实性。在低资源环境下，如跨语言转移，这个问题变得更加突出。由于现有的忠实性测量方法主要集中于英语，因此在跨语言环境中甚至衡量这种现象的程度也很困难。为了解决这个问题，作者首先提出了一种新的度量方法mFACT，通过从多个英语的忠实性测量结果中借鉴翻译基础知识为非英语摘要评估其忠实性。然后，他们提出了一种简单而有效的方法来通过跨语言转移减少幻觉，该方法将每个训练样本的损失乘以其忠实性得分。通过多种语言的广泛实验，作者证明了mFACT是最适合检测幻觉的度量方法。此外，他们发现他们的提出的加权方法可以缓解幻觉问题。

    Hallucinations pose a significant challenge to the reliability of neural models for abstractive summarisation. While automatically generated summaries may be fluent, they often lack faithfulness to the original document. This issue becomes even more pronounced in low-resource settings, such as cross-lingual transfer. With the existing faithful metrics focusing on English, even measuring the extent of this phenomenon in cross-lingual settings is hard. To address this, we first develop a novel metric, mFACT, evaluating the faithfulness of non-English summaries, leveraging translation-based transfer from multiple English faithfulness metrics. We then propose a simple but effective method to reduce hallucinations with a cross-lingual transfer, which weighs the loss of each training example by its faithfulness score. Through extensive experiments in multiple languages, we demonstrate that mFACT is the metric that is most suited to detect hallucinations. Moreover, we find that our proposed l
    
[^114]: 平方神经分布族：一种新的可计算密度模型类

    Squared Neural Families: A New Class of Tractable Density Models. (arXiv:2305.13552v1 [cs.LG])

    [http://arxiv.org/abs/2305.13552](http://arxiv.org/abs/2305.13552)

    提出一种新的可计算密度模型类——平方神经分布族，其通过对神经网络的2范数进行平方和基于某个基础度量进行归一化，严格推广了经典指数族，具有闭性条件推断和可计算的边际分布。

    

    概率分布的灵活模型是许多机器学习任务的重要组成部分。我们开发并研究了一种新的概率分布类别，称为平方神经分布族（SNEFY），通过对神经网络的2范数进行平方并基于某个基础度量进行归一化。类似于无穷宽的神经网络和高斯过程之间的广泛联系的推理，我们展示了在许多感兴趣的情况下，SNEFY具有封闭形式的标准化常数，因此是灵活且完全可计算密度模型。SNEFY严格推广了经典的指数族，对于条件推断具有闭性，并且具有可计算的边际分布。我们在各种密度估计和条件密度估计任务中展示其实用性。

    Flexible models for probability distributions are an essential ingredient in many machine learning tasks. We develop and investigate a new class of probability distributions, which we call a Squared Neural Family (SNEFY), formed by squaring the 2-norm of a neural network and normalising it with respect to a base measure. Following the reasoning similar to the well established connections between infinitely wide neural networks and Gaussian processes, we show that SNEFYs admit a closed form normalising constants in many cases of interest, thereby resulting in flexible yet fully tractable density models. SNEFYs strictly generalise classical exponential families, are closed under conditioning, and have tractable marginal distributions. Their utility is illustrated on a variety of density estimation and conditional density estimation tasks. Software available at https://github.com/RussellTsuchida/snefy.
    
[^115]: 多模态自动事实核查：一份调查

    Multimodal Automated Fact-Checking: A Survey. (arXiv:2305.13507v1 [cs.CL])

    [http://arxiv.org/abs/2305.13507](http://arxiv.org/abs/2305.13507)

    本调查提出了一个多模态自动事实核查的框架，并包括了独特的子任务，重点关注了文本，图像，音频和视频这四种模态的现实应用。纪录了相关的基准模型，讨论了未来研究的局限性和前景。

    

    错误信息，即事实上不正确的信息，通常以多种形式传达，例如带有标题的图像。 它被人们视为更可信，比其仅限于文本的对应物扩散速度更快，范围更广。 尽管越来越多的研究涉及自动事实核查（AFC），但以往的调查主要集中在文本误导方面。 在本调查中，我们构建了一个包括多模态误导独特子任务在内的AFC框架。此外，我们在我们的框架上讨论了不同社区所发展的相关术语。 我们重点关注现实世界事实核查中存在的四种模态：文本，图像，音频和视频。 我们调查了基准和模型，并讨论了未来研究的局限性和有前途的方向。

    Misinformation, i.e. factually incorrect information, is often conveyed in multiple modalities, e.g. an image accompanied by a caption. It is perceived as more credible by humans, and spreads faster and wider than its text-only counterparts. While an increasing body of research investigates automated fact-checking (AFC), previous surveys mostly focus on textual misinformation. In this survey, we conceptualise a framework for AFC including subtasks unique to multimodal misinformation. Furthermore, we discuss related terminological developed in different communities in the context of our framework. We focus on four modalities prevalent in real-world fact-checking: text, image, audio, and video. We survey benchmarks and models, and discuss limitations and promising directions for future research.
    
[^116]: 多粒度超图兴趣建模对话式推荐算法

    Multi-grained Hypergraph Interest Modeling for Conversational Recommendation. (arXiv:2305.04798v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2305.04798](http://arxiv.org/abs/2305.04798)

    本文提出了一种多粒度超图兴趣建模方法，通过利用历史对话数据丰富当前对话的上下文，从不同角度捕捉用户兴趣。采用超图结构表示复杂的语义关系，建模用户的历史对话会话，捕捉粗粒度的会话级关系。

    

    对话式推荐系统通过自然语言的多轮对话与用户进行交互，旨在为用户的即时信息需求提供高质量的推荐。尽管已经做出了很多有效的对话式推荐系统，但大多数仍然集中在当前对话的上下文信息上，通常会遇到数据稀缺的问题。因此，我们考虑利用历史对话数据来丰富当前对话的有限上下文。在本文中，我们提出了一种新颖的多粒度超图兴趣建模方法，以从不同的角度捕捉复杂历史数据下的用户兴趣。作为核心思想，我们使用超图来表示历史对话中复杂的语义关系。在我们的方法中，我们首先使用超图结构来建模用户的历史对话会话，并形成一个基于会话的超图，该超图捕捉了粗粒度的会话级关系。

    Conversational recommender system (CRS) interacts with users through multi-turn dialogues in natural language, which aims to provide high-quality recommendations for user's instant information need. Although great efforts have been made to develop effective CRS, most of them still focus on the contextual information from the current dialogue, usually suffering from the data scarcity issue. Therefore, we consider leveraging historical dialogue data to enrich the limited contexts of the current dialogue session.  In this paper, we propose a novel multi-grained hypergraph interest modeling approach to capture user interest beneath intricate historical data from different perspectives. As the core idea, we employ hypergraph to represent complicated semantic relations underlying historical dialogues. In our approach, we first employ the hypergraph structure to model users' historical dialogue sessions and form a session-based hypergraph, which captures coarse-grained, session-level relation
    
[^117]: NLI4CT：面向临床试验报告的多证据自然语言推理

    NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports. (arXiv:2305.03598v1 [cs.CL])

    [http://arxiv.org/abs/2305.03598](http://arxiv.org/abs/2305.03598)

    本文提出了一种面向临床试验报告的自然语言推理模型，通过检索支持事实来确定自然语言陈述和CTR之间的推理关系。

    

    如何解释和检索用于支持临床决策的医学证据？多年来，积累下来的临床试验报告包含了发展个性化医学所必需的信息。然而，为了找到最佳的实验治疗证据，手动检查超过400,000个临床试验报告是实际上不可行的。自然语言推理（NLI）提供了一个潜在的解决方案，通过允许可扩展计算文本蕴含关系。然而，现有的NLI模型在生物医学语料库上表现不佳，之前发布的数据集无法捕捉CTR推理的全部复杂性。在本文中，我们提出了一种新的资源，以推进关于CTR推理的NLI研究。该资源包括两个主要任务。首先，确定自然语言陈述和CTR之间的推理关系。其次，检索支持事实以证明预测的关系。我们提供了NLI4CT，一个基于CTR的语料库。

    How can we interpret and retrieve medical evidence to support clinical decisions? Clinical trial reports (CTR) amassed over the years contain indispensable information for the development of personalized medicine. However, it is practically infeasible to manually inspect over 400,000+ clinical trial reports in order to find the best evidence for experimental treatments. Natural Language Inference (NLI) offers a potential solution to this problem, by allowing the scalable computation of textual entailment. However, existing NLI models perform poorly on biomedical corpora, and previously published datasets fail to capture the full complexity of inference over CTRs. In this work, we present a novel resource to advance research on NLI for reasoning on CTRs. The resource includes two main tasks. Firstly, to determine the inference relation between a natural language statement, and a CTR. Secondly, to retrieve supporting facts to justify the predicted relation. We provide NLI4CT, a corpus of
    
[^118]: 语言、时间偏好和消费行为：大型语言模型的证据

    Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models. (arXiv:2305.02531v1 [econ.GN])

    [http://arxiv.org/abs/2305.02531](http://arxiv.org/abs/2305.02531)

    本研究分析了大型语言模型在不同语言提示下的奖励时间偏好，并发现GPT在具有较弱未来时态的语言下表现出更大的耐心，这与使用该语言的人类的偏好相似。

    

    语言对我们对时间和奖励的感知有很大的影响。这引发了一个问题，即当以不同的语言询问大型语言模型时，它们是否显示出不同的奖励时间偏好，并且它们的选择是否类似于人类的选择。本研究分析了GPT-3.5（以下简称GPT）在多种语言提示下的响应，探索了较小、较早的奖励和较大、较晚的奖励之间的偏好。我们的结果显示，当以语义含义较弱的未来时态参考（FTR），如德语和汉语，为提示语时，GPT表现出更大的耐心，相比英语和法语等具有强大FTR的语言。这些发现与现有文献一致，并表明了GPT的选择与这些语言的使用者的偏好之间的关联。然而，进一步的分析揭示了较早或较晚奖励的偏好并没有随着奖励差异系统地改变，这表明了一种词典序优先的选择。

    Language has a strong influence on our perceptions of time and rewards. This raises the question of whether large language models, when asked in different languages, show different preferences for rewards over time and if their choices are similar to those of humans. In this study, we analyze the responses of GPT-3.5 (hereafter referred to as GPT) to prompts in multiple languages, exploring preferences between smaller, sooner rewards and larger, later rewards. Our results show that GPT displays greater patience when prompted in languages with weak future tense references (FTR), such as German and Mandarin, compared to languages with strong FTR, like English and French. These findings are consistent with existing literature and suggest a correlation between GPT's choices and the preferences of speakers of these languages. However, further analysis reveals that the preference for earlier or later rewards does not systematically change with reward gaps, indicating a lexicographic preferen
    
[^119]: 分解增强推理的自我评估引导解码

    Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding. (arXiv:2305.00633v1 [cs.CL])

    [http://arxiv.org/abs/2305.00633](http://arxiv.org/abs/2305.00633)

    本论文提出了一种通过自我评估引导解码提高推理的方法，使用经过校准的自动标准探索推理搜索空间，使搜索能够产生更高质量的最终预测结果；使用自我评估引导的随机束搜索在产生推理链的质量和多样性之间平衡权衡，适应多数投票，并且可以准确判断逻辑错误，提高一致性和鲁棒性。

    

    我们提出了一种有效的提示方法，通过随机束搜索结合自我评估引导。我们的方法使用经过校准的自动标准探索推理搜索空间。这使得有效搜索能够产生更高质量的最终预测结果。使用自我评估引导的随机束搜索，我们在产生推理链的质量和多样性之间平衡权衡，从而能够适应多数投票，并在GSM8K、AQUA和StrategyQA基准测试中以少量示例准确性分别超越对应的Codex-backboned基线$6.34\%$、$9.56\%$和$5.46\%$。对我们的分解式推理分析发现，它可以指出逻辑错误并导致更高的一致性和鲁棒性。

    We propose an effective prompting approach that integrates self-evaluation guidance through stochastic beam search. Our approach explores the reasoning search space using a well-calibrated automatic criterion. This enables an efficient search to produce higher-quality final predictions. With the self-evaluation guided stochastic beam search, we also balance the quality--diversity trade-off in the generation of reasoning chains. This allows our approach to adapt well with majority voting and surpass the corresponding Codex-backboned baselines by $6.34\%$, $9.56\%$, and $5.46\%$ on the GSM8K, AQUA, and StrategyQA benchmarks, respectively, in few-shot accuracy. Analysis of our decompositional reasoning finds it pinpoints logic failures and leads to higher consistency and robustness.
    
[^120]: 用电影知识和用户网络检测影评中的剧透

    Detecting Spoilers in Movie Reviews with External Movie Knowledge and User Networks. (arXiv:2304.11411v1 [cs.AI])

    [http://arxiv.org/abs/2304.11411](http://arxiv.org/abs/2304.11411)

    该研究提出了一种新型多视角剧透检测框架，MVSD，该框架将电影知识和用户活动纳入考虑，并且在LCS数据集上展示了其优于强基线的相关实验结果。

    

    在线电影评论平台为电影产业和公众提供众包反馈，但剧透评论严重损害了用户体验。尽管已经进行了初步的研究以自动识别剧透，但其仅关注于评论内容本身，而强大的剧透检测需要将评论放入关于电影的事实和知识、用户在电影评论平台上的行为等上下文中。因此，在我们首先整理了一个基于网络的剧透检测数据集LCS和一个全面的最新电影知识库UKM之后，我们提出了MVSD，一种考虑到有关电影和用户活动的外部知识的新型多视角剧透检测框架。具体而言，MVSD构建三个互联的异构信息网络来模拟多样化的数据来源及其多视图属性，而我们设计并采用了一种新型的异构图卷积神经网络（HGCN）来融合多视图嵌入并预测剧透。我们在LCS数据集上进行了广泛的实验，并展示了MVSD在准确性和F1得分方面显著优于强基线。此外，我们还在一个流行的电影评论网站上展示了MVSD在真实剧透识别任务上的功效。

    Online movie review platforms are providing crowdsourced feedback for the film industry and the general public, while spoiler reviews greatly compromise user experience. Although preliminary research efforts were made to automatically identify spoilers, they merely focus on the review content itself, while robust spoiler detection requires putting the review into the context of facts and knowledge regarding movies, user behavior on film review platforms, and more. In light of these challenges, we first curate a large-scale network-based spoiler detection dataset LCS and a comprehensive and up-to-date movie knowledge base UKM. We then propose MVSD, a novel Multi-View Spoiler Detection framework that takes into account the external knowledge about movies and user activities on movie review platforms. Specifically, MVSD constructs three interconnecting heterogeneous information networks to model diverse data sources and their multi-view attributes, while we design and employ a novel heter
    
[^121]: Remote Sensing Change Captioning的关注网络：对远程感知变化描述的改进

    Changes to Captions: An Attentive Network for Remote Sensing Change Captioning. (arXiv:2304.01091v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.01091](http://arxiv.org/abs/2304.01091)

    本研究提出了一个关注变化到描述网络，用于准确描述遥感图像中的变化。与自然图像的变化描述不同，遥感变化描述需要捕捉影响因素的最显著变化。

    

    近年来，先进的研究集中在使用自然语言处理（NLP）技术直接学习和分析遥感图像。准确描述多时相遥感图像中发生的变化对于地理空间理解和土地规划变得越来越重要。与自然图像变化描述任务不同，遥感变化描述旨在捕捉最显著的变化，不受照明、季节效应和复杂地物覆盖等各种影响因素的限制。在本研究中，我们强调准确描述遥感图像变化的重要性，并对自然和合成图像以及遥感图像的变化描述任务进行了比较。为了应对生成准确描述的挑战，我们提出了一个称为Chg2Cap的关注变化到描述网络，用于处理两时相的遥感图像。该网络由三个主要部分组成。

    In recent years, advanced research has focused on the direct learning and analysis of remote sensing images using natural language processing (NLP) techniques. The ability to accurately describe changes occurring in multi-temporal remote sensing images is becoming increasingly important for geospatial understanding and land planning. Unlike natural image change captioning tasks, remote sensing change captioning aims to capture the most significant changes, irrespective of various influential factors such as illumination, seasonal effects, and complex land covers. In this study, we highlight the significance of accurately describing changes in remote sensing images and present a comparison of the change captioning task for natural and synthetic images and remote sensing images. To address the challenge of generating accurate captions, we propose an attentive changes-to-captions network, called Chg2Cap for short, for bi-temporal remote sensing images. The network comprises three main com
    
[^122]: 时序和非时序数据因果发现方法综述

    A Survey on Causal Discovery Methods for Temporal and Non-Temporal Data. (arXiv:2303.15027v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.15027](http://arxiv.org/abs/2303.15027)

    本文综述了时序和非时序数据因果发现的方法，探讨了不同算法在不同情况下识别因果边缘的能力。同时还介绍了可用于评估因果发现方法性能的基准数据集和常见指标。

    

    因果发现（CD）是从数据中识别系统变量间因果关系的过程。多年来，已经开发了几种方法，主要基于数据的统计特性来揭示潜在的因果机制。本文对设计用于处理独立同分布（i.i.d.）数据和时间序列数据的因果发现方法进行了广泛探讨。为此，我们首先介绍了因果发现中的常用术语，然后全面讨论了在不同情况下识别因果边缘的算法。我们进一步讨论了可用于评估因果发现方法性能的基准数据集，可用于执行因果发现的工具或软件包以及评估这些方法的常见指标。我们还在不同基准数据集上测试了一些常见因果发现算法。

    Causal Discovery (CD) is the process of identifying the cause-effect relationships among the variables of a system from data. Over the years, several methods have been developed primarily based on the statistical properties of data to uncover the underlying causal mechanism. In this study, we present an extensive discussion on the methods designed to perform causal discovery from both independent and identically distributed (i.i.d.) data and time series data. For this purpose, we first introduce the common terminologies in causal discovery, and then provide a comprehensive discussion of the algorithms designed to identify the causal edges in different settings. We further discuss some of the benchmark datasets available for evaluating the performance of the causal discovery methods, available tools or software packages to perform causal discovery readily, and the common metrics used to evaluate these methods. We also test some common causal discovery algorithms on different benchmark d
    
[^123]: TriPlaneNet：一种EG3D反演的编码器

    TriPlaneNet: An Encoder for EG3D Inversion. (arXiv:2303.13497v1 [cs.CV])

    [http://arxiv.org/abs/2303.13497](http://arxiv.org/abs/2303.13497)

    本研究介绍了一种实时方法TriPlaneNet，通过直接利用EG3D生成模型的三平面表示，建立在一个用于潜在编码的前馈卷积编码器上，并扩展了一个完全卷积的三平面数值偏移预测器，旨在弥合现有的GAN反演方法的差距。

    

    最近，基于NeRF的GAN取得了很大的进展，在高分辨率和高保真度的生成建模中引入了许多方法，可以进行新颖的视角渲染。与此同时，为了能够重新渲染或修改现有的图像或视频，必须解决一个反问题。尽管对于2D GAN反演而言，通用的基于优化的方法取得了成功，但对于3D GAN反演，这些方法可能无法产生3D一致的渲染。而像StyleGAN这样的快速编码器技术，可能也不太吸引人，因为它们缺乏身份保留能力。在我们的工作中，我们介绍了一种实时方法，通过直接利用为EG3D生成模型引入的三平面表示，弥合了这两种方法之间的差距。具体而言，我们建立在一个用于潜在编码的前馈卷积编码器上，并扩展了一个完全卷积的三平面数值偏移预测器。正如我们的工作所显示的那样，渲染结果相似。

    Recent progress in NeRF-based GANs has introduced a number of approaches for high-resolution and high-fidelity generative modeling of human heads with a possibility for novel view rendering. At the same time, one must solve an inverse problem to be able to re-render or modify an existing image or video. Despite the success of universal optimization-based methods for 2D GAN inversion, those, applied to 3D GANs, may fail to produce 3D-consistent renderings. Fast encoder-based techniques, such as those developed for StyleGAN, may also be less appealing due to the lack of identity preservation. In our work, we introduce a real-time method that bridges the gap between the two approaches by directly utilizing the tri-plane representation introduced for EG3D generative model. In particular, we build upon a feed-forward convolutional encoder for the latent code and extend it with a fully-convolutional predictor of tri-plane numerical offsets. As shown in our work, the renderings are similar in
    
[^124]: 基于惩罚的模仿学习和跨语义生成传感器融合技术在自动驾驶中的应用

    Penalty-Based Imitation Learning With Cross Semantics Generation Sensor Fusion for Autonomous Driving. (arXiv:2303.11888v1 [cs.RO])

    [http://arxiv.org/abs/2303.11888](http://arxiv.org/abs/2303.11888)

    本文介绍了一种基于惩罚的模仿学习方法和特征级多传感器融合技术，应用于自动驾驶导航中。文中重点介绍了针对激光雷达和RGB信息的融合技术，旨在提高模型对交通规则的遵守能力。

    

    随着模式识别和计算机视觉技术的快速发展，目标检测、语义分割等任务的准确度已经超过人类。自动驾驶作为一项重要的研究方向，旨在彻底改变未来的交通和出行方式。传感器对于自动驾驶的安全性和环境感知的可行性至关重要。多传感器融合由于其多维感知和集成能力的潜力而成为当前研究的热点。本文提出了一种新的特征级多传感器融合技术，用于端到端的自动驾驶导航和模仿学习。我们的论文主要关注于激光雷达和RGB信息的融合技术。我们还提供了一种全新的基于惩罚的模仿学习方法，以加强模型遵守交通规则的能力并统一模仿学习的目标。

    With the rapid development of Pattern Recognition and Computer Vision technologies, tasks like object detection or semantic segmentation have achieved even better accuracy than human beings. Based on these solid foundations, autonomous driving is becoming an important research direction, aiming to revolute the future of transportation and mobility. Sensors are critical to autonomous driving's security and feasibility to perceive the surrounding environment. Multi-Sensor fusion has become a current research hot spot because of its potential for multidimensional perception and integration ability. In this paper, we propose a novel feature-level multi-sensor fusion technology for end-to-end autonomous driving navigation with imitation learning. Our paper mainly focuses on fusion technologies for Lidar and RGB information. We also provide a brand-new penalty-based imitation learning method to reinforce the model's compliance with traffic rules and unify the objective of imitation learning 
    
[^125]: 什么让数据适合于局部连接神经网络？一种基于量子纠缠的必要且充分条件

    What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement. (arXiv:2303.11249v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.11249](http://arxiv.org/abs/2303.11249)

    本文通过采用量子物理学的理论工具，提出了一种判定数据适合于局部连接神经网络的必要且充分条件，并导出了一种相应的预处理方法。

    

    关于数据分布适用于深度学习的问题是一个基本的开放性问题。本文采用来自量子物理学的理论工具，针对包括卷积神经网络、循环神经网络和局部自注意力模型在内的广泛的局部连接神经网络，解决了这个问题。我们的主要理论结果是，在某些特征的规范划分下，当数据分布接受低量子纠缠时，特定的局部连接神经网络才能够准确地预测该数据分布。作为本结果的实际应用，我们导出了一种预处理方法，以增强数据分布适合局部连接神经网络的性能。在各种数据集上对广泛的模型进行实验，证明了我们的发现。我们希望我们使用量子纠缠将鼓励形式推理的物理工具来进一步采用。

    The question of what makes a data distribution suitable for deep learning is a fundamental open problem. Focusing on locally connected neural networks (a prevalent family of architectures that includes convolutional and recurrent neural networks as well as local self-attention models), we address this problem by adopting theoretical tools from quantum physics. Our main theoretical result states that a certain locally connected neural network is capable of accurate prediction over a data distribution if and only if the data distribution admits low quantum entanglement under certain canonical partitions of features. As a practical application of this result, we derive a preprocessing method for enhancing the suitability of a data distribution to locally connected neural networks. Experiments with widespread models over various datasets demonstrate our findings. We hope that our use of quantum entanglement will encourage further adoption of tools from physics for formally reasoning about 
    
[^126]: "Wasserstein Believer:通过可靠的潜在空间模型学习部分可观测环境下的信念更新"

    The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models. (arXiv:2303.03284v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03284](http://arxiv.org/abs/2303.03284)

    本论文提出了一种“Wasserstein Belief Updater”算法来学习部分可观测环境下的信念更新，该算法通过学习POMDP​​的潜在模型和置信更新的近似值，实现了对历史观察和行动的有效压缩，提升了算法的性能表现。

    

    部分可观测马尔可夫决策过程（POMDP）是建模代理无法感知到完整状态的环境的有用工具。因此，代理需要考虑过去的观察和行动进行推理。但是，由于历史空间指数级增长，仅仅记住完整历史通常是不可行的。保持模拟真实状态的置信概率分布可以作为历史的充分统计量，但其计算需要访问环境的模型，因此也是不可行的。最先进的算法使用递归神经网络来压缩观察-行动历史以学习充分的统计量，但它们缺乏成功的保证并可能导致次优策略。为了克服这一点，我们提出了Wasserstein Belief Updater ，这是一种RL算法，它学习POMDP​​的潜在模型和置信更新的近似值。我们的方法具有理论成果。

    Partially Observable Markov Decision Processes (POMDPs) are useful tools to model environments where the full state cannot be perceived by an agent. As such the agent needs to reason taking into account the past observations and actions. However, simply remembering the full history is generally intractable due to the exponential growth in the history space. Keeping a probability distribution that models the belief over what the true state is can be used as a sufficient statistic of the history, but its computation requires access to the model of the environment and is also intractable. State-of-the-art algorithms use Recurrent Neural Networks to compress the observation-action history aiming to learn a sufficient statistic, but they lack guarantees of success and can lead to sub-optimal policies. To overcome this, we propose the Wasserstein Belief Updater, an RL algorithm that learns a latent model of the POMDP and an approximation of the belief update. Our approach comes with theoreti
    
[^127]: 使用预训练的视觉-语言模型进行开放世界目标操作

    Open-World Object Manipulation using Pre-trained Vision-Language Models. (arXiv:2303.00905v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.00905](http://arxiv.org/abs/2303.00905)

    本文研究了使用预训练的视觉-语言模型来实现机器人的开放世界目标操作，从而解决了机器人在面对丰富语义信息时的困难挑战。

    

    为了让机器人能够按照人们的指令行动，它们必须能够将人类词汇中的丰富语义信息（例如：“你能给我拿来粉色的毛绒鲸鱼吗？”）与它们的感知观察和行动相连接。这对机器人来说带来了一个明显困难的挑战：尽管机器人学习方法能够让机器人从第一手经验中学习到许多不同的行为，但机器人不可能亲身体验到涵盖所有语义信息的情况。我们希望机器人的策略能够感知和拾取粉色的毛绒鲸鱼，即使它以前从未见过与毛绒鲸鱼互动的数据。幸运的是，互联网上的静态数据拥有丰富的语义信息，并且这些信息被捕捉在预训练的视觉-语言模型中。在本文中，我们研究了是否可以将机器人策略与这些预训练模型进行接口连接，以便让机器人完成涉及机器人以前没有见过的物体类别的指令。

    For robots to follow instructions from people, they must be able to connect the rich semantic information in human vocabulary, e.g. "can you get me the pink stuffed whale?" to their sensory observations and actions. This brings up a notably difficult challenge for robots: while robot learning approaches allow robots to learn many different behaviors from first-hand experience, it is impractical for robots to have first-hand experiences that span all of this semantic information. We would like a robot's policy to be able to perceive and pick up the pink stuffed whale, even if it has never seen any data interacting with a stuffed whale before. Fortunately, static data on the internet has vast semantic information, and this information is captured in pre-trained vision-language models. In this paper, we study whether we can interface robot policies with these pre-trained models, with the aim of allowing robots to complete instructions involving object categories that the robot has never s
    
[^128]: DualStreamFoveaNet: 一种具有解剖意识的双流融合架构用于鲁棒的中央凹点定位

    DualStreamFoveaNet: A Dual Stream Fusion Architecture with Anatomical Awareness for Robust Fovea Localization. (arXiv:2302.06961v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.06961](http://arxiv.org/abs/2302.06961)

    DualStreamFoveaNet是一种具有解剖意识的双流融合架构，通过利用视网膜和血管分布进行多线索融合，实现对鲁棒的中央凹点定位。实验证明该架构在中央凹点定位方面达到了最先进的性能。

    

    准确的中央凹点定位对于分析视网膜疾病以预防不可逆视力损失至关重要。当前基于深度学习的方法虽然优于传统方法，但仍面临一些挑战，如中央凹点周围局部解剖标记的缺失、不能鲁棒地处理病变视网膜图像和图像条件的变化。本文提出了一种新颖的基于Transformer的架构称为DualStreamFoveaNet (DSFN)用于多线索融合。该架构明确地利用视网膜和血管分布来实现长程连接和全局特征的融合，实现鲁棒的中央凹点定位。我们在双流编码器中引入了一种空间注意机制，用于提取和融合自学习的解剖信息，更注重分布在血管沿线的特征，并通过减少令牌数量显著降低计算成本。我们的广泛实验结果表明，所提出的架构达到了最先进的性能。

    Accurate fovea localization is essential for analyzing retinal diseases to prevent irreversible vision loss. While current deep learning-based methods outperform traditional ones, they still face challenges such as the lack of local anatomical landmarks around the fovea, the inability to robustly handle diseased retinal images, and the variations in image conditions. In this paper, we propose a novel transformer-based architecture called DualStreamFoveaNet (DSFN) for multi-cue fusion. This architecture explicitly incorporates long-range connections and global features using retina and vessel distributions for robust fovea localization. We introduce a spatial attention mechanism in the dual-stream encoder to extract and fuse self-learned anatomical information, focusing more on features distributed along blood vessels and significantly reducing computational costs by decreasing token numbers. Our extensive experiments show that the proposed architecture achieves state-of-the-art perform
    
[^129]: 阅读并获得回报：在与指导手册的帮助下学习玩Atari游戏

    Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals. (arXiv:2302.04449v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04449](http://arxiv.org/abs/2302.04449)

    本论文提出了阅读并奖励的框架，通过阅读Atari游戏开发者发布的指导手册，以提高强化学习算法在Atari游戏中的效率。该框架包含一个QA提取模块和一个推理模块，能够从指导手册中提取关键信息，并评估物体与智能体的交互效果。

    

    长期以来，高样本复杂性一直是强化学习面临的挑战。然而，人类学习执行任务的方式不仅仅是通过交互或演示，还包括阅读非结构化文本文档，例如指导手册。指导手册和维基页面是最丰富的数据之一，它们可以提供有关宝贵特征、策略、任务特定的环境动态和奖励结构的信息，因此我们假设利用人写的指导手册来帮助学习特定任务的策略将导致更高效和更优秀的智能体。我们提出了阅读并奖励的框架。阅读并奖励通过阅读Atari游戏开发者发布的指导手册来加速RL算法。我们的框架包括一个QA提取模块，用于提取和总结指导手册中的相关信息，以及一个推理模块，根据指导手册中的信息评估物体-智能体的交互效果。一个辅助的反馈机制可以提高效果。

    High sample complexity has long been a challenge for RL. On the other hand, humans learn to perform tasks not only from interaction or demonstrations, but also by reading unstructured text documents, e.g., instruction manuals. Instruction manuals and wiki pages are among the most abundant data that could inform agents of valuable features and policies or task-specific environmental dynamics and reward structures. Therefore, we hypothesize that the ability to utilize human-written instruction manuals to assist learning policies for specific tasks should lead to a more efficient and better-performing agent. We propose the Read and Reward framework. Read and Reward speeds up RL algorithms on Atari games by reading manuals released by the Atari game developers. Our framework consists of a QA Extraction module that extracts and summarizes relevant information from the manual and a Reasoning module that evaluates object-agent interactions based on information from the manual. An auxiliary re
    
[^130]: 基于关系Weisfeiler-Leman的链路预测理论

    A Theory of Link Prediction via Relational Weisfeiler-Leman. (arXiv:2302.02209v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02209](http://arxiv.org/abs/2302.02209)

    本文提出了一种基于关系Weisfeiler-Leman算法的理论，为知识图谱中的图神经网络提供了理论解释，并对各种模型的表达能力进行了表征，并解释了一些广泛采用的实际设计选择的优点。

    

    图神经网络是用于图结构数据表示学习的重要模型。尽管我们已经很好地理解了这些模型在简单图上的能力和局限性，但对于知识图谱，我们的理解仍然不完整。本文的目标是为知识图谱中的图神经网络提供系统性的理解，以解决链路预测等重要任务。我们的分析涉及一种统一的视角、看似不相关的模型，并解锁了一系列其他模型。通过相应的关系Weisfeiler-Leman算法，表征了各种模型的表达能力。此分析被扩展以对图神经网络类别捕捉的函数类进行精确逻辑描述。提出的理论发现解释了一些广泛采用的实际设计选择的优点，并得到了经验验证。

    Graph neural networks are prominent models for representation learning over graph-structured data. While the capabilities and limitations of these models are well-understood for simple graphs, our understanding remains incomplete in the context of knowledge graphs. Our goal is to provide a systematic understanding of the landscape of graph neural networks for knowledge graphs pertaining to the prominent task of link prediction. Our analysis entails a unifying perspective on seemingly unrelated models and unlocks a series of other models. The expressive power of various models is characterized via a corresponding relational Weisfeiler-Leman algorithm. This analysis is extended to provide a precise logical characterization of the class of functions captured by a class of graph neural networks. The theoretical findings presented in this paper explain the benefits of some widely employed practical design choices, which are validated empirically.
    
[^131]: 分布鲁棒安全强化学习的风险厌恶模型不确定性

    Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning. (arXiv:2301.12593v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12593](http://arxiv.org/abs/2301.12593)

    该论文提出了一种分布鲁棒安全强化学习框架，通过使用失真风险度量来处理模型不确定性。该方法不需要极小极大优化，具有高效且不依赖模型的特点。实验证明该框架能够在具有安全约束的控制任务中实现稳健的性能和安全性。

    

    许多现实领域需要在不确定的环境中进行安全决策。本研究引入了一个深度强化学习框架来解决这个重要问题。我们考虑过渡模型的分布，并通过使用相干失真风险度量来对模型不确定性采取风险厌恶的观点。我们通过展示它等价于一类特定的分布鲁棒安全强化学习问题，为这个框架提供了鲁棒性保证。然而，与现有的深度强化学习鲁棒性方法不同，我们的表达不涉及极小化最大优化。这导致我们的方法可以高效、不依赖模型地在单个训练环境中仅需要标准数据收集来实施。在具有安全约束的连续控制任务的实验中，我们证明了我们的框架在部署时能够产生稳健的性能和安全性。

    Many real-world domains require safe decision making in uncertain environments. In this work, we introduce a deep reinforcement learning framework for approaching this important problem. We consider a distribution over transition models, and apply a risk-averse perspective towards model uncertainty through the use of coherent distortion risk measures. We provide robustness guarantees for this framework by showing it is equivalent to a specific class of distributionally robust safe reinforcement learning problems. Unlike existing approaches to robustness in deep reinforcement learning, however, our formulation does not involve minimax optimization. This leads to an efficient, model-free implementation of our approach that only requires standard data collection from a single training environment. In experiments on continuous control tasks with safety constraints, we demonstrate that our framework produces robust performance and safety at deployment time across a range of perturbed test e
    
[^132]: 通过学习保证提高公平性

    Increasing Fairness via Combination with Learning Guarantees. (arXiv:2301.10813v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10813](http://arxiv.org/abs/2301.10813)

    该论文提出了一种公平质量度量方法，名为判别风险，旨在反映个体和群体公平性。此外，研究者还讨论了公平性是否可以在理论上得到保证。

    

    随着机器学习系统在越来越多的现实场景中得到广泛应用，对于隐藏在机器学习模型中的潜在歧视的担忧正在增加。许多技术已经被开发出来以增强公平性，包括常用的群体公平性度量和几种结合集成学习的公平感知方法。然而，现有的公平度量只能关注其中之一，即群体公平性或个体公平性，它们之间的硬性兼容性暗示了即使其中之一得到满足，仍可能存在偏见。此外，现有的提升公平性的机制通常只提供经验结果来证明其有效性，但很少有论文讨论公平性是否可以在理论上得到保证。为了解决这些问题，本文提出了一种公平质量度量方法——判别风险，以反映个体和群体公平性两个方面。此外，我们还研究了p...

    The concern about underlying discrimination hidden in ML models is increasing, as ML systems have been widely applied in more and more real-world scenarios and any discrimination hidden in them will directly affect human life. Many techniques have been developed to enhance fairness including commonly-used group fairness measures and several fairness-aware methods combining ensemble learning. However, existing fairness measures can only focus on one aspect -- either group or individual fairness, and the hard compatibility among them indicates a possibility of remaining biases even if one of them is satisfied. Moreover, existing mechanisms to boost fairness usually present empirical results to show validity, yet few of them discuss whether fairness can be boosted with certain theoretical guarantees. To address these issues, we propose a fairness quality measure named discriminative risk in this paper to reflect both individual and group fairness aspects. Furthermore, we investigate the p
    
[^133]: 大型语言模型中的事件知识：不可能性和不太可能性之间的差距

    Event knowledge in large language models: the gap between the impossible and the unlikely. (arXiv:2212.01488v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.01488](http://arxiv.org/abs/2212.01488)

    大型语言模型拥有丰富的事件知识，几乎总是将可能事件的描述比不可能事件的描述赋予更高的可能性。

    

    语言语料库中的词共现模式包含着意想不到的概念知识。通过训练大型语言模型(LLMs)来预测上下文中的词语，这些模型能够利用这些模式，在需要世界知识的各种语义任务上取得令人印象深刻的性能。关于LLMs的语义能力的重要但鲜为研究的问题是它们是否获得了常见事件的一般化知识。在这里，我们测试了五个预训练的LLMs（从2018年的BERT到2023年的MPT）是否比同一事件的不太可能的版本更可能地分配给合理的代理-患者相互作用。使用三个精心策划的最小句对集合（总数n=1,215），我们发现预训练的LLMs拥有相当大的事件知识，表现优于其他分布式语言模型。特别是，它们几乎总是将可能事件与不可能事件相比赋予更高的可能性（教师买了笔记本电脑相对于笔记本电脑买了教师）。

    Word co-occurrence patterns in language corpora contain a surprising amount of conceptual knowledge. Large language models (LLMs), trained to predict words in context, leverage these patterns to achieve impressive performance on diverse semantic tasks requiring world knowledge. An important but understudied question about LLMs' semantic abilities is whether they acquire generalized knowledge of common events. Here, we test whether five pre-trained LLMs (from 2018's BERT to 2023's MPT) assign higher likelihood to plausible descriptions of agent-patient interactions than to minimally different implausible versions of the same event. Using three curated sets of minimal sentence pairs (total n=1,215), we found that pre-trained LLMs possess substantial event knowledge, outperforming other distributional language models. In particular, they almost always assign higher likelihood to possible vs. impossible events (The teacher bought the laptop vs. The laptop bought the teacher). However, LLMs
    
[^134]: 政府中的人工智能：概念，标准和统一框架

    Artificial intelligence in government: Concepts, standards, and a unified framework. (arXiv:2210.17218v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2210.17218](http://arxiv.org/abs/2210.17218)

    本论文研究了人工智能在政府中的应用，强调了标准化操作程序和符合社会规范期望的重要性，并指出了多学科研究者在概念上的碎片化问题。

    

    最近人工智能特别是生成式语言模型的进展使政府改变的希望变得能够实现。鉴于新型人工智能系统的先进能力，至关重要的是使用标准操作程序将其嵌入，并符合社会的规范期望。多个领域的学者开始概念化人工智能应用的不同形式，强调其潜在的好处和风险。然而，文献仍然碎片化，社会科学领域的研究者如公共管理和政治科学，以及快速发展的人工智能，机器学习和机器人技术领域都在相对孤立的情况下发展概念。虽然有呼吁对政府中的人工智能进行形式化研究，但对于理解将人工智能嵌入公共领域的后果所需的理论观点的全面综合描述仍然缺乏。

    Recent advances in artificial intelligence (AI), especially in generative language modelling, hold the promise of transforming government. Given the advanced capabilities of new AI systems, it is critical that these are embedded using standard operational procedures, clear epistemic criteria, and behave in alignment with the normative expectations of society. Scholars in multiple domains have subsequently begun to conceptualize the different forms that AI applications may take, highlighting both their potential benefits and pitfalls. However, the literature remains fragmented, with researchers in social science disciplines like public administration and political science, and the fast-moving fields of AI, ML, and robotics, all developing concepts in relative isolation. Although there are calls to formalize the emerging study of AI in government, a balanced account that captures the full depth of theoretical perspectives needed to understand the consequences of embedding AI into a publi
    
[^135]: 多模块图神经网络的灵活表征促进更好的泛化能力

    Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks. (arXiv:2209.06589v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.06589](http://arxiv.org/abs/2209.06589)

    本研究探讨了图神经网络在推广到更大的图和从未见过的数据方面的局限，并提出了多模块GNN框架，通过推广单个规范非线性变换来适应新图。结果表明，多模块GNN在合成和实际数据集上均显著提高了GNN的泛化能力，并在几项具有挑战性的任务上实现了最先进的性能。

    

    图神经网络（GNN）已成为处理图结构数据的学习与推断的强大模型，但对于扩展到更大的图以及推广到从未见过的数据的基本限制的了解还不足。本文使用随机图生成器系统地研究了图的大小和结构属性如何影响GNN的预测性能，并提出多模块GNN框架，通过推广单个规范非线性变换来适应新图。结果表明，多模块GNN在合成和实际数据集上均显著提高了GNN的泛化能力，并在几项具有挑战性的任务上实现了最先进的性能。

    Graph neural networks (GNNs) have become compelling models designed to perform learning and inference on graph-structured data. However, little work has been done to understand the fundamental limitations of GNNs for scaling to larger graphs and generalizing to out-of-distribution (OOD) inputs. In this paper, we use a random graph generator to systematically investigate how the graph size and structural properties affect the predictive performance of GNNs. We present specific evidence that the average node degree is a key feature in determining whether GNNs can generalize to unseen graphs, and that the use of multiple node update functions can improve the generalization performance of GNNs when dealing with graphs of multimodal degree distributions. Accordingly, we propose a multi-module GNN framework that allows the network to adapt flexibly to new graphs by generalizing a single canonical nonlinear transformation over aggregated inputs. Our results show that the multi-module GNNs imp
    
[^136]: 半监督深度多视图立体

    Semi-supervised Deep Multi-view Stereo. (arXiv:2207.11699v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.11699](http://arxiv.org/abs/2207.11699)

    本文针对学习式多视图立体问题，探讨了仅有一部分带有深度准确值的数据的半监督设置。通过引入新的半监督分布增强框架，提出了一种方法来解决MVS中的分布间隙多义性问题。

    

    在监督和无监督设置下，学习为基础的多视图立体(MVS)取得了显著进展。为了将它们各自的优点(准确性和完整性)结合起来，同时减少对昂贵有标签数据的需求，本文探讨了仅有一小部分MVS数据附带稠密深度准确值的半监督设置下的学习式MVS问题。然而，由于视图中的场景巨大变化和灵活设置，这可能破坏了经典半监督学习的基本假设，即无标签数据和有标签数据共享相同的标签空间和数据分布，在MVS问题中称为半监督分布间隙多义性。为了处理这些问题，我们提出了一种新颖的半监督分布增强MVS框架，即SDA-MVS。对于MVS数据中基本假设适用的简单情况，一致性正则化鼓励模型预测在原始样本和...

    Significant progress has been witnessed in learning-based Multi-view Stereo (MVS) under supervised and unsupervised settings. To combine their respective merits in accuracy and completeness, meantime reducing the demand for expensive labeled data, this paper explores the problem of learning-based MVS in a semi-supervised setting that only a tiny part of the MVS data is attached with dense depth ground truth. However, due to huge variation of scenarios and flexible settings in views, it may break the basic assumption in classic semi-supervised learning, that unlabeled data and labeled data share the same label space and data distribution, named as semi-supervised distribution-gap ambiguity in the MVS problem. To handle these issues, we propose a novel semi-supervised distribution-augmented MVS framework, namely SDA-MVS. For the simple case that the basic assumption works in MVS data, consistency regularization encourages the model predictions to be consistent between original sample and
    
[^137]: 用于合作多智能体强化学习的局部优势网络

    Local Advantage Networks for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2112.12458v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.12458](http://arxiv.org/abs/2112.12458)

    这项研究提出了一种局部优势网络（LAN）算法，该算法通过对决架构和中心化评论家来学习合作多智能体的最佳响应策略，并在StarCraft II多智能体挑战基准测试上达到了最先进的性能。

    

    近期成功的离策略多智能体强化学习（MARL）算法，用于合作的部分可观测环境，主要关注于寻找分解的值函数，导致复杂的网络结构。我们的LAN算法建立在独立Q学习者的结构基础上，采用一种完全不同的方法，利用对决架构通过个体优势函数为每个智能体学习分散的最佳响应策略。通过一个中心化的评论家稳定学习，评论家的主要目标是减少个体优势的移动目标问题。评论家的网络大小与智能体数量无关，在学习后被丢弃。在StarCraft II多智能体挑战基准测试上的评估结果显示，LAN达到了最先进的性能，并且对于智能体数量具有高度可扩展性，为MARL研究开辟了一个有前景的替代方向。

    Many recent successful off-policy multi-agent reinforcement learning (MARL) algorithms for cooperative partially observable environments focus on finding factorized value functions, leading to convoluted network structures. Building on the structure of independent Q-learners, our LAN algorithm takes a radically different approach, leveraging a dueling architecture to learn for each agent a decentralized best-response policies via individual advantage functions. The learning is stabilized by a centralized critic whose primary objective is to reduce the moving target problem of the individual advantages. The critic, whose network's size is independent of the number of agents, is cast aside after learning. Evaluation on the StarCraft II multi-agent challenge benchmark shows that LAN reaches state-of-the-art performance and is highly scalable with respect to the number of agents, opening up a promising alternative direction for MARL research.
    
[^138]: 研究“量子模糊”技术的有用性

    Investigating the usefulness of Quantum Blur. (arXiv:2112.01646v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2112.01646](http://arxiv.org/abs/2112.01646)

    “量子模糊”方法是一种简单的量子软件示例，在计算机游戏、音乐和艺术等领域中具有实用价值。这项研究通过与最重要用户的讨论，确定了该方法最有用的特性，并揭示了这些特性与量子现象的关系。

    

    虽然距离量子计算完全超越传统计算还需要一些年份，但已经提供了可以在各个领域进行探索性任务的资源。这包括在计算机游戏、音乐和艺术中进行程序生成的特定任务。所谓的“量子模糊”方法代表了这一探索之旅的第一步，为我们展示了量子软件如何在这些领域中具有实用性。在这里，我们分析了“量子模糊”方法并将其与传统模糊效果进行比较。这项研究是通过与该方法的最重要用户的讨论来指导的，目的是确定哪些特性被认为最有用。特别是我们确定了这些特性如何取决于量子现象的叠加和纠缠。

    Though some years remain before quantum computation can fully outperform conventional computation, it already provides resources that can be used for exploratory purposes in various fields. This includes certain tasks for procedural generation in computer games, music and art. The so-called `Quantum Blur' method represents the first step on this journey, providing a simple proof-of-principle example of how quantum software can be useful in these areas today. Here we analyse the `Quantum Blur' method and compare it to conventional blur effects. This investigation was guided by discussions with the most prominent user of the method, to determine which features were found most useful. In particular we determine how these features depend on the quantum phenomena of superposition and entanglement.
    
[^139]: 道路网络引导的城市细粒度交通流推断

    Road Network Guided Fine-Grained Urban Traffic Flow Inference. (arXiv:2109.14251v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.14251](http://arxiv.org/abs/2109.14251)

    本文提出了一种基于道路网络的交通流量推断方法，利用道路网络的先验知识全面学习细粒度交通流的道路感知空间分布，普遍适用于城市交通流量监测和调控方案。

    

    精确推断细粒度交通流量是一个新兴但至关重要的问题，它可以帮助极大地减少所需交通监测传感器的数量以节省成本。本文发现交通流量与道路网络具有很高的相关性，但之前的研究中完全忽略了这一点，或者仅将其视为外部因素。为了解决这个问题，我们提出了一种新的路网感知交通流量放大器（RATFM），它明确利用道路网络的先验知识，全面学习细粒度交通流的道路感知空间分布。具体而言，我们首先引入了一个多方向1D卷积层来提取道路网络的语义特征。随后，我们将道路网络特征和粗粒度流量特征结合在一起，规范化道路相关交通流的短距离空间分布建模。此外，我们将道路网络特征作为查询来捕获长距离路段交通流量的关联。

    Accurate inference of fine-grained traffic flow from coarse-grained one is an emerging yet crucial problem, which can help greatly reduce the number of the required traffic monitoring sensors for cost savings. In this work, we notice that traffic flow has a high correlation with road network, which was either completely ignored or simply treated as an external factor in previous works.To facilitate this problem, we propose a novel Road-Aware Traffic Flow Magnifier (RATFM) that explicitly exploits the prior knowledge of road networks to fully learn the road-aware spatial distribution of fine-grained traffic flow. Specifically, a multi-directional 1D convolutional layer is first introduced to extract the semantic feature of the road network. Subsequently, we incorporate the road network feature and coarse-grained flow feature to regularize the short-range spatial distribution modeling of road-relative traffic flow. Furthermore, we take the road network feature as a query to capture the l
    
[^140]: 用于高效流学习的调优组合特征回放

    Tuned Compositional Feature Replays for Efficient Stream Learning. (arXiv:2104.02206v7 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2104.02206](http://arxiv.org/abs/2104.02206)

    本文提出了一种名为CRUMB的新的持续学习算法，通过重放通过重新组合特征图来缓解遗忘问题。CRUMB通过存储内存块的索引来使得在后续任务中能够回放特定的记忆，这种重建机制还可以帮助神经网络最小化灾难性遗忘。

    

    我们的大脑从瞬时的世界经验中提取出持久的、可推广的知识。人工神经网络远远不能达到相同的水平：当被要求通过按照时间顺序训练非重复视频帧来学习对象分类时（在线流学习），那些能够从重新排列的数据集中良好学习的模型在学习新的刺激时会灾难性地遗忘旧的知识。我们提出了一种新的持续学习算法，称为Compositional Replay Using Memory Blocks (CRUMB)，通过重放通过重新组合通用部分重建的特征图来缓解遗忘问题。CRUMB在卷积神经网络中串联可训练和可重用的“内存块”向量，以组合方式重建特征图张量，就像面包屑组合成一个面包一样。CRUMB存储用于重建新刺激的内存块索引，从而使得在后续任务中能够回放特定的记忆。这种重建机制还可以引导神经网络最小化灾难性遗忘。

    Our brains extract durable, generalizable knowledge from transient experiences of the world. Artificial neural networks come nowhere close: when tasked with learning to classify objects by training on non-repeating video frames in temporal order (online stream learning), models that learn well from shuffled datasets catastrophically forget old knowledge upon learning new stimuli. We propose a new continual learning algorithm, Compositional Replay Using Memory Blocks (CRUMB), which mitigates forgetting by replaying feature maps reconstructed by recombining generic parts. CRUMB concatenates trainable and re-usable "memory block" vectors to compositionally reconstruct feature map tensors in convolutional neural networks, like crumbs forming a loaf of bread. CRUMB stores the indices of memory blocks used to reconstruct new stimuli, enabling replay of specific memories during later tasks. This reconstruction mechanism also primes the neural network to minimize catastrophic forgetting by for
    
[^141]: 通过多任务表示学习理论改进少样本学习

    Improving Few-Shot Learning through Multi-task Representation Learning Theory. (arXiv:2010.01992v3 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2010.01992](http://arxiv.org/abs/2010.01992)

    本文通过多任务表示学习理论的最新进展，提出了一个新的基于谱的正则化项来改进少样本学习，并在实验证明了其有效性。

    

    本文考虑多任务表示（MTR）学习的框架，目标是利用源任务来学习一个表示，减少解决目标任务的样本复杂性。我们首先回顾了MTR理论的最新进展，并展示了在该框架内对流行的元学习算法进行分析可以提供新的见解。特别地，我们强调了梯度优化和度量优化算法在实践中的根本区别，并提出了一个理论分析来解释它。最后，我们利用得到的见解通过一种新的基于谱的正则化项来提高元学习方法的性能，并通过在少样本分类基准上的实验研究验证了其有效性。据我们所知，这是将MTR理论的最新学习界限应用于少样本分类任务的首次贡献。

    In this paper, we consider the framework of multi-task representation (MTR) learning where the goal is to use source tasks to learn a representation that reduces the sample complexity of solving a target task. We start by reviewing recent advances in MTR theory and show that they can provide novel insights for popular meta-learning algorithms when analyzed within this framework. In particular, we highlight a fundamental difference between gradient-based and metric-based algorithms in practice and put forward a theoretical analysis to explain it. Finally, we use the derived insights to improve the performance of meta-learning methods via a new spectral-based regularization term and confirm its efficiency through experimental studies on few-shot classification benchmarks. To the best of our knowledge, this is the first contribution that puts the most recent learning bounds of MTR theory into practice for the task of few-shot classification.
    

