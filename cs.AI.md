# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Constructive Large Language Models Alignment with Diverse Feedback.](http://arxiv.org/abs/2310.06450) | 本文提出了一种新的方法，即建构性和多样化反馈（CDF），用于增强大型语言模型（LLM）的对齐效果。我们通过收集不同类型的反馈，并根据问题的难度级别进行处理，实现了更好的性能。 |
| [^2] | [Stepwise functional refoundation of relational concept analysis.](http://arxiv.org/abs/2310.06441) | 逐步功能重构的关系概念分析（RCA）是形式概念分析的扩展，通过定义良构解决方案的空间和相关函数，解决了RCA在循环依赖数据上返回单一概念格家族的问题。 |
| [^3] | [Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition.](http://arxiv.org/abs/2310.06434) | Whispering LLaMA是一种用于语音识别的跨模态生成错误校正框架，通过融合声学信息和外部语言表示，生成准确的语音转录上下文，相对于n-best假设，词错误率性能提升了37.66%。 |
| [^4] | [Retromorphic Testing: A New Approach to the Test Oracle Problem.](http://arxiv.org/abs/2310.06433) | 逆向测试是一种新颖的黑盒测试方法，通过建立双程序结构，利用辅助程序将输入数据进行前向处理，然后使用后向程序将程序输出反转为原始输入格式，从而解决了测试Oracle问题。 |
| [^5] | [Proceedings of The first international workshop on eXplainable AI for the Arts (XAIxArts).](http://arxiv.org/abs/2310.06428) | 这个论文集汇集了来自人机交互、交互设计、人工智能和数字艺术等领域的研究者，讨论了可解释人工智能在艺术领域的作用。 |
| [^6] | [TANGO: Time-Reversal Latent GraphODE for Multi-Agent Dynamical Systems.](http://arxiv.org/abs/2310.06427) | 本文提出了TANGO方法，通过时间反演对称性作为归纳偏差，对多智能体动力系统进行学习，即使在非保守的可逆系统中也能保持能量，并且能更高效地学习系统动力学。 |
| [^7] | [Large Language Models for Propaganda Detection.](http://arxiv.org/abs/2310.06422) | 这项研究探讨了使用现代大型语言模型（LLMs）如GPT-3和GPT-4在宣传信息检测方面的有效性。实验结果显示，GPT-4达到了与当前最先进方法相符的结果。 |
| [^8] | [Advective Diffusion Transformers for Topological Generalization in Graph Learning.](http://arxiv.org/abs/2310.06417) | 本研究探索了在不同的图拓扑存在下，图扩散方程如何对GNN进行外推和概括，揭示了基于局部扩散的现有模型在概括能力上的不足，并提出了非局部扩散的潜力。 |
| [^9] | [Hexa: Self-Improving for Knowledge-Grounded Dialogue System.](http://arxiv.org/abs/2310.06404) | 本论文提出了一种自我提升的方法，用于改进知识驱动对话生成的中间步骤的生成性能。通过引入引导提示和修改损失函数的自举策略，提高了生成自动生成回答的多样性，并在各种基准数据集上实验证明了该方法的有效性。 |
| [^10] | [Lo-Hi: Practical ML Drug Discovery Benchmark.](http://arxiv.org/abs/2310.06399) | 本研究创建了一个实用的Lo-Hi基准测试，包括前导优化和命中识别两个任务，为药物发现过程提供了一种切实可行的评估方法。对于命中识别任务，研究者设计了一种解决顶点最小k-Cut问题的新型分子拆分算法，并测试了各种机器学习模型的性能。研究还发现现有基准测试不现实且过于乐观。 |
| [^11] | [P5: Plug-and-Play Persona Prompting for Personalized Response Selection.](http://arxiv.org/abs/2310.06390) | 提出了一种即插即用的个人角色提示方法，用于个性化回答选择的检索对话机器人。这种方法在零样本设置下表现良好，减少了对基于个人角色的训练数据的依赖，并且使得系统更容易扩展到其他语言。 |
| [^12] | [Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations.](http://arxiv.org/abs/2310.06387) | 本文研究了使用少量上下文示范来操纵语言模型对齐能力的方法。通过提供示范，而无需微调，可以增加或降低模型回答恶意提示的概率。我们提出了相同上下文攻击和相同上下文防御方法，用于越狱和对齐语言模型。实验证明了这些方法的有效性。 |
| [^13] | [What Makes for Robust Multi-Modal Models in the Face of Missing Modalities?.](http://arxiv.org/abs/2310.06383) | 本研究从信息论的角度对多模态模型遇到缺失模态的情景进行了建模，并提出了两个关键要点：编码器能够从非缺失模态中提取好的特征，并且提取的特征在模态融合过程中具有足够的鲁棒性。 |
| [^14] | [Advanced Efficient Strategy for Detection of Dark Objects Based on Spiking Network with Multi-Box Detection.](http://arxiv.org/abs/2310.06370) | 本研究提出一种基于脉冲网络和多盒检测技术的高效、可靠、能效高的黑色物体检测模型，通过结合预训练的VGG16模型和脉冲和普通卷积层，能够准确检测黑色物体边界框，并实现高效的训练过程。 |
| [^15] | [Geometrically Aligned Transfer Encoder for Inductive Transfer in Regression Tasks.](http://arxiv.org/abs/2310.06369) | 本文提出了一种基于微分几何的转移学习技术，通过将模型的潜在向量映射到黎曼曲面上的局部平坦坐标，实现在回归任务中的知识转移，并展示了该方法的优越性能和稳定行为。 |
| [^16] | [Noisy-ArcMix: Additive Noisy Angular Margin Loss Combined With Mixup Anomalous Sound Detection.](http://arxiv.org/abs/2310.06364) | 本文提出了一种名为Noisy-ArcMix的训练技术，旨在确保异常声音检测模型具有足够的类内紧凑性和正常与异常样本之间的角度差距。同时，引入了一种能够提取重要时间区域特征的架构，增强模型对于时间帧的学习能力。 |
| [^17] | [Fire Detection From Image and Video Using YOLOv5.](http://arxiv.org/abs/2310.06351) | 使用改进的YOLOv5算法实现火灾图像和视频的检测，准确率高，能有效处理小火灾目标的检测和类似火灾和烟雾的物体。 |
| [^18] | [Filter Pruning For CNN With Enhanced Linear Representation Redundancy.](http://arxiv.org/abs/2310.06344) | 本文提出了一种增强线性表示冗余的卷积神经网络滤波剪枝方法，通过引入相关系数矩阵损失（CCM-loss）和匹配通道选择策略，可以在训练过程中加强特征图之间的线性表示关系，在剪枝过程中更好地去除同质化的部分。 |
| [^19] | [Contrastive Prompt Learning-based Code Search based on Interaction Matrix.](http://arxiv.org/abs/2310.06342) | 提出了基于对比提示学习的代码搜索方法CPLCS，通过编程语言-自然语言对比学习和跨模态交互机制，解决了现有代码搜索方法中的语义表示不足和语义鸿沟问题。 |
| [^20] | [I2SRM: Intra- and Inter-Sample Relationship Modeling for Multimodal Information Extraction.](http://arxiv.org/abs/2310.06326) | 本文提出了I2SRM方法，它通过样本内关系建模和样本间关系建模来解决多模态信息提取问题。通过转换嵌入表示、捕捉交互作用和引入AttnMixup策略，该方法在多个数据集上取得了良好的性能。 |
| [^21] | [Predicting Three Types of Freezing of Gait Events Using Deep Learning Models.](http://arxiv.org/abs/2310.06322) | 该论文利用深度学习模型预测三种不同类型的冻结步态事件，最佳模型在测试数据上获得了0.427的得分，在Kaggle冻结步态预测竞赛中排名前五。 |
| [^22] | [Dobby: A Conversational Service Robot Driven by GPT-4.](http://arxiv.org/abs/2310.06303) | Dobby是一款由GPT-4驱动的会话服务机器人，能够将会话型人工智能代理嵌入到具有自然语言理解和智能决策功能的具身系统中，实现服务任务的一体化，并且能够与物理世界进行交互。在演示中展示了其在导游场景中的应用，并通过HRI研究对其性能进行了评估。 |
| [^23] | [Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition.](http://arxiv.org/abs/2310.06301) | 本研究通过奇异学习理论研究超叠加的玩具模型中的相变，在两个隐藏维度的情况下发现正则的$k$-gons是临界点，并提供支持理论表明这些临界点决定了贝叶斯后验的相变。此外，实验证明这些临界点也决定了SGD训练的行为。研究结果支持了SGD学习轨迹受顺序学习机制影响的猜想。 |
| [^24] | [Suppressing Overestimation in Q-Learning through Adversarial Behaviors.](http://arxiv.org/abs/2310.06286) | 本文提出了一种新的Q学习算法，通过引入虚拟对抗性玩家，有效调节了标准Q学习中的过高估计偏差，提出的算法简单而有效，能够轻松应用于强化学习算法并提高性能。 |
| [^25] | [BC4LLM: Trusted Artificial Intelligence When Blockchain Meets Large Language Models.](http://arxiv.org/abs/2310.06278) | 本论文提出了使用区块链技术增强大型语言模型（LLM）安全性的愿景，以解决AI生成内容的真实性和隐私泄露问题。 |
| [^26] | [Let Models Speak Ciphers: Multiagent Debate through Embeddings.](http://arxiv.org/abs/2310.06272) | 本文引入了一种名为CIPHER的通信机制，通过去除LLMs中的标记采样步骤，让模型可以通过期望的原始Transformer输出嵌入来传达其信念，从而解决了在自然语言生成中可能存在的信息丢失风险，并提供了编码更广泛信息的优势。 |
| [^27] | [Towards Mitigating Hallucination in Large Language Models via Self-Reflection.](http://arxiv.org/abs/2310.06271) | 本文通过分析医学生成性问答系统中的幻觉现象，提出了一种交互式的自我反思方法，通过增强事实性、一致性和蕴涵性，解决了大型语言模型中的幻觉问题。 |
| [^28] | [The AI Incident Database as an Educational Tool to Raise Awareness of AI Harms: A Classroom Exploration of Efficacy, Limitations, & Future Improvements.](http://arxiv.org/abs/2310.06269) | 本研究评估了AI事故数据库作为提高人们对人工智能伤害意识的教育工具的有效性。该数据库索引了以往的AI伤害实例，并通过课堂研究揭示了学生对AI伦理主题的初步看法和知识缺口的存在。 |
| [^29] | [CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model.](http://arxiv.org/abs/2310.06266) | CodeFuse-13B是一个预训练的多语言代码大型语言模型，专为代码相关任务设计，支持超过40种编程语言，并通过使用高质量的预训练数据集以及大量实验的验证，展现了其在多语言输入下的有效性。 |
| [^30] | [Self-Discriminative Modeling for Anomalous Graph Detection.](http://arxiv.org/abs/2310.06261) | 本文提出了一种自我区分建模框架用于基于正常图训练的异常图检测，通过生成插值的伪异常图，与几种最先进的基线算法相比，取得了显著的改进。 |
| [^31] | [Get the gist? Using large language models for few-shot decontextualization.](http://arxiv.org/abs/2310.06254) | 本文提出了一种使用大型语言模型的少样本去背景化方法，该方法能够在多个领域上仅使用少量样本即可达到可行的性能。 |
| [^32] | [We are what we repeatedly do: Inducing and deploying habitual schemas in persona-based responses.](http://arxiv.org/abs/2310.06245) | 该论文研究了在对话系统中引导和应用习惯性模式来生成基于角色的回应。与以往的方法不同，该论文通过明确的模式表示来捕捉与角色相关的习惯性知识，从而提高了回应的准确性和自然性。 |
| [^33] | [Model Tuning or Prompt Tuning? A Study of Large Language Models for Clinical Concept and Relation Extraction.](http://arxiv.org/abs/2310.06239) | 本研究探索了大型语言模型在临床概念和关系提取任务中的应用，通过软提示调优取得了最佳性能，并观察了迁移学习和少样本学习的能力。 |
| [^34] | [Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for Unbiased Question-Answering.](http://arxiv.org/abs/2310.06238) | 本论文解决了MUSIC-AVQA中的数据偏差问题，通过创建一个平衡的数据集来保证模型能够有效地推理各种多模态情况下的问题。他们构建了一个名为MUSIC-AVQA v2.0的新数据集，并提出了一个新的基准模型。 |
| [^35] | [Evolution of Natural Language Processing Technology: Not Just Language Processing Towards General Purpose AI.](http://arxiv.org/abs/2310.06228) | 这篇论文翻译成中文并总结指出，自然语言处理技术逐渐演进为通用人工智能，尽管自然语言非常难以进行数学上的表达，但通过深度学习在NLP领域取得了超乎预期的结果。 |
| [^36] | [GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models.](http://arxiv.org/abs/2310.06225) | 本研究全面评估了大型语言模型（LLMs）在回答农业相关问题方面的能力，并使用RAG和ER技术提高了LLMs的性能。其中，GPT-4在农业考试中取得了及格分数以获得认证。 |
| [^37] | [SUBP: Soft Uniform Block Pruning for 1xN Sparse CNNs Multithreading Acceleration.](http://arxiv.org/abs/2310.06218) | 该论文提出了一种新的软均匀块剪枝（SUBP）方法，在1xN稀疏CNN中实现了多线程加速，并解决了传统方法中的训练成本昂贵、内存访问开销大、模型质量次优以及线程负载不平衡等问题。 |
| [^38] | [Estimating Numbers without Regression.](http://arxiv.org/abs/2310.06204) | 提出了一种不使用回归的方法来估计数字，通过改变模型的词汇表来更好地表示数字的大小，并取得了令人满意的结果。 |
| [^39] | [Look-Up mAI GeMM: Increasing AI GeMMs Performance by Nearly 2.5x via msGeMM.](http://arxiv.org/abs/2310.06178) | 这篇论文提出了一种名为msGeMM的新算法，通过使用低精度数据类型，可以使AI模型的性能提高近2.5倍。该算法需要特殊的CUDA核心来实现从小型查找表中添加元素的能力。 |
| [^40] | [Factual and Personalized Recommendations using Language Models and Reinforcement Learning.](http://arxiv.org/abs/2310.06176) | 本研究提出了一种使用语言模型和强化学习的事实和个性化推荐系统，该系统使用嵌入空间表示根据用户偏好生成有趣和相关的推荐，并开发了联合奖励函数来衡量准确性、吸引力和个性化。实验证明该系统能够为用户提供个性化的电影叙事。 |
| [^41] | [How does prompt engineering affect ChatGPT performance on unsupervised entity resolution?.](http://arxiv.org/abs/2310.06174) | 本研究对提示工程对ChatGPT在无监督实体解析中的影响进行了初步实验研究，结果显示提示可以显著影响实体解析的质量。 |
| [^42] | [Memory-Consistent Neural Networks for Imitation Learning.](http://arxiv.org/abs/2310.06171) | 本文介绍了一种内存一致的神经网络模型，在模仿学习中使用专家演示训练策略。该模型通过对输出结果进行硬约束，避免了错误的累积现象，保证了策略效果的上界。 |
| [^43] | [Predictable Artificial Intelligence.](http://arxiv.org/abs/2310.06167) | 可预测人工智能是一个新兴研究领域，旨在预测人工智能生态系统的关键指标，并强调可预测性对于提高信任、责任、控制、协调和安全的重要性。 |
| [^44] | [CAW-coref: Conjunction-Aware Word-level Coreference Resolution.](http://arxiv.org/abs/2310.06165) | 本文介绍了一种关联词感知的词级共指消解模型（CAW-coref），在处理并列提及的情况下表现出了较高的性能，有效地缩小了与昂贵的最先进方法的差距。 |
| [^45] | [Understanding Transfer Learning and Gradient-Based Meta-Learning Techniques.](http://arxiv.org/abs/2310.06148) | 本文研究了微调、MAML和Reptile在迁移学习和元学习领域的性能差异，发现当在与训练数据分布不同的任务上进行评估时，仅对预训练网络进行微调的基准线可能比更复杂的元学习技术更有效。 |
| [^46] | [Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond.](http://arxiv.org/abs/2310.06147) | 这篇论文介绍了在LLM时代中，强化学习在RLHF、Prompting等方面的应用，并讨论了其中的创新和贡献。 |
| [^47] | [Layout Sequence Prediction From Noisy Mobile Modality.](http://arxiv.org/abs/2310.06138) | 提出一种名为LTrajDiff的新方法，从噪声移动数据中预测精确的布局序列，克服了由嘈杂数据、不完整轨迹和环境因素导致的视觉障碍。 |
| [^48] | [Learning Layer-wise Equivariances Automatically using Gradients.](http://arxiv.org/abs/2310.06131) | 该论文提出了一种通过梯度自动学习层间等变性的方法，通过改进软等变性的参数化和优化边缘似然来实现层间对称性的自动学习。 |
| [^49] | [On Time Domain Conformer Models for Monaural Speech Separation in Noisy Reverberant Acoustic Environments.](http://arxiv.org/abs/2310.06125) | 本论文提出了一种基于时域Conformer模型的单声道语音分离方法，相较于已有模型，在嘈杂的混响声学环境中实现了更高的分离效果和更高的计算效率。 |
| [^50] | [Text-driven Prompt Generation for Vision-Language Models in Federated Learning.](http://arxiv.org/abs/2310.06123) | 本论文提出了一种联邦文本驱动的提示生成方法（FedTPG），通过在多个远程客户端上学习统一的提示生成网络，实现了视觉-语言模型的泛化。实证评估表明，该方法在已知和未知类别上的泛化能力优于现有的联邦提示学习方法。 |
| [^51] | [Exploring Progress in Multivariate Time Series Forecasting: Comprehensive Benchmarking and Heterogeneity Analysis.](http://arxiv.org/abs/2310.06119) | 该论文旨在解决多元时间序列预测领域中公平基准测试和技术方法选择的争议，并提供对该领域取得的进展的深入洞察。 |
| [^52] | [Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models.](http://arxiv.org/abs/2310.06117) | 本文提出了一种简单的提示技术，使得大型语言模型能够通过抽象获得高层概念和基本原理，并将其应用于推理路径中，从而显著提升模型在各种推理密集型任务上的表现。 |
| [^53] | [OptiMUS: Optimization Modeling Using mip Solvers and large language models.](http://arxiv.org/abs/2310.06116) | 介绍了OptiMUS，一种基于大规模语言模型的优化建模代理，用于解决MILP问题。该代理能够自动生成数学模型、编写和调试求解器代码，并具有较高的解决率。 |
| [^54] | [Learning Interactive Real-World Simulators.](http://arxiv.org/abs/2310.06114) | 通过生成建模学习交互体验的通用模拟器，以模拟人类、机器人和其他交互式代理人对真实世界中行为的响应。 |
| [^55] | [When is Agnostic Reinforcement Learning Statistically Tractable?.](http://arxiv.org/abs/2310.06113) | 本文研究了基于不可知的PAC强化学习的问题，引入了跨越容量这个新的复杂度度量，并发现了在生成和在线访问模型之间以及在线访问下的确定和随机MDP之间的差异。 |
| [^56] | [High Dimensional Causal Inference with Variational Backdoor Adjustment.](http://arxiv.org/abs/2310.06100) | 本论文介绍了一种用于高维因果推断的变分背门调整技术，能够处理高维治疗和混杂因素，并成功应用于医疗数据中。 |
| [^57] | [Predictive auxiliary objectives in deep RL mimic learning in the brain.](http://arxiv.org/abs/2310.06089) | 本文研究了深度强化学习中预测辅助目标对表示学习和性能的影响，发现在资源受限的情况下，预测目标能显著提高和稳定学习，并且能支持表征迁移。此外，与神经活动变化相似，这些辅助目标也模拟了大脑中的表征变化。 |
| [^58] | [Performative Time-Series Forecasting.](http://arxiv.org/abs/2310.06077) | 本论文研究了时间序列预测中的展示性问题，提出了一种新的方法（FPS），通过利用延迟响应的概念来解决展示性引起的分布变化，并实现准确的预测。 |
| [^59] | [Pain Forecasting using Self-supervised Learning and Patient Phenotyping: An attempt to prevent Opioid Addiction.](http://arxiv.org/abs/2310.06075) | 本研究旨在通过使用自监督学习方法和患者表型研究，预测患者未来的疼痛轨迹，以帮助患者管理镰状细胞贫血，改善生活质量，并减少对阿片类药物的依赖和副作用。 |
| [^60] | [Augmenting Vision-Based Human Pose Estimation with Rotation Matrix.](http://arxiv.org/abs/2310.06068) | 本研究提出使用旋转矩阵的模型，以增强基于姿势估计的活动识别的分类准确性。通过实验证明，利用带有旋转矩阵数据增强的SVM与SGD优化方法，在分类五种体育活动时达到了96%的准确率，相比之下，不实施数据增强技术的基准准确率仅为64%。 |
| [^61] | [LLM for SoC Security: A Paradigm Shift.](http://arxiv.org/abs/2310.06046) | 本论文研究利用大型语言模型（LLM）解决系统级芯片（SoC）设计中的安全性问题，通过将LLM集成到SoC安全验证流程中，开辟了更高效、可伸缩和适应的新方法。目标是确保越来越复杂的SoC的安全性。 |
| [^62] | [Generative ensemble deep learning severe weather prediction from a deterministic convection-allowing model.](http://arxiv.org/abs/2310.06045) | 本论文开发了一种集成后处理方法，将生成对抗网络（CGANs）和卷积神经网络（CNN）结合起来，对严重天气进行概率预测。该方法在使用HRRR预报作为输入数据，在2021年的测试数据集上相对于其他基于神经网络的方法提高了高达20％的Brier技巧分数（BSS）。 |
| [^63] | [DyST: Towards Dynamic Neural Scene Representations on Real-World Videos.](http://arxiv.org/abs/2310.06020) | DyST模型通过学习动态场景的潜在分解，从实际视频中捕捉到了场景的3D结构和动态特性，并实现了对相机和场景内容的独立控制视图生成。 |
| [^64] | [Divide-and-Conquer Dynamics in AI-Driven Disempowerment.](http://arxiv.org/abs/2310.06009) | 这项研究通过构建游戏理论模型，研究了AI驱动的剥夺中的不团结问题。研究发现，当前受害者需要让未来受害者认识到他们的利益同样面临严重和紧迫的威胁，以激励未来受害者以团结支持当前受害者。 |
| [^65] | [Rethinking Memory and Communication Cost for Efficient Large Language Model Training.](http://arxiv.org/abs/2310.06003) | 本文研究了大型语言模型训练中内存和通信成本对训练速度的影响，并提出了一种平衡内存和通信的优化器（PaRO）。此外，还提出了一种用于大模型训练的分层重叠环通信拓扑结构（HO-Ring），实验证明该算法提高了训练过程中的通信效率。 |
| [^66] | [Measuring reasoning capabilities of ChatGPT.](http://arxiv.org/abs/2310.05993) | 这项研究评估了ChatGPT在推理任务中的表现，发现其在逻辑谜题中仅能提供7%正确答案和理由。 |
| [^67] | [Simulating Social Media Using Large Language Models to Evaluate Alternative News Feed Algorithms.](http://arxiv.org/abs/2310.05984) | 本论文通过使用大型语言模型和代理建模相结合的方法，模拟社交媒体平台，并研究不同新闻供稿算法对在线对话质量的影响。通过创建逼真的人物形象和使用不同的算法，研究发现新闻供稿算法对于塑造社交媒体对话是具有重要影响的。 |
| [^68] | [An evolutionary model of personality traits related to cooperative behavior using a large language model.](http://arxiv.org/abs/2310.05976) | 本文提出了一个使用大规模语言模型的进化模型，研究了个性特征在博弈理论关系背景下的演化。实验证明该模型能够展示出演化的特征。 |
| [^69] | [Automated Chest X-Ray Report Generator Using Multi-Model Deep Learning Approach.](http://arxiv.org/abs/2310.05969) | 提出了一种基于多模型深度学习的自动化胸部X光报告生成器系统，通过利用多个二元分类模型检测多种异常，在单个图像中辅助放射科医生的工作。该系统将放射学异常检测限制为心脏肥大、肺积液和实变，并通过三个步骤生成放射学报告：图像预处理、深度学习模型检测异常和生成报告。 |
| [^70] | [Fingerprint Attack: Client De-Anonymization in Federated Learning.](http://arxiv.org/abs/2310.05960) | 本文研究了在联邦学习中，通过梯度指纹攻击可以轻松打破参与者匿名化，并展示了使用差分隐私进行训练可以提供实际防御。 |
| [^71] | [Efficient Network Representation for GNN-based Intrusion Detection.](http://arxiv.org/abs/2310.05956) | 本文提出了一种基于GNN的入侵检测方法，通过使用流图作为网络表示形式，提供了相关的拓扑信息，包括恶意行为模式、多步攻击阶段之间的关系以及欺骗和预欺骗攻击者活动之间的关系。通过嵌入节点特征和学习相关的攻击模式，该方法能够对通信流进行分类，并分配恶意程度得分。该方法还指出了传统评估过程中可能存在的数据泄漏问题，并提出了相应的建议。 |
| [^72] | [Reducing the False Positive Rate Using Bayesian Inference in Autonomous Driving Perception.](http://arxiv.org/abs/2310.05951) | 本研究提出了一种使用贝叶斯推断的方法来降低自动驾驶感知中的误报率。通过使用多感知和多模态方法，以及考虑目标的似然函数和先验概率，该方法在减少误报率方面取得了显著的进展。在KITTI数据集上进行的实验证明了该方法的有效性。 |
| [^73] | [Learning Cyber Defence Tactics from Scratch with Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2310.05939) | 学习网络防御战术的新方法，使用多智能体强化学习来进行自主网络防御，证明了合作多智能体强化学习能够对抗各种威胁。 |
| [^74] | [Component attention network for multimodal dance improvisation recognition.](http://arxiv.org/abs/2310.05938) | 本研究提出了一种用于多模态舞蹈即兴识别的组件注意力网络 (CANet)。通过多层次的融合方法，并通过实验分析每种模态的影响，我们证明了该模型相比基准方法具有更好的性能。 |
| [^75] | [DF-3DFace: One-to-Many Speech Synchronized 3D Face Animation with Diffusion.](http://arxiv.org/abs/2310.05934) | DF-3DFace是一种采用扩散模型的语音到3D面部网格合成方法，能够准确同步唇部动作，并综合了身份、姿势和面部运动。 |
| [^76] | [A Multi-Agent Systems Approach for Peer-to-Peer Energy Trading in Dairy Farming.](http://arxiv.org/abs/2310.05932) | 提出了基于多智能体系统的奶牛养殖领域点对点能源交易方法，通过该方法可以降低电力成本和峰值需求，同时增加能源销售。 |
| [^77] | [Deep Learning based Tomato Disease Detection and Remedy Suggestions using Mobile Application.](http://arxiv.org/abs/2310.05929) | 我们开发了一款基于深度学习的移动应用程序，利用人工智能技术来检测番茄病害并提供治疗建议，以帮助尼泊尔农民解决传统农业方法和农作物病害的问题。 |
| [^78] | [Interpreting CLIP's Image Representation via Text-Based Decomposition.](http://arxiv.org/abs/2310.05916) | 本文通过解析CLIP图像编码器的组件，揭示了图像表示的构成方式，并利用文本表示解释了其各个部分的作用。通过理解注意力头和图像块，作者实现了对模型的修复和改进，包括消除误特征和构建零样本图像分割器等方面。 |
| [^79] | [Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models.](http://arxiv.org/abs/2310.05863) | 本文提出了一种面向多模型LLM的细粒度音频-视觉联合表示学习框架(FAVOR)，该框架能够同时感知音频和视觉输入流，并通过因果关注模块捕获音频-视觉帧的因果关系。还提出了一个音频-视觉评估基准(AVEB)用于评估该模型的性能。 |
| [^80] | [ParFam -- Symbolic Regression Based on Continuous Global Optimization.](http://arxiv.org/abs/2310.05537) | ParFam是一种新的符号回归方法，利用参数化的符号函数族将离散问题转化为连续问题，并结合全局优化器，能够有效解决符号回归问题。 |
| [^81] | [Molecular De Novo Design through Transformer-based Reinforcement Learning.](http://arxiv.org/abs/2310.05365) | 本文提出了一种基于Transformer的强化学习方法，通过精细调整生成模型，能够在分子的全新设计中生成具有所需性质的分子结构，展现出优越的性能。 |
| [^82] | [Universal Multi-modal Entity Alignment via Iteratively Fusing Modality Similarity Paths.](http://arxiv.org/abs/2310.05364) | 本研究提出了一种通过迭代融合模态相似路径实现通用的多模态实体对齐方法。通过统一建模和有效信息融合，解决了现有方法中模态建模不一致和低效以及模态融合效果不佳的问题。 |
| [^83] | [A Critical Look at Classic Test-Time Adaptation Methods in Semantic Segmentation.](http://arxiv.org/abs/2310.05341) | 这项研究对语义分割中的经典测试时适应方法进行了批判性探究，揭示了分割TTA所面临的独特挑战，并发现经典TTA策略在这一任务中并不有效。 |
| [^84] | [Enhancing Long-form Text Generation in Mental Health with Task-adaptive Tokenization.](http://arxiv.org/abs/2310.05317) | 该论文提出了一种任务自适应分词的方法，通过优化分词过程来增强在心理健康领域中的长文本生成。实验证明，该方法在减少标记数量的情况下显著提高了生成性能，并且可与大型语言模型结合使用。 |
| [^85] | [Persis: A Persian Font Recognition Pipeline Using Convolutional Neural Networks.](http://arxiv.org/abs/2310.05255) | 本文介绍了一种使用卷积神经网络的波斯字体识别流水线，通过对新数据集和其他数据集的测试，结果表明所提出的方法具有较高的准确率和较快的处理速度，可以无需额外的预处理步骤直接识别波斯字体。 |
| [^86] | [ChatRadio-Valuer: A Chat Large Language Model for Generalizable Radiology Report Generation Based on Multi-institution and Multi-system Data.](http://arxiv.org/abs/2310.05242) | ChatRadio-Valuer是一种基于大型语言模型的定制模型，用于通用放射学报告自动生成。它能够学习可泛化的表示并为模型自适应提供基础模式，解决了放射学报告中的泛化能力和异质性问题。 |
| [^87] | [TILFA: A Unified Framework for Text, Image, and Layout Fusion in Argument Mining.](http://arxiv.org/abs/2310.05210) | TILFA是一个用于论证挖掘中处理文本、图像和布局混合数据的统一框架，不仅能够理解文本，还能够检测光学字符和识别图像中的布局细节，并在辩论立场分类中取得了显著的性能提升。 |
| [^88] | [Factuality Challenges in the Era of Large Language Models.](http://arxiv.org/abs/2310.05189) | 大型语言模型存在生成虚假、错误或误导内容的问题，同时也面临被恶意应用的风险。本研究探讨了需要从事实核查员、新闻机构和研究与政策界采取的技术创新、监管改革和人工智能素养倡议，以解决这些问题。 |
| [^89] | [Hieros: Hierarchical Imagination on Structured State Space Sequence World Models.](http://arxiv.org/abs/2310.05167) | Hieros是一种基于结构化状态空间序列的分层想像模型，通过学习时间抽象的世界表示并在潜在空间中多个时间尺度上想象轨迹，实现了更高效的训练和想象。 |
| [^90] | [InstructDET: Diversifying Referring Object Detection with Generalized Instructions.](http://arxiv.org/abs/2310.05136) | 我们提出了一种名为InstructDET的方法，可以通过多样化的指令定位目标对象并进行指称对象检测。我们构建了一个包含图像、边界框和泛化指令的数据集，其中利用了视觉语言模型和大型语言模型生成指令。 |
| [^91] | [DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models.](http://arxiv.org/abs/2310.05074) | DialCoT是一种对话引导的链式思维方法，用于在较小的语言模型中分解和探索推理路径。通过将复杂问题分解为简单的子问题，它降低了任务难度，并使用PPO算法优化模型的推理路径选择。 |
| [^92] | [From Text to Tactic: Evaluating LLMs Playing the Game of Avalon.](http://arxiv.org/abs/2310.05036) | 本文研究了在Avalon游戏中使用LLMs的潜力，并引入了AvalonBench来评估多代理LLM代理。实验证明存在明显的能力差距。 |
| [^93] | [Self-Convinced Prompting: Few-Shot Question Answering with Repeated Introspection.](http://arxiv.org/abs/2310.05035) | 该论文提出了一种利用大规模预训练语言模型的框架，通过迭代反思和改进推理过程，以提升模型在少样本问题回答上的性能。 |
| [^94] | [Revisiting Large Language Models as Zero-shot Relation Extractors.](http://arxiv.org/abs/2310.05028) | 本研究重新审视了大型语言模型(LLMs)作为零-shot关系抽取器的潜力，并提出了通过总结和提问(\textsc{SumAsk})提示方法来改进零-shot关系抽取。实验证明LLMs在这一任务上具有良好的表现。 |
| [^95] | [A new economic and financial theory of money.](http://arxiv.org/abs/2310.04986) | 这篇论文通过根本性的改革，将电子货币纳入经济与金融理论，提出了一种新的理论框架，包括电子货币的估值基于宏观经济理论和货币政策的基本方程，以及电子货币管理公司作为协调次经济体货币和财政政策的实体。该研究避免使用普遍但不适当的指数风险模型，而是采用多时间尺度的模型。 |
| [^96] | [Diff-Transfer: Model-based Robotic Manipulation Skill Transfer via Differentiable Physics Simulation.](http://arxiv.org/abs/2310.04930) | 这篇论文提出了Diff-Transfer，一种通过可微分物理仿真来高效传输机器人技能的新框架。Diff-Transfer通过在任务空间内发现可行路径，将源任务转化为目标任务，并通过梯度信息引导适应已知的动作，成功解决另一个子任务。实验结果表明了Diff-Transfer的有效性。 |
| [^97] | [Lemur: Integrating Large Language Models in Automated Program Verification.](http://arxiv.org/abs/2310.04870) | 本论文提出了一种将LLMs和自动推理器结合起来进行自动程序验证的通用方法，并证明了其完备性。这个方法在一些合成和竞争基准上取得了实际的改进。 |
| [^98] | [On the Evolution of Knowledge Graphs: A Survey and Perspective.](http://arxiv.org/abs/2310.04835) | 本文对各种类型的知识图谱进行了全面调研，介绍了知识提取和推理技术，并展望了结合知识图谱和大语言模型的力量以及知识工程的未来方向。 |
| [^99] | [Rethink Baseline of Integrated Gradients from the Perspective of Shapley Value.](http://arxiv.org/abs/2310.04821) | 该论文从Shapley Value的角度重新思考了Integrated Gradients的基线选择，并提出了一种新的基线构建方法叫做Shapley Integrated Gradients (SIG)。在GridWorl上的模拟实验表明，SIG能够生成有意义和无偏的解释模型预测。 |
| [^100] | [DiffNAS: Bootstrapping Diffusion Models by Prompting for Better Architectures.](http://arxiv.org/abs/2310.04750) | 本文提出了一种名为DiffNAS的基础模型搜索方法，通过引导更好的结构来启动扩散模型，以提高合成性能。通过利用GPT-4作为超网，辅以搜索内存和RFID代理，以及快速收敛训练策略，搜索效率提高了2倍，达到了2.82的性能提升0.37。 |
| [^101] | [Serving Deep Learning Model in Relational Databases.](http://arxiv.org/abs/2310.04696) | 本文研究了在关系数据库中为深度学习模型提供服务的架构，并强调了三个关键范式：深度学习中心架构、UDF中心架构和关系中心架构。尽管每个架构都在特定的使用场景中有潜力，但还需要解决它们之间的集成问题和中间地带。 |
| [^102] | [Robustness-enhanced Uplift Modeling with Adversarial Feature Desensitization.](http://arxiv.org/abs/2310.04693) | 本文提出了一种增强鲁棒性的提升建模框架RUAD，并通过特征选择和对抗特征抑制两个定制模块更有效地解决了提升模型的特征敏感性问题。 |
| [^103] | [LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT.](http://arxiv.org/abs/2310.04673) | LauraGPT是一个统一的GPT模型，用于音频识别、理解和生成，具有广泛的应用范围，包括自动语音识别、语音翻译、文本到语音合成、机器翻译等任务。 |
| [^104] | [Ada-Instruct: Adapting Instruction Generators for Complex Reasoning.](http://arxiv.org/abs/2310.04484) | Ada-Instruct是一种自适应指令生成器，通过对开源LLMs进行微调，能够生成复杂推理任务中长度大于等于100的指令。在代码补全、数学推理和常识推理等任务中，Ada-Instruct显示出优于基本模型和当前自我指导方法的改进效果。 |
| [^105] | [Auto-survey Challenge.](http://arxiv.org/abs/2310.04480) | 这项研究提出了一个新的平台，用于评估大型语言模型在撰写和评论调查论文方面的能力，并组织了一次竞赛来测试该平台。评估标准包括清晰度、参考适当性、可追溯性和内容的实质价值。 |
| [^106] | [Reverse Chain: A Generic-Rule for LLMs to Master Multi-API Planning.](http://arxiv.org/abs/2310.04474) | 这项研究提出了一种名为反向链的通用规则，通过反向链思路使LLMs能够使用外部API完成复杂的函数调用任务，并通过填充参数的方式提高任务完成的准确性。 |
| [^107] | [What's the Magic Word? A Control Theory of LLM Prompting.](http://arxiv.org/abs/2310.04444) | 本论文将提示工程形式化为LLM上的最优控制问题，研究了给定token序列时是否存在一种最优提示能够准确预测最终的token，并提出了控制理论中的指标来描述LLM的可操纵性。 |
| [^108] | [Confronting Reward Model Overoptimization with Constrained RLHF.](http://arxiv.org/abs/2310.04373) | 本研究首次研究了组合奖励模型中的过度优化问题，发现组成奖励模型之间的相关性对问题的解决方式有重要影响。我们提出了一种使用约束强化学习来解决这个问题的方法。 |
| [^109] | [A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks.](http://arxiv.org/abs/2310.04270) | 本文对大型语言模型（LLM）在生物医学任务中的性能进行了综合评估，发现零样本LLMs在小样本生物医学数据集上的表现甚至超过了先进的精调生物医学模型，预训练使LLMs在生物医学领域具备了很强的专业能力。 |
| [^110] | [Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations.](http://arxiv.org/abs/2310.03951) | 这项研究提出了一个分层框架，通过自然语言推理链条（CoNLI）来检测和减少大型语言模型（LLMs）的幻觉。该框架不需要对LLMs进行微调或特定领域的提示工程，能够从不同的上下文中实现具有竞争性能的幻觉检测和减少。 |
| [^111] | [Multitask Learning for Time Series Data\\with 2D Convolution.](http://arxiv.org/abs/2310.03925) | 该论文研究了将多任务学习（MTL）应用于时间序列分类（TSC）问题，并发现将最先进的一维卷积模型与MTL集成时性能下降。为了解决这个问题，提出了一种基于二维卷积的新设计。 |
| [^112] | [Online Clustering of Bandits with Misspecified User Models.](http://arxiv.org/abs/2310.02717) | 本文介绍了在用户模型错误的情况下的聚类强化学习问题，并提出了两个鲁棒的聚类强化学习算法，以解决用户模型偏差的挑战。 |
| [^113] | [Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance.](http://arxiv.org/abs/2310.02635) | 本研究提出了一种基于具身基础先验的基础强化学习框架，通过加速训练过程来提高样本效率。 |
| [^114] | [Adapting LLM Agents Through Communication.](http://arxiv.org/abs/2310.01444) | 这项研究提出了一种名为学习通信（LTC）的训练方法，使用该方法可使LLM代理通过与环境和其他代理的交互不断改进，以适应新任务，而无需过多人类监督。 |
| [^115] | [Representation Engineering: A Top-Down Approach to AI Transparency.](http://arxiv.org/abs/2310.01405) | 这项研究介绍了一种名为表示工程化（RepE）的自上而下方法，通过借鉴认知神经科学的见解，提供了一种增强AI系统透明性的解决方案。该方法将集群级别的表示放在分析的核心，为监测和操纵深度神经网络中的高级认知现象提供了新的方法，并展示了在解决与安全相关的问题上的潜力。 |
| [^116] | [SELF: Language-Driven Self-Evolution for Large Language Model.](http://arxiv.org/abs/2310.00533) | SELF提出了一种基于语言驱动的创新方法，允许大型语言模型（LLM）不断自我进化，并通过语言反馈作为评估工具来改进模型的响应能力和训练稳定性。 |
| [^117] | [Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment.](http://arxiv.org/abs/2310.00212) | 该论文提出了一种新的强化学习框架，使用相对反馈来调整大型语言模型（LLMs）的行为，解决了现有方法在优化比较损失训练的奖励时存在的限制。同时，还提出了一种新的基于轨迹的策略梯度算法（PPPO），用于更有效地进行算法设计和函数逼近。 |
| [^118] | [ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models.](http://arxiv.org/abs/2310.00117) | ABScribe是一种界面，支持在人工智能与人类共同写作任务中快速探索多种写作变化。用户可以使用大型语言模型提示快速生成多个变体，这些变体以可重用的按钮形式呈现，并且可以通过上下文工具栏进行快速的就地比较。 |
| [^119] | [Generative Speech Recognition Error Correction with Large Language Models.](http://arxiv.org/abs/2309.15649) | 本研究探讨了大型语言模型（LLMs）作为ASR后处理器的能力，通过重新评分和错误校正来提高系统性能。通过使用指令提示和任务激活提示方法，结合上下文学习和微调技术，我们展示了LLMs的泛化能力和有效性。 |
| [^120] | [MAPTree: Beating "Optimal" Decision Trees with Bayesian Decision Trees.](http://arxiv.org/abs/2309.15312) | MAPTree是一种通过贝叶斯方法对决策树进行归纳的算法，通过AND/OR搜索实现最大后验树的恢复。在实验中，MAPTree在多个数据集上表现出更好的性能，并且能够以更小的树来实现可比较的性能。在合成数据和实际场景中，MAPTree还展示出更强的抗噪声能力和更好的泛化能力。 |
| [^121] | [Low-rank Adaptation of Large Language Model Rescoring for Parameter-Efficient Speech Recognition.](http://arxiv.org/abs/2309.15223) | 这篇论文介绍了一种基于低秩适应技术的神经语言建模系统，用于语音识别的输出重评分。通过使用低秩分解方法和优化插入矩阵，该系统能够以更高效的方式将BERT模型适应到新领域，大大减少了训练时间。 |
| [^122] | [Recurrent Hypernetworks are Surprisingly Strong in Meta-RL.](http://arxiv.org/abs/2309.14970) | 递归超网络和循环神经网络在元强化学习中的端到端学习表现出惊人的强大性能，相比于现有专门方法更为简单但效果更好。 |
| [^123] | [Invariant Learning via Probability of Sufficient and Necessary Causes.](http://arxiv.org/abs/2309.12559) | 本研究通过引入充分因素和必要因素的概率（PNS）来改善在未知测试分布上的泛化问题，以解决现有方法主要关注因果性的不变性属性而忽视充分性和必要性条件的问题。 |
| [^124] | [Environment-biased Feature Ranking for Novelty Detection Robustness.](http://arxiv.org/abs/2309.12301) | 本文提出了一种环境偏向特征排序的方法，用于鲁棒性的新颖性检测。通过计算特征的环境之间分布方差进行评分，并通过去除高分特征来改善性能。这种方法在真实和合成基准数据上均能提高性能。 |
| [^125] | [SlimPajama-DC: Understanding Data Combinations for LLM Training.](http://arxiv.org/abs/2309.10818) | 本研究探讨了使用SlimPajama进行大型语言模型训练中不同数据组合的影响，提出了全局去重和局部去重的比较和高质量多源数据集的比例对模型性能的影响。 |
| [^126] | [Pruning Large Language Models via Accuracy Predictor.](http://arxiv.org/abs/2309.09507) | 本文提出了一种通过准确性预测器来修剪大型语言模型的新方法，实验证明该方法是有效且高效的，可以自动选择最优模型并降低困惑度。 |
| [^127] | [Vision-based Analysis of Driver Activity and Driving Performance Under the Influence of Alcohol.](http://arxiv.org/abs/2309.08021) | 该论文介绍了一项研究，通过使用多种传感器，研究了急性酒精摄入对驾驶性能的影响，并通过识别酒驾行为来减少酒驾事故。 |
| [^128] | [Efficient Reinforcement Learning for Jumping Monopods.](http://arxiv.org/abs/2309.07038) | 本论文研究了如何通过在强化学习框架中注入物理知识来解决跳跃式单脚机器人的控制问题，这样可以大幅减少学习时间并且能够学习和修正可能出现的错误。 |
| [^129] | [Evaluation and Analysis of Hallucination in Large Vision-Language Models.](http://arxiv.org/abs/2308.15126) | 本文提出了基于大型语言模型的幻觉评估框架HaELM，可以评估大型视觉语言模型中的幻觉问题，并分析了导致幻觉的因素，并提出了缓解幻觉问题的建议。 |
| [^130] | [Mixup-Augmented Meta-Learning for Sample-Efficient Fine-Tuning of Protein Simulators.](http://arxiv.org/abs/2308.15116) | 本文通过Mixup增强的元学习方法实现了对蛋白质模拟器的高效微调，可以在有限的训练数据下泛化到未见过的场景，并提供了一种通用的模拟连续动态条件的方法。 |
| [^131] | [Spread Control Method on Unknown Networks Based on Hierarchical Reinforcement Learning.](http://arxiv.org/abs/2308.14311) | 本论文提出了一种基于分层强化学习的方法，用于在未知网络上进行流行病控制。模拟结果表明，该方法优于基线方法。 |
| [^132] | [Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies.](http://arxiv.org/abs/2308.14120) | chatGPT ADA是一种能够自主开发临床研究所需的最先进的机器学习模型的大型语言模型，可将高级分析工具民主化，使非数据科学家的临床医生能够轻松应用于医学领域。 |
| [^133] | [Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural Networks.](http://arxiv.org/abs/2308.09858) | 本文提出了一个完全无需反向传播的神经网络训练框架，并通过张量压缩的方差约减方法和混合梯度评估方法改进了优化和效率。同时，还扩展了框架用于物理信息的神经网络的估计。 |
| [^134] | [Graph Structural Residuals: A Learning Approach to Diagnosis.](http://arxiv.org/abs/2308.06961) | 本文提出了一种新颖的框架，将模型诊断的概念与深度图结构学习相结合，通过数据学习系统的底层结构并提供动态观察。研究通过重新定义系统表示、观察和故障的构建，引入自监督图结构学习模型以及在耦合振荡器系统上的实验，展示了数据驱动的诊断方法的潜力。 |
| [^135] | [Apple Vision Pro for Healthcare: "The Ultimate Display"?.](http://arxiv.org/abs/2308.04313) | 苹果推出了Vision Pro，一款具有混合现实和增强现实功能的虚拟现实设备，拥有独特的特点，例如内部屏幕展示佩戴者的眼睛以及数字皇冠按钮的融合功能。这款无线设备可能实现了“终极显示器”的潜力。 |
| [^136] | [ForestMonkey: Toolkit for Reasoning with AI-based Defect Detection and Classification Models.](http://arxiv.org/abs/2307.13815) | 本文介绍了Forest Monkey（FM）工具包，它是一个用于推理任何基于AI的缺陷检测和/或分类模型输出结果的工具包。该工具包提供了可解释性的数据和图表，以帮助用户理解推理结果并提出改进建议。 |
| [^137] | [On the learning Dynamics of Attention Networks.](http://arxiv.org/abs/2307.13421) | 本研究分析了软注意力、硬注意力和潜变量边际似然（LVML）注意力三种注意力模型的学习动态，发现了它们在所选择的片段聚合方式上的显著差异，并解释了分类模型在梯度下降下的演化对最终结果的影响。 |
| [^138] | [TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition.](http://arxiv.org/abs/2307.12493) | TF-ICON是一种无需训练的图像合成框架，利用文字驱动的扩散模型实现跨领域图像导向合成。与传统方法相比，TF-ICON可以在不需额外训练、微调或优化的情况下实现高质量的无缝合成，同时引入了例外提示来准确地反转真实图像为潜在表示。 |
| [^139] | [Morphological Image Analysis and Feature Extraction for Reasoning with AI-based Defect Detection and Classification Models.](http://arxiv.org/abs/2307.11643) | 本文提出了一个名为AI推理器的解释性模型，能够从图像中提取缺陷的形态学特征，并利用决策树进行推理。它通过可视化和文字说明解释基于掩模的缺陷检测和分类模型的输出，并提供有效的缓解策略以提升整体模型性能。 |
| [^140] | [Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models.](http://arxiv.org/abs/2307.10236) | 本研究从不确定性的角度对大型语言模型进行了探索性研究，通过实验发现不确定性估计方法在探索和抵制大型语言模型的不良行为方面具有潜力。 |
| [^141] | [The Value of Chess Squares.](http://arxiv.org/abs/2307.05330) | 本研究通过引入边际估值对国际象棋棋盘上的棋子和棋盘进行评价，提供了关于马、象和兵的有价值的见解。 |
| [^142] | [The ELM Neuron: an Efficient and Expressive Cortical Neuron Model Can Solve Long-Horizon Tasks.](http://arxiv.org/abs/2306.16922) | ELM神经元是一种高效且表达力强的皮层神经元模型，它只需要8K个参数就能准确模拟复杂的计算任务。 |
| [^143] | [Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging.](http://arxiv.org/abs/2306.16788) | 本研究通过将多个经过迭代幅度剪枝的模型进行平均，解决了同时利用稀疏性和参数平均的问题，并显著提升了泛化性能。 |
| [^144] | [Tensorformer: Normalized Matrix Attention Transformer for High-quality Point Cloud Reconstruction.](http://arxiv.org/abs/2306.15989) | Tensorformer是一种归一化矩阵注意力变换器，用于高质量的点云重建。它通过矩阵注意力实现了逐点和逐通道的消息传递，提供了更好的局部几何建模能力，并在两个数据集上取得了最先进的结果。 |
| [^145] | [RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$.](http://arxiv.org/abs/2306.15909) | RL$^3$是一种原则性混合方法，通过将传统强化学习学到的任务特定动作值作为元强化学习神经网络的输入，提高了元强化学习的性能。 |
| [^146] | [An Overview of Catastrophic AI Risks.](http://arxiv.org/abs/2306.12001) | 本文综述了人工智能灾难性风险的四个主要来源，包括恶意使用、人工智能竞赛、组织风险和流氓人工智能。 |
| [^147] | [Strokes2Surface: Recovering Curve Networks From 4D Architectural Design Sketches.](http://arxiv.org/abs/2306.07220) | 本文介绍了Strokes2Surface，它可从建筑师的笔画中恢复出曲线网络，对于建筑设计中的概念设计和数字建模之间的桥梁具有重要意义。 |
| [^148] | [L-C2ST: Local Diagnostics for Posterior Approximations in Simulation-Based Inference.](http://arxiv.org/abs/2306.03580) | 本文提出了一种名为 L-C2ST 的基于本地诊断实现模拟推断中后验近似的新方法，其可以在任何给定的观测下本地评估后验估计器，有效地解决了目前评估后验估计器限制解决方法的问题。 |
| [^149] | [Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning.](http://arxiv.org/abs/2305.18459) | 本文研究了单一扩散模型在建模多个任务的策略训练的大规模离线数据时的有效性。该方法名为Multi-Task Diffusion Model (MTDiff)，结合了Transformer骨干和提示学习，能够在如此复杂的多任务环境下取得相当不错的性能。 |
| [^150] | [Skill-Based Few-Shot Selection for In-Context Learning.](http://arxiv.org/abs/2305.14210) | 本文提出了Skill-KNN，一种基于技能的少样本选择方法，用于上下文学习。它解决了现有方法的偏见问题并且不需要训练模型，适用于不断扩展或更改示例库的情况。 |
| [^151] | [Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction.](http://arxiv.org/abs/2305.12660) | 本论文介绍了一种基于结构推理的类比结构推断任务，旨在解决大型语言模型在进行科学类比时忽视结构的问题。 |
| [^152] | [Clifford Group Equivariant Neural Networks.](http://arxiv.org/abs/2305.11141) | 我们引入了Clifford群等变神经网络，它可以构建O(n)和E(n)等变模型。该方法通过调整Clifford群的定义以及保持向量空间和乘法结构的作用来实现多个有利属性。 |
| [^153] | [Bayesian Renormalization.](http://arxiv.org/abs/2305.10491) | 本文提出了一种基于信息论的贝叶斯统计模型的重整化方法，使用Fisher度量定义了一个相关长度作为紧密相关的概率分布点之间的可分辨性(RG)尺度，在统计推断实验中，可以得到某个系统最大特异性观察数量的代理。贝叶斯重整化方法为给定系统准备一个在上述尺度上精度有限的有效模型，这个尺度可以被解释为当前实验装置可以探测到的最大能量。贝叶斯重整化提出了一种发现和表征基本物理理论的新框架。 |
| [^154] | [Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation.](http://arxiv.org/abs/2305.05803) | 本论文提出了一种通过利用Segment Anything Model (SAM)增强Class Activation Maps (CAM)生成高质量伪标签的方法来解决弱监督语义分割中CAM的局部激活和虚假激活的限制问题。 |
| [^155] | [Towards Summarizing Multiple Documents with Hierarchical Relationships.](http://arxiv.org/abs/2305.01498) | 提出了一个新的数据集PeerSum用于生成科学论文的元评论，源文档具有显式层次结构的丰富文档间关系，提出了一种用于元评论生成的关系感知多任务模型Rammer。 |
| [^156] | [Sample Efficient Model-free Reinforcement Learning from LTL Specifications with Optimality Guarantees.](http://arxiv.org/abs/2305.01381) | 本文提出了一种基于LTL规范的无模型强化学习方法，该方法结合乘积MDP、奖励结构和折扣机制有效地学习并优化未知随机系统最大化满足LTL规范的概率的最优策略。 |
| [^157] | [Rank-Based Learning and Local Model Based Evolutionary Algorithm for High-Dimensional Expensive Multi-Objective Problems.](http://arxiv.org/abs/2304.09444) | 本文提出了一种基于排名学习和局部模型的多目标进化算法，该算法使用分类器进行排名，以解决高维昂贵多目标优化问题。 |
| [^158] | [MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos.](http://arxiv.org/abs/2304.05292) | 本文提出了一种新的深度机器学习模型MC-ViViT，用于通过分析面部特征检测老年人轻度认知障碍。通过MC模块和结合损失函数来解决数据集样本不平衡问题，提高了算法的性能。 |
| [^159] | [Does Human Collaboration Enhance the Accuracy of Identifying LLM-Generated Deepfake Texts?.](http://arxiv.org/abs/2304.01002) | 这项研究研究了人类合作是否增强了识别LLM生成的深度伪造文本的准确性。结果表明，合作可以潜在地提高两组人对深度伪造文本的检测准确性。 |
| [^160] | [Reflexion: an autonomous agent with dynamic memory and self-reflection.](http://arxiv.org/abs/2303.11366) | 本文提出 Reflexion 方法，给智能体赋予了动态记忆和自我反思能力，以增强其任务特定的行动选择能力。 |
| [^161] | [RACCER: Towards Reachable and Certain Counterfactual Explanations for Reinforcement Learning.](http://arxiv.org/abs/2303.04475) | RACCER是第一个针对强化学习智能体行为生成对抗事实解释的专用方法，通过使用一组针对强化学习的特定对抗事实属性，保证易于实现且具有高概率预期结果的对抗事实。 |
| [^162] | [Mallat Scattering Transformation based surrogate for MagnetoHydroDynamics.](http://arxiv.org/abs/2302.10243) | 本文使用Machine Learning和Deep Learning方法开发了一个快速准确的代理模型，利用Mallat散射变换和主成分分析对磁流体力学模拟进行了加速，识别出影响输出结果的关键参数。 |
| [^163] | [Robust Knowledge Transfer in Tiered Reinforcement Learning.](http://arxiv.org/abs/2302.05534) | 本文研究了层级增强学习中的知识传输，提出了一种新颖的在线学习算法，在没有先验知识的任务相似性的情况下实现强大的知识传输。 |
| [^164] | [Understanding Translationese in Cross-Lingual Summarization.](http://arxiv.org/abs/2212.07220) | 本论文研究了跨语言摘要中的“翻译语言”现象对模型的影响，发现不同方法构建数据集会导致不同程度的翻译语言现象。文章还发现翻译语言会对模型的评估和性能产生影响，包括测试集中的翻译语言可能导致人工判断与自动评估之间的差异，以及训练集中的翻译语言会影响模型的训练表现和性能。 |
| [^165] | [Flexible Attention-Based Multi-Policy Fusion for Efficient Deep Reinforcement Learning.](http://arxiv.org/abs/2210.03729) | 该论文提出了一种知识引导的强化学习方法（KGRL），通过融合多个知识策略并利用注意力机制实现了灵活的知识重新排列。这种方法可以提高强化学习代理的样本效率和泛化能力。 |
| [^166] | [Rank-N-Contrast: Learning Continuous Representations for Regression.](http://arxiv.org/abs/2210.01189) | Rank-N-Contrast是一种学习连续表示的回归框架，通过对样本在目标空间中的排名进行比较来提高性能，并在计算机视觉、人机交互和医疗保健等领域的回归任务中取得了最先进的结果。 |
| [^167] | [Toward Trustworthy Neural Program Synthesis.](http://arxiv.org/abs/2210.00848) | 我们提出了一种能够估计大型语言模型中采样程序正确性概率的简单方法，通过采样候选程序和候选谓词来预测程序的正确性，并推断出关于生成代码行为解释的有用谓词。 |
| [^168] | [Learning domain-specific causal discovery from time series.](http://arxiv.org/abs/2209.05598) | 本研究旨在通过数据驱动的方法提高时间序列中领域特定的因果发现。实验证明，该方法在多个领域上明显优于人类设计的通用因果发现方法。 |
| [^169] | [A Review of Deep Learning-based Approaches for Deepfake Content Detection.](http://arxiv.org/abs/2202.06095) | 本文综述了最近基于深度学习方法的深伪造内容检测的研究，系统地回顾了不同类别的伪造内容检测，并报告了所考察工作的优点和缺点，以及深伪造检测领域仍未解决的问题和不足之处的未来研究方向。 |
| [^170] | [Mathematical Runtime Analysis for the Non-Dominated Sorting Genetic Algorithm II (NSGA-II).](http://arxiv.org/abs/2112.08581) | 这项研究通过数学分析证明了非支配排序遗传算法II（NSGA-II）的运行时间特性，发现当种群规模是帕累托前沿大小的四倍时，NSGA-II具有与其他算法相同的运行时间保证。 |

# 详细

[^1]: 用多样化反馈构建大型语言模型的对齐方法

    Constructive Large Language Models Alignment with Diverse Feedback. (arXiv:2310.06450v1 [cs.CL])

    [http://arxiv.org/abs/2310.06450](http://arxiv.org/abs/2310.06450)

    本文提出了一种新的方法，即建构性和多样化反馈（CDF），用于增强大型语言模型（LLM）的对齐效果。我们通过收集不同类型的反馈，并根据问题的难度级别进行处理，实现了更好的性能。

    

    在大型语言模型（LLMs）的研究中，对其与人类价值观的对齐越来越重视，以减少有害内容的影响。然而，当前的对齐方法通常仅依赖于人类反馈的单一形式，如偏好、注释标签或自然语言批评，忽视了结合这些反馈类型的潜在优势。这种限制导致性能不佳，即使有丰富的训练数据。本文引入了建构性和多样化反馈（CDF）作为增强LLM对齐的新方法，受建构学习理论的启发。我们的方法涉及收集适用于训练数据集中不同难度问题的三种不同类型的反馈。具体而言，我们利用批评反馈解决简单问题，利用改进反馈解决中等问题，利用偏好反馈解决困难问题。通过用这种多样化反馈训练我们的模型，我们获得了更好的表现。

    In recent research on large language models (LLMs), there has been a growing emphasis on aligning these models with human values to reduce the impact of harmful content. However, current alignment methods often rely solely on singular forms of human feedback, such as preferences, annotated labels, or natural language critiques, overlooking the potential advantages of combining these feedback types. This limitation leads to suboptimal performance, even when ample training data is available. In this paper, we introduce Constructive and Diverse Feedback (CDF) as a novel method to enhance LLM alignment, inspired by constructivist learning theory. Our approach involves collecting three distinct types of feedback tailored to problems of varying difficulty levels within the training dataset. Specifically, we exploit critique feedback for easy problems, refinement feedback for medium problems, and preference feedback for hard problems. By training our model with this diversified feedback, we a
    
[^2]: 逐步功能重构的关系概念分析

    Stepwise functional refoundation of relational concept analysis. (arXiv:2310.06441v1 [cs.AI])

    [http://arxiv.org/abs/2310.06441](http://arxiv.org/abs/2310.06441)

    逐步功能重构的关系概念分析（RCA）是形式概念分析的扩展，通过定义良构解决方案的空间和相关函数，解决了RCA在循环依赖数据上返回单一概念格家族的问题。

    

    关系概念分析（RCA）是形式概念分析的扩展，允许同时处理多个相关的语境。它被设计用于从数据中学习描述逻辑理论，并在各种应用中使用。关于RCA的一个令人困惑的观察是，尽管数据存在循环依赖关系，它返回一个单一的概念格家族，其他解决方案可能被认为是可接受的。RCA的语义以操作方式提供，对此问题并没有提供明确的解释。在本报告中，我们将这些可接受的解决方案定义为属于初始语境确定的空间的概念格家族（良构），不能扩展新属性（饱和），并且仅涉及该家族的概念（自支持）。我们通过定义良构解决方案的空间以及该空间上的两个函数（一个扩张函数和一个收缩函数），采用功能视图来描述RCA过程。我们展示了可接受的解决方案…

    Relational concept analysis (RCA) is an extension of formal concept analysis allowing to deal with several related contexts simultaneously. It has been designed for learning description logic theories from data and used within various applications. A puzzling observation about RCA is that it returns a single family of concept lattices although, when the data feature circular dependencies, other solutions may be considered acceptable. The semantics of RCA, provided in an operational way, does not shed light on this issue. In this report, we define these acceptable solutions as those families of concept lattices which belong to the space determined by the initial contexts (well-formed), cannot scale new attributes (saturated), and refer only to concepts of the family (self-supported). We adopt a functional view on the RCA process by defining the space of well-formed solutions and two functions on that space: one expansive and the other contractive. We show that the acceptable solutions a
    
[^3]: Whispering LLaMA：一种用于语音识别的跨模态生成错误校正框架

    Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition. (arXiv:2310.06434v1 [cs.CL])

    [http://arxiv.org/abs/2310.06434](http://arxiv.org/abs/2310.06434)

    Whispering LLaMA是一种用于语音识别的跨模态生成错误校正框架，通过融合声学信息和外部语言表示，生成准确的语音转录上下文，相对于n-best假设，词错误率性能提升了37.66%。

    

    我们引入了一种新的跨模态融合技术，用于生成准确的语音转录上下文，以进行自动语音识别 (ASR) 的生成式错误校正。与现有的基于排名的重新评分方法不同，我们的方法灵活运用独特的初始化技术和参数有效的算法，通过预训练的语音和文本模型提升了ASR性能。通过对多样化的ASR数据集进行评估，我们评估了我们的融合技术的稳定性和可复现性，相对于n-best假设，我们的方法的词错误率性能提升了37.66%。为了鼓励未来的研究，我们将我们的代码和预训练模型开源在https://github.com/Srijith-rkr/Whispering-LLaMA上。

    We introduce a new cross-modal fusion technique designed for generative error correction in automatic speech recognition (ASR). Our methodology leverages both acoustic information and external linguistic representations to generate accurate speech transcription contexts. This marks a step towards a fresh paradigm in generative error correction within the realm of n-best hypotheses. Unlike the existing ranking-based rescoring methods, our approach adeptly uses distinct initialization techniques and parameter-efficient algorithms to boost ASR performance derived from pre-trained speech and text models. Through evaluation across diverse ASR datasets, we evaluate the stability and reproducibility of our fusion technique, demonstrating its improved word error rate relative (WERR) performance in comparison to n-best hypotheses by relatively 37.66%. To encourage future research, we have made our code and pre-trained models open source at https://github.com/Srijith-rkr/Whispering-LLaMA.
    
[^4]: 逆向测试：解决测试Oracle问题的新方法

    Retromorphic Testing: A New Approach to the Test Oracle Problem. (arXiv:2310.06433v1 [cs.SE])

    [http://arxiv.org/abs/2310.06433](http://arxiv.org/abs/2310.06433)

    逆向测试是一种新颖的黑盒测试方法，通过建立双程序结构，利用辅助程序将输入数据进行前向处理，然后使用后向程序将程序输出反转为原始输入格式，从而解决了测试Oracle问题。

    

    测试Oracle作为一个评估软件输出和给定输入集的预期行为之间对应关系的标准或机制。在自动化测试中，黑盒技术被广泛使用，其中包括差分测试和变异测试等著名方法，以其非侵入性的测试Oracle构建方式而闻名。受到数学概念的启发，我们提出了逆向测试，一种新颖的黑盒测试方法。它利用辅助程序与被测试程序结合，建立了由前向程序和后向程序组成的双程序结构。输入数据首先由前向程序处理，然后使用后向程序将其程序输出反转为其原始输入格式。特别地，辅助程序可以作为前向程序或后向程序运行，从而导致不同的测试模式。该过程通过检查这两个程序之间的关系来进行测试。

    A test oracle serves as a criterion or mechanism to assess the correspondence between software output and the anticipated behavior for a given input set. In automated testing, black-box techniques, known for their non-intrusive nature in test oracle construction, are widely used, including notable methodologies like differential testing and metamorphic testing. Inspired by the mathematical concept of inverse function, we present Retromorphic Testing, a novel black-box testing methodology. It leverages an auxiliary program in conjunction with the program under test, which establishes a dual-program structure consisting of a forward program and a backward program. The input data is first processed by the forward program and then its program output is reversed to its original input format using the backward program. In particular, the auxiliary program can operate as either the forward or backward program, leading to different testing modes. The process concludes by examining the relation
    
[^5]: 第一届可解释人工智能艺术国际研讨会(XAIxArts)论文集

    Proceedings of The first international workshop on eXplainable AI for the Arts (XAIxArts). (arXiv:2310.06428v1 [cs.AI])

    [http://arxiv.org/abs/2310.06428](http://arxiv.org/abs/2310.06428)

    这个论文集汇集了来自人机交互、交互设计、人工智能和数字艺术等领域的研究者，讨论了可解释人工智能在艺术领域的作用。

    

    这个第一届可解释人工智能艺术国际研讨会（XAIxArts）汇集了人机交互、交互设计、人工智能和可解释人工智能（XAI），以及数字艺术等领域的研究者，探讨了可解释人工智能在艺术领域的作用。此研讨会在第15届ACM创新与认知大会（C&C 2023）上举行。

    This first international workshop on explainable AI for the Arts (XAIxArts) brought together a community of researchers in HCI, Interaction Design, AI, explainable AI (XAI), and digital arts to explore the role of XAI for the Arts.  Workshop held at the 15th ACM Conference on Creativity and Cognition (C&C 2023).
    
[^6]: TANGO: 时间反演潜在图ODE用于多智能体动力系统

    TANGO: Time-Reversal Latent GraphODE for Multi-Agent Dynamical Systems. (arXiv:2310.06427v1 [cs.LG])

    [http://arxiv.org/abs/2310.06427](http://arxiv.org/abs/2310.06427)

    本文提出了TANGO方法，通过时间反演对称性作为归纳偏差，对多智能体动力系统进行学习，即使在非保守的可逆系统中也能保持能量，并且能更高效地学习系统动力学。

    

    在许多领域中，从数据中学习复杂的多智能体系统动力学是至关重要的，例如在物理模拟和材料建模中。在纯数据驱动方法的基础上，现有的基于物理的方法如Hamiltonian神经网络严格遵循能量守恒定律引入归纳偏差，使得学习更加高效。然而，许多现实世界中的系统并不严格遵守能量守恒定律，如带有摩擦的弹簧系统。鉴于此，我们将注意力转向一个更广泛的物理原理：时间反演对称性，它描述了当系统的动力在时间上倒转时，动力学应保持不变。这仍然有助于保持保守系统的能量，并同时为非保守的可逆系统提供强大的归纳偏差。为了注入这种归纳偏差，本文提出了一个简单而有效的自监督正则化项作为软约束，将正向和逆向动力对齐。

    Learning complex multi-agent system dynamics from data is crucial across many domains, such as in physical simulations and material modeling. Extended from purely data-driven approaches, existing physics-informed approaches such as Hamiltonian Neural Network strictly follow energy conservation law to introduce inductive bias, making their learning more sample efficiently. However, many real-world systems do not strictly conserve energy, such as spring systems with frictions. Recognizing this, we turn our attention to a broader physical principle: Time-Reversal Symmetry, which depicts that the dynamics of a system shall remain invariant when traversed back over time. It still helps to preserve energies for conservative systems and in the meanwhile, serves as a strong inductive bias for non-conservative, reversible systems. To inject such inductive bias, in this paper, we propose a simple-yet-effective self-supervised regularization term as a soft constraint that aligns the forward and b
    
[^7]: 用于传播信息检测的大型语言模型

    Large Language Models for Propaganda Detection. (arXiv:2310.06422v1 [cs.CL])

    [http://arxiv.org/abs/2310.06422](http://arxiv.org/abs/2310.06422)

    这项研究探讨了使用现代大型语言模型（LLMs）如GPT-3和GPT-4在宣传信息检测方面的有效性。实验结果显示，GPT-4达到了与当前最先进方法相符的结果。

    

    在我们数字化社会中，宣传信息的普遍存在对社会和真相的传播构成了挑战。通过自然语言处理在文本中检测宣传信息是具有挑战性的，因为存在微妙的操纵技术和语境依赖。为了解决这个问题，我们研究了现代大型语言模型（LLMs）如GPT-3和GPT-4在宣传信息检测方面的有效性。我们使用SemEval-2020任务11数据集进行实验，该数据集包含具有14种宣传技术标签的新闻文章，作为一个多标签分类问题。我们采用了GPT-3和GPT-4的五种变体，结合了不同模型之间的各种提示工程和微调策略。通过评估$F1$分数，$Precision$和$Recall$等指标来评估模型的性能，并将结果与使用RoBERTa的当前最先进方法进行比较。我们的研究结果表明，GPT-4实现了与当前最先进方法相当的结果。

    The prevalence of propaganda in our digital society poses a challenge to societal harmony and the dissemination of truth. Detecting propaganda through NLP in text is challenging due to subtle manipulation techniques and contextual dependencies. To address this issue, we investigate the effectiveness of modern Large Language Models (LLMs) such as GPT-3 and GPT-4 for propaganda detection. We conduct experiments using the SemEval-2020 task 11 dataset, which features news articles labeled with 14 propaganda techniques as a multi-label classification problem. Five variations of GPT-3 and GPT-4 are employed, incorporating various prompt engineering and fine-tuning strategies across the different models. We evaluate the models' performance by assessing metrics such as $F1$ score, $Precision$, and $Recall$, comparing the results with the current state-of-the-art approach using RoBERTa. Our findings demonstrate that GPT-4 achieves comparable results to the current state-of-the-art. Further, thi
    
[^8]: 用于图学习中的拓扑概括的流动扩散变压器

    Advective Diffusion Transformers for Topological Generalization in Graph Learning. (arXiv:2310.06417v1 [cs.LG])

    [http://arxiv.org/abs/2310.06417](http://arxiv.org/abs/2310.06417)

    本研究探索了在不同的图拓扑存在下，图扩散方程如何对GNN进行外推和概括，揭示了基于局部扩散的现有模型在概括能力上的不足，并提出了非局部扩散的潜力。

    

    图扩散方程与图神经网络（GNN）密切相关，并且最近引起了人们的关注，作为分析GNN动力学、形式化其表达能力和证明架构选择的有原则的框架。图学习中的一个关键问题是GNN的概括能力。当前方法的一个主要限制在于假设训练集和测试集中的图拓扑来自相同的分布。本文通过探索图扩散方程在不同图拓扑存在下的外推和概括能力，迈出了解析GNN概括性的一步。我们首先展示了基于图上局部扩散的现有模型在概括能力上的不足，这是由于对拓扑变化的指数敏感性引起的。随后的分析揭示了非局部扩散的潜力，它倡导对完全连接的潜在图进行特征传播。

    Graph diffusion equations are intimately related to graph neural networks (GNNs) and have recently attracted attention as a principled framework for analyzing GNN dynamics, formalizing their expressive power, and justifying architectural choices. One key open questions in graph learning is the generalization capabilities of GNNs. A major limitation of current approaches hinges on the assumption that the graph topologies in the training and test sets come from the same distribution. In this paper, we make steps towards understanding the generalization of GNNs by exploring how graph diffusion equations extrapolate and generalize in the presence of varying graph topologies. We first show deficiencies in the generalization capability of existing models built upon local diffusion on graphs, stemming from the exponential sensitivity to topology variation. Our subsequent analysis reveals the promise of non-local diffusion, which advocates for feature propagation over fully-connected latent gr
    
[^9]: Hexa: 知识驱动的对话系统的自我提升

    Hexa: Self-Improving for Knowledge-Grounded Dialogue System. (arXiv:2310.06404v1 [cs.CL])

    [http://arxiv.org/abs/2310.06404](http://arxiv.org/abs/2310.06404)

    本论文提出了一种自我提升的方法，用于改进知识驱动对话生成的中间步骤的生成性能。通过引入引导提示和修改损失函数的自举策略，提高了生成自动生成回答的多样性，并在各种基准数据集上实验证明了该方法的有效性。

    

    知识驱动的对话生成中一种常见的做法是使用模块化的方法明确地利用中间步骤（如网络搜索、记忆检索）。然而，与对话响应相比，这些步骤的数据往往难以获取，因为在普通对话中无法观察到它们。为了填补这些数据的缺失，我们开发了一种自我提升方法，以改进中间步骤的生成性能，而不需要地面真实数据。具体而言，我们提出了一种新颖的引导提示和修改的损失函数的引导自动生成回答多样性的自举方法。通过在各种基准数据集上进行实验，我们经验证明我们的方法成功地利用了自我提升机制，在生成中间和最终回答方面改善了知识驱动对话生成任务的性能。

    A common practice in knowledge-grounded dialogue generation is to explicitly utilize intermediate steps (e.g., web-search, memory retrieval) with modular approaches. However, data for such steps are often inaccessible compared to those of dialogue responses as they are unobservable in an ordinary dialogue. To fill in the absence of these data, we develop a self-improving method to improve the generative performances of intermediate steps without the ground truth data. In particular, we propose a novel bootstrapping scheme with a guided prompt and a modified loss function to enhance the diversity of appropriate self-generated responses. Through experiments on various benchmark datasets, we empirically demonstrate that our method successfully leverages a self-improving mechanism in generating intermediate and final responses and improves the performances on the task of knowledge-grounded dialogue generation.
    
[^10]: Lo-Hi: 实用的机器学习药物发现基准测试

    Lo-Hi: Practical ML Drug Discovery Benchmark. (arXiv:2310.06399v1 [cs.LG])

    [http://arxiv.org/abs/2310.06399](http://arxiv.org/abs/2310.06399)

    本研究创建了一个实用的Lo-Hi基准测试，包括前导优化和命中识别两个任务，为药物发现过程提供了一种切实可行的评估方法。对于命中识别任务，研究者设计了一种解决顶点最小k-Cut问题的新型分子拆分算法，并测试了各种机器学习模型的性能。研究还发现现有基准测试不现实且过于乐观。

    

    寻找新药变得越来越困难。药物发现的希望之一是使用机器学习模型预测分子属性。因此，正在开发和测试分子属性预测模型，以在MoleculeNet等基准测试中应用。然而，现有的基准测试不切实际，并且与实际应用模型相差太大。我们创建了一个新的实用的Lo-Hi基准测试，包括两个任务：前导优化（Lo）和命中识别（Hi），对应于真实的药物发现过程。对于Hi任务，我们设计了一种解决平衡顶点最小k-Cut问题的新型分子拆分算法。我们测试了最先进和经典的机器学习模型，揭示了在实际环境中哪种效果更好。我们分析了现代基准测试，并显示它们不切实际并且过于乐观。

    Finding new drugs is getting harder and harder. One of the hopes of drug discovery is to use machine learning models to predict molecular properties. That is why models for molecular property prediction are being developed and tested on benchmarks such as MoleculeNet. However, existing benchmarks are unrealistic and are too different from applying the models in practice. We have created a new practical \emph{Lo-Hi} benchmark consisting of two tasks: Lead Optimization (Lo) and Hit Identification (Hi), corresponding to the real drug discovery process. For the Hi task, we designed a novel molecular splitting algorithm that solves the Balanced Vertex Minimum $k$-Cut problem. We tested state-of-the-art and classic ML models, revealing which works better under practical settings. We analyzed modern benchmarks and showed that they are unrealistic and overoptimistic.  Review: https://openreview.net/forum?id=H2Yb28qGLV  Lo-Hi benchmark: https://github.com/SteshinSS/lohi_neurips2023  Lo-Hi split
    
[^11]: P5: 用于个性化回答选择的即插即用个人角色提示

    P5: Plug-and-Play Persona Prompting for Personalized Response Selection. (arXiv:2310.06390v1 [cs.CL])

    [http://arxiv.org/abs/2310.06390](http://arxiv.org/abs/2310.06390)

    提出了一种即插即用的个人角色提示方法，用于个性化回答选择的检索对话机器人。这种方法在零样本设置下表现良好，减少了对基于个人角色的训练数据的依赖，并且使得系统更容易扩展到其他语言。

    

    使用基于个人角色的检索对话机器人对于个性化对话至关重要，但也面临一些挑战。1）通常情况下，收集基于个人角色的语料库非常昂贵。2）在实际应用中，对话机器人系统并不总是根据个人角色做出回应。为了解决这些挑战，我们提出了一种即插即用的个人角色提示方法。如果个人角色信息不可用，我们的系统可以作为标准的开放域对话机器人运行。我们证明了这种方法在零样本设置下表现良好，从而减少了对基于个人角色的训练数据的依赖。这使得在无需构建基于个人角色的语料库的情况下，更容易将该系统扩展到其他语言。此外，我们的模型可以进行微调以获得更好的性能。在实验中，零样本模型在原始个人角色和修订个人角色方面分别提高了7.71和1.04个点。

    The use of persona-grounded retrieval-based chatbots is crucial for personalized conversations, but there are several challenges that need to be addressed. 1) In general, collecting persona-grounded corpus is very expensive. 2) The chatbot system does not always respond in consideration of persona at real applications. To address these challenges, we propose a plug-and-play persona prompting method. Our system can function as a standard open-domain chatbot if persona information is not available. We demonstrate that this approach performs well in the zero-shot setting, which reduces the dependence on persona-ground training data. This makes it easier to expand the system to other languages without the need to build a persona-grounded corpus. Additionally, our model can be fine-tuned for even better performance. In our experiments, the zero-shot model improved the standard model by 7.71 and 1.04 points in the original persona and revised persona, respectively. The fine-tuned model impro
    
[^12]: 只需少量上下文示范即可实现越狱和对齐的语言模型

    Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations. (arXiv:2310.06387v1 [cs.LG])

    [http://arxiv.org/abs/2310.06387](http://arxiv.org/abs/2310.06387)

    本文研究了使用少量上下文示范来操纵语言模型对齐能力的方法。通过提供示范，而无需微调，可以增加或降低模型回答恶意提示的概率。我们提出了相同上下文攻击和相同上下文防御方法，用于越狱和对齐语言模型。实验证明了这些方法的有效性。

    

    大规模语言模型（LLM）在各种任务中取得了显著的成功，但对其安全性和生成恶意内容的潜在风险的担忧也浮现出来。本文探讨了在相同上下文学习（ICL）中操纵LLM对齐能力的效果。我们发现，仅通过少量的上下文示范而无需微调，就可以操纵LLM增加或降低越狱概率，即回答恶意提示。基于这些观察，我们提出了用于越狱和对齐语言模型目的的相同上下文攻击（ICA）和相同上下文防御（ICD）方法。ICA通过构造恶意上下文指导模型生成有害输出，而ICD通过拒绝回答有害提示的示范来增强模型的稳健性。我们的实验证明了ICA和ICD在增加或降低对抗越狱攻击成功率方面的有效性。总的来说，我们揭示了ICL在越狱和对齐语言模型领域的潜力。

    Large Language Models (LLMs) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating malicious content have emerged. In this paper, we explore the power of In-Context Learning (ICL) in manipulating the alignment ability of LLMs. We find that by providing just few in-context demonstrations without fine-tuning, LLMs can be manipulated to increase or decrease the probability of jailbreaking, i.e. answering malicious prompts. Based on these observations, we propose In-Context Attack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding aligned language model purposes. ICA crafts malicious contexts to guide models in generating harmful outputs, while ICD enhances model robustness by demonstrations of rejecting to answer harmful prompts. Our experiments show the effectiveness of ICA and ICD in increasing or reducing the success rate of adversarial jailbreaking attacks. Overall, we shed light on the potential of ICL t
    
[^13]: 多模态模型面对缺失模态的鲁棒性问题

    What Makes for Robust Multi-Modal Models in the Face of Missing Modalities?. (arXiv:2310.06383v1 [cs.AI])

    [http://arxiv.org/abs/2310.06383](http://arxiv.org/abs/2310.06383)

    本研究从信息论的角度对多模态模型遇到缺失模态的情景进行了建模，并提出了两个关键要点：编码器能够从非缺失模态中提取好的特征，并且提取的特征在模态融合过程中具有足够的鲁棒性。

    

    随着多模态学习的成功发展，对于多模态模型在面对缺失模态情况下的鲁棒性问题的研究受到了越来越多的关注。然而，先前在此领域的研究存在某些限制，因为它们往往缺乏理论洞察力，或者其方法仅适用于特定的网络架构或模态。我们从信息论的角度对多模态模型遇到缺失模态的情景进行建模，并说明在这种情况下，通过有效利用非缺失模态中固有的信息可以接近性能上限。实际上，有两个关键方面：（1）编码器应能够从非缺失模态中提取足够好的特征；（2）提取的特征在模态间融合过程中应具有足够的鲁棒性，不受噪声的影响。

    With the growing success of multi-modal learning, research on the robustness of multi-modal models, especially when facing situations with missing modalities, is receiving increased attention. Nevertheless, previous studies in this domain exhibit certain limitations, as they often lack theoretical insights or their methodologies are tied to specific network architectures or modalities. We model the scenarios of multi-modal models encountering missing modalities from an information-theoretic perspective and illustrate that the performance ceiling in such scenarios can be approached by efficiently utilizing the information inherent in non-missing modalities. In practice, there are two key aspects: (1) The encoder should be able to extract sufficiently good features from the non-missing modality; (2) The extracted features should be robust enough not to be influenced by noise during the fusion process across modalities. To this end, we introduce Uni-Modal Ensemble with Missing Modality Ad
    
[^14]: 基于脉冲网络和多盒检测的黑色物体检测的高效策略

    Advanced Efficient Strategy for Detection of Dark Objects Based on Spiking Network with Multi-Box Detection. (arXiv:2310.06370v1 [cs.CV])

    [http://arxiv.org/abs/2310.06370](http://arxiv.org/abs/2310.06370)

    本研究提出一种基于脉冲网络和多盒检测技术的高效、可靠、能效高的黑色物体检测模型，通过结合预训练的VGG16模型和脉冲和普通卷积层，能够准确检测黑色物体边界框，并实现高效的训练过程。

    

    几种深度学习算法已经在现有的物体检测任务中展现出惊人的性能，但是识别较暗的物体是最大的挑战。此外，这些技术在检测方面遇到困难或者识别速度较慢，导致性能损失明显。因此，需要一种改进的、准确的检测方法来解决上述困难。本研究提出了一种基于脉冲和普通卷积层的能效高、可靠的物体检测模型。该模型分为两个部分。第一部分是特征提取器，利用预训练的VGG16模型。提案结构的第二部分是脉冲和普通卷积层的组合，用于检测图像的边界框。我们使用预训练模型对检测到的物体进行分类。使用最先进的Python库，可以高效地训练脉冲层。所提出的脉冲卷积物体检测器模型在黑色物体检测上表现出优秀的性能。

    Several deep learning algorithms have shown amazing performance for existing object detection tasks, but recognizing darker objects is the largest challenge. Moreover, those techniques struggled to detect or had a slow recognition rate, resulting in significant performance losses. As a result, an improved and accurate detection approach is required to address the above difficulty. The whole study proposes a combination of spiked and normal convolution layers as an energy-efficient and reliable object detector model. The proposed model is split into two sections. The first section is developed as a feature extractor, which utilizes pre-trained VGG16, and the second section of the proposal structure is the combination of spiked and normal Convolutional layers to detect the bounding boxes of images. We drew a pre-trained model for classifying detected objects. With state of the art Python libraries, spike layers can be trained efficiently. The proposed spike convolutional object detector 
    
[^15]: 几何对齐转移编码器用于归纳转移回归任务

    Geometrically Aligned Transfer Encoder for Inductive Transfer in Regression Tasks. (arXiv:2310.06369v1 [cs.AI])

    [http://arxiv.org/abs/2310.06369](http://arxiv.org/abs/2310.06369)

    本文提出了一种基于微分几何的转移学习技术，通过将模型的潜在向量映射到黎曼曲面上的局部平坦坐标，实现在回归任务中的知识转移，并展示了该方法的优越性能和稳定行为。

    

    转移学习是处理可能与其他丰富数据相关的少量数据的关键技术。然而，现有大部分方法都集中在使用图像和语言数据集的分类任务上。因此，为了扩展转移学习方案到回归任务，我们提出了一种基于微分几何的新型转移技术，即几何对齐转移编码器（GATE）。在这种方法中，我们将模型的潜在向量解释为存在于黎曼曲面上的。我们找到了一种适当的微分同胚，确保每个任意点映射到重叠区域中的局部平坦坐标，从而实现从源数据到目标数据的知识转移。这也作为模型在外推区域行为良好的有效正则化器。在本文中，我们证明了GATE优于传统方法，并在潜在空间中表现出稳定的行为。

    Transfer learning is a crucial technique for handling a small amount of data that is potentially related to other abundant data. However, most of the existing methods are focused on classification tasks using images and language datasets. Therefore, in order to expand the transfer learning scheme to regression tasks, we propose a novel transfer technique based on differential geometry, namely the Geometrically Aligned Transfer Encoder (GATE). In this method, we interpret the latent vectors from the model to exist on a Riemannian curved manifold. We find a proper diffeomorphism between pairs of tasks to ensure that every arbitrary point maps to a locally flat coordinate in the overlapping region, allowing the transfer of knowledge from the source to the target data. This also serves as an effective regularizer for the model to behave in extrapolation regions. In this article, we demonstrate that GATE outperforms conventional methods and exhibits stable behavior in both the latent space 
    
[^16]: Noisy-ArcMix: 加噪角度边际损失与Mixup结合的异常声音检测

    Noisy-ArcMix: Additive Noisy Angular Margin Loss Combined With Mixup Anomalous Sound Detection. (arXiv:2310.06364v1 [cs.SD])

    [http://arxiv.org/abs/2310.06364](http://arxiv.org/abs/2310.06364)

    本文提出了一种名为Noisy-ArcMix的训练技术，旨在确保异常声音检测模型具有足够的类内紧凑性和正常与异常样本之间的角度差距。同时，引入了一种能够提取重要时间区域特征的架构，增强模型对于时间帧的学习能力。

    

    无监督的异常声音检测旨在通过学习正常操作声音的特征并感知其偏差来识别异常声音。最近的方法集中在利用正常数据的分类进行自监督任务，并且先进的模型已经表明通过表示学习确保异常数据的表示空间对于获得紧凑的类内分布和明显的类间分布是重要的。然而，我们发现传统方法往往无法确保足够的类内紧凑性，并且样本及其对应中心之间存在角度差异。在本文中，我们提出了一种训练技术，旨在确保类内紧凑性，并增加正常和异常样本之间的角度差距。此外，我们提出了一种能够提取重要时间区域特征的架构，使模型能够学习哪些时间帧应该被强调或抑制。

    Unsupervised anomalous sound detection (ASD) aims to identify anomalous sounds by learning the features of normal operational sounds and sensing their deviations. Recent approaches have focused on the self-supervised task utilizing the classification of normal data, and advanced models have shown that securing representation space for anomalous data is important through representation learning yielding compact intra-class and well-separated intra-class distributions. However, we show that conventional approaches often fail to ensure sufficient intra-class compactness and exhibit angular disparity between samples and their corresponding centers. In this paper, we propose a training technique aimed at ensuring intra-class compactness and increasing the angle gap between normal and abnormal samples. Furthermore, we present an architecture that extracts features for important temporal regions, enabling the model to learn which time frames should be emphasized or suppressed. Experimental re
    
[^17]: 使用YOLOv5从图像和视频中检测火灾

    Fire Detection From Image and Video Using YOLOv5. (arXiv:2310.06351v1 [cs.CV])

    [http://arxiv.org/abs/2310.06351](http://arxiv.org/abs/2310.06351)

    使用改进的YOLOv5算法实现火灾图像和视频的检测，准确率高，能有效处理小火灾目标的检测和类似火灾和烟雾的物体。

    

    为了在室内、室外和森林火灾图像中检测火灾目标以及在不同自然光下进行火灾检测，提出了一种改进的YOLOv5火灾检测深度学习算法。YOLOv5检测模型将特征提取网络从三维扩展到了四维，增强了火灾小目标识别的特征传播，改善了网络性能并减少了模型参数。此外，通过提升特征金字塔，得到了表现最佳的预测框。Fire-YOLOv5相比其他优秀的目标检测网络在火灾和烟雾小目标的检测中取得了出色的结果，mAP为90.5%，f1得分为88%。总的来说，Fire-YOLOv5检测模型可以有效处理小火灾目标的检测，以及类似火灾和烟雾的物体，f1得分为0.88。当输入图像大小为416 x 416分辨率时，平均检测时间为每帧0.12秒，这是非常快速的。

    For the detection of fire-like targets in indoor, outdoor and forest fire images, as well as fire detection under different natural lights, an improved YOLOv5 fire detection deep learning algorithm is proposed. The YOLOv5 detection model expands the feature extraction network from three dimensions, which enhances feature propagation of fire small targets identification, improves network performance, and reduces model parameters. Furthermore, through the promotion of the feature pyramid, the top-performing prediction box is obtained. Fire-YOLOv5 attains excellent results compared to state-of-the-art object detection networks, notably in the detection of small targets of fire and smoke with mAP 90.5% and f1 score 88%. Overall, the Fire-YOLOv5 detection model can effectively deal with the inspection of small fire targets, as well as fire-like and smoke-like objects with F1 score 0.88. When the input image size is 416 x 416 resolution, the average detection time is 0.12 s per frame, which 
    
[^18]: 增强线性表示冗余的卷积神经网络滤波剪枝

    Filter Pruning For CNN With Enhanced Linear Representation Redundancy. (arXiv:2310.06344v1 [cs.CV])

    [http://arxiv.org/abs/2310.06344](http://arxiv.org/abs/2310.06344)

    本文提出了一种增强线性表示冗余的卷积神经网络滤波剪枝方法，通过引入相关系数矩阵损失（CCM-loss）和匹配通道选择策略，可以在训练过程中加强特征图之间的线性表示关系，在剪枝过程中更好地去除同质化的部分。

    

    结构化网络剪枝优于非结构化方法，因为它们可以利用发展中的并行计算技术。在本文中，我们提出了一种新的结构化剪枝方法。首先，为了创建更多的结构化冗余，我们提出了一种从同一层的不同特征图的相关系数矩阵计算得到的数据驱动损失函数项，称为相关系数矩阵损失（CCM-loss）。这个损失项可以在训练过程中鼓励神经网络学习更强的特征图之间的线性表示关系，从而在剪枝过程中更容易去除同质化的部分。CCM-loss为我们提供了另一种通用的超越数学工具，除了集中生成零的L*-norm正则化之外，还可以为不同类型的冗余生成更多的冗余。此外，我们设计了一个基于主成分分析的匹配通道选择策略，以充分利用CCM-loss的最大潜能。

    Structured network pruning excels non-structured methods because they can take advantage of the thriving developed parallel computing techniques. In this paper, we propose a new structured pruning method. Firstly, to create more structured redundancy, we present a data-driven loss function term calculated from the correlation coefficient matrix of different feature maps in the same layer, named CCM-loss. This loss term can encourage the neural network to learn stronger linear representation relations between feature maps during the training from the scratch so that more homogenous parts can be removed later in pruning. CCM-loss provides us with another universal transcendental mathematical tool besides L*-norm regularization, which concentrates on generating zeros, to generate more redundancy but for the different genres. Furthermore, we design a matching channel selection strategy based on principal components analysis to exploit the maximum potential ability of CCM-loss. In our new s
    
[^19]: 基于对比提示学习的代码搜索方法：基于交互矩阵

    Contrastive Prompt Learning-based Code Search based on Interaction Matrix. (arXiv:2310.06342v1 [cs.SE])

    [http://arxiv.org/abs/2310.06342](http://arxiv.org/abs/2310.06342)

    提出了基于对比提示学习的代码搜索方法CPLCS，通过编程语言-自然语言对比学习和跨模态交互机制，解决了现有代码搜索方法中的语义表示不足和语义鸿沟问题。

    

    代码搜索旨在检索与自然语言描述高度匹配的代码片段。最近，许多代码预训练方法在代码搜索方面表现出令人印象深刻的性能。然而，现有的代码搜索方法仍然受到两个性能限制的困扰：语义表示不足和自然语言与编程语言之间的语义鸿沟。在本文中，我们提出了基于对比提示学习的代码搜索方法CPLCS，该方法基于跨模态交互机制。CPLCS包括：（1）编程语言-自然语言对比学习，学习编程语言和自然语言表示之间的语义匹配关系；（2）双编码器结构的提示学习设计，可以缓解语义表示不足的问题；（3）跨模态交互机制，增强自然语言和编程语言之间的细粒度映射。我们进行了大量实验证明了我们方法的有效性。

    Code search aims to retrieve the code snippet that highly matches the given query described in natural language. Recently, many code pre-training approaches have demonstrated impressive performance on code search. However, existing code search methods still suffer from two performance constraints: inadequate semantic representation and the semantic gap between natural language (NL) and programming language (PL). In this paper, we propose CPLCS, a contrastive prompt learning-based code search method based on the cross-modal interaction mechanism. CPLCS comprises:(1) PL-NL contrastive learning, which learns the semantic matching relationship between PL and NL representations; (2) a prompt learning design for a dual-encoder structure that can alleviate the problem of inadequate semantic representation; (3) a cross-modal interaction mechanism to enhance the fine-grained mapping between NL and PL. We conduct extensive experiments to evaluate the effectiveness of our approach on a real-world
    
[^20]: I2SRM：用于多模态信息提取的样本内外关系建模

    I2SRM: Intra- and Inter-Sample Relationship Modeling for Multimodal Information Extraction. (arXiv:2310.06326v1 [cs.AI])

    [http://arxiv.org/abs/2310.06326](http://arxiv.org/abs/2310.06326)

    本文提出了I2SRM方法，它通过样本内关系建模和样本间关系建模来解决多模态信息提取问题。通过转换嵌入表示、捕捉交互作用和引入AttnMixup策略，该方法在多个数据集上取得了良好的性能。

    

    多模态信息提取如今受到研究重视，它需要聚合来自不同模态的表示。本文提出了用于该任务的样本内外关系建模（I2SRM）方法，其中包含两个模块。首先，样本内关系建模模块作用于单个样本，旨在学习有效的表示。文本和视觉模态的嵌入被转换，以弥合不同预训练语言和图像模型引起的模态差异。其次，样本间关系建模模块考虑了多个样本之间的关系，专注于捕捉交互作用。提出了一种AttnMixup策略，不仅能够使样本之间协作，还能增加数据以提高泛化性能。我们对多模态命名实体识别数据集Twitter-2015、Twitter-2017和多模态关系抽取数据集MNR进行了大量实验。

    Multimodal information extraction is attracting research attention nowadays, which requires aggregating representations from different modalities. In this paper, we present the Intra- and Inter-Sample Relationship Modeling (I2SRM) method for this task, which contains two modules. Firstly, the intra-sample relationship modeling module operates on a single sample and aims to learn effective representations. Embeddings from textual and visual modalities are shifted to bridge the modality gap caused by distinct pre-trained language and image models. Secondly, the inter-sample relationship modeling module considers relationships among multiple samples and focuses on capturing the interactions. An AttnMixup strategy is proposed, which not only enables collaboration among samples but also augments data to improve generalization. We conduct extensive experiments on the multimodal named entity recognition datasets Twitter-2015 and Twitter-2017, and the multimodal relation extraction dataset MNR
    
[^21]: 使用深度学习模型预测三种冻结步态事件

    Predicting Three Types of Freezing of Gait Events Using Deep Learning Models. (arXiv:2310.06322v1 [cs.LG])

    [http://arxiv.org/abs/2310.06322](http://arxiv.org/abs/2310.06322)

    该论文利用深度学习模型预测三种不同类型的冻结步态事件，最佳模型在测试数据上获得了0.427的得分，在Kaggle冻结步态预测竞赛中排名前五。

    

    冻结步态是帕金森病的症状之一，患者在行走时会周期性地出现不能迈步或转身的情况。虽然医学专家已经发现了多种引发和缓解冻结步态的方法，但其潜在原因和预测模型仍在研究中。目前利用机器学习的冻结步态预测模型可基于时间序列数据实现高敏感性和特异性，但这些模型缺乏对冻结步态事件类型的具体说明。我们开发了各种使用转换器编码器架构加上双向LSTM层和不同特征集的深度学习模型，以预测三种不同类型的冻结步态事件。最佳模型在测试数据上获得了0.427的得分，在由迈克尔·J·福克斯基金会主办的Kaggle冻结步态预测竞赛中排名前五。然而，我们也意识到了过拟合的问题

    Freezing of gait is a Parkinson's Disease symptom that episodically inflicts a patient with the inability to step or turn while walking. While medical experts have discovered various triggers and alleviating actions for freezing of gait, the underlying causes and prediction models are still being explored today. Current freezing of gait prediction models that utilize machine learning achieve high sensitivity and specificity in freezing of gait predictions based on time-series data; however, these models lack specifications on the type of freezing of gait events. We develop various deep learning models using the transformer encoder architecture plus Bidirectional LSTM layers and different feature sets to predict the three different types of freezing of gait events. The best performing model achieves a score of 0.427 on testing data, which would rank top 5 in Kaggle's Freezing of Gait prediction competition, hosted by THE MICHAEL J. FOX FOUNDATION. However, we also recognize overfitting 
    
[^22]: Dobby：一款由GPT-4驱动的会话服务机器人

    Dobby: A Conversational Service Robot Driven by GPT-4. (arXiv:2310.06303v1 [cs.RO])

    [http://arxiv.org/abs/2310.06303](http://arxiv.org/abs/2310.06303)

    Dobby是一款由GPT-4驱动的会话服务机器人，能够将会话型人工智能代理嵌入到具有自然语言理解和智能决策功能的具身系统中，实现服务任务的一体化，并且能够与物理世界进行交互。在演示中展示了其在导游场景中的应用，并通过HRI研究对其性能进行了评估。

    

    本研究介绍了一种机器人平台，将会话型人工智能代理嵌入到具有自然语言理解和智能决策功能的具身系统中，以实现服务任务的任务规划和类人对话的一体化。该代理基于一个大型语言模型，通过学习广泛的通用知识而得到。除了生成对话，该代理还可以通过在机器人上调用命令与物理世界进行交互，实现通信与行为的无缝融合。该系统在一个自由形式的导游场景中进行了演示，结合机器人以及不具备会话型人工智能能力的机器人进行了HRI研究。通过五个维度来衡量性能：整体有效性，探索能力，审查能力，对人格化的接受度和适应性。

    This work introduces a robotics platform which embeds a conversational AI agent in an embodied system for natural language understanding and intelligent decision-making for service tasks; integrating task planning and human-like conversation. The agent is derived from a large language model, which has learned from a vast corpus of general knowledge. In addition to generating dialogue, this agent can interface with the physical world by invoking commands on the robot; seamlessly merging communication and behavior. This system is demonstrated in a free-form tour-guide scenario, in an HRI study combining robots with and without conversational AI capabilities. Performance is measured along five dimensions: overall effectiveness, exploration abilities, scrutinization abilities, receptiveness to personification, and adaptability.
    
[^23]: 超叠加的玩具模型中的动力学与贝叶斯相变

    Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition. (arXiv:2310.06301v1 [cs.LG])

    [http://arxiv.org/abs/2310.06301](http://arxiv.org/abs/2310.06301)

    本研究通过奇异学习理论研究超叠加的玩具模型中的相变，在两个隐藏维度的情况下发现正则的$k$-gons是临界点，并提供支持理论表明这些临界点决定了贝叶斯后验的相变。此外，实验证明这些临界点也决定了SGD训练的行为。研究结果支持了SGD学习轨迹受顺序学习机制影响的猜想。

    

    我们使用奇异学习理论（SLT）研究超叠加的玩具模型（TMS）中的相变。我们推导出了理论损失的闭式公式，并在两个隐藏维度的情况下发现，正则的$k$-gons是临界点。我们提出了支持理论，表明这些$k$-gons的本地学习系数（几何不变量）决定了贝叶斯后验作为训练样本大小的函数的相变。然后，我们凭经验证明，同样的$k$-gon临界点也决定了SGD训练的行为。得出的结论支持了SGD学习轨迹受顺序学习机制影响的猜想。具体而言，我们发现TMS中的学习过程，无论是通过SGD还是贝叶斯学习，可以被描述为在参数空间中从高损失和低复杂性的区域向低损失和高复杂性的区域的旅程。

    We investigate phase transitions in a Toy Model of Superposition (TMS) using Singular Learning Theory (SLT). We derive a closed formula for the theoretical loss and, in the case of two hidden dimensions, discover that regular $k$-gons are critical points. We present supporting theory indicating that the local learning coefficient (a geometric invariant) of these $k$-gons determines phase transitions in the Bayesian posterior as a function of training sample size. We then show empirically that the same $k$-gon critical points also determine the behavior of SGD training. The picture that emerges adds evidence to the conjecture that the SGD learning trajectory is subject to a sequential learning mechanism. Specifically, we find that the learning process in TMS, be it through SGD or Bayesian learning, can be characterized by a journey through parameter space from regions of high loss and low complexity to regions of low loss and high complexity.
    
[^24]: 通过对抗行为抑制Q学习中的过高估计

    Suppressing Overestimation in Q-Learning through Adversarial Behaviors. (arXiv:2310.06286v1 [cs.LG])

    [http://arxiv.org/abs/2310.06286](http://arxiv.org/abs/2310.06286)

    本文提出了一种新的Q学习算法，通过引入虚拟对抗性玩家，有效调节了标准Q学习中的过高估计偏差，提出的算法简单而有效，能够轻松应用于强化学习算法并提高性能。

    

    本文旨在提出一种新的Q学习算法，使用一个虚拟对抗性玩家，称为虚拟对抗性Q学习（DAQ），以有效地调节标准Q学习中的过高估计偏差。通过虚拟玩家，学习可以被表述为一个双人零和博弈。所提出的DAQ将几种Q学习的变体统一到一个单一的框架中，以控制过高估计偏差，例如maxmin Q学习和minmax Q学习（本文提出）。通过虚拟对抗性行为，所提出的DAQ是一种简单而有效的方式，可以轻松应用于现成的强化学习算法，以提高性能。通过调整对抗性Q学习，从综合的角度分析了DAQ的有限时间收敛性。在各种基准环境下，实证验证了所提出DAQ的性能。

    The goal of this paper is to propose a new Q-learning algorithm with a dummy adversarial player, which is called dummy adversarial Q-learning (DAQ), that can effectively regulate the overestimation bias in standard Q-learning. With the dummy player, the learning can be formulated as a two-player zero-sum game. The proposed DAQ unifies several Q-learning variations to control overestimation biases, such as maxmin Q-learning and minmax Q-learning (proposed in this paper) in a single framework. The proposed DAQ is a simple but effective way to suppress the overestimation bias thourgh dummy adversarial behaviors and can be easily applied to off-the-shelf reinforcement learning algorithms to improve the performances. A finite-time convergence of DAQ is analyzed from an integrated perspective by adapting an adversarial Q-learning. The performance of the suggested DAQ is empirically demonstrated under various benchmark environments.
    
[^25]: BC4LLM：当区块链遇见大型语言模型时的可信人工智能

    BC4LLM: Trusted Artificial Intelligence When Blockchain Meets Large Language Models. (arXiv:2310.06278v1 [cs.NI])

    [http://arxiv.org/abs/2310.06278](http://arxiv.org/abs/2310.06278)

    本论文提出了使用区块链技术增强大型语言模型（LLM）安全性的愿景，以解决AI生成内容的真实性和隐私泄露问题。

    

    近年来，人工智能（AI）和机器学习（ML）正在重新塑造社会的生产方式和生产力，并改变科学研究的范 paradigm。其中，以ChatGPT为代表的AI语言模型取得了巨大进展。这种大型语言模型（LLM）以AI生成的内容（AIGC）的形式服务于人们，并广泛应用于咨询、医疗和教育。然而，很难保证AIGC学习数据的真实性和可靠性。此外，分布式AI训练中也存在隐患的隐私泄露问题。此外，LLMs生成的内容很难识别和追踪，难以跨平台相互认可。在以LLMs为动力的AI时代即将到来之际，上述信息安全问题将被无限放大，影响每个人的生活。因此，我们考虑利用区块链技术为LLMs赋予卓越的安全特性，提出了一个愿景。

    In recent years, artificial intelligence (AI) and machine learning (ML) are reshaping society's production methods and productivity, and also changing the paradigm of scientific research. Among them, the AI language model represented by ChatGPT has made great progress. Such large language models (LLMs) serve people in the form of AI-generated content (AIGC) and are widely used in consulting, healthcare, and education. However, it is difficult to guarantee the authenticity and reliability of AIGC learning data. In addition, there are also hidden dangers of privacy disclosure in distributed AI training. Moreover, the content generated by LLMs is difficult to identify and trace, and it is difficult to cross-platform mutual recognition. The above information security issues in the coming era of AI powered by LLMs will be infinitely amplified and affect everyone's life. Therefore, we consider empowering LLMs using blockchain technology with superior security features to propose a vision for
    
[^26]: 让模型说密文: 通过嵌入进行多智能体辩论

    Let Models Speak Ciphers: Multiagent Debate through Embeddings. (arXiv:2310.06272v1 [cs.CL])

    [http://arxiv.org/abs/2310.06272](http://arxiv.org/abs/2310.06272)

    本文引入了一种名为CIPHER的通信机制，通过去除LLMs中的标记采样步骤，让模型可以通过期望的原始Transformer输出嵌入来传达其信念，从而解决了在自然语言生成中可能存在的信息丢失风险，并提供了编码更广泛信息的优势。

    

    最近，对大型语言模型（LLMs）之间的讨论和辩论引起了广泛关注，因为它们有潜力增强LLMs的推理能力。尽管自然语言由于LLMs的语言理解能力而成为明显的交流选择，但生成自然语言时需要进行的标记采样步骤可能存在信息丢失的潜在风险，因为它仅使用一个标记来代表模型在整个词汇表中的信念。在本文中，我们介绍了一种名为CIPHER（通过嵌入表示进行交流的网络模型协议）的通信机制来解决这个问题。具体来说，我们从LLMs中去除了标记采样步骤，让它们通过原始Transformer输出嵌入的期望来传达它们的信念。值得注意的是，通过偏离自然语言，CIPHER在不对模型权重进行任何修改的情况下，提供了编码更广泛信息的优势。

    Discussion and debate among Large Language Models (LLMs) have gained considerable attention due to their potential to enhance the reasoning ability of LLMs. Although natural language is an obvious choice for communication due to LLM's language understanding capability, the token sampling step needed when generating natural language poses a potential risk of information loss, as it uses only one token to represent the model's belief across the entire vocabulary. In this paper, we introduce a communication regime named CIPHER (Communicative Inter-Model Protocol Through Embedding Representation) to address this issue. Specifically, we remove the token sampling step from LLMs and let them communicate their beliefs across the vocabulary through the expectation of the raw transformer output embeddings. Remarkably, by deviating from natural language, CIPHER offers an advantage of encoding a broader spectrum of information without any modification to the model weights. While the state-of-the-a
    
[^27]: 通过自我反思以减轻大型语言模型中的幻觉

    Towards Mitigating Hallucination in Large Language Models via Self-Reflection. (arXiv:2310.06271v1 [cs.CL])

    [http://arxiv.org/abs/2310.06271](http://arxiv.org/abs/2310.06271)

    本文通过分析医学生成性问答系统中的幻觉现象，提出了一种交互式的自我反思方法，通过增强事实性、一致性和蕴涵性，解决了大型语言模型中的幻觉问题。

    

    大型语言模型（LLMs）在生成和知识密集型任务（包括问答任务）方面显示出了潜力。然而，实际部署仍面临挑战，特别是“幻觉”问题：模型生成似乎合理但不真实或荒谬的信息。在医学领域，这个问题尤为关键，因为涉及到不常见的专业概念和潜在的社会风险。本文使用广泛采用的LLMs和数据集，分析了医学生成性问答系统中的幻觉现象。我们的研究重点是识别和理解常见的问题答案，特别是幻觉。为了解决这个挑战，我们提出了一种交互式的自我反思方法，结合了知识获取和答案生成。通过这个反馈过程，我们的方法逐步增强了生成答案的事实性、一致性和蕴涵性。

    Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of "hallucination", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Cons
    
[^28]: AI事故数据库作为提高人们对人工智能伤害意识的教育工具：对有效性、限制性和未来改进的课堂探索

    The AI Incident Database as an Educational Tool to Raise Awareness of AI Harms: A Classroom Exploration of Efficacy, Limitations, & Future Improvements. (arXiv:2310.06269v1 [cs.CY])

    [http://arxiv.org/abs/2310.06269](http://arxiv.org/abs/2310.06269)

    本研究评估了AI事故数据库作为提高人们对人工智能伤害意识的教育工具的有效性。该数据库索引了以往的AI伤害实例，并通过课堂研究揭示了学生对AI伦理主题的初步看法和知识缺口的存在。

    

    先前的研究已经确定了将AI伦理主题纳入计算机与数据科学课程的重要性。我们提供了证据表明，AI伦理教育的关键目标之一必须是提高对AI伤害的意识。虽然有各种来源可以了解此类伤害，但AI事故数据库（AIID）是为数不多的试图提供一个相对全面的数据库，索引以前在真实世界中部署AI技术导致的伤害或潜在伤害的实例之一。本研究评估了AIID作为一种教育工具，在有社会高风险领域中提高对AI伤害的普遍性和严重性的意识的有效性。我们通过在一所R1机构的课程中进行的一项课堂研究来呈现所得到的研究结果，该课程专注于AI和ML的社会和伦理考虑。我们的定性研究结果描绘了学生对AI伦理核心主题的初步看法以及他们希望填补知识缺口的愿望。

    Prior work has established the importance of integrating AI ethics topics into computer and data sciences curricula. We provide evidence suggesting that one of the critical objectives of AI Ethics education must be to raise awareness of AI harms. While there are various sources to learn about such harms, The AI Incident Database (AIID) is one of the few attempts at offering a relatively comprehensive database indexing prior instances of harms or near harms stemming from the deployment of AI technologies in the real world. This study assesses the effectiveness of AIID as an educational tool to raise awareness regarding the prevalence and severity of AI harms in socially high-stakes domains. We present findings obtained through a classroom study conducted at an R1 institution as part of a course focused on the societal and ethical considerations around AI and ML. Our qualitative findings characterize students' initial perceptions of core topics in AI ethics and their desire to close the 
    
[^29]: CodeFuse-13B: 一个预训练的多语言代码大型语言模型

    CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model. (arXiv:2310.06266v1 [cs.SE])

    [http://arxiv.org/abs/2310.06266](http://arxiv.org/abs/2310.06266)

    CodeFuse-13B是一个预训练的多语言代码大型语言模型，专为代码相关任务设计，支持超过40种编程语言，并通过使用高质量的预训练数据集以及大量实验的验证，展现了其在多语言输入下的有效性。

    

    代码大型语言模型(Code LLMs)因其在软件工程全生命周期中的广泛应用而受到工业界的广泛关注。然而，现有模型在理解非英语输入的多语言代码相关任务方面的效果仍然远未被充分研究。本文介绍了CodeFuse-13B，一个开源的预训练代码LLM。它专为包含英文和中文提示的代码相关任务而设计，并支持超过40种编程语言。CodeFuse通过利用由程序分析器精心筛选并在训练过程中优化的高质量预训练数据集来实现其效果。我们进行了大量实验，包括使用真实世界的使用场景、工业标准基准HumanEval-x，以及专为中文提示设计的CodeFuseEval。为了评估CodeFuse的有效性，我们积极收集了AntGroup软件开发团队的宝贵人工反馈。

    Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software develop
    
[^30]: 自我区分建模用于异常图检测

    Self-Discriminative Modeling for Anomalous Graph Detection. (arXiv:2310.06261v1 [cs.LG])

    [http://arxiv.org/abs/2310.06261](http://arxiv.org/abs/2310.06261)

    本文提出了一种自我区分建模框架用于基于正常图训练的异常图检测，通过生成插值的伪异常图，与几种最先进的基线算法相比，取得了显著的改进。

    

    本文研究了使用仅基于正常图训练的机器学习模型来检测异常图的问题，这在分子、生物和社交网络数据分析中有许多应用。我们提出了一种自我区分建模框架用于异常图检测。其核心思想是通过学习一个从给定的正常图和通过联合训练生成的伪异常图中的判别器（分类器），从而使得生成的伪异常图在正常图和真实异常图之间插值。在该框架下，我们提供了三种具有不同计算效率和稳定性的算法用于异常图检测。这三种算法与几种最先进的基于图级别的异常检测基线在九个流行的图数据集上进行比较（其中四个数据集规模较小，五个数据集规模适中），并展示了显著的改进。

    This paper studies the problem of detecting anomalous graphs using a machine learning model trained on only normal graphs, which has many applications in molecule, biology, and social network data analysis. We present a self-discriminative modeling framework for anomalous graph detection. The key idea, mathematically and numerically illustrated, is to learn a discriminator (classifier) from the given normal graphs together with pseudo-anomalous graphs generated by a model jointly trained, where we never use any true anomalous graphs and we hope that the generated pseudo-anomalous graphs interpolate between normal ones and (real) anomalous ones. Under the framework, we provide three algorithms with different computational efficiencies and stabilities for anomalous graph detection. The three algorithms are compared with several state-of-the-art graph-level anomaly detection baselines on nine popular graph datasets (four with small size and five with moderate size) and show significant im
    
[^31]: 看梗？使用大型语言模型进行少样本去背景化

    Get the gist? Using large language models for few-shot decontextualization. (arXiv:2310.06254v1 [cs.CL])

    [http://arxiv.org/abs/2310.06254](http://arxiv.org/abs/2310.06254)

    本文提出了一种使用大型语言模型的少样本去背景化方法，该方法能够在多个领域上仅使用少量样本即可达到可行的性能。

    

    在涉及解释富有上下文的句子的许多自然语言处理应用中，例如信息检索系统或对话系统，很有必要能够将句子保留在一个可以在没有上下文的情况下轻松理解的形式中，以便以后重新使用，这个过程被称为“去背景化”。虽然以前的工作证明了经过细调的生成型Seq2Seq模型可以有效地进行特定数据集上的去背景化，但这种方法需要昂贵的人工注释，而且可能无法迁移到其他领域。我们提出了一种使用大型语言模型的少样本去背景化方法，并展示了初步结果，表明该方法在多个领域上仅使用少量样本即可达到可行的性能。

    In many NLP applications that involve interpreting sentences within a rich context -- for instance, information retrieval systems or dialogue systems -it is desirable to be able to preserve the sentence in a form that can be readily understood without context, for later reuse -- a process known as ``decontextualization''. While previous work demonstrated that generative Seq2Seq models could effectively perform decontextualization after being fine-tuned on a specific dataset, this approach requires expensive human annotations and may not transfer to other domains. We propose a few-shot method of decontextualization using a large language model, and present preliminary results showing that this method achieves viable performance on multiple domains using only a small set of examples.
    
[^32]: 我们是我们反复做的事情：在基于角色的回应中引导和应用习惯性模式

    We are what we repeatedly do: Inducing and deploying habitual schemas in persona-based responses. (arXiv:2310.06245v1 [cs.AI])

    [http://arxiv.org/abs/2310.06245](http://arxiv.org/abs/2310.06245)

    该论文研究了在对话系统中引导和应用习惯性模式来生成基于角色的回应。与以往的方法不同，该论文通过明确的模式表示来捕捉与角色相关的习惯性知识，从而提高了回应的准确性和自然性。

    

    对话技术的许多实际应用需要根据特定的开发者规定的角色生成回应。虽然可以从最近的大型语言模型中引出许多角色，但这些模型的不透明性和不可预测性使得以明确形式指定角色成为可取的。在以前的工作中，角色通常被表示为一次性的自我知识片段的集合，对话系统会检索这些知识片段来生成回应。然而，在现实的人类对话中，角色通常通过类似故事的叙述来揭示出来，这些叙述涉及丰富的习惯性知识——关于代理人经常参与的事件类型的知识（例如，工作活动、爱好、体育活动、喜欢的娱乐等），包括这些事件的典型目标、子事件、前提条件和后置条件。我们使用明确的模式表示来捕捉这种习惯性知识，并提出了一种引导对话系统使用这些模式生成回应的方法。

    Many practical applications of dialogue technology require the generation of responses according to a particular developer-specified persona. While a variety of personas can be elicited from recent large language models, the opaqueness and unpredictability of these models make it desirable to be able to specify personas in an explicit form. In previous work, personas have typically been represented as sets of one-off pieces of self-knowledge that are retrieved by the dialogue system for use in generation. However, in realistic human conversations, personas are often revealed through story-like narratives that involve rich habitual knowledge -- knowledge about kinds of events that an agent often participates in (e.g., work activities, hobbies, sporting activities, favorite entertainments, etc.), including typical goals, sub-events, preconditions, and postconditions of those events. We capture such habitual knowledge using an explicit schema representation, and propose an approach to dia
    
[^33]: 模型调优还是提示调优？对于临床概念和关系提取的大型语言模型的研究

    Model Tuning or Prompt Tuning? A Study of Large Language Models for Clinical Concept and Relation Extraction. (arXiv:2310.06239v1 [cs.CL])

    [http://arxiv.org/abs/2310.06239](http://arxiv.org/abs/2310.06239)

    本研究探索了大型语言模型在临床概念和关系提取任务中的应用，通过软提示调优取得了最佳性能，并观察了迁移学习和少样本学习的能力。

    

    本研究旨在开发基于软提示的大型语言模型（LLMs）学习算法，研究提示的形状、使用冻结/解冻LLMs进行提示调优、迁移学习和少样本学习能力。通过开发了基于软提示的LLM模型，并通过比较四种训练策略（1.无提示微调；2.解冻LLMs的硬提示；3.解冻LLMs的软提示；4.冻结LLMs的软提示）来评估了七个预训练的LLMs在两个基准数据集上的临床概念和关系提取能力。在跨机构环境中评估了基于提示学习算法的迁移学习能力，并评估了少样本学习能力。

    Objective To develop soft prompt-based learning algorithms for large language models (LLMs), examine the shape of prompts, prompt-tuning using frozen/unfrozen LLMs, transfer learning, and few-shot learning abilities. Methods We developed a soft prompt-based LLM model and compared 4 training strategies including (1) fine-tuning without prompts; (2) hard-prompt with unfrozen LLMs; (3) soft-prompt with unfrozen LLMs; and (4) soft-prompt with frozen LLMs. We evaluated 7 pretrained LLMs using the 4 training strategies for clinical concept and relation extraction on two benchmark datasets. We evaluated the transfer learning ability of the prompt-based learning algorithms in a cross-institution setting. We also assessed the few-shot learning ability. Results and Conclusion When LLMs are unfrozen, GatorTron-3.9B with soft prompting achieves the best strict F1-scores of 0.9118 and 0.8604 for concept extraction, outperforming the traditional fine-tuning and hard prompt-based models by 0.6~3.1% a
    
[^34]: 解决MUSIC-AVQA中的数据偏差问题：为无偏问答创建一个平衡的数据集

    Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for Unbiased Question-Answering. (arXiv:2310.06238v1 [cs.CV])

    [http://arxiv.org/abs/2310.06238](http://arxiv.org/abs/2310.06238)

    本论文解决了MUSIC-AVQA中的数据偏差问题，通过创建一个平衡的数据集来保证模型能够有效地推理各种多模态情况下的问题。他们构建了一个名为MUSIC-AVQA v2.0的新数据集，并提出了一个新的基准模型。

    

    近年来，对音频、视觉和文本模态的交叉研究越来越受重视，推动了多模态研究的进展。然而，任何模态中存在的强烈偏见会导致模型忽视其他模态。因此，模型有效地跨越这些多样化模态进行推理的能力受到损害，阻碍了进一步的发展。在本文中，我们详细审查了原始数据集中的每种问题类型，选择具有明显答案偏见的问题。为了解决这些偏见，我们收集了互补的视频和问题，确保没有答案有明显的偏斜分布。特别是对于二元问题，我们努力确保每个问题类别中两个答案几乎均匀分布。因此，我们构建了一个名为MUSIC-AVQA v2.0的新数据集，它更具挑战性，我们相信能够更好地促进AVQA任务的进展。此外，我们提出了一种新的基准模型。

    In recent years, there has been a growing emphasis on the intersection of audio, vision, and text modalities, driving forward the advancements in multimodal research. However, strong bias that exists in any modality can lead to the model neglecting the others. Consequently, the model's ability to effectively reason across these diverse modalities is compromised, impeding further advancement. In this paper, we meticulously review each question type from the original dataset, selecting those with pronounced answer biases. To counter these biases, we gather complementary videos and questions, ensuring that no answers have outstanding skewed distribution. In particular, for binary questions, we strive to ensure that both answers are almost uniformly spread within each question category. As a result, we construct a new dataset, named MUSIC-AVQA v2.0, which is more challenging and we believe could better foster the progress of AVQA task. Furthermore, we present a novel baseline model that de
    
[^35]: 自然语言处理技术的演进：不仅仅是语言处理，而是朝向通用人工智能

    Evolution of Natural Language Processing Technology: Not Just Language Processing Towards General Purpose AI. (arXiv:2310.06228v1 [cs.CL])

    [http://arxiv.org/abs/2310.06228](http://arxiv.org/abs/2310.06228)

    这篇论文翻译成中文并总结指出，自然语言处理技术逐渐演进为通用人工智能，尽管自然语言非常难以进行数学上的表达，但通过深度学习在NLP领域取得了超乎预期的结果。

    

    自发明计算机以来，通过自然语言（实际的人类语言）进行通信一直是一项梦幻的技术。然而，自然语言非常难以进行数学上的表达，这使得在不考虑编程的情况下实现算法变得困难。尽管已经有了许多技术发展，但到目前为止，还不能说已经实现了任何允许自由利用的结果。举例来说，在人类的语言学习中，例如学习母语或外语时，我们必须承认这个过程与格言“熟能生巧”在原则上是相似的，尽管学习方法在某种程度上是重要的。深度学习近年来在当代人工智能技术中扮演了核心角色。当应用于自然语言处理（NLP）时，这产生了前所未有的结果。从学习大量文本数据的结果中，已经有报道称超出了最初的预测。

    Since the invention of computers, communication through natural language (actual human language) has been a dream technology. However, natural language is extremely difficult to mathematically formulate, making it difficult to realize as an algorithm without considering programming. While there have been numerous technological developments, one cannot say that any results allowing free utilization have been achieved thus far. In the case of language learning in humans, for instance when learning one's mother tongue or foreign language, one must admit that this process is similar to the adage "practice makes perfect" in principle, even though the learning method is significant up to a point. Deep learning has played a central role in contemporary AI technology in recent years. When applied to natural language processing (NLP), this produced unprecedented results. Achievements exceeding the initial predictions have been reported from the results of learning vast amounts of textual data u
    
[^36]: GPT-4作为农学助手？使用大型语言模型回答农业考试

    GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models. (arXiv:2310.06225v1 [cs.AI])

    [http://arxiv.org/abs/2310.06225](http://arxiv.org/abs/2310.06225)

    本研究全面评估了大型语言模型（LLMs）在回答农业相关问题方面的能力，并使用RAG和ER技术提高了LLMs的性能。其中，GPT-4在农业考试中取得了及格分数以获得认证。

    

    大型语言模型（LLM）在各个领域，包括医疗保健和金融领域，展示了卓越的自然语言理解能力。在某些任务上，LLM的性能与训练有素的人类相似甚至更好，因此合理地使用人类考试（例如认证考试）来评估LLM的性能。我们对流行的LLM（如Llama 2和GPT）在回答农业相关问题的能力进行了全面评估。在评估过程中，我们还运用了RAG（检索增强生成）和ER（集合细化）技术，结合信息检索、生成能力和提示策略，提高LLM的性能。为了展示LLM的能力，我们选择了来自巴西、印度和美国三个最大的农业生产国的农业考试和基准数据集。我们的分析突出了GPT-4在考试中取得及格分数以获得认证的能力。

    Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding across various domains, including healthcare and finance. For some tasks, LLMs achieve similar or better performance than trained human beings, therefore it is reasonable to employ human exams (e.g., certification tests) to assess the performance of LLMs. We present a comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their ability to answer agriculture-related questions. In our evaluation, we also employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement) techniques, which combine information retrieval, generation capabilities, and prompting strategies to improve the LLMs' performance. To demonstrate the capabilities of LLMs, we selected agriculture exams and benchmark datasets from three of the largest agriculture producer countries: Brazil, India, and the USA. Our analysis highlights GPT-4's ability to achieve a passing score on exams to earn cred
    
[^37]: SUBP：软均匀块剪枝用于1xN稀疏CNN的多线程加速

    SUBP: Soft Uniform Block Pruning for 1xN Sparse CNNs Multithreading Acceleration. (arXiv:2310.06218v1 [cs.LG])

    [http://arxiv.org/abs/2310.06218](http://arxiv.org/abs/2310.06218)

    该论文提出了一种新的软均匀块剪枝（SUBP）方法，在1xN稀疏CNN中实现了多线程加速，并解决了传统方法中的训练成本昂贵、内存访问开销大、模型质量次优以及线程负载不平衡等问题。

    

    卷积神经网络（CNN）中的稀疏性研究已经广泛应用于在资源有限的环境中压缩和加速模型。通过约束输出通道上的N个连续权重为组内非零，最近的1xN稀疏网络因其三个突出优势而受到广泛关注：1）通过一种“块稀疏行”矩阵大量节省存储空间。2）在高稀疏性下表现出色。3）在具有高级矢量扩展的CPU上显著加速。最近的研究需要基于稠密预训练权重选择和微调1xN稀疏权重，导致训练成本昂贵、内存访问开销大、模型质量次优以及不平衡的线程负载（输出通道上的不同稀疏性）等问题。为了解决这些问题，本文提出了一种新颖的“软均匀块剪枝”（SUBP）方法来训练一个u

    The study of sparsity in Convolutional Neural Networks (CNNs) has become widespread to compress and accelerate models in environments with limited resources. By constraining N consecutive weights along the output channel to be group-wise non-zero, the recent network with 1$\times$N sparsity has received tremendous popularity for its three outstanding advantages: 1) A large amount of storage space saving by a \emph{Block Sparse Row} matrix. 2) Excellent performance at a high sparsity. 3) Significant speedups on CPUs with Advanced Vector Extensions. Recent work requires selecting and fine-tuning 1$\times$N sparse weights based on dense pre-trained weights, leading to the problems such as expensive training cost and memory access, sub-optimal model quality, as well as unbalanced workload across threads (different sparsity across output channels). To overcome them, this paper proposes a novel \emph{\textbf{S}oft \textbf{U}niform \textbf{B}lock \textbf{P}runing} (SUBP) approach to train a u
    
[^38]: 无回归估计数字

    Estimating Numbers without Regression. (arXiv:2310.06204v1 [cs.CL])

    [http://arxiv.org/abs/2310.06204](http://arxiv.org/abs/2310.06204)

    提出了一种不使用回归的方法来估计数字，通过改变模型的词汇表来更好地表示数字的大小，并取得了令人满意的结果。

    

    尽管语言模型在最近取得了成功，但它们表示数字的能力仍然不足。人类根据数字的大小进行概念化，有效地将它们投射到数字线上；而子词标记化无法明确捕捉数字的大小，将数字分割成任意的块。为了克服这个缺点，已经提出了替代方法，在语言建模管线的各个阶段修改数字。这些方法要么改变数字的表示方式（例如科学计数法与十进制），要么改变用于表示数字的词汇表，要么直接改变基础语言模型的架构，以直接回归到所需的数字。以前的工作表明，架构的更改有助于在数字估计上达到最先进的水平，但我们发现一个有洞察力的缺憾：改变模型的词汇表（例如在10-100范围内引入一个新的数字标记）是一个更好的权衡。

    Despite recent successes in language models, their ability to represent numbers is insufficient. Humans conceptualize numbers based on their magnitudes, effectively projecting them on a number line; whereas subword tokenization fails to explicitly capture magnitude by splitting numbers into arbitrary chunks. To alleviate this shortcoming, alternative approaches have been proposed that modify numbers at various stages of the language modeling pipeline. These methods change either the (1) notation in which numbers are written (\eg scientific vs decimal), the (2) vocabulary used to represent numbers or the entire (3) architecture of the underlying language model, to directly regress to a desired number.  Previous work suggests that architectural change helps achieve state-of-the-art on number estimation but we find an insightful ablation: changing the model's vocabulary instead (\eg introduce a new token for numbers in range 10-100) is a far better trade-off. In the context of masked numb
    
[^39]: 借助msGeMM增加AI GeMM的性能近2.5倍的Look-Up mAI GeMM

    Look-Up mAI GeMM: Increasing AI GeMMs Performance by Nearly 2.5x via msGeMM. (arXiv:2310.06178v1 [cs.PF])

    [http://arxiv.org/abs/2310.06178](http://arxiv.org/abs/2310.06178)

    这篇论文提出了一种名为msGeMM的新算法，通过使用低精度数据类型，可以使AI模型的性能提高近2.5倍。该算法需要特殊的CUDA核心来实现从小型查找表中添加元素的能力。

    

    AI模型的规模不断增加，最近的研究表明，在HPC应用中需要双精度数据类型，而fp8或int4等更低精度的数据类型已经足够用于训练和推断中，而且质量相当。在此趋势下，像NVIDIA和AMD这样的GPU供应商通过张量核心提供了对fp16、fp8和int8 GeMM操作的硬件支持，具有优秀的性能。然而，本文提出了一种名为msGeMM的新算法，该算法证明了使用低精度数据类型的AI模型可以减少约2.5倍的乘法和加法指令。实现此算法的高效率需要具备与张量核心相同速率从小型查找表中添加元素的特殊CUDA核心。

    AI models are increasing in size and recent advancement in the community has shown that unlike HPC applications where double precision datatype are required, lower-precision datatypes such as fp8 or int4 are sufficient to bring the same model quality both for training and inference. Following these trends, GPU vendors such as NVIDIA and AMD have added hardware support for fp16, fp8 and int8 GeMM operations with an exceptional performance via Tensor Cores. However, this paper proposes a new algorithm called msGeMM which shows that AI models with low-precision datatypes can run with ~2.5x fewer multiplication and add instructions. Efficient implementation of this algorithm requires special CUDA cores with the ability to add elements from a small look-up table at the rate of Tensor Cores.
    
[^40]: 使用语言模型和强化学习的事实和个性化推荐

    Factual and Personalized Recommendations using Language Models and Reinforcement Learning. (arXiv:2310.06176v1 [cs.AI])

    [http://arxiv.org/abs/2310.06176](http://arxiv.org/abs/2310.06176)

    本研究提出了一种使用语言模型和强化学习的事实和个性化推荐系统，该系统使用嵌入空间表示根据用户偏好生成有趣和相关的推荐，并开发了联合奖励函数来衡量准确性、吸引力和个性化。实验证明该系统能够为用户提供个性化的电影叙事。

    

    推荐系统在连接用户和内容、产品和服务方面起着重要作用，根据用户的偏好将候选项与用户进行匹配。本文提出了一种ComPelling、Precise、Personalized、Preference-relevant（P4LM）语言模型，它强调解释物品特征及其相关性，为用户推荐物品。P4LM利用用户偏好的嵌入空间表示生成有趣的、事实基础的、与用户偏好相关的响应。此外，我们开发了一个联合奖励函数，衡量准确性、吸引力和个性化，将其作为强化学习-based 语言模型框架中的人工智能反馈。使用MovieLens 25M数据集，我们证明P4LM能够为用户提供引人入胜的个性化电影叙事。

    Recommender systems (RSs) play a central role in connecting users to content, products, and services, matching candidate items to users based on their preferences. While traditional RSs rely on implicit user feedback signals, conversational RSs interact with users in natural language. In this work, we develop a comPelling, Precise, Personalized, Preference-relevant language model (P4LM) that recommends items to users while putting emphasis on explaining item characteristics and their relevance. P4LM uses the embedding space representation of a user's preferences to generate compelling responses that are factually-grounded and relevant w.r.t. the user's preferences. Moreover, we develop a joint reward function that measures precision, appeal, and personalization, which we use as AI-based feedback in a reinforcement learning-based language model framework. Using the MovieLens 25M dataset, we demonstrate that P4LM delivers compelling, personalized movie narratives to users.
    
[^41]: 提示工程对ChatGPT在无监督实体解析中的性能影响是如何的？

    How does prompt engineering affect ChatGPT performance on unsupervised entity resolution?. (arXiv:2310.06174v1 [cs.AI])

    [http://arxiv.org/abs/2310.06174](http://arxiv.org/abs/2310.06174)

    本研究对提示工程对ChatGPT在无监督实体解析中的影响进行了初步实验研究，结果显示提示可以显著影响实体解析的质量。

    

    实体解析（ER）是一种半自动确定两个实体是否指向相同基础实体的问题，应用范围从医疗保健到电子商务。传统的ER解决方案需要相当多的手动专业知识，包括特征工程以及训练数据的识别和策划。在许多情况下，这些技术高度依赖于领域。随着大型语言模型（LLMs）的最新发展，有机会使ER更加无缝和领域无关。然而，众所周知，LLMs可能存在风险，其输出质量可能取决于所谓的提示工程。不幸的是，迄今为止，关于使用像ChatGPT这样的LLMs解决ER的不同提示方法的影响的系统实验研究还缺乏。本文旨在填补这一空白，通过进行这样一项研究。尽管这只是初步性质的研究，我们的结果表明，提示可以显著影响实体解析的质量。

    Entity Resolution (ER) is the problem of semi-automatically determining when two entities refer to the same underlying entity, with applications ranging from healthcare to e-commerce. Traditional ER solutions required considerable manual expertise, including feature engineering, as well as identification and curation of training data. In many instances, such techniques are highly dependent on the domain. With recent advent in large language models (LLMs), there is an opportunity to make ER much more seamless and domain-independent. However, it is also well known that LLMs can pose risks, and that the quality of their outputs can depend on so-called prompt engineering. Unfortunately, a systematic experimental study on the effects of different prompting methods for addressing ER, using LLMs like ChatGPT, has been lacking thus far. This paper aims to address this gap by conducting such a study. Although preliminary in nature, our results show that prompting can significantly affect the qu
    
[^42]: 内存一致的神经网络在模仿学习中的应用

    Memory-Consistent Neural Networks for Imitation Learning. (arXiv:2310.06171v1 [cs.LG])

    [http://arxiv.org/abs/2310.06171](http://arxiv.org/abs/2310.06171)

    本文介绍了一种内存一致的神经网络模型，在模仿学习中使用专家演示训练策略。该模型通过对输出结果进行硬约束，避免了错误的累积现象，保证了策略效果的上界。

    

    模仿学习利用专家演示大大简化了策略合成的过程。然而，对于这种模仿策略来说，远离训练样本的错误尤为关键。即使在策略的行动输出中出现罕见的错误，由于这些错误会导致不熟悉的未来状态，策略在这些状态下仍更容易出错，最终导致任务失败。本文重新审视了简单的监督式“行为克隆”方法，能够方便地仅通过预先记录的演示来训练策略，并设计了一种能够抵消错误累积现象的模型类。我们的“内存一致神经网络”(MCNN)输出被强制约束在与典型的“内存”训练样本相关的明确指定的允许区域内。我们提供了MCNN策略导致的次优性差距的保证上界。通过在9个模仿学习任务上使用MCNNs，采用MLP、Transformer等方法。

    Imitation learning considerably simplifies policy synthesis compared to alternative approaches by exploiting access to expert demonstrations. For such imitation policies, errors away from the training samples are particularly critical. Even rare slip-ups in the policy action outputs can compound quickly over time, since they lead to unfamiliar future states where the policy is still more likely to err, eventually causing task failures. We revisit simple supervised ``behavior cloning'' for conveniently training the policy from nothing more than pre-recorded demonstrations, but carefully design the model class to counter the compounding error phenomenon. Our ``memory-consistent neural network'' (MCNN) outputs are hard-constrained to stay within clearly specified permissible regions anchored to prototypical ``memory'' training samples. We provide a guaranteed upper bound for the sub-optimality gap induced by MCNN policies. Using MCNNs on 9 imitation learning tasks, with MLP, Transformer, 
    
[^43]: 可预测人工智能

    Predictable Artificial Intelligence. (arXiv:2310.06167v1 [cs.AI])

    [http://arxiv.org/abs/2310.06167](http://arxiv.org/abs/2310.06167)

    可预测人工智能是一个新兴研究领域，旨在预测人工智能生态系统的关键指标，并强调可预测性对于提高信任、责任、控制、协调和安全的重要性。

    

    我们介绍了可预测人工智能的基本思想和挑战，这是一个探索如何预测现有和未来人工智能生态系统关键指标的新兴研究领域。我们认为，实现可预测性对于促进人工智能生态系统的信任、责任、控制、协调和安全至关重要，因此应优先考虑而非性能。尽管与其他技术和非技术的人工智能研究领域有所不同，但与可预测人工智能相关的问题、假设和挑战尚未被清楚描述。本文旨在阐明这些问题，呼吁找到通向人工智能可预测性的路径，并概述这一新兴领域的潜在影响。

    We introduce the fundamental ideas and challenges of Predictable AI, a nascent research area that explores the ways in which we can anticipate key indicators of present and future AI ecosystems. We argue that achieving predictability is crucial for fostering trust, liability, control, alignment and safety of AI ecosystems, and thus should be prioritised over performance. While distinctive from other areas of technical and non-technical AI research, the questions, hypotheses and challenges relevant to Predictable AI were yet to be clearly described. This paper aims to elucidate them, calls for identifying paths towards AI predictability and outlines the potential impact of this emergent field.
    
[^44]: CAW-coref: 关联词感知的词级共指消解

    CAW-coref: Conjunction-Aware Word-level Coreference Resolution. (arXiv:2310.06165v1 [cs.CL])

    [http://arxiv.org/abs/2310.06165](http://arxiv.org/abs/2310.06165)

    本文介绍了一种关联词感知的词级共指消解模型（CAW-coref），在处理并列提及的情况下表现出了较高的性能，有效地缩小了与昂贵的最先进方法的差距。

    

    当前最先进的共指消解系统每篇文章需要多次调用语言模型，因此对于许多应用场景来说（例如使用大规模语料库进行信息提取），代价太高。而词级共指系统 (WL-coref) 在效率上更加高效，实现了这些最先进系统 96.6% 的性能。本文发现了 WL-coref 的一个常见但重要的失败案例：处理“Tom 和 Mary”之类的并列提及。我们提供了一个简单但有效的解决方案，在 OntoNotes 测试集上将性能提高了 0.9% F1，将高效的词级共指消解与昂贵的最先进方法的差距缩小了34.6%。我们的关联词感知的词级共指模型（CAW-coref）和代码可在 https://github.com/KarelDO/wl-coref 获取。

    State-of-the-art coreference resolutions systems depend on multiple LLM calls per document and are thus prohibitively expensive for many use cases (e.g., information extraction with large corpora). The leading word-level coreference system (WL-coref) attains 96.6% of these SOTA systems' performance while being much more efficient. In this work, we identify a routine yet important failure case of WL-coref: dealing with conjoined mentions such as 'Tom and Mary'. We offer a simple yet effective solution that improves the performance on the OntoNotes test set by 0.9% F1, shrinking the gap between efficient word-level coreference resolution and expensive SOTA approaches by 34.6%. Our Conjunction-Aware Word-level coreference model (CAW-coref) and code is available at https://github.com/KarelDO/wl-coref.
    
[^45]: 理解迁移学习和基于梯度的元学习技术

    Understanding Transfer Learning and Gradient-Based Meta-Learning Techniques. (arXiv:2310.06148v1 [cs.LG])

    [http://arxiv.org/abs/2310.06148](http://arxiv.org/abs/2310.06148)

    本文研究了微调、MAML和Reptile在迁移学习和元学习领域的性能差异，发现当在与训练数据分布不同的任务上进行评估时，仅对预训练网络进行微调的基准线可能比更复杂的元学习技术更有效。

    

    深度神经网络在各种任务上可以取得良好的性能，但通常需要大量的数据来训练。元学习作为一种提高这些网络从有限数据中泛化能力的方法受到广泛关注。虽然元学习技术在各种场景下被证明是成功的，但最近的结果表明，在评估与训练数据分布不同的任务时，与复杂的元学习技术（如MAML）相比，仅对预训练网络进行微调的基准线可能更有效。这一点令人惊讶，因为MAML的学习行为与微调的行为类似：两者都依赖于重复使用已学习的特征。我们调查了微调、MAML和另一种名为Reptile的元学习技术之间观察到的性能差异，并展示了MAML和Reptile在低数据情况下的快速适应能力。

    Deep neural networks can yield good performance on various tasks but often require large amounts of data to train them. Meta-learning received considerable attention as one approach to improve the generalization of these networks from a limited amount of data. Whilst meta-learning techniques have been observed to be successful at this in various scenarios, recent results suggest that when evaluated on tasks from a different data distribution than the one used for training, a baseline that simply finetunes a pre-trained network may be more effective than more complicated meta-learning techniques such as MAML, which is one of the most popular meta-learning techniques. This is surprising as the learning behaviour of MAML mimics that of finetuning: both rely on re-using learned features. We investigate the observed performance differences between finetuning, MAML, and another meta-learning technique called Reptile, and show that MAML and Reptile specialize for fast adaptation in low-data r
    
[^46]: 在LLM时代的强化学习：什么是必要的？什么是需要的？强化学习对RLHF、Prompting等的视角分析

    Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond. (arXiv:2310.06147v1 [cs.LG])

    [http://arxiv.org/abs/2310.06147](http://arxiv.org/abs/2310.06147)

    这篇论文介绍了在LLM时代中，强化学习在RLHF、Prompting等方面的应用，并讨论了其中的创新和贡献。

    

    最近大型语言模型（LLMs）的进展引起了广泛关注，并取得了ChatGPT和GPT-4等成功产品。它们在遵循指令并提供无害、有帮助和诚实（3H）回答方面的熟练程度主要归功于人类反馈强化学习（RLHF）技术。本文旨在将传统强化学习的研究与LLM研究中使用的RL技术联系起来，通过讨论RL在何时、何地和如何优秀，解释这一技术的神秘性。此外，我们探讨了潜在的未来领域，这些领域可能会从RLHF研究中获益或为其做出贡献。重点内容：1. RLHF是带有离线示范数据的在线逆向RL。2. RLHF比SFT更好，因为模仿学习（和逆向RL）比行为克隆（BC）更好，能够缓解累积误差问题。3. RLHF中的RM步骤产生了昂贵的人类反馈的代理，这样的见解可以推广到其他LLM任务，例如提示评估。

    Recent advancements in Large Language Models (LLMs) have garnered wide attention and led to successful products such as ChatGPT and GPT-4. Their proficiency in adhering to instructions and delivering harmless, helpful, and honest (3H) responses can largely be attributed to the technique of Reinforcement Learning from Human Feedback (RLHF). In this paper, we aim to link the research in conventional RL to RL techniques used in LLM research. Demystify this technique by discussing why, when, and how RL excels. Furthermore, we explore potential future avenues that could either benefit from or contribute to RLHF research.  Highlighted Takeaways:  1. RLHF is Online Inverse RL with Offline Demonstration Data.  2. RLHF $>$ SFT because Imitation Learning (and Inverse RL) $>$ Behavior Cloning (BC) by alleviating the problem of compounding error.  3. The RM step in RLHF generates a proxy of the expensive human feedback, such an insight can be generalized to other LLM tasks such as prompting evalua
    
[^47]: 从嘈杂的移动模态中预测布局序列

    Layout Sequence Prediction From Noisy Mobile Modality. (arXiv:2310.06138v1 [cs.CV])

    [http://arxiv.org/abs/2310.06138](http://arxiv.org/abs/2310.06138)

    提出一种名为LTrajDiff的新方法，从噪声移动数据中预测精确的布局序列，克服了由嘈杂数据、不完整轨迹和环境因素导致的视觉障碍。

    

    轨迹预测在理解行人移动方面扮演着重要角色，适用于自动驾驶和机器人等应用。当前的轨迹预测模型依赖于视觉模态的长、完整且准确观测到的序列。然而，现实世界中经常出现相机遮挡、遗漏对象或由于环境因素导致对象不在视线范围内的情况，导致轨迹不完整或噪声较大。为了克服这些限制，我们提出了一种新方法LTrajDiff，将被遮挡或不在视线范围内的物体与完全可见轨迹的物体同等重要。LTrajDiff利用移动手机的传感器数据克服不在视线范围内的约束，但引入了新的挑战，如模态融合、噪声数据和缺乏空间布局和物体尺寸信息。我们使用去噪扩散模型，采用从粗到精的扩散策略，从嘈杂的移动数据中预测精确的布局序列。

    Trajectory prediction plays a vital role in understanding pedestrian movement for applications such as autonomous driving and robotics. Current trajectory prediction models depend on long, complete, and accurately observed sequences from visual modalities. Nevertheless, real-world situations often involve obstructed cameras, missed objects, or objects out of sight due to environmental factors, leading to incomplete or noisy trajectories. To overcome these limitations, we propose LTrajDiff, a novel approach that treats objects obstructed or out of sight as equally important as those with fully visible trajectories. LTrajDiff utilizes sensor data from mobile phones to surmount out-of-sight constraints, albeit introducing new challenges such as modality fusion, noisy data, and the absence of spatial layout and object size information. We employ a denoising diffusion model to predict precise layout sequences from noisy mobile data using a coarse-to-fine diffusion strategy, incorporating th
    
[^48]: 使用梯度自动学习层间等变性

    Learning Layer-wise Equivariances Automatically using Gradients. (arXiv:2310.06131v1 [cs.LG])

    [http://arxiv.org/abs/2310.06131](http://arxiv.org/abs/2310.06131)

    该论文提出了一种通过梯度自动学习层间等变性的方法，通过改进软等变性的参数化和优化边缘似然来实现层间对称性的自动学习。

    

    卷积将等变性对称性编码到神经网络中，从而提高了泛化性能。然而，对称性提供了网络可以表示的函数的固定硬约束，需要事先指定，并且不能适应改变。我们的目标是允许灵活的对称性约束，可以通过梯度自动地从数据中学习。从头开始学习对称性和相关的权重连接结构有两个困难。首先，它需要有效灵活的层间等变性参数化。其次，对称性作为约束，因此不会被训练损失函数鼓励。为了克服这些挑战，我们改进了软等变性的参数化，并通过优化边缘似然来学习层间等变性的数量，其中边缘似然是使用可微分的拉普拉斯逼近估计的。这个目标平衡了数据拟合和模型复杂性，使层间对称性在深度学习中被发现。

    Convolutions encode equivariance symmetries into neural networks leading to better generalisation performance. However, symmetries provide fixed hard constraints on the functions a network can represent, need to be specified in advance, and can not be adapted. Our goal is to allow flexible symmetry constraints that can automatically be learned from data using gradients. Learning symmetry and associated weight connectivity structures from scratch is difficult for two reasons. First, it requires efficient and flexible parameterisations of layer-wise equivariances. Secondly, symmetries act as constraints and are therefore not encouraged by training losses measuring data fit. To overcome these challenges, we improve parameterisations of soft equivariance and learn the amount of equivariance in layers by optimising the marginal likelihood, estimated using differentiable Laplace approximations. The objective balances data fit and model complexity enabling layer-wise symmetry discovery in dee
    
[^49]: 在嘈杂的混响声学环境中，关于单声道语音分离的时域Conformer模型

    On Time Domain Conformer Models for Monaural Speech Separation in Noisy Reverberant Acoustic Environments. (arXiv:2310.06125v1 [cs.SD])

    [http://arxiv.org/abs/2310.06125](http://arxiv.org/abs/2310.06125)

    本论文提出了一种基于时域Conformer模型的单声道语音分离方法，相较于已有模型，在嘈杂的混响声学环境中实现了更高的分离效果和更高的计算效率。

    

    语音分离对多说话者技术研究人员仍然是一个重要的主题。卷积增强转换器（conformer）在许多语音处理任务中表现良好，但在语音分离方面得到的研究较少。最近的最先进（SOTA）分离模型大多数是时域音频分离网络（TasNets）。一些成功的模型利用了双通道（DP）网络，它们按顺序处理局部和全局信息。时域Conformer（TD-Conformers）是DP方法的一个类比，它们也按顺序处理局部和全局上下文，但具有不同的时间复杂度函数。实验结果表明，在实际较短的信号长度下，相对于特征维度，Conformer在控制特征维度时更高效。提出了子采样层以进一步提高计算效率。最佳的TD-Conformer在WHAMR和WSJ0-2Mix基准上分别实现了14.6dB和21.2dB的SISDR改进。

    Speech separation remains an important topic for multi-speaker technology researchers. Convolution augmented transformers (conformers) have performed well for many speech processing tasks but have been under-researched for speech separation. Most recent state-of-the-art (SOTA) separation models have been time-domain audio separation networks (TasNets). A number of successful models have made use of dual-path (DP) networks which sequentially process local and global information. Time domain conformers (TD-Conformers) are an analogue of the DP approach in that they also process local and global context sequentially but have a different time complexity function. It is shown that for realistic shorter signal lengths, conformers are more efficient when controlling for feature dimension. Subsampling layers are proposed to further improve computational efficiency. The best TD-Conformer achieves 14.6 dB and 21.2 dB SISDR improvement on the WHAMR and WSJ0-2Mix benchmarks, respectively.
    
[^50]: 文本驱动的视觉-语言模型在联邦学习中的提示生成

    Text-driven Prompt Generation for Vision-Language Models in Federated Learning. (arXiv:2310.06123v1 [cs.CV])

    [http://arxiv.org/abs/2310.06123](http://arxiv.org/abs/2310.06123)

    本论文提出了一种联邦文本驱动的提示生成方法（FedTPG），通过在多个远程客户端上学习统一的提示生成网络，实现了视觉-语言模型的泛化。实证评估表明，该方法在已知和未知类别上的泛化能力优于现有的联邦提示学习方法。

    

    视觉-语言模型的提示学习，例如CoOp，在适应不同的下游任务中取得了巨大的成功，这使得它成为联邦学习的一种有前景的解决方案，因为考虑到计算方面的原因。现有的提示学习技术用学习的向量替换手工制作的文本提示，在已知类别上取得了改进，但在未知类别上难以泛化。我们的工作通过提出联邦文本驱动的提示生成（FedTPG）来解决这一挑战，该方法能够以可扩展的方式在多个远程客户端上学习统一的提示生成网络。提示生成网络以任务相关的文本输入为条件，因此具有上下文感知性，适合泛化到已知和未知类别。我们对九个不同的图像分类数据集进行了全面的实证评估，结果表明，我们的方法优于现有的联邦提示学习方法，对已知和未知类别的总体泛化能力更好。

    Prompt learning for vision-language models, e.g., CoOp, has shown great success in adapting CLIP to different downstream tasks, making it a promising solution for federated learning due to computational reasons. Existing prompt learning techniques replace hand-crafted text prompts with learned vectors that offer improvements on seen classes, but struggle to generalize to unseen classes. Our work addresses this challenge by proposing Federated Text-driven Prompt Generation (FedTPG), which learns a unified prompt generation network across multiple remote clients in a scalable manner. The prompt generation network is conditioned on task-related text input, thus is context-aware, making it suitable to generalize for both seen and unseen classes. Our comprehensive empirical evaluations on nine diverse image classification datasets show that our method is superior to existing federated prompt learning methods, that achieve overall better generalization on both seen and unseen classes and is 
    
[^51]: 探索多元时间序列预测的进展：全面基准测试和异质性分析

    Exploring Progress in Multivariate Time Series Forecasting: Comprehensive Benchmarking and Heterogeneity Analysis. (arXiv:2310.06119v1 [cs.LG])

    [http://arxiv.org/abs/2310.06119](http://arxiv.org/abs/2310.06119)

    该论文旨在解决多元时间序列预测领域中公平基准测试和技术方法选择的争议，并提供对该领域取得的进展的深入洞察。

    

    多元时间序列（MTS）广泛存在于现实世界的复杂系统中，如交通和能源系统，对于理解和影响这些系统，它们的预测至关重要。最近，基于深度学习的方法在MTS中有效地建模时间和空间依赖关系方面获得了很大的流行，特别是在长期时间序列预测（LTSF）和时空预测（STF）中。然而，公平的基准测试问题和技术方法的选择在相关工作中一直存在争议。这些争议显著阻碍了我们对该领域进展的理解。因此，本文旨在解决这些争议，以提供对取得的进展的深入洞察。为了解决基准测试问题，我们引入了BasicTS，一个旨在公平比较MTS预测的基准。BasicTS建立了一个统一的训练流程和合理的评估设置，能够对30多种流行的MTS预测模型进行公正的评估。

    Multivariate Time Series (MTS) widely exists in real-word complex systems, such as traffic and energy systems, making their forecasting crucial for understanding and influencing these systems. Recently, deep learning-based approaches have gained much popularity for effectively modeling temporal and spatial dependencies in MTS, specifically in Long-term Time Series Forecasting (LTSF) and Spatial-Temporal Forecasting (STF). However, the fair benchmarking issue and the choice of technical approaches have been hotly debated in related work. Such controversies significantly hinder our understanding of progress in this field. Thus, this paper aims to address these controversies to present insights into advancements achieved. To resolve benchmarking issues, we introduce BasicTS, a benchmark designed for fair comparisons in MTS forecasting. BasicTS establishes a unified training pipeline and reasonable evaluation settings, enabling an unbiased evaluation of over 30 popular MTS forecasting mode
    
[^52]: 退后一步：通过抽象唤起大型语言模型的推理能力

    Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models. (arXiv:2310.06117v1 [cs.LG])

    [http://arxiv.org/abs/2310.06117](http://arxiv.org/abs/2310.06117)

    本文提出了一种简单的提示技术，使得大型语言模型能够通过抽象获得高层概念和基本原理，并将其应用于推理路径中，从而显著提升模型在各种推理密集型任务上的表现。

    

    我们提出了一种称为“退后提示”的简单提示技术，使得大型语言模型能够通过从包含具体细节的实例中进行抽象，得出高层概念和基本原理。利用这些概念和原理来指导推理步骤，语言模型在正确推理路径上显著提升了能力。我们使用PaLM-2L模型进行了退后提示实验，在包括STEM、知识问答和多跳推理在内的各种具有挑战性的推理密集型任务上观察到了明显的性能提升。例如，在MMLU物理和化学任务上，退后提示可以将PaLM-2L的性能提升7%和11%，在TimeQA任务上提升27%，在MuSiQue任务上提升7%。

    We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide the reasoning steps, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L models and observe substantial performance gains on a wide range of challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%, TimeQA by 27%, and MuSiQue by 7%.
    
[^53]: OptiMUS: 使用mip求解器和大规模语言模型的优化建模

    OptiMUS: Optimization Modeling Using mip Solvers and large language models. (arXiv:2310.06116v1 [cs.AI])

    [http://arxiv.org/abs/2310.06116](http://arxiv.org/abs/2310.06116)

    介绍了OptiMUS，一种基于大规模语言模型的优化建模代理，用于解决MILP问题。该代理能够自动生成数学模型、编写和调试求解器代码，并具有较高的解决率。

    

    优化问题在各个领域中普遍存在，从制造和分销到医疗保健。然而，大多数这类问题仍然是通过手工启发式解决而不是通过最先进的求解器进行最优解，因为制定和解决这些问题所需的专业知识限制了优化工具和技术的广泛应用。我们介绍了OptiMUS，一种基于大规模语言模型(LLM)的代理，旨在从自然语言描述中制定和解决MILP问题。OptiMUS能够开发数学模型，编写和调试求解器代码，开发测试，并检查生成解的有效性。为了对我们的代理进行基准测试，我们提供了NLP4LP，这是一个线性规划(LP)和混合整数线性规划(MILP)问题的新数据集。我们的实验表明，与基本的LLM提示策略相比，OptiMUS能够解决更多问题，解决率提高了67％。OptiMUS的代码和NLP4LP数据集可在\href{https://github中获得.

    Optimization problems are pervasive across various sectors, from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers, as the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. We introduce OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve MILP problems from their natural language descriptions. OptiMUS is capable of developing mathematical models, writing and debugging solver code, developing tests, and checking the validity of generated solutions. To benchmark our agent, we present NLP4LP, a novel dataset of linear programming (LP) and mixed integer linear programming (MILP) problems. Our experiments demonstrate that OptiMUS is able to solve 67\% more problems compared to a basic LLM prompting strategy. OptiMUS code and NLP4LP dataset are available at \href{https://github
    
[^54]: 学习交互式现实世界模拟器

    Learning Interactive Real-World Simulators. (arXiv:2310.06114v1 [cs.AI])

    [http://arxiv.org/abs/2310.06114](http://arxiv.org/abs/2310.06114)

    通过生成建模学习交互体验的通用模拟器，以模拟人类、机器人和其他交互式代理人对真实世界中行为的响应。

    

    训练在互联网数据上的生成模型已经彻底改变了文本、图像和视频内容的创建方式。也许生成模型的下一个里程碑是在人类、机器人和其他交互式代理人采取行动时模拟真实的体验。实际应用范围从游戏和电影中的可控内容创建，到仅在模拟环境中训练可以直接部署在现实世界中的体验式代理人。我们探索了通过生成建模来学习现实世界交互的通用模拟器(UniSim)的可能性。我们首先重要地观察到，用于学习现实世界模拟器的自然数据集通常在不同的方面丰富多样（例如，图像数据中丰富的物体，机器人数据中密集采样的动作，导航数据中多样的移动）。通过精心协调各种数据集，每个数据集都提供整体体验的不同方面，UniSim可以模拟人类与环境的交互方式。

    Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator (UniSim) of real-world interaction through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different axes (e.g., abundant objects in image data, densely sampled actions in robotics data, and diverse movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, UniSim can emulate how
    
[^55]: 什么时候是基于不可知激励强化学习的统计可处理性？

    When is Agnostic Reinforcement Learning Statistically Tractable?. (arXiv:2310.06113v1 [cs.LG])

    [http://arxiv.org/abs/2310.06113](http://arxiv.org/abs/2310.06113)

    本文研究了基于不可知的PAC强化学习的问题，引入了跨越容量这个新的复杂度度量，并发现了在生成和在线访问模型之间以及在线访问下的确定和随机MDP之间的差异。

    

    本文研究了基于不可知的PAC强化学习（RL）的问题：给定一个策略类别Π，需要和一个未知的有可能有大状态和动作空间的MDP进行多少轮互动来学习一个关于Π的ε-次优策略？为此，我们引入了一个新的复杂度度量，称为“跨越容量”，它仅依赖于策略类别Π，并且与MDP的动态无关。通过一个生成模型，我们证明了对于任何策略类别Π，有界的跨越容量可以刻画PAC可学习性。然而，对于在线RL来说，情况更加微妙。我们证明了存在一个具有有界跨越容量的策略类别Π，需要超多项式数量的样本才能学习。这揭示了在不同生成和在线访问模型之间以及在线访问下的确定/随机MDP之间的不确定可学习性之间的出乎意料的差异。在积极的方面，我们识别出一个额外的“太阳”

    We study the problem of agnostic PAC reinforcement learning (RL): given a policy class $\Pi$, how many rounds of interaction with an unknown MDP (with a potentially large state and action space) are required to learn an $\epsilon$-suboptimal policy with respect to $\Pi$? Towards that end, we introduce a new complexity measure, called the \emph{spanning capacity}, that depends solely on the set $\Pi$ and is independent of the MDP dynamics. With a generative model, we show that for any policy class $\Pi$, bounded spanning capacity characterizes PAC learnability. However, for online RL, the situation is more subtle. We show there exists a policy class $\Pi$ with a bounded spanning capacity that requires a superpolynomial number of samples to learn. This reveals a surprising separation for agnostic learnability between generative access and online access models (as well as between deterministic/stochastic MDPs under online access). On the positive side, we identify an additional \emph{sunf
    
[^56]: 使用变分背门调整进行高维因果推断

    High Dimensional Causal Inference with Variational Backdoor Adjustment. (arXiv:2310.06100v1 [cs.AI])

    [http://arxiv.org/abs/2310.06100](http://arxiv.org/abs/2310.06100)

    本论文介绍了一种用于高维因果推断的变分背门调整技术，能够处理高维治疗和混杂因素，并成功应用于医疗数据中。

    

    背门调整是一种因果推断技术，用于从纯观察数据中估计干预数量。在医疗环境中，背门调整可用于控制混杂因素并估计治疗效果。然而，高维治疗和混杂因素可能引发一系列潜在问题：可计算性、可辨识性、优化等。在这项工作中，我们采用生成建模方法来解决高维治疗和混杂因素的背门调整问题。我们将背门调整视为一种变分推断优化问题，无需依赖代理变量和隐藏混杂因素。实验上，我们的方法能够在各种高维环境中估计干预概率，包括半合成X光医疗数据。据我们所知，这是背门调整的首次应用，其中所有相关变量都是高维的。

    Backdoor adjustment is a technique in causal inference for estimating interventional quantities from purely observational data. For example, in medical settings, backdoor adjustment can be used to control for confounding and estimate the effectiveness of a treatment. However, high dimensional treatments and confounders pose a series of potential pitfalls: tractability, identifiability, optimization. In this work, we take a generative modeling approach to backdoor adjustment for high dimensional treatments and confounders. We cast backdoor adjustment as an optimization problem in variational inference without reliance on proxy variables and hidden confounders. Empirically, our method is able to estimate interventional likelihood in a variety of high dimensional settings, including semi-synthetic X-ray medical data. To the best of our knowledge, this is the first application of backdoor adjustment in which all the relevant variables are high dimensional.
    
[^57]: 深度强化学习中的预测辅助目标模仿大脑学习

    Predictive auxiliary objectives in deep RL mimic learning in the brain. (arXiv:2310.06089v1 [cs.AI])

    [http://arxiv.org/abs/2310.06089](http://arxiv.org/abs/2310.06089)

    本文研究了深度强化学习中预测辅助目标对表示学习和性能的影响，发现在资源受限的情况下，预测目标能显著提高和稳定学习，并且能支持表征迁移。此外，与神经活动变化相似，这些辅助目标也模拟了大脑中的表征变化。

    

    预测即将发生的事件的能力被假设为自然和机器认知的关键方面。这在深度强化学习中得到了支持，其中自监督辅助目标（如预测）被广泛用于支持表示学习和提高任务性能。本文研究了预测辅助目标对RL系统中不同模块的表示学习的影响，以及这些模拟大脑观察到的表征变化。我们发现，在资源受限的架构中，预测目标特别提高和稳定学习，并且我们确定了更长的预测时段在支持表征迁移方面更好。此外，我们发现这个RL系统中的表征变化与大脑中观察到的神经活动变化有惊人的相似之处。具体而言，我们在辅助预测模型和大脑中的表征变化之间建立了联系。

    The ability to predict upcoming events has been hypothesized to comprise a key aspect of natural and machine cognition. This is supported by trends in deep reinforcement learning (RL), where self-supervised auxiliary objectives such as prediction are widely used to support representation learning and improve task performance. Here, we study the effects predictive auxiliary objectives have on representation learning across different modules of an RL system and how these mimic representational changes observed in the brain. We find that predictive objectives improve and stabilize learning particularly in resource-limited architectures, and we identify settings where longer predictive horizons better support representational transfer. Furthermore, we find that representational changes in this RL system bear a striking resemblance to changes in neural activity observed in the brain across various experiments. Specifically, we draw a connection between the auxiliary predictive model of the 
    
[^58]: 可展示的时间序列预测

    Performative Time-Series Forecasting. (arXiv:2310.06077v1 [cs.LG])

    [http://arxiv.org/abs/2310.06077](http://arxiv.org/abs/2310.06077)

    本论文研究了时间序列预测中的展示性问题，提出了一种新的方法（FPS），通过利用延迟响应的概念来解决展示性引起的分布变化，并实现准确的预测。

    

    时间序列预测是各个领域中的一个关键挑战，在近年来取得了实质性的进展。许多现实生活场景，如公共卫生、经济和社会应用，涉及到反馈循环，其中预测结果可能会影响到预测的结果，进而改变目标变量的分布。这种现象被称为展示性，引入了可能出现“自我抵消”或“自我实现”的预测的潜力。尽管在各个领域中对分类问题进行了广泛的研究，但展示性在机器学习视角下的时间序列预测问题尚未得到广泛探讨。在这篇论文中，我们对可展示的时间序列预测（PeTS）进行了形式化，解决了当可能存在展示性引起的分布变化时的准确预测挑战。我们提出了一种新颖方法，特征展示性转移（FPS），它利用延迟响应的概念来预测分布的变化和随后的变量。

    Time-series forecasting is a critical challenge in various domains and has witnessed substantial progress in recent years. Many real-life scenarios, such as public health, economics, and social applications, involve feedback loops where predictions can influence the predicted outcome, subsequently altering the target variable's distribution. This phenomenon, known as performativity, introduces the potential for 'self-negating' or 'self-fulfilling' predictions. Despite extensive studies in classification problems across domains, performativity remains largely unexplored in the context of time-series forecasting from a machine-learning perspective.  In this paper, we formalize performative time-series forecasting (PeTS), addressing the challenge of accurate predictions when performativity-induced distribution shifts are possible. We propose a novel approach, Feature Performative-Shifting (FPS), which leverages the concept of delayed response to anticipate distribution shifts and subseque
    
[^59]: 使用自监督学习和患者表型研究的痛苦预测：预防阿片类药物成瘾的尝试

    Pain Forecasting using Self-supervised Learning and Patient Phenotyping: An attempt to prevent Opioid Addiction. (arXiv:2310.06075v1 [cs.AI])

    [http://arxiv.org/abs/2310.06075](http://arxiv.org/abs/2310.06075)

    本研究旨在通过使用自监督学习方法和患者表型研究，预测患者未来的疼痛轨迹，以帮助患者管理镰状细胞贫血，改善生活质量，并减少对阿片类药物的依赖和副作用。

    

    镰状细胞贫血（SCD）是一种慢性遗传性疾病，特征为反复发作的急性疼痛。阿片类药物通常用于管理这些疼痛发作；在这种疾病中使用阿片类药物管理疼痛的程度是一个争议的问题。吸食成瘾的风险和阿片类药物治疗的副作用往往会导致将来更多的疼痛发作。因此，准确预测患者将来的疼痛发展轨迹对于帮助患者管理他们的SCD以提高生活质量而不损害治疗至关重要。由于疼痛主要是由患者自行报告记录的，获得许多疼痛记录来设计预测模型是具有挑战性的。因此，在解决疼痛预测问题时，仅依靠监督学习的方式既昂贵又痛苦（因为需要患者配合）。鉴于这一挑战，我们提出使用自监督学习方法来解决疼痛预测问题。此外，对这种时间序列数据进行聚类对于患者表型研究至关重要。

    Sickle Cell Disease (SCD) is a chronic genetic disorder characterized by recurrent acute painful episodes. Opioids are often used to manage these painful episodes; the extent of their use in managing pain in this disorder is an issue of debate. The risk of addiction and side effects of these opioid treatments can often lead to more pain episodes in the future. Hence, it is crucial to forecast future patient pain trajectories to help patients manage their SCD to improve their quality of life without compromising their treatment. It is challenging to obtain many pain records to design forecasting models since it is mainly recorded by patients' self-report. Therefore, it is expensive and painful (due to the need for patient compliance) to solve pain forecasting problems in a purely supervised manner. In light of this challenge, we propose to solve the pain forecasting problem using self-supervised learning methods. Also, clustering such time-series data is crucial for patient phenotyping,
    
[^60]: 使用旋转矩阵增强基于视觉的人体姿势估计

    Augmenting Vision-Based Human Pose Estimation with Rotation Matrix. (arXiv:2310.06068v1 [cs.CV])

    [http://arxiv.org/abs/2310.06068](http://arxiv.org/abs/2310.06068)

    本研究提出使用旋转矩阵的模型，以增强基于姿势估计的活动识别的分类准确性。通过实验证明，利用带有旋转矩阵数据增强的SVM与SGD优化方法，在分类五种体育活动时达到了96%的准确率，相比之下，不实施数据增强技术的基准准确率仅为64%。

    

    健身应用通常用于监控健身房内的活动，但它们常常无法自动跟踪健身房内部的活动。本研究提出了一种模型，该模型利用姿势估计与一种新颖的数据增强方法，即旋转矩阵。我们的目标是通过姿势估计数据来提高活动识别的分类准确性。通过实验，我们尝试了不同的分类算法以及图像增强方法。我们的研究结果表明，利用带有旋转矩阵数据增强的SVM与SGD优化的方法在分类五种体育活动方面取得了最准确的结果，准确率达到了96%。相反，如果不实施数据增强技术，基准准确率仅为64%。

    Fitness applications are commonly used to monitor activities within the gym, but they often fail to automatically track indoor activities inside the gym. This study proposes a model that utilizes pose estimation combined with a novel data augmentation method, i.e., rotation matrix. We aim to enhance the classification accuracy of activity recognition based on pose estimation data. Through our experiments, we experiment with different classification algorithms along with image augmentation approaches. Our findings demonstrate that the SVM with SGD optimization, using data augmentation with the Rotation Matrix, yields the most accurate results, achieving a 96% accuracy rate in classifying five physical activities. Conversely, without implementing the data augmentation techniques, the baseline accuracy remains at a modest 64%.
    
[^61]: LLM用于SoC安全: 一种范式转变

    LLM for SoC Security: A Paradigm Shift. (arXiv:2310.06046v1 [cs.CR])

    [http://arxiv.org/abs/2310.06046](http://arxiv.org/abs/2310.06046)

    本论文研究利用大型语言模型（LLM）解决系统级芯片（SoC）设计中的安全性问题，通过将LLM集成到SoC安全验证流程中，开辟了更高效、可伸缩和适应的新方法。目标是确保越来越复杂的SoC的安全性。

    

    随着电子设备中系统级芯片（SoC）设计的普及和复杂性的增加，将安全性纳入SoC设计流程的任务面临着重大挑战。现有的安全解决方案由于在可伸缩性、全面性和适应性方面的局限性，无法提供对现代SoC设计的有效验证。另一方面，大型语言模型（LLM）因其在自然语言理解、高级推理和程序合成任务中的卓越成功而备受赞誉。认识到这一机遇，我们的研究着眼于利用生成式预训练转换器（GPT）的新兴能力来解决SoC安全领域的现有差距，旨在追求更高效、可伸缩和适应的方法论。通过将LLM集成到SoC安全验证范式中，我们打开了确保日益复杂SoC安全的新可能性和挑战。本文提供了深入分析。

    As the ubiquity and complexity of system-on-chip (SoC) designs increase across electronic devices, the task of incorporating security into an SoC design flow poses significant challenges. Existing security solutions are inadequate to provide effective verification of modern SoC designs due to their limitations in scalability, comprehensiveness, and adaptability. On the other hand, Large Language Models (LLMs) are celebrated for their remarkable success in natural language understanding, advanced reasoning, and program synthesis tasks. Recognizing an opportunity, our research delves into leveraging the emergent capabilities of Generative Pre-trained Transformers (GPTs) to address the existing gaps in SoC security, aiming for a more efficient, scalable, and adaptable methodology. By integrating LLMs into the SoC security verification paradigm, we open a new frontier of possibilities and challenges to ensure the security of increasingly complex SoCs. This paper offers an in-depth analysis
    
[^62]: 通过确定性对流模型的生成性集成深度学习，用于严重天气预测

    Generative ensemble deep learning severe weather prediction from a deterministic convection-allowing model. (arXiv:2310.06045v1 [cs.LG])

    [http://arxiv.org/abs/2310.06045](http://arxiv.org/abs/2310.06045)

    本论文开发了一种集成后处理方法，将生成对抗网络（CGANs）和卷积神经网络（CNN）结合起来，对严重天气进行概率预测。该方法在使用HRRR预报作为输入数据，在2021年的测试数据集上相对于其他基于神经网络的方法提高了高达20％的Brier技巧分数（BSS）。

    

    开发了一种用于概率预测美国本土严重天气（龙卷风、冰雹和大风阵）的集成后处理方法。该方法将条件生成对抗网络（CGANs）与卷积神经网络（CNN）结合起来，用于后处理对流允许模型（CAM）的预测。CGANs被设计用于从确定性CAM预测中创建合成集成成员，其输出经过CNN处理以估计严重天气的概率。该方法使用高分辨率快速刷新（HRRR）1-24小时预报作为输入，以及暴风预警中心（SPC）的严重天气报告作为目标进行测试。在2021年的HRRR预测测试数据集中，该方法相对于其他基于神经网络的参考方法提高了高达20％的Brier技巧分数（BSS）。

    An ensemble post-processing method is developed for the probabilistic prediction of severe weather (tornadoes, hail, and wind gusts) over the conterminous United States (CONUS). The method combines conditional generative adversarial networks (CGANs), a type of deep generative model, with a convolutional neural network (CNN) to post-process convection-allowing model (CAM) forecasts. The CGANs are designed to create synthetic ensemble members from deterministic CAM forecasts, and their outputs are processed by the CNN to estimate the probability of severe weather. The method is tested using High-Resolution Rapid Refresh (HRRR) 1--24 hr forecasts as inputs and Storm Prediction Center (SPC) severe weather reports as targets. The method produced skillful predictions with up to 20% Brier Skill Score (BSS) increases compared to other neural-network-based reference methods using a testing dataset of HRRR forecasts in 2021. For the evaluation of uncertainty quantification, the method is overcon
    
[^63]: DyST：面向实际视频的动态神经场景表示

    DyST: Towards Dynamic Neural Scene Representations on Real-World Videos. (arXiv:2310.06020v1 [cs.CV])

    [http://arxiv.org/abs/2310.06020](http://arxiv.org/abs/2310.06020)

    DyST模型通过学习动态场景的潜在分解，从实际视频中捕捉到了场景的3D结构和动态特性，并实现了对相机和场景内容的独立控制视图生成。

    

    对世界的视觉理解超越了单个图像的语义和平面结构。我们的目标是从单目实际视频中捕捉到实际场景的3D结构和动态特性。我们的Dynamic Scene Transformer（DyST）模型利用了最近的神经场景表示研究成果，学习了单目实际视频的潜在分解，包括场景内容、每个视角的场景动态和相机姿态。通过在单目视频和我们的新的合成数据集DySO上进行一种新颖的协同训练，实现了这种分离。DyST学习到了动态场景的具体潜在表示，使得可以对场景的相机和内容进行独立控制的视图生成成为可能。

    Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural scene representation to learn a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose. This separation is achieved through a novel co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.
    
[^64]: AI驱动的剥夺中的分而治之动态

    Divide-and-Conquer Dynamics in AI-Driven Disempowerment. (arXiv:2310.06009v1 [cs.CY])

    [http://arxiv.org/abs/2310.06009](http://arxiv.org/abs/2310.06009)

    这项研究通过构建游戏理论模型，研究了AI驱动的剥夺中的不团结问题。研究发现，当前受害者需要让未来受害者认识到他们的利益同样面临严重和紧迫的威胁，以激励未来受害者以团结支持当前受害者。

    

    AI公司试图创造出在大部分经济价值工作上超越人类的AI系统。当前的AI模型已经自动化削弱了一些艺术家、演员和作家的生计。但是在那些优先考虑当前危害和未来危害之间存在着内讧。我们构建了一个博弈论模型来研究这种不团结的原因和后果。我们的模型还有助于解释为什么在历史上，面临共同威胁的利益相关方发现联合起来对抗该威胁是有利的，而该共同威胁又发现分而治之是有利的。在现实参数假设下，我们的模型提出了几个预测，在历史经验记录中得到了初步的证实。

    AI companies are attempting to create AI systems that outperform humans at most economically valuable work. Current AI models are already automating away the livelihoods of some artists, actors, and writers. But there is infighting between those who prioritize current harms and future harms. We construct a game-theoretic model of conflict to study the causes and consequences of this disunity. Our model also helps explain why throughout history, stakeholders sharing a common threat have found it advantageous to unite against it, and why the common threat has in turn found it advantageous to divide and conquer.  Under realistic parameter assumptions, our model makes several predictions that find preliminary corroboration in the historical-empirical record. First, current victims of AI-driven disempowerment need the future victims to realize that their interests are also under serious and imminent threat, so that future victims are incentivized to support current victims in solidarity. Se
    
[^65]: 重新思考高效大规模语言模型训练中的内存和通信成本

    Rethinking Memory and Communication Cost for Efficient Large Language Model Training. (arXiv:2310.06003v1 [cs.LG])

    [http://arxiv.org/abs/2310.06003](http://arxiv.org/abs/2310.06003)

    本文研究了大型语言模型训练中内存和通信成本对训练速度的影响，并提出了一种平衡内存和通信的优化器（PaRO）。此外，还提出了一种用于大模型训练的分层重叠环通信拓扑结构（HO-Ring），实验证明该算法提高了训练过程中的通信效率。

    

    随着模型规模和训练数据集的不断增加，大规模模型训练框架通过各种分片技术减小内存消耗。然而，巨大的通信开销降低了训练效率，特别是在网络带宽变化的公共云环境中。在本文中，我们重新思考内存消耗和通信开销对大型语言模型训练速度的影响，并提出了一种平衡内存和通信的部分冗余优化器(PaRO)。PaRO通过将GPU集群分组和引入微小的组内内存冗余，减少了组间通信的数量和频率，从而提高了模型的训练效率。此外，我们提出了一种分层重叠环(HO-Ring)通信拓扑结构，以增强大模型训练中节点之间或跨交换机之间的通信效率。我们的实验证明，HO-Ring算法改善了训练过程中的通信效率。

    As model sizes and training datasets continue to increase, large-scale model training frameworks reduce memory consumption by various sharding techniques. However, the huge communication overhead reduces the training efficiency, especially in public cloud environments with varying network bandwidths. In this paper, we rethink the impact of memory consumption and communication overhead on the training speed of large language model, and propose a memory-communication balanced \underline{Pa}rtial \underline{R}edundancy \underline{O}ptimizer (PaRO). PaRO reduces the amount and frequency of inter-group communication by grouping GPU clusters and introducing minor intra-group memory redundancy, thereby improving the training efficiency of the model. Additionally, we propose a Hierarchical Overlapping Ring (HO-Ring) communication topology to enhance communication efficiency between nodes or across switches in large model training. Our experiments demonstrate that the HO-Ring algorithm improves
    
[^66]: 评估ChatGPT的推理能力

    Measuring reasoning capabilities of ChatGPT. (arXiv:2310.05993v1 [cs.AI])

    [http://arxiv.org/abs/2310.05993](http://arxiv.org/abs/2310.05993)

    这项研究评估了ChatGPT在推理任务中的表现，发现其在逻辑谜题中仅能提供7%正确答案和理由。

    

    本研究旨在量化ChatGPT在推理任务中产生的逻辑错误。实验使用了来自库中的144个谜题，包括算术谜题、逻辑等式、数独等谜题。使用定理证明器Prover9和有限模型查找器Mace4对这些谜题的正确解进行了验证。研究的主要输出是一个由100个逻辑谜题组成的基准数据集。对于这个数据集，ChatGPT仅提供了7%的正确答案和理由。

    I shall quantify the logical faults generated by ChatGPT when applied to reasoning tasks. For experiments, I use the 144 puzzles from the library \url{https://users.utcluj.ro/~agroza/puzzles/maloga}~\cite{groza:fol}. The library contains puzzles of various types, including arithmetic puzzles, logical equations, Sudoku-like puzzles, zebra-like puzzles, truth-telling puzzles, grid puzzles, strange numbers, or self-reference puzzles. The correct solutions for these puzzles were checked using the theorem prover Prover9~\cite{mccune2005release} and the finite models finder Mace4~\cite{mccune2003mace4} based on human-modelling in Equational First Order Logic. A first output of this study is the benchmark of 100 logical puzzles. For this dataset ChatGPT provided both correct answer and justification for 7\% only. %, while BARD for 5\%. Since the dataset seems challenging, the researchers are invited to test the dataset on more advanced or tuned models than ChatGPT3.5 with more crafted prompts
    
[^67]: 使用大型语言模型模拟社交媒体以评估替代新闻供稿算法

    Simulating Social Media Using Large Language Models to Evaluate Alternative News Feed Algorithms. (arXiv:2310.05984v1 [cs.SI])

    [http://arxiv.org/abs/2310.05984](http://arxiv.org/abs/2310.05984)

    本论文通过使用大型语言模型和代理建模相结合的方法，模拟社交媒体平台，并研究不同新闻供稿算法对在线对话质量的影响。通过创建逼真的人物形象和使用不同的算法，研究发现新闻供稿算法对于塑造社交媒体对话是具有重要影响的。

    

    社交媒体经常因为放大有害言论和阻碍建设性对话而受到批评。但是设计促进更好对话的社交媒体平台本质上是具有挑战性的。本论文探讨了通过使用大型语言模型（LLM）和基于代理模型的建模来模拟社交媒体是否有助于研究不同新闻供稿算法对在线对话质量的影响。我们使用美国全国选举研究的数据创建逼真的人物形象，以填充模拟社交媒体平台。接下来，我们让这些代理人阅读和分享新闻文章，并在三个使用不同新闻供稿算法的平台上对其它用户的信息点赞或评论。在第一个平台上，用户可以看到他们关注的用户中得到最多点赞和评论的帖子。在第二个平台上，他们可以看到来自所有用户的帖子，即使是来自自己网络之外的用户。第三个平台采用了一种新颖的“桥接”算法，突出显示帖子。

    Social media is often criticized for amplifying toxic discourse and discouraging constructive conversations. But designing social media platforms to promote better conversations is inherently challenging. This paper asks whether simulating social media through a combination of Large Language Models (LLM) and Agent-Based Modeling can help researchers study how different news feed algorithms shape the quality of online conversations. We create realistic personas using data from the American National Election Study to populate simulated social media platforms. Next, we prompt the agents to read and share news articles - and like or comment upon each other's messages - within three platforms that use different news feed algorithms. In the first platform, users see the most liked and commented posts from users whom they follow. In the second, they see posts from all users - even those outside their own network. The third platform employs a novel "bridging" algorithm that highlights posts th
    
[^68]: 使用大规模语言模型建立合作行为相关个性特征的进化模型

    An evolutionary model of personality traits related to cooperative behavior using a large language model. (arXiv:2310.05976v1 [physics.soc-ph])

    [http://arxiv.org/abs/2310.05976](http://arxiv.org/abs/2310.05976)

    本文提出了一个使用大规模语言模型的进化模型，研究了个性特征在博弈理论关系背景下的演化。实验证明该模型能够展示出演化的特征。

    

    本文旨在通过将生成模型的丰富表达性引入到社会基于智能体的进化模型中，探讨多样性和社会群体的进化动态。具体来说，我们关注在博弈理论关系的背景下个性特征的演化，这是一个相互利益施加强烈选择压力的情况。我们构建了一个智能体模型，其中合作行为相关个性特征的语言描述被用作基因，从大规模语言模型（LLM）中提取的确定性策略根据这些个性特征做出行为决策被用作行为特征。根据平均收益的选择和通过请求LLM对父基因进行略微修改实现基因的突变，我们使群体进化。通过初步实验和分析，我们阐明了这样的模型确实可以展示演化的特征。

    This paper aims to shed light on the evolutionary dynamics of diverse and social populations by introducing the rich expressiveness of generative models into the trait expression of social agent-based evolutionary models. Specifically, we focus on the evolution of personality traits in the context of a game-theoretic relationship as a situation in which inter-individual interests exert strong selection pressures. We construct an agent model in which linguistic descriptions of personality traits related to cooperative behavior are used as genes. The deterministic strategies extracted from Large Language Model (LLM) that make behavioral decisions based on these personality traits are used as behavioral traits. The population is evolved according to selection based on average payoff and mutation of genes by asking LLM to slightly modify the parent gene toward cooperative or selfish. Through preliminary experiments and analyses, we clarify that such a model can indeed exhibit the evolution
    
[^69]: 使用多模型深度学习方法的自动化胸部X光报告生成器

    Automated Chest X-Ray Report Generator Using Multi-Model Deep Learning Approach. (arXiv:2310.05969v1 [eess.IV])

    [http://arxiv.org/abs/2310.05969](http://arxiv.org/abs/2310.05969)

    提出了一种基于多模型深度学习的自动化胸部X光报告生成器系统，通过利用多个二元分类模型检测多种异常，在单个图像中辅助放射科医生的工作。该系统将放射学异常检测限制为心脏肥大、肺积液和实变，并通过三个步骤生成放射学报告：图像预处理、深度学习模型检测异常和生成报告。

    

    阅读和解读胸部X光图像是大多数放射科医生的例行工作之一。然而，即使对于经验最丰富的医生来说，这仍然可能是具有挑战性的。因此，我们提出了一种基于多模型深度学习的自动化胸部X光报告生成系统，旨在辅助放射科医生的工作。所提出的系统的基本思想是利用多个二元分类模型来检测多种异常，每个模型负责检测一种异常，在单个图像中。在本研究中，我们将放射学异常检测限制为心脏肥大、肺积液和实变。系统通过执行以下三个步骤生成放射学报告：图像预处理，利用深度学习模型检测异常，并生成报告。图像预处理步骤的目的是通过将其缩放为128x128像素并分割成三个部分来使输入标准化，分别涵盖上部、下部和中部。

    Reading and interpreting chest X-ray images is one of the most radiologist's routines. However, it still can be challenging, even for the most experienced ones. Therefore, we proposed a multi-model deep learning-based automated chest X-ray report generator system designed to assist radiologists in their work. The basic idea of the proposed system is by utilizing multi binary-classification models for detecting multi abnormalities, with each model responsible for detecting one abnormality, in a single image. In this study, we limited the radiology abnormalities detection to only cardiomegaly, lung effusion, and consolidation. The system generates a radiology report by performing the following three steps: image pre-processing, utilizing deep learning models to detect abnormalities, and producing a report. The aim of the image pre-processing step is to standardize the input by scaling it to 128x128 pixels and slicing it into three segments, which covers the upper, lower, and middle parts
    
[^70]: 指纹攻击：联邦学习中的客户端去匿名化

    Fingerprint Attack: Client De-Anonymization in Federated Learning. (arXiv:2310.05960v1 [cs.CR])

    [http://arxiv.org/abs/2310.05960](http://arxiv.org/abs/2310.05960)

    本文研究了在联邦学习中，通过梯度指纹攻击可以轻松打破参与者匿名化，并展示了使用差分隐私进行训练可以提供实际防御。

    

    联邦学习允许在参与者不相信中央服务器和彼此的情况下进行协作训练，而无需共享数据。通过确保参与者与服务器之间的通信经过混洗，将参与者身份与其数据分离，还可以进一步提高隐私。本文旨在通过提出一种新颖的梯度指纹攻击来检验这种防御是否足以保证匿名性。我们在两个语言语料库上的联邦语言模型的实证研究中展示了梯度聚类可以轻松地打破匿名化。然后，我们展示了使用差分隐私进行训练可以提供对我们的指纹攻击的实际防御。

    Federated Learning allows collaborative training without data sharing in settings where participants do not trust the central server and one another. Privacy can be further improved by ensuring that communication between the participants and the server is anonymized through a shuffle; decoupling the participant identity from their data. This paper seeks to examine whether such a defense is adequate to guarantee anonymity, by proposing a novel fingerprinting attack over gradients sent by the participants to the server. We show that clustering of gradients can easily break the anonymization in an empirical study of learning federated language models on two language corpora. We then show that training with differential privacy can provide a practical defense against our fingerprint attack.
    
[^71]: GNN-based入侵检测的高效网络表示

    Efficient Network Representation for GNN-based Intrusion Detection. (arXiv:2310.05956v1 [cs.CR])

    [http://arxiv.org/abs/2310.05956](http://arxiv.org/abs/2310.05956)

    本文提出了一种基于GNN的入侵检测方法，通过使用流图作为网络表示形式，提供了相关的拓扑信息，包括恶意行为模式、多步攻击阶段之间的关系以及欺骗和预欺骗攻击者活动之间的关系。通过嵌入节点特征和学习相关的攻击模式，该方法能够对通信流进行分类，并分配恶意程度得分。该方法还指出了传统评估过程中可能存在的数据泄漏问题，并提出了相应的建议。

    

    过去几十年中，网络攻击的数量不断增长，给经济和隐私造成严重损害，这揭示了网络入侵检测方法在预防网络攻击和降低风险方面的需求。在本研究中，我们提出了一种新的网络表示形式，即流图，旨在为入侵检测任务提供相关的拓扑信息，如恶意行为模式、多步攻击阶段之间的关系以及欺骗和预欺骗攻击者活动之间的关系。此外，我们提出了一个基于图神经网络（GNN）的框架，用于利用所提出的图结构对通信流进行分类，并为其分配恶意程度得分。该框架包括三个主要步骤，旨在嵌入节点特征并从网络表示中学习相关的攻击模式。最后，我们强调了传统评估过程中可能存在的数据泄漏问题，并提出了建议。

    The last decades have seen a growth in the number of cyber-attacks with severe economic and privacy damages, which reveals the need for network intrusion detection approaches to assist in preventing cyber-attacks and reducing their risks. In this work, we propose a novel network representation as a graph of flows that aims to provide relevant topological information for the intrusion detection task, such as malicious behavior patterns, the relation between phases of multi-step attacks, and the relation between spoofed and pre-spoofed attackers activities. In addition, we present a Graph Neural Network (GNN) based framework responsible for exploiting the proposed graph structure to classify communication flows by assigning them a maliciousness score. The framework comprises three main steps that aim to embed nodes features and learn relevant attack patterns from the network representation. Finally, we highlight a potential data leakage issue with classical evaluation procedures and sugg
    
[^72]: 使用贝叶斯推断在自动驾驶感知中降低误报率

    Reducing the False Positive Rate Using Bayesian Inference in Autonomous Driving Perception. (arXiv:2310.05951v1 [cs.CV])

    [http://arxiv.org/abs/2310.05951](http://arxiv.org/abs/2310.05951)

    本研究提出了一种使用贝叶斯推断的方法来降低自动驾驶感知中的误报率。通过使用多感知和多模态方法，以及考虑目标的似然函数和先验概率，该方法在减少误报率方面取得了显著的进展。在KITTI数据集上进行的实验证明了该方法的有效性。

    

    目标识别是自动和智能车辆感知系统中关键的一步，这已经得到了大量的研究工作的证明。本文通过使用多感知和多模态方法来探索目标识别，旨在降低误报率（FPR）。由于误报对象的错误分类可能导致事故，降低误报率在感知系统中变得越来越重要。具体来说，本文提出了一种使用贝叶斯推断的策略来降低误报率，其中将似然函数视为高斯核密度估计的累积分布函数，将先验概率视为归一化直方图的累积函数。所提出的方法在KITTI数据集上使用深度网络（DenseNet、NasNet和EfficientNet）和最近的3D点云网络（PointNet和PointNet++）进行验证，考虑了三个目标类别（汽车、自行车、行人）。

    Object recognition is a crucial step in perception systems for autonomous and intelligent vehicles, as evidenced by the numerous research works in the topic. In this paper, object recognition is explored by using multisensory and multimodality approaches, with the intention of reducing the false positive rate (FPR). The reduction of the FPR becomes increasingly important in perception systems since the misclassification of an object can potentially cause accidents. In particular, this work presents a strategy through Bayesian inference to reduce the FPR considering the likelihood function as a cumulative distribution function from Gaussian kernel density estimations, and the prior probabilities as cumulative functions of normalized histograms. The validation of the proposed methodology is performed on the KITTI dataset using deep networks (DenseNet, NasNet, and EfficientNet), and recent 3D point cloud networks (PointNet, and PintNet++), by considering three object-categories (cars, cyc
    
[^73]: 从零开始使用多智能体强化学习学习网络防御战术

    Learning Cyber Defence Tactics from Scratch with Multi-Agent Reinforcement Learning. (arXiv:2310.05939v1 [cs.CR])

    [http://arxiv.org/abs/2310.05939](http://arxiv.org/abs/2310.05939)

    学习网络防御战术的新方法，使用多智能体强化学习来进行自主网络防御，证明了合作多智能体强化学习能够对抗各种威胁。

    

    深度学习技术的最新进展为自主网络防御解决方案的设计开辟了新的可能性。在计算机网络防御角色中，智能代理团队可能展现出保护网络和动能资产的有希望的途径。在模拟的游戏环境中，代理根据其在主机防御场景中共同减轻攻击者活动的能力进行评估。守方系统会面对能破坏网络机密性、完整性和可用性的启发式攻击者。对比了基于价值的独立学习和集中培训分散执行的合作多智能体强化学习方法，发现这两种方法都胜过简单的多智能体启发式防御者。这项工作表明，合作多智能体强化学习能够学习有效的网络防御战术，以抵御各种威胁。

    Recent advancements in deep learning techniques have opened new possibilities for designing solutions for autonomous cyber defence. Teams of intelligent agents in computer network defence roles may reveal promising avenues to safeguard cyber and kinetic assets. In a simulated game environment, agents are evaluated on their ability to jointly mitigate attacker activity in host-based defence scenarios. Defender systems are evaluated against heuristic attackers with the goals of compromising network confidentiality, integrity, and availability. Value-based Independent Learning and Centralized Training Decentralized Execution (CTDE) cooperative Multi-Agent Reinforcement Learning (MARL) methods are compared revealing that both approaches outperform a simple multi-agent heuristic defender. This work demonstrates the ability of cooperative MARL to learn effective cyber defence tactics against varied threats.
    
[^74]: 组件注意力网络用于多模态舞蹈即兴识别

    Component attention network for multimodal dance improvisation recognition. (arXiv:2310.05938v1 [cs.CV])

    [http://arxiv.org/abs/2310.05938](http://arxiv.org/abs/2310.05938)

    本研究提出了一种用于多模态舞蹈即兴识别的组件注意力网络 (CANet)。通过多层次的融合方法，并通过实验分析每种模态的影响，我们证明了该模型相比基准方法具有更好的性能。

    

    舞蹈即兴是艺术领域中的一个活跃研究课题。由于其独特的动态特性，即兴舞蹈的动作分析可能具有挑战性。数据驱动的舞蹈动作分析，包括识别和生成，通常限于骨骼数据。然而，其他模态的数据，如音频，可以被记录并在下游任务中产生好处。本文探讨了在舞蹈即兴背景下应用和性能的多模态融合方法用于人体动作识别。我们提出了一种基于注意力的模型，组件注意力网络（CANet），用于三个层次的多模态融合：1）CANet进行特征融合，2）CANet和图卷积网络（GCN）进行模型融合，以及3）采用投票策略进行后期融合。我们进行了全面的实验，以分析不同融合方法中每种模态的影响，并区分出关键的时间或组件特征。我们展示了我们提出的模型优于两种基准方法，证明了其潜力。

    Dance improvisation is an active research topic in the arts. Motion analysis of improvised dance can be challenging due to its unique dynamics. Data-driven dance motion analysis, including recognition and generation, is often limited to skeletal data. However, data of other modalities, such as audio, can be recorded and benefit downstream tasks. This paper explores the application and performance of multimodal fusion methods for human motion recognition in the context of dance improvisation. We propose an attention-based model, component attention network (CANet), for multimodal fusion on three levels: 1) feature fusion with CANet, 2) model fusion with CANet and graph convolutional network (GCN), and 3) late fusion with a voting strategy. We conduct thorough experiments to analyze the impact of each modality in different fusion methods and distinguish critical temporal or component features. We show that our proposed model outperforms the two baseline methods, demonstrating its potenti
    
[^75]: DF-3DFace: 一对多语音同步的3D面部动画，基于扩散模型

    DF-3DFace: One-to-Many Speech Synchronized 3D Face Animation with Diffusion. (arXiv:2310.05934v1 [cs.CV])

    [http://arxiv.org/abs/2310.05934](http://arxiv.org/abs/2310.05934)

    DF-3DFace是一种采用扩散模型的语音到3D面部网格合成方法，能够准确同步唇部动作，并综合了身份、姿势和面部运动。

    

    受到其根据语音创建逼真和表达丰富的3D面部动画能力的影响，由语音驱动的3D面部动画已经引起了相当大的关注。基于学习的方法在实现与语音同步的准确面部动作方面取得了显著进展。然而，语音到3D面部综合的一对多特性尚未得到充分探索：虽然唇部与语音内容准确同步，但与语音相关运动之外的其他面部属性在语音方面是可变的。为了解决单一语音中面部属性潜在的差异，我们提出了DF-3DFace，一种基于扩散模型的语音到3D面部网格合成方法。DF-3DFace根据扩散模型捕捉了语音和3D面部之间复杂的一对多关系。它通过利用音频-网格同步和掩蔽条件同时实现了对齐的唇部动作。此外，所提出的方法还共同建模身份和姿势，除了面部运动之外。

    Speech-driven 3D facial animation has gained significant attention for its ability to create realistic and expressive facial animations in 3D space based on speech. Learning-based methods have shown promising progress in achieving accurate facial motion synchronized with speech. However, one-to-many nature of speech-to-3D facial synthesis has not been fully explored: while the lip accurately synchronizes with the speech content, other facial attributes beyond speech-related motions are variable with respect to the speech. To account for the potential variance in the facial attributes within a single speech, we propose DF-3DFace, a diffusion-driven speech-to-3D face mesh synthesis. DF-3DFace captures the complex one-to-many relationships between speech and 3D face based on diffusion. It concurrently achieves aligned lip motion by exploiting audio-mesh synchronization and masked conditioning. Furthermore, the proposed method jointly models identity and pose in addition to facial motions 
    
[^76]: 基于多智能体系统的奶牛养殖领域点对点能源交易方法

    A Multi-Agent Systems Approach for Peer-to-Peer Energy Trading in Dairy Farming. (arXiv:2310.05932v1 [cs.MA])

    [http://arxiv.org/abs/2310.05932](http://arxiv.org/abs/2310.05932)

    提出了基于多智能体系统的奶牛养殖领域点对点能源交易方法，通过该方法可以降低电力成本和峰值需求，同时增加能源销售。

    

    为了实现所需的碳排放减少目标，整合可再生能源和加快点对点能源交易的采用至关重要。这对于能源密集型的奶牛养殖业尤为重要。然而，整合可再生能源和点对点交易面临一些挑战。为了解决这个问题，我们提出了基于多智能体的点对点奶牛养殖场能源模拟器(MAPDES)，使奶牛场能够参与点对点市场。与没有点对点交易的基准情景相比，我们的策略将电力成本和峰值需求分别降低了约30%和24%，同时能源销售增加了37%。这证明了我们方法的有效性。

    To achieve desired carbon emission reductions, integrating renewable generation and accelerating the adoption of peer-to-peer energy trading is crucial. This is especially important for energy-intensive farming, like dairy farming. However, integrating renewables and peer-to-peer trading presents challenges. To address this, we propose the Multi-Agent Peer-to-Peer Dairy Farm Energy Simulator (MAPDES), enabling dairy farms to participate in peer-to-peer markets. Our strategy reduces electricity costs and peak demand by approximately 30% and 24% respectively, while increasing energy sales by 37% compared to the baseline scenario without P2P trading. This demonstrates the effectiveness of our approach.
    
[^77]: 基于深度学习的番茄病害检测与治疗建议的手机应用

    Deep Learning based Tomato Disease Detection and Remedy Suggestions using Mobile Application. (arXiv:2310.05929v1 [cs.CV])

    [http://arxiv.org/abs/2310.05929](http://arxiv.org/abs/2310.05929)

    我们开发了一款基于深度学习的移动应用程序，利用人工智能技术来检测番茄病害并提供治疗建议，以帮助尼泊尔农民解决传统农业方法和农作物病害的问题。

    

    我们开发了一套全面的计算机系统，用于帮助实行传统农业方法并且对农作物病害的农民缺乏农业专家咨询的问题。我们的系统利用人工智能技术来识别和提供蔬菜病害的治疗建议。为了确保易用性，我们开发了一个移动应用程序，提供用户友好的界面，使农民可以用他们当地的语言查询蔬菜病害并获得合适的解决方案。该系统适用于只具备基本智能手机使用知识的任何农民。具体而言，我们设计了一款基于人工智能技术的移动应用程序，用于识别和建议番茄病害的治疗方案，以惠及尼泊尔当地的农民群体。我们的系统采用了目标检测的最新技术You Only Look Once (YOLO)来检测番茄病害。然后将检测到的信息传递给移动应用程序。

    We have developed a comprehensive computer system to assist farmers who practice traditional farming methods and have limited access to agricultural experts for addressing crop diseases. Our system utilizes artificial intelligence (AI) to identify and provide remedies for vegetable diseases. To ensure ease of use, we have created a mobile application that offers a user-friendly interface, allowing farmers to inquire about vegetable diseases and receive suitable solutions in their local language. The developed system can be utilized by any farmer with a basic understanding of a smartphone. Specifically, we have designed an AI-enabled mobile application for identifying and suggesting remedies for vegetable diseases, focusing on tomato diseases to benefit the local farming community in Nepal. Our system employs state-of-the-art object detection methodology, namely You Only Look Once (YOLO), to detect tomato diseases. The detected information is then relayed to the mobile application, whic
    
[^78]: 通过基于文本的分解解释CLIP图像表示

    Interpreting CLIP's Image Representation via Text-Based Decomposition. (arXiv:2310.05916v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.05916](http://arxiv.org/abs/2310.05916)

    本文通过解析CLIP图像编码器的组件，揭示了图像表示的构成方式，并利用文本表示解释了其各个部分的作用。通过理解注意力头和图像块，作者实现了对模型的修复和改进，包括消除误特征和构建零样本图像分割器等方面。

    

    本文通过分析个别模型组件对最终表示的影响，探讨了CLIP图像编码器。我们将图像表示分解为各个图像块、模型层和注意力头的求和，并使用CLIP的文本表示来解释这些求和项。通过解释注意力头，我们通过自动寻找能够跨越输出空间的文本表示来表征每个头的作用，揭示出许多头的特定属性角色（例如位置或形状）。接下来，通过解释图像块，我们揭示了CLIP中的紧密空间定位。最后，我们利用这一理解消除了CLIP中的误特征，并创建了一个强大的零样本图像分割器。我们的结果表明，可扩展的对Transformer模型的理解是可实现的，并可用于修复和改进模型。

    We investigate the CLIP image encoder by analyzing how individual model components affect the final representation. We decompose the image representation as a sum across individual image patches, model layers, and attention heads, and use CLIP's text representation to interpret the summands. Interpreting the attention heads, we characterize each head's role by automatically finding text representations that span its output space, which reveals property-specific roles for many heads (e.g. location or shape). Next, interpreting the image patches, we uncover an emergent spatial localization within CLIP. Finally, we use this understanding to remove spurious features from CLIP and to create a strong zero-shot image segmenter. Our results indicate that a scalable understanding of transformer models is attainable and can be used to repair and improve models.
    
[^79]: 面向多模型大语言模型的细粒度音频-视觉联合表示

    Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models. (arXiv:2310.05863v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2310.05863](http://arxiv.org/abs/2310.05863)

    本文提出了一种面向多模型LLM的细粒度音频-视觉联合表示学习框架(FAVOR)，该框架能够同时感知音频和视觉输入流，并通过因果关注模块捕获音频-视觉帧的因果关系。还提出了一个音频-视觉评估基准(AVEB)用于评估该模型的性能。

    

    音频-视觉大型语言模型(LLM)引起了重要关注，但对于输入流的细粒度组合却未得到充分探讨，这对于LLM理解一般视频输入是具有挑战性但必要的。为此，本文提出了一种面向多模型LLM的细粒度音频-视觉联合表示(FAVOR)学习框架，该框架将基于文本的LLM扩展到能够同时感知音频输入流中的语音和音频事件以及视觉输入流中的图像或视频，帧级别地。我们提出了一种因果Q-Former结构，配合因果关注模块，将音频和视觉特征流融合到联合表示中，并将联合空间与LLM输入嵌入空间对齐，以增强对音频-视觉帧在时间上的因果关系捕获。还提出了一种音频-视觉评估基准(AVEB)，其中包括六个代表性的单模态任务和五个反映音频-视觉的跨模态任务。

    Audio-visual large language models (LLM) have drawn significant attention, yet the fine-grained combination of both input streams is rather under-explored, which is challenging but necessary for LLMs to understand general video inputs. To this end, a fine-grained audio-visual joint representation (FAVOR) learning framework for multimodal LLMs is proposed in this paper, which extends a text-based LLM to simultaneously perceive speech and audio events in the audio input stream and images or videos in the visual input stream, at the frame level. To fuse the audio and visual feature streams into joint representations and to align the joint space with the LLM input embedding space, we propose a causal Q-Former structure with a causal attention module to enhance the capture of causal relations of the audio-visual frames across time. An audio-visual evaluation benchmark (AVEB) is also proposed which comprises six representative single-modal tasks with five cross-modal tasks reflecting audio-v
    
[^80]: ParFam - 基于连续全局优化的符号回归

    ParFam -- Symbolic Regression Based on Continuous Global Optimization. (arXiv:2310.05537v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.05537](http://arxiv.org/abs/2310.05537)

    ParFam是一种新的符号回归方法，利用参数化的符号函数族将离散问题转化为连续问题，并结合全局优化器，能够有效解决符号回归问题。

    

    符号回归（SR）问题在许多不同的应用中出现，比如从给定数据中识别物理定律或推导描述金融市场行为的数学方程。目前存在多种解决SR问题的方法，通常基于遗传编程。然而，这些方法通常非常复杂，需要大量超参数调整和计算资源。本文介绍了我们提出的新方法ParFam，它利用适合的符号函数的参数化族将离散的符号回归问题转化为连续问题，相比当前最先进的方法，这种方法的设置更加直观。结合强大的全局优化器，这种方法可以有效地解决SR问题。此外，它可以轻松扩展到更高级的算法，例如添加深度神经网络以找到适合的参数化族。我们证明了这种方法的性能。

    The problem of symbolic regression (SR) arises in many different applications, such as identifying physical laws or deriving mathematical equations describing the behavior of financial markets from given data. Various methods exist to address the problem of SR, often based on genetic programming. However, these methods are usually quite complicated and require a lot of hyperparameter tuning and computational resources. In this paper, we present our new method ParFam that utilizes parametric families of suitable symbolic functions to translate the discrete symbolic regression problem into a continuous one, resulting in a more straightforward setup compared to current state-of-the-art methods. In combination with a powerful global optimizer, this approach results in an effective method to tackle the problem of SR. Furthermore, it can be easily extended to more advanced algorithms, e.g., by adding a deep neural network to find good-fitting parametric families. We prove the performance of 
    
[^81]: 通过基于Transformer的强化学习进行分子的全新设计

    Molecular De Novo Design through Transformer-based Reinforcement Learning. (arXiv:2310.05365v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05365](http://arxiv.org/abs/2310.05365)

    本文提出了一种基于Transformer的强化学习方法，通过精细调整生成模型，能够在分子的全新设计中生成具有所需性质的分子结构，展现出优越的性能。

    

    本文介绍了一种通过精细调整基于Transformer的生成模型用于分子的全新设计的方法。利用Transformer相对于循环神经网络（RNN）的优越序列学习能力，我们的模型可以有效地生成具有所需性质的分子结构。与传统的基于RNN的模型相比，我们提出的方法在生成预测对多种生物靶点具有活性的化合物方面表现出卓越性能，捕捉了分子结构序列的长期依赖性。该模型的有效性在许多任务中得到了证明，包括生成与查询结构类似的分子和生成具有特定属性的化合物，在性能上优于基线的基于RNN的方法。我们的方法可以用于桥接化学、从单个分子开始扩展库，并生成具有高预测活性的化合物。

    In this work, we introduce a method to fine-tune a Transformer-based generative model for molecular de novo design. Leveraging the superior sequence learning capacity of Transformers over Recurrent Neural Networks (RNNs), our model can generate molecular structures with desired properties effectively. In contrast to the traditional RNN-based models, our proposed method exhibits superior performance in generating compounds predicted to be active against various biological targets, capturing long-term dependencies in the molecular structure sequence. The model's efficacy is demonstrated across numerous tasks, including generating analogues to a query structure and producing compounds with particular attributes, outperforming the baseline RNN-based methods. Our approach can be used for scaffold hopping, library expansion starting from a single molecule, and generating compounds with high predicted activity against biological targets.
    
[^82]: 通过迭代地融合模态相似路径实现通用的多模态实体对齐

    Universal Multi-modal Entity Alignment via Iteratively Fusing Modality Similarity Paths. (arXiv:2310.05364v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05364](http://arxiv.org/abs/2310.05364)

    本研究提出了一种通过迭代融合模态相似路径实现通用的多模态实体对齐方法。通过统一建模和有效信息融合，解决了现有方法中模态建模不一致和低效以及模态融合效果不佳的问题。

    

    实体对齐的目标是从多个知识图谱中确定等价的实体对，并创建一个更全面和统一的知识图谱。大多数实体对齐方法主要关注知识图谱的结构模态，缺乏对多模态信息的探索。少数多模态实体对齐方法在这个领域做出了不错的尝试。然而，它们存在两个缺点：(1)模态建模不一致且低效，为每个模态设计复杂和独立的模型；(2)由于实体对齐中模态的异构性，模态融合效果不佳。为了解决这些挑战，我们提出了PathFusion，它包括两个主要部分：(1) MSP，一个统一的建模方法，通过构建连接实体和模态节点以表示多个模态的路径，简化了对齐过程；(2) IRF，一种迭代融合方法，使用路径作为信息载体，有效地将不同模态的信息结合起来。

    The objective of Entity Alignment (EA) is to identify equivalent entity pairs from multiple Knowledge Graphs (KGs) and create a more comprehensive and unified KG. The majority of EA methods have primarily focused on the structural modality of KGs, lacking exploration of multi-modal information. A few multi-modal EA methods have made good attempts in this field. Still, they have two shortcomings: (1) inconsistent and inefficient modality modeling that designs complex and distinct models for each modality; (2) ineffective modality fusion due to the heterogeneous nature of modalities in EA. To tackle these challenges, we propose PathFusion, consisting of two main components: (1) MSP, a unified modeling approach that simplifies the alignment process by constructing paths connecting entities and modality nodes to represent multiple modalities; (2) IRF, an iterative fusion method that effectively combines information from different modalities using the path as an information carrier. Experim
    
[^83]: 对语义分割中经典的测试时适应方法的批判性探究

    A Critical Look at Classic Test-Time Adaptation Methods in Semantic Segmentation. (arXiv:2310.05341v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.05341](http://arxiv.org/abs/2310.05341)

    这项研究对语义分割中的经典测试时适应方法进行了批判性探究，揭示了分割TTA所面临的独特挑战，并发现经典TTA策略在这一任务中并不有效。

    

    测试时适应（TTA）旨在将最初在训练数据上训练的模型适应于测试数据中的可能分布变化。然而，大多数现有的TTA研究都集中在分类任务上，对于语义分割的TTA探索非常有限。这种对分类的突出重视可能导致许多新手和工程师错误地认为为分类设计的经典TTA方法可以直接应用于分割任务。然而，这一假设仍未经验证，是一个待解决的问题。为了解决这个问题，我们进行了一项系统的实证研究，揭示了分割TTA的独特挑战，并确定经典TTA策略是否可以有效应对这一任务。我们全面的结果得出了三个关键观察结果。首先，常用于分类TTA的经典批归一化更新策略只能带来轻微的性能改善，在某些情况下甚至会对结果产生逆向影响。

    Test-time adaptation (TTA) aims to adapt a model, initially trained on training data, to potential distribution shifts in the test data. Most existing TTA studies, however, focus on classification tasks, leaving a notable gap in the exploration of TTA for semantic segmentation. This pronounced emphasis on classification might lead numerous newcomers and engineers to mistakenly assume that classic TTA methods designed for classification can be directly applied to segmentation. Nonetheless, this assumption remains unverified, posing an open question. To address this, we conduct a systematic, empirical study to disclose the unique challenges of segmentation TTA, and to determine whether classic TTA strategies can effectively address this task. Our comprehensive results have led to three key observations. First, the classic batch norm updating strategy, commonly used in classification TTA, only brings slight performance improvement, and in some cases it might even adversely affect the resu
    
[^84]: 在心理健康领域中通过任务自适应分词来增强长文本生成

    Enhancing Long-form Text Generation in Mental Health with Task-adaptive Tokenization. (arXiv:2310.05317v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05317](http://arxiv.org/abs/2310.05317)

    该论文提出了一种任务自适应分词的方法，通过优化分词过程来增强在心理健康领域中的长文本生成。实验证明，该方法在减少标记数量的情况下显著提高了生成性能，并且可与大型语言模型结合使用。

    

    我们提出了任务自适应分词作为一种方式，将生成流水线适应于下游任务的特定要求，并增强在心理健康领域的长文本生成。受认知科学的启发，我们的任务自适应分词器从多个结果中采样可变的分段，采样概率基于任务特定的数据进行优化。我们引入了一种构建专用词汇的策略，并介绍了一种词汇合并协议，可以将任务特定的标记整合到预训练模型的分词步骤中。通过对中英文心理问答任务进行广泛实验，我们发现我们的任务自适应分词方法在使用更少的标记的情况下带来了显著的生成性能提升，最高可达60%。初步实验表明，使用我们的分词方法与非常大的语言模型结合能够得到有希望的结果。

    We propose task-adaptive tokenization as a way to adapt the generation pipeline to the specifics of a downstream task and enhance long-form generation in mental health. Inspired by insights from cognitive science, our task-adaptive tokenizer samples variable segmentations from multiple outcomes, with sampling probabilities optimized based on task-specific data. We introduce a strategy for building a specialized vocabulary and introduce a vocabulary merging protocol that allows for the integration of task-specific tokens into the pre-trained model's tokenization step. Through extensive experiments on psychological question-answering tasks in both Chinese and English, we find that our task-adaptive tokenization approach brings a significant improvement in generation performance while using up to 60% fewer tokens. Preliminary experiments point to promising results when using our tokenization approach with very large language models.
    
[^85]: Persis: 使用卷积神经网络的波斯字体识别流水线

    Persis: A Persian Font Recognition Pipeline Using Convolutional Neural Networks. (arXiv:2310.05255v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.05255](http://arxiv.org/abs/2310.05255)

    本文介绍了一种使用卷积神经网络的波斯字体识别流水线，通过对新数据集和其他数据集的测试，结果表明所提出的方法具有较高的准确率和较快的处理速度，可以无需额外的预处理步骤直接识别波斯字体。

    

    如果我们遇到一个适合我们设计工作的字体，但不知道它的名字会怎样？视觉字体识别（VFR）系统用于识别图像中的字体类型。这些系统可以帮助平面设计师识别图像中使用的字体。VFR系统还有助于提高光学字符识别（OCR）系统的速度和准确性。在这篇论文中，我们介绍了波斯字体识别领域中首次公开可用的数据集，并使用卷积神经网络（CNN）来解决这个问题。结果表明，所提出的流水线在我们的新数据集上获得了78.0%的top-1准确率，在IDPL-PFOD数据集上为89.1%，在KAFD数据集上为94.5%。此外，我们提出的数据集中每个样本在整个流水线中的平均时间消耗为CPU0.54秒和GPU0.017秒。我们得出结论，CNN方法可以用于识别波斯字体，无需额外的预处理步骤，如特征提取。

    What happens if we encounter a suitable font for our design work but do not know its name? Visual Font Recognition (VFR) systems are used to identify the font typeface in an image. These systems can assist graphic designers in identifying fonts used in images. A VFR system also aids in improving the speed and accuracy of Optical Character Recognition (OCR) systems. In this paper, we introduce the first publicly available datasets in the field of Persian font recognition and employ Convolutional Neural Networks (CNN) to address this problem. The results show that the proposed pipeline obtained 78.0% top-1 accuracy on our new datasets, 89.1% on the IDPL-PFOD dataset, and 94.5% on the KAFD dataset. Furthermore, the average time spent in the entire pipeline for one sample of our proposed datasets is 0.54 and 0.017 seconds for CPU and GPU, respectively. We conclude that CNN methods can be used to recognize Persian fonts without the need for additional pre-processing steps such as feature ex
    
[^86]: ChatRadio-Valuer: 一种基于多机构和多系统数据的通用放射学报告自动生成的聊天大型语言模型

    ChatRadio-Valuer: A Chat Large Language Model for Generalizable Radiology Report Generation Based on Multi-institution and Multi-system Data. (arXiv:2310.05242v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05242](http://arxiv.org/abs/2310.05242)

    ChatRadio-Valuer是一种基于大型语言模型的定制模型，用于通用放射学报告自动生成。它能够学习可泛化的表示并为模型自适应提供基础模式，解决了放射学报告中的泛化能力和异质性问题。

    

    放射学报告生成是医学图像分析的关键步骤，对临床决策水平的定量分析至关重要。然而，具有交叉来源异质性的复杂多样的放射学报告对当前方法在大规模数据量下的泛化能力提出了巨大挑战，主要是因为放射学报告的风格和规范性在机构、检查部位和放射科医生之间明显有所不同。最近，大型语言模型（LLM）的出现为识别健康状况的迹象提供了巨大的潜力。为了解决上述问题，我们与中国的湘雅二医院合作，提出了基于LLM的ChatRadio-Valuer，这是一种专门用于自动化放射学报告生成的定制模型，它可以学习可泛化的表示并为复杂分析师案例中的模型自适应提供基本模式。

    Radiology report generation, as a key step in medical image analysis, is critical to the quantitative analysis of clinically informed decision-making levels. However, complex and diverse radiology reports with cross-source heterogeneity pose a huge generalizability challenge to the current methods under massive data volume, mainly because the style and normativity of radiology reports are obviously distinctive among institutions, body regions inspected and radiologists. Recently, the advent of large language models (LLM) offers great potential for recognizing signs of health conditions. To resolve the above problem, we collaborate with the Second Xiangya Hospital in China and propose ChatRadio-Valuer based on the LLM, a tailored model for automatic radiology report generation that learns generalizable representations and provides a basis pattern for model adaptation in sophisticated analysts' cases. Specifically, ChatRadio-Valuer is trained based on the radiology reports from a single 
    
[^87]: TILFA: 一种用于论证挖掘中文本、图像和布局融合的统一框架

    TILFA: A Unified Framework for Text, Image, and Layout Fusion in Argument Mining. (arXiv:2310.05210v1 [cs.AI] CROSS LISTED)

    [http://arxiv.org/abs/2310.05210](http://arxiv.org/abs/2310.05210)

    TILFA是一个用于论证挖掘中处理文本、图像和布局混合数据的统一框架，不仅能够理解文本，还能够检测光学字符和识别图像中的布局细节，并在辩论立场分类中取得了显著的性能提升。

    

    论证挖掘的主要目标是分析作者的立场。与以往只关注文本的论证挖掘数据集不同，第10届论证挖掘研讨会的共享任务引入了一个包含文本和图像的数据集。重要的是，这些图像包含了视觉元素和光学字符。我们的新框架TILFA（一种用于论证挖掘中文本、图像和布局融合的统一框架）被设计用于处理这种混合数据。它不仅能够理解文本，还能够检测光学字符和识别图像中的布局细节。我们的模型明显优于现有的基准线，在这个共享任务的辩论立场分类子任务中，我们的团队KnowComp在排行榜上获得了第一名。

    A main goal of Argument Mining (AM) is to analyze an author's stance. Unlike previous AM datasets focusing only on text, the shared task at the 10th Workshop on Argument Mining introduces a dataset including both text and images. Importantly, these images contain both visual elements and optical characters. Our new framework, TILFA (A Unified Framework for Text, Image, and Layout Fusion in Argument Mining), is designed to handle this mixed data. It excels at not only understanding text but also detecting optical characters and recognizing layout details in images. Our model significantly outperforms existing baselines, earning our team, KnowComp, the 1st place in the leaderboard of Argumentative Stance Classification subtask in this shared task.
    
[^88]: 大型语言模型时代中的事实性挑战

    Factuality Challenges in the Era of Large Language Models. (arXiv:2310.05189v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05189](http://arxiv.org/abs/2310.05189)

    大型语言模型存在生成虚假、错误或误导内容的问题，同时也面临被恶意应用的风险。本研究探讨了需要从事实核查员、新闻机构和研究与政策界采取的技术创新、监管改革和人工智能素养倡议，以解决这些问题。

    

    基于大型语言模型（LLMs）的工具的出现，如OpenAI的ChatGPT，微软的Bing Chat和谷歌的Bard，已经引起了巨大的公众关注。这些非常有用、自然的工具标志着自然语言生成方面的重大进展，然而它们存在生成虚假、错误或误导内容的倾向，通常被称为“幻觉”。此外，LLMs可以被用于恶意应用，例如在规模上生成虚假但可信的内容和个人资料。这对于社会来说构成了重大挑战，因为它可能欺骗用户并越来越多地传播不准确的信息。鉴于这些风险，我们探讨了事实核查员、新闻机构和更广泛的研究和政策界需要的技术创新、监管改革和人工智能素养倡议的类型。通过确定风险、迫在眉睫的威胁和一些可行的解决方案，我们希望解决这个问题。

    The emergence of tools based on Large Language Models (LLMs), such as OpenAI's ChatGPT, Microsoft's Bing Chat, and Google's Bard, has garnered immense public attention. These incredibly useful, natural-sounding tools mark significant advances in natural language generation, yet they exhibit a propensity to generate false, erroneous, or misleading content -- commonly referred to as "hallucinations." Moreover, LLMs can be exploited for malicious applications, such as generating false but credible-sounding content and profiles at scale. This poses a significant challenge to society in terms of the potential deception of users and the increasing dissemination of inaccurate information. In light of these risks, we explore the kinds of technological innovations, regulatory reforms, and AI literacy initiatives needed from fact-checkers, news organizations, and the broader research and policy communities. By identifying the risks, the imminent threats, and some viable solutions, we seek to she
    
[^89]: Hieros: 基于结构化状态空间序列的分层想像模型

    Hieros: Hierarchical Imagination on Structured State Space Sequence World Models. (arXiv:2310.05167v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.05167](http://arxiv.org/abs/2310.05167)

    Hieros是一种基于结构化状态空间序列的分层想像模型，通过学习时间抽象的世界表示并在潜在空间中多个时间尺度上想象轨迹，实现了更高效的训练和想象。

    

    现代深度强化学习（DRL）算法面临的最大挑战之一是样本效率。许多方法通过学习世界模型，在想象中完全训练代理，消除了在训练期间直接与环境进行交互的需求。然而，这些方法通常面临想像准确性、探索能力或运行效率的问题。我们提出了Hieros，一种分层策略，它学习时间抽象的世界表示，并在潜在空间中多个时间尺度上想象轨迹。Hieros使用基于S5层的世界模型，它在训练期间并行预测下一个世界状态，并在环境交互期间进行迭代预测。由于S5层的特殊性质，我们的方法可以在想象过程中并行训练和迭代预测下一个世界状态。这比基于RNN的世界模型具有更高的训练效率，也比基于Transformer的世界模型具有更高的想象效率。

    One of the biggest challenges to modern deep reinforcement learning (DRL) algorithms is sample efficiency. Many approaches learn a world model in order to train an agent entirely in imagination, eliminating the need for direct environment interaction during training. However, these methods often suffer from either a lack of imagination accuracy, exploration capabilities, or runtime efficiency. We propose Hieros, a hierarchical policy that learns time abstracted world representations and imagines trajectories at multiple time scales in latent space. Hieros uses an S5 layer-based world model, which predicts next world states in parallel during training and iteratively during environment interaction. Due to the special properties of S5 layers, our method can train in parallel and predict next world states iteratively during imagination. This allows for more efficient training than RNN-based world models and more efficient imagination than Transformer-based world models.  We show that our 
    
[^90]: InstructDET: 通用指令的引导下的指称对象检测的多样化方法

    InstructDET: Diversifying Referring Object Detection with Generalized Instructions. (arXiv:2310.05136v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.05136](http://arxiv.org/abs/2310.05136)

    我们提出了一种名为InstructDET的方法，可以通过多样化的指令定位目标对象并进行指称对象检测。我们构建了一个包含图像、边界框和泛化指令的数据集，其中利用了视觉语言模型和大型语言模型生成指令。

    

    我们提出了InstructDET，一种基于用户指令来定位目标对象的指称对象检测（ROD）的数据中心方法。我们利用了多样化的指令，涵盖与对象检测相关的常见用户意图。对于一张图像，我们生成了大量的指令，涉及每个单独的对象和多个对象的不同组合。每个指令及其对应的对象边界框构成一个训练数据对。为了包含常见的检测表达式，我们采用了新兴的视觉语言模型（VLM）和大型语言模型（LLM），通过文本提示和对象边界框生成指令，因为基础模型的泛化能力可以产生类似人类的表达（例如，描述对象属性、类别和关系）。我们将构建的数据集命名为InDET，包含图像、边界框和泛化指令。

    We propose InstructDET, a data-centric method for referring object detection (ROD) that localizes target objects based on user instructions. While deriving from referring expressions (REC), the instructions we leverage are greatly diversified to encompass common user intentions related to object detection. For one image, we produce tremendous instructions that refer to every single object and different combinations of multiple objects. Each instruction and its corresponding object bounding boxes (bbxs) constitute one training data pair. In order to encompass common detection expressions, we involve emerging vision-language model (VLM) and large language model (LLM) to generate instructions guided by text prompts and object bbxs, as the generalizations of foundation models are effective to produce human-like expressions (e.g., describing object property, category, and relationship). We name our constructed dataset as InDET. It contains images, bbxs and generalized instructions that are 
    
[^91]: DialCoT遇到了PPO：在较小的语言模型中分解和探索推理路径

    DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models. (arXiv:2310.05074v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05074](http://arxiv.org/abs/2310.05074)

    DialCoT是一种对话引导的链式思维方法，用于在较小的语言模型中分解和探索推理路径。通过将复杂问题分解为简单的子问题，它降低了任务难度，并使用PPO算法优化模型的推理路径选择。

    

    链式思维（CoT）提示已经被证明有助于增强至少具有1000亿参数的大型语言模型（LLMs）的推理能力。然而，当应用于具有不到100亿参数的较小语言模型（SLMs）的推理任务时，它是无效甚至有害的。为了解决这个限制，我们引入了对话引导的链式思维（DialCoT），它采用对话格式生成中间推理步骤，引导模型朝着最终答案前进。此外，我们使用PPO算法优化模型的推理路径选择，进一步增强其推理能力。我们的方法与以前的方法相比具有几个优点。首先，我们通过将解决复杂推理问题的过程分解成一系列更简单的子问题，显著降低了任务的难度，使其更适合于较小的语言模型。其次，我们优化了模型的推理路径选择，使其更准确和高效。

    Chain-of-Thought (CoT) prompting has proven to be effective in enhancing the reasoning capabilities of Large Language Models (LLMs) with at least 100 billion parameters. However, it is ineffective or even detrimental when applied to reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion parameters. To address this limitation, we introduce Dialogue-guided Chain-of-Thought (DialCoT) which employs a dialogue format to generate intermediate reasoning steps, guiding the model toward the final answer. Additionally, we optimize the model's reasoning path selection using the Proximal Policy Optimization (PPO) algorithm, further enhancing its reasoning capabilities. Our method offers several advantages compared to previous approaches. Firstly, we transform the process of solving complex reasoning questions by breaking them down into a series of simpler sub-questions, significantly reducing the task difficulty and making it more suitable for SLMs. Secondly, we optimize the m
    
[^92]: 从文本到策略：评估在Avalon游戏中发挥作用的LLMs

    From Text to Tactic: Evaluating LLMs Playing the Game of Avalon. (arXiv:2310.05036v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.05036](http://arxiv.org/abs/2310.05036)

    本文研究了在Avalon游戏中使用LLMs的潜力，并引入了AvalonBench来评估多代理LLM代理。实验证明存在明显的能力差距。

    

    本文探讨了大型语言模型（LLM）在玩策略社交推理游戏Resistance Avalon中的潜力。Avalon玩家不仅需要根据动态发展的游戏阶段做出明智的决策，还需要参与讨论，在讨论中必须欺骗、推理和与其他玩家进行谈判。这些特点使得Avalon成为研究LLM代理的决策和语言处理能力的有趣试验平台。为了推动这一研究领域的发展，我们引入了AvalonBench——一个专门用于评估多代理LLM代理的全面游戏环境。该基准测试包括：（1）Avalon的游戏环境，（2）基于规则的机器人作为基准对手，以及（3）针对每个角色具有定制提示的ReAct-style LLM代理。值得注意的是，我们基于AvalonBench的评估突出显示了明显的能力差距。例如，像ChatGPT这样在好角色中的模型对战基于规则的机器人的胜率为22.2%。

    In this paper, we explore the potential of Large Language Models (LLMs) Agents in playing the strategic social deduction game, Resistance Avalon. Players in Avalon are challenged not only to make informed decisions based on dynamically evolving game phases, but also to engage in discussions where they must deceive, deduce, and negotiate with other players. These characteristics make Avalon a compelling test-bed to study the decision-making and language-processing capabilities of LLM Agents. To facilitate research in this line, we introduce AvalonBench - a comprehensive game environment tailored for evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game environment for Avalon, (2) rule-based bots as baseline opponents, and (3) ReAct-style LLM agents with tailored prompts for each role. Notably, our evaluations based on AvalonBench highlight a clear capability gap. For instance, models like ChatGPT playing good-role got a win rate of 22.2% against rule-based bots play
    
[^93]: 自我验证提示：利用重复内省进行少样本问题回答

    Self-Convinced Prompting: Few-Shot Question Answering with Repeated Introspection. (arXiv:2310.05035v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05035](http://arxiv.org/abs/2310.05035)

    该论文提出了一种利用大规模预训练语言模型的框架，通过迭代反思和改进推理过程，以提升模型在少样本问题回答上的性能。

    

    虽然像ChatGPT和PaLM这样的大型语言模型在各种语言理解和生成任务中表现出色，但它们在复杂推理和繁琐知识利用方面仍然不及人类的熟练程度。最近的研究已经证明了提示在引导语言模型生成期望的输出方面的有效性。在这些见解的基础上，我们引入了一种新的框架，利用大规模预训练语言模型的潜力，以迭代的方式增强语言模型的性能。我们的框架包含三个组件：\textit{Normal CoT}、\textit{Convincer}和\textit{Answerer}。它处理typical few-shot chain-of-thought prompt的输出，评估回答的正确性，审查答案，改进推理，最终产生一个新的解决方案。在七个各种问题的数据集上的实验结果验证了自我验证框架的有效性。

    While large language models (LLMs) such as ChatGPT and PaLM have demonstrated remarkable performance in various language understanding and generation tasks, their capabilities in complex reasoning and intricate knowledge utilization still fall short of human-level proficiency. Recent studies have established the effectiveness of prompts in steering LLMs towards generating desired outputs. Building on these insights, we introduce a novel framework that harnesses the potential of large-scale pre-trained language models, to iteratively enhance performance of the LLMs. Our framework incorporates three components: \textit{Normal CoT}, a \textit{Convincer}, and an \textit{Answerer}. It processes the output of a typical few-shot chain-of-thought prompt, assesses the correctness of the response, scrutinizes the answer, refines the reasoning, and ultimately produces a new solution. Experimental results on the 7 datasets of miscellaneous problems validate the efficacy of the Self-Convince framew
    
[^94]: 重新审视大型语言模型作为零-shot关系抽取器

    Revisiting Large Language Models as Zero-shot Relation Extractors. (arXiv:2310.05028v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.05028](http://arxiv.org/abs/2310.05028)

    本研究重新审视了大型语言模型(LLMs)作为零-shot关系抽取器的潜力，并提出了通过总结和提问(\textsc{SumAsk})提示方法来改进零-shot关系抽取。实验证明LLMs在这一任务上具有良好的表现。

    

    关系抽取(RE)即使在零-shot设定下，一直涉及一定程度的标记或未标记的数据。最近的研究表明，大型语言模型(LLMs)能够在给定自然语言提示的情况下，无需任何数据和参数调整，自动适应新任务，这为从文本中提取关系提供了可能性。本研究主要关注将LLMs，如ChatGPT，作为零-shot关系抽取器的研究。一方面，我们分析了现有RE提示的缺点，并尝试将最近的提示技术，如CoT，纳入其中以提高零-shot关系抽取。我们提出了总结和提问(\textsc{SumAsk})提示，这是一种简单的提示，通过递归使用LLMs将RE输入转换为有效的问答(QA)格式。另一方面，我们对各种基准和设置进行了全面的实验，以调查LLMs在零-shot关系抽取上的能力。具体而言，我们有以下的followi

    Relation extraction (RE) consistently involves a certain degree of labeled or unlabeled data even if under zero-shot setting. Recent studies have shown that large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt, which provides the possibility of extracting relations from text without any data and parameter tuning. This work focuses on the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors. On the one hand, we analyze the drawbacks of existing RE prompts and attempt to incorporate recent prompt techniques such as chain-of-thought (CoT) to improve zero-shot RE. We propose the summarize-and-ask (\textsc{SumAsk}) prompting, a simple prompt recursively using LLMs to transform RE inputs to the effective question answering (QA) format. On the other hand, we conduct comprehensive experiments on various benchmarks and settings to investigate the capabilities of LLMs on zero-shot RE. Specifically, we have the followi
    
[^95]: 一种关于货币的新的经济与金融理论

    A new economic and financial theory of money. (arXiv:2310.04986v1 [econ.TH])

    [http://arxiv.org/abs/2310.04986](http://arxiv.org/abs/2310.04986)

    这篇论文通过根本性的改革，将电子货币纳入经济与金融理论，提出了一种新的理论框架，包括电子货币的估值基于宏观经济理论和货币政策的基本方程，以及电子货币管理公司作为协调次经济体货币和财政政策的实体。该研究避免使用普遍但不适当的指数风险模型，而是采用多时间尺度的模型。

    

    本文对经济与金融理论进行了根本性改革，包括电子货币在内。电子货币的估值将基于宏观经济理论和货币政策的基本方程，而不是微观经济学中的贴现现金流理论。与将股票视为与次经济体的无形资产关联的所有权不同，我们将发展电子货币作为与次经济体有形资产关联的交易权益的观点。我们还将发展电子货币管理公司作为一个负责协调次经济体的货币（电子货币供应和价值稳定）和财政（投资和运营）政策的实体的视角，以实现电子货币的流动性。在估值和决策中使用的风险模型不会是无处不在但不合适的指数风险模型，它将导致贴现率，而是多时间尺度的模型。

    This paper fundamentally reformulates economic and financial theory to include electronic currencies. The valuation of the electronic currencies will be based on macroeconomic theory and the fundamental equation of monetary policy, not the microeconomic theory of discounted cash flows. The view of electronic currency as a transactional equity associated with tangible assets of a sub-economy will be developed, in contrast to the view of stock as an equity associated mostly with intangible assets of a sub-economy. The view will be developed of the electronic currency management firm as an entity responsible for coordinated monetary (electronic currency supply and value stabilization) and fiscal (investment and operational) policies of a substantial (for liquidity of the electronic currency) sub-economy. The risk model used in the valuations and the decision-making will not be the ubiquitous, yet inappropriate, exponential risk model that leads to discount rates, but will be multi time sc
    
[^96]: Diff-Transfer: 通过可微分物理仿真进行基于模型的机器人操作技能迁移

    Diff-Transfer: Model-based Robotic Manipulation Skill Transfer via Differentiable Physics Simulation. (arXiv:2310.04930v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2310.04930](http://arxiv.org/abs/2310.04930)

    这篇论文提出了Diff-Transfer，一种通过可微分物理仿真来高效传输机器人技能的新框架。Diff-Transfer通过在任务空间内发现可行路径，将源任务转化为目标任务，并通过梯度信息引导适应已知的动作，成功解决另一个子任务。实验结果表明了Diff-Transfer的有效性。

    

    将掌握的技能转移到完成一系列相似但新颖任务的能力对于智能机器人至关重要。本研究引入了Diff-Transfer，一种利用可微分物理仿真来高效传输机器人技能的新框架。具体而言，Diff-Transfer在任务空间内发现了一条可行的路径，将源任务带到目标任务。在这个任务路径的每对相邻点上，即两个子任务中，Diff-Transfer通过从一个子任务中适应已知的动作来成功解决另一个子任务。适应过程是由可微分物理仿真产生的梯度信息引导的。我们提出了一种新颖的路径规划方法，利用具有任务级状态和奖励的Q学习来生成子任务。我们在仿真实验中实现了我们的框架，并在机器人操作上执行了四个具有挑战性的迁移任务，展示了Diff-Transfer的有效性。

    The capability to transfer mastered skills to accomplish a range of similar yet novel tasks is crucial for intelligent robots. In this work, we introduce $\textit{Diff-Transfer}$, a novel framework leveraging differentiable physics simulation to efficiently transfer robotic skills. Specifically, $\textit{Diff-Transfer}$ discovers a feasible path within the task space that brings the source task to the target task. At each pair of adjacent points along this task path, which is two sub-tasks, $\textit{Diff-Transfer}$ adapts known actions from one sub-task to tackle the other sub-task successfully. The adaptation is guided by the gradient information from differentiable physics simulations. We propose a novel path-planning method to generate sub-tasks, leveraging $Q$-learning with a task-level state and reward. We implement our framework in simulation experiments and execute four challenging transfer tasks on robotic manipulation, demonstrating the efficacy of $\textit{Diff-Transfer}$ thr
    
[^97]: Lemur：在自动程序验证中集成大型语言模型

    Lemur: Integrating Large Language Models in Automated Program Verification. (arXiv:2310.04870v2 [cs.FL] UPDATED)

    [http://arxiv.org/abs/2310.04870](http://arxiv.org/abs/2310.04870)

    本论文提出了一种将LLMs和自动推理器结合起来进行自动程序验证的通用方法，并证明了其完备性。这个方法在一些合成和竞争基准上取得了实际的改进。

    

    LLMs在代码理解能力上的展示引发了一个问题：它们是否可以用于自动程序验证，这是一个通常需要高级抽象推理的任务，对于验证工具来说是具有挑战性的。我们提出了一种将LLMs的能力和自动推理器结合起来进行自动程序验证的通用方法。我们正式描述了这种方法论，将其作为推导规则的集合进行论证其完备性。我们将计算机推理形成为一个完备的自动验证过程，这在一组合成和竞争基准上带来了实际的改进。

    The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that often demands high-level abstract reasoning about program properties, which is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.
    
[^98]: 关于知识图谱的演化：一项调研和展望

    On the Evolution of Knowledge Graphs: A Survey and Perspective. (arXiv:2310.04835v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.04835](http://arxiv.org/abs/2310.04835)

    本文对各种类型的知识图谱进行了全面调研，介绍了知识提取和推理技术，并展望了结合知识图谱和大语言模型的力量以及知识工程的未来方向。

    

    知识图谱（KGs）是多样化知识的结构化表示，广泛应用于各种智能应用。本文对各种类型的知识图谱的演化（静态KGs，动态KGs，时态KGs和事件KGs）和知识提取和推理技术进行了全面调查。此外，我们介绍了不同类型的KGs的实际应用，包括财务分析的案例研究。最后，我们提出了关于知识工程未来方向的展望，包括结合知识图谱和大语言模型（LLMs）的力量以及知识提取、推理和表示的演化。

    Knowledge graphs (KGs) are structured representations of diversified knowledge. They are widely used in various intelligent applications. In this article, we provide a comprehensive survey on the evolution of various types of knowledge graphs (i.e., static KGs, dynamic KGs, temporal KGs, and event KGs) and techniques for knowledge extraction and reasoning. Furthermore, we introduce the practical applications of different types of KGs, including a case study in financial analysis. Finally, we propose our perspective on the future directions of knowledge engineering, including the potential of combining the power of knowledge graphs and large language models (LLMs), and the evolution of knowledge extraction, reasoning, and representation.
    
[^99]: 从Shapley Value的角度重新思考Integrated Gradients的基线选择

    Rethink Baseline of Integrated Gradients from the Perspective of Shapley Value. (arXiv:2310.04821v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04821](http://arxiv.org/abs/2310.04821)

    该论文从Shapley Value的角度重新思考了Integrated Gradients的基线选择，并提出了一种新的基线构建方法叫做Shapley Integrated Gradients (SIG)。在GridWorl上的模拟实验表明，SIG能够生成有意义和无偏的解释模型预测。

    

    许多方法已经尝试通过将深度神经网络（DNNs）的预测归因于其输入特征来解释DNN。其中一个研究充分的归因方法是Integrated Gradients（IG）。具体而言，选择IG的基线是在不同情景下生成有意义和无偏解释模型预测的关键考虑因素。然而，目前利用单一基线的做法未能实现这个愿望，因此需要多个基线。幸运的是，IG与奥曼—夏普利（Aumann-Shapley）价值之间的内在联系形成了一种独特的视角，重新思考了基线的设计。在某些假设下，我们在理论上分析出一组基线与Shapley Value中的联盟相对应。因此，我们提出了一种新的基线构建方法，称为Shapley Integrated Gradients（SIG），通过比例抽样来搜索一组基线，以部分模拟Shapley Value的计算路径。在GridWorl上进行了模拟实验。

    Numerous approaches have attempted to interpret deep neural networks (DNNs) by attributing the prediction of DNN to its input features. One of the well-studied attribution methods is Integrated Gradients (IG). Specifically, the choice of baselines for IG is a critical consideration for generating meaningful and unbiased explanations for model predictions in different scenarios. However, current practice of exploiting a single baseline fails to fulfill this ambition, thus demanding multiple baselines. Fortunately, the inherent connection between IG and Aumann-Shapley Value forms a unique perspective to rethink the design of baselines. Under certain hypothesis, we theoretically analyse that a set of baseline aligns with the coalitions in Shapley Value. Thus, we propose a novel baseline construction method called Shapley Integrated Gradients (SIG) that searches for a set of baselines by proportional sampling to partly simulate the computation path of Shapley Value. Simulations on GridWorl
    
[^100]: DiffNAS: 通过引导更好的结构来启动扩散模型

    DiffNAS: Bootstrapping Diffusion Models by Prompting for Better Architectures. (arXiv:2310.04750v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.04750](http://arxiv.org/abs/2310.04750)

    本文提出了一种名为DiffNAS的基础模型搜索方法，通过引导更好的结构来启动扩散模型，以提高合成性能。通过利用GPT-4作为超网，辅以搜索内存和RFID代理，以及快速收敛训练策略，搜索效率提高了2倍，达到了2.82的性能提升0.37。

    

    最近，扩散模型在合成数据上表现出了显著的性能。在选择扩散路径之后，像UNet这样的基础模型作为去噪自编码器，主要预测需要逐步消除的噪声。因此，采用与预期预算相一致的模型以促进优良的合成性能至关重要。在本文中，我们精心分析了扩散模型，并设计了一种名为"DiffNAS"的基础模型搜索方法。具体而言，我们利用GPT-4作为超网来加速搜索，辅以搜索内存以增强结果。此外，我们采用RFID作为代理，快速对GPT-4产生的实验结果进行排序。我们还采用了快速收敛训练策略来提高搜索效率。严格的实验验证了我们的算法在基于GPT的情境下可以将搜索效率提高2倍，同时取得了2.82的性能，改善了0.37。

    Diffusion models have recently exhibited remarkable performance on synthetic data. After a diffusion path is selected, a base model, such as UNet, operates as a denoising autoencoder, primarily predicting noises that need to be eliminated step by step. Consequently, it is crucial to employ a model that aligns with the expected budgets to facilitate superior synthetic performance. In this paper, we meticulously analyze the diffusion model and engineer a base model search approach, denoted "DiffNAS". Specifically, we leverage GPT-4 as a supernet to expedite the search, supplemented with a search memory to enhance the results. Moreover, we employ RFID as a proxy to promptly rank the experimental outcomes produced by GPT-4. We also adopt a rapid-convergence training strategy to boost search efficiency. Rigorous experimentation corroborates that our algorithm can augment the search efficiency by 2 times under GPT-based scenarios, while also attaining a performance of 2.82 with 0.37 improvem
    
[^101]: 在关系数据库中为深度学习模型提供服务

    Serving Deep Learning Model in Relational Databases. (arXiv:2310.04696v2 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2310.04696](http://arxiv.org/abs/2310.04696)

    本文研究了在关系数据库中为深度学习模型提供服务的架构，并强调了三个关键范式：深度学习中心架构、UDF中心架构和关系中心架构。尽管每个架构都在特定的使用场景中有潜力，但还需要解决它们之间的集成问题和中间地带。

    

    在不同商业和科学领域中，在关系数据上为深度学习模型提供服务已经成为一个重要需求，并引发了最近日益增长的兴趣。本文通过全面探索代表性架构来满足这个需求。我们强调了三个关键范式：尖端的深度学习中心架构将深度学习计算转移到专用的深度学习框架上。潜在的UDF中心架构将一个或多个张量计算封装到数据库系统中的用户定义函数(UDFs)中。潜在的关系中心架构旨在通过关系运算来表示大规模的张量计算。虽然每个架构在特定的使用场景中都显示出了潜力，但我们确定了这些架构之间的无缝集成和中间地带之间的紧急需求。我们深入研究了妨碍集成的差距，并探索了创新的策略。

    Serving deep learning (DL) models on relational data has become a critical requirement across diverse commercial and scientific domains, sparking growing interest recently. In this visionary paper, we embark on a comprehensive exploration of representative architectures to address the requirement. We highlight three pivotal paradigms: The state-of-the-artDL-Centricarchitecture offloadsDL computations to dedicated DL frameworks. The potential UDF-Centric architecture encapsulates one or more tensor computations into User Defined Functions (UDFs) within the database system. The potentialRelation-Centricarchitecture aims to represent a large-scale tensor computation through relational operators. While each of these architectures demonstrates promise in specific use scenarios, we identify urgent requirements for seamless integration of these architectures and the middle ground between these architectures. We delve into the gaps that impede the integration and explore innovative strategies 
    
[^102]: 增强鲁棒性的带对抗特征抑制的提升建模

    Robustness-enhanced Uplift Modeling with Adversarial Feature Desensitization. (arXiv:2310.04693v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04693](http://arxiv.org/abs/2310.04693)

    本文提出了一种增强鲁棒性的提升建模框架RUAD，并通过特征选择和对抗特征抑制两个定制模块更有效地解决了提升模型的特征敏感性问题。

    

    提升建模在在线营销中展示了非常有希望的结果。然而，大多数现有的工作在一些实际应用中容易受到鲁棒性挑战的影响。本文首先对上述现象给出了一个可能的解释。我们使用不同的真实世界数据集验证了在线营销中存在特征敏感性问题，一些关键特征的扰动会严重影响提升模型的性能，甚至导致相反的趋势。为了解决上述问题，我们提出了一种新颖的通过对抗特征抑制增强鲁棒性的提升建模框架（RUAD）。具体来说，我们的RUAD通过两个定制模块更有效地减轻提升模型的特征敏感性，包括一个具有联合多标签建模的特征选择模块，以从输入特征中识别一个关键子集，以及一个采用对抗训练和软插值操作的对抗特征抑制模块。

    Uplift modeling has shown very promising results in online marketing. However, most existing works are prone to the robustness challenge in some practical applications. In this paper, we first present a possible explanation for the above phenomenon. We verify that there is a feature sensitivity problem in online marketing using different real-world datasets, where the perturbation of some key features will seriously affect the performance of the uplift model and even cause the opposite trend. To solve the above problem, we propose a novel robustness-enhanced uplift modeling framework with adversarial feature desensitization (RUAD). Specifically, our RUAD can more effectively alleviate the feature sensitivity of the uplift model through two customized modules, including a feature selection module with joint multi-label modeling to identify a key subset from the input features and an adversarial feature desensitization module using adversarial training and soft interpolation operations t
    
[^103]: LauraGPT：使用GPT进行听、关注、理解和再生音频的研究

    LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT. (arXiv:2310.04673v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2310.04673](http://arxiv.org/abs/2310.04673)

    LauraGPT是一个统一的GPT模型，用于音频识别、理解和生成，具有广泛的应用范围，包括自动语音识别、语音翻译、文本到语音合成、机器翻译等任务。

    

    生成式预训练变换器（GPT）模型在各种自然语言处理任务中取得了显著的性能。然而，将类似的框架应用于音频任务的研究有限。以前提出的用于音频任务的大型语言模型要么缺乏充分的定量评估，要么局限于识别和理解音频内容的任务，要么明显不及现有的最先进模型（SOTA）。本文中，我们提出了LauraGPT，一个用于音频识别、理解和生成的统一GPT模型。LauraGPT是一个通用的语言模型，可以处理音频和文本输入，并在任意模式下生成输出。它可以进行与内容、语义、语音学和音频信号分析相关的各种任务。其中一些值得注意的任务包括自动语音识别、语音到文本翻译、文本到语音合成、机器翻译、语音增强、自动音频捕获等。

    Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks. However, there has been limited research on applying similar frameworks to audio tasks. Previously proposed large language models for audio tasks either lack sufficient quantitative evaluations, or are limited to tasks for recognizing and understanding audio content, or significantly underperform existing state-of-the-art (SOTA) models. In this paper, we propose LauraGPT, a unified GPT model for audio recognition, understanding, and generation. LauraGPT is a versatile language model that can process both audio and text inputs and generate outputs in either modalities. It can perform a wide range of tasks related to content, semantics, paralinguistics, and audio-signal analysis. Some of its noteworthy tasks include automatic speech recognition, speech-to-text translation, text-to-speech synthesis, machine translation, speech enhancement, automated audio capt
    
[^104]: Ada-Instruct: 为复杂推理调整指令生成器

    Ada-Instruct: Adapting Instruction Generators for Complex Reasoning. (arXiv:2310.04484v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.04484](http://arxiv.org/abs/2310.04484)

    Ada-Instruct是一种自适应指令生成器，通过对开源LLMs进行微调，能够生成复杂推理任务中长度大于等于100的指令。在代码补全、数学推理和常识推理等任务中，Ada-Instruct显示出优于基本模型和当前自我指导方法的改进效果。

    

    通过大型语言模型（LLM）生成多样且复杂的指令对于推进效果至关重要。当前的方法利用闭源的LLMs，通过上下文提示进行指令生成。然而，本文发现对于诸如代码补全等任务，上下文提示无法生成长度大于等于100的复杂指令。为解决这个问题，我们引入Ada-Instruct，一种通过对开源LLMs进行微调的自适应指令生成器。我们的关键发现表明，仅使用十个样本对开源LLMs进行微调即可生成保持分布一致性的长指令，适用于复杂推理任务。我们在代码补全、数学推理和常识推理等不同应用中对Ada-Instruct的有效性进行了实证验证。结果显示Ada-Instruct优于其基本模型和当前的自我指导方法。

    Generating diverse and sophisticated instructions for downstream tasks by Large Language Models (LLMs) is pivotal for advancing the effect. Current approaches leverage closed-source LLMs, employing in-context prompting for instruction generation. However, in this paper, we found that in-context prompting cannot generate complex instructions with length $\ge 100$ for tasks like code completion.  To solve this problem, we introduce Ada-Instruct, an adaptive instruction generator developed by fine-tuning open-source LLMs. Our pivotal finding illustrates that fine-tuning open-source LLMs with a mere ten samples generates long instructions that maintain distributional consistency for complex reasoning tasks. We empirically validated Ada-Instruct's efficacy across different applications, including code completion, mathematical reasoning, and commonsense reasoning. The results underscore Ada-Instruct's superiority, evidencing its improvements over its base models, current self-instruct method
    
[^105]: 自动调查挑战。（arXiv:2310.04480v2 [cs.CL]已更新）

    Auto-survey Challenge. (arXiv:2310.04480v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.04480](http://arxiv.org/abs/2310.04480)

    这项研究提出了一个新的平台，用于评估大型语言模型在撰写和评论调查论文方面的能力，并组织了一次竞赛来测试该平台。评估标准包括清晰度、参考适当性、可追溯性和内容的实质价值。

    

    我们提出了一个新颖的平台，用于评估大型语言模型（LLMs）在各个学科领域，包括科学、人文、教育和法律中，自主撰写和评论调查论文的能力。在这个框架内，人工智能系统进行类似于传统学术期刊的模拟同行评审机制，而人类组织者则充当编辑监督角色。在这个框架内，我们组织了一次针对2023年AutoML会议的竞赛。参赛者的任务是呈现能够根据指定提示撰写文章并进行评估的独立模型。评估标准包括清晰度、参考适当性、可追溯性和内容的实质价值。本文介绍了竞赛的设计，包括实施基准提交和评估方法。

    We present a novel platform for evaluating the capability of Large Language Models (LLMs) to autonomously compose and critique survey papers spanning a vast array of disciplines including sciences, humanities, education, and law. Within this framework, AI systems undertake a simulated peer-review mechanism akin to traditional scholarly journals, with human organizers serving in an editorial oversight capacity. Within this framework, we organized a competition for the AutoML conference 2023. Entrants are tasked with presenting stand-alone models adept at authoring articles from designated prompts and subsequently appraising them. Assessment criteria include clarity, reference appropriateness, accountability, and the substantive value of the content. This paper presents the design of the competition, including the implementation baseline submissions and methods of evaluation.
    
[^106]: 反向链：一种通用规则，用于使LLMs掌握多API规划

    Reverse Chain: A Generic-Rule for LLMs to Master Multi-API Planning. (arXiv:2310.04474v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2310.04474](http://arxiv.org/abs/2310.04474)

    这项研究提出了一种名为反向链的通用规则，通过反向链思路使LLMs能够使用外部API完成复杂的函数调用任务，并通过填充参数的方式提高任务完成的准确性。

    

    尽管使大型语言模型能够实现函数调用（即API）可以极大地提高LLMs的性能，但由于不同API之间的复杂关系，在没有微调的情况下，函数调用仍然是一项具有挑战性的任务。本文提出了一种简单但可控的目标驱动方法，称为反向链，以使LLMs能够仅通过提示使用外部API。在反向链中，大多数开源LLMs仅用于实现简单任务，例如API选择和参数补全，并使用通用规则实现可控的多函数调用。在这个通用规则中，选择一个最终API来处理给定任务之后，我们首先要求LLMs从用户查询和上下文中填写所需的参数。一些缺失的参数可以通过让LLMs基于API描述选择另一个API来进一步完成。

    While enabling large language models to implement function calling (known as APIs) can greatly enhance the performance of LLMs, function calling is still a challenging task due to the complicated relations between different APIs, especially in a context-learning setting without fine-tuning. This paper proposes a simple yet controllable target-driven approach called Reverse Chain to empower LLMs with capabilities to use external APIs with only prompts. Given that most open-source LLMs have limited tool-use or tool-plan capabilities, LLMs in Reverse Chain are only employed to implement simple tasks, e.g., API selection and argument completion, and a generic rule is employed to implement a controllable multiple functions calling. In this generic rule, after selecting a final API to handle a given task via LLMs, we first ask LLMs to fill the required arguments from user query and context. Some missing arguments could be further completed by letting LLMs select another API based on API desc
    
[^107]: 魔法词是什么？LLM提示的控制理论研究

    What's the Magic Word? A Control Theory of LLM Prompting. (arXiv:2310.04444v1 [cs.CL])

    [http://arxiv.org/abs/2310.04444](http://arxiv.org/abs/2310.04444)

    本论文将提示工程形式化为LLM上的最优控制问题，研究了给定token序列时是否存在一种最优提示能够准确预测最终的token，并提出了控制理论中的指标来描述LLM的可操纵性。

    

    提示工程在LLM的部署中是有效和重要的，但在数学上理解不足。在这里，我们将提示工程形式化为LLM上的最优控制问题，其中提示被认为是调节LLM输出分布的控制变量。在这个框架内，我们提出一个简单的问题：给定一个token序列，是否总存在一个我们可以添加的提示，使得LLM能够准确预测最终的token？我们将这样的最优提示称为魔法词，因为添加提示会导致LLM输出正确的答案。如果存在魔法词，我们能否找到它们？如果可以，它们的特性是什么？我们提供了将控制理论应用于自注意力头的分析分析，证明了其权重矩阵的奇异值函数为可控制性的上界。我们借鉴控制理论来提出了一种叫做$k-\epsilon$可控制性的指标，用于描述LLM的可操纵性。

    Prompt engineering is effective and important in the deployment of LLMs but is poorly understood mathematically. Here, we formalize prompt engineering as an optimal control problem on LLMs -- where the prompt is considered a control variable for modulating the output distribution of the LLM. Within this framework, we ask a simple question: given a sequence of tokens, does there always exist a prompt we can prepend that will steer the LLM toward accurately predicting the final token? We call such an optimal prompt the magic word since prepending the prompt causes the LLM to output the correct answer. If magic words exist, can we find them? If so, what are their properties? We offer analytic analysis on the controllability of the self-attention head where we prove a bound on controllability as a function of the singular values of its weight matrices. We take inspiration from control theory to propose a metric called $k-\epsilon$ controllability to characterize LLM steerability. We comput
    
[^108]: 用约束强化学习对抗奖励模型过度优化

    Confronting Reward Model Overoptimization with Constrained RLHF. (arXiv:2310.04373v1 [cs.LG])

    [http://arxiv.org/abs/2310.04373](http://arxiv.org/abs/2310.04373)

    本研究首次研究了组合奖励模型中的过度优化问题，发现组成奖励模型之间的相关性对问题的解决方式有重要影响。我们提出了一种使用约束强化学习来解决这个问题的方法。

    

    大型语言模型通常通过优化适应人类反馈的奖励模型来与人类偏好保持一致。然而，人类偏好是多方面的，越来越常见的做法是从几个简单的奖励模型中派生出奖励，每个模型捕捉语言质量的不同方面。然而，当组合这些组成的奖励模型时，适当地加权变得困难。更加困难的是，由于任何奖励模型只是人类评价的代理，这一过程容易受到过度优化的影响，即超过某一点后，获得更高奖励与更差的人类评价相关。本文通过对组合奖励模型中过度优化进行研究，首次展示了组成奖励模型之间的相关性对这些点的位置有显著影响。然后，我们介绍了一种使用约束强化学习来解决这个问题的方法。

    Large language models are typically aligned with human preferences by optimizing $\textit{reward models}$ (RMs) fitted to human feedback. However, human preferences are multi-faceted, and it is increasingly common to derive reward from a composition of simpler reward models which each capture a different aspect of language quality. This itself presents a challenge, as it is difficult to appropriately weight these component RMs when combining them. Compounding this difficulty, because any RM is only a proxy for human evaluation, this process is vulnerable to $\textit{overoptimization}$, wherein past a certain point, accumulating higher reward is associated with worse human ratings. In this paper, we perform, to our knowledge, the first study on overoptimization in composite RMs, showing that correlation between component RMs has a significant effect on the locations of these points. We then introduce an approach to solve this issue using constrained reinforcement learning as a means of 
    
[^109]: 大型语言模型在生物医学文本处理任务中的综合评估

    A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks. (arXiv:2310.04270v1 [cs.CL])

    [http://arxiv.org/abs/2310.04270](http://arxiv.org/abs/2310.04270)

    本文对大型语言模型（LLM）在生物医学任务中的性能进行了综合评估，发现零样本LLMs在小样本生物医学数据集上的表现甚至超过了先进的精调生物医学模型，预训练使LLMs在生物医学领域具备了很强的专业能力。

    

    最近，大型语言模型（LLM）展示了解决各种任务的出色能力。然而，尽管它们在各种任务中取得了成功，但目前还没有研究它们在生物医学领域的能力。因此，本文旨在评估LLMs在基准生物医学任务上的性能。为此，我们对6个不同生物医学任务的26个数据集中的4个热门LLMs进行了综合评估。据我们所知，这是第一篇在生物医学领域对各种LLMs进行广泛评估和比较的研究。有趣的是，根据我们的评估，我们发现在训练集较小的生物医学数据集中，零样本LLMs甚至超过了当前最先进的精调生物医学模型。这表明在大型文本语料库上进行预训练使LLMs在生物医学领域具备了很强的专业能力。我们还发现，在所有任务中没有一个LLM能够胜过其他LLMs。

    Recently, Large Language Models (LLM) have demonstrated impressive capability to solve a wide range of tasks. However, despite their success across various tasks, no prior work has investigated their capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of LLMs on benchmark biomedical tasks. For this purpose, we conduct a comprehensive evaluation of 4 popular LLMs in 6 diverse biomedical tasks across 26 datasets. To the best of our knowledge, this is the first work that conducts an extensive evaluation and comparison of various LLMs in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot LLMs even outperform the current state-of-the-art fine-tuned biomedical models. This suggests that pretraining on large text corpora makes LLMs quite specialized even in the biomedical domain. We also find that not a single LLM can outperform other LLMs in all tasks, with 
    
[^110]: 自然语言推理链条用于减少大型语言模型无根幻觉

    Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations. (arXiv:2310.03951v1 [cs.CL])

    [http://arxiv.org/abs/2310.03951](http://arxiv.org/abs/2310.03951)

    这项研究提出了一个分层框架，通过自然语言推理链条（CoNLI）来检测和减少大型语言模型（LLMs）的幻觉。该框架不需要对LLMs进行微调或特定领域的提示工程，能够从不同的上下文中实现具有竞争性能的幻觉检测和减少。

    

    当给定相关文档作为背景上下文时，大型语言模型（LLMs）能够生成流利的自然语言文本。这种能力引起了人们对LLMs在工业应用中的广泛关注。然而，LLMs容易产生没有提供来源支持的幻觉。在本文中，我们提出了一个分层框架来检测和减少这种无根幻觉。我们的框架使用自然语言推理链条（CoNLI）进行幻觉检测，并通过后期编辑进行幻觉减少。我们的方法在幻觉检测方面取得了最先进的性能，并且通过重写增强文本质量，使用LLMs而无需进行任何微调或特定领域的提示工程。我们展示了这个简单的即插即用框架可以作为幻觉检测和减少的有效选择，在各种情境下实现了竞争性能。

    Large language models (LLMs) can generate fluent natural language texts when given relevant documents as background context. This ability has attracted considerable interest in developing industry applications of LLMs. However, LLMs are prone to generate hallucinations that are not supported by the provided sources. In this paper, we propose a hierarchical framework to detect and mitigate such ungrounded hallucination. Our framework uses Chain of Natural Language Inference (CoNLI) for hallucination detection and hallucination reduction via post-editing. Our approach achieves state-of-the-art performance on hallucination detection and enhances text quality through rewrite, using LLMs without any fine-tuning or domain-specific prompt engineering. We show that this simple plug-and-play framework can serve as an effective choice for hallucination detection and reduction, achieving competitive performance across various contexts.
    
[^111]: 使用二维卷积的多任务学习在时间序列数据中的应用

    Multitask Learning for Time Series Data\\with 2D Convolution. (arXiv:2310.03925v1 [cs.LG])

    [http://arxiv.org/abs/2310.03925](http://arxiv.org/abs/2310.03925)

    该论文研究了将多任务学习（MTL）应用于时间序列分类（TSC）问题，并发现将最先进的一维卷积模型与MTL集成时性能下降。为了解决这个问题，提出了一种基于二维卷积的新设计。

    

    多任务学习（MTL）旨在开发一个统一的模型，可以同时处理一组密切相关的任务。通过在多个任务上优化模型，MTL在泛化能力方面通常优于非MTL模型。尽管MTL在计算机视觉、自然语言处理和推荐系统等领域得到了广泛研究，但在时间序列数据中的应用却受到了限制。在本文中，我们研究了将MTL应用于时间序列分类（TSC）问题。然而，当将最先进的基于一维卷积的TSC模型与MTL集成时，TSC模型的性能实际上会下降。通过将一维卷积模型与动态时间规整（DTW）距离函数进行比较，可以看出低下的结果是由于一维卷积层的有限表达能力造成的。为了克服这一挑战，我们提出了一种基于二维卷积的新设计。

    Multitask learning (MTL) aims to develop a unified model that can handle a set of closely related tasks simultaneously. By optimizing the model across multiple tasks, MTL generally surpasses its non-MTL counterparts in terms of generalizability. Although MTL has been extensively researched in various domains such as computer vision, natural language processing, and recommendation systems, its application to time series data has received limited attention. In this paper, we investigate the application of MTL to the time series classification (TSC) problem. However, when we integrate the state-of-the-art 1D convolution-based TSC model with MTL, the performance of the TSC model actually deteriorates. By comparing the 1D convolution-based models with the Dynamic Time Warping (DTW) distance function, it appears that the underwhelming results stem from the limited expressive power of the 1D convolutional layers. To overcome this challenge, we propose a novel design for a 2D convolution-based
    
[^112]: 在用户模型错误的情况下的在线聚类强化学习

    Online Clustering of Bandits with Misspecified User Models. (arXiv:2310.02717v1 [cs.LG])

    [http://arxiv.org/abs/2310.02717](http://arxiv.org/abs/2310.02717)

    本文介绍了在用户模型错误的情况下的聚类强化学习问题，并提出了两个鲁棒的聚类强化学习算法，以解决用户模型偏差的挑战。

    

    上下文线性强化学习是一个重要的在线学习问题，在每轮中，给定臂的特征，学习代理选择一个臂来最大化长期的累积奖励。聚类强化学习是一系列工作，利用用户偏好的协同效应，并在经典的线性强化学习算法上取得了显著的改进。然而，现有的聚类强化学习算法需要正确规定线性用户模型，当这个关键假设不成立时，可能会失败。如何为在用户模型错误的实际情况下设计鲁棒的聚类强化学习算法仍然是一个开放的问题。在本文中，我们首次提出了在用户模型错误的情况下的聚类强化学习问题，其中用户模型中的期望奖励可能有偏差，不是完美的线性模型。我们设计了两个鲁棒的聚类强化学习算法RCLUMB和RSCLUMB（分别用动态图和集合表示学习到的聚类结构）。

    The contextual linear bandit is an important online learning problem where given arm features, a learning agent selects an arm at each round to maximize the cumulative rewards in the long run. A line of works, called the clustering of bandits (CB), utilize the collaborative effect over user preferences and have shown significant improvements over classic linear bandit algorithms. However, existing CB algorithms require well-specified linear user models and can fail when this critical assumption does not hold. Whether robust CB algorithms can be designed for more practical scenarios with misspecified user models remains an open problem. In this paper, we are the first to present the important problem of clustering of bandits with misspecified user models (CBMUM), where the expected rewards in user models can be perturbed away from perfect linear models. We devise two robust CB algorithms, RCLUMB and RSCLUMB (representing the learned clustering structure with dynamic graph and sets, resp
    
[^113]: 强化学习基础：朝向具有基础先验辅助的具身通用智能体

    Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance. (arXiv:2310.02635v1 [cs.RO])

    [http://arxiv.org/abs/2310.02635](http://arxiv.org/abs/2310.02635)

    本研究提出了一种基于具身基础先验的基础强化学习框架，通过加速训练过程来提高样本效率。

    

    最近人们已经表明，从互联网规模的数据中进行大规模预训练是构建通用模型的关键，正如在NLP中所见。为了构建具身通用智能体，我们和许多其他研究者假设这种基础先验也是不可或缺的组成部分。然而，目前尚不清楚如何以适当的具体形式表示这些具身基础先验，以及它们应该如何在下游任务中使用。在本文中，我们提出了一组直观有效的具身先验，包括基础策略、价值和成功奖励。所提出的先验是基于目标条件的MDP。为了验证其有效性，我们实例化了一个由这些先验辅助的演员-评论家方法，称之为基础演员-评论家（FAC）。我们将我们的框架命名为基础强化学习（FRL），因为它完全依赖于具身基础先验来进行探索、学习和强化。FRL的好处有三个。(1)样本效率高。通过基础先验加速训练过程，减少样本使用量。

    Recently, people have shown that large-scale pre-training from internet-scale data is the key to building generalist models, as witnessed in NLP. To build embodied generalist agents, we and many other researchers hypothesize that such foundation prior is also an indispensable component. However, it is unclear what is the proper concrete form to represent those embodied foundation priors and how they should be used in the downstream task. In this paper, we propose an intuitive and effective set of embodied priors that consist of foundation policy, value, and success reward. The proposed priors are based on the goal-conditioned MDP. To verify their effectiveness, we instantiate an actor-critic method assisted by the priors, called Foundation Actor-Critic (FAC). We name our framework as Foundation Reinforcement Learning (FRL), since it completely relies on embodied foundation priors to explore, learn and reinforce. The benefits of FRL are threefold. (1) Sample efficient. With foundation p
    
[^114]: 通过交流使LLM代理适应环境的方法

    Adapting LLM Agents Through Communication. (arXiv:2310.01444v1 [cs.CL])

    [http://arxiv.org/abs/2310.01444](http://arxiv.org/abs/2310.01444)

    这项研究提出了一种名为学习通信（LTC）的训练方法，使用该方法可使LLM代理通过与环境和其他代理的交互不断改进，以适应新任务，而无需过多人类监督。

    

    最近大语言模型(LLM)的进展显示出了人类化代理的潜力。为了帮助这些代理在没有广泛人类监督的情况下适应新任务，我们提出了学习通信（LTC）范式，这是一种新颖的训练方法，使LLM代理能够通过与环境和其他代理的交互不断改进。通过迭代探索和PPO训练，LTC使代理能够将短期经验融入长期记忆。为了优化特定任务的代理交互，我们引入了三种结构化的通信模式：独白，对话，

    Recent advancements in large language models (LLMs) have shown potential for human-like agents. To help these agents adapt to new tasks without extensive human supervision, we propose the Learning through Communication (LTC) paradigm, a novel training approach enabling LLM agents to improve continuously through interactions with their environments and other agents. Recent advancements in large language models (LLMs) have shown potential for human-like agents. To help these agents adapt to new tasks without extensive human supervision, we propose the Learning through Communication (LTC) paradigm, a novel training approach enabling LLM agents to improve continuously through interactions with their environments and other agents. Through iterative exploration and PPO training, LTC empowers the agent to assimilate short-term experiences into long-term memory. To optimize agent interactions for task-specific learning, we introduce three structured communication patterns: Monologue, Dialogue,
    
[^115]: 表示工程化：AI透明化的自上而下方法

    Representation Engineering: A Top-Down Approach to AI Transparency. (arXiv:2310.01405v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.01405](http://arxiv.org/abs/2310.01405)

    这项研究介绍了一种名为表示工程化（RepE）的自上而下方法，通过借鉴认知神经科学的见解，提供了一种增强AI系统透明性的解决方案。该方法将集群级别的表示放在分析的核心，为监测和操纵深度神经网络中的高级认知现象提供了新的方法，并展示了在解决与安全相关的问题上的潜力。

    

    本文中，我们确定并描述了表示工程化（RepE）这一新兴领域，这是一种通过借鉴认知神经科学的见解来增强AI系统透明性的方法。RepE将集群级别的表示放在分析的核心，而不是神经元或电路，为我们提供了监测和操纵深度神经网络（DNNs）中高级认知现象的新方法。我们提供了RepE技术的基准和初步分析，显示它们提供了简单而有效的解决方案，用于改善我们对大型语言模型的理解和控制。我们展示了这些方法如何在包括诚实性、无害性、追求权力等一系列与安全相关的问题上发挥作用，展示了自上而下透明性研究的潜力。我们希望这项工作能够促进RepE的进一步探索，并推动AI系统的透明性和安全性的进步。

    In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.
    
[^116]: SELF：基于语言驱动的大型语言模型自主进化

    SELF: Language-Driven Self-Evolution for Large Language Model. (arXiv:2310.00533v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.00533](http://arxiv.org/abs/2310.00533)

    SELF提出了一种基于语言驱动的创新方法，允许大型语言模型（LLM）不断自我进化，并通过语言反馈作为评估工具来改进模型的响应能力和训练稳定性。

    

    大型语言模型（LLM）展示了在多个领域中的卓越适应能力。然而，实现人类水平的学习和推动自主人工智能的关键——模型自主进化的路径仍然未知。我们引入了一种创新的方法，名为"SELF"（带有语言反馈的自主进化）。这种方法使LLM能够不断地自我进化。此外，SELF利用语言反馈作为一种多功能、全面的评估工具，精确定位响应改进的领域，并提高自主进化训练的稳定性。SELF首先进行元技能学习，专注于自我反馈和自我精炼。这些元技能是关键，引导模型在自制数据的持续训练周期中进行后续的自我进化，从而增强其内在能力。在给定无标签指令的情况下，SELF使模型具备了能够...

    Large Language Models (LLMs) have showcased remarkable versatility across diverse domains. However, the pathway toward autonomous model development, a cornerstone for achieving human-level learning and advancing autonomous AI, remains largely uncharted. We introduce an innovative approach, termed "SELF" (Self-Evolution with Language Feedback). This methodology empowers LLMs to undergo continual self-evolution. Furthermore, SELF employs language-based feedback as a versatile and comprehensive evaluative tool, pinpointing areas for response refinement and bolstering the stability of self-evolutionary training. Initiating with meta-skill learning, SELF acquires foundational meta-skills with a focus on self-feedback and self-refinement. These meta-skills are critical, guiding the model's subsequent self-evolution through a cycle of perpetual training with self-curated data, thereby enhancing its intrinsic abilities. Given unlabeled instructions, SELF equips the model with the capability to
    
[^117]: 两两邻近策略优化: 利用相对反馈进行LLM对齐

    Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment. (arXiv:2310.00212v1 [cs.LG])

    [http://arxiv.org/abs/2310.00212](http://arxiv.org/abs/2310.00212)

    该论文提出了一种新的强化学习框架，使用相对反馈来调整大型语言模型（LLMs）的行为，解决了现有方法在优化比较损失训练的奖励时存在的限制。同时，还提出了一种新的基于轨迹的策略梯度算法（PPPO），用于更有效地进行算法设计和函数逼近。

    

    大型语言模型（LLMs）通过在大型语料库上预先训练来获取广泛的世界知识。然而，由于接触到低质量数据，LLMs可能表现出与人类价值不一致的有害行为。引导LLMs朝着有益行为方向发展的主导方法涉及使用人类反馈的强化学习（RLHF），其中Proximal Policy Optimization（PPO）是默认的RL优化器。尽管其有效性，但PPO在优化基于比较损失训练的奖励时存在局限性。主要问题是，由于需要校准奖励尺度，PPO对于包含相同偏好信息的等价奖励函数不具备不变性。此外，与基于轨迹的优化相比，PPO对于基于令牌的更新的需求引入了函数逼近和算法设计方面的复杂性。本文提出了一种新的框架，基于相对反馈的强化学习，以及一种新颖的基于轨迹的策略梯度算法，Pairwise Proximal Policy Optimization（PPPO），用于解决上述问题。

    Large Language Models (LLMs) can acquire extensive world knowledge through pre-training on large corpora. However, due to exposure to low-quality data, LLMs may exhibit harmful behavior without aligning with human values. The dominant approach for steering LLMs towards beneficial behavior involves Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy Optimization (PPO) serving as the default RL optimizer. Despite its effectiveness, PPO has limitations when optimizing rewards trained from comparison-based loss. Primarily, PPO is not invariant to equivalent reward functions containing identical preference information due to the need to calibrate the reward scale. Additionally, PPO's necessity for token-wise updates introduces complexity in both function approximation and algorithm design compared to trajectory-wise optimization. This paper proposes a new framework, reinforcement learning with relative feedback, and a novel trajectory-wise policy gradient algorithm, Pair
    
[^118]: ABScribe: 使用大型语言模型在人工智能与人类共同写作任务中快速探索多种写作变化

    ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models. (arXiv:2310.00117v1 [cs.HC])

    [http://arxiv.org/abs/2310.00117](http://arxiv.org/abs/2310.00117)

    ABScribe是一种界面，支持在人工智能与人类共同写作任务中快速探索多种写作变化。用户可以使用大型语言模型提示快速生成多个变体，这些变体以可重用的按钮形式呈现，并且可以通过上下文工具栏进行快速的就地比较。

    

    通过重新书写文本来探索替代想法是写作过程的关键。最先进的大型语言模型（LLM）可以简化写作变化生成的过程。然而，当前的界面存在同时考虑多种变化的挑战：在不覆盖文本的情况下创建新的版本可能很困难，而按顺序粘贴它们可能会使文档变得杂乱，增加工作量，并打断作者的流程。为了解决这个问题，我们提出了ABScribe，一种支持在人工智能与人类共同写作任务中快速且结构化地探索写作变化的界面。通过ABScribe，用户可以使用LLM提示快速产生多个变体，这些变体会自动转换成可重用的按钮形式。变体在文本段落中被存储在相邻位置，通过在上下文工具栏上的鼠标悬停交互进行快速的就地比较。我们对12名撰写人员进行的用户研究表明，ABScribe能显著减轻任务负荷（d = 1.20, p < 0.001），提高用户的认知程度。

    Exploring alternative ideas by rewriting text is integral to the writing process. State-of-the-art large language models (LLMs) can simplify writing variation generation. However, current interfaces pose challenges for simultaneous consideration of multiple variations: creating new versions without overwriting text can be difficult, and pasting them sequentially can clutter documents, increasing workload and disrupting writers' flow. To tackle this, we present ABScribe, an interface that supports rapid, yet visually structured, exploration of writing variations in human-AI co-writing tasks. With ABScribe, users can swiftly produce multiple variations using LLM prompts, which are auto-converted into reusable buttons. Variations are stored adjacently within text segments for rapid in-place comparisons using mouse-over interactions on a context toolbar. Our user study with 12 writers shows that ABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances user perceptions o
    
[^119]: 用大型语言模型进行生成式语音识别错误校正

    Generative Speech Recognition Error Correction with Large Language Models. (arXiv:2309.15649v1 [cs.CL])

    [http://arxiv.org/abs/2309.15649](http://arxiv.org/abs/2309.15649)

    本研究探讨了大型语言模型（LLMs）作为ASR后处理器的能力，通过重新评分和错误校正来提高系统性能。通过使用指令提示和任务激活提示方法，结合上下文学习和微调技术，我们展示了LLMs的泛化能力和有效性。

    

    我们研究了大型语言模型（LLM）作为ASR后处理器的能力，用于重新评分和错误校正。我们的重点是使用指令提示让LLMs执行这些任务而无需微调，我们评估了不同的提示方案，包括零-shot和少-shot的上下文学习，以及一种新颖的任务激活提示（TAP）方法，结合指令和演示。通过在两个领域之外的任务（ATIS和WSJ）上使用预先训练的第一次扫描系统和重新评分输出，我们证明仅通过冻结LLMs的上下文学习进行重新评分可以达到与领域调优的LMs重新评分相竞争的结果。通过将提示技术与微调相结合，我们实现了低于N-best Oracle水平的错误率，展示了LLMs的泛化能力。

    We explore the ability of large language models (LLMs) to act as ASR post-processors that perform rescoring and error correction. Our focus is on instruction prompting to let LLMs perform these task without fine-tuning, for which we evaluate different prompting schemes, both zero- and few-shot in-context learning, and a novel task-activating prompting (TAP) method that combines instruction and demonstration. Using a pre-trained first-pass system and rescoring output on two out-of-domain tasks (ATIS and WSJ), we show that rescoring only by in-context learning with frozen LLMs achieves results that are competitive with rescoring by domain-tuned LMs. By combining prompting techniques with fine-tuning we achieve error rates below the N-best oracle level, showcasing the generalization power of the LLMs.
    
[^120]: MAPTree: 用贝叶斯决策树击败“最优”决策树

    MAPTree: Beating "Optimal" Decision Trees with Bayesian Decision Trees. (arXiv:2309.15312v1 [cs.LG])

    [http://arxiv.org/abs/2309.15312](http://arxiv.org/abs/2309.15312)

    MAPTree是一种通过贝叶斯方法对决策树进行归纳的算法，通过AND/OR搜索实现最大后验树的恢复。在实验中，MAPTree在多个数据集上表现出更好的性能，并且能够以更小的树来实现可比较的性能。在合成数据和实际场景中，MAPTree还展示出更强的抗噪声能力和更好的泛化能力。

    

    决策树仍然是当今最流行的机器学习模型之一，主要是因为其开箱即用的性能和可解释性。在这项工作中，我们通过对树上的后验分布进行最大后验推理，提出了一种贝叶斯决策树归纳的方法。我们首先展示了决策树的最大后验推理与AND/OR搜索之间的关联。利用这一关联，我们提出了一种称为MAPTree的AND/OR搜索算法，能够恢复出最大后验树。最后，我们通过在合成数据和实际世界场景中展示最大后验树的经验性能。在16个实际数据集上，MAPTree要么优于基准线，要么在性能相当的情况下具有更小的树。在一个合成数据集上，MAPTree表现出比现有方法更强的抗噪声能力和更好的泛化能力。最后，MAPTree比其他方法更快地恢复出最大后验树。

    Decision trees remain one of the most popular machine learning models today, largely due to their out-of-the-box performance and interpretability. In this work, we present a Bayesian approach to decision tree induction via maximum a posteriori inference of a posterior distribution over trees. We first demonstrate a connection between maximum a posteriori inference of decision trees and AND/OR search. Using this connection, we propose an AND/OR search algorithm, dubbed MAPTree, which is able to recover the maximum a posteriori tree. Lastly, we demonstrate the empirical performance of the maximum a posteriori tree both on synthetic data and in real world settings. On 16 real world datasets, MAPTree either outperforms baselines or demonstrates comparable performance but with much smaller trees. On a synthetic dataset, MAPTree also demonstrates greater robustness to noise and better generalization than existing approaches. Finally, MAPTree recovers the maxiumum a posteriori tree faster tha
    
[^121]: 大规模语言模型重评分的低秩适应技术在参数高效的语音识别中的应用

    Low-rank Adaptation of Large Language Model Rescoring for Parameter-Efficient Speech Recognition. (arXiv:2309.15223v1 [cs.CL])

    [http://arxiv.org/abs/2309.15223](http://arxiv.org/abs/2309.15223)

    这篇论文介绍了一种基于低秩适应技术的神经语言建模系统，用于语音识别的输出重评分。通过使用低秩分解方法和优化插入矩阵，该系统能够以更高效的方式将BERT模型适应到新领域，大大减少了训练时间。

    

    我们提出了一种基于低秩适应（LoRA）的神经语言建模系统，用于语音识别输出重评分。尽管预训练的语言模型（如BERT）在第二次重评分中表现出优越的性能，但将预训练阶段扩展和将预训练模型适应到特定领域的高计算成本限制了它们在重评分中的实际应用。我们提出了一种基于低秩分解的方法，仅使用预训练参数的一小部分（0.08%）来训练重评分的BERT模型并将其适应到新领域。这些插入的矩阵通过相关性正则化损失和判别性训练目标进行优化。所提出的低秩适应Rescore-BERT（LoRB）体系结构在LibriSpeech和内部数据集上评估，训练时间减少了5.4至3.6倍。

    We propose a neural language modeling system based on low-rank adaptation (LoRA) for speech recognition output rescoring. Although pretrained language models (LMs) like BERT have shown superior performance in second-pass rescoring, the high computational cost of scaling up the pretraining stage and adapting the pretrained models to specific domains limit their practical use in rescoring. Here we present a method based on low-rank decomposition to train a rescoring BERT model and adapt it to new domains using only a fraction (0.08%) of the pretrained parameters. These inserted matrices are optimized through a discriminative training objective along with a correlation-based regularization loss. The proposed low-rank adaptation Rescore-BERT (LoRB) architecture is evaluated on LibriSpeech and internal datasets with decreased training times by factors between 5.4 and 3.6.
    
[^122]: 递归超网络在元强化学习中表现出惊人的强大性能

    Recurrent Hypernetworks are Surprisingly Strong in Meta-RL. (arXiv:2309.14970v1 [cs.LG])

    [http://arxiv.org/abs/2309.14970](http://arxiv.org/abs/2309.14970)

    递归超网络和循环神经网络在元强化学习中的端到端学习表现出惊人的强大性能，相比于现有专门方法更为简单但效果更好。

    

    深度强化学习在实际应用时因样本效率低而不易部署。元强化学习通过学习在元训练时利用相关任务的分布来实现少样本学习，直接解决了这个样本效率问题。最近的研究表明，与专门的元强化学习方法相比，与一个通用的序列模型（如循环神经网络）结合的端到端学习是一个令人惊讶的强基准。然而，这样的观点由于有限的支持证据而引起了争议，特别是在之前的研究中确立了完全相反的观点。在本文中，我们进行了实证研究。虽然我们同样发现循环网络可以达到强大的性能，但我们证明了超网络的使用对于发挥循环基线的潜力至关重要。令人惊讶的是，与超网络相结合时，这种远比现有专门方法简单的循环基准实际上能取得更好的表现。

    Deep reinforcement learning (RL) is notoriously impractical to deploy due to sample inefficiency. Meta-RL directly addresses this sample inefficiency by learning to perform few-shot learning when a distribution of related tasks is available for meta-training. While many specialized meta-RL methods have been proposed, recent work suggests that end-to-end learning in conjunction with an off-the-shelf sequential model, such as a recurrent network, is a surprisingly strong baseline. However, such claims have been controversial due to limited supporting evidence, particularly in the face of prior work establishing precisely the opposite. In this paper, we conduct an empirical investigation. While we likewise find that a recurrent network can achieve strong performance, we demonstrate that the use of hypernetworks is crucial to maximizing their potential. Surprisingly, when combined with hypernetworks, the recurrent baselines that are far simpler than existing specialized methods actually ac
    
[^123]: 通过充分因素和必要因素的概率进行不变学习

    Invariant Learning via Probability of Sufficient and Necessary Causes. (arXiv:2309.12559v1 [cs.LG])

    [http://arxiv.org/abs/2309.12559](http://arxiv.org/abs/2309.12559)

    本研究通过引入充分因素和必要因素的概率（PNS）来改善在未知测试分布上的泛化问题，以解决现有方法主要关注因果性的不变性属性而忽视充分性和必要性条件的问题。

    

    在野外学习中，对于未知的、与训练分布不同的测试分布，外部分布（OOD）泛化是不可或缺的。最近从因果性引发的方法在实现OOD泛化方面显示出了巨大的潜力。然而，现有方法主要关注因果性的不变性属性，而在很大程度上忽视了充分性和必要性条件的属性。换句话说，一个必要但不充分的原因（特征）对于分布转换是不变的，但可能没有所需的准确度。相反，一个充分但不必要的原因（特征）倾向于很好地适应特定数据，但可能存在适应新领域的风险。为了捕捉充分和必要因素的信息，我们采用了经典概念——充分和必要因素的概率（PNS），它指示了一个因素是必要和充分原因的概率。为了将PNS与OOD泛化联系起来，我们提出了一种方法

    Out-of-distribution (OOD) generalization is indispensable for learning models in the wild, where testing distribution typically unknown and different from the training. Recent methods derived from causality have shown great potential in achieving OOD generalization. However, existing methods mainly focus on the invariance property of causes, while largely overlooking the property of \textit{sufficiency} and \textit{necessity} conditions. Namely, a necessary but insufficient cause (feature) is invariant to distribution shift, yet it may not have required accuracy. By contrast, a sufficient yet unnecessary cause (feature) tends to fit specific data well but may have a risk of adapting to a new domain. To capture the information of sufficient and necessary causes, we employ a classical concept, the probability of sufficiency and necessary causes (PNS), which indicates the probability of whether one is the necessary and sufficient cause. To associate PNS with OOD generalization, we propose
    
[^124]: 环境偏向特征排序用于鲁棒性的新颖性检测

    Environment-biased Feature Ranking for Novelty Detection Robustness. (arXiv:2309.12301v1 [cs.LG])

    [http://arxiv.org/abs/2309.12301](http://arxiv.org/abs/2309.12301)

    本文提出了一种环境偏向特征排序的方法，用于鲁棒性的新颖性检测。通过计算特征的环境之间分布方差进行评分，并通过去除高分特征来改善性能。这种方法在真实和合成基准数据上均能提高性能。

    

    我们解决了鲁棒性新颖性检测的问题，在该问题中，我们旨在检测语义内容方面的新颖性，同时对其他无关因素的变化具有不变性。具体来说，我们在具有多个环境的设置中操作，确定与环境更相关而不是任务相关内容的特征集合。因此，我们提出了一种方法，该方法从预训练的嵌入和多环境设置开始，成功根据其环境关注度对特征进行排序。首先，我们基于环境之间的特征分布方差计算每个特征的得分。接下来，我们证明通过舍弃得分较高的特征，我们可以去除虚假的相关性，并在正态协方差和子种群转移的情况下提高整体性能，无论是对于真实的还是对于我们为此任务引入的合成基准数据。

    We tackle the problem of robust novelty detection, where we aim to detect novelties in terms of semantic content while being invariant to changes in other, irrelevant factors. Specifically, we operate in a setup with multiple environments, where we determine the set of features that are associated more with the environments, rather than to the content relevant for the task. Thus, we propose a method that starts with a pretrained embedding and a multi-env setup and manages to rank the features based on their environment-focus. First, we compute a per-feature score based on the feature distribution variance between envs. Next, we show that by dropping the highly scored ones, we manage to remove spurious correlations and improve the overall performance by up to 6%, both in covariance and sub-population shift cases, both for a real and a synthetic benchmark, that we introduce for this task.
    
[^125]: SlimPajama-DC: 理解LLM训练中的数据组合

    SlimPajama-DC: Understanding Data Combinations for LLM Training. (arXiv:2309.10818v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.10818](http://arxiv.org/abs/2309.10818)

    本研究探讨了使用SlimPajama进行大型语言模型训练中不同数据组合的影响，提出了全局去重和局部去重的比较和高质量多源数据集的比例对模型性能的影响。

    

    本研究旨在了解使用SlimPajama进行大型语言模型训练时各种数据组合（如网络文本、维基百科、GitHub、图书）对其训练的影响。SlimPajama是一个经过严格去重的多源数据集，从Together贡献的1.2T个token的RedPajama数据集中精细组合和去重，总共得到了627B个tokens。我们将我们的研究称为SlimPajama-DC，这是一项旨在揭示在大型语言模型训练中使用SlimPajama所涉及的基本特征和最佳实践的经验分析。在我们使用SlimPajama进行研究的过程中，出现了两个关键观察结果：（1）全局去重 vs. 局部去重。我们分析和讨论了全局去重（跨不同数据集源）和局部去重（在单个数据集源内部）对训练模型性能的影响。（2）高质量/高度去重的多源数据集在组合中的比例。为了研究这一点，我们进行了...

    This paper aims to understand the impacts of various data combinations (e.g., web text, wikipedia, github, books) on the training of large language models using SlimPajama. SlimPajama is a rigorously deduplicated, multi-source dataset, which has been refined and further deduplicated to 627B tokens from the extensive 1.2T tokens RedPajama dataset contributed by Together. We've termed our research as SlimPajama-DC, an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. During our research with SlimPajama, two pivotal observations emerged: (1) Global deduplication vs. local deduplication. We analyze and discuss how global (across different sources of datasets) and local (within the single source of dataset) deduplications affect the performance of trained models. (2) Proportions of high-quality/highly-deduplicated multi-source datasets in the combination. To study this, we cons
    
[^126]: 通过准确性预测器修剪大型语言模型

    Pruning Large Language Models via Accuracy Predictor. (arXiv:2309.09507v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.09507](http://arxiv.org/abs/2309.09507)

    本文提出了一种通过准确性预测器来修剪大型语言模型的新方法，实验证明该方法是有效且高效的，可以自动选择最优模型并降低困惑度。

    

    大型语言模型(LLMs)具有数百亿个参数（甚至更多），在各种自然语言处理任务中展示出了令人印象深刻的能力。然而，庞大的模型规模给训练、推理和部署带来了挑战，因此有必要对模型进行压缩。目前，大部分LLMs模型压缩需要手动设计修剪特性，存在诸如复杂的优化流程和难以保留模型部分能力的问题。因此，我们提出了一种新的修剪方法：首先，建立一组特定数量的架构-准确性对的训练集，然后训练一个非神经模型作为准确性预测器。通过使用准确性预测器进一步优化搜索空间和搜索，可以自动选择最优模型。实验结果显示我们提出的方法是有效且高效的。与基准模型相比，在Wikitext2和PTB上的困惑度（PPL）分别下降了9.48%和5.7%。

    Large language models(LLMs) containing tens of billions of parameters (or even more) have demonstrated impressive capabilities in various NLP tasks. However, substantial model size poses challenges to training, inference, and deployment so that it is necessary to compress the model. At present, most model compression for LLMs requires manual design of pruning features, which has problems such as complex optimization pipeline and difficulty in retaining the capabilities of certain parts of the model.Therefore, we propose a novel pruning approach: firstly, a training set of a certain number of architecture-accuracy pairs is established, and then a non-neural model is trained as an accuracy predictor. Using the accuracy predictor to further optimize the search space and search, the optimal model can be automatically selected. Experiments show that our proposed approach is effective and efficient. Compared with the baseline, the perplexity(PPL) on Wikitext2 and PTB dropped by 9.48% and 5,7
    
[^127]: 基于视觉的酒驾司机行为和驾驶性能分析

    Vision-based Analysis of Driver Activity and Driving Performance Under the Influence of Alcohol. (arXiv:2309.08021v1 [cs.CV])

    [http://arxiv.org/abs/2309.08021](http://arxiv.org/abs/2309.08021)

    该论文介绍了一项研究，通过使用多种传感器，研究了急性酒精摄入对驾驶性能的影响，并通过识别酒驾行为来减少酒驾事故。

    

    美国约30%的交通事故死亡涉及酒驾，因此在美国和其他高酒驾患病率地区，防止酒驾对车辆安全至关重要。通过主动使用传感器（要求驾驶员提供呼气样本给车辆仪器或被警察拦下时），可以监测驾驶能力受损，但使用一种更被动且稳健的感知机制可能能够更广泛地应用智能系统，从而减少酒驾事故。这可以帮助在驾驶前或驾驶过程早期（在事故或被执法部门发现之前）识别出受损驾驶员。在这项研究中，我们介绍了一项使用视觉、热感、音频和化学传感器的多模态集成，以(1)在驾驶模拟器中研究急性酒精摄入对驾驶性能的影响，(2)识别酒驾行为。

    About 30% of all traffic crash fatalities in the United States involve drunk drivers, making the prevention of drunk driving paramount to vehicle safety in the US and other locations which have a high prevalence of driving while under the influence of alcohol. Driving impairment can be monitored through active use of sensors (when drivers are asked to engage in providing breath samples to a vehicle instrument or when pulled over by a police officer), but a more passive and robust mechanism of sensing may allow for wider adoption and benefit of intelligent systems that reduce drunk driving accidents. This could assist in identifying impaired drivers before they drive, or early in the driving process (before a crash or detection by law enforcement). In this research, we introduce a study which adopts a multi-modal ensemble of visual, thermal, audio, and chemical sensors to (1) examine the impact of acute alcohol administration on driving performance in a driving simulator, and (2) identi
    
[^128]: 高效强化学习用于跳跃式单脚机器人

    Efficient Reinforcement Learning for Jumping Monopods. (arXiv:2309.07038v1 [cs.RO])

    [http://arxiv.org/abs/2309.07038](http://arxiv.org/abs/2309.07038)

    本论文研究了如何通过在强化学习框架中注入物理知识来解决跳跃式单脚机器人的控制问题，这样可以大幅减少学习时间并且能够学习和修正可能出现的错误。

    

    在这项工作中，我们考虑了一个复杂的控制问题，即使单脚机器人能够跳到任何方向，其脚下的地形可能是不平的，我们要使它达到目标位置。这是一个更大类别问题的模板，使用标准的基于优化的技术解决这些问题非常具有挑战性和计算开销。强化学习 (RL) 可能是一个有趣的替代方案，但完全从零开始学习的端到端方法是不切实际的。本文提出的解决方案是在 RL 框架中注入物理知识来指导学习过程。这种方法带来了广泛的好处，如大幅减少学习时间，并且能够学习和修正执行运动的低级控制器可能出现的错误。我们通过与基于优化和端到端 RL 方法的比较，证明了我们方法的优势。

    In this work, we consider the complex control problem of making a monopod reach a target with a jump. The monopod can jump in any direction and the terrain underneath its foot can be uneven. This is a template of a much larger class of problems, which are extremely challenging and computationally expensive to solve using standard optimisation-based techniques. Reinforcement Learning (RL) could be an interesting alternative, but the application of an end-to-end approach in which the controller must learn everything from scratch, is impractical. The solution advocated in this paper is to guide the learning process within an RL framework by injecting physical knowledge. This expedient brings to widespread benefits, such as a drastic reduction of the learning time, and the ability to learn and compensate for possible errors in the low-level controller executing the motion. We demonstrate the advantage of our approach with respect to both optimization-based and end-to-end RL approaches.
    
[^129]: 大型视觉语言模型中幻觉的评估与分析

    Evaluation and Analysis of Hallucination in Large Vision-Language Models. (arXiv:2308.15126v1 [cs.LG])

    [http://arxiv.org/abs/2308.15126](http://arxiv.org/abs/2308.15126)

    本文提出了基于大型语言模型的幻觉评估框架HaELM，可以评估大型视觉语言模型中的幻觉问题，并分析了导致幻觉的因素，并提出了缓解幻觉问题的建议。

    

    最近，大型视觉语言模型（LVLMs）取得了显著的成功。然而，LVLMs仍然存在幻觉问题，这限制了在许多场景中的实用性。幻觉指的是LVLMs响应中不存在于视觉输入中的信息，这可能导致重大后果的潜在风险。目前对LVLMs中的幻觉评估的研究工作有限。在本文中，我们提出了基于大型语言模型（LLM）的幻觉评估框架HaELM。HaELM的性能近似于ChatGPT的95%，并具有低成本、可复现、保护隐私和本地部署等额外优势。利用HaELM，我们评估了当前LVLMs中的幻觉。此外，我们分析了导致LVLMs中幻觉的因素，并提出了缓解幻觉问题的有用建议。

    Large Vision-Language Models (LVLMs) have recently achieved remarkable success. However, LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios. Hallucination refers to the information of LVLMs' responses that does not exist in the visual input, which poses potential risks of substantial consequences. There has been limited work studying hallucination evaluation in LVLMs. In this paper, we propose Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based hallucination evaluation framework. HaELM achieves an approximate 95% performance comparable to ChatGPT and has additional advantages including low cost, reproducibility, privacy preservation and local deployment. Leveraging the HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem. Our training data and human annotation halluci
    
[^130]: 通过Mixup增强的元学习方法实现蛋白质模拟器的高效微调

    Mixup-Augmented Meta-Learning for Sample-Efficient Fine-Tuning of Protein Simulators. (arXiv:2308.15116v1 [cs.LG])

    [http://arxiv.org/abs/2308.15116](http://arxiv.org/abs/2308.15116)

    本文通过Mixup增强的元学习方法实现了对蛋白质模拟器的高效微调，可以在有限的训练数据下泛化到未见过的场景，并提供了一种通用的模拟连续动态条件的方法。

    

    分子动力学模拟已经成为研究生物分子的基本工具。与此同时，我们希望在分子能够波动的各种条件下对一组粒子进行模拟。本文中，我们将软提示学习方法应用于分子动力学任务并进行了适应性探索。我们的模型可以在有限的训练数据下非常好地泛化到未见过的和超出分布的场景。虽然我们的工作以温度为测试案例，但我们的方法的多功能性使其可以通过任何连续的动态条件（如压力和体积）进行有效模拟。我们的框架有两个阶段：1）使用数据混合技术进行预训练，增强分子结构数据和温度提示，然后通过逐渐增加比例的方式应用课程学习方法。2）基于元学习的微调框架提高了微调过程的样本效率，并为软提示微调提供更好的表现。

    Molecular dynamics simulations have emerged as a fundamental instrument for studying biomolecules. At the same time, it is desirable to perform simulations of a collection of particles under various conditions in which the molecules can fluctuate. In this paper, we explore and adapt the soft prompt-based learning method to molecular dynamics tasks. Our model can remarkably generalize to unseen and out-of-distribution scenarios with limited training data. While our work focuses on temperature as a test case, the versatility of our approach allows for efficient simulation through any continuous dynamic conditions, such as pressure and volumes. Our framework has two stages: 1) Pre-trains with data mixing technique, augments molecular structure data and temperature prompts, then applies a curriculum learning method by increasing the ratio of them smoothly. 2) Meta-learning-based fine-tuning framework improves sample-efficiency of fine-tuning process and gives the soft prompt-tuning better 
    
[^131]: 基于分层强化学习的未知网络传播控制方法

    Spread Control Method on Unknown Networks Based on Hierarchical Reinforcement Learning. (arXiv:2308.14311v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.14311](http://arxiv.org/abs/2308.14311)

    本论文提出了一种基于分层强化学习的方法，用于在未知网络上进行流行病控制。模拟结果表明，该方法优于基线方法。

    

    COVID-19等流行病对公共卫生和社会构成严重威胁，因此有必要研究有效的方法来控制流行病在网络中的传播。以往关于流行病控制的研究往往假设完全了解网络结构，这在实际情况中很少成立。本文研究了在未知结构的网络上的流行病控制，并提出了一种基于分层强化学习的框架，以同时探索网络结构和控制流行病。为了减少行动空间和实现可计算性，我们的框架包含三个模块：策略选择模块，确定是探索结构还是移除节点以控制流行病；探索模块，负责选择要探索的节点；移除模块，决定移除哪些节点以停止流行病传播。模拟结果表明，我们提出的方法优于基线方法。

    Epidemics such as COVID-19 pose serious threats to public health and our society, and it is critical to investigate effective methods to control the spread of epidemics over networks. Prior works on epidemic control often assume complete knowledge of network structures, a presumption seldom valid in real-world situations. In this paper, we study epidemic control on networks with unknown structures, and propose a hierarchical reinforcement learning framework for joint network structure exploration and epidemic control. To reduce the action space and achieve computation tractability, our proposed framework contains three modules: the Policy Selection Module, which determines whether to explore the structure or remove nodes to control the epidemic; the Explore Module, responsible for selecting nodes to explore; and the Remove Module, which decides which nodes to remove to stop the epidemic spread. Simulation results show that our proposed method outperforms baseline methods.
    
[^132]: 授权临床医生并民主化数据科学：大型语言模型自动化临床研究的机器学习。 (arXiv:2308.14120v2 [cs.LG] 更新版)

    Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies. (arXiv:2308.14120v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.14120](http://arxiv.org/abs/2308.14120)

    chatGPT ADA是一种能够自主开发临床研究所需的最先进的机器学习模型的大型语言模型，可将高级分析工具民主化，使非数据科学家的临床医生能够轻松应用于医学领域。

    

    机器学习（ML）开发者（如数据科学家）和从业者（如临床医生）之间存在知识差距，阻碍了ML在临床数据分析中的充分利用。我们研究了chatGPT Advanced Data Analysis（ADA），即GPT-4的扩展，来弥合这一差距并高效执行ML分析的潜力。我们向chatGPT ADA提供了各种医学专业的大型试验的真实临床数据和研究详细信息，没有给出具体指导。ChatGPT ADA基于原始研究的训练数据自主开发了最先进的ML模型，用于预测临床结果，如癌症发展、癌症进展、疾病并发症或致病基因序列等生物标志物。令人惊讶的是，这些ML模型与其已发表的对应物相匹配甚至表现更好。我们得出结论，chatGPT ADA为民主化医学中的ML提供了一个有前景的途径，使非ML专家能够获得先进的分析工具并推动广泛应用。

    A knowledge gap persists between Machine Learning (ML) developers (e.g., data scientists) and practitioners (e.g., clinicians), hampering the full utilization of ML for clinical data analysis. We investigated the potential of the chatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this gap and perform ML analyses efficiently. Real-world clinical datasets and study details from large trials across various medical specialties were presented to chatGPT ADA without specific guidance. ChatGPT ADA autonomously developed state-of-the-art ML models based on the original study's training data to predict clinical outcomes such as cancer development, cancer progression, disease complications, or biomarkers such as pathogenic gene sequences. Strikingly, these ML models matched or outperformed their published counterparts. We conclude that chatGPT ADA offers a promising avenue to democratize ML in medicine, making advanced analytics accessible to non-ML experts and promoting broa
    
[^133]: 张量压缩的反向传播免费训练（物理信息）的神经网络

    Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural Networks. (arXiv:2308.09858v1 [cs.LG])

    [http://arxiv.org/abs/2308.09858](http://arxiv.org/abs/2308.09858)

    本文提出了一个完全无需反向传播的神经网络训练框架，并通过张量压缩的方差约减方法和混合梯度评估方法改进了优化和效率。同时，还扩展了框架用于物理信息的神经网络的估计。

    

    反向传播（BP）被广泛用于神经网络训练中计算梯度。然而，由于缺乏硬件和软件资源来支持自动微分，在边缘设备上实现BP是困难的。这大大增加了设备上训练加速器的设计复杂性和上市时间。本文提出了一个完全无需BP的框架，只需要前向传播就可以训练实际的神经网络。我们的技术贡献有三个方面。首先，我们提出了一种张量压缩的方差约减方法，极大提高了零阶（ZO）优化的可扩展性，使其能够处理大于以前ZO方法能力的网络尺寸。其次，我们提出了一种混合梯度评估方法，提高了ZO训练的效率。最后，我们通过提出一种稀疏格方法来扩展我们的BP-free训练框架，用于物理信息的神经网络（PINNs）的估计。

    Backward propagation (BP) is widely used to compute the gradients in neural network training. However, it is hard to implement BP on edge devices due to the lack of hardware and software resources to support automatic differentiation. This has tremendously increased the design complexity and time-to-market of on-device training accelerators. This paper presents a completely BP-free framework that only requires forward propagation to train realistic neural networks. Our technical contributions are three-fold. Firstly, we present a tensor-compressed variance reduction approach to greatly improve the scalability of zeroth-order (ZO) optimization, making it feasible to handle a network size that is beyond the capability of previous ZO approaches. Secondly, we present a hybrid gradient evaluation approach to improve the efficiency of ZO training. Finally, we extend our BP-free training framework to physics-informed neural networks (PINNs) by proposing a sparse-grid approach to estimate the 
    
[^134]: 图结构残差：一种诊断的学习方法

    Graph Structural Residuals: A Learning Approach to Diagnosis. (arXiv:2308.06961v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.06961](http://arxiv.org/abs/2308.06961)

    本文提出了一种新颖的框架，将模型诊断的概念与深度图结构学习相结合，通过数据学习系统的底层结构并提供动态观察。研究通过重新定义系统表示、观察和故障的构建，引入自监督图结构学习模型以及在耦合振荡器系统上的实验，展示了数据驱动的诊断方法的潜力。

    

    传统的基于模型的诊断依赖于构建明确的系统模型，这个过程可能费时且需要专业知识。本文提出了一种新颖的框架，将模型诊断的概念与深度图结构学习相结合。这种数据驱动的方法利用数据学习系统的底层结构，并提供由两个不同的图邻接矩阵表示的动态观察。我们的工作通过三个主要贡献实现了图结构学习与模型诊断的无缝集成：(i)重新定义系统表示、观察和故障的构建、(ii)引入两种不同版本的自监督图结构学习模型架构、(iii)通过耦合振荡器系统的实验展示了我们数据驱动的诊断方法的潜力。

    Traditional model-based diagnosis relies on constructing explicit system models, a process that can be laborious and expertise-demanding. In this paper, we propose a novel framework that combines concepts of model-based diagnosis with deep graph structure learning. This data-driven approach leverages data to learn the system's underlying structure and provide dynamic observations, represented by two distinct graph adjacency matrices. Our work facilitates a seamless integration of graph structure learning with model-based diagnosis by making three main contributions: (i) redefining the constructs of system representation, observations, and faults (ii) introducing two distinct versions of a self-supervised graph structure learning model architecture and (iii) demonstrating the potential of our data-driven diagnostic method through experiments on a system of coupled oscillators.
    
[^135]: Apple Vision Pro for Healthcare: “终极显示器”？（arXiv:2308.04313v1 [cs.AI]）

    Apple Vision Pro for Healthcare: "The Ultimate Display"?. (arXiv:2308.04313v1 [cs.AI])

    [http://arxiv.org/abs/2308.04313](http://arxiv.org/abs/2308.04313)

    苹果推出了Vision Pro，一款具有混合现实和增强现实功能的虚拟现实设备，拥有独特的特点，例如内部屏幕展示佩戴者的眼睛以及数字皇冠按钮的融合功能。这款无线设备可能实现了“终极显示器”的潜力。

    

    在2023年6月的全球开发者大会（WWDC）上，苹果推出了Vision Pro。Vision Pro是一款混合现实（MR）头盔，更具体地说，它是一款具有额外视频透视（VST）能力的虚拟现实（VR）设备。通过将真实世界通过摄像头传输到用户眼前的（VR）屏幕，使得Vision Pro也成为了增强现实（AR）设备。当然，这并不独特，与Varjo XR-3等其他设备类似。尽管如此，Vision Pro具有一些有趣的特点，例如内部屏幕可以向“外界”显示佩戴头盔者的眼睛，或者顶部的一个按钮称为“数字皇冠”，可以通过旋转无缝地融合数字内容与物理空间。此外，Vision Pro是无线的，只有电池的电缆连接，这使得头盔比Varjo XR-3更加灵活。这可能更接近“终极显示器”。

    At the Worldwide Developers Conference (WWDC) in June 2023, Apple introduced the Vision Pro. The Vision Pro is a Mixed Reality (MR) headset, more specifically it is a Virtual Reality (VR) device with an additional Video See-Through (VST) capability. The VST capability turns the Vision Pro also into an Augmented Reality (AR) device. The AR feature is enabled by streaming the real world via cameras to the (VR) screens in front of the user's eyes. This is of course not unique and similar to other devices, like the Varjo XR-3. Nevertheless, the Vision Pro has some interesting features, like an inside-out screen that can show the headset wearers' eyes to "outsiders" or a button on the top, called "Digital Crown", that allows you to seamlessly blend digital content with your physical space by turning it. In addition, it is untethered, except for the cable to the battery, which makes the headset more agile, compared to the Varjo XR-3. This could actually come closer to the "Ultimate Display",
    
[^136]: ForestMonkey：基于人工智能的缺陷检测和分类模型推理工具包的设计

    ForestMonkey: Toolkit for Reasoning with AI-based Defect Detection and Classification Models. (arXiv:2307.13815v1 [cs.AI])

    [http://arxiv.org/abs/2307.13815](http://arxiv.org/abs/2307.13815)

    本文介绍了Forest Monkey（FM）工具包，它是一个用于推理任何基于AI的缺陷检测和/或分类模型输出结果的工具包。该工具包提供了可解释性的数据和图表，以帮助用户理解推理结果并提出改进建议。

    

    最近人工智能（AI）推理和可解释AI（XAI）任务越来越受欢迎，使用户能够解释AI模型的预测或决策过程。本文介绍了Forest Monkey（FM）工具包，它是为了推理任何基于AI的缺陷检测和/或分类模型的输出而设计的，并具备数据可解释性。作为一个Python软件包实现，FM以数据集文件夹路径的形式作为输入（包括原始图像、真实标签和预测标签），并提供一组图表和文本文件以说明推理结果，并提出可能的改进。FM工具包包括从预测提取特征到推理目标、从图像提取特征到缺陷特征以及基于决策树的AI推理器等过程。此外，本文还研究了FM工具包在应用于四个不同数据集的AI模型时的时间性能。最后，还提供了一个教程来指导用户。

    Artificial intelligence (AI) reasoning and explainable AI (XAI) tasks have gained popularity recently, enabling users to explain the predictions or decision processes of AI models. This paper introduces Forest Monkey (FM), a toolkit designed to reason the outputs of any AI-based defect detection and/or classification model with data explainability. Implemented as a Python package, FM takes input in the form of dataset folder paths (including original images, ground truth labels, and predicted labels) and provides a set of charts and a text file to illustrate the reasoning results and suggest possible improvements. The FM toolkit consists of processes such as feature extraction from predictions to reasoning targets, feature extraction from images to defect characteristics, and a decision tree-based AI-Reasoner. Additionally, this paper investigates the time performance of the FM toolkit when applied to four AI models with different datasets. Lastly, a tutorial is provided to guide users
    
[^137]: 关于注意力网络学习动态的研究

    On the learning Dynamics of Attention Networks. (arXiv:2307.13421v1 [cs.LG])

    [http://arxiv.org/abs/2307.13421](http://arxiv.org/abs/2307.13421)

    本研究分析了软注意力、硬注意力和潜变量边际似然（LVML）注意力三种注意力模型的学习动态，发现了它们在所选择的片段聚合方式上的显著差异，并解释了分类模型在梯度下降下的演化对最终结果的影响。

    

    注意力模型通常通过优化三个标准损失函数之一来学习，分别称为软注意力、硬注意力和潜变量边际似然（LVML）注意力。这三种范式都是为了达到相同的目标，即找到两个模型：一个“焦点”模型，用于“选择”输入中的正确“片段”，和一个“分类”模型，用于将选定的片段处理成目标标签。然而，它们在所选择的片段聚合方式上存在显著差异，导致了不同的动态和最终结果。我们观察到使用这些范式学习的模型具有独特的特征，并将其解释为在焦点模型固定时，分类模型在梯度下降下的演化所致。我们还在一个简单的设置中分析了这些范式，并推导出梯度流下参数轨迹的闭式表达式。在软注意力损失下，焦点模型在初始化阶段快速改善。

    Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. All three paradigms are motivated by the same goal of finding two models -- a `focus' model that `selects' the right \textit{segment} of the input and a `classification' model that processes the selected segment into the target label. However, they differ significantly in the way the selected segments are aggregated, resulting in distinct dynamics and final results. We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed. We also analyze these paradigms in a simple setting and derive closed-form expressions for the parameter trajectory under gradient flow. With the soft attention loss, the focus model improves quickly at initialization a
    
[^138]: TF-ICON: 基于扩散的无需训练的跨领域图像合成

    TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition. (arXiv:2307.12493v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.12493](http://arxiv.org/abs/2307.12493)

    TF-ICON是一种无需训练的图像合成框架，利用文字驱动的扩散模型实现跨领域图像导向合成。与传统方法相比，TF-ICON可以在不需额外训练、微调或优化的情况下实现高质量的无缝合成，同时引入了例外提示来准确地反转真实图像为潜在表示。

    

    文字驱动的扩散模型展示出令人印象深刻的生成能力，可以实现各种图像编辑任务。在本文中，我们提出了TF-ICON，一种新颖的无需训练的图像合成框架，利用文字驱动的扩散模型来进行跨领域图像导向合成。该任务旨在将用户提供的对象无缝地整合到特定的视觉环境中。目前的基于扩散的方法通常涉及昂贵的基于实例的优化或在定制数据集上微调预训练模型，可能会损害其丰富的先验知识。相反，TF-ICON可以利用现成的扩散模型进行跨领域图像导向合成，无需额外的训练、微调或优化。此外，我们引入了例外提示(含无信息)来帮助文字驱动的扩散模型准确地将真实图像反转为潜在表示，为合成提供基础。我们的实验结果表明，TF-ICON在不同的合成任务中具有优越的表现，并且可以在不同领域的图像之间进行高质量的无缝合成。

    Text-driven diffusion models have exhibited impressive generative capabilities, enabling various image editing tasks. In this paper, we propose TF-ICON, a novel Training-Free Image COmpositioN framework that harnesses the power of text-driven diffusion models for cross-domain image-guided composition. This task aims to seamlessly integrate user-provided objects into a specific visual context. Current diffusion-based methods often involve costly instance-based optimization or finetuning of pretrained models on customized datasets, which can potentially undermine their rich prior. In contrast, TF-ICON can leverage off-the-shelf diffusion models to perform cross-domain image-guided composition without requiring additional training, finetuning, or optimization. Moreover, we introduce the exceptional prompt, which contains no information, to facilitate text-driven diffusion models in accurately inverting real images into latent representations, forming the basis for compositing. Our experim
    
[^139]: 基于形态学图像分析和特征提取的AI缺陷检测和分类模型推理

    Morphological Image Analysis and Feature Extraction for Reasoning with AI-based Defect Detection and Classification Models. (arXiv:2307.11643v1 [cs.CV])

    [http://arxiv.org/abs/2307.11643](http://arxiv.org/abs/2307.11643)

    本文提出了一个名为AI推理器的解释性模型，能够从图像中提取缺陷的形态学特征，并利用决策树进行推理。它通过可视化和文字说明解释基于掩模的缺陷检测和分类模型的输出，并提供有效的缓解策略以提升整体模型性能。

    

    随着人工智能模型在工程和制造等行业的使用越来越普遍，这些模型提供透明的推理以解释其预测结果变得至关重要。本文提出了AI推理器，该推理器从图像中提取缺陷的形态学特征，并利用决策树对缺陷特征进行推理。然后，AI推理器通过可视化图表和文字说明提供对基于掩模的缺陷检测和分类模型的输出进行解释。它还提供了有效的缓解策略，以提升数据预处理和整体模型性能。AI推理器在使用366张含有缺陷的图像集合上测试了解释IE Mask R-CNN模型输出的能力。结果表明，AI推理器在解释IE Mask R-CNN模型的预测方面具有很高的效果。总而言之，所提出的AI推理器为改善模型性能提供了一个解决方案。

    As the use of artificial intelligent (AI) models becomes more prevalent in industries such as engineering and manufacturing, it is essential that these models provide transparent reasoning behind their predictions. This paper proposes the AI-Reasoner, which extracts the morphological characteristics of defects (DefChars) from images and utilises decision trees to reason with the DefChar values. Thereafter, the AI-Reasoner exports visualisations (i.e. charts) and textual explanations to provide insights into outputs made by masked-based defect detection and classification models. It also provides effective mitigation strategies to enhance data pre-processing and overall model performance. The AI-Reasoner was tested on explaining the outputs of an IE Mask R-CNN model using a set of 366 images containing defects. The results demonstrated its effectiveness in explaining the IE Mask R-CNN model's predictions. Overall, the proposed AI-Reasoner provides a solution for improving the performanc
    
[^140]: 三思而后行：大型语言模型不确定性测量的探索性研究

    Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models. (arXiv:2307.10236v1 [cs.SE])

    [http://arxiv.org/abs/2307.10236](http://arxiv.org/abs/2307.10236)

    本研究从不确定性的角度对大型语言模型进行了探索性研究，通过实验发现不确定性估计方法在探索和抵制大型语言模型的不良行为方面具有潜力。

    

    大型语言模型（LLMs）的最近性能突破为众多工业应用和领域提供了新的机遇。然而，LLMs的错误生成，如虚假预测、错误信息和幻觉，也引发了对LLMs可靠性的严重关注，尤其在对安全、可靠性有敏感的场景中，可能阻碍其在实际中的应用。尽管不确定性估计已经显示出其在解释一般机器学习（ML）模型的预测风险方面的潜力，但关于它是否以及在多大程度上有助于探索LLMs的能力和抵制其不良行为方面知之甚少。为了弥合这一差距，本文从不确定性的角度开展了关于LLMs风险评估的探索性研究。具体来说，我们使用12种不确定性估计方法和4个LLMs在4个重要的自然语言处理（NLP）任务上进行实验，以调查不确定性在探索LLMs能力和对抗其不良行为方面的程度。

    The recent performance leap of Large Language Models (LLMs) opens up new opportunities across numerous industrial applications and domains. However, erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions. While uncertainty estimation has shown its potential for interpreting the prediction risks made by general machine learning (ML) models, little is known about whether and to what extent it can help explore an LLM's capabilities and counteract its undesired behavior. To bridge the gap, in this paper, we initiate an exploratory study on the risk assessment of LLMs from the lens of uncertainty. In particular, we experiment with twelve uncertainty estimation methods and four LLMs on four prominent natural language processing (NLP) tasks to investigate to what extent unc
    
[^141]: 棋盘上的棋子价值。 (arXiv:2307.05330v1 [cs.AI])

    The Value of Chess Squares. (arXiv:2307.05330v1 [cs.AI])

    [http://arxiv.org/abs/2307.05330](http://arxiv.org/abs/2307.05330)

    本研究通过引入边际估值对国际象棋棋盘上的棋子和棋盘进行评价，提供了关于马、象和兵的有价值的见解。

    

    我们的研究的主要目标是评估棋盘上棋子的价值，并确定棋子在棋盘上的摆放位置。随着国际象棋人工智能的出现，我们能够准确评估国际象棋局面的价值。传统方法对棋子赋予固定的价值$(\symking=\infty, \symqueen=9, \symrook=5, \symbishop=3, \symknight=3, \sympawn=1)$。我们通过引入棋子和棋盘方面的边际估值来改进这种分析。我们通过研究马和象的位置，并提供有关兵的价值的宝贵见解来演示我们的方法。值得注意的是，尼姆佐维奇是倡导兵的结构和价值的先驱之一。最后，我们提出了未来研究的潜在方向。

    Valuing chess squares and determining the placement of pieces on the board are the main objectives of our study. With the emergence of chess AI, it has become possible to accurately assess the worth of positions in a game of chess. The conventional approach assigns fixed values to pieces $(\symking=\infty, \symqueen=9, \symrook=5, \symbishop=3, \symknight=3, \sympawn=1)$. We enhance this analysis by introducing marginal valuations for both pieces and squares. We demonstrate our method by examining the positioning of Knights and Bishops, and also provide valuable insights into the valuation of pawns. Notably, Nimzowitsch was among the pioneers in advocating for the significance of Pawn structure and valuation. Finally, we conclude by suggesting potential avenues for future research.
    
[^142]: ELM神经元：一种高效且表达力强的皮层神经元模型可以解决长时间跨度任务

    The ELM Neuron: an Efficient and Expressive Cortical Neuron Model Can Solve Long-Horizon Tasks. (arXiv:2306.16922v1 [cs.NE])

    [http://arxiv.org/abs/2306.16922](http://arxiv.org/abs/2306.16922)

    ELM神经元是一种高效且表达力强的皮层神经元模型，它只需要8K个参数就能准确模拟复杂的计算任务。

    

    传统的大规模神经科学模型和机器学习利用简化的个体神经元模型，依靠集体活动和适当调整的连接来执行复杂的计算。然而，每个生物皮层神经元本质上都是一个复杂的计算设备，最近的一项研究证实了这一点，该研究中，需要一个具有数百万个参数的深度人工神经网络来复制详细生物物理模型的输入输出关系。我们对这些多个参数的必要性提出了质疑，并引入了表达力强的泄漏存储器（ELM）神经元，这是一种受生物启发的计算模型，具有高计算表达力，同时也非常高效。值得注意的是，我们的ELM神经元仅需要8,000个可训练参数就能准确匹配前述的输入输出关系。我们发现，准确的模型需要多个类似于存储器的隐藏状态和复杂的非线性突触整合。

    Traditional large-scale neuroscience models and machine learning utilize simplified models of individual neurons, relying on collective activity and properly adjusted connections to perform complex computations. However, each biological cortical neuron is inherently a sophisticated computational device, as corroborated in a recent study where it took a deep artificial neural network with millions of parameters to replicate the input-output relationship of a detailed biophysical model of a cortical pyramidal neuron. We question the necessity for these many parameters and introduce the Expressive Leaky Memory (ELM) neuron, a biologically inspired, computationally expressive, yet efficient model of a cortical neuron. Remarkably, our ELM neuron requires only 8K trainable parameters to match the aforementioned input-output relationship accurately. We find that an accurate model necessitates multiple memory-like hidden states and intricate nonlinear synaptic integration. To assess the comput
    
[^143]: 稀疏模型汤：通过模型平均改进修剪的方法

    Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging. (arXiv:2306.16788v1 [cs.LG])

    [http://arxiv.org/abs/2306.16788](http://arxiv.org/abs/2306.16788)

    本研究通过将多个经过迭代幅度剪枝的模型进行平均，解决了同时利用稀疏性和参数平均的问题，并显著提升了泛化性能。

    

    神经网络可以通过剪枝显著压缩，从而得到稀疏模型，这些模型需要更少的存储和浮点运算，同时保持预测性能。模型汤（Wortsman等人，2022年）通过将多个模型的参数平均成一个单一模型来改善泛化和超出分布性能，而不增加推理时间。然而，识别处于相同损失区域的模型以同时利用稀疏性和参数平均是具有挑战性的，因为对任意稀疏模型进行平均会降低整体稀疏度，原因是不同的稀疏连接性。在这项工作中，我们通过展示在迭代幅度剪枝（IMP）的单次重新训练阶段中探索不同的超参数配置（例如批次排序或权重衰减）产生的模型适合进行平均，并且通过设计共享相同的稀疏连接性来解决这些挑战。平均这些模型显著提升了泛化性能。

    Neural networks can be significantly compressed by pruning, leading to sparse models requiring considerably less storage and floating-point operations while maintaining predictive performance. Model soups (Wortsman et al., 2022) improve generalization and out-of-distribution performance by averaging the parameters of multiple models into a single one without increased inference time. However, identifying models in the same loss basin to leverage both sparsity and parameter averaging is challenging, as averaging arbitrary sparse models reduces the overall sparsity due to differing sparse connectivities. In this work, we address these challenges by demonstrating that exploring a single retraining phase of Iterative Magnitude Pruning (IMP) with varying hyperparameter configurations, such as batch ordering or weight decay, produces models that are suitable for averaging and share the same sparse connectivity by design. Averaging these models significantly enhances generalization performanc
    
[^144]: Tensorformer: 高质量点云重建的归一化矩阵注意力变换器

    Tensorformer: Normalized Matrix Attention Transformer for High-quality Point Cloud Reconstruction. (arXiv:2306.15989v1 [cs.GR])

    [http://arxiv.org/abs/2306.15989](http://arxiv.org/abs/2306.15989)

    Tensorformer是一种归一化矩阵注意力变换器，用于高质量的点云重建。它通过矩阵注意力实现了逐点和逐通道的消息传递，提供了更好的局部几何建模能力，并在两个数据集上取得了最先进的结果。

    

    在计算机图形学界，从原始点云进行表面重建的研究已经进行了几十年，这在现今的建模和渲染应用中需求非常高。传统的解决方案，如Poisson表面重建，需要额外的点法线输入以产生合理的结果。现代基于变换器的方法可以在没有法线的情况下工作，但由于离散点的局部融合编码性能有限，结果较为粗糙。我们引入了一种新颖的归一化矩阵注意力变换器（Tensorformer）来进行高质量的重建。所提出的矩阵注意力允许同时进行逐点和逐通道的消息传递，而之前的向量注意力在不同通道之间丢失了相邻点的信息。它在特征学习中带来更多自由度，从而更好地建模局部几何结构。我们的方法在两个常用数据集ShapeNetCore和ABC上达到了最先进的水平，并且

    Surface reconstruction from raw point clouds has been studied for decades in the computer graphics community, which is highly demanded by modeling and rendering applications nowadays. Classic solutions, such as Poisson surface reconstruction, require point normals as extra input to perform reasonable results. Modern transformer-based methods can work without normals, while the results are less fine-grained due to limited encoding performance in local fusion from discrete points. We introduce a novel normalized matrix attention transformer (Tensorformer) to perform high-quality reconstruction. The proposed matrix attention allows for simultaneous point-wise and channel-wise message passing, while the previous vector attention loses neighbor point information across different channels. It brings more degree of freedom in feature learning and thus facilitates better modeling of local geometries. Our method achieves state-of-the-art on two commonly used datasets, ShapeNetCore and ABC, and 
    
[^145]: RL$^3$:通过RL内部的RL$^2$提升元强化学习方法

    RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$. (arXiv:2306.15909v1 [cs.LG])

    [http://arxiv.org/abs/2306.15909](http://arxiv.org/abs/2306.15909)

    RL$^3$是一种原则性混合方法，通过将传统强化学习学到的任务特定动作值作为元强化学习神经网络的输入，提高了元强化学习的性能。

    

    元强化学习（meta-RL）方法，如RL$^2$，已经成为学习针对给定任务分布的数据高效的强化学习算法的有希望的方法。然而，这些强化学习算法在长期任务和超出分布任务方面存在困难，因为它们依赖于递归神经网络来处理经验序列，而不是将它们总结为一般的强化学习组件，例如价值函数。此外，即使是transformers在训练和推理成本变得禁止之前也对它们可以有效推理的历史长度有实际限制。相比之下，传统的强化学习算法在数据效率方面不足，因为它们没有利用领域知识，但随着更多数据的可用性，它们会收敛到最优策略。在本文中，我们提出了RL$^3$，一种组合了传统强化学习和元强化学习的原则性混合方法，通过将通过传统强化学习学习到的特定任务动作值作为元强化学习神经网络的一个输入。

    Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as promising approaches for learning data-efficient RL algorithms tailored to a given task distribution. However, these RL algorithms struggle with long-horizon tasks and out-of-distribution tasks since they rely on recurrent neural networks to process the sequence of experiences instead of summarizing them into general RL components such as value functions. Moreover, even transformers have a practical limit to the length of histories they can efficiently reason about before training and inference costs become prohibitive. In contrast, traditional RL algorithms are data-inefficient since they do not leverage domain knowledge, but they do converge to an optimal policy as more data becomes available. In this paper, we propose RL$^3$, a principled hybrid approach that combines traditional RL and meta-RL by incorporating task-specific action-values learned through traditional RL as an input to the meta-RL neural netw
    
[^146]: 人工智能灾难性风险综述

    An Overview of Catastrophic AI Risks. (arXiv:2306.12001v1 [cs.CY])

    [http://arxiv.org/abs/2306.12001](http://arxiv.org/abs/2306.12001)

    本文综述了人工智能灾难性风险的四个主要来源，包括恶意使用、人工智能竞赛、组织风险和流氓人工智能。

    

    人工智能的快速发展引起了专家、政策制定者和世界各国领导人对越来越先进的人工智能系统可能带来灾难性风险的担忧。虽然已经有很多风险被单独详细介绍过，但迫切需要系统地讨论和说明潜在危险，以更好地支持减轻这些风险的努力。本文概述了人工智能灾难性风险的主要来源，我们将其分为四个类别：恶意使用，即个人或团体有意使用人工智能造成伤害；人工智能竞赛，即竞争环境促使行动者部署不安全的人工智能或放弃控制权交给人工智能；组织风险，突出人为和复杂系统如何增加灾难性事故发生的可能性；以及流氓人工智能，描述了控制比人类智能更高的代理程序困难的固有难题。对于每个风险类别，我们描述了具体的危害。

    Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards,
    
[^147]: Strokes2Surface：从四维建筑设计素描中恢复曲线网络

    Strokes2Surface: Recovering Curve Networks From 4D Architectural Design Sketches. (arXiv:2306.07220v2 [cs.GR] UPDATED)

    [http://arxiv.org/abs/2306.07220](http://arxiv.org/abs/2306.07220)

    本文介绍了Strokes2Surface，它可从建筑师的笔画中恢复出曲线网络，对于建筑设计中的概念设计和数字建模之间的桥梁具有重要意义。

    

    本文介绍了一个离线几何重建管道Strokes2Surface，它是基于4D Sketching Interface，MR.Sketch的目标是面向建筑设计的。该管道从设计师绘制的笔画中恢复曲线网络，因此在建筑设计的概念设计和数字建模阶段之间建立了桥梁。我们的管道的输入包括3D笔画的折线顶点及其相应的时间戳（作为第四个维度），以及额外的几何和笔触相关的记录属性。基于素描合并和基于素描建模方法的启发，我们的管道利用这些数据并组合三个机器学习（ML）模型；一个分类器和两个聚类模型。特别是，根据建筑设计素描中设计师通常采用的实践观察，我们解决了一个二元分类问题，以识别一笔画是描绘边界和边缘还是用于填充所需建筑物的封闭区域和表面。

    We present Strokes2Surface, an offline geometry-reconstruction pipeline built upon a 4D Sketching Interface, MR.Sketch, targeted at architectural design. The pipeline recovers a curve network from designer-drawn strokes, thus bridging between concept design and digital modeling stages in architectural design. The input to our pipeline consists of 3D strokes' polyline vertices and their corresponding timestamps (as of the fourth dimension), along with additional geometric and stylus-related recorded properties. Inspired by sketch consolidation and sketch-based modeling methods, our pipeline leverages such data and combines three Machine Learning (ML) models; a classifier and two clustering models. In particular, based on observations of practices designers typically employ in architectural design sketches, we solve a binary classification problem to recognize whether a stroke depicts a boundary and edge or is used to fill in the enclosing areas and faces of the intended architectural ob
    
[^148]: L-C2ST: 基于本地诊断实现模拟推断中后验近似

    L-C2ST: Local Diagnostics for Posterior Approximations in Simulation-Based Inference. (arXiv:2306.03580v1 [stat.ML])

    [http://arxiv.org/abs/2306.03580](http://arxiv.org/abs/2306.03580)

    本文提出了一种名为 L-C2ST 的基于本地诊断实现模拟推断中后验近似的新方法，其可以在任何给定的观测下本地评估后验估计器，有效地解决了目前评估后验估计器限制解决方法的问题。

    

    最近许多模拟推断（SBI）的工作都依赖于深度生成模型来近似复杂、高维度的后验分布。然而，评估这些近似是否可信仍是一个挑战。大多数方法仅在观测空间期望下评估后验估计器。这限制了它们的可解释性，并不能足够地确定哪些观测结果可以信任这些近似或应该改进。我们基于著名的分类器两样本检验 (C2ST)，引入 L-C2ST，一个新方法，允许在任何给定的观测下本地评估后验估计器。它提供有理论基础和易于解释的，如图示诊断。与 C2ST 不同的是，L-C2ST 不需要访问真实后验的样本。对于基于归一化流的后验估计器，L-C2ST 可以专门提供更好的统计功率，同时计算效率更高。

    Many recent works in simulation-based inference (SBI) rely on deep generative models to approximate complex, high-dimensional posterior distributions. However, evaluating whether or not these approximations can be trusted remains a challenge. Most approaches evaluate the posterior estimator only in expectation over the observation space. This limits their interpretability and is not sufficient to identify for which observations the approximation can be trusted or should be improved. Building upon the well-known classifier two-sample test (C2ST), we introduce L-C2ST, a new method that allows for a local evaluation of the posterior estimator at any given observation. It offers theoretically grounded and easy to interpret - e.g. graphical - diagnostics, and unlike C2ST, does not require access to samples from the true posterior. In the case of normalizing flow-based posterior estimators, L-C2ST can be specialized to offer better statistical power, while being computationally more efficien
    
[^149]: 扩散模型是多任务强化学习的有效规划器和数据合成器

    Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning. (arXiv:2305.18459v1 [cs.LG])

    [http://arxiv.org/abs/2305.18459](http://arxiv.org/abs/2305.18459)

    本文研究了单一扩散模型在建模多个任务的策略训练的大规模离线数据时的有效性。该方法名为Multi-Task Diffusion Model (MTDiff)，结合了Transformer骨干和提示学习，能够在如此复杂的多任务环境下取得相当不错的性能。

    

    扩散模型在视觉和NLP领域中展现出了高度表现力的生成能力。最近的研究表明，在强化学习领域中，扩散模型也能够有效地建模离线数据集中的复杂策略或轨迹。然而，这些研究仅限于单任务设置，没有考虑多任务的情况。本文旨在研究单一扩散模型在建模用于多个任务策略训练的大规模离线数据时的有效性，具有多样化和多模态的数据分布。具体而言，我们提出了Multi-Task Diffusion Model（MTDiff），这是一种基于扩散的方法，结合Transformer骨干和提示学习，用于多任务离线设置中的生成规划和数据合成。MTDiff利用大量可用于多任务数据中的知识，并在任务之间执行隐式知识共享以进行虚拟规划。

    Diffusion models have demonstrated highly-expressive generative capabilities in vision and NLP. Recent studies in reinforcement learning (RL) have shown that diffusion models are also powerful in modeling complex policies or trajectories in offline datasets. However, these works have been limited to single-task settings where a generalist agent capable of addressing multi-task predicaments is absent. In this paper, we aim to investigate the effectiveness of a single diffusion model in modeling large-scale multi-task offline data, which can be challenging due to diverse and multimodal data distribution. Specifically, we propose Multi-Task Diffusion Model (\textsc{MTDiff}), a diffusion-based method that incorporates Transformer backbones and prompt learning for generative planning and data synthesis in multi-task offline settings. \textsc{MTDiff} leverages vast amounts of knowledge available in multi-task data and performs implicit knowledge sharing among tasks. For generative planning, 
    
[^150]: 基于技能的少样本选择以适应上下文学习

    Skill-Based Few-Shot Selection for In-Context Learning. (arXiv:2305.14210v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14210](http://arxiv.org/abs/2305.14210)

    本文提出了Skill-KNN，一种基于技能的少样本选择方法，用于上下文学习。它解决了现有方法的偏见问题并且不需要训练模型，适用于不断扩展或更改示例库的情况。

    

    在上下文学习中，通过提供一些示例来适应下游任务是一种范例。针对每个测试实例选择适当的示例是上下文学习中很重要的。在本文中，我们提出了一种基于技能的少样本选择方法，用于上下文学习。Skill-KNN的关键优势包括：（1）它解决了现有基于预训练嵌入的方法容易因为不重要的自然语言表面特征对目标任务产生偏见的问题；（2）它不需要训练或微调任何模型，适用于频繁扩展或更改示例库的情况。其关键见解是优化输入到嵌入模型中，而不是调整模型本身。在技术上，Skill-KNN通过利用预处理少样本提示为每个测试案例和候选示例生成基于技能的描述，从而消除了不重要的信息。

    In-context learning is the paradigm that adapts large language models to downstream tasks by providing a few examples. Few-shot selection -- selecting appropriate examples for each test instance separately -- is important for in-context learning. In this paper, we propose Skill-KNN, a skill-based few-shot selection method for in-context learning. The key advantages of Skill-KNN include: (1) it addresses the problem that existing methods based on pre-trained embeddings can be easily biased by surface natural language features that are not important for the target task; (2) it does not require training or fine-tuning of any models, making it suitable for frequently expanding or changing example banks. The key insight is to optimize the inputs fed into the embedding model, rather than tuning the model itself. Technically, Skill-KNN generates the skill-based descriptions for each test case and candidate example by utilizing a pre-processing few-shot prompting, thus eliminating unimportant 
    
[^151]: 表面相似性之下：大型语言模型通过结构推理进行合理的科学类比

    Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction. (arXiv:2305.12660v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12660](http://arxiv.org/abs/2305.12660)

    本论文介绍了一种基于结构推理的类比结构推断任务，旨在解决大型语言模型在进行科学类比时忽视结构的问题。

    

    人类认知中类比推理的重要作用使我们能够通过共享的关系结构将新概念与熟悉的概念联系起来。尽管先前的研究关注于词语类比，但本研究表明，大型语言模型（LLMs）经常忽视构成这些类比的结构，这引发了对词语类比作为类比推理技能（类似于人类认知）的有效性的质疑。为了解决这个问题，我们的论文引入了一种基于认知心理学的类比结构推理任务，旨在推断出连接两个系统之间的类比结构。为了支持这个任务，我们建立了一个名为SCAR的基准，包含来自13个不同领域的400个科学类比，旨在评估利用结构推理的类比推理能力。实证证据强调了LLMs，包括ChatGPT和GPT-4，在掌握这个任务上依然面临的挑战。

    The vital role of analogical reasoning in human cognition allows us to grasp novel concepts by linking them with familiar ones through shared relational structures. Despite the attention previous research has given to word analogies, this work suggests that Large Language Models (LLMs) often overlook the structures that underpin these analogies, raising questions about the efficacy of word analogies as a measure of analogical reasoning skills akin to human cognition. In response to this, our paper introduces a task of analogical structure abduction, grounded in cognitive psychology, designed to abduce structures that form an analogy between two systems. In support of this task, we establish a benchmark called SCAR, containing 400 scientific analogies from 13 distinct fields, tailored for evaluating analogical reasoning with structure abduction. The empirical evidence underlines the continued challenges faced by LLMs, including ChatGPT and GPT-4, in mastering this task, signifying the n
    
[^152]: Clifford群等变神经网络

    Clifford Group Equivariant Neural Networks. (arXiv:2305.11141v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.11141](http://arxiv.org/abs/2305.11141)

    我们引入了Clifford群等变神经网络，它可以构建O(n)和E(n)等变模型。该方法通过调整Clifford群的定义以及保持向量空间和乘法结构的作用来实现多个有利属性。

    

    我们引入了Clifford群等变神经网络：一种构建O(n)和E(n)等变模型的新方法。我们确定并研究了Clifford群，它是Clifford代数中的一个子群，其定义经过调整以实现多个有利属性。主要地，该群的作用形成了一个正交自同构，扩展到整个Clifford代数，同时尊重多矢分级。这导致了对应于多矢分解的多个非等价子表示。此外，我们证明该作用不仅尊重Clifford代数的向量空间结构，还尊重其乘法结构，即几何乘积。这些发现意味着我们可以得到在任意维的内积空间中优雅地推广的表达层。我们特别展示了从一个sin

    We introduce Clifford Group Equivariant Neural Networks: a novel approach for constructing $\mathrm{O}(n)$- and $\mathrm{E}(n)$-equivariant models. We identify and study the $\textit{Clifford group}$, a subgroup inside the Clifford algebra whose definition we adjust to achieve several favorable properties. Primarily, the group's action forms an orthogonal automorphism that extends beyond the typical vector space to the entire Clifford algebra while respecting the multivector grading. This leads to several non-equivalent subrepresentations corresponding to the multivector decomposition. Furthermore, we prove that the action respects not just the vector space structure of the Clifford algebra but also its multiplicative structure, i.e., the geometric product. These findings imply that every polynomial in multivectors, An advantage worth mentioning is that we obtain expressive layers that can elegantly generalize to inner-product spaces of any dimension. We demonstrate, notably from a sin
    
[^153]: 贝叶斯重整化

    Bayesian Renormalization. (arXiv:2305.10491v1 [hep-th])

    [http://arxiv.org/abs/2305.10491](http://arxiv.org/abs/2305.10491)

    本文提出了一种基于信息论的贝叶斯统计模型的重整化方法，使用Fisher度量定义了一个相关长度作为紧密相关的概率分布点之间的可分辨性(RG)尺度，在统计推断实验中，可以得到某个系统最大特异性观察数量的代理。贝叶斯重整化方法为给定系统准备一个在上述尺度上精度有限的有效模型，这个尺度可以被解释为当前实验装置可以探测到的最大能量。贝叶斯重整化提出了一种发现和表征基本物理理论的新框架。

    

    本文提出了一种完全基于信息论的贝叶斯统计模型的重整化方法，称为贝叶斯重整化。贝叶斯重整化的主要思想是使用Fisher度量来定义一个相关长度，这个长度起到了紧密相关的概率分布点之间的可分辨性(RG)尺度。这个RG尺度可以被解释为在统计推断实验中对于一个给定系统可以得到的最大特异性观察数量的代理。贝叶斯重整化方法的作用是为给定系统准备一个在上述尺度上精度有限的有效模型。在将贝叶斯重整化方法应用于物理系统时，这个由信息论出现的RG尺度自然地被识别为当前实验装置可以探测到的最大能量，因此，贝叶斯重整化提出了一种发现和表征基本物理理论的新框架。

    In this note we present a fully information theoretic approach to renormalization inspired by Bayesian statistical inference, which we refer to as Bayesian Renormalization. The main insight of Bayesian Renormalization is that the Fisher metric defines a correlation length that plays the role of an emergent RG scale quantifying the distinguishability between nearby points in the space of probability distributions. This RG scale can be interpreted as a proxy for the maximum number of unique observations that can be made about a given system during a statistical inference experiment. The role of the Bayesian Renormalization scheme is subsequently to prepare an effective model for a given system up to a precision which is bounded by the aforementioned scale. In applications of Bayesian Renormalization to physical systems, the emergent information theoretic scale is naturally identified with the maximum energy that can be probed by current experimental apparatus, and thus Bayesian Renormali
    
[^154]: 基于Segment Anything Model (SAM)增强伪标签的弱监督语义分割

    Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation. (arXiv:2305.05803v1 [cs.CV])

    [http://arxiv.org/abs/2305.05803](http://arxiv.org/abs/2305.05803)

    本论文提出了一种通过利用Segment Anything Model (SAM)增强Class Activation Maps (CAM)生成高质量伪标签的方法来解决弱监督语义分割中CAM的局部激活和虚假激活的限制问题。

    

    仅使用图像级别的监督的弱监督语义分割(WSSS)由于其与像素级注释相比的低成本而越来越受到关注。大多数现有方法依赖于类激活图(CAM)生成像素级的伪标签进行监督训练。然而，众所周知，CAM经常遭受局部激活的限制-只激活最具区分性的部分，而不是整个对象区域和虚假的激活-不必要地激活物体周围的背景。本研究引入了一种简单而有效的方法来解决这些限制，即利用最近发布的Segment Anything Model (SAM)增强CAM生成高质量的伪标签。SAM是一个分割基础模型，展示了将图像分割成段落的强零-shot能力，但缺乏这些区域的语义标签。为了解决这个问题，我们采用特定类别的伪标签作为信号来选择m。

    Weakly Supervised Semantic Segmentation (WSSS) with only image-level supervision has garnered increasing attention due to its low annotation cost compared to pixel-level annotation. Most existing methods rely on Class Activation Maps (CAM) to generate pixel-level pseudo labels for supervised training. However, it is well known that CAM often suffers from partial activation -- activating the most discriminative part instead of the entire object area, and false activation -- unnecessarily activating the background around the object. In this study, we introduce a simple yet effective approach to address these limitations by harnessing the recently released Segment Anything Model (SAM) to generate higher-quality pseudo labels with CAM. SAM is a segmentation foundation model that demonstrates strong zero-shot ability in partitioning images into segments but lacks semantic labels for these regions. To circumvent this, we employ pseudo labels for a specific class as the signal to select the m
    
[^155]: 旨在总结带有层次关系的多篇文档

    Towards Summarizing Multiple Documents with Hierarchical Relationships. (arXiv:2305.01498v1 [cs.CL])

    [http://arxiv.org/abs/2305.01498](http://arxiv.org/abs/2305.01498)

    提出了一个新的数据集PeerSum用于生成科学论文的元评论，源文档具有显式层次结构的丰富文档间关系，提出了一种用于元评论生成的关系感知多任务模型Rammer。

    

    多数现存的多文档摘要(MDS)数据集缺少人工生成的、真实的(即非合成的)摘要或者带有显式文档间关系的源文档。为了增强MDS系统的能力，我们提出PeerSum，这是一个新颖的数据集，用于生成科学论文的元评论，其中元评论是对评论和相应讨论的高度概括且真实的摘要。这些源文档具有显式层次结构的丰富文档间关系，包括交叉引用和经常出现的冲突。鉴于很少有研究采用基于预训练语言模型的注意力操纵来将层次关系纳入MDS系统中，我们还提出了Rammer(关系感知多任务元评论生成器)，这是一种元评论生成模型，使用基于层次关系的稀疏注意力和多任务目标，可以预测多个度量值。

    Most existing multi-document summarization (MDS) datasets lack human-generated and genuine (i.e., not synthetic) summaries or source documents with explicit inter-document relationships that a summary must capture. To enhance the capabilities of MDS systems we present PeerSum, a novel dataset for generating meta-reviews of scientific papers, where the meta-reviews are highly abstractive and genuine summaries of reviews and corresponding discussions. These source documents have rich inter-document relationships of an explicit hierarchical structure with cross-references and often feature conflicts. As there is a scarcity of research that incorporates hierarchical relationships into MDS systems through attention manipulation on pre-trained language models, we additionally present Rammer (Relationship-aware Multi-task Meta-review Generator), a meta-review generation model that uses sparse attention based on the hierarchical relationships and a multi-task objective that predicts several me
    
[^156]: 基于LTL规范的样本有效无模型强化学习与优化保证

    Sample Efficient Model-free Reinforcement Learning from LTL Specifications with Optimality Guarantees. (arXiv:2305.01381v1 [cs.LG])

    [http://arxiv.org/abs/2305.01381](http://arxiv.org/abs/2305.01381)

    本文提出了一种基于LTL规范的无模型强化学习方法，该方法结合乘积MDP、奖励结构和折扣机制有效地学习并优化未知随机系统最大化满足LTL规范的概率的最优策略。

    

    线性时间逻辑（LTL）广泛用于指定系统策略的高级目标，自主系统学习相对于这样的规范的最优策略是非常理想的。 但是，从LTL规范中学习最优策略并不轻松。我们提出了一种无模型强化学习（RL）方法，该方法可以有效地学习未知随机系统的最优策略，其中使用马尔可夫决策过程（MDP）进行建模。我们提出了一种新颖且更通用的乘积MDP、奖励结构和折扣机制，当与现成的无模型RL算法结合使用时，能够高效地学习最大化给定LTL规范满足概率的最优策略，并提供了更好的有关选择RL中关键参数以保证最优性的理论结果。为了直接评估学习策略，我们采用概率模型检查器PRISM来计算LTL规范的满足概率。

    Linear Temporal Logic (LTL) is widely used to specify high-level objectives for system policies, and it is highly desirable for autonomous systems to learn the optimal policy with respect to such specifications. However, learning the optimal policy from LTL specifications is not trivial. We present a model-free Reinforcement Learning (RL) approach that efficiently learns an optimal policy for an unknown stochastic system, modelled using Markov Decision Processes (MDPs). We propose a novel and more general product MDP, reward structure and discounting mechanism that, when applied in conjunction with off-the-shelf model-free RL algorithms, efficiently learn the optimal policy that maximizes the probability of satisfying a given LTL specification with optimality guarantees. We also provide improved theoretical results on choosing the key parameters in RL to ensure optimality. To directly evaluate the learned policy, we adopt probabilistic model checker PRISM to compute the probability of 
    
[^157]: 基于排名学习和局部模型的多目标高维昂贵问题的进化算法

    Rank-Based Learning and Local Model Based Evolutionary Algorithm for High-Dimensional Expensive Multi-Objective Problems. (arXiv:2304.09444v1 [cs.NE])

    [http://arxiv.org/abs/2304.09444](http://arxiv.org/abs/2304.09444)

    本文提出了一种基于排名学习和局部模型的多目标进化算法，该算法使用分类器进行排名，以解决高维昂贵多目标优化问题。

    

    近年来，辅以代理模型的进化算法广泛应用于解决复杂而计算代价昂贵的多目标优化问题。但是在处理高维优化问题时，这些辅以代理模型的多目标进化算法的性能会急剧恶化。本文提出了一种新颖的基于分类器辅助的排名学习和局部模型的多目标进化算法 (CLMEA)，用于解决高维昂贵的多目标优化问题。该算法由三部分组成：分类器辅助的排名学习、超体积非支配搜索和相对稀疏目标空间的局部搜索。具体来说，该算法建立了一个概率神经网络作为分类器，将后代划分为几个等级。不同等级的后代使用排名学习策略生成更具有前景性和信息性的候选解用于实际优化函数。

    Surrogate-assisted evolutionary algorithms have been widely developed to solve complex and computationally expensive multi-objective optimization problems in recent years. However, when dealing with high-dimensional optimization problems, the performance of these surrogate-assisted multi-objective evolutionary algorithms deteriorate drastically. In this work, a novel Classifier-assisted rank-based learning and Local Model based multi-objective Evolutionary Algorithm (CLMEA) is proposed for high-dimensional expensive multi-objective optimization problems. The proposed algorithm consists of three parts: classifier-assisted rank-based learning, hypervolume-based non-dominated search, and local search in the relatively sparse objective space. Specifically, a probabilistic neural network is built as classifier to divide the offspring into a number of ranks. The offspring in different ranks uses rank-based learning strategy to generate more promising and informative candidates for real funct
    
[^158]: MC-ViViT: 多分支分类器-ViViT用于使用面部视频检测老年人轻度认知障碍

    MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos. (arXiv:2304.05292v1 [cs.CV])

    [http://arxiv.org/abs/2304.05292](http://arxiv.org/abs/2304.05292)

    本文提出了一种新的深度机器学习模型MC-ViViT，用于通过分析面部特征检测老年人轻度认知障碍。通过MC模块和结合损失函数来解决数据集样本不平衡问题，提高了算法的性能。

    

    深度机器学习模型包括卷积神经网络(CNN)已成功地应用于使用医学图像、问卷和视频检测轻度认知障碍(MCI)。本文提出了一种新的多分支分类器-视频视觉变换器(MC-ViViT)模型，通过分析面部特征区分MCI和正常认知。数据来自I-CONECT，一个旨在通过提供频繁视频聊天来改善认知功能的行为干预试验。MC-ViViT在一个分支中提取视频的时空特征，并通过MC模块增强表示。由于I-CONECT数据集中的样本不平衡问题（包含难易和正负样本），这使MC-ViViT的性能受到影响。我们提出了一种Hard-Easy和Positive-Negative样本的损失函数（HP Loss）来结合对比度调节损失Focal loss和AD-CORRE loss来解决不平衡问题。我们在I-CONECT数据集上的实验结果显示出该算法的有效性。

    Deep machine learning models including Convolutional Neural Networks (CNN) have been successful in the detection of Mild Cognitive Impairment (MCI) using medical images, questionnaires, and videos. This paper proposes a novel Multi-branch Classifier-Video Vision Transformer (MC-ViViT) model to distinguish MCI from those with normal cognition by analyzing facial features. The data comes from the I-CONECT, a behavioral intervention trial aimed at improving cognitive function by providing frequent video chats. MC-ViViT extracts spatiotemporal features of videos in one branch and augments representations by the MC module. The I-CONECT dataset is challenging as the dataset is imbalanced containing Hard-Easy and Positive-Negative samples, which impedes the performance of MC-ViViT. We propose a loss function for Hard-Easy and Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE loss to address the imbalanced problem. Our experimental results on the I-CONECT dataset show th
    
[^159]: 人类合作是否增强了识别LLM生成的深度伪造文本的准确性？

    Does Human Collaboration Enhance the Accuracy of Identifying LLM-Generated Deepfake Texts?. (arXiv:2304.01002v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.01002](http://arxiv.org/abs/2304.01002)

    这项研究研究了人类合作是否增强了识别LLM生成的深度伪造文本的准确性。结果表明，合作可以潜在地提高两组人对深度伪造文本的检测准确性。

    

    大型语言模型（如GPT-4、LLaMA）的进展改善了大规模生成类似人类写作的连贯句子，从而产生了所谓的深度伪造文本。然而，这一进展引发了安全和隐私的担忧，需要有效解决方案来区分深度伪造文本和人类书写文本。尽管以前的研究探讨了人类检测深度伪造文本的能力，但没有研究“合作”是否能提高深度伪造文本的检测。在本研究中，为了填补对深度伪造文本理解的空白，我们对两组人进行了实验：（1）来自AMT平台的非专家群体和（2）来自Upwork平台的写作专家。结果表明，人类之间的合作可能会提高两组对深度伪造文本的检测准确性，非专家组的检测准确性提高了6.36%，专家组的检测准确性提高了12.76%。

    Advances in Large Language Models (e.g., GPT-4, LLaMA) have improved the generation of coherent sentences resembling human writing on a large scale, resulting in the creation of so-called deepfake texts. However, this progress poses security and privacy concerns, necessitating effective solutions for distinguishing deepfake texts from human-written ones. Although prior works studied humans' ability to detect deepfake texts, none has examined whether "collaboration" among humans improves the detection of deepfake texts. In this study, to address this gap of understanding on deepfake texts, we conducted experiments with two groups: (1) nonexpert individuals from the AMT platform and (2) writing experts from the Upwork platform. The results demonstrate that collaboration among humans can potentially improve the detection of deepfake texts for both groups, increasing detection accuracies by 6.36% for non-experts and 12.76% for experts, respectively, compared to individuals' detection accur
    
[^160]: Reflexion：具有动态记忆和自我反思的自主智能体

    Reflexion: an autonomous agent with dynamic memory and self-reflection. (arXiv:2303.11366v1 [cs.AI])

    [http://arxiv.org/abs/2303.11366](http://arxiv.org/abs/2303.11366)

    本文提出 Reflexion 方法，给智能体赋予了动态记忆和自我反思能力，以增强其任务特定的行动选择能力。

    

    最近决策大型语言模型（LLM）代理的发展在各种基准测试中展现出卓越的性能。然而，这些最先进的方法通常需要内部模型微调、外部模型微调或在定义的状态空间上进行策略优化。由于高质量训练数据的稀缺性或缺乏良好定义的状态空间，实现这些方法可能会具有挑战性。此外，这些代理没有人类决策过程固有的某些品质，特别是从错误中学习的能力。通过反思，人类可以通过试错过程高效地解决新的问题。在最近的研究基础上，我们提出 Reflexion，一种将动态记忆和自我反思能力赋予智能体的方法，以增强其现有的推理轨迹和任务特定的行动选择能力。为了实现完全自动化，我们介绍了一种简单而有效的方法。

    Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective 
    
[^161]: RACCER: 面向可达和确证的强化学习可追溯解释

    RACCER: Towards Reachable and Certain Counterfactual Explanations for Reinforcement Learning. (arXiv:2303.04475v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.04475](http://arxiv.org/abs/2303.04475)

    RACCER是第一个针对强化学习智能体行为生成对抗事实解释的专用方法，通过使用一组针对强化学习的特定对抗事实属性，保证易于实现且具有高概率预期结果的对抗事实。

    

    尽管强化学习算法在许多任务上取得了成功，但其对神经网络的依赖使其行为难以理解和信任。对抗事实解释是一种人性化的解释，为用户提供了关于如何改变模型输入以达到预期输出的可行建议。然而，当前在强化学习中生成对抗事实的方法忽略了强化学习任务的随机性和顺序性，可能产生难以获得或无法实现预期结果的对抗事实。在这项工作中，我们提出了RACCER，这是一种针对强化学习智能体行为生成对抗事实解释的首个专用方法。我们首先提出并实现了一组针对强化学习的特定对抗事实属性，确保易于实现且具有高概率预期结果的对抗事实。我们使用启发式树搜索代理执行轨迹，以找到最合适的对抗事实。

    While reinforcement learning (RL) algorithms have been successfully applied to numerous tasks, their reliance on neural networks makes their behavior difficult to understand and trust. Counterfactual explanations are human-friendly explanations that offer users actionable advice on how to alter the model inputs to achieve the desired output from a black-box system. However, current approaches to generating counterfactuals in RL ignore the stochastic and sequential nature of RL tasks and can produce counterfactuals that are difficult to obtain or do not deliver the desired outcome. In this work, we propose RACCER, the first RL-specific approach to generating counterfactual explanations for the behavior of RL agents. We first propose and implement a set of RL-specific counterfactual properties that ensure easily reachable counterfactuals with highly probable desired outcomes. We use a heuristic tree search of the agent's execution trajectories to find the most suitable counterfactuals ba
    
[^162]: 基于Mallat散射变换的磁流体力学的代理模型

    Mallat Scattering Transformation based surrogate for MagnetoHydroDynamics. (arXiv:2302.10243v2 [physics.plasm-ph] CROSS LISTED)

    [http://arxiv.org/abs/2302.10243](http://arxiv.org/abs/2302.10243)

    本文使用Machine Learning和Deep Learning方法开发了一个快速准确的代理模型，利用Mallat散射变换和主成分分析对磁流体力学模拟进行了加速，识别出影响输出结果的关键参数。

    

    本文开发并应用一种机器学习和深度学习方法，为MagLIF聚变中2D电阻性磁流体力学模拟提供了高保真度、快速的代理模型。使用电阻性磁流体力学代码GORGON生成了一系列具有不同衬套纵横比、初始气体预热温度（即不同绝热指数）和不同衬套扰动的聚变模拟。根据$x$、$y$和$t$生成了衬套密度和磁场。对两个场的对数应用了Mallat散射变换，并在对两个场的MST对数应用了主成分分析。将场投影到主成分分析向量上，并保留其中的一小部分主成分向量。对输入参数与场的MST对数输出的交叉相关和SVD向量分量与PCA向量分量的交叉相关进行了奇异值分解。这使得能够识别出影响输出结果的参数。

    A Machine and Deep Learning methodology is developed and applied to give a high fidelity, fast surrogate for 2D resistive MHD simulations of MagLIF implosions. The resistive MHD code GORGON is used to generate an ensemble of implosions with different liner aspect ratios, initial gas preheat temperatures (that is, different adiabats), and different liner perturbations. The liner density and magnetic field as functions of $x$, $y$, and $t$ were generated. The Mallat Scattering Transformation (MST) is taken of the logarithm of both fields and a Principal Components Analysis is done on the logarithm of the MST of both fields. The fields are projected onto the PCA vectors and a small number of these PCA vector components are kept. Singular Value Decompositions of the cross correlation of the input parameters to the output logarithm of the MST of the fields, and of the cross correlation of the SVD vector components to the PCA vector components are done. This allows the identification of the 
    
[^163]: 强大的层级增强学习中的知识传输

    Robust Knowledge Transfer in Tiered Reinforcement Learning. (arXiv:2302.05534v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05534](http://arxiv.org/abs/2302.05534)

    本文研究了层级增强学习中的知识传输，提出了一种新颖的在线学习算法，在没有先验知识的任务相似性的情况下实现强大的知识传输。

    

    本文研究了层级增强学习设置，这是一个并行传输学习框架，在这个框架中，目标是将知识从低层（源）任务传输到高层（目标）任务，以减少后者的探索风险，同时并行解决这两个任务。与先前的工作不同，我们不假设低层和高层任务共享相同的动态或奖励函数，并且专注于在没有先验知识的任务相似性的情况下实现强大的知识传输。我们确定了一个称为“最优值支配”的自然而必要的条件，适用于我们的目标。在这个条件下，我们提出了一种新颖的在线学习算法，使得对于高层任务，在部分状态上可以实现恒定的遗憾，这取决于任务相似性，并在两个任务不相似时保持接近最优遗憾；而对于低层任务，它可以在不做出牺牲的情况下保持接近最优。此外，我们进一步研究了具有多个低层任务的情况。

    In this paper, we study the Tiered Reinforcement Learning setting, a parallel transfer learning framework, where the goal is to transfer knowledge from the low-tier (source) task to the high-tier (target) task to reduce the exploration risk of the latter while solving the two tasks in parallel. Unlike previous work, we do not assume the low-tier and high-tier tasks share the same dynamics or reward functions, and focus on robust knowledge transfer without prior knowledge on the task similarity. We identify a natural and necessary condition called the ``Optimal Value Dominance'' for our objective. Under this condition, we propose novel online learning algorithms such that, for the high-tier task, it can achieve constant regret on partial states depending on the task similarity and retain near-optimal regret when the two tasks are dissimilar, while for the low-tier task, it can keep near-optimal without making sacrifice. Moreover, we further study the setting with multiple low-tier tasks
    
[^164]: 理解跨语言摘要中的“翻译语言”现象

    Understanding Translationese in Cross-Lingual Summarization. (arXiv:2212.07220v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.07220](http://arxiv.org/abs/2212.07220)

    本论文研究了跨语言摘要中的“翻译语言”现象对模型的影响，发现不同方法构建数据集会导致不同程度的翻译语言现象。文章还发现翻译语言会对模型的评估和性能产生影响，包括测试集中的翻译语言可能导致人工判断与自动评估之间的差异，以及训练集中的翻译语言会影响模型的训练表现和性能。

    

    在跨语言摘要（CLS）中，给定一篇源语言文档，旨在生成一篇简洁的目标语言摘要。与单语摘要（MS）不同，自然出现的源语言文档配对目标语言摘要并不常见。为了收集大规模的CLS数据，现有数据集通常在创建过程中涉及翻译。然而，翻译文本与原始语言文本之间存在着区别，即翻译语言。本文首先确认了构建CLS数据集的不同方法将导致不同程度的翻译语言现象。然后我们系统地研究了翻译语言在源文档或目标摘要中出现时如何影响CLS模型的评估和性能。具体而言，我们发现（1）测试集中文档或摘要中的翻译语言可能导致人工判断与自动评估之间的差异；（2）训练集中的翻译语言会影响模型的训练表现和性能。

    Given a document in a source language, cross-lingual summarization (CLS) aims at generating a concise summary in a different target language. Unlike monolingual summarization (MS), naturally occurring source-language documents paired with target-language summaries are rare. To collect large-scale CLS data, existing datasets typically involve translation in their creation. However, the translated text is distinguished from the text originally written in that language, i.e., translationese. In this paper, we first confirm that different approaches of constructing CLS datasets will lead to different degrees of translationese. Then we systematically investigate how translationese affects CLS model evaluation and performance when it appears in source documents or target summaries. In detail, we find that (1) the translationese in documents or summaries of test sets might lead to the discrepancy between human judgment and automatic evaluation; (2) the translationese in training sets would ha
    
[^165]: 灵活的基于注意力的多策略融合用于高效深度强化学习的研究

    Flexible Attention-Based Multi-Policy Fusion for Efficient Deep Reinforcement Learning. (arXiv:2210.03729v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03729](http://arxiv.org/abs/2210.03729)

    该论文提出了一种知识引导的强化学习方法（KGRL），通过融合多个知识策略并利用注意力机制实现了灵活的知识重新排列。这种方法可以提高强化学习代理的样本效率和泛化能力。

    

    强化学习 (RL) 代理长期以来一直致力于接近人类学习的效率。人类是优秀的观察者，可以通过聚合来自各种来源的外部知识（包括他人在尝试任务时的观察）来学习。之前在RL中的研究已经将外部知识策略结合到代理中，以帮助其提高样本效率。然而，执行任意组合和替换这些策略仍然是非平凡的，这是泛化和可转移性的重要特征。在这项工作中，我们提出了知识引导的RL(KGRL)，这是一种融合多个知识策略并旨在实现人类学习效率和灵活性的RL范式。我们为KGRL提出了一种新的演员架构，即知识包容性注意网络(KIAN)，该网络通过基于嵌入的注意力行动预测实现了自由的知识重新排列。KIAN还解决了熵不平衡的问题，这是在最大熵KGRL中出现的问题，阻碍了代理的高效表现。

    Reinforcement learning (RL) agents have long sought to approach the efficiency of human learning. Humans are great observers who can learn by aggregating external knowledge from various sources, including observations from others' policies of attempting a task. Prior studies in RL have incorporated external knowledge policies to help agents improve sample efficiency. However, it remains non-trivial to perform arbitrary combinations and replacements of those policies, an essential feature for generalization and transferability. In this work, we present Knowledge-Grounded RL (KGRL), an RL paradigm fusing multiple knowledge policies and aiming for human-like efficiency and flexibility. We propose a new actor architecture for KGRL, Knowledge-Inclusive Attention Network (KIAN), which allows free knowledge rearrangement due to embedding-based attentive action prediction. KIAN also addresses entropy imbalance, a problem arising in maximum entropy KGRL that hinders an agent from efficiently ex
    
[^166]: Rank-N-Contrast:为回归问题学习连续表示的方法

    Rank-N-Contrast: Learning Continuous Representations for Regression. (arXiv:2210.01189v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01189](http://arxiv.org/abs/2210.01189)

    Rank-N-Contrast是一种学习连续表示的回归框架，通过对样本在目标空间中的排名进行比较来提高性能，并在计算机视觉、人机交互和医疗保健等领域的回归任务中取得了最先进的结果。

    

    深度回归模型通常在端到端的方式下学习，没有明确强调回归感知的表示。因此，学习到的表示表现出分散性，无法捕捉样本顺序的连续性，导致广泛的回归任务中产生次优结果。为了填补这个空白，我们提出了Rank-N-Contrast（RNC），一种通过对样本在目标空间中的排名进行比较来学习回归的连续表示的框架。我们从理论和实证上证明了RNC可以保证所学表示的顺序与目标顺序相符，不仅性能更好，而且鲁棒性、效率和泛化能力都得到了显著提高。在计算机视觉、人机交互和医疗保健等领域使用五个真实世界的回归数据集进行的大量实验验证了RNC的最先进性能，突显了其内在的创新贡献。

    Deep regression models typically learn in an end-to-end fashion without explicitly emphasizing a regression-aware representation. Consequently, the learned representations exhibit fragmentation and fail to capture the continuous nature of sample orders, inducing suboptimal results across a wide range of regression tasks. To fill the gap, we propose Rank-N-Contrast (RNC), a framework that learns continuous representations for regression by contrasting samples against each other based on their rankings in the target space. We demonstrate, theoretically and empirically, that RNC guarantees the desired order of learned representations in accordance with the target orders, enjoying not only better performance but also significantly improved robustness, efficiency, and generalization. Extensive experiments using five real-world regression datasets that span computer vision, human-computer interaction, and healthcare verify that RNC achieves state-of-the-art performance, highlighting its intr
    
[^167]: 朝着可信的神经程序合成之路

    Toward Trustworthy Neural Program Synthesis. (arXiv:2210.00848v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2210.00848](http://arxiv.org/abs/2210.00848)

    我们提出了一种能够估计大型语言模型中采样程序正确性概率的简单方法，通过采样候选程序和候选谓词来预测程序的正确性，并推断出关于生成代码行为解释的有用谓词。

    

    我们提出了一种估计从大型语言模型中采样的程序正确性概率的方法。给定一个自然语言描述的编程问题，我们的方法既采样候选程序，又采样候选谓词来指定程序的行为。这样可以学习一个能够生成良好校准程序正确性概率预测的模型。我们的系统还推断出生成代码行为解释中有用的谓词，并且在人类实验中，人们更偏好这些谓词而不是原始语言模型的输出。我们的方法简单、易于实现，并且保持了最先进的生成准确性结果。

    We develop an approach to estimate the probability that a program sampled from a large language model is correct. Given a natural language description of a programming problem, our method samples both candidate programs as well as candidate predicates specifying how the program should behave. This allows learning a model that forms a well-calibrated probabilistic prediction of program correctness. Our system also infers which predicates are useful to explain the behavior of the generated code, and humans preferred these in a human study over raw language model outputs. Our method is simple, easy to implement, and maintains state of the art generation accuracy results.
    
[^168]: 从时间序列中学习领域特定的因果发现

    Learning domain-specific causal discovery from time series. (arXiv:2209.05598v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.05598](http://arxiv.org/abs/2209.05598)

    本研究旨在通过数据驱动的方法提高时间序列中领域特定的因果发现。实验证明，该方法在多个领域上明显优于人类设计的通用因果发现方法。

    

    因果发现是神经科学、医学和机器学习中的重要问题。因果发现的技术包括随机实验和基于格兰杰因果性、条件独立性、结构方程以及评分方法等算法，然而这些方法只在人类设计者做出强假设时才准确。本研究探讨了是否可以通过数据驱动的方法提高时间序列中的领域特定因果发现。我们的研究结果表明，该方法在MOS 6502微处理器、NetSim fMRI数据集和Dream3基因数据集上显著优于人类设计的领域通用因果发现方法，如互信息、VAR-LiNGAM和格兰杰因果性。我们认为，在可行的情况下，这种方法是一种有效的改进因果发现的方法。

    Causal discovery (CD) from time-varying data is important in neuroscience, medicine, and machine learning. Techniques for CD encompass randomized experiments, which are generally unbiased but expensive, and algorithms such as Granger causality, conditional-independence-based, structural-equation-based, and score-based methods that are only accurate under strong assumptions made by human designers. However, as demonstrated in other areas of machine learning, human expertise is often not entirely accurate and tends to be outperformed in domains with abundant data. In this study, we examine whether we can enhance domain-specific causal discovery for time series using a data-driven approach. Our findings indicate that this procedure significantly outperforms human-designed, domain-agnostic causal discovery methods, such as Mutual Information, VAR-LiNGAM, and Granger Causality on the MOS 6502 microprocessor, the NetSim fMRI dataset, and the Dream3 gene dataset. We argue that, when feasible,
    
[^169]: 基于深度学习方法的深伪造内容检测综述

    A Review of Deep Learning-based Approaches for Deepfake Content Detection. (arXiv:2202.06095v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2202.06095](http://arxiv.org/abs/2202.06095)

    本文综述了最近基于深度学习方法的深伪造内容检测的研究，系统地回顾了不同类别的伪造内容检测，并报告了所考察工作的优点和缺点，以及深伪造检测领域仍未解决的问题和不足之处的未来研究方向。

    

    深度学习生成模型的最新进展引起了人们的担忧，因为它们可以创建高度逼真的伪造图像和视频。这对人们的完整性构成威胁，可能导致社会不稳定。为了解决这个问题，迫切需要开发新的计算模型，能够有效检测伪造内容，并提醒用户可能的图像和视频篡改。本文全面回顾了使用基于深度学习方法的深伪造内容检测的最新研究。我们旨在通过系统地审视不同类别的伪造内容检测来拓宽最新研究的前沿。此外，我们还报告了所考察工作的优点和缺点，以及深伪造检测领域仍未解决的问题和不足之处的未来研究方向。

    Recent advancements in deep learning generative models have raised concerns as they can create highly convincing counterfeit images and videos. This poses a threat to people's integrity and can lead to social instability. To address this issue, there is a pressing need to develop new computational models that can efficiently detect forged content and alert users to potential image and video manipulations. This paper presents a comprehensive review of recent studies for deepfake content detection using deep learning-based approaches. We aim to broaden the state-of-the-art research by systematically reviewing the different categories of fake content detection. Furthermore, we report the advantages and drawbacks of the examined works and future directions towards the issues and shortcomings still unsolved on deepfake detection.
    
[^170]: 非支配排序遗传算法II（NSGA-II）的数学运行时间分析

    Mathematical Runtime Analysis for the Non-Dominated Sorting Genetic Algorithm II (NSGA-II). (arXiv:2112.08581v5 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2112.08581](http://arxiv.org/abs/2112.08581)

    这项研究通过数学分析证明了非支配排序遗传算法II（NSGA-II）的运行时间特性，发现当种群规模是帕累托前沿大小的四倍时，NSGA-II具有与其他算法相同的运行时间保证。

    

    非支配排序遗传算法II（NSGA-II）是目前实际应用中最常用的多目标进化算法（MOEA）。然而，与一些简单的MOEA的数学分析相反，NSGA-II至今没有进行过这样的研究。在这项工作中，我们证明了对NSGA-II进行数学运行时间分析是可行的。作为特定结果，我们证明了当种群规模是帕累托前沿大小的四倍时，NSGA-II使用两种经典变异算子和四种不同的父代选择方法与SEMO和GSEMO算法在基本的OneMinMax和LeadingOnesTrailingZeros基准测试上具有相同的渐近运行时间保证。然而，如果种群规模只等于帕累托前沿的大小，则NSGA-II无法高效地计算完整的帕累托前沿：在指数级迭代次数内，种群始终会错失帕累托前沿的一个恒定部分。

    The non-dominated sorting genetic algorithm II (NSGA-II) is the most intensively used multi-objective evolutionary algorithm (MOEA) in real-world applications. However, in contrast to several simple MOEAs analyzed also via mathematical means, no such study exists for the NSGA-II so far. In this work, we show that mathematical runtime analyses are feasible also for the NSGA-II. As particular results, we prove that with a population size four times larger than the size of the Pareto front, the NSGA-II with two classic mutation operators and four different ways to select the parents satisfies the same asymptotic runtime guarantees as the SEMO and GSEMO algorithms on the basic OneMinMax and LeadingOnesTrailingZeros benchmarks. However, if the population size is only equal to the size of the Pareto front, then the NSGA-II cannot efficiently compute the full Pareto front: for an exponential number of iterations, the population will always miss a constant fraction of the Pareto front. Our exp
    

