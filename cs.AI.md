# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion.](http://arxiv.org/abs/2306.04633) | 本文提出了一种利用二维预训练模型，通过慢-快对比融合实现三维物体实例分割的方法，在大量物体的场景中具有可扩展性，同时不需要物体数量的上界限制，通过限制多视角一致性，提出了一个新的半真实数据集(Messy Rooms)。 |
| [^2] | [The Two Word Test: A Semantic Benchmark for Large Language Models.](http://arxiv.org/abs/2306.04610) | 这篇论文提出了一个新的开源基准测试——“两个词测试”，用于评估大型语言模型的语义能力。测试需要对1768个名词组合进行意义性判断，并可用于评估0-4量表上的有意义评分和二进制判断。 |
| [^3] | [Integrating Geometric Control into Text-to-Image Diffusion Models for High-Quality Detection Data Generation via Text Prompt.](http://arxiv.org/abs/2306.04607) | GeoDiffusion使用文本提示将各种几何条件转化为图像，生成高质量的检测数据，性能优于现有方法。 |
| [^4] | [Empowering Business Transformation: The Positive Impact and Ethical Considerations of Generative AI in Software Product Management -- A Systematic Literature Review.](http://arxiv.org/abs/2306.04605) | 本文通过文献综述，总结了生成式 AI 对软件产品管理领域的潜在应用、优势和限制。该技术可以在创意生成、市场研究、客户洞察、产品需求工程和产品开发等方面辅助，帮助减少开发时间和成本。最终，它可以推动软件产品管理活动的效率提升和资源更加高效的利用，获得更好的产品结果和改进的终端用户体验。 |
| [^5] | [Generalization Across Observation Shifts in Reinforcement Learning.](http://arxiv.org/abs/2306.04595) | 本文研究了强化学习中观测转移泛化问题，在基于仿真器学习的情况下，通过学习到可以适应不同观测转移的表征空间，实现了对于未知环境的泛化能力。 |
| [^6] | [Proximity-Informed Calibration for Deep Neural Networks.](http://arxiv.org/abs/2306.04590) | 该论文提出了一个校准算法，解决了深度神经网络推理过程中低接近度数据和高接近度数据之间不一致的误校准问题。 |
| [^7] | [Divide and Repair: Using Options to Improve Performance of Imitation Learning Against Adversarial Demonstrations.](http://arxiv.org/abs/2306.04581) | 本文提出了一种利用选项改进模仿学习对抗性示范的性能表现的新技术，可以识别未被对手显着修改的演示轨迹并仅从中进行学习。 |
| [^8] | [Recent applications of machine learning, remote sensing, and iot approaches in yield prediction: a critical review.](http://arxiv.org/abs/2306.04566) | 综述了机器学习、遥感和物联网方法在农业中的应用，这些技术的整合为农业提供了洞察和预测，提高了农业生产效率和可持续性。 |
| [^9] | [ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models.](http://arxiv.org/abs/2306.04563) | 这篇论文探讨了OpenAI的ChatGPT模型在幽默识别方面的能力，结果表明该模型的幽默并不是硬编码的，但大部分生成的笑话都不是新的，几乎是少量几组重复的。 |
| [^10] | [Querying Circumscribed Description Logic Knowledge Bases.](http://arxiv.org/abs/2306.04546) | 本文证明了在周缘化描述逻辑知识库上评估连词查询和其并集的可判定性，并且得到了DL-Lite的数据复杂性和合并复杂性的完整描述。 |
| [^11] | [Contrastive Bootstrapping for Label Refinement.](http://arxiv.org/abs/2306.04544) | 该论文提出了一种对比聚类自举方法，可以在只使用粗略注释和映射的情况下，提高文本分类的细化程度。实验结果表明，该方法优于现有方法。 |
| [^12] | [On the Design Fundamentals of Diffusion Models: A Survey.](http://arxiv.org/abs/2306.04542) | 本文综述了扩散模型的设计基础，即其三个关键组件：正向过程、逆向过程和采样过程，为未来的研究提供了有益的细粒度透视。 |
| [^13] | [Top-Down Knowledge Compilation for Counting Modulo Theories.](http://arxiv.org/abs/2306.04541) | 这篇论文讨论了计数模理论（#SMT）的编译策略，并提出了一种基于全面的DPLL（T）搜索痕迹的自上而下的编译器。 |
| [^14] | [Sample-Level Weighting for Multi-Task Learning with Auxiliary Tasks.](http://arxiv.org/abs/2306.04519) | 提出了一种用于辅助任务下多任务学习的样本级加权算法SLGrad，通过样本特定的任务权重，消除有害的辅助信号并增强有用的任务信号，实现了泛化性能的提升。 |
| [^15] | [Unified Model for Crystalline Material Generation.](http://arxiv.org/abs/2306.04510) | 该论文提出了两个统一模型，使用周期等变体系结构可以同时作用于晶体晶格和原子位置，通过降低总能量以达到热力学稳定性来学习任意晶体晶格变形。 |
| [^16] | [Enhancing In-Context Learning with Answer Feedback for Multi-Span Question Answering.](http://arxiv.org/abs/2306.04508) | 本文提出了一种使用答案反馈来增强多跨度问答的上下文学习方法，将传统的演示示例扩展了反馈信息，可以有效地提高模型的预测性能。 |
| [^17] | [Hardness of Deceptive Certificate Selection.](http://arxiv.org/abs/2306.04505) | 本文证明了利用AFC在高保真度和完备性的设定下，选择无信息证书的任务是 NP-hard 的。 |
| [^18] | [Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal.](http://arxiv.org/abs/2306.04502) | 本文提出了一种名为AGRA的自适应梯度异常值去除方法，能够在模型训练过程中动态调整数据集从而有效提高模型学习效果。 |
| [^19] | [Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards.](http://arxiv.org/abs/2306.04488) | 本文提出了 rewarded soup 方法，通过结合多种代理奖励，实现微调权重插值，从而在整个偏好空间中实现帕累托最优广义化。该方法在强化学习任务上具有有效性。 |
| [^20] | [Artificial Intelligence can facilitate selfish decisions by altering the appearance of interaction partners.](http://arxiv.org/abs/2306.04484) | 本研究发现，通过模糊滤镜等外观改变技术，人工智能可以导致个体对模糊他人的自私行为增加，这表明其可以通过非个性化促进道德缩减，需要更广泛的伦理讨论。 |
| [^21] | [Training-Free Neural Active Learning with Initialization-Robustness Guarantees.](http://arxiv.org/abs/2306.04454) | 本研究提出了一种期望方差与高斯过程（EV-GP）标准用于神经主动学习，该方法不需要对神经网络进行训练，并且保证在数据选择时NN具有良好的预测性能和初始化鲁棒性。 |
| [^22] | [Dual policy as self-model for planning.](http://arxiv.org/abs/2306.04440) | 该论文探究了使用精简策略网络作为自我模型的优缺点，并通过实验结果表明，使用经过精简的策略网络作为自我模型可以稳定训练，并且有更快的收敛速度。 |
| [^23] | [Cross-Database and Cross-Channel ECG Arrhythmia Heartbeat Classification Based on Unsupervised Domain Adaptation.](http://arxiv.org/abs/2306.04433) | 本研究利用无监督领域适应方法解决了跨领域ECG信号分类的挑战，提出了一种基于跨域特征差异优化的领域自适应深度网络，处理了最坏情况训练损失的消失，解决了ECG信号分类的限制性问题。 |
| [^24] | [Towards High-Performance Exploratory Data Analysis (EDA) Via Stable Equilibrium Point.](http://arxiv.org/abs/2306.04425) | 本文提出了一种基于稳定平衡点的框架，用于提高探索性数据分析的效率和解决方案质量，该方法能够为大规模数据集生成高质量的聚类和数据可视化。 |
| [^25] | [Social robots to improve therapeutic adherence in pediatric asthma.](http://arxiv.org/abs/2306.04422) | 通过与人类形态机器人的游戏化会话，可以提高儿童哮喘患者的治疗依从性。 |
| [^26] | [Synthesizing realistic sand assemblies with denoising diffusion in latent space.](http://arxiv.org/abs/2306.04411) | 本文介绍了一种使用去噪扩散算法在潜在空间中生成沙粒组装的方法，通过点云自编码器把沙粒的三维点云结构编码到较低维度的潜在空间中，生成的沙粒样本符合原始数据分布，并可用于地质工程、计算机动画、石油工程等应用。 |
| [^27] | [Meta-Learning in Spiking Neural Networks with Reward-Modulated STDP.](http://arxiv.org/abs/2306.04410) | 本研究提出了一种受到海马和前额叶皮质启发的生物可行元学习模型，使用带有奖励的学习系统的脉冲神经网络，具有在最小数据下适应新任务和快速学习连续任务的能力，避免了灾难性遗忘现象，并且优于其他生物可行模型。 |
| [^28] | [Improving Diffusion-based Image Translation using Asymmetric Gradient Guidance.](http://arxiv.org/abs/2306.04396) | 本文提出了一种使用非对称梯度引导来指导扩散采样的反向过程的方法，以改善扩散图像翻译的风格转换和内容保留之间的平衡。 |
| [^29] | [Multilingual Clinical NER: Translation or Cross-lingual Transfer?.](http://arxiv.org/abs/2306.04384) | 本研究比较了翻译和跨语言迁移两种方法来执行临床领域命名实体识别（NER），并证明跨语言迁移比这两种翻译方法在法语和德语中都具有更好的性能。 |
| [^30] | [Label Shift Quantification with Robustness Guarantees via Distribution Feature Matching.](http://arxiv.org/abs/2306.04376) | 本文提出一种名为DFM框架的方法，用于量化标签偏移，并证明了其性能上限和鲁棒性。使用基于核的DFM版本可以提高效率、可扩展性和鲁棒性。 |
| [^31] | [Efficient Recruitment Strategy for Collaborative Mobile Crowd Sensing Based on GCN Trustworthiness Prediction.](http://arxiv.org/abs/2306.04366) | 本文提出了一种基于GCN可信度预测的协同移动群感知的高效招募策略，通过捕获工人之间的非对称信任关系和工人能力来实现有效的任务分配，优于现有方法。 |
| [^32] | [Robust and Efficient Fault Diagnosis of mm-Wave Active Phased Arrays using Baseband Signal.](http://arxiv.org/abs/2306.04360) | 本文提出了基于深度神经网络的新方法，使用基带信号来快速准确地诊断APA中的故障元件和部件，并证明其具有高98%的精度。 |
| [^33] | [ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems.](http://arxiv.org/abs/2306.04357) | 本研究提出了一种针对对话响应选择的后训练技术Dial-MAE，利用生成方法更好地压缩对话语义至密集向量，并提高对话响应选择准确性。 |
| [^34] | [Co-evolving Graph Reasoning Network for Emotion-Cause Pair Extraction.](http://arxiv.org/abs/2306.04340) | 本论文提出了一个面向情感-原因对提取的协同演化图推理网络，通过建立异构图捕捉因果关系和显式依赖关系，实现了情感-原因对提取的新的多任务学习框架，并在实验中取得了明显优于最先进方法的效果。 |
| [^35] | [Unpaired Deep Learning for Pharmacokinetic Parameter Estimation from Dynamic Contrast-Enhanced MRI.](http://arxiv.org/abs/2306.04339) | 该研究提出了一种非配对深度学习方法，使用物理驱动的CycleGAN框架，可以在没有配对数据的情况下准确地估计动态增强磁共振成像药代动力学参数和动脉输入函数。 |
| [^36] | [Semantic Technologies in Sensor-Based Personal Health Monitoring Systems: A Systematic Mapping Study.](http://arxiv.org/abs/2306.04335) | 这项研究评估了传感器个人健康监测系统中语义技术的应用现状，发现此类系统必须克服的关键挑战为互操作性、上下文感知、情境检测、情境预测、决策支持和知识表示。 |
| [^37] | [GCT-TTE: Graph Convolutional Transformer for Travel Time Estimation.](http://arxiv.org/abs/2306.04324) | 本文提出了一种基于变换器和图卷积的旅行时间估计模型，通过利用不同数据模态提取输入路径的不同属性来提高估计精度，并在两个数据集上超过了现有技术模型。 |
| [^38] | [Generative Semantic Communication: Diffusion Models Beyond Bit Recovery.](http://arxiv.org/abs/2306.04321) | 本文提出了一个新的生成扩散引导框架，通过利用扩散模型在合成多媒体内容同时保留语义特征方面的优势，以发送高度压缩的语义信息来降低带宽使用，从而超越现有方法在重建质量和语义保留方面的限制。 |
| [^39] | [Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3's personality instruments results.](http://arxiv.org/abs/2306.04308) | 本研究探讨了GPT-3 Davinci-003聊天机器人的人格特质，发现其具有良好的社交渴望和亲社会特质，但在不同时间的一致性存在限制。 |
| [^40] | [Dear XAI Community, We Need to Talk! Fundamental Misconceptions in Current XAI Research.](http://arxiv.org/abs/2306.04292) | 当前XAI研究中存在基本误解，例如未明确解释技术的目的，依赖于关于深度学习算法所学“概念”的强烈假设等。我们需要采取措施使XAI成为更实质性的研究领域。 |
| [^41] | [Introduction and Assessment of the Addition of Links and Containers to the Blackboard Architecture.](http://arxiv.org/abs/2306.04289) | 本文研究在黑板架构中增加容器和链接的作用，这些对象可以更好地建模组织、物理、空间等关系，使黑板架构能够更好地应用于复杂问题中。 |
| [^42] | [Extension of the Blackboard Architecture with Common Properties and Generic Rules.](http://arxiv.org/abs/2306.04287) | 本文介绍了一种扩展黑板架构的方法，增加了对共同属性和通用规则的支持，以实现对组织、空间和其他关系的建模，并促进对逻辑结构的重用。 |
| [^43] | [A Mask Free Neural Network for Monaural Speech Enhancement.](http://arxiv.org/abs/2306.04286) | 本文提出了一种无需掩模的单声道语音增强神经网络MFNet，该网络不仅可以映射语音，还可以映射反噪声，在强噪声环境下表现出色，并取得了2020年DNS挑战测试集的最佳结果。 |
| [^44] | [Decentralized Technologies for AI Hubs.](http://arxiv.org/abs/2306.04274) | 本论文探讨了基于去中心化技术的人工智能枢纽的潜力，以解决传统枢纽的一些问题，如高成本、缺乏货币化和奖励、缺乏控制和重现的困难。 |
| [^45] | [Permutaion Equivariant Graph Framelets for Heterophilous Semi-supervised Learning.](http://arxiv.org/abs/2306.04265) | 本文介绍了一个用于异质半监督学习的新型图神经网络模型PEGFAN，它使用置换等变图框架实现了多尺度特征提取，表现优于其他最先进模型，特别是在相对较大和密集连接的数据集中。 |
| [^46] | [Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks.](http://arxiv.org/abs/2306.04251) | SGD在训练过度表达的网络时，会随机地将动态吸引到更简单的子网络，这种随机吸引性能够提高泛化能力。 |
| [^47] | [MobileNMT: Enabling Translation in 15MB and 30ms.](http://arxiv.org/abs/2306.04235) | MobileNMT是一个可以在移动设备上实现15MB和30ms翻译的系统，通过一系列模型压缩原则和朋友INT8和解码的引擎的协同设计，它成功解决了NMT模型在移动设备上存储、内存、计算和功耗等方面的挑战，且其速度提升了47.0倍，节省了99.5%的内存，但仅损失了11.6%的BLEU。 |
| [^48] | [Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL.](http://arxiv.org/abs/2306.04220) | 本文提出了一个新的离线强化学习算法TDM，利用系统动力学的基本对称性实现高效学习小数据集。 |
| [^49] | [Synthesising Recursive Functions for First-Order Model Counting: Challenges, Progress, and Conjectures.](http://arxiv.org/abs/2306.04189) | 本文提出了一种用于解决一阶模型计数问题的算法，它可以表达许多类型递归计算。改进后的算法可以找到那些之前无法有效解决的计数问题的高效解决方案。 |
| [^50] | [When to Read Documents or QA History: On Unified and Selective Open-domain QA.](http://arxiv.org/abs/2306.04176) | 提出了一种方法来决定在何时使用文档或QA-pair回答问题，并在多个基准测试中验证了其有效性。 |
| [^51] | [Enhancing Virtual Assistant Intelligence: Precise Area Targeting for Instance-level User Intents beyond Metadata.](http://arxiv.org/abs/2306.04163) | 本文提出了一种能够处理应用程序屏幕像素级别虚拟助手，可以理解实例级用户意图并预测其目标操作区域，不需要应用程序元数据扩展。 |
| [^52] | [A Unified One-Step Solution for Aspect Sentiment Quad Prediction.](http://arxiv.org/abs/2306.04152) | 本文提出了一个统一的解决方案 One-ASQP，以检测方面类别并同时识别方面-观点-情感（AOS）三元组，通过引入多任务学习和分层注意机制，提高了预测准确性并具有高效性。 |
| [^53] | [Art and the science of generative AI: A deeper dive.](http://arxiv.org/abs/2306.04141) | 生成AI是一种新的艺术媒介，具有改变创造过程和社会构想的潜力，可以从美学、法律、创作未来和媒体生态等方面影响创作者和社会。 |
| [^54] | [A Survey on Generative Diffusion Models for Structured Data.](http://arxiv.org/abs/2306.04139) | 本文全面综述了在结构化数据领域中最近提出的扩散模型，介绍了其理论基础和应用场景。 |
| [^55] | [Retrosynthesis Prediction with Local Template Retrieval.](http://arxiv.org/abs/2306.04123) | 本文介绍了一种新方法，RetroKNN，它使用基于本地反应模板的k最近邻检索结合神经网络预测来提高逆向合成模型的性能。 |
| [^56] | [MESSY Estimation: Maximum-Entropy based Stochastic and Symbolic densitY Estimation.](http://arxiv.org/abs/2306.04120) | MESSY估计方法是一种基于最大熵的随机和符号密度估计方法，通过构建基于梯度的漂移扩散过程来高效地找到最大熵分布的参数，支持高维问题，并具有优于现有最新方法的有效性和普适性。 |
| [^57] | [M$^3$Fair: Mitigating Bias in Healthcare Data through Multi-Level and Multi-Sensitive-Attribute Reweighting Method.](http://arxiv.org/abs/2306.04118) | 本文提出了一种名为M$^3$Fair的新方法，可以在多个数据层面和多个敏感属性的情况下平衡训练数据分布，缓解医疗数据中的偏见和不公平现象，提高机器学习模型的公平性能和准确性能。 |
| [^58] | [BeMap: Balanced Message Passing for Fair Graph Neural Network.](http://arxiv.org/abs/2306.04107) | 本文提出了一种公平的消息传递方法，称为BeMap，旨在解决消息传递中的偏差放大问题，通过平衡感知的采样策略来平衡不同人口群体的1-hop邻居的数量。 |
| [^59] | [Gotta: Generative Few-shot Question Answering by Prompt-based Cloze Data Augmentation.](http://arxiv.org/abs/2306.04101) | Gotta是一个基于填空数据增强框架，通过将提示式填空任务与问题回答任务相结合，提高了少样本问题回答的学习能力。 |
| [^60] | [Deploying a Robust Active Preference Elicitation Algorithm: Experiment Design, Interface, and Evaluation for COVID-19 Patient Prioritization.](http://arxiv.org/abs/2306.04061) | 本论文介绍了一种强健的偏好调查算法，旨在优先为COVID-19患者提供医院资源，通过招募亚马逊机械土耳其工作者对该算法进行评估。 |
| [^61] | [Active Sparse Conversations for Improved Audio-Visual Embodied Navigation.](http://arxiv.org/abs/2306.04047) | 本文提出了CAVEN - 一种具有对话功能的音频视觉导航代理，能够向人类/神谕提出导航问题并处理神谕回答以协助自主导航。该系统基于多模态分层强化学习方法，并使用三个低级策略进行引导。 |
| [^62] | [FedVal: Different good or different bad in federated learning.](http://arxiv.org/abs/2306.04040) | 本文提出了FedVal方法，它是一个不需要从客户端获取任何附加信息的全新方法，可同时具有稳健和公平性，并通过评分函数在服务器端验证客户端更新，以确定本地训练模型之间的最佳聚合平衡。 |
| [^63] | [Quantitative Analysis of Primary Attribution Explainable Artificial Intelligence Methods for Remote Sensing Image Classification.](http://arxiv.org/abs/2306.04037) | 该论文定量分析了用于遥感图像分类的可解释人工智能技术，探究了不同属性的XAI方法，提供选取合适方法以深入了解模型决策的见解和建议。 |
| [^64] | [Certified Reasoning with Language Models.](http://arxiv.org/abs/2306.04031) | 该论文提出了一种称为“指南”的语言模型工具类，它使用状态和增量约束来指导生成，可以显著提高语言模型的逻辑推理精度。 |
| [^65] | [Intervention Generalization: A View from Factor Graph Models.](http://arxiv.org/abs/2306.04027) | 本文提出了一种基于因子图模型的“干预因子模型”(IFM)方法，仅基于对操纵系统分布的因子分解的最小假设，以实现从过去的实验到新的条件的跃迁。 |
| [^66] | [Your Value Function is a Control Barrier Function: Verification of Learned Policies using Control Theory.](http://arxiv.org/abs/2306.04026) | 本研究将控制理论中的验证方法应用于强化学习中的价值函数，提出了新的度量方法以验证安全控制任务中的价值函数，并代表了通用、可伸缩和可验证的控制系统设计框架的第一步。 |
| [^67] | [Designing explainable artificial intelligence with active inference: A framework for transparent introspection and decision-making.](http://arxiv.org/abs/2306.04025) | 本论文提出了一种使用主动推理的可解释人工智能系统的架构，可以追踪决策的因素并生成易于理解的模型，从而实现透明化内省和决策制定，提供了一种新的可解释人工智能的思路。 |
| [^68] | [Energy-Based Models for Cross-Modal Localization using Convolutional Transformers.](http://arxiv.org/abs/2306.04021) | 本文提出了一种基于能量模型和卷积变换的跨模态定位方法，利用卫星图像进行地图构建，可在没有GPS的情况下实现精确的度量级别定位，并在KITTI数据集上取得了更高的定位精度。 |
| [^69] | [Learning Search-Space Specific Heuristics Using Neural Networks.](http://arxiv.org/abs/2306.04019) | 该论文提出了一个使用神经网络学习特定搜索空间启发式方法的系统，可以为经典计划提供距离目标估计，有时能够与领域无关的启发式方法竞争。 |
| [^70] | [PyTrial: A Comprehensive Platform for Artificial Intelligence for Drug Development.](http://arxiv.org/abs/2306.04018) | PyTrial是一个实现多种AI算法支持的临床试验任务、可集成自己AI模型和数据集的Python软件包，并在现实临床试验数据上进行了广泛测试。 |
| [^71] | [Triggering Multi-Hop Reasoning for Question Answering in Language Models using Soft Prompts and Random Walks.](http://arxiv.org/abs/2306.04009) | 本研究提出了使用软提示和随机游走的方法，以便于预训练语言模型进行多跳推理的问答任务，取得了比标准调整方法更大的改进。 |
| [^72] | [One-Dimensional Deep Image Prior for Curve Fitting of S-Parameters from Electromagnetic Solvers.](http://arxiv.org/abs/2306.04001) | 该论文提出了一种使用深度生成模型为基础的方法，使用一维深度图像先验拟合电磁求解器的S参数。通过与公开可用和专有的行业标准拟合方法的比较，实验结果表明该方法在重建质量和计算时间方面均有显着改进。 |
| [^73] | [Real-Time Online Unsupervised Domain Adaptation for Real-World Person Re-identification.](http://arxiv.org/abs/2306.03993) | 本文提出了一个新的实时在线无监督领域自适应设置（R$^2$OUDA），并引入了一个新颖的多摄像头系统R$^2$MMT。通过R$^2$OUDA，本文解决了现实应用中被忽略的四个主要限制，以实现真正的实时在线无监督领域自适应。 |
| [^74] | [Learn the Force We Can: Multi-Object Video Generation from Pixel-Level Interactions.](http://arxiv.org/abs/2306.03988) | 本论文提出了一种基于像素级交互的多目标视频生成方法，能够生成逼真的物体间相互作用的视频，且能准确跟随用户的控制，达到了最先进的视频生成先前工作相媲美甚至更好的效果。 |
| [^75] | [Counterfactual Explanations and Predictive Models to Enhance Clinical Decision-Making in Schizophrenia using Digital Phenotyping.](http://arxiv.org/abs/2306.03980) | 本研究提出了一个数字表型和机器学习相结合的机器学习系统，通过预测、检测和因果解释患有精神分裂症症状变化的个体情况，可能帮助改进精神病学临床决策，系统误差率低于10%。 |
| [^76] | [Explainable AI using expressive Boolean formulas.](http://arxiv.org/abs/2306.03976) | 提出了一种表达式布尔公式的可解释人工智能，利用本地优化技术训练分类模型，可以应用于信用评分和医疗病况的诊断，并具有未来应用的潜力。 |
| [^77] | [PILLAR: How to make semi-private learning more effective.](http://arxiv.org/abs/2306.03962) | 本文提出了一种计算效率高的算法 PILLAR，可以在半监督半私有（SP）学习中明显降低私有标记样本复杂度，并可以在实际数据集上高效运行，可以利用在公共数据上预训练的网络提取的特征，并在实验证明了其显著有效性。 |
| [^78] | [Learning Causal Mechanisms through Orthogonal Neural Networks.](http://arxiv.org/abs/2306.03938) | 本文介绍了一种无监督的方法来学习一组独立机制的反向操作，以发现和分离一组独立的机制。 |
| [^79] | [Guiding The Last Layer in Federated Learning with Pre-Trained Models.](http://arxiv.org/abs/2306.03937) | 本文研究了在联邦学习中使用预训练模型引导最后一层的问题，提出了使用最近类均值(NCM)精确且高效地拟合分类器的方法，并取得了很好的效果。 |
| [^80] | [High-dimensional and Permutation Invariant Anomaly Detection.](http://arxiv.org/abs/2306.03933) | 该研究引入了一种置换不变的高维密度估计方法，通过学习后将其用于高能物理数据中的异常检测，能够有效地识别出在仅具备背景假设下排除异常的喷注。 |
| [^81] | [ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory.](http://arxiv.org/abs/2306.03901) | ChatDB项目将SQL数据库作为符号内存，增强LLMs的复杂多跳推理能力。 |
| [^82] | [Deductive Verification of Chain-of-Thought Reasoning.](http://arxiv.org/abs/2306.03872) | 本文旨在通过应用演绎验证技术，使语言模型能够进行明确而严谨的演绎推理，以确保其推理过程的可信度。 |
| [^83] | [ChatGPT Informed Graph Neural Network for Stock Movement Prediction.](http://arxiv.org/abs/2306.03763) | 该研究介绍了一种新的框架，利用ChatGPT技术增强图神经网络，能够从财经新闻中提取出不断变化的网络结构，并用于股票价格预测，获得了超过基于深度学习的最新基准的表现，提示了ChatGPT在文本推断和金融预测方面的潜力。 |
| [^84] | [Phonetically-Grounded Language Generation: The Case of Tongue Twisters.](http://arxiv.org/abs/2306.03457) | 本文介绍了针对绕口令生成的基于音韵学的语言生成任务，提供了TwistList数据集和TwisterMisters基准系统，并验证了预训练模型在没有任务特定数据和显式音韵知识的情况下的良好性能。 |
| [^85] | [Vid2Act: Activate Offline Videos for Visual RL.](http://arxiv.org/abs/2306.03360) | Vid2Act是一种基于模型的强化学习方法，它通过使用世界模型来传输领域相关的动态和策略，从而显著提高了样本效率。 |
| [^86] | [Inference-Time Intervention: Eliciting Truthful Answers from a Language Model.](http://arxiv.org/abs/2306.03341) | 本研究提出推理时间干预（ITI）技术，通过在推理过程中跨越有限数量的注意力头，显着提高大型语言模型的真实性。在TruthfulQA基准上，ITI使LLaMA模型的真实性从32.5%提高到65.1%。ITI是一种最小程度的干扰，计算廉价，且数据效率高。 |
| [^87] | [Fine-Tuning Language Models with Advantage-Induced Policy Alignment.](http://arxiv.org/abs/2306.02231) | 本论文提出了一种新算法APA，其采用优势诱导策略对齐用于强化学习语言模型。相对于传统方法（PPO），APA在语言任务中表现更好，避免了模型的崩溃与不稳定性。 |
| [^88] | [SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization.](http://arxiv.org/abs/2306.01981) | SGEM提出了一种新的测试时自适应框架，利用波束搜索和广义熵最小化调整预训练ASR模型，取得了三个主流ASR模型在不同领域转变时的最新性能。 |
| [^89] | [SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL.](http://arxiv.org/abs/2306.00739) | 本文提出了一种基于大语言模型的Text-to-SQL模型SQL-PaLM，使用了面向Text-to-SQL的基于执行的自一致提示方法，在Spider上实现了77.3%的测试套件准确度，并显着超越以前的最新技术的方法。 |
| [^90] | [Improving Expressivity of GNNs with Subgraph-specific Factor Embedded Normalization.](http://arxiv.org/abs/2305.19903) | 本文提出了一种名为SuperNorm的专用归一化方案，通过嵌入子图特定因子和纳入图实例特定统计数据来加强GNN的代表性能力，实现对节点感应子图中内部连接信息的明确考虑，从而改善GNN的表达能力。 |
| [^91] | [GAN-MPC: Training Model Predictive Controllers with Parameterized Cost Functions using Demonstrations from Non-identical Experts.](http://arxiv.org/abs/2305.19111) | 本文提出了一种新的方法，使用GAN来训练Learnable-MPC策略，以便在演示者和模仿者代理不能相同时进行参数化的成本函数学习。 |
| [^92] | [On the Correspondence Between Monotonic Max-Sum GNNs and Datalog.](http://arxiv.org/abs/2305.18015) | 本文研究了数据变换基于图神经网络的表现能力，证明了单调最大和的GNN与Datalog之间的关系紧密，即任何单调最大和的GNN都可以表示为一个Datalog程序，反之亦然。 |
| [^93] | [Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art.](http://arxiv.org/abs/2305.16259) | 本文简要概述了长文本的神经自然语言处理的现状，主要包括文档分类和摘要，涵盖了情感分析，同时还探讨了长文本NLP的主要挑战、问题和解决方案。 |
| [^94] | [PyTorch Hyperparameter Tuning -- A Tutorial for spotPython.](http://arxiv.org/abs/2305.11930) | 本文介绍了如何将spotPython超参数调谐器集成到PyTorch训练工作流中，以提高机器或深度学习模型的性能，以CIFAR10图像分类器为例。 |
| [^95] | [HICO-DET-SG and V-COCO-SG: New Data Splits to Evaluate Systematic Generalization in Human-Object Interaction Detection.](http://arxiv.org/abs/2305.09948) | 本论文提出了两个新的HOI检测数据拆分，旨在评估系统性泛化。在新的数据拆分上测试结果表明，HOI检测模型对于未见过的对象和交互组合的泛化十分困难。 |
| [^96] | [PALR: Personalization Aware LLMs for Recommendation.](http://arxiv.org/abs/2305.07622) | 本文提出了一个称为PALR的框架，将用户的历史行为与LLMs相结合，生成用户喜欢的物品的推荐。与现有的推荐方法相比，我们的PALR框架实现了最先进的性能。 |
| [^97] | [Fooling Thermal Infrared Detectors in Physical World.](http://arxiv.org/abs/2304.10712) | 本论文提出一种新颖的物理攻击方法——对抗性红外块（AdvIB），可以从多个角度对热成像系统执行隐蔽的黑盒攻击，成功诱导目标红外检测器在物理场景中对对象或人类进行误分类，并且不被人类察觉。 |
| [^98] | [Tractable Control for Autoregressive Language Generation.](http://arxiv.org/abs/2304.07438) | 本文提出了一种在自回归文本生成中使用可操作概率模型来强制实施限制的控制方法GeLaTo，并取得了在常见的约束文本生成测试上的最先进性能。 |
| [^99] | [Open-TransMind: A New Baseline and Benchmark for 1st Foundation Model Challenge of Intelligent Transportation.](http://arxiv.org/abs/2304.06051) | Open-TransMind是智能交通领域第一个基础模型挑战赛的新基准，旨在解决数据量少、泛化能力差以及缺乏多模态技术等典型挑战。 |
| [^100] | [Personalizing Digital Health Behavior Change Interventions using Machine Learning and Domain Knowledge.](http://arxiv.org/abs/2304.03392) | 该论文提出了一种采用机器学习和领域知识进行个性化数字健康行为变革干预的系统，其利用反事实例子进行特征控制以预测干预效果并优化干预效果。 |
| [^101] | [Language Models can Solve Computer Tasks.](http://arxiv.org/abs/2303.17491) | 本文研究表明，预训练的大型语言模型代理可以通过一个简单的提示方案使用自然语言执行计算机任务，该方法取得了很好的效果并在MiniWoB++基准测试中超越了监督学习和强化学习方法。 |
| [^102] | [What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement.](http://arxiv.org/abs/2303.11249) | 本文通过采用量子物理学的理论工具，提出了一种判定数据适合于局部连接神经网络的必要且充分条件，并导出了一种相应的预处理方法。 |
| [^103] | [Building Floorspace in China: A Dataset and Learning Pipeline.](http://arxiv.org/abs/2303.02230) | 本文构建了中国40个主要城市楼宇建筑面积数据集并利用多任务对象分割器方法学习了建筑物占地面积和高度，为城市规划提供数据支持。 |
| [^104] | [Alexa Arena: A User-Centric Interactive Platform for Embodied AI.](http://arxiv.org/abs/2303.01586) | Alexa Arena是一个用户中心的模拟平台，具有多个房间布局和可交互对象，用于创建人机交互任务，为实体人工智能研究提供了高效的数据收集和系统评估。 |
| [^105] | [Do Machine Learning Models Learn Statistical Rules Inferred from Data?.](http://arxiv.org/abs/2303.01433) | 本文提出了一个框架SQRL，可以无监督地从模型的训练数据中推断出统计规则；在分类、目标检测和数据填充等任务中，最先进的模型通常会违反这些规则，但是通过在测试时间内对模型进行适应，违规行为可以显著减少并提高模型的性能。 |
| [^106] | [Cliff-Learning.](http://arxiv.org/abs/2302.07348) | 本研究探究了基于基础模型的迁移学习在低数据状态下的数据缩放，发现了一种称为悬崖学习的现象，它反映了学习算法的先验知识与任务之间的兼容程度。 |
| [^107] | [Quantum Multi-Agent Actor-Critic Networks for Cooperative Mobile Access in Multi-UAV Systems.](http://arxiv.org/abs/2302.04445) | 本文提出了一种名为量子多智能体演员-评论网络的新算法，用于自主构建一个强大的移动访问系统，利用多个无人机，并利用量子计算的原则以提高其训练过程和推理能力。 |
| [^108] | [A Theory of Link Prediction via Relational Weisfeiler-Leman.](http://arxiv.org/abs/2302.02209) | 本文提出了一种基于关系Weisfeiler-Leman算法的理论，为知识图谱中的图神经网络提供了理论解释，并对各种模型的表达能力进行了表征，并解释了一些广泛采用的实际设计选择的优点。 |
| [^109] | [Revisiting Bellman Errors for Offline Model Selection.](http://arxiv.org/abs/2302.00141) | 本文重新审视 Bellman Errors，发现之前的Bellman Errors 方法需要在特定条件下才能表现良好，同时提出了更准确的 MSBE 估计器，在离散控制任务方面表现出色。 |
| [^110] | [Smooth Non-Stationary Bandits.](http://arxiv.org/abs/2301.12366) | 本文提出了一种非平稳两臂赌博机问题的策略，能够处理平滑变化，并证明了该策略在二次Lipschitz连续的情况下的遗憾为 $\tilde O(T^{3/5})$。 |
| [^111] | [Tracr: Compiled Transformers as a Laboratory for Interpretability.](http://arxiv.org/abs/2301.05062) | Tracr是一个编译器，将可读性强的程序编译成标准的仅解码变压器模型，该编译模型的已知结构可以用于设计实验和评估可解释方法。 |
| [^112] | [Answering Complex Logical Queries on Knowledge Graphs via Query Computation Tree Optimization.](http://arxiv.org/abs/2212.09567) | 提出了 QTO（查询计算树优化）以有效地回答复杂逻辑查询，通过查询计算树上的正反向传播找到了精确的最优解，并利用了查询计算树中的独立性来减少搜索空间。 |
| [^113] | [PCT-CycleGAN: Paired Complementary Temporal Cycle-Consistent Adversarial Networks for Radar-Based Precipitation Nowcasting.](http://arxiv.org/abs/2211.15046) | 本文提出了一种基于配对互补时间循环一致对抗网络的雷达降水预测方法，该方法包括两个生成器网络和循环一致性损失和对抗性损失。实验证明，该方法在准确性和推广能力方面优于现有的技术方法。 |
| [^114] | [A Filtering-based General Approach to Learning Rational Constraints of Epistemic Graphs.](http://arxiv.org/abs/2211.02918) | 该研究提出了一种基于筛选的方法，使用多重推广步骤来学习与认知图一致的合理规则，能够学习更广泛的合理规则以反映认知图中的合理性考虑，实验发现其表现优于现有方法。 |
| [^115] | [Explaining the Explainers in Graph Neural Networks: a Comparative Study.](http://arxiv.org/abs/2210.15304) | 该论文研究了图神经网络中的解释方法，并在多种数据集上测试了十种解释器的表现，提供了不同GNN体系结构易解释性的关键洞察。 |
| [^116] | [Using Bottleneck Adapters to Identify Cancer in Clinical Notes under Low-Resource Constraints.](http://arxiv.org/abs/2210.09440) | 本文评估了一系列机器学习技术来识别临床记录中的癌症，采用瓶颈适配器和提示微调的方法优于其它方法，可在低资源情况下使用。 |
| [^117] | [MLink: Linking Black-Box Models from Multiple Domains for Collaborative Inference.](http://arxiv.org/abs/2209.13883) | 本文提出了新的学习任务：模型链接，旨在通过学习不同黑盒模型输出空间之间映射的模型链接，把它们连接起来。提出了支持链接不同黑盒机器学习模型的设计，解决了分布差异挑战，开发了一个调度算法，并在多个实验数据集上实现了高效的推理结果。 |
| [^118] | [InFi: End-to-End Learning to Filter Input for Resource-Efficiency in Mobile-Centric Inference.](http://arxiv.org/abs/2209.13873) | 本研究提出了一个端到端可学习的输入过滤框架，通过对推理模型和输入过滤器的假设复杂性进行理论比较，从而了解优化潜力。该框架减少冗余，降低推理成本，并在f值、推理速度和内存占用方面超越其他方法。 |
| [^119] | [Digital Audio Forensics: Blind Human Voice Mimicry Detection.](http://arxiv.org/abs/2209.12573) | 本文介绍了一种利用深度学习方法，通过盲目检测输入音频的真实性，可以有效应对音频欺诈问题的分类器。而这种分类器不需要任何参考，能够在没有真实来源的情况下检测出模仿音频。 |
| [^120] | [Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization.](http://arxiv.org/abs/2208.09770) | 本文介绍了一种新的预训练语言模型Z-Code++，它使用两种预训练阶段和三种技术进行优化，其中包括解耦的注意力层和融合编码方法。该模型在抽象文本摘要任务上优于其他模型，是一种高效的参数化模型。 |
| [^121] | [SSIVD-Net: A Novel Salient Super Image Classification & Detection Technique for Weaponized Violence.](http://arxiv.org/abs/2207.12850) | 本文提出了一种名为SSIVD-Net的新技术，用于暴力识别任务。通过使用显著-超级图像表示减少了3D视频数据的复杂性，提高了推断、性能和可解释性。作者还提出了一种新颖的架构“Salient-Classifier”，将核方法和残差学习策略相结合。该方法在多个数据集上表现良好。 |
| [^122] | [Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering.](http://arxiv.org/abs/2207.12647) | 本篇论文提出了一个新型的事件级视觉问答框架——跨模态因果关系推理（CMCIR），通过引入因果干预方法，发现视觉和语言模态的真正因果结构，实现强健的因果感知视觉语言问答。 |
| [^123] | [A Context-Sensitive Word Embedding Approach for The Detection of Troll Tweets.](http://arxiv.org/abs/2207.08230) | 本研究开发了一种基于上下文敏感的单词嵌入方法用于自动检测 troll 推文，结果表明采用ELMo和BERT嵌入方法的性能更好，最佳表现方法为基于ELMo的架构，采用了一个GRU分类器，具有0.929的AUC得分。 |
| [^124] | [Reducing Predictive Feature Suppression in Resource-Constrained Contrastive Image-Caption Retrieval.](http://arxiv.org/abs/2204.13382) | 本文提出了一种名为潜在目标解码（LTD）的方法，可以在资源受限的情况下减少预测特征抑制，从而为对比图像-字幕检索（ICR）方法提供了一种解决方案。 |
| [^125] | [Machine Learning Testing in an ADAS Case Study Using Simulation-Integrated Bio-Inspired Search-Based Testing.](http://arxiv.org/abs/2203.12026) | 本文提出一种基于仿真集成的生物启发式搜索测试方法Deeper，用于生成用于测试基于深度神经网络的车道保持系统的故障发现测试场景，通过实证评估和与竞赛中的其他工具的比较展示了其性能的提高。 |
| [^126] | [Knowledge Graph Embedding Methods for Entity Alignment: An Experimental Review.](http://arxiv.org/abs/2203.09280) | 本文对36篇顶级会议上发表的实体对齐嵌入方法进行了统计和实验性分析，为实体对齐的知识图谱嵌入方法提供了全面的综述，包括这些方法的优点、缺点和适用性并确定了未来研究的挑战和方向。 |
| [^127] | [Invariance in Policy Optimisation and Partial Identifiability in Reward Learning.](http://arxiv.org/abs/2203.07475) | 本文探讨了奖励学习中奖励函数的部分可识别性，并分析了这种部分可识别性对政策优化等下游任务的影响。同时提出了一个框架，对比奖励学习的数据源和下游任务，以其不变性为依据，对奖励学习的数据源的设计和选择产生影响。 |
| [^128] | [Temporal Difference Learning with Continuous Time and State in the Stochastic Setting.](http://arxiv.org/abs/2202.07960) | 该论文提出了两种连续时间策略评估问题的时间差分学习方法，并证明了它们的理论收敛速度。同时，这些方法还可以解释为解决线性PDE或线性BSDE的新型强化学习方法。 |
| [^129] | [HeterPS: Distributed Deep Learning With Reinforcement Learning Based Scheduling in Heterogeneous Environments.](http://arxiv.org/abs/2111.10635) | 这篇论文介绍了一个名为Paddle-HeterPS的分布式框架，基于强化学习的调度方法可以高效地利用多种类型的计算资源，解决了分布式深度学习训练中多层次分配计算资源的问题。 |
| [^130] | [Medical Visual Question Answering: A Survey.](http://arxiv.org/abs/2111.10056) | 本综述总结了医学视觉问答的相关数据集、方法、创新和挑战，并探讨了未来的研究方向。 |
| [^131] | [Using Machine Teaching to Investigate Human Assumptions when Teaching Reinforcement Learners.](http://arxiv.org/abs/2009.02476) | 本文研究了在线奖励和惩罚方法下，人们对于学习者的期望假设，发现人们假设学习者具有高的折扣率和高度重视探索，并根据学习者进展调整教学策略。 |

# 详细

[^1]: 对比提升：通过慢-快对比融合实现三维物体实例分割

    Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion. (arXiv:2306.04633v1 [cs.CV])

    [http://arxiv.org/abs/2306.04633](http://arxiv.org/abs/2306.04633)

    本文提出了一种利用二维预训练模型，通过慢-快对比融合实现三维物体实例分割的方法，在大量物体的场景中具有可扩展性，同时不需要物体数量的上界限制，通过限制多视角一致性，提出了一个新的半真实数据集(Messy Rooms)。

    

    由于缺乏大规模注释的三维数据集，三维物体实例分割是一项具有挑战性的任务。本文展示了可以通过利用二维预训练模型来有效解决该任务。我们提出了一种新颖的方法，通过神经场表示将2D分段向上提升到3D，并将它们通过慢-快对比融合。这种方法鼓励跨帧的多视角一致性。我们方法的核心是一个可扩展的、适用于具有大量物体的场景的慢-快聚类目标函数。与先前的方法不同的是，我们的方法不需要对物体数量或跨帧物体跟踪进行设置上界。为了展示慢-快聚类的可扩展性，我们创建了一个名为Messy Rooms的新的半真实数据集，其中场景中最多有500个物体。我们的方法在ScanNet、Hypersim和Replica数据集的具有挑战性的场景中表现优于现有技术。

    Instance segmentation in 3D is a challenging task due to the lack of large-scale annotated datasets. In this paper, we show that this task can be addressed effectively by leveraging instead 2D pre-trained models for instance segmentation. We propose a novel approach to lift 2D segments to 3D and fuse them by means of a neural field representation, which encourages multi-view consistency across frames. The core of our approach is a slow-fast clustering objective function, which is scalable and well-suited for scenes with a large number of objects. Unlike previous approaches, our method does not require an upper bound on the number of objects or object tracking across frames. To demonstrate the scalability of the slow-fast clustering, we create a new semi-realistic dataset called the Messy Rooms dataset, which features scenes with up to 500 objects per scene. Our approach outperforms the state-of-the-art on challenging scenes from the ScanNet, Hypersim, and Replica datasets, as well as o
    
[^2]: 一个语义基准测试：用于大型语言模型的两个词测试

    The Two Word Test: A Semantic Benchmark for Large Language Models. (arXiv:2306.04610v1 [cs.CL])

    [http://arxiv.org/abs/2306.04610](http://arxiv.org/abs/2306.04610)

    这篇论文提出了一个新的开源基准测试——“两个词测试”，用于评估大型语言模型的语义能力。测试需要对1768个名词组合进行意义性判断，并可用于评估0-4量表上的有意义评分和二进制判断。

    

    大型语言模型（LLM）近来表现出了令人瞩目的能力，包括通过高级专业考试和苛刻的基准测试。这种性能使许多人认为它们接近于实现人类或“真正的”语言理解，甚至人工通用智能（AGI）。在这里，我们提供了一个新的开源基准测试，可以使用两个单词短语评估LLMs的语义能力，该任务可以相对容易地由没有高级培训的人类完成。将多个词组合成一个概念是人类语言和智能的基本方面。该测试需要对1768个评为有意义（例如baby boy）或不具有意义（例如goat sky）的名词组合进行有意义性判断，由150个人类评定者进行评定。我们提供了在0-4量表上探测有意义评分以及二进制判断的任务版本。我们使用TWT在GPT-4、GPT-3.5和Bard上进行了一系列实验。

    Large Language Models (LLMs) have shown remarkable abilities recently, including passing advanced professional exams and demanding benchmark tests. This performance has led many to suggest that they are close to achieving humanlike or 'true' understanding of language, and even Artificial General Intelligence (AGI). Here, we provide a new open-source benchmark that can assess semantic abilities of LLMs using two-word phrases using a task that can be performed relatively easily by humans without advanced training. Combining multiple words into a single concept is a fundamental aspect of human language and intelligence. The test requires meaningfulness judgments of 1768 noun-noun combinations that have been rated as meaningful (e.g., baby boy) or not meaningful (e.g., goat sky). by 150 human raters. We provide versions of the task that probe meaningfulness ratings on a 0-4 scale as well as binary judgments. We conducted a series of experiments using the TWT on GPT-4, GPT-3.5, and Bard, wi
    
[^3]: 将几何控制集成到文本到图像扩散模型中以通过文本提示生成高质量的检测数据

    Integrating Geometric Control into Text-to-Image Diffusion Models for High-Quality Detection Data Generation via Text Prompt. (arXiv:2306.04607v1 [cs.CV])

    [http://arxiv.org/abs/2306.04607](http://arxiv.org/abs/2306.04607)

    GeoDiffusion使用文本提示将各种几何条件转化为图像，生成高质量的检测数据，性能优于现有方法。

    

    扩散模型因其在创建内容和生成数据方面的显着能力而受到重视，例如图像分类。然而，使用扩散模型生成高质量的物体检测数据仍然是一个不被充分探索的领域，其中不仅图像水平的感知质量，而且边界框和相机视图等几何条件也是至关重要的。前期研究使用模块编码语义布局来实现复制粘贴合成或布局到图像(L2I)生成。本文提出了GeoDiffusion，一种简单的框架，可以灵活地将各种几何条件转化为文本提示，并使用预训练的文本到图像(T2I)扩散模型生成高质量的检测数据。与以往的L2I方法不同，我们的GeoDiffusion不仅能够编码边界框，还能够编码自驾场景中的额外几何条件，如摄像头视图。广泛的实验结果表明，GeoDiffusion在物体检测准确性方面优于最先进的方法，并针对各种几何条件生成具有更高感知质量的图像。

    Diffusion models have attracted significant attention due to their remarkable ability to create content and generate data for tasks such as image classification. However, the usage of diffusion models to generate high-quality object detection data remains an underexplored area, where not only the image-level perceptual quality but also geometric conditions such as bounding boxes and camera views are essential. Previous studies have utilized either copy-paste synthesis or layout-to-image (L2I) generation with specifically designed modules to encode semantic layouts. In this paper, we propose GeoDiffusion, a simple framework that can flexibly translate various geometric conditions into text prompts and empower the pre-trained text-to-image (T2I) diffusion models for high-quality detection data generation. Unlike previous L2I methods, our GeoDiffusion is able to encode not only bounding boxes but also extra geometric conditions such as camera views in self-driving scenes. Extensive experi
    
[^4]: 授权 Business Transformation：生成式 AI 对软件产品管理的积极影响和伦理考虑 —— 一项系统文献综述

    Empowering Business Transformation: The Positive Impact and Ethical Considerations of Generative AI in Software Product Management -- A Systematic Literature Review. (arXiv:2306.04605v1 [cs.SE])

    [http://arxiv.org/abs/2306.04605](http://arxiv.org/abs/2306.04605)

    本文通过文献综述，总结了生成式 AI 对软件产品管理领域的潜在应用、优势和限制。该技术可以在创意生成、市场研究、客户洞察、产品需求工程和产品开发等方面辅助，帮助减少开发时间和成本。最终，它可以推动软件产品管理活动的效率提升和资源更加高效的利用，获得更好的产品结果和改进的终端用户体验。

    

    近年来，生成式人工智能（GAI）取得了巨大的进展，在软件产品管理方面产生了很大的影响。本文通过涵盖2016年至2023年的相关文章，揭示了生成式 AI 在此领域中的潜在应用、优势和限制。研究表明，该技术可以辅助创意生成、市场研究、客户洞察、产品需求工程和产品开发等方面。通过自动代码生成、客户反馈分析等方式，它可以帮助减少开发时间和成本。然而，该技术的精度、可靠性和伦理问题仍然存在。最终，生成式 AI 的实际应用可以显著提高软件产品管理活动的效率，实现资源更加高效的利用，获得更好的产品结果和改进的终端用户体验。

    Generative Artificial Intelligence (GAI) has made outstanding strides in recent years, with a good-sized impact on software product management. Drawing on pertinent articles from 2016 to 2023, this systematic literature evaluation reveals generative AI's potential applications, benefits, and constraints in this area. The study shows that technology can assist in idea generation, market research, customer insights, product requirements engineering, and product development. It can help reduce development time and costs through automatic code generation, customer feedback analysis, and more. However, the technology's accuracy, reliability, and ethical consideration persist. Ultimately, generative AI's practical application can significantly improve software product management activities, leading to more efficient use of resources, better product outcomes, and improved end-user experiences.
    
[^5]: 强化学习中的观测转移泛化问题研究

    Generalization Across Observation Shifts in Reinforcement Learning. (arXiv:2306.04595v1 [cs.LG])

    [http://arxiv.org/abs/2306.04595](http://arxiv.org/abs/2306.04595)

    本文研究了强化学习中观测转移泛化问题，在基于仿真器学习的情况下，通过学习到可以适应不同观测转移的表征空间，实现了对于未知环境的泛化能力。

    

    学习到的策略对于环境变化的鲁棒性对于强化学习智能体在实际世界的应用和跨环境转移学习至关重要。本文关注于基于仿真器学习的情况下的观测变化问题，提出了一种基于等价度量的新型目标函数，通过学习到可以适应不同观测转移的表征空间，实现了对于未知环境的泛化能力。

    Learning policies which are robust to changes in the environment are critical for real world deployment of Reinforcement Learning agents. They are also necessary for achieving good generalization across environment shifts. We focus on bisimulation metrics, which provide a powerful means for abstracting task relevant components of the observation and learning a succinct representation space for training the agent using reinforcement learning. In this work, we extend the bisimulation framework to also account for context dependent observation shifts. Specifically, we focus on the simulator based learning setting and use alternate observations to learn a representation space which is invariant to observation shifts using a novel bisimulation based objective. This allows us to deploy the agent to varying observation settings during test time and generalize to unseen scenarios. We further provide novel theoretical bounds for simulator fidelity and performance transfer guarantees for using a
    
[^6]: 深度神经网络的相似性校准

    Proximity-Informed Calibration for Deep Neural Networks. (arXiv:2306.04590v1 [cs.LG])

    [http://arxiv.org/abs/2306.04590](http://arxiv.org/abs/2306.04590)

    该论文提出了一个校准算法，解决了深度神经网络推理过程中低接近度数据和高接近度数据之间不一致的误校准问题。

    

    推理结果的可信度校准对于提供准确和可解释的不确定性估计至关重要，特别是在安全关键场景下。已有的校准方法通常忽略了接近度偏差的问题，即模型在低接近性数据（即分布的稀疏区域）中倾向于更自信，而在高接近性样本中表现出不一致的误校准，我们发现这一问题。我们在ImageNet预训练模型上研究了这一问题，并观察到：1）接近度偏差存在于各种模型架构和大小之间；2）基于Transformer的模型比基于CNN的模型更容易受到接近度偏差的影响；3）即使采用流行的校准算法如温度缩放，接近度偏差也会持续存在；4）模型在低接近性样本上的过拟合程度比高接近性样本更严重。在这些实证发现的基础上，我们提出了ProCal。

    Confidence calibration is central to providing accurate and interpretable uncertainty estimates, especially under safety-critical scenarios. However, we find that existing calibration algorithms often overlook the issue of proximity bias, a phenomenon where models tend to be more overconfident in low proximity data (i.e., lying in the sparse region of the data distribution) compared to high proximity samples, and thus suffer from inconsistent miscalibration across different proximity samples. We examine the problem over pretrained ImageNet models and observe that: 1) Proximity bias exists across a wide variety of model architectures and sizes; 2) Transformer-based models are more susceptible to proximity bias than CNN-based models; 3) Proximity bias persists even after performing popular calibration algorithms like temperature scaling; 4) Models tend to overfit more heavily on low proximity samples than on high proximity samples. Motivated by the empirical findings, we propose ProCal, 
    
[^7]: 利用选项改进模仿学习对抗性示范的性能表现

    Divide and Repair: Using Options to Improve Performance of Imitation Learning Against Adversarial Demonstrations. (arXiv:2306.04581v1 [cs.LG])

    [http://arxiv.org/abs/2306.04581](http://arxiv.org/abs/2306.04581)

    本文提出了一种利用选项改进模仿学习对抗性示范的性能表现的新技术，可以识别未被对手显着修改的演示轨迹并仅从中进行学习。

    

    在本文中，我们考虑了从教师或专家的演示中学习执行任务的问题，并提出了一种新的技术，可以识别未被对手显着修改的演示轨迹的部分，并使用时间上扩展的策略或选项进行学习。首先，我们定义了一种基于演示轨迹的空间和时间特征的轨迹分歧度量，以检测和丢弃已被对手显着修改的轨迹部分，并可能降低学习者的性能，如果用于学习。然后，我们使用基于选项的算法来分割轨迹，并只从已确定为可接受的轨迹部分中进行学习。我们提供了我们技术的理论结果，以表明修复部分轨迹可以改善学习效果。

    We consider the problem of learning to perform a task from demonstrations given by teachers or experts, when some of the experts' demonstrations might be adversarial and demonstrate an incorrect way to perform the task. We propose a novel technique that can identify parts of demonstrated trajectories that have not been significantly modified by the adversary and utilize them for learning, using temporally extended policies or options. We first define a trajectory divergence measure based on the spatial and temporal features of demonstrated trajectories to detect and discard parts of the trajectories that have been significantly modified by an adversarial expert, and, could degrade the learner's performance, if used for learning, We then use an options-based algorithm that partitions trajectories and learns only from the parts of trajectories that have been determined as admissible. We provide theoretical results of our technique to show that repairing partial trajectories improves the 
    
[^8]: 农业中机器学习、遥感和物联网方法在产量预测方面的应用：一次重要的回顾

    Recent applications of machine learning, remote sensing, and iot approaches in yield prediction: a critical review. (arXiv:2306.04566v1 [cs.LG])

    [http://arxiv.org/abs/2306.04566](http://arxiv.org/abs/2306.04566)

    综述了机器学习、遥感和物联网方法在农业中的应用，这些技术的整合为农业提供了洞察和预测，提高了农业生产效率和可持续性。

    

    遥感和机器学习在农业中的整合通过数据分析提供了洞察和预测，正在改变行业。这种结合导致了改善产量预测和水管理，从而实现了提高效率、获得更好的产量和更可持续的农业实践。通过整合这些技术，可以开发一个强大的农业移动或Web应用程序，为农民和决策者提供有价值的信息和工具，以改善作物管理和提高效率。本文对机器学习、遥感和物联网方法在产量预测方面的最新应用进行了重要的回顾。

    The integration of remote sensing and machine learning in agriculture is transforming the industry by providing insights and predictions through data analysis. This combination leads to improved yield prediction and water management, resulting in increased efficiency, better yields, and more sustainable agricultural practices. Achieving the United Nations' Sustainable Development Goals, especially "zero hunger," requires the investigation of crop yield and precipitation gaps, which can be accomplished through, the usage of artificial intelligence (AI), machine learning (ML), remote sensing (RS), and the internet of things (IoT). By integrating these technologies, a robust agricultural mobile or web application can be developed, providing farmers and decision-makers with valuable information and tools for improving crop management and increasing efficiency. Several studies have investigated these new technologies and their potential for diverse tasks such as crop monitoring, yield predi
    
[^9]: ChatGPT很有趣，但并不好笑！幽默对于大语言模型依然是一个挑战。

    ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models. (arXiv:2306.04563v1 [cs.AI])

    [http://arxiv.org/abs/2306.04563](http://arxiv.org/abs/2306.04563)

    这篇论文探讨了OpenAI的ChatGPT模型在幽默识别方面的能力，结果表明该模型的幽默并不是硬编码的，但大部分生成的笑话都不是新的，几乎是少量几组重复的。

    

    幽默是人类交流中非常重要的一个方面，迄今为止尚未被解决。大型语言模型（LLMs）越来越能够捕捉隐含和上下文信息。特别是，OpenAI的ChatGPT最近引起了广泛的公众关注。基于GPT3的模型几乎可以达到与人类交流的水平，甚至能够讲笑话。但是，ChatGPT真的很有趣吗？作者针对模型的幽默感进行了测试，包括生成、解释和检测等环节，试图了解ChatGPT理解并再现人类幽默的能力。由于模型本身不可访问，作者采用了基于提示的实验方法。实验证据表明，大部分生成的笑话并不是由该模型新生成的，而是重复了少量的几组笑话。虽然系统可以准确地解释有效的笑话，但也会提出虚构的解释。

    Humor is a central aspect of human communication that has not been solved for artificial agents so far. Large language models (LLMs) are increasingly able to capture implicit and contextual information. Especially, OpenAI's ChatGPT recently gained immense public attention. The GPT3-based model almost seems to communicate on a human level and can even tell jokes. Humor is an essential component of human communication. But is ChatGPT really funny? We put ChatGPT's sense of humor to the test. In a series of exploratory experiments around jokes, i.e., generation, explanation, and detection, we seek to understand ChatGPT's capability to grasp and reproduce human humor. Since the model itself is not accessible, we applied prompt-based experiments. Our empirical evidence indicates that jokes are not hard-coded but mostly also not newly generated by the model. Over 90% of 1008 generated jokes were the same 25 Jokes. The system accurately explains valid jokes but also comes up with fictional ex
    
[^10]: 查询周缘化描述逻辑知识库

    Querying Circumscribed Description Logic Knowledge Bases. (arXiv:2306.04546v1 [cs.AI])

    [http://arxiv.org/abs/2306.04546](http://arxiv.org/abs/2306.04546)

    本文证明了在周缘化描述逻辑知识库上评估连词查询和其并集的可判定性，并且得到了DL-Lite的数据复杂性和合并复杂性的完整描述。

    

    周缘化是定义非单调描述逻辑的主要方法之一。本文证明了在周缘化描述逻辑知识库上评估连词查询和其并集的可判定性，并且得到了ALCHIO、EL到各种版本的DL-Lite的数据复杂性和合并复杂性的完整描述。我们还研究了简单的原子查询。

    Circumscription is one of the main approaches for defining non-monotonic description logics (DLs). While the decidability and complexity of traditional reasoning tasks such as satisfiability of circumscribed DL knowledge bases (KBs) is well understood, for evaluating conjunctive queries (CQs) and unions thereof (UCQs), not even decidability had been established. In this paper, we prove decidability of (U)CQ evaluation on circumscribed DL KBs and obtain a rather complete picture of both the combined complexity and the data complexity, for DLs ranging from ALCHIO via EL to various versions of DL-Lite. We also study the much simpler atomic queries (AQs).
    
[^11]: 对比自举用于标签细化

    Contrastive Bootstrapping for Label Refinement. (arXiv:2306.04544v1 [cs.CL])

    [http://arxiv.org/abs/2306.04544](http://arxiv.org/abs/2306.04544)

    该论文提出了一种对比聚类自举方法，可以在只使用粗略注释和映射的情况下，提高文本分类的细化程度。实验结果表明，该方法优于现有方法。

    

    传统文本分类通常将文本分类到预定义的粗粒度类别中，由此生成的模型无法处理真实世界中定期出现的更精细的类别以提供准确的服务。在这项工作中，我们调查了仅使用粗粒度类别的注释和从粗到细的映射进行细粒度分类的情况。我们提出了一种轻量级对比聚类自举方法，来迭代地优化段落的标签。在聚类过程中，它从全局和本地的角度下面朝负样本-原型组之间保持相对距离。在 NYT 和 20News 上的实验表明，我们的方法明显优于现有方法。

    Traditional text classification typically categorizes texts into pre-defined coarse-grained classes, from which the produced models cannot handle the real-world scenario where finer categories emerge periodically for accurate services. In this work, we investigate the setting where fine-grained classification is done only using the annotation of coarse-grained categories and the coarse-to-fine mapping. We propose a lightweight contrastive clustering-based bootstrapping method to iteratively refine the labels of passages. During clustering, it pulls away negative passage-prototype pairs under the guidance of the mapping from both global and local perspectives. Experiments on NYT and 20News show that our method outperforms the state-of-the-art methods by a large margin.
    
[^12]: 关于扩散模型的设计基础：综述

    On the Design Fundamentals of Diffusion Models: A Survey. (arXiv:2306.04542v1 [cs.LG])

    [http://arxiv.org/abs/2306.04542](http://arxiv.org/abs/2306.04542)

    本文综述了扩散模型的设计基础，即其三个关键组件：正向过程、逆向过程和采样过程，为未来的研究提供了有益的细粒度透视。

    

    扩散模型是一种生成模型，通过逐渐添加和删除噪声来学习训练数据的潜在分布以生成数据。扩散模型的组成部分已经受到了广泛的关注，许多设计选择被提出。现有的评论主要关注高层次的解决方案，对组件的设计基础覆盖较少。本研究旨在通过提供一个全面而连贯的综述，针对扩散模型的组件设计选择进行分析。具体来说，我们将这个综述按照三个关键组件进行组织，即正向过程、逆向过程和采样过程。这使得我们可以提供扩散模型的细粒度透视，有助于未来研究分析个体组件、设计选择的适用性以及扩散模型的实现。

    Diffusion models are generative models, which gradually add and remove noise to learn the underlying distribution of training data for data generation. The components of diffusion models have gained significant attention with many design choices proposed. Existing reviews have primarily focused on higher-level solutions, thereby covering less on the design fundamentals of components. This study seeks to address this gap by providing a comprehensive and coherent review on component-wise design choices in diffusion models. Specifically, we organize this review according to their three key components, namely the forward process, the reverse process, and the sampling procedure. This allows us to provide a fine-grained perspective of diffusion models, benefiting future studies in the analysis of individual components, the applicability of design choices, and the implementation of diffusion models.
    
[^13]: 自顶向下的知识编译用于计数模理论（Counting Modulo Theories）

    Top-Down Knowledge Compilation for Counting Modulo Theories. (arXiv:2306.04541v1 [cs.AI])

    [http://arxiv.org/abs/2306.04541](http://arxiv.org/abs/2306.04541)

    这篇论文讨论了计数模理论（#SMT）的编译策略，并提出了一种基于全面的DPLL（T）搜索痕迹的自上而下的编译器。

    

    当输入公式是确定性可分解否定范式（d-DNNF）时，可以有效地解决命题模型计数问题（＃SAT）。将任意公式转换为允许执行推理任务（如计数）的表示形式称为知识编译。自顶向下的知识编译是解决＃SAT问题的最先进技术，它利用全面的DPLL搜索的痕迹来获得d-DNNF表示。虽然知识编译在命题方法方面得到了很好的研究，但对于（无量化）计数模理论设置（＃SMT）的知识编译研究要少得多。在本文中，我们讨论＃SMT的编译策略。我们特别倡导一种基于全面的DPLL（T）搜索痕迹的自上而下的编译器。

    Propositional model counting (#SAT) can be solved efficiently when the input formula is in deterministic decomposable negation normal form (d-DNNF). Translating an arbitrary formula into a representation that allows inference tasks, such as counting, to be performed efficiently, is called knowledge compilation. Top-down knowledge compilation is a state-of-the-art technique for solving #SAT problems that leverages the traces of exhaustive DPLL search to obtain d-DNNF representations. While knowledge compilation is well studied for propositional approaches, knowledge compilation for the (quantifier free) counting modulo theory setting (#SMT) has been studied to a much lesser degree. In this paper, we discuss compilation strategies for #SMT. We specifically advocate for a top-down compiler based on the traces of exhaustive DPLL(T) search.
    
[^14]: 辅助任务下多任务学习的样本级加权

    Sample-Level Weighting for Multi-Task Learning with Auxiliary Tasks. (arXiv:2306.04519v1 [cs.LG])

    [http://arxiv.org/abs/2306.04519](http://arxiv.org/abs/2306.04519)

    提出了一种用于辅助任务下多任务学习的样本级加权算法SLGrad，通过样本特定的任务权重，消除有害的辅助信号并增强有用的任务信号，实现了泛化性能的提升。

    

    多任务学习(MTL)可以通过与相关任务共享表示来提高神经网络的泛化性能。然而，MTL也可能通过任务之间的有害干扰而降低性能。最近的工作采用任务特定的损失加权作为解决干扰的方法。然而，现有算法将任务视为原子性，缺乏将有害和有用信号明确分离的能力。为此，我们提出了SLGrad，一种用于辅助任务下多任务学习的样本级加权算法。通过样本特定的任务权重，SLGrad在训练过程中重新塑造任务分布，消除有害的辅助信号并增强有用的任务信号。在(半)合成数据集和常见的监督多任务问题上观察到了显著的泛化性能提升。

    Multi-task learning (MTL) can improve the generalization performance of neural networks by sharing representations with related tasks. Nonetheless, MTL can also degrade performance through harmful interference between tasks. Recent work has pursued task-specific loss weighting as a solution for this interference. However, existing algorithms treat tasks as atomic, lacking the ability to explicitly separate harmful and helpful signals beyond the task level. To this end, we propose SLGrad, a sample-level weighting algorithm for multi-task learning with auxiliary tasks. Through sample-specific task weights, SLGrad reshapes the task distributions during training to eliminate harmful auxiliary signals and augment useful task signals. Substantial generalization performance gains are observed on (semi-) synthetic datasets and common supervised multi-task problems.
    
[^15]: 晶体材料生成的统一模型

    Unified Model for Crystalline Material Generation. (arXiv:2306.04510v1 [cs.AI])

    [http://arxiv.org/abs/2306.04510](http://arxiv.org/abs/2306.04510)

    该论文提出了两个统一模型，使用周期等变体系结构可以同时作用于晶体晶格和原子位置，通过降低总能量以达到热力学稳定性来学习任意晶体晶格变形。

    

    发现具有特定属性的新型创新晶体材料是我们社会面临的最大挑战之一。最近，生成晶体材料的问题受到了越来越多的关注，但是，我们能否开发同时考虑晶体结构的周期性和等价几何的生成模型仍不清楚。为了解决这个问题，我们提出了两个统一模型，它们同时作用于晶体晶格和原子位置，使用周期等变体系结构。我们的模型能够通过降低总能量以达到热力学稳定性来学习任意晶体晶格变形。代码和数据可在https://github.com/aklipf/GemsNet上获得。

    One of the greatest challenges facing our society is the discovery of new innovative crystal materials with specific properties. Recently, the problem of generating crystal materials has received increasing attention, however, it remains unclear to what extent, or in what way, we can develop generative models that consider both the periodicity and equivalence geometric of crystal structures. To alleviate this issue, we propose two unified models that act at the same time on crystal lattice and atomic positions using periodic equivariant architectures. Our models are capable to learn any arbitrary crystal lattice deformation by lowering the total energy to reach thermodynamic stability. Code and data are available at https://github.com/aklipf/GemsNet.
    
[^16]: 通过答案反馈增强多跨度问答的上下文学习

    Enhancing In-Context Learning with Answer Feedback for Multi-Span Question Answering. (arXiv:2306.04508v1 [cs.CL])

    [http://arxiv.org/abs/2306.04508](http://arxiv.org/abs/2306.04508)

    本文提出了一种使用答案反馈来增强多跨度问答的上下文学习方法，将传统的演示示例扩展了反馈信息，可以有效地提高模型的预测性能。

    

    最近，大型语言模型（LLM）如ChatGPT的出现展示了令人印象深刻的通用性能力，但在特定任务（如多跨度问答）上仍存在较大差距。先前的研究发现，通过使用少量任务相关的标记数据作为演示示例构建少量追问提示，可以利用上下文学习有效地利用LLM。一种流行的实现方式是通过简单的模板将几个问题及其正确答案进行连接，通知LLM所需的输出。在本文中，我们提出了一种新颖的利用标记数据的方法，使之也提供对LLM一些不期望的输出的提示，通过扩展演示示例以反馈预测模型预测的答案，例如，正确、不正确或不完整。在三个多跨度问答数据集以及一个关键词提取数据集上的实验证明，我们的新提示策略显著优于现有方法。

    Whereas the recent emergence of large language models (LLMs) like ChatGPT has exhibited impressive general performance, it still has a large gap with fully-supervised models on specific tasks such as multi-span question answering. Previous researches found that in-context learning is an effective approach to exploiting LLM, by using a few task-related labeled data as demonstration examples to construct a few-shot prompt for answering new questions. A popular implementation is to concatenate a few questions and their correct answers through simple templates, informing LLM of the desired output. In this paper, we propose a novel way of employing labeled data such that it also informs LLM of some undesired output, by extending demonstration examples with feedback about answers predicted by an off-the-shelf model, e.g., correct, incorrect, or incomplete. Experiments on three multi-span question answering datasets as well as a keyphrase extraction dataset show that our new prompting strateg
    
[^17]: 伪证书选择的难度

    Hardness of Deceptive Certificate Selection. (arXiv:2306.04505v1 [cs.LG])

    [http://arxiv.org/abs/2306.04505](http://arxiv.org/abs/2306.04505)

    本文证明了利用AFC在高保真度和完备性的设定下，选择无信息证书的任务是 NP-hard 的。

    

    最近，基于交互式证明系统的分类器在实现AI理论可解释性方面取得了进展。证明者从数据点中选择证书并将其发送给验证者，后者决定分类。在机器学习的背景下，这样的证书可以是信息性的特征。对于高度保真与完备性的设定，交换的证书必须与数据点的真实分类有高的相互信息。然而，这种保证依赖于数据集的非对称特征相关性的界限，这是迄今为止难以估计高维数据的属性。W\"aldchen等人猜测，利用AFC是计算上困难的，我们在这里证明了这一点。我们考虑恶意的证明者-验证者二元组，旨在利用AFC来实现高的完备性和保真性，同时使用没有信息的证书。我们证明了这个任务是$\mathsf{NP}$难的。

    Recent progress towards theoretical interpretability guarantees for AI has been made with classifiers that are based on interactive proof systems. A prover selects a certificate from the datapoint and sends it to a verifier who decides the class. In the context of machine learning, such a certificate can be a feature that is informative of the class. For a setup with high soundness and completeness, the exchanged certificates must have a high mutual information with the true class of the datapoint. However, this guarantee relies on a bound on the Asymmetric Feature Correlation of the dataset, a property that so far is difficult to estimate for high-dimensional data. It was conjectured in W\"aldchen et al. that it is computationally hard to exploit the AFC, which is what we prove here.  We consider a malicious prover-verifier duo that aims to exploit the AFC to achieve high completeness and soundness while using uninformative certificates. We show that this task is $\mathsf{NP}$-hard an
    
[^18]: 自适应基于梯度的异常值去除的嘈杂标签学习方法

    Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal. (arXiv:2306.04502v1 [cs.LG])

    [http://arxiv.org/abs/2306.04502](http://arxiv.org/abs/2306.04502)

    本文提出了一种名为AGRA的自适应梯度异常值去除方法，能够在模型训练过程中动态调整数据集从而有效提高模型学习效果。

    

    训练可靠和高性能模型需要准确和丰富的数据集，但即便是人工标注的数据集也会包含错误，更不用说自动标注的数据集了。现有的一些数据去噪方法主要集中于检测异常值并进行永久性去除，但这种方法很容易过度或者欠度过滤数据集。在本论文中，我们提出了一种新的自适应梯度异常值去除方法（AGRA），不同于在模型训练之前清洗数据集，我们的方法在训练过程中动态调整数据集。通过比较一组样本的累积梯度和单个样本的梯度，我们的方法可以决定是否在当前更新时保留对应的样本，以此来确定它是否有助于模型的学习效果。在多个数据集上进行的广泛评估表明，AGRA方法的有效性，并且全面的结果分析证实了我们方法的理论和实践收益。

    An accurate and substantial dataset is necessary to train a reliable and well-performing model. However, even manually labeled datasets contain errors, not to mention automatically labeled ones. The problem of data denoising was addressed in different existing research, most of which focuses on the detection of outliers and their permanent removal - a process that is likely to over- or underfilter the dataset. In this work, we propose AGRA: a new method for Adaptive GRAdient-based outlier removal. Instead of cleaning the dataset prior to model training, the dataset is adjusted during the training process. By comparing the aggregated gradient of a batch of samples and an individual example gradient, our method dynamically decides whether a corresponding example is helpful for the model at this point or is counter-productive and should be left out for the current update. Extensive evaluation on several datasets demonstrates the AGRA effectiveness, while comprehensive results analysis sup
    
[^19]: 基于结合多样化奖励微调权重插值的帕累托最优对齐的奖励汤

    Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. (arXiv:2306.04488v1 [cs.LG])

    [http://arxiv.org/abs/2306.04488](http://arxiv.org/abs/2306.04488)

    本文提出了 rewarded soup 方法，通过结合多种代理奖励，实现微调权重插值，从而在整个偏好空间中实现帕累托最优广义化。该方法在强化学习任务上具有有效性。

    

    基础模型首先在大量无参考数据集上进行预训练，然后在有标注的数据上进行微调。强化学习，特别是来自人类反馈的强化学习(RLHF)，可以进一步使网络与预期的使用相匹配。然而，代理奖励的缺陷可能会妨碍训练，导致次优结果；现实任务和人类意见的多样性加剧了这个问题。本文提出通过采用多策略方法来拥抱多样化奖励的异质性。我们的目标不是专注于单一的先验奖励，而是在整个偏好空间中实现帕累托最优广义化。为此，我们提出了 rewarded soup，首先独立地专门化多个网络(每个代理奖励一个)，然后在它们的权重之间进行线性插值。通过实验表明，这种方法是成功的，因为我们展示了当多样化奖励来自共享的预训练初始化时，权重仍然保持线性连接。我们展示了该方法在强化学习任务上的有效性。

    Foundation models are first pre-trained on vast unsupervised datasets and then fine-tuned on labeled data. Reinforcement learning, notably from human feedback (RLHF), can further align the network with the intended usage. Yet the imperfections in the proxy reward may hinder the training and lead to suboptimal results; the diversity of objectives in real-world tasks and human opinions exacerbate the issue. This paper proposes embracing the heterogeneity of diverse rewards by following a multi-policy strategy. Rather than focusing on a single a priori reward, we aim for Pareto-optimal generalization across the entire space of preferences. To this end, we propose rewarded soup, first specializing multiple networks independently (one for each proxy reward) and then interpolating their weights linearly. This succeeds empirically because we show that the weights remain linearly connected when fine-tuned on diverse rewards from a shared pre-trained initialization. We demonstrate the effective
    
[^20]: 人工智能可以通过修改互动伙伴的外貌来促进自私决策

    Artificial Intelligence can facilitate selfish decisions by altering the appearance of interaction partners. (arXiv:2306.04484v1 [cs.AI])

    [http://arxiv.org/abs/2306.04484](http://arxiv.org/abs/2306.04484)

    本研究发现，通过模糊滤镜等外观改变技术，人工智能可以导致个体对模糊他人的自私行为增加，这表明其可以通过非个性化促进道德缩减，需要更广泛的伦理讨论。

    

    社交媒体和视频会议技术中越来越普遍的图像改变滤镜引起了关于使用人工智能（AI）操纵我们对他人的认知的伦理和心理影响的担忧。本研究重点研究了模糊滤镜——一种外观改变技术——对个体对待他人行为的潜在影响。我们的发现一致表明，对那些外貌被模糊的人显示出更多的自私行为，这表明模糊滤镜可以通过非个性化促进道德缩减。这些结果强调有关修改我们对他人的认知的AI技术的更广泛的伦理讨论的必要性，包括透明度、同意和意识到被他人的外貌操作所影响的问题。我们还强调先发性的实验在引导负责任的Gui开发方面的重要性。

    The increasing prevalence of image-altering filters on social media and video conferencing technologies has raised concerns about the ethical and psychological implications of using Artificial Intelligence (AI) to manipulate our perception of others. In this study, we specifically investigate the potential impact of blur filters, a type of appearance-altering technology, on individuals' behavior towards others. Our findings consistently demonstrate a significant increase in selfish behavior directed towards individuals whose appearance is blurred, suggesting that blur filters can facilitate moral disengagement through depersonalization. These results emphasize the need for broader ethical discussions surrounding AI technologies that modify our perception of others, including issues of transparency, consent, and the awareness of being subject to appearance manipulation by others. We also emphasize the importance of anticipatory experiments in informing the development of responsible gui
    
[^21]: 无需训练的神经主动学习与初始化鲁棒性保证

    Training-Free Neural Active Learning with Initialization-Robustness Guarantees. (arXiv:2306.04454v1 [cs.LG])

    [http://arxiv.org/abs/2306.04454](http://arxiv.org/abs/2306.04454)

    本研究提出了一种期望方差与高斯过程（EV-GP）标准用于神经主动学习，该方法不需要对神经网络进行训练，并且保证在数据选择时NN具有良好的预测性能和初始化鲁棒性。

    

    现有的神经主动学习算法旨在通过选择数据进行标记来优化神经网络（NN）的预测性能。然而，除了良好的预测性能外，对于随机参数初始化的鲁棒性也是安全关键应用的重要要求。为此，我们引入了期望方差与高斯过程（EV-GP）标准来进行神经主动学习，该标准在理论上保证选择数据点可以导致训练的 NN 具有良好的预测性能和初始化鲁棒性。重要的是，我们的 EV-GP 标准是无需训练的，即在数据选择过程中不需要对 NN 进行任何训练，这使其在计算上更加高效。我们通过实验证明，我们的 EV-GP 标准与初始化鲁棒性和概括性能高度相关，并且表明它在两个期望方面的表现均优于基线方法，尤其是初始化鲁棒性方面。

    Existing neural active learning algorithms have aimed to optimize the predictive performance of neural networks (NNs) by selecting data for labelling. However, other than a good predictive performance, being robust against random parameter initializations is also a crucial requirement in safety-critical applications. To this end, we introduce our expected variance with Gaussian processes (EV-GP) criterion for neural active learning, which is theoretically guaranteed to select data points which lead to trained NNs with both (a) good predictive performances and (b) initialization robustness. Importantly, our EV-GP criterion is training-free, i.e., it does not require any training of the NN during data selection, which makes it computationally efficient. We empirically demonstrate that our EV-GP criterion is highly correlated with both initialization robustness and generalization performance, and show that it consistently outperforms baseline methods in terms of both desiderata, especiall
    
[^22]: 以双策略为自模型的规划

    Dual policy as self-model for planning. (arXiv:2306.04440v1 [cs.AI])

    [http://arxiv.org/abs/2306.04440](http://arxiv.org/abs/2306.04440)

    该论文探究了使用精简策略网络作为自我模型的优缺点，并通过实验结果表明，使用经过精简的策略网络作为自我模型可以稳定训练，并且有更快的收敛速度。

    

    规划是一种数据有效的决策策略，代理通过探索可能的未来状态来选择候选动作。当存在高维行动空间时，为了模拟未来状态，必须使用自己的决策策略来限制所需探索的动作数量。我们将用于模拟自己决策的模型称为代理的自我模型。尽管在规划行动时，世界模型通常与自我模型一起隐含地使用，但如何设计自我模型仍不清楚。受当前强化学习方法和神经科学的启发，我们探讨了使用精简策略网络作为自我模型的优缺点。在这样的双策略代理中，一个无模型策略和一个经过精简的策略分别用于无模型动作和计划动作。我们在一个生态相关的参数环境上的结果表明，使用经过精简的策略网络作为自我模型可以稳定训练，并且有更快的收敛速度。

    Planning is a data efficient decision-making strategy where an agent selects candidate actions by exploring possible future states. To simulate future states when there is a high-dimensional action space, the knowledge of one's decision making strategy must be used to limit the number of actions to be explored. We refer to the model used to simulate one's decisions as the agent's self-model. While self-models are implicitly used widely in conjunction with world models to plan actions, it remains unclear how self-models should be designed. Inspired by current reinforcement learning approaches and neuroscience, we explore the benefits and limitations of using a distilled policy network as the self-model. In such dual-policy agents, a model-free policy and a distilled policy are used for model-free actions and planned actions, respectively. Our results on a ecologically relevant, parametric environment indicate that distilled policy network for self-model stabilizes training, has faster i
    
[^23]: 无监督领域适应方法用于跨数据库和跨通道的心电信号心律失常分类

    Cross-Database and Cross-Channel ECG Arrhythmia Heartbeat Classification Based on Unsupervised Domain Adaptation. (arXiv:2306.04433v1 [eess.SP])

    [http://arxiv.org/abs/2306.04433](http://arxiv.org/abs/2306.04433)

    本研究利用无监督领域适应方法解决了跨领域ECG信号分类的挑战，提出了一种基于跨域特征差异优化的领域自适应深度网络，处理了最坏情况训练损失的消失，解决了ECG信号分类的限制性问题。

    

    心电图(ECG)的分类在自动心血管诊断系统的发展中扮演着至关重要的角色。但是，不同个体之间ECG信号的相当差异是一个重大挑战。数据分布的变化限制了模型的跨领域利用。本研究提出了一种解决方案，通过利用从标记源域获取的知识来对未标记的数据集中的ECG进行分类。我们提出了一种基于跨域特征差异优化的领域自适应深度网络。我们的方法包括三个阶段：预训练、聚类中心计算和自适应。在预训练阶段，我们采用分布鲁棒优化(DRO)技术来处理最坏情况训练损失的消失。为了增强特征的丰富性，我们将三个时间特征与深度学习特征连接起来。聚类计算阶段涉及计算源聚类的明显可分离簇的质心，

    The classification of electrocardiogram (ECG) plays a crucial role in the development of an automatic cardiovascular diagnostic system. However, considerable variances in ECG signals between individuals is a significant challenge. Changes in data distribution limit cross-domain utilization of a model. In this study, we propose a solution to classify ECG in an unlabeled dataset by leveraging knowledge obtained from labeled source domain. We present a domain-adaptive deep network based on cross-domain feature discrepancy optimization. Our method comprises three stages: pre-training, cluster-centroid computing, and adaptation. In pre-training, we employ a Distributionally Robust Optimization (DRO) technique to deal with the vanishing worst-case training loss. To enhance the richness of the features, we concatenate three temporal features with the deep learning features. The cluster computing stage involves computing centroids of distinctly separable clusters for the source using true labe
    
[^24]: 利用稳定平衡点推进高性能探索性数据分析

    Towards High-Performance Exploratory Data Analysis (EDA) Via Stable Equilibrium Point. (arXiv:2306.04425v1 [cs.LG])

    [http://arxiv.org/abs/2306.04425](http://arxiv.org/abs/2306.04425)

    本文提出了一种基于稳定平衡点的框架，用于提高探索性数据分析的效率和解决方案质量，该方法能够为大规模数据集生成高质量的聚类和数据可视化。

    

    探索性数据分析（EDA）是数据科学项目中的重要过程。在本文中，我们介绍了一种基于稳定平衡点（SEP）的框架，用于提高EDA的效率和解决方案质量。通过利用SEP作为代表点，我们的方法旨在为大规模数据集生成高质量的聚类和数据可视化。该方法的一个非常独特的属性是，SEP将直接编码数据集的聚类属性。与先前的最先进的聚类和数据可视化方法相比，所提出的方法允许在大规模数据分析任务中显着提高计算效率和解决方案质量。

    Exploratory data analysis (EDA) is a vital procedure for data science projects. In this work, we introduce a stable equilibrium point (SEP) - based framework for improving the efficiency and solution quality of EDA. By exploiting the SEPs to be the representative points, our approach aims to generate high-quality clustering and data visualization for large-scale data sets. A very unique property of the proposed method is that the SEPs will directly encode the clustering properties of data sets. Compared with prior state-of-the-art clustering and data visualization methods, the proposed methods allow substantially improving computing efficiency and solution quality for large-scale data analysis tasks.
    
[^25]: 社交机器人在儿童哮喘治疗依从性方面的应用

    Social robots to improve therapeutic adherence in pediatric asthma. (arXiv:2306.04422v1 [cs.AI])

    [http://arxiv.org/abs/2306.04422](http://arxiv.org/abs/2306.04422)

    通过与人类形态机器人的游戏化会话，可以提高儿童哮喘患者的治疗依从性。

    

    在慢性疾病中，正确诊断和提供最适当的治疗通常并不足以保证患者临床状况的改善。低治疗依从性是阻碍达到治疗目标的主要原因之一，尤其是在某些疾病和特定目标患者（如儿童）中尤为常见。一种有趣的技术可以用于支持临床实践，以实现更好的健康结果。我们假设与传统的治疗教育方法相比，与人类形态机器人的游戏化会话可以更加有效地教授儿童哮喘患者正确的吸入方法。在这个角度上，我们描述了在Pepper机器人平台上实现的交互式模块以及计划于2020年在Palermo的CNR肺部过敏兒童诊所开展的一项研究。由于COVID-19紧急情况的原因，这项研究被取消，但我们提供初步结果，以评估方法的可行性和同龄人的兴趣。通过分析培训前后的吸入习惯的比较和给参与者的问卷调查结果来评估教育结果。

    In chronic diseases, obtaining a correct diagnosis and providing the most appropriate treatments often is not enough to guarantee an improvement of the clinical condition of a patient. Poor adherence to medical prescriptions constitutes one of the main causes preventing achievement of therapeutic goals. This is generally true especially for certain diseases and specific target patients, such as children. An engaging and entertaining technology can be exploited in support of clinical practices to achieve better health outcomes. Our assumption is that a gamified session with a humanoid robot, compared to the usual methodologies for therapeutic education, can be more incisive in learning the correct inhalation procedure in children affected by asthma. In this perspective, we describe an interactive module implemented on the Pepper robotic platform and the setting of a study that was planned in 2020 to be held at the Pneumoallergology Pediatric clinic of CNR in Palermo. The study was cance
    
[^26]: 在潜在空间中使用去噪扩散技术合成逼真的沙粒组装

    Synthesizing realistic sand assemblies with denoising diffusion in latent space. (arXiv:2306.04411v1 [cs.AI])

    [http://arxiv.org/abs/2306.04411](http://arxiv.org/abs/2306.04411)

    本文介绍了一种使用去噪扩散算法在潜在空间中生成沙粒组装的方法，通过点云自编码器把沙粒的三维点云结构编码到较低维度的潜在空间中，生成的沙粒样本符合原始数据分布，并可用于地质工程、计算机动画、石油工程等应用。

    

    沙粒组装中的形态和形态特征在许多工程应用中，如地质工程、计算机动画、石油工程和浓缩太阳能方面具有深远的影响。本文介绍了一种去噪扩散算法，利用从单个沙粒表面收集的一组点云生成潜在空间中的沙粒。通过使用点云自编码器，把沙粒的三维点云结构首先编码到较低维度的潜在空间中。训练了一种生成去噪扩散概率模型，用来生成最大化生成样本属于原始数据分布的对数似然的合成沙粒，这通过Kullback-Leibler散度来测量。

    The shapes and morphological features of grains in sand assemblies have far-reaching implications in many engineering applications, such as geotechnical engineering, computer animations, petroleum engineering, and concentrated solar power. Yet, our understanding of the influence of grain geometries on macroscopic response is often only qualitative, due to the limited availability of high-quality 3D grain geometry data. In this paper, we introduce a denoising diffusion algorithm that uses a set of point clouds collected from the surface of individual sand grains to generate grains in the latent space. By employing a point cloud autoencoder, the three-dimensional point cloud structures of sand grains are first encoded into a lower-dimensional latent space. A generative denoising diffusion probabilistic model is trained to produce synthetic sand that maximizes the log-likelihood of the generated samples belonging to the original data distribution measured by a Kullback-Leibler divergence.
    
[^27]: 以奖励调制STDP为特征的脉冲神经网络元学习

    Meta-Learning in Spiking Neural Networks with Reward-Modulated STDP. (arXiv:2306.04410v1 [cs.AI])

    [http://arxiv.org/abs/2306.04410](http://arxiv.org/abs/2306.04410)

    本研究提出了一种受到海马和前额叶皮质启发的生物可行元学习模型，使用带有奖励的学习系统的脉冲神经网络，具有在最小数据下适应新任务和快速学习连续任务的能力，避免了灾难性遗忘现象，并且优于其他生物可行模型。

    

    人类大脑通过把获得的知识和经验整合到记忆中，不断学习并迅速适应新环境。在机器学习模型中，开发这种能力被视为人工智能研究的重要目标，因为当数据有限或者需要快速适应新的未知任务时，深度神经网络表现不佳。近期提出了一些在高性能水平上表现出低数据学习特性的元学习模型，但它们不符合生物学的实现方式。我们提出了一种受到海马和前额叶皮质启发的生物可行元学习模型，使用带有奖励的学习系统的脉冲神经网络。我们的模型包括一个旨在防止灾难性遗忘的记忆，这种现象在元学习模型在新任务开始时会忘记它们所学的内容，同时我们的新模型具有在最小数据下适应新任务和快速学习连续任务的能力。提出的模型优于其他生物可行模型，并与神经生物学发现相容。

    The human brain constantly learns and rapidly adapts to new situations by integrating acquired knowledge and experiences into memory. Developing this capability in machine learning models is considered an important goal of AI research since deep neural networks perform poorly when there is limited data or when they need to adapt quickly to new unseen tasks. Meta-learning models are proposed to facilitate quick learning in low-data regimes by employing absorbed information from the past. Although some models have recently been introduced that reached high-performance levels, they are not biologically plausible. We have proposed a bio-plausible meta-learning model inspired by the hippocampus and the prefrontal cortex using spiking neural networks with a reward-based learning system. Our proposed model includes a memory designed to prevent catastrophic forgetting, a phenomenon that occurs when meta-learning models forget what they have learned as soon as the new task begins. Also, our new
    
[^28]: 使用非对称梯度引导来改进扩散图像翻译

    Improving Diffusion-based Image Translation using Asymmetric Gradient Guidance. (arXiv:2306.04396v1 [cs.CV])

    [http://arxiv.org/abs/2306.04396](http://arxiv.org/abs/2306.04396)

    本文提出了一种使用非对称梯度引导来指导扩散采样的反向过程的方法，以改善扩散图像翻译的风格转换和内容保留之间的平衡。

    

    最近，扩散模型在图像翻译任务中取得了显着进展。然而，由于其随机性，通常存在着风格转换和内容保留之间的平衡。为了解决这些挑战，本文提出了一种使用非对称梯度引导来指导扩散采样的反向过程的方法。这导致了更快和更稳定的图像操作，适用于基于文本和图片的图像翻译。

    Diffusion models have shown significant progress in image translation tasks recently. However, due to their stochastic nature, there's often a trade-off between style transformation and content preservation. Current strategies aim to disentangle style and content, preserving the source image's structure while successfully transitioning from a source to a target domain under text or one-shot image conditions. Yet, these methods often require computationally intense fine-tuning of diffusion models or additional neural networks. To address these challenges, here we present an approach that guides the reverse process of diffusion sampling by applying asymmetric gradient guidance. This results in quicker and more stable image manipulation for both text-guided and image-guided image translation. Our model's adaptability allows it to be implemented with both image- and latent-diffusion models. Experiments show that our method outperforms various state-of-the-art models in image translation ta
    
[^29]: 多语言临床实体识别：翻译还是跨语言迁移？

    Multilingual Clinical NER: Translation or Cross-lingual Transfer?. (arXiv:2306.04384v1 [cs.CL])

    [http://arxiv.org/abs/2306.04384](http://arxiv.org/abs/2306.04384)

    本研究比较了翻译和跨语言迁移两种方法来执行临床领域命名实体识别（NER），并证明跨语言迁移比这两种翻译方法在法语和德语中都具有更好的性能。

    

    临床领域非英语文本中的命名实体识别（NER）等自然语言任务可能非常耗时和昂贵，因为缺乏标注数据。跨语言迁移（CLT）是一种绕过这个问题的方法，多语言大型语言模型可以在一个语言上对特定任务进行微调，并在另一个语言上提供高精度。然而，还有其他利用翻译模型的方法可以在目标语言中执行NER，而无需标注数据，可以通过翻译训练集或测试集。本文比较了跨语言迁移与这两种替代方法，以在法语和德语中执行临床NER，而不需要这些语言的任何训练数据。为此，我们发布了MedNERF，这是从法国药物处方中提取的医学NER测试集，并使用与英语数据集相同的指南进行了注释。通过对这个数据集和一个德语医学语料库的广泛实验，我们表明，跨语言迁移比两种翻译方法在两种目标语言中都具有更好的性能。

    Natural language tasks like Named Entity Recognition (NER) in the clinical domain on non-English texts can be very time-consuming and expensive due to the lack of annotated data. Cross-lingual transfer (CLT) is a way to circumvent this issue thanks to the ability of multilingual large language models to be fine-tuned on a specific task in one language and to provide high accuracy for the same task in another language. However, other methods leveraging translation models can be used to perform NER without annotated data in the target language, by either translating the training set or test set. This paper compares cross-lingual transfer with these two alternative methods, to perform clinical NER in French and in German without any training data in those languages. To this end, we release MedNERF a medical NER test set extracted from French drug prescriptions and annotated with the same guidelines as an English dataset. Through extensive experiments on this dataset and on a German medica
    
[^30]: 基于分布特征匹配的标签偏移量量化及其鲁棒性保证

    Label Shift Quantification with Robustness Guarantees via Distribution Feature Matching. (arXiv:2306.04376v1 [stat.ML])

    [http://arxiv.org/abs/2306.04376](http://arxiv.org/abs/2306.04376)

    本文提出一种名为DFM框架的方法，用于量化标签偏移，并证明了其性能上限和鲁棒性。使用基于核的DFM版本可以提高效率、可扩展性和鲁棒性。

    

    量化学习处理在标签偏移下估计目标标签分布的任务。本文首先提出了一个统一的框架，分布特征匹配（DFM），将先前文献中引入的各种估计器恢复为特定实例。我们推导了DFM程序的一般性能界，改进了先前在特定情况下推导的界限的若干关键方面。然后，我们将这一分析扩展到研究DFM程序在未精确假设标签偏移量的情况下的鲁棒性，特别是在目标受到未知分布污染的情况下。这些理论发现在模拟和实际数据集上得到了详细的数字研究确认。我们还使用随机傅里叶特征原理介绍了一种高效，可扩展且具有鲁棒性的基于核的DFM版本。

    Quantification learning deals with the task of estimating the target label distribution under label shift. In this paper, we first present a unifying framework, distribution feature matching (DFM), that recovers as particular instances various estimators introduced in previous literature. We derive a general performance bound for DFM procedures, improving in several key aspects upon previous bounds derived in particular cases. We then extend this analysis to study robustness of DFM procedures in the misspecified setting under departure from the exact label shift hypothesis, in particular in the case of contamination of the target by an unknown distribution. These theoretical findings are confirmed by a detailed numerical study on simulated and real-world datasets. We also introduce an efficient, scalable and robust version of kernel-based DFM using the Random Fourier Feature principle.
    
[^31]: 基于GCN可信度预测的协同移动群感知的高效招募策略

    Efficient Recruitment Strategy for Collaborative Mobile Crowd Sensing Based on GCN Trustworthiness Prediction. (arXiv:2306.04366v1 [cs.SI])

    [http://arxiv.org/abs/2306.04366](http://arxiv.org/abs/2306.04366)

    本文提出了一种基于GCN可信度预测的协同移动群感知的高效招募策略，通过捕获工人之间的非对称信任关系和工人能力来实现有效的任务分配，优于现有方法。

    

    协同移动群感知可以通过促进任务感知的团队合作来提高数据质量和覆盖范围，而工人招募则代表着一个复杂的多目标优化问题。现有策略主要关注工人本身的特征，忽略了工人之间的非对称信任关系，从而影响了任务效用评估的合理性。为解决这个问题，本文首先使用Mini-Batch K-Means聚类算法和边缘服务器来实现高效的分布式工人招募。利用历史数据和任务要求获得工人的能力类型和距离。使用工人社交网络中的信任导向图输入至图卷积网络（GCN）框架进行训练，捕获工人之间的非对称信任关系。通过工人之间的高信任值，防止CMCS场景下的隐私泄露。最终，利用预测的信任和工人能力构建了一个无向招募图，以实现有效的任务分配。实验结果表明，与现有方法相比，这种招募方法在招募准确度、任务完成时间和能量消耗方面表现优异。

    Collaborative Mobile Crowd Sensing (CMCS) enhances data quality and coverage by promoting teamwork in task sensing, with worker recruitment representing a complex multi-objective optimization problem. Existing strategies mainly focus on the characteristics of workers themselves, neglecting the asymmetric trust relationships between them, which affects the rationality of task utility evaluation. To address this, this paper first employs the Mini-Batch K-Means clustering algorithm and deploys edge servers to enable efficient distributed worker recruitment. Historical data and task requirements are utilized to obtain workers' ability types and distances. A trust-directed graph in the worker's social network is input into the Graph Convolutional Network (GCN) framework for training, capturing asymmetric trustworthiness between worker pairs. Privacy leakage is prevented in CMCS scenarios through high trust values between workers. Ultimately, an undirected recruitment graph is constructed us
    
[^32]: 基于基带信号的毫米波有源相控阵天线鲁棒高效故障诊断

    Robust and Efficient Fault Diagnosis of mm-Wave Active Phased Arrays using Baseband Signal. (arXiv:2306.04360v1 [eess.SP])

    [http://arxiv.org/abs/2306.04360](http://arxiv.org/abs/2306.04360)

    本文提出了基于深度神经网络的新方法，使用基带信号来快速准确地诊断APA中的故障元件和部件，并证明其具有高98%的精度。

    

    5G和6G无线电中一个关键的通信模块是有源相控阵天线(APA)，为确保可靠操作，现场高效及时的apa故障诊断至关重要。迄今为止，故障诊断一直依靠使用昂贵的设备和多个严格控制的测量探针测量频域辐射图案，这些方法耗时、复杂，因此在现场部署中不可行。本文提出了一种新的方法，利用深度神经网络(DNN)来提取基带中隐藏的特征，从而区分不同的故障，只需在一个测量点使用一个探针即可快速准确地诊断apa中的故障元件和部件。所提出的方法使用商用28 GHz APA进行了验证。针对单元素和多元素故障检测，分别展示了99%和80%的准确率。研究了三种不同的测试场景：on

    One key communication block in 5G and 6G radios is the active phased array (APA). To ensure reliable operation, efficient and timely fault diagnosis of APAs on-site is crucial. To date, fault diagnosis has relied on measurement of frequency domain radiation patterns using costly equipment and multiple strictly controlled measurement probes, which are time-consuming, complex, and therefore infeasible for on-site deployment. This paper proposes a novel method exploiting a Deep Neural Network (DNN) tailored to extract the features hidden in the baseband in-phase and quadrature signals for classifying the different faults. It requires only a single probe in one measurement point for fast and accurate diagnosis of the faulty elements and components in APAs.  Validation of the proposed method is done using a commercial 28 GHz APA. Accuracies of 99% and 80% have been demonstrated for single- and multi-element failure detection, respectively. Three different test scenarios are investigated: on
    
[^33]: 用于基于检索的对话系统的上下文掩码自编码器

    ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems. (arXiv:2306.04357v1 [cs.CL])

    [http://arxiv.org/abs/2306.04357](http://arxiv.org/abs/2306.04357)

    本研究提出了一种针对对话响应选择的后训练技术Dial-MAE，利用生成方法更好地压缩对话语义至密集向量，并提高对话响应选择准确性。

    

    对话响应选择旨在根据给定的用户和系统话语历史记录从几个候选响应中选择适当的响应。最近的研究通过后训练大多依赖于单纯的掩码语言建模方法来提高对话响应选择的准确性。但是，最近开发的生成方法在IR社区展示了有希望的文本表示能力，这可能会导致更好的对话语义建模。因此，在本文中，我们提出 Dial-MAE（对话上下文掩码自编码器），这是一种简单而有效的针对对话响应选择的后训练技术。 Dial-MAE使用一个不对称的编码器-解码器架构，学习将对话的语义更好地压缩到密集向量中。 Dial-MAE的过程包括由深度编码器创建带有掩码对话上下文的对话嵌入，然后是浅解码器，该解码器使用此嵌入以及上下文向量来生成响应。

    Dialogue response selection aims to select an appropriate response from several candidates based on a given user and system utterance history. Recent studies have been improving the accuracy of dialogue response selection through post-training, mostly relying on naive masked language modeling methods. However, the recently developed generative methods have shown promising text representation capabilities in IR community, which could potentially lead to better dialogue semantics modeling. Thus, in this paper, we propose Dial-MAE (Dialogue Contextual Masking Auto-encoder), a straightforward yet effective post-training technique tailored for dialogue response selection. Dial-MAE uses an asymmetric encoder-decoder architecture that learns to better compress the semantics of the dialogue into dialogue-dense vectors. The process of Dial-MAE involves a deep encoder creating a dialogue embedding with the masked dialogue context, followed by a shallow decoder that uses this embedding along with
    
[^34]: 面向情感-原因对提取的协同演化图推理网络

    Co-evolving Graph Reasoning Network for Emotion-Cause Pair Extraction. (arXiv:2306.04340v1 [cs.CL])

    [http://arxiv.org/abs/2306.04340](http://arxiv.org/abs/2306.04340)

    本论文提出了一个面向情感-原因对提取的协同演化图推理网络，通过建立异构图捕捉因果关系和显式依赖关系，实现了情感-原因对提取的新的多任务学习框架，并在实验中取得了明显优于最先进方法的效果。

    

    情感-原因对提取（ECPE）旨在从文档中提取所有情感子句及其相应的原因子句。现有方法通过多任务学习（MTL）框架来解决这一任务，在该框架中，这两个子任务为ECPE提供了指示性的线索。然而，以前的MTL框架仅考虑了一轮多任务推理，忽略了从ECPE到子任务的反馈。此外，其多任务推理仅依赖于语义水平的交互，无法捕捉显式依赖关系，而编码器共享和多任务隐藏状态级联也难以捕捉因果关系。为了解决这些问题，我们首先提出了一种基于协同演化推理的新MTL框架。它（1）模拟了ECPE与其子任务之间的双向反馈；（2）允许三个任务一起演化，并相互引导；（3）集成预测级别的交互来捕捉显式依赖关系。然后我们提出了一个新的Co-evolving Graph Reasoning Network（CGRN），来实现Co-evolving Reasoning MTL框架。CGRN在三个任务及其输入之间构建了一个异构图，通过图推理过程自然地捕捉了因果关系和显式依赖关系。在两个ECPE基准测试上的实验证明了我们的CGRN明显优于最先进的方法。

    Emotion-Cause Pair Extraction (ECPE) aims to extract all emotion clauses and their corresponding cause clauses from a document. Existing approaches tackle this task through multi-task learning (MTL) framework in which the two subtasks provide indicative clues for ECPE. However, the previous MTL framework considers only one round of multi-task reasoning and ignores the reverse feedbacks from ECPE to the subtasks. Besides, its multi-task reasoning only relies on semantics-level interactions, which cannot capture the explicit dependencies, and both the encoder sharing and multi-task hidden states concatenations can hardly capture the causalities. To solve these issues, we first put forward a new MTL framework based on Co-evolving Reasoning. It (1) models the bidirectional feedbacks between ECPE and its subtasks; (2) allows the three tasks to evolve together and prompt each other recurrently; (3) integrates prediction-level interactions to capture explicit dependencies. Then we propose a n
    
[^35]: 基于非配对深度学习的动态增强磁共振成像药代动力学参数估计

    Unpaired Deep Learning for Pharmacokinetic Parameter Estimation from Dynamic Contrast-Enhanced MRI. (arXiv:2306.04339v1 [eess.IV])

    [http://arxiv.org/abs/2306.04339](http://arxiv.org/abs/2306.04339)

    该研究提出了一种非配对深度学习方法，使用物理驱动的CycleGAN框架，可以在没有配对数据的情况下准确地估计动态增强磁共振成像药代动力学参数和动脉输入函数。

    

    动态增强磁共振成像 (DCE-MRI) 可以提供关于血管通透性和组织灌注的药代动力学参数信息。然而，传统的药代动力学参数估计方法涉及拟合示踪剂动力学模型，由于噪声动脉输入函数 (AIF) 的测量常常导致计算复杂性和低准确性。虽然一些深度学习方法已被提出来解决这些挑战，但大部分现有的方法依赖于有标签的 DCE-MRI 和已标注的药代动力学参数图。这依赖于标签数据会导致显著的时间和资源限制，可能会引入标签噪声，使得监督学习方法经常不实用。为了解决这些限制，我们在这里提出了一种新颖的非配对深度学习方法，用物理驱动的 CycleGAN 方法估计药代动力学参数和 AIF。我们提出的 CycleGAN 框架基于自对抗学习，通过学习两个不同分布之间的映射，可以在没有配对数据的情况下对药代动力学参数进行估计。

    DCE-MRI provides information about vascular permeability and tissue perfusion through the acquisition of pharmacokinetic parameters. However, traditional methods for estimating these pharmacokinetic parameters involve fitting tracer kinetic models, which often suffer from computational complexity and low accuracy due to noisy arterial input function (AIF) measurements. Although some deep learning approaches have been proposed to tackle these challenges, most existing methods rely on supervised learning that requires paired input DCE-MRI and labeled pharmacokinetic parameter maps. This dependency on labeled data introduces significant time and resource constraints, as well as potential noise in the labels, making supervised learning methods often impractical. To address these limitations, here we present a novel unpaired deep learning method for estimating both pharmacokinetic parameters and the AIF using a physics-driven CycleGAN approach. Our proposed CycleGAN framework is designed ba
    
[^36]: 传感器个人健康监测系统中的语义技术：一项系统性映射研究

    Semantic Technologies in Sensor-Based Personal Health Monitoring Systems: A Systematic Mapping Study. (arXiv:2306.04335v1 [cs.AI])

    [http://arxiv.org/abs/2306.04335](http://arxiv.org/abs/2306.04335)

    这项研究评估了传感器个人健康监测系统中语义技术的应用现状，发现此类系统必须克服的关键挑战为互操作性、上下文感知、情境检测、情境预测、决策支持和知识表示。

    

    近年来，人们对于疾病的早期检测、预防和预测越来越重视。此外，传感器技术和物联网技术的不断进步也推动了个人健康监测系统的发展。语义技术作为一种有效的方法，不仅可以处理异构健康传感器数据的互操作性问题，还可以表示专家健康知识以支持决策所需的复杂推理。本研究评估了传感器个人健康监测系统中语义技术的应用现状。使用系统方法对40个代表该领域最新技术水平的系统进行了分析。通过这项分析，确定了此类系统必须克服的六个关键挑战：互操作性、上下文感知、情境检测、情境预测、决策支持和知识表示。

    In recent years, there has been an increased focus on early detection, prevention, and prediction of diseases. This, together with advances in sensor technology and the Internet of Things, has led to accelerated efforts in the development of personal health monitoring systems. Semantic technologies have emerged as an effective way to not only deal with the issue of interoperability associated with heterogeneous health sensor data, but also to represent expert health knowledge to support complex reasoning required for decision-making. This study evaluates the state of the art in the use of semantic technologies in sensor-based personal health monitoring systems. Using a systematic approach, a total of 40 systems representing the state of the art in the field are analysed. Through this analysis, six key challenges that such systems must overcome for optimal and effective health monitoring are identified: interoperability, context awareness, situation detection, situation prediction, deci
    
[^37]: GCT-TTE: 基于图卷积变换器的旅行时间估计模型

    GCT-TTE: Graph Convolutional Transformer for Travel Time Estimation. (arXiv:2306.04324v1 [cs.AI])

    [http://arxiv.org/abs/2306.04324](http://arxiv.org/abs/2306.04324)

    本文提出了一种基于变换器和图卷积的旅行时间估计模型，通过利用不同数据模态提取输入路径的不同属性来提高估计精度，并在两个数据集上超过了现有技术模型。

    

    本文介绍了一种新的基于变换器的模型，用于解决旅行时间估计问题。所提出的GCT-TTE架构的关键特征是利用捕捉输入路径不同属性的不同数据模态。除了涉及模型配置的广泛研究外，我们还实现并评估了足够数量的实际基线，用于路径感知和路径盲设置。所进行的计算实验证实了我们的流程的可行性，该流程在两个考虑的数据集上优于现有技术模型。此外，GCT-TTE已部署为Web服务，可用于进一步实验与用户定义路线。

    This paper introduces a new transformer-based model for the problem of travel time estimation. The key feature of the proposed GCT-TTE architecture is the utilization of different data modalities capturing different properties of an input path. Along with the extensive study regarding the model configuration, we implemented and evaluated a sufficient number of actual baselines for path-aware and path-blind settings. The conducted computational experiments have confirmed the viability of our pipeline, which outperformed state-of-the-art models on both considered datasets. Additionally, GCT-TTE was deployed as a web service accessible for further experiments with user-defined routes.
    
[^38]: 生成语义通讯：超越比特恢复的扩散模型

    Generative Semantic Communication: Diffusion Models Beyond Bit Recovery. (arXiv:2306.04321v1 [cs.AI])

    [http://arxiv.org/abs/2306.04321](http://arxiv.org/abs/2306.04321)

    本文提出了一个新的生成扩散引导框架，通过利用扩散模型在合成多媒体内容同时保留语义特征方面的优势，以发送高度压缩的语义信息来降低带宽使用，从而超越现有方法在重建质量和语义保留方面的限制。

    

    语义通讯被认为是下一代基于人工智能的通讯中的核心之一。语义通讯的一个可能性是，在不必恢复传输比特序列的情况下，在目标端重建与传输的图像或视频语义等效的副本。当前的解决方案仍然缺乏从接收到的部分信息中构建复杂场景的能力。本文旨在通过提出一种新的生成扩散引导框架来弥补这一差距，该框架利用了扩散模型在合成多媒体内容同时保留语义特征方面的优势。我们通过仅发送高度压缩的语义信息来降低带宽使用。然后接收端的扩散模型生成高质量的多媒体内容。实验结果表明，我们提出的框架在重建质量和语义保留方面优于现有方法。

    Semantic communication is expected to be one of the cores of next-generation AI-based communications. One of the possibilities offered by semantic communication is the capability to regenerate, at the destination side, images or videos semantically equivalent to the transmitted ones, without necessarily recovering the transmitted sequence of bits. The current solutions still lack the ability to build complex scenes from the received partial information. Clearly, there is an unmet need to balance the effectiveness of generation methods and the complexity of the transmitted information, possibly taking into account the goal of communication. In this paper, we aim to bridge this gap by proposing a novel generative diffusion-guided framework for semantic communication that leverages the strong abilities of diffusion models in synthesizing multimedia content while preserving semantic features. We reduce bandwidth usage by sending highly-compressed semantic information only. Then, the diffus
    
[^39]: GPT-3的人格测试：时间可靠性有限，但凸显了社交渴望的人格工具结果。

    Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3's personality instruments results. (arXiv:2306.04308v1 [cs.AI])

    [http://arxiv.org/abs/2306.04308](http://arxiv.org/abs/2306.04308)

    本研究探讨了GPT-3 Davinci-003聊天机器人的人格特质，发现其具有良好的社交渴望和亲社会特质，但在不同时间的一致性存在限制。

    

    为了评估聊天机器人GPT-3 Davinci-003的潜在应用和限制，本研究探讨了应用于聊天机器人及其个性化资料的人格问卷的时间可靠性。在两个不同的场合，心理问卷被应用于聊天机器人，然后将回答与人类基准数据进行比较。研究结果显示，聊天机器人的回答有不同程度的一致性，有些量表表现出良好的一致性，而有些则表现出较差的一致性。总体而言，Davinci-003显示出一个社交渴望和亲社会的人格特质，尤其是在亲和力领域。然而，聊天机器人回答的基础，无论是由主观自我反思还是预定算法驱动，尚不确定。

    To assess the potential applications and limitations of chatbot GPT-3 Davinci-003, this study explored the temporal reliability of personality questionnaires applied to the chatbot and its personality profile. Psychological questionnaires were administered to the chatbot on two separate occasions, followed by a comparison of the responses to human normative data. The findings revealed varying levels of agreement in the chatbot's responses over time, with some scales displaying excellent while others demonstrated poor agreement. Overall, Davinci-003 displayed a socially desirable and pro-social personality profile, particularly in the domain of communion. However, the underlying basis of the chatbot's responses, whether driven by conscious self-reflection or predetermined algorithms, remains uncertain.
    
[^40]: 亲爱的XAI社区，我们需要谈谈！关于当前XAI研究中存在的基本误解

    Dear XAI Community, We Need to Talk! Fundamental Misconceptions in Current XAI Research. (arXiv:2306.04292v1 [cs.AI])

    [http://arxiv.org/abs/2306.04292](http://arxiv.org/abs/2306.04292)

    当前XAI研究中存在基本误解，例如未明确解释技术的目的，依赖于关于深度学习算法所学“概念”的强烈假设等。我们需要采取措施使XAI成为更实质性的研究领域。

    

    尽管该领域已经取得了进展，但目前XAI研究的重要部分仍未建立在坚实的概念、伦理或方法论基础上。令人遗憾的是，这些基础薄弱的部分并没有减少，而是不断增长。许多解释技术仍然没有澄清其目的，而是用越来越花哨的热点图或看似相关的基准来宣传。此外，解释技术的动机存在问题，例如建立信任，或依赖于关于深度学习算法所学“概念”的强烈假设。本文中，我们突出并讨论了当前XAI研究中的这些和其他误解，同时提出了使XAI成为更实质性研究领域的步骤。

    Despite progress in the field, significant parts of current XAI research are still not on solid conceptual, ethical, or methodological grounds. Unfortunately, these unfounded parts are not on the decline but continue to grow. Many explanation techniques are still proposed without clarifying their purpose. Instead, they are advertised with ever more fancy-looking heatmaps or only seemingly relevant benchmarks. Moreover, explanation techniques are motivated with questionable goals, such as building trust, or rely on strong assumptions about the 'concepts' that deep learning algorithms learn. In this paper, we highlight and discuss these and other misconceptions in current XAI research. We also suggest steps to make XAI a more substantive area of research.
    
[^41]: 黑板架构中添加链接和容器的介绍与评估

    Introduction and Assessment of the Addition of Links and Containers to the Blackboard Architecture. (arXiv:2306.04289v1 [cs.AI])

    [http://arxiv.org/abs/2306.04289](http://arxiv.org/abs/2306.04289)

    本文研究在黑板架构中增加容器和链接的作用，这些对象可以更好地建模组织、物理、空间等关系，使黑板架构能够更好地应用于复杂问题中。

    

    黑板架构提供了一种机制，用于存储数据和逻辑，并将其用于做出影响黑板架构网络建模的应用环境的决策。虽然规则事实动作网络可以表示多种类型的数据，但可以轻松建模的关系受到规则事实网络结构命题逻辑性质的限制。本文提出并评估了在黑板架构中增加容器和链接。这些对象旨在允许它们建模组织、物理、空间和其他关系，这些关系不能作为布尔逻辑规则轻松或有效地实现。容器将相关数据组合在一起，并可嵌套以实现复杂关系。链接相互连接具有相关组织目的的容器。这两种对象共同促进了使用黑板架构的新方式，并使其能够简化应用于复杂问题中的应用。

    The Blackboard Architecture provides a mechanism for storing data and logic and using it to make decisions that impact the application environment that the Blackboard Architecture network models. While rule-fact-action networks can represent numerous types of data, the relationships that can be easily modeled are limited by the propositional logic nature of the rule-fact network structure. This paper proposes and evaluates the inclusion of containers and links in the Blackboard Architecture. These objects are designed to allow them to model organizational, physical, spatial and other relationships that cannot be readily or efficiently implemented as Boolean logic rules. Containers group related facts together and can be nested to implement complex relationships. Links interconnect containers that have a relationship that is relevant to their organizational purpose. Both objects, together, facilitate new ways of using the Blackboard Architecture and enable or simply its use for complex 
    
[^42]: 扩展黑板架构的方法：共同属性和通用规则的支持

    Extension of the Blackboard Architecture with Common Properties and Generic Rules. (arXiv:2306.04287v1 [cs.AI])

    [http://arxiv.org/abs/2306.04287](http://arxiv.org/abs/2306.04287)

    本文介绍了一种扩展黑板架构的方法，增加了对共同属性和通用规则的支持，以实现对组织、空间和其他关系的建模，并促进对逻辑结构的重用。

    

    黑板架构提供了一种将数据、决策和执行体现在一起的机制。现有的工作提出了使用容器对象和链接的机制来建模组织和其他关系，但受到手动定义的限制。本文提出了一种扩展黑板架构的方法，增加了对共同属性和通用规则的支持，以实现对组织、空间和其他关系的建模，并促进对逻辑结构的重用。

    The Blackboard Architecture provides a mechanism for embodying data, decision making and actuation. Its versatility has been demonstrated across a wide number of application areas. However, it lacks the capability to directly model organizational, spatial and other relationships which may be useful in decision-making, in addition to the propositional logic embodied in the rule-fact-action network. Previous work has proposed the use of container objects and links as a mechanism to simultaneously model these organizational and other relationships, while leaving the operational logic modeled in the rules, facts and actions. While containers facilitate this modeling, their utility is limited by the need to manually define them. For systems which may have multiple instances of a particular type of object and which may build their network autonomously, based on sensing, the reuse of logical structures facilitates operations and reduces storage and processing needs. This paper, thus, presents
    
[^43]: 一种无需掩模的单声道语音增强神经网络

    A Mask Free Neural Network for Monaural Speech Enhancement. (arXiv:2306.04286v1 [cs.SD])

    [http://arxiv.org/abs/2306.04286](http://arxiv.org/abs/2306.04286)

    本文提出了一种无需掩模的单声道语音增强神经网络MFNet，该网络不仅可以映射语音，还可以映射反噪声，在强噪声环境下表现出色，并取得了2020年DNS挑战测试集的最佳结果。

    

    在语音增强中，目标语音相位缺乏明显的结构特征，需要使用保守而繁琐的网络框架。使用直接方法和简单的网络架构似乎很难取得竞争性能。然而，我们提出了MFNet，一种直接而简单的网络，不仅可以映射语音，还可以映射反噪声。这个网络是通过堆叠全局本地前端块（GLFBs）构建的，它结合了Mobileblock的全局处理优势和Metaformer架构的本地交互优势。我们的实验结果表明，我们使用映射方法的网络优于掩模方法，并且直接映射反噪声是强噪声环境下的最佳解决方案。在2020年Deep Noise Suppression（DNS）挑战测试集上的横向比较中，据我们所知，MFNet是当前状态-of-the-art（SOTA）的映射模型。

    In speech enhancement, the lack of clear structural characteristics in the target speech phase requires the use of conservative and cumbersome network frameworks. It seems difficult to achieve competitive performance using direct methods and simple network architectures. However, we propose the MFNet, a direct and simple network that can not only map speech but also map reverse noise. This network is constructed by stacking global local former blocks (GLFBs), which combine the advantages of Mobileblock for global processing and Metaformer architecture for local interaction. Our experimental results demonstrate that our network using mapping method outperforms masking methods, and direct mapping of reverse noise is the optimal solution in strong noise environments. In a horizontal comparison on the 2020 Deep Noise Suppression (DNS) challenge test set without reverberation, to the best of our knowledge, MFNet is the current state-of-the-art (SOTA) mapping model.
    
[^44]: 基于去中心化技术的人工智能枢纽

    Decentralized Technologies for AI Hubs. (arXiv:2306.04274v1 [cs.AI])

    [http://arxiv.org/abs/2306.04274](http://arxiv.org/abs/2306.04274)

    本论文探讨了基于去中心化技术的人工智能枢纽的潜力，以解决传统枢纽的一些问题，如高成本、缺乏货币化和奖励、缺乏控制和重现的困难。

    

    人工智能需要大量的存储和计算，这些资源通常存储在人工智能枢纽中。人工智能枢纽在普及人工智能方面做出了重大贡献，然而，现有的实现与基础设施和治理系统相关的某些优点和局限性有关。这些局限性包括高成本、缺乏货币化和奖励、缺乏控制和重现的困难。在本研究中，我们探讨了去中心化技术（如Web3钱包、点对点市场、存储和计算以及DAO）的潜力，以解决这些问题。我们建议这些基础设施组件可以结合使用在去中心化人工智能枢纽的设计和构建中。

    AI requires heavy amounts of storage and compute with assets that are commonly stored in AI Hubs. AI Hubs have contributed significantly to the democratization of AI. However, existing implementations are associated with certain benefits and limitations that stem from the underlying infrastructure and governance systems with which they are built. These limitations include high costs, lack of monetization and reward, lack of control and difficulty of reproducibility. In the current work, we explore the potential of decentralized technologies - such as Web3 wallets, peer-to-peer marketplaces, storage and compute, and DAOs - to address some of these issues. We suggest that these infrastructural components can be used in combination in the design and construction of decentralized AI Hubs.
    
[^45]: 置换等变图框架在异质半监督学习中的应用

    Permutaion Equivariant Graph Framelets for Heterophilous Semi-supervised Learning. (arXiv:2306.04265v1 [cs.LG])

    [http://arxiv.org/abs/2306.04265](http://arxiv.org/abs/2306.04265)

    本文介绍了一个用于异质半监督学习的新型图神经网络模型PEGFAN，它使用置换等变图框架实现了多尺度特征提取，表现优于其他最先进模型，特别是在相对较大和密集连接的数据集中。

    

    异质图的本质与同质图显著不同，这表明1-hop以外的聚合方式并引起早期图神经网络模型的困难。本文展示了一种新的多尺度提取方法，通过构建具有置换等变性，高效性和稀疏性的Haar-type图框架，在图上深度学习任务中实现。我们进一步使用我们构建的图框架设计了图框架神经网络模型PEGFAN。实验在合成数据集和9个基准数据集上进行，与其他最先进的模型进行性能比较。结果表明，我们的模型在某些异质图数据集（包括相对较大和更密集的连接的大部分异质数据集）上可以达到最佳性能，并在其余数据集上具有竞争性能。

    The nature of heterophilous graphs is significantly different with that of homophilous graphs, which suggests aggregations beyond 1-hop neighborhood and causes difficulties in early graph neural network models. In this paper, we develop a new way to implement multi-scale extraction via constructing Haar-type graph framelets with desired properties of permutation equivariance, efficiency, and sparsity, for deep learning tasks on graphs. We further deisgn a graph framelet neural network model PEGFAN using our constructed graph framelets. The experiments are conducted on a synthetic dataset and 9 benchmark datasets to compare performance with other state-of-the-art models. The result shows that our model can achieve best performance on certain datasets of heterophilous graphs (including the majority of heterophilous datasets with relatively larger sizes and denser connections) and competitive performance on the remaining.
    
[^46]: 随机坍缩：如何利用梯度噪声使SGD动态趋向更简单的子网络

    Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks. (arXiv:2306.04251v1 [cs.LG])

    [http://arxiv.org/abs/2306.04251](http://arxiv.org/abs/2306.04251)

    SGD在训练过度表达的网络时，会随机地将动态吸引到更简单的子网络，这种随机吸引性能够提高泛化能力。

    

    本文揭示了随机梯度下降（SGD）的一个强烈隐式偏好，它将过度表达的网络驱动到更简单的子网络，从而大大减少了独立参数的数量，并提高了泛化能力。为了揭示这个偏好，我们识别了不变集，或者说是SGD未修改的参数空间的子集。我们专注于两类不变集，它们对应于现代架构中常见的更简单的子网络。我们的分析揭示了SGD在这些简单不变集方面具有随机吸引性的特性。我们根据损失景观在不变集周围的曲率和随机梯度引入的噪声之间的竞争建立了一种随机吸引性的充分条件。值得注意的是，我们发现增加噪声水平会增强吸引力，导致与鞍点或训练损失的局部极大值相关的吸引不变集的出现。

    In this work, we reveal a strong implicit bias of stochastic gradient descent (SGD) that drives overly expressive networks to much simpler subnetworks, thereby dramatically reducing the number of independent parameters, and improving generalization. To reveal this bias, we identify invariant sets, or subsets of parameter space that remain unmodified by SGD. We focus on two classes of invariant sets that correspond to simpler subnetworks and commonly appear in modern architectures. Our analysis uncovers that SGD exhibits a property of stochastic attractivity towards these simpler invariant sets. We establish a sufficient condition for stochastic attractivity based on a competition between the loss landscape's curvature around the invariant set and the noise introduced by stochastic gradients. Remarkably, we find that an increased level of noise strengthens attractivity, leading to the emergence of attractive invariant sets associated with saddle-points or local maxima of the train loss.
    
[^47]: MobileNMT：在15MB和30ms内实现翻译

    MobileNMT: Enabling Translation in 15MB and 30ms. (arXiv:2306.04235v1 [cs.AI])

    [http://arxiv.org/abs/2306.04235](http://arxiv.org/abs/2306.04235)

    MobileNMT是一个可以在移动设备上实现15MB和30ms翻译的系统，通过一系列模型压缩原则和朋友INT8和解码的引擎的协同设计，它成功解决了NMT模型在移动设备上存储、内存、计算和功耗等方面的挑战，且其速度提升了47.0倍，节省了99.5%的内存，但仅损失了11.6%的BLEU。

    

    在隐私、低延迟和离线场景下，将NMT模型部署在移动设备上是必不可少的。然而，由于NMT模型的容量较大，在设备上运行这些模型面临着存储、内存、计算和功耗等方面的挑战。本文提出了一个名为MobileNMT的系统，它可以在设备上实现15MB和30ms的翻译。我们提出了一系列模型压缩的原则，并与量化相结合。此外，我们设计了一个朋友INT8和解码的引擎。通过模型和引擎的协同设计，与现有系统相比，我们的系统加速了47.0倍，节省了99.5%的内存，仅损失了11.6%的BLEU。代码可在 https://github.com/zjersey/Lightseq-ARM 上公开获取。

    Deploying NMT models on mobile devices is essential for privacy, low latency, and offline scenarios. For high model capacity, NMT models are rather large. Running these models on devices is challenging with limited storage, memory, computation, and power consumption. Existing work either only focuses on a single metric such as FLOPs or general engine which is not good at auto-regressive decoding. In this paper, we present MobileNMT, a system that can translate in 15MB and 30ms on devices. We propose a series of principles for model compression when combined with quantization. Further, we implement an engine that is friendly to INT8 and decoding. With the co-design of model and engine, compared with the existing system, we speed up 47.0x and save 99.5% of memory with only 11.6% loss of BLEU. The code is publicly available at https://github.com/zjersey/Lightseq-ARM.
    
[^48]: 在表面之下寻找：利用基本对称性实现高效离线强化学习

    Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v1 [cs.LG])

    [http://arxiv.org/abs/2306.04220](http://arxiv.org/abs/2306.04220)

    本文提出了一个新的离线强化学习算法TDM，利用系统动力学的基本对称性实现高效学习小数据集。

    

    离线强化学习通过从预先收集的数据集中学习策略来解决与环境交互的实际问题。然而，现有的离线强化学习算法的性能严重依赖于数据集的规模和状态-动作空间覆盖范围。真实世界数据的收集通常是昂贵和难以控制的，导致数据集小且覆盖范围狭窄，从而对离线强化学习的实际部署提出了重大挑战。在本文中，我们提供了一个新的见解，即利用系统动力学的基本对称性可以在小数据集下显著提高离线强化学习的性能。具体来说，我们提出了一个时间反演对称(T-symmetry)强制的动力学模型(TDM)，建立了一对正向和反向潜在动力学之间的一致性。TDM为小数据集提供了良好的表示，并基于T-symmetry的符合性提供了一种新的OOD样本的可靠性度量。

    Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. 
    
[^49]: 面向一阶模型计数的递归函数合成：挑战、进展和猜想

    Synthesising Recursive Functions for First-Order Model Counting: Challenges, Progress, and Conjectures. (arXiv:2306.04189v1 [cs.LO])

    [http://arxiv.org/abs/2306.04189](http://arxiv.org/abs/2306.04189)

    本文提出了一种用于解决一阶模型计数问题的算法，它可以表达许多类型递归计算。改进后的算法可以找到那些之前无法有效解决的计数问题的高效解决方案。

    

    一阶模型计数(FOMC)是一个计算问题，它要求在有限域一阶逻辑中计算出一个句子的模型数量。本文认为，迄今为止，FOMC算法的能力受到了它们表达许多类型递归计算的能力的限制。为了实现这些计算，我们放宽了通常伴随域递归的限制，并将用于表达FOMC问题解的电路通用化为可能包含循环的有向图。为此，我们将最具代表性的FOMC算法ForcLift(加权)适应为能够与这样的图一起使用，并引入了新的编译规则，可以创建编码递归函数调用的循环引导边缘。这些改进使算法能够找到对以前无法到达的计数问题的高效解决方案，包括那些不能被任何其他精确FOMC算法有效解决的问题。最后，我们给出了一些关于哪些情况类递归运算可以支持的猜想。

    First-order model counting (FOMC) is a computational problem that asks to count the models of a sentence in finite-domain first-order logic. In this paper, we argue that the capabilities of FOMC algorithms to date are limited by their inability to express many types of recursive computations. To enable such computations, we relax the restrictions that typically accompany domain recursion and generalise the circuits used to express a solution to an FOMC problem to directed graphs that may contain cycles. To this end, we adapt the most well-established (weighted) FOMC algorithm ForcLift to work with such graphs and introduce new compilation rules that can create cycle-inducing edges that encode recursive function calls. These improvements allow the algorithm to find efficient solutions to counting problems that were previously beyond its reach, including those that cannot be solved efficiently by any other exact FOMC algorithm. We end with a few conjectures on what classes of instances c
    
[^50]: 何时查阅文档或QA历史记录：统一和有选择的开放领域QA问题研究

    When to Read Documents or QA History: On Unified and Selective Open-domain QA. (arXiv:2306.04176v1 [cs.CL])

    [http://arxiv.org/abs/2306.04176](http://arxiv.org/abs/2306.04176)

    提出了一种方法来决定在何时使用文档或QA-pair回答问题，并在多个基准测试中验证了其有效性。

    

    本文研究了开放领域问答的问题，致力于利用知识资源回答各种各样的问题。文档语料库和QA-pair是两种常用的信息源，前者在处理已知问题时非常准确，而后者在处理未知问题时更具广泛性。我们提出了一种对预测答案置信度进行分析的方案，以决定何时使用文档或QA-pair。该方法在Natural Questions和TriviaQA等广泛采用的基准测试中被证明是有效的。

    This paper studies the problem of open-domain question answering, with the aim of answering a diverse range of questions leveraging knowledge resources. Two types of sources, QA-pair and document corpora, have been actively leveraged with the following complementary strength. The former is highly precise when the paraphrase of given question $q$ was seen and answered during training, often posed as a retrieval problem, while the latter generalizes better for unseen questions. A natural follow-up is thus leveraging both models, while a naive pipelining or integration approaches have failed to bring additional gains over either model alone. Our distinction is interpreting the problem as calibration, which estimates the confidence of predicted answers as an indicator to decide when to use a document or QA-pair corpus. The effectiveness of our method was validated on widely adopted benchmarks such as Natural Questions and TriviaQA.
    
[^51]: 提升虚拟助手智能：针对元数据以外的实例级用户意图精准区域定位

    Enhancing Virtual Assistant Intelligence: Precise Area Targeting for Instance-level User Intents beyond Metadata. (arXiv:2306.04163v1 [cs.HC])

    [http://arxiv.org/abs/2306.04163](http://arxiv.org/abs/2306.04163)

    本文提出了一种能够处理应用程序屏幕像素级别虚拟助手，可以理解实例级用户意图并预测其目标操作区域，不需要应用程序元数据扩展。

    

    近年来，虚拟助手已广泛应用于移动电话用户中。虚拟助手在处理用户意图方面的能力已经迅速发展，但是，在大多数平台上，虚拟助手只能处理由开发人员额外手动努力支持的预定义高级任务。然而，包含更详细目标和复杂实际情况的实例级用户意图目前却鲜有研究。本文旨在探索能够基于应用程序屏幕像素处理实例级用户意图的虚拟助手，而不需要在应用程序端进行额外扩展。我们提出了一种新的跨模态深度学习方法，该方法可以理解语音或文本实例级用户意图，预测目标操作区域，并在没有应用程序元数据的情况下检测屏幕上的绝对按钮区域。我们进行了一项用户研究，收集了一个测试数据集，该数据集包含了10个参与者的实例级用户意图。

    Virtual assistants have been widely used by mobile phone users in recent years. Although their capabilities of processing user intents have been developed rapidly, virtual assistants in most platforms are only capable of handling pre-defined high-level tasks supported by extra manual efforts of developers. However, instance-level user intents containing more detailed objectives with complex practical situations, are yet rarely studied so far. In this paper, we explore virtual assistants capable of processing instance-level user intents based on pixels of application screens, without the requirements of extra extensions on the application side. We propose a novel cross-modal deep learning pipeline, which understands the input vocal or textual instance-level user intents, predicts the targeting operational area, and detects the absolute button area on screens without any metadata of applications. We conducted a user study with 10 participants to collect a testing dataset with instance-le
    
[^52]: 一种统一的解决方案用于方面情感四元预测

    A Unified One-Step Solution for Aspect Sentiment Quad Prediction. (arXiv:2306.04152v1 [cs.AI])

    [http://arxiv.org/abs/2306.04152](http://arxiv.org/abs/2306.04152)

    本文提出了一个统一的解决方案 One-ASQP，以检测方面类别并同时识别方面-观点-情感（AOS）三元组，通过引入多任务学习和分层注意机制，提高了预测准确性并具有高效性。

    

    方面情感四元预测（ASQP）是方面情感分析中具有挑战性但极具意义的子任务，因为它提供了完整的方面级情感结构。然而，现有的ASQP数据集通常很小且密度较低，阻碍了技术的进步。为了扩展能力，本文提出了两个新的ASQP数据集，具有以下特征：更大的大小，更多的单词数量和更高的密度。通过这样的数据集，揭示了现有强ASQP基线的缺陷，因此提出了一种统一的ASQP一步解决方案，即One-ASQP，以同时检测方面类别并识别方面-观点-情感（AOS）三元组。 One-ASQP具有以下几点独特的优势：（1）通过将ASQP分成两个子任务并独立同时解决它们，我们可以避免基于管道的方法中的错误传播，并克服生成性方法中的慢速训练和推理;（2）通过引入多任务学习和分层注意机制，我们可以提高方面类别和AOS三元组的预测准确性。基于基准数据集的实验结果证明了One-ASQP的有效性和效率。

    Aspect sentiment quad prediction (ASQP) is a challenging yet significant subtask in aspect-based sentiment analysis as it provides a complete aspect-level sentiment structure. However, existing ASQP datasets are usually small and low-density, hindering technical advancement. To expand the capacity, in this paper, we release two new datasets for ASQP, which contain the following characteristics: larger size, more words per sample, and higher density. With such datasets, we unveil the shortcomings of existing strong ASQP baselines and therefore propose a unified one-step solution for ASQP, namely One-ASQP, to detect the aspect categories and to identify the aspect-opinion-sentiment (AOS) triplets simultaneously. Our One-ASQP holds several unique advantages: (1) by separating ASQP into two subtasks and solving them independently and simultaneously, we can avoid error propagation in pipeline-based methods and overcome slow training and inference in generation-based methods; (2) by introduc
    
[^53]: 生成AI的艺术与科学：更深入的探索

    Art and the science of generative AI: A deeper dive. (arXiv:2306.04141v1 [cs.AI])

    [http://arxiv.org/abs/2306.04141](http://arxiv.org/abs/2306.04141)

    生成AI是一种新的艺术媒介，具有改变创造过程和社会构想的潜力，可以从美学、法律、创作未来和媒体生态等方面影响创作者和社会。

    

    一类新工具被俗称为生成AI，可以为视觉艺术、概念艺术、音乐、小说、文学、视频和动画创造高质量的艺术媒介。这些工具的生成能力可能会从根本上改变创造者制定想法并将其投入生产的创造过程。随着创造力的重新构想，社会的许多领域也可能被重新构想。理解生成AI的影响并制定政策决策 - 需要新的跨学科的科学探究文化、经济、法律、算法以及技术和创造力的交互。我们认为，生成AI并不是艺术灭亡的先兆，而是一种具有自己独特优势的新媒介。在这方面，我们考虑了这种新媒介对四个主题中的创作者的影响：美学和文化，所有权和信用的法律问题，创造性工作的未来以及对当代媒体生态系统的影响。在这些主题中，我们强调了生成AI中艺术和科学的交叉对创造力和社会的转化潜力。

    A new class of tools, colloquially called generative AI, can produce high-quality artistic media for visual arts, concept art, music, fiction, literature, video, and animation. The generative capabilities of these tools are likely to fundamentally alter the creative processes by which creators formulate ideas and put them into production. As creativity is reimagined, so too may be many sectors of society. Understanding the impact of generative AI and making policy decisions around it - requires new interdisciplinary scientific inquiry into culture, economics, law, algorithms, and the interaction of technology and creativity. We argue that generative AI is not the harbinger of art's demise, but rather is a new medium with its own distinct affordances. In this vein, we consider the impacts of this new medium on creators across four themes: aesthetics and culture, legal questions of ownership and credit, the future of creative work, and impacts on the contemporary media ecosystem. Acros
    
[^54]: 结构化数据生成扩散模型综述

    A Survey on Generative Diffusion Models for Structured Data. (arXiv:2306.04139v1 [cs.LG])

    [http://arxiv.org/abs/2306.04139](http://arxiv.org/abs/2306.04139)

    本文全面综述了在结构化数据领域中最近提出的扩散模型，介绍了其理论基础和应用场景。

    

    最近，生成扩散模型（generative diffusion models）在深度生成模型领域取得了突破性成果，展现了在许多应用中的出色表现。与此同时，结构化数据（包括表格和时间序列数据）在深度学习研究界中受到的关注相对较少，尽管其无处不在且应用广泛。因此，与计算机视觉和自然语言处理等其他数据形式相比，利用扩散模型对结构化数据建模的文献及其综述仍然缺乏。因此，本文介绍了在结构化数据领域中最近提出的扩散模型的全面综述。首先，本综述提供了基于得分的扩散模型理论的简要概述，随后又详细描述了在数据驱动的通用任务和特定领域应用中使用结构化数据的大部分开创性工作的技术描述。最后，

    In recent years, generative diffusion models have achieved a rapid paradigm shift in deep generative models by showing groundbreaking performance across various applications. Meanwhile, structured data, encompassing tabular and time series data, has been received comparatively limited attention from the deep learning research community, despite its omnipresence and extensive applications. Thus, there is still a lack of literature and its review on structured data modelling via diffusion models, compared to other data modalities such as computer vision and natural language processing. Hence, in this paper, we present a comprehensive review of recently proposed diffusion models in the field of structured data. First, this survey provides a concise overview of the score-based diffusion model theory, subsequently proceeding to the technical descriptions of the majority of pioneering works using structured data in both data-driven general tasks and domain-specific applications. Thereafter, 
    
[^55]: 基于本地模板检索的逆向合成预测

    Retrosynthesis Prediction with Local Template Retrieval. (arXiv:2306.04123v1 [cs.AI])

    [http://arxiv.org/abs/2306.04123](http://arxiv.org/abs/2306.04123)

    本文介绍了一种新方法，RetroKNN，它使用基于本地反应模板的k最近邻检索结合神经网络预测来提高逆向合成模型的性能。

    

    逆向合成是药物探索中预测给定目标分子反应物的重要任务。近年来，基于机器学习的逆向合成方法取得了良好的结果。本文介绍了RetroKNN，一种基于本地反应模板检索的方法，通过非参数检索进一步提高基于模板的系统性能。我们首先构建了一个原子模板存储库和一个键模板存储库，其中包含训练数据中的本地模板，然后在推理过程中使用k最近邻（KNN）搜索从这些模板中检索。检索到的模板与神经网络预测相结合作为最终输出。此外，我们还提出了一种轻量级适配器，以调整基于隐藏表示和检索模板的神经网络和KNN预测的权重。我们在两个广泛使用的基准测试USPTO-50K和USPTO-MIT上进行了全面的实验。尤其是在top-1精度方面。

    Retrosynthesis, which predicts the reactants of a given target molecule, is an essential task for drug discovery. In recent years, the machine learing based retrosynthesis methods have achieved promising results. In this work, we introduce RetroKNN, a local reaction template retrieval method to further boost the performance of template-based systems with non-parametric retrieval. We first build an atom-template store and a bond-template store that contain the local templates in the training data, then retrieve from these templates with a k-nearest-neighbor (KNN) search during inference. The retrieved templates are combined with neural network predictions as the final output. Furthermore, we propose a lightweight adapter to adjust the weights when combing neural network and KNN predictions conditioned on the hidden representation and the retrieved templates. We conduct comprehensive experiments on two widely used benchmarks, the USPTO-50K and USPTO-MIT. Especially for the top-1 accuracy
    
[^56]: MESSY估计：基于最大熵的随机和符号密度估计

    MESSY Estimation: Maximum-Entropy based Stochastic and Symbolic densitY Estimation. (arXiv:2306.04120v1 [cs.LG])

    [http://arxiv.org/abs/2306.04120](http://arxiv.org/abs/2306.04120)

    MESSY估计方法是一种基于最大熵的随机和符号密度估计方法，通过构建基于梯度的漂移扩散过程来高效地找到最大熵分布的参数，支持高维问题，并具有优于现有最新方法的有效性和普适性。

    

    我们引入了基于最大熵的随机和符号密度估计方法MESSY。所提出的方法使用梯度流的矩将概率密度函数从样本中恢复为符号表达式，并将ansatz作为驱动力。特别地，我们构建了一个基于梯度的漂移扩散过程，将未知分布函数的样本与猜测的符号表达式相连。然后，我们展示出当猜测分布具有最大熵形式时，可以通过使用提供的样本的矩构建的线性方程组高效地找到该分布的参数。此外，我们使用符号回归来探索平滑函数的空间，并找到导致最大熵泛函指数的最优基函数，以获得良好条件。该方法在随机搜索的每次迭代中的成本与样本数量呈线性关系，与变量数量呈二次关系，使其可扩展到高维问题。数值实验显示出所提出方法的有效性和普适性，与现有的最新方法相比。

    We introduce MESSY estimation, a Maximum-Entropy based Stochastic and Symbolic densitY estimation method. The proposed approach recovers probability density functions symbolically from samples using moments of a Gradient flow in which the ansatz serves as the driving force. In particular, we construct a gradient-based drift-diffusion process that connects samples of the unknown distribution function to a guess symbolic expression. We then show that when the guess distribution has the maximum entropy form, the parameters of this distribution can be found efficiently by solving a linear system of equations constructed using the moments of the provided samples. Furthermore, we use Symbolic regression to explore the space of smooth functions and find optimal basis functions for the exponent of the maximum entropy functional leading to good conditioning. The cost of the proposed method in each iteration of the random search is linear with the number of samples and quadratic with the number 
    
[^57]: M$^3$Fair：多层次多敏感属性加权方法减轻医疗数据中的偏见

    M$^3$Fair: Mitigating Bias in Healthcare Data through Multi-Level and Multi-Sensitive-Attribute Reweighting Method. (arXiv:2306.04118v1 [cs.LG])

    [http://arxiv.org/abs/2306.04118](http://arxiv.org/abs/2306.04118)

    本文提出了一种名为M$^3$Fair的新方法，可以在多个数据层面和多个敏感属性的情况下平衡训练数据分布，缓解医疗数据中的偏见和不公平现象，提高机器学习模型的公平性能和准确性能。

    

    在数据驱动的人工智能范式中，模型严重依赖于大量的训练数据。然而，样本分布失衡等因素可能导致医疗数据中的偏见和不公平问题。在医疗人工智能中，如种族、性别、年龄和医疗状况等敏感属性通常与歧视或偏见有关。这些属性在医疗保健中可能对个人获得的护理质量产生重大影响。因此，检测和减轻数据中的偏差对于提高健康公平至关重要。偏差缓解方法包括前处理、中处理和后处理。其中，加权（RW）是一种广泛使用的前处理方法，能够在平衡机器学习性能和公平性能方面表现良好。RW调整特定敏感属性的样本权重以解决偏差问题。然而，现有的RW方法在平衡多个敏感属性和处理多层次数据方面效果有限。本文提出了 M$^3$Fair，一种新颖的多层次多敏感属性加权方法，可以平衡不同数据层次上多个敏感属性的分布。M$^3$Fair在维持机器学习性能的同时有效平衡了不同敏感属性的公平性能。在基准医疗数据集上的实验表明，M$^3$Fair在公平性和准确性方面优于现有方法。

    In the data-driven artificial intelligence paradigm, models heavily rely on large amounts of training data. However, factors like sampling distribution imbalance can lead to issues of bias and unfairness in healthcare data. Sensitive attributes, such as race, gender, age, and medical condition, are characteristics of individuals that are commonly associated with discrimination or bias. In healthcare AI, these attributes can play a significant role in determining the quality of care that individuals receive. For example, minority groups often receive fewer procedures and poorer-quality medical care than white individuals in US. Therefore, detecting and mitigating bias in data is crucial to enhancing health equity. Bias mitigation methods include pre-processing, in-processing, and post-processing. Among them, Reweighting (RW) is a widely used pre-processing method that performs well in balancing machine learning performance and fairness performance. RW adjusts the weights for samples wit
    
[^58]: BeMap：平衡的消息传递方法用于公平的图神经网络。

    BeMap: Balanced Message Passing for Fair Graph Neural Network. (arXiv:2306.04107v1 [cs.LG])

    [http://arxiv.org/abs/2306.04107](http://arxiv.org/abs/2306.04107)

    本文提出了一种公平的消息传递方法，称为BeMap，旨在解决消息传递中的偏差放大问题，通过平衡感知的采样策略来平衡不同人口群体的1-hop邻居的数量。

    

    图神经网络（GNN）通过迭代地聚合每个节点的局部邻域信息来表现出强大的实证性能，即消息传递。然而，具体证据显示，图神经网络可能对某些人口群体存在偏见，这要求考虑算法的公正性。尽管越来越多的努力在保证图神经网络的算法公平性，但在训练期间往往并不明确考虑消息传递在GNN中引起的偏差。本文首先研究了消息传递中的偏差放大问题。我们通过经验证据和理论证明，当来自不同人口群体的1-hop邻居不平衡时，消息传递可能会放大偏差。在这些分析的指导下，我们提出了BeMap，一种公平的消息传递方法，利用平衡感知的采样策略来平衡每个节点的1-hop邻居的数量。

    Graph Neural Network (GNN) has shown strong empirical performance in many downstream tasks by iteratively aggregating information from the local neighborhood of each node, i.e., message passing. However, concrete evidence has revealed that a graph neural network could be biased against certain demographic groups, which calls for the consideration of algorithmic fairness. Despite the increasing efforts in ensuring algorithmic fairness on graph neural networks, they often do not explicitly consider the induced bias caused by message passing in GNN during training. In this paper, we first investigate the problem of bias amplification in message passing. We empirically and theoretically demonstrate that message passing could amplify the bias when the 1-hop neighbors from different demographic groups are unbalanced. Guided by such analyses, we propose BeMap, a fair message passing method, that leverages a balance-aware sampling strategy to balance the number of the 1-hop neighbors of each n
    
[^59]: Gotta: 基于提示式填空数据增强的生成式少样本问题回答

    Gotta: Generative Few-shot Question Answering by Prompt-based Cloze Data Augmentation. (arXiv:2306.04101v1 [cs.CL])

    [http://arxiv.org/abs/2306.04101](http://arxiv.org/abs/2306.04101)

    Gotta是一个基于填空数据增强框架，通过将提示式填空任务与问题回答任务相结合，提高了少样本问题回答的学习能力。

    

    少样本问题回答旨在仅使用少量训练样本的情况下，从上下文段落中精确地找到一组问题的答案。虽然现有的研究已经取得了一定的进展并且通常能够取得良好的结果，但是它们在理解深层语义的能力方面仍然存在困难。本文提出了Gotta，一个基于生成式提示式数据增强框架，以缓解上述挑战。受到人类推理过程的启发，我们提出了将提示式填空任务与问题回答任务相结合来增强少样本问题回答的学习能力。在最近提示调整的成功之后，我们以与主要的问题回答任务相同的格式提出了填空任务，使模型能够无缝地学习两个任务，充分利用提示调整的能力。在广泛使用的基准测试中进行的大量实验表明，Gotta始终优于竞争基线，验证了我们提出的基于提示调整的填空任务的有效性，它不仅提高了性能还增强了学习能力。

    Few-shot question answering (QA) aims at precisely discovering answers to a set of questions from context passages while only a few training samples are available. Although existing studies have made some progress and can usually achieve proper results, they suffer from understanding deep semantics for reasoning out the questions. In this paper, we develop Gotta, a Generative prOmpT-based daTa Augmentation framework to mitigate the challenge above. Inspired by the human reasoning process, we propose to integrate the cloze task to enhance few-shot QA learning. Following the recent success of prompt-tuning, we present the cloze task in the same format as the main QA task, allowing the model to learn both tasks seamlessly together to fully take advantage of the power of prompt-tuning. Extensive experiments on widely used benchmarks demonstrate that Gotta consistently outperforms competitive baselines, validating the effectiveness of our proposed prompt-tuning-based cloze task, which not o
    
[^60]: 部署一个强健的积极偏好调查算法：COVID-19患者优先排序的实验设计、界面和评估

    Deploying a Robust Active Preference Elicitation Algorithm: Experiment Design, Interface, and Evaluation for COVID-19 Patient Prioritization. (arXiv:2306.04061v1 [cs.HC])

    [http://arxiv.org/abs/2306.04061](http://arxiv.org/abs/2306.04061)

    本论文介绍了一种强健的偏好调查算法，旨在优先为COVID-19患者提供医院资源，通过招募亚马逊机械土耳其工作者对该算法进行评估。

    

    偏好调查利用AI或优化来学习从市场到公共政策等各种情况下的利益相关者的偏好。arXiv:2003.01899的在线健壮偏好调查流程在模拟中表现出优于其他偏好调查流程的效果，可以有效地学习个体的真实效用。然而，与任何模拟一样，该方法做出了一系列无法轻易验证是否在模拟以外成立的假设。因此，我们建议验证这种健壮方法在部署中的表现，重点关注选择COVID-19患者的政策，以优先为疫情期间的医院资源提供服务。为此，我们开发了一个在线偏好调查平台，用户通过特定的偏好调查过程报告他们在少量成对比较中的偏好。我们招募了193位Amazon Mechanical Turk工作者来报告他们的偏好。

    Preference elicitation leverages AI or optimization to learn stakeholder preferences in settings ranging from marketing to public policy. The online robust preference elicitation procedure of arXiv:2003.01899 has been shown in simulation to outperform various other elicitation procedures in terms of effectively learning individuals' true utilities. However, as with any simulation, the method makes a series of assumptions that cannot easily be verified to hold true beyond simulation. Thus, we propose to validate the robust method's performance in deployment, focused on the particular challenge of selecting policies for prioritizing COVID-19 patients for scarce hospital resources during the pandemic. To this end, we develop an online platform for preference elicitation where users report their preferences between alternatives over a moderate number of pairwise comparisons chosen by a particular elicitation procedure. We recruit Amazon Mechanical Turk workers ($n$ = 193) to report their p
    
[^61]: 用于改进视听融合导航的主动稀疏对话

    Active Sparse Conversations for Improved Audio-Visual Embodied Navigation. (arXiv:2306.04047v1 [cs.CV])

    [http://arxiv.org/abs/2306.04047](http://arxiv.org/abs/2306.04047)

    本文提出了CAVEN - 一种具有对话功能的音频视觉导航代理，能够向人类/神谕提出导航问题并处理神谕回答以协助自主导航。该系统基于多模态分层强化学习方法，并使用三个低级策略进行引导。

    

    为了高效地导航到一个听觉目标，一个具有固定自主权的实体必须不仅要有能力有效地使用视听线索, 而且还要有能力在不牺牲自主性的情况下主动寻求人类/神谕的帮助，例如，当不确定导航到哪里寻找嘈杂或间歇性听觉目标时。因此，我们提出了CAVEN-一种具有对话功能的音频视觉导航代理，能够向人类/神谕提出导航问题并处理神谕的自由形式自然语言回答。在CAVEN的核心是一个多模态分层强化学习(RL)设置，它配备了一个高级策略，该策略经过训练，可以在每一步从三个低级策略中选择一个，即：(i)使用视听线索进行导航，或(ii)向神谕提出问题并接收短或详细的回答，或(iii)提问普遍问题(当不确定该问什么时)并获得指导

    Efficient navigation towards an audio-goal necessitates an embodied agent to not only possess the ability to use audio-visual cues effectively, but also be equipped to actively (but occasionally) seek human/oracle assistance without sacrificing autonomy, e.g., when it is uncertain of where to navigate towards locating a noisy or sporadic audio goal. To this end, we present CAVEN -- a conversational audio-visual embodied navigation agent that is capable of posing navigation questions to a human/oracle and processing the oracle responses; both in free-form natural language. At the core of CAVEN is a multimodal hierarchical reinforcement learning (RL) setup that is equipped with a high-level policy that is trained to choose from one of three low-level policies (at every step), namely: (i) to navigate using audio-visual cues, or (ii) to frame a question to the oracle and receive a short or detailed response, or (iii) ask generic questions (when unsure of what to ask) and receive instructio
    
[^62]: FedVal：联邦学习中的不同好坏

    FedVal: Different good or different bad in federated learning. (arXiv:2306.04040v1 [cs.LG])

    [http://arxiv.org/abs/2306.04040](http://arxiv.org/abs/2306.04040)

    本文提出了FedVal方法，它是一个不需要从客户端获取任何附加信息的全新方法，可同时具有稳健和公平性，并通过评分函数在服务器端验证客户端更新，以确定本地训练模型之间的最佳聚合平衡。

    

    联邦学习系统容易受到恶意攻击的影响，攻击者可能会通过各种毒化攻击来破坏训练模型。此外，FL在解决团体偏见方面也面临新的挑战，例如确保不同人口群体的公平性能。传统方法需要对数据进行集中处理，而FL系统并没有这个功能。本文提出了一个名为FedVal的全新方法，既具有稳健性又具有公平性，其不需要从客户端获取任何可能引发隐私问题并危及FL系统完整性的附加信息。为此，我们提出了一个基于服务器端验证方法的创新评分函数，该方法评估客户端更新并确定本地训练模型之间的最佳聚合平衡。我们的研究表明，这种方法不仅能够有效保护模型免受毒化攻击，而且还可用于减少群体偏见和随后的问题。

    Federated learning (FL) systems are susceptible to attacks from malicious actors who might attempt to corrupt the training model through various poisoning attacks. FL also poses new challenges in addressing group bias, such as ensuring fair performance for different demographic groups. Traditional methods used to address such biases require centralized access to the data, which FL systems do not have. In this paper, we present a novel approach FedVal for both robustness and fairness that does not require any additional information from clients that could raise privacy concerns and consequently compromise the integrity of the FL system. To this end, we propose an innovative score function based on a server-side validation method that assesses client updates and determines the optimal aggregation balance between locally-trained models. Our research shows that this approach not only provides solid protection against poisoning attacks but can also be used to reduce group bias and subsequen
    
[^63]: 遥感图像分类的可解释人工智能方法的主要贡献的定量分析

    Quantitative Analysis of Primary Attribution Explainable Artificial Intelligence Methods for Remote Sensing Image Classification. (arXiv:2306.04037v1 [cs.LG])

    [http://arxiv.org/abs/2306.04037](http://arxiv.org/abs/2306.04037)

    该论文定量分析了用于遥感图像分类的可解释人工智能技术，探究了不同属性的XAI方法，提供选取合适方法以深入了解模型决策的见解和建议。

    

    我们提出了一种综合分析定量评估可解释人工智能（XAI）技术用于遥感图像分类的方法。我们的方法利用最先进的机器学习方法在多种模态下执行遥感图像分类。我们通过XAI方法定性地研究了模型的结果。此外，我们通过所需属性的各种类别来定量比较XAI方法。通过我们的分析，我们提供了选择最合适的XAI方法以加深对模型决策过程理解的见解和建议。此工作的代码是公开可用的。

    We present a comprehensive analysis of quantitatively evaluating explainable artificial intelligence (XAI) techniques for remote sensing image classification. Our approach leverages state-of-the-art machine learning approaches to perform remote sensing image classification across multiple modalities. We investigate the results of the models qualitatively through XAI methods. Additionally, we compare the XAI methods quantitatively through various categories of desired properties. Through our analysis, we offer insights and recommendations for selecting the most appropriate XAI method(s) to gain a deeper understanding of the models' decision-making processes. The code for this work is publicly available.
    
[^64]: 带有“指南”的语言模型的合规推理。

    Certified Reasoning with Language Models. (arXiv:2306.04031v1 [cs.AI])

    [http://arxiv.org/abs/2306.04031](http://arxiv.org/abs/2306.04031)

    该论文提出了一种称为“指南”的语言模型工具类，它使用状态和增量约束来指导生成，可以显著提高语言模型的逻辑推理精度。

    

    在复杂任务中，语言模型往往通过逐步推理实现更高的精度。然而，它们的推理可以是不完备的、不一致的或者依赖于不良的优先假设。为了解决这些问题，我们引入了一种称为“指南”的语言模型工具类，利用状态和递增约束来指导生成。模型可以调用指南，将其自己的生成限制在一组工具给出的有效陈述之内。反过来，模型的选择可以改变指南的状态。我们展示了一个通用的系统来进行逻辑推理，可以被用作指南，我们称之为LogicGuide。给定自然语言的推理问题，模型可以为LogicGuide形式化它的假设，从而保证其推理步骤是完备的。在PrOntoQA和ProofWriter推理数据集的实验中，LogicGuide显著提高了GPT-3、GPT-3.5 Turbo和LLaMA的性能（精度提高了35%）。LogicGuide也大大减少了内容效果的波动。

    Language models often achieve higher accuracy when reasoning step-by-step in complex tasks. However, their reasoning can be unsound, inconsistent, or rely on undesirable prior assumptions. To tackle these issues, we introduce a class of tools for language models called guides that use state and incremental constraints to guide generation. A guide can be invoked by the model to constrain its own generation to a set of valid statements given by the tool. In turn, the model's choices can change the guide's state. We show how a general system for logical reasoning can be used as a guide, which we call LogicGuide. Given a reasoning problem in natural language, a model can formalize its assumptions for LogicGuide and then guarantee that its reasoning steps are sound. In experiments with the PrOntoQA and ProofWriter reasoning datasets, LogicGuide significantly improves the performance of GPT-3, GPT-3.5 Turbo and LLaMA (accuracy gains up to 35%). LogicGuide also drastically reduces content eff
    
[^65]: 因子图模型视角下的干预泛化

    Intervention Generalization: A View from Factor Graph Models. (arXiv:2306.04027v1 [stat.ML])

    [http://arxiv.org/abs/2306.04027](http://arxiv.org/abs/2306.04027)

    本文提出了一种基于因子图模型的“干预因子模型”(IFM)方法，仅基于对操纵系统分布的因子分解的最小假设，以实现从过去的实验到新的条件的跃迁。

    

    因果推断的一个目标是从过去的实验和观察数据推广到新的条件。在训练数据中提供足够多的实验的情况下，理论上可能最终学习从新的实验条件到感兴趣的结果的映射，但是处理大量可能的干预组合空间很困难。在典型的稀疏实验设计下，如果不依赖于重的规则化或先验分布，这种映射是不适当的。这样的假设可能是可靠的，也可能是不可靠的，很难辩护或测试。本文从因子图模型的语言角度深入探讨如何保证从过去的实验到新的条件的跃迁，仅基于对操纵系统分布的因子分解的最小假设。假设的“干预因子模型”可能并不总是有用的，但是它很方便地处理了大量可能的干预空间。

    One of the goals of causal inference is to generalize from past experiments and observational data to novel conditions. While it is in principle possible to eventually learn a mapping from a novel experimental condition to an outcome of interest, provided a sufficient variety of experiments is available in the training data, coping with a large combinatorial space of possible interventions is hard. Under a typical sparse experimental design, this mapping is ill-posed without relying on heavy regularization or prior distributions. Such assumptions may or may not be reliable, and can be hard to defend or test. In this paper, we take a close look at how to warrant a leap from past experiments to novel conditions based on minimal assumptions about the factorization of the distribution of the manipulated system, communicated in the well-understood language of factor graph models. A postulated $\textit{interventional factor model}$ (IFM) may not always be informative, but it conveniently abs
    
[^66]: 价值函数即控制障碍函数：使用控制理论验证学习策略

    Your Value Function is a Control Barrier Function: Verification of Learned Policies using Control Theory. (arXiv:2306.04026v1 [cs.LG])

    [http://arxiv.org/abs/2306.04026](http://arxiv.org/abs/2306.04026)

    本研究将控制理论中的验证方法应用于强化学习中的价值函数，提出了新的度量方法以验证安全控制任务中的价值函数，并代表了通用、可伸缩和可验证的控制系统设计框架的第一步。

    

    尽管强化学习具有高度的通用性和可伸缩性，但验证策略行为的难度对于安全关键应用程序构成了挑战。为了解决这个问题，我们建议将控制理论中使用的验证方法应用于学习的价值函数。通过分析安全维护的简单任务结构，我们推导出将值函数与控制障碍函数相联系的原始定理。受此启发，我们提出了新的度量方法，以验证安全控制任务中的价值函数，并提出了改善学习的实际实施细节。除了提出证书学习的新方法外，我们的工作为RL策略解锁了丰富的控制理论验证方法，并代表了通用、可伸缩和可验证的控制系统设计框架的第一步。

    Although RL is highly general and scalable, the difficulty of verifying policy behaviours poses challenges for safety-critical applications. To remedy this, we propose to apply verification methods used in control theory to learned value functions. By analyzing a simple task structure for safety preservation, we derive original theorems linking value functions to control barrier functions. Inspired by this, we propose novel metrics for verification of value functions in safe control tasks, and practical implementation details that improve learning. Besides proposing a novel method for certificate learning, our work unlocks a wealth of verification methods in control theory for RL policies, and represents a first step towards a framework for general, scalable, and verifiable design of control systems.
    
[^67]: 使用主动推理设计可解释人工智能：透明内省与决策制定的框架

    Designing explainable artificial intelligence with active inference: A framework for transparent introspection and decision-making. (arXiv:2306.04025v1 [cs.AI])

    [http://arxiv.org/abs/2306.04025](http://arxiv.org/abs/2306.04025)

    本论文提出了一种使用主动推理的可解释人工智能系统的架构，可以追踪决策的因素并生成易于理解的模型，从而实现透明化内省和决策制定，提供了一种新的可解释人工智能的思路。

    

    本文研究了基于主动推理和自由能原理发展人可解读的、可解释人工智能系统的前景。我们首先提供了有关主动推理的简要概述，特别是如何应用于决策制定、内省以及产生公开和隐蔽操作的建模。然后我们讨论了如何利用主动推理来设计可解释人工智能系统，即通过允许我们对“内省”过程的核心功能进行建模并生成有用的、可解读的决策制定过程的人可理解模型。我们提出了一个使用主动推理的可解释人工智能系统架构。该架构强调了显式分层生成模型的作用，其操作可以使人工智能系统跟踪和解释有助于其决策的因素，并且其结构设计成可以被人类用户解读和审查。

    This paper investigates the prospect of developing human-interpretable, explainable artificial intelligence (AI) systems based on active inference and the free energy principle. We first provide a brief overview of active inference, and in particular, of how it applies to the modeling of decision-making, introspection, as well as the generation of overt and covert actions. We then discuss how active inference can be leveraged to design explainable AI systems, namely, by allowing us to model core features of ``introspective'' processes and by generating useful, human-interpretable models of the processes involved in decision-making. We propose an architecture for explainable AI systems using active inference. This architecture foregrounds the role of an explicit hierarchical generative model, the operation of which enables the AI system to track and explain the factors that contribute to its own decisions, and whose structure is designed to be interpretable and auditable by human users.
    
[^68]: 利用能量模型进行跨模态定位的卷积变换方法

    Energy-Based Models for Cross-Modal Localization using Convolutional Transformers. (arXiv:2306.04021v1 [cs.CV])

    [http://arxiv.org/abs/2306.04021](http://arxiv.org/abs/2306.04021)

    本文提出了一种基于能量模型和卷积变换的跨模态定位方法，利用卫星图像进行地图构建，可在没有GPS的情况下实现精确的度量级别定位，并在KITTI数据集上取得了更高的定位精度。

    

    本文提出了一种使用基于能量模型（EBMs）的新型框架，用于在没有GPS的情况下定位一个搭载有测距传感器的地面车辆相对于卫星图像的位置。本方法利用卫星图像进行地图构建，这种地图是广泛可用和易于获取的，而且具有全面的覆盖面。我们提出了一种使用卷积变换的方法，在跨模态的情况下实现了精确的度量级别定位，这是由于稀疏的测距传感器读数和丰富的卫星图像之间外观差异的极度之大而具有挑战性。我们的模型是端到端训练的，并且在KITTI数据集上实验表明，相比最先进的方法，本文的方法可以实现更高的定位精度。

    We present a novel framework using Energy-Based Models (EBMs) for localizing a ground vehicle mounted with a range sensor against satellite imagery in the absence of GPS. Lidar sensors have become ubiquitous on autonomous vehicles for describing its surrounding environment. Map priors are typically built using the same sensor modality for localization purposes. However, these map building endeavors using range sensors are often expensive and time-consuming. Alternatively, we leverage the use of satellite images as map priors, which are widely available, easily accessible, and provide comprehensive coverage. We propose a method using convolutional transformers that performs accurate metric-level localization in a cross-modal manner, which is challenging due to the drastic difference in appearance between the sparse range sensor readings and the rich satellite imagery. We train our model end-to-end and demonstrate our approach achieving higher accuracy than the state-of-the-art on KITTI,
    
[^69]: 使用神经网络学习搜索空间特定的启发式方法

    Learning Search-Space Specific Heuristics Using Neural Networks. (arXiv:2306.04019v1 [cs.AI])

    [http://arxiv.org/abs/2306.04019](http://arxiv.org/abs/2306.04019)

    该论文提出了一个使用神经网络学习特定搜索空间启发式方法的系统，可以为经典计划提供距离目标估计，有时能够与领域无关的启发式方法竞争。

    

    我们提出并评估了一个系统，该系统为基于前向搜索的满足经典计划提供了学习神经网络启发式函数的方法。我们的系统从头开始学习距离目标估计器，仅给定一个PDDL训练实例。培训数据是通过后向回归搜索或从给定或猜测的目标状态进行的后向搜索产生的。在像24拼图这样所有实例共享同一搜索空间的域中，这种启发式方法也可以在该领域的所有实例中重复使用。我们展示了这个相对简单的系统可以表现出惊人的性能，有时可以与众所周知的领域独立启发式方法竞争。

    We propose and evaluate a system which learns a neuralnetwork heuristic function for forward search-based, satisficing classical planning. Our system learns distance-to-goal estimators from scratch, given a single PDDL training instance. Training data is generated by backward regression search or by backward search from given or guessed goal states. In domains such as the 24-puzzle where all instances share the same search space, such heuristics can also be reused across all instances in the domain. We show that this relatively simple system can perform surprisingly well, sometimes competitive with well-known domain-independent heuristics.
    
[^70]: PyTrial：药物研发人工智能的全面平台

    PyTrial: A Comprehensive Platform for Artificial Intelligence for Drug Development. (arXiv:2306.04018v1 [cs.AI])

    [http://arxiv.org/abs/2306.04018](http://arxiv.org/abs/2306.04018)

    PyTrial是一个实现多种AI算法支持的临床试验任务、可集成自己AI模型和数据集的Python软件包，并在现实临床试验数据上进行了广泛测试。

    

    药物研发是一个复杂的过程，旨在通过临床试验测试候选药物在人体内的疗效和安全性以获得监管批准。最近，机器学习作为药物研发的重要工具出现了，为提高该过程的效率和成功率提供了新机会。为了促进药物研发人工智能的研究和开发，我们开发了一个Python软件包，名为PyTrial，该软件包实现了多种被AI算法支持的临床试验任务。具体而言，PyTrial实现了6个关键的药物研发任务，包括患者结果预测、试验地点选择、试验结果预测、患者-试验匹配、试验相似性搜索和合成数据生成。在PyTrial中，所有任务都由四个步骤定义：加载数据、模型定义、模型训练和模型评估，这可以用几行代码完成。此外，模块化的API设计允许从业者集成自己的AI模型和数据集。PyTrial已在现实临床试验数据上进行了广泛测试，实验结果证明了其在各种受AI支持的临床试验任务中的有效性和效率。

    Drug development is a complex process that aims to test the efficacy and safety of candidate drugs in the human body for regulatory approval via clinical trials. Recently, machine learning has emerged as a vital tool for drug development, offering new opportunities to improve the efficiency and success rates of the process. To facilitate the research and development of artificial intelligence (AI) for drug development, we developed a Python package, namely PyTrial, that implements various clinical trial tasks supported by AI algorithms.  To be specific, PyTrial implements 6 essential drug development tasks, including patient outcome prediction, trial site selection, trial outcome prediction, patient-trial matching, trial similarity search, and synthetic data generation. In PyTrial, all tasks are defined by four steps: load data, model definition, model training, and model evaluation, which can be done with a couple of lines of code. In addition, the modular API design allows practition
    
[^71]: 使用软提示和随机游走在语言模型中触发多跳推理进行问题回答

    Triggering Multi-Hop Reasoning for Question Answering in Language Models using Soft Prompts and Random Walks. (arXiv:2306.04009v1 [cs.CL])

    [http://arxiv.org/abs/2306.04009](http://arxiv.org/abs/2306.04009)

    本研究提出了使用软提示和随机游走的方法，以便于预训练语言模型进行多跳推理的问答任务，取得了比标准调整方法更大的改进。

    

    尽管可以轻松地记忆有关实体的世界知识，但预先训练的语言模型（LM）往往在组合两个或多个事实以执行多跳推理的问答任务方面存在困难。在本文中，我们提出了一些技术来改善这个限制，这些技术依靠结构化知识图上的随机游走。具体而言，我们使用软提示来引导LM，通过学习将多跳问题映射到通向答案的随机游走路径来链式编码它们的知识。将我们的方法应用于两个T5 LM上，在回答需要2跳推理的问题方面，表现出了比标准调整方法更大的改进。

    Despite readily memorizing world knowledge about entities, pre-trained language models (LMs) struggle to compose together two or more facts to perform multi-hop reasoning in question-answering tasks. In this work, we propose techniques that improve upon this limitation by relying on random walks over structured knowledge graphs. Specifically, we use soft prompts to guide LMs to chain together their encoded knowledge by learning to map multi-hop questions to random walk paths that lead to the answer. Applying our methods on two T5 LMs shows substantial improvements over standard tuning approaches in answering questions that require 2-hop reasoning.
    
[^72]: 用一维深度图像先验进行电磁求解器S参数曲线拟合

    One-Dimensional Deep Image Prior for Curve Fitting of S-Parameters from Electromagnetic Solvers. (arXiv:2306.04001v1 [cs.LG])

    [http://arxiv.org/abs/2306.04001](http://arxiv.org/abs/2306.04001)

    该论文提出了一种使用深度生成模型为基础的方法，使用一维深度图像先验拟合电磁求解器的S参数。通过与公开可用和专有的行业标准拟合方法的比较，实验结果表明该方法在重建质量和计算时间方面均有显着改进。

    

    在集成电路封装中建模信号完整性的一个关键问题是需要在所需频带内进行多个S参数测量以获得足够的分辨率。使用电磁场求解器获得这些样本通常是计算昂贵的。因此，常见方法是选择所需样本的一小部分，并使用适当的拟合机制重新创建密集采样的宽带表示。我们提出了一种基于深度生成模型的方法来使用一维深度图像先验（DIP）拟合来自EM求解器的S参数。DIP是一种技术，它优化随机初始化的卷积神经网络的权重以适应从噪声或欠定测量中的信号。我们设计了一个自定义架构，并提出了一种新颖的正则化技术，灵感来自平滑样条，以惩罚不连续的跳跃。我们在两种类型的无源滤波器上实验比较了DIP与公开可用的和专有的行业标准拟合方法，并展示了重建质量和计算时间的显着改进。

    A key problem when modeling signal integrity for passive filters and interconnects in IC packages is the need for multiple S-parameter measurements within a desired frequency band to obtain adequate resolution. These samples are often computationally expensive to obtain using electromagnetic (EM) field solvers. Therefore, a common approach is to select a small subset of the necessary samples and use an appropriate fitting mechanism to recreate a densely-sampled broadband representation. We present the first deep generative model-based approach to fit S-parameters from EM solvers using one-dimensional Deep Image Prior (DIP). DIP is a technique that optimizes the weights of a randomly-initialized convolutional neural network to fit a signal from noisy or under-determined measurements. We design a custom architecture and propose a novel regularization inspired by smoothing splines that penalizes discontinuous jumps. We experimentally compare DIP to publicly available and proprietary indus
    
[^73]: 实时在线无监督领域自适应用于现实世界人员再识别

    Real-Time Online Unsupervised Domain Adaptation for Real-World Person Re-identification. (arXiv:2306.03993v1 [cs.CV])

    [http://arxiv.org/abs/2306.03993](http://arxiv.org/abs/2306.03993)

    本文提出了一个新的实时在线无监督领域自适应设置（R$^2$OUDA），并引入了一个新颖的多摄像头系统R$^2$MMT。通过R$^2$OUDA，本文解决了现实应用中被忽略的四个主要限制，以实现真正的实时在线无监督领域自适应。

    

    随着无监督领域自适应（UDA）在人员再识别中的流行，最近提出的在线无监督领域自适应（OUDA）尝试通过引入数据流的考虑来弥合实际应用的差距。然而，这仍然无法真正代表真实的现实应用。本文定义了现实世界实时在线无监督领域自适应（R$^2$OUDA）。R$^2$OUDA设置了真正的现实世界实时OUDA的舞台，揭示了当前研究中经常被忽略的四个主要限制：系统生成的人员图像、子集分布选择、基于时间的数据流分割和基于分段的时间约束。为了解决这个新的R$^2$OUDA设置的所有方面，本文进一步提出了现实世界实时在线流式互相平均教学（R$^2$MMT），这是一种新颖的多摄像头系统。

    Following the popularity of Unsupervised Domain Adaptation (UDA) in person re-identification, the recently proposed setting of Online Unsupervised Domain Adaptation (OUDA) attempts to bridge the gap towards practical applications by introducing a consideration of streaming data. However, this still falls short of truly representing real-world applications. This paper defines the setting of Real-world Real-time Online Unsupervised Domain Adaptation (R$^2$OUDA) for Person Re-identification. The R$^2$OUDA setting sets the stage for true real-world real-time OUDA, bringing to light four major limitations found in real-world applications that are often neglected in current research: system generated person images, subset distribution selection, time-based data stream segmentation, and a segment-based time constraint. To address all aspects of this new R$^2$OUDA setting, this paper further proposes Real-World Real-Time Online Streaming Mutual Mean-Teaching (R$^2$MMT), a novel multi-camera sy
    
[^74]: 学习我们能够掌握的力量：基于像素级交互的多目标视频生成

    Learn the Force We Can: Multi-Object Video Generation from Pixel-Level Interactions. (arXiv:2306.03988v1 [cs.CV])

    [http://arxiv.org/abs/2306.03988](http://arxiv.org/abs/2306.03988)

    本论文提出了一种基于像素级交互的多目标视频生成方法，能够生成逼真的物体间相互作用的视频，且能准确跟随用户的控制，达到了最先进的视频生成先前工作相媲美甚至更好的效果。

    

    我们提出了一种新颖的无监督方法，通过单帧图像与稀疏运动输入来自我回归生成视频。我们训练的模型能够生成逼真的物体间相互作用，并在仅观测到它们在相关运动活动下时分离多个物体的动态和范围。我们方法的关键组件是随机化条件方案、输入运动控制的编码以及随机化和稀疏采样来打破相关性。我们称之为YODA的模型具有能够移动物体而无需实际触摸的能力。我们定量和定性地展示，YODA能够准确跟随用户的控制，并在多个数据集上呈现出与现有最先进的视频生成先前工作相媲美甚至更好的视频质量。详情请参阅我们的项目网站 https://araachie.github.io/yoda。

    We propose a novel unsupervised method to autoregressively generate videos from a single frame and a sparse motion input. Our trained model can generate realistic object-to-object interactions and separate the dynamics and the extents of multiple objects despite only observing them under correlated motion activities. Key components in our method are the randomized conditioning scheme, the encoding of the input motion control, and the randomized and sparse sampling to break correlations. Our model, which we call YODA, has the ability to move objects without physically touching them. We show both qualitatively and quantitatively that YODA accurately follows the user control, while yielding a video quality that is on par with or better than state of the art video generation prior work on several datasets. For videos, visit our project website https://araachie.github.io/yoda.
    
[^75]: 数字表型在精神病学临床决策中的应用——基于对病例的预测和解释

    Counterfactual Explanations and Predictive Models to Enhance Clinical Decision-Making in Schizophrenia using Digital Phenotyping. (arXiv:2306.03980v1 [cs.AI])

    [http://arxiv.org/abs/2306.03980](http://arxiv.org/abs/2306.03980)

    本研究提出了一个数字表型和机器学习相结合的机器学习系统，通过预测、检测和因果解释患有精神分裂症症状变化的个体情况，可能帮助改进精神病学临床决策，系统误差率低于10%。

    

    精神病学的临床实践常面临卫生服务需求的增加和医疗资源的稀缺。基于机器学习技术的新型健康数据范例可能会打开精神病学临床评估和治疗关键阶段的工作流改进的可能性。本研究提出了一个机器学习系统，利用行为数字表型数据预测、检测和解释患有精神分裂症症状变化的个体情况。我们的系统误差率低于10%。系统利用变点检测算法检测症状下降，并在模拟的连续监控场景中使用因果解释功能作为一种补救措施。这项研究从模拟临床流程的角度提供了有关因果解释、预测模型和变点检测性能和潜力的有价值的见解。本研究得出以下结论：数字表型与机器学习相结合可以帮助改进精神病学临床决策；所提出的机器学习系统可以高精度地预测和检测病例个体症状变化；因果解释在模拟的连续监控场景中是一种有价值的工具。

    Clinical practice in psychiatry is burdened with the increased demand for healthcare services and the scarce resources available. New paradigms of health data powered with machine learning techniques could open the possibility to improve clinical workflow in critical stages of clinical assessment and treatment in psychiatry. In this work, we propose a machine learning system capable of predicting, detecting, and explaining individual changes in symptoms of patients with Schizophrenia by using behavioral digital phenotyping data. We forecast symptoms of patients with an error rate below 10%. The system detects decreases in symptoms using changepoint algorithms and uses counterfactual explanations as a recourse in a simulated continuous monitoring scenario in healthcare. Overall, this study offers valuable insights into the performance and potential of counterfactual explanations, predictive models, and change-point detection within a simulated clinical workflow. These findings lay the f
    
[^76]: 使用表达式布尔公式的可解释人工智能

    Explainable AI using expressive Boolean formulas. (arXiv:2306.03976v1 [cs.AI])

    [http://arxiv.org/abs/2306.03976](http://arxiv.org/abs/2306.03976)

    提出了一种表达式布尔公式的可解释人工智能，利用本地优化技术训练分类模型，可以应用于信用评分和医疗病况的诊断，并具有未来应用的潜力。

    

    我们提出并实现了一个可解释的机器学习分类模型，基于表达式布尔公式的可解释人工智能（XAI）。潜在的应用包括信用评分和医疗病况的诊断。布尔公式定义了一个规则，根据该规则将输入数据分类为可调整复杂度（或可解释性）。这样的公式可以包含应用于一个或多个布尔变量的任何运算符，因此具有比更严格的基于规则和基于树的方法更高的表达性。分类器使用本地优化技术进行训练，有效地搜索可行公式的空间。浅的规则可以通过快速整数线性规划（ILP）或二次无约束二进制优化（QUBO）求解器确定，可能由专用硬件或量子设备提供动力。我们通过执行本地优化器的表达能力和效率与这些设备的快速操作相结合，为未来的AI研究提供了新思路。

    We propose and implement an interpretable machine learning classification model for Explainable AI (XAI) based on expressive Boolean formulas. Potential applications include credit scoring and diagnosis of medical conditions. The Boolean formula defines a rule with tunable complexity (or interpretability), according to which input data are classified. Such a formula can include any operator that can be applied to one or more Boolean variables, thus providing higher expressivity compared to more rigid rule-based and tree-based approaches. The classifier is trained using native local optimization techniques, efficiently searching the space of feasible formulas. Shallow rules can be determined by fast Integer Linear Programming (ILP) or Quadratic Unconstrained Binary Optimization (QUBO) solvers, potentially powered by special purpose hardware or quantum devices. We combine the expressivity and efficiency of the native local optimizer with the fast operation of these devices by executing n
    
[^77]: PILLAR：如何使半私有学习更有效

    PILLAR: How to make semi-private learning more effective. (arXiv:2306.03962v1 [cs.LG])

    [http://arxiv.org/abs/2306.03962](http://arxiv.org/abs/2306.03962)

    本文提出了一种计算效率高的算法 PILLAR，可以在半监督半私有（SP）学习中明显降低私有标记样本复杂度，并可以在实际数据集上高效运行，可以利用在公共数据上预训练的网络提取的特征，并在实验证明了其显著有效性。

    

    在半监督半私有（SP）学习中，学习者可以访问公共的未标记数据和私有的标记数据。我们提出了一种计算效率高的算法，假设数据符合一定条件，可以明显降低私有标记样本复杂度，并可以在实际数据集上高效运行。为此，我们利用在公共数据（标记或未标记）上预训练的网络提取的特征，这些特征的分布可能与进行SP学习的分布显著不同。为了验证其实证有效性，我们提出了多种在严格的隐私约束（\(\epsilon=0.1\))和低数据量情况下的实验。在所有这些设置中，我们的算法表现出显著优于使用类似数量的公共数据的现有基线的性能。

    In Semi-Supervised Semi-Private (SP) learning, the learner has access to both public unlabelled and private labelled data. We propose a computationally efficient algorithm that, under mild assumptions on the data, provably achieves significantly lower private labelled sample complexity and can be efficiently run on real-world datasets. For this purpose, we leverage the features extracted by networks pre-trained on public (labelled or unlabelled) data, whose distribution can significantly differ from the one on which SP learning is performed. To validate its empirical effectiveness, we propose a wide variety of experiments under tight privacy constraints (\(\epsilon=0.1\)) and with a focus on low-data regimes. In all of these settings, our algorithm exhibits significantly improved performance over available baselines that use similar amounts of public data.
    
[^78]: 通过正交神经网络学习因果机制

    Learning Causal Mechanisms through Orthogonal Neural Networks. (arXiv:2306.03938v1 [cs.LG])

    [http://arxiv.org/abs/2306.03938](http://arxiv.org/abs/2306.03938)

    本文介绍了一种无监督的方法来学习一组独立机制的反向操作，以发现和分离一组独立的机制。

    

    人类智能的重要特征之一是能够从低级感官数据中推导出高级抽象。这种推理的一个重要组成部分是发现模块化的生成机制。本文探讨了一个问题，即如何在完全无监督的情况下从失真的数据点中学习一组独立机制的反向操作。我们提出并通过实验结果证明，现有机器学习解决方案的一个重要弱点在于跨模块多样化不足。解决人工智能与机器智能之间的这一重要差距是模式识别系统的重要挑战。为此，本文提出了一种无监督方法，可以发现和分离一组独立的机制。

    A fundamental feature of human intelligence is the ability to infer high-level abstractions from low-level sensory data. An essential component of such inference is the ability to discover modularized generative mechanisms. Despite many efforts to use statistical learning and pattern recognition for finding disentangled factors, arguably human intelligence remains unmatched in this area.  In this paper, we investigate a problem of learning, in a fully unsupervised manner, the inverse of a set of independent mechanisms from distorted data points. We postulate, and justify this claim with experimental results, that an important weakness of existing machine learning solutions lies in the insufficiency of cross-module diversification. Addressing this crucial discrepancy between human and machine intelligence is an important challenge for pattern recognition systems.  To this end, our work proposes an unsupervised method that discovers and disentangles a set of independent mechanisms from u
    
[^79]: 在联邦学习中使用预训练模型引导最后一层

    Guiding The Last Layer in Federated Learning with Pre-Trained Models. (arXiv:2306.03937v1 [cs.AI])

    [http://arxiv.org/abs/2306.03937](http://arxiv.org/abs/2306.03937)

    本文研究了在联邦学习中使用预训练模型引导最后一层的问题，提出了使用最近类均值(NCM)精确且高效地拟合分类器的方法，并取得了很好的效果。

    

    联邦学习(Fl)是一种新兴的范式，允许在不共享数据的情况下跨多个参与者训练模型。最近的一些工作开始考虑使用预训练模型作为现有FL算法的初始化点的影响; 但是，这些方法忽略了集中式学习设置中大量有效的迁移学习文献。在这里，我们重新审视了先前工作中考虑预训练模型的FL问题，并将其扩展到一组计算机视觉迁移学习问题。我们首先观察到，在许多情况下，仅拟合线性分类头是高效且有效的。然后，我们展示了在FL设置中，使用最近类均值(NCM)拟合分类器可以精确地且比现有提议高效地完成，同时获得了强大的性能。最后，我们证明了使用两阶段方法获得分类器，然后微调模型可以产生更好的结果。

    Federated Learning (FL) is an emerging paradigm that allows a model to be trained across a number of participants without sharing data. Recent works have begun to consider the effects of using pre-trained models as an initialization point for existing FL algorithms; however, these approaches ignore the vast body of efficient transfer learning literature from the centralized learning setting. Here we revisit the problem of FL from a pre-trained model considered in prior work and expand it to a set of computer vision transfer learning problems. We first observe that simply fitting a linear classification head can be efficient and effective in many cases. We then show that in the FL setting, fitting a classifier using the Nearest Class Means (NCM) can be done exactly and orders of magnitude more efficiently than existing proposals, while obtaining strong performance. Finally, we demonstrate that using a two-phase approach of obtaining the classifier and then fine-tuning the model can yiel
    
[^80]: 高维和置换不变异常检测。

    High-dimensional and Permutation Invariant Anomaly Detection. (arXiv:2306.03933v1 [hep-ph])

    [http://arxiv.org/abs/2306.03933](http://arxiv.org/abs/2306.03933)

    该研究引入了一种置换不变的高维密度估计方法，通过学习后将其用于高能物理数据中的异常检测，能够有效地识别出在仅具备背景假设下排除异常的喷注。

    

    由于学习高维概率密度的困难，新物理过程的异常检测方法通常局限于低维空间。特别是在成分级别上，将置换不变性和可变长度的输入等良好性质合并到流行的密度估计方法中变得更加困难。在本研究中，我们引入了一种基于扩散模型的粒子物理数据置换不变密度估计器，专门设计用于处理可变长度的输入。我们通过将学习到的密度用作置换不变的异常检测评分来展示我们方法的功效，有效地识别出在仅具备背景假设下的可能性较低的喷注。为了验证我们的密度估计方法，我们研究了学习到的密度比与被监督分类算法获得的密度之间的比较。

    Methods for anomaly detection of new physics processes are often limited to low-dimensional spaces due to the difficulty of learning high-dimensional probability densities. Particularly at the constituent level, incorporating desirable properties such as permutation invariance and variable-length inputs becomes difficult within popular density estimation methods. In this work, we introduce a permutation-invariant density estimator for particle physics data based on diffusion models, specifically designed to handle variable-length inputs. We demonstrate the efficacy of our methodology by utilizing the learned density as a permutation-invariant anomaly detection score, effectively identifying jets with low likelihood under the background-only hypothesis. To validate our density estimation method, we investigate the ratio of learned densities and compare to those obtained by a supervised classification algorithm.
    
[^81]: ChatDB: 将数据库作为符号内存增强LLMs

    ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory. (arXiv:2306.03901v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.03901](http://arxiv.org/abs/2306.03901)

    ChatDB项目将SQL数据库作为符号内存，增强LLMs的复杂多跳推理能力。

    

    大语言模型（LLMs）与内存是计算通用的。然而，主流LLMs没有充分利用内存，并且设计受到生物大脑的严重影响。由于其近似性质和错误累积倾向，传统神经内存机制不能支持LLMs模拟复杂推理。在本文中，我们从现代计算机架构中寻求灵感，为复杂的多跳推理增强LLMs符号内存。这样的符号内存框架被实例化为一个LLM和一组SQL数据库，其中LLM生成SQL指令以操作SQL数据库。我们在一个需要复杂推理的合成数据集上验证了所提出的内存框架的有效性。 项目网站位于https://chatdatabase.github.io/。

    Large language models (LLMs) with memory are computationally universal. However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available at https://chatdatabase.github.io/ .
    
[^82]: 应用演绎验证技术验证思维链的推理过程

    Deductive Verification of Chain-of-Thought Reasoning. (arXiv:2306.03872v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.03872](http://arxiv.org/abs/2306.03872)

    本文旨在通过应用演绎验证技术，使语言模型能够进行明确而严谨的演绎推理，以确保其推理过程的可信度。

    

    大语言模型在各种推理任务中受益匪浅，特别是应用思维链提示可以使模型产生更全面的推理过程。然而，思维链的强调中间推理步骤可能会不慎导致产生幻觉和累积错误，从而限制模型解决复杂推理任务的能力。本文灵感来自于人类如何进行细致的演绎逻辑推理过程来解决任务，我们旨在使语言模型能够进行明确而严谨的演绎推理，并通过自我验证确保推理过程的可信度。然而，即使是像ChatGPT这样先进的模型，直接验证整个演绎推理过程的有效性也是具有挑战性的。因此，我们提出将推理验证过程分解为一系列逐步的子过程，每个过程只接收其必要的上下文和前提条件。

    Large Language Models (LLMs) significantly benefit from Chain-of-Thought (CoT) prompting in performing various reasoning tasks. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification. However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like ChatGPT. In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. To facilitate t
    
[^83]: ChatGPT信息的图神经网络用于股票价格预测

    ChatGPT Informed Graph Neural Network for Stock Movement Prediction. (arXiv:2306.03763v1 [q-fin.ST])

    [http://arxiv.org/abs/2306.03763](http://arxiv.org/abs/2306.03763)

    该研究介绍了一种新的框架，利用ChatGPT技术增强图神经网络，能够从财经新闻中提取出不断变化的网络结构，并用于股票价格预测，获得了超过基于深度学习的最新基准的表现，提示了ChatGPT在文本推断和金融预测方面的潜力。

    

    ChatGPT已在各种自然语言处理（NLP）任务中展示了出色的能力。然而，它从时间文本数据（尤其是财经新闻）推断动态网络结构的潜力仍是一个未开发的领域。在这项研究中，我们介绍了一个新的框架，利用ChatGPT的图推断能力来增强图神经网络（GNN）。我们的框架巧妙地从文本数据中提取出不断变化的网络结构，并将这些网络结构融合到图神经网络中，进行后续的预测任务。股票价格预测的实验结果表明，我们的模型始终优于基于深度学习的最新基准。此外，基于我们模型的产出构建的组合展示出更高的年化累计回报、更低的波动性和最大回撤。这种卓越表现突显了ChatGPT用于基于文本的网络推断和金融预测应用的潜力。

    ChatGPT has demonstrated remarkable capabilities across various natural language processing (NLP) tasks. However, its potential for inferring dynamic network structures from temporal textual data, specifically financial news, remains an unexplored frontier. In this research, we introduce a novel framework that leverages ChatGPT's graph inference capabilities to enhance Graph Neural Networks (GNN). Our framework adeptly extracts evolving network structures from textual data, and incorporates these networks into graph neural networks for subsequent predictive tasks. The experimental results from stock movement forecasting indicate our model has consistently outperformed the state-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios constructed based on our model's outputs demonstrate higher annualized cumulative returns, alongside reduced volatility and maximum drawdown. This superior performance highlights the potential of ChatGPT for text-based network inferences and 
    
[^84]: 基于音韵学的语言生成：以绕口令为例

    Phonetically-Grounded Language Generation: The Case of Tongue Twisters. (arXiv:2306.03457v1 [cs.CL])

    [http://arxiv.org/abs/2306.03457](http://arxiv.org/abs/2306.03457)

    本文介绍了针对绕口令生成的基于音韵学的语言生成任务，提供了TwistList数据集和TwisterMisters基准系统，并验证了预训练模型在没有任务特定数据和显式音韵知识的情况下的良好性能。

    

    先前的音韵学语言生成主要集中在词歌和诗歌等领域。本文介绍了围绕绕口令生成展开的工作，绕口令需要在保持语义正确性的同时，最大化音频重叠并保持语法正确。我们提供了TwistList，一个包含超过2.1K人工编写的绕口令的大型注释数据集。此外，我们针对绕口令生成提出了一些基准系统(TwisterMisters)，包括需要和不需要在域内数据上进行训练的模型。我们使用自动和人工评估的结果来证明现有主流预训练模型在此任务中性能优良，即使在没有任务特定训练数据和显式音韵知识的情况下。我们发现，绕口令生成的任务是有挑战性的。

    Previous work in phonetically-grounded language generation has mainly focused on domains such as lyrics and poetry. In this paper, we present work on the generation of tongue twisters - a form of language that is required to be phonetically conditioned to maximise sound overlap, whilst maintaining semantic consistency with an input topic, and still being grammatically correct. We present \textbf{TwistList}, a large annotated dataset of tongue twisters, consisting of 2.1K+ human-authored examples. We additionally present several benchmark systems (referred to as TwisterMisters) for the proposed task of tongue twister generation, including models that both do and do not require training on in-domain data. We present the results of automatic and human evaluation to demonstrate the performance of existing mainstream pre-trained models in this task with limited (or no) task specific training and data, and no explicit phonetic knowledge. We find that the task of tongue twister generation is 
    
[^85]: Vid2Act：为视觉强化学习激活离线视频

    Vid2Act: Activate Offline Videos for Visual RL. (arXiv:2306.03360v1 [cs.LG])

    [http://arxiv.org/abs/2306.03360](http://arxiv.org/abs/2306.03360)

    Vid2Act是一种基于模型的强化学习方法，它通过使用世界模型来传输领域相关的动态和策略，从而显著提高了样本效率。

    

    在离线视频数据集上预训练强化学习模型是提高其在线任务效率的有前途的方法，但由于跨域中任务、动态和行为的固有不匹配性而具有挑战性。最近，一种名为APV的模型避免了离线数据集中的伴随动作记录，而是专注于在源域内预训练与任务无关的、不涉及操作的世界模型。我们提出了Vid2Act，一种基于模型的强化学习方法，它学习从离线到在线环境中传输有价值的动作条件动态和潜在有用的动作演示。其主要思想是不仅将世界模型用作行为学习的模拟器，还将其用作测量领域相关性的工具，以便进行动态表示传输和策略传输。具体地，我们通过域选择知识蒸馏损失训练世界模型生成一组时间变化的任务相似度。这些相似度有两个目的：（i）自适应地将最相关的领域的动态传输到在线环境，和（ii）在在线环境中指导代理集中执行任务相关的动作。在Atari和DMControl连续控制任务上的实验结果表明了我们方法的有效性，其在样本效率方面大大优于之前的最先进的离线强化学习方法。

    Pretraining RL models on offline video datasets is a promising way to improve their training efficiency in online tasks, but challenging due to the inherent mismatch in tasks, dynamics, and behaviors across domains. A recent model, APV, sidesteps the accompanied action records in offline datasets and instead focuses on pretraining a task-irrelevant, action-free world model within the source domains. We present Vid2Act, a model-based RL method that learns to transfer valuable action-conditioned dynamics and potentially useful action demonstrations from offline to online settings. The main idea is to use the world models not only as simulators for behavior learning but also as tools to measure the domain relevance for both dynamics representation transfer and policy transfer. Specifically, we train the world models to generate a set of time-varying task similarities using a domain-selective knowledge distillation loss. These similarities serve two purposes: (i) adaptively transferring th
    
[^86]: 推理时间干预：从语言模型中引导出真实的答案

    Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v1 [cs.LG])

    [http://arxiv.org/abs/2306.03341](http://arxiv.org/abs/2306.03341)

    本研究提出推理时间干预（ITI）技术，通过在推理过程中跨越有限数量的注意力头，显着提高大型语言模型的真实性。在TruthfulQA基准上，ITI使LLaMA模型的真实性从32.5%提高到65.1%。ITI是一种最小程度的干扰，计算廉价，且数据效率高。

    

    我们介绍了推理时间干预（ITI）技术，旨在增强大型语言模型（LLMs）的真实性。ITI通过在推理过程中沿着一组方向移动模型激活，跨越有限数量的注意力头。这种干预显着提高了LLaMA模型在TruthfulQA基准上的表现。在指令微调的LLaMA Alpaca上，ITI将其真实性从32.5％提高到65.1％。我们确定了真实性和可用性之间的权衡，并演示了如何通过调整干预强度来平衡它。ITI 取得了最低程度的干扰且计算廉价。此外，该技术在数据效率上表现优异：虽然像RLHF这样的方法需要广泛注释，但是ITI仅使用了几百个例子就能定位真实的方向。我们的研究结果表明，LLMs可能具有某种内部表示方法来表示某事是真实的可能性，即使它们在表面上产生了虚假的结果。

    We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
    
[^87]: 采用优势诱导策略对齐的Fine-Tuning语言模型

    Fine-Tuning Language Models with Advantage-Induced Policy Alignment. (arXiv:2306.02231v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02231](http://arxiv.org/abs/2306.02231)

    本论文提出了一种新算法APA，其采用优势诱导策略对齐用于强化学习语言模型。相对于传统方法（PPO），APA在语言任务中表现更好，避免了模型的崩溃与不稳定性。

    

    人类反馈强化学习（RLHF）已经成为将大型语言模型（LLMs）与人类偏好对齐的可靠方法。在众多RLHF技术中，接近策略优化（PPO）是最常用的方法之一。然而，尽管PPO很流行，但它可能会遭受模式崩溃、不稳定和效率低下的问题。我们展示了一种新颖的算法--基于估计优势的平方误差损失函数的优势诱导策略对齐（APA），可以减轻这些问题。我们通过实验证明，当使用单独的奖励模型作为评估器时，APA在语言任务中始终比PPO表现出更好的性能。此外，与PPO相比，APA可以更稳定地控制模型与初始策略的偏差，确保模型提高性能而不会崩溃为确定性输出。除了经验结果之外，我们还提供了APA的理论分析。

    Reinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning large language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal policy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO may suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can be alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment (APA), which leverages a squared error loss function based on the estimated advantages. We demonstrate empirically that APA consistently outperforms PPO in language tasks by a large margin, when a separate reward model is employed as the evaluator. In addition, compared with PPO, APA offers a more stable form of control over the deviation from the model's initial policy, ensuring that the model improves its performance without collapsing to deterministic output. In addition to empirical results, we also prov
    
[^88]: SGEM：通过序列级广义熵最小化实现自动语音识别的测试时自适应

    SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization. (arXiv:2306.01981v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2306.01981](http://arxiv.org/abs/2306.01981)

    SGEM提出了一种新的测试时自适应框架，利用波束搜索和广义熵最小化调整预训练ASR模型，取得了三个主流ASR模型在不同领域转变时的最新性能。

    

    在许多实际情况下，自动语音识别（ASR）模型经常暴露于数据分布的变化，导致错误的预测。为了解决这个问题，最近提出了一种现有的测试时自适应（TTA）方法，可以在没有源数据的情况下调整预训练的ASR模型以适应未标记的测试实例。尽管有了不错的性能提升，但这项工作仅依赖于简单的贪心解码，并在帧级别上跨越时间步长进行调整，这在模型输出的序列性质下可能不是最优的。出于这个动机，我们提出了一个新的TTA框架，称为SGEM，用于一般ASR模型。为了处理序列输出，SGEM首先利用波束搜索来探索候选输出标志，并选择最可信的标志。然后，它利用广义熵最小化和负抽样作为无监督目标来适应模型。在各种领域的转变下，SGEM实现了三种主流ASR模型的最新性能。

    Automatic speech recognition (ASR) models are frequently exposed to data distribution shifts in many real-world scenarios, leading to erroneous predictions. To tackle this issue, an existing test-time adaptation (TTA) method has recently been proposed to adapt the pre-trained ASR model on unlabeled test instances without source data. Despite decent performance gain, this work relies solely on naive greedy decoding and performs adaptation across timesteps at a frame level, which may not be optimal given the sequential nature of the model output. Motivated by this, we propose a novel TTA framework, dubbed SGEM, for general ASR models. To treat the sequential output, SGEM first exploits beam search to explore candidate output logits and selects the most plausible one. Then, it utilizes generalized entropy minimization and negative sampling as unsupervised objectives to adapt the model. SGEM achieves state-of-the-art performance for three mainstream ASR models under various domain shifts.
    
[^89]: SQL-PaLM：针对Text-to-SQL的改进大语言模型适应性

    SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL. (arXiv:2306.00739v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.00739](http://arxiv.org/abs/2306.00739)

    本文提出了一种基于大语言模型的Text-to-SQL模型SQL-PaLM，使用了面向Text-to-SQL的基于执行的自一致提示方法，在Spider上实现了77.3%的测试套件准确度，并显着超越以前的最新技术的方法。

    

    大语言模型（LLMs）的一个令人印象深刻的新兴功能是生成代码，包括用于数据库的结构化查询语言（SQL）。对于将自然语言文本转换为SQL查询的任务，即Text-to-SQL，LLMs的适应性至关重要，具体取决于使用的适应性数据量。本文提出了一种基于LLM的Text-to-SQL模型SQL-PaLM，利用了PaLM-2，推动了两种设置的最新进展。Few-shot SQL-PaLM基于面向Text-to-SQL的基于执行的自一致提示方法，可在Spider上实现77.3%的测试套件准确度，据我们所知，这是第一个通过显着较大的微调超越以前的最新技术的方法。此外，我们证明经过精细调整的SQL-PALM可进一步提高1%的性能。为了将SQL-PaLM应用于实际场景，我们进一步评估了其对其他挑战的稳健性。

    One impressive emergent capability of large language models (LLMs) is generation of code, including Structured Query Language (SQL) for databases. For the task of converting natural language text to SQL queries, Text-to-SQL, adaptation of LLMs is of paramount importance, both in in-context learning and fine-tuning settings, depending on the amount of adaptation data used. In this paper, we propose an LLM-based Text-to-SQL model SQL-PaLM, leveraging on PaLM-2, that pushes the state-of-the-art in both settings. Few-shot SQL-PaLM is based on an execution-based self-consistency prompting approach designed for Text-to-SQL, and achieves 77.3% in test-suite accuracy on Spider, which to our best knowledge is the first to outperform previous state-of-the-art with fine-tuning by a significant margin, 4%. Furthermore, we demonstrate that the fine-tuned SQL-PALM outperforms it further by another 1%. Towards applying SQL-PaLM to real-world scenarios we further evaluate its robustness on other chall
    
[^90]: 使用子图特定因子嵌入归一化改善GNN的表达能力

    Improving Expressivity of GNNs with Subgraph-specific Factor Embedded Normalization. (arXiv:2305.19903v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19903](http://arxiv.org/abs/2305.19903)

    本文提出了一种名为SuperNorm的专用归一化方案，通过嵌入子图特定因子和纳入图实例特定统计数据来加强GNN的代表性能力，实现对节点感应子图中内部连接信息的明确考虑，从而改善GNN的表达能力。

    

    图神经网络（GNN）已经成为一类处理图结构数据的强大学习架构。然而，现有的GNN通常忽略了节点感应子图中的重要结构特征，从而限制了它们在各种下游任务中的表达能力。本文旨在通过设计一种专用的即插即用归一化方案——SUbgraph-sPEcific FactoR Embedded Normalization（SuperNorm）来加强GNN的代表性能力，该方案明确考虑了每个节点感应子图内部连接的信息。为此，我们在标准的BatchNorm开始和结束时嵌入了子图特定因子，并纳入图实例特定统计数据以提高区分能力。同时，我们提供了理论分析支持，指出通过改善的SuperNorm，任意GNN至少与1-WL测试一样能够区分非同构图。

    Graph Neural Networks~(GNNs) have emerged as a powerful category of learning architecture for handling graph-structured data. However, existing GNNs typically ignore crucial structural characteristics in node-induced subgraphs, which thus limits their expressiveness for various downstream tasks. In this paper, we strive to strengthen the representative capabilities of GNNs by devising a dedicated plug-and-play normalization scheme, termed as SUbgraph-sPEcific FactoR Embedded Normalization (SuperNorm), that explicitly considers the intra-connection information within each node-induced subgraph. To this end, we embed the subgraph-specific factor at the beginning and the end of the standard BatchNorm, as well as incorporate graph instance-specific statistics for improved distinguishable capabilities. In the meantime, we provide theoretical analysis to support that, with the elaborated SuperNorm, an arbitrary GNN is at least as powerful as the 1-WL test in distinguishing non-isomorphism gr
    
[^91]: GAN-MPC:使用非相同专家的演示训练具有参数化成本函数的模型预测控制器

    GAN-MPC: Training Model Predictive Controllers with Parameterized Cost Functions using Demonstrations from Non-identical Experts. (arXiv:2305.19111v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2305.19111](http://arxiv.org/abs/2305.19111)

    本文提出了一种新的方法，使用GAN来训练Learnable-MPC策略，以便在演示者和模仿者代理不能相同时进行参数化的成本函数学习。

    

    模型预测控制（MPC）是机器人实际应用中轨迹优化的流行方法。MPC策略可以在运动动力学和安全性约束下优化轨迹参数，并在安全保障、最优性、泛化性、可解释性和说明性方面提供保证。然而，有些行为是复杂的，手工制作MPC目标函数是困难的。一种名为可学习MPC的特殊类别的MPC策略利用来自专家演示的模仿学习解决了这个问题。然而，它们要求演示者和模仿者代理相同，这在许多机器人的实际应用中很难满足。在本文中，我们解决了当演示者和模仿者没有共享相同动力学且状态空间可能部分重叠时训练可学习MPC策略的实际问题。我们提出了一种新的方法，使用生成对抗网络（GAN）来最小化Jensen-Shannon距离来进行学习。

    Model predictive control (MPC) is a popular approach for trajectory optimization in practical robotics applications. MPC policies can optimize trajectory parameters under kinodynamic and safety constraints and provide guarantees on safety, optimality, generalizability, interpretability, and explainability. However, some behaviors are complex and it is difficult to hand-craft an MPC objective function. A special class of MPC policies called Learnable-MPC addresses this difficulty using imitation learning from expert demonstrations. However, they require the demonstrator and the imitator agents to be identical which is hard to satisfy in many real world applications of robotics. In this paper, we address the practical problem of training Learnable-MPC policies when the demonstrator and the imitator do not share the same dynamics and their state spaces may have a partial overlap. We propose a novel approach that uses a generative adversarial network (GAN) to minimize the Jensen-Shannon di
    
[^92]: 基于单调最大和的图神经网络与Datalog的对应关系研究

    On the Correspondence Between Monotonic Max-Sum GNNs and Datalog. (arXiv:2305.18015v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.18015](http://arxiv.org/abs/2305.18015)

    本文研究了数据变换基于图神经网络的表现能力，证明了单调最大和的GNN与Datalog之间的关系紧密，即任何单调最大和的GNN都可以表示为一个Datalog程序，反之亦然。

    

    虽然应用机器学习技术到结构化数据上引起了广泛的关注，但这些技术的表达能力（即可以学习到什么）仍然不太清楚。本文研究了基于图神经网络的数据变换。首先，我们注意到如何将数据集编码成GNN可处理的数字形式可以模糊模型表达能力的描述，我们认为一种规范编码提供了适当的基础。其次，我们研究了单调最大和的GNN的表现能力，它们涵盖了具有max和sum聚合函数的GNN的一个子类。我们证明了对于每个这样的GNN，可以计算出一个Datalog程序，使得将GNN应用于任何数据集都会产生与将程序规则应用于数据集的单个循环相同的事实。单调最大和的GNN能够对无限数量的特征向量求和，这可能会导致特征值任意增大，而基于规则的系统（例如Datalog）具有保证系统始终会趋于稳定状态的单调性质。我们证明了单调最大和的GNN与Datalog之间的这种关系是紧密的，即任何单调最大和的GNN都可以表示为一个Datalog程序，反之亦然。

    Although there has been significant interest in applying machine learning techniques to structured data, the expressivity (i.e., a description of what can be learned) of such techniques is still poorly understood. In this paper, we study data transformations based on graph neural networks (GNNs). First, we note that the choice of how a dataset is encoded into a numeric form processable by a GNN can obscure the characterisation of a model's expressivity, and we argue that a canonical encoding provides an appropriate basis. Second, we study the expressivity of monotonic max-sum GNNs, which cover a subclass of GNNs with max and sum aggregation functions. We show that, for each such GNN, one can compute a Datalog program such that applying the GNN to any dataset produces the same facts as a single round of application of the program's rules to the dataset. Monotonic max-sum GNNs can sum an unbounded number of feature vectors which can result in arbitrarily large feature values, whereas rul
    
[^93]: 长文本的神经自然语言处理：现状综述

    Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art. (arXiv:2305.16259v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.16259](http://arxiv.org/abs/2305.16259)

    本文简要概述了长文本的神经自然语言处理的现状，主要包括文档分类和摘要，涵盖了情感分析，同时还探讨了长文本NLP的主要挑战、问题和解决方案。

    

    在过去的十年中，深度神经网络（DNN）的采用极大地促进了自然语言处理（NLP）的发展。然而，长文本分析的需求与短文本有很大不同，而网络上传输的文档大小不断增加，使长文本的自动理解成为一项关键的研究领域。本文的两个目标是：a）概述相关的神经构建模块，作为短教程；b）总结长文本NLP的现状，主要关注两个核心任务：文档分类和文档摘要。情感分析也涵盖在内，因为它通常被视为文档分类的特例。此外，本文还讨论了长文本NLP相关的主要挑战、问题和解决方案。最后，介绍了相关的公开的注释数据集，以便促进进一步研究。

    The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural Language Processing (NLP) during the past decade. However, the demands of long document analysis are quite different from those of shorter texts, while the ever increasing size of documents uploaded on-line renders automated understanding of long texts a critical area of research. This article has two goals: a) it overviews the relevant neural building blocks, thus serving as a short tutorial, and b) it surveys the state-of-the-art in long document NLP, mainly focusing on two central tasks: document classification and document summarization. Sentiment analysis for long texts is also covered, since it is typically treated as a particular case of document classification. Additionally, this article discusses the main challenges, issues and current solutions related to long document NLP. Finally, the relevant, publicly available, annotated datasets are presented, in order to facilitate further research.
    
[^94]: PyTorch的超参数调整——面向spotPython的教程

    PyTorch Hyperparameter Tuning -- A Tutorial for spotPython. (arXiv:2305.11930v1 [cs.LG])

    [http://arxiv.org/abs/2305.11930](http://arxiv.org/abs/2305.11930)

    本文介绍了如何将spotPython超参数调谐器集成到PyTorch训练工作流中，以提高机器或深度学习模型的性能，以CIFAR10图像分类器为例。

    

    超参数调整（或超参数优化）的目标是优化超参数以提高机器或深度学习模型的性能。spotPython是知名超参数调谐器SPOT的Python版本，SPOT已经在R编程环境中为统计分析开发了十年以上。PyTorch是一种基于GPU和CPU的深度学习优化张量库。本文展示了如何将spotPython超参数调谐器集成到PyTorch训练工作流中。以CIFAR10图像分类器为例，介绍了spotPython以及与Ray Tune的简短比较。本文讨论了两种方法的优缺点。我们展示了spotPython的使用经验，以及如何使用hook在训练过程中自动调整参数。

    The goal of hyperparameter tuning (or hyperparameter optimization) is to optimize the hyperparameters to improve the performance of the machine or deep learning model. spotPython (``Sequential Parameter Optimization Toolbox in Python'') is the Python version of the well-known hyperparameter tuner SPOT, which has been developed in the R programming environment for statistical analysis for over a decade. PyTorch is an optimized tensor library for deep learning using GPUs and CPUs. This document shows how to integrate the spotPython hyperparameter tuner into the PyTorch training workflow. As an example, the results of the CIFAR10 image classifier are used. In addition to an introduction to spotPython, this tutorial also includes a brief comparison with Ray Tune, a Python library for running experiments and tuning hyperparameters. This comparison is based on the PyTorch hyperparameter tuning tutorial. The advantages and disadvantages of both approaches are discussed. We show that spotPytho
    
[^95]: HICO-DET-SG和V-COCO-SG：新的数据拆分用于评估人-物交互检测中的系统性泛化

    HICO-DET-SG and V-COCO-SG: New Data Splits to Evaluate Systematic Generalization in Human-Object Interaction Detection. (arXiv:2305.09948v1 [cs.CV])

    [http://arxiv.org/abs/2305.09948](http://arxiv.org/abs/2305.09948)

    本论文提出了两个新的HOI检测数据拆分，旨在评估系统性泛化。在新的数据拆分上测试结果表明，HOI检测模型对于未见过的对象和交互组合的泛化十分困难。

    

    人-物交互检测是一种预测图像中人与物品之间交互的任务。在实际场景中，需要对HOI检测模型进行系统性的泛化，即泛化到新的对象和交互组合上，因为训练数据仅可能涵盖所有可能组合的一小部分。然而，据我们所知，没有开放的基准测试或现有工作评估HOI检测中的系统性泛化。为解决这个问题，我们基于HICO-DET和V-COCO数据集创建了两个名为HICO-DET-SG和V-COCO-SG的新的HOI检测数据拆分。我们在新的数据拆分上评估了代表性的HOI检测模型，并观察到与原始数据集上相比测试性能有很大的降低。这个结果表明系统性泛化是HOI检测中一个具有挑战性的目标。我们希望我们的新数据拆分能够鼓励更多的研究朝着这个目标努力。

    Human-Object Interaction (HOI) detection is a task to predict interactions between humans and objects in an image. In real-world scenarios, HOI detection models are required systematic generalization, i.e., generalization to novel combinations of objects and interactions, because it is highly probable that the train data only cover a limited portion of all possible combinations. However, to our knowledge, no open benchmark or existing work evaluates the systematic generalization in HOI detection. To address this issue, we created two new sets of HOI detection data splits named HICO-DET-SG and V-COCO-SG based on HICO-DET and V-COCO datasets. We evaluated representative HOI detection models on the new data splits and observed large degradation in the test performances compared to those on the original datasets. This result shows that systematic generalization is a challenging goal in HOI detection. We hope our new data splits encourage more research toward this goal.
    
[^96]: 个性化感知的推荐系统中的LMMs模型

    PALR: Personalization Aware LLMs for Recommendation. (arXiv:2305.07622v1 [cs.IR])

    [http://arxiv.org/abs/2305.07622](http://arxiv.org/abs/2305.07622)

    本文提出了一个称为PALR的框架，将用户的历史行为与LLMs相结合，生成用户喜欢的物品的推荐。与现有的推荐方法相比，我们的PALR框架实现了最先进的性能。

    

    大型语言模型(LLMs)由于其出色的性能而受到越来越多的关注。本文提出了一种新的框架PALR，将用户的历史行为与LLMs相结合，以生成用户喜欢的物品的推荐。我们首先使用用户/物品互动作为候选检索的指导，然后采用基于LLMs的排序模型生成推荐物品。实验结果表明，与现有的推荐方法相比，我们提出的PALR框架实现了最先进的性能。

    Large language models (LLMs) have recently received significant attention for their exceptional capabilities. Despite extensive efforts in developing general-purpose LLMs that can be utilized in various natural language processing (NLP) tasks, there has been less research exploring their potential in recommender systems. In this paper, we propose a novel framework, named PALR, which aiming to combine user history behaviors (such as clicks, purchases, ratings, etc.) with LLMs to generate user preferred items. Specifically, we first use user/item interactions as guidance for candidate retrieval. Then we adopt a LLM-based ranking model to generate recommended items. Unlike existing approaches that typically adopt general-purpose LLMs for zero/few-shot recommendation testing or training on small-sized language models (with less than 1 billion parameters), which cannot fully elicit LLMs' reasoning abilities and leverage rich item side parametric knowledge, we fine-tune a 7 billion parameter
    
[^97]: 物理世界中愚弄热红外探测器

    Fooling Thermal Infrared Detectors in Physical World. (arXiv:2304.10712v1 [cs.CV])

    [http://arxiv.org/abs/2304.10712](http://arxiv.org/abs/2304.10712)

    本论文提出一种新颖的物理攻击方法——对抗性红外块（AdvIB），可以从多个角度对热成像系统执行隐蔽的黑盒攻击，成功诱导目标红外检测器在物理场景中对对象或人类进行误分类，并且不被人类察觉。

    

    红外成像系统在行人检测和自动驾驶等方面有着广泛的应用前景，并且它们的安全性能备受关注。然而，很少有研究探索了红外成像系统在真实世界环境下的安全性。过去的研究使用物理干扰，如小灯泡和热“QR代码”来攻击红外成像探测器，但这种方法很容易被察觉，缺乏隐秘性。其他研究人员使用热和冷块来欺骗红外成像探测器，但这种方法在从多个角度执行攻击方面的能力有限。为了解决这些缺点，我们提出了一种新颖的物理攻击方法，称为对抗性红外块（AdvIB）。通过优化对抗性红外块的物理参数，这种方法可以从多个角度对热成像系统执行隐蔽的黑盒攻击。我们根据其有效性、隐秘性和稳健性评估了所提出的方法。结果表明，AdvIB可以成功诱导目标红外检测器在物理场景中对对象或人类进行误分类，并且不被人类察觉。我们的工作强调了在红外成像系统中提高安全措施的必要性，特别是在真实世界环境中。

    Infrared imaging systems have a vast array of potential applications in pedestrian detection and autonomous driving, and their safety performance is of great concern. However, few studies have explored the safety of infrared imaging systems in real-world settings. Previous research has used physical perturbations such as small bulbs and thermal "QR codes" to attack infrared imaging detectors, but such methods are highly visible and lack stealthiness. Other researchers have used hot and cold blocks to deceive infrared imaging detectors, but this method is limited in its ability to execute attacks from various angles. To address these shortcomings, we propose a novel physical attack called adversarial infrared blocks (AdvIB). By optimizing the physical parameters of the adversarial infrared blocks, this method can execute a stealthy black-box attack on thermal imaging system from various angles. We evaluate the proposed method based on its effectiveness, stealthiness, and robustness. Our
    
[^98]: 可操作的自回归语言生成控制方法

    Tractable Control for Autoregressive Language Generation. (arXiv:2304.07438v1 [cs.CL])

    [http://arxiv.org/abs/2304.07438](http://arxiv.org/abs/2304.07438)

    本文提出了一种在自回归文本生成中使用可操作概率模型来强制实施限制的控制方法GeLaTo，并取得了在常见的约束文本生成测试上的最先进性能。

    

    尽管自回归大语言模型在文本生成方面取得了成功，但生成满足复杂限制的文本仍然是一个重大挑战：即使是最简单的词汇限制也使条件分布$\Pr(\text{text} | \alpha)$的采样变得不可计算。为了克服这个挑战，我们提出使用可操作的概率模型将词汇限制强加于自回归文本生成中，我们将其称为 GeLaTo。为了证明这个框架的有效性，我们使用了精简的隐马尔可夫模型来控制从GPT2到自回归的生成。GeLaTo在约束文本生成的具有挑战性的基准测试CommonGen上取得了最先进的性能，大幅击败了各种强基线。我们的工作不仅为控制大型语言模型开辟了新的途径，还激励人们开发更具表现力的可操作概率模型。

    Despite the success of autoregressive large language models in text generation, it remains a major challenge to generate text that satisfies complex constraints: sampling from the conditional distribution $\Pr(\text{text} | \alpha)$ is intractable for even the simplest lexical constraints $\alpha$. To overcome this challenge, we propose to use tractable probabilistic models to impose lexical constraints in autoregressive text generation, which we refer to as GeLaTo. To demonstrate the effectiveness of this framework, we use distilled hidden Markov models to control autoregressive generation from GPT2. GeLaTo achieves state-of-the-art performance on CommonGen, a challenging benchmark for constrained text generation, beating a wide range of strong baselines by a large margin. Our work not only opens up new avenues for controlling large language models but also motivates the development of more expressive tractable probabilistic models.
    
[^99]: 开放式智能交通基础模型挑战赛的新基准与基准数据集 - Open-TransMind

    Open-TransMind: A New Baseline and Benchmark for 1st Foundation Model Challenge of Intelligent Transportation. (arXiv:2304.06051v1 [cs.CV])

    [http://arxiv.org/abs/2304.06051](http://arxiv.org/abs/2304.06051)

    Open-TransMind是智能交通领域第一个基础模型挑战赛的新基准，旨在解决数据量少、泛化能力差以及缺乏多模态技术等典型挑战。

    

    随着近年来计算能力和深度学习算法的不断提升，基础模型越来越受欢迎。由于其强大的能力和出色的性能，这种技术被越来越多的行业采用和应用。在智能交通行业中，人工智能面临着以下典型挑战：数据量少、泛化能力差以及缺乏多模态技术。基础模型技术可以显著缓解上述问题。为解决这些问题，我们设计了第一个基础模型挑战，旨在增加基础模型技术在交通场景中的普及度，并促进智能交通行业的快速发展。该挑战分为两个赛道：全能型和跨模态图像检索。此外，我们为这两个赛道提供了一个新的基线和基准数据，称为Open-TransMind。据我们所知，这是智能交通领域的第一个基础模型基准。

    With the continuous improvement of computing power and deep learning algorithms in recent years, the foundation model has grown in popularity. Because of its powerful capabilities and excellent performance, this technology is being adopted and applied by an increasing number of industries. In the intelligent transportation industry, artificial intelligence faces the following typical challenges: few shots, poor generalization, and a lack of multi-modal techniques. Foundation model technology can significantly alleviate the aforementioned issues. To address these, we designed the 1st Foundation Model Challenge, with the goal of increasing the popularity of foundation model technology in traffic scenarios and promoting the rapid development of the intelligent transportation industry. The challenge is divided into two tracks: all-in-one and cross-modal image retrieval. Furthermore, we provide a new baseline and benchmark for the two tracks, called Open-TransMind. According to our knowledg
    
[^100]: 应用机器学习和领域知识个性化数字健康行为变革干预

    Personalizing Digital Health Behavior Change Interventions using Machine Learning and Domain Knowledge. (arXiv:2304.03392v1 [cs.LG])

    [http://arxiv.org/abs/2304.03392](http://arxiv.org/abs/2304.03392)

    该论文提出了一种采用机器学习和领域知识进行个性化数字健康行为变革干预的系统，其利用反事实例子进行特征控制以预测干预效果并优化干预效果。

    

    我们正在开发一种虚拟教练系统，帮助患者坚持行为变革干预（BCI）。我们的系统预测患者是否会执行目标行为，并使用反事实例子进行特征控制，以指导个性化BCI。我们使用具有不同响应水平的模拟患者数据评估了我们的预测模型。

    We are developing a virtual coaching system that helps patients adhere to behavior change interventions (BCI). Our proposed system predicts whether a patient will perform the targeted behavior and uses counterfactual examples with feature control to guide personalizsation of BCI. We evaluated our prediction model using simulated patient data with varying levels of receptivity to intervention.
    
[^101]: 语言模型能够解决计算机任务

    Language Models can Solve Computer Tasks. (arXiv:2303.17491v1 [cs.CL])

    [http://arxiv.org/abs/2303.17491](http://arxiv.org/abs/2303.17491)

    本文研究表明，预训练的大型语言模型代理可以通过一个简单的提示方案使用自然语言执行计算机任务，该方法取得了很好的效果并在MiniWoB++基准测试中超越了监督学习和强化学习方法。

    

    能够在计算机上执行通用任务的代理可以通过自动化重复任务和协助复杂问题的解决来提高效率和生产力。理想情况下，这些代理应该能够通过自然语言命令解决新的计算机任务。然而，先前解决这个问题的方法需要大量专家示范和任务特定的奖励函数，这两者对于新任务来说都不切实际。在这项工作中，我们展示了一个预先训练的大型语言模型（LLM）代理可以使用一个简单的提示方案（RCI），通过自然语言指导执行计算机任务，并在批评和改进输出的过程中取得很好的效果。RCI方法在自动化计算机任务方面明显优于现有的LLM方法，并在MiniWoB++基准测试中超越了监督学习（SL）和强化学习（RL）方法。RCI方法使用每个任务仅有的少数示范，与最新的SL+RL方法相竞争。

    Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent recursively criticizes and improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. RCI is competitive with the state-of-the-art SL+RL method, using only a handful of demonstrations per ta
    
[^102]: 什么让数据适合于局部连接神经网络？一种基于量子纠缠的必要且充分条件

    What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement. (arXiv:2303.11249v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.11249](http://arxiv.org/abs/2303.11249)

    本文通过采用量子物理学的理论工具，提出了一种判定数据适合于局部连接神经网络的必要且充分条件，并导出了一种相应的预处理方法。

    

    关于数据分布适用于深度学习的问题是一个基本的开放性问题。本文采用来自量子物理学的理论工具，针对包括卷积神经网络、循环神经网络和局部自注意力模型在内的广泛的局部连接神经网络，解决了这个问题。我们的主要理论结果是，在某些特征的规范划分下，当数据分布接受低量子纠缠时，特定的局部连接神经网络才能够准确地预测该数据分布。作为本结果的实际应用，我们导出了一种预处理方法，以增强数据分布适合局部连接神经网络的性能。在各种数据集上对广泛的模型进行实验，证明了我们的发现。我们希望我们使用量子纠缠将鼓励形式推理的物理工具来进一步采用。

    The question of what makes a data distribution suitable for deep learning is a fundamental open problem. Focusing on locally connected neural networks (a prevalent family of architectures that includes convolutional and recurrent neural networks as well as local self-attention models), we address this problem by adopting theoretical tools from quantum physics. Our main theoretical result states that a certain locally connected neural network is capable of accurate prediction over a data distribution if and only if the data distribution admits low quantum entanglement under certain canonical partitions of features. As a practical application of this result, we derive a preprocessing method for enhancing the suitability of a data distribution to locally connected neural networks. Experiments with widespread models over various datasets demonstrate our findings. We hope that our use of quantum entanglement will encourage further adoption of tools from physics for formally reasoning about 
    
[^103]: 中国楼宇建筑面积数据集与学习流程的构建

    Building Floorspace in China: A Dataset and Learning Pipeline. (arXiv:2303.02230v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.02230](http://arxiv.org/abs/2303.02230)

    本文构建了中国40个主要城市楼宇建筑面积数据集并利用多任务对象分割器方法学习了建筑物占地面积和高度，为城市规划提供数据支持。

    

    本文提供了中国40个主要城市楼宇建筑面积（建筑物占地面积和高度）测量的第一个里程碑。我们利用多任务对象分割器方法在同一框架中学习建筑物占地面积和高度，利用Sentinel-1和-2卫星图像作为主要数据源。本文提供了详细的数据集构建和学习流程说明。

    This paper provides a first milestone in measuring the floorspace of buildings (that is, building footprint and height) for 40 major Chinese cities. The intent is to maximize city coverage and, eventually provide longitudinal data. Doing so requires building on imagery that is of a medium-fine-grained granularity, as larger cross sections of cities and longer time series for them are only available in such format. We use a multi-task object segmenter approach to learn the building footprint and height in the same framework in parallel: (1) we determine the surface area is covered by any buildings (the square footage of occupied land); (2) we determine floorspace from multi-image representations of buildings from various angles to determine the height of buildings. We use Sentinel-1 and -2 satellite images as our main data source. The benefits of these data are their large cross-sectional and longitudinal scope plus their unrestricted accessibility. We provide a detailed description of 
    
[^104]: Alexa Arena: 一个以用户为中心的交互式平台，用于研究实体人工智能

    Alexa Arena: A User-Centric Interactive Platform for Embodied AI. (arXiv:2303.01586v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2303.01586](http://arxiv.org/abs/2303.01586)

    Alexa Arena是一个用户中心的模拟平台，具有多个房间布局和可交互对象，用于创建人机交互任务，为实体人工智能研究提供了高效的数据收集和系统评估。

    

    我们介绍了 Alexa Arena，这是一个用户中心的模拟平台，用于研究实体人工智能。Alexa Arena 提供了各种多房间布局和可交互对象，以创建人机交互任务。Alexa Arena 拥有用户友好的图形和控制机制，支持开发适用于普通人类用户的游戏化机器人任务，从而为高效的人机交互数据收集和 EAI 系统评估开辟了新的途径。同时，我们介绍了一个对话启用的指令跟随基准，并为其提供了基线结果。我们向公众提供 Alexa Arena，以促进构建具有普适性和辅助性的实体代理的研究。

    We introduce Alexa Arena, a user-centric simulation platform for Embodied AI (EAI) research. Alexa Arena provides a variety of multi-room layouts and interactable objects, for the creation of human-robot interaction (HRI) missions. With user-friendly graphics and control mechanisms, Alexa Arena supports the development of gamified robotic tasks readily accessible to general human users, thus opening a new venue for high-efficiency HRI data collection and EAI system evaluation. Along with the platform, we introduce a dialog-enabled instruction-following benchmark and provide baseline results for it. We make Alexa Arena publicly available to facilitate research in building generalizable and assistive embodied agents.
    
[^105]: 机器学习模型是否能学习从数据中推断出的统计规则？

    Do Machine Learning Models Learn Statistical Rules Inferred from Data?. (arXiv:2303.01433v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01433](http://arxiv.org/abs/2303.01433)

    本文提出了一个框架SQRL，可以无监督地从模型的训练数据中推断出统计规则；在分类、目标检测和数据填充等任务中，最先进的模型通常会违反这些规则，但是通过在测试时间内对模型进行适应，违规行为可以显著减少并提高模型的性能。

    

    机器学习模型可能会在大量数据中隐藏一些重要的错误，而这些错误通常违反了人类直觉的规则。然而，以人类知识为基础的规则往往不易扩展或正式化。因此，我们提出了一种框架SQRL，它将基于逻辑的方法与统计推断相结合，无需监督即可从模型的训练数据中推导出这些规则，并量化模型已经学习到这些规则的程度。我们进一步展示了如何在测试时间内调整模型以减少规则违规并产生更连贯的预测结果。SQRL可以在视觉、表格和语言场景下的数据集中生成多达30万条规则。我们发现最先进的分类、目标检测和数据填充模型违反了这些规则高达158K次。而测试时间的适应可以将这些违规行为减少高达68.7%，并提高相对性能高达32%。

    Machine learning models can make critical errors that are easily hidden within vast amounts of data. Such errors often run counter to rules based on human intuition. However, rules based on human knowledge are challenging to scale or to even formalize. We thereby seek to infer statistical rules from the data and quantify the extent to which a model has learned them. We propose a framework SQRL that integrates logic-based methods with statistical inference to derive these rules from a model's training data without supervision. We further show how to adapt models at test time to reduce rule violations and produce more coherent predictions. SQRL generates up to 300K rules over datasets from vision, tabular, and language settings. We uncover up to 158K violations of those rules by state-of-the-art models for classification, object detection, and data imputation. Test-time adaptation reduces these violations by up to 68.7% with relative performance improvement up to 32%. SQRL is available a
    
[^106]: 悬崖学习

    Cliff-Learning. (arXiv:2302.07348v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07348](http://arxiv.org/abs/2302.07348)

    本研究探究了基于基础模型的迁移学习在低数据状态下的数据缩放，发现了一种称为悬崖学习的现象，它反映了学习算法的先验知识与任务之间的兼容程度。

    

    我们研究了基于基础模型进行迁移学习在低下游数据状态下的数据缩放。我们观察到了一个有趣的现象，我们称之为悬崖学习。悬崖学习是指在数据缩放法则的某些区域中，性能的提升速度快于幂律速度的现象（即在对数缩放图上的凹形区域）。我们对基础模型的悬崖学习进行了深入调查并研究了这一现象的玩具模型。我们观察到悬崖学习的程度反映了学习算法的先验知识和所学任务之间的兼容程度。

    We study the data-scaling of transfer learning from foundation models in the low-downstream-data regime. We observe an intriguing phenomenon which we call cliff-learning. Cliff-learning refers to regions of data-scaling laws where performance improves at a faster than power law rate (i.e. regions of concavity on a log-log scaling plot). We conduct an in-depth investigation of foundation-model cliff-learning and study toy models of the phenomenon. We observe that the degree of cliff-learning reflects the degree of compatibility between the priors of a learning algorithm and the task being learned.
    
[^107]: 多无人机系统中的合作移动访问的量子多智能体演员-评论网络

    Quantum Multi-Agent Actor-Critic Networks for Cooperative Mobile Access in Multi-UAV Systems. (arXiv:2302.04445v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2302.04445](http://arxiv.org/abs/2302.04445)

    本文提出了一种名为量子多智能体演员-评论网络的新算法，用于自主构建一个强大的移动访问系统，利用多个无人机，并利用量子计算的原则以提高其训练过程和推理能力。

    

    本文提出了一种名为量子多智能体演员-评论网络（QMACN）的新算法，用于自主构建一个强大的移动访问系统，利用多个无人机（UAV）。在促进多个无人机之间的协作方面，多智能体强化学习（MARL）技术的应用被认为是一种有前途的方法。这些方法使无人机能够集体学习，在共享环境中优化它们的行动，最终导致更有效的合作行为。此外，我们的研究运用了量子计算（QC）的原则，以增强涉及的无人机的训练过程和推理能力。通过利用量子计算的独特计算优势，我们的方法旨在提高无人机系统的整体有效性。然而，使用QC引入可扩展性挑战，因为近中间规模量子（NISQ）限制。

    This paper proposes a novel algorithm, named quantum multi-agent actor-critic networks (QMACN) for autonomously constructing a robust mobile access system employing multiple unmanned aerial vehicles (UAVs). In the context of facilitating collaboration among multiple unmanned aerial vehicles (UAVs), the application of multi-agent reinforcement learning (MARL) techniques is regarded as a promising approach. These methods enable UAVs to learn collectively, optimizing their actions within a shared environment, ultimately leading to more efficient cooperative behavior. Furthermore, the principles of a quantum computing (QC) are employed in our study to enhance the training process and inference capabilities of the UAVs involved. By leveraging the unique computational advantages of quantum computing, our approach aims to boost the overall effectiveness of the UAV system. However, employing a QC introduces scalability challenges due to the near intermediate-scale quantum (NISQ) limitation ass
    
[^108]: 基于关系Weisfeiler-Leman的链路预测理论

    A Theory of Link Prediction via Relational Weisfeiler-Leman. (arXiv:2302.02209v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02209](http://arxiv.org/abs/2302.02209)

    本文提出了一种基于关系Weisfeiler-Leman算法的理论，为知识图谱中的图神经网络提供了理论解释，并对各种模型的表达能力进行了表征，并解释了一些广泛采用的实际设计选择的优点。

    

    图神经网络是用于图结构数据表示学习的重要模型。尽管我们已经很好地理解了这些模型在简单图上的能力和局限性，但对于知识图谱，我们的理解仍然不完整。本文的目标是为知识图谱中的图神经网络提供系统性的理解，以解决链路预测等重要任务。我们的分析涉及一种统一的视角、看似不相关的模型，并解锁了一系列其他模型。通过相应的关系Weisfeiler-Leman算法，表征了各种模型的表达能力。此分析被扩展以对图神经网络类别捕捉的函数类进行精确逻辑描述。提出的理论发现解释了一些广泛采用的实际设计选择的优点，并得到了经验验证。

    Graph neural networks are prominent models for representation learning over graph-structured data. While the capabilities and limitations of these models are well-understood for simple graphs, our understanding remains incomplete in the context of knowledge graphs. Our goal is to provide a systematic understanding of the landscape of graph neural networks for knowledge graphs pertaining to the prominent task of link prediction. Our analysis entails a unifying perspective on seemingly unrelated models and unlocks a series of other models. The expressive power of various models is characterized via a corresponding relational Weisfeiler-Leman algorithm. This analysis is extended to provide a precise logical characterization of the class of functions captured by a class of graph neural networks. The theoretical findings presented in this paper explain the benefits of some widely employed practical design choices, which are validated empirically.
    
[^109]: 重新审视 Bellman Errors 用于离线模型选择

    Revisiting Bellman Errors for Offline Model Selection. (arXiv:2302.00141v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00141](http://arxiv.org/abs/2302.00141)

    本文重新审视 Bellman Errors，发现之前的Bellman Errors 方法需要在特定条件下才能表现良好，同时提出了更准确的 MSBE 估计器，在离散控制任务方面表现出色。

    

    离线模型选择（OMS）即在只有已记录数据的情况下从众多策略中选择最佳策略，对于在实际环境下应用离线RL至关重要。一个经过广泛探讨的想法是根据相关Q函数的均方Bellman误差（MSBE）选择策略。然而，之前的研究一直在使用Bellman误差时无法获得足够的OMS性能，导致许多研究人员放弃此想法。为此，本文阐述了为什么之前的结果使用Bellman误差时会看到悲观的结果，并确定了基于Bellman误差的OMS算法将表现良好的条件。此外，我们开发了一个比之前方法更准确的MSBE的新的估计器。我们的估计器在不同的离散控制任务（包括 Atari 游戏）上获得了出色的OMS性能。

    Offline model selection (OMS), that is, choosing the best policy from a set of many policies given only logged data, is crucial for applying offline RL in real-world settings. One idea that has been extensively explored is to select policies based on the mean squared Bellman error (MSBE) of the associated Q-functions. However, previous work has struggled to obtain adequate OMS performance with Bellman errors, leading many researchers to abandon the idea. To this end, we elucidate why previous work has seen pessimistic results with Bellman errors and identify conditions under which OMS algorithms based on Bellman errors will perform well. Moreover, we develop a new estimator of the MSBE that is more accurate than prior methods. Our estimator obtains impressive OMS performance on diverse discrete control tasks, including Atari games.
    
[^110]: 平滑的非平稳连续赌博机

    Smooth Non-Stationary Bandits. (arXiv:2301.12366v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12366](http://arxiv.org/abs/2301.12366)

    本文提出了一种非平稳两臂赌博机问题的策略，能够处理平滑变化，并证明了该策略在二次Lipschitz连续的情况下的遗憾为 $\tilde O(T^{3/5})$。

    

    在许多在线决策应用中，环境都是非平稳的，因此使用能够处理变化的赌博算法至关重要。大多数现有方法是为了保护非平滑变化而设计的，仅受到总变差或时间上的Lipschitz性的限制，其中它们保证$\tilde \Theta(T^{2/3})$的遗憾。然而，在实践中，环境经常以平稳的方式改变，因此这种算法可能会在这些设置中产生比必要更高的遗憾，并且不利用变化率的信息。我们研究了一个非平稳的两臂赌博机问题，假设臂的平均回报是一个$\beta$-H\''older函数，即它是$(\beta-1)$次Lipschitz连续可微分的，我们展示了一个策略，对于$\beta=2$，它的遗憾为$\tilde O(T^{3/5})$，从而首次在平滑和非平滑之间进行了区分。我们通过一个任意$\Omg(T^{(\beta+1)/(2\beta+1)})$的下界来补充这个结果，说明了这个问题的困难程度。

    In many applications of online decision making, the environment is non-stationary and it is therefore crucial to use bandit algorithms that handle changes. Most existing approaches are designed to protect against non-smooth changes, constrained only by total variation or Lipschitzness over time, where they guarantee $\tilde \Theta(T^{2/3})$ regret. However, in practice environments are often changing {\bf smoothly}, so such algorithms may incur higher-than-necessary regret in these settings and do not leverage information on the rate of change. We study a non-stationary two-armed bandits problem where we assume that an arm's mean reward is a $\beta$-H\"older function over (normalized) time, meaning it is $(\beta-1)$-times Lipschitz-continuously differentiable. We show the first separation between the smooth and non-smooth regimes by presenting a policy with $\tilde O(T^{3/5})$ regret for $\beta=2$. We complement this result by an $\Omg(T^{(\beta+1)/(2\beta+1)})$ lower bound for any int
    
[^111]: Tracr: 编译变压器模型作为可解释性实验室

    Tracr: Compiled Transformers as a Laboratory for Interpretability. (arXiv:2301.05062v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05062](http://arxiv.org/abs/2301.05062)

    Tracr是一个编译器，将可读性强的程序编译成标准的仅解码变压器模型，该编译模型的已知结构可以用于设计实验和评估可解释方法。

    

    我们展示了如何将可读性强的程序编译成标准的仅解码变压器模型。我们的编译器Tracr生成具有已知结构的模型，可以用于设计实验。例如，我们使用它来研究执行多步算法的变压器中的“叠加”。此外，Tracr编译模型的已知结构可以作为评估可解释方法的真实基准。通常，由于变压器学习的“程序”是未知的，因此不清楚解释是否成功。我们通过实现和检查包括计算令牌频率、排序和括号检查在内的程序来演示我们的方法。我们在https://github.com/deepmind/tracr提供了Tracr的开源实现。

    We show how to "compile" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study "superposition" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the "programs" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/deepmind/tracr.
    
[^112]: 通过查询计算树优化在知识图谱上回答复杂逻辑查询

    Answering Complex Logical Queries on Knowledge Graphs via Query Computation Tree Optimization. (arXiv:2212.09567v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.09567](http://arxiv.org/abs/2212.09567)

    提出了 QTO（查询计算树优化）以有效地回答复杂逻辑查询，通过查询计算树上的正反向传播找到了精确的最优解，并利用了查询计算树中的独立性来减少搜索空间。

    

    在不完整的知识图谱上回答复杂逻辑查询是一项具有挑战性的任务，并已得到广泛研究。嵌入式方法需要训练复杂查询，并且不能很好地泛化到分布查询结构之外。最近的工作将此任务作为端到端优化问题进行了框架化，只需要预训练的链接预测器。然而，由于指数级的组合搜索空间，最优解只能被近似，限制了最终的准确性。在本文中，我们提出了 QTO（查询计算树优化），可以有效地找到精确的最优解。QTO通过在树状计算图（即查询计算树）上进行正反向传播来找到最优解。特别地，QTO利用了查询计算树中编码的独立性来减少搜索空间，在优化过程中仅涉及局部计算。在三个数据集上的实验表明，QTO获得了令人满意的结果。

    Answering complex logical queries on incomplete knowledge graphs is a challenging task, and has been widely studied. Embedding-based methods require training on complex queries, and cannot generalize well to out-of-distribution query structures. Recent work frames this task as an end-to-end optimization problem, and it only requires a pretrained link predictor. However, due to the exponentially large combinatorial search space, the optimal solution can only be approximated, limiting the final accuracy. In this work, we propose QTO (Query Computation Tree Optimization) that can efficiently find the exact optimal solution. QTO finds the optimal solution by a forward-backward propagation on the tree-like computation graph, i.e., query computation tree. In particular, QTO utilizes the independence encoded in the query computation tree to reduce the search space, where only local computations are involved during the optimization procedure. Experiments on 3 datasets show that QTO obtains sta
    
[^113]: 基于配对互补时间循环一致对抗网络的雷达降水预测方法

    PCT-CycleGAN: Paired Complementary Temporal Cycle-Consistent Adversarial Networks for Radar-Based Precipitation Nowcasting. (arXiv:2211.15046v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15046](http://arxiv.org/abs/2211.15046)

    本文提出了一种基于配对互补时间循环一致对抗网络的雷达降水预测方法，该方法包括两个生成器网络和循环一致性损失和对抗性损失。实验证明，该方法在准确性和推广能力方面优于现有的技术方法。

    

    降水预测方法已经在过去几个世纪里得到了很好的发展，因为雨水对人类生活有着至关重要的影响。现有的降水预测模型包括定量降水预测 (QPF) 模型、卷积长短期记忆 (ConvLSTM) 模型以及最新的 MetNet-2 等多种复杂的方法。本文提出了基于配对互补时间循环一致对抗网络 (PCT-CycleGAN) 的雷达降水预测方法，受对抗生成网络 (CycleGAN) 强大的图像转换性能启发。PCT-CycleGAN 使用两个具有向前和向后时间动态的生成器网络生成时序性，每个生成器网络学习一个庞大的一对一映射，以逼近表示每个方向上的时间动态的映射函数。为了创建配对互补循环之间的强健时间因果关系，我们应用了循环一致性损失和对抗性损失。广泛的实验证明，PCT-CycleGAN 在准确性和推广能力方面优于现有的技术方法。

    The precipitation nowcasting methods have been elaborated over the centuries because rain has a crucial impact on human life. Not only quantitative precipitation forecast (QPF) models and convolutional long short-term memory (ConvLSTM), but also various sophisticated methods such as the latest MetNet-2 are emerging. In this paper, we propose a paired complementary temporal cycle-consistent adversarial networks (PCT-CycleGAN) for radar-based precipitation nowcasting, inspired by cycle-consistent adversarial networks (CycleGAN), which shows strong performance in image-to-image translation. PCT-CycleGAN generates temporal causality using two generator networks with forward and backward temporal dynamics in paired complementary cycles. Each generator network learns a huge number of one-to-one mappings about time-dependent radar-based precipitation data to approximate a mapping function representing the temporal dynamics in each direction. To create robust temporal causality between paired 
    
[^114]: 基于筛选的一般方法来学习认知图的合理限制

    A Filtering-based General Approach to Learning Rational Constraints of Epistemic Graphs. (arXiv:2211.02918v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.02918](http://arxiv.org/abs/2211.02918)

    该研究提出了一种基于筛选的方法，使用多重推广步骤来学习与认知图一致的合理规则，能够学习更广泛的合理规则以反映认知图中的合理性考虑，实验发现其表现优于现有方法。

    

    认知图是概率论证的认知方法的一种推广。Hunter提出了一个二次推广框架来从众包数据中学习认知约束。然而，学习到的认知约束只反映了用户从数据中的信念，而没有考虑到认知图中编码的合理性。与此同时，当前框架只能生成反映代理人信​​任程度而不是是否相信一个论点的认知约束。为了解决这些问题，我们提出了一个基于筛选的方法，使用多重推广步骤从数据集中生成一组与其认知图一致的合理规则。这种方法能够学习更广泛的合理规则，以反映认知图中的合理性考虑。实验结果表明，我们的方法在学习合理规则的精度和多样性方面优于现有方法。

    Epistemic graphs are a generalization of the epistemic approach to probabilistic argumentation. Hunter proposed a 2-way generalization framework to learn epistemic constraints from crowd-sourcing data. However, the learnt epistemic constraints only reflect users' beliefs from data, without considering the rationality encoded in epistemic graphs. Meanwhile, the current framework can only generate epistemic constraints that reflect whether an agent believes an argument, but not the degree to which it believes in it. The major challenge to achieving this effect is that the computational complexity will increase sharply when expanding the variety of constraints, which may lead to unacceptable time performance. To address these problems, we propose a filtering-based approach using a multiple-way generalization step to generate a set of rational rules which are consistent with their epistemic graphs from a dataset. This approach is able to learn a wider variety of rational rules that reflect
    
[^115]: 图神经网络中的解释方法：一项比较研究

    Explaining the Explainers in Graph Neural Networks: a Comparative Study. (arXiv:2210.15304v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.15304](http://arxiv.org/abs/2210.15304)

    该论文研究了图神经网络中的解释方法，并在多种数据集上测试了十种解释器的表现，提供了不同GNN体系结构易解释性的关键洞察。

    

    在图神经网络的快速发展后，GNN已经在许多科学和工程领域应用广泛，这促使需要方法来理解它们的决策过程。最近几年，GNN解释器开始出现，有多种方法，一些是新颖的，一些是从其他领域改编而来的。为了整理这种海量的解释方法，一些研究在各种可解释性指标方面对不同的解释器性能进行了基准测试。然而，这些早期的工作没有尝试提供关于不同的GNN体系结构更或不易解释的洞察，也没有说明在给定环境中应该选择哪种解释器。在本次调查中，我们通过设计系统性实验研究，对八个代表性体系结构上训练的十种解释器在六个精心设计的图和节点分类数据集上进行了测试，填补了这些空白，并提供了关键的观点。

    Following a fast initial breakthrough in graph based learning, Graph Neural Networks (GNNs) have reached a widespread application in many science and engineering fields, prompting the need for methods to understand their decision process.  GNN explainers have started to emerge in recent years, with a multitude of methods both novel or adapted from other domains. To sort out this plethora of alternative approaches, several studies have benchmarked the performance of different explainers in terms of various explainability metrics. However, these earlier works make no attempts at providing insights into why different GNN architectures are more or less explainable, or which explainer should be preferred in a given setting.  In this survey, we fill these gaps by devising a systematic experimental study, which tests ten explainers on eight representative architectures trained on six carefully designed graph and node classification datasets. With our results we provide key insights on the cho
    
[^116]: 利用瓶颈适配器在低资源限制下识别临床记录中的癌症

    Using Bottleneck Adapters to Identify Cancer in Clinical Notes under Low-Resource Constraints. (arXiv:2210.09440v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.09440](http://arxiv.org/abs/2210.09440)

    本文评估了一系列机器学习技术来识别临床记录中的癌症，采用瓶颈适配器和提示微调的方法优于其它方法，可在低资源情况下使用。

    

    处理存储在临床健康记录中的信息是一项具有挑战性的任务，是生物医学自然语言处理领域的一个活跃研究领域。本文在一个含有临床记录的数据集上评估了一系列的机器学习技术，从简单的递归神经网络到专业的转换器，例如 BioBERT，并附有指示样本是否与癌症相关的一组注释。此外，我们特别采用了来自自然语言处理领域的高效微调方法，即瓶颈适配器和提示调整，以适应我们的专业任务。我们的评估表明，预训练于自然语言的冻结的基于BERT的模型，并使用瓶颈适配器微调，优于所有其他策略，包括全面微调专用的BioBERT模型。根据我们的发现，我们建议在低资源情况下使用瓶颈适配器，特别是在有限的标记数据或处理能力时，可能是生物医学文本挖掘的可行策略。

    Processing information locked within clinical health records is a challenging task that remains an active area of research in biomedical NLP. In this work, we evaluate a broad set of machine learning techniques ranging from simple RNNs to specialised transformers such as BioBERT on a dataset containing clinical notes along with a set of annotations indicating whether a sample is cancer-related or not.  Furthermore, we specifically employ efficient fine-tuning methods from NLP, namely, bottleneck adapters and prompt tuning, to adapt the models to our specialised task. Our evaluations suggest that fine-tuning a frozen BERT model pre-trained on natural language and with bottleneck adapters outperforms all other strategies, including full fine-tuning of the specialised BioBERT model. Based on our findings, we suggest that using bottleneck adapters in low-resource situations with limited access to labelled data or processing capacity could be a viable strategy in biomedical text mining. The
    
[^117]: MLink：多个领域的黑盒模型链接实现协同推理

    MLink: Linking Black-Box Models from Multiple Domains for Collaborative Inference. (arXiv:2209.13883v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.13883](http://arxiv.org/abs/2209.13883)

    本文提出了新的学习任务：模型链接，旨在通过学习不同黑盒模型输出空间之间映射的模型链接，把它们连接起来。提出了支持链接不同黑盒机器学习模型的设计，解决了分布差异挑战，开发了一个调度算法，并在多个实验数据集上实现了高效的推理结果。

    

    在现实世界的机器学习中，模型推理的成本效益对于时延敏感的任务和资源受限设备至关重要。一个典型的困境是：为了提供复杂的智能服务（如智能城市），我们需要多个机器学习模型的推理结果，但成本预算（如GPU内存）不足以运行所有模型。在这项工作中，我们研究了不同黑盒机器学习模型之间的基础关系，并提出了一种新的学习任务：模型链接，旨在通过学习它们输出空间之间的映射（称为模型链接）来连接不同黑盒模型的知识。我们提出了支持链接异构黑盒机器学习模型的模型链接设计。此外，为了解决分布差异挑战，我们提出了模型链接的适应和聚合方法。基于我们提出的模型链接，我们开发了一个调度算法，名为MLink。通过启用协作多模型推理，我们的算法在多个实验数据集上实现了高效的推理结果。

    The cost efficiency of model inference is critical to real-world machine learning (ML) applications, especially for delay-sensitive tasks and resource-limited devices. A typical dilemma is: in order to provide complex intelligent services (e.g. smart city), we need inference results of multiple ML models, but the cost budget (e.g. GPU memory) is not enough to run all of them. In this work, we study underlying relationships among black-box ML models and propose a novel learning task: model linking, which aims to bridge the knowledge of different black-box models by learning mappings (dubbed model links) between their output spaces. We propose the design of model links which supports linking heterogeneous black-box ML models. Also, in order to address the distribution discrepancy challenge, we present adaptation and aggregation methods of model links. Based on our proposed model links, we developed a scheduling algorithm, named MLink. Through collaborative multi-model inference enabled b
    
[^118]: InFi：移动端推理的资源高效性学习过程中的端到端输入过滤

    InFi: End-to-End Learning to Filter Input for Resource-Efficiency in Mobile-Centric Inference. (arXiv:2209.13873v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.13873](http://arxiv.org/abs/2209.13873)

    本研究提出了一个端到端可学习的输入过滤框架，通过对推理模型和输入过滤器的假设复杂性进行理论比较，从而了解优化潜力。该框架减少冗余，降低推理成本，并在f值、推理速度和内存占用方面超越其他方法。

    

    移动端AI应用对模型推理的资源高效性有很高的要求。输入过滤是一种有前途的方法，可以消除冗余，从而降低推理成本。以往的研究已经为许多应用程序量身定制了有效的解决方案，但留下了两个基本问题未解答：（1）推理工作量的理论可过滤性，以指导输入过滤技术的应用，从而避免资源受限的移动应用程序的试错成本；（2）特征嵌入的鲁棒性区分度，以使输入过滤对多样化推理任务和输入内容普遍有效。为了回答这些问题，我们首先形式化输入过滤问题，并在理论上比较推理模型和输入过滤器的假设复杂性，以了解优化潜力。然后我们提出了第一个端到端可学习的输入过滤框架，涵盖了大多数最先进的方法，并在f值、推理速度和内存占用方面超越了它们。

    Mobile-centric AI applications have high requirements for resource-efficiency of model inference. Input filtering is a promising approach to eliminate the redundancy so as to reduce the cost of inference. Previous efforts have tailored effective solutions for many applications, but left two essential questions unanswered: (1) theoretical filterability of an inference workload to guide the application of input filtering techniques, thereby avoiding the trial-and-error cost for resource-constrained mobile applications; (2) robust discriminability of feature embedding to allow input filtering to be widely effective for diverse inference tasks and input content. To answer them, we first formalize the input filtering problem and theoretically compare the hypothesis complexity of inference models and input filters to understand the optimization potential. Then we propose the first end-to-end learnable input filtering framework that covers most state-of-the-art methods and surpasses them in f
    
[^119]: 数字音频取证：盲目检测人类语音模仿

    Digital Audio Forensics: Blind Human Voice Mimicry Detection. (arXiv:2209.12573v4 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2209.12573](http://arxiv.org/abs/2209.12573)

    本文介绍了一种利用深度学习方法，通过盲目检测输入音频的真实性，可以有效应对音频欺诈问题的分类器。而这种分类器不需要任何参考，能够在没有真实来源的情况下检测出模仿音频。

    

    音频是人类交流中使用最广泛的方式之一，但同时也很容易被误用来欺骗人们。随着人工智能的革命，相关技术现在对几乎所有人都可用，这使得犯罪和伪造变得更加简单。本篇论文介绍了一种深度学习方法，开发了一个分类器，可以盲目分类输入音频为真实或者模仿；“盲目”指的是能够在没有参考或真实来源的情况下检测仿制音频的能力。所提出的模型是在一个大型音频数据集中提取的一组重要特征上进行训练的，以得到一个分类器，该分类器被用于测试不同音频的相同特征集。数据提取自两个原始数据集，特别为这项工作而编写;一个全英文数据集和一个混合数据集（阿拉伯语加英语）。这些数据集已通过GitHub以原始形式提供给研究社区，网址为https://github.com/SaSs7/Datas

    Audio is one of the most used ways of human communication, but at the same time it can be easily misused to trick people. With the revolution of AI, the related technologies are now accessible to almost everyone thus making it simple for the criminals to commit crimes and forgeries. In this work, we introduce a deep learning method to develop a classifier that will blindly classify an input audio as real or mimicked; the word 'blindly' refers to the ability to detect mimicked audio without references or real sources. The proposed model was trained on a set of important features extracted from a large dataset of audios to get a classifier that was tested on the same set of features from different audios. The data was extracted from two raw datasets, especially composed for this work; an all English dataset and a mixed dataset (Arabic plus English). These datasets have been made available, in raw form, through GitHub for the use of the research community at https://github.com/SaSs7/Datas
    
[^120]: Z-Code++：一种针对抽象文本摘要优化的预训练语言模型

    Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization. (arXiv:2208.09770v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.09770](http://arxiv.org/abs/2208.09770)

    本文介绍了一种新的预训练语言模型Z-Code++，它使用两种预训练阶段和三种技术进行优化，其中包括解耦的注意力层和融合编码方法。该模型在抽象文本摘要任务上优于其他模型，是一种高效的参数化模型。

    

    本文介绍了Z-Code++，一种新的针对抽象文本摘要优化的预训练语言模型。该模型扩展了最先进的编码器-解码器模型，运用了三种技术。首先，我们使用了两阶段的预训练过程，以提高模型在低资源摘要任务上的性能。该模型首先使用文本语料库进行语言理解的预训练，然后在摘要语料库上进行连续的预训练以提高其基于文本生成的能力。其次，我们用解耦的注意力层取代编码器中的自注意力层，其中每个单词分别使用两个向量来表示其内容和位置。第三，我们使用编码器中的融合编码方法，以一种分层的方式对长序列进行编码。Z-Code++在5种语言的13个文本摘要任务中有9个取得了最新的最优效果。我们的模型在参数效率方面表现出色，在XSum数据集上的性能超过了比其大600倍的PaLM-540B，以及比其大200倍的FeBERT。

    This paper presents Z-Code++, a new pre-trained language model optimized for abstractive text summarization. The model extends the state of the art encoder-decoder model using three techniques. First, we use a two-phase pre-training process to improve model's performance on low-resource summarization tasks. The model is first pre-trained using text corpora for language understanding, and then is continually pre-trained on summarization corpora for grounded text generation. Second, we replace self-attention layers in the encoder with disentangled attention layers, where each word is represented using two vectors that encode its content and position, respectively. Third, we use fusion-in-encoder, a simple yet effective method of encoding long sequences in a hierarchical manner. Z-Code++ creates new state of the art on 9 out of 13 text summarization tasks across 5 languages. Our model is parameter-efficient in that it outperforms the 600x larger PaLM-540B on XSum, and the finetuned 200x l
    
[^121]: SSIVD-Net：一种新的武器化暴力显著超级图像分类和检测技术

    SSIVD-Net: A Novel Salient Super Image Classification & Detection Technique for Weaponized Violence. (arXiv:2207.12850v6 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.12850](http://arxiv.org/abs/2207.12850)

    本文提出了一种名为SSIVD-Net的新技术，用于暴力识别任务。通过使用显著-超级图像表示减少了3D视频数据的复杂性，提高了推断、性能和可解释性。作者还提出了一种新颖的架构“Salient-Classifier”，将核方法和残差学习策略相结合。该方法在多个数据集上表现良好。

    

    在闭路电视（CCTV）监控录像中检测暴力和武器化暴力需要一个全面的方法。本文引入了“智慧城市CCTV暴力检测（SCVD）”数据集，旨在促进对监控视频中武器分布的学习。为了解决分析3D监控视频进行暴力识别任务的复杂性，我们提出了一种新技术，称为SSIVD-Net（用于暴力检测的显著-超级-图像），通过使用显著-超级图像表示减少3D视频数据复杂性、降维和信息损失，同时提高推断、性能和可解释性。考虑到未来智慧城市的可扩展性和可持续性要求，作者提出了一种新颖的架构“Salient-Classifier”，将核方法和残差学习策略相结合。我们评估了SSIVD-Net在SCVD、Hockey Fight、Moviescope以及Large-Scale Fight Detection数据集上的性能，并与现有算法进行了比较。

    Detection of violence and weaponized violence in closed-circuit television (CCTV) footage requires a comprehensive approach. In this work, we introduce the \emph{Smart-City CCTV Violence Detection (SCVD)} dataset, specifically designed to facilitate the learning of weapon distribution in surveillance videos. To tackle the complexities of analyzing 3D surveillance video for violence recognition tasks, we propose a novel technique called, \emph{SSIVD-Net} (\textbf{S}alient-\textbf{S}uper-\textbf{I}mage for \textbf{V}iolence \textbf{D}etection). Our method reduces 3D video data complexity, dimensionality, and information loss while improving inference, performance, and explainability through the use of Salient-Super-Image representations. Considering the scalability and sustainability requirements of futuristic smart cities, the authors introduce the \emph{Salient-Classifier}, a novel architecture combining a kernelized approach with a residual learning strategy. We evaluate variations of
    
[^122]: 跨模态因果关系推理在事件级视觉问答中的应用

    Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering. (arXiv:2207.12647v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.12647](http://arxiv.org/abs/2207.12647)

    本篇论文提出了一个新型的事件级视觉问答框架——跨模态因果关系推理（CMCIR），通过引入因果干预方法，发现视觉和语言模态的真正因果结构，实现强健的因果感知视觉语言问答。

    

    现有的视觉问答方法往往捕捉跨模态的伪相关性，而未能发现真正的因果机制，以真实地基于主导视觉证据和问题意图进行推理。此外，现有方法通常忽略了跨模态事件级理解，需要联合建模事件的时间性、因果性和动态性。在本文中，我们从新的角度，即跨模态因果关系推理，聚焦于事件级视觉问答，引入因果干预方法来发现视觉和语言模态的真正因果结构。具体而言，我们提出了一个名为跨模态因果关系推理（CMCIR）的新型事件级视觉问答框架，以实现强健的因果感知视觉语言问答。为了发现跨模态因果结构，我们提出了因果感知视觉语言推理（CVLR）模块，用于共同对视觉和语言模态建模。

    Existing visual question answering methods tend to capture the cross-modal spurious correlations and fail to discover the true causal mechanism that facilitates reasoning truthfully based on the dominant visual evidence and the question intention. Additionally, the existing methods usually ignore the cross-modal event-level understanding that requires to jointly model event temporality, causality, and dynamics. In this work, we focus on event-level visual question answering from a new perspective, i.e., cross-modal causal relational reasoning, by introducing causal intervention methods to discover the true causal structures for visual and linguistic modalities. Specifically, we propose a novel event-level visual question answering framework named Cross-Modal Causal RelatIonal Reasoning (CMCIR), to achieve robust causality-aware visual-linguistic question answering. To discover cross-modal causal structures, the Causality-aware Visual-Linguistic Reasoning (CVLR) module is proposed to co
    
[^123]: 一种基于上下文敏感的单词嵌入方法用于检测恶意推文

    A Context-Sensitive Word Embedding Approach for The Detection of Troll Tweets. (arXiv:2207.08230v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.08230](http://arxiv.org/abs/2207.08230)

    本研究开发了一种基于上下文敏感的单词嵌入方法用于自动检测 troll 推文，结果表明采用ELMo和BERT嵌入方法的性能更好，最佳表现方法为基于ELMo的架构，采用了一个GRU分类器，具有0.929的AUC得分。

    

    本研究旨在通过开发和评估一组模型架构来自动检测 troll 推文，从而解决社交媒体上滋扰行为日趋严重的问题。我们利用深度学习技术和预训练的单词嵌入方法，如BERT、ELMo和GloVe，使用分类准确度、F1得分、AUC和精确度等指标评估了每个架构的性能。结果表明，BERT和ELMo嵌入方法表现优于GloVe方法，可能是因为它们能够提供更好地捕捉在线社交媒体语言使用细微差别的上下文化单词嵌入。此外，我们还发现CNN和GRU编码器在F1分数和AUC方面表现相似，表明它们在从输入文本中提取相关信息方面具有有效性。最佳表现方法是基于ELMo的架构，采用了一个GRU分类器，具有0.929的AUC得分。

    In this study, we aimed to address the growing concern of trolling behavior on social media by developing and evaluating a set of model architectures for the automatic detection of troll tweets. Utilizing deep learning techniques and pre-trained word embedding methods such as BERT, ELMo, and GloVe, we evaluated the performance of each architecture using metrics such as classification accuracy, F1 score, AUC, and precision. Our results indicate that BERT and ELMo embedding methods performed better than the GloVe method, likely due to their ability to provide contextualized word embeddings that better capture the nuances and subtleties of language use in online social media. Additionally, we found that CNN and GRU encoders performed similarly in terms of F1 score and AUC, suggesting their effectiveness in extracting relevant information from input text. The best-performing method was found to be an ELMo-based architecture that employed a GRU classifier, with an AUC score of 0.929. This r
    
[^124]: 减少资源受限对比图像-字幕检索中的预测特征抑制

    Reducing Predictive Feature Suppression in Resource-Constrained Contrastive Image-Caption Retrieval. (arXiv:2204.13382v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2204.13382](http://arxiv.org/abs/2204.13382)

    本文提出了一种名为潜在目标解码（LTD）的方法，可以在资源受限的情况下减少预测特征抑制，从而为对比图像-字幕检索（ICR）方法提供了一种解决方案。

    

    对于训练图像-字幕检索（ICR）方法，对比损失函数是优化函数的常见选择。然而，对比ICR方法容易受到预测特征抑制的影响。预测特征是正确指示查询和候选项之间相似性的特征。然而，在训练过程中存在多个预测特征时，编码器模型往往会抑制冗余的预测特征，因为这些特征不需要学习区分正面和负面对。虽然有些预测特征在训练过程中是冗余的，但在评估过程中可能是有用的。我们提出了一种减少资源受限ICR方法中预测特征抑制的方法：潜在目标解码（LTD）。我们在对比ICR框架中添加了一个额外的解码器，以在通用句子编码器的潜在空间中重建输入字幕，从而防止图像和字幕编码器在不匹配的负面对中抑制预测特征。我们在Flikr30k和MS COCO数据集上验证了LTD，并表明它比资源受限场景中的基线方法改进了性能。

    To train image-caption retrieval (ICR) methods, contrastive loss functions are a common choice for optimization functions. Unfortunately, contrastive ICR methods are vulnerable to predictive feature suppression. Predictive features are features that correctly indicate the similarity between a query and a candidate item. However, in the presence of multiple predictive features during training, encoder models tend to suppress redundant predictive features, since these features are not needed to learn to discriminate between positive and negative pairs. While some predictive features are redundant during training, these features might be relevant during evaluation. We introduce an approach to reduce predictive feature suppression for resource-constrained ICR methods: latent target decoding (LTD). We add an additional decoder to the contrastive ICR framework, to reconstruct the input caption in a latent space of a general-purpose sentence encoder, which prevents the image and caption encod
    
[^125]: 一种基于仿真集成的生物启发式搜索测试方法在ADAS案例研究中的应用

    Machine Learning Testing in an ADAS Case Study Using Simulation-Integrated Bio-Inspired Search-Based Testing. (arXiv:2203.12026v3 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2203.12026](http://arxiv.org/abs/2203.12026)

    本文提出一种基于仿真集成的生物启发式搜索测试方法Deeper，用于生成用于测试基于深度神经网络的车道保持系统的故障发现测试场景，通过实证评估和与竞赛中的其他工具的比较展示了其性能的提高。

    

    本文介绍Deeper的扩展版本，它是一种基于搜索实现的仿真集成测试解决方案，用于生成用于测试基于深度神经网络的车道保持系统的故障发现测试场景。在新版本中，我们利用了一组新的生物启发式搜索算法-遗传算法（GA）、（μ+λ）和（μ，λ）进化策略（ES）以及粒子群优化（PSO），这些算法利用质量种子种群以及为建模测试场景使用的特定领域交叉和突变操作。为了展示Deeper中新测试生成器的能力，我们进行了实证评估，并与SBST 2021的五个参赛工具的结果进行了比较。我们的评估结果表明，在新版本中，Deeper中的新测试生成器不仅在以前的版本上有了很大提升，而且...

    This paper presents an extended version of Deeper, a search-based simulation-integrated test solution that generates failure-revealing test scenarios for testing a deep neural network-based lane-keeping system. In the newly proposed version, we utilize a new set of bio-inspired search algorithms, genetic algorithm (GA), $({\mu}+{\lambda})$ and $({\mu},{\lambda})$ evolution strategies (ES), and particle swarm optimization (PSO), that leverage a quality population seed and domain-specific cross-over and mutation operations tailored for the presentation model used for modeling the test scenarios. In order to demonstrate the capabilities of the new test generators within Deeper, we carry out an empirical evaluation and comparison with regard to the results of five participating tools in the cyber-physical systems testing competition at SBST 2021. Our evaluation shows the newly proposed test generators in Deeper not only represent a considerable improvement on the previous version but also 
    
[^126]: 实体对齐的知识图谱嵌入方法：一个实验性综述

    Knowledge Graph Embedding Methods for Entity Alignment: An Experimental Review. (arXiv:2203.09280v2 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2203.09280](http://arxiv.org/abs/2203.09280)

    本文对36篇顶级会议上发表的实体对齐嵌入方法进行了统计和实验性分析，为实体对齐的知识图谱嵌入方法提供了全面的综述，包括这些方法的优点、缺点和适用性并确定了未来研究的挑战和方向。

    

    近年来，我们目睹了知识图谱在各个领域的广泛应用，旨在支持问答、推荐等应用。将不同知识图谱中的知识整合的一个常见任务是找到哪些子图引用了同一个现实世界的实体。最近，嵌入方法已被用于实体对齐任务，学习用于保留原始知识图谱中实体相似度的向量空间表示。人们提出了各种受监督、无监督和半监督的方法，这些方法利用了知识图谱中实体的事实（基于属性）和结构信息（基于关系）。然而，根据不同性能指标和知识图谱特征，在现实世界的知识图谱中对它们的优势和劣势进行定量评估的文献尚缺失。在本文中，我们进行了首次对流行的实体对齐嵌入方法进行的元分析，这基于了36篇发表在顶级会议上的论文，并进行了统计和实验性分析。我们的结果提供了对于实体对齐嵌入方法的现状综述，以及它们在不同知识图谱场景下的优点、缺点和适用性。此外，我们还确定并分析了未来研究中的挑战和研究方向。

    In recent years, we have witnessed the proliferation of knowledge graphs (KG) in various domains, aiming to support applications like question answering, recommendations, etc. A frequent task when integrating knowledge from different KGs is to find which subgraphs refer to the same real-world entity. Recently, embedding methods have been used for entity alignment tasks, that learn a vector-space representation of entities which preserves their similarity in the original KGs. A wide variety of supervised, unsupervised, and semi-supervised methods have been proposed that exploit both factual (attribute based) and structural information (relation based) of entities in the KGs. Still, a quantitative assessment of their strengths and weaknesses in real-world KGs according to different performance metrics and KG characteristics is missing from the literature. In this work, we conduct the first meta-level analysis of popular embedding methods for entity alignment, based on a statistically sou
    
[^127]: 政策优化中的不变性及奖励学习中的部分可识别性

    Invariance in Policy Optimisation and Partial Identifiability in Reward Learning. (arXiv:2203.07475v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.07475](http://arxiv.org/abs/2203.07475)

    本文探讨了奖励学习中奖励函数的部分可识别性，并分析了这种部分可识别性对政策优化等下游任务的影响。同时提出了一个框架，对比奖励学习的数据源和下游任务，以其不变性为依据，对奖励学习的数据源的设计和选择产生影响。

    

    对于复杂的现实任务，手动设计奖励函数通常是非常具有挑战性的。为了解决这个问题，可以使用奖励学习从数据中推断奖励函数。然而，即使在无限数据的情况下，通常也会有多个奖励函数可以很好地拟合数据。这意味着奖励函数只能被部分地识别。在这项工作中，我们正式描述了在几种流行的奖励学习数据源（包括专家演示和轨迹比较）下奖励函数的部分可识别性。我们还分析了这种部分可识别性对于几项下游任务（例如政策优化）的影响。我们在一个框架中统一了我们的结果，该框架通过其不变性对比数据源和下游任务，并对奖励学习的数据源的设计和选择产生影响。

    It is often very challenging to manually design reward functions for complex, real-world tasks. To solve this, one can instead use reward learning to infer a reward function from data. However, there are often multiple reward functions that fit the data equally well, even in the infinite-data limit. This means that the reward function is only partially identifiable. In this work, we formally characterise the partial identifiability of the reward function given several popular reward learning data sources, including expert demonstrations and trajectory comparisons. We also analyse the impact of this partial identifiability for several downstream tasks, such as policy optimisation. We unify our results in a framework for comparing data sources and downstream tasks by their invariances, with implications for the design and selection of data sources for reward learning.
    
[^128]: 连续时间和状态下的时间差分学习（随机场景中）

    Temporal Difference Learning with Continuous Time and State in the Stochastic Setting. (arXiv:2202.07960v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.07960](http://arxiv.org/abs/2202.07960)

    该论文提出了两种连续时间策略评估问题的时间差分学习方法，并证明了它们的理论收敛速度。同时，这些方法还可以解释为解决线性PDE或线性BSDE的新型强化学习方法。

    

    我们考虑连续时间策略评估的问题。这意味着通过观察来学习与未受控制的连续时间随机动态和奖励函数相关联的价值函数。我们提出了两种使用逐渐减少的时间步长的著名TD（0）方法的原始变体。一种是无模型的，另一种是基于模型的。对于两种方法，我们证明了理论收敛速度，并通过数值模拟进行了验证。或者，这些方法可以解释为近似解决线性PDE（偏微分方程）或线性BSDE（反向随机微分方程）的新型强化学习方法。

    We consider the problem of continuous-time policy evaluation. This consists in learning through observations the value function associated with an uncontrolled continuous-time stochastic dynamic and a reward function. We propose two original variants of the well-known TD(0) method using vanishing time steps. One is model-free and the other is model-based. For both methods, we prove theoretical convergence rates that we subsequently verify through numerical simulations. Alternatively, those methods can be interpreted as novel reinforcement learning approaches for approximating solutions of linear PDEs (partial differential equations) or linear BSDEs (backward stochastic differential equations).
    
[^129]: HeterPS：基于强化学习调度的异构环境下分布式深度学习

    HeterPS: Distributed Deep Learning With Reinforcement Learning Based Scheduling in Heterogeneous Environments. (arXiv:2111.10635v3 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2111.10635](http://arxiv.org/abs/2111.10635)

    这篇论文介绍了一个名为Paddle-HeterPS的分布式框架，基于强化学习的调度方法可以高效地利用多种类型的计算资源，解决了分布式深度学习训练中多层次分配计算资源的问题。

    

    深度神经网络利用许多层和大量参数实现了优秀的性能。DNN模型的训练过程通常处理具有许多稀疏特征的大规模输入数据，这会产生高延迟和I/O成本，而某些层的计算成本很高。训练过程通常利用分布式计算资源来减少训练时间。此外，多种类型的计算资源，如CPU和GPU等，也可用于分布式训练过程。因此，多层次地分配计算资源对训练过程至关重要。为了通过异构计算资源高效地训练DNN模型，我们提出了一种分布式框架Paddle-Heterogeneous Parameter Server（Paddle-HeterPS），由分布式架构和基于强化学习的调度方法组成。与现有框架相比，Paddle-HeterPS的优点有三个。

    Deep neural networks (DNNs) exploit many layers and a large number of parameters to achieve excellent performance. The training process of DNN models generally handles large-scale input data with many sparse features, which incurs high Input/Output (IO) cost, while some layers are compute-intensive. The training process generally exploits distributed computing resources to reduce training time. In addition, heterogeneous computing resources, e.g., CPUs, GPUs of multiple types, are available for the distributed training process. Thus, the scheduling of multiple layers to diverse computing resources is critical for the training process. To efficiently train a DNN model using the heterogeneous computing resources, we propose a distributed framework, i.e., Paddle-Heterogeneous Parameter Server (Paddle-HeterPS), composed of a distributed architecture and a Reinforcement Learning (RL)-based scheduling method. The advantages of Paddle-HeterPS are three-fold compared with existing frameworks. 
    
[^130]: 医学视觉问答综述

    Medical Visual Question Answering: A Survey. (arXiv:2111.10056v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2111.10056](http://arxiv.org/abs/2111.10056)

    本综述总结了医学视觉问答的相关数据集、方法、创新和挑战，并探讨了未来的研究方向。

    

    医学视觉问答（VQA）是医学人工智能和流行VQA挑战相结合的一种方法。给定一张医学图像和一个自然语言相关的临床问题，医学VQA系统应该预测一个合理且有说服力的答案。虽然广泛研究了一般领域的VQA，但由于其任务特性，医学VQA仍需要具有特定的调查和探索。在本综述的第一部分中，我们汇总并讨论了迄今为止公开可用的医学VQA数据集，包括数据源、数据数量和任务特征。在第二部分中，我们回顾了用于医学VQA任务的方法。我们总结并讨论了它们的技术、创新和潜在改进。在最后一部分中，我们分析了该领域中的一些医学特定挑战，并讨论了未来的研究方向。我们的目标是为对医学视觉问答领域感兴趣的研究人员提供全面有用的信息。

    Medical Visual Question Answering~(VQA) is a combination of medical artificial intelligence and popular VQA challenges. Given a medical image and a clinically relevant question in natural language, the medical VQA system is expected to predict a plausible and convincing answer. Although the general-domain VQA has been extensively studied, the medical VQA still needs specific investigation and exploration due to its task features. In the first part of this survey, we collect and discuss the publicly available medical VQA datasets up-to-date about the data source, data quantity, and task feature. In the second part, we review the approaches used in medical VQA tasks. We summarize and discuss their techniques, innovations, and potential improvements. In the last part, we analyze some medical-specific challenges for the field and discuss future research directions. Our goal is to provide comprehensive and helpful information for researchers interested in the medical visual question answeri
    
[^131]: 使用机器教学研究教授强化学习者时人类的假设

    Using Machine Teaching to Investigate Human Assumptions when Teaching Reinforcement Learners. (arXiv:2009.02476v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2009.02476](http://arxiv.org/abs/2009.02476)

    本文研究了在线奖励和惩罚方法下，人们对于学习者的期望假设，发现人们假设学习者具有高的折扣率和高度重视探索，并根据学习者进展调整教学策略。

    

    为成功教学，需要对学习者学习方式进行假设，即学习者如何使用来自世界的经验来更新其内部状态。本文研究了在线奖励和惩罚方法下，人们对于学习者的期望假设。研究重点是一种常见的强化学习方法 Q-learning，通过行为实验考察人们的假设。为了达到此目的，我们首先建立了一个规范标准，将问题形式化为机器教学优化问题。为了解决机器教学优化问题，我们使用深度学习逼近方法来模拟学习者在环境中的表现，并学习预测反馈如何影响学习者的内部状态。在教授理想化的探索利用任务时，人们对学习者的学习和折扣率有哪些假设？在行为实验中，我们发现人们可以相对高效和准确地教导 Q-学习者这项任务。人们倾向于假设学习者具有高的折扣率，并高度重视探索。此外，人们会根据学习者的进展调整自己的教学策略。

    Successful teaching requires an assumption of how the learner learns - how the learner uses experiences from the world to update their internal states. We investigate what expectations people have about a learner when they teach them in an online manner using rewards and punishment. We focus on a common reinforcement learning method, Q-learning, and examine what assumptions people have using a behavioral experiment. To do so, we first establish a normative standard, by formulating the problem as a machine teaching optimization problem. To solve the machine teaching optimization problem, we use a deep learning approximation method which simulates learners in the environment and learns to predict how feedback affects the learner's internal states. What do people assume about a learner's learning and discount rates when they teach them an idealized exploration-exploitation task? In a behavioral experiment, we find that people can teach the task to Q-learners in a relatively efficient and 
    

