# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Synthesizing Moving People with 3D Control.](http://arxiv.org/abs/2401.10889) | 本文提出了一种基于扩散模型的框架，用于从单张图像中生成具有逼真移动的人物动画，并成功处理了人体不可见部分的合成问题。 |
| [^2] | [SCENES: Subpixel Correspondence Estimation With Epipolar Supervision.](http://arxiv.org/abs/2401.10886) | SCENES提出了一种使用极线监督的亚像素对应关系估计方法，通过仅使用相机姿态信息而不需要3D结构，以放宽对数据集的特征要求。 |
| [^3] | [Reinforcement learning for question answering in programming domain using public community scoring as a human feedback.](http://arxiv.org/abs/2401.10882) | 本研究通过整合强化学习和Stack Overflow评分，提高了GPT Neo在编程问答中的性能，同时指出传统语言度量方法在编程领域的局限性。 |
| [^4] | [Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning.](http://arxiv.org/abs/2401.10862) | 本文研究了剪枝对齐的LLMs的保护措施，发现剪枝LLM参数可以显著增强其抵抗“越狱”提示攻击的能力，并且对其他LLM行为也可能有更普遍的效果。同时，引入了一个有害任务数据集，证明剪枝有助于集中注意力在与任务相关的标记上。突出的聊天模型表现出很高的易感性。 |
| [^5] | [Advancements in eHealth Data Analytics through Natural Language Processing and Deep Learning.](http://arxiv.org/abs/2401.10850) | 通过自然语言处理和深度学习在eHealth数据分析中的进展，可以有效处理和解释医疗领域的大量文本数据，从而提高医疗服务和整个医疗领域的效率和知识水平。 |
| [^6] | [Source-Free and Image-Only Unsupervised Domain Adaptation for Category Level Object Pose Estimation.](http://arxiv.org/abs/2401.10848) | 本研究提出了一种无源和仅图像无监督领域适应的方法，用于类别级目标姿态估计。通过利用物体特定子部分在跨域情景中的稳定性，我们可以有效地更新模型。使用简单的立方体网格和基于神经特征激活的生成模型，我们可以在没有3D或深度数据的情况下适应充满干扰的目标领域。 |
| [^7] | [Using LLMs to discover emerging coded antisemitic hate-speech emergence in extremist social media.](http://arxiv.org/abs/2401.10841) | 这项研究提出了一种方法，可以检测新出现的编码恶意术语，为极端社交媒体中的反犹太恶意言论提供了解决方案。 |
| [^8] | [Symbolic Cognitive Diagnosis via Hybrid Optimization for Intelligent Education Systems.](http://arxiv.org/abs/2401.10840) | 本文提出了一种符号认知诊断（SCD）框架，通过利用符号树明确表示学生与练习互动，以及梯度优化方法学习参数，同时提高了泛化能力和可解释性。 |
| [^9] | [Holonic Learning: A Flexible Agent-based Distributed Machine Learning Framework.](http://arxiv.org/abs/2401.10839) | Holonic Learning (HoL) is a flexible and privacy-focused learning framework designed for training deep learning models. It leverages holonic concepts to establish a structured self-similar hierarchy, allowing more nuanced control over collaborations. HoL has extensive design and flexibility potentials. |
| [^10] | [Understanding Video Transformers via Universal Concept Discovery.](http://arxiv.org/abs/2401.10831) | 本文研究了视频Transformer的可解释性问题，引入了视频Transformer概念发现算法来解释其决策过程，并揭示了时空推理机制和对象为中心的表示。 |
| [^11] | [Optimisation in Neurosymbolic Learning Systems.](http://arxiv.org/abs/2401.10819) | 神经符号学习系统的优化研究了如何将深度学习与符号推理相结合，并探讨了模糊推理和概率推理在神经符号学习中的应用，得到了一些令人惊讶的结果，如与乌鸦悖论的联系。 |
| [^12] | [Co-Pilot for Health: Personalized Algorithmic AI Nudging to Improve Health Outcomes.](http://arxiv.org/abs/2401.10816) | 该研究通过使用基于图神经网络的推荐系统和来自可穿戴设备的健康行为数据，设计并实施了一个人工智能驱动平台，实现了个性化和情境引导，能够提高参与者的日常活动水平和中等至剧烈运动时长。 |
| [^13] | [Learning to Visually Connect Actions and their Effects.](http://arxiv.org/abs/2401.10805) | 该论文提出了视觉连接动作和其效果的概念（CATE），用于视频理解。研究表明，不同的任务形式产生了捕捉直观动作特性的表示，但模型表现不佳，人类的表现明显优于它们。该研究为未来的努力奠定了基础，并希望能激发出高级形式和模型的灵感。 |
| [^14] | [Metric Dynamic Equilibrium Logic.](http://arxiv.org/abs/2401.10781) | 该论文提出了度量动态平衡逻辑，它是一种基于ASP的方法，用于指定定性和定量的动态约束。它是Equilibrium Logic的最通用的时间扩展之一。 |
| [^15] | [Interactions with Prompt Problems: A New Way to Teach Programming with Large Language Models.](http://arxiv.org/abs/2401.10759) | 本文提出了一种新的编程教学方法，即使用提示问题，以与大型语言模型进行交互。学生通过视觉方式获得一个问题，并将其转换为LLMs能够理解的提示，以生成能够通过所有测试用例的代码。 |
| [^16] | [BoolGebra: Attributed Graph-learning for Boolean Algebraic Manipulation.](http://arxiv.org/abs/2401.10753) | BoolGebra是一种方法，使用属性图学习和神经网络来改进布尔代数操作的逻辑综合。它通过减小搜索空间并高效定位优化空间，实现了跨设计推论的通用性和规模化潜力。 |
| [^17] | [EFO: the Emotion Frame Ontology.](http://arxiv.org/abs/2401.10751) | 本文提出了一种情绪框架本体(EFO)，它将情绪视为语义框架，并使用一组语义角色来捕捉情绪经验的不同方面。EFO采用基于模式的本体论设计，并与DOLCE基本本体论对齐，可以用于建模多种情绪理论，并在Emotion Ontology Network中交叉链接。本文以Ekman的基本情绪(BE)理论作为EFO-BE模块进行示例，并演示了如何对表示情绪情况进行自动推理。通过从Framester知识库中提取BE情绪框架对EFO-BE进行了评估。 |
| [^18] | [Multimodal Sentiment Analysis with Missing Modality: A Knowledge-Transfer Approach.](http://arxiv.org/abs/2401.10747) | 本文提出了一种知识迁移方法，用于在缺失模态下进行多模态情感分析。通过翻译不同模态之间的内容以重构缺失的音频模态，并利用跨模态注意机制进行情感预测，实验证明了该方法在多个数据集上表现出显著的改进和与完整多模态监督方法相媲美的效果。 |
| [^19] | [A Systematic Evaluation of Euclidean Alignment with Deep Learning for EEG Decoding.](http://arxiv.org/abs/2401.10746) | 本研究系统评估了使用深度学习和欧几里得对齐对脑电解码的影响。结果表明，欧几里得对齐能够显著提高解码率，并且减少了收敛时间。 |
| [^20] | [Ethical Artificial Intelligence Principles and Guidelines for the Governance and Utilization of Highly Advanced Large Language Models.](http://arxiv.org/abs/2401.10745) | 本文探讨了如何利用道德人工智能原则和指南来解决高级大型语言模型的治理和利用问题。 |
| [^21] | [FinLLMs: A Framework for Financial Reasoning Dataset Generation with Large Language Models.](http://arxiv.org/abs/2401.10744) | FinLLMs是一种基于大型语言模型的金融推理数据集生成框架，利用常见金融公式生成金融问答数据，旨在解决有限数据资源和高昂的手工注释成本问题。 |
| [^22] | [Dynamic Q&A of Clinical Documents with Large Language Models.](http://arxiv.org/abs/2401.10733) | 本研究介绍了一种使用大型语言模型进行临床文档动态问答的自然语言接口。通过Langchain和Transformer-based LLMs驱动的聊天机器人，用户可以用自然语言查询临床笔记并获得相关答案。实验结果显示Wizard Vicuna具有出色的准确性，但计算要求较高。模型优化方案提高了约48倍的延迟。然而，模型产生幻象和多样化医疗案例评估的限制仍然存在。解决这些挑战对于发掘临床笔记的价值和推进基于AI的临床决策至关重要。 |
| [^23] | [Proceedings 14th International Conference on Automated Deduction in Geometry.](http://arxiv.org/abs/2401.10725) | 第14届几何自动推理国际会议（ADG）在塞尔维亚贝尔格莱德举办，重点关注教育中的推理。特邀报告嘉宾分别探讨了几何的形式化、算术化和自动化，以及双曲几何的自动化、形式化和可视化。 |
| [^24] | [Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge.](http://arxiv.org/abs/2401.10712) | 本论文提出了一种叫做Q&A提示的方法，通过挖掘图像中的问题-回答对来发现丰富的视觉线索，以帮助AI模型更好地理解复杂视觉问题，提高跨模态推理能力。 |
| [^25] | [Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering.](http://arxiv.org/abs/2401.10711) | 本论文提出了一种使用大型多模型的弱监督高斯对比基础模型来处理视频问答问题的方法。通过将问题和答案对作为事件描述，找到多个关键帧作为目标时刻，并利用这些时刻作为伪标签来强制LMMs进行推理。所提出的方法使用轻量级的基于高斯的对比基础模块（GCG）来学习时效结构。 |
| [^26] | [Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model.](http://arxiv.org/abs/2401.10700) | 本论文提出了一种安全离线强化学习方法，通过可行性导向的扩散模型来平衡安全约束满足、奖励最大化和离线数据集的行为规范化。通过可达性分析，将硬安全约束转化为在离线数据集中识别最大可行区域。 |
| [^27] | [Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models.](http://arxiv.org/abs/2401.10690) | 本研究引入EAUC作为一种新的度量标准，用以揭示迪亚德回归模型中隐藏的偏见和不公平问题。传统的全局错误度量标准如RMSE和MAE无法捕捉到这种问题。 |
| [^28] | [Towards End-to-End GPS Localization with Neural Pseudorange Correction.](http://arxiv.org/abs/2401.10685) | 本论文提出了一个端到端的GPS定位框架E2E-PrNet，通过直接训练神经网络PrNet来进行伪距修正，实验结果表明其优于现有端到端GPS定位方法。 |
| [^29] | [A Simple Framework to Accelerate Multilingual Language Model for Monolingual Text Generation.](http://arxiv.org/abs/2401.10660) | 这项研究介绍了一种新颖的框架，旨在加速非英语语言的文本生成。通过预测更大的语言单元并针对目标语言进行调整，该框架降低了解码步骤的数量，并将生成速度提高了1.9倍。 |
| [^30] | [A Comprehensive Survey on Deep-Learning-based Vehicle Re-Identification: Models, Data Sets and Challenges.](http://arxiv.org/abs/2401.10643) | 本文通过全面调查了基于深度学习的车辆再识别方法，包括分类、数据集、评估标准以及未来的挑战和研究方向 |
| [^31] | [Fast Butterfly-Core Community Search For Large Labeled Graphs.](http://arxiv.org/abs/2401.10642) | 本文提出了一个基于蝶形核社区结构的快速社区搜索模型，通过随机游走与重启算法和蝶形度来评估社区内顶点的重要性，并通过更高效的顶点距离更新方法来提高操作效率，从而有效地在大型图中寻找密集连接的子图。 |
| [^32] | [An Effective Index for Truss-based Community Search on Large Directed Graphs.](http://arxiv.org/abs/2401.10641) | 该论文提出了一种基于D-truss的索引方法，在大型有向图上实现了高效的社区搜索，解决了现有方法消耗过多计算资源的问题。 |
| [^33] | [A comprehensive study on fidelity metrics for XAI.](http://arxiv.org/abs/2401.10640) | 这项研究提出了一种验证可解释人工智能方法忠诚度的方法，并通过应用于两个不同实验中的现有度量进行了评估，成为这些度量的客观基准，超越了现有方法。 |
| [^34] | [ZnTrack -- Data as Code.](http://arxiv.org/abs/2401.10603) | ZnTrack是一个Python驱动的数据版本控制工具，通过简化大型数据集为Python脚本的形式，提供了一个以数据为代码的概念，实现了对参数跟踪、工作流设计以及数据存储和共享的用户友好界面。 |
| [^35] | [Rethinking the Soft Conflict Pseudo Boolean Constraint on MaxSAT Local Search Solvers.](http://arxiv.org/abs/2401.10589) | 本论文提出将软冲突伪布尔约束转化为局部搜索方法的子句加权系统，并提出了一种自适应子句加权策略。基于这些方法，提出了一种名为SPB-MaxSAT的新型局部搜索算法，为MaxSAT局部搜索解算器的子句加权提供了新的视角。 |
| [^36] | [PuriDefense: Randomized Local Implicit Adversarial Purification for Defending Black-box Query-based Attacks.](http://arxiv.org/abs/2401.10586) | PuriDefense是一种高效的防御机制，通过使用轻量级净化模型进行随机路径净化，减缓基于查询的攻击的收敛速度，并有效防御黑盒基于查询的攻击。 |
| [^37] | [CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents.](http://arxiv.org/abs/2401.10568) | CivRealm是一个受文明游戏启发的环境，要求代理人在复杂的情境中进行学习和推理，以应对变化的游戏规则和随机环境。 |
| [^38] | [OrchMoE: Efficient Multi-Adapter Learning with Task-Skill Synergy.](http://arxiv.org/abs/2401.10559) | OrchMoE通过利用模块化技能架构和自动任务识别，提升了参数效率微调领域的性能，实现了对多任务学习的重大进展。 |
| [^39] | [AAT: Adapting Audio Transformer for Various Acoustics Recognition Tasks.](http://arxiv.org/abs/2401.10544) | AAT提出了一种基于Adapter微调的高效微调方法，能够在不牺牲模型的通用性的情况下获得下游任务的知识。 |
| [^40] | [Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences.](http://arxiv.org/abs/2401.10529) | Mementos是一个新的基准测试，旨在评估多模态大型语言模型在图像序列推理中的能力。研究发现，现有的MLLM在准确描述图像序列的动态信息方面存在困难，容易导致物体及其行为的错误描述或错觉。 |
| [^41] | [Cross-lingual Editing in Multilingual Language Models.](http://arxiv.org/abs/2401.10521) | 本论文介绍了跨语言模型编辑（XME）范式，通过在一种语言中编辑事实并观察其对其他语言的更新传播，研究了多语言语言模型中的模型编辑技术（MET）的性能限制。 |
| [^42] | [Episodic Reinforcement Learning with Expanded State-reward Space.](http://arxiv.org/abs/2401.10516) | 通过引入扩展的状态-奖励空间，我们提出了一种高效的情节式强化学习（DRL）框架，可以改善DRL中状态与奖励空间之间的不对齐问题，从而提高值的估计准确性和策略性能。 |
| [^43] | [A match made in consistency heaven: when large language models meet evolutionary algorithms.](http://arxiv.org/abs/2401.10510) | 大型语言模型和进化算法的结合具有强大的一致性，包括标记嵌入和基因型-表现型映射、位置编码和适应性塑造、位置嵌入和选择、注意力和交叉、前馈神经网络和突变、模型训练和参数更新以及多任务学习和多目标优化等多个核心特征。本文分析了现有的耦合研究，并为未来的研究提供了基本路线和关键挑战。 |
| [^44] | [FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial Analysis.](http://arxiv.org/abs/2401.10506) | 本论文提出了一种面向金融分析的基于LLMs的模型无关的文本到SQL框架FinSQL，同时还提供了一个实用的金融分析文本到SQL基准数据集BULL，从提示构造和参数化的角度为金融文本到SQL提供了系统化的处理。 |
| [^45] | [Causal Layering via Conditional Entropy.](http://arxiv.org/abs/2401.10495) | 本文提出了一种通过条件熵Oracle来恢复图层析的方法，用于离散分布情况下的因果发现。算法通过比较节点的条件熵和噪声的无条件熵，通过删除源或汇点来实现节点的分离。算法具有可证明的正确性和二次时间复杂度。 |
| [^46] | [Enhancing Scalability in Recommender Systems through Lottery Ticket Hypothesis and Knowledge Distillation-based Neural Network Pruning.](http://arxiv.org/abs/2401.10484) | 本研究通过将彩票票据假设和知识蒸馏框架相结合，提出了一种剪枝神经网络的创新方法，以解决推荐系统在边缘设备上的可扩展性问题。经实验证明，该方法能够有效地降低功耗和模型尺寸，并实现高达66.67%的GPU计算功率减少。此外，本研究还首次应用了彩票票据假设和知识蒸馏技术于推荐系统领域，对该领域作出了重要贡献。 |
| [^47] | [Escape Sky-high Cost: Early-stopping Self-Consistency for Multi-step Reasoning.](http://arxiv.org/abs/2401.10480) | 本文提出了一种称为提前停止自一致性（ESC）的简单且可扩展的采样策略，用于降低多步推理任务中自一致性的成本，通过大量实验证明了其有效性。 |
| [^48] | [LDReg: Local Dimensionality Regularized Self-Supervised Learning.](http://arxiv.org/abs/2401.10474) | 本文提出了一种叫做LDReg的本地维度正则化方法，用于解决自监督学习中的维度坍缩问题。通过增加局部内在维度，LDReg能够改善表示的性能。 |
| [^49] | [DeepEdit: Knowledge Editing as Decoding with Constraints.](http://arxiv.org/abs/2401.10471) | DeepEdit是一种神经符号方法，通过更好的推理一致性和对更新知识的意识，提高了大型语言模型的知识编辑能力，对多跳问题数据集MQuaKE取得了显著的进展。 |
| [^50] | [Learning Backdoors for Mixed Integer Programs with Contrastive Learning.](http://arxiv.org/abs/2401.10467) | 本论文提出了使用对比学习方法来学习混合整数规划的后门，通过收集用于训练的后门并训练图注意力网络模型来预测后门，取得了比Gurobi和先前模型更好的性能改进。 |
| [^51] | [Critical Data Size of Language Models from a Grokking Perspective.](http://arxiv.org/abs/2401.10463) | 本文从理解的角度探讨了语言模型中的关键数据规模，证明了只有当语言模型达到关键大小时才会发生泛化，同时揭示了更大的模型需要更多数据的趋势。 |
| [^52] | [Investigating Training Strategies and Model Robustness of Low-Rank Adaptation for Language Modeling in Speech Recognition.](http://arxiv.org/abs/2401.10447) | 本研究研究了语音识别中低秩适应的训练策略和模型鲁棒性。通过引入不同的LoRA训练策略，实现了相对词错误率的降低，并研究了模型对输入扰动的稳定性。实验结果表明，高级LoRA变体导致了某些扰动的性能下降。 |
| [^53] | [Large Language Models are Efficient Learners of Noise-Robust Speech Recognition.](http://arxiv.org/abs/2401.10446) | 本文通过引入噪声信息作为条件器，并从N-best列表中提取语言空间噪声嵌入，教会了大型语言模型（LLMs）进行噪声去除，从而实现了噪声鲁棒语音识别的生成式错误纠正（GER）。 |
| [^54] | [Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?.](http://arxiv.org/abs/2401.10444) | 本文讨论了如何通过引入计算认知架构来改进现有的LLMs，并强调了双过程架构和混合神经符号方法的重要性。同时，文章也指出需要改革计算认知架构以适应人工智能和计算技术的进步。总体而言，本文主张采用多学科、互惠互利的方法来开发更好的AI模型和理解人类心灵。 |
| [^55] | [Learning a Prior for Monte Carlo Search by Replaying Solutions to Combinatorial Problems.](http://arxiv.org/abs/2401.10431) | 本论文提出了一种通过统计已解决问题信息来自动计算先验知识的方法，该方法适用于多个困难的组合问题，并且可以显著提高搜索结果。 |
| [^56] | [Generalized Nested Rollout Policy Adaptation with Limited Repetitions.](http://arxiv.org/abs/2401.10420) | 本研究通过限制重复次数，避免了在广义嵌套展开策略自适应中出现重复的最佳序列，提高了算法在不同组合问题上的性能。 |
| [^57] | [Can Large Language Model Summarizers Adapt to Diverse Scientific Communication Goals?.](http://arxiv.org/abs/2401.10415) | 本研究探讨了大型语言模型在科学摘要任务中的可控性。通过控制风格特征，非微调的语言模型在评论生成任务中优于人类，同时基于关键词的引导可以改善模型的可控性。然而，模型在生成长摘要和高度抽象的简化摘要方面有限。总体而言，大型语言模型在摘要任务中表现出强大的通用能力，但在复杂控制方面有限。 |
| [^58] | [Distribution Consistency based Self-Training for Graph Neural Networks with Sparse Labels.](http://arxiv.org/abs/2401.10394) | 本文提出了一种基于分布一致性的自训练方法，用于解决图神经网络中少样本节点分类的挑战，通过消除训练集和测试集之间的分布转移，提高自训练的有效性。 |
| [^59] | [Catastrophic Interference is Mitigated in Naturalistic Power-Law Learning Environments.](http://arxiv.org/abs/2401.10393) | 本研究在自然学习环境中通过回忆方法减轻了灾难性干扰，该方法受到功率法则的启发。 |
| [^60] | [Agricultural Object Detection with You Look Only Once (YOLO) Algorithm: A Bibliometric and Systematic Literature Review.](http://arxiv.org/abs/2401.10379) | YOLO算法在农业物体检测中具有先进的性能特点，可实现实时检测、准确性好，并被广泛应用于各种农业任务。本研究通过文献综述对YOLO在农业领域的进展和应用进行了全面记录和评价。 |
| [^61] | [MutaBot: A Mutation Testing Approach for Chatbots.](http://arxiv.org/abs/2401.10372) | MutaBot是一种用于聊天机器人的变异测试工具，可以在多个层级上对对话流程、意图和上下文进行变异。它为开发者提供了针对聊天机器人的特定故障类型的支持，是一个有效评估测试套件有效性的工具。 |
| [^62] | [Hierarchical Federated Learning in Multi-hop Cluster-Based VANETs.](http://arxiv.org/abs/2401.10361) | 本文介绍了一种在多跳基于集群的车联网中的分层联邦学习（HFL）框架，该框架利用平均相对速度和余弦相似度对FL模型参数进行加权组合以作为聚类度量，以解决数据多样性和高车辆移动性带来的挑战。 |
| [^63] | [Keeping Deep Learning Models in Check: A History-Based Approach to Mitigate Overfitting.](http://arxiv.org/abs/2401.10359) | 这篇论文提出了一种简单而强大的基于历史的方法来检测和防止深度学习模型的过拟合问题，为软件工程中深度学习模型的使用提供了质量和可靠性的保证。 |
| [^64] | [Exploring General Intelligence via Gated Graph Transformer in Functional Connectivity Studies.](http://arxiv.org/abs/2401.10348) | 本文引入了门控图变换器（GGT）框架，用来预测基于功能连接性（FC）的认知指标，并通过费城神经发育队列（PNC）实证验证了其优越的预测能力，进一步凸显了其在识别与人类认知过程相关联的关键神经连接中的潜力。 |
| [^65] | [ELRT: Efficient Low-Rank Training for Compact Convolutional Neural Networks.](http://arxiv.org/abs/2401.10341) | 本文提出了ELRT，一种高效低秩训练解决方案，可用于高准确性、高紧凑性的低秩卷积神经网络模型。与现有解决方案相比，ELRT具有较少的准确性下降和不需要更新全尺寸模型的优势。 |
| [^66] | [Noise Contrastive Estimation-based Matching Framework for Low-resource Security Attack Pattern Recognition.](http://arxiv.org/abs/2401.10337) | 该论文提出了一种基于噪声对比估计的低资源安全攻击模式识别匹配框架，通过直接语义相似度决定文本与攻击模式之间的关联，以降低大量类别、标签分布不均和标签空间复杂性带来的学习难度。 |
| [^67] | [DrugAssist: A Large Language Model for Molecule Optimization.](http://arxiv.org/abs/2401.10334) | DrugAssist是一个交互式分子优化模型，通过人机对话实现优化，利用LLM的强交互性和泛化能力，在药物发现中取得了领先的结果。 |
| [^68] | [Improving One-class Recommendation with Multi-tasking on Various Preference Intensities.](http://arxiv.org/abs/2401.10316) | 本研究提出了一个多任务框架，考虑了隐式反馈中不同偏好强度的情况，并在其中引入了注意力图卷积层来探索高阶关系。这使得表示更具鲁棒性和泛化性。 |
| [^69] | [LangProp: A code optimization framework using Language Models applied to driving.](http://arxiv.org/abs/2401.10314) | LangProp是一种用于自动驾驶的代码优化框架，利用语言模型迭代优化生成的代码。它通过评估代码性能和捕捉异常来改进生成的代码，展示了在CARLA中实现自动驾驶的概念验证。 |
| [^70] | [Mathematical Algorithm Design for Deep Learning under Societal and Judicial Constraints: The Algorithmic Transparency Requirement.](http://arxiv.org/abs/2401.10310) | 这篇论文探讨了在社会和司法约束下，对深度学习进行数学算法设计的挑战。研究者提出了算法透明性的要求，并使用数学框架来分析在计算模型中实现透明实施的可行性。 |
| [^71] | [On the Readiness of Scientific Data for a Fair and Transparent Use in Machine Learning.](http://arxiv.org/abs/2401.10304) | 本研究分析了科学数据文档如何满足机器学习社区和监管机构对其在机器学习技术中使用的需求，并提出了一套建议指南。 |
| [^72] | [A Hierarchical Framework with Spatio-Temporal Consistency Learning for Emergence Detection in Complex Adaptive Systems.](http://arxiv.org/abs/2401.10300) | 本研究提出了一个分层框架，通过学习系统和代理的表示，使用时空一致性学习来捕捉复杂适应性系统中的现象，并解决了现有方法不能捕捉空间模式和建模非线性关系的问题。 |
| [^73] | [An attempt to generate new bridge types from latent space of generative flow.](http://arxiv.org/abs/2401.10299) | 本研究通过归一化流生成潜在空间中的新桥梁类型，并解决了高维矩阵行列式计算和神经网络可逆变换的挑战。 |
| [^74] | [Design and development of opto-neural processors for simulation of neural networks trained in image detection for potential implementation in hybrid robotics.](http://arxiv.org/abs/2401.10289) | 本论文设计了一种光电神经处理器，用于模拟经图像检测训练过的生物神经网络在混合机器人中的应用。通过光遗传学实现精确激活的反向传播STDP算法，实现了与传统神经网络训练算法相媲美的准确度。 |
| [^75] | [Top in Chinese Data Processing: English Code Models.](http://arxiv.org/abs/2401.10286) | 在中文数据处理中，基于代码的语言模型在非编程中文任务中表现出色，尤其是在对中文幻觉敏感的任务中。此研究为讨论“中文房间”思想实验提供了独特的视角。 |
| [^76] | [MorpheusNet: Resource efficient sleep stage classifier for embedded on-line systems.](http://arxiv.org/abs/2401.10284) | MorpheusNet是一个资源效率的睡眠阶段分类器，可以在嵌入式系统上实时预测睡眠阶段，适用于边缘计算，消耗少量能源。 |
| [^77] | [BioDiffusion: A Versatile Diffusion Model for Biomedical Signal Synthesis.](http://arxiv.org/abs/2401.10282) | BioDiffusion是一种用于生物医学信号合成的多功能扩散模型，能够产生高保真度、非稳态的多变量信号。通过利用这些合成的信号，可以有效解决生物医学信号机器学习任务中的数据不足、数据不平衡和标签复杂性等问题，提高准确性。 |
| [^78] | [GANs for EVT Based Model Parameter Estimation in Real-time Ultra-Reliable Communication.](http://arxiv.org/abs/2401.10280) | 本文提出了一种将极值理论（EVT）和生成对抗网络（GANs）相结合的新方法，用于在实时环境中实现超可靠通信中精确的信道建模。通过使用GPD进行极端事件的分布建模，并使用GANs来估计GPD的参数，该方法与传统的GAN配置不同之处在于在GAN结构中增加了一个模块，直接估计GPD的参数。 |
| [^79] | [A systematic review of geospatial location embedding approaches in large language models: A path to spatial AI systems.](http://arxiv.org/abs/2401.10279) | 这篇论文系统综述了在大型语言模型中的地理位置嵌入方法，提出了四种主要的嵌入主题，并强调了在空间形态和生成模态方面进一步发展的需求。 |
| [^80] | [EEGFormer: Towards Transferable and Interpretable Large-Scale EEG Foundation Model.](http://arxiv.org/abs/2401.10278) | 本论文提出了一种名为EEGFormer的脑电基础模型，通过在大规模的复合EEG数据上进行预训练，该模型能够学习到可迁移和可解释的通用表示，为脑电信号的分析提供了有力的工具。 |
| [^81] | [Knowledge-Assisted Dual-Stage Evolutionary Optimization of Large-Scale Crude Oil Scheduling.](http://arxiv.org/abs/2401.10274) | 基于知识辅助的双阶段进化优化方法用于解决大规模原油调度问题，通过全局搜索和局部优化改进传统优化方法的效率和性能。 |
| [^82] | [Revolutionizing Pharma: Unveiling the AI and LLM Trends in the Pharmaceutical Industry.](http://arxiv.org/abs/2401.10273) | 这篇论文批判性概述了制药行业中人工智能的新兴趋势和重大进展，特别强调了机器学习算法等先进人工智能技术在制药运营的贡献。 |
| [^83] | [Multi-Source Collaborative Gradient Discrepancy Minimization for Federated Domain Generalization.](http://arxiv.org/abs/2401.10272) | 本文提出了一种名为联合多源梯度差异最小化的方法，用于联邦域泛化。该方法通过域内和域间梯度匹配的方式，减少了来自不同源域的数据隔离所带来的域差距，从而使得学习的模型能够在未知域上具有良好的泛化能力。 |
| [^84] | [The complementary contributions of academia and industry to AI research.](http://arxiv.org/abs/2401.10268) | 工业界的研究团队在人工智能研究中有更高的关注度和引用率，且更有可能产生最先进的模型。而学术界的团队则更倾向于产生具有更高程度创新的工作，出现非常不寻常和典型的论文。这种影响力-创新度优势在不同领域、团队规模、资历和声望下均存在。 |
| [^85] | [HyperSense: Accelerating Hyper-Dimensional Computing for Intelligent Sensor Data Processing.](http://arxiv.org/abs/2401.10267) | HyperSense是一个协同设计的硬件和软件系统，能够根据传感器数据中的物体存在预测有效地控制数据生成速率，通过使用低精度ADC减少冗余数据降低机器学习系统的成本，利用超维度计算的特点分析实时的低精度传感器数据，在处理噪声、以内存为中心和实时学习方面具有优势。该系统结合了高性能的物体检测软件和实时的硬件预测，引入了智能传感器控制的新概念。在软件和硬件评估中表现出卓越的性能。 |
| [^86] | [Intelligent Condition Monitoring of Industrial Plants: An Overview of Methodologies and Uncertainty Management Strategies.](http://arxiv.org/abs/2401.10266) | 本论文综述了工业厂房智能状态监测和故障检测和诊断方法，重点关注了Tennessee Eastman Process。调研总结了最流行和最先进的深度学习和机器学习算法，并探讨了算法的优劣势。还讨论了不平衡数据和无标记样本等挑战，以及深度学习模型如何应对。比较了不同算法在Tennessee Eastman Process上的准确性和规格。 |
| [^87] | [Harnessing Transparent Learning Analytics for Individualized Support through Auto-detection of Engagement in Face-to-Face Collaborative Learning.](http://arxiv.org/abs/2401.10264) | 这篇论文介绍了一种透明的学习分析方法，通过自动检测个体学生在面对面协作学习中的参与度，为个性化支持提供了可能性。 |
| [^88] | [Null Space Properties of Neural Networks with Applications to Image Steganography.](http://arxiv.org/abs/2401.10262) | 本文研究了神经网络的零空间特性，发现了神经网络的固有弱点，并针对此弱点提出了一种图像隐写术方法。 |
| [^89] | [Zero Bubble Pipeline Parallelism.](http://arxiv.org/abs/2401.10241) | 本论文介绍了一种新的调度策略，在同步训练语义下成功实现了零管道泡沫，通过将反向计算分为两部分，设计了优于基线方法的新颖管道调度。此外，还提出了一种自动找到最优调度的算法，并引入新颖技术绕过同步操作实现零泡沫。实验证明，在相似条件下，本方法在吞吐量方面优于1F1B调度23%。 |
| [^90] | [Imitation Learning Inputting Image Feature to Each Layer of Neural Network.](http://arxiv.org/abs/2401.09691) | 本文提出了一种在模仿学习中解决多模态数据处理挑战的方法，通过将数据输入到每个神经网络层中，放大与期望输出的相关性较低的数据的影响，并通过实验证明了成功率的显著提高。 |
| [^91] | [Aligning Large Language Models with Counterfactual DPO.](http://arxiv.org/abs/2401.09566) | 本文研究了在大型语言模型中使用反事实对抗优化框架，以实现风格对齐，避免人类干预，并成功培养出可取行为和减轻不可取行为。 |
| [^92] | [CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational AutoEncoder.](http://arxiv.org/abs/2401.08897) | CFASL是一种用于解缠学习的新方法，它将对称性学习与VAE集成，无需任何数据集因子信息的先验知识，具有三个新特征：对齐潜在向量维度到可学习对称代码簿中的对称性，学习复合对称性来表达未知因素的变化，以及引入群等变编码器和解码器来训练VAE。 |
| [^93] | [RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning.](http://arxiv.org/abs/2401.08326) | RoTBench是一个多级基准，用于评估大型语言模型在工具学习中的鲁棒性。研究发现，LLMs在真实世界的噪声下表现出的稳定性需得到提高。 |
| [^94] | [INACIA: Integrating Large Language Models in Brazilian Audit Courts: Opportunities and Challenges.](http://arxiv.org/abs/2401.05273) | 本文介绍了INACIA系统，这是一个将大型语言模型整合到巴西审计法院中的系统，可以自动化案件分析的各个阶段，并展示了其在从案件文件中提取信息、评估合法性和生成司法建议方面的潜力。 |
| [^95] | [Deep Efficient Private Neighbor Generation for Subgraph Federated Learning.](http://arxiv.org/abs/2401.04336) | 本文提出了FedDEP，用于解决子图联邦学习中的信息传播不完整的问题，并提出了一系列新颖的技术设计，包括深度邻居生成和高效的私密领域生成。 |
| [^96] | [A Community Detection and Graph Neural Network Based Link Prediction Approach for Scientific Literature.](http://arxiv.org/abs/2401.02542) | 本研究提出了一种将社区检测和图神经网络相结合的创新方法，以增强科学文献网络中的链接预测能力。通过将社区检测的结果与图神经网络模型结合，我们突破了链接预测中的可扩展性和分辨率限制，提高了预测准确性。 |
| [^97] | [Diffusion Model with Perceptual Loss.](http://arxiv.org/abs/2401.00110) | 本研究介绍了一种使用感知损失的扩散模型，通过无分类器指导实现了生成更真实样本的目的。 |
| [^98] | [Domain Generalization with Vital Phase Augmentation.](http://arxiv.org/abs/2312.16451) | 本论文介绍了一种使用有限变化的输入数据相位而不是固定相位的方法来解决领域泛化中的相位波动问题，并提出了一种根据领域不变特征程度区分相位的方法。 |
| [^99] | [Rethinking Dimensional Rationale in Graph Contrastive Learning from Causal Perspective.](http://arxiv.org/abs/2312.10401) | 本论文从因果角度重新思考图对比学习中的维度理论，并提出在图中捕捉维度理论的方法，以改善性能，并解决图模型学习中的问题。以上方法在实验中得到验证。 |
| [^100] | [Weathering Ongoing Uncertainty: Learning and Planning in a Time-Varying Partially Observable Environment.](http://arxiv.org/abs/2312.03263) | 本研究结合了时变马尔科夫决策过程和部分可观测性，提出了在不确定、随机和时变环境中进行学习和规划的方法。通过记忆优先状态估计和规划策略的集成，我们实现了对长期奖励的优化，在仿真和硬件验证中取得了良好的结果。 |
| [^101] | [A ripple in time: a discontinuity in American history.](http://arxiv.org/abs/2312.01185) | 该论文通过使用向量嵌入和非线性降维方法，发现GPT-2与UMAP的结合可以提供更好的分离和聚类效果。同时，经过微调的DistilBERT模型可用于识别总统和演讲的年份。 |
| [^102] | [Adaptive Image Registration: A Hybrid Approach Integrating Deep Learning and Optimization Functions for Enhanced Precision.](http://arxiv.org/abs/2311.15497) | 本研究提出了一种自适应图像配准的混合方法，将深度学习和优化函数相结合，通过使用学习方法的输出作为优化的初始参数，并在计算上重点处理损失最大的图像对，取得了1.6%的性能提升和1.0%的变形场平滑度提升。 |
| [^103] | [INTERVENOR: Prompt the Coding Ability of Large Language Models with the Interactive Chain of Repairing.](http://arxiv.org/abs/2311.09868) | INTERVENOR模型通过模拟人类修复代码的行为，使用交互式修复链条来引导大型语言模型的编码能力，取得了显著的性能提升。 |
| [^104] | [A Foundation Graph Model.](http://arxiv.org/abs/2311.03976) | 本文提出了一个基于对抗性对比学习的基础图模型FoToM，该模型通过节点和边特征排除进行图预训练，在多个领域上实现了正向迁移，并取得了显著的性能提升。 |
| [^105] | [SAGE: Smart home Agent with Grounded Execution.](http://arxiv.org/abs/2311.00772) | SAGE框架通过替换手动定义的推理逻辑，实现了基于实际执行的智能家居助手，提高了灵活性。它可以学习用户偏好，与设备交互，监视设备，并理解自然的设备引用。在评估中，SAGE在43个智能家居任务中成功完成了23个任务，优于现有的LLM基准。 |
| [^106] | [Towards Robust Offline Reinforcement Learning under Diverse Data Corruption.](http://arxiv.org/abs/2310.12955) | 本文研究了在多种数据损坏情况下，离线强化学习算法的性能。研究发现，隐式Q-learning（IQL）在各种离线强化学习算法中展现出了较强的鲁棒性能，其采用的监督策略学习方案为关键。然而，在动力学损坏下，IQL仍然存在Q函数的重尾目标问题。 |
| [^107] | [How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition.](http://arxiv.org/abs/2310.05492) | 本研究探讨了大规模语言模型在监督微调过程中，特别是数学推理和代码生成能力方面，数据组合的影响。实验结果显示，较大模型在相同数据量下表现出更好的性能，通过增加微调数据和模型参数，数学推理和代码生成能力得到显著提升。 |
| [^108] | [MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain Everyday Tasks.](http://arxiv.org/abs/2310.04965) | 该论文提出了一个新的基准挑战MultiScript，旨在解决现有脚本学习方法对于开放领域日常任务的限制。论文介绍了两个多模式脚本学习任务，并提供了对应的输入和输出要求。 |
| [^109] | [LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models.](http://arxiv.org/abs/2309.14393) | 本研究提出了LLMCarbon，一个针对密集型和MoE LLMs设计的端到端碳足迹预测模型，解决了现有工具的限制，并显著提升了估计的准确性。 |
| [^110] | [Exploring Self-Reinforcement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models.](http://arxiv.org/abs/2309.10444) | 本文提出了一个自我强化大型语言模型框架，自动生成和评估学生生成的解释，用于改进学生资源共享中学生生成的多项选择题的解释质量。 |
| [^111] | [How Transferable are Attribute Controllers on Pretrained Multilingual Translation Models?.](http://arxiv.org/abs/2309.08565) | 本文研究了如何将预训练的多语言翻译模型中的属性控制器迁移到没有监督数据的语言。通过全面分析不同数据场景下的训练和推断时控制技术，揭示了它们在零样本性能和领域鲁棒性上的相对优势和劣势。 |
| [^112] | [Automated Test Case Generation Using Code Models and Domain Adaptation.](http://arxiv.org/abs/2308.08033) | 本研究提出了一个完全自动化的测试框架，利用开发人员编写的测试和可用的代码模型生成可编译、易读的单元测试。 |
| [^113] | [GBSD: Generative Bokeh with Stage Diffusion.](http://arxiv.org/abs/2306.08251) | 本文提出了第一个基于生成对抗网络技术的文本到图像模型GBSD，并使用阶段扩散技术合成具有虚化风格的照片。 |
| [^114] | [Synapse: Leveraging Few-Shot Exemplars for Human-Level Computer Control.](http://arxiv.org/abs/2306.07863) | 本文探究了使用大型语言模型提示少量示例来实现人类级别的计算机控制；通过分解演示、过滤状态并重新构造任务描述，示例检索等步骤，Synapse 具备了适应多任务、泛化多环境的能力。 |
| [^115] | [Onboard Science Instrument Autonomy for the Detection of Microscopy Biosignatures on the Ocean Worlds Life Surveyor.](http://arxiv.org/abs/2304.13189) | 在太阳系的冰卫星寻找外星生命，需要用一套补充仪器对多个独立的生物标志进行采样。机载科学仪器自主性（OSIA）是一种新兴学科，可以评估、总结和优先考虑观测仪器数据以最大化科学回报。 |
| [^116] | [Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method.](http://arxiv.org/abs/2304.11171) | 本文提出了一种基于颗粒球计算的自适应多粒度表示和计算方法，能够提高机器学习的效率、鲁棒性和可解释性。 |
| [^117] | [OTS: A One-shot Learning Approach for Text Spotting in Historical Manuscripts.](http://arxiv.org/abs/2304.00746) | 提出了一种基于单次学习的历史手稿文本检测方法 OTS， 尤其对于低资源检测任务，使用新型的“环形损失”损失函数提高了检测能力，同时创建了包含古代东巴象形文字的手稿数据集。 |
| [^118] | [TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins.](http://arxiv.org/abs/2303.15954) | TraffNet是一个学习交通量生成原因的深度学习框架，将车辆轨迹数据表示为异构图，利用递归神经网络结构实现了对交通生成原因的预测。 |
| [^119] | [Prismer: A Vision-Language Model with An Ensemble of Experts.](http://arxiv.org/abs/2303.02506) | Prismer是一种数据和参数高效的视觉语言模型，它利用了一组领域专家的集合，通过汇集这些专家知识并将其适应于各种视觉语言推理任务，实现了与当前最先进模型竞争的微调和少样本学习性能，同时需要少至两个数量级的训练数据。 |
| [^120] | [MCWDST: a Minimum-Cost Weighted Directed Spanning Tree Algorithm for Real-Time Fake News Mitigation in Social Media.](http://arxiv.org/abs/2302.12190) | 本论文提出了一种用于社交媒体实时虚假新闻缓解的算法，通过使用新的深度学习架构来检测假新闻并采取实时的网络感知策略来减少其传播。 |
| [^121] | [IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing.](http://arxiv.org/abs/2301.13359) | 该论文提出了一个IM-IAD工业制造中的图像异常检测基准，通过统一的设置评估了16个算法在7个数据集上的性能，旨在解决目前这一领域研究的不规范问题。 |
| [^122] | [On the Adversarial Robustness of Camera-based 3D Object Detection.](http://arxiv.org/abs/2301.10766) | 介绍了基于相机的三维物体检测对抗性鲁棒性的全面调查，发现鸟瞰图表示对定位攻击具有更强的鲁棒性，且不需要深度估计的方法具有更好的对抗性。 |
| [^123] | [Improving Faithfulness of Abstractive Summarization by Controlling Confounding Effect of Irrelevant Sentences.](http://arxiv.org/abs/2212.09726) | 通过控制无关句子的混淆效应，本文提出了一种改进抽象摘要准确性的方法，并在AnswerSumm数据集上实现了20\%的准确性提升。 |
| [^124] | [Distribution Fitting for Combating Mode Collapse in Generative Adversarial Networks.](http://arxiv.org/abs/2212.01521) | 本文提出了一种用于解决生成对抗网络中模式崩溃问题的分布拟合方法。通过全局分布拟合和局部分布拟合，可以限制生成的数据分布，从而提高生成对抗网络的性能。 |
| [^125] | [Choreographer: Learning and Adapting Skills in Imagination.](http://arxiv.org/abs/2211.13350) | 本论文提出了一种建模师代理，利用世界模型在想象中学习并适应技能。该方法通过解耦探索和技能学习过程，能够在模型的潜在状态空间中发现技能，并通过元控制器进行高效的适应。建模师能够从离线数据和同时收集数据中学习技能。 |
| [^126] | [Exploring Local Explanations of Nonlinear Models Using Animated Linear Projections.](http://arxiv.org/abs/2205.05359) | 本文介绍了一种使用动态线性投影方法来分析流行的非线性模型的局部解释的方法，探索预测变量之间的交互如何影响变量重要性估计，这对于理解模型的可解释性非常有用。 |
| [^127] | [Few-shot Quality-Diversity Optimization.](http://arxiv.org/abs/2109.06826) | 本文提出了一种几乎零样本的质量多样性优化方法，通过利用参数空间中的优化路径信息构建先验种群，可以在未见环境中进行少样本适应。 |
| [^128] | [Efficient Attention: Attention with Linear Complexities.](http://arxiv.org/abs/1812.01243) | 本文提出了一种高效注意力机制，可以在大幅减少内存和计算成本的情况下实现与传统点积注意力等效的效果。这种高效机制的应用使得注意力模块可以更广泛地集成到网络中，从而提高准确性，并且在物体检测和实例分割等任务中取得了显著的性能提升。 |

# 详细

[^1]: 使用3D控制合成移动人物

    Synthesizing Moving People with 3D Control. (arXiv:2401.10889v1 [cs.CV])

    [http://arxiv.org/abs/2401.10889](http://arxiv.org/abs/2401.10889)

    本文提出了一种基于扩散模型的框架，用于从单张图像中生成具有逼真移动的人物动画，并成功处理了人体不可见部分的合成问题。

    

    本文提出了一种基于扩散模型的框架，用于从单张图像中为给定的目标3D运动序列生成人物动画。我们的方法包含两个核心组成部分：a) 学习关于人体和服装不可见部分的先验知识，b) 以适当的服装和纹理渲染新的人体姿势。对于第一部分，我们学习了一种填充扩散模型，以给定单张图像生成人物的不可见部分。我们在纹理映射空间上训练这个模型，使其对姿势和视角不变，从而提高了样本效率。其次，我们开发了一个基于扩散的渲染流水线，由3D人体姿势控制。这可以产生逼真的人物新姿势的渲染图像，包括服装、头发和未知区域的合理补充。这种分解的方法使我们的方法能够生成一系列图像，既符合3D姿势中的目标运动，也符合视觉上与输入图像的相似性。

    In this paper, we present a diffusion model-based framework for animating people from a single image for a given target 3D motion sequence. Our approach has two core components: a) learning priors about invisible parts of the human body and clothing, and b) rendering novel body poses with proper clothing and texture. For the first part, we learn an in-filling diffusion model to hallucinate unseen parts of a person given a single image. We train this model on texture map space, which makes it more sample-efficient since it is invariant to pose and viewpoint. Second, we develop a diffusion-based rendering pipeline, which is controlled by 3D human poses. This produces realistic renderings of novel poses of the person, including clothing, hair, and plausible in-filling of unseen regions. This disentangled approach allows our method to generate a sequence of images that are faithful to the target motion in the 3D pose and, to the input image in terms of visual similarity. In addition to tha
    
[^2]: SCENES: 使用极线监督的亚像素对应关系估计

    SCENES: Subpixel Correspondence Estimation With Epipolar Supervision. (arXiv:2401.10886v1 [cs.CV])

    [http://arxiv.org/abs/2401.10886](http://arxiv.org/abs/2401.10886)

    SCENES提出了一种使用极线监督的亚像素对应关系估计方法，通过仅使用相机姿态信息而不需要3D结构，以放宽对数据集的特征要求。

    

    从场景的两个或更多视角中提取点对应关系是一个基础的计算机视觉问题，特别重要的是相对相机姿态估计和结构运动。现有的基于局部特征匹配的方法，通过在大规模数据集上进行对应关系监督训练，能够在测试集上获得高度准确的匹配。然而，它们在不同特征的新数据集上往往无法很好地泛化，这与经典的特征提取器不同。相反，它们需要微调，假设可以获得地面实况对应关系或地面实况相机姿态和3D结构。我们通过取消对3D结构（如深度图或点云）的要求，只需要相机姿态信息（可以从里程计获得）来放松这个假设。我们通过将对应关系损失替换为极线损失来实现，极线损失鼓励假设匹配点位于相关的极线上。虽然它相对来说比对应关系较弱，但一定程度上可以在无需3D结构的情况下估计出相机的相对姿态。

    Extracting point correspondences from two or more views of a scene is a fundamental computer vision problem with particular importance for relative camera pose estimation and structure-from-motion. Existing local feature matching approaches, trained with correspondence supervision on large-scale datasets, obtain highly-accurate matches on the test sets. However, they do not generalise well to new datasets with different characteristics to those they were trained on, unlike classic feature extractors. Instead, they require finetuning, which assumes that ground-truth correspondences or ground-truth camera poses and 3D structure are available. We relax this assumption by removing the requirement of 3D structure, e.g., depth maps or point clouds, and only require camera pose information, which can be obtained from odometry. We do so by replacing correspondence losses with epipolar losses, which encourage putative matches to lie on the associated epipolar line. While weaker than corresponde
    
[^3]: 使用公共社区评分作为人类反馈，在编程领域中使用强化学习进行问答的研究

    Reinforcement learning for question answering in programming domain using public community scoring as a human feedback. (arXiv:2401.10882v1 [cs.CL])

    [http://arxiv.org/abs/2401.10882](http://arxiv.org/abs/2401.10882)

    本研究通过整合强化学习和Stack Overflow评分，提高了GPT Neo在编程问答中的性能，同时指出传统语言度量方法在编程领域的局限性。

    

    本研究通过整合人类反馈的强化学习和来自Stack Overflow的评分，探讨了在编程领域中提高GPT Neo 125M在社区问答中的性能的方法。采用了两种不同的奖励模型训练策略进行微调，通过Proximal Policy Optimization (PPO)实现。这种方法在性能改进方面与GPT Neo 2.7B参数变体相当。此外，还引入了辅助评分机制，显示了传统语言度量在编程领域中评估响应的局限性。通过准确的分析，本文研究了将强化学习从人类反馈应用于编程社区问答的复杂性，并强调了需要领域特定的评估方法的必要性。

    In this study, we investigate the enhancement of the GPT Neo 125M performance in Community Question Answering (CQA) with a focus on programming, through the integration of Reinforcement Learning from Human Feedback (RLHF) and the utilization of scores from Stack Overflow. Two distinct reward model training strategies are employed for fine-tuning with Proximal Policy Optimization (PPO). Notably, the improvements in performance achieved through this method are comparable to those of GPT Neo 2.7B parameter variant. Additionally, an auxiliary scoring mechanism is introduced, which demonstrates the limitations of conventional linguistic metrics in evaluating responses in the programming domain. Through accurate analysis, this paper looks at the divergence between traditional linguistic metrics and our human-preferences-based reward model, underscoring the imperative for domain-specific evaluation methods. By elucidating the complexities involved in applying RLHF to programming CQA and accen
    
[^4]: 基于剪枝的保护: 在不进行微调的情况下增加对齐的LLMs的越狱抵抗力

    Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning. (arXiv:2401.10862v1 [cs.LG])

    [http://arxiv.org/abs/2401.10862](http://arxiv.org/abs/2401.10862)

    本文研究了剪枝对齐的LLMs的保护措施，发现剪枝LLM参数可以显著增强其抵抗“越狱”提示攻击的能力，并且对其他LLM行为也可能有更普遍的效果。同时，引入了一个有害任务数据集，证明剪枝有助于集中注意力在与任务相关的标记上。突出的聊天模型表现出很高的易感性。

    

    大型语言模型（LLMs）容易受到“越狱”提示的攻击，这种攻击可以诱使这些模型生成有害和违法内容。本文表明，剪枝LLM参数多达20％可以显著增加它们对此类攻击的抵抗力，而无需额外训练并且不损害其在标准基准测试中的性能。有趣的是，我们发现剪枝后观察到的增强安全性与模型的初始安全训练水平相关，这暗示剪枝的效果可能更普遍，也可能适用于超出安全性范畴的其他LLM行为。另外，我们还介绍了一个包含五个类别、插入到十个不同越狱提示中的225个有害任务的精选数据集，表明剪枝有助于LLMs集中注意力在越狱提示中与任务相关的标记上。最后，我们的实验揭示了突出的聊天模型（如LLaMA-2 Chat，Vicuna和Mistral Instruct）具有很高的易感性。

    Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type of attack that can coax these models into generating harmful and illegal content. In this paper, we show that pruning up to 20% of LLM parameters markedly increases their resistance to such attacks without additional training and without sacrificing their performance in standard benchmarks. Intriguingly, we discovered that the enhanced safety observed post-pruning correlates to the initial safety training level of the model, hinting that the effect of pruning could be more general and may hold for other LLM behaviors beyond safety. Additionally, we introduce a curated dataset of 225 harmful tasks across five categories, inserted into ten different Jailbreaking prompts, showing that pruning aids LLMs in concentrating attention on task-relevant tokens in jailbreaking prompts. Lastly, our experiments reveal that the prominent chat models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high susceptibi
    
[^5]: 通过自然语言处理和深度学习在eHealth数据分析中的进展

    Advancements in eHealth Data Analytics through Natural Language Processing and Deep Learning. (arXiv:2401.10850v1 [cs.CL])

    [http://arxiv.org/abs/2401.10850](http://arxiv.org/abs/2401.10850)

    通过自然语言处理和深度学习在eHealth数据分析中的进展，可以有效处理和解释医疗领域的大量文本数据，从而提高医疗服务和整个医疗领域的效率和知识水平。

    

    医疗环境通常被称为“信息丰富”但也“知识匮乏”。医疗系统从各种来源收集大量数据，包括实验室报告、医疗信函、医疗工具或程序的日志、医疗处方等。这些庞大的数据集可以提供有关医疗服务和整个医疗领域的宝贵知识和信息，例如通过分析患者症状进行疾病预测或通过发现疾病的行为因素进行疾病预防。不幸的是，只有相对较少的文本eHealth数据被处理和解释，其中一个重要因素是在执行大数据操作时的困难。在医疗领域，检测特定领域的多词术语是一项关键任务，因为它们可以用几个词来定义一个整个概念。术语可以定义为一个语言结构或概念，它由一个或多个词组成。

    The healthcare environment is commonly referred to as "information-rich" but also "knowledge poor". Healthcare systems collect huge amounts of data from various sources: lab reports, medical letters, logs of medical tools or programs, medical prescriptions, etc. These massive sets of data can provide great knowledge and information that can improve the medical services, and overall the healthcare domain, such as disease prediction by analyzing the patient's symptoms or disease prevention, by facilitating the discovery of behavioral factors for diseases. Unfortunately, only a relatively small volume of the textual eHealth data is processed and interpreted, an important factor being the difficulty in efficiently performing Big Data operations. In the medical field, detecting domain-specific multi-word terms is a crucial task as they can define an entire concept with a few words. A term can be defined as a linguistic structure or a concept, and it is composed of one or more words with a s
    
[^6]: 无源和仅图像无监督领域适应的类别级目标姿态估计

    Source-Free and Image-Only Unsupervised Domain Adaptation for Category Level Object Pose Estimation. (arXiv:2401.10848v1 [cs.CV])

    [http://arxiv.org/abs/2401.10848](http://arxiv.org/abs/2401.10848)

    本研究提出了一种无源和仅图像无监督领域适应的方法，用于类别级目标姿态估计。通过利用物体特定子部分在跨域情景中的稳定性，我们可以有效地更新模型。使用简单的立方体网格和基于神经特征激活的生成模型，我们可以在没有3D或深度数据的情况下适应充满干扰的目标领域。

    

    我们考虑在适应过程中仅使用RGB图像到目标领域进行无源无监督类别级别姿态估计的问题，而不需要访问源领域数据或3D注释。收集和注释现实世界的3D数据和相应的图像是一项费时费力的过程，但不可避免，因为即使是3D姿态域适应方法也需要目标领域中的3D数据。我们介绍了一种名为3DUDA的方法，可以适应于充满干扰的目标领域，而无需3D或深度数据。我们的关键观点源于这样的观察：特定的物体子部分在域外情景中保持稳定，从而可以有效地利用这些不变的子组件进行模型更新。我们将物体类别表示为简单的立方体网格，并利用在每个网格顶点上学习的神经特征激活的生成模型进行建模。我们专注于个体局部稳健的网格顶点特征，并对其进行迭代更新。

    We consider the problem of source-free unsupervised category-level pose estimation from only RGB images to a target domain without any access to source domain data or 3D annotations during adaptation. Collecting and annotating real-world 3D data and corresponding images is laborious, expensive, yet unavoidable process, since even 3D pose domain adaptation methods require 3D data in the target domain. We introduce 3DUDA, a method capable of adapting to a nuisance-ridden target domain without 3D or depth data. Our key insight stems from the observation that specific object subparts remain stable across out-of-domain (OOD) scenarios, enabling strategic utilization of these invariant subcomponents for effective model updates. We represent object categories as simple cuboid meshes, and harness a generative model of neural feature activations modeled at each mesh vertex learnt using differential rendering. We focus on individual locally robust mesh vertex features and iteratively update them
    
[^7]: 使用LLMs发现极端社交媒体中的编码反犹太恶意言论的出现

    Using LLMs to discover emerging coded antisemitic hate-speech emergence in extremist social media. (arXiv:2401.10841v1 [cs.CL])

    [http://arxiv.org/abs/2401.10841](http://arxiv.org/abs/2401.10841)

    这项研究提出了一种方法，可以检测新出现的编码恶意术语，为极端社交媒体中的反犹太恶意言论提供了解决方案。

    

    网络仇恨言论的蔓延给社交媒体平台带来了一个难题。一个特殊的挑战与使用编码语言的群体有关，这些群体既想为其用户创造归属感，又想回避检测。编码语言发展迅速，并且随着时间的推移使用方式不同。本文提出了一种检测新出现的编码恶意术语的方法论。该方法在在线反犹太言论的环境中进行了测试。该方法考虑了从社交媒体平台上抓取的帖子，通常是极端主义用户使用的。帖子是使用与以前已知的针对犹太人的仇恨言论相关的种子表达式进行抓取的。该方法首先通过识别每个帖子最具代表性的表达式，并计算它们在整个语料库中的频率。过滤掉语法不一致的表达式和之前遇到过的表达式，以便关注新出现的良好形式的术语。然后进行了语义评估。

    Online hate speech proliferation has created a difficult problem for social media platforms. A particular challenge relates to the use of coded language by groups interested in both creating a sense of belonging for its users and evading detection. Coded language evolves quickly and its use varies over time. This paper proposes a methodology for detecting emerging coded hate-laden terminology. The methodology is tested in the context of online antisemitic discourse. The approach considers posts scraped from social media platforms, often used by extremist users. The posts are scraped using seed expressions related to previously known discourse of hatred towards Jews. The method begins by identifying the expressions most representative of each post and calculating their frequency in the whole corpus. It filters out grammatically incoherent expressions as well as previously encountered ones so as to focus on emergent well-formed terminology. This is followed by an assessment of semantic s
    
[^8]: 通过混合优化实现符号认知诊断的智能教育系统

    Symbolic Cognitive Diagnosis via Hybrid Optimization for Intelligent Education Systems. (arXiv:2401.10840v1 [cs.CY])

    [http://arxiv.org/abs/2401.10840](http://arxiv.org/abs/2401.10840)

    本文提出了一种符号认知诊断（SCD）框架，通过利用符号树明确表示学生与练习互动，以及梯度优化方法学习参数，同时提高了泛化能力和可解释性。

    

    认知诊断评估是学生学习的基本且关键的任务。它建模了学生与练习的互动，并发现了学生对每个知识属性的熟练程度。在实际智能教育系统中，认知诊断方法的泛化能力和可解释性同样重要。然而，由于复杂的学生与练习互动，大多数现有方法很难二者兼顾。为此，本文提出了一种符号认知诊断（SCD）框架，以同时提高泛化能力和可解释性。该SCD框架利用符号树明确地表示复杂的学生与练习互动函数，并利用梯度优化方法有效地学习学生和练习参数。与此同时，挑战在于我们需要传递离散的符号表示和连续的参数优化。

    Cognitive diagnosis assessment is a fundamental and crucial task for student learning. It models the student-exercise interaction, and discovers the students' proficiency levels on each knowledge attribute. In real-world intelligent education systems, generalization and interpretability of cognitive diagnosis methods are of equal importance. However, most existing methods can hardly make the best of both worlds due to the complicated student-exercise interaction. To this end, this paper proposes a symbolic cognitive diagnosis~(SCD) framework to simultaneously enhance generalization and interpretability. The SCD framework incorporates the symbolic tree to explicably represent the complicated student-exercise interaction function, and utilizes gradient-based optimization methods to effectively learn the student and exercise parameters. Meanwhile, the accompanying challenge is that we need to tunnel the discrete symbolic representation and continuous parameter optimization. To address thi
    
[^9]: 弹性的基于代理的分布式机器学习框架：Holonic Learning

    Holonic Learning: A Flexible Agent-based Distributed Machine Learning Framework. (arXiv:2401.10839v1 [cs.DC])

    [http://arxiv.org/abs/2401.10839](http://arxiv.org/abs/2401.10839)

    Holonic Learning (HoL) is a flexible and privacy-focused learning framework designed for training deep learning models. It leverages holonic concepts to establish a structured self-similar hierarchy, allowing more nuanced control over collaborations. HoL has extensive design and flexibility potentials.

    

    在过去的十年中，数据和计算资源的普及性不断增加，推动了机器学习范式向更分布式的方法转变。这种转变不仅旨在解决可扩展性和资源分布挑战，还要解决紧迫的隐私和安全问题。为了对这一持续讨论做出贡献，本文介绍了Holonic Learning (HoL)，这是一个专为训练深度学习模型而设计的协作和注重隐私的学习框架。通过利用Holonic概念，HoL框架在学习过程中建立了一个结构化的自相似层次结构，通过每个holon的个体模型聚合方法以及它们的内部holon承诺和通信模式，实现对协作的更细致控制。Holonic Learning在其通用形式中提供了广泛的设计和灵活性潜力。为了经验分析和展示其效果，本文实现了Holo

    Ever-increasing ubiquity of data and computational resources in the last decade have propelled a notable transition in the machine learning paradigm towards more distributed approaches. Such a transition seeks to not only tackle the scalability and resource distribution challenges but also to address pressing privacy and security concerns. To contribute to the ongoing discourse, this paper introduces Holonic Learning (HoL), a collaborative and privacy-focused learning framework designed for training deep learning models. By leveraging holonic concepts, the HoL framework establishes a structured self-similar hierarchy in the learning process, enabling more nuanced control over collaborations through the individual model aggregation approach of each holon, along with their intra-holon commitment and communication patterns. HoL, in its general form, provides extensive design and flexibility potentials. For empirical analysis and to demonstrate its effectiveness, this paper implements Holo
    
[^10]: 通过通用概念发现理解视频Transformer

    Understanding Video Transformers via Universal Concept Discovery. (arXiv:2401.10831v1 [cs.CV])

    [http://arxiv.org/abs/2401.10831](http://arxiv.org/abs/2401.10831)

    本文研究了视频Transformer的可解释性问题，引入了视频Transformer概念发现算法来解释其决策过程，并揭示了时空推理机制和对象为中心的表示。

    

    本文研究了基于概念的视频Transformer表示的可解释性问题。具体而言，我们试图解释基于自动发现的高层时空概念的视频Transformer的决策过程。以往关于基于概念的可解释性的研究仅集中在图像级任务上。相比之下，视频模型处理了额外的时间维度，增加了复杂性，并在识别动态概念方面面临挑战。在这项工作中，我们通过引入第一个视频Transformer概念发现(VTCD)算法系统地解决了这些挑战。为此，我们提出了一种有效的无监督方法，用于识别视频Transformer表示的单元（概念）并对其对模型输出的重要性进行排名。得到的概念具有很强的可解释性，揭示了视频中的时空推理机制和以对象为中心的表示。

    This paper studies the problem of concept-based interpretability of transformer representations for videos. Concretely, we seek to explain the decision-making process of video transformers based on high-level, spatiotemporal concepts that are automatically discovered. Prior research on concept-based interpretability has concentrated solely on image-level tasks. Comparatively, video models deal with the added temporal dimension, increasing complexity and posing challenges in identifying dynamic concepts over time. In this work, we systematically address these challenges by introducing the first Video Transformer Concept Discovery (VTCD) algorithm. To this end, we propose an efficient approach for unsupervised identification of units of video transformer representations - concepts, and ranking their importance to the output of a model. The resulting concepts are highly interpretable, revealing spatio-temporal reasoning mechanisms and object-centric representations in unstructured video m
    
[^11]: 神经符号学习系统的优化

    Optimisation in Neurosymbolic Learning Systems. (arXiv:2401.10819v1 [cs.AI])

    [http://arxiv.org/abs/2401.10819](http://arxiv.org/abs/2401.10819)

    神经符号学习系统的优化研究了如何将深度学习与符号推理相结合，并探讨了模糊推理和概率推理在神经符号学习中的应用，得到了一些令人惊讶的结果，如与乌鸦悖论的联系。

    

    神经符号人工智能旨在将深度学习与符号人工智能相结合。这种集成有许多优势，例如减少训练神经网络所需的数据量，提高模型答案的可解释性和可理解性以及验证训练系统的正确性。本研究探讨了神经符号学习，在其中我们同时使用符号语言表示数据和背景知识。我们如何连接符号和神经组件以传达这些知识呢？其中一种方法是模糊推理，它研究的是真实程度。例如，身高并不是二元概念。相反，概率推理研究的是某件事情成为真实或将发生的概率。我们的第一个研究问题探讨了不同形式的模糊推理与学习的结合。我们发现一些令人惊讶的结果，例如与乌鸦悖论的联系，即当我们观察到一个绿色的苹果时，我们确认“乌鸦是黑色的”。在本研究中，我们没有使用背景知识。

    Neurosymbolic AI aims to integrate deep learning with symbolic AI. This integration has many promises, such as decreasing the amount of data required to train a neural network, improving the explainability and interpretability of answers given by models and verifying the correctness of trained systems. We study neurosymbolic learning, where we have both data and background knowledge expressed using symbolic languages. How do we connect the symbolic and neural components to communicate this knowledge? One option is fuzzy reasoning, which studies degrees of truth. For example, being tall is not a binary concept. Instead, probabilistic reasoning studies the probability that something is true or will happen. Our first research question studies how different forms of fuzzy reasoning combine with learning. We find surprising results like a connection to the Raven paradox stating we confirm "ravens are black" when we observe a green apple. In this study, we did not use the background knowledg
    
[^12]: Co-Pilot for Health: 个性化算法AI引导改善健康结果

    Co-Pilot for Health: Personalized Algorithmic AI Nudging to Improve Health Outcomes. (arXiv:2401.10816v1 [cs.HC])

    [http://arxiv.org/abs/2401.10816](http://arxiv.org/abs/2401.10816)

    该研究通过使用基于图神经网络的推荐系统和来自可穿戴设备的健康行为数据，设计并实施了一个人工智能驱动平台，实现了个性化和情境引导，能够提高参与者的日常活动水平和中等至剧烈运动时长。

    

    在全球范围内自动塑造大型人群的健康行为，跨可穿戴设备类型和疾病状况具有巨大的潜力来改善全球健康结果。我们设计并实施了一个基于图神经网络（GNN）推荐系统和来自可穿戴健身设备的精细健康行为数据的人工智能驱动平台，用于数字算法引导。在此我们描述了该平台在新加坡针对$n=84,764$个个体的12周期间进行个性化和情境引导的有效性结果。我们统计验证了目标组中接受此类AI优化日常引导的参与者相较于控制组中未接受任何引导的匹配参与者，其每天的步数增加了6.17%（$p = 3.09\times10^{-4}$），每周中等至剧烈运动（MVPA）分钟增加了7.61%（$p = 1.16\times10^{-2}$）。此外，此类引导非常可行。

    The ability to shape health behaviors of large populations automatically, across wearable types and disease conditions at scale has tremendous potential to improve global health outcomes. We designed and implemented an AI driven platform for digital algorithmic nudging, enabled by a Graph-Neural Network (GNN) based Recommendation System, and granular health behavior data from wearable fitness devices. Here we describe the efficacy results of this platform with its capabilities of personalized and contextual nudging to $n=84,764$ individuals over a 12-week period in Singapore. We statistically validated that participants in the target group who received such AI optimized daily nudges increased daily physical activity like step count by 6.17% ($p = 3.09\times10^{-4}$) and weekly minutes of Moderate to Vigorous Physical Activity (MVPA) by 7.61% ($p = 1.16\times10^{-2}$), compared to matched participants in control group who did not receive any nudges. Further, such nudges were very well r
    
[^13]: 学习视觉连接动作和其效果

    Learning to Visually Connect Actions and their Effects. (arXiv:2401.10805v1 [cs.CV])

    [http://arxiv.org/abs/2401.10805](http://arxiv.org/abs/2401.10805)

    该论文提出了视觉连接动作和其效果的概念（CATE），用于视频理解。研究表明，不同的任务形式产生了捕捉直观动作特性的表示，但模型表现不佳，人类的表现明显优于它们。该研究为未来的努力奠定了基础，并希望能激发出高级形式和模型的灵感。

    

    在这项工作中，我们引入了视觉连接动作和其效果（CATE）的新概念，用于视频理解。CATE可以在任务规划和从示范中学习等领域中应用。我们提出了不同基于CATE的任务形式，如动作选择和动作指定，其中视频理解模型以语义和细粒度的方式连接动作和效果。我们观察到不同的形式产生了捕捉直观动作特性的表示。我们还设计了各种基线模型用于动作选择和动作指定。尽管任务具有直观性，但我们观察到模型困难重重，人类表现明显优于它们。本研究旨在为未来的努力奠定基础，展示了连接视频理解中动作和效果的灵活性和多功能性，希望能激发出高级形式和模型的灵感。

    In this work, we introduce the novel concept of visually Connecting Actions and Their Effects (CATE) in video understanding. CATE can have applications in areas like task planning and learning from demonstration. We propose different CATE-based task formulations, such as action selection and action specification, where video understanding models connect actions and effects at semantic and fine-grained levels. We observe that different formulations produce representations capturing intuitive action properties. We also design various baseline models for action selection and action specification. Despite the intuitive nature of the task, we observe that models struggle, and humans outperform them by a large margin. The study aims to establish a foundation for future efforts, showcasing the flexibility and versatility of connecting actions and effects in video understanding, with the hope of inspiring advanced formulations and models.
    
[^14]: 度量动态平衡逻辑

    Metric Dynamic Equilibrium Logic. (arXiv:2401.10781v1 [cs.AI])

    [http://arxiv.org/abs/2401.10781](http://arxiv.org/abs/2401.10781)

    该论文提出了度量动态平衡逻辑，它是一种基于ASP的方法，用于指定定性和定量的动态约束。它是Equilibrium Logic的最通用的时间扩展之一。

    

    在基于线性时间的Answer Set Programming（ASP）的时间扩展中，动态系统的行为通过状态序列来捕捉。虽然这种表示反映了它们的相对顺序，但是它省略了与每个状态相关联的具体时间。然而，在许多应用中，时间约束很重要，例如，在规划和调度同时进行时。我们通过开发基于度量的线性时间动态平衡逻辑来解决这个问题，其中动态操作符受到整数区间的约束。由此产生的度量动态平衡逻辑为基于ASP的定性和定量动态约束规范提供了基础。因此，在Equilibrium Logic的一系列时间扩展中，它是最通用的。具体而言，我们展示它包括了Temporal、Dynamic、Metric和regular Equilibrium Logic，以及在排中律成立时的经典对应物。

    In temporal extensions of Answer Set Programming (ASP) based on linear-time, the behavior of dynamic systems is captured by sequences of states. While this representation reflects their relative order, it abstracts away the specific times associated with each state. In many applications, however, timing constraints are important like, for instance, when planning and scheduling go hand in hand. We address this by developing a metric extension of linear-time Dynamic Equilibrium Logic, in which dynamic operators are constrained by intervals over integers. The resulting Metric Dynamic Equilibrium Logic provides the foundation of an ASP-based approach for specifying qualitative and quantitative dynamic constraints. As such, it constitutes the most general among a whole spectrum of temporal extensions of Equilibrium Logic. In detail, we show that it encompasses Temporal, Dynamic, Metric, and regular Equilibrium Logic, as well as its classic counterparts once the law of the excluded middle is
    
[^15]: 使用提示问题进行编程教学的新方法：与大型语言模型的交互

    Interactions with Prompt Problems: A New Way to Teach Programming with Large Language Models. (arXiv:2401.10759v1 [cs.HC])

    [http://arxiv.org/abs/2401.10759](http://arxiv.org/abs/2401.10759)

    本文提出了一种新的编程教学方法，即使用提示问题，以与大型语言模型进行交互。学生通过视觉方式获得一个问题，并将其转换为LLMs能够理解的提示，以生成能够通过所有测试用例的代码。

    

    大型语言模型（LLMs）颠覆了计算机教育几十年来的教学方法。以前，学生通过编写许多小问题来学习编程，对代码阅读和理解的重视程度较低。最近的研究表明，由LLMs驱动的自由代码生成工具可以轻松解决用自然语言提出的初级编程问题。在本文中，我们提出了一种新的编程教学方式，即使用提示问题。学生通过视觉方式获得一个问题，指示输入如何转换为输出，然后将其转换为LLMs能够理解的提示。当由学生提示生成的代码能够通过所有测试用例时，问题被认为是正确的。在本文中，我们介绍了此工具的设计，讨论了学生在学习中与此工具的互动，并提供了关于这种新型编程问题及集成LLMs的设计工具的见解。

    Large Language Models (LLMs) have upended decades of pedagogy in computing education. Students previously learned to code through \textit{writing} many small problems with less emphasis on code reading and comprehension. Recent research has shown that free code generation tools powered by LLMs can solve introductory programming problems presented in natural language with ease. In this paper, we propose a new way to teach programming with Prompt Problems. Students receive a problem visually, indicating how input should be transformed to output, and must translate that to a prompt for an LLM to decipher. The problem is considered correct when the code that is generated by the student prompt can pass all test cases. In this paper we present the design of this tool, discuss student interactions with it as they learn, and provide insights into this new class of programming problems as well as the design tools that integrate LLMs.
    
[^16]: BoolGebra: 用于布尔代数操作的属性图学习

    BoolGebra: Attributed Graph-learning for Boolean Algebraic Manipulation. (arXiv:2401.10753v1 [cs.AR])

    [http://arxiv.org/abs/2401.10753](http://arxiv.org/abs/2401.10753)

    BoolGebra是一种方法，使用属性图学习和神经网络来改进布尔代数操作的逻辑综合。它通过减小搜索空间并高效定位优化空间，实现了跨设计推论的通用性和规模化潜力。

    

    布尔代数操作是电子设计自动化（EDA）设计流程中的核心。现有方法往往难以充分利用优化机会，并且往往受到搜索空间爆炸和可扩展性效率有限的困扰。本文提出了BoolGebra，一种新颖的用于布尔代数操作的属性图学习方法，旨在改进基础逻辑综合。BoolGebra结合了图神经网络（GNNs），并将结构和功能信息的初始特征嵌入作为输入。全连接神经网络被用作直接优化结果预测的预测器，显著减少搜索空间并高效定位优化空间。实验涉及培训BoolGebra模型关于设计特定和跨设计推论，使用训练模型的BoolGebra展示了跨设计推论的通用性和规模化潜力。

    Boolean algebraic manipulation is at the core of logic synthesis in Electronic Design Automation (EDA) design flow. Existing methods struggle to fully exploit optimization opportunities, and often suffer from an explosive search space and limited scalability efficiency. This work presents BoolGebra, a novel attributed graph-learning approach for Boolean algebraic manipulation that aims to improve fundamental logic synthesis. BoolGebra incorporates Graph Neural Networks (GNNs) and takes initial feature embeddings from both structural and functional information as inputs. A fully connected neural network is employed as the predictor for direct optimization result predictions, significantly reducing the search space and efficiently locating the optimization space. The experiments involve training the BoolGebra model w.r.t design-specific and cross-design inferences using the trained model, where BoolGebra demonstrates generalizability for cross-design inference and its potential to scale 
    
[^17]: EFO：情绪框架本体论

    EFO: the Emotion Frame Ontology. (arXiv:2401.10751v1 [cs.AI])

    [http://arxiv.org/abs/2401.10751](http://arxiv.org/abs/2401.10751)

    本文提出了一种情绪框架本体(EFO)，它将情绪视为语义框架，并使用一组语义角色来捕捉情绪经验的不同方面。EFO采用基于模式的本体论设计，并与DOLCE基本本体论对齐，可以用于建模多种情绪理论，并在Emotion Ontology Network中交叉链接。本文以Ekman的基本情绪(BE)理论作为EFO-BE模块进行示例，并演示了如何对表示情绪情况进行自动推理。通过从Framester知识库中提取BE情绪框架对EFO-BE进行了评估。

    

    情绪在各个学科中一直是激烈辩论的主题。尽管存在许多理论和定义，但关于什么是情绪以及如何对其进行建模和分类仍然没有共识。在本文中，我们提出了一种基于OWL框架的情绪本体论，即情绪框架本体(EFO) 。EFO将情绪视为语义框架，并具有一组捕捉情绪经验的不同方面的语义角色。EFO采用基于模式的本体论设计，并与DOLCE基本本体论对齐。EFO用于建模多种情绪理论，可以将其作为Emotion Ontology Network的模块进行交叉链接。在本文中，我们通过将Ekman的基本情绪(BE)理论建模为EFO-BE模块来进行示例，并演示如何对情绪情况的表示进行自动推理。通过从Framester知识库中提取BE情绪框架对EFO-BE进行了评估。

    Emotions are a subject of intense debate in various disciplines. Despite the proliferation of theories and definitions, there is still no consensus on what emotions are, and how to model the different concepts involved when we talk about - or categorize - them. In this paper, we propose an OWL frame-based ontology of emotions: the Emotion Frames Ontology (EFO). EFO treats emotions as semantic frames, with a set of semantic roles that capture the different aspects of emotional experience. EFO follows pattern-based ontology design, and is aligned to the DOLCE foundational ontology. EFO is used to model multiple emotion theories, which can be cross-linked as modules in an Emotion Ontology Network. In this paper, we exemplify it by modeling Ekman's Basic Emotions (BE) Theory as an EFO-BE module, and demonstrate how to perform automated inferences on the representation of emotion situations. EFO-BE has been evaluated by lexicalizing the BE emotion frames from within the Framester knowledge 
    
[^18]: 缺失模态下的多模态情感分析:一种知识迁移方法

    Multimodal Sentiment Analysis with Missing Modality: A Knowledge-Transfer Approach. (arXiv:2401.10747v1 [cs.SD])

    [http://arxiv.org/abs/2401.10747](http://arxiv.org/abs/2401.10747)

    本文提出了一种知识迁移方法，用于在缺失模态下进行多模态情感分析。通过翻译不同模态之间的内容以重构缺失的音频模态，并利用跨模态注意机制进行情感预测，实验证明了该方法在多个数据集上表现出显著的改进和与完整多模态监督方法相媲美的效果。

    

    多模态情感分析旨在通过视觉、语言和声音线索来识别个体表达的情绪。然而，现有研究大多假设在训练和测试过程中所有模态都是可用的，这使得它们的算法容易受到缺失模态的影响。在本文中，我们提出了一种新颖的知识迁移网络，用于在不同模态之间进行翻译，以重构缺失的音频模态。此外，我们还开发了一种跨模态注意机制，以保留重构和观察到的模态的最大信息，用于情感预测。在三个公开数据集上进行的大量实验证明了相对于基线算法的显著改进，并实现了与具有完整多模态监督的先前方法相媲美的结果。

    Multimodal sentiment analysis aims to identify the emotions expressed by individuals through visual, language, and acoustic cues. However, most of the existing research efforts assume that all modalities are available during both training and testing, making their algorithms susceptible to the missing modality scenario. In this paper, we propose a novel knowledge-transfer network to translate between different modalities to reconstruct the missing audio modalities. Moreover, we develop a cross-modality attention mechanism to retain the maximal information of the reconstructed and observed modalities for sentiment prediction. Extensive experiments on three publicly available datasets demonstrate significant improvements over baselines and achieve comparable results to the previous methods with complete multi-modality supervision.
    
[^19]: 利用深度学习对脑电解码中的欧几里得对齐进行系统评估

    A Systematic Evaluation of Euclidean Alignment with Deep Learning for EEG Decoding. (arXiv:2401.10746v1 [eess.SP])

    [http://arxiv.org/abs/2401.10746](http://arxiv.org/abs/2401.10746)

    本研究系统评估了使用深度学习和欧几里得对齐对脑电解码的影响。结果表明，欧几里得对齐能够显著提高解码率，并且减少了收敛时间。

    

    脑电图（EEG）信号经常用于各种脑机接口（BCI）任务。尽管深度学习（DL）技术显示出有希望的结果，但它们受到大量数据要求的限制。通过利用来自多个受试者的数据，迁移学习能够更有效地训练DL模型。一种越来越受欢迎的技术是欧几里得对齐（EA），因为它易于使用、计算复杂度低并且与深度学习模型兼容。然而，很少有研究评估其对共享和个体DL模型的训练效果的影响。在这项工作中，我们系统地评估了EA与DL相结合在解码BCI信号中的效果。我们使用EA来训练来自多个受试者的共享模型，并评估其对新受试者的可迁移性。我们的实验结果表明，它将目标受试者的解码率提高了4.33％，并且收敛时间缩短了超过70％。我们还为个体模型进行了训练。

    Electroencephalography (EEG) signals are frequently used for various Brain-Computer Interface (BCI) tasks. While Deep Learning (DL) techniques have shown promising results, they are hindered by the substantial data requirements. By leveraging data from multiple subjects, transfer learning enables more effective training of DL models. A technique that is gaining popularity is Euclidean Alignment (EA) due to its ease of use, low computational complexity, and compatibility with Deep Learning models. However, few studies evaluate its impact on the training performance of shared and individual DL models. In this work, we systematically evaluate the effect of EA combined with DL for decoding BCI signals. We used EA to train shared models with data from multiple subjects and evaluated its transferability to new subjects. Our experimental results show that it improves decoding in the target subject by 4.33% and decreases convergence time by more than 70%. We also trained individual models for 
    
[^20]: 对高级大型语言模型的治理和利用的道德人工智能原则和指南

    Ethical Artificial Intelligence Principles and Guidelines for the Governance and Utilization of Highly Advanced Large Language Models. (arXiv:2401.10745v1 [cs.CY])

    [http://arxiv.org/abs/2401.10745](http://arxiv.org/abs/2401.10745)

    本文探讨了如何利用道德人工智能原则和指南来解决高级大型语言模型的治理和利用问题。

    

    鉴于ChatGPT、LaMDA和其他大型语言模型（LLMs）的成功，技术行业和其他行业对LLMs的开发和使用有所增加。虽然LLMs的水平尚未超过人类智能，但总有一天会达到这一点。这种LLMs可以称为高级LLMs。目前，由于尚未达到这一点，使用道德人工智能（AI）原则和指南来解决高级LLMs的问题还受到限制。然而，这是一个问题，因为一旦达到这一点，我们将无法充分准备好以道德和最佳方式处理其产生的后果，这将导致不可预期的后果。本文讨论了如何利用道德人工智能原则和指南来解决高级LLMs的问题。

    Given the success of ChatGPT, LaMDA and other large language models (LLMs), there has been an increase in development and usage of LLMs within the technology sector and other sectors. While the level in which LLMs has not reached a level where it has surpassed human intelligence, there will be a time when it will. Such LLMs can be referred to as advanced LLMs. Currently, there are limited usage of ethical artificial intelligence (AI) principles and guidelines addressing advanced LLMs due to the fact that we have not reached that point yet. However, this is a problem as once we do reach that point, we will not be adequately prepared to deal with the aftermath of it in an ethical and optimal way, which will lead to undesired and unexpected consequences. This paper addresses this issue by discussing what ethical AI principles and guidelines can be used to address highly advanced LLMs.
    
[^21]: FinLLMs：基于大型语言模型的金融推理数据集生成框架

    FinLLMs: A Framework for Financial Reasoning Dataset Generation with Large Language Models. (arXiv:2401.10744v1 [cs.AI])

    [http://arxiv.org/abs/2401.10744](http://arxiv.org/abs/2401.10744)

    FinLLMs是一种基于大型语言模型的金融推理数据集生成框架，利用常见金融公式生成金融问答数据，旨在解决有限数据资源和高昂的手工注释成本问题。

    

    大型语言模型（LLMs）通常依赖于大量的训练数据集。在金融领域，创建包含表格和长文本混合的数值推理数据集通常需要大量的手工注释开销。为了解决有限的数据资源并降低注释成本，我们引入了FinLLMs，一种基于常见金融公式使用大型语言模型生成金融问答数据的方法。首先，我们编制了一个常见金融公式列表，并根据这些公式使用的变量构建了一个图形。然后，我们通过合并具有相同变量的公式作为新元素来扩充公式集合。具体来说，我们通过遍历构建的图形，探索通过手工注释获得的公式，并合并具有共享变量的公式。最后，利用GPT-3.5，我们生成了包含表格信息和长文本内容的金融问答数据，借助已收集的公式集合。

    Large Language models (LLMs) usually rely on extensive training datasets. In the financial domain, creating numerical reasoning datasets that include a mix of tables and long text often involves substantial manual annotation expenses. To address the limited data resources and reduce the annotation cost, we introduce FinLLMs, a method for generating financial question-answering data based on common financial formulas using Large Language Models. First, we compile a list of common financial formulas and construct a graph based on the variables these formulas employ. We then augment the formula set by combining those that share identical variables as new elements. Specifically, we explore formulas obtained by manual annotation and merge those formulas with shared variables by traversing the constructed graph. Finally, utilizing GPT-3.5, we generate financial question-answering data that encompasses both tabular information and long textual content, building on the collected formula set. O
    
[^22]: 使用大型语言模型进行临床文档的动态问答

    Dynamic Q&A of Clinical Documents with Large Language Models. (arXiv:2401.10733v1 [cs.IR])

    [http://arxiv.org/abs/2401.10733](http://arxiv.org/abs/2401.10733)

    本研究介绍了一种使用大型语言模型进行临床文档动态问答的自然语言接口。通过Langchain和Transformer-based LLMs驱动的聊天机器人，用户可以用自然语言查询临床笔记并获得相关答案。实验结果显示Wizard Vicuna具有出色的准确性，但计算要求较高。模型优化方案提高了约48倍的延迟。然而，模型产生幻象和多样化医疗案例评估的限制仍然存在。解决这些挑战对于发掘临床笔记的价值和推进基于AI的临床决策至关重要。

    

    电子健康记录（EHR）中收录了临床笔记中的重要患者数据。随着这些笔记数量和复杂度的增加，手动提取变得具有挑战性。本研究利用大型语言模型（LLMs）引入了一种自然语言接口，用于对临床笔记进行动态问答。我们的聊天机器人由Langchain和基于Transformer的LLMs驱动，允许用户用自然语言发出查询，并从临床笔记中获得相关答案。通过使用各种嵌入模型和先进的LLMs进行实验，结果表明Wizard Vicuna在准确性方面表现优异，尽管计算要求较高。模型优化，包括权重量化，将延迟提高了约48倍。有希望的结果显示了临床笔记中的价值潜力，但仍存在模型产生幻象和有限的多样化医疗案例评估等挑战。解决这些问题对于发掘临床笔记的价值和推动AI驱动的临床决策至关重要。

    Electronic health records (EHRs) house crucial patient data in clinical notes. As these notes grow in volume and complexity, manual extraction becomes challenging. This work introduces a natural language interface using large language models (LLMs) for dynamic question-answering on clinical notes. Our chatbot, powered by Langchain and transformer-based LLMs, allows users to query in natural language, receiving relevant answers from clinical notes. Experiments, utilizing various embedding models and advanced LLMs, show Wizard Vicuna's superior accuracy, albeit with high compute demands. Model optimization, including weight quantization, improves latency by approximately 48 times. Promising results indicate potential, yet challenges such as model hallucinations and limited diverse medical case evaluations remain. Addressing these gaps is crucial for unlocking the value in clinical notes and advancing AI-driven clinical decision-making.
    
[^23]: 第14届几何自动推理国际会议论文集

    Proceedings 14th International Conference on Automated Deduction in Geometry. (arXiv:2401.10725v1 [cs.LO])

    [http://arxiv.org/abs/2401.10725](http://arxiv.org/abs/2401.10725)

    第14届几何自动推理国际会议（ADG）在塞尔维亚贝尔格莱德举办，重点关注教育中的推理。特邀报告嘉宾分别探讨了几何的形式化、算术化和自动化，以及双曲几何的自动化、形式化和可视化。

    

    ADG是一个交流几何和自动推理交叉领域思想和观点、展示研究成果和进展、展示软件工具的论坛。会议每两年举办一次。之前的ADG会议分别在2021年的Hagenberg（在线，因COVID-19而延期）、2018年的南宁、2016年的斯特拉斯堡、2014年的科英布拉、2012年的爱丁堡、2010年的慕尼黑、2008年的上海、2006年的庞特韦德拉、2004年的盖恩斯维尔、2002年的Hagenberg、2000年的苏黎世、1998年的北京和1996年的图卢兹举办。第14届ADG于2023年9月20-22日在塞尔维亚贝尔格莱德举办，本届ADG还设置了一个特别关注的主题——教育中的推理。特邀报告嘉宾包括法国斯特拉斯堡大学的Julien Narboux：“几何的形式化、算术化和自动化”，塞尔维亚贝尔格莱德大学的Filip Mari\'c：“双曲几何的自动化、形式化和可视化”，以及University of

    ADG is a forum to exchange ideas and views, to present research results and progress, and to demonstrate software tools at the intersection between geometry and automated deduction. The conference is held every two years. The previous editions of ADG were held in Hagenberg in 2021 (online, postponed from 2020 due to COVID-19), Nanning in 2018, Strasbourg in 2016, Coimbra in 2014, Edinburgh in 2012, Munich in 2010, Shanghai in 2008, Pontevedra in 2006, Gainesville in 2004, Hagenberg in 2002, Zurich in 2000, Beijing in 1998, and Toulouse in 1996.  The 14th edition, ADG 2023, was held in Belgrade, Serbia, in September 20-22, 2023. This edition of ADG had an additional special focus topic, Deduction in Education.  Invited Speakers: Julien Narboux, University of Strasbourg, France "Formalisation, arithmetization and automatisation of geometry"; Filip Mari\'c, University of Belgrade, Serbia, "Automatization, formalization and visualization of hyperbolic geometry"; Zlatan Magajna, University 
    
[^24]: Q&A提示：通过挖掘问题-回答提示来发现丰富的视觉线索，以满足对多样世界知识的视觉问答的需求

    Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge. (arXiv:2401.10712v1 [cs.CV])

    [http://arxiv.org/abs/2401.10712](http://arxiv.org/abs/2401.10712)

    本论文提出了一种叫做Q&A提示的方法，通过挖掘图像中的问题-回答对来发现丰富的视觉线索，以帮助AI模型更好地理解复杂视觉问题，提高跨模态推理能力。

    

    随着多模态大型语言模型的突破，回答需要高级推理能力和世界知识的复杂视觉问题比以往任何时候都更重要。然而，为AI模型配备强大的跨模态推理能力仍然具有挑战性，因为人类的认知方案尚未系统地被理解。在本文中，我们相信，如果我们能尽可能收集给定图像中的视觉线索，我们将能更准确地识别图像，更好地理解问题，更容易回忆相关知识，并最终推理出答案。我们通过在图像中挖掘问题-回答对来发现这些丰富的视觉线索，并将它们作为提示发送到多模态大型语言模型中。我们称之为Q&A提示的方法。具体而言，我们首先使用训练集中的图像-答案对和相应的问题作为输入和输出来训练一个视觉问题生成模型。

    With the breakthrough of multi-modal large language models, answering complex visual questions that demand advanced reasoning abilities and world knowledge has become a much more important testbed for developing AI models than ever. However, equipping AI models with robust cross-modality reasoning ability remains challenging since the cognition scheme of humans has not been understood systematically. In this paper, we believe that if we can collect visual clues in the given image as much as possible, we will recognize the image more accurately, understand the question better, recall relevant knowledge more easily, and finally reason out the answer. We discover these rich visual clues by mining question-answer pairs in images and sending them into multi-modal large language models as prompts. We call the proposed method Q&A Prompts. Specifically, we first use the image-answer pairs and the corresponding questions in the training set as inputs and outputs to train a visual question gener
    
[^25]: 使用大型多模型的弱监督高斯对比基础模型来处理视频问答问题

    Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering. (arXiv:2401.10711v1 [cs.CV])

    [http://arxiv.org/abs/2401.10711](http://arxiv.org/abs/2401.10711)

    本论文提出了一种使用大型多模型的弱监督高斯对比基础模型来处理视频问答问题的方法。通过将问题和答案对作为事件描述，找到多个关键帧作为目标时刻，并利用这些时刻作为伪标签来强制LMMs进行推理。所提出的方法使用轻量级的基于高斯的对比基础模块（GCG）来学习时效结构。

    

    视频问答（VideoQA）旨在基于观察到的视频信息回答自然语言问题。尽管大型多模型（LMMs）在图像语言理解和推理方面取得了近期的成功，但它们在处理视频问答方面还不足够，仅仅是将均匀采样的帧作为视觉输入，忽略了与问题相关的视觉线索。此外，现有的视频问答数据集中没有针对问题关键时间戳的人工注释。基于此，我们提出了一种新的弱监督框架，强制LMMs使用问题关键时刻作为视觉输入推理出答案。具体来说，我们将问题和答案对合并为事件描述，以找到多个关键帧作为目标时刻，这些时刻将作为伪标签。通过将这些伪标签作为额外的弱监督，我们设计了一个轻量级的基于高斯的对比基础模块（GCG）。GCG学习多个高斯函数来描述时效结构。

    Video Question Answering (VideoQA) aims to answer natural language questions based on the information observed in videos. Despite the recent success of Large Multimodal Models (LMMs) in image-language understanding and reasoning, they deal with VideoQA insufficiently by simply taking uniformly sampled frames as visual inputs, which ignores question-relevant visual clues. Moreover, there are no human annotations for question-critical timestamps in existing VideoQA datasets. In light of this, we propose a novel weakly supervised framework to enforce the LMMs to reason out the answers with question-critical moments as visual inputs. Specifically, we fuse the question and answer pairs as event descriptions to find multiple keyframes as target moments, which will be pseudo-labels. With these pseudo-labels as additionally weak supervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG learns multiple Gaussian functions to characterize the temporal structure o
    
[^26]: 安全离线强化学习与可行性导向扩散模型

    Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model. (arXiv:2401.10700v1 [cs.LG])

    [http://arxiv.org/abs/2401.10700](http://arxiv.org/abs/2401.10700)

    本论文提出了一种安全离线强化学习方法，通过可行性导向的扩散模型来平衡安全约束满足、奖励最大化和离线数据集的行为规范化。通过可达性分析，将硬安全约束转化为在离线数据集中识别最大可行区域。

    

    安全离线强化学习是一种有效避免风险的在线交互以实现安全策略学习的方法。大多数现有的方法只强制软约束，即将期望的安全违规约束在预定的阈值以下。然而，这可能导致潜在的不安全结果，在安全关键场景中是不可接受的。另一种选择是强制零违规的硬约束。然而，在离线设置中，这可能具有挑战性，因为需要在安全约束满足、奖励最大化和离线数据集所施加的行为正则化之间取得适当的平衡。有趣的是，我们发现通过安全控制理论的可达性分析，硬安全约束可以等效地转化为识别给定离线数据集的最大可行区域。这无缝地将原始的三重问题转化为依赖于可行性的目标，即在可行性范围内最大化奖励值。

    Safe offline RL is a promising way to bypass risky online interactions towards safe policy learning. Most existing methods only enforce soft constraints, i.e., constraining safety violations in expectation below thresholds predetermined. This can lead to potentially unsafe outcomes, thus unacceptable in safety-critical scenarios. An alternative is to enforce the hard constraint of zero violation. However, this can be challenging in offline setting, as it needs to strike the right balance among three highly intricate and correlated aspects: safety constraint satisfaction, reward maximization, and behavior regularization imposed by offline datasets. Interestingly, we discover that via reachability analysis of safe-control theory, the hard safety constraint can be equivalently translated to identifying the largest feasible region given the offline dataset. This seamlessly converts the original trilogy problem to a feasibility-dependent objective, i.e., maximizing reward value within the f
    
[^27]: 超越RMSE和MAE：引入EAUC来揭示偏见和不公平的迪亚德回归模型中的隐藏因素

    Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models. (arXiv:2401.10690v1 [cs.LG])

    [http://arxiv.org/abs/2401.10690](http://arxiv.org/abs/2401.10690)

    本研究引入EAUC作为一种新的度量标准，用以揭示迪亚德回归模型中隐藏的偏见和不公平问题。传统的全局错误度量标准如RMSE和MAE无法捕捉到这种问题。

    

    迪亚德回归模型用于预测一对实体的实值结果，在许多领域中都是基础的（例如，在推荐系统中预测用户对产品的评分），在许多其他领域中也有许多潜力但尚未深入探索（例如，在个性化药理学中近似确定患者的适当剂量）。本研究中，我们证明个体实体观察值分布的非均匀性导致了最先进模型中的严重偏见预测，偏向于实体的观察过去值的平均值，并在另类但同样重要的情况下提供比随机预测更差的预测能力。我们表明，全局错误度量标准如均方根误差（RMSE）和平均绝对误差（MAE）不足以捕捉到这种现象，我们将其命名为另类偏见，并引入另类-曲线下面积（EAUC）作为一个新的补充度量，可以在所有研究的模型中量化它。

    Dyadic regression models, which predict real-valued outcomes for pairs of entities, are fundamental in many domains (e.g. predicting the rating of a user to a product in Recommender Systems) and promising and under exploration in many others (e.g. approximating the adequate dosage of a drug for a patient in personalized pharmacology). In this work, we demonstrate that non-uniformity in the observed value distributions of individual entities leads to severely biased predictions in state-of-the-art models, skewing predictions towards the average of observed past values for the entity and providing worse-than-random predictive power in eccentric yet equally important cases. We show that the usage of global error metrics like Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) is insufficient to capture this phenomenon, which we name eccentricity bias, and we introduce Eccentricity-Area Under the Curve (EAUC) as a new complementary metric that can quantify it in all studied models
    
[^28]: 用神经假伪距修正实现端到端GPS定位

    Towards End-to-End GPS Localization with Neural Pseudorange Correction. (arXiv:2401.10685v1 [cs.LG])

    [http://arxiv.org/abs/2401.10685](http://arxiv.org/abs/2401.10685)

    本论文提出了一个端到端的GPS定位框架E2E-PrNet，通过直接训练神经网络PrNet来进行伪距修正，实验结果表明其优于现有端到端GPS定位方法。

    

    伪距误差是GPS定位不准确的根本原因。以往的数据驱动方法使用手工制作的中间标签进行伪距误差回归和消除。与之不同的是，我们提出了一个端到端的GPS定位框架E2E-PrNet，通过使用GPS接收机状态的真实值计算最终任务损失，直接训练一个用于伪距修正的神经网络PrNet。损失对可学习参数的梯度通过可微非线性最小二乘优化器反向传播到PrNet。通过使用Android手机收集的GPS数据进行验证，结果显示E2E-PrNet优于最先进的端到端GPS定位方法。

    Pseudorange errors are the root cause of localization inaccuracy in GPS. Previous data-driven methods regress and eliminate pseudorange errors using handcrafted intermediate labels. Unlike them, we propose an end-to-end GPS localization framework, E2E-PrNet, to train a neural network for pseudorange correction (PrNet) directly using the final task loss calculated with the ground truth of GPS receiver states. The gradients of the loss with respect to learnable parameters are backpropagated through a differentiable nonlinear least squares optimizer to PrNet. The feasibility is verified with GPS data collected by Android phones, showing that E2E-PrNet outperforms the state-of-the-art end-to-end GPS localization methods.
    
[^29]: 一种用于单语文本生成的加速多语言语言模型的简单框架

    A Simple Framework to Accelerate Multilingual Language Model for Monolingual Text Generation. (arXiv:2401.10660v1 [cs.CL])

    [http://arxiv.org/abs/2401.10660](http://arxiv.org/abs/2401.10660)

    这项研究介绍了一种新颖的框架，旨在加速非英语语言的文本生成。通过预测更大的语言单元并针对目标语言进行调整，该框架降低了解码步骤的数量，并将生成速度提高了1.9倍。

    

    最近大型语言模型的进展不仅在英语而且在非英语语言中都促进了复杂的语言任务的执行。然而，大多数语言模型的标记器（如Llama）在以英语为中心的语料库上训练，倾向于在非英语语言中过分分割标记。这个问题在非罗马字母语言中尤为明显，这些语言通常在字符或Unicode级别上被划分，导致文本生成速度较慢。为了解决这个问题，我们的研究介绍了一个新颖的框架，旨在加速这些语言的文本生成。该框架预测比传统的多语言标记器更大的语言单元，并且专门针对目标语言进行了调整，从而减少了解码所需的步骤数。我们的实证结果表明，与标准解码相比，所提出的框架将生成速度提高了1.9倍，同时保持了预先训练模型的性能。

    Recent advancements in large language models have facilitated the execution of complex language tasks, not only in English but also in non-English languages. However, the tokenizers of most language models, such as Llama, trained on English-centric corpora, tend to excessively fragment tokens in non-English languages. This issue is especially pronounced in non-roman alphabetic languages, which are often divided at a character or even Unicode level, leading to slower text generation. To address this, our study introduces a novel framework designed to expedite text generation in these languages. This framework predicts larger linguistic units than those of conventional multilingual tokenizers and is specifically tailored to the target language, thereby reducing the number of decoding steps required. Our empirical results demonstrate that the proposed framework increases the generation speed by a factor of 1.9 compared to standard decoding while maintaining the performance of a pre-traine
    
[^30]: 基于深度学习的车辆再识别综合调查：模型、数据集和挑战

    A Comprehensive Survey on Deep-Learning-based Vehicle Re-Identification: Models, Data Sets and Challenges. (arXiv:2401.10643v1 [cs.CV])

    [http://arxiv.org/abs/2401.10643](http://arxiv.org/abs/2401.10643)

    本文通过全面调查了基于深度学习的车辆再识别方法，包括分类、数据集、评估标准以及未来的挑战和研究方向

    

    车辆再识别（ReID）旨在将来自分布在不同交通环境的摄像头网络中的车辆图像进行关联。在车辆中心技术范畴中，该任务具有重要意义，在部署智能交通系统（ITS）和推进智慧城市倡议方面发挥着关键作用。近年来，深度学习的快速发展显著推动了车辆 ReID 技术的演进。因此，对基于深度学习的车辆再识别方法进行全面调查已变得迫切且不可避免。本文深入探讨了应用于车辆 ReID 的深度学习技术，概述了这些方法的分类，包括有监督和无监督方法，深入研究了这些分类中的现有研究，介绍了数据集和评估标准，并勾勒了未来的挑战和潜在的研究方向

    Vehicle re-identification (ReID) endeavors to associate vehicle images collected from a distributed network of cameras spanning diverse traffic environments. This task assumes paramount importance within the spectrum of vehicle-centric technologies, playing a pivotal role in deploying Intelligent Transportation Systems (ITS) and advancing smart city initiatives. Rapid advancements in deep learning have significantly propelled the evolution of vehicle ReID technologies in recent years. Consequently, undertaking a comprehensive survey of methodologies centered on deep learning for vehicle re-identification has become imperative and inescapable. This paper extensively explores deep learning techniques applied to vehicle ReID. It outlines the categorization of these methods, encompassing supervised and unsupervised approaches, delves into existing research within these categories, introduces datasets and evaluation criteria, and delineates forthcoming challenges and potential research dire
    
[^31]: 大型标记图的快速蝶形核社区搜索

    Fast Butterfly-Core Community Search For Large Labeled Graphs. (arXiv:2401.10642v1 [cs.SI])

    [http://arxiv.org/abs/2401.10642](http://arxiv.org/abs/2401.10642)

    本文提出了一个基于蝶形核社区结构的快速社区搜索模型，通过随机游走与重启算法和蝶形度来评估社区内顶点的重要性，并通过更高效的顶点距离更新方法来提高操作效率，从而有效地在大型图中寻找密集连接的子图。

    

    社区搜索旨在在图中识别与查询顶点对应的密集连接的子图。然而，现有的异构图社区搜索方法需要帮助识别跨组社区，并且存在效率问题，使其不适用于大型图。本文提出了一种基于蝶形核社区（BCC）结构的快速社区搜索模型，用于处理异构图。通过随机游走与重启（RWR）算法和蝶形度来全面评估社区内顶点的重要性，从而快速更新领导顶点以保持跨组连贯性。此外，我们设计了一种更高效的顶点距离更新方法，以最小化顶点访问次数并增强操作效率。对几个真实世界的时间图进行了大量实验，验证了该解决方案的有效性和效率。

    Community Search (CS) aims to identify densely interconnected subgraphs corresponding to query vertices within a graph. However, existing heterogeneous graph-based community search methods need help identifying cross-group communities and suffer from efficiency issues, making them unsuitable for large graphs. This paper presents a fast community search model based on the Butterfly-Core Community (BCC) structure for heterogeneous graphs. The Random Walk with Restart (RWR) algorithm and butterfly degree comprehensively evaluate the importance of vertices within communities, allowing leader vertices to be rapidly updated to maintain cross-group cohesion. Moreover, we devised a more efficient method for updating vertex distances, which minimizes vertex visits and enhances operational efficiency. Extensive experiments on several real-world temporal graphs demonstrate the effectiveness and efficiency of this solution.
    
[^32]: 基于桁架的大型有向图社区搜索的有效索引

    An Effective Index for Truss-based Community Search on Large Directed Graphs. (arXiv:2401.10641v1 [cs.SI])

    [http://arxiv.org/abs/2401.10641](http://arxiv.org/abs/2401.10641)

    该论文提出了一种基于D-truss的索引方法，在大型有向图上实现了高效的社区搜索，解决了现有方法消耗过多计算资源的问题。

    

    社区搜索是社区检测的一个衍生问题，可以在线和个性化地发现社区，在大规模实际网络中应用广泛。尽管对于无向图已进行了大量研究，但最近对于有向图中的社区搜索问题需要更多关注。最近提出的D-truss模型在检索到的社区质量上取得了良好的结果。然而，现有的基于D-truss的工作无法在大型图上进行高效的社区搜索，因为检索最大D-truss所需的计算资源太多。为了解决这个问题，我们引入了一种创新的合并关系，即D-truss-connected，以捕捉D-truss中边缘的内在密度和凝聚力。该关系将所有原始图中的边缘划分为一系列D-truss-connected类。然后，我们基于D-truss构造了一个简洁且紧凑的索引，称为ConDTruss。

    Community search is a derivative of community detection that enables online and personalized discovery of communities and has found extensive applications in massive real-world networks. Recently, there needs to be more focus on the community search issue within directed graphs, even though substantial research has been carried out on undirected graphs. The recently proposed D-truss model has achieved good results in the quality of retrieved communities. However, existing D-truss-based work cannot perform efficient community searches on large graphs because it consumes too many computing resources to retrieve the maximal D-truss. To overcome this issue, we introduce an innovative merge relation known as D-truss-connected to capture the inherent density and cohesiveness of edges within D-truss. This relation allows us to partition all the edges in the original graph into a series of D-truss-connected classes. Then, we construct a concise and compact index, ConDTruss, based on D-truss-co
    
[^33]: XAI的忠诚度度量的综合研究

    A comprehensive study on fidelity metrics for XAI. (arXiv:2401.10640v1 [cs.CV])

    [http://arxiv.org/abs/2401.10640](http://arxiv.org/abs/2401.10640)

    这项研究提出了一种验证可解释人工智能方法忠诚度的方法，并通过应用于两个不同实验中的现有度量进行了评估，成为这些度量的客观基准，超越了现有方法。

    

    可解释的人工智能(XAI)系统的使用引入了一系列需要解决的挑战。在此，我们重点研究如何正确选择XAI方法，这是该领域的一个开放问题。这个任务的困难在于缺乏标准答案。一些作者提出了度量不同XAI方法忠诚度的方法。这些度量缺乏验证，并存在有关的分歧。在本研究中，我们提出了一种新的方法来验证忠诚度度量，使用一个众所周知的透明模型，即决策树。这个模型使我们能够获得完美忠诚度的解释。我们的提议构成了这些度量的第一个客观基准，便于比较现有的提案，并超越现有的方法。我们将我们的基准应用于评估两个不同实验中现有的忠诚度度量，每个实验使用包含52,000张图片的公共数据集。这些数据集的图片大小为。

    The use of eXplainable Artificial Intelligence (XAI) systems has introduced a set of challenges that need resolution. Herein, we focus on how to correctly select an XAI method, an open questions within the field. The inherent difficulty of this task is due to the lack of a ground truth. Several authors have proposed metrics to approximate the fidelity of different XAI methods. These metrics lack verification and have concerning disagreements. In this study, we proposed a novel methodology to verify fidelity metrics, using a well-known transparent model, namely a decision tree. This model allowed us to obtain explanations with perfect fidelity. Our proposal constitutes the first objective benchmark for these metrics, facilitating a comparison of existing proposals, and surpassing existing methods. We applied our benchmark to assess the existing fidelity metrics in two different experiments, each using public datasets comprising 52,000 images. The images from these datasets had a size a 
    
[^34]: ZnTrack -- 数据即代码

    ZnTrack -- Data as Code. (arXiv:2401.10603v1 [cs.SE])

    [http://arxiv.org/abs/2401.10603](http://arxiv.org/abs/2401.10603)

    ZnTrack是一个Python驱动的数据版本控制工具，通过简化大型数据集为Python脚本的形式，提供了一个以数据为代码的概念，实现了对参数跟踪、工作流设计以及数据存储和共享的用户友好界面。

    

    过去十年来，计算机领域取得了巨大的突破，而且没有迹象表明这种趋势会在短期内减慢。机器学习、大规模计算资源和增加的行业关注导致了对数据管理、模拟和模型生成的计算机驱动解决方案的投资增加。然而，随着计算能力的增长，数据的扩大也带来了数据存储、共享和追踪的复杂性。在这项工作中，我们介绍了一个名为ZnTrack的Python驱动数据版本控制工具。ZnTrack基于已建立的版本控制系统，提供了一个用户友好且易于使用的界面，用于跟踪实验中的参数、设计工作流、以及存储和共享数据。通过将大型数据集简化为一个简单的Python脚本，产生了“数据即代码”的概念，这是本文所介绍的工作的核心组成部分，也是计算时代继续演变的一个无疑重要的概念。ZnTrack提供了一种方法，可以将数据当作代码处理。

    The past decade has seen tremendous breakthroughs in computation and there is no indication that this will slow any time soon. Machine learning, large-scale computing resources, and increased industry focus have resulted in rising investments in computer-driven solutions for data management, simulations, and model generation. However, with this growth in computation has come an even larger expansion of data and with it, complexity in data storage, sharing, and tracking. In this work, we introduce ZnTrack, a Python-driven data versioning tool. ZnTrack builds upon established version control systems to provide a user-friendly and easy-to-use interface for tracking parameters in experiments, designing workflows, and storing and sharing data. From this ability to reduce large datasets to a simple Python script emerges the concept of Data as Code, a core component of the work presented here and an undoubtedly important concept as the age of computation continues to evolve. ZnTrack offers an
    
[^35]: 重新思考MaxSAT局部搜索解算器中的软冲突伪布尔约束

    Rethinking the Soft Conflict Pseudo Boolean Constraint on MaxSAT Local Search Solvers. (arXiv:2401.10589v1 [cs.AI])

    [http://arxiv.org/abs/2401.10589](http://arxiv.org/abs/2401.10589)

    本论文提出将软冲突伪布尔约束转化为局部搜索方法的子句加权系统，并提出了一种自适应子句加权策略。基于这些方法，提出了一种名为SPB-MaxSAT的新型局部搜索算法，为MaxSAT局部搜索解算器的子句加权提供了新的视角。

    

    MaxSAT是著名的NP完全可满足性问题（SAT）的优化版本。MaxSAT算法主要包括完全解算器和局部搜索不完全解算器。在许多完全解算器中，一旦找到更好的解，就会生成一个软冲突伪布尔（SPB）约束，强制算法寻找更好的解。在许多局部搜索算法中，子句加权是一种有效引导搜索方向的关键技术。本文提出将SPB约束转化为局部搜索方法的子句加权系统，使算法能够找到更好的解。我们进一步提出了一种自适应子句加权策略，打破了使用常数值来调整子句权重的传统。基于上述方法，我们提出了一种名为SPB-MaxSAT的新型局部搜索算法，为MaxSAT局部搜索解算器的子句加权提供了新的视角。广泛的实验表明了SPB-MaxSAT算法的优异性能。

    MaxSAT is an optimization version of the famous NP-complete Satisfiability problem (SAT). Algorithms for MaxSAT mainly include complete solvers and local search incomplete solvers. In many complete solvers, once a better solution is found, a Soft conflict Pseudo Boolean (SPB) constraint will be generated to enforce the algorithm to find better solutions. In many local search algorithms, clause weighting is a key technique for effectively guiding the search directions. In this paper, we propose to transfer the SPB constraint into the clause weighting system of the local search method, leading the algorithm to better solutions. We further propose an adaptive clause weighting strategy that breaks the tradition of using constant values to adjust clause weights. Based on the above methods, we propose a new local search algorithm called SPB-MaxSAT that provides new perspectives for clause weighting on MaxSAT local search solvers. Extensive experiments demonstrate the excellent performance of
    
[^36]: PuriDefense：用于防御黑盒基于查询的攻击的随机局部隐式对抗净化

    PuriDefense: Randomized Local Implicit Adversarial Purification for Defending Black-box Query-based Attacks. (arXiv:2401.10586v1 [cs.CR])

    [http://arxiv.org/abs/2401.10586](http://arxiv.org/abs/2401.10586)

    PuriDefense是一种高效的防御机制，通过使用轻量级净化模型进行随机路径净化，减缓基于查询的攻击的收敛速度，并有效防御黑盒基于查询的攻击。

    

    黑盒基于查询的攻击对机器学习作为服务系统构成重大威胁，因为它们可以生成对抗样本而不需要访问目标模型的架构和参数。传统的防御机制，如对抗训练、梯度掩盖和输入转换，要么带来巨大的计算成本，要么损害非对抗输入的测试准确性。为了应对这些挑战，我们提出了一种高效的防御机制PuriDefense，在低推理成本的级别上使用轻量级净化模型的随机路径净化。这些模型利用局部隐式函数并重建自然图像流形。我们的理论分析表明，这种方法通过将随机性纳入净化过程来减缓基于查询的攻击的收敛速度。对CIFAR-10和ImageNet的大量实验验证了我们提出的净化器防御的有效性。

    Black-box query-based attacks constitute significant threats to Machine Learning as a Service (MLaaS) systems since they can generate adversarial examples without accessing the target model's architecture and parameters. Traditional defense mechanisms, such as adversarial training, gradient masking, and input transformations, either impose substantial computational costs or compromise the test accuracy of non-adversarial inputs. To address these challenges, we propose an efficient defense mechanism, PuriDefense, that employs random patch-wise purifications with an ensemble of lightweight purification models at a low level of inference cost. These models leverage the local implicit function and rebuild the natural image manifold. Our theoretical analysis suggests that this approach slows down the convergence of query-based attacks by incorporating randomness into purifications. Extensive experiments on CIFAR-10 and ImageNet validate the effectiveness of our proposed purifier-based defen
    
[^37]: CivRealm: 一个在文明游戏中的学习与推理奇幻之旅

    CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents. (arXiv:2401.10568v1 [cs.AI])

    [http://arxiv.org/abs/2401.10568](http://arxiv.org/abs/2401.10568)

    CivRealm是一个受文明游戏启发的环境，要求代理人在复杂的情境中进行学习和推理，以应对变化的游戏规则和随机环境。

    

    决策代理的泛化包括两个基本元素：从过去经验中学习和在新情境中进行推理。然而，在大多数交互环境中，对学习的重视往往以牺牲推理复杂性为代价。在本文中，我们介绍了CivRealm，一个受到文明游戏启发的环境。文明游戏与人类历史和社会的深刻契合需要复杂的学习，而其不断变化的情境则要求强大的推理能力。特别是，CivRealm建立了一个有着不完全信息和变化人数的总和游戏；它呈现了众多复杂的特征，挑战代理人处理开放的、随机的环境，需要外交和谈判技巧。在CivRealm中，我们为两种典型的代理类型提供了接口：基于张量的学习型代理和基于语言的推理型代理。

    The generalization of decision-making agents encompasses two fundamental elements: learning from past experiences and reasoning in novel contexts. However, the predominant emphasis in most interactive environments is on learning, often at the expense of complexity in reasoning. In this paper, we introduce CivRealm, an environment inspired by the Civilization game. Civilization's profound alignment with human history and society necessitates sophisticated learning, while its ever-changing situations demand strong reasoning to generalize. Particularly, CivRealm sets up an imperfect-information general-sum game with a changing number of players; it presents a plethora of complex features, challenging the agent to deal with open-ended stochastic environments that require diplomacy and negotiation skills. Within CivRealm, we provide interfaces for two typical agent types: tensor-based agents that focus on learning, and language-based agents that emphasize reasoning. To catalyze further rese
    
[^38]: OrchMoE：具有任务-技能协同效应的高效多适配器学习

    OrchMoE: Efficient Multi-Adapter Learning with Task-Skill Synergy. (arXiv:2401.10559v1 [cs.LG])

    [http://arxiv.org/abs/2401.10559](http://arxiv.org/abs/2401.10559)

    OrchMoE通过利用模块化技能架构和自动任务识别，提升了参数效率微调领域的性能，实现了对多任务学习的重大进展。

    

    我们通过创新的多适配器方法OrchMoE推进了参数效率微调（PEFT）领域，利用模块化技能架构增强神经网络的前向传递。与依赖显式任务识别输入的先前模型不同，OrchMoE自动识别任务类别，简化学习过程。这是通过一个整合机制实现的，包括自动任务分类模块和任务-技能分配模块，共同推断任务特定的分类并调整技能分配矩阵。我们在“超自然指令”数据集上进行了广泛的评估，该数据集包含1,600个多样的指令任务，结果表明OrchMoE在性能和样本利用效率方面明显优于可比的多适配器基线，并且在相同参数限制下运行。这些发现表明，OrchMoE在多任务学习方面取得了重大进展。

    We advance the field of Parameter-Efficient Fine-Tuning (PEFT) with our novel multi-adapter method, OrchMoE, which capitalizes on modular skill architecture for enhanced forward transfer in neural networks. Unlike prior models that depend on explicit task identification inputs, OrchMoE automatically discerns task categories, streamlining the learning process. This is achieved through an integrated mechanism comprising an Automatic Task Classification module and a Task-Skill Allocation module, which collectively deduce task-specific classifications and tailor skill allocation matrices. Our extensive evaluations on the 'Super Natural Instructions' dataset, featuring 1,600 diverse instructional tasks, indicate that OrchMoE substantially outperforms comparable multi-adapter baselines in terms of both performance and sample utilization efficiency, all while operating within the same parameter constraints. These findings suggest that OrchMoE offers a significant leap forward in multi-task le
    
[^39]: AAT: 适应不同声学识别任务的音频Transformer

    AAT: Adapting Audio Transformer for Various Acoustics Recognition Tasks. (arXiv:2401.10544v1 [cs.SD])

    [http://arxiv.org/abs/2401.10544](http://arxiv.org/abs/2401.10544)

    AAT提出了一种基于Adapter微调的高效微调方法，能够在不牺牲模型的通用性的情况下获得下游任务的知识。

    

    最近，Transformer被引入到声学识别领域。它们使用监督学习和半监督学习等方法在大规模数据集上进行预训练，展示了稳健的通用性 - 它容易对下游任务进行微调并显示出更强的性能。然而，目前主要使用的微调方法仍然是完全微调，即在训练过程中更新所有参数。这不仅导致显著的内存使用和时间成本，还损害了模型的通用性。其他微调方法要么无法解决这个问题，要么无法达到相匹配的性能。因此，我们对现有微调方法进行了全面的分析，并提出了一种基于Adapter微调的高效微调方法，即AAT。核心思想是冻结音频Transformer模型并插入额外的可学习Adapter，以高效获取下游任务的知识而不损害模型的通用性。

    Recently, Transformers have been introduced into the field of acoustics recognition. They are pre-trained on large-scale datasets using methods such as supervised learning and semi-supervised learning, demonstrating robust generality--It fine-tunes easily to downstream tasks and shows more robust performance. However, the predominant fine-tuning method currently used is still full fine-tuning, which involves updating all parameters during training. This not only incurs significant memory usage and time costs but also compromises the model's generality. Other fine-tuning methods either struggle to address this issue or fail to achieve matching performance. Therefore, we conducted a comprehensive analysis of existing fine-tuning methods and proposed an efficient fine-tuning approach based on Adapter tuning, namely AAT. The core idea is to freeze the audio Transformer model and insert extra learnable Adapters, efficiently acquiring downstream task knowledge without compromising the model'
    
[^40]: Mementos: 一种针对图像序列的多模态大型语言模型推理的综合基准测试

    Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences. (arXiv:2401.10529v1 [cs.CV])

    [http://arxiv.org/abs/2401.10529](http://arxiv.org/abs/2401.10529)

    Mementos是一个新的基准测试，旨在评估多模态大型语言模型在图像序列推理中的能力。研究发现，现有的MLLM在准确描述图像序列的动态信息方面存在困难，容易导致物体及其行为的错误描述或错觉。

    

    多模态大型语言模型（MLLMs）在处理各种视觉语言任务方面展示了高超的能力。然而，目前的MLLM基准测试主要用于评估基于单个图像的静态信息的推理能力，而现代MLLM在从图像序列中进行推断的能力，在理解不断变化的世界方面的重要性却被较少研究。为了解决这一挑战，本文引入了一个新的基准测试Mementos，用于评估MLLM的序列图像推理能力。Mementos包括4761个具有不同长度的多样的图像序列。我们还采用了GPT-4辅助方法来评估MLLM的推理性能。通过对Mementos中包括GPT-4V和Gemini在内的九个最新MLLM进行仔细评估，我们发现它们在准确描述所给图像序列的动态信息方面存在困难，往往导致对象及其对应行为的错误描述或错觉。

    Multimodal Large Language Models (MLLMs) have demonstrated proficiency in handling a variety of visual-language tasks. However, current MLLM benchmarks are predominantly designed to evaluate reasoning based on static information about a single image, and the ability of modern MLLMs to extrapolate from image sequences, which is essential for understanding our ever-changing world, has been less investigated. To address this challenge, this paper introduces Mementos, a new benchmark designed to assess MLLMs' sequential image reasoning abilities. Mementos features 4,761 diverse image sequences with varying lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning performance. Through a careful evaluation of nine recent MLLMs on Mementos, including GPT-4V and Gemini, we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors. Our quantitati
    
[^41]: 多语言语言模型中的跨语言编辑

    Cross-lingual Editing in Multilingual Language Models. (arXiv:2401.10521v1 [cs.CL])

    [http://arxiv.org/abs/2401.10521](http://arxiv.org/abs/2401.10521)

    本论文介绍了跨语言模型编辑（XME）范式，通过在一种语言中编辑事实并观察其对其他语言的更新传播，研究了多语言语言模型中的模型编辑技术（MET）的性能限制。

    

    大规模语言模型（LLM）的训练需要大量的数据和计算资源，而更新过时的LLM需要大量的工作和资源。虽然出现了许多模型编辑技术（MET）以便在不重新训练的情况下高效更新模型输出，但在多语言LLM中，其中的知识以多种语言存储，这仍然是一个未深入研究的领域。本研究介绍了跨语言模型编辑（XME）范式，在该范式中，一个事实在一种语言中被编辑，观察其在其他语言中的更新传播。为了研究XME范式，我们使用BLOOM、mBERT和XLM-RoBERTa进行了实验，使用了两种写作脚本，即拉丁语（英语、法语和西班牙语）和印地语（印地语、古吉拉特语和孟加拉语）。结果显示，在XME设置下，当前最先进的MET存在明显的性能限制，特别是当涉及的语言属于两个不同的语族时。

    The training of large language models (LLMs) necessitates substantial data and computational resources, and updating outdated LLMs entails significant efforts and resources. While numerous model editing techniques (METs) have emerged to efficiently update model outputs without retraining, their effectiveness in multilingual LLMs, where knowledge is stored in diverse languages, remains an underexplored research area. This research paper introduces the cross-lingual model editing (\textbf{XME}) paradigm, wherein a fact is edited in one language, and the subsequent update propagation is observed across other languages. To investigate the XME paradigm, we conducted experiments using BLOOM, mBERT, and XLM-RoBERTa using the two writing scripts: \textit{Latin} (English, French, and Spanish) and \textit{Indic} (Hindi, Gujarati, and Bengali). The results reveal notable performance limitations of state-of-the-art METs under the XME setting, mainly when the languages involved belong to two distin
    
[^42]: 扩展状态-奖励空间的情节式强化学习

    Episodic Reinforcement Learning with Expanded State-reward Space. (arXiv:2401.10516v1 [cs.LG])

    [http://arxiv.org/abs/2401.10516](http://arxiv.org/abs/2401.10516)

    通过引入扩展的状态-奖励空间，我们提出了一种高效的情节式强化学习（DRL）框架，可以改善DRL中状态与奖励空间之间的不对齐问题，从而提高值的估计准确性和策略性能。

    

    借助深度神经网络的力量，深度强化学习（DRL）在游戏、医疗保健和自动驾驶等各个领域取得了巨大的经验成功。尽管取得了这些进展，但是由于有效策略需要大量的环境样本，DRL仍被认为是数据效率低下的。最近，基于情节控制（EC）的无模型DRL方法通过从情节记忆中回顾过去的经验实现了样本效率。然而，现有的基于EC的方法由于忽略了利用（过去的）检索状态的广泛信息，存在状态和奖励空间之间的潜在不对齐的限制，这可能导致值估计不准确和策略性能下降。为了解决这个问题，我们引入了一种具有扩展状态-奖励空间的高效EC型DRL框架，其中用作输入的扩展状态和用于训练的扩展奖励都包含历史和当前信息。

    Empowered by deep neural networks, deep reinforcement learning (DRL) has demonstrated tremendous empirical successes in various domains, including games, health care, and autonomous driving. Despite these advancements, DRL is still identified as data-inefficient as effective policies demand vast numbers of environmental samples. Recently, episodic control (EC)-based model-free DRL methods enable sample efficiency by recalling past experiences from episodic memory. However, existing EC-based methods suffer from the limitation of potential misalignment between the state and reward spaces for neglecting the utilization of (past) retrieval states with extensive information, which probably causes inaccurate value estimation and degraded policy performance. To tackle this issue, we introduce an efficient EC-based DRL framework with expanded state-reward space, where the expanded states used as the input and the expanded rewards used in the training both contain historical and current informa
    
[^43]: 天作之合：大型语言模型与进化算法的结合

    A match made in consistency heaven: when large language models meet evolutionary algorithms. (arXiv:2401.10510v1 [cs.NE])

    [http://arxiv.org/abs/2401.10510](http://arxiv.org/abs/2401.10510)

    大型语言模型和进化算法的结合具有强大的一致性，包括标记嵌入和基因型-表现型映射、位置编码和适应性塑造、位置嵌入和选择、注意力和交叉、前馈神经网络和突变、模型训练和参数更新以及多任务学习和多目标优化等多个核心特征。本文分析了现有的耦合研究，并为未来的研究提供了基本路线和关键挑战。

    

    预训练的大型语言模型（LLMs）在生成创造性的自然文本方面具有强大的能力。进化算法（EAs）可以发现复杂实际问题的多样解决方案。本文通过比较文本序列生成和进化的共同特点和方向性，阐述了LLMs与EAs之间的强大一致性，包括多个一对一的核心特征：标记嵌入和基因型-表现型映射、位置编码和适应性塑造、位置嵌入和选择、注意力和交叉、前馈神经网络和突变、模型训练和参数更新以及多任务学习和多目标优化。在这种一致性视角下，分析了现有的耦合研究，包括进化微调和LLM增强型EAs。借助这些洞见，我们概述了未来在LLMs和EAs耦合方面的基本研究路线，并突出了其中的关键挑战。

    Pre-trained large language models (LLMs) have powerful capabilities for generating creative natural text. Evolutionary algorithms (EAs) can discover diverse solutions to complex real-world problems. Motivated by the common collective and directionality of text sequence generation and evolution, this paper illustrates the strong consistency of LLMs and EAs, which includes multiple one-to-one key characteristics: token embedding and genotype-phenotype mapping, position encoding and fitness shaping, position embedding and selection, attention and crossover, feed-forward neural network and mutation, model training and parameter update, and multi-task learning and multi-objective optimization. Based on this consistency perspective, existing coupling studies are analyzed, including evolutionary fine-tuning and LLM-enhanced EAs. Leveraging these insights, we outline a fundamental roadmap for future research in coupling LLMs and EAs, while highlighting key challenges along the way. The consist
    
[^44]: FinSQL：面向金融分析的基于模型无关的基于LLMs的文本到SQL框架

    FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial Analysis. (arXiv:2401.10506v1 [cs.CL])

    [http://arxiv.org/abs/2401.10506](http://arxiv.org/abs/2401.10506)

    本论文提出了一种面向金融分析的基于LLMs的模型无关的文本到SQL框架FinSQL，同时还提供了一个实用的金融分析文本到SQL基准数据集BULL，从提示构造和参数化的角度为金融文本到SQL提供了系统化的处理。

    

    这项研究关注的是文本到SQL的领域，该领域为操作关系数据库提供了无代码接口，在金融分析中引起了广泛关注；因为金融专业人员可能不擅长SQL编程。然而，迄今为止，还没有为金融分析提供实用的文本到SQL基准数据集，并且现有的文本到SQL方法没有考虑到金融应用中数据库的独特特点，如通常存在的宽表。为了解决这些问题，我们收集了一个实用的金融分析文本到SQL基准数据集，并提出了一种基于模型无关的大型语言模型（LLMs）的金融分析文本到SQL框架。这个基准数据集BULL是从恒生科技公司的实际金融分析业务中收集的，包括基金、股票和宏观经济的数据库。除此之外，提出的基于LLMs的文本到SQL框架FinSQL从提示构造和参数化的角度为金融文本到SQL提供了系统化的处理。

    Text-to-SQL, which provides zero-code interface for operating relational databases, has gained much attention in financial analysis; because, financial professionals may not well-skilled in SQL programming. However, until now, there is no practical Text-to-SQL benchmark dataset for financial analysis, and existing Text-to-SQL methods have not considered the unique characteristics of databases in financial applications, such as commonly existing wide tables. To address these issues, we collect a practical Text-to-SQL benchmark dataset and propose a model-agnostic Large Language Model (LLMs)-based Text-to-SQL framework for financial analysis. The benchmark dataset, BULL, is collected from the practical financial analysis business of Hundsun Technologies Inc., including databases for fund, stock, and macro economy. Besides, the proposed LLMs-based Text-to-SQL framework, FinSQL, provides a systematic treatment for financial Text-to-SQL from the perspectives of prompt construction, paramete
    
[^45]: 条件熵下的因果层析

    Causal Layering via Conditional Entropy. (arXiv:2401.10495v1 [cs.LG])

    [http://arxiv.org/abs/2401.10495](http://arxiv.org/abs/2401.10495)

    本文提出了一种通过条件熵Oracle来恢复图层析的方法，用于离散分布情况下的因果发现。算法通过比较节点的条件熵和噪声的无条件熵，通过删除源或汇点来实现节点的分离。算法具有可证明的正确性和二次时间复杂度。

    

    因果发现旨在从可观测到的数据中恢复关于未观测到的因果图的信息。层析是对变量进行排序，将因果放在效应之前。在本文中，我们提供了通过访问条件熵Oracle来恢复图层析的方法，当分布是离散的时候。我们的算法通过不断从图中删除源或汇点来工作。在适当的假设和条件下，我们可以通过比较它们的条件熵与噪声的无条件熵来将源或汇点与其余节点分离开来。我们的算法在最坏情况下的时间复杂度是二次的，并且经过证明是正确的。主要的假设是忠实性和单射噪声，以及已知噪声熵或沿着有向路径弱单调递增的噪声熵之一。此外，我们需要忠实性的一个非常温和的扩展，或者严格单调递增的噪声熵，或者扩展到无限值。

    Causal discovery aims to recover information about an unobserved causal graph from the observable data it generates. Layerings are orderings of the variables which place causes before effects. In this paper, we provide ways to recover layerings of a graph by accessing the data via a conditional entropy oracle, when distributions are discrete. Our algorithms work by repeatedly removing sources or sinks from the graph. Under appropriate assumptions and conditioning, we can separate the sources or sinks from the remainder of the nodes by comparing their conditional entropy to the unconditional entropy of their noise. Our algorithms are provably correct and run in worst-case quadratic time. The main assumptions are faithfulness and injective noise, and either known noise entropies or weakly monotonically increasing noise entropies along directed paths. In addition, we require one of either a very mild extension of faithfulness, or strictly monotonically increasing noise entropies, or expan
    
[^46]: 基于彩票票据假设和知识蒸馏的神经网络剪枝技术增强推荐系统的可扩展性

    Enhancing Scalability in Recommender Systems through Lottery Ticket Hypothesis and Knowledge Distillation-based Neural Network Pruning. (arXiv:2401.10484v1 [cs.IR])

    [http://arxiv.org/abs/2401.10484](http://arxiv.org/abs/2401.10484)

    本研究通过将彩票票据假设和知识蒸馏框架相结合，提出了一种剪枝神经网络的创新方法，以解决推荐系统在边缘设备上的可扩展性问题。经实验证明，该方法能够有效地降低功耗和模型尺寸，并实现高达66.67%的GPU计算功率减少。此外，本研究还首次应用了彩票票据假设和知识蒸馏技术于推荐系统领域，对该领域作出了重要贡献。

    

    本研究介绍了一种创新的方法，旨在高效地剪枝神经网络，特别关注它们在边缘设备上的部署。我们的方法将彩票票据假设（LTH）与知识蒸馏（KD）框架相结合，形成了三种不同的剪枝模型。这些模型旨在解决推荐系统中的可扩展性问题，其中深度学习模型的复杂性妨碍了它们的实际部署。通过巧妙应用剪枝技术，我们有效地降低了功耗和模型尺寸，同时不影响准确性。我们使用两个真实世界数据集对两个基准进行了实证评估。令人满意的是，我们的方法在GPU计算功率上实现了高达66.67%的降低。值得注意的是，我们的研究通过首次应用LTH和KD技术，对推荐系统领域做出了贡献。

    This study introduces an innovative approach aimed at the efficient pruning of neural networks, with a particular focus on their deployment on edge devices. Our method involves the integration of the Lottery Ticket Hypothesis (LTH) with the Knowledge Distillation (KD) framework, resulting in the formulation of three distinct pruning models. These models have been developed to address scalability issue in recommender systems, whereby the complexities of deep learning models have hindered their practical deployment. With judicious application of the pruning techniques, we effectively curtail the power consumption and model dimensions without compromising on accuracy. Empirical evaluation has been performed using two real world datasets from diverse domains against two baselines. Gratifyingly, our approaches yielded a GPU computation-power reduction of up to 66.67%. Notably, our study contributes to the field of recommendation system by pioneering the application of LTH and KD.
    
[^47]: 逃离高昂成本：多步推理的提前停止自一致性

    Escape Sky-high Cost: Early-stopping Self-Consistency for Multi-step Reasoning. (arXiv:2401.10480v1 [cs.CL])

    [http://arxiv.org/abs/2401.10480](http://arxiv.org/abs/2401.10480)

    本文提出了一种称为提前停止自一致性（ESC）的简单且可扩展的采样策略，用于降低多步推理任务中自一致性的成本，通过大量实验证明了其有效性。

    

    自一致性是一种广泛应用于思维链推理的解码策略。尽管在各种多步推理任务中带来了显著的性能提升，但自一致性是一种高成本的方法，需要进行多次预设大小的抽样。本文提出了一种简单且可扩展的采样过程——提前停止自一致性（ESC），以极大降低自一致性的成本，同时不损害性能。在此基础上，还进一步推导了一种控制方案，可以动态选择不同任务和模型的性能和成本平衡。为了证明ESC的有效性，我们在三个常见的推理任务类别上进行了大量实验：算术推理、常识推理和符号推理。实验结果表明，在包括MATH在内的六个基准测试中，ESC将思维链推理的平均抽样次数显著减少了（-33.8%）。

    Self-consistency (SC) has been a widely used decoding strategy for chain-of-thought reasoning. Despite bringing significant performance improvements across a variety of multi-step reasoning tasks, it is a high-cost method that requires multiple sampling with the preset size. In this paper, we propose a simple and scalable sampling process, \textbf{E}arly-Stopping \textbf{S}elf-\textbf{C}onsistency (ESC), to greatly reduce the cost of SC without sacrificing performance. On this basis, one control scheme for ESC is further derivated to dynamically choose the performance-cost balance for different tasks and models. To demonstrate ESC's effectiveness, we conducted extensive experiments on three popular categories of reasoning tasks: arithmetic, commonsense and symbolic reasoning over language models with varying scales. The empirical results show that ESC reduces the average number of sampling of chain-of-thought reasoning by a significant margin on six benchmarks, including MATH (-33.8%),
    
[^48]: LDReg: 本地维度正则化的自监督学习

    LDReg: Local Dimensionality Regularized Self-Supervised Learning. (arXiv:2401.10474v1 [cs.LG])

    [http://arxiv.org/abs/2401.10474](http://arxiv.org/abs/2401.10474)

    本文提出了一种叫做LDReg的本地维度正则化方法，用于解决自监督学习中的维度坍缩问题。通过增加局部内在维度，LDReg能够改善表示的性能。

    

    通过自监督学习（SSL）学习的表示可能容易出现维度坍缩，其中学习的表示子空间维度极低，因此无法表示完整的数据分布和模态。维度坍缩也被称为“填充不足”现象，是下游任务性能下降的主要原因之一。之前的工作在全局层面上研究了SSL的维度坍缩问题。在本文中，我们证明表示可以在全局上覆盖高维空间，但在局部上会坍缩。为了解决这个问题，我们提出了一种称为“本地维度正则化（LDReg）”的方法。我们的公式是基于Fisher-Rao度量的推导，用于比较和优化每个数据点在渐进小半径处的局部距离分布。通过增加局部内在维度，我们通过一系列实验证明LDReg可以改善表示。

    Representations learned via self-supervised learning (SSL) can be susceptible to dimensional collapse, where the learned representation subspace is of extremely low dimensionality and thus fails to represent the full data distribution and modalities. Dimensional collapse also known as the "underfilling" phenomenon is one of the major causes of degraded performance on downstream tasks. Previous work has investigated the dimensional collapse problem of SSL at a global level. In this paper, we demonstrate that representations can span over high dimensional space globally, but collapse locally. To address this, we propose a method called $\textit{local dimensionality regularization (LDReg)}$. Our formulation is based on the derivation of the Fisher-Rao metric to compare and optimize local distance distributions at an asymptotically small radius for each data point. By increasing the local intrinsic dimensionality, we demonstrate through a range of experiments that LDReg improves the repres
    
[^49]: DeepEdit: 带有约束的解码式知识编辑

    DeepEdit: Knowledge Editing as Decoding with Constraints. (arXiv:2401.10471v1 [cs.CL])

    [http://arxiv.org/abs/2401.10471](http://arxiv.org/abs/2401.10471)

    DeepEdit是一种神经符号方法，通过更好的推理一致性和对更新知识的意识，提高了大型语言模型的知识编辑能力，对多跳问题数据集MQuaKE取得了显著的进展。

    

    我们将大型语言模型（LLMs）的知识编辑视为带有约束的解码过程。我们提出了DeepEdit（基于深度优先搜索的渐进式解码知识编辑），这是一种神经符号方法，通过更好的推理一致性、问题相关性和对更新知识的意识来改进知识编辑。DeepEdit可灵活应用于所有黑盒LLMs：不需要访问模型参数、表示或输出词汇分布。DeepEdit逐步产生高质量的推理步骤，以实现有效的知识编辑。它利用深度优先搜索来修改LLMs的输出，从而提高输出对问题的相关性和对更新知识的意识。在知识编辑方面，DeepEdit在控制LLMs产生更简洁的推理方面表现出色。在MQuaKE上，DeepEdit在定量上取得了显著的进展，这是一个具有挑战性的多跳问题数据集。

    We develop a new perspective of knowledge editing for large language models (LLMs) as decoding with constraints. We propose DeepEdit (Depth-first Search based Progressive Decoding for Knowledge Editing), a neuro-symbolic method that improves knowledge editing with better coherence of reasoning, relevance to the question, and awareness of updated knowledge. DeepEdit can be flexibly applied to all black-box LLMs: it does not require any access to the model parameters, representations, or output vocabulary distributions. DeepEdit progressively produces the high-quality reasoning steps towards effective knowledge editing. It utilizes a depth-first search to revise the LLMs' output, which improves the output's informativeness to the input question and awareness of the updated knowledge. Qualitatively, DeepEdit effectively controls LLMs to produce more succinct reasoning in accord with knowledge editing. Quantitatively, DeepEdit yields significant gains on MQuaKE, a challenging multi-hop que
    
[^50]: 使用对比学习学习混合整数规划的后门

    Learning Backdoors for Mixed Integer Programs with Contrastive Learning. (arXiv:2401.10467v1 [cs.AI])

    [http://arxiv.org/abs/2401.10467](http://arxiv.org/abs/2401.10467)

    本论文提出了使用对比学习方法来学习混合整数规划的后门，通过收集用于训练的后门并训练图注意力网络模型来预测后门，取得了比Gurobi和先前模型更好的性能改进。

    

    许多现实世界中的问题可以有效地建模为混合整数规划（MIP）并使用分支定界方法进行求解。先前的研究表明存在MIP后门，即一小组变量，如果优先在可能的情况下在它们上进行分支，则可以加快运行时间。然而，寻找能提高运行时间的高质量后门仍是一个未解决的问题。先前的工作通过排名学习估计随机采样的后门相对求解器速度，然后决定是否使用。本文中，我们利用蒙特卡洛树搜索方法收集用于训练的后门，而不是依赖随机采样，并且采用对比学习框架训练图注意力网络模型来预测后门。我们的方法在四个常见的MIP问题领域上进行评估，表现出对比Gurobi和先前模型的性能改进。

    Many real-world problems can be efficiently modeled as Mixed Integer Programs (MIPs) and solved with the Branch-and-Bound method. Prior work has shown the existence of MIP backdoors, small sets of variables such that prioritizing branching on them when possible leads to faster running times. However, finding high-quality backdoors that improve running times remains an open question. Previous work learns to estimate the relative solver speed of randomly sampled backdoors through ranking and then decide whether to use it. In this paper, we utilize the Monte-Carlo tree search method to collect backdoors for training, rather than relying on random sampling, and adapt a contrastive learning framework to train a Graph Attention Network model to predict backdoors. Our method, evaluated on four common MIP problem domains, demonstrates performance improvements over both Gurobi and previous models.
    
[^51]: 从理解的角度探讨语言模型的关键数据规模

    Critical Data Size of Language Models from a Grokking Perspective. (arXiv:2401.10463v1 [cs.CL])

    [http://arxiv.org/abs/2401.10463](http://arxiv.org/abs/2401.10463)

    本文从理解的角度探讨了语言模型中的关键数据规模，证明了只有当语言模型达到关键大小时才会发生泛化，同时揭示了更大的模型需要更多数据的趋势。

    

    我们探讨了语言模型中的关键数据规模，这是从快速记忆到缓慢泛化的一个基本转变阈值。我们将这个相变形式化为Grokking配置下的数据效率假说，并确定了语言模型训练动力学中的数据不足、充足和过剩阶段。我们通过重新调整初始化和权重衰减，开发出了一种Grokking配置，稳定地在简化的语言模型上重现了Grokking。我们表明只有当语言模型达到关键大小时才会发生泛化。我们分析了样本级和模型级的Grokking，验证了提出的数据效率假说。我们的实验揭示了语言数据集的关键数据集大小处发生的更平滑的相变。随着模型大小的增加，这个临界点也变得更大，这表明更大的模型需要更多的数据。我们的研究结果加深了对语言模型训练的理解，提供了一种新颖的视角。

    We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data. Our results deepen the understanding of language model training, offering a novel pers
    
[^52]: 语音识别中低秩适应的训练策略和模型鲁棒性研究

    Investigating Training Strategies and Model Robustness of Low-Rank Adaptation for Language Modeling in Speech Recognition. (arXiv:2401.10447v1 [cs.CL])

    [http://arxiv.org/abs/2401.10447](http://arxiv.org/abs/2401.10447)

    本研究研究了语音识别中低秩适应的训练策略和模型鲁棒性。通过引入不同的LoRA训练策略，实现了相对词错误率的降低，并研究了模型对输入扰动的稳定性。实验结果表明，高级LoRA变体导致了某些扰动的性能下降。

    

    随着资源有限的硬件设备的普及，使用低秩适应（LoRA）与冻结预训练语言模型（PLMs）已成为一种主流、资源高效的建模方法。本研究首先探讨了如何通过引入各种LoRA训练策略来提高模型性能，在公开的Librispeech数据集上实现了相对词错误率降低3.50％，在消息领域的内部数据集上实现了3.67％的降低。为了进一步评估基于LoRA的二次传递语音识别模型的稳定性，我们研究了对输入扰动的鲁棒性。这些扰动源于同音字替代和一种名为N-best Perturbation-based Rescoring Robustness（NPRR）的新度量标准，这两种方法都用于衡量重评分模型性能的相对降解程度。我们的实验结果表明，虽然LoRA的高级变体（例如动态秩分配的LoRA）导致了$1$-best扰动的性能下降。

    The use of low-rank adaptation (LoRA) with frozen pretrained language models (PLMs) has become increasing popular as a mainstream, resource-efficient modeling approach for memory-constrained hardware. In this study, we first explore how to enhance model performance by introducing various LoRA training strategies, achieving relative word error rate reductions of 3.50\% on the public Librispeech dataset and of 3.67\% on an internal dataset in the messaging domain. To further characterize the stability of LoRA-based second-pass speech recognition models, we examine robustness against input perturbations. These perturbations are rooted in homophone replacements and a novel metric called N-best Perturbation-based Rescoring Robustness (NPRR), both designed to measure the relative degradation in the performance of rescoring models. Our experimental results indicate that while advanced variants of LoRA, such as dynamic rank-allocated LoRA, lead to performance degradation in $1$-best perturbati
    
[^53]: 大型语言模型是噪声鲁棒语音识别的高效学习者

    Large Language Models are Efficient Learners of Noise-Robust Speech Recognition. (arXiv:2401.10446v1 [cs.CL])

    [http://arxiv.org/abs/2401.10446](http://arxiv.org/abs/2401.10446)

    本文通过引入噪声信息作为条件器，并从N-best列表中提取语言空间噪声嵌入，教会了大型语言模型（LLMs）进行噪声去除，从而实现了噪声鲁棒语音识别的生成式错误纠正（GER）。

    

    最近对大型语言模型（LLMs）的进展促进了自动语音识别（ASR）的生成式错误纠正（GER），利用LLMs的丰富语言知识和强大的推理能力来改善识别结果。最新的研究提出了一个GER基准测试，并使用HyPoradise数据集通过高效的LLM微调从ASR N-best假设到地面真实转录的映射，这显示出极大的效果，但在噪声鲁棒ASR方面缺乏具体性。在这项工作中，我们将基准测试扩展到噪声条件下，并研究是否可以教会LLMs像噪声鲁棒ASR一样执行去噪。其中一个解决方案是将噪声信息作为条件器引入LLM中。然而，直接从音频编码器中引入噪声嵌入可能会对LLM微调造成损害，因为存在跨模态差距。因此，我们提出了从N-best列表中提取语言空间噪声嵌入来表示源语音的噪声条件的方法。

    Recent advances in large language models (LLMs) have promoted generative error correction (GER) for automatic speech recognition (ASR), which leverages the rich linguistic knowledge and powerful reasoning ability of LLMs to improve recognition results. The latest work proposes a GER benchmark with HyPoradise dataset to learn the mapping from ASR N-best hypotheses to ground-truth transcription by efficient LLM finetuning, which shows great effectiveness but lacks specificity on noise-robust ASR. In this work, we extend the benchmark to noisy conditions and investigate if we can teach LLMs to perform denoising for GER just like what robust ASR do}, where one solution is introducing noise information as a conditioner into LLM. However, directly incorporating noise embeddings from audio encoder could harm the LLM tuning due to cross-modality gap. To this end, we propose to extract a language-space noise embedding from the N-best list to represent the noise conditions of source speech, whic
    
[^54]: 一个新的认知架构是否可以根本改进LLMs？反之亦然。

    Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?. (arXiv:2401.10444v1 [cs.AI])

    [http://arxiv.org/abs/2401.10444](http://arxiv.org/abs/2401.10444)

    本文讨论了如何通过引入计算认知架构来改进现有的LLMs，并强调了双过程架构和混合神经符号方法的重要性。同时，文章也指出需要改革计算认知架构以适应人工智能和计算技术的进步。总体而言，本文主张采用多学科、互惠互利的方法来开发更好的AI模型和理解人类心灵。

    

    本文讨论了解决当前以LLM为中心的人工智能系统的局限性所需的条件。文章认为，吸收人类认知和心理学方面的见解，并应用计算认知架构，可以开发出更加能力强、可靠性更高、更接近人类的系统。文中强调了双过程架构和混合神经符号方法在解决当前LLMs的局限性方面的重要性。同时，文章也强调了需要对计算认知架构进行改革，以更好地反映人工智能和计算技术的进步。总体而言，本文主张采用多学科、互惠互利的方法来开发更好的AI模型和理解人类心灵。

    The paper discusses what is needed to address the limitations of current LLM-centered AI systems. The paper argues that incorporating insights from human cognition and psychology, as embodied by a computational cognitive architecture, can help develop systems that are more capable, more reliable, and more human-like. It emphasizes the importance of the dual-process architecture and the hybrid neuro-symbolic approach in addressing the limitations of current LLMs. In the opposite direction, the paper also highlights the need for an overhaul of computational cognitive architectures to better reflect advances in AI and computing technology. Overall, the paper advocates for a multidisciplinary, mutually beneficial approach towards developing better models both for AI and for understanding the human mind.
    
[^55]: 通过重播组合问题的解来学习蒙特卡罗搜索的先验知识

    Learning a Prior for Monte Carlo Search by Replaying Solutions to Combinatorial Problems. (arXiv:2401.10431v1 [cs.AI])

    [http://arxiv.org/abs/2401.10431](http://arxiv.org/abs/2401.10431)

    本论文提出了一种通过统计已解决问题信息来自动计算先验知识的方法，该方法适用于多个困难的组合问题，并且可以显著提高搜索结果。

    

    蒙特卡罗搜索在多个困难的组合问题中取得了出色的结果。使用先验知识对搜索过程中的非均匀展开进行优化，相比于均匀展开，可以显著提高结果。通常使用专门针对组合问题的手工启发式算法作为先验知识。我们提出了一种自动计算先验知识的方法，它利用已解决问题的统计信息。这是一种简单而通用的方法，在展开过程中不会增加计算成本，并且可以带来显著的性能提升。该方法被应用于三个困难的组合问题：拉丁方阵补全、Kakuro和逆转RNA折叠。

    Monte Carlo Search gives excellent results in multiple difficult combinatorial problems. Using a prior to perform non uniform playouts during the search improves a lot the results compared to uniform playouts. Handmade heuristics tailored to the combinatorial problem are often used as priors. We propose a method to automatically compute a prior. It uses statistics on solved problems. It is a simple and general method that incurs no computational cost at playout time and that brings large performance gains. The method is applied to three difficult combinatorial problems: Latin Square Completion, Kakuro, and Inverse RNA Folding.
    
[^56]: 有限重复的广义嵌套展开策略自适应

    Generalized Nested Rollout Policy Adaptation with Limited Repetitions. (arXiv:2401.10420v1 [cs.AI])

    [http://arxiv.org/abs/2401.10420](http://arxiv.org/abs/2401.10420)

    本研究通过限制重复次数，避免了在广义嵌套展开策略自适应中出现重复的最佳序列，提高了算法在不同组合问题上的性能。

    

    广义嵌套展开策略自适应(GNRPA)是一种用于优化一系列选择的蒙特卡洛搜索算法。我们提出通过限制在给定层级找到的最佳序列的重复次数，避免使用过于确定性的策略，无法找到不同的选择序列。实验证明，这种方法在逆向RNA制作，具有时间窗口的旅行商问题和弱Schur问题这三个组合问题上改进了算法的性能。

    Generalized Nested Rollout Policy Adaptation (GNRPA) is a Monte Carlo search algorithm for optimizing a sequence of choices. We propose to improve on GNRPA by avoiding too deterministic policies that find again and again the same sequence of choices. We do so by limiting the number of repetitions of the best sequence found at a given level. Experiments show that it improves the algorithm for three different combinatorial problems: Inverse RNA Folding, the Traveling Salesman Problem with Time Windows and the Weak Schur problem.
    
[^57]: 大型语言模型摘要机能否适应不同科学传播目标？

    Can Large Language Model Summarizers Adapt to Diverse Scientific Communication Goals?. (arXiv:2401.10415v1 [cs.CL])

    [http://arxiv.org/abs/2401.10415](http://arxiv.org/abs/2401.10415)

    本研究探讨了大型语言模型在科学摘要任务中的可控性。通过控制风格特征，非微调的语言模型在评论生成任务中优于人类，同时基于关键词的引导可以改善模型的可控性。然而，模型在生成长摘要和高度抽象的简化摘要方面有限。总体而言，大型语言模型在摘要任务中表现出强大的通用能力，但在复杂控制方面有限。

    

    在这项工作中，我们研究了大型语言模型 (LLMs) 在科学摘要任务中的可控性。我们确定了表征论文评论、摘要和简化摘要等不同类型摘要的关键风格和内容覆盖因素。通过控制风格特征，我们发现非微调的LLMs在MuP评论生成任务中表现优于人类，无论是在与参考摘要的相似度还是在人类偏好方面。此外，我们还表明，我们可以通过基于关键词的无分类器引导 (CFG) 来改善LLMs的可控性，在arXiv和PubMed上实现与强微调基线相当的词汇重叠。然而，我们的结果还表明，LLMs无法一致地生成超过8个句子的长摘要。此外，这些模型在生成高度抽象的简化摘要方面能力有限。虽然LLMs表现出很强的通用摘要能力，但在不昂贵的微调措施下，对内容的复杂控制能力有限。

    In this work, we investigate the controllability of large language models (LLMs) on scientific summarization tasks. We identify key stylistic and content coverage factors that characterize different types of summaries such as paper reviews, abstracts, and lay summaries. By controlling stylistic features, we find that non-fine-tuned LLMs outperform humans in the MuP review generation task, both in terms of similarity to reference summaries and human preferences. Also, we show that we can improve the controllability of LLMs with keyword-based classifier-free guidance (CFG) while achieving lexical overlap comparable to strong fine-tuned baselines on arXiv and PubMed. However, our results also indicate that LLMs cannot consistently generate long summaries with more than 8 sentences. Furthermore, these models exhibit limited capacity to produce highly abstractive lay summaries. Although LLMs demonstrate strong generic summarization competency, sophisticated content control without costly fi
    
[^58]: 基于分布一致性的稀疏标签图神经网络自训练

    Distribution Consistency based Self-Training for Graph Neural Networks with Sparse Labels. (arXiv:2401.10394v1 [cs.LG])

    [http://arxiv.org/abs/2401.10394](http://arxiv.org/abs/2401.10394)

    本文提出了一种基于分布一致性的自训练方法，用于解决图神经网络中少样本节点分类的挑战，通过消除训练集和测试集之间的分布转移，提高自训练的有效性。

    

    少样本节点分类对于图神经网络(GNNs)来说是一个重大挑战，因为标记和未标记节点之间的监督不足和潜在的分布转移问题。自训练是一种广泛流行的框架，利用大量未标记数据扩展训练集，通过给选定的未标记节点分配伪标签。已经开展了许多基于置信度、信息增益等选择策略的努力。然而，这些方法中没有一个考虑到训练和测试节点集之间的分布转移。伪标签步骤可能增加这种转移甚至引入新的转移，从而阻碍自训练的有效性。因此，在这项研究中，我们探索了在自训练过程中明确地消除扩展训练集和测试集之间的分布转移的潜力。为此，我们提出了一种新颖的分布一致图自训练(DC-GST)框架，用于辨别

    Few-shot node classification poses a significant challenge for Graph Neural Networks (GNNs) due to insufficient supervision and potential distribution shifts between labeled and unlabeled nodes. Self-training has emerged as a widely popular framework to leverage the abundance of unlabeled data, which expands the training set by assigning pseudo-labels to selected unlabeled nodes. Efforts have been made to develop various selection strategies based on confidence, information gain, etc. However, none of these methods takes into account the distribution shift between the training and testing node sets. The pseudo-labeling step may amplify this shift and even introduce new ones, hindering the effectiveness of self-training. Therefore, in this work, we explore the potential of explicitly bridging the distribution shift between the expanded training set and test set during self-training. To this end, we propose a novel Distribution-Consistent Graph Self-Training (DC-GST) framework to identif
    
[^59]: 自然的功率法则学习环境中能够减轻灾难性干扰

    Catastrophic Interference is Mitigated in Naturalistic Power-Law Learning Environments. (arXiv:2401.10393v1 [cs.LG])

    [http://arxiv.org/abs/2401.10393](http://arxiv.org/abs/2401.10393)

    本研究在自然学习环境中通过回忆方法减轻了灾难性干扰，该方法受到功率法则的启发。

    

    神经网络通常遭受灾难性干扰（CI）：在学习新任务时，先前学习任务的表现显著下降。这与人类形成鲜明对比，人类可以连续学习新任务而不会明显忘记先前的任务。先前的工作已经探索了各种减轻CI的技术，例如正则化、回忆、生成性回放和浓缩方法。本研究采用了一种不同的方法，该方法受到认知科学研究的指导，该研究表明在自然环境中，遇到任务的概率与最后一次执行任务的时间成功率法则递减。我们认为，在模拟自然学习环境中进行减轻CI技术的真实评估是必要的。因此，我们评估了在类似人类面临的功率法则环境中训练简单的回忆方法时，CI的减轻程度。我们的工作探索了这种基于回忆的新方法。

    Neural networks often suffer from catastrophic interference (CI): performance on previously learned tasks drops off significantly when learning a new task. This contrasts strongly with humans, who can sequentially learn new tasks without appreciably forgetting previous tasks. Prior work has explored various techniques for mitigating CI such as regularization, rehearsal, generative replay, and distillation methods. The current work takes a different approach, one guided by cognitive science research showing that in naturalistic environments, the probability of encountering a task decreases as a power-law of the time since it was last performed. We argue that a realistic evaluation of techniques for the mitigation of CI should be performed in simulated naturalistic learning environments. Thus, we evaluate the extent of mitigation of CI when training simple rehearsal-based methods in power-law environments similar to the ones humans face. Our work explores this novel rehearsal-based appro
    
[^60]: 农业物体检测的You Look Only Once(YOLO)算法：一项文献计量学和系统性文献综述

    Agricultural Object Detection with You Look Only Once (YOLO) Algorithm: A Bibliometric and Systematic Literature Review. (arXiv:2401.10379v1 [cs.CV])

    [http://arxiv.org/abs/2401.10379](http://arxiv.org/abs/2401.10379)

    YOLO算法在农业物体检测中具有先进的性能特点，可实现实时检测、准确性好，并被广泛应用于各种农业任务。本研究通过文献综述对YOLO在农业领域的进展和应用进行了全面记录和评价。

    

    视觉是农业中几种数字技术和工具的重要组成部分。You Look Only Once (YOLO)物体检测器由于其具有最先进性能，在农业领域内相对短的时间内广受欢迎。YOLO提供实时检测，准确性良好，并在监测、监视、感知、自动化和机器人等各种农业任务中得到了实施。YOLO在农业中的研究和应用正在快速加速，但存在碎片化和多学科的问题。此外，物体检测器的性能特征（如准确性、速度、计算）影响了农业技术的实施和采用速度。因此，本研究旨在收集广泛的文献，以记录和批判性评价YOLO在农业物体识别中的进展和应用。首先，我们进行了对257篇文章的文献计量学综述，以了解农业领域中YOLO的学术景观。

    Vision is a major component in several digital technologies and tools used in agriculture. The object detector, You Look Only Once (YOLO), has gained popularity in agriculture in a relatively short span due to its state-of-the-art performance. YOLO offers real-time detection with good accuracy and is implemented in various agricultural tasks, including monitoring, surveillance, sensing, automation, and robotics. The research and application of YOLO in agriculture are accelerating rapidly but are fragmented and multidisciplinary. Moreover, the performance characteristics (i.e., accuracy, speed, computation) of the object detector influence the rate of technology implementation and adoption in agriculture. Thus, the study aims to collect extensive literature to document and critically evaluate the advances and application of YOLO for agricultural object recognition. First, we conducted a bibliometric review of 257 articles to understand the scholarly landscape of YOLO in agricultural dom
    
[^61]: MutaBot: 一种用于聊天机器人的变异测试方法

    MutaBot: A Mutation Testing Approach for Chatbots. (arXiv:2401.10372v1 [cs.SE])

    [http://arxiv.org/abs/2401.10372](http://arxiv.org/abs/2401.10372)

    MutaBot是一种用于聊天机器人的变异测试工具，可以在多个层级上对对话流程、意图和上下文进行变异。它为开发者提供了针对聊天机器人的特定故障类型的支持，是一个有效评估测试套件有效性的工具。

    

    变异测试是一种评估测试套件有效性的技术，它通过向程序中注入人工故障来实现。尽管适用于许多平台和编程语言，但目前尚无适用于对话式聊天机器人的变异测试工具，而这些机器人是通过自然语言界面与用户进行交互的日益流行的解决方案。本文介绍了一种称为MutaBot的变异测试工具，它可针对对话式聊天机器人的多个层级进行变异，包括对话流程、意图和上下文。我们设计了这个工具可以潜在地针对多个平台，同时我们实现了对Google Dialogflow聊天机器人的初步支持。我们使用三个Dialogflow聊天机器人和测试用例来评估该工具。

    Mutation testing is a technique aimed at assessing the effectiveness of test suites by seeding artificial faults into programs. Although available for many platforms and languages, no mutation testing tool is currently available for conversational chatbots, which represent an increasingly popular solution to design systems that can interact with users through a natural language interface. Note that since conversations must be explicitly engineered by the developers of conversational chatbots, these systems are exposed to specific types of faults not supported by existing mutation testing tools.  In this paper, we present MutaBot, a mutation testing tool for conversational chatbots. MutaBot addresses mutations at multiple levels, including conversational flows, intents, and contexts. We designed the tool to potentially target multiple platforms, while we implemented initial support for Google Dialogflow chatbots. We assessed the tool with three Dialogflow chatbots and test cases generat
    
[^62]: 在多跳基于集群的车联网中的分层联邦学习

    Hierarchical Federated Learning in Multi-hop Cluster-Based VANETs. (arXiv:2401.10361v1 [cs.LG])

    [http://arxiv.org/abs/2401.10361](http://arxiv.org/abs/2401.10361)

    本文介绍了一种在多跳基于集群的车联网中的分层联邦学习（HFL）框架，该框架利用平均相对速度和余弦相似度对FL模型参数进行加权组合以作为聚类度量，以解决数据多样性和高车辆移动性带来的挑战。

    

    在车联网中使用分层联邦学习（FL）引起了研究界的极大兴趣，因为通过通信本地数据集梯度而不是原始数据，可以减少传输开销并保护用户隐私。然而，在车联网中实施FL面临着有限的通信资源、高车辆移动性和数据分布的统计多样性等挑战。为了解决这些问题，本文引入了一种新的分层联邦学习（HFL）框架，用于基于多跳聚类的车联网。所提出的方法利用FL模型参数的平均相对速度和余弦相似度的加权组合作为聚类度量，以考虑数据多样性和高车辆移动性。这个度量可以在非独立同分布（non-IID）数据场景中保证最小变化簇头的收敛性，并解决与该场景相关的复杂性问题。

    The usage of federated learning (FL) in Vehicular Ad hoc Networks (VANET) has garnered significant interest in research due to the advantages of reducing transmission overhead and protecting user privacy by communicating local dataset gradients instead of raw data. However, implementing FL in VANETs faces challenges, including limited communication resources, high vehicle mobility, and the statistical diversity of data distributions. In order to tackle these issues, this paper introduces a novel framework for hierarchical federated learning (HFL) over multi-hop clustering-based VANET. The proposed method utilizes a weighted combination of the average relative speed and cosine similarity of FL model parameters as a clustering metric to consider both data diversity and high vehicle mobility. This metric ensures convergence with minimum changes in cluster heads while tackling the complexities associated with non-independent and identically distributed (non-IID) data scenarios. Additionall
    
[^63]: 保持深度学习模型可控性: 基于历史的方法来缓解过拟合

    Keeping Deep Learning Models in Check: A History-Based Approach to Mitigate Overfitting. (arXiv:2401.10359v1 [cs.SE])

    [http://arxiv.org/abs/2401.10359](http://arxiv.org/abs/2401.10359)

    这篇论文提出了一种简单而强大的基于历史的方法来检测和防止深度学习模型的过拟合问题，为软件工程中深度学习模型的使用提供了质量和可靠性的保证。

    

    在软件工程中，深度学习模型越来越多地被用于关键任务，如漏洞检测和代码审查。然而，过拟合仍然是一个挑战，影响着利用深度学习模型的软件系统的质量、可靠性和可信度。当前使用的过拟合检测和防止方法都有一些限制，如需要修改模型结构和高计算资源消耗。在本文中，我们提出了一种简单而强大的方法，可以基于训练历史（即验证损失）同时检测和防止过拟合。我们的方法首先在过拟合模型的训练历史上训练一个时间序列分类器。然后，这个分类器被用来检测一个训练好的模型是否过拟合。此外，我们训练好的分类器也可以用来防止过拟合。

    In software engineering, deep learning models are increasingly deployed for critical tasks such as bug detection and code review. However, overfitting remains a challenge that affects the quality, reliability, and trustworthiness of software systems that utilize deep learning models. Overfitting can be (1) prevented (e.g., using dropout or early stopping) or (2) detected in a trained model (e.g., using correlation-based approaches). Both overfitting detection and prevention approaches that are currently used have constraints (e.g., requiring modification of the model structure, and high computing resources). In this paper, we propose a simple, yet powerful approach that can both detect and prevent overfitting based on the training history (i.e., validation losses). Our approach first trains a time series classifier on training histories of overfit models. This classifier is then used to detect if a trained model is overfit. In addition, our trained classifier can be used to prevent ove
    
[^64]: 通过门控图变换器在功能连接性研究中探索智能

    Exploring General Intelligence via Gated Graph Transformer in Functional Connectivity Studies. (arXiv:2401.10348v1 [q-bio.NC])

    [http://arxiv.org/abs/2401.10348](http://arxiv.org/abs/2401.10348)

    本文引入了门控图变换器（GGT）框架，用来预测基于功能连接性（FC）的认知指标，并通过费城神经发育队列（PNC）实证验证了其优越的预测能力，进一步凸显了其在识别与人类认知过程相关联的关键神经连接中的潜力。

    

    功能连接性（FC）是从功能磁共振成像（fMRI）中提取的一项重要工具，用于阐明各种精神疾病的复杂性，并勾勒出支撑人脑认知和行为动力学的神经途径。虽然图神经网络（GNN）提供了一种结构化的方法来表示神经影像数据，但它们受到需预定义图结构来描述脑区之间关联的限制，而这一细节并非仅由FC提供。为了弥合这一鸿沟，我们介绍了门控图变换器（GGT）框架，旨在基于FC预测认知指标。对费城神经发育队列（PNC）的实证验证突出了我们模型的优越预测能力，进一步凸显了它在识别与人类认知过程相关联的关键神经连接中的潜力。

    Functional connectivity (FC) as derived from fMRI has emerged as a pivotal tool in elucidating the intricacies of various psychiatric disorders and delineating the neural pathways that underpin cognitive and behavioral dynamics inherent to the human brain. While Graph Neural Networks (GNNs) offer a structured approach to represent neuroimaging data, they are limited by their need for a predefined graph structure to depict associations between brain regions, a detail not solely provided by FCs. To bridge this gap, we introduce the Gated Graph Transformer (GGT) framework, designed to predict cognitive metrics based on FCs. Empirical validation on the Philadelphia Neurodevelopmental Cohort (PNC) underscores the superior predictive prowess of our model, further accentuating its potential in identifying pivotal neural connectivities that correlate with human cognitive processes.
    
[^65]: ELRT：高效低秩训练紧凑卷积神经网络

    ELRT: Efficient Low-Rank Training for Compact Convolutional Neural Networks. (arXiv:2401.10341v1 [cs.CV])

    [http://arxiv.org/abs/2401.10341](http://arxiv.org/abs/2401.10341)

    本文提出了ELRT，一种高效低秩训练解决方案，可用于高准确性、高紧凑性的低秩卷积神经网络模型。与现有解决方案相比，ELRT具有较少的准确性下降和不需要更新全尺寸模型的优势。

    

    低秩压缩是一种流行的模型压缩技术，在文献中已经得到了广泛研究。另一方面，低秩训练作为一种从头开始训练低秩卷积神经网络的替代方法，尚未得到充分利用。不同于低秩压缩，低秩训练不需要预训练的全秩模型，整个训练阶段始终在低秩结构上进行，为实际应用带来了吸引人的好处。然而，现有的低秩训练解决方案仍面临一些挑战，如显著的准确性下降和/或在训练过程中仍需更新全尺寸模型。在本文中，我们对低秩卷积神经网络训练进行了系统研究。通过确定适当的低秩格式和性能改进策略，我们提出了ELRT，一种用于高准确性、高紧凑性、低秩卷积神经网络模型的高效低秩训练解决方案。

    Low-rank compression, a popular model compression technique that produces compact convolutional neural networks (CNNs) with low rankness, has been well-studied in the literature. On the other hand, low-rank training, as an alternative way to train low-rank CNNs from scratch, has been exploited little yet. Unlike low-rank compression, low-rank training does not need pre-trained full-rank models, and the entire training phase is always performed on the low-rank structure, bringing attractive benefits for practical applications. However, the existing low-rank training solutions still face several challenges, such as a considerable accuracy drop and/or still needing to update full-size models during the training. In this paper, we perform a systematic investigation on low-rank CNN training. By identifying the proper low-rank format and performance-improving strategy, we propose ELRT, an efficient low-rank training solution for high-accuracy, high-compactness, low-rank CNN models. Our exten
    
[^66]: 基于噪声对比估计的低资源安全攻击模式识别匹配框架

    Noise Contrastive Estimation-based Matching Framework for Low-resource Security Attack Pattern Recognition. (arXiv:2401.10337v1 [cs.LG])

    [http://arxiv.org/abs/2401.10337](http://arxiv.org/abs/2401.10337)

    该论文提出了一种基于噪声对比估计的低资源安全攻击模式识别匹配框架，通过直接语义相似度决定文本与攻击模式之间的关联，以降低大量类别、标签分布不均和标签空间复杂性带来的学习难度。

    

    战术、技术和程序（TTPs）是网络安全领域中复杂的攻击模式，在文本知识库中有详细的描述。在网络安全写作中识别TTPs，通常称为TTP映射，是一个重要而具有挑战性的任务。传统的学习方法通常以经典的多类或多标签分类设置为目标。由于存在大量的类别（即TTPs），标签分布的不均衡和标签空间的复杂层次结构，这种设置限制了模型的学习能力。我们采用了一种不同的学习范式来解决这个问题，其中将文本与TTP标签之间的直接语义相似度决定为文本分配给TTP标签，从而减少了仅仅在大型标签空间上竞争的复杂性。为此，我们提出了一种具有有效的基于采样的学习比较机制的神经匹配架构，促进学习过程。

    Tactics, Techniques and Procedures (TTPs) represent sophisticated attack patterns in the cybersecurity domain, described encyclopedically in textual knowledge bases. Identifying TTPs in cybersecurity writing, often called TTP mapping, is an important and challenging task. Conventional learning approaches often target the problem in the classical multi-class or multilabel classification setting. This setting hinders the learning ability of the model due to a large number of classes (i.e., TTPs), the inevitable skewness of the label distribution and the complex hierarchical structure of the label space. We formulate the problem in a different learning paradigm, where the assignment of a text to a TTP label is decided by the direct semantic similarity between the two, thus reducing the complexity of competing solely over the large labeling space. To that end, we propose a neural matching architecture with an effective sampling-based learn-to-compare mechanism, facilitating the learning pr
    
[^67]: DrugAssist：一个用于分子优化的大型语言模型

    DrugAssist: A Large Language Model for Molecule Optimization. (arXiv:2401.10334v1 [q-bio.QM])

    [http://arxiv.org/abs/2401.10334](http://arxiv.org/abs/2401.10334)

    DrugAssist是一个交互式分子优化模型，通过人机对话实现优化，利用LLM的强交互性和泛化能力，在药物发现中取得了领先的结果。

    

    最近，大型语言模型（LLMs）在各种任务上展现出令人印象深刻的性能，吸引了越来越多的尝试将LLMs应用于药物发现领域。然而，在药物发现流程中，分子优化是一个关键任务，但目前LLMs在这个领域的参与很少。大多数现有方法仅关注捕捉数据中提供的化学结构的潜在模式，而没有利用专家反馈。这些非交互式方法忽视了药物发现过程实际上需要专家经验和迭代改进的事实。为了填补这个空白，我们提出了DrugAssist，一个通过人机对话利用LLM的强交互性和泛化能力进行分子优化的交互式模型。DrugAssist在单一和多个性质优化方面取得了领先的结果，同时展示了巨大的潜力。

    Recently, the impressive performance of large language models (LLMs) on a wide range of tasks has attracted an increasing number of attempts to apply LLMs in drug discovery. However, molecule optimization, a critical task in the drug discovery pipeline, is currently an area that has seen little involvement from LLMs. Most of existing approaches focus solely on capturing the underlying patterns in chemical structures provided by the data, without taking advantage of expert feedback. These non-interactive approaches overlook the fact that the drug discovery process is actually one that requires the integration of expert experience and iterative refinement. To address this gap, we propose DrugAssist, an interactive molecule optimization model which performs optimization through human-machine dialogue by leveraging LLM's strong interactivity and generalizability. DrugAssist has achieved leading results in both single and multiple property optimization, simultaneously showcasing immense pot
    
[^68]: 在不同偏好强度上通过多任务提升单类推荐系统

    Improving One-class Recommendation with Multi-tasking on Various Preference Intensities. (arXiv:2401.10316v1 [cs.IR])

    [http://arxiv.org/abs/2401.10316](http://arxiv.org/abs/2401.10316)

    本研究提出了一个多任务框架，考虑了隐式反馈中不同偏好强度的情况，并在其中引入了注意力图卷积层来探索高阶关系。这使得表示更具鲁棒性和泛化性。

    

    在单类推荐问题中，需要根据用户的隐式反馈来进行推荐，该反馈是通过用户的行为和不行为进行推断的。现有的方法通过编码来自训练数据中观察到的积极和消极交互来获取用户和物品的表示。然而，这些方法假设隐式反馈中的所有积极信号都反映了固定的偏好强度，这是不现实的。因此，用这些方法学习到的表示通常无法捕捉反映不同偏好强度的信息性实体特征。在本文中，我们提出了一个多任务框架，考虑了隐式反馈中每个信号的不同偏好强度。实体的表示需要同时满足每个子任务的目标，使其更加稳健和泛化。此外，我们还将注意力图卷积层引入到用户-物品的高阶关系中进行探索。

    In the one-class recommendation problem, it's required to make recommendations basing on users' implicit feedback, which is inferred from their action and inaction. Existing works obtain representations of users and items by encoding positive and negative interactions observed from training data. However, these efforts assume that all positive signals from implicit feedback reflect a fixed preference intensity, which is not realistic. Consequently, representations learned with these methods usually fail to capture informative entity features that reflect various preference intensities.  In this paper, we propose a multi-tasking framework taking various preference intensities of each signal from implicit feedback into consideration. Representations of entities are required to satisfy the objective of each subtask simultaneously, making them more robust and generalizable. Furthermore, we incorporate attentive graph convolutional layers to explore high-order relationships in the user-item
    
[^69]: LangProp: 一种应用于自动驾驶的使用语言模型的代码优化框架

    LangProp: A code optimization framework using Language Models applied to driving. (arXiv:2401.10314v1 [cs.SE])

    [http://arxiv.org/abs/2401.10314](http://arxiv.org/abs/2401.10314)

    LangProp是一种用于自动驾驶的代码优化框架，利用语言模型迭代优化生成的代码。它通过评估代码性能和捕捉异常来改进生成的代码，展示了在CARLA中实现自动驾驶的概念验证。

    

    LangProp是一个框架，用于在监督/强化学习环境中迭代优化大型语言模型(LLM)生成的代码。虽然LLM能够零-shot地生成合理的解决方案，但这些解决方案往往是次优的。特别是对于代码生成任务，初始代码可能在某些边缘情况下失败。LangProp自动评估数据集上的代码性能，并捕捉任何异常，并将结果反馈给LLM进行训练，以使LLM可以迭代改进其生成的代码。通过采用基于度量和数据驱动的训练范式来进行代码优化过程，可以轻松地借鉴传统机器学习技术，如模仿学习、DAgger和强化学习。我们展示了在CARLA中自动驾驶的代码优化的第一个概念验证，证明了LangProp可以生成可解释和透明的驾驶代码。

    LangProp is a framework for iteratively optimizing code generated by large language models (LLMs) in a supervised/reinforcement learning setting. While LLMs can generate sensible solutions zero-shot, the solutions are often sub-optimal. Especially for code generation tasks, it is likely that the initial code will fail on certain edge cases. LangProp automatically evaluates the code performance on a dataset of input-output pairs, as well as catches any exceptions, and feeds the results back to the LLM in the training loop, so that the LLM can iteratively improve the code it generates. By adopting a metricand data-driven training paradigm for this code optimization procedure, one could easily adapt findings from traditional machine learning techniques such as imitation learning, DAgger, and reinforcement learning. We demonstrate the first proof of concept of automated code optimization for autonomous driving in CARLA, showing that LangProp can generate interpretable and transparent dri
    
[^70]: 在社会和司法约束下对深度学习进行数学算法设计: 算法透明性要求

    Mathematical Algorithm Design for Deep Learning under Societal and Judicial Constraints: The Algorithmic Transparency Requirement. (arXiv:2401.10310v1 [cs.LG])

    [http://arxiv.org/abs/2401.10310](http://arxiv.org/abs/2401.10310)

    这篇论文探讨了在社会和司法约束下，对深度学习进行数学算法设计的挑战。研究者提出了算法透明性的要求，并使用数学框架来分析在计算模型中实现透明实施的可行性。

    

    深度学习在可信度方面仍然存在缺陷，这描述了一种可理解、公平、安全和可靠的方法。为了减轻人工智能的潜在风险，通过监管指南提出了与可信度相关的明确义务，例如,在欧洲AI法案中。因此，一个核心问题是可以实现多大程度上的可信度深度学习。建立构成可信度的描述性属性要求能够追溯影响算法计算的因素，即算法的实现是透明的。受当前深度学习模型演化需要改变计算技术的观察启发，我们得出一个数学框架，使我们能够分析在计算模型中是否有可能实现透明实施。我们应用我们的可信度框架来分析数字和模拟计算模型中逆问题的深度学习方法。

    Deep learning still has drawbacks in terms of trustworthiness, which describes a comprehensible, fair, safe, and reliable method. To mitigate the potential risk of AI, clear obligations associated to trustworthiness have been proposed via regulatory guidelines, e.g., in the European AI Act. Therefore, a central question is to what extent trustworthy deep learning can be realized. Establishing the described properties constituting trustworthiness requires that the factors influencing an algorithmic computation can be retraced, i.e., the algorithmic implementation is transparent. Motivated by the observation that the current evolution of deep learning models necessitates a change in computing technology, we derive a mathematical framework which enables us to analyze whether a transparent implementation in a computing model is feasible. We exemplarily apply our trustworthiness framework to analyze deep learning approaches for inverse problems in digital and analog computing models represe
    
[^71]: 论科学数据在机器学习中的公正和透明使用的准备情况

    On the Readiness of Scientific Data for a Fair and Transparent Use in Machine Learning. (arXiv:2401.10304v1 [cs.LG])

    [http://arxiv.org/abs/2401.10304](http://arxiv.org/abs/2401.10304)

    本研究分析了科学数据文档如何满足机器学习社区和监管机构对其在机器学习技术中使用的需求，并提出了一套建议指南。

    

    为了确保机器学习系统的公平性和可信性，最近的立法举措和机器学习社区的相关研究指出需要记录用于训练机器学习模型的数据。此外，为了实现可重复性，许多科学领域的数据共享实践近年来也有了发展。在这个意义上，学术机构采用了这些实践，鼓励研究人员将他们的数据和技术文件发布在同行评议的出版物上，如数据论文。本研究分析了科学数据文档如何满足机器学习社区和监管机构对其在机器学习技术中使用的需求。我们对4041篇不同领域的数据论文样本进行了评估，评估其完整性和覆盖范围，并研究了近年来的趋势，特别关注了最多和最少被记录的方面。作为结果，我们提出了一套数据创建者的建议指南。

    To ensure the fairness and trustworthiness of machine learning (ML) systems, recent legislative initiatives and relevant research in the ML community have pointed out the need to document the data used to train ML models. Besides, data-sharing practices in many scientific domains have evolved in recent years for reproducibility purposes. In this sense, the adoption of these practices by academic institutions has encouraged researchers to publish their data and technical documentation in peer-reviewed publications such as data papers. In this study, we analyze how this scientific data documentation meets the needs of the ML community and regulatory bodies for its use in ML technologies. We examine a sample of 4041 data papers of different domains, assessing their completeness and coverage of the requested dimensions, and trends in recent years, putting special emphasis on the most and least documented dimensions. As a result, we propose a set of recommendation guidelines for data creato
    
[^72]: 用于复杂适应性系统中的现象检测的具有时空一致性学习的分层框架

    A Hierarchical Framework with Spatio-Temporal Consistency Learning for Emergence Detection in Complex Adaptive Systems. (arXiv:2401.10300v1 [cs.MA])

    [http://arxiv.org/abs/2401.10300](http://arxiv.org/abs/2401.10300)

    本研究提出了一个分层框架，通过学习系统和代理的表示，使用时空一致性学习来捕捉复杂适应性系统中的现象，并解决了现有方法不能捕捉空间模式和建模非线性关系的问题。

    

    在由交互代理组成的复杂适应性系统（CAS）中，现象是一种全局属性，在现实世界的动态系统中很普遍，例如网络层次的交通拥堵。检测它的形成和消散有助于监测系统的状态，并发出有害现象的警报信号。由于CAS没有集中式控制器，基于每个代理的局部观察来检测现象是可取但具有挑战性的。现有的工作不能捕捉与现象相关的空间模式，并且无法建模代理之间的非线性关系。本文提出了一个分层框架，通过学习系统表示和代理表示来解决这两个问题，其中时空一致性学习器针对代理的非线性关系和系统的复杂演化进行了定制。通过保留最新100个代理的状态和历史状态来学习代理和系统的表示，

    Emergence, a global property of complex adaptive systems (CASs) constituted by interactive agents, is prevalent in real-world dynamic systems, e.g., network-level traffic congestions. Detecting its formation and evaporation helps to monitor the state of a system, allowing to issue a warning signal for harmful emergent phenomena. Since there is no centralized controller of CAS, detecting emergence based on each agent's local observation is desirable but challenging. Existing works are unable to capture emergence-related spatial patterns, and fail to model the nonlinear relationships among agents. This paper proposes a hierarchical framework with spatio-temporal consistency learning to solve these two problems by learning the system representation and agent representations, respectively. Especially, spatio-temporal encoders are tailored to capture agents' nonlinear relationships and the system's complex evolution. Representations of the agents and the system are learned by preserving the
    
[^73]: 从生成流的潜在空间中生成新的桥梁类型的尝试

    An attempt to generate new bridge types from latent space of generative flow. (arXiv:2401.10299v1 [cs.LG])

    [http://arxiv.org/abs/2401.10299](http://arxiv.org/abs/2401.10299)

    本研究通过归一化流生成潜在空间中的新桥梁类型，并解决了高维矩阵行列式计算和神经网络可逆变换的挑战。

    

    通过介绍不同分布之间的坐标和概率变换的示例，简明扼要地介绍了归一化流的基本原理。从随机变量函数的分布的角度解释了概率变换的本质，并引入了概率变换的缩放因子雅可比行列式。将数据集视为来自总体的样本，获取归一化流本质上是通过采样调查对总体的数值特征进行统计推断，然后使用最大似然估计方法建立损失函数。本文介绍了归一化流如何巧妙地解决了高维矩阵行列式计算和神经网络可逆变换两个主要应用挑战。利用三跨梁桥、拱桥、斜拉桥和悬索桥的对称结构图像数据集，构建了新的生成桥梁类型。

    Through examples of coordinate and probability transformation between different distributions, the basic principle of normalizing flow is introduced in a simple and concise manner. From the perspective of the distribution of random variable function, the essence of probability transformation is explained, and the scaling factor Jacobian determinant of probability transformation is introduced. Treating the dataset as a sample from the population, obtaining normalizing flow is essentially through sampling surveys to statistically infer the numerical features of the population, and then the loss function is established by using the maximum likelihood estimation method. This article introduces how normalizing flow cleverly solves the two major application challenges of high-dimensional matrix determinant calculation and neural network reversible transformation. Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge, constr
    
[^74]: 光电神经处理器的设计与开发，用于在混合机器人中实现经图像检测训练过的神经网络的模拟

    Design and development of opto-neural processors for simulation of neural networks trained in image detection for potential implementation in hybrid robotics. (arXiv:2401.10289v1 [cs.ET])

    [http://arxiv.org/abs/2401.10289](http://arxiv.org/abs/2401.10289)

    本论文设计了一种光电神经处理器，用于模拟经图像检测训练过的生物神经网络在混合机器人中的应用。通过光遗传学实现精确激活的反向传播STDP算法，实现了与传统神经网络训练算法相媲美的准确度。

    

    神经网络已广泛应用于图像处理、运动控制、物体检测等各种处理应用中。生物神经网络具有低功耗、更快的处理速度和生物现实性的优势。光遗传学为生物神经元提供高度的空间和时间控制，并在训练活生生的神经网络方面有潜力。本研究提出了一种通过光遗传学实现精确激活的反向传播STDP算法间接训练的模拟活生生的神经网络，其准确度可与传统神经网络训练算法相媲美。

    Neural networks have been employed for a wide range of processing applications like image processing, motor control, object detection and many others. Living neural networks offer advantages of lower power consumption, faster processing, and biological realism. Optogenetics offers high spatial and temporal control over biological neurons and presents potential in training live neural networks. This work proposes a simulated living neural network trained indirectly by backpropagating STDP based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithms.
    
[^75]: 中文数据处理中的佼佼者：英文代码模型

    Top in Chinese Data Processing: English Code Models. (arXiv:2401.10286v1 [cs.CL])

    [http://arxiv.org/abs/2401.10286](http://arxiv.org/abs/2401.10286)

    在中文数据处理中，基于代码的语言模型在非编程中文任务中表现出色，尤其是在对中文幻觉敏感的任务中。此研究为讨论“中文房间”思想实验提供了独特的视角。

    

    尽管在语言模型的应用中，任务与训练语料之间的对齐是一个基本的共识，但我们的一系列实验和我们设计的评估指标表明，基于代码的大型语言模型(LLMs)在非编程中文任务中的表现明显优于与任务紧密匹配的训练数据。此外，在对中文幻觉敏感程度较高的任务中，展示较少中文语言特征的模型表现更好。我们的实验结果可以通过简单地用代码模型替换基础模型，在中文数据处理任务中，如为检索增强生成(RAG)准备数据，很容易得到复制。此外，我们的研究为讨论“中文房间”思想实验提供了独特的视角。

    While the alignment between tasks and training corpora is a fundamental consensus in the application of language models, our series of experiments and the metrics we designed reveal that code-based Large Language Models (LLMs) significantly outperform models trained on data that is closely matched to the tasks in non-coding Chinese tasks. Moreover, in tasks high sensitivity to Chinese hallucinations, models exhibiting fewer linguistic features of the Chinese language achieve better performance. Our experimental results can be easily replicated in Chinese data processing tasks, such as preparing data for Retrieval-Augmented Generation (RAG), by simply replacing the base model with a code-based model. Additionally, our research offers a distinct perspective for discussion on the philosophical "Chinese Room" thought experiment.
    
[^76]: MorpheusNet：用于嵌入式在线系统的资源效率的睡眠阶段分类器

    MorpheusNet: Resource efficient sleep stage classifier for embedded on-line systems. (arXiv:2401.10284v1 [eess.SP])

    [http://arxiv.org/abs/2401.10284](http://arxiv.org/abs/2401.10284)

    MorpheusNet是一个资源效率的睡眠阶段分类器，可以在嵌入式系统上实时预测睡眠阶段，适用于边缘计算，消耗少量能源。

    

    睡眠阶段分类（SSC）是一项耗时的任务，需要专家检查数小时的电生理记录进行手动分类。这在利用睡眠阶段进行治疗目的时是一个限制因素。随着可穿戴设备的价格越来越可承受和扩张，自动化SSC可能使得规模扩大的睡眠基础治疗成为可能。深度学习作为一种潜在的自动化方法已经引起了越来越多的关注，先前的研究已经显示出与手动专家评分相当的准确性。然而，以前的方法需要大量的内存和计算资源，这限制了实时分类和在边缘部署模型的能力。为了填补这个空白，我们的目标是提供一个能够在实时预测睡眠阶段的模型，而不需要访问外部计算资源（例如移动手机、云）。该算法具有低功耗的特点，能够在嵌入式电池供电系统上使用。

    Sleep Stage Classification (SSC) is a labor-intensive task, requiring experts to examine hours of electrophysiological recordings for manual classification. This is a limiting factor when it comes to leveraging sleep stages for therapeutic purposes. With increasing affordability and expansion of wearable devices, automating SSC may enable deployment of sleep-based therapies at scale. Deep Learning has gained increasing attention as a potential method to automate this process. Previous research has shown accuracy comparable to manual expert scores. However, previous approaches require sizable amount of memory and computational resources. This constrains the ability to classify in real time and deploy models on the edge. To address this gap, we aim to provide a model capable of predicting sleep stages in real-time, without requiring access to external computational sources (e.g., mobile phone, cloud). The algorithm is power efficient to enable use on embedded battery powered systems. Our
    
[^77]: BioDiffusion：用于生物医学信号合成的多功能扩散模型

    BioDiffusion: A Versatile Diffusion Model for Biomedical Signal Synthesis. (arXiv:2401.10282v1 [eess.SP])

    [http://arxiv.org/abs/2401.10282](http://arxiv.org/abs/2401.10282)

    BioDiffusion是一种用于生物医学信号合成的多功能扩散模型，能够产生高保真度、非稳态的多变量信号。通过利用这些合成的信号，可以有效解决生物医学信号机器学习任务中的数据不足、数据不平衡和标签复杂性等问题，提高准确性。

    

    生物医学信号的机器学习任务通常面临有限的数据可用性、不平衡的数据集、标签复杂性和测量噪声的干扰等问题。这些挑战经常阻碍机器学习算法的最佳训练。为了解决这些问题，我们引入了BioDiffusion，这是一种针对合成多变量生物医学信号进行优化的基于扩散的概率模型。BioDiffusion在产生高保真度、非稳态的多变量信号方面表现出色，可用于无条件、标签条件和信号条件生成等多个任务。利用这些合成的信号为上述挑战提供了一个显著的解决方案。我们的研究包括对合成数据质量进行的定性和定量评估，强调其在与生物医学信号相关的机器学习任务中提高准确性的能力。此外，与当前主流的时间系信息生成模型相比，BioDiffusion在质量和效率方面表现出更好的性能。

    Machine learning tasks involving biomedical signals frequently grapple with issues such as limited data availability, imbalanced datasets, labeling complexities, and the interference of measurement noise. These challenges often hinder the optimal training of machine learning algorithms. Addressing these concerns, we introduce BioDiffusion, a diffusion-based probabilistic model optimized for the synthesis of multivariate biomedical signals. BioDiffusion demonstrates excellence in producing high-fidelity, non-stationary, multivariate signals for a range of tasks including unconditional, label-conditional, and signal-conditional generation. Leveraging these synthesized signals offers a notable solution to the aforementioned challenges. Our research encompasses both qualitative and quantitative assessments of the synthesized data quality, underscoring its capacity to bolster accuracy in machine learning tasks tied to biomedical signals. Furthermore, when juxtaposed with current leading tim
    
[^78]: GANs用于实时超可靠通信中基于EVT的模型参数估计

    GANs for EVT Based Model Parameter Estimation in Real-time Ultra-Reliable Communication. (arXiv:2401.10280v1 [eess.SP])

    [http://arxiv.org/abs/2401.10280](http://arxiv.org/abs/2401.10280)

    本文提出了一种将极值理论（EVT）和生成对抗网络（GANs）相结合的新方法，用于在实时环境中实现超可靠通信中精确的信道建模。通过使用GPD进行极端事件的分布建模，并使用GANs来估计GPD的参数，该方法与传统的GAN配置不同之处在于在GAN结构中增加了一个模块，直接估计GPD的参数。

    

    第六代（6G）系统中的超可靠低延迟通信（URLLC）范式在无线通信信道中处理稀有和极端事件时，严重依赖精确的信道建模。本文探索了一种将极值理论（EVT）和生成对抗网络（GANs）相结合的新方法，在实时环境中实现精确的信道建模。所提出的方法利用GPD（Generalized Pareto Distribution）采用EVT来对极端事件的分布进行建模。然后，使用GANs估计GPD的参数。与传统的GAN配置侧重于估计整体分布不同，本文提出的方法在GAN结构中增加一个额外的模块，目的是直接估计广义帕累托分布（GPD）的参数。

    The Ultra-Reliable Low-Latency Communications (URLLC) paradigm in sixth-generation (6G) systems heavily relies on precise channel modeling, especially when dealing with rare and extreme events within wireless communication channels. This paper explores a novel methodology integrating Extreme Value Theory (EVT) and Generative Adversarial Networks (GANs) to achieve the precise channel modeling in real-time. The proposed approach harnesses EVT by employing the Generalized Pareto Distribution (GPD) to model the distribution of extreme events. Subsequently, Generative Adversarial Networks (GANs) are employed to estimate the parameters of the GPD. In contrast to conventional GAN configurations that focus on estimating the overall distribution, the proposed approach involves the incorporation of an additional block within the GAN structure. This specific augmentation is designed with the explicit purpose of directly estimating the parameters of the Generalized Pareto Distribution (GPD). Throu
    
[^79]: 大型语言模型中地理位置嵌入方法的系统综述：迈向空间人工智能系统的路径

    A systematic review of geospatial location embedding approaches in large language models: A path to spatial AI systems. (arXiv:2401.10279v1 [cs.IR])

    [http://arxiv.org/abs/2401.10279](http://arxiv.org/abs/2401.10279)

    这篇论文系统综述了在大型语言模型中的地理位置嵌入方法，提出了四种主要的嵌入主题，并强调了在空间形态和生成模态方面进一步发展的需求。

    

    地理位置嵌入（GLE）帮助大型语言模型（LLM）吸收和分析空间数据。GLE在地理人工智能（GeoAI）中的出现是由于我们复杂当代空间中对更深入的地理认知的需求以及LLM在生成人工智能中提取深层含义的成功。我们在Google Scholar、Science Direct和arXiv上搜索了关于地理位置嵌入和LLM的论文，并审查了着重于通过LLM实现更深入空间“知识”的文章。我们筛选了304个标题、30个摘要和18篇全文论文，揭示了四个GLE主题 - 实体位置嵌入（ELE）、文档位置嵌入（DLE）、序列位置嵌入（SLE）和令牌位置嵌入（TLE）。综述以表格和叙述的形式呈现，包括“空间”和“LLM”之间的对话。尽管GLE通过叠加空间数据有助于理解空间，但强调了在空间形态和生成模态方面的进一步发展的需求。

    Geospatial Location Embedding (GLE) helps a Large Language Model (LLM) assimilate and analyze spatial data. GLE emergence in Geospatial Artificial Intelligence (GeoAI) is precipitated by the need for deeper geospatial awareness in our complex contemporary spaces and the success of LLMs in extracting deep meaning in Generative AI. We searched Google Scholar, Science Direct, and arXiv for papers on geospatial location embedding and LLM and reviewed articles focused on gaining deeper spatial "knowing" through LLMs. We screened 304 titles, 30 abstracts, and 18 full-text papers that reveal four GLE themes - Entity Location Embedding (ELE), Document Location Embedding (DLE), Sequence Location Embedding (SLE), and Token Location Embedding (TLE). Synthesis is tabular and narrative, including a dialogic conversation between "Space" and "LLM." Though GLEs aid spatial understanding by superimposing spatial data, they emphasize the need to advance in the intricacies of spatial modalities and gener
    
[^80]: EEGFormer: 实现可迁移和可解释的大规模脑电基础模型

    EEGFormer: Towards Transferable and Interpretable Large-Scale EEG Foundation Model. (arXiv:2401.10278v1 [eess.SP])

    [http://arxiv.org/abs/2401.10278](http://arxiv.org/abs/2401.10278)

    本论文提出了一种名为EEGFormer的脑电基础模型，通过在大规模的复合EEG数据上进行预训练，该模型能够学习到可迁移和可解释的通用表示，为脑电信号的分析提供了有力的工具。

    

    自我监督学习在自然语言处理和计算机视觉领域中被证明是一种高效的方法。在脑电图(EEG)数据等大量无标签数据存在的情况下，它也适用于脑信号，这些数据在从癫痫检测到波形分析等各种真实医学应用中存在。现有的利用自我监督学习进行EEG建模的工作主要集中在对应于单个下游任务的每个独立数据集的预训练上，这无法充分利用丰富的数据，而且可能会导致缺乏泛化性的次优解决方案。此外，这些方法依赖于难以理解的端到端模型学习。本文介绍了一种新颖的EEG基础模型，名为EEGFormer，它在大规模复合EEG数据上进行预训练。预训练模型不仅可以学习适应性能的EEG信号的通用表示，还可以提供可解释性。

    Self-supervised learning has emerged as a highly effective approach in the fields of natural language processing and computer vision. It is also applicable to brain signals such as electroencephalography (EEG) data, given the abundance of available unlabeled data that exist in a wide spectrum of real-world medical applications ranging from seizure detection to wave analysis. The existing works leveraging self-supervised learning on EEG modeling mainly focus on pretraining upon each individual dataset corresponding to a single downstream task, which cannot leverage the power of abundant data, and they may derive sub-optimal solutions with a lack of generalization. Moreover, these methods rely on end-to-end model learning which is not easy for humans to understand. In this paper, we present a novel EEG foundation model, namely EEGFormer, pretrained on large-scale compound EEG data. The pretrained model cannot only learn universal representations on EEG signals with adaptable performance 
    
[^81]: 基于知识辅助的大规模原油调度的双阶段进化优化

    Knowledge-Assisted Dual-Stage Evolutionary Optimization of Large-Scale Crude Oil Scheduling. (arXiv:2401.10274v1 [cs.NE])

    [http://arxiv.org/abs/2401.10274](http://arxiv.org/abs/2401.10274)

    基于知识辅助的双阶段进化优化方法用于解决大规模原油调度问题，通过全局搜索和局部优化改进传统优化方法的效率和性能。

    

    随着现代炼油厂原油调度的规模扩大，出现了具有数千个二进制变量和非线性约束的大规模原油调度问题（LSCOSPs），这对于传统优化方法来说是具有挑战性的。为了解决LSCOSPs，我们以海上进口炼油厂的实际原油调度为例，从原油卸货、运输、原油蒸馏装置加工和中间产品库存管理等方面对LSCOSPs进行建模。在提出的模型的基础上，我们开发了一个由启发式规则驱动的双阶段进化算法（简称DSEA/HR），其中双阶段搜索机制包括全局搜索和局部优化。在全局搜索阶段，我们根据经验操作知识设计了几个启发式规则，以在混合变量空间中生成具有良好性能的初始种群，并加速收敛。在局部优化阶段，我们采用修复策略。

    With the scaling up of crude oil scheduling in modern refineries, large-scale crude oil scheduling problems (LSCOSPs) emerge with thousands of binary variables and non-linear constraints, which are challenging to be optimized by traditional optimization methods. To solve LSCOSPs, we take the practical crude oil scheduling from a marine-access refinery as an example and start with modeling LSCOSPs from crude unloading, transportation, crude distillation unit processing, and inventory management of intermediate products. On the basis of the proposed model, a dual-stage evolutionary algorithm driven by heuristic rules (denoted by DSEA/HR) is developed, where the dual-stage search mechanism consists of global search and local refinement. In the global search stage, we devise several heuristic rules based on the empirical operating knowledge to generate a well-performing initial population and accelerate convergence in the mixed variables space. In the local refinement stage, a repair strat
    
[^82]: 革新制药业：揭开药物行业中人工智能和法律语言模型的趋势

    Revolutionizing Pharma: Unveiling the AI and LLM Trends in the Pharmaceutical Industry. (arXiv:2401.10273v1 [cs.CY])

    [http://arxiv.org/abs/2401.10273](http://arxiv.org/abs/2401.10273)

    这篇论文批判性概述了制药行业中人工智能的新兴趋势和重大进展，特别强调了机器学习算法等先进人工智能技术在制药运营的贡献。

    

    本文对制药行业中人工智能的新兴趋势和重大进展进行了批判性概述。详细介绍了其在研发、动物测试、临床试验、医院临床阶段、生产、监管事务、质量控制和其他支持领域等关键运营领域的应用，文章系统地研究了人工智能在每个领域的作用。特别强调了机器学习算法等先进人工智能技术在制药运营各个方面的贡献。通过这一全面分析，文章突出了人工智能在重塑制药业未来的变革潜力。

    This document offers a critical overview of the emerging trends and significant advancements in artificial intelligence (AI) within the pharmaceutical industry. Detailing its application across key operational areas, including research and development, animal testing, clinical trials, hospital clinical stages, production, regulatory affairs, quality control and other supporting areas, the paper categorically examines AI's role in each sector. Special emphasis is placed on cutting-edge AI technologies like machine learning algorithms and their contributions to various aspects of pharmaceutical operations. Through this comprehensive analysis, the paper highlights the transformative potential of AI in reshaping the pharmaceutical industry's future.
    
[^83]: 联合多源梯度差异最小化用于联邦域泛化

    Multi-Source Collaborative Gradient Discrepancy Minimization for Federated Domain Generalization. (arXiv:2401.10272v1 [cs.CV])

    [http://arxiv.org/abs/2401.10272](http://arxiv.org/abs/2401.10272)

    本文提出了一种名为联合多源梯度差异最小化的方法，用于联邦域泛化。该方法通过域内和域间梯度匹配的方式，减少了来自不同源域的数据隔离所带来的域差距，从而使得学习的模型能够在未知域上具有良好的泛化能力。

    

    联邦域泛化旨在从多个分散的源域学习一个域不变的模型，以在未知的目标域上进行部署。由于隐私问题，来自不同源域的数据被保持隔离，这在弥合域差距方面带来了挑战。为解决这个问题，我们提出了一种联合多源梯度差异最小化（MCGDM）方法用于联邦域泛化。具体而言，我们提出了原始图像和增强图像之间的域内梯度匹配，以避免对隔离域内的域特定信息进行过拟合。此外，我们提出了在其他域的协作下进行域间梯度匹配，可以进一步减少分散域之间的域偏移。通过结合域内和域间梯度匹配，我们的方法使得学习的模型能够在未知域上具有良好的泛化能力。此外，我们的方法可以扩展到联邦域适应任务。

    Federated Domain Generalization aims to learn a domain-invariant model from multiple decentralized source domains for deployment on unseen target domain. Due to privacy concerns, the data from different source domains are kept isolated, which poses challenges in bridging the domain gap. To address this issue, we propose a Multi-source Collaborative Gradient Discrepancy Minimization (MCGDM) method for federated domain generalization. Specifically, we propose intra-domain gradient matching between the original images and augmented images to avoid overfitting the domain-specific information within isolated domains. Additionally, we propose inter-domain gradient matching with the collaboration of other domains, which can further reduce the domain shift across decentralized domains. Combining intra-domain and inter-domain gradient matching, our method enables the learned model to generalize well on unseen domains. Furthermore, our method can be extended to the federated domain adaptation ta
    
[^84]: 学术界和工业界对人工智能研究的互补贡献

    The complementary contributions of academia and industry to AI research. (arXiv:2401.10268v1 [cs.CY])

    [http://arxiv.org/abs/2401.10268](http://arxiv.org/abs/2401.10268)

    工业界的研究团队在人工智能研究中有更高的关注度和引用率，且更有可能产生最先进的模型。而学术界的团队则更倾向于产生具有更高程度创新的工作，出现非常不寻常和典型的论文。这种影响力-创新度优势在不同领域、团队规模、资历和声望下均存在。

    

    人工智能在工业界和学术界中都取得了巨大的发展。然而，工业界近期的突破性进展引起了人们对学术研究在该领域中的作用的新视角。在这里，我们对过去25年中两个环境中产生的人工智能的影响和类型进行了描述，并建立了几种模式。我们发现，由工业界研究人员组成的团队发表的文章往往更受关注，具有更高的被引用和引发引用颠覆的可能性，且更有可能产生最先进的模型。相反，我们发现纯学术团队发表了大部分的人工智能研究，并倾向于产生更高程度的创新工作，单篇论文有数倍的可能性是非常不寻常和典型的。工业界和学术界在影响力-创新度方面的优势不受子领域、团队规模、资历和声望的影响。我们发现学术界产生了更多综述和分析型论文，而工业界则更多地注重应用和技术发展。

    Artificial intelligence (AI) has seen tremendous development in industry and academia. However, striking recent advances by industry have stunned the world, inviting a fresh perspective on the role of academic research in this field. Here, we characterize the impact and type of AI produced by both environments over the last 25 years and establish several patterns. We find that articles published by teams consisting exclusively of industry researchers tend to get greater attention, with a higher chance of being highly cited and citation-disruptive, and several times more likely to produce state-of-the-art models. In contrast, we find that exclusively academic teams publish the bulk of AI research and tend to produce higher novelty work, with single papers having several times higher likelihood of being unconventional and atypical. The respective impact-novelty advantages of industry and academia are robust to controls for subfield, team size, seniority, and prestige. We find that academ
    
[^85]: HyperSense: 加速超维度计算以用于智能传感器数据处理

    HyperSense: Accelerating Hyper-Dimensional Computing for Intelligent Sensor Data Processing. (arXiv:2401.10267v1 [cs.AR])

    [http://arxiv.org/abs/2401.10267](http://arxiv.org/abs/2401.10267)

    HyperSense是一个协同设计的硬件和软件系统，能够根据传感器数据中的物体存在预测有效地控制数据生成速率，通过使用低精度ADC减少冗余数据降低机器学习系统的成本，利用超维度计算的特点分析实时的低精度传感器数据，在处理噪声、以内存为中心和实时学习方面具有优势。该系统结合了高性能的物体检测软件和实时的硬件预测，引入了智能传感器控制的新概念。在软件和硬件评估中表现出卓越的性能。

    

    引入HyperSense，我们协同设计的硬件和软件系统根据传感器数据中的物体存在预测有效地控制模拟到数字转换器（ADC）模块的数据生成速率。针对不断增加的传感器数量和数据速率所带来的挑战，HyperSense使用高效的低精度ADC减少冗余的数字数据，降低了机器学习系统的成本。利用神经启发的超维度计算（HDC），HyperSense分析实时的原始低精度传感器数据，在处理噪声、以内存为中心和实时学习方面具有优势。我们提出的HyperSense模型将高性能的物体检测软件与实时的硬件预测结合起来，引入了智能传感器控制的新概念。全面的软件和硬件评估展示了我们解决方案的卓越性能，通过最高的曲线下面积（AUC）和最陡的接收器操作特性（ROC）曲线证明了这一点。

    Introducing HyperSense, our co-designed hardware and software system efficiently controls Analog-to-Digital Converter (ADC) modules' data generation rate based on object presence predictions in sensor data. Addressing challenges posed by escalating sensor quantities and data rates, HyperSense reduces redundant digital data using energy-efficient low-precision ADC, diminishing machine learning system costs. Leveraging neurally-inspired HyperDimensional Computing (HDC), HyperSense analyzes real-time raw low-precision sensor data, offering advantages in handling noise, memory-centricity, and real-time learning.  Our proposed HyperSense model combines high-performance software for object detection with real-time hardware prediction, introducing the novel concept of Intelligent Sensor Control. Comprehensive software and hardware evaluations demonstrate our solution's superior performance, evidenced by the highest Area Under the Curve (AUC) and sharpest Receiver Operating Characteristic (ROC
    
[^86]: 工业厂房智能状态监测: 方法论和不确定性管理策略综述

    Intelligent Condition Monitoring of Industrial Plants: An Overview of Methodologies and Uncertainty Management Strategies. (arXiv:2401.10266v1 [cs.LG])

    [http://arxiv.org/abs/2401.10266](http://arxiv.org/abs/2401.10266)

    本论文综述了工业厂房智能状态监测和故障检测和诊断方法，重点关注了Tennessee Eastman Process。调研总结了最流行和最先进的深度学习和机器学习算法，并探讨了算法的优劣势。还讨论了不平衡数据和无标记样本等挑战，以及深度学习模型如何应对。比较了不同算法在Tennessee Eastman Process上的准确性和规格。

    

    状态监测在现代工业系统的安全性和可靠性中起着重要作用。人工智能（AI）方法作为一种在工业应用中日益受到学术界和行业关注的增长主题和一种强大的故障识别方式。本文概述了工业厂房智能状态监测和故障检测和诊断方法，重点关注开源基准Tennessee Eastman Process（TEP）。在这项调查中，总结了用于工业厂房状态监测、故障检测和诊断的最流行和最先进的深度学习（DL）和机器学习（ML）算法，并研究了每种算法的优点和缺点。还涵盖了不平衡数据、无标记样本以及深度学习模型如何处理这些挑战。最后，比较了利用Tennessee Eastman Process的不同算法的准确性和规格。

    Condition monitoring plays a significant role in the safety and reliability of modern industrial systems. Artificial intelligence (AI) approaches are gaining attention from academia and industry as a growing subject in industrial applications and as a powerful way of identifying faults. This paper provides an overview of intelligent condition monitoring and fault detection and diagnosis methods for industrial plants with a focus on the open-source benchmark Tennessee Eastman Process (TEP). In this survey, the most popular and state-of-the-art deep learning (DL) and machine learning (ML) algorithms for industrial plant condition monitoring, fault detection, and diagnosis are summarized and the advantages and disadvantages of each algorithm are studied. Challenges like imbalanced data, unlabelled samples and how deep learning models can handle them are also covered. Finally, a comparison of the accuracies and specifications of different algorithms utilizing the Tennessee Eastman Process 
    
[^87]: 透明学习分析在面对面协作学习中的个性化支持中的应用

    Harnessing Transparent Learning Analytics for Individualized Support through Auto-detection of Engagement in Face-to-Face Collaborative Learning. (arXiv:2401.10264v1 [cs.CY])

    [http://arxiv.org/abs/2401.10264](http://arxiv.org/abs/2401.10264)

    这篇论文介绍了一种透明的学习分析方法，通过自动检测个体学生在面对面协作学习中的参与度，为个性化支持提供了可能性。

    

    利用学习分析研究和支持协作学习已有多年。最近，通过各种人工智能方法的自动化方法在建模和预测协作学习任务中的学生参与和表现方面取得了有希望的结果。然而，由于学习分析设计和实施中使用了“黑盒”方法的缺乏透明度和可解释性，教学和学习实践的指导可能会成为一个挑战。一方面，机器学习算法和模型创建的黑盒阻止用户获得教育上有意义的学习和教学建议。另一方面，仅关注群体和队列级别的分析可能会难以为协作小组中的个别学生提供具体的支持。本文提出了一种透明的方法来自动检测学生在过程中的个体参与度。

    Using learning analytics to investigate and support collaborative learning has been explored for many years. Recently, automated approaches with various artificial intelligence approaches have provided promising results for modelling and predicting student engagement and performance in collaborative learning tasks. However, due to the lack of transparency and interpretability caused by the use of "black box" approaches in learning analytics design and implementation, guidance for teaching and learning practice may become a challenge. On the one hand, the black box created by machine learning algorithms and models prevents users from obtaining educationally meaningful learning and teaching suggestions. On the other hand, focusing on group and cohort level analysis only can make it difficult to provide specific support for individual students working in collaborative groups. This paper proposes a transparent approach to automatically detect student's individual engagement in the process 
    
[^88]: 神经网络的零空间特性及其在图像隐写术中的应用

    Null Space Properties of Neural Networks with Applications to Image Steganography. (arXiv:2401.10262v1 [cs.CV])

    [http://arxiv.org/abs/2401.10262](http://arxiv.org/abs/2401.10262)

    本文研究了神经网络的零空间特性，发现了神经网络的固有弱点，并针对此弱点提出了一种图像隐写术方法。

    

    本文研究了神经网络的零空间特性。我们将零空间定义从线性映射扩展到非线性映射，并讨论了神经网络中零空间的存在。给定神经网络的零空间可以告诉我们哪些输入数据对最终预测没有任何贡献，从而可以利用它来欺骗神经网络。这揭示了神经网络中的固有弱点，可以被利用。本文描述了一个应用，即图像隐写术的方法。通过在诸如MNIST之类的图像数据集上进行实验证明，我们可以使用零空间成分来强制神经网络选择所选的隐藏图像类，即使整体图像看起来完全不同。最后，我们通过显示人类观察者能够看到的内容与神经网络用于进行预测的图像部分之间的比较，展示了神经网络的“观测”情况。

    This paper explores the null space properties of neural networks. We extend the null space definition from linear to nonlinear maps and discuss the presence of a null space in neural networks. The null space of a given neural network can tell us the part of the input data that makes no contribution to the final prediction so that we can use it to trick the neural network. This reveals an inherent weakness in neural networks that can be exploited. One application described here leads to a method of image steganography. Through experiments on image datasets such as MNIST, we show that we can use null space components to force the neural network to choose a selected hidden image class, even though the overall image can be made to look like a completely different image. We conclude by showing comparisons between what a human viewer would see, and the part of the image that the neural network is actually using to make predictions and, hence, show that what the neural network ``sees'' is com
    
[^89]: 零泡沫管道并行性

    Zero Bubble Pipeline Parallelism. (arXiv:2401.10241v1 [cs.DC])

    [http://arxiv.org/abs/2401.10241](http://arxiv.org/abs/2401.10241)

    本论文介绍了一种新的调度策略，在同步训练语义下成功实现了零管道泡沫，通过将反向计算分为两部分，设计了优于基线方法的新颖管道调度。此外，还提出了一种自动找到最优调度的算法，并引入新颖技术绕过同步操作实现零泡沫。实验证明，在相似条件下，本方法在吞吐量方面优于1F1B调度23%。

    

    管道并行性是大规模分布式训练的关键组成部分之一，然而其效率受到被视为不可避免的管道泡沫的影响。在这项工作中，我们引入了一种调度策略，据我们所知，这是第一次在同步训练语义下成功实现零管道泡沫。这个改进背后的关键思想是将反向计算分为两部分，一部分计算输入的梯度，另一部分计算参数的梯度。基于这个思想，我们手工设计了新颖的管道调度方法，显著优于基线方法。我们还开发了一个根据特定的模型配置和内存限制自动找到最优调度的算法。此外，为了真正实现零泡沫，我们引入了一种新颖的技术，在优化器步骤中绕过同步操作。实验评估表明，我们的方法在吞吐量方面比1F1B调度高出多达23%。

    Pipeline parallelism is one of the key components for large-scale distributed training, yet its efficiency suffers from pipeline bubbles which were deemed inevitable. In this work, we introduce a scheduling strategy that, to our knowledge, is the first to successfully achieve zero pipeline bubbles under synchronous training semantics. The key idea behind this improvement is to split the backward computation into two parts, one that computes gradient for the input and another that computes for the parameters. Based on this idea, we handcraft novel pipeline schedules that significantly outperform the baseline methods. We further develop an algorithm that automatically finds an optimal schedule based on specific model configuration and memory limit. Additionally, to truly achieve zero bubble, we introduce a novel technique to bypass synchronizations during the optimizer step. Experimental evaluations show that our method outperforms the 1F1B schedule up to 23% in throughput under a simila
    
[^90]: 将图像特征输入到神经网络的每一层的模仿学习

    Imitation Learning Inputting Image Feature to Each Layer of Neural Network. (arXiv:2401.09691v1 [cs.RO])

    [http://arxiv.org/abs/2401.09691](http://arxiv.org/abs/2401.09691)

    本文提出了一种在模仿学习中解决多模态数据处理挑战的方法，通过将数据输入到每个神经网络层中，放大与期望输出的相关性较低的数据的影响，并通过实验证明了成功率的显著提高。

    

    模仿学习使得机器人能够通过训练数据学习并复制人类行为。最近机器学习的进展使得能够直接处理高维观测数据（如图像）的端到端学习方法成为可能。然而，在处理多个模态的数据时，这些方法面临着一个关键挑战，即在使用短采样周期时无意中忽略与期望输出的相关性较低的数据。本文提出了一种有效的方法来解决这个挑战，通过将数据输入到每个神经网络层中，放大与输出相关性较低的数据的影响。所提出的方法有效地将多样的数据源纳入到学习过程中。通过使用原始图像和关节信息作为输入进行简单的拾取放置操作的实验，即使处理来自短采样周期的数据，也证明了成功率显著提高。

    Imitation learning enables robots to learn and replicate human behavior from training data. Recent advances in machine learning enable end-to-end learning approaches that directly process high-dimensional observation data, such as images. However, these approaches face a critical challenge when processing data from multiple modalities, inadvertently ignoring data with a lower correlation to the desired output, especially when using short sampling periods. This paper presents a useful method to address this challenge, which amplifies the influence of data with a relatively low correlation to the output by inputting the data into each neural network layer. The proposed approach effectively incorporates diverse data sources into the learning process. Through experiments using a simple pick-and-place operation with raw images and joint information as input, significant improvements in success rates are demonstrated even when dealing with data from short sampling periods.
    
[^91]: 使用反事实对抗优化实现大型语言模型的对齐

    Aligning Large Language Models with Counterfactual DPO. (arXiv:2401.09566v1 [cs.CL])

    [http://arxiv.org/abs/2401.09566](http://arxiv.org/abs/2401.09566)

    本文研究了在大型语言模型中使用反事实对抗优化框架，以实现风格对齐，避免人类干预，并成功培养出可取行为和减轻不可取行为。

    

    大型语言模型(LLMs)的进步在各种应用中展示了卓越的能力。这些模型在生成上下文连贯且涵盖广泛主题的文本补全方面表现出色。然而，它们训练所需的大量数据使得在预训练和指令调整阶段对齐响应风格变得具有挑战性。因此，通常会采用额外的对齐阶段，进一步使用人类偏好数据对模型进行训练，以更好地将其输出与人类期望对齐。虽然这个过程本身并没有引入新的能力，但它突出了模型固有的生成风格。本文研究了在直接偏好优化(DPO)框架内利用反事实提示来对齐模型的风格，而不依赖人类干预。我们证明了这种方法有效地培养了可取的行为，减轻了不可取的行为。

    Advancements in large language models (LLMs) have demonstrated remarkable capabilities across a diverse range of applications. These models excel in generating text completions that are contextually coherent and cover an extensive array of subjects. However, the vast datasets required for their training make aligning response styles during the pretraining and instruction tuning phases challenging. Consequently, an additional alignment phase is typically employed, wherein the model is further trained with human preference data to better align its outputs with human expectations. While this process doesn't introduce new capabilities per se, it does accentuate generation styles innate to the model. This paper explores the utilization of counterfactual prompting within the framework of Direct Preference Optimization (DPO) to align the model's style without relying on human intervention. We demonstrate that this method effectively instils desirable behaviour, mitigates undesirable ones, and
    
[^92]: CFASL：用于变分自编码器中的解缠学习的复合因子对齐对称学习

    CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational AutoEncoder. (arXiv:2401.08897v1 [cs.LG])

    [http://arxiv.org/abs/2401.08897](http://arxiv.org/abs/2401.08897)

    CFASL是一种用于解缠学习的新方法，它将对称性学习与VAE集成，无需任何数据集因子信息的先验知识，具有三个新特征：对齐潜在向量维度到可学习对称代码簿中的对称性，学习复合对称性来表达未知因素的变化，以及引入群等变编码器和解码器来训练VAE。

    

    输入和潜在向量的对称性为VAE中的解缠学习提供了宝贵的见解。然而，只有少数几篇论文提出了一种无监督方法，甚至这些方法在训练数据中也需要已知的因子信息。我们提出了一种新的方法，Composite Factor-Aligned Symmetry Learning (CFASL)，将其集成到VAE中，用于学习基于对称性的解缠，无监督学习中不需要任何数据集因子信息的知识。CFASL包括三个用于学习基于对称性的解缠的新特征：1)注入归纳偏置，将潜在向量维度对齐到明确可学习的对称代码簿中的因子对齐对称性；2)学习一个复合对称性，通过学习代码簿中的因子对齐对称性，来表达两个随机样本之间的未知因素的变化；3)在训练VAE时，引入具有群等变编码器和解码器的两个条件。此外，我们提出了一种扩展的评估指标。

    Symmetries of input and latent vectors have provided valuable insights for disentanglement learning in VAEs.However, only a few works were proposed as an unsupervised method, and even these works require known factor information in training data. We propose a novel method, Composite Factor-Aligned Symmetry Learning (CFASL), which is integrated into VAEs for learning symmetry-based disentanglement in unsupervised learning without any knowledge of the dataset factor information.CFASL incorporates three novel features for learning symmetry-based disentanglement: 1) Injecting inductive bias to align latent vector dimensions to factor-aligned symmetries within an explicit learnable symmetry codebook 2) Learning a composite symmetry to express unknown factors change between two random samples by learning factor-aligned symmetries within the codebook 3) Inducing group equivariant encoder and decoder in training VAEs with the two conditions. In addition, we propose an extended evaluation metri
    
[^93]: RoTBench: 评估大型语言模型在工具学习中的鲁棒性的多级基准

    RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning. (arXiv:2401.08326v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.08326](http://arxiv.org/abs/2401.08326)

    RoTBench是一个多级基准，用于评估大型语言模型在工具学习中的鲁棒性。研究发现，LLMs在真实世界的噪声下表现出的稳定性需得到提高。

    

    工具学习作为大型语言模型（LLMs）与物理世界之间互动的重要手段，引起了广泛的兴趣。当前的研究主要强调LLMs在结构良好的环境中利用工具的能力，但忽视了它们在面对真实世界中不可避免的噪声时的稳定性。为了弥合这一差距，我们引入了RoTBench，这是一个用于评估LLMs在工具学习中鲁棒性的多级基准。具体而言，我们建立了五个外部环境，每个环境都具有不同级别的噪声（即清洁、轻微、中等、重度和联合），对模型在工具选择、参数识别和内容填充三个关键阶段的抗干扰能力进行了深入分析。六个广泛使用的模型的实验表明，提高LLMs在工具学习中的鲁棒性迫在眉睫。例如，当没有实质性的噪声存在时，GPT-4的性能甚至从80.00下降到58.10。

    Tool learning has generated widespread interest as a vital means of interaction between Large Language Models (LLMs) and the physical world. Current research predominantly emphasizes LLMs' capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world. To bridge this gap, we introduce RoTBench, a multi-level benchmark for evaluating the robustness of LLMs in tool learning. Specifically, we establish five external environments, each featuring varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union), providing an in-depth analysis of the model's resilience across three critical phases: tool selection, parameter identification, and content filling. Experiments involving six widely-used models underscore the urgent necessity for enhancing the robustness of LLMs in tool learning. For instance, the performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is no substanti
    
[^94]: INACIA：将大型语言模型整合到巴西审计法院中的机会和挑战

    INACIA: Integrating Large Language Models in Brazilian Audit Courts: Opportunities and Challenges. (arXiv:2401.05273v1 [cs.CL])

    [http://arxiv.org/abs/2401.05273](http://arxiv.org/abs/2401.05273)

    本文介绍了INACIA系统，这是一个将大型语言模型整合到巴西审计法院中的系统，可以自动化案件分析的各个阶段，并展示了其在从案件文件中提取信息、评估合法性和生成司法建议方面的潜力。

    

    本文介绍了INACIA（基于人工智能的辅助指令系统），这是一个开创性的系统，旨在将大型语言模型（LLMs）整合到巴西联邦审计法院（TCU）的运营框架中。该系统自动化了案件分析的各个阶段，包括基本信息提取、可受理性审查、Periculum in mora和Fumus boni iuris分析以及建议生成。通过一系列实验，我们展示了INACIA从案件文件中提取相关信息、评估其合法性并生成司法建议的潜力。利用验证数据集和LLMs，我们的评估方法提供了一种创新的方法来评估系统性能，与人类判断高度相关。结果突显了INACIA处理复杂法律任务的能力，表明其适用于增加法律系统的效率和司法公正性。

    This paper introduces INACIA (Instru\c{c}\~ao Assistida com Intelig\^encia Artificial), a groundbreaking system designed to integrate Large Language Models (LLMs) into the operational framework of Brazilian Federal Court of Accounts (TCU). The system automates various stages of case analysis, including basic information extraction, admissibility examination, Periculum in mora and Fumus boni iuris analyses, and recommendations generation. Through a series of experiments, we demonstrate INACIA's potential in extracting relevant information from case documents, evaluating its legal plausibility, and generating judicial recommendations. Utilizing a validation dataset alongside LLMs, our evaluation methodology presents an innovative approach to assessing system performance, correlating highly with human judgment. The results highlight INACIA's proficiency in handling complex legal tasks, indicating its suitability for augmenting efficiency and judicial fairness within legal systems. The pap
    
[^95]: 深度高效的私密领域生成用于子图联邦学习

    Deep Efficient Private Neighbor Generation for Subgraph Federated Learning. (arXiv:2401.04336v1 [cs.LG])

    [http://arxiv.org/abs/2401.04336](http://arxiv.org/abs/2401.04336)

    本文提出了FedDEP，用于解决子图联邦学习中的信息传播不完整的问题，并提出了一系列新颖的技术设计，包括深度邻居生成和高效的私密领域生成。

    

    在现实应用中，巨大图通常以非中心化子图的形式由多个数据所有者分散存储。为了保护数据隐私，在不损害数据隐私的前提下，考虑到子图联邦学习（subgraph FL）场景是很自然的，其中每个本地客户端持有整个全局图的子图，以获取全局一般化的图挖掘模型。为了解决由于缺少跨子图邻居而导致的局部子图上的信息传播不完整的独特挑战，以前的工作通过缺失邻居生成器和GNN的联合FL来增加本地邻域。然而，它们在FL的效用性、效率性和隐私目标方面存在深层次的限制。在这项工作中，我们提出了FedDEP来全面解决子图FL中的这些挑战。FedDEP包括一系列新颖的技术设计：(1) 利用潜在缺失邻居的GNN嵌入进行深度邻居生成；(2) Effic...

    Behemoth graphs are often fragmented and separately stored by multiple data owners as distributed subgraphs in many realistic applications. Without harming data privacy, it is natural to consider the subgraph federated learning (subgraph FL) scenario, where each local client holds a subgraph of the entire global graph, to obtain globally generalized graph mining models. To overcome the unique challenge of incomplete information propagation on local subgraphs due to missing cross-subgraph neighbors, previous works resort to the augmentation of local neighborhoods through the joint FL of missing neighbor generators and GNNs. Yet their technical designs have profound limitations regarding the utility, efficiency, and privacy goals of FL. In this work, we propose FedDEP to comprehensively tackle these challenges in subgraph FL. FedDEP consists of a series of novel technical designs: (1) Deep neighbor generation through leveraging the GNN embeddings of potential missing neighbors; (2) Effic
    
[^96]: 一种基于社区检测和图神经网络的科学文献链接预测方法

    A Community Detection and Graph Neural Network Based Link Prediction Approach for Scientific Literature. (arXiv:2401.02542v1 [cs.SI])

    [http://arxiv.org/abs/2401.02542](http://arxiv.org/abs/2401.02542)

    本研究提出了一种将社区检测和图神经网络相结合的创新方法，以增强科学文献网络中的链接预测能力。通过将社区检测的结果与图神经网络模型结合，我们突破了链接预测中的可扩展性和分辨率限制，提高了预测准确性。

    

    本研究介绍了一种创新的方法，将社区检测算法与图神经网络（GNN）模型相结合，以增强科学文献网络中的链接预测能力。我们特别关注利用Louvain社区检测算法揭示这些网络中的潜在社区结构，并将其纳入GNN架构中以预测潜在链接。我们的方法展示了在复杂网络中理解社区动态的重要性，并利用社区检测和GNN的优势来提高预测准确性。通过对代表科学合作和引文关系的二分图进行大量实验，我们的方法不仅突显了社区检测和GNN之间的协同作用，还解决了链接预测中的一些普遍挑战，如可扩展性和分辨率限制。结果表明，纳入社区级别信息可以显著提高链接预测的性能。

    This study introduces an innovative approach that integrates community detection algorithms with Graph Neural Network (GNN) models to enhance link prediction in scientific literature networks. We specifically focus on the utilization of the Louvain community detection algorithm to uncover latent community structures within these networks, which are then incorporated into GNN architectures to predict potential links. Our methodology demonstrates the importance of understanding community dynamics in complex networks and leverages the strengths of both community detection and GNNs to improve predictive accuracy. Through extensive experiments on bipartite graphs representing scientific collaborations and citations, our approach not only highlights the synergy between community detection and GNNs but also addresses some of the prevalent challenges in link prediction, such as scalability and resolution limits. The results suggest that incorporating community-level information can significant
    
[^97]: 使用感知损失的扩散模型

    Diffusion Model with Perceptual Loss. (arXiv:2401.00110v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2401.00110](http://arxiv.org/abs/2401.00110)

    本研究介绍了一种使用感知损失的扩散模型，通过无分类器指导实现了生成更真实样本的目的。

    

    使用均方误差损失训练的扩散模型倾向于生成不真实的样本。目前的最先进模型依靠无分类器指导来改善样本质量，然而其惊人的效果尚未完全理解。本文中，我们展示了无分类器指导的有效性在一定程度上源自其作为一种隐式感知指导的形式。因此，我们可以直接在扩散训练中加入感知损失来提高样本质量。由于扩散训练中使用的分数匹配目标与无监督训练感知网络时使用的去噪自动编码器目标非常相似，因此扩散模型本身就是一个感知网络，并可以用于生成有意义的感知损失。我们提出了一种新颖的自感知目标，其结果是扩散模型能够生成更真实的样本。对于条件生成，我们的方法仅改善样本质量，而不与条件绑定。

    Diffusion models trained with mean squared error loss tend to generate unrealistic samples. Current state-of-the-art models rely on classifier-free guidance to improve sample quality, yet its surprising effectiveness is not fully understood. In this paper, We show that the effectiveness of classifier-free guidance partly originates from it being a form of implicit perceptual guidance. As a result, we can directly incorporate perceptual loss in diffusion training to improve sample quality. Since the score matching objective used in diffusion training strongly resembles the denoising autoencoder objective used in unsupervised training of perceptual networks, the diffusion model itself is a perceptual network and can be used to generate meaningful perceptual loss. We propose a novel self-perceptual objective that results in diffusion models capable of generating more realistic samples. For conditional generation, our method only improves sample quality without entanglement with the condit
    
[^98]: 具有重要阶段增强的领域泛化

    Domain Generalization with Vital Phase Augmentation. (arXiv:2312.16451v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.16451](http://arxiv.org/abs/2312.16451)

    本论文介绍了一种使用有限变化的输入数据相位而不是固定相位的方法来解决领域泛化中的相位波动问题，并提出了一种根据领域不变特征程度区分相位的方法。

    

    深度神经网络在图像分类方面表现出色。然而，它们在受损的输入数据下的性能显著下降。领域泛化方法已被提出，用于训练针对于分布外数据的稳健模型。在频域中进行数据增强是其中一种方法，它使模型能够学习相位特征以建立域不变表示。这种方法改变输入数据的振幅而保持相位不变。然而，使用固定相位会使模型对相位波动变得敏感，因为在分布外数据中振幅和相位波动常常发生。为解决这个问题，本研究引入了一种使用输入数据相位的有限变化而不是保持固定相位的方法。基于领域不变特征程度因相位而异的假设，我们提出了一种基于这种程度区分相位的方法。此外，我们还提出了一种方法来。

    Deep neural networks have shown remarkable performance in image classification. However, their performance significantly deteriorates with corrupted input data. Domain generalization methods have been proposed to train robust models against out-of-distribution data. Data augmentation in the frequency domain is one of such approaches that enable a model to learn phase features to establish domain-invariant representations. This approach changes the amplitudes of the input data while preserving the phases. However, using fixed phases leads to susceptibility to phase fluctuations because amplitudes and phase fluctuations commonly occur in out-of-distribution. In this study, to address this problem, we introduce an approach using finite variation of the phases of input data rather than maintaining fixed phases. Based on the assumption that the degree of domain-invariant features varies for each phase, we propose a method to distinguish phases based on this degree. In addition, we propose a
    
[^99]: 从因果角度重新思考图对比学习中的维度理论

    Rethinking Dimensional Rationale in Graph Contrastive Learning from Causal Perspective. (arXiv:2312.10401v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.10401](http://arxiv.org/abs/2312.10401)

    本论文从因果角度重新思考图对比学习中的维度理论，并提出在图中捕捉维度理论的方法，以改善性能，并解决图模型学习中的问题。以上方法在实验中得到验证。

    

    图对比学习是一种在图中捕捉不变信息的学习范式。最近的研究专注于探索图的结构理论，从而增加不变信息的可区分性。然而，这些方法可能导致图模型朝向解释图的可解释性进行错误学习，因此学习到的噪声和与任务无关的信息干扰了图的预测。为了探索图的内在理论，我们提出了从图中捕捉维度理论的方法，但这在文献中并没有得到足够的关注。通过实验验证了上述路径的可行性。为了阐明维度理论对性能改进的内在机制，我们从因果角度重新思考图对比学习中的维度理论。

    Graph contrastive learning is a general learning paradigm excelling at capturing invariant information from diverse perturbations in graphs. Recent works focus on exploring the structural rationale from graphs, thereby increasing the discriminability of the invariant information. However, such methods may incur in the mis-learning of graph models towards the interpretability of graphs, and thus the learned noisy and task-agnostic information interferes with the prediction of graphs. To this end, with the purpose of exploring the intrinsic rationale of graphs, we accordingly propose to capture the dimensional rationale from graphs, which has not received sufficient attention in the literature. The conducted exploratory experiments attest to the feasibility of the aforementioned roadmap. To elucidate the innate mechanism behind the performance improvement arising from the dimensional rationale, we rethink the dimensional rationale in graph contrastive learning from a causal perspective a
    
[^100]: 气候不确定性中的学习和规划：在时变部分可观测环境中的应用

    Weathering Ongoing Uncertainty: Learning and Planning in a Time-Varying Partially Observable Environment. (arXiv:2312.03263v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2312.03263](http://arxiv.org/abs/2312.03263)

    本研究结合了时变马尔科夫决策过程和部分可观测性，提出了在不确定、随机和时变环境中进行学习和规划的方法。通过记忆优先状态估计和规划策略的集成，我们实现了对长期奖励的优化，在仿真和硬件验证中取得了良好的结果。

    

    在不确定、随机和时变环境中，最优决策对于自主系统来说是一个重大挑战。随着时间的推移，环境的变化可以对系统的最优决策策略产生显著影响。为了对这样的环境进行建模，我们的工作将之前的时变马尔科夫决策过程(time-varying Markov Decision Processes, TVMDP)的概念与部分可观测性相结合，引入了时变部分可观测马尔科夫决策过程(time-varying Partially Observable Markov Decision Processes, TV-POMDP)。我们提出了一个双管齐下的方法来在TV-POMDP中准确估计和规划：1）记忆优先状态估计(Memory Prioritized State Estimation, MPSE)，利用加权记忆提供更准确的时变转移估计；2）MPSE集成的规划策略，优化长期奖励的同时考虑时间约束。我们使用仿真和硬件验证了所提出的框架和算法，机器人在一个部分可观测、时变的环境中进行探索。

    Optimal decision-making presents a significant challenge for autonomous systems operating in uncertain, stochastic and time-varying environments. Environmental variability over time can significantly impact the system's optimal decision making strategy for mission completion. To model such environments, our work combines the previous notion of Time-Varying Markov Decision Processes (TVMDP) with partial observability and introduces Time-Varying Partially Observable Markov Decision Processes (TV-POMDP). We propose a two-pronged approach to accurately estimate and plan within the TV-POMDP: 1) Memory Prioritized State Estimation (MPSE), which leverages weighted memory to provide more accurate time-varying transition estimates; and 2) an MPSE-integrated planning strategy that optimizes long-term rewards while accounting for temporal constraint. We validate the proposed framework and algorithms using simulations and hardware, with robots exploring a partially observable, time-varying environ
    
[^101]: 时间中的涟漪：美国历史中的不连续性

    A ripple in time: a discontinuity in American history. (arXiv:2312.01185v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.01185](http://arxiv.org/abs/2312.01185)

    该论文通过使用向量嵌入和非线性降维方法，发现GPT-2与UMAP的结合可以提供更好的分离和聚类效果。同时，经过微调的DistilBERT模型可用于识别总统和演讲的年份。

    

    在这篇论文中，我们使用来自Kaggle的国情咨文数据集对美国历史的总体时间线及咨文本身的特点和性质进行了一些令人惊讶（也有些不那么令人惊讶）的观察。我们的主要方法是使用向量嵌入，如BERT（DistilBERT）和GPT-2。虽然广泛认为BERT（及其变体）最适合NLP分类任务，但我们发现GPT-2结合UMAP等非线性降维方法可以提供更好的分离和更强的聚类效果。这使得GPT-2 + UMAP成为一个有趣的替代方案。在我们的情况下，不需要对模型进行微调，预训练的GPT-2模型就足够好用。我们还使用了经过微调的DistilBERT模型来检测哪位总统发表了哪篇演讲，并取得了非常好的结果（准确率为93\% - 95\%，具体取决于运行情况）。为了确定写作年份，我们还执行了一个类似的任务。

    In this note we use the State of the Union Address (SOTU) dataset from Kaggle to make some surprising (and some not so surprising) observations pertaining to the general timeline of American history, and the character and nature of the addresses themselves. Our main approach is using vector embeddings, such as BERT (DistilBERT) and GPT-2.  While it is widely believed that BERT (and its variations) is most suitable for NLP classification tasks, we find out that GPT-2 in conjunction with nonlinear dimension reduction methods such as UMAP provide better separation and stronger clustering. This makes GPT-2 + UMAP an interesting alternative. In our case, no model fine-tuning is required, and the pre-trained out-of-the-box GPT-2 model is enough.  We also used a fine-tuned DistilBERT model for classification detecting which President delivered which address, with very good results (accuracy 93\% - 95\% depending on the run). An analogous task was performed to determine the year of writing, an
    
[^102]: 自适应图像配准：将深度学习和优化函数相结合的混合方法以提高精度

    Adaptive Image Registration: A Hybrid Approach Integrating Deep Learning and Optimization Functions for Enhanced Precision. (arXiv:2311.15497v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2311.15497](http://arxiv.org/abs/2311.15497)

    本研究提出了一种自适应图像配准的混合方法，将深度学习和优化函数相结合，通过使用学习方法的输出作为优化的初始参数，并在计算上重点处理损失最大的图像对，取得了1.6%的性能提升和1.0%的变形场平滑度提升。

    

    图像配准传统上采用两种不同的方法：依赖于强大深度神经网络的学习方法和应用复杂数学变换来使图像变形的优化方法。当然，这两种范式都有优点和缺点，在这项工作中，我们试图将它们各自的优势结合在一个简化的框架中，使用学习方法的输出作为优化的初始参数，同时优先考虑对损失最大的图像对进行计算。我们的研究结果显示，在测试数据上提高了1.6%的性能，同时保持相同的推理时间，并且变形场平滑度方面获得了1.0%的性能提升。

    Image registration has traditionally been done using two distinct approaches: learning based methods, relying on robust deep neural networks, and optimization-based methods, applying complex mathematical transformations to warp images accordingly. Of course, both paradigms offer advantages and disadvantages, and, in this work, we seek to combine their respective strengths into a single streamlined framework, using the outputs of the learning based method as initial parameters for optimization while prioritizing computational power for the image pairs that offer the greatest loss. Our investigations showed improvements of up to 1.6% in test data, while maintaining the same inference time, and a substantial 1.0% points performance gain in deformation field smoothness.
    
[^103]: INTERVENOR: 使用交互式修复链条来引导大型语言模型的编码能力

    INTERVENOR: Prompt the Coding Ability of Large Language Models with the Interactive Chain of Repairing. (arXiv:2311.09868v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2311.09868](http://arxiv.org/abs/2311.09868)

    INTERVENOR模型通过模拟人类修复代码的行为，使用交互式修复链条来引导大型语言模型的编码能力，取得了显著的性能提升。

    

    本文提出了一种名为INTERVENOR的交互式修复链条（INTERactiVE chaiN Of Repairing），模拟了人类修复代码的行为（迭代判断、重新思考和修复），并促进了大型语言模型（LLMs）的编码能力。具体而言，INTERVENOR采用了两个基于LLM的代理，即Code Learner和Code Teacher，它们在代码修复中扮演不同的角色，并通过互动来修复生成的代码。Code Learner根据Code Teacher的指导生成和修复代码，而Code Teacher根据编译器的反馈重新思考代码错误，并迭代生成修复链条（CoR）以引导Code Learner的代码修复过程。实验证明，INTERVENOR优于最先进的方法，在代码生成和代码转换任务上相对于GPT-3.5模型分别取得了约13%和4.5%的提升。进一步分析表明，CoR能够揭示bug的原因。

    This paper proposes INTERactiVE chaiN Of Repairing (INTERVENOR), which mimics human code repairing behavior (iteratively judging, rethinking, and repairing) and prompts the coding ability of regard Large Language Models (LLMs). Specifically, INTERVENOR employs two LLM based agents, Code Learner and Code Teacher, to play different roles in code repairing and work interactively to repair the generated codes. The Code Learner is asked to generate and repair code according to the instructions from the Code Teacher. The Code Teacher rethinks the code errors according to the corresponding feedback from compilers and iteratively generates the chain-of-repairing (CoR) to guide the code repairing process for Code Learner. Our experiments show that INTERVENOR outperforms the state-of-the-art methods and achieves about 13% and 4.5% improvements over the GPT-3.5 model in code generation and code translation tasks, respectively. Our further analyses show that CoR can illuminate the bug reasons and 
    
[^104]: 一个基础图模型

    A Foundation Graph Model. (arXiv:2311.03976v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.03976](http://arxiv.org/abs/2311.03976)

    本文提出了一个基于对抗性对比学习的基础图模型FoToM，该模型通过节点和边特征排除进行图预训练，在多个领域上实现了正向迁移，并取得了显著的性能提升。

    

    无监督图表示学习的主要优势是在数据或标签稀缺的情况下，可以对预训练模型进行微调。现有的方法是针对特定领域的，保持预训练和目标数据集之间的节点和边属性一致。这使得无法在其他领域进行迁移。能够在任意任务和领域上实现正向迁移的模型将成为第一个基础图模型。在这项工作中，我们使用对抗性对比学习提出了FoToM，一种基于节点和边特征排除的图预训练方法。我们使用FoToM在多个图领域上进行预训练，得到了第一个基础图模型。我们在来自多个领域的评估数据集上展示了正向迁移。在所有数据集上，性能最差时与有监督基线相当，76%的数据集在95%置信度下都显著优于有监督基线（P≤0.01），误差减少了8%至40%。

    The principal benefit of unsupervised graph representation learning is that a pre-trained model can be fine-tuned where data or labels are scarce. Existing approaches are domain specific, maintaining consistent node and edge attributes across the pre-training and target datasets. This precludes transfer to other domains. A model capable of positive transfer on arbitrary tasks and domains would represent the first foundation graph model.  In this work we use adversarial contrastive learning to present FoToM, a graph pre-training method based on node and edge feature exclusion. We use FoToM to pre-train models over multiple graph domains, producing the first foundation graph models. We demonstrate positive transfer on evaluation datasets from multiple domains, including domains not present in pre-training data. On all datasets performance is at worst on-par and on 76% significantly better than a supervised baseline ($P \leq 0.01$), with an 8 to 40% reduction in error at 95% confidence. C
    
[^105]: SAGE: 具有基于实际执行的智能家居助手

    SAGE: Smart home Agent with Grounded Execution. (arXiv:2311.00772v1 [cs.AI])

    [http://arxiv.org/abs/2311.00772](http://arxiv.org/abs/2311.00772)

    SAGE框架通过替换手动定义的推理逻辑，实现了基于实际执行的智能家居助手，提高了灵活性。它可以学习用户偏好，与设备交互，监视设备，并理解自然的设备引用。在评估中，SAGE在43个智能家居任务中成功完成了23个任务，优于现有的LLM基准。

    

    本文介绍了SAGE（具有基于实际执行的智能家居助手）框架，该框架通过将手动定义的推理逻辑替换为LLM驱动的自主代理系统，以最大程度地提高智能家居助手的灵活性。SAGE通过协调一系列工具整合用户偏好、设备状态和外部因素（如天气和电视节目表）。SAGE的功能包括从自然语言表达中学习用户偏好，通过阅读设备的API文档与设备交互，编写代码以持续监视设备，并理解自然的设备引用。为了评估SAGE，我们开发了一个包含43个高度挑战的智能家居任务的基准，SAGE成功完成了23个任务，显著优于现有的LLM基准（5/43）。

    This article introduces SAGE (Smart home Agent with Grounded Execution), a framework designed to maximize the flexibility of smart home assistants by replacing manually-defined inference logic with an LLM-powered autonomous agent system. SAGE integrates information about user preferences, device states, and external factors (such as weather and TV schedules) through the orchestration of a collection of tools. SAGE's capabilities include learning user preferences from natural-language utterances, interacting with devices by reading their API documentation, writing code to continuously monitor devices, and understanding natural device references. To evaluate SAGE, we develop a benchmark of 43 highly challenging smart home tasks, where SAGE successfully achieves 23 tasks, significantly outperforming existing LLM-enabled baselines (5/43).
    
[^106]: 构建具有多样数据损坏情况下鲁棒性的离线强化学习

    Towards Robust Offline Reinforcement Learning under Diverse Data Corruption. (arXiv:2310.12955v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.12955](http://arxiv.org/abs/2310.12955)

    本文研究了在多种数据损坏情况下，离线强化学习算法的性能。研究发现，隐式Q-learning（IQL）在各种离线强化学习算法中展现出了较强的鲁棒性能，其采用的监督策略学习方案为关键。然而，在动力学损坏下，IQL仍然存在Q函数的重尾目标问题。

    

    离线强化学习（RL）是一种有前途的方法，可以从离线数据集中学习强化策略，而无需与环境进行昂贵或不安全的交互。然而，人们在真实环境中收集的数据集往往存在噪声，甚至可能被恶意损坏，这可能会严重影响离线强化学习的性能。本研究首先对当前离线强化学习算法在包括状态、动作、奖励和动力学在内的全面数据损坏情况下的性能进行了调查。我们的大量实验显示，隐式Q-learning（IQL）在各种离线强化学习算法中表现出了可靠的抗数据损坏能力。此外，我们还进行了经验和理论分析，以了解IQL的鲁棒性能，并将其监督策略学习方案确定为关键因素。尽管相对鲁棒，但IQL在动力学损坏下仍然存在Q函数的重尾目标问题。

    Offline reinforcement learning (RL) presents a promising approach for learning reinforced policies from offline datasets without the need for costly or unsafe interactions with the environment. However, datasets collected by humans in real-world environments are often noisy and may even be maliciously corrupted, which can significantly degrade the performance of offline RL. In this work, we first investigate the performance of current offline RL algorithms under comprehensive data corruption, including states, actions, rewards, and dynamics. Our extensive experiments reveal that implicit Q-learning (IQL) demonstrates remarkable resilience to data corruption among various offline RL algorithms. Furthermore, we conduct both empirical and theoretical analyses to understand IQL's robust performance, identifying its supervised policy learning scheme as the key factor. Despite its relative robustness, IQL still suffers from heavy-tail targets of Q functions under dynamics corruption. To tack
    
[^107]: 大规模语言模型对监督微调数据组合的影响

    How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. (arXiv:2310.05492v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05492](http://arxiv.org/abs/2310.05492)

    本研究探讨了大规模语言模型在监督微调过程中，特别是数学推理和代码生成能力方面，数据组合的影响。实验结果显示，较大模型在相同数据量下表现出更好的性能，通过增加微调数据和模型参数，数学推理和代码生成能力得到显著提升。

    

    大规模语言模型（LLMs）具备大量的预训练标记和参数，展现出数学推理、代码生成和指令跟随等能力。这些能力通过监督微调（SFT）进一步增强。开源社区已经研究了针对每种能力的临时SFT，而专有LLMs可以适用于所有能力。因此，研究如何通过SFT解锁多重能力变得重要。在本研究中，我们特别关注SFT过程中数学推理、代码生成和人类对齐能力之间的数据组合。从规模的角度，我们研究了模型能力与各种因素之间的关系，包括数据量、数据组合比例、模型参数和SFT策略。我们的实验发现不同的能力表现出不同的扩展模式，较大的模型通常在相同的数据量下表现出更优异的性能。数学推理和代码生成能力通过微调数据和模型参数的增加而获得显著的性能提升。

    Large language models (LLMs) with enormous pre-training tokens and parameter amounts emerge abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). The open-source community has studied on ad-hoc SFT for each ability, while proprietary LLMs are versatile for all abilities. It is important to investigate how to unlock them with multiple abilities via SFT. In this study, we specifically focus on the data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. From a scaling perspective, we investigate the relationship between model abilities and various factors including data amounts, data composition ratio, model parameters, and SFT strategies. Our experiments reveal that different abilities exhibit different scaling patterns, and larger models generally show superior performance with the same amount of data. Mathematical reasoning and code
    
[^108]: MULTISCRIPT: 多模式脚本学习用于支持开放领域的日常任务

    MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain Everyday Tasks. (arXiv:2310.04965v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.04965](http://arxiv.org/abs/2310.04965)

    该论文提出了一个新的基准挑战MultiScript，旨在解决现有脚本学习方法对于开放领域日常任务的限制。论文介绍了两个多模式脚本学习任务，并提供了对应的输入和输出要求。

    

    从视频演示中自动生成脚本（即文本描述的关键步骤序列）并推理后续步骤对于现代AI虚拟助手来引导人们完成日常任务，尤其是陌生任务至关重要。然而，当前的生成式脚本学习方法很大程度上依赖于结构良好的前置步骤的文本和/或图像描述，或者限于特定领域，导致与真实世界中用户场景存在差距。为了解决这些限制，我们提出了一个新的基准挑战——MultiScript，其中包含两个关于面向任务的多模式脚本学习的新任务：（1）多模式脚本生成，和（2）后续步骤预测。对于这两个任务，输入包括目标任务名称和演示视频，预期输出为（1）基于演示视频的结构化步骤描述的序列，和（2）针对每个步骤的单一文本描述。

    Automatically generating scripts (i.e. sequences of key steps described in text) from video demonstrations and reasoning about the subsequent steps are crucial to the modern AI virtual assistants to guide humans to complete everyday tasks, especially unfamiliar ones. However, current methods for generative script learning rely heavily on well-structured preceding steps described in text and/or images or are limited to a certain domain, resulting in a disparity with real-world user scenarios. To address these limitations, we present a new benchmark challenge -- MultiScript, with two new tasks on task-oriented multimodal script learning: (1) multimodal script generation, and (2) subsequent step prediction. For both tasks, the input consists of a target task name and a video illustrating what has been done to complete the target task, and the expected output is (1) a sequence of structured step descriptions in text based on the demonstration video, and (2) a single text description for th
    
[^109]: LLMCarbon: 对大型语言模型的端到端碳足迹建模

    LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models. (arXiv:2309.14393v1 [cs.CL])

    [http://arxiv.org/abs/2309.14393](http://arxiv.org/abs/2309.14393)

    本研究提出了LLMCarbon，一个针对密集型和MoE LLMs设计的端到端碳足迹预测模型，解决了现有工具的限制，并显著提升了估计的准确性。

    

    大型语言模型（LLMs）的碳足迹是一个重要关注点，包括它们的训练、推理、实验和存储过程中的排放，包括运营和固定碳排放。一个重要方面是在LLMs训练之前准确估计其碳影响，这在很大程度上依赖于GPU的使用。现有研究已报告了LLMs训练的碳足迹，但只有一个工具mlco2能够在实际训练之前预测新的神经网络的碳足迹。然而，mlco2存在一些严重的限制。它不能扩展其对密集或专家混合（MoE）LLMs的估计，忽视了关键的架构参数，仅关注GPU，并不能建模固化的碳足迹。为了解决这些问题，我们引入了LLMCarbon，一个为密集型和MoE LLMs设计的端到端碳足迹预测模型。与mlco2相比，LLMCarbon显著增强了准确性。

    The carbon footprint associated with large language models (LLMs) is a significant concern, encompassing emissions from their training, inference, experimentation, and storage processes, including operational and embodied carbon emissions. An essential aspect is accurately estimating the carbon impact of emerging LLMs even before their training, which heavily relies on GPU usage. Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations. It cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs, disregards critical architectural parameters, focuses solely on GPUs, and cannot model embodied carbon footprints. Addressing these gaps, we introduce \textit{LLMCarbon}, an end-to-end carbon footprint projection model designed for both dense and MoE LLMs. Compared to mlco2, LLMCarbon significantly enhances the ac
    
[^110]: 利用大型语言模型探索自我强化以改进学生生成的多项选择题解释

    Exploring Self-Reinforcement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models. (arXiv:2309.10444v1 [cs.AI])

    [http://arxiv.org/abs/2309.10444](http://arxiv.org/abs/2309.10444)

    本文提出了一个自我强化大型语言模型框架，自动生成和评估学生生成的解释，用于改进学生资源共享中学生生成的多项选择题的解释质量。

    

    学生资源共享涉及学生生成和分享学习资源。在学生生成多项选择题时，创建解释是一个关键步骤，因为它有助于对相关概念的深入理解。然而，学生往往由于主题理解有限和仅仅重申问题、干扰因素和正确答案的倾向而难以编写有效的解释。为了帮助支撑这个任务，在这项工作中，我们提出了一个自我强化的大型语言模型框架，旨在自动生成和评估解释。该框架由三个模块组成，生成与学生对齐的解释，评估这些解释以确保其质量，并迭代增强解释。如果一个解释的评估分数低于定义的阈值，框架会迭代地优化和重新评估解释。重要的是，我们的框架模拟了一个学生学习的过程。

    Learnersourcing involves students generating and sharing learning resources with their peers. When learnersourcing multiple-choice questions, creating explanations for the generated questions is a crucial step as it facilitates a deeper understanding of the related concepts. However, it is often difficult for students to craft effective explanations due to limited subject understanding and a tendency to merely restate the question stem, distractors, and correct answer. To help scaffold this task, in this work we propose a self-reinforcement large-language-model framework, with the goal of generating and evaluating explanations automatically. Comprising three modules, the framework generates student-aligned explanations, evaluates these explanations to ensure their quality and iteratively enhances the explanations. If an explanation's evaluation score falls below a defined threshold, the framework iteratively refines and reassesses the explanation. Importantly, our framework emulates th
    
[^111]: 预训练多语言翻译模型上的属性控制器能否迁移到其他语言？

    How Transferable are Attribute Controllers on Pretrained Multilingual Translation Models?. (arXiv:2309.08565v1 [cs.CL])

    [http://arxiv.org/abs/2309.08565](http://arxiv.org/abs/2309.08565)

    本文研究了如何将预训练的多语言翻译模型中的属性控制器迁移到没有监督数据的语言。通过全面分析不同数据场景下的训练和推断时控制技术，揭示了它们在零样本性能和领域鲁棒性上的相对优势和劣势。

    

    最近，将机器翻译模型定制为符合细粒度属性（如形式）已取得了巨大进展。然而，当前方法大多依赖于至少一些带有属性注释的监督数据。因此，数据稀缺仍然是将此定制能力普及到更广泛语言范围，尤其是低资源语言的一个瓶颈。鉴于最近在预训练大规模多语言翻译模型方面取得的进展，我们将它们作为对没有监督数据的语言进行属性控制能力迁移的基础。在这项工作中，我们基于预训练的NLLB-200模型对属性控制器的迁移进行了全面分析。我们研究了在各种数据场景下的训练和推断时控制技术，并揭示了它们在零样本性能和领域鲁棒性上的相对优势和劣势。我们显示出两种范式是互补的，通过一致的改进来证明。

    Customizing machine translation models to comply with fine-grained attributes such as formality has seen tremendous progress recently. However, current approaches mostly rely on at least some supervised data with attribute annotation. Data scarcity therefore remains a bottleneck to democratizing such customization possibilities to a wider range of languages, lower-resource ones in particular. Given recent progress in pretrained massively multilingual translation models, we use them as a foundation to transfer the attribute controlling capabilities to languages without supervised data. In this work, we present a comprehensive analysis of transferring attribute controllers based on a pretrained NLLB-200 model. We investigate both training- and inference-time control techniques under various data scenarios, and uncover their relative strengths and weaknesses in zero-shot performance and domain robustness. We show that both paradigms are complementary, as shown by consistent improvements o
    
[^112]: 使用代码模型和领域适应性的自动化测试用例生成

    Automated Test Case Generation Using Code Models and Domain Adaptation. (arXiv:2308.08033v1 [cs.SE])

    [http://arxiv.org/abs/2308.08033](http://arxiv.org/abs/2308.08033)

    本研究提出了一个完全自动化的测试框架，利用开发人员编写的测试和可用的代码模型生成可编译、易读的单元测试。

    

    最先进的自动化测试生成技术，例如基于搜索的测试，通常对开发人员创建的测试用例一无所知。因此，它们通常生成的测试用例不易阅读，并且可能无法检测所有复杂缺陷，而开发人员编写的测试用例则可以。在这项研究中，我们利用基于Transformer的代码模型生成可以补充基于搜索测试生成的单元测试。具体而言，我们使用CodeT5，即最先进的大型代码模型，并对测试生成下游任务进行微调。我们使用Methods2test数据集对CodeT5进行微调，并使用Defects4j进行项目级领域适应性和评估。本研究的主要贡献是提出了一个完全自动化的测试框架，利用开发人员编写的测试和可用的代码模型生成可编译、易读的单元测试。结果显示，我们的方法可以生成新的测试用例，覆盖了已经被测试过的代码行。

    State-of-the-art automated test generation techniques, such as search-based testing, are usually ignorant about what a developer would create as a test case. Therefore, they typically create tests that are not human-readable and may not necessarily detect all types of complex bugs developer-written tests would do. In this study, we leverage Transformer-based code models to generate unit tests that can complement search-based test generation. Specifically, we use CodeT5, i.e., a state-of-the-art large code model, and fine-tune it on the test generation downstream task. For our analysis, we use the Methods2test dataset for fine-tuning CodeT5 and Defects4j for project-level domain adaptation and evaluation. The main contribution of this study is proposing a fully automated testing framework that leverages developer-written tests and available code models to generate compilable, human-readable unit tests. Results show that our approach can generate new test cases that cover lines that were
    
[^113]: GBSD: 带有阶段扩散的生成虚化效果

    GBSD: Generative Bokeh with Stage Diffusion. (arXiv:2306.08251v1 [cs.CV])

    [http://arxiv.org/abs/2306.08251](http://arxiv.org/abs/2306.08251)

    本文提出了第一个基于生成对抗网络技术的文本到图像模型GBSD，并使用阶段扩散技术合成具有虚化风格的照片。

    

    Bokeh效果是一种艺术技巧，可以使照片中的失焦区域模糊，近期由于文本到图像合成技术的发展以及智能手机的普及和照片分享应用程序的广泛使用，这种效果备受关注。以往的虚化效果渲染工作都是针对已有照片进行事后图像处理，使用经典计算机图形学或神经渲染技术产生类似的模糊效果，但是往往存在深度不连续的图像伪影，而且限制于练习数据测试的虚化效果。近期的扩散模型可以合成具有艺术风格的图像，但是要求生成高维度掩模或者进行昂贵的调整，可能影响整体图像特征。本文提出了GBSD，这是第一个基于生成对抗网络技术的文本到图像模型，具有虚化风格的照片。受扩散模型中逐步合成图像的启发，我们的方法使用了阶段扩散技术。

    The bokeh effect is an artistic technique that blurs out-of-focus areas in a photograph and has gained interest due to recent developments in text-to-image synthesis and the ubiquity of smart-phone cameras and photo-sharing apps. Prior work on rendering bokeh effects have focused on post hoc image manipulation to produce similar blurring effects in existing photographs using classical computer graphics or neural rendering techniques, but have either depth discontinuity artifacts or are restricted to reproducing bokeh effects that are present in the training data. More recent diffusion based models can synthesize images with an artistic style, but either require the generation of high-dimensional masks, expensive fine-tuning, or affect global image characteristics. In this paper, we present GBSD, the first generative text-to-image model that synthesizes photorealistic images with a bokeh style. Motivated by how image synthesis occurs progressively in diffusion models, our approach combi
    
[^114]: Synapse：利用少量示例为实现人类级别的计算机控制打下基础

    Synapse: Leveraging Few-Shot Exemplars for Human-Level Computer Control. (arXiv:2306.07863v1 [cs.AI])

    [http://arxiv.org/abs/2306.07863](http://arxiv.org/abs/2306.07863)

    本文探究了使用大型语言模型提示少量示例来实现人类级别的计算机控制；通过分解演示、过滤状态并重新构造任务描述，示例检索等步骤，Synapse 具备了适应多任务、泛化多环境的能力。

    

    本文探究了使用大型语言模型（LLMs）提示少量示例来进行计算机自动化的设计。虽然以前的提示方法着重于自我纠正，但我们发现仅仅有良好结构的示例就足以实现人类级别的性能。我们提出了 Synapse，一种上下文计算机控制代理，在 MiniWob++ 基准测试中展现了人类级别的性能。Synapse 由三个主要组件组成：1）状态条件分解，根据代理需要新环境状态将演示分为示例集，实现了时间抽象；2）结构化提示，过滤状态并重新构造每个集合的任务描述以改善计划的正确性；3）示例检索，将传入的任务与示例数据库中的对应示例相关联，以实现多任务适应和泛化。Synapse 克服了上下文长度限制，减少了多步控制中的错误，可以更有效灵活地使用语言模型进行计算机自动化。

    This paper investigates the design of few-shot exemplars for computer automation through prompting large language models (LLMs). While previous prompting approaches focus on self-correction, we find that well-structured exemplars alone are sufficient for human-level performance. We present Synapse, an in-context computer control agent demonstrating human-level performance on the MiniWob++ benchmark. Synapse consists of three main components: 1) state-conditional decomposition, which divides demonstrations into exemplar sets based on the agent's need for new environment states, enabling temporal abstraction; 2) structured prompting, which filters states and reformulates task descriptions for each set to improve planning correctness; and 3) exemplar retrieval, which associates incoming tasks with corresponding exemplars in an exemplar database for multi-task adaptation and generalization. Synapse overcomes context length limits, reduces errors in multi-step control, and allows for more e
    
[^115]: 搜寻海洋世界生命探测器上用于检测显微生物生物标志的仪器自主性

    Onboard Science Instrument Autonomy for the Detection of Microscopy Biosignatures on the Ocean Worlds Life Surveyor. (arXiv:2304.13189v1 [astro-ph.IM])

    [http://arxiv.org/abs/2304.13189](http://arxiv.org/abs/2304.13189)

    在太阳系的冰卫星寻找外星生命，需要用一套补充仪器对多个独立的生物标志进行采样。机载科学仪器自主性（OSIA）是一种新兴学科，可以评估、总结和优先考虑观测仪器数据以最大化科学回报。

    

    寻找外星生命是一项重要的科学探索，对文明级别具有影响。太阳系中的冰卫星因其液态海洋使其成为微观生命潜在寄生地，是探索重要目标。然而，生命的精确定义缺乏使制定探测策略面临基本挑战。为增加无歧义探测的机会，一套补充仪器必须对多个独立的生物标志进行采样（例如，组成，运动/行为，可见结构）。这样的仪器套件可以产生比像土卫六或木卫二这样的遥远海洋世界传输的原始数据多10,000倍。为解决这种带宽限制，机载科学仪器自主性（OSIA）是一种评估、总结和优先考虑观测仪器数据以最大化科学回报的飞行系统的新兴学科。我们描述了两个开发的OSIA实现方案。

    The quest to find extraterrestrial life is a critical scientific endeavor with civilization-level implications. Icy moons in our solar system are promising targets for exploration because their liquid oceans make them potential habitats for microscopic life. However, the lack of a precise definition of life poses a fundamental challenge to formulating detection strategies. To increase the chances of unambiguous detection, a suite of complementary instruments must sample multiple independent biosignatures (e.g., composition, motility/behavior, and visible structure). Such an instrument suite could generate 10,000x more raw data than is possible to transmit from distant ocean worlds like Enceladus or Europa. To address this bandwidth limitation, Onboard Science Instrument Autonomy (OSIA) is an emerging discipline of flight systems capable of evaluating, summarizing, and prioritizing observational instrument data to maximize science return. We describe two OSIA implementations developed a
    
[^116]: 颗粒球计算：一种高效、鲁棒和可解释的自适应多粒度表示和计算方法

    Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method. (arXiv:2304.11171v1 [cs.LG])

    [http://arxiv.org/abs/2304.11171](http://arxiv.org/abs/2304.11171)

    本文提出了一种基于颗粒球计算的自适应多粒度表示和计算方法，能够提高机器学习的效率、鲁棒性和可解释性。

    

    人类认知具有“先大后小”的认知机制，因此具有自适应的多粒度描述能力。这导致了有效性、鲁棒性和可解释性等计算特性。本文提出了一种新的基于颗粒球计算的自适应多粒度表示和计算方法。他们将这种方法应用于几个机器学习任务，并证明其相对于其他最先进的方法的有效性。

    Human cognition has a ``large-scale first'' cognitive mechanism, therefore possesses adaptive multi-granularity description capabilities. This results in computational characteristics such as efficiency, robustness, and interpretability. Although most existing artificial intelligence learning methods have certain multi-granularity features, they do not fully align with the ``large-scale first'' cognitive mechanism. Multi-granularity granular-ball computing is an important model method developed in recent years. This method can use granular-balls of different sizes to adaptively represent and cover the sample space, and perform learning based on granular-balls. Since the number of coarse-grained "granular-ball" is smaller than the number of sample points, granular-ball computing is more efficient; the coarse-grained characteristics of granular-balls are less likely to be affected by fine-grained sample points, making them more robust; the multi-granularity structure of granular-balls ca
    
[^117]: 基于单次学习的历史手稿文本检测方法 OTS

    OTS: A One-shot Learning Approach for Text Spotting in Historical Manuscripts. (arXiv:2304.00746v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.00746](http://arxiv.org/abs/2304.00746)

    提出了一种基于单次学习的历史手稿文本检测方法 OTS， 尤其对于低资源检测任务，使用新型的“环形损失”损失函数提高了检测能力，同时创建了包含古代东巴象形文字的手稿数据集。

    

    历史手稿处理面临有限的注释训练数据和新类别出现等挑战。为解决这一问题，我们提出了一种基于单次学习的文本检测方法 OTS，通过仅一个注释样本，准确可靠地检测出新颖字符。灵感源自认知研究，引入空间对齐模块，基于一个支持图像发现、关注和学习查询图像中最具有区别性的空间区域。尤其是，针对低资源检测任务通常面临样本不平衡问题，我们提出了一种名为“环形损失”的新型损失函数，可以使距离度量的嵌入空间更具有区分性。该方法高效，只需要少量的训练样本，具有处理新颖字符和符号的显著能力。为了增强数据集的多样性，我们创建了一个包含古代东巴象形文字（DBH）的手稿数据集。

    Historical manuscript processing poses challenges like limited annotated training data and novel class emergence. To address this, we propose a novel One-shot learning-based Text Spotting (OTS) approach that accurately and reliably spots novel characters with just one annotated support sample. Drawing inspiration from cognitive research, we introduce a spatial alignment module that finds, focuses on, and learns the most discriminative spatial regions in the query image based on one support image. Especially, since the low-resource spotting task often faces the problem of example imbalance, we propose a novel loss function called torus loss which can make the embedding space of distance metric more discriminative. Our approach is highly efficient and requires only a few training samples while exhibiting the remarkable ability to handle novel characters, and symbols. To enhance dataset diversity, a new manuscript dataset that contains the ancient Dongba hieroglyphics (DBH) is created. We
    
[^118]: TraffNet：学习道路网络数字孪生交通生成因果关系

    TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins. (arXiv:2303.15954v1 [cs.LG])

    [http://arxiv.org/abs/2303.15954](http://arxiv.org/abs/2303.15954)

    TraffNet是一个学习交通量生成原因的深度学习框架，将车辆轨迹数据表示为异构图，利用递归神经网络结构实现了对交通生成原因的预测。

    

    道路网络数字孪生（RNDT）在开发下一代智能交通系统中发挥着关键作用，可以实现更精确的交通规划和控制。为了支持实时决策，RNDT需要一个模型，从在线传感器数据中动态学习交通模式并生成高保真模拟结果。尽管基于图神经网络的当前交通预测技术已经实现了最先进的性能，但是这些技术仅通过挖掘历史交通数据中的相关性来预测未来交通，而忽略了交通生成的原因，例如交通需求和路径选择。因此，它们的性能对于实时决策是不可靠的。为了填补这一差距，我们引入了一个新的深度学习框架称为 TraffNet，该框架从车辆轨迹数据中学习交通量的因果性。首先，我们使用异构图来表示道路网络，使模型能够并入预测所需的其他数据，然后我们提出了一种新颖的递归神经网络结构，从而能够预测交通量的因果联系。

    Road network digital twins (RNDTs) play a critical role in the development of next-generation intelligent transportation systems, enabling more precise traffic planning and control. To support just-in-time (JIT) decision making, RNDTs require a model that dynamically learns the traffic patterns from online sensor data and generates high-fidelity simulation results. Although current traffic prediction techniques based on graph neural networks have achieved state-of-the-art performance, these techniques only predict future traffic by mining correlations in historical traffic data, disregarding the causes of traffic generation, such as traffic demands and route selection. Therefore, their performance is unreliable for JIT decision making. To fill this gap, we introduce a novel deep learning framework called TraffNet that learns the causality of traffic volume from vehicle trajectory data. First, we use a heterogeneous graph to represent the road network, allowing the model to incorporate 
    
[^119]: Prismer: 一种具有专家集合的视觉语言模型

    Prismer: A Vision-Language Model with An Ensemble of Experts. (arXiv:2303.02506v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02506](http://arxiv.org/abs/2303.02506)

    Prismer是一种数据和参数高效的视觉语言模型，它利用了一组领域专家的集合，通过汇集这些专家知识并将其适应于各种视觉语言推理任务，实现了与当前最先进模型竞争的微调和少样本学习性能，同时需要少至两个数量级的训练数据。

    Prismer is a data- and parameter-efficient vision-language model that leverages an ensemble of domain experts, achieving fine-tuned and few-shot learning performance competitive with current state-of-the-art models, whilst requiring up to two orders of magnitude less training data.

    最近的视觉语言模型展示了令人印象深刻的多模态生成能力。然而，通常它们需要在大规模数据集上训练庞大的模型。作为一种更可扩展的替代方案，我们介绍了Prismer，一种数据和参数高效的视觉语言模型，它利用了一组领域专家的集合。Prismer只需要训练少量组件，大部分网络权重从现成的预训练领域专家中继承，并在训练期间保持冻结状态。通过利用来自各种领域的专家，我们展示了Prismer可以有效地汇集这些专家知识并将其适应于各种视觉语言推理任务。在我们的实验中，我们展示了Prismer实现了与当前最先进模型竞争的微调和少样本学习性能，同时需要少至两个数量级的训练数据。代码可在https://github.com/NVlabs/prismer获得。

    Recent vision-language models have shown impressive multi-modal generation capabilities. However, typically they require training huge models on massive datasets. As a more scalable alternative, we introduce Prismer, a data- and parameter-efficient vision-language model that leverages an ensemble of domain experts. Prismer only requires training of a small number of components, with the majority of network weights inherited from readily-available, pre-trained domain experts, and kept frozen during training. By leveraging experts from a wide range of domains, we show that Prismer can efficiently pool this expert knowledge and adapt it to various vision-language reasoning tasks. In our experiments, we show that Prismer achieves fine-tuned and few-shot learning performance which is competitive with current state-of-the-art models, whilst requiring up to two orders of magnitude less training data. Code is available at https://github.com/NVlabs/prismer.
    
[^120]: MCWDST: 一种用于社交媒体实时虚假新闻缓解的最小成本加权有向生成树算法

    MCWDST: a Minimum-Cost Weighted Directed Spanning Tree Algorithm for Real-Time Fake News Mitigation in Social Media. (arXiv:2302.12190v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2302.12190](http://arxiv.org/abs/2302.12190)

    本论文提出了一种用于社交媒体实时虚假新闻缓解的算法，通过使用新的深度学习架构来检测假新闻并采取实时的网络感知策略来减少其传播。

    

    互联网的广泛普及和手持设备的可用性使得社交媒体具有类似报纸的影响力。人们可以在社交媒体上即时获取廉价的信息。然而，这种便利性也带来了危险，任何用户都可以自由发布任何内容，而不论其真实性。因此，需要检测虚假信息，也称为假新闻。本文提出了一个端到端的解决方案，可以准确地检测假新闻，并在实时中免于传播它们的网络节点上进行免疫。为了检测假新闻，我们提出了两种新的堆栈深度学习架构，利用卷积和双向LSTM层。为了缓解假新闻的传播，我们提出了一种实时网络感知策略，它（1）为检测到的节点构建了一个最小成本加权的有向生成树，并且（2）通过使用一种新的分数化危险性方法来免疫该树中的节点。

    The widespread availability of internet access and handheld devices confers to social media a power similar to the one newspapers used to have. People seek affordable information on social media and can reach it within seconds. Yet this convenience comes with dangers; any user may freely post whatever they please and the content can stay online for a long period, regardless of its truthfulness. A need to detect untruthful information, also known as fake news, arises. In this paper, we present an end-to-end solution that accurately detects fake news and immunizes network nodes that spread them in real-time. To detect fake news, we propose two new stack deep learning architectures that utilize convolutional and bidirectional LSTM layers. To mitigate the spread of fake news, we propose a real-time network-aware strategy that (1) constructs a minimum-cost weighted directed spanning tree for a detected node, and (2) immunizes nodes in that tree by scoring their harmfulness using a novel ran
    
[^121]: IM-IAD：工业制造中的图像异常检测基准

    IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing. (arXiv:2301.13359v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.13359](http://arxiv.org/abs/2301.13359)

    该论文提出了一个IM-IAD工业制造中的图像异常检测基准，通过统一的设置评估了16个算法在7个数据集上的性能，旨在解决目前这一领域研究的不规范问题。

    

    图像异常检测（IAD）是工业制造中一项新兴且重要的计算机视觉任务。近年来，许多先进的算法已经被发布，但它们的性能存在很大差异。我们认为，缺乏实际的工业制造设置很可能阻碍了这些方法在实际应用中的发展和使用。据我们所知，IAD方法尚未经过系统评估。因此，这使得研究人员很难分析它们，因为它们是为不同或特殊情况而设计的。为了解决这个问题，我们首先提出了一个统一的工业制造设置来评估这些算法的性能，包括几个方面，如各种监督级别（无监督vs半监督）、少样本学习、持续学习、噪声标签、内存使用和推断速度。此外，我们巧妙地构建了一个完整的图像异常检测基准（IM-IAD），该基准在统一的设置下包括了16个算法和7个主流数据集。

    Image anomaly detection (IAD) is an emerging and vital computer vision task in industrial manufacturing (IM). Recently many advanced algorithms have been published, but their performance deviates greatly. We realize that the lack of actual IM settings most probably hinders the development and usage of these methods in real-world applications. As far as we know, IAD methods are not evaluated systematically. As a result, this makes it difficult for researchers to analyze them because they are designed for different or special cases. To solve this problem, we first propose a uniform IM setting to assess how well these algorithms perform, which includes several aspects, i.e., various levels of supervision (unsupervised vs. semi-supervised), few-shot learning, continual learning, noisy labels, memory usage, and inference speed. Moreover, we skillfully build a comprehensive image anomaly detection benchmark (IM-IAD) that includes 16 algorithms on 7 mainstream datasets with uniform settings. 
    
[^122]: 关于基于相机的三维物体检测的对抗性鲁棒性研究

    On the Adversarial Robustness of Camera-based 3D Object Detection. (arXiv:2301.10766v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.10766](http://arxiv.org/abs/2301.10766)

    介绍了基于相机的三维物体检测对抗性鲁棒性的全面调查，发现鸟瞰图表示对定位攻击具有更强的鲁棒性，且不需要深度估计的方法具有更好的对抗性。

    

    近年来，基于相机的三维物体检测因其在低计算成本下实现高性能的能力而受到广泛关注。然而，这些方法在面对针对安全关键领域，如自动驾驶的部署时，其对抗攻击的鲁棒性并未得到充分的研究。本研究首次对领先的基于相机的三维物体检测方法在各种对抗条件下的鲁棒性进行了全面调查。我们系统地分析了这些模型在白盒和黑盒两个攻击设置下的弹性，重点关注分类和定位这两个主要目标。另外，我们深入探讨了基于像素和基于补丁两种对抗攻击技术。我们的实验得出了四个有趣的发现：(a)鸟瞰图表示对定位攻击具有更强的鲁棒性；(b)不需要深度估计的方法具有更好的对抗性。

    In recent years, camera-based 3D object detection has gained widespread attention for its ability to achieve high performance with low computational cost. However, the robustness of these methods to adversarial attacks has not been thoroughly examined, especially when considering their deployment in safety-critical domains like autonomous driving. In this study, we conduct the first comprehensive investigation of the robustness of leading camera-based 3D object detection approaches under various adversarial conditions. We systematically analyze the resilience of these models under two attack settings: white-box and black-box; focusing on two primary objectives: classification and localization. Additionally, we delve into two types of adversarial attack techniques: pixel-based and patch-based. Our experiments yield four interesting findings: (a) bird's-eye-view-based representations exhibit stronger robustness against localization attacks; (b) depth-estimation-free approaches have the p
    
[^123]: 通过控制无关句子的混淆效应，提高抽象摘要的准确性

    Improving Faithfulness of Abstractive Summarization by Controlling Confounding Effect of Irrelevant Sentences. (arXiv:2212.09726v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09726](http://arxiv.org/abs/2212.09726)

    通过控制无关句子的混淆效应，本文提出了一种改进抽象摘要准确性的方法，并在AnswerSumm数据集上实现了20\%的准确性提升。

    

    尽管最先进的摘要系统在生成看似流利的摘要方面取得了令人印象深刻的进展，但缺乏事实正确性仍然是一个问题。本文表明，无关的输入文本部分可能导致事实不一致，作为混淆因素。为此，我们利用因果效应的信息理论度量来量化混淆的程度，并准确衡量其对摘要性能的影响。基于从理论结果中得出的见解，当有人工注释的相关句子可用时，我们设计了一个简单的多任务模型来控制这种混淆。关键是，我们对数据分布进行了原则性的刻画，从而确保了使用人工注释的相关句子来生成准确的摘要。我们的方法在AnswerSumm上（参考文献：fabbri2021answersumm）上强基准的基础上将准确性得分提高了20\%。

    Lack of factual correctness is an issue that still plagues state-of-the-art summarization systems despite their impressive progress on generating seemingly fluent summaries. In this paper, we show that factual inconsistency can be caused by irrelevant parts of the input text, which act as confounders. To that end, we leverage information-theoretic measures of causal effects to quantify the amount of confounding and precisely quantify how they affect the summarization performance. Based on insights derived from our theoretical results, we design a simple multi-task model to control such confounding by leveraging human-annotated relevant sentences when available. Crucially, we give a principled characterization of data distributions where such confounding can be large thereby necessitating the use of human annotated relevant sentences to generate factual summaries. Our approach improves faithfulness scores by 20\% over strong baselines on AnswerSumm \citep{fabbri2021answersumm}, a conver
    
[^124]: 用于解决生成对抗网络中的模式崩溃问题的分布拟合方法

    Distribution Fitting for Combating Mode Collapse in Generative Adversarial Networks. (arXiv:2212.01521v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01521](http://arxiv.org/abs/2212.01521)

    本文提出了一种用于解决生成对抗网络中模式崩溃问题的分布拟合方法。通过全局分布拟合和局部分布拟合，可以限制生成的数据分布，从而提高生成对抗网络的性能。

    

    模式崩溃是生成对抗网络中一个重要的未解决问题。本文从一种新的角度考察了模式崩溃的原因。由于训练过程中的非均匀采样，一些子分布可能被错过。因此，即使生成的分布与真实分布不同，GAN目标仍然可以达到最小值。为了解决这个问题，我们提出了一种带有惩罚项的全局分布拟合（GDF）方法来限制生成的数据分布。当生成的分布与真实分布不同时，GDF将使目标变得更难达到最小值，而原始的全局最小值不变。针对无法获得整体真实数据的情况，我们还提出了一种局部分布拟合（LDF）方法。在几个基准测试上的实验证明了GDF和LDF的有效性和竞争性能。

    Mode collapse is a significant unsolved issue of generative adversarial networks. In this work, we examine the causes of mode collapse from a novel perspective. Due to the nonuniform sampling in the training process, some sub-distributions may be missed when sampling data. As a result, even when the generated distribution differs from the real one, the GAN objective can still achieve the minimum. To address the issue, we propose a global distribution fitting (GDF) method with a penalty term to confine the generated data distribution. When the generated distribution differs from the real one, GDF will make the objective harder to reach the minimal value, while the original global minimum is not changed. To deal with the circumstance when the overall real data is unreachable, we also propose a local distribution fitting (LDF) method. Experiments on several benchmarks demonstrate the effectiveness and competitive performance of GDF and LDF.
    
[^125]: 建模师：在想象中学习和适应技能

    Choreographer: Learning and Adapting Skills in Imagination. (arXiv:2211.13350v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.13350](http://arxiv.org/abs/2211.13350)

    本论文提出了一种建模师代理，利用世界模型在想象中学习并适应技能。该方法通过解耦探索和技能学习过程，能够在模型的潜在状态空间中发现技能，并通过元控制器进行高效的适应。建模师能够从离线数据和同时收集数据中学习技能。

    

    无监督技能学习旨在在没有外部监督的情况下学习丰富的行为库，使人工智能代理能够控制和影响环境。然而，如果缺乏适当的知识和探索，技能可能只能控制环境的有限区域，限制其适用性。此外，如何利用学得的技能行为以数据高效的方式适应后续任务还不清楚。我们提出了建模师，一种基于模型的代理，利用其世界模型在想象中学习和适应技能。我们的方法将探索和技能学习过程解耦，能够在模型的潜在状态空间中发现技能。在适应过程中，代理使用元控制器来评估和调整通过在想象中并行部署已学得的技能，以高效地适应它们。建模师能够从离线数据和同时收集数据中学习技能。

    Unsupervised skill learning aims to learn a rich repertoire of behaviors without external supervision, providing artificial agents with the ability to control and influence the environment. However, without appropriate knowledge and exploration, skills may provide control only over a restricted area of the environment, limiting their applicability. Furthermore, it is unclear how to leverage the learned skill behaviors for adapting to downstream tasks in a data-efficient manner. We present Choreographer, a model-based agent that exploits its world model to learn and adapt skills in imagination. Our method decouples the exploration and skill learning processes, being able to discover skills in the latent state space of the model. During adaptation, the agent uses a meta-controller to evaluate and adapt the learned skills efficiently by deploying them in parallel in imagination. Choreographer is able to learn skills both from offline data, and by collecting data simultaneously with an exp
    
[^126]: 使用动态线性投影方法探索非线性模型的局部解释

    Exploring Local Explanations of Nonlinear Models Using Animated Linear Projections. (arXiv:2205.05359v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.05359](http://arxiv.org/abs/2205.05359)

    本文介绍了一种使用动态线性投影方法来分析流行的非线性模型的局部解释的方法，探索预测变量之间的交互如何影响变量重要性估计，这对于理解模型的可解释性非常有用。

    

    机器学习模型的预测能力日益增强，但与参数统计模型相比，其复杂性和可解释性下降。这种折衷导致了可解释的人工智能（XAI）的出现，提供了诸如局部解释（LE）和局部变量归因（LVA）之类的方法，以揭示模型如何使用预测变量进行预测。然而，LVA通常不能有效处理预测变量之间的关联。为了理解预测变量之间的交互如何影响变量重要性估计，可以将LVA转换为线性投影，并使用径向游览。这对于学习模型如何犯错，或异常值的影响，或观测值的聚类也非常有用。本文使用各种流行的非线性模型（包括随机森林和神经网络）的示例来说明这种方法。

    The increased predictive power of machine learning models comes at the cost of increased complexity and loss of interpretability, particularly in comparison to parametric statistical models. This trade-off has led to the emergence of eXplainable AI (XAI) which provides methods, such as local explanations (LEs) and local variable attributions (LVAs), to shed light on how a model use predictors to arrive at a prediction. These provide a point estimate of the linear variable importance in the vicinity of a single observation. However, LVAs tend not to effectively handle association between predictors. To understand how the interaction between predictors affects the variable importance estimate, we can convert LVAs into linear projections and use the radial tour. This is also useful for learning how a model has made a mistake, or the effect of outliers, or the clustering of observations. The approach is illustrated with examples from categorical (penguin species, chocolate types) and quant
    
[^127]: 几乎零样本质量多样性优化

    Few-shot Quality-Diversity Optimization. (arXiv:2109.06826v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.06826](http://arxiv.org/abs/2109.06826)

    本文提出了一种几乎零样本的质量多样性优化方法，通过利用参数空间中的优化路径信息构建先验种群，可以在未见环境中进行少样本适应。

    

    在过去的几年中，已经有大量的研究致力于利用以前的学习经验和设计少样本学习方法和元学习方法，涉及的问题领域从计算机视觉到基于强化学习的控制。一个明显的例外是质量多样性(QD)优化，我们在这个方向上几乎没有做出努力。QD方法已经被证明是处理强化学习中的欺骗性极小值和稀疏奖励的有效工具。然而，由于它们依赖于本质上具有低样本效率的进化过程，它们仍然很昂贵。我们表明，通过利用参数空间中优化路径的信息，可以构建先验种群，当在未见环境中使用该种群初始化QD方法时，可以进行零样本适应。我们提出的方法不需要反向传播，实现起来简单。

    In the past few years, a considerable amount of research has been dedicated to the exploitation of previous learning experiences and the design of Few-shot and Meta Learning approaches, in problem domains ranging from Computer Vision to Reinforcement Learning based control. A notable exception, where to the best of our knowledge, little to no effort has been made in this direction is Quality-Diversity (QD) optimization. QD methods have been shown to be effective tools in dealing with deceptive minima and sparse rewards in Reinforcement Learning. However, they remain costly due to their reliance on inherently sample inefficient evolutionary processes. We show that, given examples from a task distribution, information about the paths taken by optimization in parameter space can be leveraged to build a prior population, which when used to initialize QD methods in unseen environments, allows for few-shot adaptation. Our proposed method does not require backpropagation. It is simple to impl
    
[^128]: 高效注意力：具有线性复杂度的注意力机制

    Efficient Attention: Attention with Linear Complexities. (arXiv:1812.01243v10 [cs.CV] UPDATED)

    [http://arxiv.org/abs/1812.01243](http://arxiv.org/abs/1812.01243)

    本文提出了一种高效注意力机制，可以在大幅减少内存和计算成本的情况下实现与传统点积注意力等效的效果。这种高效机制的应用使得注意力模块可以更广泛地集成到网络中，从而提高准确性，并且在物体检测和实例分割等任务中取得了显著的性能提升。

    

    点积注意力在计算机视觉和自然语言处理中有广泛应用。然而，它的内存和计算成本随着输入大小的增加呈二次增长。这种增长限制了其在高分辨率输入上的应用。为解决这个缺点，本文提出了一种新颖的高效注意力机制，它与点积注意力等效，但内存和计算成本大大降低。其资源效率使得注意力模块能更广泛、灵活地集成到网络中，从而提高准确性。经验证明了其优势的有效性。高效注意力模块显著提升了对MS-COCO 2017上的物体检测器和实例分割器的性能。此外，资源效率使得复杂模型能够使用注意力，而高成本限制了使用点积注意力。以立体视觉为例，一个具有高效注意力的模型实现了最先进的准确性。

    Dot-product attention has wide applications in computer vision and natural language processing. However, its memory and computational costs grow quadratically with the input size. Such growth prohibits its application on high-resolution inputs. To remedy this drawback, this paper proposes a novel efficient attention mechanism equivalent to dot-product attention but with substantially less memory and computational costs. Its resource efficiency allows more widespread and flexible integration of attention modules into a network, which leads to better accuracies. Empirical evaluations demonstrated the effectiveness of its advantages. Efficient attention modules brought significant performance boosts to object detectors and instance segmenters on MS-COCO 2017. Further, the resource efficiency democratizes attention to complex models, where high costs prohibit the use of dot-product attention. As an exemplar, a model with efficient attention achieved state-of-the-art accuracies for stereo d
    

