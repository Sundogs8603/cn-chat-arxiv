# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Measuring Social Norms of Large Language Models](https://arxiv.org/abs/2404.02491) | 论文提出了一个挑战，测试大型语言模型对社会规范的理解，构建了一个数据集涵盖广泛的社会规范问题，通过多代理框架基于大型语言模型来提高模型对社会规范的理解能力。 |
| [^2] | [NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields](https://arxiv.org/abs/2404.01300) | 通过使用Masked AutoEncoders，本文提出了NeRF-MAE用于自监督三维表示学习，利用标准的三维Vision Transformers适应NeRF的独特公式，将NeRF的体积网格作为密集输入，以产生有效的三维表示。 |
| [^3] | [AID: Attention Interpolation of Text-to-Image Diffusion](https://arxiv.org/abs/2403.17924) | 提出了一种新颖的无训练技术，即Attention Interpolation via Diffusion (AID)，通过内/外插值注意力层、插值注意力与自注意力融合以提高保真度，以及应用贝塔分布进行选择以增加平滑度来改进文本到图像插值的问题。 |
| [^4] | [Sphere Neural-Networks for Rational Reasoning](https://arxiv.org/abs/2403.15297) | 该论文提出了一种球形神经网络（SphNNs）来进行理性推理，通过将计算构建块从向量推广到球体，实现了人类类似的推理能力，并开发了用于三段论推理的SphNN。 |
| [^5] | [A Clustering Method with Graph Maximum Decoding Information](https://arxiv.org/abs/2403.13846) | CMDI聚类方法创新性地将二维结构信息理论融入聚类过程中，弥补了基于图的模型聚类方法中忽略的随机游走访问节点和数据中嵌入的结构信息的不确定性。 |
| [^6] | [Fast Benchmarking of Asynchronous Multi-Fidelity Optimization on Zero-Cost Benchmarks](https://arxiv.org/abs/2403.01888) | 通过引入用户友好的Python软件包，研究提出了有效的并行HPO方法，避免长时间等待实现快速评估。 |
| [^7] | [Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey](https://arxiv.org/abs/2403.01255) | 先进的深度学习技术如深度迁移学习、联邦学习和强化学习解决了自动语音识别中训练数据领域假设不符合实际情况的问题，提高了性能并降低计算成本。 |
| [^8] | [Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models](https://arxiv.org/abs/2402.17177) | Sora是一种文本到视频生成的人工智能模型，展示出在模拟物理世界方面的潜力，具有广泛的应用前景和挑战，未来发展具有重要意义。 |
| [^9] | [Chimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all Tokens](https://arxiv.org/abs/2402.15758) | 提出了Chimera框架，用于加速大型语言模型推理，通过引入轻量级的草稿模型和两种策略，利用先前生成的令牌来预测后续单词，以解决解码过程中的准确性和效率问题 |
| [^10] | [SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization](https://arxiv.org/abs/2402.13919) | 该研究提出了一种创新流程，利用GPT-3.5和GPT-4生成高质量反馈，以增强临床笔记摘要中的事实一致性，弥补了专家注释数据的高成本和有限可用性问题。 |
| [^11] | [Overconfident and Unconfident AI Hinder Human-AI Collaboration](https://arxiv.org/abs/2402.07632) | 人工智能的过度自信和缺乏自信会阻碍人机协作，披露信心水平和提供反馈有助于认识到人工智能的信心不一致，但参与者往往因此不信任人工智能的建议，导致协作结果较差。 |
| [^12] | [GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding](https://arxiv.org/abs/2402.06764) | 该论文提出了GLaM方法，通过邻域划分和生成子图编码，对领域知识图进行大型语言模型的微调。该方法的创新之处在于能够实现对实际应用中的多步推理，并减少虚构。 |
| [^13] | [ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling](https://arxiv.org/abs/2402.06118) | ViGoR通过细粒度奖励建模提高了大型视觉语言模型在视觉对接方面的性能，通过人工评估和自动化方法有效地解决了视觉对接中的误差问题。 |
| [^14] | [Multi-Level Feature Aggregation and Recursive Alignment Network for Real-Time Semantic Segmentation](https://arxiv.org/abs/2402.02286) | 该论文提出了一种在实时推理速度下实现高分割准确性的多级特征聚合和递归对齐网络。使用ResNet-18作为骨干，通过多级特征聚合模块和递归对齐模块来提高模型性能。 |
| [^15] | [Explaining latent representations of generative models with large multimodal models](https://arxiv.org/abs/2402.01858) | 该研究提出了一个框架，利用大型多模态模型全面解释生成模型中的每个潜在因素。对于不同生成模型的解释，我们量化了解释的不确定性，通过多模态模型进行了性能评估，且定性地展示了潜在因素的变化效果。最后，我们讨论了最先进的大型多模态模型的解释能力和局限性。 |
| [^16] | [Exploring the Limitations of Graph Reasoning in Large Language Models](https://arxiv.org/abs/2402.01805) | 本文测试了5种不同的大型语言模型在图推理问题上的推理深度，并发现了LLMs的局限性、偏见和属性。我们发现LLMs对于节点遍历自由度的平均度数呈反向关系，k-shot提示对图推理任务有负面影响，并且LLMs存在积极的回应偏差，无法识别有效解的缺失。我们还提出了一种新的图推理提示技术。 |
| [^17] | [Graph Edits for Counterfactual Explanations: A comparative study](https://arxiv.org/abs/2401.11609) | 研究通过比较监督和无监督的图神经网络方法，扩展了图编辑作为反事实解释的先前努力，探讨了将输入数据表示为图形对于生成黑箱图像分类器最小且有意义的反事实解释的性能和时间效率最佳的方法。 |
| [^18] | [Online Advertisements with LLMs: Opportunities and Challenges](https://arxiv.org/abs/2311.07601) | 本文探讨了在线广告系统中利用大型语言模型（LLM）的潜力，提出了一个LLM广告的通用框架，并讨论了基于LLM的动态创意优化的前景。 |
| [^19] | [SelectLLM: Can LLMs Select Important Instructions to Annotate?.](http://arxiv.org/abs/2401.16553) | 这项工作提出了一种名为SelectLLM的新方法，利用LLMs选择高质量指令。通过提示LLMs估计每个无标签指令的有用性和影响力，并使用聚类算法将指令分为多个聚类。 |
| [^20] | [Methods and strategies for improving the novel view synthesis quality of neural radiation field.](http://arxiv.org/abs/2401.12451) | 这项研究总结了神经辐射场技术在提高图像渲染质量方面的各种方法和策略，为研究人员提供了当前状态和未来演化方向的理解，从而推动NeRF技术在相关领域的应用。 |
| [^21] | [A decoder-only foundation model for time-series forecasting.](http://arxiv.org/abs/2310.10688) | 本论文介绍了一种基于补丁解码器式注意力模型的时间序列预测基础模型，该模型在零样本情况下在各种公共数据集上的性能接近最先进的监督预测模型。 |
| [^22] | [Can We Edit Multimodal Large Language Models?.](http://arxiv.org/abs/2310.08475) | 本文提出了编辑多模式大型语言模型（MLLMs）的挑战，并构建了一个新的基准用于评估和比较不同编辑方法的效果。实验结果表明，编辑多模式LLMs仍然存在困难，但这项工作为NLP社区提供了宝贵的见解。 |
| [^23] | [Adaptive Hybrid Model for Enhanced Stock Market Predictions Using Improved VMD and Stacked Informer.](http://arxiv.org/abs/2310.01884) | 本研究提出了一种自适应混合模型，通过改进的VMD、特征工程和堆叠Informer，结合自适应损失函数，成功应用于股市预测。实验证明该模型在预测准确性、响应性和泛化能力方面优于传统和其他混合模型，对于小企业和特征工程的预测建模有潜在的优化方向和未来发展方向。 |
| [^24] | [SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning via Salient Channels.](http://arxiv.org/abs/2309.08513) | 本文提出了一种名为“显著通道微调”的简单而有效的方法，通过选择特征图中的部分通道进行微调，实现在低数据资源场景下低参数成本的高效微调，并在多个下游任务中优于全面微调方法。 |
| [^25] | [Confident Feature Ranking.](http://arxiv.org/abs/2307.15361) | 提出了一种确定性特征排序的方法，该方法通过特征重要性值的两两比较，可以产生排序和同时的置信区间，并且可以选择前k个集合。 |
| [^26] | [Integrated Sensing-Communication-Computation for Edge Artificial Intelligence.](http://arxiv.org/abs/2306.01162) | 面向边缘人工智能的集成感知-通信-计算技术对于提高资源利用率以及实现边缘学习和边缘人工智能推理任务的定制目标至关重要。 |
| [^27] | [Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement.](http://arxiv.org/abs/2305.14497) | Self-Polish是一种方法，通过促使模型逐步完善给定的问题以使其更易理解和可解决，以增强大型语言模型的推理能力。 |
| [^28] | [InstructIE: A Chinese Instruction-based Information Extraction Dataset.](http://arxiv.org/abs/2305.11527) | 介绍了一份中文的基于指令的信息提取数据集InstructIE，其中包括了270,000个弱监督的数据和1,000个高质量注释实例。实验结果表明当前的模型表现有待改进，该任务仍存在挑战。 |
| [^29] | [RAMiT: Reciprocal Attention Mixing Transformer for Lightweight Image Restoration.](http://arxiv.org/abs/2305.11474) | 本文提出了轻量级图像恢复的互惠式注意力混合Transformer（RAMiT）。通过使用双向注意力以及一种新的数据增强类型——强度掩码，有效地提高了恢复效果，同时大大减少了参数数量。 |
| [^30] | [A Survey on the Densest Subgraph Problem and its Variants.](http://arxiv.org/abs/2303.14467) | 本调查全面介绍了稠密子图问题及其变种，在讨论最新结果的同时，还提出了应用广泛的开放性问题。 |
| [^31] | [A Hybrid ANN-SNN Architecture for Low-Power and Low-Latency Visual Perception.](http://arxiv.org/abs/2303.14176) | 本文提出了一种混合ANN-SNN体系结构，通过用低速率的辅助ANN初始化状态，解决了SNN与经典ANN在准确性、延迟、功耗方面的平衡性问题，实现了高时间分辨率、低延迟和低功耗的预测。 |
| [^32] | [Curvature-Balanced Feature Manifold Learning for Long-Tailed Classification.](http://arxiv.org/abs/2303.12307) | 本文提出了一种曲率平衡特征流形学习的方法，探究了感知流形的几何特性对分类难度的影响，发现曲率不平衡会导致模型不公平。 |
| [^33] | [COMET: X86 Cost Model Explanation Framework.](http://arxiv.org/abs/2302.06836) | 本文提出了一个用于生成x86成本模型解释的框架COMET，通过提供解释来提高机器学习成本模型的可解释性和可推广性。研究显示该框架所提供的语义信息与成本预测误差呈负相关。 |

# 详细

[^1]: 测量大型语言模型的社会规范

    Measuring Social Norms of Large Language Models

    [https://arxiv.org/abs/2404.02491](https://arxiv.org/abs/2404.02491)

    论文提出了一个挑战，测试大型语言模型对社会规范的理解，构建了一个数据集涵盖广泛的社会规范问题，通过多代理框架基于大型语言模型来提高模型对社会规范的理解能力。

    

    我们提出了一个新的挑战，以检验大型语言模型是否理解社会规范。与现有数据集不同，我们的数据集要求具有解决社会规范的基本理解。我们的数据集包含最大的社会规范技能集，包括402项技能和12,383个问题，涵盖了从观点和论点到文化和法律等广泛的社会规范。我们根据K-12课程设计了我们的数据集。这使得可以直接将大型语言模型的社会理解能力与人类进行比较，更具体地说是与小学生进行比较。尽管先前的工作在我们的基准测试中产生几乎随机的准确度，但最近的大型语言模型（如GPT3.5-Turbo和LLaMA2-Chat）能够显著提高性能，仅略低于人类性能。然后，我们提出了一个基于大型语言模型的多代理框架，以提高模型理解社会规范的能力。

    arXiv:2404.02491v1 Announce Type: cross  Abstract: We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms.
    
[^2]: NeRF-MAE: 自监督三维表示学习中的Masked AutoEncoders

    NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields

    [https://arxiv.org/abs/2404.01300](https://arxiv.org/abs/2404.01300)

    通过使用Masked AutoEncoders，本文提出了NeRF-MAE用于自监督三维表示学习，利用标准的三维Vision Transformers适应NeRF的独特公式，将NeRF的体积网格作为密集输入，以产生有效的三维表示。

    

    由于神经场在计算机视觉和机器人领域的卓越能力，能够理解三维视觉世界，如推断语义、几何和动态等，本文探讨了神经场在从二维图像中密集表示三维场景的自监督预训练，具体使用Masked AutoEncoders的可能性。我们借鉴了将transformers扩展到新数据模态的令人惊讶的成功，利用标准的三维Vision Transformers来适应NeRF的独特公式。我们将NeRF的体积网格作为transformer的密集输入，与其他三维表示（如点云）进行对比，其信息密度可能不均匀，而表示是不规则的。由于将masked autoencoders应用于类似NeRF这样的隐式表示的困难，我们选择提取一个显式的表示。

    arXiv:2404.01300v1 Announce Type: cross  Abstract: Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we ask the question: Can we scale their self-supervised pretraining, specifically using masked autoencoders, to generate effective 3D representations from posed RGB images. Owing to the astounding success of extending transformers to novel data modalities, we employ standard 3D Vision Transformers to suit the unique formulation of NeRFs. We leverage NeRF's volumetric grid as a dense input to the transformer, contrasting it with other 3D representations such as pointclouds where the information density can be uneven, and the representation is irregular. Due to the difficulty of applying masked autoencoders to an implicit representation, such as NeRF, we opt for extracting an explicit repres
    
[^3]: AID: 文本到图像扩散的注重插值

    AID: Attention Interpolation of Text-to-Image Diffusion

    [https://arxiv.org/abs/2403.17924](https://arxiv.org/abs/2403.17924)

    提出了一种新颖的无训练技术，即Attention Interpolation via Diffusion (AID)，通过内/外插值注意力层、插值注意力与自注意力融合以提高保真度，以及应用贝塔分布进行选择以增加平滑度来改进文本到图像插值的问题。

    

    arXiv:2403.17924v1 公告类型: 跨领域 摘要: 有条件的扩散模型可以在各种设置中创建看不见的图像，有助于图像插值。潜在空间中的插值已经得到了深入研究，但是具有特定条件（如文本或姿势）的插值却了解不多。简单的方法，如在条件空间中的线性插值，通常会导致图像缺乏一致性、平滑度和保真度。为此，我们引入一个名为Attention Interpolation via Diffusion (AID)的新颖无训练技术。我们的主要贡献包括1）提出了一个内/外插值注意力层；2）将插值注意力与自注意力融合以提高保真度；3）应用贝塔分布进行选择以增加平滑度。我们还提出了一种变体，Prompt-guided Attention Interpolation via Diffusion (PAID)，它将插值视为依赖于条件的生成过程。这种方法可以创建出更具创造性的新图像。

    arXiv:2403.17924v1 Announce Type: cross  Abstract: Conditional diffusion models can create unseen images in various settings, aiding image interpolation. Interpolation in latent spaces is well-studied, but interpolation with specific conditions like text or poses is less understood. Simple approaches, such as linear interpolation in the space of conditions, often result in images that lack consistency, smoothness, and fidelity. To that end, we introduce a novel training-free technique named Attention Interpolation via Diffusion (AID). Our key contributions include 1) proposing an inner/outer interpolated attention layer; 2) fusing the interpolated attention with self-attention to boost fidelity; and 3) applying beta distribution to selection to increase smoothness. We also present a variant, Prompt-guided Attention Interpolation via Diffusion (PAID), that considers interpolation as a condition-dependent generative process. This method enables the creation of new images with greater con
    
[^4]: 用于理性推理的球形神经网络

    Sphere Neural-Networks for Rational Reasoning

    [https://arxiv.org/abs/2403.15297](https://arxiv.org/abs/2403.15297)

    该论文提出了一种球形神经网络（SphNNs）来进行理性推理，通过将计算构建块从向量推广到球体，实现了人类类似的推理能力，并开发了用于三段论推理的SphNN。

    

    大型语言模型（LLMs）如ChatGPT的成功得到了广泛的认可，其类人问题回答的能力以及不断提升的推理性能都证明了这一点。然而，LLMs是否会进行推理仍然不清楚。传统神经网络如何在定性上扩展以超越统计范式并实现高级认知是一个未解之谜。在这里，我们通过将计算构建块从向量推广到球体的方式提出了一种极简的定性扩展。我们提出了球形神经网络（SphNNs）用于通过模型构建和检查进行类人推理，并为三段论推理开发了SphNN，这是人类理性的缩影。SphNN不使用训练数据，而是使用邻域空间关系的神经符号转换映射来指导从当前球形配置向目标的转换。

    arXiv:2403.15297v1 Announce Type: new  Abstract: The success of Large Language Models (LLMs), e.g., ChatGPT, is witnessed by their planetary popularity, their capability of human-like question-answering, and also by their steadily improved reasoning performance. However, it remains unclear whether LLMs reason. It is an open problem how traditional neural networks can be qualitatively extended to go beyond the statistic paradigm and achieve high-level cognition. Here, we present a minimalist qualitative extension by generalising computational building blocks from vectors to spheres. We propose Sphere Neural Networks (SphNNs) for human-like reasoning through model construction and inspection, and develop SphNN for syllogistic reasoning, a microcosm of human rationality. Instead of training data, SphNN uses a neuro-symbolic transition map of neighbourhood spatial relations to guide transformations from the current sphere configuration towards the target. SphNN is the first neural model th
    
[^5]: 一种具有图最大解码信息的聚类方法

    A Clustering Method with Graph Maximum Decoding Information

    [https://arxiv.org/abs/2403.13846](https://arxiv.org/abs/2403.13846)

    CMDI聚类方法创新性地将二维结构信息理论融入聚类过程中，弥补了基于图的模型聚类方法中忽略的随机游走访问节点和数据中嵌入的结构信息的不确定性。

    

    基于图模型的聚类方法因其在各种知识领域中的广泛适用性而备受关注。其能够与其他相关应用无缝集成的适应性赋予了基于图模型的聚类分析能力，可以强大地从数据集中提取“自然关联”或“图结构”，有助于建模数据点之间的关系。尽管这种方法效果显著，但当前利用基于图的模型的聚类方法忽略了节点之间随机游走访问以及数据中嵌入的结构信息所带来的不确定性。为填补这一空白，我们提出了一种新颖的基于图的模型内最大化解码信息的聚类方法，命名为CMDI。CMDI创新地将二维结构信息理论纳入到聚类过程中，包括两个阶段：图结构提取和图顶点

    arXiv:2403.13846v1 Announce Type: cross  Abstract: The clustering method based on graph models has garnered increased attention for its widespread applicability across various knowledge domains. Its adaptability to integrate seamlessly with other relevant applications endows the graph model-based clustering analysis with the ability to robustly extract "natural associations" or "graph structures" within datasets, facilitating the modelling of relationships between data points. Despite its efficacy, the current clustering method utilizing the graph-based model overlooks the uncertainty associated with random walk access between nodes and the embedded structural information in the data. To address this gap, we present a novel Clustering method for Maximizing Decoding Information within graph-based models, named CMDI. CMDI innovatively incorporates two-dimensional structural information theory into the clustering process, consisting of two phases: graph structure extraction and graph vert
    
[^6]: 零成本基准上异步多保真度优化的快速基准测试

    Fast Benchmarking of Asynchronous Multi-Fidelity Optimization on Zero-Cost Benchmarks

    [https://arxiv.org/abs/2403.01888](https://arxiv.org/abs/2403.01888)

    通过引入用户友好的Python软件包，研究提出了有效的并行HPO方法，避免长时间等待实现快速评估。

    

    尽管深度学习取得了许多成功，但其结果往往取决于超参数的精心选择。然而，深度学习训练的耗时性使得超参数优化(HPO)是一项昂贵的工作，拖慢了高效HPO工具的开发。本工作通过引入一个用户友好的Python软件包，来解决这一挑战，促进零成本基准下高效的并行HPO。我们的方法根据存储在文件系统中的信息计算精确的返回顺序，消除了长时间的等待，实现了更快的HPO评估。

    arXiv:2403.01888v1 Announce Type: new  Abstract: While deep learning has celebrated many successes, its results often hinge on the meticulous selection of hyperparameters (HPs). However, the time-consuming nature of deep learning training makes HP optimization (HPO) a costly endeavor, slowing down the development of efficient HPO tools. While zero-cost benchmarks, which provide performance and runtime without actual training, offer a solution for non-parallel setups, they fall short in parallel setups as each worker must communicate its queried runtime to return its evaluation in the exact order. This work addresses this challenge by introducing a user-friendly Python package that facilitates efficient parallel HPO with zero-cost benchmarks. Our approach calculates the exact return order based on the information stored in file system, eliminating the need for long waiting times and enabling much faster HPO evaluations. We first verify the correctness of our approach through extensive t
    
[^7]: 使用先进的深度学习方法的自动语音识别：一项调查

    Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey

    [https://arxiv.org/abs/2403.01255](https://arxiv.org/abs/2403.01255)

    先进的深度学习技术如深度迁移学习、联邦学习和强化学习解决了自动语音识别中训练数据领域假设不符合实际情况的问题，提高了性能并降低计算成本。

    

    arXiv:2403.01255v1 公告类型：跨领域 摘要：深度学习（DL）的最新进展对自动语音识别（ASR）构成了重大挑战。ASR依赖于包括机密数据在内的大量训练数据，并需要大量的计算和存储资源。启用自适应系统可以提高动态环境下的ASR性能。DL技术假设训练和测试数据来自同一领域，但并非总是如此。深度迁移学习（DTL）、联邦学习（FL）和强化学习（RL）等先进的DL技术解决了这些问题。DTL可以利用小而相关的数据集构建高性能模型，FL使得在不拥有数据集的情况下训练模型，RL优化动态环境下的决策，降低计算成本。本调查全面审阅了基于DTL、FL和RL的ASR框架，旨在提供对最新发展的见解，并协助研究人员。

    arXiv:2403.01255v1 Announce Type: cross  Abstract: Recent advancements in deep learning (DL) have posed a significant challenge for automatic speech recognition (ASR). ASR relies on extensive training datasets, including confidential ones, and demands substantial computational and storage resources. Enabling adaptive systems improves ASR performance in dynamic environments. DL techniques assume training and testing data originate from the same domain, which is not always true. Advanced DL techniques like deep transfer learning (DTL), federated learning (FL), and reinforcement learning (RL) address these issues. DTL allows high-performance models using small yet related datasets, FL enables training on confidential data without dataset possession, and RL optimizes decision-making in dynamic environments, reducing computation costs. This survey offers a comprehensive review of DTL, FL, and RL-based ASR frameworks, aiming to provide insights into the latest developments and aid researcher
    
[^8]: Sora: 大型视觉模型背景、技术、局限性和机遇的综述

    Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models

    [https://arxiv.org/abs/2402.17177](https://arxiv.org/abs/2402.17177)

    Sora是一种文本到视频生成的人工智能模型，展示出在模拟物理世界方面的潜力，具有广泛的应用前景和挑战，未来发展具有重要意义。

    

    Sora是由OpenAI于2024年2月发布的一种文本到视频生成的人工智能模型。这个模型经过训练，可以根据文本指令生成逼真或想象的场景视频，并在模拟物理世界方面显示出潜力。本文基于公开的技术报告和逆向工程，对这个模型的背景、相关技术、应用、尚存的挑战以及文本到视频人工智能模型的未来方向进行了全面回顾。首先我们追溯了Sora的发展历程，并调查了用于构建这个"世界模拟器"的基础技术。然后，我们详细描述了Sora在从电影制作和教育到营销等多个行业中的应用和潜在影响。我们讨论了需要解决的主要挑战和局限性，以便广泛部署Sora，如确保安全和无偏见的视频生成。最后，我们讨论了Sora以及视频生成技术未来的发展。

    arXiv:2402.17177v1 Announce Type: cross  Abstract: Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this "world simulator". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video gene
    
[^9]: Chimera: 融合所有令牌的无损解码方法，加速大型语言模型推理

    Chimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all Tokens

    [https://arxiv.org/abs/2402.15758](https://arxiv.org/abs/2402.15758)

    提出了Chimera框架，用于加速大型语言模型推理，通过引入轻量级的草稿模型和两种策略，利用先前生成的令牌来预测后续单词，以解决解码过程中的准确性和效率问题

    

    大型语言模型（LLMs）在各种任务中展示了显著的能力。然而，它们的广泛应用被资源密集型的解码过程所阻碍。为了解决这一挑战，目前的方法已经合并了额外的解码头，以实现对多个后续令牌的并行预测，从而实现推理加速。然而，这些解码头的准确性远不及自回归解码方法。鉴于这些限制，我们提出了Chimera，这是一个专门为推测采样设计的新框架。在这个框架内，我们引入了一个轻量级的草稿模型，能够有效利用先前生成的令牌来预测后续单词。为了确保准确性和效率，我们在轻量级草稿模型中提出了两种策略。首先，我们专注于在底层捕获短程依赖性。其次，我们利用

    arXiv:2402.15758v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across various tasks. However, their widespread application is hindered by the resource-intensive decoding process. To address this challenge, current approaches have incorporated additional decoding heads to enable parallel prediction of multiple subsequent tokens, thereby achieving inference acceleration. Nevertheless, the accuracy of these decoding heads falls short of the auto-regressive decoding approach.   In light of these limitations, we propose Chimera, a novel framework specifically designed for speculative sampling. Within this framework, we introduce a lightweight draft model that effectively utilizes previously generated tokens to predict subsequent words. To ensure both accuracy and efficiency, we present two strategies within the lightweight draft model. Firstly, we focus on capturing short-range dependencies at the bottom layer. Secondly, we leverage
    
[^10]: SYNFAC-EDIT: 用于临床摘要中的事实对齐的合成模仿编辑反馈

    SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization

    [https://arxiv.org/abs/2402.13919](https://arxiv.org/abs/2402.13919)

    该研究提出了一种创新流程，利用GPT-3.5和GPT-4生成高质量反馈，以增强临床笔记摘要中的事实一致性，弥补了专家注释数据的高成本和有限可用性问题。

    

    大型语言模型（LLMs）如GPT和Llama在摘要任务上取得了重大进展，但在事实不准确方面存在困难，这是临床NLP应用中的关键问题，错误可能导致严重后果。为了解决事实对齐的专家注释数据成本高昂且有限的问题，本研究引入了一种创新的流程，利用GPT-3.5和GPT-4生成高质量反馈，旨在增强临床笔记摘要中的事实一致性。我们的研究主要关注编辑反馈，在没有额外注释的情况下，模拟了医疗专业人员改善AI系统输出的实际场景。尽管GPT在各种临床NLP任务中都表现出了专业水平，比如医学执照考试，但对其提供改善较弱LM或LLM生成质量的专业级编辑反馈的研究很少。

    arXiv:2402.13919v1 Announce Type: cross  Abstract: Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences. To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate high-quality feedback aimed at enhancing factual consistency in clinical note summarization. Our research primarily focuses on edit feedback, mirroring the practical scenario in which medical professionals refine AI system outputs without the need for additional annotations. Despite GPT's proven expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on its capacity to deliver expert-level edit feedback for improving weaker LMs or LLMs generation qualit
    
[^11]: 过于自信和缺乏自信的人工智能阻碍人机协作

    Overconfident and Unconfident AI Hinder Human-AI Collaboration

    [https://arxiv.org/abs/2402.07632](https://arxiv.org/abs/2402.07632)

    人工智能的过度自信和缺乏自信会阻碍人机协作，披露信心水平和提供反馈有助于认识到人工智能的信心不一致，但参与者往往因此不信任人工智能的建议，导致协作结果较差。

    

    随着人工智能的进步，人机协作在专业和日常场景中越来越普遍。在这种协作中，人工智能可以表达其对自己表现的信心水平，作为人类评估人工智能建议的重要指标。然而，人工智能可能表现出过度自信或缺乏自信，即其表达的信心高于或低于其实际表现，这可能导致人们错误地评估人工智能的建议。我们的研究调查了人工智能过度自信和缺乏自信对人类信任、接受人工智能建议和协作结果的影响。我们的研究发现，披露人工智能的信心水平和表现反馈有助于更好地认识人工智能信心不一致。然而，参与者往往会因为察觉到这种不一致而不信任人工智能的建议，导致拒绝人工智能的建议，并且在协作任务中表现更差。相反，没有这些提示的情况下，参与者更容易信任人工智能并接受其建议，从而在协作任务中表现更好。

    As artificial intelligence (AI) advances, human-AI collaboration has become increasingly prevalent across both professional and everyday settings. In such collaboration, AI can express its confidence level about its performance, serving as a crucial indicator for humans to evaluate AI's suggestions. However, AI may exhibit overconfidence or underconfidence--its expressed confidence is higher or lower than its actual performance--which may lead humans to mistakenly evaluate AI advice. Our study investigates the influences of AI's overconfidence and underconfidence on human trust, their acceptance of AI suggestions, and collaboration outcomes. Our study reveal that disclosing AI confidence levels and performance feedback facilitates better recognition of AI confidence misalignments. However, participants tend to withhold their trust as perceiving such misalignments, leading to a rejection of AI suggestions and subsequently poorer performance in collaborative tasks. Conversely, without su
    
[^12]: 通过邻域划分和生成子图编码对领域知识图对齐进行大型语言模型微调的GLaM研究

    GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding

    [https://arxiv.org/abs/2402.06764](https://arxiv.org/abs/2402.06764)

    该论文提出了GLaM方法，通过邻域划分和生成子图编码，对领域知识图进行大型语言模型的微调。该方法的创新之处在于能够实现对实际应用中的多步推理，并减少虚构。

    

    将大型语言模型（LLMs）与从特定领域数据派生的知识图集成，代表了朝着更强大和事实推理的重要进展。随着这些模型变得越来越强大，使它们能够在现实世界的知识图上进行多步推理，并尽量减少虚构是至关重要的。然而，大型语言模型在处理互连实体的领域专用图时，能力仍然有限。因此，有必要填补这一技术上的重要差距。

    Integrating large language models (LLMs) with knowledge graphs derived from domain-specific data represents an important advancement towards more powerful and factual reasoning. As these models grow more capable, it is crucial to enable them to perform multi-step inferences over real-world knowledge graphs while minimizing hallucination. While large language models excel at conversation and text generation, their ability to reason over domain-specialized graphs of interconnected entities remains limited. For example, can we query a LLM to identify the optimal contact in a professional network for a specific goal, based on relationships and attributes in a private database? The answer is no--such capabilities lie beyond current methods. However, this question underscores a critical technical gap that must be addressed. Many high-value applications in areas such as science, security, and e-commerce rely on proprietary knowledge graphs encoding unique structures, relationships, and logica
    
[^13]: ViGoR：通过细粒度奖励建模改进大规模视觉语言模型的视觉对接

    ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling

    [https://arxiv.org/abs/2402.06118](https://arxiv.org/abs/2402.06118)

    ViGoR通过细粒度奖励建模提高了大型视觉语言模型在视觉对接方面的性能，通过人工评估和自动化方法有效地解决了视觉对接中的误差问题。

    

    通过将自然语言理解、大语言模型的生成能力和广泛知识与图像感知相结合，最近的大规模视觉语言模型（LVLMs）在现实世界中展示了前所未有的推理能力。然而，生成的文本往往在视觉输入中存在不准确的对接，导致错误，如产生幻觉的不存在场景元素、遗漏重要的场景部分，以及推测对象之间的属性和关系时出现错误。为了解决这些问题，我们引入了一个新颖的框架ViGoR（通过细粒度奖励建模进行视觉对接），它利用细粒度奖励建模来显著提升基于预训练基线的LVLMs的视觉对接能力。这种改进通过使用比完全监督更便宜的人工评估和自动化方法高效实现。我们通过多个基准测试的多个指标展示了我们方法的有效性。

    By combining natural language understanding and the generation capabilities and breadth of knowledge of large language models with image perception, recent large vision language models (LVLMs) have shown unprecedented reasoning capabilities in the real world. However, the generated text often suffers from inaccurate grounding in the visual input, resulting in errors such as hallucinating nonexistent scene elements, missing significant parts of the scene, and inferring incorrect attributes and relationships between objects. To address these issues, we introduce a novel framework, ViGoR (Visual Grounding Through Fine-Grained Reward Modeling) that utilizes fine-grained reward modeling to significantly enhance the visual grounding of LVLMs over pre-trained baselines. This improvement is efficiently achieved using much cheaper human evaluations instead of full supervisions, as well as automated methods. We show the effectiveness of our approach through numerous metrics on several benchmarks
    
[^14]: 多级特征聚合和递归对齐网络用于实时语义分割

    Multi-Level Feature Aggregation and Recursive Alignment Network for Real-Time Semantic Segmentation

    [https://arxiv.org/abs/2402.02286](https://arxiv.org/abs/2402.02286)

    该论文提出了一种在实时推理速度下实现高分割准确性的多级特征聚合和递归对齐网络。使用ResNet-18作为骨干，通过多级特征聚合模块和递归对齐模块来提高模型性能。

    

    实时语义分割对于实际应用非常重要。然而，许多方法都着重于降低计算复杂性和模型大小，但同时牺牲了准确性。在一些场景下，如自主导航和驾驶员辅助系统，准确性和速度同样重要。为了解决这个问题，我们提出了一种新颖的多级特征聚合和递归对齐网络（MFARANet），旨在实现高分割准确性和实时推理速度。我们使用ResNet-18作为骨干来保证效率，并提出了三个核心组件来弥补浅骨干引起的模型容量减少。具体而言，我们首先设计多级特征聚合模块（MFAM），将编码器中的分层特征聚合到每个尺度，以便于后续的空间对齐和多尺度推理。然后，我们通过结合基于流的对齐来建立递归对齐模块（RAM）。

    Real-time semantic segmentation is a crucial research for real-world applications. However, many methods lay particular emphasis on reducing the computational complexity and model size, while largely sacrificing the accuracy. In some scenarios, such as autonomous navigation and driver assistance system, accuracy and speed are equally important. To tackle this problem, we propose a novel Multi-level Feature Aggregation and Recursive Alignment Network (MFARANet), aiming to achieve high segmentation accuracy at real-time inference speed. We employ ResNet-18 as the backbone to ensure efficiency, and propose three core components to compensate for the reduced model capacity due to the shallow backbone. Specifically, we first design Multi-level Feature Aggregation Module (MFAM) to aggregate the hierarchical features in the encoder to each scale to benefit subsequent spatial alignment and multi-scale inference. Then, we build Recursive Alignment Module (RAM) by combining the flow-based alignm
    
[^15]: 使用大型多模态模型解释生成模型的潜在表示

    Explaining latent representations of generative models with large multimodal models

    [https://arxiv.org/abs/2402.01858](https://arxiv.org/abs/2402.01858)

    该研究提出了一个框架，利用大型多模态模型全面解释生成模型中的每个潜在因素。对于不同生成模型的解释，我们量化了解释的不确定性，通过多模态模型进行了性能评估，且定性地展示了潜在因素的变化效果。最后，我们讨论了最先进的大型多模态模型的解释能力和局限性。

    

    学习可解释的数据生成潜在因素表示是人工智能发展的重要课题。随着大型多模态模型的兴起，它可以将图像与文本对齐以生成答案。在本研究中，我们提出了一个框架，利用大型多模态模型全面解释生成模型中的每个潜在因素。我们进一步量化评估了我们生成的解释的不确定性，在多个大型多模态模型之间评估了解释生成的性能，并定性地可视化了每个潜在因素的变化，以学习不同生成模型对解释的解缠效果。最后，我们讨论了最先进的大型多模态模型的解释能力和局限性。

    Learning interpretable representations of data generative latent factors is an important topic for the development of artificial intelligence. With the rise of the large multimodal model, it can align images with text to generate answers. In this work, we propose a framework to comprehensively explain each latent factor in the generative models using a large multimodal model. We further measure the uncertainty of our generated explanations, quantitatively evaluate the performance of explanation generation among multiple large multimodal models, and qualitatively visualize the variations of each latent factor to learn the disentanglement effects of different generative models on explanations. Finally, we discuss the explanatory capabilities and limitations of state-of-the-art large multimodal models.
    
[^16]: 探索大型语言模型中图推理的局限性

    Exploring the Limitations of Graph Reasoning in Large Language Models

    [https://arxiv.org/abs/2402.01805](https://arxiv.org/abs/2402.01805)

    本文测试了5种不同的大型语言模型在图推理问题上的推理深度，并发现了LLMs的局限性、偏见和属性。我们发现LLMs对于节点遍历自由度的平均度数呈反向关系，k-shot提示对图推理任务有负面影响，并且LLMs存在积极的回应偏差，无法识别有效解的缺失。我们还提出了一种新的图推理提示技术。

    

    预训练的大型语言模型仅通过基于语言的提示就展示了各种类型的推理能力。然而，在本文中，我们通过图推理问题测试了5种不同的大型语言模型（GPT-4，GPT-3.5，Claude-2，Llama-2和Palm-2）的推理深度。特别地，我们设计了10个不同的图遍历问题，每个问题代表着逐步增加的复杂性水平。此外，我们通过对不同图大小以及不同形式的k-shot提示的设置分析了模型的性能。通过这个基准测试过程，我们凸显了LLMs的各种局限性、偏见和属性，比如与每个节点的遍历自由度的平均度数呈反向关系，k-shot提示对图推理任务的整体负面影响，以及积极的回应偏差导致LLMs无法识别有效解的缺失。最后，我们提出一种新的提示技术，专门用于图推理。

    Pretrained Large Language Models have demonstrated various types of reasoning capabilities through language-based prompts alone. However, in this paper, we test the depth of graph reasoning for 5 different LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) through the problems of graph reasoning. In particular, we design 10 distinct problems of graph traversal, each representing increasing levels of complexity. Further, we analyze the performance of models across various settings such as varying sizes of graphs as well as different forms of k-shot prompting. We highlight various limitations, biases, and properties of LLMs through this benchmarking process, such as an inverse relation to the average degrees of freedom of traversal per node in graphs, the overall negative impact of k-shot prompting on graph reasoning tasks, and a positive response bias which prevents LLMs from identifying the absence of a valid solution. Finally, we propose a new prompting technique specially designed f
    
[^17]: 图编辑用于反事实解释：一项比较研究

    Graph Edits for Counterfactual Explanations: A comparative study

    [https://arxiv.org/abs/2401.11609](https://arxiv.org/abs/2401.11609)

    研究通过比较监督和无监督的图神经网络方法，扩展了图编辑作为反事实解释的先前努力，探讨了将输入数据表示为图形对于生成黑箱图像分类器最小且有意义的反事实解释的性能和时间效率最佳的方法。

    

    反事实已被确立为一种流行的可解释性技术，利用一组最小的编辑来改变分类器的预测。在考虑图像上的概念反事实时，请求的编辑应对应输入数据中存在的显著概念。同时，概念距离由知识图谱定义，确保概念编辑的最优性。在这项工作中，我们通过进行比较研究扩展了以图编辑为反事实解释的先前努力，在这项研究中涵盖了监督和无监督的图神经网络（GNN）方法。到此为止，我们提出了以下重要的研究问题：我们应该将输入数据表示为图形，这是生成黑箱图像分类器的最小和有意义的反事实解释在性能和时间效率方面最佳的GNN方法吗？

    arXiv:2401.11609v2 Announce Type: replace-cross  Abstract: Counterfactuals have been established as a popular explainability technique which leverages a set of minimal edits to alter the prediction of a classifier. When considering conceptual counterfactuals on images, the edits requested should correspond to salient concepts present in the input data. At the same time, conceptual distances are defined by knowledge graphs, ensuring the optimality of conceptual edits. In this work, we extend previous endeavors on graph edits as counterfactual explanations by conducting a comparative study which encompasses both supervised and unsupervised Graph Neural Network (GNN) approaches. To this end, we pose the following significant research question: should we represent input data as graphs, which is the optimal GNN approach in terms of performance and time efficiency to generate minimal and meaningful counterfactual explanations for black-box image classifiers?
    
[^18]: 在线广告与LLMs：机遇与挑战

    Online Advertisements with LLMs: Opportunities and Challenges

    [https://arxiv.org/abs/2311.07601](https://arxiv.org/abs/2311.07601)

    本文探讨了在线广告系统中利用大型语言模型（LLM）的潜力，提出了一个LLM广告的通用框架，并讨论了基于LLM的动态创意优化的前景。

    

    本文探讨了在在线广告系统中利用大型语言模型（LLM）的潜力。我们深入研究了这个系统必须满足的隐私、延迟、可靠性以及用户和广告商的满意度等关键要求。我们进一步介绍了一个LLM广告的通用框架，包括修改、竞标、预测和拍卖模块。我们提出了每个模块的不同设计考虑，并对其实用性和实施中的技术挑战进行了深入研究。最后，我们探讨了基于LLM的动态创意优化的前景，以显著提升广告对用户的吸引力，并讨论了它所面临的额外挑战。

    arXiv:2311.07601v2 Announce Type: replace-cross Abstract: This paper explores the potential for leveraging Large Language Models (LLM) in the realm of online advertising systems. We delve into essential requirements including privacy, latency, reliability as well as the satisfaction of users and advertisers which such a system must fulfill. We further introduce a general framework for LLM advertisement, consisting of modification, bidding, prediction, and auction modules. Different design considerations for each module is presented, with an in-depth examination of their practicality and the technical challenges inherent to their implementation. Finally, we explore the prospect of LLM-based dynamic creative optimization as a means to significantly enhance the appeal of advertisements to users and discuss its additional challenges.
    
[^19]: SelectLLM：LLMs能否选择重要的指令进行注释？

    SelectLLM: Can LLMs Select Important Instructions to Annotate?. (arXiv:2401.16553v1 [cs.CL])

    [http://arxiv.org/abs/2401.16553](http://arxiv.org/abs/2401.16553)

    这项工作提出了一种名为SelectLLM的新方法，利用LLMs选择高质量指令。通过提示LLMs估计每个无标签指令的有用性和影响力，并使用聚类算法将指令分为多个聚类。

    

    使用大量且多样化的指令数据集训练大型语言模型(LLMs)可以使模型理解和遵循人类指令。最近的研究表明，使用一小组高质量的指令可以超过使用大量更嘈杂的指令。由于指令是无标签的，且响应是自然文本，传统的主动学习方案无法直接应用于选择无标签指令。在这项工作中，我们提出了一种新的指令选择方法，称为SelectLLM，它利用LLMs选择高质量指令。我们的高级思想是利用LLMs通过提示来估计每个指令在没有相应标签（即响应）的情况下的有用性和影响力。SelectLLM包括两个步骤：使用聚类算法（例如CoreSet）将无标签指令划分为多个聚类，然后提示LLMs在其中选择高质量指令。

    Training large language models (LLMs) with a large and diverse instruction dataset aligns the models to comprehend and follow human instructions. Recent works have shown that using a small set of high-quality instructions can outperform using large yet more noisy ones. Because instructions are unlabeled and their responses are natural text, traditional active learning schemes with the model's confidence cannot be directly applied to the selection of unlabeled instructions. In this work, we propose a novel method for instruction selection, called SelectLLM, that leverages LLMs for the selection of high-quality instructions. Our high-level idea is to use LLMs to estimate the usefulness and impactfulness of each instruction without the corresponding labels (i.e., responses), via prompting. SelectLLM involves two steps: dividing the unlabelled instructions using a clustering algorithm (e.g., CoreSet) to multiple clusters, and then prompting LLMs to choose high-quality instructions within e
    
[^20]: 提高神经辐射场新视图合成质量的方法和策略

    Methods and strategies for improving the novel view synthesis quality of neural radiation field. (arXiv:2401.12451v1 [cs.CV])

    [http://arxiv.org/abs/2401.12451](http://arxiv.org/abs/2401.12451)

    这项研究总结了神经辐射场技术在提高图像渲染质量方面的各种方法和策略，为研究人员提供了当前状态和未来演化方向的理解，从而推动NeRF技术在相关领域的应用。

    

    神经辐射场（NeRF）技术可以从2D图像中学习场景的3D隐式模型，并合成逼真的新视图图像。该技术受到工业界的广泛关注，并具有良好的应用前景。为了解决NeRF图像渲染质量需要提高的问题，许多研究人员在过去三年中提出了各种方法来改善渲染质量。本研究对最新的相关论文进行了分类和审查，分析了质量改进背后的技术原理，并讨论了质量改进方法的未来演化方向。这项研究可以帮助研究人员快速了解该领域技术的当前状态和演化背景，有助于激发开发更高效算法的发展，并推动NeRF技术在相关领域的应用。

    Neural Radiation Field (NeRF) technology can learn a 3D implicit model of a scene from 2D images and synthesize realistic novel view images. This technology has received widespread attention from the industry and has good application prospects. In response to the problem that the rendering quality of NeRF images needs to be improved, many researchers have proposed various methods to improve the rendering quality in the past three years. The latest relevant papers are classified and reviewed, the technical principles behind quality improvement are analyzed, and the future evolution direction of quality improvement methods is discussed. This study can help researchers quickly understand the current state and evolutionary context of technology in this field, which is helpful in inspiring the development of more efficient algorithms and promoting the application of NeRF technology in related fields.
    
[^21]: 一种仅解码器的时间序列预测基础模型

    A decoder-only foundation model for time-series forecasting. (arXiv:2310.10688v1 [cs.CL])

    [http://arxiv.org/abs/2310.10688](http://arxiv.org/abs/2310.10688)

    本论文介绍了一种基于补丁解码器式注意力模型的时间序列预测基础模型，该模型在零样本情况下在各种公共数据集上的性能接近最先进的监督预测模型。

    

    受到自然语言处理中大型语言模型的最新进展的启发，我们设计了一种用于预测的时间序列基础模型，其在各种公共数据集上的开箱即用的零样本性能接近每个个别数据集上最先进的监督预测模型的准确性。我们的模型基于在大型时间序列语料库上预训练的补丁解码器式注意力模型，并可以适用于不同的预测历史长度、预测长度和时间粒度。

    Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
    
[^22]: 我们能编辑多模式大型语言模型吗？

    Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.08475](http://arxiv.org/abs/2310.08475)

    本文提出了编辑多模式大型语言模型（MLLMs）的挑战，并构建了一个新的基准用于评估和比较不同编辑方法的效果。实验结果表明，编辑多模式LLMs仍然存在困难，但这项工作为NLP社区提供了宝贵的见解。

    

    本文关注编辑多模式大型语言模型（MLLMs）。与编辑单模式LLMs相比，多模式模型的编辑更具挑战性，需要更高级别的审查和慎重考虑。为了促进这一领域的研究，我们构建了一个新的基准，称为MMEdit，用于编辑多模式LLMs，并建立了一套创新的度量标准进行评估。我们进行了包括各种模型编辑基线的综合实验，并分析了编辑多模式LLMs的不同组件的影响。根据经验，我们发现之前的基线在某种程度上可以实现编辑多模式LLMs，但效果仍然不理想，表明这个任务可能存在的困难。我们希望我们的工作能为NLP社区提供见解。代码和数据集可在https://github.com/zjunlp/EasyEdit获取。

    In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.
    
[^23]: 自适应混合模型的改进VMD和堆叠Informer在增强股市预测中的应用

    Adaptive Hybrid Model for Enhanced Stock Market Predictions Using Improved VMD and Stacked Informer. (arXiv:2310.01884v1 [cs.LG])

    [http://arxiv.org/abs/2310.01884](http://arxiv.org/abs/2310.01884)

    本研究提出了一种自适应混合模型，通过改进的VMD、特征工程和堆叠Informer，结合自适应损失函数，成功应用于股市预测。实验证明该模型在预测准确性、响应性和泛化能力方面优于传统和其他混合模型，对于小企业和特征工程的预测建模有潜在的优化方向和未来发展方向。

    

    本文介绍了一种创新的自适应混合模型，利用改进的变分模态分解（VMD）、特征工程（FE）和堆叠Informer，并结合自适应损失函数，用于股市预测。通过严格的实验，所提出的模型，命名为VMGCformer（Adam+GC+enhanced informer），在处理股市数据的复杂动态和波动性方面展示出显著的熟练度。基于多个基准数据集得出的实验结果突显出该模型在预测准确性、响应性和泛化能力方面相对传统和其他混合模型的优势。本研究进一步强调了优化的潜在途径，并介绍了进一步增强预测建模的未来方向，特别是针对小企业和特征工程。

    This paper introduces an innovative adaptive hybrid model for stock market predictions, leveraging the capabilities of an enhanced Variational Mode Decomposition (VMD), Feature Engineering (FE), and stacked Informer integrated with an adaptive loss function. Through rigorous experimentation, the proposed model, termed Adam+GC+enhanced informer (We name it VMGCformer), demonstrates significant proficiency in addressing the intricate dynamics and volatile nature of stock market data. Experimental results, derived from multiple benchmark datasets, underscore the model's superiority in terms of prediction accuracy, responsiveness, and generalization capabilities over traditional and other hybrid models. The research further highlights potential avenues for optimization and introduces future directions to enhance predictive modeling, especially for small enterprises and feature engineering.
    
[^24]: 通过显著通道实现参数高效微调的简单基准模型

    SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning via Salient Channels. (arXiv:2309.08513v1 [cs.CV])

    [http://arxiv.org/abs/2309.08513](http://arxiv.org/abs/2309.08513)

    本文提出了一种名为“显著通道微调”的简单而有效的方法，通过选择特征图中的部分通道进行微调，实现在低数据资源场景下低参数成本的高效微调，并在多个下游任务中优于全面微调方法。

    

    预训练的视觉Transformer在各种下游任务中具有强大的表示优势。最近，已经提出了许多参数高效的微调方法，并且实验证明，仅微调额外的1%参数就能在低数据资源场景下超越全面微调。然而，这些方法在微调多样的下游任务时忽视了任务特定的信息。本文提出了一种简单而有效的方法称为“显著通道微调”（SCT），通过将模型与任务图像进行前向传播，选择特征图中的部分通道，使得我们只需要微调其中的1/8通道，从而显著降低参数成本并在VTAB-1K基准测试中的18个任务中优于全面微调。这仅增加了0.11M ViT-B参数，相比全面微调，减少了780倍。

    Pre-trained vision transformers have strong representation benefits to various downstream tasks. Recently, many parameter-efficient fine-tuning (PEFT) methods have been proposed, and their experiments demonstrate that tuning only 1% of extra parameters could surpass full fine-tuning in low-data resource scenarios. However, these methods overlook the task-specific information when fine-tuning diverse downstream tasks. In this paper, we propose a simple yet effective method called "Salient Channel Tuning" (SCT) to leverage the task-specific information by forwarding the model with the task images to select partial channels in a feature map that enables us to tune only 1/8 channels leading to significantly lower parameter costs. Experiments outperform full fine-tuning on 18 out of 19 tasks in the VTAB-1K benchmark by adding only 0.11M parameters of the ViT-B, which is 780$\times$ fewer than its full fine-tuning counterpart. Furthermore, experiments on domain generalization and few-shot le
    
[^25]: 确定性特征排序

    Confident Feature Ranking. (arXiv:2307.15361v1 [stat.ML])

    [http://arxiv.org/abs/2307.15361](http://arxiv.org/abs/2307.15361)

    提出了一种确定性特征排序的方法，该方法通过特征重要性值的两两比较，可以产生排序和同时的置信区间，并且可以选择前k个集合。

    

    特征重要性的解释通常依赖于特征的相对顺序而不是数值本身，也就是排序。然而，由于计算重要性值时使用的样本量较小，排序可能不稳定。我们提出了一种事后重要性方法，可以产生一种排序和同时的置信区间。基于特征重要性值的两两比较，我们的方法可以保证高概率包含“真实”（无限样本）排序，并允许选择前k个集合。

    Interpretation of feature importance values often relies on the relative order of the features rather than on the value itself, referred to as ranking. However, the order may be unstable due to the small sample sizes used in calculating the importance values. We propose that post-hoc importance methods produce a ranking and simultaneous confident intervals for the rankings. Based on pairwise comparisons of the feature importance values, our method is guaranteed to include the ``true'' (infinite sample) ranking with high probability and allows for selecting top-k sets.
    
[^26]: 面向边缘人工智能的集成感知-通信-计算 （Integrated Sensing-Communication-Computation） （arXiv：2306.01162v1 [cs.IT]）

    Integrated Sensing-Communication-Computation for Edge Artificial Intelligence. (arXiv:2306.01162v1 [cs.IT])

    [http://arxiv.org/abs/2306.01162](http://arxiv.org/abs/2306.01162)

    面向边缘人工智能的集成感知-通信-计算技术对于提高资源利用率以及实现边缘学习和边缘人工智能推理任务的定制目标至关重要。

    

    边缘人工智能是实现万物智能的一种有前途的解决方案，可用于数字孪生、全息投影、语义通信和自动驾驶等高级技术。边缘人工智能任务的性能，包括边缘学习和边缘人工智能推理，取决于三个高度耦合的过程的质量，即数据获取的感知、信息提取的计算和信息传输的通信。然而，这三个模块需要为增强自己的服务质量而竞争网络资源。为此，集成感知-通信-计算（ISCC）对于提高资源利用率以及实现边缘人工智能任务的定制目标至关重要。通过研究三个模块之间的相互作用，本文提出了各种 ISCC 方案，适用于联邦边缘学习任务和边缘人工智能推理任务。

    Edge artificial intelligence (AI) has been a promising solution towards 6G to empower a series of advanced techniques such as digital twin, holographic projection, semantic communications, and auto-driving, for achieving intelligence of everything. The performance of edge AI tasks, including edge learning and edge AI inference, depends on the quality of three highly coupled processes, i.e., sensing for data acquisition, computation for information extraction, and communication for information transmission. However, these three modules need to compete for network resources for enhancing their own quality-of-services. To this end, integrated sensing-communication-computation (ISCC) is of paramount significance for improving resource utilization as well as achieving the customized goals of edge AI tasks. By investigating the interplay among the three modules, this article presents various kinds of ISCC schemes for federated edge learning tasks and edge AI inference tasks in both applicati
    
[^27]: 自我精磨：通过问题精化增强大型语言模型的推理能力

    Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement. (arXiv:2305.14497v1 [cs.CL])

    [http://arxiv.org/abs/2305.14497](http://arxiv.org/abs/2305.14497)

    Self-Polish是一种方法，通过促使模型逐步完善给定的问题以使其更易理解和可解决，以增强大型语言模型的推理能力。

    

    利用 Chain-of-Thought（CoT）等提示方法可以增强大型语言模型的推理能力，研究人员广泛探索了合理化和答案生成过程。然而，他们忽略了低质量推理问题可能会显着影响推理性能的潜在挑战。本文提出了Self-Polish（SP）方法，通过促使模型逐步完善给定的问题以使其更易理解和可解决，以促进模型的问题解决过程。具体而言，该方法教授模型消除无关信息，重新排列逻辑结构，并将局部条件并行组织成新的条件。 SP与所有其他提示方法正交，方便将其与最先进的技术集成以进一步提高性能。我们在五个基准测试中进行了彻底的实验以证明其有效性。

    Prompting methods such as Chain-of-Thought (CoT) have shed new light on enhancing the reasoning capabilities of large language models, and researchers have extensively explored the generation process of rationales and answers. However, they have overlooked the potential challenges posed by the poor quality of reasoning problems, which may influence the reasoning performance significantly. In this work, we propose Self-Polish (SP), a novel method that facilitates the model's problem-solving process by prompting them to progressively refine the given problems to be more comprehensible and solvable. Specifically, the method teaches models to eliminate irrelevant information, rearrange the logic structure and organize local conditions into new ones parallelly. SP is orthogonal to all other prompting methods, making it convenient to integrate with state-of-the-art techniques for further improvement. We conduct thorough experiments on five benchmarks to illustrate the effectiveness of the pr
    
[^28]: InstructIE: 一份基于指令的中文信息提取数据集

    InstructIE: A Chinese Instruction-based Information Extraction Dataset. (arXiv:2305.11527v1 [cs.CL])

    [http://arxiv.org/abs/2305.11527](http://arxiv.org/abs/2305.11527)

    介绍了一份中文的基于指令的信息提取数据集InstructIE，其中包括了270,000个弱监督的数据和1,000个高质量注释实例。实验结果表明当前的模型表现有待改进，该任务仍存在挑战。

    

    我们引入了一项新的信息提取任务，称为基于指令的信息提取 (Instruction-based IE)，它旨在要求系统遵循特定的指令或指南来提取信息。为了促进该领域的研究，我们构建了一个数据集，称为InstructIE，其中包括来自中文维基百科的 270,000 个弱监督数据和 1,000 个高质量众包注释实例。我们进一步评估了各种基线模型在InstructIE数据集上的表现。结果表明，尽管当前的模型表现很有希望，但仍有改进的空间。此外，我们进行了全面的案例研究分析，强调了基于指令的信息提取任务中固有的挑战。代码和数据集可在 https://github.com/zjunlp/DeepKE/tree/main/example/llm 找到。

    We introduce a new Information Extraction (IE) task dubbed Instruction-based IE, which aims to ask the system to follow specific instructions or guidelines to extract information. To facilitate research in this area, we construct a dataset called InstructIE, consisting of 270,000 weakly supervised data from Chinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We further evaluate the performance of various baseline models on the InstructIE dataset. The results reveal that although current models exhibit promising performance, there is still room for improvement. Furthermore, we conduct a comprehensive case study analysis, underlining the challenges inherent in the Instruction-based IE task. Code and dataset are available at https://github.com/zjunlp/DeepKE/tree/main/example/llm.
    
[^29]: RAMiT：轻量级图像恢复的互惠式注意力混合Transformer

    RAMiT: Reciprocal Attention Mixing Transformer for Lightweight Image Restoration. (arXiv:2305.11474v1 [cs.CV])

    [http://arxiv.org/abs/2305.11474](http://arxiv.org/abs/2305.11474)

    本文提出了轻量级图像恢复的互惠式注意力混合Transformer（RAMiT）。通过使用双向注意力以及一种新的数据增强类型——强度掩码，有效地提高了恢复效果，同时大大减少了参数数量。

    

    尽管近年来许多工作在图像恢复（IR）领域取得了进展，但它们往往面临参数过多的问题。另一个问题是，大多数基于Transformer的IR方法只依靠本地或全局特征，导致接受域有限或存在参数不足的问题。为了解决这些问题，我们提出了一种轻量级IR网络：互惠注意力混合Transformer（RAMiT）。它采用我们提出的维度互惠注意力混合Transformer（D-RAMiT）块，在使用不同数量的多头并行计算双向（空间和通道）自注意力的情况下。双向关注帮助彼此弥补对方的缺点，然后混合。此外，我们引入了一种分层互惠注意力混合（H-RAMi）层，它补偿像素级信息丢失并利用语义信息，同时保持高效的分层结构。此外，在IR任务中，我们重新审视了数据增强策略并提出了一种新的数据增强类型——强度掩码，以提高所提出模型的鲁棒性。广泛的实验表明，在各种IR任务中，包括图像去噪、图像去模糊和JPEG图像去块，我们的所提出的方法在大大减少参数的情况下，优于现有最先进的方法。

    Although many recent works have made advancements in the image restoration (IR) field, they often suffer from an excessive number of parameters. Another issue is that most Transformer-based IR methods focus only on either local or global features, leading to limited receptive fields or deficient parameter issues. To address these problems, we propose a lightweight IR network, Reciprocal Attention Mixing Transformer (RAMiT). It employs our proposed dimensional reciprocal attention mixing Transformer (D-RAMiT) blocks, which compute bi-dimensional (spatial and channel) self-attentions in parallel with different numbers of multi-heads. The bi-dimensional attentions help each other to complement their counterpart's drawbacks and are then mixed. Additionally, we introduce a hierarchical reciprocal attention mixing (H-RAMi) layer that compensates for pixel-level information losses and utilizes semantic information while maintaining an efficient hierarchical structure. Furthermore, we revisit 
    
[^30]: 稠密子图问题及其变种的概述调查

    A Survey on the Densest Subgraph Problem and its Variants. (arXiv:2303.14467v1 [cs.DS])

    [http://arxiv.org/abs/2303.14467](http://arxiv.org/abs/2303.14467)

    本调查全面介绍了稠密子图问题及其变种，在讨论最新结果的同时，还提出了应用广泛的开放性问题。

    

    稠密子图问题要求在给定的图中找到一个顶点子集，其诱导子图的密度度量最大化。这个问题在算法文献中已经引起了五十年来的广泛关注，提出了许多变种，并且有许多应用程序建立在这个基本定义之上。近年来，该问题的研究兴趣再次复苏，2022年和2023年发表了一些开创性的结果。本篇调查提供了基本结果的深入概述和文献中提出的许多变种的详尽覆盖，特别关注最新结果。调查还提供了应用的全面概述，并讨论了这个永恒研究话题中的一些有趣的未解问题。

    The Densest Subgraph Problem requires to find, in a given graph, a subset of vertices whose induced subgraph maximizes a measure of density. The problem has received a great deal of attention in the algorithmic literature over the last five decades, with many variants proposed and many applications built on top of this basic definition. Recent years have witnessed a revival of research interest on this problem with several interesting contributions, including some groundbreaking results, published in 2022 and 2023. This survey provides a deep overview of the fundamental results and an exhaustive coverage of the many variants proposed in the literature, with a special attention on the most recent results. The survey also presents a comprehensive overview of applications and discusses some interesting open problems for this evergreen research topic.
    
[^31]: 低功耗低延迟视觉感知的混合ANN-SNN体系结构

    A Hybrid ANN-SNN Architecture for Low-Power and Low-Latency Visual Perception. (arXiv:2303.14176v1 [cs.CV])

    [http://arxiv.org/abs/2303.14176](http://arxiv.org/abs/2303.14176)

    本文提出了一种混合ANN-SNN体系结构，通过用低速率的辅助ANN初始化状态，解决了SNN与经典ANN在准确性、延迟、功耗方面的平衡性问题，实现了高时间分辨率、低延迟和低功耗的预测。

    

    脉冲神经网络（SNN）是一类仿生神经网络，通过异步和稀疏处理，承诺将低功耗和低延迟的推理应用于边缘设备。然而，作为时间模型，SNN依赖于表达丰富的状态才能生成与经典人工神经网络（ANN）相当的预测结果。这些状态仅在长时间瞬态周期后收敛，并在没有输入数据的情况下迅速衰减，导致更高的延迟、功耗和较低的准确性。本文通过使用以低速率运行的辅助ANN初始化状态来解决这个问题。然后，SNN使用状态以高时间分辨率生成预测，直到下一个初始化阶段。因此，我们的混合ANN-SNN模型结合了两个世界的优点：优化了ANN和SNN之间的性能平衡，不受长时间状态转换和状态衰减的影响，可以实现高时间分辨率、低延迟和低功耗的预测。我们展示了该模型在低功耗/低延迟视觉感知任务上的良好表现。

    Spiking Neural Networks (SNN) are a class of bio-inspired neural networks that promise to bring low-power and low-latency inference to edge devices through asynchronous and sparse processing. However, being temporal models, SNNs depend heavily on expressive states to generate predictions on par with classical artificial neural networks (ANNs). These states converge only after long transient periods, and quickly decay without input data, leading to higher latency, power consumption, and lower accuracy. This work addresses this issue by initializing the state with an auxiliary ANN running at a low rate. The SNN then uses the state to generate predictions with high temporal resolution until the next initialization phase. Our hybrid ANN-SNN model thus combines the best of both worlds: It does not suffer from long state transients and state decay thanks to the ANN, and can generate predictions with high temporal resolution, low latency, and low power thanks to the SNN. We show for the task 
    
[^32]: 长尾分类的曲率平衡特征流形学习

    Curvature-Balanced Feature Manifold Learning for Long-Tailed Classification. (arXiv:2303.12307v1 [cs.CV])

    [http://arxiv.org/abs/2303.12307](http://arxiv.org/abs/2303.12307)

    本文提出了一种曲率平衡特征流形学习的方法，探究了感知流形的几何特性对分类难度的影响，发现曲率不平衡会导致模型不公平。

    

    为了应对长尾分类的挑战，研究人员已经提出了几种方法来减少模型偏差，其中大多数假设样本较少的类是弱类。然而，最近的研究表明，尾部类别并不总是难以学习的，而在样本平衡的数据集上观察到了模型偏差，这表明存在其他影响模型偏差的因素。在本文中，我们系统地提出了一系列用于深度神经网络中感知流形的几何度量，并探讨了感知流形的几何特性对分类难度和学习如何塑造感知流形的几何特性的影响。一个意外的发现是：类别准确度和感知流形的分离程度之间的相关性在训练过程中逐渐减小，而与曲率的负相关性逐渐增加，这表明曲率不平衡导致模型不公平。

    To address the challenges of long-tailed classification, researchers have proposed several approaches to reduce model bias, most of which assume that classes with few samples are weak classes. However, recent studies have shown that tail classes are not always hard to learn, and model bias has been observed on sample-balanced datasets, suggesting the existence of other factors that affect model bias. In this work, we systematically propose a series of geometric measurements for perceptual manifolds in deep neural networks, and then explore the effect of the geometric characteristics of perceptual manifolds on classification difficulty and how learning shapes the geometric characteristics of perceptual manifolds. An unanticipated finding is that the correlation between the class accuracy and the separation degree of perceptual manifolds gradually decreases during training, while the negative correlation with the curvature gradually increases, implying that curvature imbalance leads to m
    
[^33]: COMET: X86成本模型解释框架

    COMET: X86 Cost Model Explanation Framework. (arXiv:2302.06836v2 [cs.PF] UPDATED)

    [http://arxiv.org/abs/2302.06836](http://arxiv.org/abs/2302.06836)

    本文提出了一个用于生成x86成本模型解释的框架COMET，通过提供解释来提高机器学习成本模型的可解释性和可推广性。研究显示该框架所提供的语义信息与成本预测误差呈负相关。

    

    基于机器学习的程序成本模型已被证明可以提供相当准确的程序成本预测。它们可以取代主流编译器中经过大量工程设计的分析性程序成本模型，但它们的黑匣子本质阻碍了它们的采用。在本研究中，我们提出了第一个框架，COMET，用于为x86成本模型生成忠实、可推广和直观的解释。COMET将可解释性特别引入ML-based成本模型，例如Ithemal。我们生成并比较Ithemal的COMET解释与手工精细的分析模型uiCA的COMET解释。我们的实证发现显示在给定的x86基本块的成本模型的COMET解释中，语义更丰富的特征的突出程度与成本预测误差呈负相关。

    ML-based program cost models have been shown to yield fairly accurate program cost predictions. They can replace heavily-engineered analytical program cost models in mainstream compilers, but their black-box nature discourages their adoption. In this work, we propose the first framework, COMET, for generating faithful, generalizable, and intuitive explanations for x86 cost models. COMET brings interpretability specifically to ML-based cost models, such as Ithemal. We generate and compare COMET's explanations for Ithemal against COMET's explanations for a hand-crafted, accurate analytical model, uiCA. Our empirical findings show an inverse correlation between the error in the cost prediction of a cost model and the prominence of semantically-richer features in COMET's explanations for the cost model for a given x86 basic block.
    

