# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Neural Lithography: Close the Design-to-Manufacturing Gap in Computational Optics with a 'Real2Sim' Learned Photolithography Simulator.](http://arxiv.org/abs/2309.17343) | 本文提出了神经光刻技术来解决计算光学中的设计到制造间隔问题。通过将预训练的光刻模拟器整合到光学设计循环中，我们成功实现了光学器件的设计和制造的一致性。 |
| [^2] | [MixQuant: Mixed Precision Quantization with a Bit-width Optimization Search.](http://arxiv.org/abs/2309.17341) | 本研究提出了一种称为MixQuant的混合精度量化算法，在每个层权重上找到了最佳的量化位宽，从而减少了量化模型的准确性降低问题。 |
| [^3] | [Improving Trajectory Prediction in Dynamic Multi-Agent Environment by Dropping Waypoints.](http://arxiv.org/abs/2309.17338) | 本文引入了一种新的框架，通过航点去除技术促进了显式的时间学习，并显著提高了轨迹预测的效果。 |
| [^4] | [Toward Operationalizing Pipeline-aware ML Fairness: A Research Agenda for Developing Practical Guidelines and Tools.](http://arxiv.org/abs/2309.17337) | 该论文提出了一种实现机器学习公平性的管道意识方法，并强调需要指南和工具来在实践中应用这种方法。 |
| [^5] | [Asynchronous Graph Generators.](http://arxiv.org/abs/2309.17335) | 异步图生成器（AGG）是一种新型的图神经网络架构，通过节点生成进行数据插补，并隐式学习传感器测量的因果图表示，取得了state-of-the-art的结果。 |
| [^6] | [Efficient Anatomical labeling of Pulmonary Tree Structures via Implicit Point-Graph Networks.](http://arxiv.org/abs/2309.17329) | 本文介绍了一种通过隐式点图网络高效解剖标记肺部树状结构的方法，提供了SOTA准确度和可用的表面，同时还提供了一个用于评估该方法的数据集。 |
| [^7] | [Assessing Look-Ahead Bias in Stock Return Predictions Generated By GPT Sentiment Analysis.](http://arxiv.org/abs/2309.17322) | 本研究通过基于GPT情感分析的财经新闻标题的交易策略，评估了股票回报预测中的前瞻性偏差。研究发现，在去除相关公司标识符的去偏策略下，匿名化的标题表现出更好的交易绩效。 |
| [^8] | [Building Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models.](http://arxiv.org/abs/2309.17319) | 这篇论文讨论了地理空间人工智能基础模型的隐私和安全风险，并提出了预防和控制策略蓝图。 |
| [^9] | [AutoAgents: A Framework for Automatic Agent Generation.](http://arxiv.org/abs/2309.17288) | AutoAgents是一个创新的框架，它能够根据不同的任务自动生成和协调多个专业代理，以构建一个适应不同任务的AI团队，提高多智能体协作的适应性和效率。 |
| [^10] | [STRONG -- Structure Controllable Legal Opinion Summary Generation.](http://arxiv.org/abs/2309.17280) | 本研究提出了一种结构可控的法律意见摘要生成方法，该方法利用预测的论证角色信息来指导模型生成连贯摘要，具有优于强基准模型的性能。 |
| [^11] | [Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT4.](http://arxiv.org/abs/2309.17277) | Suspicion-Agent是一种创新代理程序，利用具备高阶心灵理论意识的GPT4在不完全信息游戏中表现出良好的适应性和影响他人行为的能力。 |
| [^12] | [Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency.](http://arxiv.org/abs/2309.17272) | 本文提出了一个名为多角度自一致性（MPSC）的框架，用于提升大规模语言模型在复杂的代码生成任务中的性能。该框架通过从多个角度采样多个输出并构建一个多部分图，利用交叉一致性和内一致性信息来选择最优输出。 |
| [^13] | [A Foundation Model for General Moving Object Segmentation in Medical Images.](http://arxiv.org/abs/2309.17264) | 本文提出了一种用于医学图像中移动目标分割的基础模型iMOS，通过对序列中只有少量图像进行注释，即可实现高精度的分割效果 |
| [^14] | [PlaceNav: Topological Navigation through Place Recognition.](http://arxiv.org/abs/2309.17260) | PlaceNav是一种通过地点识别进行拓扑导航的方法，将机器人无关部分分为导航特定和通用的计算机视觉组件，通过使用非机器人来源的大规模数据集增加训练数据的可用性，同时通过地点识别来提高导航性能。新模型的性能提高了76%。 |
| [^15] | [Knowledge Graphs for the Life Sciences: Recent Developments, Challenges and Opportunities.](http://arxiv.org/abs/2309.17255) | 这篇论文综述了在生命科学领域中使用知识图谱的最新发展和进展，并展望了这些技术在未来对这些领域的影响。 |
| [^16] | [Forest Mixing: investigating the impact of multiple search trees and a shared refinements pool on ontology learning.](http://arxiv.org/abs/2309.17252) | 该研究探讨了森林混合方法对本体学习的影响，通过使用多个搜索树和共享的细化池来提高算法性能，以促进对本体中类表达式的查找和探索。 |
| [^17] | [Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering.](http://arxiv.org/abs/2309.17249) | 本研究提出了一种名为批量校准（BC）的方法，用于解决大型语言模型中提示脆弱性和偏见因素导致的性能下降问题。BC通过控制批量输入的上下文偏见，统一了现有的校准方法，并具有零-shot和仅推理的特点。 |
| [^18] | [MORPH: Design Co-optimization with Reinforcement Learning via a Differentiable Hardware Model Proxy.](http://arxiv.org/abs/2309.17227) | MORPH是一种使用强化学习进行硬件设计参数和控制策略的协同优化的方法。通过引入可微分的硬件模型代理，MORPH能够实现有效的优化并保持优化后的硬件代理接近其真实对应物。 |
| [^19] | [RSAM: Learning on manifolds with Riemannian Sharpness-aware Minimization.](http://arxiv.org/abs/2309.17215) | RSAM是一种在流形上学习的算法，通过将Sharpness-Aware Minimization (SAM)推广到Riemannian流形，引入了流形上sharpness的概念，并通过理论分析证明了其与泛化能力的关系。通过该算法的应用，我们展示了其在提升泛化能力方面的有效性。 |
| [^20] | [ComSD: Balancing Behavioral Quality and Diversity in Unsupervised Skill Discovery.](http://arxiv.org/abs/2309.17203) | ComSD提出了一种新方法，通过更合理的互信息估计和动态加权的内在奖励来平衡无监督技能发现中的行为质量和多样性。 |
| [^21] | [An Investigation Into Race Bias in Random Forest Models Based on Breast DCE-MRI Derived Radiomics Features.](http://arxiv.org/abs/2309.17197) | 本文通过研究使用放射组学特征训练的随机森林模型中的种族偏见，发现从DCE-MRI数据中提取的放射组学特征包含种族可辨识的信息。基于这些数据训练的模型能以60-70%的准确率预测白人和黑人种族，且基于种族不平衡数据的训练会导致模型产生偏见行为。 |
| [^22] | [PARF: Primitive-Aware Radiance Fusion for Indoor Scene Novel View Synthesis.](http://arxiv.org/abs/2309.17190) | 本文提出了一种基元感知的辐射融合方法，通过利用语义解析和基元提取来加速重建过程，并成功将语义、基元和辐射信息融合到单一框架中，实现了快速重建、高质量渲染和方便的编辑功能。 |
| [^23] | [Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training.](http://arxiv.org/abs/2309.17179) | Alphazero类似的树搜索框架TS-LLM可以利用学习的价值函数指导大型语言模型的解码和训练，不仅适用于推理任务，还适用于其他任务，并且在不同大小的语言模型上具有普适性和可扩展性 |
| [^24] | [RLAdapter: Bridging Large Language Models to Reinforcement Learning in Open Worlds.](http://arxiv.org/abs/2309.17176) | RLAdapter引入了一个框架，将大型语言模型（LLM）与强化学习相结合，以提高在稀疏奖励环境中的策略学习性能。这通过解决LLM在理解下游任务方面的困难，以及通过避免使用不可访问的模型权重或大量计算资源来微调LLM的方式实现。 |
| [^25] | [A Vision-Guided Robotic System for Grasping Harvested Tomato Trusses in Cluttered Environments.](http://arxiv.org/abs/2309.17170) | 提出了一种用于在混乱环境中抓取已采摘的西红柿穗果的视觉引导机器人系统。该系统利用基于深度学习的视觉系统来识别穗果并确定适合抓取的位置，通过在线学习来排序抓取姿势，并实现无触觉传感器或几何模型的夹持抓取。实验表明，该系统具有100%的清理率和93%的一次性成功抓取率。 |
| [^26] | [An evaluation of GPT models for phenotype concept recognition.](http://arxiv.org/abs/2309.17169) | 本研究评估了最新的GPT模型在临床深度表型学中的性能，并发现目前这些模型尚未达到最先进的性能水平。 |
| [^27] | [DyVal: Graph-informed Dynamic Evaluation of Large Language Models.](http://arxiv.org/abs/2309.17167) | DyVal是一种基于图形信息的大型语言模型动态评估协议，通过动态生成具有可控复杂性的评估样本，评估了各种LLM在推理任务上的性能，发现它们在这些挑战性样本上表现更差。 |
| [^28] | [Advances in Kidney Biopsy Structural Assessment through Dense Instance Segmentation.](http://arxiv.org/abs/2309.17166) | 这项研究提出了一种基于密集实例分割的肾脏活检结构评估的方法，能够自动统计解剖结构上的统计数据，从而减少工作量和观察者间变异性。 |
| [^29] | [Compromise in Multilateral Negotiations and the Global Regulation of Artificial Intelligence.](http://arxiv.org/abs/2309.17158) | 本文研究了多边谈判中的全球妥协，解释了尽管联合国教科文组织成员国代表了多种偏好，但在人工智能规范方面如何实现全球妥协。通过独特的主要来源，本文揭示了实现折衷的实践。 |
| [^30] | [Age Group Discrimination via Free Handwriting Indicators.](http://arxiv.org/abs/2309.17156) | 这项研究通过分析手写数据和计算相关指标，提出了一种使用仪器化墨水笔对老龄人群进行年龄分类的创新方法。 |
| [^31] | [Using Large Language Models for Qualitative Analysis can Introduce Serious Bias.](http://arxiv.org/abs/2309.17147) | 使用大型语言模型（LLMs）对定性分析进行注释可能引入偏见和测量误差，相比之下，使用高质量的人工注释和灵活编码对简单监督模型进行训练可以更好地减少偏见和误差。 |
| [^32] | [Prototype Generation: Robust Feature Visualisation for Data Independent Interpretability.](http://arxiv.org/abs/2309.17144) | 提出了原型生成，一种针对图像分类模型的数据无关解释性的更严格和稳健的特征可视化方法。通过生成自然激活路径的输入来反驳之前不可信的特征可视化算法的观点，并通过定量测量生成的原型的内部激活与自然图像之间的相似性来证实这一点。解释生成的原型可以揭示模型学习到的虚假相关性和偏见，这是定量方法无法识别的。 |
| [^33] | [Revisiting Cephalometric Landmark Detection from the view of Human Pose Estimation with Lightweight Super-Resolution Head.](http://arxiv.org/abs/2309.17143) | 本文探讨了人体姿势估计与颅侧定位的关系，并开发出基于人体姿势估计的稳健基准，以提高颅侧定位的性能。 |
| [^34] | [Benchmarking the Abilities of Large Language Models for RDF Knowledge Graph Creation and Comprehension: How Well Do LLMs Speak Turtle?.](http://arxiv.org/abs/2309.17122) | 本论文评估了各种大型语言模型在RDF知识图创建和理解中的能力，提出了一组任务来探究模型的解析、理解、分析和创建能力，并且对商业可用和免费的离线模型进行了评估和比较。 |
| [^35] | [Meta-Path Learning for Multi-relational Graph Neural Networks.](http://arxiv.org/abs/2309.17113) | 这项工作提出了一种新方法来学习具有高准确性的元路径和元路径图神经网络，关键是使用评分函数来衡量关系的潜在信息量。在实验中，该方法在合成和真实世界实验中表现出比现有的多关系GNNs更好的性能。 |
| [^36] | [Dynamic Interpretability for Model Comparison via Decision Rules.](http://arxiv.org/abs/2309.17095) | 本文提出了DeltaXplainer，一种模型无关的方法，用于描述两个二元分类器之间的差异。通过实验验证了DeltaXplainer在涉及不同类型概念漂移的各种模型比较场景中的有效性。 |
| [^37] | [GAIA-1: A Generative World Model for Autonomous Driving.](http://arxiv.org/abs/2309.17080) | GAIA-1是一个使用视频、文本和动作输入生成逼真驾驶场景的生成世界模型，能够精细控制自车行为和场景特征，并在学习上下文感知、泛化能力和几何理解方面取得了显著成果。 |
| [^38] | [SCALE: Synergized Collaboration of Asymmetric Language Translation Engines.](http://arxiv.org/abs/2309.17061) | SCALE是一种将专业翻译模型和大型语言模型连接为统一翻译引擎的协同框架，通过引入STM的翻译进一步提升LLM的性能，从而在低资源环境中显著优于其他模型。 |
| [^39] | [Tell Me a Story! Narrative-Driven XAI with Large Language Models.](http://arxiv.org/abs/2309.17057) | 这项研究提出了基于大型语言模型的XAIstories框架，通过叙事方式解释AI预测，其中SHAPstories基于SHAP解释解释预测得分，CFstories基于CF解释解释决策。研究结果表明，超过90%的普通读者认可SHAPstories生成的叙事的说服力，92%的数据科学家认为SHAPstories能够提高非专业人士的易用性和信心。 |
| [^40] | [On Continuity of Robust and Accurate Classifiers.](http://arxiv.org/abs/2309.17048) | 本文研究了稳健和准确分类器的连续性，提出了当假设连续时，其稳健性和准确性是不兼容的观点。 |
| [^41] | [Refined Kolmogorov Complexity of Analog, Evolving and Stochastic Recurrent Neural Networks.](http://arxiv.org/abs/2309.17032) | 本研究通过对解析、演化和随机神经网络的科尔莫哥洛夫复杂性进行刻画，提供了对其超图灵计算能力的精确理解，发现了解析网络、演化网络和随机网络之间的层级结构。 |
| [^42] | [Scalable Multi-Temporal Remote Sensing Change Data Generation via Simulating Stochastic Change Process.](http://arxiv.org/abs/2309.17031) | 本文提出了一种通过生成模型实现的可扩展的多时期遥感变化数据生成方法，通过模拟随机变化过程，解决了收集、预处理和注释多时期遥感图像的难题。 |
| [^43] | [Benchmarking Cognitive Biases in Large Language Models as Evaluators.](http://arxiv.org/abs/2309.17012) | 本研究对15个不同大小的大型语言模型进行了评估，发现它们作为评估器存在认知偏差，尤其在文本质量评估中表现出较强的偏见，这对其鲁棒性提出了质疑。同时，研究还发现了人类和机器偏好之间的相关性。 |
| [^44] | [Medical Foundation Models are Susceptible to Targeted Misinformation Attacks.](http://arxiv.org/abs/2309.17007) | 本研究揭示了医学领域中大型语言模型(LLM)的脆弱性，通过操纵模型权重的很小比例，可以故意注入错误的生物医学事实，并且这些错误信息会被模型输出传播。面对这种易受攻击性，我们需要采取措施确保这些模型在医疗实践中的可靠和安全使用。 |
| [^45] | [Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks.](http://arxiv.org/abs/2309.17002) | 本文研究了深度学习中预训练数据中的标签噪声对下游任务的影响，并通过在合成噪声数据集上的实验证明，在预训练中的轻微噪声可以提高领域内的性能，但会损害领域外的性能。为了减轻噪声的影响，提出了一种轻量级的黑盒调整方法（NMTune）。 |
| [^46] | [A Closer Look at Bearing Fault Classification Approaches.](http://arxiv.org/abs/2309.17001) | 这项研究对轴承故障分类方法进行了深入研究，关注了轴承故障诊断的重要性，以及使用机器学习和振动数据来预测故障的方法。 |
| [^47] | [Reliability Quantification of Deep Reinforcement Learning-based Control.](http://arxiv.org/abs/2309.16977) | 本论文提出了一种用于量化深度强化学习控制系统可靠性的方法，通过使用两个神经网络来评估控制的可靠性差异。 |
| [^48] | [A Quantum States Preparation Method Based on Difference-Driven Reinforcement Learning.](http://arxiv.org/abs/2309.16972) | 本文提出了一种基于差分驱动增强学习的量子态制备方法，改进了奖励函数和行为选择策略，以提高两比特量子系统的制备速度和保真度。 |
| [^49] | [Discrete-Choice Model with Generalized Additive Utility Network.](http://arxiv.org/abs/2309.16970) | 本论文提出了一种基于广义可加模型的神经网络架构，称为广义可加效用网络（GAUNet），用于离散选择模型。这些模型在预测准确性上可以与ASU-DNN相媲美，并且相比以前的模型具有更好的解释性。 |
| [^50] | [On Generating Explanations for Reinforcement Learning Policies: An Empirical Study.](http://arxiv.org/abs/2309.16960) | 本文通过引入一组线性时态逻辑（LTL）公式，介绍了一种生成强化学习策略解释的方法，并展示了其在模拟夺旗环境中的有效性。 |
| [^51] | [Denoising Diffusion Bridge Models.](http://arxiv.org/abs/2309.16948) | 本论文提出了一种去噪扩散桥模型（DDBMs），该模型通过学习扩散桥的分数，并基于学习到的分数求解微分方程来实现从一个分布到另一个分布的映射，从而将多类生成模型统一起来。 |
| [^52] | [Robust Asynchronous Collaborative 3D Detection via Bird's Eye View Flow.](http://arxiv.org/abs/2309.16940) | CoBEVFlow是一种鲁棒的异步协作式三维检测系统，通过补偿智能体之间的异步协作信息对齐来解决信息不匹配问题。 |
| [^53] | [PC-Adapter: Topology-Aware Adapter for Efficient Domain Adaption on Point Clouds with Rectified Pseudo-label.](http://arxiv.org/abs/2309.16936) | PC-Adapter是一种拓扑感知适配器，通过保留全局形状信息和学习目标领域的局部特征，实现了点云领域自适应。还提出了一种分层扩展的伪标签矫正策略，以解决伪标签的误类问题。 |
| [^54] | [TranDRL: A Transformer-Driven Deep Reinforcement Learning Enabled Prescriptive Maintenance Framework.](http://arxiv.org/abs/2309.16935) | TranDRL是一种基于Transformer驱动的深度强化学习支持的预防性维护框架，结合了复杂时间模式捕捉和经济高效维护建议，显著提高了剩余寿命（RUL）预测准确性和维护行动优化。 |
| [^55] | [Learning to Receive Help: Intervention-Aware Concept Embedding Models.](http://arxiv.org/abs/2309.16928) | 这项研究提出了一种干预感知的概念嵌入模型，用于提高神经架构对概念干预的响应性，并解决了概念干预顺序和模型架构的依赖性的问题。 |
| [^56] | [Mode Connectivity and Data Heterogeneity of Federated Learning.](http://arxiv.org/abs/2309.16923) | 本研究通过模式连通性的经验和理论研究发现，减少数据异质性可以增加客户端和全局模式之间的连通性，建立了全局模式连通性的定量界限。 |
| [^57] | [ACGAN-GNNExplainer: Auxiliary Conditional Generative Explainer for Graph Neural Networks.](http://arxiv.org/abs/2309.16918) | 本论文提出了一种新的图神经网络解释方法ACGAN-GNNExplainer，将辅助分类器生成对抗网络（ACGAN）引入到GNN解释领域。通过利用生成器为原始输入图生成解释，并借助鉴别器监督生成过程，提高解释的准确性和保真度。实验证明了该方法的有效性和实用性。 |
| [^58] | [ONNXExplainer: an ONNX Based Generic Framework to Explain Neural Networks Using Shapley Values.](http://arxiv.org/abs/2309.16916) | ONNXExplainer是一个基于ONNX的通用框架，使用Shapley值解释神经网络。它提供了自动微分和优化方法，实现了一次性部署和高效的解释计算。 |
| [^59] | [ASAP: Automated Sequence Planning for Complex Robotic Assembly with Physical Feasibility.](http://arxiv.org/abs/2309.16909) | ASAP是一个基于物理的计划方法，用于自动生成一般形状组装的物理可行性序列。它通过考虑重力和使用高效的树搜索算法，能够在大型数据集上生成物理实际的组装序列规划，适用于仿真和真实世界机器人设置。 |
| [^60] | [Sourcing Investment Targets for Venture and Growth Capital Using Multivariate Time Series Transformer.](http://arxiv.org/abs/2309.16888) | 这个论文介绍了一种新颖的方法，利用Transformer-based Multivariate Time Series Classifier (TMTSC)来预测风险投资和成长资本的候选公司的成功可能性，以优化投资目标的选择。通过对相关的方法进行了全面回顾和实验验证，证明了该方法的有效性。 |
| [^61] | [Investigating Human-Identifiable Features Hidden in Adversarial Perturbations.](http://arxiv.org/abs/2309.16878) | 该研究揭示了对抗扰动中隐藏的人类可识别特征，并发现了掩蔽效应和生成效应在不同类型攻击中的表现。通过分析多个攻击算法生成的扰动，发现它们在一定程度上存在相似性。 |
| [^62] | [Preface: A Data-driven Volumetric Prior for Few-shot Ultra High-resolution Face Synthesis.](http://arxiv.org/abs/2309.16859) | 本文提出了一种基于数据驱动的体积化人脸先验，使得能合成先前训练分布外的超高分辨率的新视图。通过训练身份数量有限，结合基于稀疏标记点的3D对齐，该模型能够学习到一个平滑的几何和外观的潜在空间，并通过拟合到2-3个摄像机视图来获取高质量的新对象的体积化表示。 |
| [^63] | [Multi-Bellman operator for convergence of $Q$-learning with linear function approximation.](http://arxiv.org/abs/2309.16819) | 本文通过引入新的多Bellman算子，对线性函数逼近下的$Q$-learning进行了收敛性研究，并提出了多$Q$-learning算法。通过探索多Bellman算子的性质，我们找到了使其成为压缩映射的条件，获得了比传统Bellman算子更好的固定点保证。该算法收敛到多Bellman算子的不动点，可以得到任意精度的解。在验证过程中，我们将该方法应用于常用环境中，展示了其有效性和适用性。 |
| [^64] | [Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution.](http://arxiv.org/abs/2309.16797) | Promptbreeder是一种通过提示演化进行自我改进的通用机制，它可以优化大型语言模型的推理能力，并在算术和常识推理上优于现有的提示策略。 |
| [^65] | [Photonic Accelerators for Image Segmentation in Autonomous Driving and Defect Detection.](http://arxiv.org/abs/2309.16783) | 该论文研究了光子加速器在自动驾驶和缺陷检测中的图像分割应用。研究发现，使用光子加速器执行特定的分割模型可以在准确性上忽略损失，并讨论了在模型被干扰时修复准确性的技术。 |
| [^66] | [Intriguing properties of generative classifiers.](http://arxiv.org/abs/2309.16779) | 生成分类器展示了记录破纪录的人类形状偏好、接近人类级别的超出分布准确性、与人类分类错误的最先进对齐以及理解某些知觉幻象的新兴特性，揭示了零样本生成模型出奇地接近人类物体识别数据。 |
| [^67] | [How many words does ChatGPT know? The answer is ChatWords.](http://arxiv.org/abs/2309.16777) | 这项研究通过开发ChatWords，一个自动化测试系统，为评估ChatGPT和其他NLP AI工具的词汇知识做出了贡献，以解决它们在产生错误答案方面的限制和错觉问题。 |
| [^68] | [Neural scaling laws for phenotypic drug discovery.](http://arxiv.org/abs/2309.16773) | 本研究调查了规模对于辅助小分子药物发现模型的影响，并发现DNN在表型药物发现任务中并没有持续改进，通过引入逆生物过程预训练可以显著提升模型性能。 |
| [^69] | [XVO: Generalized Visual Odometry via Cross-Modal Self-Training.](http://arxiv.org/abs/2309.16772) | XVO是一种通过跨模态自我训练的泛化视觉里程计方法，可以在不同数据集和环境设置下具有强大的自给自足操作的训练模型。其关键创新和贡献包括通过半监督训练学习通用的直接VO回归网络以及使用多模式监督任务来促进泛化表示。 |
| [^70] | [Persona-Coded Poly-Encoder: Persona-Guided Multi-Stream Conversational Sentence Scoring.](http://arxiv.org/abs/2309.16770) | 本论文提出了一种新颖的Persona编码多流程对话句子评分方法，利用个人角色信息来提高对话生成的质量。 |
| [^71] | [Memory in Plain Sight: A Survey of the Uncanny Resemblances between Diffusion Models and Associative Memories.](http://arxiv.org/abs/2309.16750) | 本调查综述了扩散模型（DMs）和关联记忆（AMs）之间的数学联系，揭示了DMs是如何利用能量函数进行去噪数据的，并讨论了未来研究方向。 |
| [^72] | [Discovering environments with XRM.](http://arxiv.org/abs/2309.16748) | 本文提出了一种用于发现环境的算法 XRM，它通过训练两个孪生网络，每个网络从训练数据的一半中学习，并模仿其兄弟网络的错误分类，解决了现有方法需要依赖人工注释环境信息的问题。 |
| [^73] | [Harnessing Diverse Data for Global Disaster Prediction: A Multimodal Framework.](http://arxiv.org/abs/2309.16747) | 本研究提出了一种新颖的多模式灾害预测框架，结合了天气统计数据、卫星图像和文本信息。通过整合多个数据源，对于不同类型的灾害预测，模型的性能得到了增强。 |
| [^74] | [High Throughput Training of Deep Surrogates from Large Ensemble Runs.](http://arxiv.org/abs/2309.16743) | 该论文提出了一个用于加速数值解算器的深度代理的高通量训练方法，通过从大量集合运行的模拟中在线训练模型，利用多级并行性生成丰富的数据集，直接流式传输数据以避免I/O瓶颈和存储问题，同时使用训练储备池来减轻流式传输的固有偏差。实验结果显示，使用该方法能够在2小时内在8TB的数据上训练出准确率提升了47%、批量吞吐量提高了13倍的热方程代理网络。 |
| [^75] | [Supervised Learning Models for Early Detection of Albuminuria Risk in Type-2 Diabetes Mellitus Patients.](http://arxiv.org/abs/2309.16742) | 该论文研究了2型糖尿病患者白蛋白尿的早期预测问题，并开发了一种监督学习模型。通过使用不同的监督学习算法对184条数据进行训练，得出了预测结果。 |
| [^76] | [Multi-Modal Financial Time-Series Retrieval Through Latent Space Projections.](http://arxiv.org/abs/2309.16741) | 本文提出了一种通过深度编码器在低维潜空间中存储金融时间序列的多模态数据的框架，以捕捉数据的重要特征。 |
| [^77] | [Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities.](http://arxiv.org/abs/2309.16739) | 本文探讨了将大型语言模型(LLMs)部署在6G边缘的潜力和挑战。我们介绍了由LLMs支持的关键应用，并从响应时间、带宽成本和数据隐私等方面分析了云端部署面临的问题。我们提出了6G移动边缘计算(MEC)系统可能解决这些问题的方案，并讨论了边缘训练和边缘推理的创新技术。 |
| [^78] | [Resilience of Deep Learning applications: a systematic survey of analysis and hardening techniques.](http://arxiv.org/abs/2309.16733) | 这篇论文系统调查了深度学习应用对底层硬件故障的韧性，并提供了未来研究的方向。 |
| [^79] | [SimPINNs: Simulation-Driven Physics-Informed Neural Networks for Enhanced Performance in Nonlinear Inverse Problems.](http://arxiv.org/abs/2309.16729) | 本文提出了一种基于仿真驱动的物理信息神经网络(SimPINNs)的方法，用于增强非线性反问题的性能。这种方法通过深度学习和融合观测数据和仿真数据的混合损失函数来推断控制物理系统的未知参数。实验结果表明，在轨道复位问题上，SimPINNs的准确性和鲁棒性优于标准PINNs的表现。 |
| [^80] | [GPT-Lab: Next Generation Of Optimal Chemistry Discovery By GPT Driven Robotic Lab.](http://arxiv.org/abs/2309.16721) | GPT-Lab这项研究介绍了一种利用GPT模型赋予机器人人类般智能的范式，其通过挖掘文献、分析材料和方法，并通过高通量合成验证结果，实现了快速材料发现和验证的潜力。 |
| [^81] | [Towards Safe Autonomy in Hybrid Traffic: Detecting Unpredictable Abnormal Behaviors of Human Drivers via Information Sharing.](http://arxiv.org/abs/2309.16716) | 该论文提出了一种在混合交通中实现安全自主性的算法，通过信息共享检测人类驾驶员的不可预测异常行为，提高了轨迹预测的准确性，并能快速检测异常的人类驾驶模式切换。 |
| [^82] | [MV-DeepSDF: Implicit Modeling with Multi-Sweep Point Clouds for 3D Vehicle Reconstruction in Autonomous Driving.](http://arxiv.org/abs/2309.16715) | 该论文提出了一个名为MV-DeepSDF的新框架，利用多次扫描的点云来重建自动驾驶中的三维车辆。与现有方法相比，该方法主要解决了嘈杂和稀疏点云的问题，并通过分析多次扫描的一致性和互补性来提高重建质量。 |
| [^83] | [UAV-assisted Semantic Communication with Hybrid Action Reinforcement Learning.](http://arxiv.org/abs/2309.16713) | 本文提出了一个利用无人机的上行语义通信方案，通过混合动作强化学习框架实现了在数据收集效率和计算能量成本之间的平衡，并取得了显著的改善结果。 |
| [^84] | [General Lipschitz: Certified Robustness Against Resolvable Semantic Transformations via Transformation-Dependent Randomized Smoothing.](http://arxiv.org/abs/2309.16710) | 我们提出了一种新的框架“一般李普希茨（GL）”，用于对神经网络进行认证，以抵御可组合的可解释的语义干扰。方法在ImageNet数据集上与最先进的方法表现相当。 |
| [^85] | [AIR: Threats of Adversarial Attacks on Deep Learning-Based Information Recovery.](http://arxiv.org/abs/2309.16706) | 这项研究探讨了在对抗情况下，深度学习信息恢复模型的鲁棒性。通过对最先进的DeepReceiver模型进行对抗攻击，实验结果表明DeepReceiver对设计的攻击方法是脆弱的。 |
| [^86] | [Prediction and Interpretation of Vehicle Trajectories in the Graph Spectral Domain.](http://arxiv.org/abs/2309.16702) | 本研究提出了一种在图谱领域中预测车辆轨迹的深度学习网络，它使用了图谱表示的有益特征并在性能上取得了多达25%的提升。 |
| [^87] | [MVMR: Evaluating Natural Language Video Localization Bias over Multiple Reliable Videos Pool.](http://arxiv.org/abs/2309.16701) | 本文提出了一个名为MVMR的任务，旨在给定文本查询从大量视频集中定位视频帧。我们通过已有数据集进行相似性筛选来构建数据集，并引入三个MVMR数据集。我们采用了嵌入式文本相似度匹配和视频-语言对齐技术来计算相关性得分，并为MVMR任务开发了一个强大的模型，Reliable Mutual Matching Network (RMMN)。 |
| [^88] | [Inappropriate Benefits and Identification of ChatGPT Misuse in Programming Tests: A Controlled Experiment.](http://arxiv.org/abs/2309.16697) | 这项研究通过受控实验发现，ChatGPT在编程测试中被滥用可能导致非正当利益和抄袭行为，并提出了手动识别ChatGPT辅助程序和学生对ChatGPT的看法的方法。研究结果显示，使用ChatGPT的学生完成编程测试的速度是没有使用ChatGPT的学生的两倍。 |
| [^89] | [Duality Principle and Biologically Plausible Learning: Connecting the Representer Theorem and Hebbian Learning.](http://arxiv.org/abs/2309.16687) | 本研究探索了一种规范方法，相似性匹配，用于推导和理解神经计算的算法基础，并且发现表示定理是研究生物合理学习算法的重要视角。 |
| [^90] | [Alternate Learning based Sparse Semantic Communications for Visual Transmission.](http://arxiv.org/abs/2309.16681) | 本文提出了一种基于交替学习的稀疏语义传输系统SparseSBC，通过两个独立的深度神经网络模型进行编码和解码，解决了信道的非可微性问题，在视觉传输中展现出显著的性能。 |
| [^91] | [Mixup Your Own Pairs.](http://arxiv.org/abs/2309.16633) | 本文提出了一种名为SupReMix的方法，通过混合样本，特别是混合负样本和混合正样本，来解决回归问题中表示学习的挑战。这种方法能够提供更好的性能和更准确的回归结果。 |
| [^92] | [AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models.](http://arxiv.org/abs/2309.16414) | 本研究提出了一种名为AutoCLIP的方法，用于自动调谐视觉语言模型的零样本分类器。AutoCLIP通过为每个提示模板分配图像特定的权重，从而改进了从编码类别描述符推导零样本分类器的方式。 |
| [^93] | [LogicMP: A Neuro-symbolic Approach for Encoding First-order Logic Constraints.](http://arxiv.org/abs/2309.15458) | 本文提出了一种名为LogicMP的新颖神经层，该层通过均场变分推断将一阶逻辑约束编码进神经网络中。通过有效缓解一阶逻辑模型的推断困难，LogicMP在图形、图像和文本任务中表现出比竞争对手更好的性能和效率。 |
| [^94] | [Maximum Diffusion Reinforcement Learning.](http://arxiv.org/abs/2309.15293) | 最大扩散强化学习是一种克服强化学习中数据相关性问题的方法，通过解耦代理的经验实现持续学习，并在各种测试中表现出色。 |
| [^95] | [3D Reconstruction with Generalizable Neural Fields using Scene Priors.](http://arxiv.org/abs/2309.15164) | 使用场景先验的可推广神经场方法，利用单个RGB-D图像映射场景，无需融合模块即可重建完整的场景，具有较好的灵活性和扩展性。 |
| [^96] | [Supersonic: Learning to Generate Source Code Optimisations in C/C++.](http://arxiv.org/abs/2309.14846) | Supersonic 是一个神经方法，用于在C/C++中进行源代码优化。与GPT-3.5-Turbo和GPT-4相比，它在代码优化任务上表现更好，并且改变的程度更小。 |
| [^97] | [Multiple Different Explanations for Image Classifiers.](http://arxiv.org/abs/2309.14309) | 这篇论文介绍了一种算法和工具，可以为图像分类器的输出计算多个解释，从而提高对分类器行为的洞察力。 |
| [^98] | [Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning.](http://arxiv.org/abs/2309.13860) | Fast-HuBERT是一种高效的自监督语音表示学习训练框架，通过分析HuBERT预训练过程中的计算成本，并引入一系列效率优化策略，实现了在Librispeech 960h基准上的5.2倍加速，并且在前人工作中得到了一致的改进。 |
| [^99] | [AI-Copilot for Business Optimisation: A Framework and A Case Study in Production Scheduling.](http://arxiv.org/abs/2309.13218) | 这篇论文提出了一个AI-企业优化的协同辅助系统，通过采用大型语言模型和微调预训练模型的方法，实现了减少人类专业知识需求的目标。 |
| [^100] | [DRG-LLaMA : Tuning LLaMA Model to Predict Diagnosis-related Group for Hospitalized Patients.](http://arxiv.org/abs/2309.12625) | DRG-LLaMA是一个通过在临床笔记上细调的大型语言模型，用于改进住院患者的诊断相关分组预测。在多个评估指标上，DRG-LLaMA-7B相对于其他模型取得了显著的改进，并能够高准确度地预测基本DRG和并发症/合并症（CC）/重大并发症或合并症（MCC）的状态。 |
| [^101] | [Contrastive Decoding Improves Reasoning in Large Language Models.](http://arxiv.org/abs/2309.09117) | 对比解码是一种简单、计算量轻、无需训练的文本生成方法，通过最大化强模型和弱模型之间的似然差异，在各种推理任务上取得了显著改进。 |
| [^102] | [Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning.](http://arxiv.org/abs/2309.06553) | 这项工作介绍了一种基于离线逆向强化学习的提示评估与优化方法，通过利用离线数据集和逆向强化学习，预测提示性能、提高成本效益、生成易读的结果。 |
| [^103] | [PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis.](http://arxiv.org/abs/2309.05833) | 本文提出了一种通过提示检索增强的大语言模型（LLM）来增强云事件根本原因分析工具中置信度估计的方法。 |
| [^104] | [Neural-Hidden-CRF: A Robust Weakly-Supervised Sequence Labeler.](http://arxiv.org/abs/2309.05086) | Neural-Hidden-CRF是一种鲁棒的弱监督序列标注模型，通过引入隐藏CRF层和利用强大的语言模型，它在各项基准测试中取得了最新的最优结果。 |
| [^105] | [The Role of Communication and Reference Songs in the Mixing Process: Insights from Professional Mix Engineers.](http://arxiv.org/abs/2309.03404) | 本论文研究了专业混音师与客户的互动以及他们如何利用客户的反馈指导混音过程。结果表明，混音师通过使用参考曲目和合作沟通等方式，与客户建立起对混音期望音效的默契约定。 |
| [^106] | [Pre-trained Neural Recommenders: A Transferable Zero-Shot Framework for Recommendation Systems.](http://arxiv.org/abs/2309.01188) | 本论文提出了一种预训练的神经推荐系统框架，可以在新领域中构建推荐系统，减少或无需重新训练，并且不需要使用任何辅助信息。通过利用用户-项目交互矩阵的统计特征，实现了零样本推荐的挑战。 |
| [^107] | [Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models.](http://arxiv.org/abs/2308.16149) | Jais和Jais-chat是新的以阿拉伯语为中心的开放式生成式大型语言模型，具有13亿参数，在阿拉伯语方面表现出优异的知识和推理能力，并且在英语方面也具有竞争力。这些模型的发布旨在促进阿拉伯语LLMs的研究。 |
| [^108] | [A Scale-Invariant Task Balancing Approach for Multi-Task Learning.](http://arxiv.org/abs/2308.12029) | 这篇论文提出了一种尺度不变的多任务学习方法（SI-MTL），通过对任务损失进行对数变换和对任务梯度进行归一化，解决了多任务学习中的任务平衡问题，并在多个基准数据集上取得了领先的性能。 |
| [^109] | [Pose Modulated Avatars from Video.](http://arxiv.org/abs/2308.11951) | 本文提出了一种基于神经辐射场和稀疏的摄像机组来重建动态人体运动和形状的方法。与现有的角色模型不同，我们的方法在频域中是自适应和显式的，并通过建模身体部位之间的相关性来解决衣物和皮肤的变形建模挑战。实验结果表明，我们的网络在性能上优于现有方法。 |
| [^110] | [Backward Reasoning in Large Language Models for Verification.](http://arxiv.org/abs/2308.07758) | 本文研究了在大型语言模型中使用反向推理进行验证的方法。作者提出了一种新颖的技术，通过屏蔽问题中的一个标记，并要求语言模型预测被屏蔽的标记来验证候选答案。同时，作者还提出了一种结合正向和反向推理的方法来估计候选答案的概率。 |
| [^111] | [Towards a Causal Probabilistic Framework for Prediction, Action-Selection & Explanations for Robot Block-Stacking Tasks.](http://arxiv.org/abs/2308.06203) | 这项工作提出了一个新颖的因果性概率框架，用于解决机器人堆积方块任务的问题，通过结合因果推断，使机器人能够理解、推理和解释其环境。 |
| [^112] | [An Overview Of Temporal Commonsense Reasoning and Acquisition.](http://arxiv.org/abs/2308.00002) | 本文综述了时间常识推理领域的研究进展，重点关注通过增强语言模型的性能来提高推理能力，并对多个数据集进行评估。然而，这些增强模型仍然难以达到人类水平的推理能力。 |
| [^113] | [XSkill: Cross Embodiment Skill Discovery.](http://arxiv.org/abs/2307.09955) | 本研究提出了XSkill，一种跨体现的技能发现框架，能够从无标签的人类和机器人操纵视频中纯粹地发现跨体现技能原型，并通过条件扩散策略将这些技能转移到机器人动作中，在未见任务中完成学习到的技能的组合。仿真和真实环境中的实验结果表明，这些发现的技能原型能够有效地促进技能转移和组合，从而构建出更通用和可扩展的模仿学习框架。 |
| [^114] | [Long-range Meta-path Search through Progressive Sampling on Large-scale Heterogeneous Information Networks.](http://arxiv.org/abs/2307.08430) | 本论文研究了在大规模异构信息网络中利用长程依赖的重要性，并提出了一种名为长程元路径搜索和渐进抽样（LMSPS）的自动框架。通过动态缩小搜索空间和专门的元路径选择，实验证明了LMSPS的有效性。 |
| [^115] | [Espaloma-0.3.0: Machine-learned molecular mechanics force field for the simulation of protein-ligand systems and beyond.](http://arxiv.org/abs/2307.07085) | Espaloma-0.3.0是一个用于蛋白质-配体系统模拟的机器学习分子力学力场，通过能量和力的拟合纳入量子化学数据进行训练，具有灵活性和可扩展性。 |
| [^116] | [ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models.](http://arxiv.org/abs/2307.00398) | ProbVLM是一种概率适配器，用于估计大规模视觉-语言模型中嵌入的概率分布，以解决固有的嵌入歧义问题，并在多个数据集上展示了其在检索任务中的优越性能表现。 |
| [^117] | [Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning.](http://arxiv.org/abs/2306.14565) | 本论文通过引入第一个大型多样化的视觉指令调整数据集，提出了一种解决大规模多模态模型中幻觉问题的方法。通过设计包含正负指令的数据集和提出的评估方法，能够更准确地衡量模型产生的幻觉。 |
| [^118] | [The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement.](http://arxiv.org/abs/2306.09633) | 谷歌2021年在《自然》杂志上发表的一篇论文声称其使用强化学习在芯片设计领域进行了创新，但两项独立的评估表明，谷歌的方法不如人类设计师、不如一个众所周知的算法（模拟退火），并且也不如普遍可用的商业软件，文章的完整性也遭到了严重的损害。 |
| [^119] | [Ada-NAV: Adaptive Trajectory-Based Sample Efficient Policy Learning for Robotic Navigation.](http://arxiv.org/abs/2306.06192) | Ada-NAV是一种自适应轨迹优化策略学习方法，采用降低策略随机性的方法平衡探索与利用，提高机器人导航任务的采样效率。在真实世界的测试中表现优异，可以在更短的采样时间内取得更高的性能。 |
| [^120] | [Fault Identification of Rotating Machinery Based on Dynamic Feature Reconstruction Signal Graph.](http://arxiv.org/abs/2306.05281) | 本文提出了一种动态特征重构信号图法，在旋转机械故障诊断模型中取得了关键进展，能够动态地选择最优子带的特征系数矩阵，进行适应性信号重构，并从中提取深层特征。 |
| [^121] | [Don't trust your eyes: on the (un)reliability of feature visualizations.](http://arxiv.org/abs/2306.04719) | 本文探讨了神经网络如何从像素中提取模式的问题，并研究了特征可视化的可靠性。实验证据表明，由于优化过程中固有的限制，特征可视化能够可靠理解的功能集非常有限，对于解释神经网络如何处理自然图像的解释能力产生怀疑。 |
| [^122] | [Modeling Human-like Concept Learning with Bayesian Inference over Natural Language.](http://arxiv.org/abs/2306.02797) | 该论文通过在自然语言中进行贝叶斯推理来模拟人类类人概念学习，使用大型语言模型作为提议分布并拟合先验以更好地模拟人类学习者，并在生成性和逻辑性概念上进行实验评估。 |
| [^123] | [Dynamic Neighborhood Construction for Structured Large Discrete Action Spaces.](http://arxiv.org/abs/2305.19891) | 本研究针对无法处理的结构化大离散动作空间（SLDAS）提出了一种名为动态邻域构建（DNC）的新型利用策略，通过可扩展的邻域探索启发式方法，高效地探索连续代理动作周围的离散邻域。 |
| [^124] | [Contextual Vision Transformers for Robust Representation Learning.](http://arxiv.org/abs/2305.19402) | 上下文视觉变换器(ContextViT)用于生成图像的鲁棒特征表示，引入了一个额外的上下文令牌，可以解释掉特定于组的协变量结构，同时保持跨组共享的核心视觉特征，能够在监督微调、半监督学习以及主动学习等方面得到应用。 |
| [^125] | [Multiscale Positive-Unlabeled Detection of AI-Generated Texts.](http://arxiv.org/abs/2305.18149) | 本文提出了一种多尺度正负样本的训练框架，以解决多尺度AI生成文本的检测问题。通过将短机器文本标记为“未标记”来重新表述文本分类问题，并提出了一个规则化损失函数来优化检测性能，有效性能显著优于现有的方法。 |
| [^126] | [Kernel-SSL: Kernel KL Divergence for Self-supervised Learning.](http://arxiv.org/abs/2305.17326) | 本文提出了一种名为Kernel-SSL的自监督学习方法，将多种现有非对比学习方法建立在了再生核希尔伯特空间（RKHS）理解之上并优化了其中的均值嵌入和协方差算子，实验结果显示，在ImageNet数据集下表现显著超越最先进的方法，提高了4.6%。 |
| [^127] | [Implicit bias of SGD in $L_{2}$-regularized linear DNNs: One-way jumps from high to low rank.](http://arxiv.org/abs/2305.16038) | 在$L_{2}$-正则化线性深度神经网络中，使用SGD会产生从更高秩最小值到更低秩最小值的单向跳跃，并且不会跳回。 |
| [^128] | [Feature-aligned N-BEATS with Sinkhorn divergence.](http://arxiv.org/abs/2305.15196) | 这是一个基于Sinkhorn距离的特征对其N-BEATS模型，它通过对齐堆栈中的边际特征概率测度来进行领域广义的时间序列预测，同时保留了N-BEATS的可解释性和预测能力。 |
| [^129] | [Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations.](http://arxiv.org/abs/2305.12715) | 本文提出了不精确标签学习（ILL）框架，利用期望最大化算法对不精确标签信息进行最大似然估计，为各种不精确标签配置问题提供了统一的解决方案。 |
| [^130] | [Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy.](http://arxiv.org/abs/2305.11616) | 这项研究提出了一种使用显著性图来促进深度集成多样性的方法，用于改善OOD检测、校准和准确性，能够优于传统的集成技术，并在OpenOOD基准测试上证明了其有效性。 |
| [^131] | [PESTS: Persian_English Cross Lingual Corpus for Semantic Textual Similarity.](http://arxiv.org/abs/2305.07893) | 本研究提出了跨语言的语义相似性模型PESTS，并通过波斯语-英语的跨语言语料库来验证模型的准确性。 |
| [^132] | [GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering.](http://arxiv.org/abs/2305.03403) | 介绍了一种名为CAAFE的上下文感知自动特征工程方法，它利用大型语言模型根据数据集描述生成更多具有语义意义的特征，能够提高大多数数据集的性能，平均ROC AUC表现提高至0.822。 |
| [^133] | [Active Self-Supervised Learning: A Few Low-Cost Relationships Are All You Need.](http://arxiv.org/abs/2303.15256) | 本研究提出了积极自监督学习（PAL）框架，通过查询样本之间的语义关系来解决自监督学习中需要知道正视图的限制。PAL不仅提供了理论上有基础的学习框架，还能够嵌入先验知识并支持监督和半监督学习。 |
| [^134] | [Efficiently Explaining CSPs with Unsatisfiable Subset Optimization (extended algorithms and examples).](http://arxiv.org/abs/2303.11712) | 本文提出对于约束满足问题，采用一种高效的算法以易于理解的方式解释解决方案，这一方法可以让人类更好地理解机器所做的决策。 |
| [^135] | [Hierarchical Generative Adversarial Imitation Learning with Mid-level Input Generation for Autonomous Driving on Urban Environments.](http://arxiv.org/abs/2302.04823) | 本研究提出了一种名为hGAIL的架构，用于解决车辆的自主导航问题，通过将感知信息直接映射到低级动作的同时，学习车辆环境的中级输入表示。 |
| [^136] | [Generating Dispatching Rules for the Interrupting Swap-Allowed Blocking Job Shop Problem Using Graph Neural Network and Reinforcement Learning.](http://arxiv.org/abs/2302.02506) | 本研究提出了一种基于图神经网络和强化学习的方法，用于生成适应性调度规则来解决中断交换允许的阻塞工件车间问题，并开发了一个模拟器来模拟实际工业生产中的中断和交换情况。 |
| [^137] | [Ensemble Learning for Fusion of Multiview Vision with Occlusion and Missing Information: Framework and Evaluations with Real-World Data and Applications in Driver Hand Activity Recognition.](http://arxiv.org/abs/2301.12592) | 本研究提出了一种集成学习的多视角视觉融合方法，用于处理在真实世界应用中可能存在的间歇性信息缺失问题。通过设计学习框架和提出的插补方案，实现了对驾驶员手势活动的准确分类和位置估计，提高了自动驾驶的安全性能。 |
| [^138] | [MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis.](http://arxiv.org/abs/2211.05862) | 提出了一种新的数据增强方法MixUp-MIL用于多示例学习，通过应用切片内插值方法可以改善甲状腺癌诊断的准确性。 |

# 详细

[^1]: 神经光刻：用"Real2Sim"学习到的光刻模拟器来消除计算光学中的设计到制造间隔

    Neural Lithography: Close the Design-to-Manufacturing Gap in Computational Optics with a 'Real2Sim' Learned Photolithography Simulator. (arXiv:2309.17343v1 [physics.optics])

    [http://arxiv.org/abs/2309.17343](http://arxiv.org/abs/2309.17343)

    本文提出了神经光刻技术来解决计算光学中的设计到制造间隔问题。通过将预训练的光刻模拟器整合到光学设计循环中，我们成功实现了光学器件的设计和制造的一致性。

    

    我们引入神经光刻来解决计算光学中的“设计到制造”间隔问题。具有大设计自由度的计算光学可以实现超越传统光学的先进功能和性能。然而，现有的设计方法往往忽视了制造过程的数值建模，这可能导致设计和制造的光学器件之间存在显著性能差异。为了弥合这一差距，我们首次提出了一个完全可微分的设计框架，将经过预训练的光刻模拟器整合到基于模型的光学设计循环中。通过融合物理模拟建模和基于实验数据的训练，我们的光刻模拟器在设计过程中充当了制造可行性的正则化器，补偿了光刻过程中引入的结构差异。我们通过两个典型任务来证明我们方法的有效性。

    We introduce neural lithography to address the 'design-to-manufacturing' gap in computational optics. Computational optics with large design degrees of freedom enable advanced functionalities and performance beyond traditional optics. However, the existing design approaches often overlook the numerical modeling of the manufacturing process, which can result in significant performance deviation between the design and the fabricated optics. To bridge this gap, we, for the first time, propose a fully differentiable design framework that integrates a pre-trained photolithography simulator into the model-based optical design loop. Leveraging a blend of physics-informed modeling and data-driven training using experimentally collected datasets, our photolithography simulator serves as a regularizer on fabrication feasibility during design, compensating for structure discrepancies introduced in the lithography process. We demonstrate the effectiveness of our approach through two typical tasks 
    
[^2]: MixQuant: 带有位宽优化搜索的混合精度量化

    MixQuant: Mixed Precision Quantization with a Bit-width Optimization Search. (arXiv:2309.17341v1 [cs.LG])

    [http://arxiv.org/abs/2309.17341](http://arxiv.org/abs/2309.17341)

    本研究提出了一种称为MixQuant的混合精度量化算法，在每个层权重上找到了最佳的量化位宽，从而减少了量化模型的准确性降低问题。

    

    量化是一种创建高效深度神经网络（DNNs）的技术，它通过在比f32浮点精度更低的位宽上执行计算和存储张量来实现。量化减少了模型大小和推理延迟，因此可以在计算资源受限和实时系统上部署DNNs。然而，量化可能会导致由舍入误差引起的数值不稳定性，从而导致计算不准确，进而降低了量化模型的准确性。类似于先前的研究表明，偏置和激活对量化更敏感，最好保持全精度或用更高的位宽进行量化，我们表明一些权重比其他权重更敏感，应在其量化位宽上反映出来。为此，我们提出了MixQuant，一种基于舍入误差的搜索算法，以找到每个层权重的最佳定制量化位宽。

    Quantization is a technique for creating efficient Deep Neural Networks (DNNs), which involves performing computations and storing tensors at lower bit-widths than f32 floating point precision. Quantization reduces model size and inference latency, and therefore allows for DNNs to be deployed on platforms with constrained computational resources and real-time systems. However, quantization can lead to numerical instability caused by roundoff error which leads to inaccurate computations and therefore, a decrease in quantized model accuracy. Similarly to prior works, which have shown that both biases and activations are more sensitive to quantization and are best kept in full precision or quantized with higher bit-widths, we show that some weights are more sensitive than others which should be reflected on their quantization bit-width. To that end we propose MixQuant, a search algorithm that finds the optimal custom quantization bit-width for each layer weight based on roundoff error and
    
[^3]: 在动态多智能体环境中通过去掉航点来改进轨迹预测

    Improving Trajectory Prediction in Dynamic Multi-Agent Environment by Dropping Waypoints. (arXiv:2309.17338v1 [cs.RO])

    [http://arxiv.org/abs/2309.17338](http://arxiv.org/abs/2309.17338)

    本文引入了一种新的框架，通过航点去除技术促进了显式的时间学习，并显著提高了轨迹预测的效果。

    

    轨迹的多样和不确定性本质给准确建模带来了巨大的挑战。运动预测系统必须有效地从过去学习空间和时间信息，以预测智能体的未来轨迹。许多现有方法通过堆叠模型中的单独组件学习时间运动，以捕捉时间特征。本文介绍了一种新颖的框架，称为Temporal Waypoint Dropping（TWD），通过航点去除技术促进显式的时间学习。通过航点去除学习可以迫使模型改善其对智能体之间的时间关联的理解，从而显著提高轨迹预测的效果。轨迹预测方法常常假设观测到的轨迹航点序列是完整的，忽略了现实世界中可能存在缺失值的情况，这可能会影响其性能。

    The inherently diverse and uncertain nature of trajectories presents a formidable challenge in accurately modeling them. Motion prediction systems must effectively learn spatial and temporal information from the past to forecast the future trajectories of the agent. Many existing methods learn temporal motion via separate components within stacked models to capture temporal features. This paper introduces a novel framework, called Temporal Waypoint Dropping (TWD), that promotes explicit temporal learning through the waypoint dropping technique. Learning through waypoint dropping can compel the model to improve its understanding of temporal correlations among agents, thus leading to a significant enhancement in trajectory prediction. Trajectory prediction methods often operate under the assumption that observed trajectory waypoint sequences are complete, disregarding real-world scenarios where missing values may occur, which can influence their performance. Moreover, these models freque
    
[^4]: 向实现管道意识的机器学习公平性迈进：开发实际指南和工具的研究议程。

    Toward Operationalizing Pipeline-aware ML Fairness: A Research Agenda for Developing Practical Guidelines and Tools. (arXiv:2309.17337v1 [cs.LG])

    [http://arxiv.org/abs/2309.17337](http://arxiv.org/abs/2309.17337)

    该论文提出了一种实现机器学习公平性的管道意识方法，并强调需要指南和工具来在实践中应用这种方法。

    

    虽然算法公平性是一个蓬勃发展的研究领域，但在实践中，减少偏见问题常常被简化为通过强制执行任意选择的公平性度量标准来实现，要么是在优化阶段强制执行公平性约束，要么是在后处理模型输出时，或者通过操纵训练数据。最近的工作呼吁机器学习社区采取更全面的方法来解决公平性问题，通过系统地研究通过机器学习流程中所做的许多设计选择，并确定针对问题根本原因而不是其症状的干预措施。尽管我们赞同这种基于管道的方法是在实际场景中解决算法不公平性最合适的方法，但我们认为目前几乎没有方法在实践中"实施"这种方法。根据我们作为教育者和实践者的经验，我们首先证明了没有清晰的指南和工具包，即使拥有专业的机器学习知识的个人也会遇到困难。

    While algorithmic fairness is a thriving area of research, in practice, mitigating issues of bias often gets reduced to enforcing an arbitrarily chosen fairness metric, either by enforcing fairness constraints during the optimization step, post-processing model outputs, or by manipulating the training data. Recent work has called on the ML community to take a more holistic approach to tackle fairness issues by systematically investigating the many design choices made through the ML pipeline, and identifying interventions that target the issue's root cause, as opposed to its symptoms. While we share the conviction that this pipeline-based approach is the most appropriate for combating algorithmic unfairness on the ground, we believe there are currently very few methods of \emph{operationalizing} this approach in practice. Drawing on our experience as educators and practitioners, we first demonstrate that without clear guidelines and toolkits, even individuals with specialized ML knowled
    
[^5]: 异步图生成器

    Asynchronous Graph Generators. (arXiv:2309.17335v1 [cs.LG])

    [http://arxiv.org/abs/2309.17335](http://arxiv.org/abs/2309.17335)

    异步图生成器（AGG）是一种新型的图神经网络架构，通过节点生成进行数据插补，并隐式学习传感器测量的因果图表示，取得了state-of-the-art的结果。

    

    我们引入了异步图生成器（AGG），这是一种用于多通道时间序列的新型图神经网络架构。AGG将观测值建模为动态图上的节点，并通过转导式节点生成进行数据插补。AGG不依赖于循环组件或对时间规律的假设，使用可学习的嵌入将测量值、时间戳和元数据直接表示在节点中，并利用注意机制来学习变量之间的关系。这样，所提出的架构隐式地学习传感器测量的因果图表示，可以基于未见时间戳和元数据对新的测量进行预测。我们将所提出的AGG在概念和实证两方面与之前的工作进行了比较，并简要讨论了数据增强对AGG性能的影响。实验结果表明，AGG在t

    We introduce the asynchronous graph generator (AGG), a novel graph neural network architecture for multi-channel time series which models observations as nodes on a dynamic graph and can thus perform data imputation by transductive node generation. Completely free from recurrent components or assumptions about temporal regularity, AGG represents measurements, timestamps and metadata directly in the nodes via learnable embeddings, to then leverage attention to learn expressive relationships across the variables of interest. This way, the proposed architecture implicitly learns a causal graph representation of sensor measurements which can be conditioned on unseen timestamps and metadata to predict new measurements by an expansion of the learnt graph. The proposed AGG is compared both conceptually and empirically to previous work, and the impact of data augmentation on the performance of AGG is also briefly discussed. Our experiments reveal that AGG achieved state-of-the-art results in t
    
[^6]: 通过隐式点图网络高效解剖标记肺部树状结构

    Efficient Anatomical labeling of Pulmonary Tree Structures via Implicit Point-Graph Networks. (arXiv:2309.17329v1 [cs.CV])

    [http://arxiv.org/abs/2309.17329](http://arxiv.org/abs/2309.17329)

    本文介绍了一种通过隐式点图网络高效解剖标记肺部树状结构的方法，提供了SOTA准确度和可用的表面，同时还提供了一个用于评估该方法的数据集。

    

    肺部疾病在全球范围内是导致死亡的主要原因之一。治愈肺部疾病需要更好地理解肺部系统内的许多复杂的3D树状结构，如气道、动脉和静脉。在理论上，它们可以通过高分辨率图像堆栈进行建模。然而，基于密集体素网格的标准CNN方法代价过高。为了解决这个问题，我们引入了一种基于点的方法，保留了树骨架的图连通性，并结合了隐式表面表示。它以较低的计算成本提供了SOTA准确度，生成的模型具有可用的表面。由于公开可访问的数据稀缺，我们还整理了一套广泛的数据集来评估我们的方法，并将其公开。

    Pulmonary diseases rank prominently among the principal causes of death worldwide. Curing them will require, among other things, a better understanding of the many complex 3D tree-shaped structures within the pulmonary system, such as airways, arteries, and veins. In theory, they can be modeled using high-resolution image stacks. Unfortunately, standard CNN approaches operating on dense voxel grids are prohibitively expensive. To remedy this, we introduce a point-based approach that preserves graph connectivity of tree skeleton and incorporates an implicit surface representation. It delivers SOTA accuracy at a low computational cost and the resulting models have usable surfaces. Due to the scarcity of publicly accessible data, we have also curated an extensive dataset to evaluate our approach and will make it public.
    
[^7]: 通过GPT情感分析评估股票回报预测中的前瞻性偏差

    Assessing Look-Ahead Bias in Stock Return Predictions Generated By GPT Sentiment Analysis. (arXiv:2309.17322v1 [q-fin.GN])

    [http://arxiv.org/abs/2309.17322](http://arxiv.org/abs/2309.17322)

    本研究通过基于GPT情感分析的财经新闻标题的交易策略，评估了股票回报预测中的前瞻性偏差。研究发现，在去除相关公司标识符的去偏策略下，匿名化的标题表现出更好的交易绩效。

    

    大型语言模型（LLMs），包括ChatGPT，可以从新闻文本的情绪中提取有益的交易信号。然而，对这些策略进行回溯测试是一项挑战，因为LLMs是在多年的数据上进行训练的，如果训练和回溯测试期间重叠，回溯测试将产生偏倚的结果。这种偏差可以有两种形式：一种是前瞻性偏差，即LLM可能具有关于新闻文章之后的股票回报的具体知识；另一种是干扰效应，即所述公司的一般知识干扰了文本情感的测量。我们通过基于财经新闻标题情感的交易策略来调查这些偏差来源。我们将基于原始标题的交易绩效与去除相关公司标识符的去偏策略进行比较。令人惊讶的是，在样本内（LLM训练窗口内），我们发现匿名化标题的绩效优于原始标题，这表明干扰效应的存在。

    Large language models (LLMs), including ChatGPT, can extract profitable trading signals from the sentiment in news text. However, backtesting such strategies poses a challenge because LLMs are trained on many years of data, and backtesting produces biased results if the training and backtesting periods overlap. This bias can take two forms: a look-ahead bias, in which the LLM may have specific knowledge of the stock returns that followed a news article, and a distraction effect, in which general knowledge of the companies named interferes with the measurement of a text's sentiment. We investigate these sources of bias through trading strategies driven by the sentiment of financial news headlines. We compare trading performance based on the original headlines with de-biased strategies in which we remove the relevant company's identifiers from the text. In-sample (within the LLM training window), we find, surprisingly, that the anonymized headlines outperform, indicating that the distrac
    
[^8]: 构建保护隐私和安全的地理空间人工智能基础模型

    Building Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models. (arXiv:2309.17319v1 [cs.AI])

    [http://arxiv.org/abs/2309.17319](http://arxiv.org/abs/2309.17319)

    这篇论文讨论了地理空间人工智能基础模型的隐私和安全风险，并提出了预防和控制策略蓝图。

    

    最近几年，我们在人工智能的基础模型中取得了实质性的进展，包括语言、视觉和多模态模型。最近的研究突出了在地理空间人工智能中使用基础模型的潜力，称为GeoAI基础模型或Geo-Foundation模型，用于地理问题回答、遥感图像理解、地图生成和基于位置的服务等。然而，GeoAI基础模型的开发和应用可能带来严重的隐私和安全风险，这些风险到目前为止尚未得到充分的讨论或解决。本文介绍了GeoAI基础模型在整个生命周期中的潜在隐私和安全风险，并提出了全面的预防和控制策略蓝图。通过这篇展望性论文，我们希望引起地理空间领域的研究人员和决策者对GeoAI基础模型中固有的隐私和安全风险的关注。

    In recent years we have seen substantial advances in foundation models for artificial intelligence, including language, vision, and multimodal models. Recent studies have highlighted the potential of using foundation models in geospatial artificial intelligence, known as GeoAI Foundation Models or Geo-Foundation Models, for geographic question answering, remote sensing image understanding, map generation, and location-based services, among others. However, the development and application of GeoAI foundation models can pose serious privacy and security risks, which have not been fully discussed or addressed to date. This paper introduces the potential privacy and security risks throughout the lifecycle of GeoAI foundation models and proposes a comprehensive blueprint for preventative and control strategies. Through this vision paper, we hope to draw the attention of researchers and policymakers in geospatial domains to these privacy and security risks inherent in GeoAI foundation models
    
[^9]: AutoAgents: 一个自动生成智能代理的框架

    AutoAgents: A Framework for Automatic Agent Generation. (arXiv:2309.17288v1 [cs.AI])

    [http://arxiv.org/abs/2309.17288](http://arxiv.org/abs/2309.17288)

    AutoAgents是一个创新的框架，它能够根据不同的任务自动生成和协调多个专业代理，以构建一个适应不同任务的AI团队，提高多智能体协作的适应性和效率。

    

    大型语言模型（LLM）在多智能体系统的自动化任务解决中取得了显著进展。然而，大多数现有基于LLM的多智能体方法依赖于预定义的代理来处理简单任务，限制了多智能体协作在不同场景中的适应性。因此，我们引入了AutoAgents，一个创新的框架，根据不同任务自适应生成和协调多个专业代理以构建AI团队。具体而言，AutoAgents通过动态生成多个所需代理来耦合任务和角色之间的关系，并且根据生成的专家代理为当前任务规划解决方案。多个专业代理相互协作以高效完成任务。同时，观察者角色被纳入框架中以反思指定的计划和代理的响应，并对其进行改进。我们对多种基准测试进行的实验表明，AutoAgents能够根据不同的任务适应性生成和协调多个专业代理，实现高效的任务完成。

    Large language models (LLMs) have enabled remarkable advances in automated task-solving with multi-agent systems. However, most existing LLM-based multi-agent approaches rely on predefined agents to handle simple tasks, limiting the adaptability of multi-agent collaboration to different scenarios. Therefore, we introduce AutoAgents, an innovative framework that adaptively generates and coordinates multiple specialized agents to build an AI team according to different tasks. Specifically, AutoAgents couples the relationship between tasks and roles by dynamically generating multiple required agents based on task content and planning solutions for the current task based on the generated expert agents. Multiple specialized agents collaborate with each other to efficiently accomplish tasks. Concurrently, an observer role is incorporated into the framework to reflect on the designated plans and agents' responses and improve upon them. Our experiments on various benchmarks demonstrate that Au
    
[^10]: STRONG - 结构可控的法律意见摘要生成

    STRONG -- Structure Controllable Legal Opinion Summary Generation. (arXiv:2309.17280v1 [cs.CL])

    [http://arxiv.org/abs/2309.17280](http://arxiv.org/abs/2309.17280)

    本研究提出了一种结构可控的法律意见摘要生成方法，该方法利用预测的论证角色信息来指导模型生成连贯摘要，具有优于强基准模型的性能。

    

    我们提出了一种适用于长篇法律意见结构可控的摘要生成方法，考虑了文档的论证结构。我们的方法利用预测的论证角色信息来指导模型生成具有一定结构模式的连贯摘要。我们在法律意见数据集上展示了我们方法的有效性，并证明其在ROUGE、BERTScore和结构相似性方面优于几个强基准模型。

    We propose an approach for the structure controllable summarization of long legal opinions that considers the argument structure of the document. Our approach involves using predicted argument role information to guide the model in generating coherent summaries that follow a provided structure pattern. We demonstrate the effectiveness of our approach on a dataset of legal opinions and show that it outperforms several strong baselines with respect to ROUGE, BERTScore, and structure similarity.
    
[^11]: Suspicion-Agent: 使用具备心灵理论意识的GPT4在不完全信息游戏中进行对局

    Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT4. (arXiv:2309.17277v1 [cs.AI])

    [http://arxiv.org/abs/2309.17277](http://arxiv.org/abs/2309.17277)

    Suspicion-Agent是一种创新代理程序，利用具备高阶心灵理论意识的GPT4在不完全信息游戏中表现出良好的适应性和影响他人行为的能力。

    

    不同于完全信息游戏，其中每个玩家都知道所有元素，不完全信息游戏模拟了在不确定或不完整信息下进行决策的现实世界复杂性。最近在大规模语言模型（LLM）上进行训练的GPT-4以其知识检索和推理能力而闻名。本文探讨了将GPT-4的学习知识应用于不完全信息游戏的可行性。为了实现这一目标，我们引入了一种创新代理程序\textbf{Suspicion-Agent}，该代理程序利用GPT-4的能力在不完全信息游戏中进行对局。通过合适的提示工程来实现不同的功能，基于GPT-4的Suspicion-Agent展示了在一系列不完全信息纸牌游戏中的显著适应能力。重要的是，GPT-4展示了强大的高阶心灵理论（Theory of Mind）能力，这意味着它能够理解他人并有意识地影响他人的行为。利用这一点，我们设计了一个计划

    Unlike perfect information games, where all elements are known to every player, imperfect information games emulate the real-world complexities of decision-making under uncertain or incomplete information. GPT-4, the recent breakthrough in large language models (LLMs) trained on massive passive data, is notable for its knowledge retrieval and reasoning abilities. This paper delves into the applicability of GPT-4's learned knowledge for imperfect information games. To achieve this, we introduce \textbf{Suspicion-Agent}, an innovative agent that leverages GPT-4's capabilities for performing in imperfect information games. With proper prompt engineering to achieve different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable adaptability across a range of imperfect information card games. Importantly, GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it can understand others and intentionally impact others' behavior. Leveraging this, we design a plann
    
[^12]: 提升大规模语言模型在编码中的能力通过多角度自一致性

    Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency. (arXiv:2309.17272v1 [cs.CL])

    [http://arxiv.org/abs/2309.17272](http://arxiv.org/abs/2309.17272)

    本文提出了一个名为多角度自一致性（MPSC）的框架，用于提升大规模语言模型在复杂的代码生成任务中的性能。该框架通过从多个角度采样多个输出并构建一个多部分图，利用交叉一致性和内一致性信息来选择最优输出。

    

    大规模语言模型（LLMs）在文本生成方面展现了卓越的能力。然而，在复杂的推理任务，如代码生成中，LLMs仍然难以在一次尝试中生成正确的答案。先前的研究通过聚合多个输出，利用它们之间的一致性来探索解决方案。然而，这些研究没有全面地从不同的角度捕捉这种一致性。在本文中，我们提出了一种名为多角度自一致性（MPSC）框架的新的解码策略，用于LLM，它将来自多个角度的输出之间的交叉一致性和单个角度内的内一致性结合起来。具体而言，我们要求LLMs对给定查询从各个角度采样多个多样化的输出，并基于它们构建一个多部分图。通过两个预定义的一致性度量，我们将交叉一致性和内一致性信息嵌入到图中。最佳选择是根据这些一致性度量来选择输出。

    Large language models (LLMs) have exhibited remarkable ability in textual generation. However, in complex reasoning tasks such as code generation, generating the correct answer in a single attempt remains a formidable challenge for LLMs. Previous research has explored solutions by aggregating multiple outputs, leveraging the consistency among them. However, none of them have comprehensively captured this consistency from different perspectives. In this paper, we propose the Multi-Perspective Self-Consistency (MPSC) framework, a novel decoding strategy for LLM that incorporates both inter-consistency across outputs from multiple perspectives and intra-consistency within a single perspective. Specifically, we ask LLMs to sample multiple diverse outputs from various perspectives for a given query and then construct a multipartite graph based on them. With two predefined measures of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice is th
    
[^13]: 一种用于医学图像中一般移动目标分割的基础模型

    A Foundation Model for General Moving Object Segmentation in Medical Images. (arXiv:2309.17264v1 [cs.CV])

    [http://arxiv.org/abs/2309.17264](http://arxiv.org/abs/2309.17264)

    本文提出了一种用于医学图像中移动目标分割的基础模型iMOS，通过对序列中只有少量图像进行注释，即可实现高精度的分割效果

    

    医学图像分割旨在描绘感兴趣的解剖或病理结构，在临床诊断中起着关键作用。构建高精度的深度分割模型需要大量高质量的注释数据。然而，医学注释非常繁琐耗时，特别是对于医学视频或3D体积，由于巨大的标签空间和差的帧间一致性。最近，在自然图像中，一个名为Moving Object Segmentation (MOS)的基本任务在技术上取得了重大进展。它的目标是在图像序列中从背景中描绘移动物体，只需要最小的注释。在本文中，我们提出了第一个用于医学图像中MOS的基础模型，名为iMOS。对一个大规模多模态医学数据集进行的大量实验验证了所提出的iMOS的有效性。具体而言，只需对序列中少量的图像进行注释，iMOS就可以实现了

    Medical image segmentation aims to delineate the anatomical or pathological structures of interest, playing a crucial role in clinical diagnosis. A substantial amount of high-quality annotated data is crucial for constructing high-precision deep segmentation models. However, medical annotation is highly cumbersome and time-consuming, especially for medical videos or 3D volumes, due to the huge labeling space and poor inter-frame consistency. Recently, a fundamental task named Moving Object Segmentation (MOS) has made significant advancements in natural images. Its objective is to delineate moving objects from the background within image sequences, requiring only minimal annotations. In this paper, we propose the first foundation model, named iMOS, for MOS in medical images. Extensive experiments on a large multi-modal medical dataset validate the effectiveness of the proposed iMOS. Specifically, with the annotation of only a small number of images in the sequence, iMOS can achieve sati
    
[^14]: PlaceNav: 通过地点识别进行拓扑导航

    PlaceNav: Topological Navigation through Place Recognition. (arXiv:2309.17260v1 [cs.RO])

    [http://arxiv.org/abs/2309.17260](http://arxiv.org/abs/2309.17260)

    PlaceNav是一种通过地点识别进行拓扑导航的方法，将机器人无关部分分为导航特定和通用的计算机视觉组件，通过使用非机器人来源的大规模数据集增加训练数据的可用性，同时通过地点识别来提高导航性能。新模型的性能提高了76%。

    

    最近的研究结果表明，将拓扑导航分为机器人无关和机器人特定的组件可以提高导航性能，通过使用不同类型机器人收集的数据来训练机器人无关部分。然而，导航方法仍受到适合训练数据的稀缺性和计算缩放性差的限制。在本文中，我们提出了一个名为PlaceNav的方法，将机器人无关部分分为导航特定和通用的计算机视觉组件。我们利用视觉地点识别来选择拓扑导航流程中的子目标。这使得子目标选择更高效，并能够利用非机器人来源的大规模数据集增加训练数据的可用性。地点识别使得贝叶斯滤波成为可能，进一步通过增加子目标的时间一致性来提高导航性能。我们的实验结果验证了这一设计，并且新模型的性能提高了76%。

    Recent results suggest that splitting topological navigation into robot-independent and robot-specific components improves navigation performance by enabling the robot-independent part to be trained with data collected by different robot types. However, the navigation methods are still limited by the scarcity of suitable training data and suffer from poor computational scaling. In this work, we present~\methodname, subdividing the robot-independent part into navigation-specific and generic computer vision components. We utilize visual place recognition for the subgoal selection of the topological navigation pipeline. This makes subgoal selection more efficient and enables leveraging large-scale datasets from non-robotics sources, increasing training data availability. Bayes filtering, enabled by place recognition, further improves navigation performance by increasing the temporal consistency of subgoals. Our experimental results verify the design and the new model obtains a 76% higher 
    
[^15]: 生命科学领域的知识图谱：最新发展、挑战和机遇

    Knowledge Graphs for the Life Sciences: Recent Developments, Challenges and Opportunities. (arXiv:2309.17255v1 [cs.AI])

    [http://arxiv.org/abs/2309.17255](http://arxiv.org/abs/2309.17255)

    这篇论文综述了在生命科学领域中使用知识图谱的最新发展和进展，并展望了这些技术在未来对这些领域的影响。

    

    生命科学是研究生物和生命过程的学科，包括化学、生物学、医学和一系列其他相关学科。生命科学的研究工作非常依赖数据，因为它们产生和消费大量科学数据，其中很多数据具有关系和图结构。数据的数量和其中涉及的科学概念和关系的复杂性推动了应用先进的知识驱动技术来管理和解释数据，最终目标是推动科学发现。在这篇综述和观点论文中，我们讨论了知识图谱在生命科学中的最新发展和进展，并展望了这些技术在未来对这些领域的影响。我们重点关注三个主题：知识图谱的构建和管理，以及在新发现的过程中使用知识图谱和相关技术。

    The term life sciences refers to the disciplines that study living organisms and life processes, and include chemistry, biology, medicine, and a range of other related disciplines. Research efforts in life sciences are heavily data-driven, as they produce and consume vast amounts of scientific data, much of which is intrinsically relational and graph-structured.  The volume of data and the complexity of scientific concepts and relations referred to therein promote the application of advanced knowledge-driven technologies for managing and interpreting data, with the ultimate aim to advance scientific discovery.  In this survey and position paper, we discuss recent developments and advances in the use of graph-based technologies in life sciences and set out a vision for how these technologies will impact these fields into the future. We focus on three broad topics: the construction and management of Knowledge Graphs (KGs), the use of KGs and associated technologies in the discovery of ne
    
[^16]: 森林混合：研究多个搜索树和共享的细化池对本体学习的影响

    Forest Mixing: investigating the impact of multiple search trees and a shared refinements pool on ontology learning. (arXiv:2309.17252v1 [cs.AI])

    [http://arxiv.org/abs/2309.17252](http://arxiv.org/abs/2309.17252)

    该研究探讨了森林混合方法对本体学习的影响，通过使用多个搜索树和共享的细化池来提高算法性能，以促进对本体中类表达式的查找和探索。

    

    我们旨在开发白盒机器学习算法。本文主要关注学习描述逻辑中公理的算法。我们扩展了DL-Learner工具中的Class Expression Learning for Ontology Engineering (CELOE)算法。该方法使用多个搜索树和一个共享的细化池，以将搜索空间划分为较小的子空间。我们引入了来自每棵树的最佳类表达式的合取操作，保留给出最多信息的结果。其目的是促进从不同的起始类进行探索，并简化本体中查找类表达式的过程。

    We aim at development white-box machine learning algorithms. We focus here on algorithms for learning axioms in description logic. We extend the Class Expression Learning for Ontology Engineering (CELOE) algorithm contained in the DL-Learner tool. The approach uses multiple search trees and a shared pool of refinements in order to split the search space in smaller subspaces. We introduce the conjunction operation of best class expressions from each tree, keeping the results which give the most information. The aim is to foster exploration from a diverse set of starting classes and to streamline the process of finding class expressions in ontologies. %, particularly in large search spaces. The current implementation and settings indicated that the Forest Mixing approach did not outperform the traditional CELOE. Despite these results, the conceptual proposal brought forward by this approach may stimulate future improvements in class expression finding in ontologies. % and influence. % th
    
[^17]: 批量校准：重新思考上下文学习和提示工程的校准方法

    Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering. (arXiv:2309.17249v1 [cs.CL])

    [http://arxiv.org/abs/2309.17249](http://arxiv.org/abs/2309.17249)

    本研究提出了一种名为批量校准（BC）的方法，用于解决大型语言模型中提示脆弱性和偏见因素导致的性能下降问题。BC通过控制批量输入的上下文偏见，统一了现有的校准方法，并具有零-shot和仅推理的特点。

    

    提示和上下文学习已成为大型语言模型（LLM）的高效学习范式。然而，LLM存在提示脆弱性和各种偏见因素，包括但不限于格式、选择性的表达方式和上下文学习示例。为解决这个导致性能下降的问题，已经开发了校准方法来减轻这些偏见的影响并恢复LLM的性能。在这项工作中，我们首先对现有的校准方法进行了系统分析，提供了统一的观点并揭示了失败案例。受这些分析的启发，我们提出了批量校准（BC），这是一种简单而直观的方法，可以从批量输入中控制上下文偏见，统一了各种先前的方法，并有效地解决了上述问题。BC是零-shot、仅推理和额外成本可忽略。在少-shot设置中，我们进一步扩展BC以实现全部翻译

    Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches, and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allo
    
[^18]: MORPH：通过可微分的硬件模型代理使用强化学习进行设计协同优化

    MORPH: Design Co-optimization with Reinforcement Learning via a Differentiable Hardware Model Proxy. (arXiv:2309.17227v1 [cs.RO])

    [http://arxiv.org/abs/2309.17227](http://arxiv.org/abs/2309.17227)

    MORPH是一种使用强化学习进行硬件设计参数和控制策略的协同优化的方法。通过引入可微分的硬件模型代理，MORPH能够实现有效的优化并保持优化后的硬件代理接近其真实对应物。

    

    我们介绍了一种名为MORPH的方法，用于使用强化学习在仿真中进行硬件设计参数和控制策略的协同优化。与大多数协同优化方法类似，MORPH依赖于正在优化的硬件模型，通常是基于物理定律进行模拟。然而，这样的模型往往难以集成到有效的优化例程中。为了解决这个问题，我们引入了一个代理硬件模型，它始终是可微分的，并且能够使用RL和长期控制策略有效地进行协同优化。MORPH的设计目标是确保优化后的硬件代理与其真实对应物尽可能接近，同时仍然能够完成任务。我们在仿真的2D伸手和3D多指操纵任务上演示了我们的方法。

    We introduce MORPH, a method for co-optimization of hardware design parameters and control policies in simulation using reinforcement learning. Like most co-optimization methods, MORPH relies on a model of the hardware being optimized, usually simulated based on the laws of physics. However, such a model is often difficult to integrate into an effective optimization routine. To address this, we introduce a proxy hardware model, which is always differentiable and enables efficient co-optimization alongside a long-horizon control policy using RL. MORPH is designed to ensure that the optimized hardware proxy remains as close as possible to its realistic counterpart, while still enabling task completion. We demonstrate our approach on simulated 2D reaching and 3D multi-fingered manipulation tasks.
    
[^19]: RSAM：使用Riemannian Sharpness-aware Minimization在流形上学习

    RSAM: Learning on manifolds with Riemannian Sharpness-aware Minimization. (arXiv:2309.17215v1 [cs.LG])

    [http://arxiv.org/abs/2309.17215](http://arxiv.org/abs/2309.17215)

    RSAM是一种在流形上学习的算法，通过将Sharpness-Aware Minimization (SAM)推广到Riemannian流形，引入了流形上sharpness的概念，并通过理论分析证明了其与泛化能力的关系。通过该算法的应用，我们展示了其在提升泛化能力方面的有效性。

    

    如今，了解损失函数空间的几何结构有望提升模型的泛化能力。在这项工作中，我们借鉴了之前将几何原理应用于优化的研究，并提出了一种改进受限制优化问题鲁棒性和泛化能力的新方法。事实上，本文旨在将Sharpness-Aware Minimization (SAM)优化器推广到Riemannian流形。为了支持流形上的“sharpness”概念，我们首先对流形上的sharpness引入了一种新的定义。为了证明这个概念的有效性，我们提出了一个理论分析来描述流形sharpness与泛化能力之间的关系，并呈现了一个更紧密的泛化缺口上限，这是之前未知的结果。受到这个分析的启发，我们提出了我们的算法，Riemannian Sharpness-Aware Minimization (RSAM)。为了展示RSAM在提升泛化能力方面的能力，我们在一个约束优化问题上评估和对比了我们的算法。

    Nowadays, understanding the geometry of the loss landscape shows promise in enhancing a model's generalization ability. In this work, we draw upon prior works that apply geometric principles to optimization and present a novel approach to improve robustness and generalization ability for constrained optimization problems. Indeed, this paper aims to generalize the Sharpness-Aware Minimization (SAM) optimizer to Riemannian manifolds. In doing so, we first extend the concept of sharpness and introduce a novel notion of sharpness on manifolds. To support this notion of sharpness, we present a theoretical analysis characterizing generalization capabilities with respect to manifold sharpness, which demonstrates a tighter bound on the generalization gap, a result not known before. Motivated by this analysis, we introduce our algorithm, Riemannian Sharpness-Aware Minimization (RSAM). To demonstrate RSAM's ability to enhance generalization ability, we evaluate and contrast our algorithm on a br
    
[^20]: ComSD: 在无监督技能发现中平衡行为质量和多样性

    ComSD: Balancing Behavioral Quality and Diversity in Unsupervised Skill Discovery. (arXiv:2309.17203v1 [cs.LG])

    [http://arxiv.org/abs/2309.17203](http://arxiv.org/abs/2309.17203)

    ComSD提出了一种新方法，通过更合理的互信息估计和动态加权的内在奖励来平衡无监督技能发现中的行为质量和多样性。

    

    无监督技能发现的理想方法能够在没有外部奖励的情况下产生多样且合格的技能，同时发现的技能集能够以各种方式高效地适应下游任务。本文提出了Contrastive multi-objectives Skill Discovery (ComSD)，通过更合理的互信息估计和动态加权的内在奖励来减轻发现的行为在质量和多样性之间的冲突。

    Learning diverse and qualified behaviors for utilization and adaptation without supervision is a key ability of intelligent creatures. Ideal unsupervised skill discovery methods are able to produce diverse and qualified skills in the absence of extrinsic reward, while the discovered skill set can efficiently adapt to downstream tasks in various ways. Maximizing the Mutual Information (MI) between skills and visited states can achieve ideal skill-conditioned behavior distillation in theory. However, it's difficult for recent advanced methods to well balance behavioral quality (exploration) and diversity (exploitation) in practice, which may be attributed to the unreasonable MI estimation by their rigid intrinsic reward design. In this paper, we propose Contrastive multi-objectives Skill Discovery (ComSD) which tries to mitigate the quality-versus-diversity conflict of discovered behaviors through a more reasonable MI estimation and a dynamically weighted intrinsic reward. ComSD proposes
    
[^21]: 基于乳腺DCE-MRI特征的随机森林模型中的种族偏见调查

    An Investigation Into Race Bias in Random Forest Models Based on Breast DCE-MRI Derived Radiomics Features. (arXiv:2309.17197v1 [cs.LG])

    [http://arxiv.org/abs/2309.17197](http://arxiv.org/abs/2309.17197)

    本文通过研究使用放射组学特征训练的随机森林模型中的种族偏见，发现从DCE-MRI数据中提取的放射组学特征包含种族可辨识的信息。基于这些数据训练的模型能以60-70%的准确率预测白人和黑人种族，且基于种族不平衡数据的训练会导致模型产生偏见行为。

    

    最近的研究表明，人工智能（AI）模型在使用受保护属性不平衡的数据进行训练时可能会表现出偏见。大部分现有的工作都集中在深度学习模型上，但是利用手工特征的经典人工智能技术也可能受到这种偏见的影响。在本文中，我们研究了使用放射组学特征训练的随机森林（RF）模型中可能存在的种族偏见。我们的应用是利用乳腺癌患者的动态增强磁共振成像（DCE-MRI）预测肿瘤分子亚型。我们的研究结果表明，从DCE-MRI数据中提取的放射组学特征确实包含种族可辨识的信息，并且基于这些数据训练的RF模型可以以60-70％的准确率预测白人和黑人种族，具体取决于所使用的特征子集。此外，基于种族不平衡数据训练的RF模型似乎会产生偏见行为，表现出某种程度的种族偏见。

    Recent research has shown that artificial intelligence (AI) models can exhibit bias in performance when trained using data that are imbalanced by protected attribute(s). Most work to date has focused on deep learning models, but classical AI techniques that make use of hand-crafted features may also be susceptible to such bias. In this paper we investigate the potential for race bias in random forest (RF) models trained using radiomics features. Our application is prediction of tumour molecular subtype from dynamic contrast enhanced magnetic resonance imaging (DCE-MRI) of breast cancer patients. Our results show that radiomics features derived from DCE-MRI data do contain race-identifiable information, and that RF models can be trained to predict White and Black race from these data with 60-70% accuracy, depending on the subset of features used. Furthermore, RF models trained to predict tumour molecular subtype using race-imbalanced data seem to produce biased behaviour, exhibiting bet
    
[^22]: PARF: 室内场景新视角合成的基元感知辐射融合方法

    PARF: Primitive-Aware Radiance Fusion for Indoor Scene Novel View Synthesis. (arXiv:2309.17190v1 [cs.CV])

    [http://arxiv.org/abs/2309.17190](http://arxiv.org/abs/2309.17190)

    本文提出了一种基元感知的辐射融合方法，通过利用语义解析和基元提取来加速重建过程，并成功将语义、基元和辐射信息融合到单一框架中，实现了快速重建、高质量渲染和方便的编辑功能。

    

    本文提出了一种快速场景辐射场重建方法，具有强大的新视角合成性能和方便的场景编辑功能。其关键思想是充分利用语义解析和基元提取来约束和加速辐射场重建过程。为了实现这一目标，本文提出了一种基元感知混合渲染策略，以充分发挥体素渲染和基元渲染的优点。我们进一步提供了一个重建流程，通过迭代地对每个输入帧进行基元解析和辐射场学习，成功地将语义、基元和辐射信息融合到一个单一的框架中。广泛的评估证明了我们方法的快速重建能力、高渲染质量和方便的编辑功能。

    This paper proposes a method for fast scene radiance field reconstruction with strong novel view synthesis performance and convenient scene editing functionality. The key idea is to fully utilize semantic parsing and primitive extraction for constraining and accelerating the radiance field reconstruction process. To fulfill this goal, a primitive-aware hybrid rendering strategy was proposed to enjoy the best of both volumetric and primitive rendering. We further contribute a reconstruction pipeline conducts primitive parsing and radiance field learning iteratively for each input frame which successfully fuses semantic, primitive, and radiance information into a single framework. Extensive evaluations demonstrate the fast reconstruction ability, high rendering quality, and convenient editing functionality of our method.
    
[^23]: Alphazero类似的树搜索可以指导大型语言模型的解码和训练

    Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training. (arXiv:2309.17179v1 [cs.LG])

    [http://arxiv.org/abs/2309.17179](http://arxiv.org/abs/2309.17179)

    Alphazero类似的树搜索框架TS-LLM可以利用学习的价值函数指导大型语言模型的解码和训练，不仅适用于推理任务，还适用于其他任务，并且在不同大小的语言模型上具有普适性和可扩展性

    

    大型语言模型 (LLM) 通常采用采样或束搜索，结合 Chain-of-Thought (CoT) 等提示来提高推理和解码能力。最近的研究如 Tree-of-Thought (ToT) 和 Reasoning via Planning (RAP) 旨在通过利用树搜索算法来引导多步推理，来增强LLM的推理能力。这些方法主要关注LLM在推理过程中的推理能力，并且严重依赖人为设计的提示来激活LLM作为一个价值函数，缺乏普适性和可扩展性。为了解决这些限制，我们提出了一种AlphaZero类似的用于LLM的树搜索框架 (称为TS-LLM)，系统地说明了如何通过学习的价值函数利用树搜索来指导LLM的解码能力。TS-LLM在两个关键方面与众不同：(1)通过利用学习的价值函数，我们的方法可以普适地应用于除了推理之外的不同任务 (例如RLHF对齐)，以及任何大小的LLM，而不需要提示

    Large language models (LLMs) typically employ sampling or beam search, accompanied by prompts such as Chain-of-Thought (CoT), to boost reasoning and decoding ability. Recent work like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the reasoning capabilities of LLMs by utilizing tree-search algorithms to guide multi-step reasoning. These methods mainly focus on LLMs' reasoning ability during inference and heavily rely on human-designed prompts to activate LLM as a value function, which lacks general applicability and scalability. To address these limitations, we present an AlphaZero-like tree-search framework for LLMs (termed TS-LLM), systematically illustrating how tree-search with a learned value function can guide LLMs' decoding ability. TS-LLM distinguishes itself in two key ways: (1) Leveraging a learned value function, our approach can be generally applied to different tasks beyond reasoning (such as RLHF alignment), and LLMs of any size, without prompting a
    
[^24]: RLAdapter：在开放环境中将大型语言模型与强化学习相结合

    RLAdapter: Bridging Large Language Models to Reinforcement Learning in Open Worlds. (arXiv:2309.17176v1 [cs.AI])

    [http://arxiv.org/abs/2309.17176](http://arxiv.org/abs/2309.17176)

    RLAdapter引入了一个框架，将大型语言模型（LLM）与强化学习相结合，以提高在稀疏奖励环境中的策略学习性能。这通过解决LLM在理解下游任务方面的困难，以及通过避免使用不可访问的模型权重或大量计算资源来微调LLM的方式实现。

    

    强化学习在决策问题中取得了显著的成功，但通常需要与环境进行大量的交互，在稀疏奖励环境中学习有意义的策略是具有挑战性的。大型语言模型（LLM）可以为代理提供有价值的指导，从而增强RL算法在这些环境中的性能。然而，LLM通常在理解下游任务方面遇到困难，这阻碍了它们在这些任务中最优地帮助代理的能力。缓解这个问题的常见方法是使用与任务相关的数据来微调LLM，使其能够为RL代理提供有用的指导。然而，这种方法遇到了一些困难，比如无法访问的模型权重或需要大量的计算资源，使其不切实际。在这项工作中，我们引入了RLAdapter，这是一个框架，在RL算法和LLM之间建立更好的连接，从而整合它们的优势和能力。

    While reinforcement learning (RL) shows remarkable success in decision-making problems, it often requires a lot of interactions with the environment, and in sparse-reward environments, it is challenging to learn meaningful policies. Large Language Models (LLMs) can potentially provide valuable guidance to agents in learning policies, thereby enhancing the performance of RL algorithms in such environments. However, LLMs often encounter difficulties in understanding downstream tasks, which hinders their ability to optimally assist agents in these tasks. A common approach to mitigating this issue is to fine-tune the LLMs with task-related data, enabling them to offer useful guidance for RL agents. However, this approach encounters several difficulties, such as inaccessible model weights or the need for significant computational resources, making it impractical. In this work, we introduce RLAdapter, a framework that builds a better connection between RL algorithms and LLMs by incorporating
    
[^25]: 用于在混乱环境中抓取已采摘的西红柿穗果的视觉引导机器人系统

    A Vision-Guided Robotic System for Grasping Harvested Tomato Trusses in Cluttered Environments. (arXiv:2309.17170v1 [cs.RO])

    [http://arxiv.org/abs/2309.17170](http://arxiv.org/abs/2309.17170)

    提出了一种用于在混乱环境中抓取已采摘的西红柿穗果的视觉引导机器人系统。该系统利用基于深度学习的视觉系统来识别穗果并确定适合抓取的位置，通过在线学习来排序抓取姿势，并实现无触觉传感器或几何模型的夹持抓取。实验表明，该系统具有100%的清理率和93%的一次性成功抓取率。

    

    目前，对于西红柿的称重和包装需要大量的人工操作。自动化的主要障碍在于开发一个可靠的用于已采摘的穗果的机器人抓取系统的困难。我们提出了一种方法来抓取堆放在装箱中的穗果，这是它们在采摘后常见的存储和运输方式。该方法包括一个基于深度学习的视觉系统，首先识别出装箱中的单个穗果，然后确定茎部的适合抓取的位置。为此，我们引入了一个具有在线学习能力的抓取姿势排序算法。在选择了最有前景的抓取姿势之后，机器人执行一种无需触觉传感器或几何模型的夹持抓取。实验室实验证明，配备了一个手眼一体的RGB-D相机的机器人操纵器从堆中捡起所有的穗果的清理率达到100%。93%的穗果在第一次尝试时成功抓取。

    Currently, truss tomato weighing and packaging require significant manual work. The main obstacle to automation lies in the difficulty of developing a reliable robotic grasping system for already harvested trusses. We propose a method to grasp trusses that are stacked in a crate with considerable clutter, which is how they are commonly stored and transported after harvest. The method consists of a deep learning-based vision system to first identify the individual trusses in the crate and then determine a suitable grasping location on the stem. To this end, we have introduced a grasp pose ranking algorithm with online learning capabilities. After selecting the most promising grasp pose, the robot executes a pinch grasp without needing touch sensors or geometric models. Lab experiments with a robotic manipulator equipped with an eye-in-hand RGB-D camera showed a 100% clearance rate when tasked to pick all trusses from a pile. 93% of the trusses were successfully grasped on the first try,
    
[^26]: 评估GPT模型在表型概念识别中的应用

    An evaluation of GPT models for phenotype concept recognition. (arXiv:2309.17169v1 [cs.CL])

    [http://arxiv.org/abs/2309.17169](http://arxiv.org/abs/2309.17169)

    本研究评估了最新的GPT模型在临床深度表型学中的性能，并发现目前这些模型尚未达到最先进的性能水平。

    

    目标：临床深度表型学在罕见疾病患者的诊断以及构建协调护理计划中起到关键作用。该过程依赖于使用本体概念对患者档案进行建模和整理，通常使用人类表型本体进行。机器学习方法已被广泛采用来支持这项表型概念识别任务。随着大型语言模型在大多数自然语言处理任务中的显著应用，我们在本研究中评估了最新的生成式预训练变压器（GPT）模型在临床深度表型学中的性能。材料和方法：研究的实验设置包括七个不同特异性级别的提示、两个GPT模型（gpt-3.5和gpt-4.0）以及一个已建立的表型识别黄金标准。结果：我们的结果显示，目前这些模型尚未达到最先进的性能水平。最佳运行结果采用了少量样本学习，实现了最佳表现。

    Objective: Clinical deep phenotyping plays a critical role in both the diagnosis of patients with rare disorders as well as in building care coordination plans. The process relies on modelling and curating patient profiles using ontology concepts, usually from the Human Phenotype Ontology. Machine learning methods have been widely adopted to support this phenotype concept recognition task. With the significant shift in the use of large language models (LLMs) for most NLP tasks, herewithin, we examine the performance of the latest Generative Pre-trained Transformer (GPT) models underpinning ChatGPT in clinical deep phenotyping. Materials and Methods: The experimental setup of the study included seven prompts of various levels of specificity, two GPT models (gpt-3.5 and gpt-4.0) and an established gold standard for phenotype recognition. Results: Our results show that, currently, these models have not yet achieved state of the art performance. The best run, using few-shots learning, achi
    
[^27]: DyVal: 基于图形信息的大型语言模型动态评估

    DyVal: Graph-informed Dynamic Evaluation of Large Language Models. (arXiv:2309.17167v1 [cs.AI])

    [http://arxiv.org/abs/2309.17167](http://arxiv.org/abs/2309.17167)

    DyVal是一种基于图形信息的大型语言模型动态评估协议，通过动态生成具有可控复杂性的评估样本，评估了各种LLM在推理任务上的性能，发现它们在这些挑战性样本上表现更差。

    

    大型语言模型在各种评估基准中取得了显著的性能。然而，人们对它们的性能提出了担忧，因为它们庞大的训练语料库中可能存在数据污染。此外，当前基准的静态性质和固定复杂性可能无法充分衡量LLM的进步能力。在本文中，我们介绍了DyVal，一种新颖、通用且灵活的动态评估LLM的协议。基于我们提出的动态评估框架，我们利用有向无环图的结构优势构建了基于图形信息的DyVal，以动态生成具有可控复杂性的评估样本。DyVal生成了包括数学、逻辑推理和算法问题在内的具有挑战性的推理任务的评估集。我们评估了从Flan-T5-large到ChatGPT和GPT4的各种LLM。实验表明，LLM在DyVal生成的评估样本上表现更差

    Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns about their performance are raised on potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a novel, general, and flexible evaluation protocol for dynamic evaluation of LLMs. Based on our proposed dynamic evaluation framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to ChatGPT and GPT4. Experiments demonstrate that LLMs perform worse in DyVal-generated evaluation samples with di
    
[^28]: 通过密集实例分割在肾脏活检结构评估方面的进展

    Advances in Kidney Biopsy Structural Assessment through Dense Instance Segmentation. (arXiv:2309.17166v1 [cs.CV])

    [http://arxiv.org/abs/2309.17166](http://arxiv.org/abs/2309.17166)

    这项研究提出了一种基于密集实例分割的肾脏活检结构评估的方法，能够自动统计解剖结构上的统计数据，从而减少工作量和观察者间变异性。

    

    肾脏活检是肾脏疾病诊断的金标准。专家肾脏病理学家制定的病变评分是半定量的，并且存在高的观察者间变异性。因此，通过对分割的解剖对象进行自动统计可以显著减少工作量和这种观察者间变异性。然而，活检的实例分割是一个具有挑战性的问题，原因有：（a）平均数量较大（约300至1000个）密集接触的解剖结构，（b）具有多个类别（至少3个），（c）尺寸和形状各异。目前使用的实例分割模型不能以高效通用的方式同时解决这些挑战。在本文中，我们提出了第一个不需要锚点的实例分割模型，该模型将扩散模型、变换器模块和RCNN（区域卷积神经网络）结合起来。我们的模型在一台NVIDIA GeForce RTX 3090 GPU上进行训练，但可以提供可观的结果。

    The kidney biopsy is the gold standard for the diagnosis of kidney diseases. Lesion scores made by expert renal pathologists are semi-quantitative and suffer from high inter-observer variability. Automatically obtaining statistics per segmented anatomical object, therefore, can bring significant benefits in reducing labor and this inter-observer variability. Instance segmentation for a biopsy, however, has been a challenging problem due to (a) the on average large number (around 300 to 1000) of densely touching anatomical structures, (b) with multiple classes (at least 3) and (c) in different sizes and shapes. The currently used instance segmentation models cannot simultaneously deal with these challenges in an efficient yet generic manner. In this paper, we propose the first anchor-free instance segmentation model that combines diffusion models, transformer modules, and RCNNs (regional convolution neural networks). Our model is trained on just one NVIDIA GeForce RTX 3090 GPU, but can 
    
[^29]: 多边谈判中的折衷与全球人工智能规范

    Compromise in Multilateral Negotiations and the Global Regulation of Artificial Intelligence. (arXiv:2309.17158v1 [cs.CY])

    [http://arxiv.org/abs/2309.17158](http://arxiv.org/abs/2309.17158)

    本文研究了多边谈判中的全球妥协，解释了尽管联合国教科文组织成员国代表了多种偏好，但在人工智能规范方面如何实现全球妥协。通过独特的主要来源，本文揭示了实现折衷的实践。

    

    随着人工智能（AI）技术在全球范围内的传播，国际讨论越来越集中于AI对民主、人权、基本自由、安全以及经济和社会发展的影响。在这个背景下，联合国教科文组织于2021年11月通过的《人工智能伦理建议》已成为人工智能发展和部署的首个全球规范框架。本文通过独特的一组主要来源，包括书面立场和记录的讨论，解释了尽管联合国教科文组织成员国代表了多种自由主义和主权主义偏好的立场，但在人工智能规范方面如何实现全球妥协。借鉴博尔唐斯基的实用社会学，本文对多边谈判实践进行了概念化，并将多边妥协归因于各方的实际做法。

    As artificial intelligence (AI) technologies spread worldwide, international discussions have increasingly focused on their consequences for democracy, human rights, fundamental freedoms, security, and economic and social development. In this context, UNESCO's Recommendation on the Ethics of Artificial Intelligence, adopted in November 2021, has emerged as the first global normative framework for AI development and deployment. The intense negotiations of every detail of the document brought forth numerous controversies among UNESCO member states. Drawing on a unique set of primary sources, including written positions and recorded deliberations, this paper explains the achievement of global compromise on AI regulation despite the multiplicity of UNESCO member-state positions representing a variety of liberal and sovereignist preferences. Building upon Boltanski's pragmatic sociology, it conceptualises the practice of multilateral negotiations and attributes the multilateral compromise t
    
[^30]: 通过自由手写指标进行年龄群体歧视

    Age Group Discrimination via Free Handwriting Indicators. (arXiv:2309.17156v1 [cs.LG])

    [http://arxiv.org/abs/2309.17156](http://arxiv.org/abs/2309.17156)

    这项研究通过分析手写数据和计算相关指标，提出了一种使用仪器化墨水笔对老龄人群进行年龄分类的创新方法。

    

    随着全球老龄人口的增长，老弱群体的患病率预计将增加，给医疗系统带来重大挑战。老弱是一种与衰老相关的综合征，其特征是健康逐渐下降，对压力的脆弱性增加，死亡风险增加。老弱对公共卫生造成了重大负担，降低了患者生活质量。目前缺乏一种普遍接受的评估老弱程度的方法和标准化定义，这突显了一个关键的研究缺口。鉴于这一缺口和早期预防的重要性，本研究提出了一种创新方法，使用仪器化墨水笔对手写进行生态学评估，以对不同年龄群体进行分类。分析了来自80名不同年龄组（20-40岁、41-60岁、61-70岁和70+岁）的健康参与者的非图像手写数据。从原始数据中计算了14个与手势和震颤相关的指标，并在五个分类任务中使用了这些指标。

    The growing global elderly population is expected to increase the prevalence of frailty, posing significant challenges to healthcare systems. Frailty, a syndrome associated with ageing, is characterised by progressive health decline, increased vulnerability to stressors and increased risk of mortality. It represents a significant burden on public health and reduces the quality of life of those affected. The lack of a universally accepted method to assess frailty and a standardised definition highlights a critical research gap. Given this lack and the importance of early prevention, this study presents an innovative approach using an instrumented ink pen to ecologically assess handwriting for age group classification. Content-free handwriting data from 80 healthy participants in different age groups (20-40, 41-60, 61-70 and 70+) were analysed. Fourteen gesture- and tremor-related indicators were computed from the raw data and used in five classification tasks. These tasks included discr
    
[^31]: 使用大型语言模型进行定性分析可能引入严重偏见

    Using Large Language Models for Qualitative Analysis can Introduce Serious Bias. (arXiv:2309.17147v1 [cs.CL])

    [http://arxiv.org/abs/2309.17147](http://arxiv.org/abs/2309.17147)

    使用大型语言模型（LLMs）对定性分析进行注释可能引入偏见和测量误差，相比之下，使用高质量的人工注释和灵活编码对简单监督模型进行训练可以更好地减少偏见和误差。

    

    大型语言模型（LLMs）正在迅速普及，但对社会科学研究的影响尚未被很好理解。本文探讨LLMs是否能够帮助我们分析大量开放性面谈数据，以罗兴亚难民在孟加拉国科克斯巴扎的访谈记录为应用案例。我们发现，在使用LLMs对文本进行注释时需要非常谨慎，因为存在引入偏见的风险，这可能导致误导性推断。我们这里所指的偏见是技术意义上的，即LLMs在注释访谈记录时的错误不是与访谈对象的特征无关的随机误差。通过使用高质量的人工注释和灵活编码对简单监督模型进行训练，可以减少测量误差和偏见，优于LLMs的注释。因此，考虑到必须有一些高质量的注释以评估LLM是否引入偏见，我们认为最好的选择可能是使用较简单的监督模型。

    Large Language Models (LLMs) are quickly becoming ubiquitous, but the implications for social science research are not yet well understood. This paper asks whether LLMs can help us analyse large-N qualitative data from open-ended interviews, with an application to transcripts of interviews with Rohingya refugees in Cox's Bazaar, Bangladesh. We find that a great deal of caution is needed in using LLMs to annotate text as there is a risk of introducing biases that can lead to misleading inferences. We here mean bias in the technical sense, that the errors that LLMs make in annotating interview transcripts are not random with respect to the characteristics of the interview subjects. Training simpler supervised models on high-quality human annotations with flexible coding leads to less measurement error and bias than LLM annotations. Therefore, given that some high quality annotations are necessary in order to asses whether an LLM introduces bias, we argue that it is probably preferable to
    
[^32]: 原型生成: 针对数据无关解释性的稳健特征可视化

    Prototype Generation: Robust Feature Visualisation for Data Independent Interpretability. (arXiv:2309.17144v1 [cs.CV])

    [http://arxiv.org/abs/2309.17144](http://arxiv.org/abs/2309.17144)

    提出了原型生成，一种针对图像分类模型的数据无关解释性的更严格和稳健的特征可视化方法。通过生成自然激活路径的输入来反驳之前不可信的特征可视化算法的观点，并通过定量测量生成的原型的内部激活与自然图像之间的相似性来证实这一点。解释生成的原型可以揭示模型学习到的虚假相关性和偏见，这是定量方法无法识别的。

    

    我们引入了原型生成，这是一种对图像分类模型进行模型无关、数据无关解释性的更严格和更稳健的特征可视化方法。我们展示了它能够生成导致自然激活路径的输入，从而反驳了之前声称特征可视化算法不可信的观点，原因是由于其内部激活是不自然的。我们通过定量测量我们生成的原型的内部激活与自然图像之间的相似性来证实这些观点。我们还演示了如何通过解释生成的原型来获得重要的洞察，突出显示出定量方法无法识别的模型学习到的虚假相关性和偏见。

    We introduce Prototype Generation, a stricter and more robust form of feature visualisation for model-agnostic, data-independent interpretability of image classification models. We demonstrate its ability to generate inputs that result in natural activation paths, countering previous claims that feature visualisation algorithms are untrustworthy due to the unnatural internal activations. We substantiate these claims by quantitatively measuring similarity between the internal activations of our generated prototypes and natural images. We also demonstrate how the interpretation of generated prototypes yields important insights, highlighting spurious correlations and biases learned by models which quantitative methods over test-sets cannot identify.
    
[^33]: 从轻量级超分辨头部的人体姿势估计角度重新审视颅侧定位的重要性

    Revisiting Cephalometric Landmark Detection from the view of Human Pose Estimation with Lightweight Super-Resolution Head. (arXiv:2309.17143v1 [cs.CV])

    [http://arxiv.org/abs/2309.17143](http://arxiv.org/abs/2309.17143)

    本文探讨了人体姿势估计与颅侧定位的关系，并开发出基于人体姿势估计的稳健基准，以提高颅侧定位的性能。

    

    颅侧定位的精确定位在正畸学和正颌学领域具有重要意义，因其有潜力自动化关键点标记。在颅侧定位尤其是颅侧测量方面，已经观察到现有方法往往缺乏标准化的流程和设计合理的偏差减少过程，这对它们的性能有显著影响。本文重新审视了相关任务——人体姿势估计（HPE），它与颅侧定位的相似之处很多，并强调从前者领域中转移技术以有益于后者的潜力。受此启发，我们开发了一个基于知名HPE代码库MMPose的稳健且可适应的基准，该基准可以作为实现出色颅侧定位性能的可靠基准线。此外，我们在框架中引入了一个提升设计，以进一步提高性能。

    Accurate localization of cephalometric landmarks holds great importance in the fields of orthodontics and orthognathics due to its potential for automating key point labeling. In the context of landmark detection, particularly in cephalometrics, it has been observed that existing methods often lack standardized pipelines and well-designed bias reduction processes, which significantly impact their performance. In this paper, we revisit a related task, human pose estimation (HPE), which shares numerous similarities with cephalometric landmark detection (CLD), and emphasize the potential for transferring techniques from the former field to benefit the latter. Motivated by this insight, we have developed a robust and adaptable benchmark based on the well-established HPE codebase known as MMPose. This benchmark can serve as a dependable baseline for achieving exceptional CLD performance. Furthermore, we introduce an upscaling design within the framework to further enhance performance. This 
    
[^34]: 基准测试大型语言模型在RDF知识图创建和理解中的能力：LLMs如何进行Turtle语言解析？

    Benchmarking the Abilities of Large Language Models for RDF Knowledge Graph Creation and Comprehension: How Well Do LLMs Speak Turtle?. (arXiv:2309.17122v1 [cs.AI])

    [http://arxiv.org/abs/2309.17122](http://arxiv.org/abs/2309.17122)

    本论文评估了各种大型语言模型在RDF知识图创建和理解中的能力，提出了一组任务来探究模型的解析、理解、分析和创建能力，并且对商业可用和免费的离线模型进行了评估和比较。

    

    大型语言模型（LLMs）在自然语言处理和编码任务方面取得了显著进展，然而它们在表示数据的形式语言中的工作能力，特别是在知识图工程领域中，仍未得到充分研究。为了评估各种LLMs的能力，我们创建了一组五个任务，以探究它们处理Turtle语法的知识图解析、理解、分析和创建的能力。这些任务具有不同的复杂性程度，并能随问题规模而扩展，已集成到我们的自动评估系统LLM-KG-Bench中。评估包括四个商业可用的LLMs - GPT-3.5、GPT-4、Claude 1.3 和 Claude 2.0，以及两个可免费使用的离线模型 GPT4All Vicuna 和 GPT4All Falcon 13B。这项分析深入了解了LLMs在其在R中应用的优势和不足之处。

    Large Language Models (LLMs) are advancing at a rapid pace, with significant improvements at natural language processing and coding tasks. Yet, their ability to work with formal languages representing data, specifically within the realm of knowledge graph engineering, remains under-investigated. To evaluate the proficiency of various LLMs, we created a set of five tasks that probe their ability to parse, understand, analyze, and create knowledge graphs serialized in Turtle syntax. These tasks, each embodying distinct degrees of complexity and being able to scale with the size of the problem, have been integrated into our automated evaluation system, the LLM-KG-Bench. The evaluation encompassed four commercially available LLMs - GPT-3.5, GPT-4, Claude 1.3, and Claude 2.0, as well as two freely accessible offline models, GPT4All Vicuna and GPT4All Falcon 13B. This analysis offers an in-depth understanding of the strengths and shortcomings of LLMs in relation to their application within R
    
[^35]: 多关系图神经网络的元路径学习

    Meta-Path Learning for Multi-relational Graph Neural Networks. (arXiv:2309.17113v1 [cs.LG])

    [http://arxiv.org/abs/2309.17113](http://arxiv.org/abs/2309.17113)

    这项工作提出了一种新方法来学习具有高准确性的元路径和元路径图神经网络，关键是使用评分函数来衡量关系的潜在信息量。在实验中，该方法在合成和真实世界实验中表现出比现有的多关系GNNs更好的性能。

    

    现有的多关系图神经网络使用两种策略来确定信息相关的关系：要么将这个问题简化为低级权重学习，要么依赖于手工设计的关系依赖链，称为元路径。然而，前一种方法在存在大量关系的情况下（例如，知识图谱）面临挑战，而后一种方法需要大量领域专业知识来确定相关的元路径。在这项工作中，我们提出了一种新方法来学习元路径和元路径图神经网络，这些网络基于少量有信息量的元路径具有高准确性。我们方法的关键要素是一个评分函数，用于衡量在元路径的增量构建中关系的潜在信息量。我们的实验评估表明，即使有大量关系，我们的方法仍能正确识别相关的元路径，并在合成和真实世界实验中明显优于现有的多关系GNNs。

    Existing multi-relational graph neural networks use one of two strategies for identifying informative relations: either they reduce this problem to low-level weight learning, or they rely on handcrafted chains of relational dependencies, called meta-paths. However, the former approach faces challenges in the presence of many relations (e.g., knowledge graphs), while the latter requires substantial domain expertise to identify relevant meta-paths. In this work we propose a novel approach to learn meta-paths and meta-path GNNs that are highly accurate based on a small number of informative meta-paths. Key element of our approach is a scoring function for measuring the potential informativeness of a relation in the incremental construction of the meta-path. Our experimental evaluation shows that the approach manages to correctly identify relevant meta-paths even with a large number of relations, and substantially outperforms existing multi-relational GNNs on synthetic and real-world exper
    
[^36]: 通过决策规则进行模型比较的动态可解释性

    Dynamic Interpretability for Model Comparison via Decision Rules. (arXiv:2309.17095v1 [cs.LG])

    [http://arxiv.org/abs/2309.17095](http://arxiv.org/abs/2309.17095)

    本文提出了DeltaXplainer，一种模型无关的方法，用于描述两个二元分类器之间的差异。通过实验验证了DeltaXplainer在涉及不同类型概念漂移的各种模型比较场景中的有效性。

    

    可解释的人工智能（XAI）方法大多被用来研究和阐明单个机器学习模型，并没有被设计成能够有效捕捉和解释多个模型之间的差异。这篇论文解决了理解和解释机器学习模型之间差异的挑战，对于模型选择、监控和生命周期管理在现实世界应用中至关重要。我们提出了DeltaXplainer，一种模型无关的方法，用于生成基于规则的解释，描述两个二元分类器之间的差异。为了评估DeltaXplainer的有效性，我们在合成和实际数据集上进行了实验，涵盖了涉及不同类型概念漂移的各种模型比较场景。

    Explainable AI (XAI) methods have mostly been built to investigate and shed light on single machine learning models and are not designed to capture and explain differences between multiple models effectively. This paper addresses the challenge of understanding and explaining differences between machine learning models, which is crucial for model selection, monitoring and lifecycle management in real-world applications. We propose DeltaXplainer, a model-agnostic method for generating rule-based explanations describing the differences between two binary classifiers. To assess the effectiveness of DeltaXplainer, we conduct experiments on synthetic and real-world datasets, covering various model comparison scenarios involving different types of concept drift.
    
[^37]: GAIA-1: 一种用于自动驾驶的生成世界模型

    GAIA-1: A Generative World Model for Autonomous Driving. (arXiv:2309.17080v1 [cs.CV])

    [http://arxiv.org/abs/2309.17080](http://arxiv.org/abs/2309.17080)

    GAIA-1是一个使用视频、文本和动作输入生成逼真驾驶场景的生成世界模型，能够精细控制自车行为和场景特征，并在学习上下文感知、泛化能力和几何理解方面取得了显著成果。

    

    自动驾驶承诺为交通带来变革性的改进，但构建能够安全导航真实世界场景的系统仍然具有挑战性。一个关键问题在于有效预测世界随着车辆行为的演变可能出现的各种潜在结果。为了解决这个挑战，我们介绍了GAIA-1（生成智能与自主驾驶），一种生成世界模型，它利用视频、文本和动作输入生成逼真的驾驶场景，同时对自车行为和场景特征提供精细控制。我们的方法将世界建模视为一个无监督的序列建模问题，通过将输入映射到离散标记，并预测序列中的下一个标记来进行。我们的模型从中获得的特征包括学习高级结构和场景动态、上下文感知、泛化能力和几何理解。GAIA-1学到的表示的威力

    Autonomous driving promises transformative improvements to transportation, but building systems capable of safely navigating the unstructured complexity of real-world scenarios remains challenging. A critical problem lies in effectively predicting the various potential outcomes that may emerge in response to the vehicle's actions as the world evolves.  To address this challenge, we introduce GAIA-1 ('Generative AI for Autonomy'), a generative world model that leverages video, text, and action inputs to generate realistic driving scenarios while offering fine-grained control over ego-vehicle behavior and scene features. Our approach casts world modeling as an unsupervised sequence modeling problem by mapping the inputs to discrete tokens, and predicting the next token in the sequence. Emerging properties from our model include learning high-level structures and scene dynamics, contextual awareness, generalization, and understanding of geometry. The power of GAIA-1's learned representati
    
[^38]: SCALE: 不对称语言翻译引擎的协同合作

    SCALE: Synergized Collaboration of Asymmetric Language Translation Engines. (arXiv:2309.17061v1 [cs.CL])

    [http://arxiv.org/abs/2309.17061](http://arxiv.org/abs/2309.17061)

    SCALE是一种将专业翻译模型和大型语言模型连接为统一翻译引擎的协同框架，通过引入STM的翻译进一步提升LLM的性能，从而在低资源环境中显著优于其他模型。

    

    本文介绍了SCALE，一种将紧凑的专业翻译模型（STM）和通用的大型语言模型（LLM）连接为统一翻译引擎的协同框架。通过将STM的翻译引入三元组的上下文演示中，SCALE实现了LLM的细化和枢轴能力，从而减轻了LLM的语言偏差和STM的并行数据偏差，在不牺牲广泛性的前提下增强了LLM的专业性，促进了无需昂贵LLM精调的持续学习。我们的综合实验结果表明，在挑战性的低资源环境中，SCALE明显优于少样本LLMs（GPT-4）和专业模型（NLLB）。此外，在Xhosa到英语的翻译中，SCALE的性能持续提升4个BLEURT分数，当配备仅600M参数的紧凑模型时，SCALE在COMET分数上超过了少样本GPT-4的2.5个分数和BLEURT分数上3.8个分数。SCALE还能有效地进行解释。

    In this paper, we introduce SCALE, a collaborative framework that connects compact Specialized Translation Models (STMs) and general-purpose Large Language Models (LLMs) as one unified translation engine. By introducing translation from STM into the triplet in-context demonstrations, SCALE unlocks refinement and pivoting ability of LLM, thus mitigating language bias of LLM and parallel data bias of STM, enhancing LLM speciality without sacrificing generality, and facilitating continual learning without expensive LLM fine-tuning. Our comprehensive experiments show that SCALE significantly outperforms both few-shot LLMs (GPT-4) and specialized models (NLLB) in challenging low-resource settings. Moreover, in Xhosa to English translation, SCALE experiences consistent improvement by a 4 BLEURT score without tuning LLM and surpasses few-shot GPT-4 by 2.5 COMET score and 3.8 BLEURT score when equipped with a compact model consisting of merely 600M parameters. SCALE could also effectively expl
    
[^39]: 讲给我听！基于大型语言模型的叙事驱动可解释人工智能

    Tell Me a Story! Narrative-Driven XAI with Large Language Models. (arXiv:2309.17057v1 [cs.AI])

    [http://arxiv.org/abs/2309.17057](http://arxiv.org/abs/2309.17057)

    这项研究提出了基于大型语言模型的XAIstories框架，通过叙事方式解释AI预测，其中SHAPstories基于SHAP解释解释预测得分，CFstories基于CF解释解释决策。研究结果表明，超过90%的普通读者认可SHAPstories生成的叙事的说服力，92%的数据科学家认为SHAPstories能够提高非专业人士的易用性和信心。

    

    在当今重要领域中，黑盒机器学习模型的盛行加大了对可解释人工智能（XAI）的需求。广泛使用的SHAP值虽然量化了特征重要性，但往往过于复杂，缺乏人性化的解释。此外，反事实（CF）解释展示了“如果”但没有解释“为什么”。为了弥合这一差距，我们引入了XAIstories。利用大型语言模型，XAIstories提供了叙事来阐明AI预测：基于SHAP解释的SHAPstories解释预测得分，而基于CF解释的CFstories解释决策。我们的结果令人震惊：超过90%的调查普通读者认为SHAPstories生成的叙事是有说服力的。数据科学家主要认为SHAPstories在向普通读者传达解释方面具有价值，92%的数据科学家表示这将有助于非专业人士的易用性和信心。

    In today's critical domains, the predominance of black-box machine learning models amplifies the demand for Explainable AI (XAI). The widely used SHAP values, while quantifying feature importance, are often too intricate and lack human-friendly explanations. Furthermore, counterfactual (CF) explanations present `what ifs' but leave users grappling with the 'why'. To bridge this gap, we introduce XAIstories. Leveraging Large Language Models, XAIstories provide narratives that shed light on AI predictions: SHAPstories do so based on SHAP explanations to explain a prediction score, while CFstories do so for CF explanations to explain a decision. Our results are striking: over 90% of the surveyed general audience finds the narrative generated by SHAPstories convincing. Data scientists primarily see the value of SHAPstories in communicating explanations to a general audience, with 92% of data scientists indicating that it will contribute to the ease and confidence of nonspecialists in under
    
[^40]: 关于稳健和准确分类器连续性的研究

    On Continuity of Robust and Accurate Classifiers. (arXiv:2309.17048v1 [cs.LG])

    [http://arxiv.org/abs/2309.17048](http://arxiv.org/abs/2309.17048)

    本文研究了稳健和准确分类器的连续性，提出了当假设连续时，其稳健性和准确性是不兼容的观点。

    

    学习模型的可靠性是成功应用机器学习于各种领域的关键。创建一个稳健的模型，特别是一个不受对抗攻击影响的模型，需要全面理解对抗样本现象。然而，由于机器学习中问题的复杂性，很难描述这一现象。在文献中已经证明了对抗性训练可以提高假设的稳健性，但这种改进是以自然样本性能下降为代价的。因此，有人提出假设的稳健性和准确性是相互矛盾的。本文提出了另一种观点，假设的连续性与其稳健性和准确性是不兼容的。换句话说，连续函数不能有效地学习最佳稳健假设。为此，我们将引入一个系统研究谐波和ho的框架。

    The reliability of a learning model is key to the successful deployment of machine learning in various applications. Creating a robust model, particularly one unaffected by adversarial attacks, requires a comprehensive understanding of the adversarial examples phenomenon. However, it is difficult to describe the phenomenon due to the complicated nature of the problems in machine learning. It has been shown that adversarial training can improve the robustness of the hypothesis. However, this improvement comes at the cost of decreased performance on natural samples. Hence, it has been suggested that robustness and accuracy of a hypothesis are at odds with each other. In this paper, we put forth the alternative proposal that it is the continuity of a hypothesis that is incompatible with its robustness and accuracy. In other words, a continuous function cannot effectively learn the optimal robust hypothesis. To this end, we will introduce a framework for a rigorous study of harmonic and ho
    
[^41]: 改进的解析、演化和随机循环神经网络的科尔莫哥洛夫复杂性

    Refined Kolmogorov Complexity of Analog, Evolving and Stochastic Recurrent Neural Networks. (arXiv:2309.17032v1 [cs.CC])

    [http://arxiv.org/abs/2309.17032](http://arxiv.org/abs/2309.17032)

    本研究通过对解析、演化和随机神经网络的科尔莫哥洛夫复杂性进行刻画，提供了对其超图灵计算能力的精确理解，发现了解析网络、演化网络和随机网络之间的层级结构。

    

    我们通过解析权重、演化权重和实际概率的科尔莫哥洛夫复杂性，提供了对解析、演化和随机神经网络超图灵计算能力的精确刻画。首先，我们得到了一系列解析网络的无穷层级，这些层级是通过其底层实际权重的科尔莫哥洛夫复杂性定义的。这些层级位于复杂性类 $\mathbf{P}$ 和 $\mathbf{P/poly}$ 之间。然后，我们将这个结果推广到演化网络。我们得到了一系列基于科尔莫哥洛夫复杂性的演化网络的层级结构，这些层级也位于 $\mathbf{P}$ 和 $\mathbf{P/poly}$ 之间。最后，我们将这些结果扩展到使用实际概率作为随机性源的随机网络。因此，我们得到了一系列基于概率的随机网络的无穷层级，这些层级在科尔莫哥洛夫复杂性上建立了桥梁。

    We provide a refined characterization of the super-Turing computational power of analog, evolving, and stochastic neural networks based on the Kolmogorov complexity of their real weights, evolving weights, and real probabilities, respectively. First, we retrieve an infinite hierarchy of classes of analog networks defined in terms of the Kolmogorov complexity of their underlying real weights. This hierarchy is located between the complexity classes $\mathbf{P}$ and $\mathbf{P/poly}$. Then, we generalize this result to the case of evolving networks. A similar hierarchy of Kolomogorov-based complexity classes of evolving networks is obtained. This hierarchy also lies between $\mathbf{P}$ and $\mathbf{P/poly}$. Finally, we extend these results to the case of stochastic networks employing real probabilities as source of randomness. An infinite hierarchy of stochastic networks based on the Kolmogorov complexity of their probabilities is therefore achieved. In this case, the hierarchy bridges
    
[^42]: 可扩展的多时期遥感变化数据生成方法通过模拟随机变化过程

    Scalable Multi-Temporal Remote Sensing Change Data Generation via Simulating Stochastic Change Process. (arXiv:2309.17031v1 [cs.CV])

    [http://arxiv.org/abs/2309.17031](http://arxiv.org/abs/2309.17031)

    本文提出了一种通过生成模型实现的可扩展的多时期遥感变化数据生成方法，通过模拟随机变化过程，解决了收集、预处理和注释多时期遥感图像的难题。

    

    了解地表的时态动态是多时期遥感图像分析的任务，深度视觉模型通过标记的多时期图像大大推动了这一任务的进展。然而，大规模收集、预处理和注释多时期遥感图像并不容易，因为这需要花费大量的资源和知识。在本文中，我们提出了一种通过生成模型实现的可扩展的多时期遥感变化数据生成方法，该方法既廉价又自动化，从而缓解了这些问题。我们的主要思想是通过时间上的随机变化过程进行模拟。我们将随机变化过程视为概率语义状态转换，即生成概率变化模型（GPCM），将复杂的模拟问题分解为两个更易追踪的子问题，即变化事件模拟和语义变化合成。为了解决这两个问题，我们提出了变化生成器（Changen），这是一种基于GAN的GPCM，可以实现对生成过程的控制。

    Understanding the temporal dynamics of Earth's surface is a mission of multi-temporal remote sensing image analysis, significantly promoted by deep vision models with its fuel -- labeled multi-temporal images. However, collecting, preprocessing, and annotating multi-temporal remote sensing images at scale is non-trivial since it is expensive and knowledge-intensive. In this paper, we present a scalable multi-temporal remote sensing change data generator via generative modeling, which is cheap and automatic, alleviating these problems. Our main idea is to simulate a stochastic change process over time. We consider the stochastic change process as a probabilistic semantic state transition, namely generative probabilistic change model (GPCM), which decouples the complex simulation problem into two more trackable sub-problems, \ie, change event simulation and semantic change synthesis. To solve these two problems, we present the change generator (Changen), a GAN-based GPCM, enabling contro
    
[^43]: 作为评估器的大型语言模型中认知偏差的基准测试

    Benchmarking Cognitive Biases in Large Language Models as Evaluators. (arXiv:2309.17012v1 [cs.CL])

    [http://arxiv.org/abs/2309.17012](http://arxiv.org/abs/2309.17012)

    本研究对15个不同大小的大型语言模型进行了评估，发现它们作为评估器存在认知偏差，尤其在文本质量评估中表现出较强的偏见，这对其鲁棒性提出了质疑。同时，研究还发现了人类和机器偏好之间的相关性。

    

    最近的研究表明，大型语言模型（LLMs）通过简单的提示和上下文学习作为自动评估器非常有效。本研究组装了15个大小不同的LLMs，并通过其他LLMs的偏好排名来评估它们的输出响应，例如System Star比System Square更好。然后，我们引入了用于评估LLMs输出中六种不同认知偏差的认知偏差基准测试（CoBBLEr），如自我中心偏差，即模型更喜欢将自己的输出在评估中排名较高。我们发现LLMs是有偏见的文本质量评估器，在每个评估中都表现出对我们偏见基准的强烈迹象（在所有模型上的平均比较约为40%），这对它们作为评估器的鲁棒性提出了质疑。此外，我们还研究了人类和机器偏好之间的相关性，并计算了平均的Rank-Biased O值。

    Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased O
    
[^44]: 医学基础模型易受有针对性的错误信息攻击

    Medical Foundation Models are Susceptible to Targeted Misinformation Attacks. (arXiv:2309.17007v1 [cs.LG])

    [http://arxiv.org/abs/2309.17007](http://arxiv.org/abs/2309.17007)

    本研究揭示了医学领域中大型语言模型(LLM)的脆弱性，通过操纵模型权重的很小比例，可以故意注入错误的生物医学事实，并且这些错误信息会被模型输出传播。面对这种易受攻击性，我们需要采取措施确保这些模型在医疗实践中的可靠和安全使用。

    

    大型语言模型(LLM)在医学领域拥有广泛的知识，并能够跨越多个领域进行医学信息的推理，对未来的医学应用具有很大的潜力。在这项研究中，我们展示了LLM在医学领域存在的一个令人担忧的脆弱性。通过有针对性地操纵模型权重的1.1％，我们可以故意注入一个错误的生物医学事实。错误信息会在模型的输出中传播，同时对其他生物医学任务的性能没有影响。我们在一组1,038个错误的生物医学事实中验证了我们的发现。这种特殊的易受攻击性引发了人们对在医疗环境中应用LLM的安全性和可信度的严重关注。这凸显了确保这些模型在医学实践中可靠和安全使用的需求，需要采取强有力的保护措施，进行彻底的验证机制，并对这些模型的访问进行严格管理。

    Large language models (LLMs) have broad medical knowledge and can reason about medical information across many domains, holding promising potential for diverse medical applications in the near future. In this study, we demonstrate a concerning vulnerability of LLMs in medicine. Through targeted manipulation of just 1.1% of the model's weights, we can deliberately inject an incorrect biomedical fact. The erroneous information is then propagated in the model's output, whilst its performance on other biomedical tasks remains intact. We validate our findings in a set of 1,038 incorrect biomedical facts. This peculiar susceptibility raises serious security and trustworthiness concerns for the application of LLMs in healthcare settings. It accentuates the need for robust protective measures, thorough verification mechanisms, and stringent management of access to these models, ensuring their reliable and safe use in medical practice.
    
[^45]: 理解和减轻预训练中的标签噪声对下游任务的影响

    Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks. (arXiv:2309.17002v1 [cs.LG])

    [http://arxiv.org/abs/2309.17002](http://arxiv.org/abs/2309.17002)

    本文研究了深度学习中预训练数据中的标签噪声对下游任务的影响，并通过在合成噪声数据集上的实验证明，在预训练中的轻微噪声可以提高领域内的性能，但会损害领域外的性能。为了减轻噪声的影响，提出了一种轻量级的黑盒调整方法（NMTune）。

    

    在深度学习中，先在大规模数据集上进行预训练，然后在下游任务上进行微调已经成为一种标准做法。然而，预训练数据通常包含标签噪声，这可能对模型的泛化能力产生不利影响。本文旨在了解预训练数据集中噪声的性质，并减轻其对下游任务的影响。具体而言，通过在合成噪声的ImageNet-1K和YFCC15M数据集上进行大量实验，我们证明在预训练中的轻微噪声可以促进领域内的转移性能，即训练和测试数据具有相同的分布；然而，它总是会损害领域外的性能，即训练和测试数据具有不同的分布。我们通过实验证实，预训练中的噪声会不同地塑造特征空间。然后我们提出了一种轻量级的黑盒调整方法（NMTune）来使特征空间达到映射并减轻噪声的影响。

    Pre-training on large-scale datasets and then fine-tuning on downstream tasks have become a standard practice in deep learning. However, pre-training data often contain label noise that may adversely affect the generalization of the model. This paper aims to understand the nature of noise in pre-training datasets and to mitigate its impact on downstream tasks. More specifically, through extensive experiments of supervised pre-training models on synthetic noisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise in pre-training can benefit in-domain (ID) transfer performance, where the training and testing data share the same distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing data distribution are different. We empirically verify that the reason behind is noise in pre-training shapes the feature space differently. We then propose a lightweight black-box tuning method (NMTune) to affine the feature space to mitigate the m
    
[^46]: 对轴承故障分类方法的深入研究

    A Closer Look at Bearing Fault Classification Approaches. (arXiv:2309.17001v1 [cs.LG])

    [http://arxiv.org/abs/2309.17001](http://arxiv.org/abs/2309.17001)

    这项研究对轴承故障分类方法进行了深入研究，关注了轴承故障诊断的重要性，以及使用机器学习和振动数据来预测故障的方法。

    

    近年来，由于轴承存在于各个行业的旋转设备中，并且对高效运营的需求不断增加，轴承故障诊断引起了越来越多的关注。及时检测和准确预测轴承故障可以帮助减少意外机器停机的可能性，并改进维护计划，从而避免损失产能。最近的技术进步使得可以使用各种传感器对这些设备的健康状况进行规模化监测，并使用现代机器学习方法，包括深度学习架构来预测故障。通过加速运行至故障的过程中采集振动数据，或者通过引入已知故障来采集振动数据，可以在各种工作条件下（如转速、轴承负载、轴承故障类型和数据采集频率）进行数据采集。然而，在使用振动数据开发轴承故障分类模型的过程中存在以下问题。

    Rolling bearing fault diagnosis has garnered increased attention in recent years owing to its presence in rotating machinery across various industries, and an ever increasing demand for efficient operations. Prompt detection and accurate prediction of bearing failures can help reduce the likelihood of unexpected machine downtime and enhance maintenance schedules, averting lost productivity. Recent technological advances have enabled monitoring the health of these assets at scale using a variety of sensors, and predicting the failures using modern Machine Learning (ML) approaches including deep learning architectures. Vibration data has been collected using accelerated run-to-failure of overloaded bearings, or by introducing known failure in bearings, under a variety of operating conditions such as rotating speed, load on the bearing, type of bearing fault, and data acquisition frequency. However, in the development of bearing failure classification models using vibration data there is 
    
[^47]: 深度强化学习在控制系统中的可靠性量化

    Reliability Quantification of Deep Reinforcement Learning-based Control. (arXiv:2309.16977v1 [eess.SY])

    [http://arxiv.org/abs/2309.16977](http://arxiv.org/abs/2309.16977)

    本论文提出了一种用于量化深度强化学习控制系统可靠性的方法，通过使用两个神经网络来评估控制的可靠性差异。

    

    深度强化学习（DRL）在安全关键系统中的可靠性量化是人工智能在实际应用中的重要挑战。本研究提出了一种用于量化DRL控制可靠性的方法。首先，应用了一种现有方法——随机噪声提取，以明确需要解决的问题。其次，提出了一种新的可靠性量化方法来解决这些问题。该方法使用两个神经网络来量化可靠性：参考网络和评估网络。它们具有相同的结构和相同的初始参数。在训练之前，两个网络的输出相同。在训练过程中，评估网络的参数被更新，以最大化训练数据上的参考网络和评估网络之间的差异。因此，可以基于两个网络的输出差异评估特定状态下DRL控制的可靠性。

    Reliability quantification of deep reinforcement learning (DRL)-based control is a significant challenge for the practical application of artificial intelligence (AI) in safety-critical systems. This study proposes a method for quantifying the reliability of DRL-based control. First, an existing method, random noise distillation, was applied to the reliability evaluation to clarify the issues to be solved. Second, a novel method for reliability quantification was proposed to solve these issues. The reliability is quantified using two neural networks: reference and evaluator. They have the same structure with the same initial parameters. The outputs of the two networks were the same before training. During training, the evaluator network parameters were updated to maximize the difference between the reference and evaluator networks for trained data. Thus, the reliability of the DRL-based control for a state can be evaluated based on the difference in output between the two networks. The
    
[^48]: 基于差分驱动增强学习的量子态制备方法

    A Quantum States Preparation Method Based on Difference-Driven Reinforcement Learning. (arXiv:2309.16972v1 [quant-ph])

    [http://arxiv.org/abs/2309.16972](http://arxiv.org/abs/2309.16972)

    本文提出了一种基于差分驱动增强学习的量子态制备方法，改进了奖励函数和行为选择策略，以提高两比特量子系统的制备速度和保真度。

    

    由于两比特量子系统的状态空间较大，并且现有量子态制备方法采用阶梯型奖励函数，收敛速度慢，在有限条件下很难高保真度制备所需的目标量子态。为解决上述问题，本文提出了一种改进奖励函数和行为选择策略的差分驱动增强学习算法，用于两比特量子系统的量子态制备。首先，构建了一个模型，包括对量子门类型和量子态演化时间的限制。在制备过程中，设计了一种加权差分动态奖励函数，辅助算法快速获得最大期望累积奖励。然后，采取自适应ε-greedy行为选择策略，以在探索和利用之间取得一定平衡。

    Due to the large state space of the two-qubit system, and the adoption of ladder reward function in the existing quantum state preparation methods, the convergence speed is slow and it is difficult to prepare the desired target quantum state with high fidelity under limited conditions. To solve the above problems, a difference-driven reinforcement learning (RL) algorithm for quantum state preparation of two-qubit system is proposed by improving the reward function and action selection strategy. Firstly, a model is constructed for the problem of preparing quantum states of a two-qubit system, with restrictions on the type of quantum gates and the time for quantum state evolution. In the preparation process, a weighted differential dynamic reward function is designed to assist the algorithm quickly obtain the maximum expected cumulative reward. Then, an adaptive e-greedy action selection strategy is adopted to achieve a balance between exploration and utilization to a certain extent, the
    
[^49]: 具有广义可加效用网络的离散选择模型

    Discrete-Choice Model with Generalized Additive Utility Network. (arXiv:2309.16970v1 [cs.AI])

    [http://arxiv.org/abs/2309.16970](http://arxiv.org/abs/2309.16970)

    本论文提出了一种基于广义可加模型的神经网络架构，称为广义可加效用网络（GAUNet），用于离散选择模型。这些模型在预测准确性上可以与ASU-DNN相媲美，并且相比以前的模型具有更好的解释性。

    

    离散选择模型是分析决策行为的强大框架，为政策制定者和企业提供有价值的见解。在实践中，使用线性效用函数的多项式逻辑模型（MNLs）因其易于使用和可解释性而被广泛使用。最近，已经开发了具有神经网络（例如ASU-DNN）的MNLs，并且在行为选择的预测精度上比传统MNLs更高。然而，这些模型由于复杂结构而缺乏解释性。我们基于广义可加模型开发了一种具有新颖神经网络架构的效用函数，称为广义可加效用网络（GAUNet），用于离散选择模型。我们使用在东京收集的出行调查数据评估了具有GAUNet的MNL的性能。我们的模型在准确性上与ASU-DNN相当，并且相比以前的模型具有改进的解释性。

    Discrete-choice models are a powerful framework for analyzing decision-making behavior to provide valuable insights for policymakers and businesses. Multinomial logit models (MNLs) with linear utility functions have been used in practice because they are ease to use and interpretable. Recently, MNLs with neural networks (e.g., ASU-DNN) have been developed, and they have achieved higher prediction accuracy in behavior choice than classical MNLs. However, these models lack interpretability owing to complex structures. We developed utility functions with a novel neural-network architecture based on generalized additive models, named generalized additive utility network ( GAUNet), for discrete-choice models. We evaluated the performance of the MNL with GAUNet using the trip survey data collected in Tokyo. Our models were comparable to ASU-DNN in accuracy and exhibited improved interpretability compared to previous models.
    
[^50]: 生成强化学习策略解释的实证研究

    On Generating Explanations for Reinforcement Learning Policies: An Empirical Study. (arXiv:2309.16960v1 [cs.AI])

    [http://arxiv.org/abs/2309.16960](http://arxiv.org/abs/2309.16960)

    本文通过引入一组线性时态逻辑（LTL）公式，介绍了一种生成强化学习策略解释的方法，并展示了其在模拟夺旗环境中的有效性。

    

    本文引入了一组设计用于提供策略解释的线性时态逻辑（LTL）公式。我们的重点是构建既阐明策略所实现的最终目标又阐明其执行过程中所维持的前提条件的解释。这些基于LTL的解释具有结构化表示，特别适用于局部搜索技术。通过模拟的夺旗环境，证明了我们提出的方法的有效性。论文最后提出了未来研究的建议方向。

    In this paper, we introduce a set of \textit{Linear Temporal Logic} (LTL) formulae designed to provide explanations for policies. Our focus is on crafting explanations that elucidate both the ultimate objectives accomplished by the policy and the prerequisites it upholds throughout its execution. These LTL-based explanations feature a structured representation, which is particularly well-suited for local-search techniques. The effectiveness of our proposed approach is illustrated through a simulated capture the flag environment. The paper concludes with suggested directions for future research.
    
[^51]: 去噪扩散桥模型

    Denoising Diffusion Bridge Models. (arXiv:2309.16948v1 [cs.CV])

    [http://arxiv.org/abs/2309.16948](http://arxiv.org/abs/2309.16948)

    本论文提出了一种去噪扩散桥模型（DDBMs），该模型通过学习扩散桥的分数，并基于学习到的分数求解微分方程来实现从一个分布到另一个分布的映射，从而将多类生成模型统一起来。

    

    扩散模型是强大的生成模型，它使用随机过程将噪声映射到数据。然而，对于许多应用，如图像编辑，模型的输入不是随机噪声的分布。因此，扩散模型必须依赖于繁琐的方法，如引导或投影采样，以在生成过程中加入这些信息。在我们的工作中，我们提出了去噪扩散桥模型（DDBMs），这是一种基于扩散桥的范例，扩散桥是一族过程，其在给定的端点下插值两个配对分布。我们的方法通过学习扩散桥的分数，基于学习到的分数求解一个（随机的）微分方程，从一个端点分布映射到另一个端点分布。我们的方法自然地统一了几类生成模型，如基于分数的扩散模型和OT-Flow-Matching，使我们能够将现有的设计和架构选择适应到更通用的模型中。

    Diffusion models are powerful generative models that map noise to data using stochastic processes. However, for many applications such as image editing, the model input comes from a distribution that is not random noise. As such, diffusion models must rely on cumbersome methods like guidance or projected sampling to incorporate this information in the generative process. In our work, we propose Denoising Diffusion Bridge Models (DDBMs), a natural alternative to this paradigm based on diffusion bridges, a family of processes that interpolate between two paired distributions given as endpoints. Our method learns the score of the diffusion bridge from data and maps from one endpoint distribution to the other by solving a (stochastic) differential equation based on the learned score. Our method naturally unifies several classes of generative models, such as score-based diffusion models and OT-Flow-Matching, allowing us to adapt existing design and architectural choices to our more general 
    
[^52]: 鲁棒的异步协作式鸟瞰视角三维检测

    Robust Asynchronous Collaborative 3D Detection via Bird's Eye View Flow. (arXiv:2309.16940v1 [cs.CV])

    [http://arxiv.org/abs/2309.16940](http://arxiv.org/abs/2309.16940)

    CoBEVFlow是一种鲁棒的异步协作式三维检测系统，通过补偿智能体之间的异步协作信息对齐来解决信息不匹配问题。

    

    通过促进多个智能体之间的沟通，协作感知可以大大提升每个智能体的感知能力。然而，在现实世界中，由于通信延迟、中断和时钟不对齐，智能体之间的时间不同步是不可避免的。这个问题在多智能体融合过程中导致信息不匹配，严重动摇了协作的基础。为了解决这个问题，我们提出了CoBEVFlow，一种基于鸟瞰视角流的异步鲁棒协作式三维感知系统。CoBEVFlow的关键观点是通过补偿运动来使多个智能体发送的异步协作信息对齐。为了建模场景中的运动，我们提出了鸟瞰视角流，它是与每个空间位置对应的运动向量的集合。基于鸟瞰视角流，异步感知特征可以重新分配到适当的位置，减轻异步的影响。CoBEVFlow具有两个优点：(i) CoBEVFlow可以处理异步协作

    By facilitating communication among multiple agents, collaborative perception can substantially boost each agent's perception ability. However, temporal asynchrony among agents is inevitable in real-world due to communication delays, interruptions, and clock misalignments. This issue causes information mismatch during multi-agent fusion, seriously shaking the foundation of collaboration. To address this issue, we propose CoBEVFlow, an asynchrony-robust collaborative 3D perception system based on bird's eye view (BEV) flow. The key intuition of CoBEVFlow is to compensate motions to align asynchronous collaboration messages sent by multiple agents. To model the motion in a scene, we propose BEV flow, which is a collection of the motion vector corresponding to each spatial location. Based on BEV flow, asynchronous perceptual features can be reassigned to appropriate positions, mitigating the impact of asynchrony. CoBEVFlow has two advantages: (i) CoBEVFlow can handle asynchronous collabor
    
[^53]: PC-Adapter: 基于拟合伪标签的点云领域自适应的拓扑感知适配器

    PC-Adapter: Topology-Aware Adapter for Efficient Domain Adaption on Point Clouds with Rectified Pseudo-label. (arXiv:2309.16936v1 [cs.CV])

    [http://arxiv.org/abs/2309.16936](http://arxiv.org/abs/2309.16936)

    PC-Adapter是一种拓扑感知适配器，通过保留全局形状信息和学习目标领域的局部特征，实现了点云领域自适应。还提出了一种分层扩展的伪标签矫正策略，以解决伪标签的误类问题。

    

    由于物体尺度，传感器角度和自遮挡引起的数据分布变化，理解从真实世界中捕获的点云具有挑战性。以往的研究通过结合自监督学习，自训练和对抗训练等最新学习原则来解决这个问题，但这导致了显著的计算开销。为了简洁而强大地对点云进行领域自适应，我们重新思考了点云数据在领域偏移场景下的独特挑战，并发现保持源领域的全局几何形状信息以及目标伪标签趋势对源标签分布有偏的重要性。在我们的观察下，我们提出了一个适配器引导的领域自适应方法PC-Adapter，通过基于注意力机制的适配器保留源领域的全局形状信息，同时通过另一个配备了图卷积的适配器学习目标领域的局部特征。此外，我们提出了一种分层扩展的伪标签矫正策略，有效减少伪标签的误类问题。

    Understanding point clouds captured from the real-world is challenging due to shifts in data distribution caused by varying object scales, sensor angles, and self-occlusion. Prior works have addressed this issue by combining recent learning principles such as self-supervised learning, self-training, and adversarial training, which leads to significant computational overhead.Toward succinct yet powerful domain adaptation for point clouds, we revisit the unique challenges of point cloud data under domain shift scenarios and discover the importance of the global geometry of source data and trends of target pseudo-labels biased to the source label distribution. Motivated by our observations, we propose an adapter-guided domain adaptation method, PC-Adapter, that preserves the global shape information of the source domain using an attention-based adapter, while learning the local characteristics of the target domain via another adapter equipped with graph convolution. Additionally, we propo
    
[^54]: TranDRL：一种基于Transformer驱动的深度强化学习支持的预防性维护框架

    TranDRL: A Transformer-Driven Deep Reinforcement Learning Enabled Prescriptive Maintenance Framework. (arXiv:2309.16935v1 [cs.LG])

    [http://arxiv.org/abs/2309.16935](http://arxiv.org/abs/2309.16935)

    TranDRL是一种基于Transformer驱动的深度强化学习支持的预防性维护框架，结合了复杂时间模式捕捉和经济高效维护建议，显著提高了剩余寿命（RUL）预测准确性和维护行动优化。

    

    工业系统需要可靠的预测性维护策略来提高运营效率并减少停机时间。本文介绍了一种新颖的综合框架，利用Transformer神经网络和深度强化学习（DRL）算法来优化维护行动。我们的方法采用Transformer模型来有效捕捉传感器数据中的复杂时间模式，从而准确预测设备的剩余寿命（RUL）。同时，我们框架中的DRL组件提供了经济高效和及时的维护建议。我们在NASA C-MPASS数据集上验证了我们框架的有效性，结果显示在RUL预测准确性和维护行动优化方面取得了显著进展。因此，我们的创新方法为预防性维护提供了一种创新的数据驱动方法，解决了工业运营中的关键挑战，并带来了更多发展机遇。

    Industrial systems demand reliable predictive maintenance strategies to enhance operational efficiency and reduce downtime. This paper introduces a novel, integrated framework that leverages the power of transformer neural networks and deep reinforcement learning (DRL) algorithms to optimize maintenance actions. Our approach employs the transformer model to effectively capture complex temporal patterns in sensor data, thereby accurately predicting the Remaining Useful Life (RUL) of equipment. Simultaneously, the DRL component of our framework provides cost-effective and timely maintenance recommendations. We validate the efficacy of our framework on the NASA C-MPASS dataset, where it demonstrates significant advancements in both RUL prediction accuracy and the optimization of maintenance actions. Consequently, our pioneering approach provides an innovative data-driven methodology for prescriptive maintenance, addressing key challenges in industrial operations and leading the way to mor
    
[^55]: 学习接受帮助：干预感知的概念嵌入模型

    Learning to Receive Help: Intervention-Aware Concept Embedding Models. (arXiv:2309.16928v1 [cs.LG])

    [http://arxiv.org/abs/2309.16928](http://arxiv.org/abs/2309.16928)

    这项研究提出了一种干预感知的概念嵌入模型，用于提高神经架构对概念干预的响应性，并解决了概念干预顺序和模型架构的依赖性的问题。

    

    概念瓶颈模型（CBMs）通过使用一组高级概念构建和解释神经架构的预测，以解决其不透明性的问题。这些模型的一个特殊属性是它们允许概念干预，用户可以纠正被错误预测的概念，从而提高模型的性能。然而，最近的研究表明，干预有效性可能严重依赖于干预概念的顺序以及模型的架构和训练超参数。我们认为，这源于CBM在训练时缺乏模型适应概念干预的激励。为了解决这个问题，我们提出了干预感知的概念嵌入模型（IntCEMs），这是一种基于CBM的新型架构和训练范式，可以提高模型对测试时干预的响应性。我们的模型以端到端的方式学习了一个概念干预策略，从中可以采样有意义的干预轨迹。

    Concept Bottleneck Models (CBMs) tackle the opacity of neural architectures by constructing and explaining their predictions using a set of high-level concepts. A special property of these models is that they permit concept interventions, wherein users can correct mispredicted concepts and thus improve the model's performance. Recent work, however, has shown that intervention efficacy can be highly dependent on the order in which concepts are intervened on and on the model's architecture and training hyperparameters. We argue that this is rooted in a CBM's lack of train-time incentives for the model to be appropriately receptive to concept interventions. To address this, we propose Intervention-aware Concept Embedding models (IntCEMs), a novel CBM-based architecture and training paradigm that improves a model's receptiveness to test-time interventions. Our model learns a concept intervention policy in an end-to-end fashion from where it can sample meaningful intervention trajectories a
    
[^56]: 联邦学习的模式连通性和数据异质性

    Mode Connectivity and Data Heterogeneity of Federated Learning. (arXiv:2309.16923v1 [cs.LG])

    [http://arxiv.org/abs/2309.16923](http://arxiv.org/abs/2309.16923)

    本研究通过模式连通性的经验和理论研究发现，减少数据异质性可以增加客户端和全局模式之间的连通性，建立了全局模式连通性的定量界限。

    

    联邦学习（FL）可以让多个客户端在保持数据私密性的同时进行模型训练。以往的研究表明，客户端之间的数据异质性导致了更新之间的漂移。然而，对于客户端和全局模式之间的关系研究较少，不清楚这些更新漂移到了哪里。我们通过使用模式连通性进行经验和理论研究，模式连通性可以衡量在不同模式之间的参数路径上性能的变化（即连通性）。经验证明，减少数据异质性使得不同路径上的连通性更加相似，形成了客户端和全局模式之间更多的低误差重叠。我们还发现，在线性连接两个全局模式时存在连通性障碍，而考虑非线性模式连通性后连通性障碍消失。理论上，我们使用均场理论或dropout方法，建立了全局模式连通性的定量界限。

    Federated learning (FL) enables multiple clients to train a model while keeping their data private collaboratively. Previous studies have shown that data heterogeneity between clients leads to drifts across client updates. However, there are few studies on the relationship between client and global modes, making it unclear where these updates end up drifting. We perform empirical and theoretical studies on this relationship by utilizing mode connectivity, which measures performance change (i.e., connectivity) along parametric paths between different modes. Empirically, reducing data heterogeneity makes the connectivity on different paths more similar, forming more low-error overlaps between client and global modes. We also find that a barrier to connectivity occurs when linearly connecting two global modes, while it disappears with considering non-linear mode connectivity. Theoretically, we establish a quantitative bound on the global-mode connectivity using mean-field theory or dropou
    
[^57]: ACGAN-GNNExplainer：用于图神经网络的辅助条件生成解释器

    ACGAN-GNNExplainer: Auxiliary Conditional Generative Explainer for Graph Neural Networks. (arXiv:2309.16918v1 [cs.LG])

    [http://arxiv.org/abs/2309.16918](http://arxiv.org/abs/2309.16918)

    本论文提出了一种新的图神经网络解释方法ACGAN-GNNExplainer，将辅助分类器生成对抗网络（ACGAN）引入到GNN解释领域。通过利用生成器为原始输入图生成解释，并借助鉴别器监督生成过程，提高解释的准确性和保真度。实验证明了该方法的有效性和实用性。

    

    图神经网络（GNNs）已经在各种实际应用中证明了其有效性，但其基本机制仍然是一个谜。为了解决这个挑战并实现可靠的决策，近年来提出了许多GNN解释器。然而，这些方法常常面临一些限制，包括对特定实例的依赖性，对未见过的图的一般性不足，可能产生无效的解释以及生成过程中产生的不充分的保真度。为了克服这些限制，本文将辅助分类器生成对抗网络（ACGAN）引入到GNN解释领域，并提出了一种新的GNN解释器ACGAN-GNNExplainer。我们的方法利用生成器为原始输入图生成解释，并运用鉴别器来监督生成过程，确保解释的保真度并提高准确性。实验评估分别在合成数据集和真实世界的图数据集上进行。

    Graph neural networks (GNNs) have proven their efficacy in a variety of real-world applications, but their underlying mechanisms remain a mystery. To address this challenge and enable reliable decision-making, many GNN explainers have been proposed in recent years. However, these methods often encounter limitations, including their dependence on specific instances, lack of generalizability to unseen graphs, producing potentially invalid explanations, and yielding inadequate fidelity. To overcome these limitations, we, in this paper, introduce the Auxiliary Classifier Generative Adversarial Network (ACGAN) into the field of GNN explanation and propose a new GNN explainer dubbed~\emph{ACGAN-GNNExplainer}. Our approach leverages a generator to produce explanations for the original input graphs while incorporating a discriminator to oversee the generation process, ensuring explanation fidelity and improving accuracy. Experimental evaluations conducted on both synthetic and real-world graph
    
[^58]: ONNXExplainer:基于ONNX的通用框架，使用Shapley值解释神经网络

    ONNXExplainer: an ONNX Based Generic Framework to Explain Neural Networks Using Shapley Values. (arXiv:2309.16916v1 [cs.LG])

    [http://arxiv.org/abs/2309.16916](http://arxiv.org/abs/2309.16916)

    ONNXExplainer是一个基于ONNX的通用框架，使用Shapley值解释神经网络。它提供了自动微分和优化方法，实现了一次性部署和高效的解释计算。

    

    理解神经网络模型为什么会做出某些决策与推理性能一样重要。已经提出了各种方法来帮助解释神经网络模型的预测，其中Shapley值最受欢迎。SHAP包是解释使用TensorFlow或PyTorch实现的神经网络的Shapley值的领先实现，但缺乏跨平台支持、一次性部署且效率低下。为了解决这些问题，我们提出了ONNXExplainer，它是一个使用ONNX生态系统中的Shapley值来解释神经网络的通用框架。在ONNXExplainer中，我们开发了自己的自动微分和优化方法，不仅实现了神经网络推理和解释的一次性部署，还显著提高了解释的效率，并减少了内存消耗。为了公平比较目的，我们还在TensorFlow和PyTorch中实现了相同的优化。

    Understanding why a neural network model makes certain decisions can be as important as the inference performance. Various methods have been proposed to help practitioners explain the prediction of a neural network model, of which Shapley values are most popular. SHAP package is a leading implementation of Shapley values to explain neural networks implemented in TensorFlow or PyTorch but lacks cross-platform support, one-shot deployment and is highly inefficient. To address these problems, we present the ONNXExplainer, which is a generic framework to explain neural networks using Shapley values in the ONNX ecosystem. In ONNXExplainer, we develop its own automatic differentiation and optimization approach, which not only enables One-Shot Deployment of neural networks inference and explanations, but also significantly improves the efficiency to compute explanation with less memory consumption. For fair comparison purposes, we also implement the same optimization in TensorFlow and PyTorch
    
[^59]: ASAP: 自动化复杂机器人组装的物理可行性序列规划

    ASAP: Automated Sequence Planning for Complex Robotic Assembly with Physical Feasibility. (arXiv:2309.16909v1 [cs.RO])

    [http://arxiv.org/abs/2309.16909](http://arxiv.org/abs/2309.16909)

    ASAP是一个基于物理的计划方法，用于自动生成一般形状组装的物理可行性序列。它通过考虑重力和使用高效的树搜索算法，能够在大型数据集上生成物理实际的组装序列规划，适用于仿真和真实世界机器人设置。

    

    复杂产品的自动化组装需要一个系统能够自动规划一个物理可行的动作序列来组装多个部件。在本文中，我们提出了ASAP，一种基于物理的计划方法，用于自动生成一般形状组装的序列。ASAP考虑了重力，设计了一个序列，其中每个子组件在有限数量的零件被保持和支撑表面的情况下保持物理稳定。我们应用高效的树搜索算法来减少确定这样一个组装序列的组合复杂性。搜索可以由几何启发式或训练有模拟标签数据的图神经网络来引导。最后，我们展示了ASAP在数百个复杂产品组装的大型数据集上生成物理实际的组装序列规划的优越性能。我们进一步证明了ASAP在仿真和真实世界机器人设置上的适用性。

    The automated assembly of complex products requires a system that can automatically plan a physically feasible sequence of actions for assembling many parts together. In this paper, we present ASAP, a physics-based planning approach for automatically generating such a sequence for general-shaped assemblies. ASAP accounts for gravity to design a sequence where each sub-assembly is physically stable with a limited number of parts being held and a support surface. We apply efficient tree search algorithms to reduce the combinatorial complexity of determining such an assembly sequence. The search can be guided by either geometric heuristics or graph neural networks trained on data with simulation labels. Finally, we show the superior performance of ASAP at generating physically realistic assembly sequence plans on a large dataset of hundreds of complex product assemblies. We further demonstrate the applicability of ASAP on both simulation and real-world robotic setups. Project website: asa
    
[^60]: 使用多元时间序列转换器获取风险投资和成长资本的投资目标

    Sourcing Investment Targets for Venture and Growth Capital Using Multivariate Time Series Transformer. (arXiv:2309.16888v1 [cs.LG])

    [http://arxiv.org/abs/2309.16888](http://arxiv.org/abs/2309.16888)

    这个论文介绍了一种新颖的方法，利用Transformer-based Multivariate Time Series Classifier (TMTSC)来预测风险投资和成长资本的候选公司的成功可能性，以优化投资目标的选择。通过对相关的方法进行了全面回顾和实验验证，证明了该方法的有效性。

    

    本文探讨了数据驱动方法在私募股权（PE）行业中的应用，特别是在为风险投资（VC）和成长资本（GC）寻找投资目标（即公司）方面。我们对相关方法进行了全面的回顾，并提出了一种新颖的方法，利用基于Transformer的多元时间序列分类器（TMTSC）来预测任何候选公司的成功可能性。我们的研究目标是通过将寻找投资问题正式定义为多元时间序列分类任务，优化VC和GC投资的寻找效果。我们依次介绍了我们实现的关键组成部分，这些部分共同 contribut 到了在VC/GC寻找中成功应用TMTSC：输入特征、模型架构、优化目标以及基于投资者的数据增强和划分。我们在四个数据集上进行了大量实验，并将其与三个流行的基准线进行了对比。

    This paper addresses the growing application of data-driven approaches within the Private Equity (PE) industry, particularly in sourcing investment targets (i.e., companies) for Venture Capital (VC) and Growth Capital (GC). We present a comprehensive review of the relevant approaches and propose a novel approach leveraging a Transformer-based Multivariate Time Series Classifier (TMTSC) for predicting the success likelihood of any candidate company. The objective of our research is to optimize sourcing performance for VC and GC investments by formally defining the sourcing problem as a multivariate time series classification task. We consecutively introduce the key components of our implementation which collectively contribute to the successful application of TMTSC in VC/GC sourcing: input features, model architecture, optimization target, and investor-centric data augmentation and split. Our extensive experiments on four datasets, benchmarked towards three popular baselines, demonstrat
    
[^61]: 揭示对抗扰动中隐藏的人类可识别特征

    Investigating Human-Identifiable Features Hidden in Adversarial Perturbations. (arXiv:2309.16878v1 [cs.LG])

    [http://arxiv.org/abs/2309.16878](http://arxiv.org/abs/2309.16878)

    该研究揭示了对抗扰动中隐藏的人类可识别特征，并发现了掩蔽效应和生成效应在不同类型攻击中的表现。通过分析多个攻击算法生成的扰动，发现它们在一定程度上存在相似性。

    

    神经网络在各种机器学习任务中表现出色，但对抗扰动并不免疫。这种脆弱性对实际应用有着重要影响。虽然已经进行了大量的研究，但神经网络为何易受对抗攻击的根本原因尚未完全理解。我们的研究的核心是探索对抗扰动中的人类可识别特征，我们使用了五种攻击算法和三个数据集进行实验。此外，我们发现了在人类可识别特征中表现出的两种不同效应。在非定向攻击中，掩蔽效应更为显著，而在定向攻击中，生成效应更为常见。通过像素级注释，我们提取这些特征并证明它们能够破坏目标模型。此外，我们的研究结果表明，多个攻击算法生成的扰动在平均情况下存在显著的相似性。

    Neural networks perform exceedingly well across various machine learning tasks but are not immune to adversarial perturbations. This vulnerability has implications for real-world applications. While much research has been conducted, the underlying reasons why neural networks fall prey to adversarial attacks are not yet fully understood. Central to our study, which explores up to five attack algorithms across three datasets, is the identification of human-identifiable features in adversarial perturbations. Additionally, we uncover two distinct effects manifesting within human-identifiable features. Specifically, the masking effect is prominent in untargeted attacks, while the generation effect is more common in targeted attacks. Using pixel-level annotations, we extract such features and demonstrate their ability to compromise target models. In addition, our findings indicate a notable extent of similarity in perturbations across different attack algorithms when averaged over multiple m
    
[^62]: 前言：一种基于数据驱动的体积化先验用于少样本超高分辨率人脸合成

    Preface: A Data-driven Volumetric Prior for Few-shot Ultra High-resolution Face Synthesis. (arXiv:2309.16859v1 [cs.CV])

    [http://arxiv.org/abs/2309.16859](http://arxiv.org/abs/2309.16859)

    本文提出了一种基于数据驱动的体积化人脸先验，使得能合成先前训练分布外的超高分辨率的新视图。通过训练身份数量有限，结合基于稀疏标记点的3D对齐，该模型能够学习到一个平滑的几何和外观的潜在空间，并通过拟合到2-3个摄像机视图来获取高质量的新对象的体积化表示。

    

    NeRFs已经实现了包括复杂的头发和皮肤在内的人脸高度真实的合成，这些方法通常需要大量的多视图输入图像，使该过程对硬件要求较高且繁琐，限制了它在无约束环境中的适用性。我们提出了一种新颖的体积化人脸先验，使得能合成先前训练分布外的超高分辨率的新视图。这个先验模型由一个与身份相关的NeRF组成，经过在已知摄像机标定的多样人类低分辨率多视图图像数据集上训练。尽管训练身份数量有限，通过简单的基于稀疏标记点的3D对齐，我们的模型能够学习到一个平滑的几何和外观的潜在空间。通过将模型拟合到任意分辨率的2或3个摄像机视图，可以获得高质量的新对象的体积化表示。

    NeRFs have enabled highly realistic synthesis of human faces including complex appearance and reflectance effects of hair and skin. These methods typically require a large number of multi-view input images, making the process hardware intensive and cumbersome, limiting applicability to unconstrained settings. We propose a novel volumetric human face prior that enables the synthesis of ultra high-resolution novel views of subjects that are not part of the prior's training distribution. This prior model consists of an identity-conditioned NeRF, trained on a dataset of low-resolution multi-view images of diverse humans with known camera calibration. A simple sparse landmark-based 3D alignment of the training dataset allows our model to learn a smooth latent space of geometry and appearance despite a limited number of training identities. A high-quality volumetric representation of a novel subject can be obtained by model fitting to 2 or 3 camera views of arbitrary resolution. Importantly,
    
[^63]: 用于线性函数逼近的多Bellman算子对$Q$-learning的收敛性的研究

    Multi-Bellman operator for convergence of $Q$-learning with linear function approximation. (arXiv:2309.16819v1 [cs.LG])

    [http://arxiv.org/abs/2309.16819](http://arxiv.org/abs/2309.16819)

    本文通过引入新的多Bellman算子，对线性函数逼近下的$Q$-learning进行了收敛性研究，并提出了多$Q$-learning算法。通过探索多Bellman算子的性质，我们找到了使其成为压缩映射的条件，获得了比传统Bellman算子更好的固定点保证。该算法收敛到多Bellman算子的不动点，可以得到任意精度的解。在验证过程中，我们将该方法应用于常用环境中，展示了其有效性和适用性。

    

    本文研究了线性函数逼近下$Q$-learning的收敛性。我们的主要贡献是引入了一种新颖的多Bellman算子，该算子扩展了传统的Bellman算子。通过探索该算子的性质，我们确定了在投影的多Bellman算子变为压缩映射时的条件，从而提供了比Bellman算子更好的固定点保证。为了利用这些洞察力，我们提出了具有线性函数逼近的多$Q$-learning算法。我们证明了该算法收敛到投影的多Bellman算子的不动点，从而获得了任意精度的解。最后，我们通过将该方法应用于众所周知的环境来验证我们的方法，展示了我们发现的有效性和适用性。

    We study the convergence of $Q$-learning with linear function approximation. Our key contribution is the introduction of a novel multi-Bellman operator that extends the traditional Bellman operator. By exploring the properties of this operator, we identify conditions under which the projected multi-Bellman operator becomes contractive, providing improved fixed-point guarantees compared to the Bellman operator. To leverage these insights, we propose the multi $Q$-learning algorithm with linear function approximation. We demonstrate that this algorithm converges to the fixed-point of the projected multi-Bellman operator, yielding solutions of arbitrary accuracy. Finally, we validate our approach by applying it to well-known environments, showcasing the effectiveness and applicability of our findings.
    
[^64]: Promptbreeder: 通过提示演化实现自我参照的自我改进

    Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution. (arXiv:2309.16797v1 [cs.CL])

    [http://arxiv.org/abs/2309.16797](http://arxiv.org/abs/2309.16797)

    Promptbreeder是一种通过提示演化进行自我改进的通用机制，它可以优化大型语言模型的推理能力，并在算术和常识推理上优于现有的提示策略。

    

    像Chain-of-Thought Prompting这样的流行提示策略可以显著改进大型语言模型（LLMs）在各个领域的推理能力。然而，这种手工制作的提示策略通常不够优化。在本文中，我们提出了Promptbreeder，一种通用的自我参照自我改进机制，用于进化和调整给定领域的提示。Promptbreeder通过LLM驱动，对一组任务提示进行突变，并在训练集上评估其适应性。关键是，这些任务提示的突变是由LLM生成并在演化过程中自我改进的突变提示来控制的。换句话说，Promptbreeder不仅改进任务提示，还改进了改进这些任务提示的突变提示。Promptbreeder在常用的算术和常识推理基准测试中优于Chain-of-Thought和Plan-and-Solve Prompting等最先进的提示策略。

    Popular prompt strategies like Chain-of-Thought Prompting can dramatically improve the reasoning abilities of Large Language Models (LLMs) in various domains. However, such hand-crafted prompt-strategies are often sub-optimal. In this paper, we present Promptbreeder, a general-purpose self-referential self-improvement mechanism that evolves and adapts prompts for a given domain. Driven by an LLM, Promptbreeder mutates a population of task-prompts, and subsequently evaluates them for fitness on a training set. Crucially, the mutation of these task-prompts is governed by mutation-prompts that the LLM generates and improves throughout evolution in a self-referential way. That is, Promptbreeder is not just improving task-prompts, but it is also improving the mutationprompts that improve these task-prompts. Promptbreeder outperforms state-of-the-art prompt strategies such as Chain-of-Thought and Plan-and-Solve Prompting on commonly used arithmetic and commonsense reasoning benchmarks. Furth
    
[^65]: 光子加速器用于自动驾驶中的图像分割和缺陷检测

    Photonic Accelerators for Image Segmentation in Autonomous Driving and Defect Detection. (arXiv:2309.16783v1 [cs.CV])

    [http://arxiv.org/abs/2309.16783](http://arxiv.org/abs/2309.16783)

    该论文研究了光子加速器在自动驾驶和缺陷检测中的图像分割应用。研究发现，使用光子加速器执行特定的分割模型可以在准确性上忽略损失，并讨论了在模型被干扰时修复准确性的技术。

    

    光子计算承诺比传统的数字硬件更快速和能量更高效的深度神经网络（DNN）推断。光子计算的进步可以对依赖于快速、准确和能量高效执行图像分割模型的应用，如自动驾驶和缺陷检测，产生深远影响。本文研究了在光子加速器上进行图像分割，探索了适合光子加速器的图像分割DNN架构类型以及在光子加速器上执行不同图像分割模型的吞吐量和能量效率，以及其中的权衡。具体地，我们证明了当在光子加速器上执行时，某些分割模型的准确性损失可以忽略不计（与数字float32模型相比），并探讨了它们的稳健性的经验推理。我们还讨论了在模型的修复准确性的技术（在模型被干扰时）。

    Photonic computing promises faster and more energy-efficient deep neural network (DNN) inference than traditional digital hardware. Advances in photonic computing can have profound impacts on applications such as autonomous driving and defect detection that depend on fast, accurate and energy efficient execution of image segmentation models. In this paper, we investigate image segmentation on photonic accelerators to explore: a) the types of image segmentation DNN architectures that are best suited for photonic accelerators, and b) the throughput and energy efficiency of executing the different image segmentation models on photonic accelerators, along with the trade-offs involved therein. Specifically, we demonstrate that certain segmentation models exhibit negligible loss in accuracy (compared to digital float32 models) when executed on photonic accelerators, and explore the empirical reasoning for their robustness. We also discuss techniques for recovering accuracy in the case of mod
    
[^66]: 生成分类器的有趣属性

    Intriguing properties of generative classifiers. (arXiv:2309.16779v1 [cs.CV])

    [http://arxiv.org/abs/2309.16779](http://arxiv.org/abs/2309.16779)

    生成分类器展示了记录破纪录的人类形状偏好、接近人类级别的超出分布准确性、与人类分类错误的最先进对齐以及理解某些知觉幻象的新兴特性，揭示了零样本生成模型出奇地接近人类物体识别数据。

    

    识别对象的最佳范式是判别式推理（快速但潜在容易出现快捷学习）还是使用生成模型（较慢但潜在更稳健）？我们借鉴了最新的生成模型进展，将文本到图像模型转化为分类器。这使得我们能够研究其行为，并将其与判别模型和人类心理物理数据进行比较。我们报道了生成分类器的四个有趣的新兴特性：它们显示出破纪录的人类形状偏好（对于Imagen达到99%），接近人类级别的超出分布准确性，与人类分类错误的最先进对齐以及它们理解某些知觉幻象。我们的结果表明，尽管目前模拟人类物体识别的主导范式是判别式推理，零样本生成模型出奇地接近人类物体识别数据。

    What is the best paradigm to recognize objects -- discriminative inference (fast but potentially prone to shortcut learning) or using a generative model (slow but potentially more robust)? We build on recent advances in generative modeling that turn text-to-image models into classifiers. This allows us to study their behavior and to compare them against discriminative models and human psychophysical data. We report four intriguing emergent properties of generative classifiers: they show a record-breaking human-like shape bias (99% for Imagen), near human-level out-of-distribution accuracy, state-of-the-art alignment with human classification errors, and they understand certain perceptual illusions. Our results indicate that while the current dominant paradigm for modeling human object recognition is discriminative inference, zero-shot generative models approximate human object recognition data surprisingly well.
    
[^67]: ChatGPT知道多少个单词？答案是ChatWords。

    How many words does ChatGPT know? The answer is ChatWords. (arXiv:2309.16777v1 [cs.CL])

    [http://arxiv.org/abs/2309.16777](http://arxiv.org/abs/2309.16777)

    这项研究通过开发ChatWords，一个自动化测试系统，为评估ChatGPT和其他NLP AI工具的词汇知识做出了贡献，以解决它们在产生错误答案方面的限制和错觉问题。

    

    ChatGPT的引入引起了人工智能（AI）自然语言处理（NLP）的关注。ChatGPT的采用率呈指数增长，数百万用户在各种任务和应用领域中进行实验，并取得了令人瞩目的结果。然而，ChatGPT存在着限制和错觉，例如产生看似可信但完全错误的答案。评估ChatGPT和类似的AI工具的性能是一个复杂的问题，正从不同的视角进行探索。在这项工作中，我们通过ChatWords，一个自动化测试系统，为评估ChatGPT对任意单词集的知识做出了贡献。ChatWords设计为可扩展、易于使用和适应性强，也可以评估其他NLP AI工具。ChatWords已公开可用，其主要目标是促进对AI工具的词汇知识的研究。通过两个案例研究，证明了ChatWords的好处：评估对AI工具的知识。

    The introduction of ChatGPT has put Artificial Intelligence (AI) Natural Language Processing (NLP) in the spotlight. ChatGPT adoption has been exponential with millions of users experimenting with it in a myriad of tasks and application domains with impressive results. However, ChatGPT has limitations and suffers hallucinations, for example producing answers that look plausible but they are completely wrong. Evaluating the performance of ChatGPT and similar AI tools is a complex issue that is being explored from different perspectives. In this work, we contribute to those efforts with ChatWords, an automated test system, to evaluate ChatGPT knowledge of an arbitrary set of words. ChatWords is designed to be extensible, easy to use, and adaptable to evaluate also other NLP AI tools. ChatWords is publicly available and its main goal is to facilitate research on the lexical knowledge of AI tools. The benefits of ChatWords are illustrated with two case studies: evaluating the knowledge tha
    
[^68]: 神经规模定律在表型药物发现中的应用

    Neural scaling laws for phenotypic drug discovery. (arXiv:2309.16773v1 [cs.LG])

    [http://arxiv.org/abs/2309.16773](http://arxiv.org/abs/2309.16773)

    本研究调查了规模对于辅助小分子药物发现模型的影响，并发现DNN在表型药物发现任务中并没有持续改进，通过引入逆生物过程预训练可以显著提升模型性能。

    

    近期深度神经网络在自然语言处理和计算机视觉领域的突破是通过模型和数据规模的提升而非新的计算范例的发现。本研究探讨了规模是否也能对用于辅助小分子药物发现的模型产生类似的影响。我们通过大规模和系统化的分析深度神经网络的规模、数据训练集和学习方法如何相互影响，并对我们的表型化学竞技场（Pheno-CA）基准进行精确度测试：这是一个基于图像高内涵筛选数据的多样化药物开发任务集。令人惊讶的是，我们发现在Pheno-CA任务中明确被监督的深度神经网络在数据和模型规模扩大时并没有持续改进。为了解决这个问题，我们引入了一项新的前体任务，即逆生物过程（IBP），它设计成类似于在自然语言处理中取得成功的因果目标函数。我们确实发现深度神经网络的性能取决于引入IBP任务的方式，当使用IBP预训练时，模型在Pheno-CA任务上的性能得到了显著改善。我们的结果揭示了在表型药物发现中，与深度神经网络的规模和学习方式相关的因素对模型的性能产生重要影响。

    Recent breakthroughs by deep neural networks (DNNs) in natural language processing (NLP) and computer vision have been driven by a scale-up of models and data rather than the discovery of novel computing paradigms. Here, we investigate if scale can have a similar impact for models designed to aid small molecule drug discovery. We address this question through a large-scale and systematic analysis of how DNN size, data diet, and learning routines interact to impact accuracy on our Phenotypic Chemistry Arena (Pheno-CA) benchmark: a diverse set of drug development tasks posed on image-based high content screening data. Surprisingly, we find that DNNs explicitly supervised to solve tasks in the Pheno-CA do not continuously improve as their data and model size is scaled-up. To address this issue, we introduce a novel precursor task, the Inverse Biological Process (IBP), which is designed to resemble the causal objective functions that have proven successful for NLP. We indeed find that DNNs
    
[^69]: XVO: 通过跨模态自我训练的泛化视觉里程计方法

    XVO: Generalized Visual Odometry via Cross-Modal Self-Training. (arXiv:2309.16772v1 [cs.CV])

    [http://arxiv.org/abs/2309.16772](http://arxiv.org/abs/2309.16772)

    XVO是一种通过跨模态自我训练的泛化视觉里程计方法，可以在不同数据集和环境设置下具有强大的自给自足操作的训练模型。其关键创新和贡献包括通过半监督训练学习通用的直接VO回归网络以及使用多模式监督任务来促进泛化表示。

    

    我们提出了XVO，一种半监督学习方法，用于在不同数据集和环境设置下具有强大的自给自足操作的泛化单目视觉里程计（VO）模型的训练。与通常研究单个数据集内已知校准的标准单目VO方法不同，XVO可以高效地通过视觉场景语义（即不依赖于任何已知相机参数）学习恢复相对位姿，并从YouTube上的大量无约束和异构的车载摄像头视频进行自我训练来优化运动估计模型。我们的关键贡献有两个方面：第一，我们经验证明了半监督训练对于学习通用的直接VO回归网络的好处。第二，我们证明了多模式监督的有效性，包括分割、光流、深度和音频辅助预测任务，以促进VO任务的泛化表示。具体而言，我们发现音频预测任务对于总结摘要的关键创新和贡献有促进作用。

    We propose XVO, a semi-supervised learning method for training generalized monocular Visual Odometry (VO) models with robust off-the-self operation across diverse datasets and settings. In contrast to standard monocular VO approaches which often study a known calibration within a single dataset, XVO efficiently learns to recover relative pose with real-world scale from visual scene semantics, i.e., without relying on any known camera parameters. We optimize the motion estimation model via self-training from large amounts of unconstrained and heterogeneous dash camera videos available on YouTube. Our key contribution is twofold. First, we empirically demonstrate the benefits of semi-supervised training for learning a general-purpose direct VO regression network. Second, we demonstrate multi-modal supervision, including segmentation, flow, depth, and audio auxiliary prediction tasks, to facilitate generalized representations for the VO task. Specifically, we find audio prediction task to
    
[^70]: Persona编码多流程对话句子评分：Persona引导的多流程对话句子评分方法

    Persona-Coded Poly-Encoder: Persona-Guided Multi-Stream Conversational Sentence Scoring. (arXiv:2309.16770v1 [cs.CL])

    [http://arxiv.org/abs/2309.16770](http://arxiv.org/abs/2309.16770)

    本论文提出了一种新颖的Persona编码多流程对话句子评分方法，利用个人角色信息来提高对话生成的质量。

    

    机器学习和深度学习的最新进展已经在许多实际应用中广泛应用于对话AI。然而，要利用可以提供对话背景或个性化调整的辅助信息以提高对话质量仍然很具挑战性。例如，关于使用个人角色信息来提高对话质量的研究仅有限，即使是最先进的对话AI技术也无法有效地利用来自多种来源的辅助数据信号，例如多模式交互数据、人口统计学数据和社会确定因素数据等。在本文中，我们提出了一种新颖的Persona编码多流程对话句子评分方法，它利用多流程编码方案中的个人角色信息来提高对话生成的质量。为了展示所提出方法的有效性，我们在两个不同的基于个人角色的对话数据集上评估了我们的方法，并与参考方法进行了比较。

    Recent advances in machine learning and deep learning have led to the widespread use of Conversational AI in many practical applications. However, it is still very challenging to leverage auxiliary information that can provide conversational context or personalized tuning to improve the quality of conversations. For example, there has only been limited research on using an individuals persona information to improve conversation quality, and even state-of-the-art conversational AI techniques are unable to effectively leverage signals from heterogeneous sources of auxiliary data, such as multi-modal interaction data, demographics, SDOH data, etc. In this paper, we present a novel Persona-Coded Poly-Encoder method that leverages persona information in a multi-stream encoding scheme to improve the quality of response generation for conversations. To show the efficacy of the proposed method, we evaluate our method on two different persona-based conversational datasets, and compared against 
    
[^71]: 眼中记忆：扩散模型和关联记忆之间的神秘相似之处的调查

    Memory in Plain Sight: A Survey of the Uncanny Resemblances between Diffusion Models and Associative Memories. (arXiv:2309.16750v1 [cs.LG])

    [http://arxiv.org/abs/2309.16750](http://arxiv.org/abs/2309.16750)

    本调查综述了扩散模型（DMs）和关联记忆（AMs）之间的数学联系，揭示了DMs是如何利用能量函数进行去噪数据的，并讨论了未来研究方向。

    

    扩散模型（DMs）最近在许多生成基准测试中取得了最新的成果。然而，对它们的数学描述有很多种方式，这使得人们很难对其工作原理进行简单理解。在这项调查中，我们从动力系统和常微分方程（ODE）的角度提供了DMs的简明概述，揭示了一种与其高度相关但常常被忽视的能量模型类别，称为关联记忆（AMs）的数学联系。基于能量的AMs是一个理论框架，其行为与去噪DMs非常相似，但它们使我们能够直接计算一个Lyapunov能量函数，在其上可以执行梯度下降以去噪数据。然后，我们总结了能量AMs的40年历史，从最初的Hopfield网络开始，并讨论了通过描述它们的相似性和差异程度揭示出来的AMs和DMs的新研究方向。

    Diffusion Models (DMs) have recently set state-of-the-art on many generation benchmarks. However, there are myriad ways to describe them mathematically, which makes it difficult to develop a simple understanding of how they work. In this survey, we provide a concise overview of DMs from the perspective of dynamical systems and Ordinary Differential Equations (ODEs) which exposes a mathematical connection to the highly related yet often overlooked class of energy-based models, called Associative Memories (AMs). Energy-based AMs are a theoretical framework that behave much like denoising DMs, but they enable us to directly compute a Lyapunov energy function on which we can perform gradient descent to denoise data. We then summarize the 40 year history of energy-based AMs, beginning with the original Hopfield Network, and discuss new research directions for AMs and DMs that are revealed by characterizing the extent of their similarities and differences
    
[^72]: 用XRM发现环境

    Discovering environments with XRM. (arXiv:2309.16748v1 [cs.LG])

    [http://arxiv.org/abs/2309.16748](http://arxiv.org/abs/2309.16748)

    本文提出了一种用于发现环境的算法 XRM，它通过训练两个孪生网络，每个网络从训练数据的一半中学习，并模仿其兄弟网络的错误分类，解决了现有方法需要依赖人工注释环境信息的问题。

    

    成功的跨领域泛化需要环境注释。然而，这些注释的获取是资源密集型的，并且它们对模型性能的影响受人类注释者的期望和感知偏差的限制。因此，为了实现应用领域全面泛化的鲁棒性AI系统，我们必须开发一种算法来自动发现引发广泛泛化的环境。目前的提案根据训练误差将示例划分为不同的类，但存在一个根本问题。这些方法添加了超参数和早停策略，而这些参数是无法在没有人类注释环境的验证集的情况下进行调整的，而这些信息正是要发现的信息。在本文中，我们提出了 Cross-Risk-Minimization (XRM) 来解决这个问题。XRM 训练两个孪生网络，每个网络从训练数据的一个随机一半中学习，同时模仿其兄弟网络所做的自信的错误分类。XRM 提供了超参数调整的方法，并且不需要依赖人工注释的环境信息。

    Successful out-of-distribution generalization requires environment annotations. Unfortunately, these are resource-intensive to obtain, and their relevance to model performance is limited by the expectations and perceptual biases of human annotators. Therefore, to enable robust AI systems across applications, we must develop algorithms to automatically discover environments inducing broad generalization. Current proposals, which divide examples based on their training error, suffer from one fundamental problem. These methods add hyper-parameters and early-stopping criteria that are impossible to tune without a validation set with human-annotated environments, the very information subject to discovery. In this paper, we propose Cross-Risk-Minimization (XRM) to address this issue. XRM trains two twin networks, each learning from one random half of the training data, while imitating confident held-out mistakes made by its sibling. XRM provides a recipe for hyper-parameter tuning, does not 
    
[^73]: 利用多样数据进行全球灾害预测的多模式框架

    Harnessing Diverse Data for Global Disaster Prediction: A Multimodal Framework. (arXiv:2309.16747v1 [cs.LG])

    [http://arxiv.org/abs/2309.16747](http://arxiv.org/abs/2309.16747)

    本研究提出了一种新颖的多模式灾害预测框架，结合了天气统计数据、卫星图像和文本信息。通过整合多个数据源，对于不同类型的灾害预测，模型的性能得到了增强。

    

    随着气候变化的加剧，准确的全球规模灾害预测变得越来越紧迫。本研究提出了一种新颖的多模式灾害预测框架，结合了天气统计数据、卫星图像和文本信息。我们特别关注"洪水"和"滑坡"的预测，因为它们与气象和地形因素有关。该模型根据可用数据进行精细设计，并采用策略来解决类别不平衡问题。尽管我们的研究结果表明整合多个数据源可以增强模型性能，但增强的程度取决于每种灾害的具体性质和其独特的潜在原因。

    As climate change intensifies, the urgency for accurate global-scale disaster predictions grows. This research presents a novel multimodal disaster prediction framework, combining weather statistics, satellite imagery, and textual insights. We particularly focus on "flood" and "landslide" predictions, given their ties to meteorological and topographical factors. The model is meticulously crafted based on the available data and we also implement strategies to address class imbalance. While our findings suggest that integrating multiple data sources can bolster model performance, the extent of enhancement differs based on the specific nature of each disaster and their unique underlying causes.
    
[^74]: 从大型集合运行中高通量训练深度代理

    High Throughput Training of Deep Surrogates from Large Ensemble Runs. (arXiv:2309.16743v1 [cs.LG])

    [http://arxiv.org/abs/2309.16743](http://arxiv.org/abs/2309.16743)

    该论文提出了一个用于加速数值解算器的深度代理的高通量训练方法，通过从大量集合运行的模拟中在线训练模型，利用多级并行性生成丰富的数据集，直接流式传输数据以避免I/O瓶颈和存储问题，同时使用训练储备池来减轻流式传输的固有偏差。实验结果显示，使用该方法能够在2小时内在8TB的数据上训练出准确率提升了47%、批量吞吐量提高了13倍的热方程代理网络。

    

    近年来，深度学习方法在加速数值解算器方面取得了突破，这些方法可以提供对物理世界的忠实但计算密集型的模拟。这些深度代理通常通过同一解算器生成的有限量的数据有监督地进行训练，而我们提出了一个开源框架，可以从大量集合运行的模拟中在线训练这些模型。它利用多个级别的并行性来生成丰富的数据集，并通过直接流式传输生成的数据来避免I/O瓶颈和存储问题。训练储备池在最大化GPU吞吐量的同时，减轻了流式传输的固有偏差。实验结果表明，通过提出的方法，在2小时内能够在8TB的数据上训练完全连接网络作为热方程的代理，相比传统的离线过程，准确率提升了47％，批量吞吐量提高了13倍。

    Recent years have seen a surge in deep learning approaches to accelerate numerical solvers, which provide faithful but computationally intensive simulations of the physical world. These deep surrogates are generally trained in a supervised manner from limited amounts of data slowly generated by the same solver they intend to accelerate. We propose an open-source framework that enables the online training of these models from a large ensemble run of simulations. It leverages multiple levels of parallelism to generate rich datasets. The framework avoids I/O bottlenecks and storage issues by directly streaming the generated data. A training reservoir mitigates the inherent bias of streaming while maximizing GPU throughput. Experiment on training a fully connected network as a surrogate for the heat equation shows the proposed approach enables training on 8TB of data in 2 hours with an accuracy improved by 47% and a batch throughput multiplied by 13 compared to a traditional offline proced
    
[^75]: 早期检测2型糖尿病患者白蛋白尿风险的监督学习模型

    Supervised Learning Models for Early Detection of Albuminuria Risk in Type-2 Diabetes Mellitus Patients. (arXiv:2309.16742v1 [cs.LG])

    [http://arxiv.org/abs/2309.16742](http://arxiv.org/abs/2309.16742)

    该论文研究了2型糖尿病患者白蛋白尿的早期预测问题，并开发了一种监督学习模型。通过使用不同的监督学习算法对184条数据进行训练，得出了预测结果。

    

    糖尿病，尤其是2型糖尿病，仍然是一个重大的健康问题。与糖尿病相关的一个主要问题是其并发症的发展。糖尿病肾病是糖尿病的一种慢性并发症，不利地影响肾脏，导致肾脏损伤。诊断糖尿病肾病涉及考虑各种标准之一，其中之一是尿液中白蛋白的病理学病理学数量，称为白蛋白尿。因此，对糖尿病患者尿液中白蛋白尿的早期预测具有及时预防措施的潜力。本研究旨在开发一种监督学习模型，以预测2型糖尿病患者患有白蛋白尿的风险。所选的监督学习算法包括朴素贝叶斯，支持向量机（SVM），决策树，随机森林，AdaBoost，XGBoost和多层感知器（MLP）。我们的私有数据集包括184条糖尿病并发症风险因素的条目被用来训练算法

    Diabetes, especially T2DM, continues to be a significant health problem. One of the major concerns associated with diabetes is the development of its complications. Diabetic nephropathy, one of the chronic complication of diabetes, adversely affects the kidneys, leading to kidney damage. Diagnosing diabetic nephropathy involves considering various criteria, one of which is the presence of a pathologically significant quantity of albumin in urine, known as albuminuria. Thus, early prediction of albuminuria in diabetic patients holds the potential for timely preventive measures. This study aimed to develop a supervised learning model to predict the risk of developing albuminuria in T2DM patients. The selected supervised learning algorithms included Na\"ive Bayes, Support Vector Machine (SVM), decision tree, random forest, AdaBoost, XGBoost, and Multi-Layer Perceptron (MLP). Our private dataset, comprising 184 entries of diabetes complications risk factors, was used to train the algorithm
    
[^76]: 多模态金融时间序列通过潜空间投影的检索

    Multi-Modal Financial Time-Series Retrieval Through Latent Space Projections. (arXiv:2309.16741v1 [cs.LG])

    [http://arxiv.org/abs/2309.16741](http://arxiv.org/abs/2309.16741)

    本文提出了一种通过深度编码器在低维潜空间中存储金融时间序列的多模态数据的框架，以捕捉数据的重要特征。

    

    金融公司通常处理和存储产生连续且高频的数十亿条时间序列数据。为了支持高效的数据存储和检索，出现了专门的时间序列数据库和系统。这些数据库支持通过类似于约束化结构化查询语言（SQL）的格式对时间序列进行索引和查询，以实现像“月度价格回报大于5%的股票”这样的查询，并以严格的格式表达。然而，这样的查询不能捕捉到高维时间序列数据的内在复杂性，它们往往可以通过图像或语言（例如“处于低波动性状态的股票”）更好地描述。而且，在时间序列空间中进行搜索所需的存储、计算时间和检索复杂度往往是非平凡的。在本文中，我们提出并演示了一种利用深度编码器在低维潜空间中存储金融时间序列的多模态数据的框架，使得潜空间投影可以捕捉到数据的重要特征。

    Financial firms commonly process and store billions of time-series data, generated continuously and at a high frequency. To support efficient data storage and retrieval, specialized time-series databases and systems have emerged. These databases support indexing and querying of time-series by a constrained Structured Query Language(SQL)-like format to enable queries like "Stocks with monthly price returns greater than 5%", and expressed in rigid formats. However, such queries do not capture the intrinsic complexity of high dimensional time-series data, which can often be better described by images or language (e.g., "A stock in low volatility regime"). Moreover, the required storage, computational time, and retrieval complexity to search in the time-series space are often non-trivial. In this paper, we propose and demonstrate a framework to store multi-modal data for financial time-series in a lower-dimensional latent space using deep encoders, such that the latent space projections ca
    
[^77]: 将大型语言模型推至6G边缘：视野、挑战和机遇

    Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities. (arXiv:2309.16739v1 [cs.LG])

    [http://arxiv.org/abs/2309.16739](http://arxiv.org/abs/2309.16739)

    本文探讨了将大型语言模型(LLMs)部署在6G边缘的潜力和挑战。我们介绍了由LLMs支持的关键应用，并从响应时间、带宽成本和数据隐私等方面分析了云端部署面临的问题。我们提出了6G移动边缘计算(MEC)系统可能解决这些问题的方案，并讨论了边缘训练和边缘推理的创新技术。

    

    大型语言模型(LLMs)展示了显著的能力，正在改变人工智能的发展并有可能塑造我们的未来。然而，由于LLMs的多模态特性，当前的基于云的部署面临着一些关键挑战：1) 响应时间长；2) 高带宽成本；以及3) 违反数据隐私。6G移动边缘计算(MEC)系统可能解决这些迫切问题。本文探讨了在6G边缘部署LLMs的潜力。我们首先介绍了由多模态LLMs提供支持的关键应用，包括机器人技术和医疗保健，以突出在终端用户附近部署LLMs的需求。然后，我们确定了在边缘部署LLMs时面临的关键挑战，并设想了适用于LLMs的6G MEC架构。此外，我们深入探讨了两个设计方面，即LLMs的边缘训练和边缘推理。在这两个方面，考虑到边缘的固有资源限制，我们讨论了各种前沿技术。

    Large language models (LLMs), which have shown remarkable capabilities, are revolutionizing AI development and potentially shaping our future. However, given their multimodality, the status quo cloud-based deployment faces some critical challenges: 1) long response time; 2) high bandwidth costs; and 3) the violation of data privacy. 6G mobile edge computing (MEC) systems may resolve these pressing issues. In this article, we explore the potential of deploying LLMs at the 6G edge. We start by introducing killer applications powered by multimodal LLMs, including robotics and healthcare, to highlight the need for deploying LLMs in the vicinity of end users. Then, we identify the critical challenges for LLM deployment at the edge and envision the 6G MEC architecture for LLMs. Furthermore, we delve into two design aspects, i.e., edge training and edge inference for LLMs. In both aspects, considering the inherent resource limitations at the edge, we discuss various cutting-edge techniques, i
    
[^78]: 深度学习应用的韧性：分析和加固技术的系统调查

    Resilience of Deep Learning applications: a systematic survey of analysis and hardening techniques. (arXiv:2309.16733v1 [cs.LG])

    [http://arxiv.org/abs/2309.16733](http://arxiv.org/abs/2309.16733)

    这篇论文系统调查了深度学习应用对底层硬件故障的韧性，并提供了未来研究的方向。

    

    机器学习（ML）目前被广泛应用于各种领域，是最有效的人工智能（AI）技术之一，如视觉、自主系统等。这一趋势促使人们对ML应用在底层硬件故障影响下的分析和设计做出了大量贡献。本文通过一次深入的回顾系统地调查了关于深度学习（ML技术之一）对抗硬件故障的韧性的已有知识，清晰地呈现了这一文献流的优点和缺点，并提出了未来的研究方向。文章基于2019年1月至2023年3月间发表的163篇科学论文，采用分类框架来解读和突出研究的相似之处和特点，从工作的主要范围、采用的故障和错误模型等多个参数进行分类。

    Machine Learning (ML) is currently being exploited in numerous applications being one of the most effective Artificial Intelligence (AI) technologies, used in diverse fields, such as vision, autonomous systems, and alike. The trend motivated a significant amount of contributions to the analysis and design of ML applications against faults affecting the underlying hardware. The authors investigate the existing body of knowledge on Deep Learning (among ML techniques) resilience against hardware faults systematically through a thoughtful review in which the strengths and weaknesses of this literature stream are presented clearly and then future avenues of research are set out. The review is based on 163 scientific articles published between January 2019 and March 2023. The authors adopt a classifying framework to interpret and highlight research similarities and peculiarities, based on several parameters, starting from the main scope of the work, the adopted fault and error models, to the
    
[^79]: SimPINNs: 用于增强非线性反问题性能的基于仿真驱动的物理信息神经网络

    SimPINNs: Simulation-Driven Physics-Informed Neural Networks for Enhanced Performance in Nonlinear Inverse Problems. (arXiv:2309.16729v1 [cs.LG])

    [http://arxiv.org/abs/2309.16729](http://arxiv.org/abs/2309.16729)

    本文提出了一种基于仿真驱动的物理信息神经网络(SimPINNs)的方法，用于增强非线性反问题的性能。这种方法通过深度学习和融合观测数据和仿真数据的混合损失函数来推断控制物理系统的未知参数。实验结果表明，在轨道复位问题上，SimPINNs的准确性和鲁棒性优于标准PINNs的表现。

    

    本文引入了一种通过利用深度学习技术解决反问题的新方法。目标是根据观测数据推断控制物理系统的未知参数。我们关注在潜在的正向模型表现出显著非线性行为且未知参数空间的维度明显小于观测数据的情况下。我们提出的方法基于物理信息神经网络(PINNs)，通过一个混合损失函数训练，该损失函数结合了观测数据和由已知(近似)物理模型生成的仿真数据。在轨道复位问题上的实验结果表明，我们的方法超过了标准PINNs的性能，提供了更高的准确性和鲁棒性。

    This paper introduces a novel approach to solve inverse problems by leveraging deep learning techniques. The objective is to infer unknown parameters that govern a physical system based on observed data. We focus on scenarios where the underlying forward model demonstrates pronounced nonlinear behaviour, and where the dimensionality of the unknown parameter space is substantially smaller than that of the observations. Our proposed method builds upon physics-informed neural networks (PINNs) trained with a hybrid loss function that combines observed data with simulated data generated by a known (approximate) physical model. Experimental results on an orbit restitution problem demonstrate that our approach surpasses the performance of standard PINNs, providing improved accuracy and robustness.
    
[^80]: GPT-Lab: 利用GPT驱动的机器人实验室实现下一代最优化学化学发现

    GPT-Lab: Next Generation Of Optimal Chemistry Discovery By GPT Driven Robotic Lab. (arXiv:2309.16721v1 [cs.AI])

    [http://arxiv.org/abs/2309.16721](http://arxiv.org/abs/2309.16721)

    GPT-Lab这项研究介绍了一种利用GPT模型赋予机器人人类般智能的范式，其通过挖掘文献、分析材料和方法，并通过高通量合成验证结果，实现了快速材料发现和验证的潜力。

    

    将机器人整合到化学实验中提高了实验效率，但由于缺乏理解文献的人类智能，它们很少在实验设计中提供帮助。因此，实现从实验设计到验证的全过程自主性仍然是一个挑战。引入生成式预训练转换器（GPT），特别是GPT-4，到机器人实验中提供了一种解决方案。我们提出了GPT-Lab，这是一种利用GPT模型赋予机器人人类般智能的范式。通过我们的机器人实验平台，GPT-Lab从文献中挖掘材料和方法，并通过高通量合成验证结果。作为示范，GPT-Lab分析了500篇文章，鉴定出18种潜在试剂，并成功制备出具有均方根误差（RMSE）为2.68％的准确湿度比色传感器。这展示了我们系统快速材料发现和验证的潜力。

    The integration of robots in chemical experiments has enhanced experimental efficiency, but lacking the human intelligence to comprehend literature, they seldom provide assistance in experimental design. Therefore, achieving full-process autonomy from experiment design to validation in self-driven laboratories (SDL) remains a challenge. The introduction of Generative Pre-trained Transformers (GPT), particularly GPT-4, into robotic experimentation offers a solution. We introduce GPT-Lab, a paradigm that employs GPT models to give robots human-like intelligence. With our robotic experimentation platform, GPT-Lab mines literature for materials and methods and validates findings through high-throughput synthesis. As a demonstration, GPT-Lab analyzed 500 articles, identified 18 potential reagents, and successfully produced an accurate humidity colorimetric sensor with a root mean square error (RMSE) of 2.68%. This showcases the rapid materials discovery and validation potential of our syste
    
[^81]: 在混合交通中实现安全自主性: 通过信息共享检测人类驾驶员的不可预测异常行为

    Towards Safe Autonomy in Hybrid Traffic: Detecting Unpredictable Abnormal Behaviors of Human Drivers via Information Sharing. (arXiv:2309.16716v1 [cs.RO])

    [http://arxiv.org/abs/2309.16716](http://arxiv.org/abs/2309.16716)

    该论文提出了一种在混合交通中实现安全自主性的算法，通过信息共享检测人类驾驶员的不可预测异常行为，提高了轨迹预测的准确性，并能快速检测异常的人类驾驶模式切换。

    

    混合交通即包括自主车辆和人驾驶车辆，将在一段时间内成为自主车辆实践的常态。与自主车辆不同，人驾驶车辆可能表现出突然的异常行为，如不可预测地切换到危险驾驶模式，将其周围的车辆置于风险之中；这种不希望的模式切换可能来自多种人类驾驶员因素，包括疲劳、酒后驾驶、分心、攻击性等。另一方面，现代车辆间通信技术使得自主车辆能够有效和可靠地共享稀缺的实时信息。在本文中，我们提出了目前已知最有效的算法，可以（1）通过有效融合周围自主车辆共享的实时信息，显著提高轨迹预测；（2）准确快速地检测异常的人类驾驶模式切换或...

    Hybrid traffic which involves both autonomous and human-driven vehicles would be the norm of the autonomous vehicles practice for a while. On the one hand, unlike autonomous vehicles, human-driven vehicles could exhibit sudden abnormal behaviors such as unpredictably switching to dangerous driving modes, putting its neighboring vehicles under risks; such undesired mode switching could arise from numbers of human driver factors, including fatigue, drunkenness, distraction, aggressiveness, etc. On the other hand, modern vehicle-to-vehicle communication technologies enable the autonomous vehicles to efficiently and reliably share the scarce run-time information with each other. In this paper, we propose, to the best of our knowledge, the first efficient algorithm that can (1) significantly improve trajectory prediction by effectively fusing the run-time information shared by surrounding autonomous vehicles, and can (2) accurately and quickly detect abnormal human driving mode switches or 
    
[^82]: MV-DeepSDF：使用多次扫描的点云进行自动驾驶中的三维车辆重建的隐式建模

    MV-DeepSDF: Implicit Modeling with Multi-Sweep Point Clouds for 3D Vehicle Reconstruction in Autonomous Driving. (arXiv:2309.16715v1 [cs.CV])

    [http://arxiv.org/abs/2309.16715](http://arxiv.org/abs/2309.16715)

    该论文提出了一个名为MV-DeepSDF的新框架，利用多次扫描的点云来重建自动驾驶中的三维车辆。与现有方法相比，该方法主要解决了嘈杂和稀疏点云的问题，并通过分析多次扫描的一致性和互补性来提高重建质量。

    

    从嘈杂和稀疏的部分点云中重建三维车辆对于自动驾驶具有重要意义。现有的大多数三维重建方法不能直接应用于这个问题，因为它们在处理带有微小噪声的密集输入时经过精心设计。在这项工作中，我们提出了一个新的框架，称为MV-DeepSDF，它从多次扫描的点云中估计最优的符号距离函数(SDF)表示，以重建野外车辆。尽管已经存在一些基于SDF的隐式建模方法，但它们只关注单视图的重建，导致重建质量较低。相反，我们首先分析了潜在特征空间中的多次扫描一致性和互补性，提出将隐式空间形状估计问题转化为元素到集合特征提取问题。然后，我们设计了一种新的架构来提取单独的元素级表示并将它们聚合起来生成一个集合级预测。

    Reconstructing 3D vehicles from noisy and sparse partial point clouds is of great significance to autonomous driving. Most existing 3D reconstruction methods cannot be directly applied to this problem because they are elaborately designed to deal with dense inputs with trivial noise. In this work, we propose a novel framework, dubbed MV-DeepSDF, which estimates the optimal Signed Distance Function (SDF) shape representation from multi-sweep point clouds to reconstruct vehicles in the wild. Although there have been some SDF-based implicit modeling methods, they only focus on single-view-based reconstruction, resulting in low fidelity. In contrast, we first analyze multi-sweep consistency and complementarity in the latent feature space and propose to transform the implicit space shape estimation problem into an element-to-set feature extraction problem. Then, we devise a new architecture to extract individual element-level representations and aggregate them to generate a set-level predic
    
[^83]: 无人机辅助语义通信与混合动作强化学习

    UAV-assisted Semantic Communication with Hybrid Action Reinforcement Learning. (arXiv:2309.16713v1 [cs.NI])

    [http://arxiv.org/abs/2309.16713](http://arxiv.org/abs/2309.16713)

    本文提出了一个利用无人机的上行语义通信方案，通过混合动作强化学习框架实现了在数据收集效率和计算能量成本之间的平衡，并取得了显著的改善结果。

    

    本文旨在探索利用无人机进行上行语义通信，以提高偏远地区元宇宙用户的数据收集效率。为了在平衡重建质量和计算能量成本之间减少上行数据收集时间，我们提出了一个混合动作强化学习框架，用于在语义模型规模、信道分配、传输功率和无人机轨迹上做出决策。变量被划分为离散类型和连续类型，并通过两个不同的强化学习代理进行优化以生成组合动作。仿真结果表明，所提出的混合动作强化学习框架能够在不同参数设置下有效提高上行语义数据收集的效率，并优于基准场景。

    In this paper, we aim to explore the use of uplink semantic communications with the assistance of UAV in order to improve data collection effiicency for metaverse users in remote areas. To reduce the time for uplink data collection while balancing the trade-off between reconstruction quality and computational energy cost, we propose a hybrid action reinforcement learning (RL) framework to make decisions on semantic model scale, channel allocation, transmission power, and UAV trajectory. The variables are classified into discrete type and continuous type, which are optimized by two different RL agents to generate the combined action. Simulation results indicate that the proposed hybrid action reinforcement learning framework can effectively improve the efficiency of uplink semantic data collection under different parameter settings and outperforms the benchmark scenarios.
    
[^84]: 通过依赖于变换的随机平滑，提供一般李普希茨：针对可解释的语义变换的认证健壮性

    General Lipschitz: Certified Robustness Against Resolvable Semantic Transformations via Transformation-Dependent Randomized Smoothing. (arXiv:2309.16710v1 [cs.CV])

    [http://arxiv.org/abs/2309.16710](http://arxiv.org/abs/2309.16710)

    我们提出了一种新的框架“一般李普希茨（GL）”，用于对神经网络进行认证，以抵御可组合的可解释的语义干扰。方法在ImageNet数据集上与最先进的方法表现相当。

    

    随机平滑是目前构建图像分类器的最先进方法，可以证明其对于有界干扰的抗性。然而，构建针对语义变换（例如，图像模糊、平移、Gamma矫正）及其组合的合理证书更为复杂。在本工作中，我们提出了一种新的框架“一般李普希茨（GL）”，用于对神经网络进行认证，以抵御可组合的可解释的语义干扰。在该框架中，我们分析了平滑分类器与变换参数之间的依赖关系，并推导出相应的健壮性证书。我们的方法在ImageNet数据集上与最先进的方法表现相当。

    Randomized smoothing is the state-of-the-art approach to construct image classifiers that are provably robust against additive adversarial perturbations of bounded magnitude. However, it is more complicated to construct reasonable certificates against semantic transformation (e.g., image blurring, translation, gamma correction) and their compositions. In this work, we propose \emph{General Lipschitz (GL),} a new framework to certify neural networks against composable resolvable semantic perturbations. Within the framework, we analyze transformation-dependent Lipschitz-continuity of smoothed classifiers w.r.t. transformation parameters and derive corresponding robustness certificates. Our method performs comparably to state-of-the-art approaches on the ImageNet dataset.
    
[^85]: AIR: 深度学习信息恢复中对抗攻击的威胁

    AIR: Threats of Adversarial Attacks on Deep Learning-Based Information Recovery. (arXiv:2309.16706v1 [cs.CR])

    [http://arxiv.org/abs/2309.16706](http://arxiv.org/abs/2309.16706)

    这项研究探讨了在对抗情况下，深度学习信息恢复模型的鲁棒性。通过对最先进的DeepReceiver模型进行对抗攻击，实验结果表明DeepReceiver对设计的攻击方法是脆弱的。

    

    无线通信系统通常由一个传输器和一个接收器组成，传输器将信息传输，接收器从接收到的扭曲信号中恢复原始信息。深度学习已被用于改善复杂信道环境下接收器的性能，并取得了最先进的性能。然而，其鲁棒性尚未被研究。为了评估深度学习信息恢复模型在对抗情况下的鲁棒性，我们研究了对最先进的深度学习信息恢复模型DeepReceiver的对抗攻击。我们将该问题建模为一个带有功率和峰均比 (PAPR) 约束的优化问题。我们根据对手对DeepReceiver模型和/或测试样本的了解设计了不同的对抗攻击方法。大量实验表明，在所有考虑的场景中，DeepReceiver对设计的攻击方法是脆弱的。

    A wireless communications system usually consists of a transmitter which transmits the information and a receiver which recovers the original information from the received distorted signal. Deep learning (DL) has been used to improve the performance of the receiver in complicated channel environments and state-of-the-art (SOTA) performance has been achieved. However, its robustness has not been investigated. In order to evaluate the robustness of DL-based information recovery models under adversarial circumstances, we investigate adversarial attacks on the SOTA DL-based information recovery model, i.e., DeepReceiver. We formulate the problem as an optimization problem with power and peak-to-average power ratio (PAPR) constraints. We design different adversarial attack methods according to the adversary's knowledge of DeepReceiver's model and/or testing samples. Extensive experiments show that the DeepReceiver is vulnerable to the designed attack methods in all of the considered scenari
    
[^86]: 在图谱领域中预测和解释车辆轨迹

    Prediction and Interpretation of Vehicle Trajectories in the Graph Spectral Domain. (arXiv:2309.16702v1 [cs.AI])

    [http://arxiv.org/abs/2309.16702](http://arxiv.org/abs/2309.16702)

    本研究提出了一种在图谱领域中预测车辆轨迹的深度学习网络，它使用了图谱表示的有益特征并在性能上取得了多达25%的提升。

    

    本研究对交通场景的图谱表示进行了全面分析和解释。通过基于时空车辆交互图的多维图傅里叶变换，观测到的交通场景可以转化到图谱领域。由于这些谱场景表示成功地融入了交通场景的复杂和交互性质，因此这种有益的特征表示被用于预测车辆轨迹。本研究介绍了GFTNNv2，一种在图谱领域中预测车辆轨迹的深度学习网络。在公开可用的高清数据集和NGSIM上评估了GFTNNv2，与最先进的预测方法相比，性能提升了多达25%。

    This work provides a comprehensive analysis and interpretation of the graph spectral representation of traffic scenarios. Based on a spatio-temporal vehicle interaction graph, an observed traffic scenario can be transformed into the graph spectral domain by means of the multidimensional Graph Fourier Transformation. Since these spectral scenario representations have shown to successfully incorporate the complex and interactive nature of traffic scenarios, the beneficial feature representation is employed for the purpose of predicting vehicle trajectories. This work introduces GFTNNv2, a deep learning network predicting vehicle trajectories in the graph spectral domain. Evaluation of the GFTNNv2 on the publicly available datasets highD and NGSIM shows a performance gain of up to 25% in comparison to state-of-the-art prediction approaches.
    
[^87]: MVMR: 在多个可靠视频集中评估自然语言视频定位偏差

    MVMR: Evaluating Natural Language Video Localization Bias over Multiple Reliable Videos Pool. (arXiv:2309.16701v1 [cs.CV])

    [http://arxiv.org/abs/2309.16701](http://arxiv.org/abs/2309.16701)

    本文提出了一个名为MVMR的任务，旨在给定文本查询从大量视频集中定位视频帧。我们通过已有数据集进行相似性筛选来构建数据集，并引入三个MVMR数据集。我们采用了嵌入式文本相似度匹配和视频-语言对齐技术来计算相关性得分，并为MVMR任务开发了一个强大的模型，Reliable Mutual Matching Network (RMMN)。

    

    随着近年来多媒体内容的激增，自然语言视频定位成为一个关键问题，它致力于检测与给定自然语言查询匹配的视频片段。然而，以往的研究都没有探索在存在多个正负视频的大量语料库中定位一个时刻。本文提出了一个名为MVMR（Massive Videos Moment Retrieval）的任务，旨在给定文本查询从大量视频集中定位视频帧。对于这个任务，我们提出了一种通过对现有视频定位数据集进行相似性筛选来构建数据集的方法，并引入了三个MVMR数据集。具体来说，我们采用基于嵌入的文本相似度匹配和视频-语言对齐技术来计算目标查询与视频之间的相关性得分，从而定义正负集。针对提出的MVMR任务，我们进一步开发了一个强大的模型，Reliable Mutual Matching Network (RMMN)。

    With the explosion of multimedia content in recent years, natural language video localization, which focuses on detecting video moment that matches a given natural language query, has become a critical problem. However, none of the previous research explores localizing a moment from a large corpus where multiple positive and negative videos exist. In this paper, we propose an MVMR (Massive Videos Moment Retrieval) task, which aims to localize video frames from a massive set of videos given a text query. For this task, we suggest methods for constructing datasets by employing similarity filtering on the existing video localization datasets and introduce three MVMR datasets. Specifically, we employ embedding-based text similarity matching and video-language grounding techniques to calculate the relevance score between a target query and videos to define positive and negative sets. For the proposed MVMR task, we further develop a strong model, Reliable Mutual Matching Network (RMMN), whic
    
[^88]: 在编程测试中非正当利益和ChatGPT滥用的识别：一个受控实验

    Inappropriate Benefits and Identification of ChatGPT Misuse in Programming Tests: A Controlled Experiment. (arXiv:2309.16697v1 [cs.AI])

    [http://arxiv.org/abs/2309.16697](http://arxiv.org/abs/2309.16697)

    这项研究通过受控实验发现，ChatGPT在编程测试中被滥用可能导致非正当利益和抄袭行为，并提出了手动识别ChatGPT辅助程序和学生对ChatGPT的看法的方法。研究结果显示，使用ChatGPT的学生完成编程测试的速度是没有使用ChatGPT的学生的两倍。

    

    虽然ChatGPT可以帮助学生学习编程，但它也可能被滥用来进行抄袭，这是违反学术诚信的行为。学生可以要求ChatGPT完成编程任务，并从他人的作品中生成解决方案，但却未能正确致谢来源。为了应对这种新型的抄袭行为，我们进行了一项受控实验，以衡量使用ChatGPT的非正当利益，包括完成时间和编程表现。我们还报告了如何通过学生在使用ChatGPT时的行为和通过调查了解学生对ChatGPT的看法来手动识别通过ChatGPT辅助完成的程序。实验中有17名学生参与，他们被要求完成两个编程测试，根据测试被分为两组：一组无需帮助完成测试，另一组可使用ChatGPT辅助完成。我们的研究表明，使用ChatGPT的学生完成编程测试的速度是没有使用ChatGPT的学生的两倍。

    While ChatGPT may help students to learn to program, it can be misused to do plagiarism, a breach of academic integrity. Students can ask ChatGPT to complete a programming task, generating a solution from other people's work without proper acknowledgment of the source(s). To help address this new kind of plagiarism, we performed a controlled experiment measuring the inappropriate benefits of using ChatGPT in terms of completion time and programming performance. We also reported how to manually identify programs aided with ChatGPT (via student behavior while using ChatGPT) and student perspective of ChatGPT (via a survey). Seventeen students participated in the experiment. They were asked to complete two programming tests. They were divided into two groups per the test: one group should complete the test without help while the other group should complete it with ChatGPT. Our study shows that students with ChatGPT complete programming tests two times faster than those without ChatGPT, th
    
[^89]: 对偶原理和生物合理学习：连接表示定理和Hebbian学习

    Duality Principle and Biologically Plausible Learning: Connecting the Representer Theorem and Hebbian Learning. (arXiv:2309.16687v1 [cs.NE])

    [http://arxiv.org/abs/2309.16687](http://arxiv.org/abs/2309.16687)

    本研究探索了一种规范方法，相似性匹配，用于推导和理解神经计算的算法基础，并且发现表示定理是研究生物合理学习算法的重要视角。

    

    最近引入了一种叫做相似性匹配的规范方法，用于推导和理解神经计算的算法基础，着重于无监督问题。它涉及从计算目标中推导算法，并评估其与解剖和生理观察的兼容性。特别地，它通过考虑双重替代而非主要形式的流行模型（如PCA）来引入神经结构。然而，它与表示定理的关联尚未被探索。在这项工作中，我们提出利用这种方法的教导来探索监督学习算法，并阐述了Hebbian学习的概念。我们研究了正则化的监督学习，并阐明了神经结构的出现以及加性更新和乘性更新规则之间的差异。在这项工作中，我们的重点不在于开发新算法，而在于展示表示定理提供了研究生物合理学习算法的完美视角。

    A normative approach called Similarity Matching was recently introduced for deriving and understanding the algorithmic basis of neural computation focused on unsupervised problems. It involves deriving algorithms from computational objectives and evaluating their compatibility with anatomical and physiological observations. In particular, it introduces neural architectures by considering dual alternatives instead of primal formulations of popular models such as PCA. However, its connection to the Representer theorem remains unexplored. In this work, we propose to use teachings from this approach to explore supervised learning algorithms and clarify the notion of Hebbian learning. We examine regularized supervised learning and elucidate the emergence of neural architecture and additive versus multiplicative update rules. In this work, we focus not on developing new algorithms but on showing that the Representer theorem offers the perfect lens to study biologically plausible learning alg
    
[^90]: 基于交替学习的稀疏语义传输的视觉传输

    Alternate Learning based Sparse Semantic Communications for Visual Transmission. (arXiv:2309.16681v1 [cs.IT])

    [http://arxiv.org/abs/2309.16681](http://arxiv.org/abs/2309.16681)

    本文提出了一种基于交替学习的稀疏语义传输系统SparseSBC，通过两个独立的深度神经网络模型进行编码和解码，解决了信道的非可微性问题，在视觉传输中展现出显著的性能。

    

    语义通信（SemCom）通过仅尝试恢复数据的基本语义信息，相对于传统的位级准确传输展现出很大的优势。为了解决信道的非可微性，本文提出了一种名为SparseSBC的基于交替学习的SemCom系统，用于视觉传输。SparseSBC在发射端和接收端分别利用两个基于深度神经网络（DNN）的模型，并以交替的方式学习编码和解码，而不是现有文献中的联合优化，以解决信道的非可微性。具体而言，还采用了“自批评”训练方案以实现稳定训练。此外，基于DNN的发射机使用二进制量化模块，在对语义准确性影响最小的基础上生成一组稀疏的位于推导“语义基”的位。广泛的仿真结果验证了SparseSBC的性能显著。

    Semantic communication (SemCom) demonstrates strong superiority over conventional bit-level accurate transmission, by only attempting to recover the essential semantic information of data. In this paper, in order to tackle the non-differentiability of channels, we propose an alternate learning based SemCom system for visual transmission, named SparseSBC. Specially, SparseSBC leverages two separate Deep Neural Network (DNN)-based models at the transmitter and receiver, respectively, and learns the encoding and decoding in an alternate manner, rather than the joint optimization in existing literature, so as to solving the non-differentiability in the channel. In particular, a ``self-critic" training scheme is leveraged for stable training. Moreover, the DNN-based transmitter generates a sparse set of bits in deduced ``semantic bases", by further incorporating a binary quantization module on the basis of minimal detrimental effect to the semantic accuracy. Extensive simulation results val
    
[^91]: 混合你自己的对比对

    Mixup Your Own Pairs. (arXiv:2309.16633v1 [cs.LG])

    [http://arxiv.org/abs/2309.16633](http://arxiv.org/abs/2309.16633)

    本文提出了一种名为SupReMix的方法，通过混合样本，特别是混合负样本和混合正样本，来解决回归问题中表示学习的挑战。这种方法能够提供更好的性能和更准确的回归结果。

    

    在表示学习中，回归问题传统上比分类问题受到的关注较少。直接应用为分类设计的表示学习技术到回归问题往往会导致潜空间中碎片化的表示，从而产生次优的性能。本文认为，由于忽视了两个关键方面：序序感知和难度，对于回归问题而言，对比学习的潜能被忽视了。为了解决这些挑战，我们提倡“混合自己的对比对进行监督性对比回归”，而不仅仅依靠真实/增强样本。具体来说，我们提出了混合式监督对比回归学习（SupReMix）。它在嵌入级别上以锚点包含的混合（锚点和一个不同的负样本的混合）作为困难负对，以锚点排除的混合（两个不同的负样本的混合）作为困难正对。这一策略形成了困难样本对学习的方式。

    In representation learning, regression has traditionally received less attention than classification. Directly applying representation learning techniques designed for classification to regression often results in fragmented representations in the latent space, yielding sub-optimal performance. In this paper, we argue that the potential of contrastive learning for regression has been overshadowed due to the neglect of two crucial aspects: ordinality-awareness and hardness. To address these challenges, we advocate "mixup your own contrastive pairs for supervised contrastive regression", instead of relying solely on real/augmented samples. Specifically, we propose Supervised Contrastive Learning for Regression with Mixup (SupReMix). It takes anchor-inclusive mixtures (mixup of the anchor and a distinct negative sample) as hard negative pairs and anchor-exclusive mixtures (mixup of two distinct negative samples) as hard positive pairs at the embedding level. This strategy formulates harde
    
[^92]: AutoCLIP: 自动调谐视觉语言模型的零样本分类器

    AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models. (arXiv:2309.16414v1 [cs.CV])

    [http://arxiv.org/abs/2309.16414](http://arxiv.org/abs/2309.16414)

    本研究提出了一种名为AutoCLIP的方法，用于自动调谐视觉语言模型的零样本分类器。AutoCLIP通过为每个提示模板分配图像特定的权重，从而改进了从编码类别描述符推导零样本分类器的方式。

    

    基于视觉语言模型（如CLIP）构建的分类器在广泛的图像分类任务中展现了出色的零样本性能。先前的工作研究了根据提示模板自动创建每个类别的描述符集的不同方式，包括手工设计的模板、从大型语言模型获取的模板以及从随机单词和字符构建的模板。然而，从相应的编码类别描述符导出零样本分类器几乎没有改变：将图像的平均编码类别描述符与编码图像之间的余弦相似度最大化以进行分类。然而，当某些描述符比其他描述符更好地匹配给定图像上的视觉线索时，将所有类别描述符等权重可能不是最优的。在这项工作中，我们提出了一种自动调谐零样本分类器的方法AutoCLIP。AutoCLIP为每个提示模板分配了图像特定的权重，这些权重是从s

    Classifiers built upon vision-language models such as CLIP have shown remarkable zero-shot performance across a broad range of image classification tasks. Prior work has studied different ways of automatically creating descriptor sets for every class based on prompt templates, ranging from manually engineered templates over templates obtained from a large language model to templates built from random words and characters. In contrast, deriving zero-shot classifiers from the respective encoded class descriptors has remained nearly unchanged, that is: classify to the class that maximizes the cosine similarity between its averaged encoded class descriptors and the encoded image. However, weighting all class descriptors equally can be suboptimal when certain descriptors match visual clues on a given image better than others. In this work, we propose AutoCLIP, a method for auto-tuning zero-shot classifiers. AutoCLIP assigns to each prompt template per-image weights, which are derived from s
    
[^93]: LogicMP: 一种将一阶逻辑约束编码的神经符号方法

    LogicMP: A Neuro-symbolic Approach for Encoding First-order Logic Constraints. (arXiv:2309.15458v1 [cs.AI])

    [http://arxiv.org/abs/2309.15458](http://arxiv.org/abs/2309.15458)

    本文提出了一种名为LogicMP的新颖神经层，该层通过均场变分推断将一阶逻辑约束编码进神经网络中。通过有效缓解一阶逻辑模型的推断困难，LogicMP在图形、图像和文本任务中表现出比竞争对手更好的性能和效率。

    

    将一阶逻辑约束与神经网络集成是一个关键但具有挑战性的问题，因为它涉及建模复杂的相关性以满足约束。本文提出了一种新颖的神经层LogicMP，其层对MLN进行均场变分推断。它可以插入任何现成的神经网络以编码一阶逻辑约束，同时保持模块化和效率。通过利用MLN中的结构和对称性，我们从理论上证明了我们设计良好、高效的均场迭代能够有效缓解MLN推断的困难，将推断从顺序计算降低为一系列并行的张量操作。在图形、图像和文本的三类任务上的实证结果表明，LogicMP在性能和效率上都优于先进的竞争对手。

    Integrating first-order logic constraints (FOLCs) with neural networks is a crucial but challenging problem since it involves modeling intricate correlations to satisfy the constraints. This paper proposes a novel neural layer, LogicMP, whose layers perform mean-field variational inference over an MLN. It can be plugged into any off-the-shelf neural network to encode FOLCs while retaining modularity and efficiency. By exploiting the structure and symmetries in MLNs, we theoretically demonstrate that our well-designed, efficient mean-field iterations effectively mitigate the difficulty of MLN inference, reducing the inference from sequential calculation to a series of parallel tensor operations. Empirical results in three kinds of tasks over graphs, images, and text show that LogicMP outperforms advanced competitors in both performance and efficiency.
    
[^94]: 最大扩散强化学习

    Maximum Diffusion Reinforcement Learning. (arXiv:2309.15293v1 [cs.LG])

    [http://arxiv.org/abs/2309.15293](http://arxiv.org/abs/2309.15293)

    最大扩散强化学习是一种克服强化学习中数据相关性问题的方法，通过解耦代理的经验实现持续学习，并在各种测试中表现出色。

    

    所有机器学习都建立在数据独立且同分布的假设上。然而，在强化学习中，当数据是依次从代理经验中收集而来时，这一假设通常不成立。因此，我们提出了一种名为最大扩散强化学习的方法，利用统计力学中的遍历过程来克服这些限制。我们的方法通过解耦代理的经验，可证明地使代理在单次部署中能够持续学习，而不受初始化方式的影响。此外，我们证明了我们的方法推广了众所周知的最大熵技术，并且通过在流行的基准测试中稳定超过了最先进的性能水平。我们的研究成果极大地促进了物理学、学习和控制的交叉领域，为强化学习代理（如行走机器人和自动驾驶汽车）的透明可靠决策提供了一条道路。

    The assumption that data are independent and identically distributed underpins all machine learning. When data are collected sequentially from agent experiences this assumption does not generally hold, as in reinforcement learning. Here, we derive a method that overcomes these limitations by exploiting the statistical mechanics of ergodic processes, which we term maximum diffusion reinforcement learning. By decorrelating agent experiences, our approach provably enables agents to learn continually in single-shot deployments regardless of how they are initialized. Moreover, we prove our approach generalizes well-known maximum entropy techniques, and show that it robustly exceeds state-of-the-art performance across popular benchmarks. Our results at the nexus of physics, learning, and control pave the way towards more transparent and reliable decision-making in reinforcement learning agents, such as locomoting robots and self-driving cars.
    
[^95]: 使用场景先验的可推广神经场进行3D重建

    3D Reconstruction with Generalizable Neural Fields using Scene Priors. (arXiv:2309.15164v1 [cs.CV])

    [http://arxiv.org/abs/2309.15164](http://arxiv.org/abs/2309.15164)

    使用场景先验的可推广神经场方法，利用单个RGB-D图像映射场景，无需融合模块即可重建完整的场景，具有较好的灵活性和扩展性。

    

    最近神经场的进展极大地推动了高保真度的3D场景重建。然而，大多数现有方法对每个场景都要从头开始训练一个独立的网络，这种方法不具有可扩展性，效率低下，且在有限视角下无法产生良好的结果。而基于学习的多视图立体方法在一定程度上解决了这个问题，但其多视图设置使得它在扩展和广泛应用方面不太灵活。相反，我们引入了训练可推广的融入场景先验的神经场（NFPs）方法。NFP网络将任何单视角的RGB-D图像映射成有符号距离和辐射值。通过在体积空间中合并单帧，可以重建完整的场景，无需融合模块，从而提供更好的灵活性。场景先验可以在大规模数据集上进行训练，使得能够快速适应使用较少视角重建新场景。NFP不仅展示了SOTA场景重建性能，

    High-fidelity 3D scene reconstruction has been substantially advanced by recent progress in neural fields. However, most existing methods train a separate network from scratch for each individual scene. This is not scalable, inefficient, and unable to yield good results given limited views. While learning-based multi-view stereo methods alleviate this issue to some extent, their multi-view setting makes it less flexible to scale up and to broad applications. Instead, we introduce training generalizable Neural Fields incorporating scene Priors (NFPs). The NFP network maps any single-view RGB-D image into signed distance and radiance values. A complete scene can be reconstructed by merging individual frames in the volumetric space WITHOUT a fusion module, which provides better flexibility. The scene priors can be trained on large-scale datasets, allowing for fast adaptation to the reconstruction of a new scene with fewer views. NFP not only demonstrates SOTA scene reconstruction performa
    
[^96]: Supersonic: 学习在C/C++中生成源代码优化

    Supersonic: Learning to Generate Source Code Optimisations in C/C++. (arXiv:2309.14846v1 [cs.SE])

    [http://arxiv.org/abs/2309.14846](http://arxiv.org/abs/2309.14846)

    Supersonic 是一个神经方法，用于在C/C++中进行源代码优化。与GPT-3.5-Turbo和GPT-4相比，它在代码优化任务上表现更好，并且改变的程度更小。

    

    软件优化在保持功能的同时改善资源效率。传统上，这是由开发人员和编译器完成的过程。本文介绍了第三种选择，即在源代码级别进行自动优化。我们提出了Supersonic，一个针对优化的轻微源代码修改的神经方法。使用seq2seq模型，Supersonic在C / C ++程序对（$x_{t}$，$x_{t+1}$）上进行训练，其中$x_{t+1}$是$x_{t}$的优化版本，并输出一个差异。Supersonic的性能在竞技编程任务上与OpenAI的GPT-3.5-Turbo和GPT-4进行了基准测试。实验表明，Supersonic不仅在代码优化任务上胜过了这两个模型，而且改变的程度比GPT-3.5-Turbo小了600多倍，比GPT-4小了3700多倍。

    Software optimization refines programs for resource efficiency while preserving functionality. Traditionally, it is a process done by developers and compilers. This paper introduces a third option, automated optimization at the source code level. We present Supersonic, a neural approach targeting minor source code modifications for optimization. Using a seq2seq model, Supersonic is trained on C/C++ program pairs ($x_{t}$, $x_{t+1}$), where $x_{t+1}$ is an optimized version of $x_{t}$, and outputs a diff. Supersonic's performance is benchmarked against OpenAI's GPT-3.5-Turbo and GPT-4 on competitive programming tasks. The experiments show that Supersonic not only outperforms both models on the code optimization task, but also minimizes the extent of change with a more than 600x smaller than GPT-3.5-Turbo and 3700x smaller than GPT-4.
    
[^97]: 图像分类器的多个不同解释

    Multiple Different Explanations for Image Classifiers. (arXiv:2309.14309v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.14309](http://arxiv.org/abs/2309.14309)

    这篇论文介绍了一种算法和工具，可以为图像分类器的输出计算多个解释，从而提高对分类器行为的洞察力。

    

    现有的图像分类器解释工具通常只会给出一种对于图像的解释。然而，对于许多图像来说，无论是人类还是图像分类器都接受多个解释来解释图像标签。因此，限制解释的数量只有一个严重限制了对分类器行为的洞察力。在本文中，我们描述了一种算法和工具REX，用于计算黑盒图像分类器对给定图像的输出的多个解释。我们的算法基于因果理论的可靠方法。我们分析了其理论复杂性，并提供了实验结果，显示REX在ImageNet-mini基准测试中找到的多个解释比之前的工作多7倍。

    Existing explanation tools for image classifiers usually give only one single explanation for an image. For many images, however, both humans and image classifiers accept more than one explanation for the image label. Thus, restricting the number of explanations to just one severely limits the insight into the behavior of the classifier. In this paper, we describe an algorithm and a tool, REX, for computing multiple explanations of the output of a black-box image classifier for a given image. Our algorithm uses a principled approach based on causal theory. We analyse its theoretical complexity and provide experimental results showing that REX finds multiple explanations on 7 times more images than the previous work on the ImageNet-mini benchmark.
    
[^98]: Fast-HuBERT: 一种高效的自监督语音表示学习训练框架

    Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning. (arXiv:2309.13860v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.13860](http://arxiv.org/abs/2309.13860)

    Fast-HuBERT是一种高效的自监督语音表示学习训练框架，通过分析HuBERT预训练过程中的计算成本，并引入一系列效率优化策略，实现了在Librispeech 960h基准上的5.2倍加速，并且在前人工作中得到了一致的改进。

    

    近年来，自监督学习（SSL）方法在语音处理任务中取得了显著进展。各种基于语音的SSL模型已经开发出来，并在一系列下游任务（包括语音识别）上展示了有希望的性能。然而，现有的基于语音的SSL模型在计算成本方面面临着共同的困境，这可能会阻碍它们的潜在应用和深入学术研究。为了解决这个问题，我们首先分析了HuBERT预训练过程中不同模块的计算成本，然后引入了一系列效率优化策略，即本文中提出的Fast-HuBERT。所提出的Fast-HuBERT可以在Librispeech 960h基准上使用8个V100 GPU在1.1天内进行训练，而不会导致性能下降，相比原始实现，加速了5.2倍。此外，我们还探索了Fast-HuBERT中的两种经过充分研究的技术，并展示了与之前工作中报告的一致的改进。

    Recent years have witnessed significant advancements in self-supervised learning (SSL) methods for speech-processing tasks. Various speech-based SSL models have been developed and present promising performance on a range of downstream tasks including speech recognition. However, existing speech-based SSL models face a common dilemma in terms of computational cost, which might hinder their potential application and in-depth academic research. To address this issue, we first analyze the computational cost of different modules during HuBERT pre-training and then introduce a stack of efficiency optimizations, which is named Fast-HuBERT in this paper. The proposed Fast-HuBERT can be trained in 1.1 days with 8 V100 GPUs on the Librispeech 960h benchmark, without performance degradation, resulting in a 5.2x speedup, compared to the original implementation. Moreover, we explore two well-studied techniques in the Fast-HuBERT and demonstrate consistent improvements as reported in previous work.
    
[^99]: AI-企业优化的协同辅助：一个框架和在生产调度中的案例研究。

    AI-Copilot for Business Optimisation: A Framework and A Case Study in Production Scheduling. (arXiv:2309.13218v1 [cs.AI])

    [http://arxiv.org/abs/2309.13218](http://arxiv.org/abs/2309.13218)

    这篇论文提出了一个AI-企业优化的协同辅助系统，通过采用大型语言模型和微调预训练模型的方法，实现了减少人类专业知识需求的目标。

    

    企业优化是寻找和实施高效和具有成本效益的运营方式，以为企业带来竞争优势的过程。综合问题表述是企业优化的一个重要组成部分，它围绕着人类专业知识展开，因此很有可能成为瓶颈。随着大型语言模型（LLMs）的最新进展，通过人工智能（AI）可以潜在地减少问题表述中所需的人类专业知识。然而，开发用于问题表述的LLM具有挑战性，由于训练数据要求、令牌限制以及LLM中缺乏适当的性能度量。为了减少大量训练数据的需求，最近人们开始关注对预训练的LLM进行微调以适应下游任务，而不是从头开始训练一个特定任务的LLM。在本文中，我们采用了这种方法，提出了一个AI-企业优化的协同辅助系统。

    Business optimisation is the process of finding and implementing efficient and cost-effective means of operation to bring a competitive advantage for businesses. Synthesizing problem formulations is an integral part of business optimisation which is centred around human expertise, thus with a high potential of becoming a bottleneck. With the recent advancements in Large Language Models (LLMs), human expertise needed in problem formulation can potentially be minimized using Artificial Intelligence (AI). However, developing a LLM for problem formulation is challenging, due to training data requirements, token limitations, and the lack of appropriate performance metrics in LLMs. To minimize the requirement of large training data, considerable attention has recently been directed towards fine-tuning pre-trained LLMs for downstream tasks, rather than training a LLM from scratch for a specific task. In this paper, we adopt this approach and propose an AI-Copilot for business optimisation by 
    
[^100]: DRG-LLaMA: 调优LLaMA模型以预测住院患者的诊断相关分组

    DRG-LLaMA : Tuning LLaMA Model to Predict Diagnosis-related Group for Hospitalized Patients. (arXiv:2309.12625v1 [cs.AI])

    [http://arxiv.org/abs/2309.12625](http://arxiv.org/abs/2309.12625)

    DRG-LLaMA是一个通过在临床笔记上细调的大型语言模型，用于改进住院患者的诊断相关分组预测。在多个评估指标上，DRG-LLaMA-7B相对于其他模型取得了显著的改进，并能够高准确度地预测基本DRG和并发症/合并症（CC）/重大并发症或合并症（MCC）的状态。

    

    在美国住院付费系统中，诊断相关分组（DRG）起着关键作用，但目前的分组过程耗时。我们引入了DRG-LLaMA，一个在临床笔记上进行细调的大型语言模型（LLM），以改善DRG预测。使用Meta的LLaMA作为基础模型，我们通过在236,192个MIMIC-IV出院摘要上进行低秩适应（LoRA）优化。在输入令牌长度为512的情况下，DRG-LLaMA-7B实现的宏平均F1分数为0.327，顶级预测准确度为52.0％，宏平均AUC为0.986。令人印象深刻的是，DRG-LLaMA-7B在这个任务上超过了之前报道的领先模型，相对于ClinicalBERT的宏平均F1分数提高了40.3％，相对于CAML提高了35.7％。当应用DRG-LLaMA来预测基本DRG和并发症/合并症（CC）/重大并发症或合并症（MCC）时，基本DRG的顶级预测准确度达到了67.8％，而CC/MCC状态的预测准确度达到了67.5％。

    In the U.S. inpatient payment system, the Diagnosis-Related Group (DRG) plays a key role but its current assignment process is time-consuming. We introduce DRG-LLaMA, a large language model (LLM) fine-tuned on clinical notes for improved DRG prediction. Using Meta's LLaMA as the base model, we optimized it with Low-Rank Adaptation (LoRA) on 236,192 MIMIC-IV discharge summaries. With an input token length of 512, DRG-LLaMA-7B achieved a macro-averaged F1 score of 0.327, a top-1 prediction accuracy of 52.0% and a macro-averaged Area Under the Curve (AUC) of 0.986. Impressively, DRG-LLaMA-7B surpassed previously reported leading models on this task, demonstrating a relative improvement in macro-averaged F1 score of 40.3% compared to ClinicalBERT and 35.7% compared to CAML. When DRG-LLaMA is applied to predict base DRGs and complication or comorbidity (CC) / major complication or comorbidity (MCC), the top-1 prediction accuracy reached 67.8% for base DRGs and 67.5% for CC/MCC status. DRG-L
    
[^101]: 对比解码提高大型语言模型的推理能力

    Contrastive Decoding Improves Reasoning in Large Language Models. (arXiv:2309.09117v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.09117](http://arxiv.org/abs/2309.09117)

    对比解码是一种简单、计算量轻、无需训练的文本生成方法，通过最大化强模型和弱模型之间的似然差异，在各种推理任务上取得了显著改进。

    

    我们证明了对比解码——一种由Li等人提出的简单、计算量轻、无需训练的文本生成方法——在各种推理任务上超过贪婪解码的性能。最初被证明可以改善长文本生成的感知质量，对比解码搜索最大化强模型和弱模型之间似然差异加权的字符串。我们展示了对比解码在HellaSwag常识推理基准上使LLaMA-65B超过了LLaMA 2、GPT-3.5和PaLM 2-L，并且在GSM8K数学单词推理基准上超过了LLaMA 2、GPT-3.5和PaLM-540B，此外还在一系列其他任务上有了改进。分析表明，对比解码通过防止某些抽象推理错误以及避免在思维链条中复制输入的部分等简单模式来改进现有方法。整体而言，对比解码表现突出。

    We demonstrate that Contrastive Decoding -- a simple, computationally light, and training-free text generation method proposed by Li et al 2022 -- achieves large out-of-the-box improvements over greedy decoding on a variety of reasoning tasks. Originally shown to improve the perceived quality of long-form text generation, Contrastive Decoding searches for strings that maximize a weighted difference in likelihood between strong and weak models. We show that Contrastive Decoding leads LLaMA-65B to outperform LLaMA 2, GPT-3.5 and PaLM 2-L on the HellaSwag commonsense reasoning benchmark, and to outperform LLaMA 2, GPT-3.5 and PaLM-540B on the GSM8K math word reasoning benchmark, in addition to improvements on a collection of other tasks. Analysis suggests that Contrastive Decoding improves over existing methods by preventing some abstract reasoning errors, as well as by avoiding simpler modes such as copying sections of the input during chain-of-thought. Overall, Contrastive Decoding outp
    
[^102]: 离线逆向强化学习下的提示评估与优化

    Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning. (arXiv:2309.06553v1 [cs.CL])

    [http://arxiv.org/abs/2309.06553](http://arxiv.org/abs/2309.06553)

    这项工作介绍了一种基于离线逆向强化学习的提示评估与优化方法，通过利用离线数据集和逆向强化学习，预测提示性能、提高成本效益、生成易读的结果。

    

    最近，像ChatGPT这样的大型语言模型（LLM）的发展取得了显著的性能，通过利用人类专业知识。然而，充分揭示LLMs在复杂任务中的潜力需要在自然语言提示的广阔搜索空间中进行导航。虽然提示工程显示出潜力，但试错尝试中所需的人工设计提示和相关成本带来了重大挑战。关键是，提示优化的效率取决于昂贵的提示评估过程。本工作介绍了Prompt-OIRL，这是一种基于离线逆向强化学习的方法，旨在弥合有效提示评估和可负担性之间的差距。我们的方法利用专家评估的离线数据集，运用逆向强化学习获得一个针对离线、查询依赖型提示评估的奖励模型。Prompt-OIRL的优点是多方面的：它预测提示的性能，成本高效，生成易读的结果。

    The recent advances in the development of Large Language Models (LLMs) like ChatGPT have achieved remarkable performance by leveraging human expertise. Yet, fully eliciting LLMs' potential for complex tasks requires navigating the vast search space of natural language prompts. While prompt engineering has shown promise, the requisite human-crafted prompts in trial-and-error attempts and the associated costs pose significant challenges. Crucially, the efficiency of prompt optimization hinges on the costly procedure of prompt evaluation. This work introduces Prompt-OIRL, an approach rooted in offline inverse reinforcement learning that seeks to bridge the gap between effective prompt evaluation and affordability. Our method draws on offline datasets from expert evaluations, employing Inverse-RL to derive a reward model for offline, query-dependent prompt evaluations. The advantages of Prompt-OIRL are manifold: it predicts prompt performance, is cost-efficient, produces human-readable res
    
[^103]: PACE: 使用GPT-4进行云事件根本原因分析中的提示和增加以进行校准的置信度估计

    PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis. (arXiv:2309.05833v1 [cs.CL])

    [http://arxiv.org/abs/2309.05833](http://arxiv.org/abs/2309.05833)

    本文提出了一种通过提示检索增强的大语言模型（LLM）来增强云事件根本原因分析工具中置信度估计的方法。

    

    近年来，IT行业向基于云的平台的转变强调了云事件根本原因分析的重要性，以确保服务的可靠性和维护客户信任。核心问题是有效确定根本原因，由于当代云基础设施的复杂性，这一任务变得具有挑战性。尽管出现了许多用于根本原因识别的基于AI的工具，但它们的适用性仍受到其输出质量不一致的限制。本文介绍了一种通过提示检索增强的大语言模型（LLM）来增强根本原因分析工具中置信度估计的方法。此方法分为两个阶段。首先，模型根据历史事件数据评估自身的置信度，考虑其对证据的评估强度。然后，模型审核由预测器生成的根本原因。然后，优化步骤将这些评估结合起来确定最终的置信度估计。

    In recent years, the transition to cloud-based platforms in the IT sector has emphasized the significance of cloud incident root cause analysis to ensure service reliability and maintain customer trust. Central to this process is the efficient determination of root causes, a task made challenging due to the complex nature of contemporary cloud infrastructures. Despite the proliferation of AI-driven tools for root cause identification, their applicability remains limited by the inconsistent quality of their outputs. This paper introduces a method for enhancing confidence estimation in root cause analysis tools by prompting retrieval-augmented large language models (LLMs). This approach operates in two phases. Initially, the model evaluates its confidence based on historical incident data, considering its assessment of the evidence strength. Subsequently, the model reviews the root cause generated by the predictor. An optimization step then combines these evaluations to determine the fin
    
[^104]: Neural-Hidden-CRF: 一种鲁棒的弱监督序列标注模型

    Neural-Hidden-CRF: A Robust Weakly-Supervised Sequence Labeler. (arXiv:2309.05086v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.05086](http://arxiv.org/abs/2309.05086)

    Neural-Hidden-CRF是一种鲁棒的弱监督序列标注模型，通过引入隐藏CRF层和利用强大的语言模型，它在各项基准测试中取得了最新的最优结果。

    

    我们提出了一种基于神经网络的无向图模型 Neural-Hidden-CRF 来解决弱监督序列标注问题。在概率无向图理论的框架下，Neural-Hidden-CRF融合了一个隐藏CRF层，用于建模单词序列、潜在真实序列和弱标签序列之间的变量关系并具有全局视角。在Neural-Hidden-CRF中，我们可以利用强大的语言模型BERT或其他深度模型为潜在真实序列提供丰富的上下文语义知识，并使用隐藏的CRF层来捕捉内部标签之间的依赖关系。Neural-Hidden-CRF在概念上简单而在实践中强大。它在一个众包基准测试和三个弱监督基准测试上取得了最新的最优结果，包括在平均泛化和推理性能方面超过了最新的高级模型CHMM的2.80 F1分和2.23 F1分。

    We propose a neuralized undirected graphical model called Neural-Hidden-CRF to solve the weakly-supervised sequence labeling problem. Under the umbrella of probabilistic undirected graph theory, the proposed Neural-Hidden-CRF embedded with a hidden CRF layer models the variables of word sequence, latent ground truth sequence, and weak label sequence with the global perspective that undirected graphical models particularly enjoy. In Neural-Hidden-CRF, we can capitalize on the powerful language model BERT or other deep models to provide rich contextual semantic knowledge to the latent ground truth sequence, and use the hidden CRF layer to capture the internal label dependencies. Neural-Hidden-CRF is conceptually simple and empirically powerful. It obtains new state-of-the-art results on one crowdsourcing benchmark and three weak-supervision benchmarks, including outperforming the recent advanced model CHMM by 2.80 F1 points and 2.23 F1 points in average generalization and inference perfo
    
[^105]: 通讯和参考曲目在混音过程中的作用：来自专业混音师的见解

    The Role of Communication and Reference Songs in the Mixing Process: Insights from Professional Mix Engineers. (arXiv:2309.03404v1 [cs.HC])

    [http://arxiv.org/abs/2309.03404](http://arxiv.org/abs/2309.03404)

    本论文研究了专业混音师与客户的互动以及他们如何利用客户的反馈指导混音过程。结果表明，混音师通过使用参考曲目和合作沟通等方式，与客户建立起对混音期望音效的默契约定。

    

    有效的音乐混音需要技术和创造力的精湛，但与客户的清晰沟通至关重要。混音师必须理解客户的期望和偏好，并共同努力实现所需的音效。通过使用参考曲目和艺术家与工程师之间交换的演示混音等指南，通常可以达成对混音期望音效的默契约定，有时还可以使用语义术语进行言说。本文介绍了一项由两个阶段构成的探索性研究的发现，旨在了解专业混音师如何与客户互动，并利用他们的反馈指导混音过程。在第一阶段，对五名混音师进行了半结构化面谈，旨在了解他们的沟通策略、创造过程和决策标准。基于这些访谈的推论，设计了一个在线问卷，并对22名混音师进行了调查。

    Effective music mixing requires technical and creative finesse, but clear communication with the client is crucial. The mixing engineer must grasp the client's expectations, and preferences, and collaborate to achieve the desired sound. The tacit agreement for the desired sound of the mix is often established using guides like reference songs and demo mixes exchanged between the artist and the engineer and sometimes verbalised using semantic terms. This paper presents the findings of a two-phased exploratory study aimed at understanding how professional mixing engineers interact with clients and use their feedback to guide the mixing process. For phase one, semi-structured interviews were conducted with five mixing engineers with the aim of gathering insights about their communication strategies, creative processes, and decision-making criteria. Based on the inferences from these interviews, an online questionnaire was designed and administered to a larger group of 22 mixing engineers 
    
[^106]: 预训练的神经推荐系统：一种可迁移的零样本框架

    Pre-trained Neural Recommenders: A Transferable Zero-Shot Framework for Recommendation Systems. (arXiv:2309.01188v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2309.01188](http://arxiv.org/abs/2309.01188)

    本论文提出了一种预训练的神经推荐系统框架，可以在新领域中构建推荐系统，减少或无需重新训练，并且不需要使用任何辅助信息。通过利用用户-项目交互矩阵的统计特征，实现了零样本推荐的挑战。

    

    现代神经协同过滤技术对于电子商务、社交媒体和内容共享平台的成功至关重要。然而，尽管技术有所进步，但对于每个新的应用领域，我们仍需要从头开始训练一个NCF模型。相反，预训练的视觉和语言模型通常直接应用于各种应用程序，要么是零样本情况，要么是有限的微调。受到预训练模型的影响，我们探索了在新领域中支持构建推荐系统的预训练模型的可能性，只需最少或不需要重新训练，而无需使用任何辅助用户或项目信息。零样本推荐在没有辅助信息的情况下是具有挑战性的，因为当没有重叠的用户或项目时，我们无法在数据集之间建立用户和项目之间的关联。我们的基本见解是用户-项目交互矩阵的统计特征在不同领域中普遍可用。

    Modern neural collaborative filtering techniques are critical to the success of e-commerce, social media, and content-sharing platforms. However, despite technical advances -- for every new application domain, we need to train an NCF model from scratch. In contrast, pre-trained vision and language models are routinely applied to diverse applications directly (zero-shot) or with limited fine-tuning. Inspired by the impact of pre-trained models, we explore the possibility of pre-trained recommender models that support building recommender systems in new domains, with minimal or no retraining, without the use of any auxiliary user or item information. Zero-shot recommendation without auxiliary information is challenging because we cannot form associations between users and items across datasets when there are no overlapping users or items. Our fundamental insight is that the statistical characteristics of the user-item interaction matrix are universally available across different domains 
    
[^107]: Jais和Jais-chat：以阿拉伯语为中心的基础和指导调优的开放式生成式大型语言模型

    Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models. (arXiv:2308.16149v1 [cs.CL])

    [http://arxiv.org/abs/2308.16149](http://arxiv.org/abs/2308.16149)

    Jais和Jais-chat是新的以阿拉伯语为中心的开放式生成式大型语言模型，具有13亿参数，在阿拉伯语方面表现出优异的知识和推理能力，并且在英语方面也具有竞争力。这些模型的发布旨在促进阿拉伯语LLMs的研究。

    

    我们介绍了Jais和Jais-chat，这是新的最先进的以阿拉伯语为中心的基础和指导调优的开放式生成式大型语言模型（LLMs）。这些模型基于GPT-3的仅解码器架构，并在阿拉伯语和英语文本的混合物中进行预训练，包括各种编程语言的源代码。这些模型具有130亿个参数，根据广泛的评估结果，在阿拉伯语知识和推理能力方面表现优于任何现有的开放式阿拉伯语和多语言模型。此外，尽管在训练时使用的英语数据要少得多，但这些模型在英语方面与类似规模的以英语为中心的开放模型相比仍具有竞争力。我们提供了模型的训练、调优、安全对齐和评估的详细描述。我们发布了模型的两个开放版本--基础Jais模型和指导调优的Jais-chat变种--旨在促进阿拉伯语LLMs的研究。详见https://hugging

    We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://hugging
    
[^108]: 一种针对多任务学习的尺度不变任务平衡方法

    A Scale-Invariant Task Balancing Approach for Multi-Task Learning. (arXiv:2308.12029v1 [cs.LG])

    [http://arxiv.org/abs/2308.12029](http://arxiv.org/abs/2308.12029)

    这篇论文提出了一种尺度不变的多任务学习方法（SI-MTL），通过对任务损失进行对数变换和对任务梯度进行归一化，解决了多任务学习中的任务平衡问题，并在多个基准数据集上取得了领先的性能。

    

    多任务学习（MTL）是一种同时学习多个相关任务的学习范式，在各个领域取得了巨大的成功。然而，任务平衡仍然是MTL中的一个重要挑战，损失/梯度尺度的不平衡经常导致性能折中。本文提出了一种尺度不变的多任务学习（SI-MTL）方法，从损失和梯度角度缓解了任务平衡问题。具体来说，SI-MTL包含对所有任务损失进行的对数变换，以确保在损失水平上具有尺度不变性，以及一种梯度平衡方法SI-G，它将所有任务的梯度归一化为与最大梯度范数相同的大小。在几个基准数据集上进行的大量实验一致证明了SI-G的有效性和SI-MTL的最先进性能。

    Multi-task learning (MTL), a learning paradigm to learn multiple related tasks simultaneously, has achieved great success in various fields. However, task-balancing remains a significant challenge in MTL, with the disparity in loss/gradient scales often leading to performance compromises. In this paper, we propose a Scale-Invariant Multi-Task Learning (SI-MTL) method to alleviate the task-balancing problem from both loss and gradient perspectives. Specifically, SI-MTL contains a logarithm transformation which is performed on all task losses to ensure scale-invariant at the loss level, and a gradient balancing method, SI-G, which normalizes all task gradients to the same magnitude as the maximum gradient norm. Extensive experiments conducted on several benchmark datasets consistently demonstrate the effectiveness of SI-G and the state-of-the-art performance of SI-MTL.
    
[^109]: 从视频中生成姿势调节的虚拟角色

    Pose Modulated Avatars from Video. (arXiv:2308.11951v1 [cs.CV])

    [http://arxiv.org/abs/2308.11951](http://arxiv.org/abs/2308.11951)

    本文提出了一种基于神经辐射场和稀疏的摄像机组来重建动态人体运动和形状的方法。与现有的角色模型不同，我们的方法在频域中是自适应和显式的，并通过建模身体部位之间的相关性来解决衣物和皮肤的变形建模挑战。实验结果表明，我们的网络在性能上优于现有方法。

    

    使用由骨架驱动的神经辐射场（NeRF）和稀疏的摄像机组可以重建动态人体运动和形状。然而，模拟衣物和皮肤的变形与骨架姿势之间的关系仍然是一个挑战。与隐式学习或依赖代理表面的现有角色模型不同，我们的方法源于观察到不同的姿势需要不同的频率分配。忽视这种区别会在平滑区域产生噪点伪影，或使锐利区域的细粒度纹理和形状细节模糊。我们开发了一个在频域中是自适应和显式的双分支神经网络。第一个分支是一个图神经网络，本地建模身体部位之间的相关性，以骨架姿势作为输入。第二个分支将这些相关特征组合到一组全局频率中，然后调节特征编码。我们的实验证明，我们的网络优于现有的状态。

    It is now possible to reconstruct dynamic human motion and shape from a sparse set of cameras using Neural Radiance Fields (NeRF) driven by an underlying skeleton. However, a challenge remains to model the deformation of cloth and skin in relation to skeleton pose. Unlike existing avatar models that are learned implicitly or rely on a proxy surface, our approach is motivated by the observation that different poses necessitate unique frequency assignments. Neglecting this distinction yields noisy artifacts in smooth areas or blurs fine-grained texture and shape details in sharp regions. We develop a two-branch neural network that is adaptive and explicit in the frequency domain. The first branch is a graph neural network that models correlations among body parts locally, taking skeleton pose as input. The second branch combines these correlation features to a set of global frequencies and then modulates the feature encoding. Our experiments demonstrate that our network outperforms state
    
[^110]: 在大型语言模型中使用反向推理进行验证

    Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])

    [http://arxiv.org/abs/2308.07758](http://arxiv.org/abs/2308.07758)

    本文研究了在大型语言模型中使用反向推理进行验证的方法。作者提出了一种新颖的技术，通过屏蔽问题中的一个标记，并要求语言模型预测被屏蔽的标记来验证候选答案。同时，作者还提出了一种结合正向和反向推理的方法来估计候选答案的概率。

    

    链式思考（Chain-of-Though, CoT）提示在各种推理任务中表现出了很好的性能。最近，Self-Consistency提出了一种方法，即通过采样一组不同的推理链，这些链可能导致不同的答案，然后选择得票最多的答案。本文提出了一种新颖的方法，即在验证候选答案时使用反向推理。我们使用一个简单的模板，即``如果我们知道上述问题的答案是候选答案，那么未知变量x的值是多少？''，将问题中的一个标记屏蔽，并要求语言模型预测被屏蔽的标记。直观上讲，如果提供的候选答案是正确的，语言模型应该能够成功预测被屏蔽的标记。我们进一步提出了FOBAR方法，将正向和反向推理结合起来估计候选答案的概率。我们在六个数据集和三个实验中进行了广泛的实验。

    Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
    
[^111]: 为机器人堆积方块任务构建因果性概率框架

    Towards a Causal Probabilistic Framework for Prediction, Action-Selection & Explanations for Robot Block-Stacking Tasks. (arXiv:2308.06203v1 [cs.RO])

    [http://arxiv.org/abs/2308.06203](http://arxiv.org/abs/2308.06203)

    这项工作提出了一个新颖的因果性概率框架，用于解决机器人堆积方块任务的问题，通过结合因果推断，使机器人能够理解、推理和解释其环境。

    

    现实世界中的不确定性意味着系统设计者无法预测并明确设计出机器人可能遇到的所有场景。因此，以这种方式设计的机器人在高度受控的环境之外容易出现故障。因果模型提供了一个原则性的框架，用于编码机器人与其环境相互作用的因果关系的形式化知识，并结合现实世界机器人通常遇到的噪声和不确定性的概率表示。结合因果推断，这些模型使自主代理能够理解、推理和解释其环境。在这项工作中，我们关注机器人堆积方块任务的问题，因为它展示了许多应用所需的基本感知和操作能力，包括仓库物流和家庭人工支持机器人。我们提出了一个新颖的因果性概率框架，将物理模拟功能嵌入到这个任务中。

    Uncertainties in the real world mean that is impossible for system designers to anticipate and explicitly design for all scenarios that a robot might encounter. Thus, robots designed like this are fragile and fail outside of highly-controlled environments. Causal models provide a principled framework to encode formal knowledge of the causal relationships that govern the robot's interaction with its environment, in addition to probabilistic representations of noise and uncertainty typically encountered by real-world robots. Combined with causal inference, these models permit an autonomous agent to understand, reason about, and explain its environment. In this work, we focus on the problem of a robot block-stacking task due to the fundamental perception and manipulation capabilities it demonstrates, required by many applications including warehouse logistics and domestic human support robotics. We propose a novel causal probabilistic framework to embed a physics simulation capability int
    
[^112]: 时间常识推理与获取概述

    An Overview Of Temporal Commonsense Reasoning and Acquisition. (arXiv:2308.00002v1 [cs.AI])

    [http://arxiv.org/abs/2308.00002](http://arxiv.org/abs/2308.00002)

    本文综述了时间常识推理领域的研究进展，重点关注通过增强语言模型的性能来提高推理能力，并对多个数据集进行评估。然而，这些增强模型仍然难以达到人类水平的推理能力。

    

    时间常识推理是指理解短语、动作和事件的典型时间背景并将其应用于需要这种知识的问题推理的能力。这种能力在时间自然语言处理任务中至关重要，可能应用于时间线摘要、时间问答和时间自然语言推断等方面。最近的研究表明，大型语言模型虽然善于生成语法正确的句子和解决分类任务，但在推理过程中往往会采取捷径，并陷入简单的语言陷阱。本文章概述了在时间常识推理领域的研究，特别关注通过各种增强方式提高语言模型的性能以及对越来越多数据集的评估。然而，这些增强模型在推理任务上仍然难以达到人类的水平。

    Temporal commonsense reasoning refers to the ability to understand the typical temporal context of phrases, actions, and events, and use it to reason over problems requiring such knowledge. This trait is essential in temporal natural language processing tasks, with possible applications such as timeline summarization, temporal question answering, and temporal natural language inference. Recent research on the performance of large language models suggests that, although they are adept at generating syntactically correct sentences and solving classification tasks, they often take shortcuts in their reasoning and fall prey to simple linguistic traps. This article provides an overview of research in the domain of temporal commonsense reasoning, particularly focusing on enhancing language model performance through a variety of augmentations and their evaluation across a growing number of datasets. However, these augmented models still struggle to approach human performance on reasoning task
    
[^113]: XSkill：跨体现技能发现

    XSkill: Cross Embodiment Skill Discovery. (arXiv:2307.09955v1 [cs.RO])

    [http://arxiv.org/abs/2307.09955](http://arxiv.org/abs/2307.09955)

    本研究提出了XSkill，一种跨体现的技能发现框架，能够从无标签的人类和机器人操纵视频中纯粹地发现跨体现技能原型，并通过条件扩散策略将这些技能转移到机器人动作中，在未见任务中完成学习到的技能的组合。仿真和真实环境中的实验结果表明，这些发现的技能原型能够有效地促进技能转移和组合，从而构建出更通用和可扩展的模仿学习框架。

    

    人类示范视频是机器人学习的广泛数据源，并且是表达所需行为的直观用户界面。然而，直接从非结构化的人类视频中提取可重用的机器人操纵技能面临着体现差异和未观察到的行动参数的挑战。为了弥合这种体现差距，本文介绍了XSkill，一种模仿学习框架，它从无标签的人类和机器人操纵视频中纯粹地发现名为技能原型的跨体现表示，使用条件扩散策略将技能表示转移到机器人动作，并最终使用人类提示视频完成学习到的技能来完成未见任务。我们在仿真和真实环境中的实验表明，发现的技能原型促进了未见任务的技能转移和组合，从而实现了更通用和可扩展的模仿学习框架。

    Human demonstration videos are a widely available data source for robot learning and an intuitive user interface for expressing desired behavior. However, directly extracting reusable robot manipulation skills from unstructured human videos is challenging due to the big embodiment difference and unobserved action parameters. To bridge this embodiment gap, this paper introduces XSkill, an imitation learning framework that 1) discovers a cross-embodiment representation called skill prototypes purely from unlabeled human and robot manipulation videos, 2) transfers the skill representation to robot actions using conditional diffusion policy, and finally, 3) composes the learned skill to accomplish unseen tasks specified by a human prompt video. Our experiments in simulation and real-world environments show that the discovered skill prototypes facilitate both skill transfer and composition for unseen tasks, resulting in a more general and scalable imitation learning framework. The performan
    
[^114]: 在大规模异构信息网络上通过渐进抽样进行长程元路径搜索

    Long-range Meta-path Search through Progressive Sampling on Large-scale Heterogeneous Information Networks. (arXiv:2307.08430v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2307.08430](http://arxiv.org/abs/2307.08430)

    本论文研究了在大规模异构信息网络中利用长程依赖的重要性，并提出了一种名为长程元路径搜索和渐进抽样（LMSPS）的自动框架。通过动态缩小搜索空间和专门的元路径选择，实验证明了LMSPS的有效性。

    

    长程依赖的利用在同质图中有广泛研究，但在大规模异构信息网络中很少研究，这主要是由于高成本和有效信息利用的困难。为此，我们研究了不同元路径的重要性，并提出了一种在异构信息网络中利用长程依赖的自动框架，称为长程元路径搜索和渐进抽样（LMSPS）。具体来说，为了在没有先验知识的情况下发现各种数据集或任务的元路径，我们开发了一个包含所有目标节点相关元路径的搜索空间。通过渐进抽样算法，我们动态地缩小搜索空间，以跳数无关的时间复杂度驱动，从而得到一个由当前异构信息网络和任务驱动的紧凑搜索空间。利用抽样评估策略作为指导，我们进行了专门和具有表达力的元路径选择。对八个异构数据集进行的大量实验证明了LMSPS的有效性。

    Utilizing long-range dependency, though extensively studied in homogeneous graphs, is rarely studied in large-scale heterogeneous information networks (HINs), whose main challenge is the high costs and the difficulty in utilizing effective information. To this end, we investigate the importance of different meta-paths and propose an automatic framework for utilizing long-range dependency in HINs, called Long-range Meta-path Search through Progressive Sampling (LMSPS). Specifically, to discover meta-paths for various datasets or tasks without prior, we develop a search space with all target-node-related meta-paths. With a progressive sampling algorithm, we dynamically shrink the search space with hop-independent time complexity, leading to a compact search space driven by the current HIN and task. Utilizing a sampling evaluation strategy as the guidance, we conduct a specialized and expressive meta-path selection. Extensive experiments on eight heterogeneous datasets demonstrate that LM
    
[^115]: Espaloma-0.3.0: 用于蛋白质-配体系统模拟的机器学习分子力学力场及其扩展

    Espaloma-0.3.0: Machine-learned molecular mechanics force field for the simulation of protein-ligand systems and beyond. (arXiv:2307.07085v1 [physics.chem-ph])

    [http://arxiv.org/abs/2307.07085](http://arxiv.org/abs/2307.07085)

    Espaloma-0.3.0是一个用于蛋白质-配体系统模拟的机器学习分子力学力场，通过能量和力的拟合纳入量子化学数据进行训练，具有灵活性和可扩展性。

    

    分子力学（MM）力场是通过简单的一对一和多项式项来表征分子系统能量景观的模型。传统上，它们依赖于人工专家策划、不灵活且难以扩展的离散化化学参数赋值规则，即原子或价态类型。最近，人们对使用图神经网络替代此过程，并使参数化方案能够直接从量子化学计算或凝聚相数据中以端到端的可微分方式进行学习表示出了巨大兴趣。在本文中，我们通过将能量和力的适应性拟合直接纳入训练过程，扩展了Espaloma的端到端可微分力场构建方法。基于OpenMM SPICE数据集，我们策划了一个包含与生物分子建模广泛相关的化学空间的数据集，涵盖小分子、蛋白质和RNA。最终得到了一个适用于蛋白质-配体系统模拟的机器学习分子力学力场。

    Molecular mechanics (MM) force fields -- the models that characterize the energy landscape of molecular systems via simple pairwise and polynomial terms -- have traditionally relied on human expert-curated, inflexible, and poorly extensible discrete chemical parameter assignment rules, namely atom or valence types. Recently, there has been significant interest in using graph neural networks to replace this process, while enabling the parametrization scheme to be learned in an end-to-end differentiable manner directly from quantum chemical calculations or condensed-phase data. In this paper, we extend the Espaloma end-to-end differentiable force field construction approach by incorporating both energy and force fitting directly to quantum chemical data into the training process. Building on the OpenMM SPICE dataset, we curate a dataset containing chemical spaces highly relevant to the broad interest of biomolecular modeling, covering small molecules, proteins, and RNA. The resulting for
    
[^116]: ProbVLM: 冻结视觉-语言模型的概率适配器

    ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models. (arXiv:2307.00398v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.00398](http://arxiv.org/abs/2307.00398)

    ProbVLM是一种概率适配器，用于估计大规模视觉-语言模型中嵌入的概率分布，以解决固有的嵌入歧义问题，并在多个数据集上展示了其在检索任务中的优越性能表现。

    

    大规模视觉-语言模型（VLM）如CLIP成功地在图像和文本之间找到对应关系。通过标准的确定性映射过程，将图像或文本样本映射到嵌入空间中的一个向量。这是有问题的：由于多个样本（图像或文本）可以抽象出物理世界中的相同概念，确定性嵌入不反映嵌入空间中的固有歧义性。我们提出了ProbVLM，一种概率适配器，通过事后方式在预训练的VLM中通过内部/外部模态对齐估计嵌入的概率分布，而无需大规模数据集或计算。在四个具有挑战性的数据集上，即COCO、Flickr、CUB和Oxford-flowers，我们估计了两个VLM（CLIP和BLIP）的多模态嵌入不确定性，量化了嵌入不确定性在检索任务中的校准，并表明ProbVLM优于其他方法。此外，我们提出了主动学习和模型...

    Large-scale vision-language models (VLMs) like CLIP successfully find correspondences between images and text. Through the standard deterministic mapping process, an image or a text sample is mapped to a single vector in the embedding space. This is problematic: as multiple samples (images or text) can abstract the same concept in the physical world, deterministic embeddings do not reflect the inherent ambiguity in the embedding space. We propose ProbVLM, a probabilistic adapter that estimates probability distributions for the embeddings of pre-trained VLMs via inter/intra-modal alignment in a post-hoc manner without needing large-scale datasets or computing. On four challenging datasets, i.e., COCO, Flickr, CUB, and Oxford-flowers, we estimate the multi-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantify the calibration of embedding uncertainties in retrieval tasks and show that ProbVLM outperforms other methods. Furthermore, we propose active learning and model 
    
[^117]: 通过强化指令调整来减轻大规模多模态模型中的幻觉问题

    Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning. (arXiv:2306.14565v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.14565](http://arxiv.org/abs/2306.14565)

    本论文通过引入第一个大型多样化的视觉指令调整数据集，提出了一种解决大规模多模态模型中幻觉问题的方法。通过设计包含正负指令的数据集和提出的评估方法，能够更准确地衡量模型产生的幻觉。

    

    尽管多模态任务取得了可喜的进展，但当前的大规模多模态模型（LMM）很容易在描述图像和人类指令时产生不一致的幻觉。本文通过引入第一个大型多样化的视觉指令调整数据集LRV-Instruction来解决这个问题。我们的数据集包含由GPT4生成的12万个视觉指令，涵盖了16个开放式指令和答案的视觉与语言任务。与现有研究主要关注正指令样本不同，我们设计了LRV-Instruction以包含更多针对更强的视觉指令调整的正负指令。我们的负指令在两个语义层次上设计：（i）不存在元素操作和（ii）存在元素操作。为了更有效地衡量LMM所产生的幻觉，我们提出了一种新的方法，即GPT4辅助的视觉指令评估（GAVIE）。

    Despite the promising progress in multi-modal tasks, current large multi-modal models (LMM) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction. Our dataset consists of 120k visual instructions generated by GPT4, covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at two semantic levels: (i) Nonexistent Element Manipulation and (ii) Existent Element Manipulation. To efficiently measure the hallucination generated by LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a novel approach to eva
    
[^118]: 虚假黎明：重新评估谷歌强化学习在芯片宏观布局中的应用

    The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement. (arXiv:2306.09633v1 [cs.LG])

    [http://arxiv.org/abs/2306.09633](http://arxiv.org/abs/2306.09633)

    谷歌2021年在《自然》杂志上发表的一篇论文声称其使用强化学习在芯片设计领域进行了创新，但两项独立的评估表明，谷歌的方法不如人类设计师、不如一个众所周知的算法（模拟退火），并且也不如普遍可用的商业软件，文章的完整性也遭到了严重的损害。

    

    谷歌2021年在《自然》杂志上发表的有关使用强化学习设计芯片的论文，因为所声称的结果缺乏充分的文件记录和关键步骤的说明，引发争议并受到媒体的批评报道。 而两项独立的评估填补了空白，证明谷歌强化学习落后于人类设计师、落后于一种众所周知的算法（模拟退火），并且还落后于普遍可用的商业软件。交叉检查的数据表明，由于行为、分析和报告中的错误，该《自然》文章的完整性受到了严重的损害。

    Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.
    
[^119]: Ada-NAV：用于机器人导航的自适应轨迹优化策略学习方法

    Ada-NAV: Adaptive Trajectory-Based Sample Efficient Policy Learning for Robotic Navigation. (arXiv:2306.06192v1 [cs.RO])

    [http://arxiv.org/abs/2306.06192](http://arxiv.org/abs/2306.06192)

    Ada-NAV是一种自适应轨迹优化策略学习方法，采用降低策略随机性的方法平衡探索与利用，提高机器人导航任务的采样效率。在真实世界的测试中表现优异，可以在更短的采样时间内取得更高的性能。

    

    强化学习方法在学习机器人导航策略方面十分有效，但其采样效率低的问题也十分明显。在策略优化中，这种效率低下部分来自于未能适当地平衡探索与利用的问题，特别是在面对非静态时。为了加入探索与利用的平衡以提高采样效率，我们提出了Ada-NAV，一种自适应轨迹长度方案，其中长度随着策略的随机性（用其Shannon或差分熵表示）的减小而增加。我们的自适应轨迹长度方案由于更频繁的梯度更新强调了训练开始时的探索，后来则更强调利用。在网格世界，仿真机器人环境和真实世界机器人实验中，我们证明了该方法的优点，表现在性能和采样效率上均优于常数和随机采样的轨迹长度。在固定的样本预算下，相对于现有的基准方法，Ada-NAV的性能提高了高达46％，采样数量减少了高达80％。

    Reinforcement learning methods, while effective for learning robotic navigation strategies, are known to be highly sample inefficient. This sample inefficiency comes in part from not suitably balancing the explore-exploit dilemma, especially in the presence of non-stationarity, during policy optimization. To incorporate a balance of exploration-exploitation for sample efficiency, we propose Ada-NAV, an adaptive trajectory length scheme where the length grows as a policy's randomness, represented by its Shannon or differential entropy, decreases. Our adaptive trajectory length scheme emphasizes exploration at the beginning of training due to more frequent gradient updates and emphasizes exploitation later on with longer trajectories. In gridworld, simulated robotic environments, and real-world robotic experiments, we demonstrate the merits of the approach over constant and randomly sampled trajectory lengths in terms of performance and sample efficiency. For a fixed sample budget, Ada-N
    
[^120]: 基于动态特征重构信号图的旋转机械故障识别

    Fault Identification of Rotating Machinery Based on Dynamic Feature Reconstruction Signal Graph. (arXiv:2306.05281v1 [eess.SP])

    [http://arxiv.org/abs/2306.05281](http://arxiv.org/abs/2306.05281)

    本文提出了一种动态特征重构信号图法，在旋转机械故障诊断模型中取得了关键进展，能够动态地选择最优子带的特征系数矩阵，进行适应性信号重构，并从中提取深层特征。

    

    为了提高在旋转机械强噪声下识别故障的性能，本文提出了一种动态特征重构信号图法，它在所提出的端到端故障诊断模型中起着关键作用。具体来说，首先利用小波包分解（WPD）将原始机械信号分解为包括系数矩阵在内的多个子带。然后，利用最初定义的两个要素MDD和DDD，提出了一种基于L2能量范数的动态特征选择方法（DFSL），它可以根据能量范数分布的差异动态地选择WPD的特征系数矩阵，使每个子信号能够进行适应性信号重构。接下来，将最优特征子带的系数矩阵进行重构和重新组合，得到特征信号图。最后，利用2D-卷积神经网络（2D-CNN）从特征信号图中提取深层特征。

    To improve the performance in identifying the faults under strong noise for rotating machinery, this paper presents a dynamic feature reconstruction signal graph method, which plays the key role of the proposed end-to-end fault diagnosis model. Specifically, the original mechanical signal is first decomposed by wavelet packet decomposition (WPD) to obtain multiple subbands including coefficient matrix. Then, with originally defined two feature extraction factors MDD and DDD, a dynamic feature selection method based on L2 energy norm (DFSL) is proposed, which can dynamically select the feature coefficient matrix of WPD based on the difference in the distribution of norm energy, enabling each sub-signal to take adaptive signal reconstruction. Next the coefficient matrices of the optimal feature sub-bands are reconstructed and reorganized to obtain the feature signal graphs. Finally, deep features are extracted from the feature signal graphs by 2D-Convolutional neural network (2D-CNN). Ex
    
[^121]: 不要相信你的眼睛：关于特征可视化的（不）可靠性。

    Don't trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v1 [cs.CV])

    [http://arxiv.org/abs/2306.04719](http://arxiv.org/abs/2306.04719)

    本文探讨了神经网络如何从像素中提取模式的问题，并研究了特征可视化的可靠性。实验证据表明，由于优化过程中固有的限制，特征可视化能够可靠理解的功能集非常有限，对于解释神经网络如何处理自然图像的解释能力产生怀疑。

    

    神经网络是如何从像素中提取模式的？特征可视化通过优化来可视化高激活的模式，试图回答这个重要问题。如今，可视化方法构成了我们对神经网络内部工作的了解的基础，作为一种机械式的可解释性。在这里，我们问：特征可视化有多可靠？我们通过开发网络电路来诈骗特征可视化，使其显示完全与自然输入的正常网络行为毫无联系的任意模式。然后，我们提供证据表明在标准，未操纵网络中发生了类似的现象：特征可视化与标准输入处理非常不同，对神经网络如何处理自然图像的解释能力产生怀疑。我们通过理论证明支撑这一经验发现，由于优化过程中固有的限制，可以通过特征可视化可靠理解的功能集极其有限。

    How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to "explain" how neural networks process natural images. We underpin this empirical finding by theory proving that the set of functions that can be reliably understood by feature visualization is extr
    
[^122]: 用贝叶斯推理模拟人类类人概念学习

    Modeling Human-like Concept Learning with Bayesian Inference over Natural Language. (arXiv:2306.02797v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02797](http://arxiv.org/abs/2306.02797)

    该论文通过在自然语言中进行贝叶斯推理来模拟人类类人概念学习，使用大型语言模型作为提议分布并拟合先验以更好地模拟人类学习者，并在生成性和逻辑性概念上进行实验评估。

    

    我们通过在自然语言中进行贝叶斯推理来模拟对抽象符号概念的学习。为了高效推理，我们使用一个大型语言模型作为提议分布。我们根据人类数据拟合先验以更好地模拟人类学习者，并在生成性和逻辑性概念上进行评估。

    We model learning of abstract symbolic concepts by performing Bayesian inference over utterances in natural language. For efficient inference, we use a large language model as a proposal distribution. We fit a prior to human data to better model human learners, and evaluate on both generative and logical concepts.
    
[^123]: 结构化大离散动作空间的动态邻域构建

    Dynamic Neighborhood Construction for Structured Large Discrete Action Spaces. (arXiv:2305.19891v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19891](http://arxiv.org/abs/2305.19891)

    本研究针对无法处理的结构化大离散动作空间（SLDAS）提出了一种名为动态邻域构建（DNC）的新型利用策略，通过可扩展的邻域探索启发式方法，高效地探索连续代理动作周围的离散邻域。

    

    大离散动作空间（LDAS）是强化学习中的一个核心挑战。现有的解决方案可以处理多达几百万个动作的非结构化LDAS。然而，在物流、生产和运输系统等许多现实应用中，动作空间具有组合结构，其规模甚至在小规模实例上也超过了数百万个动作。幸运的是，这样的动作空间呈现出一定的结构，例如等间距的离散资源单位。在这项工作中，我们专注于处理当前基准测试无法处理的结构化LDAS（SLDAS）。我们提出了一种名为动态邻域构建（DNC）的新型利用策略，用于SLDAS。我们提出了一种可扩展的邻域探索启发式方法，利用这种策略，在具有高达$10^{73}$个动作的结构化动作空间中高效地探索连续代理动作周围的离散邻域。我们通过与三个标杆算法进行基准测试来展示我们方法的性能。

    Large discrete action spaces (LDAS) remain a central challenge in reinforcement learning. Existing solution approaches can handle unstructured LDAS with up to a few million actions. However, many real-world applications in logistics, production, and transportation systems have combinatorial action spaces, whose size grows well beyond millions of actions, even on small instances. Fortunately, such action spaces exhibit structure, e.g., equally spaced discrete resource units. With this work, we focus on handling structured LDAS (SLDAS) with sizes that cannot be handled by current benchmarks: we propose Dynamic Neighborhood Construction (DNC), a novel exploitation paradigm for SLDAS. We present a scalable neighborhood exploration heuristic that utilizes this paradigm and efficiently explores the discrete neighborhood around the continuous proxy action in structured action spaces with up to $10^{73}$ actions. We demonstrate the performance of our method by benchmarking it against three sta
    
[^124]: 上下文视觉变换器用于强健的表示学习

    Contextual Vision Transformers for Robust Representation Learning. (arXiv:2305.19402v1 [cs.CV])

    [http://arxiv.org/abs/2305.19402](http://arxiv.org/abs/2305.19402)

    上下文视觉变换器(ContextViT)用于生成图像的鲁棒特征表示，引入了一个额外的上下文令牌，可以解释掉特定于组的协变量结构，同时保持跨组共享的核心视觉特征，能够在监督微调、半监督学习以及主动学习等方面得到应用。

    

    本文介绍了一种上下文视觉变换器(ContextViT)的方法，用于生成图像的鲁棒特征表示，特别是针对表现出分组结构的图像，如协变量。ContextViT引入了一个额外的上下文令牌来编码特定于组的信息，允许模型解释掉特定于组的协变量结构，同时保持跨组共享的核心视觉特征。具体而言，在给定输入图像的情况下，Context-ViT将共享相同协变量的图像映射到该上下文令牌，并添加到输入图像令牌中，以捕获将模型条件化为组成员身份的效果。此外，我们还引入了一个上下文推断网络，可以在运行时预测这些令牌，只需要给出一些来自组分布的样本即可，使得ContextViT可以推广到新的测试分布。我们通过各种应用程序说明了ContextViT的性能。在监督微调中，我们证明了将预训练的ViTs与额外的上下文令牌相结合可以提高图像分类基准的准确性。我们还展示了ContextViT可以用于半监督学习，在CIFAR-10数据集上仅使用10%的标记样本即可实现最先进的表现。最后，我们展示了ContextViT可以通过引导选择来自少数群体的更具信息价值的样本，从而提高主动学习的效率。

    We present Contextual Vision Transformers (ContextViT), a method for producing robust feature representations for images exhibiting grouped structure such as covariates. ContextViT introduces an extra context token to encode group-specific information, allowing the model to explain away group-specific covariate structures while keeping core visual features shared across groups. Specifically, given an input image, Context-ViT maps images that share the same covariate into this context token appended to the input image tokens to capture the effects of conditioning the model on group membership. We furthermore introduce a context inference network to predict such tokens on the fly given a few samples from a group distribution, enabling ContextViT to generalize to new testing distributions at inference time. We illustrate the performance of ContextViT through a diverse range of applications. In supervised fine-tuning, we demonstrate that augmenting pre-trained ViTs with additional context 
    
[^125]: 多尺度正负样本检测AI生成文本

    Multiscale Positive-Unlabeled Detection of AI-Generated Texts. (arXiv:2305.18149v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.18149](http://arxiv.org/abs/2305.18149)

    本文提出了一种多尺度正负样本的训练框架，以解决多尺度AI生成文本的检测问题。通过将短机器文本标记为“未标记”来重新表述文本分类问题，并提出了一个规则化损失函数来优化检测性能，有效性能显著优于现有的方法。

    

    最近发布的大型语言模型（LLM）如ChatGPT等在生成类似于人类的文本方面令人惊讶，但它们可能被用于制造虚假的学术文本、虚假新闻、虚假推特等。先前的作品提出了检测这些多尺度AI生成文本的方法，包括简单的ML分类器、基于预训练模型的训练不可知方法和精调的语言分类模型。然而，主流检测器在构建时没有考虑到文本长度的因素：短文本的缺乏信息特征，使其更难检测。针对多尺度文本检测的挑战，本文提出了一个多尺度正负样本（MPU）训练框架。首先，我们承认短的机器文本具有类人属性，并将文本分类重新表述为一个正负样本问题，即通过在训练期间标记这些短的机器文本为"unlabeled"来解决这个问题。在这个正负样本的背景下，我们提出了

    Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are astonishing at generating human-like texts, but they may get misused for fake scholarly texts, fake news, fake tweets, et cetera. Previous works have proposed methods to detect these multiscale AI-generated texts, including simple ML classifiers, pretrained-model-based training-agnostic methods, and finetuned language classification models. However, mainstream detectors are formulated without considering the factor of corpus length: shorter corpuses are harder to detect compared with longer ones for shortage of informative features. In this paper, a Multiscale Positive-Unlabeled (MPU) training framework is proposed to address the challenge of multiscale text detection. Firstly, we acknowledge the human-resemblance property of short machine texts, and rephrase text classification as a Positive-Unlabeled (PU) problem by marking these short machine texts as "unlabeled" during training. In this PU context, we propose the le
    
[^126]: 基于内核KL散度的自监督学习方法Kernel-SSL

    Kernel-SSL: Kernel KL Divergence for Self-supervised Learning. (arXiv:2305.17326v1 [cs.LG])

    [http://arxiv.org/abs/2305.17326](http://arxiv.org/abs/2305.17326)

    本文提出了一种名为Kernel-SSL的自监督学习方法，将多种现有非对比学习方法建立在了再生核希尔伯特空间（RKHS）理解之上并优化了其中的均值嵌入和协方差算子，实验结果显示，在ImageNet数据集下表现显著超越最先进的方法，提高了4.6%。

    

    对比学习通常将一个正锚点样本与许多负样本进行比较，来完成自监督学习（SSL）。相反，非对比学习，例如BYOL、SimSiam和Barlow Twins等方法，在没有显式使用负样本的情况下完成SSL。受对比学习现有分析的启发，我们提供了多种现有非对比学习方法的再生核希尔伯特空间（RKHS）理解。随后，我们提出了一种新的损失函数Kernel-SSL，直接优化RKHS中的均值嵌入和协方差算子。实验中，我们的方法Kernel-SSL在线性评估设置下在ImageNet数据集上大幅优于最先进的方法。具体来说，在进行100个epoch的预训练时，我们的方法比SimCLR表现提高了4.6%。

    Contrastive learning usually compares one positive anchor sample with lots of negative samples to perform Self-Supervised Learning (SSL). Alternatively, non-contrastive learning, as exemplified by methods like BYOL, SimSiam, and Barlow Twins, accomplishes SSL without the explicit use of negative samples. Inspired by the existing analysis for contrastive learning, we provide a reproducing kernel Hilbert space (RKHS) understanding of many existing non-contrastive learning methods. Subsequently, we propose a novel loss function, Kernel-SSL, which directly optimizes the mean embedding and the covariance operator within the RKHS. In experiments, our method Kernel-SSL outperforms state-of-the-art methods by a large margin on ImageNet datasets under the linear evaluation settings. Specifically, when performing 100 epochs pre-training, our method outperforms SimCLR by 4.6%.
    
[^127]: $L_{2}$正则线性深度神经网络中隐性SGD偏差：从高秩到低秩的单向跳跃。

    Implicit bias of SGD in $L_{2}$-regularized linear DNNs: One-way jumps from high to low rank. (arXiv:2305.16038v1 [cs.LG])

    [http://arxiv.org/abs/2305.16038](http://arxiv.org/abs/2305.16038)

    在$L_{2}$-正则化线性深度神经网络中，使用SGD会产生从更高秩最小值到更低秩最小值的单向跳跃，并且不会跳回。

    

    具有多个隐藏层的深度线性网络（DLN）的$L_{2}$正则化损失具有多个局部最小值，对应于具有不同秩的矩阵。在矩阵完成等任务中，目标是收敛到最小秩局部最小值，该局部最小值仍适合训练数据。虽然可以轻松避免低估秩的局部最小值，因为它们不适合数据，但梯度下降可能会陷入高估秩的局部最小值。我们证明，使用SGD，总是有从更高秩最小值跳跃到更低秩最小值的概率，但跳回的概率为零。更精确地说，我们定义了一系列集合$B_{1}\subset B_{2}\subset\cdots\subset B_{R}$，使得$B_{r}$包含秩$r$或更少的所有最小值（而不是更多），对于足够小的岭参数$\lambda$和学习率$\eta$，它们是吸收的：SGD离开$B_{r}$的概率为0，从任何起点开始，SGD进入$B_{r}$的概率非零。

    The $L_{2}$-regularized loss of Deep Linear Networks (DLNs) with more than one hidden layers has multiple local minima, corresponding to matrices with different ranks. In tasks such as matrix completion, the goal is to converge to the local minimum with the smallest rank that still fits the training data. While rank-underestimating minima can easily be avoided since they do not fit the data, gradient descent might get stuck at rank-overestimating minima. We show that with SGD, there is always a probability to jump from a higher rank minimum to a lower rank one, but the probability of jumping back is zero. More precisely, we define a sequence of sets $B_{1}\subset B_{2}\subset\cdots\subset B_{R}$ so that $B_{r}$ contains all minima of rank $r$ or less (and not more) that are absorbing for small enough ridge parameters $\lambda$ and learning rates $\eta$: SGD has prob. 0 of leaving $B_{r}$, and from any starting point there is a non-zero prob. for SGD to go in $B_{r}$.
    
[^128]: 基于Sinkhorn距离的特征对其N-BEATS

    Feature-aligned N-BEATS with Sinkhorn divergence. (arXiv:2305.15196v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15196](http://arxiv.org/abs/2305.15196)

    这是一个基于Sinkhorn距离的特征对其N-BEATS模型，它通过对齐堆栈中的边际特征概率测度来进行领域广义的时间序列预测，同时保留了N-BEATS的可解释性和预测能力。

    

    我们提出了基于Sinkhorn距离的特征对其N-BEATS作为一个领域广义时间序列预测模型。它是N-BEATS的非平凡扩展，采用了双重残差叠加原则（Oreshkin等人[42]）并将其转化为一个表示学习框架。具体而言，它围绕着由N-BEATS每个堆栈的残差和特征提取算子的复杂组合产生的边际特征概率测度，并通过一种近似最优传输距离（Sinkhorn距离）将它们堆叠地对齐。训练损失由来自多个源域的经验风险最小化（即预测损失）和Sinkhorn距离计算的对齐损失组成，使得模型能够在多个源数据序列中堆叠地学习不变特征，同时保留N-BEATS的可解释设计和预测能力。我们提供了全面的实验评估和消融研究，并展示了相应的结果。

    We propose Feature-aligned N-BEATS as a domain-generalized time series forecasting model. It is a nontrivial extension of N-BEATS with doubly residual stacking principle (Oreshkin et al.[42]) into a representation learning framework. In particular, it revolves around marginal feature probability measures induced by the intricate composition of residual and feature extracting operators of N-BEATS in each stack and aligns them stack-wisely via an approximate of an optimal transport distance referred to as the Sinkhorn divergence. The training loss consists of an empirical risk minimization from multiple source domains, i.e., forecasting loss, and an alignment loss calculated with the Sinkhorn divergence, which allows the model to learn invariant features stack-wisely across multiple source data sequences while retaining N-BEATS's interpretable design and forecasting power. Comprehensive experimental evaluations with ablation studies are provided and the corresponding results demonstrate 
    
[^129]: 不精确标签学习：学习各种不精确标签配置的统一框架

    Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations. (arXiv:2305.12715v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.12715](http://arxiv.org/abs/2305.12715)

    本文提出了不精确标签学习（ILL）框架，利用期望最大化算法对不精确标签信息进行最大似然估计，为各种不精确标签配置问题提供了统一的解决方案。

    

    本文介绍了不精确标签学习（ILL）框架，这是一种处理机器学习任务中普遍存在的各种不精确标签配置的统一方法。ILL利用期望最大化（EM）算法对不精确标签信息进行最大似然估计（MLE），将精确标签视为潜在变量。与以前试图从不精确标签信息中推断正确标签的多功能方法相比，我们的ILL框架考虑了不精确标签信息强加的所有可能标签，允许对任何不精确标签的统一解决方案。通过全面的实验结果，我们展示了ILL可以无缝地适应各种情况，包括部分标签学习、半监督学习、噪声标签学习以及这些配置的混合。值得注意的是，我们的简单方法超过了现有的处理不精确标签的技术，标志着第一个统一解决这个问题的方法。

    In this paper, we introduce the imprecise label learning (ILL) framework, a unified approach to handle various imprecise label configurations, which are commonplace challenges in machine learning tasks. ILL leverages an expectation-maximization (EM) algorithm for the maximum likelihood estimation (MLE) of the imprecise label information, treating the precise labels as latent variables. Compared to previous versatile methods attempting to infer correct labels from the imprecise label information, our ILL framework considers all possible labeling imposed by the imprecise label information, allowing a unified solution to deal with any imprecise labels. With comprehensive experimental results, we demonstrate that ILL can seamlessly adapt to various situations, including partial label learning, semi-supervised learning, noisy label learning, and a mixture of these settings. Notably, our simple method surpasses the existing techniques for handling imprecise labels, marking the first unified 
    
[^130]: 多样化深度集成：一种使用显著性图的方法以增强OOD检测、校准和准确性

    Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy. (arXiv:2305.11616v1 [cs.CV])

    [http://arxiv.org/abs/2305.11616](http://arxiv.org/abs/2305.11616)

    这项研究提出了一种使用显著性图来促进深度集成多样性的方法，用于改善OOD检测、校准和准确性，能够优于传统的集成技术，并在OpenOOD基准测试上证明了其有效性。

    

    深度集成在分类和 OOD 检测方面取得了最先进的成果；然而，由于集成中学习的模式的同质性，它们的效果仍然有限。为了克服这一挑战，本研究引入了一种促进集成成员之间多样性的新方法，该方法利用显著性图。通过整合显著性图多样化，我们的方法在多个分类和OOD检测任务中优于传统的集成技术，同时也提高了校准性。在已建立的OpenOOD基准测试上的实验凸显了我们的方法在实际应用中的潜力。

    Deep ensembles achieved state-of-the-art results in classification and out-of-distribution (OOD) detection; however, their effectiveness remains limited due to the homogeneity of learned patterns within the ensemble. To overcome this challenge, our study introduces a novel approach that promotes diversity among ensemble members by leveraging saliency maps. By incorporating saliency map diversification, our method outperforms conventional ensemble techniques in multiple classification and OOD detection tasks, while also improving calibration. Experiments on well-established OpenOOD benchmarks highlight the potential of our method in practical applications.
    
[^131]: PESTS: 波斯语-英语跨语言语料库用于语义文本相似度

    PESTS: Persian_English Cross Lingual Corpus for Semantic Textual Similarity. (arXiv:2305.07893v1 [cs.CL])

    [http://arxiv.org/abs/2305.07893](http://arxiv.org/abs/2305.07893)

    本研究提出了跨语言的语义相似性模型PESTS，并通过波斯语-英语的跨语言语料库来验证模型的准确性。

    

    近来，语义文本相似度成为自然语言处理中备受关注的组件。在计算语言学和自然语言处理中，评估单词、短语、段落和文本之间的语义相似性很重要。同时，语义相似性度量要求在源和目标语言中提供具有一定语义相似性的句子对。许多跨语言的语义相似度模型使用机器翻译来弥补跨语言语料库不可用的不足，但机器翻译的误差会降低模型的准确性。然而，在使用语义相似度特征实现机器翻译时，用相同的机器翻译模型可以提高结果的准确性。

    One of the components of natural language processing that has received a lot of investigation recently is semantic textual similarity. In computational linguistics and natural language processing, assessing the semantic similarity of words, phrases, paragraphs, and texts is crucial. Calculating the degree of semantic resemblance between two textual pieces, paragraphs, or phrases provided in both monolingual and cross-lingual versions is known as semantic similarity. Cross lingual semantic similarity requires corpora in which there are sentence pairs in both the source and target languages with a degree of semantic similarity between them. Many existing cross lingual semantic similarity models use a machine translation due to the unavailability of cross lingual semantic similarity dataset, which the propagation of the machine translation error reduces the accuracy of the model. On the other hand, when we want to use semantic similarity features for machine translation the same machine t
    
[^132]: GPT用于半自动化数据科学：引入CAAFE实现上下文感知自动特征工程

    GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering. (arXiv:2305.03403v1 [cs.AI])

    [http://arxiv.org/abs/2305.03403](http://arxiv.org/abs/2305.03403)

    介绍了一种名为CAAFE的上下文感知自动特征工程方法，它利用大型语言模型根据数据集描述生成更多具有语义意义的特征，能够提高大多数数据集的性能，平均ROC AUC表现提高至0.822。

    

    随着自动化机器学习（AutoML）领域的发展，将领域知识纳入这些系统中变得越来越重要。我们利用大型语言模型（LLMs）的强大功能提出了一种方法来实现这一目标。具体地，我们介绍了一种用于表格数据的特征工程方法，名为上下文感知自动特征工程（CAAFE），它利用LLM根据数据集的描述生成更多具有语义意义的特征。该方法产生用于创建新特征的Python代码，并提供生成特征的效用说明。尽管方法论上很简单，但CAAFE提高了14个数据集中11个数据集的性能，与2个数据集并列，只有1个数据集性能下降，从而使所有数据集的平均ROC AUC表现从0.798提升至0.822。对于所评估的数据集，这一改进与使用随机森林（AUC 0.782）代替逻辑回归（AUC 0.754）所获得的平均改进相似。此外，

    As the field of automated machine learning (AutoML) advances, it becomes increasingly important to include domain knowledge within these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to generate additional semantically meaningful features for tabular datasets based on their descriptions. The method produces both Python code for creating new features and explanations for the utility of the generated features.  Despite being methodologically simple, CAAFE enhances performance on 11 out of 14 datasets, ties on 2 and looses on 1 - boosting mean ROC AUC performance from 0.798 to 0.822 across all datasets. On the evaluated datasets, this improvement is similar to the average improvement achieved by using a random forest (AUC 0.782) instead of logistic regression (AUC 0.754).  Furthermore,
    
[^133]: 主动自监督学习: 只需少量低成本的关系即可。

    Active Self-Supervised Learning: A Few Low-Cost Relationships Are All You Need. (arXiv:2303.15256v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.15256](http://arxiv.org/abs/2303.15256)

    本研究提出了积极自监督学习（PAL）框架，通过查询样本之间的语义关系来解决自监督学习中需要知道正视图的限制。PAL不仅提供了理论上有基础的学习框架，还能够嵌入先验知识并支持监督和半监督学习。

    

    自监督学习（SSL）已成为学习无标记数据的可转移表示的首选解决方案。然而，SSL需要构建已知语义相似的样本，即正视图。这种需求是SSL的主要限制，通常通过临时策略来处理，例如对相同输入应用已知的数据增强。在这项工作中，我们通过积极主动学习（PAL）将这个原则形式化和推广，其中一个oracle查询样本之间的语义关系。PAL实现了三个主要目标。首先，它揭示了一个基于相似性图的理论上有基础的学习框架，超越了SSL，可根据所使用的oracle扩展到处理监督和半监督学习。其次，它为嵌入先验知识（如观测的标签）提供了一致的算法，而无需更改训练流程中的任何内容。第三，它提供了一个适当的主动学习框架。

    Self-Supervised Learning (SSL) has emerged as the solution of choice to learn transferable representations from unlabeled data. However, SSL requires to build samples that are known to be semantically akin, i.e. positive views. Requiring such knowledge is the main limitation of SSL and is often tackled by ad-hoc strategies e.g. applying known data-augmentations to the same input. In this work, we formalize and generalize this principle through Positive Active Learning (PAL) where an oracle queries semantic relationships between samples. PAL achieves three main objectives. First, it unveils a theoretically grounded learning framework beyond SSL, based on similarity graphs, that can be extended to tackle supervised and semi-supervised learning depending on the employed oracle. Second, it provides a consistent algorithm to embed a priori knowledge, e.g. some observed labels, into any SSL losses without any change in the training pipeline. Third, it provides a proper active learning framew
    
[^134]: 高效解释 CSPs 的不可满足子集优化（扩展算法和示例）

    Efficiently Explaining CSPs with Unsatisfiable Subset Optimization (extended algorithms and examples). (arXiv:2303.11712v1 [cs.AI])

    [http://arxiv.org/abs/2303.11712](http://arxiv.org/abs/2303.11712)

    本文提出对于约束满足问题，采用一种高效的算法以易于理解的方式解释解决方案，这一方法可以让人类更好地理解机器所做的决策。

    

    我们在最近提出的方法基础上，为逐步以易于理解方式解释约束满足问题（CSP）的解决方案。这里的解释是一系列简单的推断步骤，其中简单性使用成本函数量化。解释生成算法依赖于从导出的不可满足公式中提取最小不满子集（MUS），利用所谓的非冗余解释和 MUS 之间的一一对应关系。然而，MUS 提取算法不提供任何针对给定成本函数的子集最小性或最优性保证。因此，我们在这些形式基础上建立，并着手改进的主要要点，即如何高效地生成可证明是最优的解释（与给定成本度量相关）。为此，我们开发了（1）基于命中集的算法，用于查找最佳受限不可满足子集；（2）一种重用相关信息的方法，可在不同的解释生成阶段提高效率。

    We build on a recently proposed method for stepwise explaining solutions of Constraint Satisfaction Problems (CSP) in a human-understandable way. An explanation here is a sequence of simple inference steps where simplicity is quantified using a cost function. The algorithms for explanation generation rely on extracting Minimal Unsatisfiable Subsets (MUS) of a derived unsatisfiable formula, exploiting a one-to-one correspondence between so-called non-redundant explanations and MUSs. However, MUS extraction algorithms do not provide any guarantee of subset minimality or optimality with respect to a given cost function. Therefore, we build on these formal foundations and tackle the main points of improvement, namely how to generate explanations efficiently that are provably optimal (with respect to the given cost metric). For that, we developed (1) a hitting set-based algorithm for finding the optimal constrained unsatisfiable subsets; (2) a method for re-using relevant information over m
    
[^135]: 基于分层生成对抗模拟学习的自动驾驶在城市环境中的应用

    Hierarchical Generative Adversarial Imitation Learning with Mid-level Input Generation for Autonomous Driving on Urban Environments. (arXiv:2302.04823v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04823](http://arxiv.org/abs/2302.04823)

    本研究提出了一种名为hGAIL的架构，用于解决车辆的自主导航问题，通过将感知信息直接映射到低级动作的同时，学习车辆环境的中级输入表示。

    

    对于现实中的城市导航场景，设计健壮的控制策略并不是一项简单的任务。在端到端的方法中，这些策略必须将车辆摄像头获得的高维图像映射到低级动作，如转向和油门。本研究提出了一种名为hGAIL的架构，用于解决车辆的自主导航问题，通过将感知信息直接映射到低级动作的同时，学习车辆环境的中级输入表示。

    Deriving robust control policies for realistic urban navigation scenarios is not a trivial task. In an end-to-end approach, these policies must map high-dimensional images from the vehicle's cameras to low-level actions such as steering and throttle. While pure Reinforcement Learning (RL) approaches are based exclusively on rewards,Generative Adversarial Imitation Learning (GAIL) agents learn from expert demonstrations while interacting with the environment, which favors GAIL on tasks for which a reward signal is difficult to derive. In this work, the hGAIL architecture was proposed to solve the autonomous navigation of a vehicle in an end-to-end approach, mapping sensory perceptions directly to low-level actions, while simultaneously learning mid-level input representations of the agent's environment. The proposed hGAIL consists of an hierarchical Adversarial Imitation Learning architecture composed of two main modules: the GAN (Generative Adversarial Nets) which generates the Bird's-
    
[^136]: 使用图神经网络和强化学习生成能中断交换的阻塞工件车间问题中的调度规则

    Generating Dispatching Rules for the Interrupting Swap-Allowed Blocking Job Shop Problem Using Graph Neural Network and Reinforcement Learning. (arXiv:2302.02506v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02506](http://arxiv.org/abs/2302.02506)

    本研究提出了一种基于图神经网络和强化学习的方法，用于生成适应性调度规则来解决中断交换允许的阻塞工件车间问题，并开发了一个模拟器来模拟实际工业生产中的中断和交换情况。

    

    中断交换允许的阻塞工件车间问题（ISBJSSP）是一个复杂的调度问题，能够通过解决存储能力不足和意外生产中断的问题，实际模拟许多制造规划和物流应用。由于机器故障或维护的随机干扰，工业生产环境通常选择采用调度规则来实现自适应的实时重调度，而不是需要在问题条件动态变化时进行昂贵的重新计算的传统方法。为了生成ISBJSSP问题的调度规则，我们引入了一种动态的分离图公式，其中的节点和边可持续进行删除和添加。这种公式使得能够利用图神经网络和强化学习来训练自适应调度器。此外，还开发了一个模拟器来模拟中断、交换等情况。

    The interrupting swap-allowed blocking job shop problem (ISBJSSP) is a complex scheduling problem that is able to model many manufacturing planning and logistics applications realistically by addressing both the lack of storage capacity and unforeseen production interruptions. Subjected to random disruptions due to machine malfunction or maintenance, industry production settings often choose to adopt dispatching rules to enable adaptive, real-time re-scheduling, rather than traditional methods that require costly re-computation on the new configuration every time the problem condition changes dynamically. To generate dispatching rules for the ISBJSSP problem, we introduce a dynamic disjunctive graph formulation characterized by nodes and edges subjected to continuous deletions and additions. This formulation enables the training of an adaptive scheduler utilizing graph neural networks and reinforcement learning. Furthermore, a simulator is developed to simulate interruption, swapping, 
    
[^137]: 多视角视觉融合的集成学习在遮挡和缺失信息情况下的应用：框架和基于真实世界数据的司机手势识别评估

    Ensemble Learning for Fusion of Multiview Vision with Occlusion and Missing Information: Framework and Evaluations with Real-World Data and Applications in Driver Hand Activity Recognition. (arXiv:2301.12592v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12592](http://arxiv.org/abs/2301.12592)

    本研究提出了一种集成学习的多视角视觉融合方法，用于处理在真实世界应用中可能存在的间歇性信息缺失问题。通过设计学习框架和提出的插补方案，实现了对驾驶员手势活动的准确分类和位置估计，提高了自动驾驶的安全性能。

    

    多传感器框架为集成学习和传感器融合提供了机会，以利用冗余和补充信息，有助于在真实世界安全应用中进行连续司机状态监测。我们定义了由遮挡、噪声或传感器故障引起的间歇性信息缺失问题，并设计了一个围绕这些数据缺口的学习框架，提出并分析了一种填补信息缺失的插补方案。我们将这些思想应用于基于摄像头的手势活动分类任务，以实现自动驾驶的安全性。我们表明，基于并行卷积神经网络的后期融合方法，在同组受试者验证时，甚至可以超过最佳单一摄像头模型在估计手中物体的位置时，我们的多摄像头框架表现最好，而且在跨组验证时也表现出色。

    Multi-sensor frameworks provide opportunities for ensemble learning and sensor fusion to make use of redundancy and supplemental information, helpful in real-world safety applications such as continuous driver state monitoring which necessitate predictions even in cases where information may be intermittently missing. We define this problem of intermittent instances of missing information (by occlusion, noise, or sensor failure) and design a learning framework around these data gaps, proposing and analyzing an imputation scheme to handle missing information. We apply these ideas to tasks in camera-based hand activity classification for robust safety during autonomous driving. We show that a late-fusion approach between parallel convolutional neural networks can outperform even the best-placed single camera model in estimating the hands' held objects and positions when validated on within-group subjects, and that our multi-camera framework performs best on average in cross-group validat
    
[^138]: MixUp-MIL：一种新的数据增强方法用于多示例学习并在甲状腺癌诊断中的研究

    MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis. (arXiv:2211.05862v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.05862](http://arxiv.org/abs/2211.05862)

    提出了一种新的数据增强方法MixUp-MIL用于多示例学习，通过应用切片内插值方法可以改善甲状腺癌诊断的准确性。

    

    在像素或补丁级别注释缺失的情况下，多示例学习展示了一种基于整张切片图像的诊断的强大方法。尽管整张切片图像的大小很大，但个别切片的数量往往相对较小，导致标记样本数量少。为了提高训练效果，我们提出并研究了基于特征向量的线性插值（即MixUp）思想的不同数据增强策略。基于最先进的多示例学习架构和两个甲状腺癌数据集，进行了一项详尽的研究，考虑了多种常见的数据增强策略。尽管基于原始MixUp方法的策略表现出准确度降低，但使用一种新颖的切片内插值方法却实现了一致的准确度提高。

    Multiple instance learning exhibits a powerful approach for whole slide image-based diagnosis in the absence of pixel- or patch-level annotations. In spite of the huge size of hole slide images, the number of individual slides is often rather small, leading to a small number of labeled samples. To improve training, we propose and investigate different data augmentation strategies for multiple instance learning based on the idea of linear interpolations of feature vectors (known as MixUp). Based on state-of-the-art multiple instance learning architectures and two thyroid cancer data sets, an exhaustive study is conducted considering a range of common data augmentation strategies. Whereas a strategy based on to the original MixUp approach showed decreases in accuracy, the use of a novel intra-slide interpolation method led to consistent increases in accuracy.
    

