<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26426;&#22120;&#20154;&#33258;&#20027;&#28165;&#27905;&#31995;&#32479;&#20013;&#30340;&#28151;&#21512;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;100&#20010;&#23454;&#20363;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.06531</link><description>&lt;p&gt;
&#38754;&#21521;&#33258;&#20027;&#28165;&#27905;&#30340;&#22810;&#26426;&#22120;&#20154;&#28151;&#21512;&#20219;&#21153;&#20998;&#37197;&#30340;&#23454;&#36341;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Practical Multi-Robot Hybrid Tasks Allocation for Autonomous Cleaning. (arXiv:2303.06531v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26426;&#22120;&#20154;&#33258;&#20027;&#28165;&#27905;&#31995;&#32479;&#20013;&#30340;&#28151;&#21512;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;100&#20010;&#23454;&#20363;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel robust mixed-integer linear programming model for multi-robot hybrid-task allocation in uncertain autonomous cleaning systems, and establishes a dataset of 100 instances.
&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#20998;&#37197;&#22312;&#22810;&#26426;&#22120;&#20154;&#33258;&#20027;&#28165;&#27905;&#31995;&#32479;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22810;&#20010;&#26426;&#22120;&#20154;&#19968;&#36215;&#24037;&#20316;&#20197;&#28165;&#27905;&#22823;&#38754;&#31215;&#21306;&#22495;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#30456;&#20851;&#30740;&#31350;&#23384;&#22312;&#20960;&#20010;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#30830;&#23450;&#24615;&#30340;&#21333;&#20219;&#21153;&#20998;&#37197;&#65292;&#32780;&#19981;&#32771;&#34385;&#19981;&#30830;&#23450;&#24037;&#20316;&#29615;&#22659;&#20013;&#30340;&#28151;&#21512;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20026;&#19981;&#30830;&#23450;&#30340;&#33258;&#20027;&#28165;&#27905;&#31995;&#32479;&#30340;&#22810;&#26426;&#22120;&#20154;&#28151;&#21512;&#20219;&#21153;&#20998;&#37197;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#27169;&#22411;&#26469;&#24314;&#27169;&#28165;&#27905;&#29615;&#22659;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#28151;&#21512;&#28165;&#27905;&#20219;&#21153;&#39034;&#24207;&#21644;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#31561;&#23454;&#38469;&#32422;&#26463;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;100&#20010;&#23454;&#20363;&#65292;&#27599;&#20010;&#23454;&#20363;&#37117;&#26377;2D&#25163;&#21160;&#26631;&#35760;&#30340;&#22270;&#20687;&#21644;3D&#27169;&#22411;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#20840;&#38754;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task allocation plays a vital role in multi-robot autonomous cleaning systems, where multiple robots work together to clean a large area. However, there are several problems in relevant research to date. Most current studies mainly focus on deterministic, single-task allocation for cleaning robots, without considering hybrid tasks in uncertain working environments. Moreover, there is a lack of datasets and benchmarks for relevant research. In this paper, we contribute to multi-robot hybrid-task allocation for uncertain autonomous cleaning systems by addressing these problems. First, we model the uncertainties in the cleaning environment via robust optimization and propose a novel robust mixed-integer linear programming model with practical constraints including hybrid cleaning task order and robot's ability. Second, we establish a dataset of 100 instances made from floor plans, each of which has 2D manually-labeled images and a 3D model. Third, we provide comprehensive results on the c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#28436;&#21592;-&#35780;&#35770;&#23478;QD-RL&#26694;&#26550;&#65292;&#29992;&#20110;QD-RL&#35774;&#32622;&#20013;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;RL&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#65292;PGA-ME&#65288;SAC&#65289;&#21644;PGA-ME&#65288;DroQ&#65289;&#65292;&#23558;&#28145;&#24230;RL&#30340;&#26368;&#26032;&#36827;&#23637;&#24212;&#29992;&#20110;QD-RL&#35774;&#32622;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;QD-RL&#31639;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#20154;&#24418;&#29615;&#22659;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.06164</link><description>&lt;p&gt;
&#29702;&#35299;&#36136;&#37327;&#22810;&#26679;&#24615;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Understanding the Synergies between Quality-Diversity and Deep Reinforcement Learning. (arXiv:2303.06164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#28436;&#21592;-&#35780;&#35770;&#23478;QD-RL&#26694;&#26550;&#65292;&#29992;&#20110;QD-RL&#35774;&#32622;&#20013;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;RL&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#65292;PGA-ME&#65288;SAC&#65289;&#21644;PGA-ME&#65288;DroQ&#65289;&#65292;&#23558;&#28145;&#24230;RL&#30340;&#26368;&#26032;&#36827;&#23637;&#24212;&#29992;&#20110;QD-RL&#35774;&#32622;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;QD-RL&#31639;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#20154;&#24418;&#29615;&#22659;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Generalized Actor-Critic QD-RL framework for actor-critic deep RL methods in the QD-RL setting. The framework introduces two new algorithms, PGA-ME (SAC) and PGA-ME (DroQ), which apply recent advancements in Deep RL to the QD-RL setting and solve the humanoid environment problem that existing QD-RL algorithms cannot solve.
&lt;/p&gt;
&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;QD&#65289;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#24050;&#32463;&#23548;&#33268;&#20102;&#24378;&#22823;&#30340;&#28151;&#21512;QD-RL&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#24182;&#24102;&#26469;&#20102;&#20004;&#20010;&#39046;&#22495;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#20182;RL&#31639;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#20808;&#21069;&#30340;&#28151;&#21512;&#26041;&#27861;&#20013;&#20165;&#20351;&#29992;&#20102;&#21333;&#20010;&#28145;&#24230;RL&#31639;&#27861;&#65288;TD3&#65289;&#12290;&#27492;&#22806;&#65292;QD&#21644;RL&#20043;&#38388;&#30340;&#20248;&#21270;&#36807;&#31243;&#23384;&#22312;&#26681;&#26412;&#24046;&#24322;&#65292;&#38656;&#35201;&#26356;&#21152;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#28436;&#21592;-&#35780;&#35770;&#23478;QD-RL&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;QD-RL&#35774;&#32622;&#20013;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;RL&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#26465;&#30740;&#31350;&#28145;&#24230;RL&#22312;QD-RL&#35774;&#32622;&#20013;&#30340;&#35265;&#35299;&#30340;&#36335;&#24452;&#65292;&#36825;&#26159;&#22312;QD-RL&#20013;&#21462;&#24471;&#36827;&#23637;&#30340;&#37325;&#35201;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#65292;PGA-ME&#65288;SAC&#65289;&#21644;PGA-ME&#65288;DroQ&#65289;&#65292;&#23558;&#28145;&#24230;RL&#30340;&#26368;&#26032;&#36827;&#23637;&#24212;&#29992;&#20110;QD-RL&#35774;&#32622;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;QD-RL&#31639;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#20154;&#24418;&#29615;&#22659;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The synergies between Quality-Diversity (QD) and Deep Reinforcement Learning (RL) have led to powerful hybrid QD-RL algorithms that have shown tremendous potential, and brings the best of both fields. However, only a single deep RL algorithm (TD3) has been used in prior hybrid methods despite notable progress made by other RL algorithms. Additionally, there are fundamental differences in the optimization procedures between QD and RL which would benefit from a more principled approach. We propose Generalized Actor-Critic QD-RL, a unified modular framework for actor-critic deep RL methods in the QD-RL setting. This framework provides a path to study insights from Deep RL in the QD-RL setting, which is an important and efficient way to make progress in QD-RL. We introduce two new algorithms, PGA-ME (SAC) and PGA-ME (DroQ) which apply recent advancements in Deep RL to the QD-RL setting, and solves the humanoid environment which was not possible using existing QD-RL algorithms. However, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32452;&#32455;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#27010;&#29575;&#28857;&#20113;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#22330;&#26223;&#22797;&#26434;&#24230;&#33258;&#21160;&#35843;&#25972;&#27169;&#22411;&#22797;&#26434;&#24230;&#65292;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.00047</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#32452;&#32455;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#27010;&#29575;&#28857;&#20113;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Point Cloud Modeling via Self-Organizing Gaussian Mixture Models. (arXiv:2302.00047v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32452;&#32455;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#27010;&#29575;&#28857;&#20113;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#22330;&#26223;&#22797;&#26434;&#24230;&#33258;&#21160;&#35843;&#25972;&#27169;&#22411;&#22797;&#26434;&#24230;&#65292;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a probabilistic point cloud modeling method based on self-organizing Gaussian mixture models, which can automatically adjust the model complexity according to the scene complexity, and has better generalization performance compared to existing techniques.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#30340;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#26377;&#38480;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#23545;&#31354;&#38388;&#28857;&#20113;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#20854;&#20013;&#32452;&#20214;&#30340;&#25968;&#37327;&#22522;&#20110;&#22330;&#26223;&#22797;&#26434;&#24615;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#21033;&#29992;&#20449;&#24687;&#35770;&#23398;&#20064;&#20013;&#30340;&#33258;&#32452;&#32455;&#21407;&#29702;&#65292;&#26681;&#25454;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#33258;&#21160;&#35843;&#25972;GMM&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#12290;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#19981;&#21516;&#22330;&#26223;&#22797;&#26434;&#24230;&#30340;&#23454;&#38469;&#25968;&#25454;&#19978;&#19982;&#29616;&#26377;&#30340;&#28857;&#20113;&#24314;&#27169;&#25216;&#26415;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This letter presents a continuous probabilistic modeling methodology for spatial point cloud data using finite Gaussian Mixture Models (GMMs) where the number of components are adapted based on the scene complexity. Few hierarchical and adaptive methods have been proposed to address the challenge of balancing model fidelity with size. Instead, state-of-the-art mapping approaches require tuning parameters for specific use cases, but do not generalize across diverse environments. To address this gap, we utilize a self-organizing principle from information-theoretic learning to automatically adapt the complexity of the GMM model based on the relevant information in the sensor data. The approach is evaluated against existing point cloud modeling techniques on real-world data with varying degrees of scene complexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22478;&#24066;&#31354;&#20013;&#20986;&#34892;&#65288;UAM&#65289;&#22312;&#22823;&#37117;&#24066;&#20132;&#36890;&#20013;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#30830;&#23450;&#20102;&#23558;UAM&#34701;&#20837;&#22478;&#24066;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#21253;&#25324;&#23545;&#29616;&#26377;&#20132;&#36890;&#27169;&#24335;&#21644;&#25317;&#22581;&#30340;&#24433;&#21709;&#65307;&#23433;&#20840;&#20998;&#26512;&#21644;&#39118;&#38505;&#35780;&#20272;&#65307;&#28508;&#22312;&#30340;&#32463;&#27982;&#21644;&#29615;&#22659;&#25928;&#30410;&#65307;&#20197;&#21450;&#20026;UAM&#21644;&#22320;&#38754;&#20132;&#36890;&#24320;&#21457;&#20849;&#20139;&#22522;&#30784;&#35774;&#26045;&#21644;&#36335;&#32447;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;UAM&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#22914;&#32553;&#30701;&#26053;&#34892;&#26102;&#38388;&#21644;&#25913;&#21892;&#26381;&#21153;&#19981;&#36275;&#22320;&#21306;&#30340;&#21487;&#36798;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.12901</link><description>&lt;p&gt;
&#27169;&#25311;&#22478;&#24066;&#31354;&#20013;&#20986;&#34892;&#34701;&#20837;&#29616;&#26377;&#20132;&#36890;&#31995;&#32479;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Simulating the Integration of Urban Air Mobility into Existing Transportation Systems: A Survey. (arXiv:2301.12901v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22478;&#24066;&#31354;&#20013;&#20986;&#34892;&#65288;UAM&#65289;&#22312;&#22823;&#37117;&#24066;&#20132;&#36890;&#20013;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#30830;&#23450;&#20102;&#23558;UAM&#34701;&#20837;&#22478;&#24066;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#21253;&#25324;&#23545;&#29616;&#26377;&#20132;&#36890;&#27169;&#24335;&#21644;&#25317;&#22581;&#30340;&#24433;&#21709;&#65307;&#23433;&#20840;&#20998;&#26512;&#21644;&#39118;&#38505;&#35780;&#20272;&#65307;&#28508;&#22312;&#30340;&#32463;&#27982;&#21644;&#29615;&#22659;&#25928;&#30410;&#65307;&#20197;&#21450;&#20026;UAM&#21644;&#22320;&#38754;&#20132;&#36890;&#24320;&#21457;&#20849;&#20139;&#22522;&#30784;&#35774;&#26045;&#21644;&#36335;&#32447;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;UAM&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#22914;&#32553;&#30701;&#26053;&#34892;&#26102;&#38388;&#21644;&#25913;&#21892;&#26381;&#21153;&#19981;&#36275;&#22320;&#21306;&#30340;&#21487;&#36798;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper surveys the current state of research on urban air mobility (UAM) in metropolitan-scale traffic using simulation techniques, identifying key challenges and opportunities for integrating UAM into urban transportation systems, including impacts on existing traffic patterns and congestion, safety analysis and risk assessment, potential economic and environmental benefits, and the development of shared infrastructure and routes for UAM and ground-based transportation. The potential benefits of UAM, such as reduced travel times and improved accessibility for underserved areas, are also discussed.
&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#31354;&#20013;&#20986;&#34892;&#65288;UAM&#65289;&#26377;&#21487;&#33021;&#24443;&#24213;&#25913;&#21464;&#22823;&#37117;&#24066;&#22320;&#21306;&#30340;&#20132;&#36890;&#26041;&#24335;&#65292;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#20132;&#36890;&#26041;&#24335;&#65292;&#32531;&#35299;&#25317;&#22581;&#65292;&#25552;&#39640;&#21487;&#36798;&#24615;&#12290;&#28982;&#32780;&#65292;&#23558;UAM&#34701;&#20837;&#29616;&#26377;&#20132;&#36890;&#31995;&#32479;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#20854;&#23545;&#20132;&#36890;&#27969;&#37327;&#21644;&#23481;&#37327;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#35843;&#26597;&#65292;&#20351;&#29992;&#27169;&#25311;&#25216;&#26415;&#35843;&#26597;&#20102;UAM&#22312;&#22823;&#37117;&#24066;&#20132;&#36890;&#20013;&#30340;&#30740;&#31350;&#29616;&#29366;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#23558;UAM&#34701;&#20837;&#22478;&#24066;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#21253;&#25324;&#23545;&#29616;&#26377;&#20132;&#36890;&#27169;&#24335;&#21644;&#25317;&#22581;&#30340;&#24433;&#21709;&#65307;&#23433;&#20840;&#20998;&#26512;&#21644;&#39118;&#38505;&#35780;&#20272;&#65307;&#28508;&#22312;&#30340;&#32463;&#27982;&#21644;&#29615;&#22659;&#25928;&#30410;&#65307;&#20197;&#21450;&#20026;UAM&#21644;&#22320;&#38754;&#20132;&#36890;&#24320;&#21457;&#20849;&#20139;&#22522;&#30784;&#35774;&#26045;&#21644;&#36335;&#32447;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;UAM&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#22914;&#32553;&#30701;&#26053;&#34892;&#26102;&#38388;&#21644;&#25913;&#21892;&#26381;&#21153;&#19981;&#36275;&#22320;&#21306;&#30340;&#21487;&#36798;&#24615;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Urban air mobility (UAM) has the potential to revolutionize transportation in metropolitan areas, providing a new mode of transportation that could alleviate congestion and improve accessibility. However, the integration of UAM into existing transportation systems is a complex task that requires a thorough understanding of its impact on traffic flow and capacity. In this paper, we conduct a survey to investigate the current state of research on UAM in metropolitan-scale traffic using simulation techniques. We identify key challenges and opportunities for the integration of UAM into urban transportation systems, including impacts on existing traffic patterns and congestion; safety analysis and risk assessment; potential economic and environmental benefits; and the development of shared infrastructure and routes for UAM and ground-based transportation. We also discuss the potential benefits of UAM, such as reduced travel times and improved accessibility for underserved areas. Our survey 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#20197;&#36827;&#34892;&#35821;&#35328;&#24341;&#23548;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#35821;&#35328;&#21644;&#24863;&#30693;&#21464;&#21270;&#65292;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#65292;&#19981;&#38656;&#35201;&#20013;&#38388;&#30417;&#30563;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#31526;&#21495;&#25512;&#29702;&#26500;&#36896;&#65292;&#22312;&#28508;&#22312;&#30340;&#31070;&#32463;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#19978;&#25805;&#20316;&#65292;&#20801;&#35768;&#23545;&#36755;&#20837;&#22330;&#26223;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2211.06652</link><description>&lt;p&gt;
&#23398;&#20064;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#20197;&#36827;&#34892;&#35821;&#35328;&#24341;&#23548;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning Neuro-symbolic Programs for Language Guided Robot Manipulation. (arXiv:2211.06652v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06652
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#20197;&#36827;&#34892;&#35821;&#35328;&#24341;&#23548;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#35821;&#35328;&#21644;&#24863;&#30693;&#21464;&#21270;&#65292;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#65292;&#19981;&#38656;&#35201;&#20013;&#38388;&#30417;&#30563;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#31526;&#21495;&#25512;&#29702;&#26500;&#36896;&#65292;&#22312;&#28508;&#22312;&#30340;&#31070;&#32463;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#19978;&#25805;&#20316;&#65292;&#20801;&#35768;&#23545;&#36755;&#20837;&#22330;&#26223;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method for learning neuro-symbolic programs for language guided robot manipulation, which can handle linguistic and perceptual variations, is end-to-end trainable, and requires no intermediate supervision. The method uses symbolic reasoning constructs that operate on a latent neural object-centric representation, allowing for deeper reasoning over the input scene.
&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#36755;&#20837;&#22330;&#26223;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#65292;&#36755;&#20986;&#19968;&#20010;&#21487;&#20197;&#30001;&#26426;&#22120;&#20154;&#25191;&#34892;&#30340;&#25805;&#20316;&#31243;&#24207;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#20197;&#19979;&#38480;&#21046;&#20043;&#19968;&#65306;&#65288;i&#65289;&#20381;&#36182;&#25163;&#24037;&#32534;&#30721;&#30340;&#27010;&#24565;&#31526;&#21495;&#65292;&#38480;&#21046;&#20102;&#36229;&#20986;&#35757;&#32451;&#26399;&#38388;&#25152;&#35265;&#30340;&#19968;&#33324;&#21270;&#33021;&#21147;[1]&#65288;ii&#65289;&#20174;&#25351;&#20196;&#20013;&#25512;&#26029;&#20986;&#21160;&#20316;&#24207;&#21015;&#65292;&#20294;&#38656;&#35201;&#23494;&#38598;&#30340;&#23376;&#30446;&#26631;&#30417;&#30563;[2]&#25110;&#65288;iii&#65289;&#32570;&#20047;&#35299;&#37322;&#22797;&#26434;&#25351;&#20196;&#25152;&#38656;&#30340;&#35821;&#20041;&#65292;&#36825;&#31181;&#35821;&#20041;&#38656;&#35201;&#26356;&#28145;&#20837;&#30340;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#25512;&#29702;[3]&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#35821;&#35328;&#21644;&#24863;&#30693;&#21464;&#21270;&#65292;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#65292;&#19981;&#38656;&#35201;&#20013;&#38388;&#30417;&#30563;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20351;&#29992;&#31526;&#21495;&#25512;&#29702;&#26500;&#36896;&#65292;&#36825;&#20123;&#26500;&#36896;&#22312;&#28508;&#22312;&#30340;&#31070;&#32463;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#19978;&#25805;&#20316;&#65292;&#20801;&#35768;&#23545;&#36755;&#20837;&#22330;&#26223;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#21253;&#25324;&#20998;&#23618;&#25351;&#20196;&#35299;&#26512;&#22120;&#21644;&#21160;&#20316;&#27169;&#25311;&#22120;&#65292;&#20197;&#23398;&#20064;&#35299;&#32806;&#30340;&#34892;&#21160;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a natural language instruction and an input scene, our goal is to train a model to output a manipulation program that can be executed by the robot. Prior approaches for this task possess one of the following limitations: (i) rely on hand-coded symbols for concepts limiting generalization beyond those seen during training [1] (ii) infer action sequences from instructions but require dense sub-goal supervision [2] or (iii) lack semantics required for deeper object-centric reasoning inherent in interpreting complex instructions [3]. In contrast, our approach can handle linguistic as well as perceptual variations, end-to-end trainable and requires no intermediate supervision. The proposed model uses symbolic reasoning constructs that operate on a latent neural object-centric representation, allowing for deeper reasoning over the input scene. Central to our approach is a modular structure consisting of a hierarchical instruction parser and an action simulator to learn disentangled act
&lt;/p&gt;</description></item><item><title>D-Shape&#26159;&#19968;&#31181;&#26032;&#30340;&#32467;&#21512;IL&#21644;RL&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22870;&#21169;&#22609;&#24418;&#21644;&#30446;&#26631;&#26465;&#20214;&#21270;RL&#30340;&#24605;&#24819;&#26469;&#35299;&#20915;&#27425;&#20248;&#28436;&#31034;&#19982;&#22238;&#25253;&#26368;&#22823;&#21270;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#33021;&#22815;&#22312;&#31232;&#30095;&#22870;&#21169;&#32593;&#26684;&#19990;&#30028;&#39046;&#22495;&#20013;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#19968;&#33268;&#22320;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2210.14428</link><description>&lt;p&gt;
D-Shape: &#36890;&#36807;&#30446;&#26631;&#26465;&#20214;&#21270;&#23454;&#29616;&#28436;&#31034;&#24418;&#29366;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
D-Shape: Demonstration-Shaped Reinforcement Learning via Goal Conditioning. (arXiv:2210.14428v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14428
&lt;/p&gt;
&lt;p&gt;
D-Shape&#26159;&#19968;&#31181;&#26032;&#30340;&#32467;&#21512;IL&#21644;RL&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22870;&#21169;&#22609;&#24418;&#21644;&#30446;&#26631;&#26465;&#20214;&#21270;RL&#30340;&#24605;&#24819;&#26469;&#35299;&#20915;&#27425;&#20248;&#28436;&#31034;&#19982;&#22238;&#25253;&#26368;&#22823;&#21270;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#33021;&#22815;&#22312;&#31232;&#30095;&#22870;&#21169;&#32593;&#26684;&#19990;&#30028;&#39046;&#22495;&#20013;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#19968;&#33268;&#22320;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
D-Shape is a new method that combines imitation learning (IL) and reinforcement learning (RL) using reward shaping and goal-conditioned RL to resolve the conflict between suboptimal demonstrations and return-maximization objective of RL. It improves sample efficiency and consistently converges to the optimal policy in sparse-reward gridworld domains.
&lt;/p&gt;
&lt;p&gt;
&#23558;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30456;&#32467;&#21512;&#26159;&#35299;&#20915;&#33258;&#20027;&#34892;&#20026;&#33719;&#21462;&#20013;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#26679;&#20570;&#30340;&#26041;&#27861;&#36890;&#24120;&#20551;&#23450;&#25152;&#38656;&#30340;&#34892;&#20026;&#28436;&#31034;&#30001;&#19987;&#23478;&#25552;&#20379;&#65292;&#35813;&#19987;&#23478;&#30456;&#23545;&#20110;&#20219;&#21153;&#22870;&#21169;&#34920;&#29616;&#26368;&#20339;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25552;&#20379;&#30340;&#28436;&#31034;&#26159;&#27425;&#20248;&#30340;&#65292;&#21017;&#38754;&#20020;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#65292;&#21363;IL&#30340;&#28436;&#31034;&#21305;&#37197;&#30446;&#26631;&#19982;RL&#30340;&#22238;&#25253;&#26368;&#22823;&#21270;&#30446;&#26631;&#20914;&#31361;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;D-Shape&#65292;&#19968;&#31181;&#26032;&#30340;&#32467;&#21512;IL&#21644;RL&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22870;&#21169;&#22609;&#24418;&#21644;&#30446;&#26631;&#26465;&#20214;&#21270;RL&#30340;&#24605;&#24819;&#26469;&#35299;&#20915;&#19978;&#36848;&#20914;&#31361;&#12290;D-Shape&#20801;&#35768;&#20174;&#27425;&#20248;&#28436;&#31034;&#20013;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#25214;&#21040;&#30456;&#23545;&#20110;&#20219;&#21153;&#22870;&#21169;&#30340;&#26368;&#20248;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#31232;&#30095;&#22870;&#21169;&#32593;&#26684;&#19990;&#30028;&#39046;&#22495;&#23454;&#39564;&#39564;&#35777;&#20102;D-Shape&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;RL&#65292;&#24182;&#19988;&#33021;&#22815;&#19968;&#33268;&#22320;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
While combining imitation learning (IL) and reinforcement learning (RL) is a promising way to address poor sample efficiency in autonomous behavior acquisition, methods that do so typically assume that the requisite behavior demonstrations are provided by an expert that behaves optimally with respect to a task reward. If, however, suboptimal demonstrations are provided, a fundamental challenge appears in that the demonstration-matching objective of IL conflicts with the return-maximization objective of RL. This paper introduces D-Shape, a new method for combining IL and RL that uses ideas from reward shaping and goal-conditioned RL to resolve the above conflict. D-Shape allows learning from suboptimal demonstrations while retaining the ability to find the optimal policy with respect to the task reward. We experimentally validate D-Shape in sparse-reward gridworld domains, showing that it both improves over RL in terms of sample efficiency and converges consistently to the optimal polic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;PDDLStream&#38382;&#39064;&#30340;&#27714;&#35299;&#22120;LAZY&#65292;&#23427;&#22312;&#21160;&#20316;&#39592;&#26550;&#19978;&#32500;&#25252;&#21333;&#20010;&#38598;&#25104;&#25628;&#32034;&#65292;&#38543;&#30528;&#22312;&#36816;&#21160;&#35268;&#21010;&#26399;&#38388;&#25042;&#24816;&#22320;&#32472;&#21046;&#21487;&#33021;&#36816;&#21160;&#30340;&#26679;&#26412;&#65292;&#36880;&#28176;&#21464;&#24471;&#26356;&#20855;&#20960;&#20309;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#23398;&#20064;&#27169;&#22411;&#30340;&#30446;&#26631;&#23548;&#21521;&#31574;&#30053;&#21644;&#24403;&#21069;&#36816;&#21160;&#37319;&#26679;&#25968;&#25454;&#21512;&#24182;&#21040;LAZY&#20013;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#24341;&#23548;&#20219;&#21153;&#35268;&#21010;&#22120;&#65292;&#36825;&#23548;&#33268;&#20102;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#23545;&#35937;&#12289;&#30446;&#26631;&#21644;&#21021;&#22987;&#26465;&#20214;&#30340;&#26410;&#35265;&#27979;&#35797;&#29615;&#22659;&#20013;&#35780;&#20272;&#21487;&#34892;&#35299;&#25628;&#32034;&#30340;&#26174;&#30528;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2210.14055</link><description>&lt;p&gt;
&#24102;&#21453;&#39304;&#30340;&#31574;&#30053;&#24341;&#23548;&#25042;&#24816;&#25628;&#32034;&#29992;&#20110;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Policy-Guided Lazy Search with Feedback for Task and Motion Planning. (arXiv:2210.14055v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;PDDLStream&#38382;&#39064;&#30340;&#27714;&#35299;&#22120;LAZY&#65292;&#23427;&#22312;&#21160;&#20316;&#39592;&#26550;&#19978;&#32500;&#25252;&#21333;&#20010;&#38598;&#25104;&#25628;&#32034;&#65292;&#38543;&#30528;&#22312;&#36816;&#21160;&#35268;&#21010;&#26399;&#38388;&#25042;&#24816;&#22320;&#32472;&#21046;&#21487;&#33021;&#36816;&#21160;&#30340;&#26679;&#26412;&#65292;&#36880;&#28176;&#21464;&#24471;&#26356;&#20855;&#20960;&#20309;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#23398;&#20064;&#27169;&#22411;&#30340;&#30446;&#26631;&#23548;&#21521;&#31574;&#30053;&#21644;&#24403;&#21069;&#36816;&#21160;&#37319;&#26679;&#25968;&#25454;&#21512;&#24182;&#21040;LAZY&#20013;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#24341;&#23548;&#20219;&#21153;&#35268;&#21010;&#22120;&#65292;&#36825;&#23548;&#33268;&#20102;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#23545;&#35937;&#12289;&#30446;&#26631;&#21644;&#21021;&#22987;&#26465;&#20214;&#30340;&#26410;&#35265;&#27979;&#35797;&#29615;&#22659;&#20013;&#35780;&#20272;&#21487;&#34892;&#35299;&#25628;&#32034;&#30340;&#26174;&#30528;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a solver LAZY for PDDLStream problems, which maintains a single integrated search over action skeletons, gradually becoming more geometrically informed as samples of possible motions are lazily drawn during motion planning. Meanwhile, learned models of goal-directed policies and current motion sampling data are incorporated in LAZY to adaptively guide the task planner, leading to significant speed-ups in the search for a feasible solution evaluated over unseen test environments of varying numbers of objects, goals, and initial conditions.
&lt;/p&gt;
&lt;p&gt;
PDDLStream&#27714;&#35299;&#22120;&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#65288;TAMP&#65289;&#38382;&#39064;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;PDDL&#25193;&#23637;&#21040;&#20855;&#26377;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;PDDLStream&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#31995;&#21015;PDDL&#35268;&#21010;&#38382;&#39064;&#65292;&#28982;&#21518;&#20351;&#29992;&#29616;&#25104;&#30340;&#35268;&#21010;&#22120;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#38271;&#26102;&#38388;&#36816;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LAZY&#65292;&#19968;&#31181;&#29992;&#20110;PDDLStream&#38382;&#39064;&#30340;&#27714;&#35299;&#22120;&#65292;&#23427;&#22312;&#21160;&#20316;&#39592;&#26550;&#19978;&#32500;&#25252;&#21333;&#20010;&#38598;&#25104;&#25628;&#32034;&#65292;&#38543;&#30528;&#22312;&#36816;&#21160;&#35268;&#21010;&#26399;&#38388;&#25042;&#24816;&#22320;&#32472;&#21046;&#21487;&#33021;&#36816;&#21160;&#30340;&#26679;&#26412;&#65292;&#36880;&#28176;&#21464;&#24471;&#26356;&#20855;&#20960;&#20309;&#20449;&#24687;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#30446;&#26631;&#23548;&#21521;&#31574;&#30053;&#30340;&#23398;&#20064;&#27169;&#22411;&#21644;&#24403;&#21069;&#36816;&#21160;&#37319;&#26679;&#25968;&#25454;&#21512;&#24182;&#21040;LAZY&#20013;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#24341;&#23548;&#20219;&#21153;&#35268;&#21010;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#23548;&#33268;&#20102;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#23545;&#35937;&#12289;&#30446;&#26631;&#21644;&#21021;&#22987;&#26465;&#20214;&#30340;&#26410;&#35265;&#27979;&#35797;&#29615;&#22659;&#20013;&#35780;&#20272;&#21487;&#34892;&#35299;&#25628;&#32034;&#30340;&#26174;&#30528;&#21152;&#36895;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;TAMP&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PDDLStream solvers have recently emerged as viable solutions for Task and Motion Planning (TAMP) problems, extending PDDL to problems with continuous action spaces. Prior work has shown how PDDLStream problems can be reduced to a sequence of PDDL planning problems, which can then be solved using off-the-shelf planners. However, this approach can suffer from long runtimes. In this paper we propose LAZY, a solver for PDDLStream problems that maintains a single integrated search over action skeletons, which gets progressively more geometrically informed, as samples of possible motions are lazily drawn during motion planning. We explore how learned models of goal-directed policies and current motion sampling data can be incorporated in LAZY to adaptively guide the task planner. We show that this leads to significant speed-ups in the search for a feasible solution evaluated over unseen test environments of varying numbers of objects, goals, and initial conditions. We evaluate our TAMP appro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#27169;&#25311;&#23558;&#29289;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#34920;&#31034;&#20026;&#36830;&#32493;&#23494;&#24230;&#22330;&#30340;&#23545;&#35937;&#30340;&#36816;&#21160;&#65292;&#20174;&#23494;&#24230;&#22330;&#20013;&#20272;&#35745;&#29289;&#20307;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#22330;&#30340;&#21487;&#24494;&#25509;&#35302;&#27169;&#22411;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33258;&#20027;&#26500;&#24314;&#29289;&#20307;&#27169;&#22411;&#24182;&#20248;&#21270;&#31070;&#32463;&#23545;&#35937;&#30340;&#25235;&#21462;&#21644;&#25805;&#20316;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2210.09420</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#30340;&#21160;&#21147;&#23398;&#22686;&#24378;&#31070;&#32463;&#23545;&#35937;&#30340;&#29289;&#29702;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Differentiable Physics Simulation of Dynamics-Augmented Neural Objects. (arXiv:2210.09420v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#27169;&#25311;&#23558;&#29289;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#34920;&#31034;&#20026;&#36830;&#32493;&#23494;&#24230;&#22330;&#30340;&#23545;&#35937;&#30340;&#36816;&#21160;&#65292;&#20174;&#23494;&#24230;&#22330;&#20013;&#20272;&#35745;&#29289;&#20307;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#22330;&#30340;&#21487;&#24494;&#25509;&#35302;&#27169;&#22411;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33258;&#20027;&#26500;&#24314;&#29289;&#20307;&#27169;&#22411;&#24182;&#20248;&#21270;&#31070;&#32463;&#23545;&#35937;&#30340;&#25235;&#21462;&#21644;&#25805;&#20316;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a differentiable pipeline for simulating the motion of objects that represent their geometry as a continuous density field parameterized as a deep network, estimates the dynamical properties of the object from the density field, introduces a differentiable contact model based on the density field for computing normal and friction forces resulting from collisions, and allows a robot to autonomously build object models and optimize grasps and manipulation trajectories of neural objects.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#27169;&#25311;&#23558;&#29289;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#34920;&#31034;&#20026;&#36830;&#32493;&#23494;&#24230;&#22330;&#30340;&#23545;&#35937;&#30340;&#36816;&#21160;&#65292;&#35813;&#23494;&#24230;&#22330;&#34987;&#21442;&#25968;&#21270;&#20026;&#28145;&#24230;&#32593;&#32476;&#12290;&#36825;&#21253;&#25324;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#21644;&#20854;&#20182;&#30456;&#20851;&#27169;&#22411;&#12290;&#20174;&#23494;&#24230;&#22330;&#20013;&#65292;&#25105;&#20204;&#20272;&#35745;&#29289;&#20307;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#21253;&#25324;&#20854;&#36136;&#37327;&#12289;&#36136;&#24515;&#21644;&#24815;&#24615;&#30697;&#38453;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#22330;&#30340;&#21487;&#24494;&#25509;&#35302;&#27169;&#22411;&#65292;&#29992;&#20110;&#35745;&#31639;&#30001;&#30896;&#25758;&#20135;&#29983;&#30340;&#27861;&#21521;&#21644;&#25705;&#25830;&#21147;&#12290;&#36825;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#33258;&#20027;&#26500;&#24314;&#29289;&#20307;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#20174;&#38745;&#27490;&#22270;&#20687;&#21644;&#36816;&#21160;&#20013;&#30340;&#29289;&#20307;&#35270;&#39057;&#20013;&#35270;&#35273;&#19978;&#21644;&#21160;&#24577;&#19978;&#37117;&#26159;&#20934;&#30830;&#30340;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#21160;&#21147;&#23398;&#22686;&#24378;&#31070;&#32463;&#23545;&#35937;&#65288;DANOs&#65289;&#20351;&#29992;&#29616;&#26377;&#30340;&#21487;&#24494;&#20998;&#27169;&#25311;&#24341;&#25806;Dojo&#36827;&#34892;&#27169;&#25311;&#65292;&#19982;&#20854;&#20182;&#26631;&#20934;&#27169;&#25311;&#23545;&#35937;&#65288;&#22914;&#29699;&#20307;&#12289;&#24179;&#38754;&#21644;&#20197;URDF&#25351;&#23450;&#30340;&#26426;&#22120;&#20154;&#65289;&#36827;&#34892;&#20132;&#20114;&#12290;&#26426;&#22120;&#20154;&#21487;&#20197;&#20351;&#29992;&#36825;&#20010;&#27169;&#25311;&#26469;&#20248;&#21270;&#31070;&#32463;&#23545;&#35937;&#30340;&#25235;&#21462;&#21644;&#25805;&#20316;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a differentiable pipeline for simulating the motion of objects that represent their geometry as a continuous density field parameterized as a deep network. This includes Neural Radiance Fields (NeRFs), and other related models. From the density field, we estimate the dynamical properties of the object, including its mass, center of mass, and inertia matrix. We then introduce a differentiable contact model based on the density field for computing normal and friction forces resulting from collisions. This allows a robot to autonomously build object models that are visually and \emph{dynamically} accurate from still images and videos of objects in motion. The resulting Dynamics-Augmented Neural Objects (DANOs) are simulated with an existing differentiable simulation engine, Dojo, interacting with other standard simulation objects, such as spheres, planes, and robots specified as URDFs. A robot can use this simulation to optimize grasps and manipulation trajectories of neural ob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#30452;&#25509;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#29289;&#20307;&#35270;&#39057;&#19978;&#36827;&#34892;&#31867;&#21035;&#32423;6D&#23039;&#24577;&#20272;&#35745;&#12290;&#36890;&#36807;&#34920;&#38754;&#23884;&#20837;&#23398;&#20064;&#20102;&#36755;&#20837;&#22270;&#20687;&#21644;&#35268;&#33539;&#24418;&#29366;&#20043;&#38388;&#30340;&#23494;&#38598;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#20960;&#20309;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#12290;&#23398;&#20064;&#21040;&#30340;&#23545;&#24212;&#20851;&#31995;&#21487;&#20197;&#24212;&#29992;&#20110;6D&#23039;&#24577;&#20272;&#35745;&#21644;&#20854;&#20182;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2210.07199</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#20960;&#20309;&#23545;&#24212;&#29992;&#20110;&#37326;&#22806;&#31867;&#21035;&#32423;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Geometric Correspondence for Category-Level 6D Object Pose Estimation in the Wild. (arXiv:2210.07199v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#30452;&#25509;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#29289;&#20307;&#35270;&#39057;&#19978;&#36827;&#34892;&#31867;&#21035;&#32423;6D&#23039;&#24577;&#20272;&#35745;&#12290;&#36890;&#36807;&#34920;&#38754;&#23884;&#20837;&#23398;&#20064;&#20102;&#36755;&#20837;&#22270;&#20687;&#21644;&#35268;&#33539;&#24418;&#29366;&#20043;&#38388;&#30340;&#23494;&#38598;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#20960;&#20309;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#12290;&#23398;&#20064;&#21040;&#30340;&#23545;&#24212;&#20851;&#31995;&#21487;&#20197;&#24212;&#29992;&#20110;6D&#23039;&#24577;&#20272;&#35745;&#21644;&#20854;&#20182;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a self-supervised learning approach for category-level 6D object pose estimation in the wild, which reconstructs the canonical 3D shape of an object category and learns dense correspondences between input images and the canonical shape via surface embedding. The proposed novel geometrical cycle-consistency losses construct cycles across 2D-3D spaces, across different instances and different time steps. The learned correspondence can be applied for 6D pose estimation and other tasks.
&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#27880;&#37322;&#65292;&#23427;&#20173;&#28982;&#36828;&#26410;&#35299;&#20915;&#12290;&#24403;&#36716;&#21521;&#31867;&#21035;&#32423;6D&#23039;&#24577;&#26102;&#65292;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#23545;&#26410;&#35265;&#23454;&#20363;&#36827;&#34892;&#27867;&#21270;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#21463;&#21040;&#20174;&#27169;&#25311;&#25110;&#20174;&#20154;&#31867;&#25910;&#38598;&#30340;&#27880;&#37322;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#30452;&#25509;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#29289;&#20307;&#35270;&#39057;&#19978;&#36827;&#34892;&#31867;&#21035;&#32423;6D&#23039;&#24577;&#20272;&#35745;&#65292;&#20811;&#26381;&#20102;&#36825;&#19968;&#38556;&#30861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37325;&#26500;&#20102;&#29289;&#20307;&#31867;&#21035;&#30340;&#35268;&#33539;3D&#24418;&#29366;&#65292;&#24182;&#36890;&#36807;&#34920;&#38754;&#23884;&#20837;&#23398;&#20064;&#20102;&#36755;&#20837;&#22270;&#20687;&#21644;&#35268;&#33539;&#24418;&#29366;&#20043;&#38388;&#30340;&#23494;&#38598;&#23545;&#24212;&#20851;&#31995;&#12290;&#23545;&#20110;&#35757;&#32451;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#20960;&#20309;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#65292;&#23427;&#20204;&#22312;2D-3D&#31354;&#38388;&#12289;&#19981;&#21516;&#23454;&#20363;&#21644;&#19981;&#21516;&#26102;&#38388;&#27493;&#20043;&#38388;&#26500;&#24314;&#24490;&#29615;&#12290;&#23398;&#20064;&#21040;&#30340;&#23545;&#24212;&#20851;&#31995;&#21487;&#20197;&#24212;&#29992;&#20110;6D&#23039;&#24577;&#20272;&#35745;&#21644;&#20854;&#20182;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While 6D object pose estimation has wide applications across computer vision and robotics, it remains far from being solved due to the lack of annotations. The problem becomes even more challenging when moving to category-level 6D pose, which requires generalization to unseen instances. Current approaches are restricted by leveraging annotations from simulation or collected from humans. In this paper, we overcome this barrier by introducing a self-supervised learning approach trained directly on large-scale real-world object videos for category-level 6D pose estimation in the wild. Our framework reconstructs the canonical 3D shape of an object category and learns dense correspondences between input images and the canonical shape via surface embedding. For training, we propose novel geometrical cycle-consistency losses which construct cycles across 2D-3D spaces, across different instances and different time steps. The learned correspondence can be applied for 6D pose estimation and othe
&lt;/p&gt;</description></item><item><title>ParaGon&#26159;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#35299;&#26512;&#21644;&#35270;&#35273;&#23450;&#20301;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#25351;&#20196;&#35299;&#26512;&#20026;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#22270;&#24418;&#34920;&#31034;&#65292;&#20197;&#21333;&#29420;&#23450;&#20301;&#23545;&#35937;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#25512;&#29702;&#20851;&#20110;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#29289;&#20307;&#25918;&#32622;&#12290;</title><link>http://arxiv.org/abs/2210.00215</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#35299;&#26512;&#21644;&#35270;&#35273;&#23450;&#20301;&#22312;&#29289;&#20307;&#25918;&#32622;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Differentiable Parsing and Visual Grounding of Natural Language Instructions for Object Placement. (arXiv:2210.00215v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00215
&lt;/p&gt;
&lt;p&gt;
ParaGon&#26159;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#35299;&#26512;&#21644;&#35270;&#35273;&#23450;&#20301;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#25351;&#20196;&#35299;&#26512;&#20026;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#22270;&#24418;&#34920;&#31034;&#65292;&#20197;&#21333;&#29420;&#23450;&#20301;&#23545;&#35937;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#25512;&#29702;&#20851;&#20110;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#29289;&#20307;&#25918;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
ParaGon is a differentiable method for natural language instruction parsing and visual grounding in object placement tasks. It parses language instructions into an object-centric graph representation to ground objects individually and uses a novel particle-based graph neural network to reason about object placements with uncertainty.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;PARsing And visual GrOuNding (ParaGon)&#65292;&#29992;&#20110;&#22312;&#29289;&#20307;&#25918;&#32622;&#20219;&#21153;&#20013;&#23545;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#23450;&#20301;&#12290;&#33258;&#28982;&#35821;&#35328;&#36890;&#24120;&#29992;&#32452;&#21512;&#24615;&#21644;&#27495;&#20041;&#24615;&#25551;&#36848;&#23545;&#35937;&#21644;&#31354;&#38388;&#20851;&#31995;&#65292;&#36825;&#26159;&#26377;&#25928;&#35821;&#35328;&#23450;&#20301;&#30340;&#20004;&#20010;&#20027;&#35201;&#38556;&#30861;&#12290;&#23545;&#20110;&#32452;&#21512;&#24615;&#65292;ParaGon&#23558;&#35821;&#35328;&#25351;&#20196;&#35299;&#26512;&#20026;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#22270;&#24418;&#34920;&#31034;&#65292;&#20197;&#21333;&#29420;&#23450;&#20301;&#23545;&#35937;&#12290;&#23545;&#20110;&#27495;&#20041;&#24615;&#65292;ParaGon&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#25512;&#29702;&#20851;&#20110;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#29289;&#20307;&#25918;&#32622;&#12290;&#26412;&#36136;&#19978;&#65292;ParaGon&#23558;&#35299;&#26512;&#31639;&#27861;&#38598;&#25104;&#21040;&#27010;&#29575;&#30340;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#23427;&#26159;&#23436;&#20840;&#21487;&#24494;&#20998;&#30340;&#65292;&#24182;&#20174;&#25968;&#25454;&#20013;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;&#65292;&#20197;&#23545;&#25239;&#22797;&#26434;&#30340;&#65292;&#27169;&#31946;&#30340;&#35821;&#35328;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new method, PARsing And visual GrOuNding (ParaGon), for grounding natural language in object placement tasks. Natural language generally describes objects and spatial relations with compositionality and ambiguity, two major obstacles to effective language grounding. For compositionality, ParaGon parses a language instruction into an object-centric graph representation to ground objects individually. For ambiguity, ParaGon uses a novel particle-based graph neural network to reason about object placements with uncertainty. Essentially, ParaGon integrates a parsing algorithm into a probabilistic, data-driven learning framework. It is fully differentiable and trained end-to-end from data for robustness against complex, ambiguous language input.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#30340;&#21435;&#20013;&#24515;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#29420;&#31435;&#22320;&#26368;&#23567;&#21270;&#19982;&#30446;&#26631;&#35775;&#38382;&#20998;&#24067;&#30340;&#30456;&#24212;&#20998;&#37327;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#21487;&#20197;&#23454;&#29616;&#25910;&#25947;&#21040;&#29983;&#25104;&#30446;&#26631;&#20998;&#24067;&#30340;&#32852;&#21512;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2206.00233</link><description>&lt;p&gt;
DM$^2$: &#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#30340;&#21435;&#20013;&#24515;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DM$^2$: Decentralized Multi-Agent Reinforcement Learning for Distribution Matching. (arXiv:2206.00233v3 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#30340;&#21435;&#20013;&#24515;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#29420;&#31435;&#22320;&#26368;&#23567;&#21270;&#19982;&#30446;&#26631;&#35775;&#38382;&#20998;&#24067;&#30340;&#30456;&#24212;&#20998;&#37327;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#21487;&#20197;&#23454;&#29616;&#25910;&#25947;&#21040;&#29983;&#25104;&#30446;&#26631;&#20998;&#24067;&#30340;&#32852;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a decentralized multi-agent reinforcement learning method based on distribution matching, where each agent independently minimizes the distribution mismatch to the corresponding component of a target visitation distribution, achieving convergence to the joint policy that generated the target distribution.
&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#38598;&#20013;&#24335;&#26426;&#21046;&#25110;&#26174;&#24335;&#36890;&#20449;&#21327;&#35758;&#20197;&#30830;&#20445;&#25910;&#25947;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#21305;&#37197;&#22312;&#19981;&#20381;&#36182;&#20110;&#38598;&#20013;&#24335;&#32452;&#20214;&#25110;&#26174;&#24335;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#20013;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#29420;&#31435;&#22320;&#26368;&#23567;&#21270;&#19982;&#30446;&#26631;&#35775;&#38382;&#20998;&#24067;&#30340;&#30456;&#24212;&#20998;&#37327;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#26368;&#23567;&#21270;&#20854;&#20010;&#20307;&#20998;&#24067;&#19981;&#21305;&#37197;&#21487;&#20197;&#23454;&#29616;&#25910;&#25947;&#21040;&#29983;&#25104;&#30446;&#26631;&#20998;&#24067;&#30340;&#32852;&#21512;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#30446;&#26631;&#20998;&#24067;&#26469;&#33258;&#20248;&#21270;&#21512;&#20316;&#20219;&#21153;&#30340;&#32852;&#21512;&#31574;&#30053;&#65292;&#21017;&#35813;&#20219;&#21153;&#22870;&#21169;&#21644;&#20998;&#24067;&#21305;&#37197;&#22870;&#21169;&#30340;&#32452;&#21512;&#30340;&#26368;&#20248;&#31574;&#30053;&#26159;&#30456;&#21516;&#30340;&#32852;&#21512;&#31574;&#30053;&#12290;&#36825;&#19968;&#35265;&#35299;&#34987;&#29992;&#26469;&#21046;&#23450;&#19968;&#20010;&#23454;&#29992;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current approaches to multi-agent cooperation rely heavily on centralized mechanisms or explicit communication protocols to ensure convergence. This paper studies the problem of distributed multi-agent learning without resorting to centralized components or explicit communication. It examines the use of distribution matching to facilitate the coordination of independent agents. In the proposed scheme, each agent independently minimizes the distribution mismatch to the corresponding component of a target visitation distribution. The theoretical analysis shows that under certain conditions, each agent minimizing its individual distribution mismatch allows the convergence to the joint policy that generated the target distribution. Further, if the target distribution is from a joint policy that optimizes a cooperative task, the optimal policy for a combination of this task reward and the distribution matching reward is the same joint policy. This insight is used to formulate a practical al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#29289;&#20307;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#34920;&#31034;&#19981;&#33021;&#24456;&#22909;&#22320;&#36716;&#31227;&#21040;&#20854;&#20182;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#30001;&#30417;&#30563;&#26041;&#27861;&#23398;&#20064;&#30340;&#34920;&#31034;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.06333</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#20307;&#24863;&#30693;&#34920;&#31034;&#22312;&#22810;&#29289;&#20307;&#22330;&#26223;&#20013;&#36827;&#34892;&#35270;&#35273;&#36816;&#21160;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Visuomotor Control in Multi-Object Scenes Using Object-Aware Representations. (arXiv:2205.06333v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.06333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#29289;&#20307;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#34920;&#31034;&#19981;&#33021;&#24456;&#22909;&#22320;&#36716;&#31227;&#21040;&#20854;&#20182;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#30001;&#30417;&#30563;&#26041;&#27861;&#23398;&#20064;&#30340;&#34920;&#31034;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the effectiveness of using object-aware representation learning techniques for robotic tasks, to address the problem that current methodologies learn task specific representations that do not necessarily transfer well to other tasks, and that representations learned by supervised methods require large labeled datasets for each task that are expensive to collect in the real world.
&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#30340;&#24863;&#30693;&#29702;&#35299;&#20197;&#21450;&#20854;&#19981;&#21516;&#32452;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#23545;&#20110;&#25104;&#21151;&#23436;&#25104;&#26426;&#22120;&#20154;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#34920;&#31034;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#20294;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#26041;&#27861;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#34920;&#31034;&#65292;&#19981;&#19968;&#23450;&#33021;&#22815;&#24456;&#22909;&#22320;&#36716;&#31227;&#21040;&#20854;&#20182;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#30001;&#30417;&#30563;&#26041;&#27861;&#23398;&#20064;&#30340;&#34920;&#31034;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25910;&#38598;&#36215;&#26469;&#24456;&#26114;&#36149;&#12290;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#33719;&#21462;&#34920;&#31034;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22823;&#22810;&#26159;&#29289;&#20307;&#26080;&#20851;&#30340;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;&#27492;&#24471;&#21040;&#30340;&#34920;&#31034;&#23545;&#20110;&#20855;&#26377;&#35768;&#22810;&#32452;&#20214;&#30340;&#22330;&#26223;&#30340;&#36890;&#29992;&#26426;&#22120;&#20154;&#20219;&#21153;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#29289;&#20307;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perceptual understanding of the scene and the relationship between its different components is important for successful completion of robotic tasks. Representation learning has been shown to be a powerful technique for this, but most of the current methodologies learn task specific representations that do not necessarily transfer well to other tasks. Furthermore, representations learned by supervised methods require large labeled datasets for each task that are expensive to collect in the real world. Using self-supervised learning to obtain representations from unlabeled data can mitigate this problem. However, current self-supervised representation learning methods are mostly object agnostic, and we demonstrate that the resulting representations are insufficient for general purpose robotics tasks as they fail to capture the complexity of scenes with many components. In this paper, we explore the effectiveness of using object-aware representation learning techniques for robotic tasks. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25197;&#30697;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#30452;&#25509;&#39044;&#27979;&#20851;&#33410;&#25197;&#30697;&#65292;&#36991;&#20813;&#20351;&#29992;PD&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#22235;&#36275;&#21160;&#29289;&#33021;&#22815;&#31359;&#36234;&#21508;&#31181;&#22320;&#24418;&#24182;&#25269;&#25239;&#22806;&#37096;&#24178;&#25200;&#65292;&#21516;&#26102;&#20445;&#25345;&#36816;&#21160;&#12290;</title><link>http://arxiv.org/abs/2203.05194</link><description>&lt;p&gt;
&#23398;&#20064;&#22235;&#36275;&#21160;&#29289;&#36816;&#21160;&#30340;&#25197;&#30697;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Learning Torque Control for Quadrupedal Locomotion. (arXiv:2203.05194v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25197;&#30697;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#30452;&#25509;&#39044;&#27979;&#20851;&#33410;&#25197;&#30697;&#65292;&#36991;&#20813;&#20351;&#29992;PD&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#22235;&#36275;&#21160;&#29289;&#33021;&#22815;&#31359;&#36234;&#21508;&#31181;&#22320;&#24418;&#24182;&#25269;&#25239;&#22806;&#37096;&#24178;&#25200;&#65292;&#21516;&#26102;&#20445;&#25345;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a torque-based reinforcement learning framework that directly predicts joint torques, avoiding the use of a PD controller. The framework is validated through extensive experiments, where a quadruped is capable of traversing various terrain and resisting external disturbances while maintaining locomotion.
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#25104;&#20026;&#24320;&#21457;&#22235;&#36275;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#19978;&#65292;&#29992;&#20110;&#36816;&#21160;&#30340;RL&#35774;&#35745;&#36981;&#24490;&#22522;&#20110;&#20301;&#32622;&#30340;&#33539;&#20363;&#65292;&#20854;&#20013;RL&#31574;&#30053;&#20197;&#20302;&#39057;&#29575;&#36755;&#20986;&#30446;&#26631;&#20851;&#33410;&#20301;&#32622;&#65292;&#28982;&#21518;&#30001;&#39640;&#39057;&#27604;&#20363;-&#23548;&#25968;&#65288;PD&#65289;&#25511;&#21046;&#22120;&#36319;&#36394;&#20197;&#20135;&#29983;&#20851;&#33410;&#25197;&#30697;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23545;&#20110;&#22235;&#36275;&#21160;&#29289;&#36816;&#21160;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#65292;&#24050;&#32463;&#20174;&#22522;&#20110;&#20301;&#32622;&#30340;&#25511;&#21046;&#33539;&#20363;&#36716;&#21521;&#22522;&#20110;&#25197;&#30697;&#30340;&#25511;&#21046;&#12290;&#37492;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25197;&#30697;&#30340;RL&#26694;&#26550;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#26367;&#20195;&#22522;&#20110;&#20301;&#32622;&#30340;RL&#33539;&#20363;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;RL&#31574;&#30053;&#30452;&#25509;&#22312;&#39640;&#39057;&#29575;&#19979;&#39044;&#27979;&#20851;&#33410;&#25197;&#30697;&#65292;&#20174;&#32780;&#36991;&#20813;&#20351;&#29992;PD&#25511;&#21046;&#22120;&#12290;&#25152;&#25552;&#20986;&#30340;&#23398;&#20064;&#25197;&#30697;&#25511;&#21046;&#26694;&#26550;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#22312;&#36825;&#20123;&#23454;&#39564;&#20013;&#65292;&#22235;&#36275;&#21160;&#29289;&#33021;&#22815;&#31359;&#36234;&#21508;&#31181;&#22320;&#24418;&#24182;&#25269;&#25239;&#22806;&#37096;&#24178;&#25200;&#65292;&#21516;&#26102;&#20445;&#25345;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has become a promising approach to developing controllers for quadrupedal robots. Conventionally, an RL design for locomotion follows a position-based paradigm, wherein an RL policy outputs target joint positions at a low frequency that are then tracked by a high-frequency proportional-derivative (PD) controller to produce joint torques. In contrast, for the model-based control of quadrupedal locomotion, there has been a paradigm shift from position-based control to torque-based control. In light of the recent advances in model-based control, we explore an alternative to the position-based RL paradigm, by introducing a torque-based RL framework, where an RL policy directly predicts joint torques at a high frequency, thus circumventing the use of a PD controller. The proposed learning torque control framework is validated with extensive experiments, in which a quadruped is capable of traversing various terrain and resisting external disturbances while followi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#20449;&#24687;&#30340;&#31526;&#21495;&#35268;&#21010;&#22120;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20248;&#20808;&#32423;&#25490;&#24207;&#65292;&#20197;&#26368;&#20339;&#20248;&#20808;&#26041;&#24335;&#25193;&#23637;&#23545;&#35937;&#21644;&#20107;&#23454;&#38598;&#21512;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#22312;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#20013;&#30340;&#38271;&#26399;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;7&#33258;&#30001;&#24230;&#26426;&#26800;&#33218;&#30340;&#22534;&#21472;&#25805;&#32437;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2111.13144</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#20013;&#20351;&#29992;&#27969;&#36827;&#34892;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Learning to Search in Task and Motion Planning with Streams. (arXiv:2111.13144v5 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.13144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#20449;&#24687;&#30340;&#31526;&#21495;&#35268;&#21010;&#22120;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20248;&#20808;&#32423;&#25490;&#24207;&#65292;&#20197;&#26368;&#20339;&#20248;&#20808;&#26041;&#24335;&#25193;&#23637;&#23545;&#35937;&#21644;&#20107;&#23454;&#38598;&#21512;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#22312;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#20013;&#30340;&#38271;&#26399;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;7&#33258;&#30001;&#24230;&#26426;&#26800;&#33218;&#30340;&#22534;&#21472;&#25805;&#32437;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a geometrically informed symbolic planner that expands the set of objects and facts in a best-first manner, prioritized by a Graph Neural Network that is learned from prior search computations, improving the long-term reasoning ability in task and motion planning. The algorithm is applied to a 7DOF robotic arm in block-stacking manipulation tasks.
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#20013;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#38382;&#39064;&#23558;&#31163;&#25955;&#20219;&#21153;&#21464;&#37327;&#19978;&#30340;&#31526;&#21495;&#35268;&#21010;&#19982;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#21464;&#37327;&#19978;&#30340;&#36816;&#21160;&#20248;&#21270;&#30456;&#32467;&#21512;&#12290;&#26368;&#36817;&#30340;&#20316;&#21697;&#65292;&#22914;PDDLStream&#65292;&#19987;&#27880;&#20110;&#20048;&#35266;&#35268;&#21010;&#65292;&#20351;&#29992;&#36880;&#27493;&#22686;&#38271;&#30340;&#23545;&#35937;&#38598;&#65292;&#30452;&#21040;&#25214;&#21040;&#21487;&#34892;&#30340;&#36712;&#36857;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#38598;&#21512;&#26159;&#20197;&#24191;&#24230;&#20248;&#20808;&#30340;&#26041;&#24335;&#31351;&#20030;&#25193;&#23637;&#30340;&#65292;&#32780;&#19981;&#32771;&#34385;&#25163;&#22836;&#38382;&#39064;&#30340;&#36923;&#36753;&#21644;&#20960;&#20309;&#32467;&#26500;&#65292;&#36825;&#20351;&#24471;&#20855;&#26377;&#22823;&#37327;&#23545;&#35937;&#30340;&#38271;&#26399;&#25512;&#29702;&#21464;&#24471;&#32791;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20309;&#20449;&#24687;&#30340;&#31526;&#21495;&#35268;&#21010;&#22120;&#65292;&#20197;&#26368;&#20339;&#20248;&#20808;&#26041;&#24335;&#25193;&#23637;&#23545;&#35937;&#21644;&#20107;&#23454;&#38598;&#21512;&#65292;&#30001;&#20808;&#21069;&#30340;&#25628;&#32034;&#35745;&#31639;&#23398;&#20064;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20248;&#20808;&#32423;&#25490;&#24207;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22256;&#38590;&#24773;&#20917;&#19979;&#35268;&#21010;&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#24212;&#29992;&#20110;7&#33258;&#30001;&#24230;&#26426;&#26800;&#33218;&#22312;&#22534;&#21472;&#25805;&#32437;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task and motion planning problems in robotics combine symbolic planning over discrete task variables with motion optimization over continuous state and action variables. Recent works such as PDDLStream have focused on optimistic planning with an incrementally growing set of objects until a feasible trajectory is found. However, this set is exhaustively expanded in a breadth-first manner, regardless of the logical and geometric structure of the problem at hand, which makes long-horizon reasoning with large numbers of objects prohibitively time-consuming. To address this issue, we propose a geometrically informed symbolic planner that expands the set of objects and facts in a best-first manner, prioritized by a Graph Neural Network that is learned from prior search computations. We evaluate our approach on a diverse set of problems and demonstrate an improved ability to plan in difficult scenarios. We also apply our algorithm on a 7DOF robotic arm in block-stacking manipulation tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#36328;&#27169;&#24577;&#20914;&#31361;&#35299;&#20915;&#30340;&#31070;&#32463;&#26426;&#22120;&#20154;&#33539;&#20363;&#65292;&#20351;&#26426;&#22120;&#20154;&#34920;&#29616;&#20986;&#31867;&#20154;&#30340;&#31038;&#20132;&#20851;&#27880;&#65292;&#20026;&#22686;&#24378;&#20154;&#26426;&#31038;&#20132;&#20114;&#21160;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2111.01906</link><description>&lt;p&gt;
&#35757;&#32451;&#36807;&#30340;&#20154;&#24418;&#26426;&#22120;&#20154;&#21487;&#20197;&#25191;&#34892;&#31867;&#20154;&#30340;&#36328;&#27169;&#24577;&#31038;&#20132;&#20851;&#27880;&#21644;&#20914;&#31361;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
A trained humanoid robot can perform human-like crossmodal social attention and conflict resolution. (arXiv:2111.01906v5 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.01906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#36328;&#27169;&#24577;&#20914;&#31361;&#35299;&#20915;&#30340;&#31070;&#32463;&#26426;&#22120;&#20154;&#33539;&#20363;&#65292;&#20351;&#26426;&#22120;&#20154;&#34920;&#29616;&#20986;&#31867;&#20154;&#30340;&#31038;&#20132;&#20851;&#27880;&#65292;&#20026;&#22686;&#24378;&#20154;&#26426;&#31038;&#20132;&#20114;&#21160;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study adopts the neurorobotic paradigm of crossmodal conflict resolution to make a robot express human-like social attention, providing a new approach to enhance human-robot social interaction.
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#24378;&#20154;&#26426;&#31038;&#20132;&#20114;&#21160;&#65292;&#26426;&#22120;&#20154;&#22312;&#22797;&#26434;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#22788;&#29702;&#22810;&#20010;&#31038;&#20132;&#32447;&#32034;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36328;&#27169;&#24577;&#36755;&#20837;&#20449;&#24687;&#30340;&#19981;&#19968;&#33268;&#24615;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#36825;&#21487;&#33021;&#23545;&#26426;&#22120;&#20154;&#30340;&#22788;&#29702;&#36896;&#25104;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#36328;&#27169;&#24577;&#20914;&#31361;&#35299;&#20915;&#30340;&#31070;&#32463;&#26426;&#22120;&#20154;&#33539;&#20363;&#65292;&#20351;&#26426;&#22120;&#20154;&#34920;&#29616;&#20986;&#31867;&#20154;&#30340;&#31038;&#20132;&#20851;&#27880;&#12290;&#25105;&#20204;&#23545;37&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#19968;&#39033;&#34892;&#20026;&#23454;&#39564;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22278;&#26700;&#20250;&#35758;&#22330;&#26223;&#65292;&#26377;&#19977;&#20010;&#21160;&#30011;&#21270;&#30340;&#22836;&#20687;&#65292;&#20197;&#25552;&#39640;&#29983;&#24577;&#25928;&#24230;&#12290;&#27599;&#20010;&#22836;&#20687;&#37117;&#25140;&#30528;&#21307;&#29992;&#21475;&#32617;&#65292;&#36974;&#30422;&#20102;&#40763;&#23376;&#12289;&#22068;&#24052;&#21644;&#19979;&#24052;&#30340;&#38754;&#37096;&#32447;&#32034;&#12290;&#20013;&#22830;&#22836;&#20687;&#31227;&#21160;&#20854;&#30524;&#30555;&#27880;&#35270;&#65292;&#32780;&#22806;&#22260;&#22836;&#20687;&#21017;&#21457;&#20986;&#22768;&#38899;&#12290;&#20957;&#35270;&#26041;&#21521;&#21644;&#22768;&#38899;&#20301;&#32622;&#35201;&#20040;&#26159;&#31354;&#38388;&#19978;&#19968;&#33268;&#30340;&#65292;&#35201;&#20040;&#26159;&#19981;&#19968;&#33268;&#30340;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20013;&#22830;&#22836;&#20687;&#30340;&#21160;&#24577;&#20957;&#35270;&#21487;&#20197;&#35302;&#21457;&#36328;&#27169;&#24577;&#31038;&#20132;&#20851;&#27880;&#21453;&#24212;&#12290;&#29305;&#21035;&#26159;&#65292;&#20154;&#31867;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
To enhance human-robot social interaction, it is essential for robots to process multiple social cues in a complex real-world environment. However, incongruency of input information across modalities is inevitable and could be challenging for robots to process. To tackle this challenge, our study adopted the neurorobotic paradigm of crossmodal conflict resolution to make a robot express human-like social attention. A behavioural experiment was conducted on 37 participants for the human study. We designed a round-table meeting scenario with three animated avatars to improve ecological validity. Each avatar wore a medical mask to obscure the facial cues of the nose, mouth, and jaw. The central avatar shifted its eye gaze while the peripheral avatars generated sound. Gaze direction and sound locations were either spatially congruent or incongruent. We observed that the central avatar's dynamic gaze could trigger crossmodal social attention responses. In particular, human performances are 
&lt;/p&gt;</description></item></channel></rss>