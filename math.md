# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Data Dependent Regret Guarantees Against General Comparators for Full or Bandit Feedback.](http://arxiv.org/abs/2303.06526) | 该论文提出了一个数据相关的在线学习算法框架，可以在全专家反馈和Bandit反馈设置中具有数据相关的遗憾保证，适用于各种问题场景。 |
| [^2] | [Multistage Stochastic Optimization via Kernels.](http://arxiv.org/abs/2303.06515) | 我们提出了一种基于核的多阶段随机优化方法，能够在多维设置中表现良好，并且在数据规模较大时仍然可行。 |
| [^3] | [Deep Reinforcement Learning Based Power Allocation for Minimizing AoI and Energy Consumption in MIMO-NOMA IoT Systems.](http://arxiv.org/abs/2303.06411) | 本文提出了一种基于深度强化学习的MIMO-NOMA IoT系统功率分配方法，以最小化AoI和能耗。 |
| [^4] | [Learning to Precode for Integrated Sensing and Communications Systems.](http://arxiv.org/abs/2303.06381) | 本文提出了一种无监督学习神经模型，用于设计集成感知和通信（ISAC）系统的传输预编码器，以最大化最坏情况下的目标照明功率，同时确保所有用户的最小信干噪比（SINR）。通过数值模拟，证明了该方法在存在信道估计误差的情况下优于传统的基于优化的方法，同时产生较小的计算复杂度，并且在不同的信道条件下具有良好的泛化能力。 |
| [^5] | [Privacy-Preserving Cooperative Visible Light Positioning for Nonstationary Environment: A Federated Learning Perspective.](http://arxiv.org/abs/2303.06361) | 本文提出了一种基于联邦学习的合作可见光定位方案，通过共同训练适应环境变化的全局模型，提高了在非静态环境下的定位精度和泛化能力。 |
| [^6] | [Fast computation of permutation equivariant layers with the partition algebra.](http://arxiv.org/abs/2303.06208) | 本文提出了一种利用分区代数计算置换等变层输出和梯度的新算法，可以在输入大小的线性和二次时间内计算，有效性得到了在几个基准数据集上的证明。 |
| [^7] | [Deflated HeteroPCA: Overcoming the curse of ill-conditioning in heteroskedastic PCA.](http://arxiv.org/abs/2303.06198) | 本文提出了一种新的算法，称为缩减异方差PCA，它在克服病态问题的同时实现了近乎最优和无条件数的理论保证。 |
| [^8] | [Probabilistic Overview of Probabilities of Default for Low Default Portfolios by K. Pluto and D. Tasche.](http://arxiv.org/abs/2303.06148) | 本文概述了K. Pluto和D. Tasche提出的违约概率估计方法，详细列出了假设和推导，为早期职业分析师或学者提供更多的清晰度，特别是关于借款人独立性、条件独立性和概率分布之间的交互作用，如二项式、贝塔、正态分布等的假设。同时，还展示了违约概率与$\sqrt{\varrho}X-\sqrt{1-\varrho}Y$的联合分布之间的关系。 |
| [^9] | [Enhanced Adaptive Gradient Algorithms for Nonconvex-PL Minimax Optimization.](http://arxiv.org/abs/2303.03984) | 本文提出了一类增强的基于动量的梯度下降上升方法（即MSGDA和AdaMSGDA）来解决非凸-PL极小极大问题，其中AdaMSGDA算法可以使用各种自适应学习率来更新变量$x$和$y$，而不依赖于任何全局和坐标自适应学习率。理论上，我们证明了我们的MSGDA和AdaMSGDA方法在找到$\epsilon$-稳定解时，只需要在每个循环中进行一次采样，就可以获得已知的最佳样本（梯度）复杂度$O(\epsilon^{-3})$。 |
| [^10] | [On Penalty-based Bilevel Gradient Descent Method.](http://arxiv.org/abs/2302.05185) | 本文提出了基于惩罚的双层梯度下降算法，解决了下层非强凸约束双层问题，实验表明该算法有效。 |
| [^11] | [Quantum Monte Carlo algorithm for solving Black-Scholes PDEs for high-dimensional option pricing in finance and its proof of overcoming the curse of dimensionality.](http://arxiv.org/abs/2301.09241) | 本文提出了一种量子蒙特卡罗算法，用于解决高维Black-Scholes PDE的高维期权定价，其复杂度被多项式地限制，克服了维度诅咒。 |
| [^12] | [Duality for Neural Networks through Reproducing Kernel Banach Spaces.](http://arxiv.org/abs/2211.05020) | 本文提出了一种新的再生核Banach空间（RKBS）方法，用于解决神经网络中的对偶性问题，构建了神经网络的鞍点问题，可用于原始-对偶优化的整个领域。 |
| [^13] | [A Dynamical System View of Langevin-Based Non-Convex Sampling.](http://arxiv.org/abs/2210.13867) | 本文提出了一种新的框架，通过利用动力系统理论中的几个工具来解决非凸采样中的重要挑战。对于一大类最先进的采样方案，它们在Wasserstein距离下的最后迭代收敛可以归结为对它们的连续时间对应物的研究，这是更好理解的。 |
| [^14] | [Communication-Efficient Topologies for Decentralized Learning with $O(1)$ Consensus Rate.](http://arxiv.org/abs/2210.07881) | 本文提出了一种新的拓扑结构家族EquiTopo，它具有（几乎）恒定的度数和与网络大小无关的共识速率，用于衡量混合效率。 |
| [^15] | [On the Stability Analysis of Open Federated Learning Systems.](http://arxiv.org/abs/2209.12307) | 本文研究了开放式联邦学习系统的稳定性问题，提出了一种新的性能度量，即开放式FL系统的稳定性，并在假设本地客户端函数是强凸和平滑的情况下，理论上量化了两种FL算法的稳定半径。 |
| [^16] | [Optimization with access to auxiliary information.](http://arxiv.org/abs/2206.00395) | 本文研究了在计算目标函数梯度很昂贵或有限的情况下，给定一些辅助函数的情况下，如何最小化目标函数。作者提出了两种通用的新算法，并证明了这个框架可以受益于目标和辅助信息之间的Hessian相似性假设。 |
| [^17] | [Causal inference with misspecified exposure mappings: separating definitions and assumptions.](http://arxiv.org/abs/2103.06471) | 本文提出了一种新的方法，将曝光映射的两个作用分开，从而在曝光被错误指定时精确估计曝光效应，避免了常常是可疑的假设。 |

# 详细

[^1]: 数据相关的在线学习算法框架

    Data Dependent Regret Guarantees Against General Comparators for Full or Bandit Feedback. (arXiv:2303.06526v1 [cs.LG])

    [http://arxiv.org/abs/2303.06526](http://arxiv.org/abs/2303.06526)

    该论文提出了一个数据相关的在线学习算法框架，可以在全专家反馈和Bandit反馈设置中具有数据相关的遗憾保证，适用于各种问题场景。

    This paper proposes a data-dependent online learning algorithm framework that has data-dependent regret guarantees in both full expert feedback and bandit feedback settings, applicable for a wide variety of problem scenarios.

    我们研究了对抗性在线学习问题，并创建了一个完全在线的算法框架，具有在全专家反馈和Bandit反馈设置中具有数据相关的遗憾保证。我们研究了我们的算法对一般比较器的预期性能，使其适用于各种问题场景。我们的算法从通用预测角度工作，使用的性能度量是对任意比较器序列的预期遗憾，即我们的损失与竞争损失序列之间的差异。竞争类可以设计为包括固定臂选择、切换Bandit、上下文Bandit、周期Bandit或任何其他感兴趣的竞争。竞争类中的序列通常由具体应用程序确定，并应相应地设计。我们的算法既不使用也不需要任何有关损失序列的初步信息，完全在线。其

    We study the adversarial online learning problem and create a completely online algorithmic framework that has data dependent regret guarantees in both full expert feedback and bandit feedback settings. We study the expected performance of our algorithm against general comparators, which makes it applicable for a wide variety of problem scenarios. Our algorithm works from a universal prediction perspective and the performance measure used is the expected regret against arbitrary comparator sequences, which is the difference between our losses and a competing loss sequence. The competition class can be designed to include fixed arm selections, switching bandits, contextual bandits, periodic bandits or any other competition of interest. The sequences in the competition class are generally determined by the specific application at hand and should be designed accordingly. Our algorithm neither uses nor needs any preliminary information about the loss sequences and is completely online. Its
    
[^2]: 基于核的多阶段随机优化

    Multistage Stochastic Optimization via Kernels. (arXiv:2303.06515v1 [math.OC])

    [http://arxiv.org/abs/2303.06515](http://arxiv.org/abs/2303.06515)

    我们提出了一种基于核的多阶段随机优化方法，能够在多维设置中表现良好，并且在数据规模较大时仍然可行。

    

    我们提出了一种非参数、数据驱动、可行的方法来解决多阶段随机优化问题，其中决策不影响不确定性。所提出的框架将决策变量表示为再生核希尔伯特空间的元素，并执行函数随机梯度下降来最小化经验正则化损失。通过结合基于函数子空间投影的稀疏化技术，我们能够克服标准核方法引入的计算复杂度随着数据大小的增加而增加的问题。我们证明了所提出的方法在具有辅助信息的多阶段随机优化中是渐近最优的。在各种随机库存管理问题的计算实验中，我们的方法在多维设置中表现良好，并且在数据规模较大时仍然可行。最后，通过计算库存控制问题的最优损失的下界，我们展示了所提出的方法的有效性。

    We develop a non-parametric, data-driven, tractable approach for solving multistage stochastic optimization problems in which decisions do not affect the uncertainty. The proposed framework represents the decision variables as elements of a reproducing kernel Hilbert space and performs functional stochastic gradient descent to minimize the empirical regularized loss. By incorporating sparsification techniques based on function subspace projections we are able to overcome the computational complexity that standard kernel methods introduce as the data size increases. We prove that the proposed approach is asymptotically optimal for multistage stochastic optimization with side information. Across various computational experiments on stochastic inventory management problems, {our method performs well in multidimensional settings} and remains tractable when the data size is large. Lastly, by computing lower bounds for the optimal loss of the inventory control problem, we show that the propo
    
[^3]: 基于深度强化学习的MIMO-NOMA IoT系统功率分配，以最小化AoI和能耗

    Deep Reinforcement Learning Based Power Allocation for Minimizing AoI and Energy Consumption in MIMO-NOMA IoT Systems. (arXiv:2303.06411v1 [cs.IT])

    [http://arxiv.org/abs/2303.06411](http://arxiv.org/abs/2303.06411)

    本文提出了一种基于深度强化学习的MIMO-NOMA IoT系统功率分配方法，以最小化AoI和能耗。

    This paper proposes a deep reinforcement learning based power allocation method for MIMO-NOMA IoT systems to minimize AoI and energy consumption.

    多输入多输出和非正交多址（MIMO-NOMA）物联网（IoT）系统可以显著提高信道容量和频谱效率，以支持实时应用。时延（AoI）是实时应用的重要指标，但没有文献最小化MIMO-NOMA IoT系统的AoI，这促使我们进行这项工作。在MIMO-NOMA IoT系统中，基站（BS）确定样本收集要求并为每个IoT设备分配传输功率。每个设备根据样本收集要求确定是否采样数据，并采用分配的功率将采样的数据通过MIMO-NOMA信道传输到BS。然后，BS采用连续干扰消除（SIC）技术解码每个设备传输的数据信号。样本收集要求和功率分配将影响系统的AoI和能耗。这是至关重要的。

    Multi-input multi-out and non-orthogonal multiple access (MIMO-NOMA) internet-of-things (IoT) systems can improve channel capacity and spectrum efficiency distinctly to support the real-time applications. Age of information (AoI) is an important metric for real-time application, but there is no literature have minimized AoI of the MIMO-NOMA IoT system, which motivates us to conduct this work. In MIMO-NOMA IoT system, the base station (BS) determines the sample collection requirements and allocates the transmission power for each IoT device. Each device determines whether to sample data according to the sample collection requirements and adopts the allocated power to transmit the sampled data to the BS over MIMO-NOMA channel. Afterwards, the BS employs successive interference cancelation (SIC) technique to decode the signal of the data transmitted by each device. The sample collection requirements and power allocation would affect AoI and energy consumption of the system. It is critical
    
[^4]: 学习预编码用于集成感知和通信系统

    Learning to Precode for Integrated Sensing and Communications Systems. (arXiv:2303.06381v1 [eess.SP])

    [http://arxiv.org/abs/2303.06381](http://arxiv.org/abs/2303.06381)

    本文提出了一种无监督学习神经模型，用于设计集成感知和通信（ISAC）系统的传输预编码器，以最大化最坏情况下的目标照明功率，同时确保所有用户的最小信干噪比（SINR）。通过数值模拟，证明了该方法在存在信道估计误差的情况下优于传统的基于优化的方法，同时产生较小的计算复杂度，并且在不同的信道条件下具有良好的泛化能力。

    This paper proposes an unsupervised learning neural model to design transmit precoders for integrated sensing and communication (ISAC) systems to maximize the worst-case target illumination power while ensuring a minimum signal-to-interference-plus-noise ratio (SINR) for all the users. The proposed method outperforms traditional optimization-based methods in presence of channel estimation errors while incurring lesser computational complexity and generalizing well across different channel conditions that were not shown during training.

    本文提出了一种无监督学习神经模型，用于设计集成感知和通信（ISAC）系统的传输预编码器，以最大化最坏情况下的目标照明功率，同时确保所有用户的最小信干噪比（SINR）。从上行导频和回波中学习传输预编码器的问题可以看作是一个参数化函数估计问题，我们提出使用神经网络模型来学习这个函数。为了学习神经网络参数，我们开发了一种基于一阶最优性条件的损失函数，以纳入SINR和功率约束。通过数值模拟，我们证明了所提出的方法在存在信道估计误差的情况下优于传统的基于优化的方法，同时产生较小的计算复杂度，并且在不同的信道条件下具有良好的泛化能力，这些条件在训练期间没有显示出来。

    In this paper, we present an unsupervised learning neural model to design transmit precoders for integrated sensing and communication (ISAC) systems to maximize the worst-case target illumination power while ensuring a minimum signal-to-interference-plus-noise ratio (SINR) for all the users. The problem of learning transmit precoders from uplink pilots and echoes can be viewed as a parameterized function estimation problem and we propose to learn this function using a neural network model. To learn the neural network parameters, we develop a novel loss function based on the first-order optimality conditions to incorporate the SINR and power constraints. Through numerical simulations, we demonstrate that the proposed method outperforms traditional optimization-based methods in presence of channel estimation errors while incurring lesser computational complexity and generalizing well across different channel conditions that were not shown during training.
    
[^5]: 面向非静态环境的隐私保护合作可见光定位：联邦学习视角

    Privacy-Preserving Cooperative Visible Light Positioning for Nonstationary Environment: A Federated Learning Perspective. (arXiv:2303.06361v1 [eess.SP])

    [http://arxiv.org/abs/2303.06361](http://arxiv.org/abs/2303.06361)

    本文提出了一种基于联邦学习的合作可见光定位方案，通过共同训练适应环境变化的全局模型，提高了在非静态环境下的定位精度和泛化能力。

    This paper proposes a cooperative visible light positioning scheme based on federated learning, which improves the positioning accuracy and generalization capability in nonstationary environments by jointly training a global model adaptive to environmental changes without sharing private data of users.

    可见光定位（VLP）作为一种有前途的室内定位技术，已经引起了足够的关注。然而，在非静态环境下，由于高度时变的信道，VLP的性能受到限制。为了提高非静态环境下的定位精度和泛化能力，本文提出了一种基于联邦学习（FL）的合作VLP方案。利用FL框架，用户可以共同训练适应环境变化的全局模型，而不共享用户的私有数据。此外，提出了一种合作可见光定位网络（CVPosNet），以加速收敛速度和提高定位精度。仿真结果表明，所提出的方案在非静态环境下优于基准方案。

    Visible light positioning (VLP) has drawn plenty of attention as a promising indoor positioning technique. However, in nonstationary environments, the performance of VLP is limited because of the highly time-varying channels. To improve the positioning accuracy and generalization capability in nonstationary environments, a cooperative VLP scheme based on federated learning (FL) is proposed in this paper. Exploiting the FL framework, a global model adaptive to environmental changes can be jointly trained by users without sharing private data of users. Moreover, a Cooperative Visible-light Positioning Network (CVPosNet) is proposed to accelerate the convergence rate and improve the positioning accuracy. Simulation results show that the proposed scheme outperforms the benchmark schemes, especially in nonstationary environments.
    
[^6]: 利用分区代数快速计算置换等变层

    Fast computation of permutation equivariant layers with the partition algebra. (arXiv:2303.06208v1 [cs.LG])

    [http://arxiv.org/abs/2303.06208](http://arxiv.org/abs/2303.06208)

    本文提出了一种利用分区代数计算置换等变层输出和梯度的新算法，可以在输入大小的线性和二次时间内计算，有效性得到了在几个基准数据集上的证明。

    This paper proposes a new algorithm for computing the output and gradient of permutation equivariant linear layers using the partition algebra, which can be computed in time linear and quadratic in the input size, respectively. The effectiveness of the approach is demonstrated on several benchmark datasets.

    线性神经网络层，无论是等变还是不变于其输入的排列，都是现代深度学习架构的核心构建块。例如DeepSets的层，以及出现在transformers和一些图神经网络的注意力块中的线性层。置换等变线性层的空间可以被识别为某个对称群表示的不变子空间，并且最近的工作通过展示一组基础，其向量是标准基础元素在对称群作用下轨道的总和，来参数化这个空间。参数化打开了通过梯度下降学习置换等变线性层权重的可能性。置换等变线性层的空间是分区代数的一般化，这是一种在统计物理学中首次发现的对象，与对称群的表示论有着深刻的联系，而上述基础与分区代数的基础密切相关。在本文中，我们展示了如何利用这种联系，在输入大小的线性时间内计算置换等变线性层的输出，并在输入大小的二次时间内计算损失相对于权重的梯度。我们的方法基于一种计算分区代数在向量上作用的新算法，我们称之为“分区卷积”。我们展示了分区卷积可以在输入向量大小的线性时间内计算，并且可以用于在输入大小的线性和二次时间内计算置换等变线性层的输出和梯度，分别。我们还展示了如何使用分区卷积来计算某些非线性置换等变层的输出和梯度，并在几个基准数据集上展示了我们方法的有效性。

    Linear neural network layers that are either equivariant or invariant to permutations of their inputs form core building blocks of modern deep learning architectures. Examples include the layers of DeepSets, as well as linear layers occurring in attention blocks of transformers and some graph neural networks. The space of permutation equivariant linear layers can be identified as the invariant subspace of a certain symmetric group representation, and recent work parameterized this space by exhibiting a basis whose vectors are sums over orbits of standard basis elements with respect to the symmetric group action. A parameterization opens up the possibility of learning the weights of permutation equivariant linear layers via gradient descent. The space of permutation equivariant linear layers is a generalization of the partition algebra, an object first discovered in statistical physics with deep connections to the representation theory of the symmetric group, and the basis described abo
    
[^7]: 克服异方差PCA中病态问题的缩减算法

    Deflated HeteroPCA: Overcoming the curse of ill-conditioning in heteroskedastic PCA. (arXiv:2303.06198v1 [math.ST])

    [http://arxiv.org/abs/2303.06198](http://arxiv.org/abs/2303.06198)

    本文提出了一种新的算法，称为缩减异方差PCA，它在克服病态问题的同时实现了近乎最优和无条件数的理论保证。

    This paper proposes a novel algorithm, called Deflated-HeteroPCA, that overcomes the curse of ill-conditioning in heteroskedastic PCA while achieving near-optimal and condition-number-free theoretical guarantees.

    本文关注于从受污染的数据中估计低秩矩阵X*的列子空间。当存在异方差噪声和不平衡的维度（即n2 >> n1）时，如何在容纳最广泛的信噪比范围的同时获得最佳的统计精度变得特别具有挑战性。虽然最先进的算法HeteroPCA成为解决这个问题的强有力的解决方案，但它遭受了“病态问题的诅咒”，即随着X*的条件数增长，其性能会下降。为了克服这个关键问题而不影响允许的信噪比范围，我们提出了一种新的算法，称为缩减异方差PCA，它在$\ell_2$和$\ell_{2,\infty}$统计精度方面实现了近乎最优和无条件数的理论保证。所提出的算法将谱分成两部分

    This paper is concerned with estimating the column subspace of a low-rank matrix $\boldsymbol{X}^\star \in \mathbb{R}^{n_1\times n_2}$ from contaminated data. How to obtain optimal statistical accuracy while accommodating the widest range of signal-to-noise ratios (SNRs) becomes particularly challenging in the presence of heteroskedastic noise and unbalanced dimensionality (i.e., $n_2\gg n_1$). While the state-of-the-art algorithm $\textsf{HeteroPCA}$ emerges as a powerful solution for solving this problem, it suffers from "the curse of ill-conditioning," namely, its performance degrades as the condition number of $\boldsymbol{X}^\star$ grows. In order to overcome this critical issue without compromising the range of allowable SNRs, we propose a novel algorithm, called $\textsf{Deflated-HeteroPCA}$, that achieves near-optimal and condition-number-free theoretical guarantees in terms of both $\ell_2$ and $\ell_{2,\infty}$ statistical accuracy. The proposed algorithm divides the spectrum
    
[^8]: 低违约组合的违约概率的概率论概述

    Probabilistic Overview of Probabilities of Default for Low Default Portfolios by K. Pluto and D. Tasche. (arXiv:2303.06148v1 [q-fin.RM])

    [http://arxiv.org/abs/2303.06148](http://arxiv.org/abs/2303.06148)

    本文概述了K. Pluto和D. Tasche提出的违约概率估计方法，详细列出了假设和推导，为早期职业分析师或学者提供更多的清晰度，特别是关于借款人独立性、条件独立性和概率分布之间的交互作用，如二项式、贝塔、正态分布等的假设。同时，还展示了违约概率与$\sqrt{\varrho}X-\sqrt{1-\varrho}Y$的联合分布之间的关系。

    This article provides a probabilistic overview of the default probability estimation method proposed by K. Pluto and D. Tasche, including detailed assumptions and derivations. It aims to clarify the assumptions of borrower independence, conditional independence, and interaction between probability distributions such as binomial, beta, and normal for early career analysts or scholars. Additionally, it shows the relationship between the probability of default and the joint distribution of $\sqrt{\varrho}X-\sqrt{1-\varrho}Y$, where $X$ is the standard normal and $Y$ is the beta-normal distribution.

    本文对K. Pluto和D. Tasche提出的违约概率估计方法进行了概率论概述。列出了关于系统因素影响下涉及违约概率的不等式的详细假设和推导。作者预计为早期职业分析师或学者提供更多的清晰度，特别是关于借款人独立性、条件独立性和概率分布之间的交互作用，如二项式、贝塔、正态分布等的假设。还展示了违约概率与$\sqrt{\varrho}X-\sqrt{1-\varrho}Y$的联合分布之间的关系，其中$X$包括但不限于标准正态分布，$Y$包括但不限于贝塔-正态分布，$X,\,Y$是独立的。

    This article gives a probabilistic overview of the widely used method of default probability estimation proposed by K. Pluto and D. Tasche. There are listed detailed assumptions and derivation of the inequality where the probability of default is involved under the influence of systematic factor. The author anticipates adding more clarity, especially for early career analysts or scholars, regarding the assumption of borrowers' independence, conditional independence and interaction between the probability distributions such as binomial, beta, normal and others. There is also shown the relation between the probability of default and the joint distribution of $\sqrt{\varrho}X-\sqrt{1-\varrho}Y$, where $X$, including but not limiting, is the standard normal, $Y$ admits, including but not limiting, the beta-normal distribution and $X,\,Y$ are independent.
    
[^9]: 非凸-PL极小极大优化的增强自适应梯度算法

    Enhanced Adaptive Gradient Algorithms for Nonconvex-PL Minimax Optimization. (arXiv:2303.03984v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2303.03984](http://arxiv.org/abs/2303.03984)

    本文提出了一类增强的基于动量的梯度下降上升方法（即MSGDA和AdaMSGDA）来解决非凸-PL极小极大问题，其中AdaMSGDA算法可以使用各种自适应学习率来更新变量$x$和$y$，而不依赖于任何全局和坐标自适应学习率。理论上，我们证明了我们的MSGDA和AdaMSGDA方法在找到$\epsilon$-稳定解时，只需要在每个循环中进行一次采样，就可以获得已知的最佳样本（梯度）复杂度$O(\epsilon^{-3})$。

    This paper proposes a class of enhanced momentum-based gradient descent ascent methods (MSGDA and AdaMSGDA) to solve nonconvex-PL minimax problems, where the AdaMSGDA algorithm can use various adaptive learning rates to update variables x and y without relying on any global and coordinate-wise adaptive learning rates. Theoretical analysis shows that MSGDA and AdaMSGDA methods have the best known sample (gradient) complexity of O(ε−3) in finding an ε-stationary solution.

    本文研究了一类非凸非凹的极小极大优化问题（即$\min_x\max_y f(x,y)$），其中$f(x,y)$在$x$上可能是非凸的，在$y$上是非凹的，并满足Polyak-Lojasiewicz（PL）条件。此外，我们提出了一类增强的基于动量的梯度下降上升方法（即MSGDA和AdaMSGDA）来解决这些随机非凸-PL极小极大问题。特别地，我们的AdaMSGDA算法可以使用各种自适应学习率来更新变量$x$和$y$，而不依赖于任何全局和坐标自适应学习率。理论上，我们提出了一种有效的收敛分析框架来解决我们的方法。具体而言，我们证明了我们的MSGDA和AdaMSGDA方法在找到$\epsilon$-稳定解（即$\mathbb{E}\|\nabla F(x)\|\leq \epsilon$，其中$F(x)=\max_y f(x,y)$）时，只需要在每个循环中进行一次采样，就可以获得已知的最佳样本（梯度）复杂度$O(\epsilon^{-3})$。

    In the paper, we study a class of nonconvex nonconcave minimax optimization problems (i.e., $\min_x\max_y f(x,y)$), where $f(x,y)$ is possible nonconvex in $x$, and it is nonconcave and satisfies the Polyak-Lojasiewicz (PL) condition in $y$. Moreover, we propose a class of enhanced momentum-based gradient descent ascent methods (i.e., MSGDA and AdaMSGDA) to solve these stochastic Nonconvex-PL minimax problems. In particular, our AdaMSGDA algorithm can use various adaptive learning rates in updating the variables $x$ and $y$ without relying on any global and coordinate-wise adaptive learning rates. Theoretically, we present an effective convergence analysis framework for our methods. Specifically, we prove that our MSGDA and AdaMSGDA methods have the best known sample (gradient) complexity of $O(\epsilon^{-3})$ only requiring one sample at each loop in finding an $\epsilon$-stationary solution (i.e., $\mathbb{E}\|\nabla F(x)\|\leq \epsilon$, where $F(x)=\max_y f(x,y)$). This manuscript 
    
[^10]: 基于惩罚的双层梯度下降方法研究

    On Penalty-based Bilevel Gradient Descent Method. (arXiv:2302.05185v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05185](http://arxiv.org/abs/2302.05185)

    本文提出了基于惩罚的双层梯度下降算法，解决了下层非强凸约束双层问题，实验表明该算法有效。

    This paper proposes a penalty-based bilevel gradient descent algorithm to solve the constrained bilevel problem without lower-level strong convexity, and experiments show its efficiency.

    双层优化在超参数优化、元学习和强化学习等领域有广泛应用，但是双层优化问题难以解决。最近的可扩展双层算法主要集中在下层目标函数是强凸或无约束的双层优化问题上。在本文中，我们通过惩罚方法来解决双层问题。我们证明，在一定条件下，惩罚重构可以恢复原始双层问题的解。此外，我们提出了基于惩罚的双层梯度下降（PBGD）算法，并证明了其在下层非强凸约束双层问题上的有限时间收敛性。实验展示了所提出的PBGD算法的效率。

    Bilevel optimization enjoys a wide range of applications in hyper-parameter optimization, meta-learning and reinforcement learning. However, bilevel optimization problems are difficult to solve. Recent progress on scalable bilevel algorithms mainly focuses on bilevel optimization problems where the lower-level objective is either strongly convex or unconstrained. In this work, we tackle the bilevel problem through the lens of the penalty method. We show that under certain conditions, the penalty reformulation recovers the solutions of the original bilevel problem. Further, we propose the penalty-based bilevel gradient descent (PBGD) algorithm and establish its finite-time convergence for the constrained bilevel problem without lower-level strong convexity. Experiments showcase the efficiency of the proposed PBGD algorithm.
    
[^11]: 量子蒙特卡罗算法在金融中高维期权定价中解决Black-Scholes PDE及其克服维度诅咒的证明

    Quantum Monte Carlo algorithm for solving Black-Scholes PDEs for high-dimensional option pricing in finance and its proof of overcoming the curse of dimensionality. (arXiv:2301.09241v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2301.09241](http://arxiv.org/abs/2301.09241)

    本文提出了一种量子蒙特卡罗算法，用于解决高维Black-Scholes PDE的高维期权定价，其复杂度被多项式地限制，克服了维度诅咒。

    This paper proposes a quantum Monte Carlo algorithm for high-dimensional option pricing by solving high-dimensional Black-Scholes PDEs with correlation, and proves that its computational complexity is polynomially bounded in the space dimension and the reciprocal of the prescribed accuracy, overcoming the curse of dimensionality.

    本文提供了一种量子蒙特卡罗算法，用于解决具有相关性的高维Black-Scholes PDE的高维期权定价。期权的支付函数为一般形式，只需要连续且分段仿射（CPWA），涵盖了金融中使用的大多数相关支付函数。我们提供了算法的严格误差分析和复杂度分析。特别地，我们证明了我们的算法的计算复杂度在PDE的空间维度$d$和所需精度$\varepsilon$的倒数中被多项式地限制，从而证明了我们的量子蒙特卡罗算法不会受到维度诅咒的影响。

    In this paper we provide a quantum Monte Carlo algorithm to solve high-dimensional Black-Scholes PDEs with correlation for high-dimensional option pricing. The payoff function of the option is of general form and is only required to be continuous and piece-wise affine (CPWA), which covers most of the relevant payoff functions used in finance. We provide a rigorous error analysis and complexity analysis of our algorithm. In particular, we prove that the computational complexity of our algorithm is bounded polynomially in the space dimension $d$ of the PDE and the reciprocal of the prescribed accuracy $\varepsilon$ and so demonstrate that our quantum Monte Carlo algorithm does not suffer from the curse of dimensionality.
    
[^12]: 通过再生核Banach空间的神经网络对偶性

    Duality for Neural Networks through Reproducing Kernel Banach Spaces. (arXiv:2211.05020v3 [math.FA] UPDATED)

    [http://arxiv.org/abs/2211.05020](http://arxiv.org/abs/2211.05020)

    本文提出了一种新的再生核Banach空间（RKBS）方法，用于解决神经网络中的对偶性问题，构建了神经网络的鞍点问题，可用于原始-对偶优化的整个领域。

    This paper proposes a new method using Reproducing Kernel Banach spaces (RKBS) to solve the duality problem in neural networks, constructing the saddle point problem for neural networks, which can be used in the whole field of primal-dual optimization.

    再生核希尔伯特空间（RKHS）已经成为机器学习各个领域中非常成功的工具。最近，Barron空间已被用于证明神经网络的泛化误差界限。不幸的是，由于权重的强非线性耦合，Barron空间无法用RKHS的术语理解。这可以通过使用更一般的再生核Banach空间（RKBS）来解决。我们展示了这些Barron空间属于一类积分RKBS。这个类也可以理解为RKHS空间的无限并集。此外，我们展示了这种RKBS的对偶空间，再次是一个RKBS，其中数据和参数的角色互换，形成一个包括再生核的伴随RKBS对。这使我们能够构建神经网络的鞍点问题，可用于原始-对偶优化的整个领域。

    Reproducing Kernel Hilbert spaces (RKHS) have been a very successful tool in various areas of machine learning. Recently, Barron spaces have been used to prove bounds on the generalisation error for neural networks. Unfortunately, Barron spaces cannot be understood in terms of RKHS due to the strong nonlinear coupling of the weights. This can be solved by using the more general Reproducing Kernel Banach spaces (RKBS). We show that these Barron spaces belong to a class of integral RKBS. This class can also be understood as an infinite union of RKHS spaces. Furthermore, we show that the dual space of such RKBSs, is again an RKBS where the roles of the data and parameters are interchanged, forming an adjoint pair of RKBSs including a reproducing kernel. This allows us to construct the saddle point problem for neural networks, which can be used in the whole field of primal-dual optimisation.
    
[^13]: Langevin-Based Non-Convex Sampling的动力学系统视角

    A Dynamical System View of Langevin-Based Non-Convex Sampling. (arXiv:2210.13867v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13867](http://arxiv.org/abs/2210.13867)

    本文提出了一种新的框架，通过利用动力系统理论中的几个工具来解决非凸采样中的重要挑战。对于一大类最先进的采样方案，它们在Wasserstein距离下的最后迭代收敛可以归结为对它们的连续时间对应物的研究，这是更好理解的。

    This paper proposes a new framework that uses tools from the theory of dynamical systems to address important challenges in non-convex sampling. For a large class of state-of-the-art sampling schemes, their last-iterate convergence in Wasserstein distances can be reduced to the study of their continuous-time counterparts, which is much better understood.

    非凸采样是机器学习中的一个关键挑战，对于深度学习中的非凸优化以及近似概率推断都至关重要。尽管其重要性，理论上仍存在许多重要挑战：现有的保证通常仅适用于平均迭代而不是更理想的最后迭代，缺乏捕捉变量尺度（如Wasserstein距离）的收敛度量，主要适用于随机梯度Langevin动力学等基本方案。在本文中，我们开发了一个新的框架，通过利用动力系统理论中的几个工具来解决上述问题。我们的关键结果是，对于一大类最先进的采样方案，它们在Wasserstein距离下的最后迭代收敛可以归结为对它们的连续时间对应物的研究，这是更好理解的。结合MCMC采样的标准假设，我们的理论立即产生了

    Non-convex sampling is a key challenge in machine learning, central to non-convex optimization in deep learning as well as to approximate probabilistic inference. Despite its significance, theoretically there remain many important challenges: Existing guarantees (1) typically only hold for the averaged iterates rather than the more desirable last iterates, (2) lack convergence metrics that capture the scales of the variables such as Wasserstein distances, and (3) mainly apply to elementary schemes such as stochastic gradient Langevin dynamics. In this paper, we develop a new framework that lifts the above issues by harnessing several tools from the theory of dynamical systems. Our key result is that, for a large class of state-of-the-art sampling schemes, their last-iterate convergence in Wasserstein distances can be reduced to the study of their continuous-time counterparts, which is much better understood. Coupled with standard assumptions of MCMC sampling, our theory immediately yie
    
[^14]: 带有$O(1)$共识速率的分散式学习的通信高效拓扑结构

    Communication-Efficient Topologies for Decentralized Learning with $O(1)$ Consensus Rate. (arXiv:2210.07881v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2210.07881](http://arxiv.org/abs/2210.07881)

    本文提出了一种新的拓扑结构家族EquiTopo，它具有（几乎）恒定的度数和与网络大小无关的共识速率，用于衡量混合效率。

    This paper proposes a new family of topologies, EquiTopo, which has an (almost) constant degree and a network-size-independent consensus rate that is used to measure the mixing efficiency.

    分散式优化是分布式学习中的新兴范例，其中代理通过点对点通信实现网络范围内的解决方案，而无需中央服务器。由于通信往往比计算慢，因此当每个代理每次迭代仅与少数相邻代理通信时，它们可以比使用更多代理或中央服务器更快地完成迭代。然而，到达网络范围内的解决方案所需的总迭代次数受到代理信息通过通信“混合”的速度的影响。我们发现，流行的通信拓扑结构要么具有较大的最大度数（例如星形和完全图），要么在混合信息方面效果不佳（例如环和网格）。为了解决这个问题，我们提出了一种新的拓扑结构家族EquiTopo，它具有（几乎）恒定的度数和与网络大小无关的共识速率，用于衡量混合效率。在所提出的家族中，EquiStatic的度数为$

    Decentralized optimization is an emerging paradigm in distributed learning in which agents achieve network-wide solutions by peer-to-peer communication without the central server. Since communication tends to be slower than computation, when each agent communicates with only a few neighboring agents per iteration, they can complete iterations faster than with more agents or a central server. However, the total number of iterations to reach a network-wide solution is affected by the speed at which the agents' information is ``mixed'' by communication. We found that popular communication topologies either have large maximum degrees (such as stars and complete graphs) or are ineffective at mixing information (such as rings and grids). To address this problem, we propose a new family of topologies, EquiTopo, which has an (almost) constant degree and a network-size-independent consensus rate that is used to measure the mixing efficiency.  In the proposed family, EquiStatic has a degree of $
    
[^15]: 开放式联邦学习系统的稳定性分析

    On the Stability Analysis of Open Federated Learning Systems. (arXiv:2209.12307v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.12307](http://arxiv.org/abs/2209.12307)

    本文研究了开放式联邦学习系统的稳定性问题，提出了一种新的性能度量，即开放式FL系统的稳定性，并在假设本地客户端函数是强凸和平滑的情况下，理论上量化了两种FL算法的稳定半径。

    This paper studies the stability issue of open federated learning systems, proposes a new performance metric, namely the stability of open FL systems, and theoretically quantifies the stability radius of two FL algorithms under the assumption that local clients' functions are strongly convex and smooth.

    我们考虑开放式联邦学习系统，其中客户端可能在联邦学习过程中加入和/或离开系统。由于存在客户端数量的变化，无法保证在开放系统中收敛到固定模型。因此，我们采用一种新的性能度量，称为开放式FL系统的稳定性，它量化了在开放系统中学习模型的大小。在假设本地客户端函数是强凸和平滑的情况下，我们理论上量化了两种FL算法（即本地SGD和本地Adam）的稳定半径。我们观察到，这个半径依赖于几个关键参数，包括函数条件数以及随机梯度的方差。我们的理论结果在合成和真实世界基准数据集上通过数值模拟进一步验证。

    We consider the open federated learning (FL) systems, where clients may join and/or leave the system during the FL process. Given the variability of the number of present clients, convergence to a fixed model cannot be guaranteed in open systems. Instead, we resort to a new performance metric that we term the stability of open FL systems, which quantifies the magnitude of the learned model in open systems. Under the assumption that local clients' functions are strongly convex and smooth, we theoretically quantify the radius of stability for two FL algorithms, namely local SGD and local Adam. We observe that this radius relies on several key parameters, including the function condition number as well as the variance of the stochastic gradient. Our theoretical results are further verified by numerical simulations on both synthetic and real-world benchmark data-sets.
    
[^16]: 具备辅助信息的优化

    Optimization with access to auxiliary information. (arXiv:2206.00395v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00395](http://arxiv.org/abs/2206.00395)

    本文研究了在计算目标函数梯度很昂贵或有限的情况下，给定一些辅助函数的情况下，如何最小化目标函数。作者提出了两种通用的新算法，并证明了这个框架可以受益于目标和辅助信息之间的Hessian相似性假设。

    This paper investigates the fundamental optimization question of minimizing a target function with expensive or limited gradient computation, given access to some auxiliary side function with cheaper or more available gradients. The authors propose two generic new algorithms and prove that this framework can benefit from the Hessian similarity assumption between the target and side information.

    我们研究了基本的优化问题，即在计算目标函数$f(x)$的梯度很昂贵或有限的情况下，给定一些辅助函数$h(x)$的情况下，如何最小化目标函数。这个公式涵盖了许多实际相关的设置，如i）在SGD中重复使用批次，ii）迁移学习，iii）联邦学习，iv）使用压缩模型/丢弃等进行训练。我们提出了两种通用的新算法，适用于所有这些设置，并证明仅使用目标和辅助信息之间的Hessian相似性假设，我们可以从这个框架中受益。

    We investigate the fundamental optimization question of minimizing a target function $f(x)$ whose gradients are expensive to compute or have limited availability, given access to some auxiliary side function $h(x)$ whose gradients are cheap or more available. This formulation captures many settings of practical relevance such as i) re-using batches in SGD, ii) transfer learning, iii) federated learning, iv) training with compressed models/dropout, etc. We propose two generic new algorithms which are applicable in all these settings and prove using only an assumption on the Hessian similarity between the target and side information that we can benefit from this framework.
    
[^17]: 误差曝光映射下的因果推断：区分定义和假设

    Causal inference with misspecified exposure mappings: separating definitions and assumptions. (arXiv:2103.06471v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2103.06471](http://arxiv.org/abs/2103.06471)

    本文提出了一种新的方法，将曝光映射的两个作用分开，从而在曝光被错误指定时精确估计曝光效应，避免了常常是可疑的假设。

    This paper proposes a new method to separate the two roles of exposure mappings, which allows for precise estimation of exposure effects even when the exposures are misspecified, avoiding often questionable assumptions.

    当实验单位相互作用时，曝光映射有助于研究复杂的因果效应。目前的方法要求实验者在定义感兴趣的效应和对干扰结构施加假设时使用相同的曝光映射。然而，在实践中，这两个角色很少重合，实验者被迫做出常常是可疑的假设，即他们的曝光映射是正确的。本文认为，曝光映射目前所起的两个作用可以，而且通常应该分开，这样曝光就可以用来定义效应，而不必假设它们捕捉了实验中的完整因果结构。本文通过提供曝光效应可以在曝光被错误指定时精确估计的条件，证明了这种方法在实践中是可行的。一些重要的问题仍然没有解决。

    Exposure mappings facilitate investigations of complex causal effects when units interact in experiments. Current methods require experimenters to use the same exposure mappings both to define the effect of interest and to impose assumptions on the interference structure. However, the two roles rarely coincide in practice, and experimenters are forced to make the often questionable assumption that their exposures are correctly specified. This paper argues that the two roles exposure mappings currently serve can, and typically should, be separated, so that exposures are used to define effects without necessarily assuming that they are capturing the complete causal structure in the experiment. The paper shows that this approach is practically viable by providing conditions under which exposure effects can be precisely estimated when the exposures are misspecified. Some important questions remain open.
    

