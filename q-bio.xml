<rss version="2.0"><channel><title>Chat Arxiv q-bio</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for q-bio</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#32512;&#26641;&#30340;&#20013;&#38388;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#36136;&#35889;&#35270;&#20026;&#21270;&#23398;&#20844;&#24335;&#30340;&#38598;&#21512;&#26469;&#39044;&#27979;&#20998;&#23376;&#30340;&#36136;&#35889;&#65292;&#20811;&#26381;&#20102;&#21270;&#23398;&#23376;&#20844;&#24335;&#30340;&#32452;&#21512;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06470</link><description>&lt;p&gt;
&#22522;&#20110;&#21069;&#32512;&#26641;&#30340;&#20998;&#23376;&#36136;&#35889;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Prefix-tree Decoding for Predicting Mass Spectra from Molecules. (arXiv:2303.06470v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#32512;&#26641;&#30340;&#20013;&#38388;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#36136;&#35889;&#35270;&#20026;&#21270;&#23398;&#20844;&#24335;&#30340;&#38598;&#21512;&#26469;&#39044;&#27979;&#20998;&#23376;&#30340;&#36136;&#35889;&#65292;&#20811;&#26381;&#20102;&#21270;&#23398;&#23376;&#20844;&#24335;&#30340;&#32452;&#21512;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an intermediate strategy for predicting mass spectra from molecules by treating mass spectra as sets of chemical formulae, which are themselves multisets of atoms, and decoding the formula set using a prefix tree structure, atom-type by atom-type, overcoming the combinatorial possibilities for chemical subformulae.
&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#39044;&#27979;&#20998;&#23376;&#30340;&#36136;&#35889;&#24050;&#32463;&#23454;&#29616;&#20102;&#20020;&#24202;&#30456;&#20851;&#20195;&#35874;&#29289;&#30340;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#39044;&#27979;&#24037;&#20855;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21344;&#25454;&#20102;&#20004;&#20010;&#26497;&#31471;&#65292;&#35201;&#20040;&#36890;&#36807;&#36807;&#24230;&#21018;&#24615;&#30340;&#32422;&#26463;&#21644;&#36739;&#24046;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#32452;&#21512;&#20998;&#23376;&#26469;&#36827;&#34892;&#25805;&#20316;&#65292;&#35201;&#20040;&#36890;&#36807;&#35299;&#30721;&#26377;&#25439;&#21644;&#38750;&#29289;&#29702;&#31163;&#25955;&#21270;&#30340;&#20809;&#35889;&#21521;&#37327;&#26469;&#36827;&#34892;&#25805;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20013;&#38388;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#36136;&#35889;&#35270;&#20026;&#21270;&#23398;&#20844;&#24335;&#30340;&#38598;&#21512;&#26469;&#39044;&#27979;&#20998;&#23376;&#30340;&#36136;&#35889;&#65292;&#36825;&#20123;&#21270;&#23398;&#20844;&#24335;&#26412;&#36523;&#26159;&#21407;&#23376;&#30340;&#22810;&#37325;&#38598;&#21512;&#12290;&#22312;&#39318;&#20808;&#23545;&#36755;&#20837;&#20998;&#23376;&#22270;&#36827;&#34892;&#32534;&#30721;&#21518;&#65292;&#25105;&#20204;&#35299;&#30721;&#19968;&#32452;&#21270;&#23398;&#23376;&#20844;&#24335;&#65292;&#27599;&#20010;&#21270;&#23398;&#23376;&#20844;&#24335;&#25351;&#23450;&#36136;&#35889;&#20013;&#30340;&#19968;&#20010;&#39044;&#27979;&#23792;&#65292;&#20854;&#24378;&#24230;&#30001;&#31532;&#20108;&#20010;&#27169;&#22411;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#36890;&#36807;&#20351;&#29992;&#21069;&#32512;&#26641;&#32467;&#26500;&#65292;&#36880;&#20010;&#21407;&#23376;&#31867;&#22411;&#22320;&#35299;&#30721;&#20844;&#24335;&#38598;&#65292;&#20811;&#26381;&#20102;&#21270;&#23398;&#23376;&#20844;&#24335;&#30340;&#32452;&#21512;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational predictions of mass spectra from molecules have enabled the discovery of clinically relevant metabolites. However, such predictive tools are still limited as they occupy one of two extremes, either operating (a) by fragmenting molecules combinatorially with overly rigid constraints on potential rearrangements and poor time complexity or (b) by decoding lossy and nonphysical discretized spectra vectors. In this work, we introduce a new intermediate strategy for predicting mass spectra from molecules by treating mass spectra as sets of chemical formulae, which are themselves multisets of atoms. After first encoding an input molecular graph, we decode a set of chemical subformulae, each of which specify a predicted peak in the mass spectra, the intensities of which are predicted by a second model. Our key insight is to overcome the combinatorial possibilities for chemical subformulae by decoding the formula set using a prefix tree structure, atom-type by atom-type, represent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21487;&#38752;&#21644;&#21487;&#25193;&#23637;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65288;iMIIC&#65289;&#65292;&#24182;&#22312;&#26469;&#33258;&#32654;&#22269;&#30417;&#27979;&#12289;&#27969;&#34892;&#30149;&#23398;&#21644;&#32456;&#26411;&#32467;&#26524;&#35745;&#21010;&#30340;396,179&#21517;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#20854;&#29420;&#29305;&#33021;&#21147;&#12290;&#36229;&#36807;90&#65285;&#30340;&#39044;&#27979;&#22240;&#26524;&#25928;&#24212;&#26159;&#27491;&#30830;&#30340;&#65292;&#32780;&#20854;&#20313;&#30340;&#24847;&#22806;&#30452;&#25509;&#21644;&#38388;&#25509;&#22240;&#26524;&#25928;&#24212;&#21487;&#20197;&#35299;&#37322;&#20026;&#35786;&#26029;&#31243;&#24207;&#12289;&#27835;&#30103;&#26102;&#38388;&#12289;&#24739;&#32773;&#20559;&#22909;&#25110;&#31038;&#20250;&#32463;&#27982;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2303.06423</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#32593;&#32476;&#65292;&#20197;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;40&#19975;&#20221;&#21307;&#30103;&#35760;&#24405;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Learning interpretable causal networks from very large datasets, application to 400,000 medical records of breast cancer patients. (arXiv:2303.06423v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21487;&#38752;&#21644;&#21487;&#25193;&#23637;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65288;iMIIC&#65289;&#65292;&#24182;&#22312;&#26469;&#33258;&#32654;&#22269;&#30417;&#27979;&#12289;&#27969;&#34892;&#30149;&#23398;&#21644;&#32456;&#26411;&#32467;&#26524;&#35745;&#21010;&#30340;396,179&#21517;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#20854;&#29420;&#29305;&#33021;&#21147;&#12290;&#36229;&#36807;90&#65285;&#30340;&#39044;&#27979;&#22240;&#26524;&#25928;&#24212;&#26159;&#27491;&#30830;&#30340;&#65292;&#32780;&#20854;&#20313;&#30340;&#24847;&#22806;&#30452;&#25509;&#21644;&#38388;&#25509;&#22240;&#26524;&#25928;&#24212;&#21487;&#20197;&#35299;&#37322;&#20026;&#35786;&#26029;&#31243;&#24207;&#12289;&#27835;&#30103;&#26102;&#38388;&#12289;&#24739;&#32773;&#20559;&#22909;&#25110;&#31038;&#20250;&#32463;&#27982;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a more reliable and scalable causal discovery method (iMIIC) and showcases its unique capabilities on healthcare data from 396,179 breast cancer patients from the US Surveillance, Epidemiology, and End Results program. Over 90% of predicted causal effects appear correct, while the remaining unexpected direct and indirect causal effects can be interpreted in terms of diagnostic procedures, therapeutic timing, patient preference or socio-economic disparity.
&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#22240;&#26524;&#25928;&#24212;&#26159;&#31185;&#23398;&#30740;&#31350;&#30340;&#26680;&#24515;&#65292;&#20294;&#24403;&#21482;&#26377;&#35266;&#23519;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#36825;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#22240;&#26524;&#32593;&#32476;&#38590;&#20197;&#23398;&#20064;&#21644;&#35299;&#37322;&#65292;&#24182;&#19988;&#20165;&#38480;&#20110;&#30456;&#23545;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#31181;&#26356;&#21487;&#38752;&#21644;&#21487;&#25193;&#23637;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65288;iMIIC&#65289;&#65292;&#22522;&#20110;&#19968;&#33324;&#30340;&#20114;&#20449;&#24687;&#26368;&#22823;&#21407;&#21017;&#65292;&#23427;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#25512;&#26029;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#31934;&#24230;&#65292;&#21516;&#26102;&#21306;&#20998;&#20102;&#30495;&#27491;&#30340;&#21407;&#22240;&#21644;&#20551;&#23450;&#30340;&#21644;&#28508;&#22312;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;iMIIC&#22312;&#26469;&#33258;&#32654;&#22269;&#30417;&#27979;&#12289;&#27969;&#34892;&#30149;&#23398;&#21644;&#32456;&#26411;&#32467;&#26524;&#35745;&#21010;&#30340;396,179&#21517;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#21512;&#25104;&#21644;&#29616;&#23454;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#19978;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;&#36229;&#36807;90&#65285;&#30340;&#39044;&#27979;&#22240;&#26524;&#25928;&#24212;&#26159;&#27491;&#30830;&#30340;&#65292;&#32780;&#20854;&#20313;&#30340;&#24847;&#22806;&#30452;&#25509;&#21644;&#38388;&#25509;&#22240;&#26524;&#25928;&#24212;&#21487;&#20197;&#35299;&#37322;&#20026;&#35786;&#26029;&#31243;&#24207;&#12289;&#27835;&#30103;&#26102;&#38388;&#12289;&#24739;&#32773;&#20559;&#22909;&#25110;&#31038;&#20250;&#32463;&#27982;&#24046;&#36317;&#12290;iMIIC&#30340;&#29420;&#29305;&#33021;&#21147;&#24320;&#36767;&#20102;&#21457;&#29616;&#21487;&#38752;&#21644;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#32593;&#32476;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering causal effects is at the core of scientific investigation but remains challenging when only observational data is available. In practice, causal networks are difficult to learn and interpret, and limited to relatively small datasets. We report a more reliable and scalable causal discovery method (iMIIC), based on a general mutual information supremum principle, which greatly improves the precision of inferred causal relations while distinguishing genuine causes from putative and latent causal effects. We showcase iMIIC on synthetic and real-life healthcare data from 396,179 breast cancer patients from the US Surveillance, Epidemiology, and End Results program. More than 90\% of predicted causal effects appear correct, while the remaining unexpected direct and indirect causal effects can be interpreted in terms of diagnostic procedures, therapeutic timing, patient preference or socio-economic disparity. iMIIC's unique capabilities open up new avenues to discover reliable and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#31579;&#26597;&#21628;&#20986;&#27668;&#20013;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#65288;VOC&#65289;&#30340;Raman&#20809;&#35889;&#25968;&#25454;&#65292;&#21487;&#21487;&#38752;&#22320;&#39044;&#27979;&#32954;&#30284;&#24739;&#32773;&#21450;&#20854;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2303.06340</link><description>&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;&#30340;Raman&#20809;&#35889;&#25968;&#25454;&#32954;&#30284;&#26234;&#33021;&#35786;&#26029;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Intelligent diagnostic scheme for lung cancer screening with Raman spectra data by tensor network machine learning. (arXiv:2303.06340v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#31579;&#26597;&#21628;&#20986;&#27668;&#20013;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#65288;VOC&#65289;&#30340;Raman&#20809;&#35889;&#25968;&#25454;&#65292;&#21487;&#21487;&#38752;&#22320;&#39044;&#27979;&#32954;&#30284;&#24739;&#32773;&#21450;&#20854;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a tensor-network machine learning method to reliably predict lung cancer patients and their stages via screening Raman spectra data of Volatile organic compounds (VOCs) in exhaled breath.
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24050;&#32463;&#22312;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#20013;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#24433;&#21709;&#65292;&#20174;&#23398;&#26415;&#30740;&#31350;&#21040;&#20020;&#24202;&#24212;&#29992;&#65292;&#20363;&#22914;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#26816;&#27979;&#21644;&#35786;&#26029;&#12289;&#27835;&#30103;&#20248;&#21270;&#20197;&#21450;&#33647;&#29289;&#21457;&#29616;&#20013;&#26032;&#30340;&#27835;&#30103;&#38774;&#28857;&#30340;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;AI&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#20005;&#37325;&#21463;&#21040;&#38750;&#21487;&#35299;&#37322;&#24615;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#20250;&#19981;&#21487;&#25511;&#22320;&#23548;&#33268;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#23545;&#20110;ML&#30340;&#21487;&#35299;&#37322;&#24615;&#23588;&#20854;&#37325;&#35201;&#65292;&#22240;&#20026;&#28040;&#36153;&#32773;&#24517;&#39035;&#20174;&#22362;&#23454;&#30340;&#22522;&#30784;&#25110;&#20196;&#20154;&#20449;&#26381;&#30340;&#35299;&#37322;&#20013;&#33719;&#24471;&#24517;&#35201;&#30340;&#23433;&#20840;&#24863;&#21644;&#20449;&#20219;&#24863;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#65288;TN&#65289;-ML&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#31579;&#26597;&#21628;&#20986;&#27668;&#20013;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#65288;VOC&#65289;&#30340;Raman&#20809;&#35889;&#25968;&#25454;&#65292;&#21487;&#21487;&#38752;&#22320;&#39044;&#27979;&#32954;&#30284;&#24739;&#32773;&#21450;&#20854;&#38454;&#27573;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#36866;&#29992;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#24182;&#34987;&#35748;&#20026;&#26159;&#38750;&#20405;&#20837;&#24615;&#32954;&#30284;&#31579;&#26597;&#30340;&#29702;&#24819;&#26041;&#24335;&#12290;TN-ML&#30340;&#39044;&#27979;&#22522;&#20110;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has brought tremendous impacts on biomedical sciences from academic researches to clinical applications, such as in biomarkers' detection and diagnosis, optimization of treatment, and identification of new therapeutic targets in drug discovery. However, the contemporary AI technologies, particularly deep machine learning (ML), severely suffer from non-interpretability, which might uncontrollably lead to incorrect predictions. Interpretability is particularly crucial to ML for clinical diagnosis as the consumers must gain necessary sense of security and trust from firm grounds or convincing interpretations. In this work, we propose a tensor-network (TN)-ML method to reliably predict lung cancer patients and their stages via screening Raman spectra data of Volatile organic compounds (VOCs) in exhaled breath, which are generally suitable as biomarkers and are considered to be an ideal way for non-invasive lung cancer screening. The prediction of TN-ML is based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#32467;&#26500;&#30340;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#26126;&#30830;&#22320;&#32534;&#30721;&#34507;&#30333;&#36136;&#32467;&#26500;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26500;&#24863;&#30693;&#34507;&#30333;&#36136;&#34920;&#31034;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06275</link><description>&lt;p&gt;
&#32467;&#21512;&#22522;&#20110;&#32467;&#26500;&#30340;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Enhancing Protein Language Models with Structure-based Encoder and Pre-training. (arXiv:2303.06275v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#32467;&#26500;&#30340;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#26126;&#30830;&#22320;&#32534;&#30721;&#34507;&#30333;&#36136;&#32467;&#26500;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26500;&#24863;&#30693;&#34507;&#30333;&#36136;&#34920;&#31034;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes enhancing protein language models with structure-based encoder and pre-training to explicitly encode protein structures for better structure-aware protein representations, and empirically verifies its effectiveness.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#34507;&#30333;&#36136;&#24207;&#21015;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#34507;&#30333;&#36136;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#23613;&#31649;&#33021;&#22815;&#38544;&#24335;&#22320;&#25429;&#33719;&#27531;&#22522;&#38388;&#30340;&#25509;&#35302;&#20449;&#24687;&#65292;&#20294;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;PLMs&#19981;&#33021;&#26126;&#30830;&#22320;&#32534;&#30721;&#34507;&#30333;&#36136;&#32467;&#26500;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26500;&#24863;&#30693;&#34507;&#30333;&#36136;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#32467;&#26500;&#23545;&#20110;&#30830;&#23450;&#21151;&#33021;&#24456;&#37325;&#35201;&#65292;&#20294;&#23578;&#26410;&#25506;&#32034;&#22312;&#21487;&#29992;&#34507;&#30333;&#36136;&#32467;&#26500;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#20197;&#25913;&#36827;&#36825;&#20123;PLMs&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#20351;&#29992;&#22522;&#20110;&#32467;&#26500;&#30340;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#26469;&#22686;&#24378;PLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein language models (PLMs) pre-trained on large-scale protein sequence corpora have achieved impressive performance on various downstream protein understanding tasks. Despite the ability to implicitly capture inter-residue contact information, transformer-based PLMs cannot encode protein structures explicitly for better structure-aware protein representations. Besides, the power of pre-training on available protein structures has not been explored for improving these PLMs, though structures are important to determine functions. To tackle these limitations, in this work, we enhance the PLMs with structure-based encoder and pre-training. We first explore feasible model architectures to combine the advantages of a state-of-the-art PLM (i.e., ESM-1b1) and a state-of-the-art protein structure encoder (i.e., GearNet). We empirically verify the ESM-GearNet that connects two encoders in a series way as the most effective combination model. To further improve the effectiveness of ESM-GearNe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;k-mer&#20998;&#24067;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#33410;&#32422;&#36164;&#28304;&#24182;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.06154</link><description>&lt;p&gt;
&#22522;&#20110;k-mer&#20998;&#24067;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#36164;&#28304;&#33410;&#32422;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
Resource saving taxonomy classification with k-mer distributions and machine learning. (arXiv:2303.06154v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;k-mer&#20998;&#24067;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#33410;&#32422;&#36164;&#28304;&#24182;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a resource-saving classification method based on k-mer distributions and machine learning, which can improve the performance of classifiers and reduce the consumption of energy.
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#39640;&#36890;&#37327;&#27979;&#24207;&#25216;&#26415;&#65288;&#22914;&#23439;&#22522;&#22240;&#32452;&#27979;&#24207;&#65289;&#29983;&#25104;&#25968;&#30334;&#19975;&#20010;&#24207;&#21015;&#65292;&#38656;&#35201;&#26681;&#25454;&#23427;&#20204;&#30340;&#20998;&#31867;&#32423;&#21035;&#36827;&#34892;&#20998;&#31867;&#12290;&#29616;&#20195;&#26041;&#27861;&#35201;&#20040;&#24212;&#29992;&#26412;&#22320;&#27604;&#23545;&#21644;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#65288;&#22914;MMseqs2&#65289;&#30340;&#27604;&#36739;&#65292;&#35201;&#20040;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;&#22914;DeepMicrobes&#21644;BERTax&#65289;&#12290;&#22522;&#20110;&#27604;&#23545;&#30340;&#26041;&#27861;&#22312;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#25104;&#26412;&#39640;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#25968;&#25454;&#24211;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#12290;&#23545;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#30828;&#20214;&#36827;&#34892;&#35745;&#31639;&#65292;&#36825;&#28040;&#32791;&#22823;&#37327;&#33021;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20174;DNA&#20013;&#33719;&#24471;&#30340;k-mer&#20998;&#24067;&#20316;&#20026;&#29305;&#24449;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;&#23376;&#31354;&#38388;k&#26368;&#36817;&#37051;&#31639;&#27861;&#12289;&#31070;&#32463;&#32593;&#32476;&#25110;&#34955;&#35013;&#20915;&#31574;&#26641;&#65289;&#26469;&#20998;&#31867;&#20854;&#20998;&#31867;&#36215;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#31354;&#38388;&#25968;&#25454;&#38598;&#24179;&#34913;&#26041;&#27861;&#65292;&#20801;&#35768;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#38598;&#24182;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#27604;&#36739;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern high throughput sequencing technologies like metagenomic sequencing generate millions of sequences which have to be classified based on their taxonomic rank. Modern approaches either apply local alignment and comparison to existing data sets like MMseqs2 or use deep neural networks as it is done in DeepMicrobes and BERTax. Alignment-based approaches are costly in terms of runtime, especially since databases get larger and larger. For the deep learning-based approaches, specialized hardware is necessary for a computation, which consumes large amounts of energy. In this paper, we propose to use $k$-mer distributions obtained from DNA as features to classify its taxonomic origin using machine learning approaches like the subspace $k$-nearest neighbors algorithm, neural networks or bagged decision trees. In addition, we propose a feature space data set balancing approach, which allows reducing the data set for training and improves the performance of the classifiers. By comparing pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;TopExpert&#65292;&#21033;&#29992;&#25299;&#25169;&#29305;&#23450;&#30340;&#39044;&#27979;&#27169;&#22411;&#65288;&#31216;&#20026;&#19987;&#23478;&#65289;&#65292;&#27599;&#20010;&#19987;&#23478;&#36127;&#36131;&#27599;&#20010;&#20849;&#20139;&#31867;&#20284;&#25299;&#25169;&#35821;&#20041;&#30340;&#20998;&#23376;&#32452;&#65292;&#20197;&#25552;&#39640;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.13693</link><description>&lt;p&gt;
&#23398;&#20064;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#25299;&#25169;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Learning Topology-Specific Experts for Molecular Property Prediction. (arXiv:2302.13693v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TopExpert&#65292;&#21033;&#29992;&#25299;&#25169;&#29305;&#23450;&#30340;&#39044;&#27979;&#27169;&#22411;&#65288;&#31216;&#20026;&#19987;&#23478;&#65289;&#65292;&#27599;&#20010;&#19987;&#23478;&#36127;&#36131;&#27599;&#20010;&#20849;&#20139;&#31867;&#20284;&#25299;&#25169;&#35821;&#20041;&#30340;&#20998;&#23376;&#32452;&#65292;&#20197;&#25552;&#39640;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes TopExpert, which leverages topology-specific prediction models (referred to as experts) to improve the performance of molecular property prediction by assigning each expert to a molecular group sharing similar topological semantics.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#39044;&#27979;&#20998;&#23376;&#23646;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#30340;&#26368;&#32463;&#20856;&#30340;&#21270;&#23398;&#20449;&#24687;&#23398;&#20219;&#21153;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#26377;&#25928;&#65292;&#20294;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35266;&#23519;&#21040;&#65292;&#20026;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#30340;&#22810;&#31181;&#20998;&#23376;&#35757;&#32451;&#21333;&#20010;GNN&#27169;&#22411;&#20250;&#38480;&#21046;&#20854;&#39044;&#27979;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TopExpert&#65292;&#21033;&#29992;&#25299;&#25169;&#29305;&#23450;&#30340;&#39044;&#27979;&#27169;&#22411;&#65288;&#31216;&#20026;&#19987;&#23478;&#65289;&#65292;&#27599;&#20010;&#19987;&#23478;&#36127;&#36131;&#27599;&#20010;&#20849;&#20139;&#31867;&#20284;&#25299;&#25169;&#35821;&#20041;&#30340;&#20998;&#23376;&#32452;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#27599;&#20010;&#19987;&#23478;&#22312;&#19982;&#20854;&#30456;&#24212;&#30340;&#25299;&#25169;&#32452;&#19968;&#36215;&#35757;&#32451;&#26102;&#23398;&#20064;&#29305;&#23450;&#20110;&#25299;&#25169;&#30340;&#21028;&#21035;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#25353;&#20854;&#25299;&#25169;&#27169;&#24335;&#23545;&#20998;&#23376;&#36827;&#34892;&#20998;&#32452;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#32858;&#31867;&#30340;&#38376;&#25511;&#27169;&#22359;&#65292;&#23558;&#36755;&#20837;&#20998;&#23376;&#20998;&#37197;&#21040;&#20854;&#20013;&#19968;&#20010;&#32858;&#31867;&#20013;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#33258;&#30417;&#30563;&#36827;&#19968;&#27493;&#20248;&#21270;&#38376;&#25511;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, graph neural networks (GNNs) have been successfully applied to predicting molecular properties, which is one of the most classical cheminformatics tasks with various applications. Despite their effectiveness, we empirically observe that training a single GNN model for diverse molecules with distinct structural patterns limits its prediction performance. In this paper, motivated by this observation, we propose TopExpert to leverage topology-specific prediction models (referred to as experts), each of which is responsible for each molecular group sharing similar topological semantics. That is, each expert learns topology-specific discriminative features while being trained with its corresponding topological group. To tackle the key challenge of grouping molecules by their topological patterns, we introduce a clustering-based gating module that assigns an input molecule into one of the clusters and further optimizes the gating module with two different types of self-supervision:
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#21160;&#21147;&#31995;&#32479;&#30340;&#35270;&#35282;&#65292;&#26469;&#35299;&#37322;&#24847;&#35782;&#30340;&#20016;&#23500;&#24615;&#21644;&#38590;&#20197;&#35328;&#35828;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#24847;&#35782;&#20307;&#39564;&#30340;&#20016;&#23500;&#24615;&#23545;&#24212;&#20110;&#24847;&#35782;&#29366;&#24577;&#20013;&#30340;&#20449;&#24687;&#37327;&#65292;&#32780;&#38590;&#20197;&#35328;&#35828;&#24615;&#21017;&#23545;&#24212;&#20110;&#19981;&#21516;&#22788;&#29702;&#38454;&#27573;&#20002;&#22833;&#30340;&#20449;&#24687;&#37327;&#12290;</title><link>http://arxiv.org/abs/2302.06403</link><description>&lt;p&gt;
&#29616;&#35937;&#24847;&#35782;&#29366;&#24577;&#30340;&#20016;&#23500;&#24615;&#21644;&#38590;&#20197;&#35328;&#35828;&#24615;&#30340;&#26469;&#28304;
&lt;/p&gt;
&lt;p&gt;
Sources of Richness and Ineffability for Phenomenally Conscious States. (arXiv:2302.06403v3 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#21160;&#21147;&#31995;&#32479;&#30340;&#35270;&#35282;&#65292;&#26469;&#35299;&#37322;&#24847;&#35782;&#30340;&#20016;&#23500;&#24615;&#21644;&#38590;&#20197;&#35328;&#35828;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#24847;&#35782;&#20307;&#39564;&#30340;&#20016;&#23500;&#24615;&#23545;&#24212;&#20110;&#24847;&#35782;&#29366;&#24577;&#20013;&#30340;&#20449;&#24687;&#37327;&#65292;&#32780;&#38590;&#20197;&#35328;&#35828;&#24615;&#21017;&#23545;&#24212;&#20110;&#19981;&#21516;&#22788;&#29702;&#38454;&#27573;&#20002;&#22833;&#30340;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides an information theoretic dynamical systems perspective on the richness and ineffability of consciousness. In their framework, the richness of conscious experience corresponds to the amount of information in a conscious state and ineffability corresponds to the amount of information lost at different stages of processing.
&lt;/p&gt;
&lt;p&gt;
&#24847;&#35782;&#29366;&#24577;&#65288;&#21363;&#26377;&#26576;&#31181;&#24863;&#21463;&#30340;&#29366;&#24577;&#65289;&#20284;&#20046;&#26082;&#20016;&#23500;&#21448;&#20805;&#28385;&#32454;&#33410;&#65292;&#21448;&#38590;&#20197;&#23436;&#20840;&#25551;&#36848;&#25110;&#22238;&#24518;&#12290;&#29305;&#21035;&#26159;&#38590;&#20197;&#35328;&#35828;&#24615;&#30340;&#38382;&#39064;&#26159;&#21746;&#23398;&#19978;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#37096;&#20998;&#28608;&#21457;&#20102;&#35299;&#37322;&#40511;&#27807;&#30340;&#20449;&#24565;&#65306;&#24847;&#35782;&#19981;&#33021;&#24402;&#32467;&#20026;&#22522;&#30784;&#29289;&#29702;&#36807;&#31243;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#21160;&#21147;&#31995;&#32479;&#30340;&#35270;&#35282;&#65292;&#26469;&#35299;&#37322;&#24847;&#35782;&#30340;&#20016;&#23500;&#24615;&#21644;&#38590;&#20197;&#35328;&#35828;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#24847;&#35782;&#20307;&#39564;&#30340;&#20016;&#23500;&#24615;&#23545;&#24212;&#20110;&#24847;&#35782;&#29366;&#24577;&#20013;&#30340;&#20449;&#24687;&#37327;&#65292;&#32780;&#38590;&#20197;&#35328;&#35828;&#24615;&#21017;&#23545;&#24212;&#20110;&#19981;&#21516;&#22788;&#29702;&#38454;&#27573;&#20002;&#22833;&#30340;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#24037;&#20316;&#35760;&#24518;&#20013;&#30340;&#21560;&#24341;&#23376;&#21160;&#21147;&#23398;&#22914;&#20309;&#23548;&#33268;&#25105;&#20204;&#21407;&#22987;&#20307;&#39564;&#30340;&#36139;&#20047;&#22238;&#24518;&#65292;&#35821;&#35328;&#30340;&#31163;&#25955;&#31526;&#21495;&#24615;&#36136;&#19981;&#36275;&#20197;&#25551;&#36848;&#20307;&#39564;&#30340;&#20016;&#23500;&#21644;&#39640;&#32500;&#32467;&#26500;&#65292;&#20197;&#21450;&#35748;&#30693;&#21151;&#33021;&#30456;&#20284;&#24615;&#22914;&#20309;&#24433;&#21709;&#20307;&#39564;&#30340;&#20849;&#20139;&#21644;&#20132;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conscious states (states that there is something it is like to be in) seem both rich or full of detail, and ineffable or hard to fully describe or recall. The problem of ineffability, in particular, is a longstanding issue in philosophy that partly motivates the explanatory gap: the belief that consciousness cannot be reduced to underlying physical processes. Here, we provide an information theoretic dynamical systems perspective on the richness and ineffability of consciousness. In our framework, the richness of conscious experience corresponds to the amount of information in a conscious state and ineffability corresponds to the amount of information lost at different stages of processing. We describe how attractor dynamics in working memory would induce impoverished recollections of our original experiences, how the discrete symbolic nature of language is insufficient for describing the rich and high-dimensional structure of experiences, and how similarity in the cognitive function o
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#31070;&#32463;&#20803;&#22810;&#26679;&#24615;&#21487;&#20197;&#35299;&#20915;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#36208;&#21521;&#31070;&#32463;&#20154;&#24037;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.09245</link><description>&lt;p&gt;
&#36208;&#21521;&#31070;&#32463;&#20154;&#24037;&#26234;&#33021;&#65306;&#23558;&#31070;&#32463;&#20803;&#22810;&#26679;&#24615;&#24341;&#20837;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Towards NeuroAI: Introducing Neuronal Diversity into Artificial Neural Networks. (arXiv:2301.09245v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09245
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#31070;&#32463;&#20803;&#22810;&#26679;&#24615;&#21487;&#20197;&#35299;&#20915;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#36208;&#21521;&#31070;&#32463;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introducing neuronal diversity can solve the fundamental problems of artificial neural networks and lead to NeuroAI.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#25972;&#20010;&#21382;&#21490;&#19978;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#19968;&#30452;&#23545;&#36234;&#26469;&#36234;&#28145;&#20837;&#30340;&#22823;&#33041;&#29702;&#35299;&#25345;&#24320;&#25918;&#24577;&#24230;&#24182;&#19981;&#26029;&#21463;&#21040;&#21551;&#21457;&#65292;&#20363;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;neocognitron&#30340;&#21551;&#21457;&#12290;&#26681;&#25454;&#26032;&#20852;&#39046;&#22495;&#31070;&#32463;&#20154;&#24037;&#26234;&#33021;&#30340;&#21160;&#26426;&#65292;&#22823;&#37327;&#30340;&#31070;&#32463;&#31185;&#23398;&#30693;&#35782;&#21487;&#20197;&#36890;&#36807;&#36171;&#20104;&#32593;&#32476;&#26356;&#24378;&#22823;&#30340;&#33021;&#21147;&#26469;&#20652;&#21270;&#19979;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#30693;&#36947;&#65292;&#20154;&#31867;&#22823;&#33041;&#26377;&#35768;&#22810;&#24418;&#24577;&#21644;&#21151;&#33021;&#19981;&#21516;&#30340;&#31070;&#32463;&#20803;&#65292;&#32780;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20960;&#20046;&#23436;&#20840;&#24314;&#31435;&#22312;&#21333;&#19968;&#31070;&#32463;&#20803;&#31867;&#22411;&#19978;&#12290;&#22312;&#20154;&#31867;&#22823;&#33041;&#20013;&#65292;&#31070;&#32463;&#20803;&#22810;&#26679;&#24615;&#26159;&#21508;&#31181;&#29983;&#29289;&#26234;&#33021;&#34892;&#20026;&#30340;&#19968;&#20010;&#21551;&#21160;&#22240;&#32032;&#12290;&#30001;&#20110;&#20154;&#24037;&#32593;&#32476;&#26159;&#20154;&#31867;&#22823;&#33041;&#30340;&#32553;&#24433;&#65292;&#24341;&#20837;&#31070;&#32463;&#20803;&#22810;&#26679;&#24615;&#24212;&#35813;&#26377;&#21161;&#20110;&#35299;&#20915;&#20154;&#24037;&#32593;&#32476;&#30340;&#35832;&#22914;&#25928;&#29575;&#12289;&#35299;&#37322;&#24615;&#31561;&#22522;&#26412;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Throughout history, the development of artificial intelligence, particularly artificial neural networks, has been open to and constantly inspired by the increasingly deepened understanding of the brain, such as the inspiration of neocognitron, which is the pioneering work of convolutional neural networks. Per the motives of the emerging field: NeuroAI, a great amount of neuroscience knowledge can help catalyze the next generation of AI by endowing a network with more powerful capabilities. As we know, the human brain has numerous morphologically and functionally different neurons, while artificial neural networks are almost exclusively built on a single neuron type. In the human brain, neuronal diversity is an enabling factor for all kinds of biological intelligent behaviors. Since an artificial network is a miniature of the human brain, introducing neuronal diversity should be valuable in terms of addressing those essential problems of artificial networks such as efficiency, interpret
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#35299;&#37322;&#22240;&#32032;&#20197;&#22686;&#24378;&#21306;&#20998;&#24615;&#34920;&#31034;&#25552;&#21462;&#65292;&#20197;&#21453;&#21521;&#20445;&#35777;&#32454;&#31890;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36866;&#29992;&#20110;&#31070;&#32463;&#24433;&#20687;&#21644;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#39640;&#32500;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2301.00815</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23156;&#20799;&#33041;&#21487;&#35299;&#37322;&#26041;&#27861;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A attention way in Explainable methods for infant brain. (arXiv:2301.00815v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#35299;&#37322;&#22240;&#32032;&#20197;&#22686;&#24378;&#21306;&#20998;&#24615;&#34920;&#31034;&#25552;&#21462;&#65292;&#20197;&#21453;&#21521;&#20445;&#35777;&#32454;&#31890;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36866;&#29992;&#20110;&#31070;&#32463;&#24433;&#20687;&#21644;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#39640;&#32500;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an explainable geometric deep network that enhances discriminative representation extraction by end-to-end learning of explanation factors, which is a more intuitive strategy to inversely assure fine-grained explainability, suitable for high-dimensional data in neuroimaging and neuroscience studies containing noisy, redundant, and task-irrelevant information.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36328;&#23398;&#31185;&#24212;&#29992;&#20013;&#37096;&#32626;&#21487;&#38752;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#38656;&#35201;&#23398;&#20064;&#27169;&#22411;&#36755;&#20986;&#20934;&#30830;&#19988;&#65288;&#26356;&#37325;&#35201;&#30340;&#26159;&#65289;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20197;&#20107;&#21518;&#26041;&#24335;&#35299;&#37322;&#32593;&#32476;&#36755;&#20986;&#65292;&#38544;&#21547;&#22320;&#20551;&#35774;&#24544;&#23454;&#30340;&#35299;&#37322;&#26469;&#33258;&#20934;&#30830;&#30340;&#39044;&#27979;/&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#30456;&#21453;&#30340;&#35266;&#28857;&#65292;&#21363;&#35299;&#37322;&#25552;&#21319;&#65288;&#29978;&#33267;&#20915;&#23450;&#65289;&#20998;&#31867;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#31471;&#21040;&#31471;&#23398;&#20064;&#35299;&#37322;&#22240;&#32032;&#20197;&#22686;&#24378;&#21306;&#20998;&#24615;&#34920;&#31034;&#25552;&#21462;&#21487;&#33021;&#26159;&#19968;&#31181;&#26356;&#30452;&#35266;&#30340;&#31574;&#30053;&#65292;&#20197;&#21453;&#21521;&#20445;&#35777;&#32454;&#31890;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20363;&#22914;&#22312;&#37027;&#20123;&#21253;&#21547;&#22122;&#22768;&#65292;&#20887;&#20313;&#21644;&#20219;&#21153;&#26080;&#20851;&#20449;&#24687;&#30340;&#39640;&#32500;&#25968;&#25454;&#30340;&#31070;&#32463;&#24433;&#20687;&#21644;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#28145;&#24230;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying reliable deep learning techniques in interdisciplinary applications needs learned models to output accurate and ({even more importantly}) explainable predictions. Existing approaches typically explicate network outputs in a post-hoc fashion, under an implicit assumption that faithful explanations come from accurate predictions/classifications. We have an opposite claim that explanations boost (or even determine) classification. That is, end-to-end learning of explanation factors to augment discriminative representation extraction could be a more intuitive strategy to inversely assure fine-grained explainability, e.g., in those neuroimaging and neuroscience studies with high-dimensional data containing noisy, redundant, and task-irrelevant information. In this paper, we propose such an explainable geometric deep network dubbed.
&lt;/p&gt;</description></item></channel></rss>