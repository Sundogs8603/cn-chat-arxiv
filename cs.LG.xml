<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#32852;&#37030;&#20998;&#26512;&#21644;&#24046;&#20998;&#38544;&#31169;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22522;&#20110;&#21069;&#32512;&#26641;&#31639;&#27861;&#30340;&#28909;&#38376;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#22312;Reddit&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#36825;&#20123;&#25913;&#36827;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.11749</link><description>&lt;p&gt;
&#20351;&#29992;&#32852;&#37030;&#20998;&#26512;&#30340;&#24046;&#20998;&#38544;&#31169;&#28909;&#38376;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Heavy Hitter Detection using Federated Analytics. (arXiv:2307.11749v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#32852;&#37030;&#20998;&#26512;&#21644;&#24046;&#20998;&#38544;&#31169;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22522;&#20110;&#21069;&#32512;&#26641;&#31639;&#27861;&#30340;&#28909;&#38376;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#22312;Reddit&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#36825;&#20123;&#25913;&#36827;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#25913;&#36827;&#22522;&#20110;&#21069;&#32512;&#26641;&#31639;&#27861;&#30340;&#24046;&#20998;&#38544;&#31169;&#28909;&#38376;&#26816;&#27979;&#30340;&#23454;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20551;&#35774;&#27599;&#20010;&#29992;&#25143;&#37117;&#26377;&#22810;&#20010;&#25968;&#25454;&#28857;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#36328;&#25152;&#26377;&#29992;&#25143;&#25968;&#25454;&#30340;&#23613;&#21487;&#33021;&#22810;&#30340;&#26368;&#24120;&#35265;&#25968;&#25454;&#28857;&#65292;&#24182;&#20351;&#29992;&#32858;&#21512;&#21644;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36229;&#21442;&#25968;&#35843;&#25972;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#28385;&#36275;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#38544;&#31169;&#32422;&#26463;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#26696;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#22312;&#22810;&#27425;&#36816;&#34892;&#31639;&#27861;&#26102;&#24341;&#20837;&#25298;&#32477;&#21015;&#34920;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;Reddit&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#26469;&#27979;&#35797;&#36825;&#20123;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study practical heuristics to improve the performance of prefix-tree based algorithms for differentially private heavy hitter detection. Our model assumes each user has multiple data points and the goal is to learn as many of the most frequent data points as possible across all users' data with aggregate and local differential privacy. We propose an adaptive hyperparameter tuning algorithm that improves the performance of the algorithm while satisfying computational, communication and privacy constraints. We explore the impact of different data-selection schemes as well as the impact of introducing deny lists during multiple runs of the algorithm. We test these improvements using extensive experimentation on the Reddit dataset~\cite{caldas2018leaf} on the task of learning the most frequent words.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#27169;&#22411;&#26469;&#27169;&#25311;&#29616;&#23454;&#30340;&#22312;&#32447;&#24191;&#21578;&#25293;&#21334;&#29615;&#22659;&#65292;&#24182;&#21457;&#29616;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;"&#36719;&#24213;&#20215;"&#21487;&#20197;&#25552;&#39640;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#65292;&#21363;&#20351;&#25237;&#26631;&#32773;&#26469;&#33258;&#30456;&#21516;&#30340;&#20154;&#32676;&#12290;</title><link>http://arxiv.org/abs/2307.11732</link><description>&lt;p&gt;
&#25512;&#36827;&#24191;&#21578;&#25293;&#21334;&#30340;&#29616;&#23454;&#24615;&#65306;&#23454;&#38469;&#35265;&#35299;&#19982;&#24314;&#27169;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Advancing Ad Auction Realism: Practical Insights &amp; Modeling Implications. (arXiv:2307.11732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#27169;&#22411;&#26469;&#27169;&#25311;&#29616;&#23454;&#30340;&#22312;&#32447;&#24191;&#21578;&#25293;&#21334;&#29615;&#22659;&#65292;&#24182;&#21457;&#29616;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;"&#36719;&#24213;&#20215;"&#21487;&#20197;&#25552;&#39640;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#65292;&#21363;&#20351;&#25237;&#26631;&#32773;&#26469;&#33258;&#30456;&#21516;&#30340;&#20154;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#24191;&#21578;&#25293;&#21334;&#23398;&#20064;&#27169;&#22411;&#65292;&#20801;&#35768;&#32771;&#34385;&#24403;&#20195;&#22312;&#32447;&#25293;&#21334;&#30340;&#22235;&#20010;&#20851;&#38190;&#29616;&#23454;&#29305;&#24449;&#65306;&#65288;1&#65289;&#24191;&#21578;&#27133;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#25628;&#32034;&#26597;&#35810;&#20855;&#26377;&#19981;&#21516;&#30340;&#20215;&#20540;&#21644;&#28857;&#20987;&#29575;&#65292;&#65288;2&#65289;&#31454;&#20105;&#24191;&#21578;&#21830;&#30340;&#25968;&#37327;&#21644;&#36523;&#20221;&#26159;&#19981;&#21487;&#35266;&#23519;&#30340;&#65292;&#24182;&#19988;&#22312;&#27599;&#27425;&#31454;&#25293;&#20013;&#20250;&#21457;&#29983;&#26356;&#25913;&#65292;&#65288;3&#65289;&#24191;&#21578;&#21830;&#20165;&#25509;&#25910;&#21040;&#37096;&#20998;&#30340;&#27719;&#24635;&#21453;&#39304;&#65292;&#65288;4&#65289;&#20184;&#27454;&#35268;&#21017;&#21482;&#37096;&#20998;&#30830;&#23450;&#12290;&#25105;&#20204;&#23558;&#24191;&#21578;&#21830;&#24314;&#27169;&#20026;&#21463;&#23545;&#25239;&#24615;&#36172;&#21338;&#31639;&#27861;&#39537;&#21160;&#30340;&#20195;&#29702;&#65292;&#29420;&#31435;&#20110;&#25293;&#21334;&#26426;&#21046;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#20102;&#27169;&#25311;&#24191;&#21578;&#21830;&#30340;&#34892;&#20026;&#65292;&#36827;&#34892;&#21453;&#20107;&#23454;&#20998;&#26512;&#12289;&#39044;&#27979;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#26356;&#22797;&#26434;&#30340;&#29615;&#22659;&#20013;&#65292;&#21363;&#20351;&#25237;&#26631;&#32773;&#26469;&#33258;&#30456;&#21516;&#30340;&#20154;&#32676;&#65292;"&#36719;&#24213;&#20215;"&#20063;&#21487;&#20197;&#25552;&#39640;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#35266;&#23519;&#21040;&#30340;&#31454;&#26631;&#20013;&#25512;&#26029;&#24191;&#21578;&#21830;&#20215;&#20540;&#20998;&#24067;&#65292;&#20174;&#32780;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#38469;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a learning model of online ad auctions that allows for the following four key realistic characteristics of contemporary online auctions: (1) ad slots can have different values and click-through rates depending on users' search queries, (2) the number and identity of competing advertisers are unobserved and change with each auction, (3) advertisers only receive partial, aggregated feedback, and (4) payment rules are only partially specified. We model advertisers as agents governed by an adversarial bandit algorithm, independent of auction mechanism intricacies. Our objective is to simulate the behavior of advertisers for counterfactual analysis, prediction, and inference purposes. Our findings reveal that, in such richer environments, "soft floors" can enhance key performance metrics even when bidders are drawn from the same population. We further demonstrate how to infer advertiser value distributions from observed bids, thereby affirming the practical efficacy of o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#23433;&#20840;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#23433;&#20840;&#27169;&#22359;&#65292;&#36890;&#36807;&#32467;&#21512;&#21152;&#23494;&#25216;&#26415;&#21644;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;&#25216;&#26415;&#26469;&#23545;&#25239;&#36890;&#20449;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2307.11730</link><description>&lt;p&gt;
&#36890;&#36807;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;&#20943;&#36731;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense. (arXiv:2307.11730v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#23433;&#20840;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#23433;&#20840;&#27169;&#22359;&#65292;&#36890;&#36807;&#32467;&#21512;&#21152;&#23494;&#25216;&#26415;&#21644;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;&#25216;&#26415;&#26469;&#23545;&#25239;&#36890;&#20449;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#30340;&#20852;&#36215;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#22312;&#32852;&#37030;&#21442;&#19982;&#26041;&#20043;&#38388;&#36827;&#34892;&#35757;&#32451;&#65292;&#20419;&#36827;&#20102;&#20998;&#25955;&#24335;&#27169;&#22411;&#32858;&#21512;&#24182;&#20943;&#23569;&#23545;&#26381;&#21153;&#22120;&#30340;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24341;&#20837;&#20102;&#29420;&#29305;&#30340;&#36890;&#20449;&#23433;&#20840;&#25361;&#25112;&#65292;&#23578;&#26410;&#22312;&#25991;&#29486;&#20013;&#24471;&#21040;&#20805;&#20998;&#35299;&#20915;&#12290;&#36825;&#20123;&#25361;&#25112;&#20027;&#35201;&#28304;&#20110;&#32858;&#21512;&#36807;&#31243;&#30340;&#20998;&#25955;&#24615;&#36136;&#12289;&#21442;&#19982;&#32773;&#30340;&#22810;&#26679;&#21270;&#35282;&#33394;&#21644;&#36131;&#20219;&#20197;&#21450;&#32570;&#20047;&#30417;&#31649;&#21644;&#32531;&#35299;&#23041;&#32961;&#30340;&#20013;&#22830;&#26426;&#26500;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#39318;&#20808;&#30028;&#23450;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#31361;&#20986;&#20102;DFL&#36890;&#20449;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#38024;&#23545;&#36825;&#20123;&#30830;&#23450;&#30340;&#39118;&#38505;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#20026;DFL&#24179;&#21488;&#35774;&#35745;&#30340;&#23433;&#20840;&#27169;&#22359;&#26469;&#23545;&#25239;&#22522;&#20110;&#36890;&#20449;&#30340;&#25915;&#20987;&#12290;&#35813;&#27169;&#22359;&#32467;&#21512;&#20102;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#21152;&#23494;&#31561;&#23433;&#20840;&#25216;&#26415;&#19982;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;&#65288;MTD&#65289;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of Decentralized Federated Learning (DFL) has enabled the training of machine learning models across federated participants, fostering decentralized model aggregation and reducing dependence on a server. However, this approach introduces unique communication security challenges that have yet to be thoroughly addressed in the literature. These challenges primarily originate from the decentralized nature of the aggregation process, the varied roles and responsibilities of the participants, and the absence of a central authority to oversee and mitigate threats. Addressing these challenges, this paper first delineates a comprehensive threat model, highlighting the potential risks of DFL communications. In response to these identified risks, this work introduces a security module designed for DFL platforms to counter communication-based attacks. The module combines security techniques such as symmetric and asymmetric encryption with Moving Target Defense (MTD) techniques, including
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20999;&#29255;Wasserstein&#25439;&#22833;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;SGD&#36712;&#36857;&#36924;&#36817;&#20102;&#26799;&#24230;&#27969;&#26041;&#31243;&#30340;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2307.11714</link><description>&lt;p&gt;
&#20351;&#29992;&#20999;&#29255;Wasserstein&#25439;&#22833;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;SGD&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses. (arXiv:2307.11714v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20999;&#29255;Wasserstein&#25439;&#22833;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;SGD&#36712;&#36857;&#36924;&#36817;&#20102;&#26799;&#24230;&#27969;&#26041;&#31243;&#30340;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;&#36817;&#24180;&#26469;&#24341;&#21457;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;Wasserstein&#36317;&#31163;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#20960;&#20309;&#19978;&#21512;&#29702;&#21644;&#30452;&#35266;&#30340;&#27604;&#36739;&#27010;&#29575;&#27979;&#24230;&#30340;&#26041;&#27861;&#12290;&#20986;&#20110;&#35745;&#31639;&#21407;&#22240;&#65292;&#20999;&#29255;Wasserstein&#65288;SW&#65289;&#36317;&#31163;&#20316;&#20026;Wasserstein&#36317;&#31163;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#34987;&#24341;&#20837;&#65292;&#24182;&#19988;&#24050;&#32463;&#34987;&#29992;&#20110;&#35757;&#32451;&#29983;&#25104;&#24335;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#12290;&#34429;&#28982;&#22312;&#36825;&#26679;&#30340;&#35774;&#32622;&#20013;&#23454;&#38469;&#35266;&#23519;&#21040;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#24615;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23545;&#20110;&#36825;&#19968;&#35266;&#23519;&#27809;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;&#20511;&#37492;Bianchi&#31561;&#20154;&#65288;2022&#65289;&#20851;&#20110;SGD&#22312;&#38750;&#20809;&#28369;&#21644;&#38750;&#20984;&#20989;&#25968;&#19978;&#25910;&#25947;&#24615;&#30340;&#26368;&#26032;&#24037;&#20316;&#65292;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#30693;&#35782;&#31354;&#30333;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#30340;&#19978;&#19979;&#25991;&#65292;&#20351;&#24471;SW&#25439;&#22833;&#23545;NN&#21442;&#25968;&#30340;&#22266;&#23450;&#27493;&#38271;SGD&#36712;&#36857;&#25910;&#25947;&#21040;&#65288;&#27425;&#65289;&#26799;&#24230;&#27969;&#26041;&#31243;&#30340;&#38598;&#21512;&#12290;&#26356;&#20934;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#38543;&#30528;&#27493;&#38271;&#20943;&#23567;&#65292;&#36825;&#20123;&#36712;&#36857;&#36924;&#36817;&#20102;&#26799;&#24230;&#27969;&#26041;&#31243;&#30340;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport has sparked vivid interest in recent years, in particular thanks to the Wasserstein distance, which provides a geometrically sensible and intuitive way of comparing probability measures. For computational reasons, the Sliced Wasserstein (SW) distance was introduced as an alternative to the Wasserstein distance, and has seen uses for training generative Neural Networks (NNs). While convergence of Stochastic Gradient Descent (SGD) has been observed practically in such a setting, there is to our knowledge no theoretical guarantee for this observation. Leveraging recent works on convergence of SGD on non-smooth and non-convex functions by Bianchi et al. (2022), we aim to bridge that knowledge gap, and provide a realistic context under which fixed-step SGD trajectories for the SW loss on NN parameters converge. More precisely, we show that the trajectories approach the set of (sub)-gradient flow equations as the step decreases. Under stricter assumptions, we show a much st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;JoinGym&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26597;&#35810;&#20248;&#21270;&#29615;&#22659;&#12290;&#36890;&#36807;&#23558;&#21152;&#20837;&#39034;&#24207;&#36873;&#25321;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29616;&#65292;&#35813;&#23454;&#29616;&#23436;&#20840;&#22522;&#20110;&#31163;&#32447;&#36319;&#36394;&#65292;&#24182;&#19988;&#26080;&#38656;&#35774;&#32622;&#31995;&#32479;&#21363;&#21487;&#36827;&#34892;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;3300&#20010;&#26032;SQL&#26597;&#35810;&#30340;&#25152;&#26377;&#21487;&#33021;&#21152;&#20837;&#36861;&#36394;&#12290;</title><link>http://arxiv.org/abs/2307.11704</link><description>&lt;p&gt;
JoinGym: &#19968;&#31181;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26597;&#35810;&#20248;&#21270;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
JoinGym: An Efficient Query Optimization Environment for Reinforcement Learning. (arXiv:2307.11704v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;JoinGym&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26597;&#35810;&#20248;&#21270;&#29615;&#22659;&#12290;&#36890;&#36807;&#23558;&#21152;&#20837;&#39034;&#24207;&#36873;&#25321;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29616;&#65292;&#35813;&#23454;&#29616;&#23436;&#20840;&#22522;&#20110;&#31163;&#32447;&#36319;&#36394;&#65292;&#24182;&#19988;&#26080;&#38656;&#35774;&#32622;&#31995;&#32479;&#21363;&#21487;&#36827;&#34892;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;3300&#20010;&#26032;SQL&#26597;&#35810;&#30340;&#25152;&#26377;&#21487;&#33021;&#21152;&#20837;&#36861;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;JoinGym&#65292;&#19968;&#31181;&#39640;&#25928;&#19988;&#36731;&#37327;&#32423;&#30340;&#24378;&#21270;&#23398;&#20064;&#26597;&#35810;&#20248;&#21270;&#29615;&#22659;&#12290;&#21152;&#20837;&#39034;&#24207;&#36873;&#25321;&#65288;JOS&#65289;&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;NP-hard&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#29992;&#20110;&#25968;&#25454;&#24211;&#26597;&#35810;&#20248;&#21270;&#65292;&#21487;&#20316;&#20026;RL&#31639;&#27861;&#27867;&#21270;&#33021;&#21147;&#30340;&#23454;&#38469;&#27979;&#35797;&#24179;&#21488;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#23558;&#24038;&#28145;&#21644;&#32321;&#33538;&#30340;JOS&#38382;&#39064;&#30340;&#27599;&#20010;&#21464;&#31181;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#24182;&#25552;&#20379;&#31526;&#21512;&#26631;&#20934;Gymnasium API&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#24378;&#35843;&#25105;&#20204;&#30340;&#23454;&#29616;JoinGym&#23436;&#20840;&#22522;&#20110;&#25152;&#26377;&#21487;&#33021;&#36830;&#25509;&#30340;&#31163;&#32447;&#36861;&#36394;&#65292;&#20351;RL&#20174;&#19994;&#32773;&#33021;&#22815;&#36731;&#26494;&#24555;&#36895;&#22320;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#25968;&#25454;&#31649;&#29702;&#38382;&#39064;&#19978;&#27979;&#35797;&#20182;&#20204;&#30340;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#35774;&#32622;&#20219;&#20309;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20174;IMDB&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;3300&#20010;&#26032;SQL&#26597;&#35810;&#30340;&#25152;&#26377;&#21487;&#33021;&#36830;&#25509;&#36861;&#36394;&#12290;&#22312;&#23545;&#27969;&#34892;&#30340;RL&#31639;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#33267;&#23569;&#26377;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present \textsc{JoinGym}, an efficient and lightweight query optimization environment for reinforcement learning (RL). Join order selection (JOS) is a classic NP-hard combinatorial optimization problem from database query optimization and can serve as a practical testbed for the generalization capabilities of RL algorithms. We describe how to formulate each of the left-deep and bushy variants of the JOS problem as a Markov Decision Process (MDP), and we provide an implementation adhering to the standard Gymnasium API. We highlight that our implementation \textsc{JoinGym} is completely based on offline traces of all possible joins, which enables RL practitioners to easily and quickly test their methods on a realistic data management problem without needing to setup any systems. Moreover, we also provide all possible join traces on $3300$ novel SQL queries generated from the IMDB dataset. Upon benchmarking popular RL algorithms, we find that at least one method can obta
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20223;&#30495;&#29615;&#22659;&#26469;&#22686;&#24378;&#20861;&#21307;&#23398;&#20013;&#30340;&#25968;&#25454;&#37319;&#38598;&#21644;&#35786;&#26029;&#65292;&#29305;&#21035;&#20851;&#27880;&#29356;&#21482;&#30340;&#27493;&#24577;&#20998;&#26512;&#12290;&#36890;&#36807;&#29983;&#25104;&#21453;&#26144;&#22810;&#26679;&#26465;&#20214;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#27491;&#24120;&#21644;&#24322;&#24120;&#27493;&#24577;&#65292;&#24182;&#25506;&#32034;&#20102;&#25668;&#20687;&#22836;&#35270;&#35282;&#23545;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#20223;&#30495;&#26041;&#27861;&#26377;&#26395;&#25552;&#39640;&#20861;&#21307;&#35786;&#26029;&#30340;&#31934;&#30830;&#24615;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11695</link><description>&lt;p&gt;
&#20351;&#29992;&#20223;&#30495;&#25216;&#26415;&#22312;&#20861;&#21307;&#23398;&#20013;&#26657;&#20934;&#30495;&#23454;&#25968;&#25454;&#37319;&#38598;
&lt;/p&gt;
&lt;p&gt;
Using simulation to calibrate real data acquisition in veterinary medicine. (arXiv:2307.11695v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11695
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20223;&#30495;&#29615;&#22659;&#26469;&#22686;&#24378;&#20861;&#21307;&#23398;&#20013;&#30340;&#25968;&#25454;&#37319;&#38598;&#21644;&#35786;&#26029;&#65292;&#29305;&#21035;&#20851;&#27880;&#29356;&#21482;&#30340;&#27493;&#24577;&#20998;&#26512;&#12290;&#36890;&#36807;&#29983;&#25104;&#21453;&#26144;&#22810;&#26679;&#26465;&#20214;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#27491;&#24120;&#21644;&#24322;&#24120;&#27493;&#24577;&#65292;&#24182;&#25506;&#32034;&#20102;&#25668;&#20687;&#22836;&#35270;&#35282;&#23545;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#20223;&#30495;&#26041;&#27861;&#26377;&#26395;&#25552;&#39640;&#20861;&#21307;&#35786;&#26029;&#30340;&#31934;&#30830;&#24615;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20861;&#21307;&#23398;&#20013;&#21019;&#26032;&#30340;&#20351;&#29992;&#20223;&#30495;&#29615;&#22659;&#26469;&#22686;&#24378;&#25968;&#25454;&#37319;&#38598;&#21644;&#35786;&#26029;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#29356;&#21482;&#30340;&#27493;&#24577;&#20998;&#26512;&#12290;&#30740;&#31350;&#21033;&#29992;Blender&#21644;Blenderproc&#24211;&#29983;&#25104;&#21453;&#26144;&#22810;&#26679;&#35299;&#21078;&#12289;&#29615;&#22659;&#21644;&#34892;&#20026;&#26465;&#20214;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#29983;&#25104;&#30340;&#25968;&#25454;&#20197;&#22270;&#24418;&#24418;&#24335;&#34920;&#31034;&#65292;&#24182;&#36827;&#34892;&#26631;&#20934;&#21270;&#20197;&#23454;&#29616;&#26368;&#20339;&#20998;&#26512;&#25928;&#26524;&#65292;&#28982;&#21518;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20197;&#35782;&#21035;&#27491;&#24120;&#21644;&#24322;&#24120;&#27493;&#24577;&#12290;&#21019;&#24314;&#20102;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#25668;&#20687;&#22836;&#35282;&#24230;&#31890;&#24230;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#36827;&#19968;&#27493;&#30740;&#31350;&#25668;&#20687;&#22836;&#35270;&#35282;&#23545;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#22522;&#20110;&#20223;&#30495;&#30340;&#26041;&#27861;&#26377;&#26395;&#36890;&#36807;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#25968;&#25454;&#37319;&#38598;&#21644;&#26356;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25512;&#21160;&#20861;&#21307;&#35786;&#26029;&#30340;&#36827;&#23637;&#12290;&#36890;&#36807;&#25972;&#21512;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#24739;&#32773;&#25968;&#25454;&#65292;&#35813;&#30740;&#31350;&#20026;&#25913;&#21892;&#20861;&#21307;&#23398;&#30340;&#21457;&#23637;&#22880;&#23450;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the innovative use of simulation environments to enhance data acquisition and diagnostics in veterinary medicine, focusing specifically on gait analysis in dogs. The study harnesses the power of Blender and the Blenderproc library to generate synthetic datasets that reflect diverse anatomical, environmental, and behavioral conditions. The generated data, represented in graph form and standardized for optimal analysis, is utilized to train machine learning algorithms for identifying normal and abnormal gaits. Two distinct datasets with varying degrees of camera angle granularity are created to further investigate the influence of camera perspective on model accuracy. Preliminary results suggest that this simulation-based approach holds promise for advancing veterinary diagnostics by enabling more precise data acquisition and more effective machine learning models. By integrating synthetic and real-world patient data, the study lays a robust foundation for improving o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#20010;&#24615;&#21270;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#24182;&#36827;&#34892;&#33647;&#29289;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23567;&#22411;&#30340;&#20010;&#24615;&#21270;&#25968;&#25454;&#38598;&#65292;&#19981;&#20381;&#36182;&#20110;&#25991;&#26412;&#35821;&#26009;&#24211;&#12289;&#20998;&#23376;&#25351;&#32441;&#25110;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11694</link><description>&lt;p&gt;
SynerGPT:&#19978;&#19979;&#25991;&#23398;&#20064;&#29992;&#20110;&#20010;&#24615;&#21270;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#39044;&#27979;&#21644;&#33647;&#29289;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design. (arXiv:2307.11694v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#20010;&#24615;&#21270;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#24182;&#36827;&#34892;&#33647;&#29289;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23567;&#22411;&#30340;&#20010;&#24615;&#21270;&#25968;&#25454;&#38598;&#65292;&#19981;&#20381;&#36182;&#20110;&#25991;&#26412;&#35821;&#26009;&#24211;&#12289;&#20998;&#23376;&#25351;&#32441;&#25110;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#33647;&#29289;&#30340;&#21327;&#21516;&#32452;&#21512;&#21487;&#20197;&#21152;&#36895;&#30284;&#30151;&#27835;&#30103;&#30340;&#21457;&#29616;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#27963;&#26816;&#32454;&#32990;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#32622;&#21644;&#27169;&#22411;&#29992;&#20110;&#19978;&#19979;&#25991;&#20013;&#30340;&#33647;&#29289;&#21327;&#21516;&#23398;&#20064;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#23567;&#30340;&#8220;&#20010;&#24615;&#21270;&#25968;&#25454;&#38598;&#8221;&#65292;&#20854;&#20013;&#21253;&#21547;&#29305;&#23450;&#30284;&#30151;&#38774;&#32454;&#32990;&#19978;&#19979;&#25991;&#20013;&#30340;10-20&#20010;&#33647;&#29289;&#21327;&#21516;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#39044;&#27979;&#35813;&#19978;&#19979;&#25991;&#20013;&#30340;&#39069;&#22806;&#33647;&#29289;&#21327;&#21516;&#20851;&#31995;&#12290;&#21463;&#26368;&#36817;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#35813;&#24037;&#20316;&#36890;&#36807;&#39044;&#35757;&#32451;GPT&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26469;&#8220;&#19978;&#19979;&#25991;&#23398;&#20064;&#8221;&#24120;&#35265;&#30340;&#21151;&#33021;&#31867;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181; &#26032;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#20351;GPT&#27169;&#22411;&#33021;&#22815;&#19978;&#19979;&#25991;&#23398;&#20064;&#8220;&#33647;&#29289;&#21327;&#21516;&#21151;&#33021;&#8221;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411; - &#19981;&#20351;&#29992;&#20219;&#20309;&#25991;&#26412;&#35821;&#26009;&#24211;&#65292;&#20998;&#23376;&#25351;&#32441;&#65292;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#25110;&#20219;&#20309;&#20854;&#20182;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782; - &#33021;&#22815;&#21462;&#24471;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#19978;&#19979;&#25991;&#26041;&#27861;&#19982;&#36951;&#20256;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#20248;&#21270;&#27169;&#22411;&#25552;&#31034;&#24182;&#36873;&#25321;&#21327;&#21516;&#20505;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting synergistic drug combinations can help accelerate discovery of cancer treatments, particularly therapies personalized to a patient's specific tumor via biopsied cells. In this paper, we propose a novel setting and models for in-context drug synergy learning. We are given a small "personalized dataset" of 10-20 drug synergy relationships in the context of specific cancer cell targets. Our goal is to predict additional drug synergy relationships in that context. Inspired by recent work that pre-trains a GPT language model (LM) to "in-context learn" common function classes, we devise novel pre-training schemes that enable a GPT model to in-context learn "drug synergy functions". Our model -- which does not use any textual corpora, molecular fingerprints, protein interaction or any other domain-specific knowledge -- is able to achieve competitive results. We further integrate our in-context approach with a genetic algorithm to optimize model prompts and select synergy candidates
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#39318;&#27425;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#26469;&#30740;&#31350;&#36890;&#29992;&#20195;&#25968;&#30340;&#29468;&#24819;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;&#22270;&#32593;&#32476;&#20197;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.11688</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22270;&#32593;&#32476;&#29992;&#20110;&#24418;&#25104;&#36890;&#29992;&#20195;&#25968;&#29468;&#24819;
&lt;/p&gt;
&lt;p&gt;
Interpretable Graph Networks Formulate Universal Algebra Conjectures. (arXiv:2307.11688v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11688
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#39318;&#27425;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#26469;&#30740;&#31350;&#36890;&#29992;&#20195;&#25968;&#30340;&#29468;&#24819;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;&#22270;&#32593;&#32476;&#20197;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#30340;&#23835;&#36215;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#25506;&#31350;&#20960;&#21313;&#24180;&#26469;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#22256;&#38590;&#25968;&#23398;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#36890;&#29992;&#20195;&#25968;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#23436;&#20840;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#27425;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#26469;&#30740;&#31350;&#36890;&#29992;&#20195;&#25968;&#30340;&#29468;&#24819;&#65292;&#32467;&#21512;&#31561;&#24335;&#21644;&#25299;&#25169;&#29305;&#24449;&#36827;&#34892;&#24314;&#27169;&#12290;&#34429;&#28982;&#25299;&#25169;&#34920;&#31034;&#27861;&#21487;&#20197;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20998;&#26512;&#36825;&#20123;&#23646;&#24615;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26377;&#38480;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30452;&#25509;&#29992;&#20110;&#39564;&#35777;&#29616;&#26377;&#29468;&#24819;&#25110;&#25552;&#20986;&#26032;&#29468;&#24819;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#31639;&#27861;&#65292;&#22522;&#20110;&#36890;&#29992;&#20195;&#25968;&#30340;&#29468;&#24819;&#29983;&#25104;&#21487;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#23618;&#26469;&#26500;&#24314;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#22270;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#35299;&#37322;&#30340;&#22270;&#32593;&#32476;&#33021;&#22815;&#22686;&#24378;...
&lt;/p&gt;
&lt;p&gt;
The rise of Artificial Intelligence (AI) recently empowered researchers to investigate hard mathematical problems which eluded traditional approaches for decades. Yet, the use of AI in Universal Algebra (UA) -- one of the fields laying the foundations of modern mathematics -- is still completely unexplored. This work proposes the first use of AI to investigate UA's conjectures with an equivalent equational and topological characterization. While topological representations would enable the analysis of such properties using graph neural networks, the limited transparency and brittle explainability of these models hinder their straightforward use to empirically validate existing conjectures or to formulate new ones. To bridge these gaps, we propose a general algorithm generating AI-ready datasets based on UA's conjectures, and introduce a novel neural layer to build fully interpretable graph networks. The results of our experiments demonstrate that interpretable graph networks: (i) enhan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#36890;&#29992;&#21270;&#30340;&#20132;&#26131;&#25191;&#34892;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#38459;&#30861;&#20102;&#23454;&#38469;&#24212;&#29992;&#12290;&#20316;&#32773;&#36890;&#36807;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#21160;&#24577;&#19978;&#19979;&#25991;&#24314;&#27169;&#26469;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#23398;&#20064;&#19978;&#19979;&#25991;&#30340;&#32039;&#20945;&#34920;&#31034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.11685</link><description>&lt;p&gt;
&#38754;&#21521;&#36890;&#29992;&#21270;&#30340;&#20132;&#26131;&#25191;&#34892;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Generalizable Reinforcement Learning for Trade Execution. (arXiv:2307.11685v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#36890;&#29992;&#21270;&#30340;&#20132;&#26131;&#25191;&#34892;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#38459;&#30861;&#20102;&#23454;&#38469;&#24212;&#29992;&#12290;&#20316;&#32773;&#36890;&#36807;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#21160;&#24577;&#19978;&#19979;&#25991;&#24314;&#27169;&#26469;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#23398;&#20064;&#19978;&#19979;&#25991;&#30340;&#32039;&#20945;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#30340;&#20132;&#26131;&#25191;&#34892;&#26159;&#22312;&#32473;&#23450;&#26102;&#38388;&#20869;&#20197;&#26368;&#20302;&#30340;&#20132;&#26131;&#25104;&#26412;&#21334;&#20986;&#65288;&#25110;&#20080;&#20837;&#65289;&#32473;&#23450;&#36164;&#20135;&#30340;&#36807;&#31243;&#12290;&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#20248;&#21270;&#30340;&#20132;&#26131;&#25191;&#34892;&#65292;&#20197;&#20174;&#24066;&#22330;&#25968;&#25454;&#20013;&#23398;&#20064;&#26356;&#26234;&#33021;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#35768;&#22810;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23384;&#22312;&#26174;&#33879;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20248;&#21270;&#30340;&#20132;&#26131;&#25191;&#34892;&#20013;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#20248;&#21270;&#30340;&#20132;&#26131;&#25191;&#34892;&#24314;&#27169;&#20026;&#24102;&#26377;&#21160;&#24577;&#19978;&#19979;&#25991;&#65288;ORDC&#65289;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#34920;&#31034;&#19981;&#33021;&#21463;&#21040;&#20132;&#26131;&#31574;&#30053;&#24433;&#21709;&#24182;&#20197;&#31163;&#32447;&#26041;&#24335;&#25910;&#38598;&#30340;&#24066;&#22330;&#21464;&#37327;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#21457;&#29616;&#36807;&#25311;&#21512;&#38382;&#39064;&#26159;&#30001;&#20110;&#31163;&#32447;&#29615;&#22659;&#20013;&#19978;&#19979;&#25991;&#31354;&#38388;&#24040;&#22823;&#19988;&#19978;&#19979;&#25991;&#26679;&#26412;&#26377;&#38480;&#25152;&#23548;&#33268;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#19978;&#19979;&#25991;&#30340;&#32039;&#20945;&#34920;&#31034;&#26469;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;...
&lt;/p&gt;
&lt;p&gt;
Optimized trade execution is to sell (or buy) a given amount of assets in a given time with the lowest possible trading cost. Recently, reinforcement learning (RL) has been applied to optimized trade execution to learn smarter policies from market data. However, we find that many existing RL methods exhibit considerable overfitting which prevents them from real deployment. In this paper, we provide an extensive study on the overfitting problem in optimized trade execution. First, we model the optimized trade execution as offline RL with dynamic context (ORDC), where the context represents market variables that cannot be influenced by the trading policy and are collected in an offline manner. Under this framework, we derive the generalization bound and find that the overfitting issue is caused by large context space and limited context samples in the offline setting. Accordingly, we propose to learn compact representations for context to address the overfitting problem, either by levera
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#25209;&#27425;&#22823;&#23567;&#23545;&#20110;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;&#21270;&#21487;&#20197;&#25552;&#20379;&#25913;&#21892;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.11684</link><description>&lt;p&gt;
&#23567;&#25209;&#37327;&#21270;&#23545;&#20110;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#25552;&#20379;&#20102;&#25913;&#21892;&#30340;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Minibatching Offers Improved Generalization Performance for Second Order Optimizers. (arXiv:2307.11684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#25209;&#27425;&#22823;&#23567;&#23545;&#20110;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;&#21270;&#21487;&#20197;&#25552;&#20379;&#25913;&#21892;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#38750;&#24120;&#32791;&#36153;&#35745;&#31639;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#26426;&#22120;&#23398;&#20064;&#31185;&#23398;&#23478;&#36890;&#24120;&#20381;&#36182;&#20110;&#38543;&#26426;&#19968;&#38454;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36827;&#34892;&#22823;&#37327;&#25163;&#24037;&#35843;&#25972;&#65292;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#19981;&#21516;&#38543;&#26426;&#31639;&#27861;&#65288;&#21253;&#25324;&#20108;&#38454;&#26041;&#27861;&#65289;&#30340;&#24615;&#33021;&#21464;&#24322;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#32463;&#39564;&#24615;&#30740;&#31350;&#65292;&#23558;&#24615;&#33021;&#35270;&#20026;&#21516;&#19968;&#27169;&#22411;&#30340;&#22810;&#20010;&#35757;&#32451;&#20250;&#35805;&#20013;&#30340;&#21709;&#24212;&#21464;&#37327;&#12290;&#20351;&#29992;&#20108;&#22240;&#32032;&#26041;&#24046;&#20998;&#26512;(ANOVA)&#19982;&#20132;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#34920;&#26126;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#25209;&#27425;&#22823;&#23567;&#23545;&#26041;&#27861;&#30340;&#23792;&#20540;&#20934;&#30830;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#24182;&#19988;&#23436;&#25972;&#25209;&#27425;&#36890;&#24120;&#34920;&#29616;&#26368;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20108;&#38454;&#20248;&#21270;&#22120;(SOOs)&#22312;&#29305;&#23450;&#25209;&#27425;&#22823;&#23567;&#19978;&#36890;&#24120;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#26041;&#24046;&#65292;&#36825;&#34920;&#26126;&#23427;&#20204;&#21487;&#33021;&#38656;&#35201;&#36739;&#23569;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#27169;&#22411;&#35757;&#32451;&#30340;&#24635;&#20307;&#26102;&#38388;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training deep neural networks (DNNs) used in modern machine learning is computationally expensive. Machine learning scientists, therefore, rely on stochastic first-order methods for training, coupled with significant hand-tuning, to obtain good performance. To better understand performance variability of different stochastic algorithms, including second-order methods, we conduct an empirical study that treats performance as a response variable across multiple training sessions of the same model. Using 2-factor Analysis of Variance (ANOVA) with interactions, we show that batch size used during training has a statistically significant effect on the peak accuracy of the methods, and that full batch largely performed the worst. In addition, we found that second-order optimizers (SOOs) generally exhibited significantly lower variance at specific batch sizes, suggesting they may require less hyperparameter tuning, leading to a reduced overall time to solution for model training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#36866;&#24212;&#30340;&#27979;&#35797;&#38450;&#24481;&#31574;&#30053;&#65292;&#36890;&#36807;&#25237;&#24433;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21040;&#26368;&#31283;&#20581;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#38477;&#20302;&#20102;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#27979;&#35797;&#26102;&#38388;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2307.11672</link><description>&lt;p&gt;
&#24555;&#36895;&#33258;&#36866;&#24212;&#27979;&#35797;&#38450;&#24481;&#19982;&#31283;&#20581;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Fast Adaptive Test-Time Defense with Robust Features. (arXiv:2307.11672v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#36866;&#24212;&#30340;&#27979;&#35797;&#38450;&#24481;&#31574;&#30053;&#65292;&#36890;&#36807;&#25237;&#24433;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21040;&#26368;&#31283;&#20581;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#38477;&#20302;&#20102;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#27979;&#35797;&#26102;&#38388;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#30340;&#27979;&#35797;&#38450;&#24481;&#34987;&#29992;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#23545;&#27169;&#22411;&#21442;&#25968;&#25110;&#36755;&#20837;&#36827;&#34892;&#39069;&#22806;&#30340;&#20248;&#21270;&#23548;&#33268;&#25512;&#29702;&#26102;&#38388;&#22823;&#24133;&#22686;&#21152;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#27979;&#35797;&#38450;&#24481;&#31574;&#30053;&#65292;&#23427;&#21487;&#20197;&#19982;&#20219;&#20309;&#29616;&#26377;&#65288;&#31283;&#20581;&#30340;&#65289;&#35757;&#32451;&#36807;&#31243;&#36731;&#26494;&#38598;&#25104;&#65292;&#24182;&#19988;&#26080;&#38656;&#39069;&#22806;&#30340;&#27979;&#35797;&#26102;&#38388;&#35745;&#31639;&#12290;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#29305;&#24449;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#65292;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#25237;&#24433;&#21040;&#26368;&#31283;&#20581;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#38477;&#20302;&#23545;&#38750;&#31283;&#20581;&#26041;&#21521;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#22312;&#24191;&#20041;&#21487;&#21152;&#24615;&#27169;&#22411;&#21644;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#20989;&#25968;&#65288;NTK&#65289;&#31561;&#20215;&#27861;&#35777;&#26126;&#20102;&#29305;&#24449;&#30697;&#38453;&#30340;&#39030;&#23618;&#29305;&#24449;&#31354;&#38388;&#26356;&#21152;&#31283;&#20581;&#12290;&#25105;&#20204;&#22312;CIFAR-10&#21644;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#29992;&#20110;&#20960;&#20010;&#31283;&#20581;&#24615;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive test-time defenses are used to improve the robustness of deep neural networks to adversarial examples. However, existing methods significantly increase the inference time due to additional optimization on the model parameters or the input at test time. In this work, we propose a novel adaptive test-time defense strategy that is easy to integrate with any existing (robust) training procedure without additional test-time computation. Based on the notion of robustness of features that we present, the key idea is to project the trained models to the most robust feature space, thereby reducing the vulnerability to adversarial attacks in non-robust directions. We theoretically show that the top eigenspace of the feature matrix are more robust for a generalized additive model and support our argument for a large width neural network with the Neural Tangent Kernel (NTK) equivalence. We conduct extensive experiments on CIFAR-10 and CIFAR-100 datasets for several robustness benchmarks, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#39640;&#25928;&#20869;&#28857;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#36951;&#25022;&#26368;&#23567;&#21270;&#26041;&#38754;&#20855;&#26377;&#26368;&#23567;&#21487;&#33021;&#20540;$O(\sqrt{T \log T})$&#65292;&#24182;&#19988;&#26159;&#33258;&#36866;&#24212;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.11668</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#39640;&#25928;&#20869;&#28857;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Efficient Interior-Point Method for Online Convex Optimization. (arXiv:2307.11668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#39640;&#25928;&#20869;&#28857;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#36951;&#25022;&#26368;&#23567;&#21270;&#26041;&#38754;&#20855;&#26377;&#26368;&#23567;&#21487;&#33021;&#20540;$O(\sqrt{T \log T})$&#65292;&#24182;&#19988;&#26159;&#33258;&#36866;&#24212;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#26032;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;$T$&#20010;&#26102;&#38388;&#27573;&#21518;&#30340;&#36951;&#25022;&#20026;$O(\sqrt{T \log T})$&#65292;&#36825;&#26159;&#38500;&#23545;&#25968;&#39033;&#22806;&#30340;&#26368;&#23567;&#21487;&#33021;&#20540;&#12290;&#27492;&#22806;&#65292;&#26032;&#31639;&#27861;&#26159;&#33258;&#36866;&#24212;&#30340;&#65292;&#36951;&#25022;&#30028;&#19981;&#20165;&#36866;&#29992;&#20110;&#26102;&#38388;&#27573;$1,\ldots,T$&#65292;&#36824;&#36866;&#29992;&#20110;&#27599;&#20010;&#23376;&#21306;&#38388;$s,s+1,\ldots,t$&#12290;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#19982;&#26032;&#24341;&#20837;&#30340;&#20869;&#28857;&#31639;&#27861;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#30456;&#21305;&#37197;&#65306;&#22312;$n$&#32500;&#31354;&#38388;&#20013;&#65292;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#26032;&#31639;&#27861;&#23454;&#36136;&#19978;&#35299;&#20915;&#20102;&#19968;&#20010;$n$&#38454;&#32447;&#24615;&#26041;&#31243;&#32452;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;$n$&#32500;&#31354;&#38388;&#20013;&#27714;&#35299;&#19968;&#20123;&#32422;&#26463;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new algorithm for regret minimization in online convex optimization is described. The regret of the algorithm after $T$ time periods is $O(\sqrt{T \log T})$ - which is the minimum possible up to a logarithmic term. In addition, the new algorithm is adaptive, in the sense that the regret bounds hold not only for the time periods $1,\ldots,T$ but also for every sub-interval $s,s+1,\ldots,t$. The running time of the algorithm matches that of newly introduced interior point algorithms for regret minimization: in $n$-dimensional space, during each iteration the new algorithm essentially solves a system of linear equations of order $n$, rather than solving some constrained convex optimization problem in $n$ dimensions and possibly many constraints.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;GPT-4&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#36866;&#24212;CLIP&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;CLIP&#30340;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#22312;&#19987;&#38376;&#32454;&#31890;&#24230;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20102;&#36739;&#22823;&#30340;0-shot&#36801;&#31227;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.11661</link><description>&lt;p&gt;
&#29992;GPT-4&#22686;&#24378;CLIP&#65306;&#21033;&#29992;&#35270;&#35273;&#25551;&#36848;&#20316;&#20026;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts. (arXiv:2307.11661v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;GPT-4&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#36866;&#24212;CLIP&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;CLIP&#30340;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#22312;&#19987;&#38376;&#32454;&#31890;&#24230;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20102;&#36739;&#22823;&#30340;0-shot&#36801;&#31227;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22914;CLIP&#22312;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#33391;&#22909;&#24615;&#33021;&#65292;&#20174;&#32780;&#38761;&#26032;&#20102;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#12290;VLMs&#36890;&#36807;&#35774;&#35745;&#19982;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#25552;&#31034;&#26469;0-shot&#36866;&#24212;&#19979;&#28216;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#25552;&#31034;&#24037;&#31243;&#21033;&#29992;&#20102;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#39564;&#35777;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#24847;&#21619;&#30528;&#23427;&#20204;&#21487;&#20197;&#29992;&#20316;&#20808;&#36827;&#30340;&#20114;&#32852;&#32593;&#25628;&#32034;&#24037;&#20855;&#12290;&#23427;&#20204;&#36824;&#21487;&#20197;&#34987;&#25805;&#20316;&#20197;&#25552;&#20379;&#20219;&#20309;&#32467;&#26500;&#21270;&#30340;&#35270;&#35273;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;GPT-4&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#36866;&#24212;CLIP&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;CLIP&#30340;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#19987;&#38376;&#32454;&#31890;&#24230;&#25968;&#25454;&#38598;&#65288;&#22914;EuroSAT&#65288;~7&#65285;&#65289;&#12289;DTD&#65288;~7&#65285;&#65289;&#12289;SUN397&#65288;~4.6&#65285;&#65289;&#21644;CUB&#65288;~3.3&#65285;&#65289;&#65289;&#19978;&#26174;&#31034;&#20986;&#20102;&#36739;&#22823;&#30340;0-shot&#36801;&#31227;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#23569;&#37327;&#26679;&#26412;&#36866;&#37197;&#22120;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#36873;&#25321;&#26368;&#20339;&#30340;s
&lt;/p&gt;
&lt;p&gt;
Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have revolutionized visual representation learning by providing good performance on downstream datasets. VLMs are 0-shot adapted to a downstream dataset by designing prompts that are relevant to the dataset. Such prompt engineering makes use of domain expertise and a validation dataset. Meanwhile, recent developments in generative pretrained models like GPT-4 mean they can be used as advanced internet search tools. They can also be manipulated to provide visual information in any structure. In this work, we show that GPT-4 can be used to generate text that is visually descriptive and how this can be used to adapt CLIP to downstream tasks. We show considerable improvements in 0-shot transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD (~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt. We also design a simple few-shot adapter that learns to choose the best possible s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#24378;&#30423;&#21453;&#39304;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#65292;&#33021;&#20934;&#30830;&#35780;&#20272;&#22870;&#21169;&#19982;&#31995;&#32479;&#20581;&#24247;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.11655</link><description>&lt;p&gt;
&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bandits with Deterministically Evolving States. (arXiv:2307.11655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#24378;&#30423;&#21453;&#39304;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#65292;&#33021;&#20934;&#30830;&#35780;&#20272;&#22870;&#21169;&#19982;&#31995;&#32479;&#20581;&#24247;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#19982;&#24378;&#30423;&#21453;&#39304;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#30830;&#23450;&#24615;&#28436;&#21270;&#21644;&#19981;&#21487;&#35266;&#27979;&#30340;&#29366;&#24577;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20027;&#35201;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#30340;&#23398;&#20064;&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#22312;&#27599;&#19968;&#36718;&#33719;&#24471;&#30340;&#22870;&#21169;&#26159;&#36873;&#25321;&#34892;&#21160;&#30340;&#30701;&#26399;&#22870;&#21169;&#21644;&#31995;&#32479;&#30340;&#8220;&#20581;&#24247;&#8221;&#31243;&#24230;&#65288;&#21363;&#36890;&#36807;&#20854;&#29366;&#24577;&#27979;&#37327;&#65289;&#30340;&#20989;&#25968;&#12290;&#20363;&#22914;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#24179;&#21488;&#20174;&#29992;&#25143;&#23545;&#29305;&#23450;&#31867;&#22411;&#20869;&#23481;&#30340;&#21442;&#19982;&#20013;&#33719;&#24471;&#30340;&#22870;&#21169;&#19981;&#20165;&#21462;&#20915;&#20110;&#20855;&#20307;&#20869;&#23481;&#30340;&#22266;&#26377;&#29305;&#24449;&#65292;&#36824;&#21462;&#20915;&#20110;&#29992;&#25143;&#19982;&#24179;&#21488;&#19978;&#20854;&#20182;&#31867;&#22411;&#20869;&#23481;&#20114;&#21160;&#21518;&#20854;&#20559;&#22909;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#955;&#8712;[0,1]&#65288;&#20363;&#22914;&#65292;&#29992;&#25143;&#30340;&#20559;&#22909;&#22240;&#20808;&#21069;&#20869;&#23481;&#28040;&#36153;&#32780;&#24555;&#36895;&#21464;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a model for learning with bandit feedback while accounting for deterministically evolving and unobservable states that we call Bandits with Deterministically Evolving States. The workhorse applications of our model are learning for recommendation systems and learning for online ads. In both cases, the reward that the algorithm obtains at each round is a function of the short-term reward of the action chosen and how ``healthy'' the system is (i.e., as measured by its state). For example, in recommendation systems, the reward that the platform obtains from a user's engagement with a particular type of content depends not only on the inherent features of the specific content, but also on how the user's preferences have evolved as a result of interacting with other types of content on the platform. Our general model accounts for the different rate $\lambda \in [0,1]$ at which the state evolves (e.g., how fast a user's preferences shift as a result of previous content consumption
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Kronecker&#22270;&#30340;&#21487;&#25193;&#23637;&#22810;&#26234;&#33021;&#20307;&#25216;&#33021;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#29366;&#24577;&#36716;&#25442;&#22270;&#30340;Fiedler&#21521;&#37327;&#65292;&#30452;&#25509;&#35745;&#31639;&#20855;&#26377;&#21327;&#20316;&#25506;&#32034;&#34892;&#20026;&#30340;&#22810;&#26234;&#33021;&#20307;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11629</link><description>&lt;p&gt;
&#22522;&#20110;Kronecker&#22270;&#30340;&#21487;&#25193;&#23637;&#22810;&#26234;&#33021;&#20307;&#25216;&#33021;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Scalable Multi-agent Skill Discovery based on Kronecker Graphs. (arXiv:2307.11629v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Kronecker&#22270;&#30340;&#21487;&#25193;&#23637;&#22810;&#26234;&#33021;&#20307;&#25216;&#33021;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#29366;&#24577;&#36716;&#25442;&#22270;&#30340;Fiedler&#21521;&#37327;&#65292;&#30452;&#25509;&#35745;&#31639;&#20855;&#26377;&#21327;&#20316;&#25506;&#32034;&#34892;&#20026;&#30340;&#22810;&#26234;&#33021;&#20307;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#33021;&#21457;&#29616;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24050;&#32463;&#29992;&#20110;&#25913;&#21892;&#21333;&#26234;&#33021;&#20307;&#22330;&#26223;&#19979;&#31232;&#30095;&#22870;&#21169;&#20449;&#21495;&#30340;&#25506;&#32034;&#65292;&#36890;&#36807;&#36830;&#25509;&#29366;&#24577;&#36716;&#25442;&#22270;&#30340;Fiedler&#21521;&#37327;&#25152;&#25552;&#20379;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#26368;&#36828;&#30340;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#65292;&#30001;&#20110;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#29616;&#26377;&#30740;&#31350;&#20173;&#20381;&#36182;&#20110;&#21333;&#26234;&#33021;&#20307;&#25216;&#33021;&#21457;&#29616;&#65292;&#35201;&#20040;&#21464;&#24471;&#38590;&#20197;&#23454;&#29616;&#65292;&#35201;&#20040;&#26080;&#27861;&#30452;&#25509;&#21457;&#29616;&#25913;&#21892;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#36830;&#36890;&#24615;&#30340;&#32852;&#21512;&#25216;&#33021;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#30452;&#25509;&#35745;&#31639;&#20855;&#26377;&#21327;&#20316;&#25506;&#32034;&#34892;&#20026;&#30340;&#22810;&#26234;&#33021;&#20307;&#25216;&#33021;&#65292;&#21516;&#26102;&#20139;&#21463;&#21040;&#20998;&#35299;&#30340;&#20415;&#21033;&#24615;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#36817;&#20284;&#20026;&#19968;&#20010;Kronecker&#22270;&#65292;&#22522;&#20110;&#27492;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#21508;&#20010;&#26234;&#33021;&#20307;&#30340;&#36716;&#25442;&#22270;&#30340;&#25289;&#26222;&#25289;&#26031;&#35889;&#30452;&#25509;&#20272;&#35745;&#20854;Fiedler&#21521;&#37327;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#32771;&#34385;&#21040;&#30452;&#25509;&#35745;&#31639;&#25289;&#26222;&#25289;&#26031;&#35889;&#22312;&#35745;&#31639;&#19978;&#30340;&#22797;&#26434;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
Covering skill (a.k.a., option) discovery has been developed to improve the exploration of RL in single-agent scenarios with sparse reward signals, through connecting the most distant states in the embedding space provided by the Fiedler vector of the state transition graph. Given that joint state space grows exponentially with the number of agents in multi-agent systems, existing researches still relying on single-agent option discovery either become prohibitive or fail to directly discover joint options that improve the connectivity of the joint state space. In this paper, we show how to directly compute multi-agent options with collaborative exploratory behaviors while still enjoying the ease of decomposition. Our key idea is to approximate the joint state space as a Kronecker graph, based on which we can directly estimate its Fiedler vector using the Laplacian spectrum of individual agents' transition graphs. Further, considering that directly computing the Laplacian spectrum is in
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OMIGA&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#38544;&#24335;&#30340;&#20840;&#23616;&#21040;&#23616;&#37096;&#20215;&#20540;&#27491;&#21017;&#21270;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#20248;&#38597;&#22320;&#26725;&#25509;&#20102;&#22810;&#26234;&#33021;&#20307;&#20215;&#20540;&#20998;&#35299;&#21644;&#31574;&#30053;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.11620</link><description>&lt;p&gt;
&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#19982;&#38544;&#24335;&#20840;&#23616;&#21040;&#23616;&#37096;&#20215;&#20540;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Offline Multi-Agent Reinforcement Learning with Implicit Global-to-Local Value Regularization. (arXiv:2307.11620v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11620
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OMIGA&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#38544;&#24335;&#30340;&#20840;&#23616;&#21040;&#23616;&#37096;&#20215;&#20540;&#27491;&#21017;&#21270;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#20248;&#38597;&#22320;&#26725;&#25509;&#20102;&#22810;&#26234;&#33021;&#20307;&#20215;&#20540;&#20998;&#35299;&#21644;&#31574;&#30053;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30001;&#20110;&#20854;&#22312;&#26080;&#38656;&#29615;&#22659;&#20132;&#20114;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#31574;&#30053;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;&#22312;&#21333;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#24222;&#22823;&#30340;&#32852;&#21512;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#21644;&#32039;&#23494;&#32806;&#21512;&#30340;&#22810;&#26234;&#33021;&#20307;&#34892;&#20026;&#20026;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;&#22686;&#21152;&#20102;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31163;&#32447;MARL&#30740;&#31350;&#20165;&#22312;&#21333;&#20010;&#26234;&#33021;&#20307;&#19978;&#24212;&#29992;&#19982;&#31163;&#32447;&#25968;&#25454;&#30456;&#20851;&#30340;&#27491;&#21017;&#21270;&#65292;&#32780;&#26410;&#23436;&#20840;&#32771;&#34385;&#20840;&#23616;&#23618;&#38754;&#19978;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OMIGA&#65292;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20855;&#26377;&#38544;&#24335;&#20840;&#23616;&#21040;&#23616;&#37096;&#20215;&#20540;&#27491;&#21017;&#21270;&#12290;OMIGA&#25552;&#20379;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#65292;&#23558;&#20840;&#23616;&#23618;&#38754;&#30340;&#20215;&#20540;&#27491;&#21017;&#21270;&#36716;&#21270;&#20026;&#31561;&#25928;&#30340;&#38544;&#24335;&#23616;&#37096;&#20215;&#20540;&#27491;&#21017;&#21270;&#65292;&#24182;&#21516;&#26102;&#23454;&#29616;&#26679;&#26412;&#20869;&#23398;&#20064;&#65292;&#20174;&#32780;&#20248;&#38597;&#22320;&#26725;&#25509;&#20102;&#22810;&#26234;&#33021;&#20307;&#20215;&#20540;&#20998;&#35299;&#21644;&#31574;&#30053;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) has received considerable attention in recent years due to its attractive capability of learning policies from offline datasets without environmental interactions. Despite some success in the single-agent setting, offline multi-agent RL (MARL) remains to be a challenge. The large joint state-action space and the coupled multi-agent behaviors pose extra complexities for offline policy optimization. Most existing offline MARL studies simply apply offline data-related regularizations on individual agents, without fully considering the multi-agent system at the global level. In this work, we present OMIGA, a new offline m ulti-agent RL algorithm with implicit global-to-local v alue regularization. OMIGA provides a principled framework to convert global-level value regularization into equivalent implicit local value regularizations and simultaneously enables in-sample learning, thus elegantly bridging multi-agent value decomposition and policy learning wi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#20581;&#30340;&#20840;&#24322;&#27493;&#26041;&#27861;&#65288;R-FAST&#65289;&#65292;&#22312;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#23436;&#32654;&#21516;&#27493;&#30340;&#20302;&#25928;&#24615;&#21644;&#19981;&#21487;&#33021;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#37319;&#29992;&#40065;&#26834;&#30340;&#26799;&#24230;&#36319;&#36394;&#31574;&#30053;&#21644;&#28789;&#27963;&#30340;&#36890;&#20449;&#26550;&#26500;&#65292;&#28040;&#38500;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#25968;&#25454;&#21253;&#20002;&#22833;&#30340;&#24433;&#21709;&#65292;&#24182;&#23454;&#29616;&#20102;&#26399;&#26395;&#30340;&#37051;&#22495;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2307.11617</link><description>&lt;p&gt;
&#24378;&#20581;&#30340;&#20840;&#24322;&#27493;&#26041;&#27861;&#29992;&#20110;&#36890;&#29992;&#26550;&#26500;&#19978;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Robust Fully-Asynchronous Methods for Distributed Training over General Architecture. (arXiv:2307.11617v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11617
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#20581;&#30340;&#20840;&#24322;&#27493;&#26041;&#27861;&#65288;R-FAST&#65289;&#65292;&#22312;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#23436;&#32654;&#21516;&#27493;&#30340;&#20302;&#25928;&#24615;&#21644;&#19981;&#21487;&#33021;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#37319;&#29992;&#40065;&#26834;&#30340;&#26799;&#24230;&#36319;&#36394;&#31574;&#30053;&#21644;&#28789;&#27963;&#30340;&#36890;&#20449;&#26550;&#26500;&#65292;&#28040;&#38500;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#25968;&#25454;&#21253;&#20002;&#22833;&#30340;&#24433;&#21709;&#65292;&#24182;&#23454;&#29616;&#20102;&#26399;&#26395;&#30340;&#37051;&#22495;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#23436;&#32654;&#30340;&#21516;&#27493;&#26159;&#20302;&#25928;&#29978;&#33267;&#19981;&#21487;&#33021;&#30340;&#65292;&#30001;&#20110;&#24310;&#36831;&#12289;&#25968;&#25454;&#20002;&#22833;&#21644;&#24310;&#36831;&#36739;&#22823;&#30340;&#35774;&#22791;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#20581;&#30340;&#20840;&#24322;&#27493;&#38543;&#26426;&#26799;&#24230;&#36319;&#36394;&#26041;&#27861;&#65288;R-FAST&#65289;&#65292;&#20854;&#20013;&#27599;&#20010;&#35774;&#22791;&#20197;&#33258;&#24049;&#30340;&#36895;&#24230;&#36827;&#34892;&#26412;&#22320;&#35745;&#31639;&#21644;&#36890;&#20449;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#24418;&#24335;&#30340;&#21516;&#27493;&#12290;&#19982;&#29616;&#26377;&#30340;&#24322;&#27493;&#20998;&#24067;&#24335;&#31639;&#27861;&#19981;&#21516;&#65292;R-FAST&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#35774;&#35745;&#33391;&#22909;&#30340;&#36741;&#21161;&#21464;&#37327;&#26469;&#36319;&#36394;&#21644;&#32531;&#20914;&#25972;&#20307;&#26799;&#24230;&#21521;&#37327;&#30340;&#40065;&#26834;&#26799;&#24230;&#36319;&#36394;&#31574;&#30053;&#65292;&#28040;&#38500;&#35774;&#22791;&#38388;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#20801;&#35768;&#25968;&#25454;&#21253;&#20002;&#22833;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20004;&#20010;&#29983;&#25104;&#26641;&#22270;&#36827;&#34892;&#36890;&#20449;&#65292;&#21482;&#35201;&#20004;&#32773;&#20849;&#20139;&#33267;&#23569;&#19968;&#20010;&#20849;&#21516;&#30340;&#26681;&#33410;&#28857;&#65292;&#23601;&#33021;&#23454;&#29616;&#28789;&#27963;&#30340;&#36890;&#20449;&#26550;&#26500;&#35774;&#35745;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;R-FAST&#23545;&#20110;&#24179;&#28369;&#21644;&#24378;&#20984;&#38382;&#39064;&#65292;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#30340;&#26399;&#26395;&#37051;&#22495;&#65292;&#24182;&#20855;&#26377;&#20960;&#20309;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perfect synchronization in distributed machine learning problems is inefficient and even impossible due to the existence of latency, package losses and stragglers. We propose a Robust Fully-Asynchronous Stochastic Gradient Tracking method (R-FAST), where each device performs local computation and communication at its own pace without any form of synchronization. Different from existing asynchronous distributed algorithms, R-FAST can eliminate the impact of data heterogeneity across devices and allow for packet losses by employing a robust gradient tracking strategy that relies on properly designed auxiliary variables for tracking and buffering the overall gradient vector. More importantly, the proposed method utilizes two spanning-tree graphs for communication so long as both share at least one common root, enabling flexible designs in communication architectures. We show that R-FAST converges in expectation to a neighborhood of the optimum with a geometric rate for smooth and strongly
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#37327;&#23376;&#33258;&#26059;&#38142;&#20013;&#65292;&#36890;&#36807;&#26368;&#20248;&#25511;&#21046;&#26102;&#38388;&#20381;&#36182;&#30340;&#8220;&#21464;&#20998;&#32416;&#32544;&#22686;&#24378;&#8221;&#22330;&#65288;VEEF&#65289;&#21487;&#25345;&#32493;&#35825;&#23548;&#32416;&#32544;&#30340;&#24377;&#36947;&#20256;&#25773;&#65292;&#30452;&#21040;&#36798;&#21040;&#39281;&#21644;&#12290;&#36825;&#19982;&#27809;&#26377;VEEF&#30340;&#24773;&#20917;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#32416;&#32544;&#22686;&#38271;&#22312;&#36798;&#21040;&#39281;&#21644;&#20043;&#21069;&#23601;&#20559;&#31163;&#20102;&#32447;&#24615;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2307.11609</link><description>&lt;p&gt;
&#25345;&#32493;&#30340;&#24377;&#36947;&#32416;&#32544;&#20256;&#25773;&#19982;&#37327;&#23376;&#33258;&#26059;&#38142;&#20013;&#30340;&#26368;&#20248;&#25511;&#21046;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Persistent Ballistic Entanglement Spreading with Optimal Control in Quantum Spin Chains. (arXiv:2307.11609v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#37327;&#23376;&#33258;&#26059;&#38142;&#20013;&#65292;&#36890;&#36807;&#26368;&#20248;&#25511;&#21046;&#26102;&#38388;&#20381;&#36182;&#30340;&#8220;&#21464;&#20998;&#32416;&#32544;&#22686;&#24378;&#8221;&#22330;&#65288;VEEF&#65289;&#21487;&#25345;&#32493;&#35825;&#23548;&#32416;&#32544;&#30340;&#24377;&#36947;&#20256;&#25773;&#65292;&#30452;&#21040;&#36798;&#21040;&#39281;&#21644;&#12290;&#36825;&#19982;&#27809;&#26377;VEEF&#30340;&#24773;&#20917;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#32416;&#32544;&#22686;&#38271;&#22312;&#36798;&#21040;&#39281;&#21644;&#20043;&#21069;&#23601;&#20559;&#31163;&#20102;&#32447;&#24615;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32416;&#32544;&#20256;&#25773;&#26159;&#29702;&#35299;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#22312;&#24179;&#34913;&#21644;&#38750;&#24179;&#34913;&#29366;&#24577;&#19979;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#8220;&#21464;&#20998;&#32416;&#32544;&#22686;&#24378;&#8221;&#22330;&#65288;VEEF&#65289;&#21487;&#22312;&#37327;&#23376;&#33258;&#26059;&#38142;&#20013;&#25345;&#32493;&#22320;&#35825;&#23548;&#32416;&#32544;&#30340;&#24377;&#36947;&#20256;&#25773;&#12290;VEEF&#26159;&#26102;&#38388;&#20381;&#36182;&#30340;&#65292;&#36890;&#36807;&#26368;&#20248;&#25511;&#21046;&#20197;&#26368;&#22823;&#21270;&#26368;&#32456;&#24577;&#30340;&#20108;&#20998;&#32416;&#32544;&#29109;&#65288;EE&#65289;&#12290;&#36825;&#31181;&#32447;&#24615;&#22686;&#38271;&#25345;&#32493;&#21040;EE&#36798;&#21040;&#30495;&#23454;&#39281;&#21644;&#24230;$\tilde{S} = - \log_{2} 2^{-\frac{N}{2}}=\frac{N}{2}$&#65292;&#20854;&#20013;$N$&#26159;&#33258;&#26059;&#24635;&#25968;&#12290;&#22312;&#26102;&#38388;$t \leq \frac{N}{2v}$&#26102;&#65292;EE&#28385;&#36275;$S(t) = v t$&#65292;&#20854;&#20013;$v$&#26159;&#36895;&#24230;&#12290;&#36825;&#20123;&#32467;&#26524;&#19982;&#27809;&#26377;VEEF&#30340;&#34892;&#20026;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#27809;&#26377;VEEF&#26102;&#65292;EE&#36890;&#24120;&#22312;&#38271;&#26102;&#38388;&#26497;&#38480;&#19979;&#36235;&#20110;&#19968;&#20010;&#27425;&#39281;&#21644;&#20540;&#65292;&#21363;Page&#20540;$\tilde{S}_{P} =\tilde{S} - \frac{1}{2\ln{2}}$&#65292;&#19988;&#32416;&#32544;&#22686;&#38271;&#22312;&#36798;&#21040;Page&#20540;&#20043;&#21069;&#23601;&#20559;&#31163;&#20102;&#32447;&#24615;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entanglement propagation provides a key routine to understand quantum many-body dynamics in and out of equilibrium. In this work, we uncover that the ``variational entanglement-enhancing'' field (VEEF) robustly induces a persistent ballistic spreading of entanglement in quantum spin chains. The VEEF is time dependent, and is optimally controlled to maximize the bipartite entanglement entropy (EE) of the final state. Such a linear growth persists till the EE reaches the genuine saturation $\tilde{S} = - \log_{2} 2^{-\frac{N}{2}}=\frac{N}{2}$ with $N$ the total number of spins. The EE satisfies $S(t) = v t$ for the time $t \leq \frac{N}{2v}$, with $v$ the velocity. These results are in sharp contrast with the behaviors without VEEF, where the EE generally approaches a sub-saturation known as the Page value $\tilde{S}_{P} =\tilde{S} - \frac{1}{2\ln{2}}$ in the long-time limit, and the entanglement growth deviates from being linear before the Page value is reached. The dependence between t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30830;&#23450;&#26368;&#23567;&#21442;&#25968;&#38598;&#65292;&#26377;&#25928;&#25551;&#36848;&#38543;&#26426;&#36807;&#31243;&#21160;&#21147;&#23398;&#65292;&#24182;&#29983;&#25104;&#33021;&#20934;&#30830;&#22797;&#21046;&#39044;&#26399;&#38543;&#26426;&#34892;&#20026;&#30340;&#26032;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2307.11608</link><description>&lt;p&gt;
&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#23398;&#20064;&#38543;&#26426;&#36807;&#31243;&#30340;&#26368;&#23567;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning minimal representations of stochastic processes with variational autoencoders. (arXiv:2307.11608v1 [cond-mat.soft])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30830;&#23450;&#26368;&#23567;&#21442;&#25968;&#38598;&#65292;&#26377;&#25928;&#25551;&#36848;&#38543;&#26426;&#36807;&#31243;&#21160;&#21147;&#23398;&#65292;&#24182;&#29983;&#25104;&#33021;&#20934;&#30830;&#22797;&#21046;&#39044;&#26399;&#38543;&#26426;&#34892;&#20026;&#30340;&#26032;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#36807;&#31243;&#22312;&#31185;&#23398;&#20013;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#24191;&#27867;&#29992;&#20110;&#27169;&#25311;&#21508;&#31181;&#33258;&#28982;&#29616;&#35937;&#12290;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#38543;&#26426;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#23427;&#20204;&#24456;&#38590;&#36827;&#34892;&#34920;&#24449;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#26377;&#25928;&#25551;&#36848;&#38543;&#26426;&#36807;&#31243;&#21160;&#21147;&#23398;&#25152;&#38656;&#30340;&#26368;&#23567;&#21442;&#25968;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#22312;&#25193;&#23637;&#30340;&#946;-&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#19978;&#12290;&#36890;&#36807;&#19982;&#20856;&#22411;&#25193;&#25955;&#27169;&#22411;&#30456;&#23545;&#24212;&#30340;&#27169;&#25311;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#25552;&#21462;&#33021;&#20934;&#30830;&#25551;&#36848;&#36825;&#20123;&#21160;&#21147;&#23398;&#30340;&#26368;&#23567;&#30456;&#20851;&#21442;&#25968;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#24544;&#23454;&#22797;&#21046;&#39044;&#26399;&#38543;&#26426;&#34892;&#20026;&#30340;&#26032;&#36712;&#36857;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#33021;&#22815;&#33258;&#21160;&#21457;&#29616;&#25551;&#36848;&#38543;&#26426;&#36807;&#31243;&#30340;&#26410;&#30693;&#21442;&#25968;&#65292;&#20174;&#32780;&#22686;&#36827;&#23545;&#21508;&#20010;&#39046;&#22495;&#20013;&#22797;&#26434;&#29616;&#35937;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic processes have found numerous applications in science, as they are broadly used to model a variety of natural phenomena. Due to their intrinsic randomness and uncertainty, they are however difficult to characterize. Here, we introduce an unsupervised machine learning approach to determine the minimal set of parameters required to effectively describe the dynamics of a stochastic process. Our method builds upon an extended $\beta$-variational autoencoder architecture. By means of simulated datasets corresponding to paradigmatic diffusion models, we showcase its effectiveness in extracting the minimal relevant parameters that accurately describe these dynamics. Furthermore, the method enables the generation of new trajectories that faithfully replicate the expected stochastic behavior. Overall, our approach enables for the autonomous discovery of unknown parameters describing stochastic processes, hence enhancing our comprehension of complex phenomena across various fields.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#26367;&#20195;&#29305;&#24449;&#36873;&#25321;&#30340;&#27010;&#24565;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32422;&#26463;&#23450;&#20041;&#20102;&#26367;&#20195;&#29305;&#24449;&#38598;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#25511;&#21046;&#26367;&#20195;&#30340;&#25968;&#37327;&#21644;&#24046;&#24322;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#38382;&#39064;&#30340;NP-hard&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#20256;&#32479;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#20316;&#20026;&#30446;&#26631;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#26367;&#20195;&#29305;&#24449;&#38598;&#30830;&#23454;&#21487;&#20197;&#20855;&#26377;&#39640;&#39044;&#27979;&#36136;&#37327;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#20960;&#20010;&#24433;&#21709;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2307.11607</link><description>&lt;p&gt;
&#21033;&#29992;&#26367;&#20195;&#29305;&#24449;&#36873;&#25321;&#25214;&#21040;&#26368;&#20248;&#30340;&#22810;&#26679;&#29305;&#24449;&#38598;
&lt;/p&gt;
&lt;p&gt;
Finding Optimal Diverse Feature Sets with Alternative Feature Selection. (arXiv:2307.11607v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#26367;&#20195;&#29305;&#24449;&#36873;&#25321;&#30340;&#27010;&#24565;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32422;&#26463;&#23450;&#20041;&#20102;&#26367;&#20195;&#29305;&#24449;&#38598;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#25511;&#21046;&#26367;&#20195;&#30340;&#25968;&#37327;&#21644;&#24046;&#24322;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#38382;&#39064;&#30340;NP-hard&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#20256;&#32479;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#20316;&#20026;&#30446;&#26631;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#26367;&#20195;&#29305;&#24449;&#38598;&#30830;&#23454;&#21487;&#20197;&#20855;&#26377;&#39640;&#39044;&#27979;&#36136;&#37327;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#20960;&#20010;&#24433;&#21709;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#26159;&#33719;&#21462;&#23567;&#22411;&#12289;&#21487;&#35299;&#37322;&#19988;&#39640;&#31934;&#24230;&#39044;&#27979;&#27169;&#22411;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#36890;&#24120;&#21482;&#33021;&#24471;&#21040;&#19968;&#20010;&#29305;&#24449;&#38598;&#65292;&#36825;&#22312;&#26576;&#20123;&#22330;&#26223;&#19979;&#21487;&#33021;&#19981;&#36275;&#22815;&#12290;&#20363;&#22914;&#65292;&#29992;&#25143;&#21487;&#33021;&#23545;&#23547;&#25214;&#20855;&#26377;&#30456;&#20284;&#39044;&#27979;&#36136;&#37327;&#20294;&#25552;&#20379;&#19981;&#21516;&#25968;&#25454;&#35299;&#37322;&#30340;&#26367;&#20195;&#29305;&#24449;&#38598;&#24863;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26367;&#20195;&#29305;&#24449;&#36873;&#25321;&#65292;&#24182;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#32422;&#26463;&#23450;&#20041;&#20102;&#26367;&#20195;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#25143;&#21487;&#20197;&#25511;&#21046;&#26367;&#20195;&#30340;&#25968;&#37327;&#21644;&#24046;&#24322;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#24182;&#23637;&#31034;&#20102;&#20854;NP-hard&#24615;&#36136;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#20256;&#32479;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#20316;&#20026;&#30446;&#26631;&#38598;&#25104;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;30&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#26367;&#20195;&#29305;&#24449;&#36873;&#25321;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26367;&#20195;&#29305;&#24449;&#38598;&#30830;&#23454;&#21487;&#33021;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#36136;&#37327;&#65292;&#24182;&#20998;&#26512;&#20102;&#20960;&#20010;&#24433;&#21709;&#36825;&#19968;&#32467;&#26524;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature selection is popular for obtaining small, interpretable, yet highly accurate prediction models. Conventional feature-selection methods typically yield one feature set only, which might not suffice in some scenarios. For example, users might be interested in finding alternative feature sets with similar prediction quality, offering different explanations of the data. In this article, we introduce alternative feature selection and formalize it as an optimization problem. In particular, we define alternatives via constraints and enable users to control the number and dissimilarity of alternatives. Next, we analyze the complexity of this optimization problem and show NP-hardness. Further, we discuss how to integrate conventional feature-selection methods as objectives. Finally, we evaluate alternative feature selection with 30 classification datasets. We observe that alternative feature sets may indeed have high prediction quality, and we analyze several factors influencing this ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#31354;&#38388;&#38382;&#39064;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#39640;&#25928;&#35757;&#32451;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#30740;&#31350;CNNs&#22312;&#24213;&#23618;&#20449;&#21495;&#20026;&#22266;&#23450;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#36136;&#65292;&#21457;&#29616;&#22312;&#22266;&#23450;&#20449;&#21495;&#30340;&#23567;&#31383;&#21475;&#19978;&#35757;&#32451;&#30340;CNN&#21487;&#20197;&#22312;&#26356;&#22823;&#30340;&#31383;&#21475;&#19978;&#21462;&#24471;&#25509;&#36817;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2307.11588</link><description>&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#22266;&#23450;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#21487;&#36801;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
Transferability of Convolutional Neural Networks in Stationary Learning Tasks. (arXiv:2307.11588v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#31354;&#38388;&#38382;&#39064;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#39640;&#25928;&#35757;&#32451;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#30740;&#31350;CNNs&#22312;&#24213;&#23618;&#20449;&#21495;&#20026;&#22266;&#23450;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#36136;&#65292;&#21457;&#29616;&#22312;&#22266;&#23450;&#20449;&#21495;&#30340;&#23567;&#31383;&#21475;&#19978;&#35757;&#32451;&#30340;CNN&#21487;&#20197;&#22312;&#26356;&#22823;&#30340;&#31383;&#21475;&#19978;&#21462;&#24471;&#25509;&#36817;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30828;&#20214;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#37319;&#38598;&#30340;&#26368;&#26032;&#36827;&#23637;&#21152;&#36895;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#22312;&#38271;&#26102;&#38388;&#30340;&#30740;&#31350;&#20013;&#65292;&#22686;&#21152;&#27169;&#22411;&#22797;&#26434;&#24615;&#36890;&#24120;&#21487;&#20197;&#25552;&#21319;&#21508;&#31181;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36235;&#21183;&#27491;&#22312;&#21464;&#24471;&#19981;&#21487;&#25345;&#32493;&#65292;&#38656;&#35201;&#23547;&#25214;&#35745;&#31639;&#19978;&#26356;&#36731;&#37327;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#31354;&#38388;&#38382;&#39064;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#39640;&#25928;&#35757;&#32451;&#30340;&#26032;&#26694;&#26550;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;CNNs&#22312;&#24213;&#23618;&#20449;&#21495;&#20026;&#22266;&#23450;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#36136;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#22266;&#23450;&#20449;&#21495;&#30340;&#23567;&#31383;&#21475;&#19978;&#35757;&#32451;&#30340;CNN&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#26356;&#22823;&#30340;&#31383;&#21475;&#19978;&#21462;&#24471;&#25509;&#36817;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#25903;&#25345;&#20102;&#36825;&#19968;&#35266;&#28857;&#65292;&#24182;&#32473;&#20986;&#20102;&#24615;&#33021;&#38477;&#20302;&#30340;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#20004;&#20010;&#20219;&#21153;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#20998;&#26512;&#65306;&#22810;&#30446;&#26631;&#36319;&#36394;&#21644;&#25353;&#38656;&#31227;&#21160;&#22522;&#30784;&#35774;&#26045;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
Recent advances in hardware and big data acquisition have accelerated the development of deep learning techniques. For an extended period of time, increasing the model complexity has led to performance improvements for various tasks. However, this trend is becoming unsustainable and there is a need for alternative, computationally lighter methods. In this paper, we introduce a novel framework for efficient training of convolutional neural networks (CNNs) for large-scale spatial problems. To accomplish this we investigate the properties of CNNs for tasks where the underlying signals are stationary. We show that a CNN trained on small windows of such signals achieves a nearly performance on much larger windows without retraining. This claim is supported by our theoretical analysis, which provides a bound on the performance degradation. Additionally, we conduct thorough experimental analysis on two tasks: multi-target tracking and mobile infrastructure on demand. Our results show that the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35821;&#38899;&#21040;&#25991;&#26412;&#30340;&#27169;&#24577;&#36716;&#25442;&#26041;&#27861;&#65292;&#22312;MELD&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#22522;&#20110;&#35821;&#38899;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#36825;&#34920;&#26126;&#20102;&#27169;&#24577;&#36716;&#25442;&#22312;&#26367;&#20195;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.11584</link><description>&lt;p&gt;
&#24515;&#24577;&#36716;&#21464;&#65306;&#36890;&#36807;&#35821;&#38899;&#21040;&#25991;&#26412;&#30340;&#27169;&#24577;&#36716;&#25442;&#25552;&#39640;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
A Change of Heart: Improving Speech Emotion Recognition through Speech-to-Text Modality Conversion. (arXiv:2307.11584v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35821;&#38899;&#21040;&#25991;&#26412;&#30340;&#27169;&#24577;&#36716;&#25442;&#26041;&#27861;&#65292;&#22312;MELD&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#22522;&#20110;&#35821;&#38899;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#36825;&#34920;&#26126;&#20102;&#27169;&#24577;&#36716;&#25442;&#22312;&#26367;&#20195;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27169;&#24577;&#36716;&#25442;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#25552;&#39640;&#22312;MELD&#25968;&#25454;&#38598;&#19978;&#30340;&#24773;&#24863;&#35782;&#21035;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#23454;&#39564;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#31532;&#19968;&#20010;&#23454;&#39564;&#20351;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#21644;&#25991;&#26412;&#20998;&#31867;&#22120;&#23454;&#29616;&#30340;&#31216;&#20026;&#27169;&#24577;&#36716;&#25442;&#30340;&#26041;&#27861;&#65307;&#31532;&#20108;&#20010;&#23454;&#39564;&#20551;&#35774;&#26377;&#23436;&#32654;&#30340;ASR&#36755;&#20986;&#65292;&#24182;&#30740;&#31350;&#20102;&#27169;&#24577;&#36716;&#25442;&#23545;SER&#30340;&#24433;&#21709;&#65292;&#27492;&#26041;&#27861;&#31216;&#20026;&#27169;&#24577;&#36716;&#25442;++&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31532;&#19968;&#20010;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#32780;&#31532;&#20108;&#20010;&#26041;&#27861;&#22312;MELD&#25968;&#25454;&#38598;&#19978;&#30340;SER&#21152;&#26435;-F1&#65288;WF1&#65289;&#24471;&#20998;&#26041;&#38754;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#35821;&#38899;&#30340;&#26041;&#27861;&#12290;&#36825;&#39033;&#30740;&#31350;&#31361;&#20986;&#20102;&#27169;&#24577;&#36716;&#25442;&#22312;&#21487;&#20197;&#20351;&#29992;&#26367;&#20195;&#27169;&#24577;&#36827;&#34892;&#30340;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech Emotion Recognition (SER) is a challenging task. In this paper, we introduce a modality conversion concept aimed at enhancing emotion recognition performance on the MELD dataset. We assess our approach through two experiments: first, a method named Modality-Conversion that employs automatic speech recognition (ASR) systems, followed by a text classifier; second, we assume perfect ASR output and investigate the impact of modality conversion on SER, this method is called Modality-Conversion++. Our findings indicate that the first method yields substantial results, while the second method outperforms state-of-the-art (SOTA) speech-based approaches in terms of SER weighted-F1 (WF1) score on the MELD dataset. This research highlights the potential of modality conversion for tasks that can be conducted in alternative modalities.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FMT&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#36890;&#36807;&#26816;&#27979;&#24182;&#31227;&#38500;&#35757;&#32451;&#29992;&#20110;&#20174;&#36755;&#20837;&#20013;&#25552;&#21462;&#21518;&#38376;&#20449;&#24687;&#30340;&#21518;&#38376;&#29305;&#24449;&#22270;&#65292;&#20174;&#32780;&#26377;&#25928;&#38450;&#27490;&#21518;&#38376;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2307.11565</link><description>&lt;p&gt;
FMT: &#36890;&#36807;&#29305;&#24449;&#22270;&#27979;&#35797;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#31227;&#38500;&#21518;&#38376;&#29305;&#24449;&#22270;
&lt;/p&gt;
&lt;p&gt;
FMT: Removing Backdoor Feature Maps via Feature Map Testing in Deep Neural Networks. (arXiv:2307.11565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11565
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FMT&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#36890;&#36807;&#26816;&#27979;&#24182;&#31227;&#38500;&#35757;&#32451;&#29992;&#20110;&#20174;&#36755;&#20837;&#20013;&#25552;&#21462;&#21518;&#38376;&#20449;&#24687;&#30340;&#21518;&#38376;&#29305;&#24449;&#22270;&#65292;&#20174;&#32780;&#26377;&#25928;&#38450;&#27490;&#21518;&#38376;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#20851;&#38190;&#24212;&#29992;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21644;&#21307;&#23398;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#21518;&#38376;&#25915;&#20987;&#26159;&#36890;&#36807;&#21521;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#20013;&#28155;&#21152;&#20154;&#24037;&#27169;&#24335;&#23454;&#29616;&#30340;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#31574;&#30053;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#36870;&#21521;&#24037;&#31243;&#26469;&#22797;&#29616;&#25915;&#20987;&#32773;&#29983;&#25104;&#30340;&#21518;&#38376;&#35302;&#21457;&#22120;&#65292;&#24182;&#36890;&#36807;&#23558;&#35302;&#21457;&#22120;&#28155;&#21152;&#21040;&#36755;&#20837;&#20013;&#24182;&#20351;&#29992;&#30495;&#23454;&#26631;&#31614;&#24494;&#35843;&#27169;&#22411;&#26469;&#20462;&#22797;DNN&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#19968;&#26086;&#25915;&#20987;&#32773;&#29983;&#25104;&#30340;&#35302;&#21457;&#22120;&#22797;&#26434;&#32780;&#19988;&#19981;&#21487;&#35265;&#65292;&#38450;&#24481;&#32773;&#23601;&#26080;&#27861;&#25104;&#21151;&#22797;&#29616;&#35302;&#21457;&#22120;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;&#35302;&#21457;&#22120;&#27809;&#26377;&#34987;&#26377;&#25928;&#21435;&#38500;&#65292;DNN&#27169;&#22411;&#23558;&#26080;&#27861;&#34987;&#20462;&#22797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#24449;&#22270;&#27979;&#35797;&#65288;FMT&#65289;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#38450;&#24481;&#31574;&#30053;&#19981;&#21516;&#65292;&#23427;&#20204;&#19987;&#27880;&#20110;&#22797;&#29616;&#21518;&#38376;&#35302;&#21457;&#22120;&#65292;FMT&#35797;&#22270;&#26816;&#27979;&#35757;&#32451;&#29992;&#20110;&#20174;&#36755;&#20837;&#20013;&#25552;&#21462;&#21518;&#38376;&#20449;&#24687;&#30340;&#21518;&#38376;&#29305;&#24449;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have been widely used in many critical applications, such as autonomous vehicles and medical diagnosis. However, their security is threatened by backdoor attack, which is achieved by adding artificial patterns to specific training data. Existing defense strategies primarily focus on using reverse engineering to reproduce the backdoor trigger generated by attackers and subsequently repair the DNN model by adding the trigger into inputs and fine-tuning the model with ground-truth labels. However, once the trigger generated by the attackers is complex and invisible, the defender can not successfully reproduce the trigger. Consequently, the DNN model will not be repaired since the trigger is not effectively removed.  In this work, we propose Feature Map Testing~(FMT). Different from existing defense strategies, which focus on reproducing backdoor triggers, FMT tries to detect the backdoor feature maps, which are trained to extract backdoor information from the inputs. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20302;&#32500;&#34920;&#31034;&#65292;&#22312;El Ni\~no Southern Oscillation&#65288;ENSO&#65289;&#20013;&#21457;&#29616;&#20102;&#26032;&#30340;&#26497;&#31471;El Ni\~no&#31867;&#21035;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#19982;&#20856;&#22411;EP El Ni\~no&#19981;&#21516;&#12290;EP El Ni\~nos&#65292;CP La Ni\~nas&#21644;Extreme El Ni\~nos&#23545;&#36328;&#21313;&#24180;&#23610;&#24230;&#30340;ENSO&#26368;&#20855;&#24433;&#21709;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.11552</link><description>&lt;p&gt;
El Ni\~no Southern Oscillation&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
A multi-modal representation of El Ni\~no Southern Oscillation Diversity. (arXiv:2307.11552v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11552
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20302;&#32500;&#34920;&#31034;&#65292;&#22312;El Ni\~no Southern Oscillation&#65288;ENSO&#65289;&#20013;&#21457;&#29616;&#20102;&#26032;&#30340;&#26497;&#31471;El Ni\~no&#31867;&#21035;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#19982;&#20856;&#22411;EP El Ni\~no&#19981;&#21516;&#12290;EP El Ni\~nos&#65292;CP La Ni\~nas&#21644;Extreme El Ni\~nos&#23545;&#36328;&#21313;&#24180;&#23610;&#24230;&#30340;ENSO&#26368;&#20855;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
El Ni\~no Southern Oscillation (ENSO)&#36890;&#36807;&#36196;&#36947;&#22826;&#24179;&#27915;&#28201;&#26262;&#65288;El Ni\~no&#65289;&#21644;&#23506;&#20919;&#65288;La Ni\~na&#65289;&#28023;&#34920;&#28201;&#24230;&#24322;&#24120;&#65288;SSTA&#65289;&#30340;&#20132;&#26367;&#38454;&#27573;&#26469;&#25551;&#36848;&#12290;&#23613;&#31649;El Ni\~no&#21644;La Ni\~na&#26159;&#26126;&#30830;&#23450;&#20041;&#30340;&#27668;&#20505;&#27169;&#24335;&#65292;&#20294;&#27809;&#26377;&#20004;&#20010;&#20107;&#20214;&#26159;&#30456;&#21516;&#30340;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;ENSO&#22810;&#26679;&#24615;&#20027;&#35201;&#20197;SSTA&#23792;&#20540;&#30340;&#32463;&#24230;&#20301;&#32622;&#26469;&#25551;&#36848;&#65292;&#29992;&#20110;&#22312;&#19996;&#22826;&#24179;&#27915;&#65288;EP&#65289;&#21644;&#20013;&#22826;&#24179;&#27915;&#65288;CP&#65289;&#31867;&#22411;&#20013;&#23450;&#20041;&#21452;&#23792;&#20998;&#31867;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#22826;&#24179;&#27915;SSTA&#30340;&#20302;&#32500;&#34920;&#31034;&#26469;&#35777;&#26126;&#20108;&#36827;&#21046;&#20998;&#31867;&#25104;&#21592;&#23545;&#25551;&#36848;ENSO&#20107;&#20214;&#19981;&#21512;&#36866;&#12290;&#36890;&#36807;&#27169;&#31946;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#22235;&#20010;&#24050;&#30693;&#30340;ENSO&#31867;&#21035;&#65292;&#20197;&#21450;&#31532;&#20116;&#20010;&#31867;&#21035;&#65306;&#26497;&#31471;El Ni\~no&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26497;&#31471;El Ni\~nos&#22312;&#20854;&#24378;&#24230;&#21644;&#26102;&#38388;&#28436;&#21270;&#26041;&#38754;&#19982;&#20856;&#22411;&#30340;EP El Ni\~nos&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;CP La Ni\~nas&#65292;EP El Ni\~nos&#21644;Extreme El Ni\~nos&#23545;&#36328;&#21313;&#24180;&#23610;&#24230;&#30340;ENSO&#26368;&#20855;&#36129;&#29486;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The El Ni\~no-Southern Oscillation (ENSO) is characterized by alternating periods of warm (El Ni\~no) and cold (La Ni\~na) sea surface temperature anomalies (SSTA) in the equatorial Pacific. Although El Ni\~no and La Ni\~na are well-defined climate patterns, no two events are alike. To date, ENSO diversity has been described primarily in terms of the longitudinal location of peak SSTA, used to define a bimodal classification of events in Eastern Pacific (EP) and Central Pacific (CP) types. Here, we use low-dimensional representations of Pacific SSTAs to argue that binary categorical memberships are unsuitable to describe ENSO events. Using fuzzy unsupervised clustering, we recover the four known ENSO categories, along with a fifth category: an Extreme El Ni\~no. We show that Extreme El Ni\~nos differ both in their intensity and temporal evolution from canonical EP El Ni\~nos. We also find that CP La Ni\~nas, EP El Ni\~nos, and Extreme El Ni\~nos contribute the most to interdecadal ENSO
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#25176;&#21345;&#39532;&#20811;&#30913;&#25511;&#21046;&#20013;&#30340;&#20851;&#38190;&#32570;&#28857;&#65292;&#36890;&#36807;&#25913;&#36827;&#31639;&#27861;&#21644;&#35757;&#32451;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25511;&#21046;&#31934;&#24230;&#12289;&#20943;&#23567;&#31283;&#24577;&#35823;&#24046;&#21644;&#32553;&#30701;&#23398;&#20064;&#26032;&#20219;&#21153;&#25152;&#38656;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#27169;&#25311;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.11546</link><description>&lt;p&gt;
&#23454;&#29616;&#21487;&#34892;&#30340;&#25176;&#21345;&#39532;&#20811;&#30913;&#25511;&#21046;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards practical reinforcement learning for tokamak magnetic control. (arXiv:2307.11546v1 [physics.plasm-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11546
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#25176;&#21345;&#39532;&#20811;&#30913;&#25511;&#21046;&#20013;&#30340;&#20851;&#38190;&#32570;&#28857;&#65292;&#36890;&#36807;&#25913;&#36827;&#31639;&#27861;&#21644;&#35757;&#32451;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25511;&#21046;&#31934;&#24230;&#12289;&#20943;&#23567;&#31283;&#24577;&#35823;&#24046;&#21644;&#32553;&#30701;&#23398;&#20064;&#26032;&#20219;&#21153;&#25152;&#38656;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#27169;&#25311;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#23454;&#26102;&#25511;&#21046;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#31561;&#31163;&#23376;&#20307;&#30913;&#25511;&#21046;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#30340;&#21453;&#39304;&#25511;&#21046;&#26041;&#27861;&#30456;&#27604;&#65292;&#20173;&#23384;&#22312;&#26174;&#33879;&#30340;&#32570;&#28857;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;RL&#26041;&#27861;&#30340;&#20851;&#38190;&#32570;&#28857;&#65306;&#23454;&#29616;&#26356;&#39640;&#30340;&#23545;&#31561;&#31163;&#23376;&#20307;&#23646;&#24615;&#30340;&#25511;&#21046;&#31934;&#24230;&#65292;&#20943;&#23567;&#31283;&#24577;&#35823;&#24046;&#65292;&#24182;&#20943;&#23569;&#23398;&#20064;&#26032;&#20219;&#21153;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;\cite{degrave2022magnetic}&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#23545;&#20195;&#29702;&#32467;&#26500;&#21644;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#31639;&#27861;&#25913;&#36827;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#25311;&#32467;&#26524;&#65292;&#26174;&#31034;&#20102;&#24418;&#29366;&#31934;&#24230;&#25552;&#39640;&#20102;65&#65285;&#65292;&#23454;&#29616;&#20102;&#31561;&#31163;&#23376;&#20307;&#30005;&#27969;&#22312;&#38271;&#26399;&#20559;&#24046;&#19978;&#30340;&#22823;&#24133;&#20943;&#23569;&#65292;&#24182;&#19988;&#23558;&#23398;&#20064;&#26032;&#20219;&#21153;&#25152;&#38656;&#30340;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;3&#20493;&#25110;&#26356;&#22810;&#12290;&#25105;&#20204;&#20351;&#29992;&#21319;&#32423;&#21518;&#30340;RL&#25511;&#21046;&#22120;&#22312;TCV&#25176;&#21345;&#39532;&#20811;&#19978;&#36827;&#34892;&#20102;&#26032;&#30340;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#36798;&#21040;&#30340;&#27169;&#25311;&#32467;&#26524;&#65292;&#24182;&#25351;&#20986;...
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has shown promising results for real-time control systems, including the domain of plasma magnetic control. However, there are still significant drawbacks compared to traditional feedback control approaches for magnetic confinement. In this work, we address key drawbacks of the RL method; achieving higher control accuracy for desired plasma properties, reducing the steady-state error, and decreasing the required time to learn new tasks. We build on top of \cite{degrave2022magnetic}, and present algorithmic improvements to the agent architecture and training procedure. We present simulation results that show up to 65\% improvement in shape accuracy, achieve substantial reduction in the long-term bias of the plasma current, and additionally reduce the training time required to learn new tasks by a factor of 3 or more. We present new experiments using the upgraded RL-based controllers on the TCV tokamak, which validate the simulation results achieved, and point
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20801;&#35768;&#27169;&#22411;&#20999;&#20998;&#30340;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#35757;&#32451;&#24310;&#36831;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.11532</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#20999;&#20998;&#30340;&#20801;&#35768;&#36793;&#32536;&#32852;&#21512;&#23398;&#20064;&#30340;&#35757;&#32451;&#24310;&#36831;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Training Latency Minimization for Model-Splitting Allowed Federated Edge Learning. (arXiv:2307.11532v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20801;&#35768;&#27169;&#22411;&#20999;&#20998;&#30340;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#35757;&#32451;&#24310;&#36831;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#32531;&#35299;&#23458;&#25143;&#22312;&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#25152;&#38754;&#20020;&#30340;&#35745;&#31639;&#33021;&#21147;&#19981;&#36275;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#36793;&#32536;&#35745;&#31639;&#21644;&#27169;&#22411;&#20999;&#20998;&#25552;&#20986;&#20102;&#19968;&#20010;&#20801;&#35768;&#27169;&#22411;&#20999;&#20998;&#30340;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65288;SFL&#65289;&#65292;&#26088;&#22312;&#22312;&#19981;&#25439;&#22833;&#27979;&#35797;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#35757;&#32451;&#24310;&#36831;&#12290;&#22312;&#21516;&#27493;&#20840;&#23616;&#26356;&#26032;&#35774;&#32622;&#19979;&#65292;&#23436;&#25104;&#19968;&#36718;&#20840;&#23616;&#35757;&#32451;&#30340;&#24310;&#36831;&#21462;&#20915;&#20110;&#23458;&#25143;&#31471;&#23436;&#25104;&#26412;&#22320;&#35757;&#32451;&#20250;&#35805;&#30340;&#26368;&#22823;&#24310;&#36831;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#24310;&#36831;&#26368;&#23567;&#21270;&#38382;&#39064;&#65288;TLMP&#65289;&#34987;&#24314;&#27169;&#20026;&#19968;&#20010;&#26368;&#23567;&#21270;&#26368;&#22823;&#20540;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#22238;&#24402;&#26041;&#27861;&#26469;&#25311;&#21512;AI&#27169;&#22411;&#30340;&#20999;&#21106;&#23618;&#21644;&#20854;&#20182;&#21442;&#25968;&#20043;&#38388;&#30340;&#37327;&#21270;&#20851;&#31995;&#65292;&#20174;&#32780;&#23558;TLMP&#36716;&#21270;&#20026;&#19968;&#20010;&#36830;&#32493;&#38382;&#39064;&#12290;&#32771;&#34385;&#21040;TLMP&#20013;&#28041;&#21450;&#30340;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#21363;&#23458;&#25143;&#31471;&#30340;&#20999;&#21106;&#23618;&#36873;&#25321;&#38382;&#39064;&#21644;&#35745;&#31639;&#36164;&#28304;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
To alleviate the shortage of computing power faced by clients in training deep neural networks (DNNs) using federated learning (FL), we leverage the edge computing and split learning to propose a model-splitting allowed FL (SFL) framework, with the aim to minimize the training latency without loss of test accuracy. Under the synchronized global update setting, the latency to complete a round of global training is determined by the maximum latency for the clients to complete a local training session. Therefore, the training latency minimization problem (TLMP) is modelled as a minimizing-maximum problem. To solve this mixed integer nonlinear programming problem, we first propose a regression method to fit the quantitative-relationship between the cut-layer and other parameters of an AI-model, and thus, transform the TLMP into a continuous problem. Considering that the two subproblems involved in the TLMP, namely, the cut-layer selection problem for the clients and the computing resource 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#12289;&#38899;&#39057;&#29305;&#24449;&#20540;&#21644;&#25991;&#26412;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#26469;&#26816;&#27979;&#35270;&#39057;&#20869;&#23481;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;</title><link>http://arxiv.org/abs/2307.11519</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#22810;&#27169;&#24577;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Hate Speech Detection using Machine Learning. (arXiv:2307.11519v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#12289;&#38899;&#39057;&#29305;&#24449;&#20540;&#21644;&#25991;&#26412;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#26469;&#26816;&#27979;&#35270;&#39057;&#20869;&#23481;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#29992;&#25143;&#21644;&#23186;&#20307;&#20869;&#23481;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#36861;&#36394;&#38899;&#39057;&#21644;&#35270;&#39057;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#21464;&#24471;&#38750;&#24120;&#22256;&#38590;&#12290;&#23558;&#35270;&#39057;&#25110;&#38899;&#39057;&#36716;&#25442;&#20026;&#25991;&#26412;&#24182;&#19981;&#33021;&#20934;&#30830;&#26816;&#27979;&#21040;&#20167;&#24680;&#35328;&#35770;&#65292;&#22240;&#20026;&#20154;&#20204;&#26377;&#26102;&#20250;&#23558;&#20167;&#24680;&#35789;&#27719;&#20316;&#20026;&#24189;&#40664;&#25110;&#24841;&#24555;&#30340;&#24847;&#21619;&#20351;&#29992;&#65292;&#24182;&#22312;&#35270;&#39057;&#20013;&#20351;&#29992;&#19981;&#21516;&#30340;&#22768;&#35843;&#25110;&#23637;&#31034;&#19981;&#21516;&#30340;&#21160;&#20316;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#22823;&#22810;&#26159;&#22522;&#20110;&#21333;&#19968;&#27169;&#24577;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#12289;&#38899;&#39057;&#20013;&#30340;&#29305;&#24449;&#20540;&#12289;&#25991;&#26412;&#21644;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#26816;&#27979;&#35270;&#39057;&#20869;&#23481;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the continuous growth of internet users and media content, it is very hard to track down hateful speech in audio and video. Converting video or audio into text does not detect hate speech accurately as human sometimes uses hateful words as humorous or pleasant in sense and also uses different voice tones or show different action in the video. The state-ofthe-art hate speech detection models were mostly developed on a single modality. In this research, a combined approach of multimodal system has been proposed to detect hate speech from video contents by extracting feature images, feature values extracted from the audio, text and used machine learning and Natural language processing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21327;&#21464;&#37327;&#20559;&#31227;&#33258;&#36866;&#24212;&#20013;&#30340;&#19968;&#33324;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#24050;&#26377;&#32467;&#26524;&#24471;&#21040;&#20102;&#26032;&#30340;&#32467;&#26524;&#12290;&#22312;&#24369;&#24179;&#28369;&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#23454;&#29616;&#19982;&#26631;&#20934;&#30417;&#30563;&#23398;&#20064;&#20013;&#30456;&#21516;&#31934;&#24230;&#25152;&#38656;&#30340;&#26679;&#26412;&#37327;&#35201;&#27604;&#29616;&#26377;&#20998;&#26512;&#35777;&#26126;&#30340;&#23569;&#12290;</title><link>http://arxiv.org/abs/2307.11503</link><description>&lt;p&gt;
&#22312;&#21327;&#21464;&#37327;&#20559;&#31227;&#33258;&#36866;&#24212;&#20013;&#30340;&#19968;&#33324;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
General regularization in covariate shift adaptation. (arXiv:2307.11503v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21327;&#21464;&#37327;&#20559;&#31227;&#33258;&#36866;&#24212;&#20013;&#30340;&#19968;&#33324;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#24050;&#26377;&#32467;&#26524;&#24471;&#21040;&#20102;&#26032;&#30340;&#32467;&#26524;&#12290;&#22312;&#24369;&#24179;&#28369;&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#23454;&#29616;&#19982;&#26631;&#20934;&#30417;&#30563;&#23398;&#20064;&#20013;&#30456;&#21516;&#31934;&#24230;&#25152;&#38656;&#30340;&#26679;&#26412;&#37327;&#35201;&#27604;&#29616;&#26377;&#20998;&#26512;&#35777;&#26126;&#30340;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26679;&#26412;&#37325;&#21152;&#26435;&#26159;&#32416;&#27491;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHS)&#20013;&#30001;&#26410;&#26469;&#25968;&#25454;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#24341;&#36215;&#30340;&#26368;&#23567;&#20108;&#20056;&#23398;&#20064;&#31639;&#27861;&#38169;&#35823;&#30340;&#26368;&#24120;&#29992;&#26041;&#27861;&#20043;&#19968;&#12290;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#65292;&#26679;&#26412;&#26435;&#37325;&#26159;&#30001;&#26410;&#26469;&#25968;&#25454;&#20998;&#24067;&#23545;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#20272;&#35745;Radon-Nikod\'ym&#23548;&#25968;&#30340;&#20540;&#30830;&#23450;&#30340;&#12290;&#26412;&#30740;&#31350;&#22238;&#39038;&#20102;&#22312;RKHS&#20013;&#37325;&#26032;&#21152;&#26435;&#26680;&#22238;&#24402;&#30340;&#24050;&#30693;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#24471;&#21040;&#26032;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#24369;&#24179;&#28369;&#26465;&#20214;&#19979;&#34920;&#26126;&#65292;&#20026;&#20102;&#23454;&#29616;&#19982;&#26631;&#20934;&#30417;&#30563;&#23398;&#20064;&#20013;&#25968;&#25454;&#20998;&#24067;&#24046;&#24322;&#30456;&#21516;&#31934;&#24230;&#30340;&#26679;&#26412;&#25968;&#30446;&#35201;&#27604;&#29616;&#26377;&#30340;&#20998;&#26512;&#35777;&#26126;&#30340;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sample reweighting is one of the most widely used methods for correcting the error of least squares learning algorithms in reproducing kernel Hilbert spaces (RKHS), that is caused by future data distributions that are different from the training data distribution. In practical situations, the sample weights are determined by values of the estimated Radon-Nikod\'ym derivative, of the future data distribution w.r.t.~the training data distribution. In this work, we review known error bounds for reweighted kernel regression in RKHS and obtain, by combination, novel results. We show under weak smoothness conditions, that the amount of samples, needed to achieve the same order of accuracy as in the standard supervised learning without differences in data distributions, is smaller than proven by state-of-the-art analyses.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33258;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;TSDiff&#12290;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#36741;&#21161;&#32593;&#32476;&#25110;&#35757;&#32451;&#36807;&#31243;&#30340;&#25913;&#21464;&#65292;&#22312;&#39044;&#27979;&#12289;&#25913;&#36827;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31561;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.11494</link><description>&lt;p&gt;
&#39044;&#27979;&#12289;&#25913;&#36827;&#12289;&#21512;&#25104;&#65306;&#38754;&#21521;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33258;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting. (arXiv:2307.11494v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33258;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;TSDiff&#12290;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#36741;&#21161;&#32593;&#32476;&#25110;&#35757;&#32451;&#36807;&#31243;&#30340;&#25913;&#21464;&#65292;&#22312;&#39044;&#27979;&#12289;&#25913;&#36827;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31561;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20043;&#21069;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#38024;&#23545;&#29305;&#23450;&#39044;&#27979;&#25110;&#22635;&#34917;&#20219;&#21153;&#30340;&#26465;&#20214;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#38754;&#21521;&#22810;&#31181;&#26102;&#38388;&#24207;&#21015;&#24212;&#29992;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26465;&#20214;&#19979;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TSDiff&#65292;&#19968;&#31181;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#30340;&#26080;&#26465;&#20214;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#33258;&#24341;&#23548;&#26426;&#21046;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#24471;TSDiff&#33021;&#22815;&#20026;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#26465;&#20214;&#35774;&#32622;&#65292;&#32780;&#26080;&#38656;&#36741;&#21161;&#32593;&#32476;&#25110;&#25913;&#21464;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;&#39044;&#27979;&#12289;&#25913;&#36827;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#34920;&#26126;TSDiff&#19982;&#20960;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#26465;&#20214;&#39044;&#27979;&#26041;&#27861;&#30456;&#31454;&#20105;&#65288;&#39044;&#27979;&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;TSDiff&#23398;&#21040;&#30340;&#38544;&#24615;&#27010;&#29575;&#23494;&#24230;&#26469;&#36845;&#20195;&#22320;&#25913;&#36827;p
&lt;/p&gt;
&lt;p&gt;
Diffusion models have achieved state-of-the-art performance in generative modeling tasks across various domains. Prior works on time series diffusion models have primarily focused on developing conditional models tailored to specific forecasting or imputation tasks. In this work, we explore the potential of task-agnostic, unconditional diffusion models for several time series applications. We propose TSDiff, an unconditionally trained diffusion model for time series. Our proposed self-guidance mechanism enables conditioning TSDiff for downstream tasks during inference, without requiring auxiliary networks or altering the training procedure. We demonstrate the effectiveness of our method on three different time series tasks: forecasting, refinement, and synthetic data generation. First, we show that TSDiff is competitive with several task-specific conditional forecasting methods (predict). Second, we leverage the learned implicit probability density of TSDiff to iteratively refine the p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#20998;&#26512;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#20272;&#35745;&#21644;&#20998;&#31867;&#30149;&#20154;&#30340;&#28508;&#22312;&#29366;&#24577;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#21487;&#20197;&#23398;&#20064;&#12289;&#21487;&#35270;&#21270;&#21644;&#32858;&#31867;&#19982;&#30142;&#30149;&#36827;&#23637;&#30456;&#20851;&#30340;&#30149;&#20154;&#28508;&#22312;&#29366;&#24577;&#30340;&#26102;&#38388;&#21464;&#21270;&#65292;&#24182;&#25104;&#21151;&#21457;&#29616;&#20102;&#19982;&#39044;&#21518;&#30456;&#20851;&#30340;&#28508;&#22312;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2307.11487</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#20998;&#26512;&#30340;&#26694;&#26550;&#29992;&#20110;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#20272;&#35745;&#21644;&#20998;&#31867;&#30149;&#20154;&#30340;&#28508;&#22312;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
A New Deep State-Space Analysis Framework for Patient Latent State Estimation and Classification from EHR Time Series Data. (arXiv:2307.11487v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11487
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#20998;&#26512;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#20272;&#35745;&#21644;&#20998;&#31867;&#30149;&#20154;&#30340;&#28508;&#22312;&#29366;&#24577;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#21487;&#20197;&#23398;&#20064;&#12289;&#21487;&#35270;&#21270;&#21644;&#32858;&#31867;&#19982;&#30142;&#30149;&#36827;&#23637;&#30456;&#20851;&#30340;&#30149;&#20154;&#28508;&#22312;&#29366;&#24577;&#30340;&#26102;&#38388;&#21464;&#21270;&#65292;&#24182;&#25104;&#21151;&#21457;&#29616;&#20102;&#19982;&#39044;&#21518;&#30456;&#20851;&#30340;&#28508;&#22312;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30142;&#30149;&#65292;&#21253;&#25324;&#30284;&#30151;&#21644;&#24930;&#24615;&#30142;&#30149;&#65292;&#38656;&#35201;&#38271;&#26399;&#27835;&#30103;&#21644;&#38271;&#26399;&#31574;&#30053;&#12290;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#19987;&#27880;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#24050;&#32463;&#20986;&#29616;&#26469;&#35299;&#20915;&#36825;&#20010;&#38656;&#27714;&#12290;&#26377;&#25928;&#30340;&#27835;&#30103;&#31574;&#30053;&#19981;&#20165;&#20165;&#28041;&#21450;&#25429;&#25417;&#30149;&#20154;&#27979;&#35797;&#25968;&#20540;&#30340;&#39034;&#24207;&#21464;&#21270;&#12290;&#23427;&#38656;&#35201;&#19968;&#20010;&#21487;&#35299;&#37322;&#21644;&#20020;&#24202;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#25429;&#25417;&#30149;&#20154;&#38543;&#26102;&#38388;&#30340;&#20869;&#37096;&#29366;&#24577;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#20998;&#26512;&#26694;&#26550;&#8221;&#65292;&#20351;&#29992;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#23545;EHR&#30340;&#26102;&#38388;&#24207;&#21015;&#26080;&#30417;&#30563;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#12289;&#21487;&#35270;&#21270;&#21644;&#32858;&#31867;&#19982;&#30142;&#30149;&#36827;&#23637;&#30456;&#20851;&#30340;&#30149;&#20154;&#28508;&#22312;&#29366;&#24577;&#30340;&#26102;&#38388;&#21464;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;12,695&#21517;&#30284;&#30151;&#24739;&#32773;&#30340;&#26102;&#38388;&#24207;&#21015;&#23454;&#39564;&#23460;&#25968;&#25454;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#20272;&#35745;&#28508;&#22312;&#29366;&#24577;&#65292;&#25105;&#20204;&#25104;&#21151;&#21457;&#29616;&#20102;&#19982;&#39044;&#21518;&#30456;&#20851;&#30340;&#28508;&#22312;&#29366;&#24577;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#32858;&#31867;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#30149;&#20154;&#29366;&#24577;&#21644;&#27835;&#30103;&#21709;&#24212;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many diseases, including cancer and chronic conditions, require extended treatment periods and long-term strategies. Machine learning and AI research focusing on electronic health records (EHRs) have emerged to address this need. Effective treatment strategies involve more than capturing sequential changes in patient test values. It requires an explainable and clinically interpretable model by capturing the patient's internal state over time.  In this study, we propose the "deep state-space analysis framework," using time-series unsupervised learning of EHRs with a deep state-space model. This framework enables learning, visualizing, and clustering of temporal changes in patient latent states related to disease progression.  We evaluated our framework using time-series laboratory data from 12,695 cancer patients. By estimating latent states, we successfully discover latent states related to prognosis. By visualization and cluster analysis, the temporal transition of patient status and 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#34987;&#23457;&#26597;&#21644;&#26410;&#34987;&#23457;&#26597;&#30149;&#20154;&#30340;&#20449;&#24687;&#65292;&#39044;&#27979;&#38750;&#23567;&#32454;&#32990;&#32954;&#30284;&#65288;NSCLC&#65289;&#30149;&#20154;&#30340;&#25972;&#20307;&#29983;&#23384;&#12290;</title><link>http://arxiv.org/abs/2307.11465</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#25972;&#20307;&#29983;&#23384;&#20998;&#26512;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Approach for Overall Survival Analysis with Missing Values. (arXiv:2307.11465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11465
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#34987;&#23457;&#26597;&#21644;&#26410;&#34987;&#23457;&#26597;&#30149;&#20154;&#30340;&#20449;&#24687;&#65292;&#39044;&#27979;&#38750;&#23567;&#32454;&#32990;&#32954;&#30284;&#65288;NSCLC&#65289;&#30149;&#20154;&#30340;&#25972;&#20307;&#29983;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#24212;&#29992;&#20110;&#32954;&#30284;&#30740;&#31350;&#65292;&#23588;&#20854;&#26159;&#38750;&#23567;&#32454;&#32990;&#32954;&#30284;&#65288;NSCLC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#12290;&#23545;&#20110;&#30149;&#20154;&#29366;&#24577;&#30340;&#25972;&#20307;&#29983;&#23384;&#65288;OS&#65289;&#26159;&#19968;&#20010;&#37325;&#35201;&#25351;&#26631;&#65292;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#29983;&#23384;&#27010;&#29575;&#19981;&#21516;&#30340;&#20122;&#32452;&#65292;&#20174;&#32780;&#23454;&#29616;&#20010;&#20307;&#21270;&#27835;&#30103;&#21644;&#25913;&#21892;&#25972;&#20307;&#29983;&#23384;&#29575;&#12290;&#22312;&#36825;&#20010;&#20998;&#26512;&#20013;&#65292;&#38656;&#35201;&#32771;&#34385;&#20004;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#27599;&#20010;&#30149;&#20154;&#30340;&#21487;&#29992;&#20449;&#24687;&#65292;&#21033;&#29992;&#26410;&#34987;&#23457;&#26597;&#30340;&#65288;&#21363;&#27515;&#20129;&#65289;&#21644;&#34987;&#23457;&#26597;&#30340;&#65288;&#21363;&#24184;&#23384;&#32773;&#65289;&#30149;&#20154;&#30340;&#20449;&#24687;&#65292;&#20063;&#35201;&#32771;&#34385;&#21040;&#27515;&#20129;&#26102;&#38388;&#12290;&#20854;&#27425;&#65292;&#19981;&#23436;&#25972;&#25968;&#25454;&#22788;&#29702;&#26159;&#21307;&#23398;&#39046;&#22495;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#25554;&#34917;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#20010;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#34987;&#23457;&#26597;&#21644;&#26410;&#34987;&#23457;&#26597;&#30340;&#30149;&#20154;&#21450;&#20854;&#21487;&#29992;&#29305;&#24449;&#20013;&#26377;&#25928;&#23398;&#20064;&#65292;&#39044;&#27979;NSCLC&#30149;&#20154;&#30340;OS&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most challenging fields where Artificial Intelligence (AI) can be applied is lung cancer research, specifically non-small cell lung cancer (NSCLC). In particular, overall survival (OS) is a vital indicator of patient status, helping to identify subgroups with diverse survival probabilities, enabling tailored treatment and improved OS rates. In this analysis, there are two challenges to take into account. First, few studies effectively exploit the information available from each patient, leveraging both uncensored (i.e., dead) and censored (i.e., survivors) patients, considering also the death times. Second, the handling of incomplete data is a common issue in the medical field. This problem is typically tackled through the use of imputation methods. Our objective is to present an AI model able to overcome these limits, effectively learning from both censored and uncensored patients and their available features, for the prediction of OS for NSCLC patients. We present a novel 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26102;&#38388;&#19978;&#37325;&#26032;&#32553;&#25918;&#38169;&#35823;&#24230;&#37327;&#65292;&#25913;&#21892;&#20102;&#38271;&#26399;&#35760;&#24518;&#23398;&#20064;&#30340;&#20559;&#21521;&#20110;&#30701;&#26399;&#35760;&#24518;&#30340;&#38382;&#39064;&#65292;&#24182;&#32531;&#35299;&#20102;&#26799;&#24230;&#28040;&#22833;&#30340;&#22256;&#25200;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#36866;&#24403;&#30340;&#26102;&#38388;&#19978;&#37325;&#26032;&#32553;&#25918;&#38169;&#35823;&#22312;&#26377;&#25928;&#30340;&#38271;&#26399;&#35760;&#24518;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.11462</link><description>&lt;p&gt;
&#36890;&#36807;&#26102;&#38388;&#37325;&#26032;&#32553;&#25918;&#38169;&#35823;&#20197;&#25913;&#21892;&#38271;&#26399;&#35760;&#24518;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improve Long-term Memory Learning Through Rescaling the Error Temporally. (arXiv:2307.11462v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26102;&#38388;&#19978;&#37325;&#26032;&#32553;&#25918;&#38169;&#35823;&#24230;&#37327;&#65292;&#25913;&#21892;&#20102;&#38271;&#26399;&#35760;&#24518;&#23398;&#20064;&#30340;&#20559;&#21521;&#20110;&#30701;&#26399;&#35760;&#24518;&#30340;&#38382;&#39064;&#65292;&#24182;&#32531;&#35299;&#20102;&#26799;&#24230;&#28040;&#22833;&#30340;&#22256;&#25200;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#36866;&#24403;&#30340;&#26102;&#38388;&#19978;&#37325;&#26032;&#32553;&#25918;&#38169;&#35823;&#22312;&#26377;&#25928;&#30340;&#38271;&#26399;&#35760;&#24518;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24207;&#21015;&#24314;&#27169;&#20013;&#38271;&#26399;&#35760;&#24518;&#23398;&#20064;&#30340;&#38169;&#35823;&#24230;&#37327;&#36873;&#25321;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#24120;&#29992;&#38169;&#35823;&#24230;&#37327;&#65288;&#21253;&#25324;&#24179;&#22343;&#32477;&#23545;/&#24179;&#26041;&#35823;&#24046;&#65289;&#23545;&#30701;&#26399;&#35760;&#24518;&#30340;&#20559;&#21521;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#23398;&#20064;&#32447;&#24615;&#20989;&#25968;&#26102;&#65292;&#25152;&#26377;&#26102;&#38388;&#19978;&#26377;&#27491;&#26435;&#37325;&#30340;&#38169;&#35823;&#37117;&#20559;&#21521;&#20110;&#30701;&#26399;&#35760;&#24518;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#31181;&#20559;&#24046;&#24182;&#25913;&#21892;&#38271;&#26399;&#35760;&#24518;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#26102;&#38388;&#19978;&#37325;&#26032;&#32553;&#25918;&#30340;&#38169;&#35823;&#12290;&#38500;&#20102;&#20943;&#23569;&#23545;&#30701;&#26399;&#35760;&#24518;&#30340;&#20559;&#21521;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;&#36824;&#21487;&#20197;&#32531;&#35299;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#38271;&#26399;&#35760;&#24518;&#20219;&#21153;&#21644;&#24207;&#21015;&#27169;&#22411;&#19978;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25968;&#20540;&#32467;&#26524;&#30830;&#35748;&#20102;&#36866;&#24403;&#30340;&#26102;&#38388;&#19978;&#37325;&#26032;&#32553;&#25918;&#35823;&#24046;&#23545;&#20110;&#26377;&#25928;&#30340;&#38271;&#26399;&#35760;&#24518;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#23450;&#37327;&#20998;&#26512;&#24207;&#21015;&#24314;&#27169;&#20013;&#19981;&#21516;&#38169;&#35823;&#23545;&#30701;&#26399;&#35760;&#24518;&#20559;&#21521;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the error metric selection for long-term memory learning in sequence modelling. We examine the bias towards short-term memory in commonly used errors, including mean absolute/squared error. Our findings show that all temporally positive-weighted errors are biased towards short-term memory in learning linear functionals. To reduce this bias and improve long-term memory learning, we propose the use of a temporally rescaled error. In addition to reducing the bias towards short-term memory, this approach can also alleviate the vanishing gradient issue. We conduct numerical experiments on different long-memory tasks and sequence models to validate our claims. Numerical results confirm the importance of appropriate temporally rescaled error for effective long-term memory learning. To the best of our knowledge, this is the first work that quantitatively analyzes different errors' memory bias towards short-term memory in sequence modelling.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24310;&#36831;&#34917;&#20607;&#36229;bolic PIDE&#25511;&#21046;&#30340;&#31070;&#32463;&#31639;&#23376;&#26694;&#26550;&#65292;&#25193;&#23637;&#20102;&#22522;&#26412;PDE&#25511;&#21046;&#32467;&#26524;&#21040;&#19968;&#20010;&#28041;&#21450;&#29366;&#24577;&#21644;&#31995;&#32479;&#24310;&#36831;&#30340;&#39640;&#32423;&#36229;bolic&#31867;&#12290;&#20351;&#29992;DeepONet&#36924;&#36817;&#31639;&#23376;&#65292;&#21487;&#20197;&#22312;&#26080;&#38480;&#32500;&#24230;&#19978;&#24314;&#31435;&#31283;&#23450;&#30340;&#38381;&#29615;&#21453;&#39304;&#65292;&#24182;&#24320;&#21457;&#20102;&#36924;&#36817;&#30340;&#35266;&#27979;&#32773;&#21644;&#36755;&#20986;&#21453;&#39304;&#23450;&#24459;&#12290;</title><link>http://arxiv.org/abs/2307.11436</link><description>&lt;p&gt;
&#24310;&#36831;&#34917;&#20607;&#36229;bolic PIDE&#25511;&#21046;&#30340;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Neural Operators for Delay-Compensating Control of Hyperbolic PIDEs. (arXiv:2307.11436v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11436
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24310;&#36831;&#34917;&#20607;&#36229;bolic PIDE&#25511;&#21046;&#30340;&#31070;&#32463;&#31639;&#23376;&#26694;&#26550;&#65292;&#25193;&#23637;&#20102;&#22522;&#26412;PDE&#25511;&#21046;&#32467;&#26524;&#21040;&#19968;&#20010;&#28041;&#21450;&#29366;&#24577;&#21644;&#31995;&#32479;&#24310;&#36831;&#30340;&#39640;&#32423;&#36229;bolic&#31867;&#12290;&#20351;&#29992;DeepONet&#36924;&#36817;&#31639;&#23376;&#65292;&#21487;&#20197;&#22312;&#26080;&#38480;&#32500;&#24230;&#19978;&#24314;&#31435;&#31283;&#23450;&#30340;&#38381;&#29615;&#21453;&#39304;&#65292;&#24182;&#24320;&#21457;&#20102;&#36924;&#36817;&#30340;&#35266;&#27979;&#32773;&#21644;&#36755;&#20986;&#21453;&#39304;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24341;&#20837;&#30340;DeepONet&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#20174;&#22522;&#26412;&#30340;&#36229;bolic&#21644;&#25311;bolic PDE&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;&#19968;&#20010;&#39640;&#32423;&#36229;bolic&#31867;&#65292;&#20854;&#20013;&#21253;&#25324;&#29366;&#24577;&#21644;&#31995;&#32479;&#36755;&#20986;&#25110;&#36755;&#20837;&#30340;&#24310;&#36831;&#12290;PDE&#21453;&#21521;&#35774;&#35745;&#20135;&#29983;&#30340;&#22686;&#30410;&#20989;&#25968;&#26159;&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;&#36755;&#20986;&#65292;&#23558;&#31354;&#38388;&#22495;&#19978;&#30340;&#20989;&#25968;&#26144;&#23556;&#21040;&#31354;&#38388;&#22495;&#19978;&#30340;&#20989;&#25968;&#65292;&#20854;&#20013;&#35813;&#22686;&#30410;&#29983;&#25104;&#31639;&#23376;&#30340;&#36755;&#20837;&#26159;PDE&#30340;&#31995;&#25968;&#12290;&#20351;&#29992;DeepONet&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#35813;&#31639;&#23376;&#65292;&#21487;&#20197;&#35777;&#26126;&#20854;&#20219;&#24847;&#31934;&#24230;&#32039;&#23494;&#12290;&#19968;&#26086;&#22312;&#26080;&#38480;&#32500;&#24230;&#19978;&#20135;&#29983;&#20102;&#36825;&#20010;&#36924;&#36817;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#23601;&#21487;&#20197;&#22312;&#20351;&#29992;&#36817;&#20284;&#22686;&#30410;&#30340;&#21453;&#39304;&#19979;&#24314;&#31435;&#23553;&#38381;&#29615;&#30340;&#31283;&#23450;&#24615;&#12290;&#38500;&#20102;&#25552;&#20379;&#20840;&#29366;&#24577;&#21453;&#39304;&#19979;&#30340;&#36825;&#20123;&#32467;&#26524;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#23637;&#20102;DeepONet&#36924;&#36817;&#30340;&#35266;&#27979;&#32773;&#21644;&#36755;&#20986;&#21453;&#39304;&#23450;&#24459;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#31070;&#32463;&#31639;&#23376;&#36924;&#36817;&#19979;&#30340;&#31283;&#23450;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently introduced DeepONet operator-learning framework for PDE control is extended from the results for basic hyperbolic and parabolic PDEs to an advanced hyperbolic class that involves delays on both the state and the system output or input. The PDE backstepping design produces gain functions that are outputs of a nonlinear operator, mapping functions on a spatial domain into functions on a spatial domain, and where this gain-generating operator's inputs are the PDE's coefficients. The operator is approximated with a DeepONet neural network to a degree of accuracy that is provably arbitrarily tight. Once we produce this approximation-theoretic result in infinite dimension, with it we establish stability in closed loop under feedback that employs approximate gains. In addition to supplying such results under full-state feedback, we also develop DeepONet-approximated observers and output-feedback laws and prove their own stabilizing properties under neural operator approximations.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25512;&#29702;&#38454;&#27573;&#24341;&#20837;&#25209;&#22788;&#29702;&#23545;&#33021;&#28304;&#28040;&#32791;&#21644;&#21709;&#24212;&#26102;&#38388;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#25209;&#22788;&#29702;&#23545;&#36825;&#20004;&#20010;&#25351;&#26631;&#37117;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.11434</link><description>&lt;p&gt;
&#20026;&#29615;&#20445;&#20154;&#24037;&#26234;&#33021;&#32780;&#25209;&#22788;&#29702; - &#25506;&#32034;&#25512;&#29702;&#36807;&#31243;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Batching for Green AI -- An Exploratory Study on Inference. (arXiv:2307.11434v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11434
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25512;&#29702;&#38454;&#27573;&#24341;&#20837;&#25209;&#22788;&#29702;&#23545;&#33021;&#28304;&#28040;&#32791;&#21644;&#21709;&#24212;&#26102;&#38388;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#25209;&#22788;&#29702;&#23545;&#36825;&#20004;&#20010;&#25351;&#26631;&#37117;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#21457;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#25209;&#22823;&#23567;&#26159;&#19968;&#20010;&#38656;&#35201;&#35843;&#25972;&#30340;&#37325;&#35201;&#21442;&#25968;&#12290;&#38500;&#20102;&#20854;&#20182;&#36136;&#37327;&#25351;&#26631;&#22806;&#65292;&#23427;&#23545;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#12289;&#35757;&#32451;&#26102;&#38388;&#21644;&#24182;&#34892;&#24615;&#20855;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#36825;&#20010;&#20107;&#23454;&#26159;&#20247;&#25152;&#21608;&#30693;&#24182;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#38454;&#27573;&#65292;&#24403;&#27169;&#22411;&#34987;&#26368;&#32456;&#29992;&#25143;&#29992;&#20110;&#25512;&#29702;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#24341;&#20837;&#25209;&#22823;&#23567;&#30340;&#28508;&#22312;&#22909;&#22788;&#23384;&#22312;&#24573;&#35270;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#36755;&#20837;&#25209;&#22788;&#29702;&#23545;&#20110;&#20116;&#20010;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#34987;&#35748;&#20026;&#26159;&#26368;&#20808;&#36827;&#30340;&#23436;&#20840;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#28304;&#28040;&#32791;&#21644;&#21709;&#24212;&#26102;&#38388;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25209;&#22788;&#29702;&#23545;&#36825;&#20004;&#20010;&#25351;&#26631;&#37117;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21576;&#29616;&#20102;&#36807;&#21435;&#21313;&#24180;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#28304;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#30340;&#26102;&#38388;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24635;&#20307;&#19978;&#65292;&#33021;&#28304;&#28040;&#32791;&#22312;&#36825;&#27573;&#26102;&#38388;&#20869;&#26126;&#26174;&#19978;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The batch size is an essential parameter to tune during the development of new neural networks. Amongst other quality indicators, it has a large degree of influence on the model's accuracy, generalisability, training times and parallelisability. This fact is generally known and commonly studied. However, during the application phase of a deep learning model, when the model is utilised by an end-user for inference, we find that there is a disregard for the potential benefits of introducing a batch size. In this study, we examine the effect of input batching on the energy consumption and response times of five fully-trained neural networks for computer vision that were considered state-of-the-art at the time of their publication. The results suggest that batching has a significant effect on both of these metrics. Furthermore, we present a timeline of the energy efficiency and accuracy of neural networks over the past decade. We find that in general, energy consumption rises at a much ste
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#21435;&#20013;&#24515;&#21270;&#25968;&#25454;&#39537;&#21160;&#24211;&#23384;&#31649;&#29702;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#24182;&#25511;&#21046;&#27599;&#20010;&#23454;&#20307;&#30340;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#27169;&#25311;&#19981;&#21516;&#20379;&#24212;&#38142;&#32593;&#32476;&#21644;&#19981;&#30830;&#23450;&#24615;&#27700;&#24179;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#22810;&#26234;&#33021;&#20307;&#21464;&#20307;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.11432</link><description>&lt;p&gt;
&#20998;&#26512;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#21435;&#20013;&#24515;&#21270;&#24211;&#23384;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Multi-Agent Reinforcement Learning for Decentralized Inventory Control Systems. (arXiv:2307.11432v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#21435;&#20013;&#24515;&#21270;&#25968;&#25454;&#39537;&#21160;&#24211;&#23384;&#31649;&#29702;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#24182;&#25511;&#21046;&#27599;&#20010;&#23454;&#20307;&#30340;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#27169;&#25311;&#19981;&#21516;&#20379;&#24212;&#38142;&#32593;&#32476;&#21644;&#19981;&#30830;&#23450;&#24615;&#27700;&#24179;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#22810;&#26234;&#33021;&#20307;&#21464;&#20307;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#35299;&#20915;&#24211;&#23384;&#31649;&#29702;&#38382;&#39064;&#30340;&#26041;&#27861;&#20551;&#35774;&#20449;&#24687;&#30340;&#38598;&#20013;&#65292;&#19982;&#23454;&#38469;&#20379;&#24212;&#38142;&#32593;&#32476;&#30340;&#32452;&#32455;&#32422;&#26463;&#19981;&#20860;&#23481;&#12290;&#24211;&#23384;&#31649;&#29702;&#38382;&#39064;&#26159;&#36816;&#31609;&#23398;&#20013;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#35268;&#21010;&#38382;&#39064;&#65292;&#28041;&#21450;&#20026;&#20379;&#24212;&#38142;&#20013;&#30340;&#33410;&#28857;&#25214;&#21040;&#26368;&#20248;&#30340;&#37325;&#26032;&#35746;&#36141;&#31574;&#30053;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;&#20851;&#20110;&#35813;&#38382;&#39064;&#30340;&#38598;&#20013;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#24212;&#29992;&#20110;&#30001;&#29420;&#31435;&#23454;&#20307;&#32452;&#25104;&#30340;&#30495;&#23454;&#19990;&#30028;&#20379;&#24212;&#38142;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#33258;&#28982;&#22320;&#20998;&#35299;&#25104;&#19982;&#29420;&#31435;&#23454;&#20307;&#30456;&#20851;&#30340;&#23376;&#38382;&#39064;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#21435;&#20013;&#24515;&#21270;&#25968;&#25454;&#39537;&#21160;&#30340;&#24211;&#23384;&#31649;&#29702;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#27599;&#20010;&#23454;&#20307;&#30001;&#19968;&#20010;&#26234;&#33021;&#20307;&#25511;&#21046;&#12290;&#36890;&#36807;&#27169;&#25311;&#19981;&#21516;&#20379;&#24212;&#38142;&#32593;&#32476;&#21644;&#19981;&#30830;&#23450;&#24615;&#27700;&#24179;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#22810;&#26234;&#33021;&#20307;&#21464;&#20307;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most solutions to the inventory management problem assume a centralization of information that is incompatible with organisational constraints in real supply chain networks. The inventory management problem is a well-known planning problem in operations research, concerned with finding the optimal re-order policy for nodes in a supply chain. While many centralized solutions to the problem exist, they are not applicable to real-world supply chains made up of independent entities. The problem can however be naturally decomposed into sub-problems, each associated with an independent entity, turning it into a multi-agent system. Therefore, a decentralized data-driven solution to inventory management problems using multi-agent reinforcement learning is proposed where each entity is controlled by an agent. Three multi-agent variations of the proximal policy optimization algorithm are investigated through simulations of different supply chain networks and levels of uncertainty. The centralize
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36890;&#20449;&#29702;&#35770;&#20013;&#32467;&#21512;&#27880;&#24847;&#21147;&#21644;&#30456;&#23545;&#29109;&#30340;&#27010;&#24565;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36890;&#20449;&#20013;&#20351;&#29992;&#27880;&#24847;&#21147;&#23548;&#21521;&#30340;&#21152;&#26435;&#30456;&#23545;&#29109;&#26159;&#19981;&#36866;&#24403;&#30340;&#65292;&#32780;&#36866;&#24403;&#30340;&#27880;&#24847;&#21147;&#36890;&#20449;&#21487;&#36890;&#36807;&#21457;&#36865;&#32773;&#20165;&#38656;&#35201;&#20102;&#35299;&#25509;&#25910;&#32773;&#30340;&#25928;&#29992;&#20989;&#25968;&#26469;&#23454;&#29616;&#26368;&#20339;&#36890;&#30693;&#12290;</title><link>http://arxiv.org/abs/2307.11423</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#23545;&#29109;&#36890;&#20449;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Attention to Entropic Communication. (arXiv:2307.11423v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11423
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36890;&#20449;&#29702;&#35770;&#20013;&#32467;&#21512;&#27880;&#24847;&#21147;&#21644;&#30456;&#23545;&#29109;&#30340;&#27010;&#24565;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36890;&#20449;&#20013;&#20351;&#29992;&#27880;&#24847;&#21147;&#23548;&#21521;&#30340;&#21152;&#26435;&#30456;&#23545;&#29109;&#26159;&#19981;&#36866;&#24403;&#30340;&#65292;&#32780;&#36866;&#24403;&#30340;&#27880;&#24847;&#21147;&#36890;&#20449;&#21487;&#36890;&#36807;&#21457;&#36865;&#32773;&#20165;&#38656;&#35201;&#20102;&#35299;&#25509;&#25910;&#32773;&#30340;&#25928;&#29992;&#20989;&#25968;&#26469;&#23454;&#29616;&#26368;&#20339;&#36890;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#30340;&#27010;&#24565;&#26159;&#25351;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#24378;&#35843;&#29305;&#23450;&#25968;&#25454;&#37325;&#35201;&#24615;&#30340;&#25968;&#20540;&#26435;&#37325;&#65292;&#22312;&#36890;&#20449;&#29702;&#35770;&#20013;&#30456;&#23545;&#29109;&#65288;RE&#65292;&#20063;&#31216;&#20026;&#24211;&#23572;&#24052;&#20811;-&#21202;&#24067;&#21202;&#25955;&#24230;&#65289;&#21457;&#25381;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#36825;&#20123;&#27010;&#24565;&#65292;&#21363;&#27880;&#24847;&#21147;&#21644;RE&#12290;RE&#24341;&#23548;&#24102;&#23485;&#26377;&#38480;&#36890;&#20449;&#20013;&#30340;&#26368;&#20339;&#32534;&#30721;&#20197;&#21450;&#36890;&#36807;&#26368;&#22823;&#29109;&#21407;&#29702;&#65288;MEP&#65289;&#36827;&#34892;&#26368;&#20339;&#28040;&#24687;&#35299;&#30721;&#12290;&#22312;&#32534;&#30721;&#22330;&#26223;&#20013;&#65292;RE&#21487;&#20197;&#20174;&#22235;&#20010;&#35201;&#27714;&#20013;&#25512;&#23548;&#20986;&#26469;&#65292;&#21363;&#20998;&#26512;&#24615;&#12289;&#23616;&#37096;&#24615;&#12289;&#36866;&#24403;&#24615;&#21644;&#26657;&#20934;&#24615;&#12290;&#32780;&#29992;&#20110;&#36890;&#20449;&#20013;&#27880;&#24847;&#21147;&#23548;&#21521;&#30340;&#21152;&#26435;RE&#23454;&#38469;&#19978;&#26159;&#19981;&#36866;&#24403;&#30340;&#12290;&#20026;&#20102;&#30475;&#21040;&#36866;&#24403;&#30340;&#27880;&#24847;&#21147;&#36890;&#20449;&#26159;&#22914;&#20309;&#20986;&#29616;&#30340;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#22330;&#26223;&#65292;&#21363;&#28040;&#24687;&#21457;&#36865;&#32773;&#24076;&#26395;&#30830;&#20445;&#25509;&#25910;&#32773;&#33021;&#22815;&#25191;&#34892;&#30693;&#24773;&#30340;&#25805;&#20316;&#12290;&#22914;&#26524;&#25509;&#25910;&#32773;&#20351;&#29992;MEP&#35299;&#30721;&#28040;&#24687;&#65292;&#21017;&#21457;&#36865;&#32773;&#21482;&#38656;&#35201;&#30693;&#36947;&#25509;&#25910;&#32773;&#30340;&#25928;&#29992;&#20989;&#25968;&#26469;&#36827;&#34892;&#26368;&#20339;&#36890;&#30693;&#65292;&#19981;&#38656;&#35201;&#30693;&#36947;&#25509;&#25910;&#32773;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The concept of attention, numerical weights that emphasize the importance of particular data, has proven to be very relevant in artificial intelligence. Relative entropy (RE, aka Kullback-Leibler divergence) plays a central role in communication theory. Here we combine these concepts, attention and RE. RE guides optimal encoding of messages in bandwidth-limited communication as well as optimal message decoding via the maximum entropy principle (MEP). In the coding scenario, RE can be derived from four requirements, namely being analytical, local, proper, and calibrated. Weighted RE, used for attention steering in communications, turns out to be improper. To see how proper attention communication can emerge, we analyze a scenario of a message sender who wants to ensure that the receiver of the message can perform well-informed actions. If the receiver decodes the message using the MEP, the sender only needs to know the receiver's utility function to inform optimally, but not the receive
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#31616;&#21270;&#26377;&#38480;&#20803;&#26041;&#27861;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#36719;&#20307;&#26426;&#22120;&#20154;&#30340;&#30452;&#25509;&#21644;&#36870;&#21521;&#24314;&#27169;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#32039;&#20945;&#27169;&#22411;&#22312;&#24314;&#27169;&#25928;&#26524;&#21644;&#25928;&#29575;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.11408</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#31616;&#21270;&#30340;&#26377;&#38480;&#20803;&#26041;&#27861;&#27169;&#22411;&#65292;&#30452;&#25509;&#21644;&#21453;&#21521;&#24314;&#27169;&#36719;&#20307;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Direct and inverse modeling of soft robots by learning a condensed FEM model. (arXiv:2307.11408v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#31616;&#21270;&#26377;&#38480;&#20803;&#26041;&#27861;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#36719;&#20307;&#26426;&#22120;&#20154;&#30340;&#30452;&#25509;&#21644;&#36870;&#21521;&#24314;&#27169;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#32039;&#20945;&#27169;&#22411;&#22312;&#24314;&#27169;&#25928;&#26524;&#21644;&#25928;&#29575;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#20803;&#26041;&#27861;(FEM)&#26159;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#36719;&#20307;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#24378;&#22823;&#24314;&#27169;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38750;&#25968;&#20540;&#35745;&#31639;&#19987;&#23478;&#26469;&#35828;&#65292;&#23427;&#22312;&#25511;&#21046;&#26041;&#38754;&#30340;&#24212;&#29992;&#21487;&#33021;&#20250;&#24456;&#22256;&#38590;&#65306;&#23427;&#38656;&#35201;&#20248;&#21270;&#35745;&#31639;&#20197;&#20351;&#20854;&#23454;&#26102;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#32039;&#20945;&#20294;&#36275;&#22815;&#20016;&#23500;&#30340;&#26426;&#26800;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#36873;&#25321;&#22522;&#20110;FEM&#27169;&#22411;&#22312;&#33268;&#21160;&#22120;/&#25928;&#24212;&#22120;&#31354;&#38388;&#20013;&#25552;&#20379;&#30340;&#38750;&#32447;&#24615;&#39034;&#20174;&#25968;&#25454;&#30340;&#31934;&#31616;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#32039;&#20945;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#21512;&#29702;&#37327;&#30340;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#19988;&#22312;&#24314;&#27169;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#22240;&#20026;&#25105;&#20204;&#21487;&#20197;&#25512;&#23548;&#20986;&#26426;&#22120;&#20154;&#30340;&#30452;&#25509;&#21644;&#36870;&#36816;&#21160;&#23398;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#19968;&#20123;&#21333;&#29420;&#23398;&#20064;&#30340;&#27169;&#22411;&#36827;&#34892;&#32806;&#21512;&#65292;&#29305;&#21035;&#26159;&#22312;&#30001;&#20004;&#20010;&#36719;&#20307;&#25163;&#25351;&#32452;&#25104;&#30340;&#22841;&#29226;&#30340;&#20363;&#23376;&#19978;&#12290;&#36890;&#36807;&#27604;&#36739;&#20174;&#23436;&#25972;&#30340;FEM&#27169;&#22411;&#27966;&#29983;&#30340;&#36870;&#27169;&#22411;&#21644;&#20174;&#32039;&#20945;&#23398;&#20064;&#29256;&#26412;&#27966;&#29983;&#30340;&#36870;&#27169;&#22411;&#65292;&#36824;&#23637;&#31034;&#20102;&#20854;&#20182;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Finite Element Method (FEM) is a powerful modeling tool for predicting the behavior of soft robots. However, its use for control can be difficult for non-specialists of numerical computation: it requires an optimization of the computation to make it real-time. In this paper, we propose a learning-based approach to obtain a compact but sufficiently rich mechanical representation. Our choice is based on nonlinear compliance data in the actuator/effector space provided by a condensation of the FEM model. We demonstrate that this compact model can be learned with a reasonable amount of data and, at the same time, be very efficient in terms of modeling, since we can deduce the direct and inverse kinematics of the robot. We also show how to couple some models learned individually in particular on an example of a gripper composed of two soft fingers. Other results are shown by comparing the inverse model derived from the full FEM model and the one from the compact learned version. This wo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#27169;&#22411;Pionono&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#35266;&#23519;&#32773;&#38388;&#21644;&#35266;&#23519;&#32773;&#20869;&#21464;&#24322;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25429;&#25417;&#27599;&#20010;&#26631;&#35760;&#32773;&#30340;&#26631;&#35760;&#34892;&#20026;&#24182;&#23558;&#20854;&#19982;&#22270;&#20687;&#29305;&#24449;&#38598;&#25104;&#65292;&#20135;&#29983;&#27010;&#29575;&#20998;&#21106;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;Pionono&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#39044;&#27979;&#22810;&#20010;&#19968;&#33268;&#30340;&#20998;&#21106;&#22270;&#65292;&#20026;&#35786;&#26029;&#36807;&#31243;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.11397</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#35266;&#23519;&#32773;&#38388;&#21644;&#35266;&#23519;&#32773;&#20869;&#21464;&#24322;&#30340;&#27010;&#29575;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Modeling of Inter- and Intra-observer Variability in Medical Image Segmentation. (arXiv:2307.11397v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#27169;&#22411;Pionono&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#35266;&#23519;&#32773;&#38388;&#21644;&#35266;&#23519;&#32773;&#20869;&#21464;&#24322;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25429;&#25417;&#27599;&#20010;&#26631;&#35760;&#32773;&#30340;&#26631;&#35760;&#34892;&#20026;&#24182;&#23558;&#20854;&#19982;&#22270;&#20687;&#29305;&#24449;&#38598;&#25104;&#65292;&#20135;&#29983;&#27010;&#29575;&#20998;&#21106;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;Pionono&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#39044;&#27979;&#22810;&#20010;&#19968;&#33268;&#30340;&#20998;&#21106;&#22270;&#65292;&#20026;&#35786;&#26029;&#36807;&#31243;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#35266;&#23519;&#32773;&#38388;&#21644;&#35266;&#23519;&#32773;&#20869;&#30340;&#21464;&#24322;&#24615;&#65292;&#21363;&#20351;&#26159;&#22312;&#21307;&#23398;&#19987;&#23478;&#20043;&#38388;&#20063;&#23384;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;&#27010;&#29575;&#35266;&#23519;&#32773;&#38388;&#21644;&#35266;&#23519;&#32773;&#20869;&#21464;&#24322;&#32593;&#32476;&#65288;Pionono&#65289;&#12290;&#23427;&#36890;&#36807;&#22810;&#32500;&#27010;&#29575;&#20998;&#24067;&#25429;&#25417;&#27599;&#20010;&#26631;&#35760;&#32773;&#30340;&#26631;&#35760;&#34892;&#20026;&#65292;&#24182;&#23558;&#27492;&#20449;&#24687;&#19982;&#22270;&#20687;&#30340;&#29305;&#24449;&#22270;&#38598;&#25104;&#36215;&#26469;&#65292;&#20135;&#29983;&#27010;&#29575;&#20998;&#21106;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#22312;STAPLE&#12289;&#27010;&#29575;U-Net&#21644;&#22522;&#20110;&#28151;&#28102;&#30697;&#38453;&#30340;&#27169;&#22411;&#31561;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;Pionono&#39044;&#27979;&#22810;&#20010;&#19968;&#33268;&#30340;&#20998;&#21106;&#22270;&#65292;&#27169;&#25311;&#20102;&#35780;&#20998;&#32773;&#30340;&#19987;&#19994;&#24847;&#35265;&#65292;&#20026;&#35786;&#26029;&#36807;&#31243;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#12290;&#22312;&#30495;&#23454;&#30340;&#30284;&#30151;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;Pionono&#30340;&#39640;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#21307;&#23398;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical image segmentation is a challenging task, particularly due to interand intra-observer variability, even between medical experts. In this paper, we propose a novel model, called Probabilistic Inter-Observer and iNtra-Observer variation NetwOrk (Pionono). It captures the labeling behavior of each rater with a multidimensional probability distribution and integrates this information with the feature maps of the image to produce probabilistic segmentation predictions. The model is optimized by variational inference and can be trained end-to-end. It outperforms state-of-the-art models such as STAPLE, Probabilistic U-Net, and models based on confusion matrices. Additionally, Pionono predicts multiple coherent segmentation maps that mimic the rater's expert opinion, which provides additional valuable information for the diagnostic process. Experiments on real-world cancer segmentation datasets demonstrate the high accuracy and efficiency of Pionono, making it a powerful tool for med
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32508;&#21512;&#27979;&#37327;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;CFU&#65292;&#26088;&#22312;&#26377;&#25928;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#20013;&#30340;&#20844;&#27491;&#24615;-&#25928;&#29992;&#26435;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.11379</link><description>&lt;p&gt;
&#26397;&#30528;&#26356;&#22909;&#30340;&#20844;&#27491;&#24615;-&#25928;&#29992;&#26435;&#34913;&#30340;&#26041;&#21521;&#65306;&#19968;&#31181;&#22522;&#20110;&#32508;&#21512;&#27979;&#37327;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Towards Better Fairness-Utility Trade-off: A Comprehensive Measurement-Based Reinforcement Learning Framework. (arXiv:2307.11379v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32508;&#21512;&#27979;&#37327;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;CFU&#65292;&#26088;&#22312;&#26377;&#25928;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#20013;&#30340;&#20844;&#27491;&#24615;-&#25928;&#29992;&#26435;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#34987;&#24191;&#27867;&#29992;&#20110;&#20855;&#26377;&#31038;&#20250;&#24433;&#21709;&#30340;&#20915;&#31574;&#65292;&#22914;&#38134;&#34892;&#36151;&#27454;&#23457;&#26680;&#12289;&#21009;&#20107;&#21028;&#20915;&#21644;&#31616;&#21382;&#31579;&#36873;&#12290;&#22914;&#20309;&#22312;&#32500;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#30830;&#20445;&#20844;&#27491;&#24615;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#20844;&#27491;&#24615;&#26159;&#19968;&#20010;&#22797;&#26434;&#19988;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#27010;&#24565;&#65292;&#26377;&#36229;&#36807;70&#31181;&#19981;&#21516;&#30340;&#27979;&#37327;&#25351;&#26631;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#35268;&#23450;&#36890;&#24120;&#23545;&#20351;&#29992;&#21738;&#20010;&#25351;&#26631;&#27169;&#31946;&#19981;&#28165;&#65292;&#24182;&#19988;&#19981;&#21516;&#30340;&#32452;&#32455;&#21487;&#33021;&#20559;&#22909;&#19981;&#21516;&#30340;&#20844;&#27491;&#24615;&#25351;&#26631;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#36890;&#36807;&#32508;&#21512;&#25163;&#27573;&#26469;&#25913;&#21892;&#20844;&#27491;&#24615;&#12290;&#29616;&#26377;&#30340;&#32531;&#35299;&#25216;&#26415;&#36890;&#24120;&#38024;&#23545;&#29305;&#23450;&#30340;&#20844;&#27491;&#24615;&#25351;&#26631;&#65292;&#24182;&#19988;&#22312;&#21516;&#26102;&#25913;&#36827;&#22810;&#20010;&#20844;&#27491;&#24615;&#27010;&#24565;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CFU&#65288;Comprehensive Fairness-Utility&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20197;&#26377;&#25928;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#20013;&#30340;&#20844;&#27491;&#24615;-&#25928;&#29992;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning is widely used to make decisions with societal impact such as bank loan approving, criminal sentencing, and resume filtering. How to ensure its fairness while maintaining utility is a challenging but crucial issue. Fairness is a complex and context-dependent concept with over 70 different measurement metrics. Since existing regulations are often vague in terms of which metric to use and different organizations may prefer different fairness metrics, it is important to have means of improving fairness comprehensively. Existing mitigation techniques often target at one specific fairness metric and have limitations in improving multiple notions of fairness simultaneously. In this work, we propose CFU (Comprehensive Fairness-Utility), a reinforcement learning-based framework, to efficiently improve the fairness-utility trade-off in machine learning classifiers. A comprehensive measurement that can simultaneously consider multiple fairness notions as well as utility is estab
&lt;/p&gt;</description></item><item><title>LatentAugment&#26159;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#25805;&#32437;GAN&#30340;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#25968;&#25454;&#22686;&#24378;&#30340;&#20302;&#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#26356;&#22810;&#26679;&#21270;&#21644;&#30495;&#23454;&#24615;&#26356;&#39640;&#30340;&#21512;&#25104;&#22270;&#20687;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.11375</link><description>&lt;p&gt;
LatentAugment: &#36890;&#36807;&#24341;&#23548;&#25805;&#32437;GAN&#30340;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
LatentAugment: Data Augmentation via Guided Manipulation of GAN's Latent Space. (arXiv:2307.11375v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11375
&lt;/p&gt;
&lt;p&gt;
LatentAugment&#26159;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#25805;&#32437;GAN&#30340;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#25968;&#25454;&#22686;&#24378;&#30340;&#20302;&#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#26356;&#22810;&#26679;&#21270;&#21644;&#30495;&#23454;&#24615;&#26356;&#39640;&#30340;&#21512;&#25104;&#22270;&#20687;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#26159;&#19968;&#31181;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#20943;&#23569;&#36807;&#25311;&#21512;&#24182;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;&#25968;&#25454;&#22686;&#24378;&#21482;&#33021;&#20135;&#29983;&#20855;&#26377;&#26377;&#38480;&#22810;&#26679;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#20855;&#26377;&#30495;&#23454;&#22270;&#20687;&#22806;&#35266;&#30340;&#21512;&#25104;&#26679;&#26412;&#26469;&#35299;&#38145;&#25968;&#25454;&#38598;&#20013;&#30340;&#39069;&#22806;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#38590;&#20197;&#21516;&#26102;&#28385;&#36275;&#19977;&#20010;&#20851;&#38190;&#35201;&#27714;&#65306;&#20445;&#30495;&#24230;&#21644;&#39640;&#36136;&#37327;&#26679;&#26412;&#12289;&#22810;&#26679;&#24615;&#21644;&#27169;&#24335;&#35206;&#30422;&#12289;&#20197;&#21450;&#24555;&#36895;&#37319;&#26679;&#12290;&#23454;&#38469;&#19978;&#65292;GAN&#21487;&#20197;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#20294;&#27169;&#24335;&#35206;&#30422;&#24046;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#25968;&#25454;&#22686;&#24378;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LatentAugment&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#23427;&#20811;&#26381;&#20102;GAN&#30340;&#20302;&#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#20854;&#22312;&#25968;&#25454;&#22686;&#24378;&#24212;&#29992;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#22312;&#27809;&#26377;&#22806;&#37096;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;LatentAugment&#20462;&#25913;&#28508;&#22312;&#21521;&#37327;&#24182;&#23558;&#20854;&#31227;&#21160;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#29305;&#23450;&#21306;&#22495;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#22686;&#21152;&#21512;&#25104;&#22270;&#20687;&#30340;&#22810;&#26679;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation (DA) is a technique to increase the quantity and diversity of the training data, and by that alleviate overfitting and improve generalisation. However, standard DA produces synthetic data for augmentation with limited diversity. Generative Adversarial Networks (GANs) may unlock additional information in a dataset by generating synthetic samples having the appearance of real images. However, these models struggle to simultaneously address three key requirements: fidelity and high-quality samples; diversity and mode coverage; and fast sampling. Indeed, GANs generate high-quality samples rapidly, but have poor mode coverage, limiting their adoption in DA applications. We propose LatentAugment, a DA strategy that overcomes the low diversity of GANs, opening up for use in DA applications. Without external supervision, LatentAugment modifies latent vectors and moves them into latent space regions to maximise the synthetic images' diversity and fidelity. It is also agnostic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;Fenchel&#23545;&#20598;&#26041;&#27861;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#23398;&#20064;&#19982;&#19987;&#23478;&#30456;&#19968;&#33268;&#30340;&#22810;&#26679;&#30340;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11373</link><description>&lt;p&gt;
&#36890;&#36807;Fenchel&#23545;&#20598;&#23454;&#29616;&#22810;&#26679;&#30340;&#31163;&#32447;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Diverse Offline Imitation via Fenchel Duality. (arXiv:2307.11373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;Fenchel&#23545;&#20598;&#26041;&#27861;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#23398;&#20064;&#19982;&#19987;&#23478;&#30456;&#19968;&#33268;&#30340;&#22810;&#26679;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#39046;&#22495;&#65292;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21508;&#31181;&#24037;&#20316;&#25552;&#20986;&#20102;&#20197;&#20114;&#20449;&#24687;&#20026;&#22522;&#30784;&#30340;&#30446;&#26631;&#65292;&#20316;&#20026;&#20869;&#22312;&#39537;&#21160;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#38656;&#35201;&#22312;&#32447;&#29615;&#22659;&#35775;&#38382;&#30340;&#31639;&#27861;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;\textit{&#31163;&#32447;}&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#38382;&#39064;&#24418;&#24335;&#21270;&#32771;&#34385;&#20102;&#22312;KL-&#25955;&#24230;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30446;&#26631;&#12290;&#26356;&#30830;&#20999;&#22320;&#35828;&#65292;&#32422;&#26463;&#30830;&#20445;&#27599;&#20010;&#25216;&#33021;&#30340;&#29366;&#24577;&#21344;&#29992;&#20445;&#25345;&#22312;&#19968;&#20010;&#20855;&#26377;&#33391;&#22909;&#29366;&#24577;&#25805;&#20316;&#35206;&#30422;&#29575;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#33539;&#22260;&#20869;&#19982;&#19987;&#23478;&#30340;&#29366;&#24577;&#21344;&#29992;&#36924;&#36817;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#36830;&#25509;Fenchel&#23545;&#20598;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#65292;&#24182;&#32473;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#31163;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#19982;&#19987;&#23478;&#30456;&#19968;&#33268;&#30340;&#22810;&#26679;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been significant recent progress in the area of unsupervised skill discovery, with various works proposing mutual information based objectives, as a source of intrinsic motivation. Prior works predominantly focused on designing algorithms that require online access to the environment. In contrast, we develop an \textit{offline} skill discovery algorithm. Our problem formulation considers the maximization of a mutual information objective constrained by a KL-divergence. More precisely, the constraints ensure that the state occupancy of each skill remains close to the state occupancy of an expert, within the support of an offline dataset with good state-action coverage. Our main contribution is to connect Fenchel duality, reinforcement learning and unsupervised skill discovery, and to give a simple offline algorithm for learning diverse skills that are aligned with an expert.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#30340;&#36229;&#24179;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38543;&#26426;&#20998;&#31163;&#36229;&#24179;&#38754;&#23450;&#29702;&#65288;RSH&#65289;&#26469;&#21152;&#24378;&#20998;&#31163;&#36229;&#24179;&#38754;&#23450;&#29702;&#65292;&#21033;&#29992;RSH&#25105;&#20204;&#24471;&#21040;&#20102;&#22810;&#38754;&#20307;&#23398;&#20064;&#20013;&#30340;&#31639;&#27861;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.11371</link><description>&lt;p&gt;
&#38543;&#26426;&#20998;&#31163;&#36229;&#24179;&#38754;&#23450;&#29702;&#21644;&#23398;&#20064;&#22810;&#38754;&#20307;
&lt;/p&gt;
&lt;p&gt;
Random Separating Hyperplane Theorem and Learning Polytopes. (arXiv:2307.11371v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11371
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#30340;&#36229;&#24179;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38543;&#26426;&#20998;&#31163;&#36229;&#24179;&#38754;&#23450;&#29702;&#65288;RSH&#65289;&#26469;&#21152;&#24378;&#20998;&#31163;&#36229;&#24179;&#38754;&#23450;&#29702;&#65292;&#21033;&#29992;RSH&#25105;&#20204;&#24471;&#21040;&#20102;&#22810;&#38754;&#20307;&#23398;&#20064;&#20013;&#30340;&#31639;&#27861;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31163;&#36229;&#24179;&#38754;&#23450;&#29702;&#26159;&#20984;&#20960;&#20309;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#32467;&#26524;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#32467;&#26524;&#26159;&#38543;&#26426;&#20998;&#31163;&#36229;&#24179;&#38754;&#23450;&#29702;&#65288;RSH&#65289;&#65292;&#23427;&#26159;&#23545;&#22810;&#38754;&#20307;&#30340;&#19968;&#20010;&#21152;&#24378;&#12290;RSH&#26029;&#35328;&#65292;&#22914;&#26524;&#28857;a&#19982;&#20855;&#26377;k&#20010;&#39030;&#28857;&#21644;&#21333;&#20301;&#30452;&#24452;&#30340;&#22810;&#38754;&#20307;K&#22312;$\Re^d$&#20013;&#30340;&#36317;&#31163;&#33267;&#23569;&#20026;$\delta$&#65292;&#20854;&#20013;$\delta$&#26159;$(0,1)$&#20043;&#38388;&#30340;&#19968;&#20010;&#22266;&#23450;&#24120;&#25968;&#65292;&#21017;&#38543;&#26426;&#36873;&#25321;&#30340;&#36229;&#24179;&#38754;&#20197;&#33267;&#23569;$1/poly(k)$&#30340;&#27010;&#29575;&#23558;a&#21644;K&#20998;&#31163;&#65292;&#24182;&#19988;&#36793;&#30028;&#33267;&#23569;&#20026;$\Omega \left( \delta/\sqrt{d} \right)$&#12290;&#25105;&#20204;&#32467;&#26524;&#30340;&#19968;&#20010;&#30452;&#25509;&#25512;&#35770;&#26159;&#65292;&#39318;&#27425;&#36817;&#20046;&#26368;&#20248;&#30340;&#36793;&#30028;&#22312;&#20174;&#20998;&#31163;&#39044;&#35328;&#26426;&#21040;&#20248;&#21270;&#39044;&#35328;&#26426;&#30340;&#32422;&#31616;&#20013;&#38169;&#35823;&#22686;&#21152;&#12290;RSH&#22312;&#23398;&#20064;&#22810;&#38754;&#20307;&#20013;&#20855;&#26377;&#31639;&#27861;&#24212;&#29992;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#34987;&#31216;&#20026;&#8220;Hausdorff&#38382;&#39064;&#8221;&#65292;&#21363;&#22312;&#32473;&#23450;&#20102;&#20851;&#20110;K&#30340;&#20248;&#21270;&#39044;&#35328;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#19968;&#20010;&#21333;&#20301;&#30452;&#24452;&#22810;&#38754;&#20307;K&#65292;&#20351;&#20854;&#22312;Hausdorff&#36317;&#31163;$\delta$&#20869;&#12290;&#21033;&#29992;RSH&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#21487;&#20197;&#23398;&#20064;&#22810;&#38754;&#20307;K&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Separating Hyperplane theorem is a fundamental result in Convex Geometry with myriad applications. Our first result, Random Separating Hyperplane Theorem (RSH), is a strengthening of this for polytopes. $\rsh$ asserts that if the distance between $a$ and a polytope $K$ with $k$ vertices and unit diameter in $\Re^d$ is at least $\delta$, where $\delta$ is a fixed constant in $(0,1)$, then a randomly chosen hyperplane separates $a$ and $K$ with probability at least $1/poly(k)$ and margin at least $\Omega \left(\delta/\sqrt{d} \right)$. An immediate consequence of our result is the first near optimal bound on the error increase in the reduction from a Separation oracle to an Optimization oracle over a polytope.  RSH has algorithmic applications in learning polytopes. We consider a fundamental problem, denoted the ``Hausdorff problem'', of learning a unit diameter polytope $K$ within Hausdorff distance $\delta$, given an optimization oracle for $K$. Using RSH, we show that with polynom
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20013;&#30340;&#29616;&#23454;&#24046;&#36317;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#32553;&#23567;&#29616;&#23454;&#24046;&#36317;&#30340;&#31574;&#30053;&#65306;&#22495;&#38543;&#26426;&#21270;&#21644;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#12290;&#36825;&#20123;&#31574;&#30053;&#22312;&#19968;&#20010;&#20132;&#36890;&#20223;&#30495;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#20132;&#36890;&#20223;&#30495;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.11357</link><description>&lt;p&gt;
&#21033;&#29992;&#22495;&#38543;&#26426;&#21270;&#21644;&#20803;&#23398;&#20064;&#26469;&#32553;&#23567;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20013;&#30340;&#29616;&#23454;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Bridging the Reality Gap of Reinforcement Learning based Traffic Signal Control using Domain Randomization and Meta Learning. (arXiv:2307.11357v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11357
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20013;&#30340;&#29616;&#23454;&#24046;&#36317;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#32553;&#23567;&#29616;&#23454;&#24046;&#36317;&#30340;&#31574;&#30053;&#65306;&#22495;&#38543;&#26426;&#21270;&#21644;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#12290;&#36825;&#20123;&#31574;&#30053;&#22312;&#19968;&#20010;&#20132;&#36890;&#20223;&#30495;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#20132;&#36890;&#20223;&#30495;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#25506;&#32034;&#65292;&#20294;&#26159;&#30446;&#21069;&#23578;&#26410;&#26377;&#36825;&#26679;&#30340;&#31995;&#32479;&#22312;&#23454;&#36341;&#20013;&#24471;&#21040;&#37096;&#32626;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#21487;&#33021;&#23548;&#33268;&#29616;&#23454;&#24046;&#36317;&#30340;&#27169;&#25311;&#21442;&#25968;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20004;&#31181;&#26377;&#26395;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#30340;&#31574;&#30053;&#65306;&#22495;&#38543;&#26426;&#21270;&#21644;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#12290;&#36825;&#20004;&#31181;&#31574;&#30053;&#37117;&#22312;&#19968;&#20010;&#20132;&#36890;&#20223;&#30495;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#34987;&#23884;&#20837;&#21040;LemgoRL&#26694;&#26550;&#20013;&#65292;&#35813;&#26694;&#26550;&#23558;&#36924;&#30495;&#30340;&#12289;&#23433;&#20840;&#20851;&#38190;&#30340;&#35201;&#27714;&#25972;&#21512;&#21040;&#25511;&#21046;&#31995;&#32479;&#20013;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#19981;&#21516;&#30340;&#20132;&#36890;&#20223;&#30495;&#27169;&#22411;&#19978;&#35780;&#20272;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has been widely explored in Traffic Signal Control (TSC) applications, however, still no such system has been deployed in practice. A key barrier to progress in this area is the reality gap, the discrepancy that results from differences between simulation models and their real-world equivalents. In this paper, we address this challenge by first presenting a comprehensive analysis of potential simulation parameters that contribute to this reality gap. We then also examine two promising strategies that can bridge this gap: Domain Randomization (DR) and Model-Agnostic Meta-Learning (MAML). Both strategies were trained with a traffic simulation model of an intersection. In addition, the model was embedded in LemgoRL, a framework that integrates realistic, safety-critical requirements into the control system. Subsequently, we evaluated the performance of the two methods on a separate model of the same intersection that was developed with a different traffic simul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#38543;&#26426;&#29305;&#24449;&#20998;&#26512;&#65292;&#23545;&#21333;&#20010;&#27880;&#24847;&#23618;&#30340;&#23398;&#20064;&#21644;&#27867;&#21270;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#38543;&#26426;&#37319;&#26679;&#30340;&#20851;&#38190;&#30697;&#38453;&#21644;&#21487;&#35757;&#32451;&#20540;&#30697;&#38453;&#30340;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#29305;&#24449;&#27880;&#24847;&#23618;&#21487;&#20197;&#34920;&#31034;&#19968;&#31867;&#19982;&#20851;&#38190;&#21521;&#37327;&#32622;&#25442;&#26080;&#20851;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#25552;&#20379;&#20102;&#23398;&#20064;&#36825;&#20123;&#30446;&#26631;&#20989;&#25968;&#30340;&#39118;&#38505;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2307.11353</link><description>&lt;p&gt;
&#19968;&#20010;&#21333;&#19968;&#30340;&#27880;&#24847;&#23618;&#33021;&#23398;&#21040;&#20160;&#20040;&#65311;&#36890;&#36807;&#38543;&#26426;&#29305;&#24449;&#35270;&#35282;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
What can a Single Attention Layer Learn? A Study Through the Random Features Lens. (arXiv:2307.11353v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#38543;&#26426;&#29305;&#24449;&#20998;&#26512;&#65292;&#23545;&#21333;&#20010;&#27880;&#24847;&#23618;&#30340;&#23398;&#20064;&#21644;&#27867;&#21270;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#38543;&#26426;&#37319;&#26679;&#30340;&#20851;&#38190;&#30697;&#38453;&#21644;&#21487;&#35757;&#32451;&#20540;&#30697;&#38453;&#30340;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#29305;&#24449;&#27880;&#24847;&#23618;&#21487;&#20197;&#34920;&#31034;&#19968;&#31867;&#19982;&#20851;&#38190;&#21521;&#37327;&#32622;&#25442;&#26080;&#20851;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#25552;&#20379;&#20102;&#23398;&#20064;&#36825;&#20123;&#30446;&#26631;&#20989;&#25968;&#30340;&#39118;&#38505;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#23618;&#26159;Transformer&#26550;&#26500;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#24207;&#21015;&#26144;&#23556;&#21040;&#36755;&#20986;&#24207;&#21015;&#65292;&#22312;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#31361;&#30772;&#12290;&#26412;&#25991;&#23545;&#21333;&#20010;&#22810;&#22836;&#27880;&#24847;&#23618;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#36755;&#20837;&#26159;&#19968;&#31995;&#21015;&#20851;&#38190;&#21521;&#37327;&#21644;&#19968;&#20010;&#29420;&#31435;&#30340;&#26597;&#35810;&#21521;&#37327;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#38543;&#26426;&#29305;&#24449;&#35774;&#32622;&#65292;&#20854;&#20013;&#27880;&#24847;&#23618;&#20855;&#26377;&#22823;&#37327;&#22836;&#37096;&#65292;&#20855;&#26377;&#38543;&#26426;&#37319;&#26679;&#30340;&#20923;&#32467;&#26597;&#35810;&#21644;&#20851;&#38190;&#30697;&#38453;&#20197;&#21450;&#21487;&#35757;&#32451;&#30340;&#20540;&#30697;&#38453;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#19968;&#20010;&#38543;&#26426;&#29305;&#24449;&#30340;&#27880;&#24847;&#23618;&#21487;&#20197;&#34920;&#31034;&#19968;&#31867;&#19982;&#20851;&#38190;&#21521;&#37327;&#32622;&#25442;&#26080;&#20851;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#23398;&#20064;&#36825;&#20123;&#30446;&#26631;&#20989;&#25968;&#30340;&#23450;&#37327;&#36807;&#37327;&#39118;&#38505;&#30028;&#38480;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#22836;&#37096;&#21644;&#38543;&#26426;&#29305;&#24449;&#27880;&#24847;&#23618;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#30456;&#27604;&#29616;&#26377;&#30340;&#38543;&#26426;&#32447;&#24615;&#21151;&#33021;&#27169;&#22411;&#26377;&#20960;&#20010;&#27880;&#24847;&#32467;&#26500;&#19978;&#30340;&#29420;&#29305;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention layers -- which map a sequence of inputs to a sequence of outputs -- are core building blocks of the Transformer architecture which has achieved significant breakthroughs in modern artificial intelligence. This paper presents a rigorous theoretical study on the learning and generalization of a single multi-head attention layer, with a sequence of key vectors and a separate query vector as input. We consider the random feature setting where the attention layer has a large number of heads, with randomly sampled frozen query and key matrices, and trainable value matrices. We show that such a random-feature attention layer can express a broad class of target functions that are permutation invariant to the key vectors. We further provide quantitative excess risk bounds for learning these target functions from finite samples, using random feature attention with finitely many heads.  Our results feature several implications unique to the attention structure compared with existing ra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;$\texttt{Count-MORL}$&#65292;&#36890;&#36807;&#21033;&#29992;&#35745;&#25968;&#20445;&#23432;&#24615;&#26469;&#37327;&#21270;&#27169;&#22411;&#20272;&#35745;&#35823;&#24046;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#35745;&#25968;&#20445;&#23432;&#24615;&#22312;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#25552;&#20379;&#20102;&#25509;&#36817;&#26368;&#20248;&#24615;&#33021;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.11352</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#22522;&#20110;&#35745;&#25968;&#20445;&#23432;&#24615;&#30340;&#32467;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Model-based Offline Reinforcement Learning with Count-based Conservatism. (arXiv:2307.11352v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;$\texttt{Count-MORL}$&#65292;&#36890;&#36807;&#21033;&#29992;&#35745;&#25968;&#20445;&#23432;&#24615;&#26469;&#37327;&#21270;&#27169;&#22411;&#20272;&#35745;&#35823;&#24046;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#35745;&#25968;&#20445;&#23432;&#24615;&#22312;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#25552;&#20379;&#20102;&#25509;&#36817;&#26368;&#20248;&#24615;&#33021;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{Count-MORL}$&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#20102;&#29366;&#24577;&#21160;&#20316;&#23545;&#30340;&#35745;&#25968;&#20272;&#35745;&#26469;&#37327;&#21270;&#27169;&#22411;&#20272;&#35745;&#35823;&#24046;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#20010;&#35777;&#26126;&#20102;&#22522;&#20110;&#35745;&#25968;&#20445;&#23432;&#24615;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#20272;&#35745;&#35823;&#24046;&#19982;&#29366;&#24577;&#21160;&#20316;&#23545;&#30340;&#39057;&#29575;&#25104;&#21453;&#27604;&#30340;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#22522;&#20110;&#35745;&#25968;&#20445;&#23432;&#27169;&#22411;&#19979;&#23398;&#20064;&#30340;&#31574;&#30053;&#25552;&#20379;&#20102;&#25509;&#36817;&#26368;&#20248;&#24615;&#33021;&#30340;&#20445;&#35777;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#20351;&#29992;&#21704;&#24076;&#32534;&#30721;&#23454;&#29616;&#30340;$\texttt{Count-MORL}$&#22312;D4RL&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;$\href{https://github.com/oh-lab/Count-MORL}{https://github.com/oh-lab/Count-MORL}$&#19978;&#36827;&#34892;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a model-based offline reinforcement learning method that integrates count-based conservatism, named $\texttt{Count-MORL}$. Our method utilizes the count estimates of state-action pairs to quantify model estimation error, marking the first algorithm of demonstrating the efficacy of count-based conservatism in model-based offline deep RL to the best of our knowledge. For our proposed method, we first show that the estimation error is inversely proportional to the frequency of state-action pairs. Secondly, we demonstrate that the learned policy under the count-based conservative model offers near-optimality performance guarantees. Through extensive numerical experiments, we validate that $\texttt{Count-MORL}$ with hash code implementation significantly outperforms existing offline RL algorithms on the D4RL benchmark datasets. The code is accessible at $\href{https://github.com/oh-lab/Count-MORL}{https://github.com/oh-lab/Count-MORL}$.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#20302;&#21442;&#25968;&#35268;&#21010;&#36873;&#25321;&#24615;&#25512;&#26029;&#35745;&#31639;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;p&#20540;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#26469;&#20445;&#35777;&#25152;&#38656;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.11351</link><description>&lt;p&gt;
&#21442;&#25968;&#35268;&#21010;&#30340;&#36873;&#25321;&#24615;&#25512;&#26029;&#20013;&#30340;&#26377;&#30028;P&#20540;
&lt;/p&gt;
&lt;p&gt;
Bounded P-values in Parametric Programming-based Selective Inference. (arXiv:2307.11351v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#20302;&#21442;&#25968;&#35268;&#21010;&#36873;&#25321;&#24615;&#25512;&#26029;&#35745;&#31639;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;p&#20540;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#26469;&#20445;&#35777;&#25152;&#38656;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#24615;&#25512;&#26029;&#65288;SI&#65289;&#20316;&#20026;&#19968;&#31181;&#36866;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20551;&#35774;&#26816;&#39564;&#30340;&#26377;&#21069;&#26223;&#30340;&#26694;&#26550;&#65292;&#19968;&#30452;&#21463;&#21040;&#30740;&#31350;&#20851;&#27880;&#12290;SI&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#20551;&#35774;&#34987;&#36873;&#20013;&#30340;&#20107;&#20214;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#25512;&#26029;&#12290;&#20026;&#20102;&#36827;&#34892;SI&#65292;&#24517;&#39035;&#20197;&#21487;&#36861;&#36394;&#30340;&#24418;&#24335;&#23545;&#36825;&#20010;&#20107;&#20214;&#36827;&#34892;&#25551;&#36848;&#12290;&#24403;&#36873;&#25321;&#20107;&#20214;&#38590;&#20197;&#25551;&#36848;&#26102;&#65292;&#21487;&#20197;&#24341;&#20837;&#39069;&#22806;&#30340;&#26465;&#20214;&#20197;&#20351;&#20854;&#21487;&#22788;&#29702;&#12290;&#36825;&#20123;&#39069;&#22806;&#30340;&#26465;&#20214;&#24448;&#24448;&#20250;&#23548;&#33268;&#21151;&#25928;&#30340;&#25439;&#22833;&#65292;&#36825;&#19968;&#38382;&#39064;&#34987;&#31216;&#20026;&#36807;&#24230;&#26465;&#20214;&#21270;&#12290;&#22522;&#20110;&#21442;&#25968;&#35268;&#21010;&#30340;SI&#65288;PP-based SI&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#35299;&#20915;&#36807;&#24230;&#26465;&#20214;&#21270;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;PP-based SI&#30340;&#20027;&#35201;&#38382;&#39064;&#26159;&#30001;&#20110;&#38656;&#35201;&#23436;&#20840;&#22320;&#25506;&#32034;&#25968;&#25454;&#31354;&#38388;&#32780;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#35777;&#25152;&#38656;&#31934;&#24230;&#65292;&#36890;&#36807;&#25552;&#20986;&#35745;&#31639;p&#20540;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#25628;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selective inference (SI) has been actively studied as a promising framework for statistical hypothesis testing for data-driven hypotheses. The basic idea of SI is to make inferences conditional on an event that a hypothesis is selected. In order to perform SI, this event must be characterized in a traceable form. When selection event is too difficult to characterize, additional conditions are introduced for tractability. This additional conditions often causes the loss of power, and this issue is referred to as over-conditioning. Parametric programming-based SI (PP-based SI) has been proposed as one way to address the over-conditioning issue. The main problem of PP-based SI is its high computational cost due to the need to exhaustively explore the data space. In this study, we introduce a procedure to reduce the computational cost while guaranteeing the desired precision, by proposing a method to compute the upper and lower bounds of p-values. We also proposed three types of search str
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#36125;&#21494;&#26031;&#20844;&#24335;&#24212;&#29992;&#20110;&#27169;&#22411;&#21442;&#25968;&#21644;&#27169;&#22411;&#36755;&#20837;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#23545;&#25239;&#24615;&#26679;&#26412;&#21487;&#36801;&#31227;&#24615;&#30340;&#26041;&#27861;&#65292;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#20855;&#26377;&#26174;&#33879;&#25552;&#39640;&#25928;&#26524;&#65292;&#24182;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#26032;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2307.11334</link><description>&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#25915;&#20987;&#25552;&#39640;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#21487;&#36801;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Transferability of Adversarial Examples via Bayesian Attacks. (arXiv:2307.11334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11334
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#36125;&#21494;&#26031;&#20844;&#24335;&#24212;&#29992;&#20110;&#27169;&#22411;&#21442;&#25968;&#21644;&#27169;&#22411;&#36755;&#20837;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#23545;&#25239;&#24615;&#26679;&#26412;&#21487;&#36801;&#31227;&#24615;&#30340;&#26041;&#27861;&#65292;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#20855;&#26377;&#26174;&#33879;&#25552;&#39640;&#25928;&#26524;&#65292;&#24182;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#23545;&#25105;&#20204;&#22312;ICLR&#19978;&#21457;&#34920;&#24037;&#20316;&#30340;&#37325;&#35201;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;ICLR&#24037;&#20316;&#25552;&#20986;&#20102;&#23558;&#36125;&#21494;&#26031;&#20844;&#24335;&#24212;&#29992;&#20110;&#27169;&#22411;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#20174;&#32780;&#26377;&#25928;&#27169;&#25311;&#20102;&#26080;&#38480;&#22810;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#21512;&#12290;&#32780;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#36125;&#21494;&#26031;&#20844;&#24335;&#24212;&#29992;&#20110;&#27169;&#22411;&#36755;&#20837;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#23637;&#65292;&#20351;&#24471;&#27169;&#22411;&#36755;&#20837;&#21644;&#27169;&#22411;&#21442;&#25968;&#37117;&#33021;&#22815;&#36827;&#34892;&#32852;&#21512;&#22810;&#26679;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#65306;1&#65289;&#23545;&#27169;&#22411;&#36755;&#20837;&#21644;&#27169;&#22411;&#21442;&#25968;&#21516;&#26102;&#24212;&#29992;&#36125;&#21494;&#26031;&#20844;&#24335;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21487;&#36801;&#31227;&#24615;&#65307;2&#65289;&#36890;&#36807;&#24341;&#20837;&#23545;&#27169;&#22411;&#36755;&#20837;&#21518;&#39564;&#20998;&#24067;&#30340;&#39640;&#32423;&#36817;&#20284;&#65292;&#25915;&#20987;&#26080;&#38656;&#27169;&#22411;&#24494;&#35843;&#26102;&#65292;&#23545;&#25239;&#24615;&#21487;&#36801;&#31227;&#24615;&#24471;&#21040;&#36827;&#19968;&#27493;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#25152;&#26377;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#23545;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a substantial extension of our work published at ICLR. Our ICLR work advocated for enhancing transferability in adversarial examples by incorporating a Bayesian formulation into model parameters, which effectively emulates the ensemble of infinitely many deep neural networks, while, in this paper, we introduce a novel extension by incorporating the Bayesian formulation into the model input as well, enabling the joint diversification of both the model input and model parameters. Our empirical findings demonstrate that: 1) the combination of Bayesian formulations for both the model input and model parameters yields significant improvements in transferability; 2) by introducing advanced approximations of the posterior distribution over the model input, adversarial transferability achieves further enhancement, surpassing all state-of-the-arts when attacking without model fine-tuning. Moreover, we propose a principled approach to fine-tune model parameters in such an ext
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#20449;&#24687;&#35770;&#30340;&#37096;&#20998;&#20449;&#24687;&#20998;&#35299;&#65288;PID&#65289;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20851;&#20110;&#25935;&#24863;&#23646;&#24615;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#26435;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;&#20998;&#35299;&#21457;&#29616;&#20102;&#19977;&#31181;&#19981;&#20844;&#24179;&#26469;&#28304;&#65292;&#20998;&#21035;&#26159;&#21807;&#19968;&#19981;&#24179;&#31561;&#24615;&#12289;&#20887;&#20313;&#19981;&#24179;&#31561;&#24615;&#21644;&#25513;&#30422;&#19981;&#24179;&#31561;&#24615;&#65292;&#25581;&#31034;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#22522;&#26412;&#38480;&#21046;&#21644;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2307.11333</link><description>&lt;p&gt;
&#25581;&#31034;&#32852;&#37030;&#23398;&#20064;&#20013;&#23616;&#37096;&#21644;&#20840;&#23616;&#20844;&#24179;&#24615;&#26435;&#34913;&#30340;&#20449;&#24687;&#20998;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Demystifying Local and Global Fairness Trade-offs in Federated Learning Using Partial Information Decomposition. (arXiv:2307.11333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#20449;&#24687;&#35770;&#30340;&#37096;&#20998;&#20449;&#24687;&#20998;&#35299;&#65288;PID&#65289;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20851;&#20110;&#25935;&#24863;&#23646;&#24615;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#26435;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;&#20998;&#35299;&#21457;&#29616;&#20102;&#19977;&#31181;&#19981;&#20844;&#24179;&#26469;&#28304;&#65292;&#20998;&#21035;&#26159;&#21807;&#19968;&#19981;&#24179;&#31561;&#24615;&#12289;&#20887;&#20313;&#19981;&#24179;&#31561;&#24615;&#21644;&#25513;&#30422;&#19981;&#24179;&#31561;&#24615;&#65292;&#25581;&#31034;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#22522;&#26412;&#38480;&#21046;&#21644;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#65292;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20851;&#20110;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#12289;&#31181;&#26063;&#31561;&#65289;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#26435;&#34913;&#38382;&#39064;&#12290;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#8220;&#20840;&#23616;&#20844;&#24179;&#24615;&#8221;&#65288;&#27169;&#22411;&#22312;&#25152;&#26377;&#23458;&#25143;&#31471;&#19978;&#30340;&#19981;&#24179;&#31561;&#31243;&#24230;&#65289;&#25110;&#8220;&#23616;&#37096;&#20844;&#24179;&#24615;&#8221;&#65288;&#27169;&#22411;&#22312;&#27599;&#20010;&#20010;&#20307;&#23458;&#25143;&#31471;&#19978;&#30340;&#19981;&#24179;&#31561;&#31243;&#24230;&#65289;&#65292;&#32780;&#24182;&#19981;&#24635;&#26159;&#32771;&#34385;&#23427;&#20204;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#23545;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20840;&#23616;&#20844;&#24179;&#24615;&#21644;&#23616;&#37096;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#19968;&#20010;&#26159;&#21542;&#26263;&#31034;&#21478;&#19968;&#20010;&#65292;&#25105;&#20204;&#32570;&#20047;&#29702;&#35299;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#20449;&#24687;&#35770;&#20013;&#30340;&#37096;&#20998;&#20449;&#24687;&#20998;&#35299;&#65288;PID&#65289;&#26041;&#27861;&#65292;&#39318;&#20808;&#30830;&#23450;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#19977;&#31181;&#19981;&#20844;&#24179;&#26469;&#28304;&#65292;&#21363;&#8220;&#21807;&#19968;&#19981;&#24179;&#31561;&#24615;&#8221;&#12289;&#8220;&#20887;&#20313;&#19981;&#24179;&#31561;&#24615;&#8221;&#21644;&#8220;&#25513;&#30422;&#19981;&#24179;&#31561;&#24615;&#8221;&#12290;&#36890;&#36807;&#20856;&#22411;&#26696;&#20363;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#36825;&#19977;&#31181;&#19981;&#24179;&#31561;&#24615;&#22914;&#20309;&#24433;&#21709;&#20840;&#23616;&#21644;&#23616;&#37096;&#20844;&#24179;&#24615;&#12290;&#36825;&#31181;&#20998;&#35299;&#24110;&#21161;&#25105;&#20204;&#25512;&#23548;&#20986;&#20840;&#23616;&#21644;&#23616;&#37096;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#22522;&#26412;&#38480;&#21046;&#21644;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present an information-theoretic perspective to group fairness trade-offs in federated learning (FL) with respect to sensitive attributes, such as gender, race, etc. Existing works mostly focus on either \emph{global fairness} (overall disparity of the model across all clients) or \emph{local fairness} (disparity of the model at each individual client), without always considering their trade-offs. There is a lack of understanding of the interplay between global and local fairness in FL, and if and when one implies the other. To address this gap, we leverage a body of work in information theory called partial information decomposition (PID) which first identifies three sources of unfairness in FL, namely, \emph{Unique Disparity}, \emph{Redundant Disparity}, and \emph{Masked Disparity}. Using canonical examples, we demonstrate how these three disparities contribute to global and local fairness. This decomposition helps us derive fundamental limits and trade-offs between
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#21442;&#25968;&#21487;&#35782;&#21035;&#24615;&#12290;&#36890;&#36807;&#19968;&#20010;&#36816;&#21160;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#21442;&#25968;&#20272;&#35745;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#34429;&#28982;&#26576;&#20123;&#21442;&#25968;&#21487;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#35782;&#21035;&#20986;&#26469;&#65292;&#20294;&#20854;&#20182;&#21442;&#25968;&#20173;&#28982;&#19981;&#21487;&#35782;&#21035;&#12290;&#36825;&#34920;&#26126;&#19981;&#21487;&#35782;&#21035;&#24615;&#26159;&#23454;&#39564;&#35774;&#32622;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#38656;&#35201;&#25913;&#21464;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.11332</link><description>&lt;p&gt;
&#36229;&#36234;&#25910;&#25947;&#24615;&#65306;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35782;&#21035;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond Convergence: Identifiability of Machine Learning and Deep Learning Models. (arXiv:2307.11332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#21442;&#25968;&#21487;&#35782;&#21035;&#24615;&#12290;&#36890;&#36807;&#19968;&#20010;&#36816;&#21160;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#21442;&#25968;&#20272;&#35745;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#34429;&#28982;&#26576;&#20123;&#21442;&#25968;&#21487;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#35782;&#21035;&#20986;&#26469;&#65292;&#20294;&#20854;&#20182;&#21442;&#25968;&#20173;&#28982;&#19981;&#21487;&#35782;&#21035;&#12290;&#36825;&#34920;&#26126;&#19981;&#21487;&#35782;&#21035;&#24615;&#26159;&#23454;&#39564;&#35774;&#32622;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#38656;&#35201;&#25913;&#21464;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21442;&#25968;&#20248;&#21270;&#21644;&#22238;&#24402;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#26426;&#22120;&#23398;&#20064;&#30340;&#36870;&#38382;&#39064;&#37117;&#26159;&#8220;&#21487;&#35782;&#21035;&#30340;&#8221;&#65292;&#36825;&#24847;&#21619;&#30528;&#27169;&#22411;&#21442;&#25968;&#21487;&#33021;&#26080;&#27861;&#20174;&#21487;&#29992;&#25968;&#25454;&#21644;&#25968;&#25454;&#27169;&#22411;&#30340;&#36755;&#20837;-&#36755;&#20986;&#20851;&#31995;&#20013;&#21807;&#19968;&#30830;&#23450;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#20197;&#36816;&#21160;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#21442;&#25968;&#20272;&#35745;&#20026;&#37325;&#28857;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#21442;&#25968;&#21487;&#35782;&#21035;&#24615;&#30340;&#27010;&#24565;&#12290;&#21033;&#29992;&#21452;&#36275;&#24377;&#31783;&#36136;&#28857;&#20154;&#31867;&#34892;&#36208;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#34920;&#31034;&#19981;&#21516;&#27493;&#24577;&#27169;&#24335;&#21644;&#26465;&#20214;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#23581;&#35797;&#20272;&#35745;&#20010;&#20307;&#21442;&#25968;&#65292;&#21253;&#25324;&#36136;&#37327;&#12289;&#21018;&#24230;&#21644;&#24179;&#34913;&#33151;&#38271;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#26576;&#20123;&#21442;&#25968;&#21487;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#35782;&#21035;&#20986;&#26469;&#65292;&#20294;&#20854;&#20182;&#21442;&#25968;&#20173;&#28982;&#19981;&#21487;&#35782;&#21035;&#65292;&#36825;&#20984;&#26174;&#20102;&#19981;&#21487;&#35782;&#21035;&#24615;&#26159;&#23454;&#39564;&#35774;&#32622;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#38656;&#35201;&#25913;&#21464;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) and deep learning models are extensively used for parameter optimization and regression problems. However, not all inverse problems in ML are ``identifiable,'' indicating that model parameters may not be uniquely determined from the available data and the data model's input-output relationship. In this study, we investigate the notion of model parameter identifiability through a case study focused on parameter estimation from motion sensor data. Utilizing a bipedal-spring mass human walk dynamics model, we generate synthetic data representing diverse gait patterns and conditions. Employing a deep neural network, we attempt to estimate subject-wise parameters, including mass, stiffness, and equilibrium leg length. The results show that while certain parameters can be identified from the observation data, others remain unidentifiable, highlighting that unidentifiability is an intrinsic limitation of the experimental setup, necessitating a change in data collection a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36866;&#24212;&#21040;&#34394;&#25311;&#29615;&#22659;&#20013;&#65292;&#20197;&#23454;&#29616;&#20154;&#26426;&#21512;&#20316;&#20013;&#33258;&#28982;&#21644;&#30452;&#35266;&#30340;&#25163;&#21183;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2307.11327</link><description>&lt;p&gt;
&#20174;&#30495;&#23454;&#29615;&#22659;&#21040;&#34394;&#25311;&#29615;&#22659;&#30340;&#36890;&#20449;&#32858;&#28966;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31995;&#32479;&#36866;&#24212;&#65306;&#29992;&#20110;&#20154;&#26426;&#21512;&#20316;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Systematic Adaptation of Communication-focused Machine Learning Models from Real to Virtual Environments for Human-Robot Collaboration. (arXiv:2307.11327v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11327
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36866;&#24212;&#21040;&#34394;&#25311;&#29615;&#22659;&#20013;&#65292;&#20197;&#23454;&#29616;&#20154;&#26426;&#21512;&#20316;&#20013;&#33258;&#28982;&#21644;&#30452;&#35266;&#30340;&#25163;&#21183;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#29616;&#23454;&#22312;&#28216;&#25103;&#12289;&#21307;&#23398;&#12289;&#22521;&#35757;&#20197;&#21450;&#20154;&#26426;&#21512;&#20316;&#30028;&#38754;&#30340;&#24320;&#21457;&#20013;&#34920;&#29616;&#20986;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#21033;&#29992;&#33258;&#28982;&#21644;&#30452;&#35266;&#30340;&#25163;&#21183;&#35782;&#21035;&#20197;&#23454;&#29616;&#21327;&#20316;&#26426;&#22120;&#20154;&#30340;&#20307;&#29616;&#24335;&#36828;&#31243;&#25805;&#20316;&#65292;&#38656;&#35201;&#21019;&#24314;&#22823;&#22411;&#30340;&#26631;&#27880;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#19981;&#21516;&#30340;&#24212;&#29992;&#65292;&#36825;&#21487;&#33021;&#22312;&#35745;&#31639;&#25110;&#32463;&#27982;&#19978;&#20855;&#26377;&#38480;&#21046;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Virtual reality has proved to be useful in applications in several fields ranging from gaming, medicine, and training to development of interfaces that enable human-robot collaboration. It empowers designers to explore applications outside of the constraints posed by the real world environment and develop innovative solutions and experiences. Hand gestures recognition which has been a topic of much research and subsequent commercialization in the real world has been possible because of the creation of large, labelled datasets. In order to utilize the power of natural and intuitive hand gestures in the virtual domain for enabling embodied teleoperation of collaborative robots, similarly large datasets must be created so as to keep the working interface easy to learn and flexible enough to add more gestures. Depending on the application, this may be computationally or economically prohibitive. Thus, the adaptation of trained deep learning models that perform well in the real environment 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#33832;&#36203;&#21202;&#20197;&#21335;&#38750;&#27954;&#35937;&#31227;&#21160;&#30340;&#27169;&#24335;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#23395;&#33410;&#21464;&#21270;&#21644;&#38477;&#38632;&#27169;&#24335;&#31561;&#21160;&#24577;&#39537;&#21160;&#22240;&#32032;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#39044;&#27979;&#29983;&#24577;&#22240;&#32032;&#23545;&#35937;&#36801;&#24473;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#24182;&#20026;&#21046;&#23450;&#20445;&#25252;&#31574;&#30053;&#25552;&#20379;&#20102;&#32508;&#21512;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2307.11325</link><description>&lt;p&gt;
&#33832;&#36203;&#21202;&#20197;&#21335;&#38750;&#27954;&#35937;&#31227;&#21160;&#30340;&#20998;&#26512;&#65306;&#29983;&#24577;&#23398;&#12289;&#27668;&#20505;&#21644;&#20445;&#25252;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Analysis of Elephant Movement in Sub-Saharan Africa: Ecological, Climatic, and Conservation Perspectives. (arXiv:2307.11325v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#33832;&#36203;&#21202;&#20197;&#21335;&#38750;&#27954;&#35937;&#31227;&#21160;&#30340;&#27169;&#24335;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#23395;&#33410;&#21464;&#21270;&#21644;&#38477;&#38632;&#27169;&#24335;&#31561;&#21160;&#24577;&#39537;&#21160;&#22240;&#32032;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#39044;&#27979;&#29983;&#24577;&#22240;&#32032;&#23545;&#35937;&#36801;&#24473;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#24182;&#20026;&#21046;&#23450;&#20445;&#25252;&#31574;&#30053;&#25552;&#20379;&#20102;&#32508;&#21512;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35937;&#19982;&#29615;&#22659;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#29983;&#24577;&#23398;&#21644;&#20445;&#25252;&#31574;&#30053;&#37117;&#26377;&#28145;&#36828;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#26041;&#27861;&#26469;&#35299;&#35835;&#33832;&#36203;&#21202;&#20197;&#21335;&#38750;&#27954;&#35937;&#31227;&#21160;&#30340;&#22797;&#26434;&#27169;&#24335;&#65292;&#37325;&#28857;&#20851;&#27880;&#23395;&#33410;&#21464;&#21270;&#21644;&#38477;&#38632;&#27169;&#24335;&#31561;&#20851;&#38190;&#29983;&#24577;&#39537;&#21160;&#22240;&#32032;&#12290;&#23613;&#31649;&#22260;&#32469;&#36825;&#20123;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#22240;&#32032;&#23384;&#22312;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#23545;&#38750;&#27954;&#21160;&#24577;&#26223;&#35266;&#32972;&#26223;&#19979;&#35937;&#36801;&#24473;&#34892;&#20026;&#30340;&#20840;&#38754;&#35270;&#35282;&#12290;&#25105;&#20204;&#32508;&#21512;&#30340;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#39044;&#27979;&#36825;&#20123;&#29983;&#24577;&#20915;&#23450;&#22240;&#32032;&#23545;&#35937;&#36801;&#24473;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#36825;&#26159;&#24314;&#31435;&#30693;&#24773;&#30340;&#20445;&#25252;&#31574;&#30053;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#32771;&#34385;&#21040;&#20840;&#29699;&#27668;&#20505;&#21464;&#21270;&#23545;&#23395;&#33410;&#21644;&#38477;&#38632;&#27169;&#24335;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#39044;&#27979;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#26410;&#26469;&#21487;&#33021;&#20250;&#23545;&#35937;&#30340;&#34892;&#21160;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25104;&#26524;&#26088;&#22312;&#19981;&#20165;&#25512;&#36827;&#23545;&#31227;&#21160;&#29983;&#24577;&#23398;&#30340;&#29702;&#35299;&#65292;&#21516;&#26102;&#20063;&#20026;&#20445;&#25252;&#23454;&#36341;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
The interaction between elephants and their environment has profound implications for both ecology and conservation strategies. This study presents an analytical approach to decipher the intricate patterns of elephant movement in Sub-Saharan Africa, concentrating on key ecological drivers such as seasonal variations and rainfall patterns. Despite the complexities surrounding these influential factors, our analysis provides a holistic view of elephant migratory behavior in the context of the dynamic African landscape. Our comprehensive approach enables us to predict the potential impact of these ecological determinants on elephant migration, a critical step in establishing informed conservation strategies. This projection is particularly crucial given the impacts of global climate change on seasonal and rainfall patterns, which could substantially influence elephant movements in the future. The findings of our work aim to not only advance the understanding of movement ecology but also f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; XLDA &#30340;&#26694;&#26550;&#65292;&#21487;&#22312;&#36793;&#32536;&#19978;&#36827;&#34892;&#26497;&#31471;&#20998;&#31867;&#65292;&#20854;&#20013;&#20351;&#29992;&#30340;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#20998;&#31867;&#22120;&#31561;&#25928;&#20110;&#20840;&#36830;&#25509;&#65288;FC&#65289;&#23618;&#65292;&#36890;&#36807;&#20248;&#21270;&#23454;&#29616;&#20102;&#21152;&#36895;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26497;&#31471;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.11317</link><description>&lt;p&gt;
XLDA: &#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#29992;&#20110;&#22312;&#36793;&#32536;&#19978;&#36827;&#34892;&#26497;&#31471;&#20998;&#31867;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
XLDA: Linear Discriminant Analysis for Scaling Continual Learning to Extreme Classification at the Edge. (arXiv:2307.11317v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; XLDA &#30340;&#26694;&#26550;&#65292;&#21487;&#22312;&#36793;&#32536;&#19978;&#36827;&#34892;&#26497;&#31471;&#20998;&#31867;&#65292;&#20854;&#20013;&#20351;&#29992;&#30340;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#20998;&#31867;&#22120;&#31561;&#25928;&#20110;&#20840;&#36830;&#25509;&#65288;FC&#65289;&#23618;&#65292;&#36890;&#36807;&#20248;&#21270;&#23454;&#29616;&#20102;&#21152;&#36895;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26497;&#31471;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24335;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#22312;&#36793;&#32536;&#19978;&#37096;&#32626;&#21463;&#38480;&#31867;&#21035;&#65288;&#26368;&#22810;1000&#20010;&#65289;&#30340;&#22686;&#37327;&#23398;&#20064;&#20013;&#24050;&#32463;&#34987;&#35777;&#26126;&#26377;&#25928;&#65292;&#20294;&#22312;&#26497;&#24230;&#20998;&#31867;&#22330;&#26223;&#20013;&#23578;&#26410;&#24471;&#21040;&#35777;&#26126;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#65306;&#65288;a&#65289;XLDA&#65292;&#19968;&#31181;&#29992;&#20110;&#36793;&#32536;&#37096;&#32626;&#20013;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;LDA&#20998;&#31867;&#22120;&#34987;&#35777;&#26126;&#19982;FC&#23618;&#31561;&#25928;&#65292;&#21253;&#25324;&#22312;&#26497;&#24230;&#20998;&#31867;&#22330;&#26223;&#20013;&#65307;&#65288;b&#65289;&#20248;&#21270;&#20197;&#23454;&#29616;&#22522;&#20110;XLDA&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#20854;&#20013;&#23384;&#22312;&#21487;&#29992;&#35745;&#31639;&#36164;&#28304;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25209;&#22788;&#29702;&#35757;&#32451;&#26041;&#27861;&#19979;&#30340;42&#20493;&#21152;&#36895;&#21644;&#26368;&#36817;&#37051;&#25628;&#32034;&#22312;AliProducts&#65288;50k&#31867;&#21035;&#65289;&#21644;Google Landmarks V2&#65288;81k&#31867;&#21035;&#65289;&#31561;&#26497;&#31471;&#25968;&#25454;&#38598;&#19978;&#30340;5&#20493;&#25512;&#29702;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Streaming Linear Discriminant Analysis (LDA) while proven in Class-incremental Learning deployments at the edge with limited classes (upto 1000), has not been proven for deployment in extreme classification scenarios. In this paper, we present: (a) XLDA, a framework for Class-IL in edge deployment where LDA classifier is proven to be equivalent to FC layer including in extreme classification scenarios, and (b) optimizations to enable XLDA-based training and inference for edge deployment where there is a constraint on available compute resources. We show up to 42x speed up using a batched training approach and up to 5x inference speedup with nearest neighbor search on extreme datasets like AliProducts (50k classes) and Google Landmarks V2 (81k classes)
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#20219;&#21153;&#35299;&#20915;&#22120;&#21644;&#33258;&#26657;&#20934;&#22120;&#65292;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#26679;&#26412;&#12289;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#20854;&#20182;&#23454;&#38469;&#25361;&#25112;&#19979;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2307.11316</link><description>&lt;p&gt;
&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#20219;&#21153;&#35299;&#20915;&#22120;&#21644;&#33258;&#26657;&#20934;&#22120;
&lt;/p&gt;
&lt;p&gt;
Making Pre-trained Language Models both Task-solvers and Self-calibrators. (arXiv:2307.11316v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11316
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#20219;&#21153;&#35299;&#20915;&#22120;&#21644;&#33258;&#26657;&#20934;&#22120;&#65292;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#26679;&#26412;&#12289;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#20854;&#20182;&#23454;&#38469;&#25361;&#25112;&#19979;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#21508;&#31181;&#23454;&#38469;&#31995;&#32479;&#20013;&#20316;&#20026;&#39592;&#24178;&#12290;&#23545;&#20110;&#39640;&#39118;&#38505;&#24212;&#29992;&#65292;&#21512;&#29702;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#23545;&#20110;&#39044;&#27979;&#21516;&#26679;&#37325;&#35201;&#12290;&#34429;&#28982;PLMs&#30340;&#24120;&#35268;&#32622;&#20449;&#24230;&#20998;&#25968;&#24050;&#32463;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#65292;&#20294;&#23427;&#20204;&#22312;&#38169;&#35823;&#39044;&#27979;&#20013;&#22987;&#32456;&#21464;&#24471;&#36807;&#20110;&#33258;&#20449;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#24341;&#20837;&#39069;&#22806;&#30340;&#26657;&#20934;&#20219;&#21153;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#33719;&#24471;&#39069;&#22806;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#20854;&#21021;&#22987;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;&#28982;&#32780;&#65292;&#23427;&#21482;&#26159;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#20551;&#35774;&#24341;&#20837;&#30340;&#26657;&#20934;&#20219;&#21153;&#26377;&#20016;&#23500;&#30340;&#39069;&#22806;&#21487;&#29992;&#26679;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#23454;&#38469;&#24773;&#20917;&#65292;&#25105;&#20204;&#38656;&#35201;&#26377;&#25928;&#21033;&#29992;&#35757;&#32451;&#26679;&#26412;&#65292;&#20351;PLMs&#25104;&#20026;&#20219;&#21153;&#35299;&#20915;&#22120;&#21644;&#33258;&#26657;&#20934;&#22120;&#12290;&#25552;&#20986;&#20102;&#19977;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#26377;&#38480;&#30340;&#35757;&#32451;&#26679;&#26412;&#12289;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#8230;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) serve as backbones for various real-world systems. For high-stake applications, it's equally essential to have reasonable confidence estimations in predictions. While the vanilla confidence scores of PLMs can already be effectively utilized, PLMs consistently become overconfident in their wrong predictions, which is not desirable in practice. Previous work shows that introducing an extra calibration task can mitigate this issue. The basic idea involves acquiring additional data to train models in predicting the confidence of their initial predictions. However, it only demonstrates the feasibility of this kind of method, assuming that there are abundant extra available samples for the introduced calibration task. In this work, we consider the practical scenario that we need to effectively utilize training samples to make PLMs both task-solvers and self-calibrators. Three challenges are presented, including limited training samples, data imbalance, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24418;&#24577;&#23398;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#26102;&#31354;&#27169;&#24335;&#35782;&#21035;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#19988;&#33021;&#22815;&#23398;&#20064;&#31361;&#35302;&#26435;&#37325;&#21644;&#35843;&#25972;&#26102;&#38388;&#28388;&#27874;&#22120;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.11314</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#24418;&#24577;&#23398;&#30340;&#22312;&#32447;&#23398;&#20064;&#29992;&#20110;&#26102;&#31354;&#27169;&#24335;&#35782;&#21035;&#65292; &#20351;&#29992;&#27491;&#21521;&#26102;&#38388;&#32447;&#12290; (arXiv:2307.11314v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
Neuromorphic Online Learning for Spatiotemporal Patterns with a Forward-only Timeline. (arXiv:2307.11314v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24418;&#24577;&#23398;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#26102;&#31354;&#27169;&#24335;&#35782;&#21035;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#19988;&#33021;&#22815;&#23398;&#20064;&#31361;&#35302;&#26435;&#37325;&#21644;&#35843;&#25972;&#26102;&#38388;&#28388;&#27874;&#22120;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#26159;&#20855;&#26377;&#39640;&#33021;&#25928;&#30340;&#29983;&#29289;&#21512;&#29702;&#35745;&#31639;&#27169;&#22411;&#12290;&#31070;&#32463;&#20803;&#21644;&#31361;&#35302;&#30340;&#26102;&#38388;&#21160;&#24577;&#20351;&#20854;&#33021;&#22815;&#26816;&#27979;&#26102;&#38388;&#27169;&#24335;&#24182;&#29983;&#25104;&#24207;&#21015;&#12290;&#20256;&#32479;&#19978;&#20351;&#29992;&#26102;&#38388;&#21453;&#21521;&#20256;&#25773;(BPTT)&#26469;&#35757;&#32451;SNNs&#65292;&#20294;&#23545;&#20110;&#23884;&#20837;&#24335;&#24212;&#29992;&#30340;&#22312;&#32447;&#23398;&#20064;&#26469;&#35828;&#65292;BPTT&#19981;&#36866;&#21512;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#39640;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#20197;&#21450;&#24310;&#36831;&#26102;&#38388;&#36739;&#38271;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20351;&#29992;&#39640;&#24230;&#31616;&#21270;&#30340;&#33033;&#20914;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#27809;&#26377;&#31361;&#35302;&#21160;&#21147;&#23398;&#21644;&#37325;&#32622;&#21453;&#39304;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;LIF&#31070;&#32463;&#20803;&#32452;&#25104;&#30340;SNNs&#30340;&#26102;&#31354;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;(SOLSA)&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#25351;&#25968;&#34928;&#20943;&#30340;&#31361;&#35302;&#21644;&#36719;&#37325;&#32622;&#12290;&#35813;&#31639;&#27861;&#19981;&#20165;&#21487;&#20197;&#23398;&#20064;&#31361;&#35302;&#26435;&#37325;&#65292;&#36824;&#21487;&#20197;&#35843;&#25972;&#19982;&#31361;&#35302;&#30456;&#20851;&#30340;&#26102;&#38388;&#28388;&#27874;&#22120;&#12290;&#19982;BPTT&#31639;&#27861;&#30456;&#27604;&#65292;SOLSA&#31639;&#27861;&#19981;&#20165;&#22312;&#24615;&#33021;&#19978;&#26356;&#20248;&#31168;&#65292;&#32780;&#19988;&#36890;&#36807;&#36866;&#24212;&#24615;&#23398;&#20064;&#23454;&#29616;&#36739;&#20302;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) are bio-plausible computing models with high energy efficiency. The temporal dynamics of neurons and synapses enable them to detect temporal patterns and generate sequences. While Backpropagation Through Time (BPTT) is traditionally used to train SNNs, it is not suitable for online learning of embedded applications due to its high computation and memory cost as well as extended latency. Previous works have proposed online learning algorithms, but they often utilize highly simplified spiking neuron models without synaptic dynamics and reset feedback, resulting in subpar performance. In this work, we present Spatiotemporal Online Learning for Synaptic Adaptation (SOLSA), specifically designed for online learning of SNNs composed of Leaky Integrate and Fire (LIF) neurons with exponentially decayed synapses and soft reset. The algorithm not only learns the synaptic weight but also adapts the temporal filters associated to the synapses. Compared to the BPTT al
&lt;/p&gt;</description></item><item><title>PI-VEGAN&#26159;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#25511;&#21046;&#29289;&#29702;&#23450;&#24459;&#34701;&#20837;&#27169;&#22411;&#24182;&#24341;&#20837;&#21464;&#20998;&#32534;&#30721;&#22120;&#26469;&#35299;&#20915;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#21508;&#31181;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.11289</link><description>&lt;p&gt;
PI-VEGAN: &#29289;&#29702;&#20449;&#24687;&#21464;&#20998;&#23884;&#20837;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#24212;&#29992;&#20110;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
PI-VEGAN: Physics Informed Variational Embedding Generative Adversarial Networks for Stochastic Differential Equations. (arXiv:2307.11289v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11289
&lt;/p&gt;
&lt;p&gt;
PI-VEGAN&#26159;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#25511;&#21046;&#29289;&#29702;&#23450;&#24459;&#34701;&#20837;&#27169;&#22411;&#24182;&#24341;&#20837;&#21464;&#20998;&#32534;&#30721;&#22120;&#26469;&#35299;&#20915;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#21508;&#31181;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#31867;&#22411;&#65292;&#31216;&#20026;&#29289;&#29702;&#20449;&#24687;&#21464;&#20998;&#23884;&#20837;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;PI-VEGAN&#65289;&#65292;&#23427;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#27491;&#21521;&#12289;&#21453;&#21521;&#21644;&#28151;&#21512;&#38382;&#39064;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#25511;&#21046;&#26041;&#31243;&#26159;&#24050;&#30693;&#30340;&#65292;&#20294;&#21482;&#26377;&#31995;&#32479;&#21442;&#25968;&#30340;&#23569;&#37327;&#20256;&#24863;&#22120;&#27979;&#37327;&#32467;&#26524;&#21487;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#24494;&#20998;&#23558;&#25511;&#21046;&#29289;&#29702;&#23450;&#24459;&#34701;&#20837;PI-VEGAN&#20013;&#65292;&#21516;&#26102;&#24341;&#20837;&#21464;&#20998;&#32534;&#30721;&#22120;&#26469;&#36817;&#20284;&#27979;&#37327;&#32467;&#26524;&#23454;&#38469;&#20998;&#24067;&#30340;&#28508;&#22312;&#21464;&#37327;&#12290;&#36825;&#20123;&#28508;&#22312;&#21464;&#37327;&#34987;&#34701;&#20837;&#29983;&#25104;&#22120;&#20013;&#65292;&#20197;&#20415;&#20934;&#30830;&#22320;&#23398;&#20064;&#38543;&#26426;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#65292;&#20998;&#21035;&#26159;&#32534;&#30721;&#22120;&#12289;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#65292;&#27599;&#20010;&#32452;&#20214;&#37117;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20132;&#26367;&#36827;&#34892;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new category of physics-informed neural networks called physics informed variational embedding generative adversarial network (PI-VEGAN), that effectively tackles the forward, inverse, and mixed problems of stochastic differential equations. In these scenarios, the governing equations are known, but only a limited number of sensor measurements of the system parameters are available. We integrate the governing physical laws into PI-VEGAN with automatic differentiation, while introducing a variational encoder for approximating the latent variables of the actual distribution of the measurements. These latent variables are integrated into the generator to facilitate accurate learning of the characteristics of the stochastic partial equations. Our model consists of three components, namely the encoder, generator, and discriminator, each of which is updated alternatively employing the stochastic gradient descent algorithm. We evaluate the effectiveness of PI-VEGAN in addressing 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20869;&#26680;&#30340;&#31163;&#32447;&#32972;&#26223;&#21452;&#21521;&#31454;&#26631;&#32773;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#20559;&#22909;&#21453;&#39304;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20351;&#29992;&#22343;&#21248;&#37319;&#26679;&#19978;&#19979;&#25991;&#30340;&#30456;&#20284;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.11288</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#26680;&#30340;&#31163;&#32447;&#32972;&#26223;&#21452;&#21521;&#31454;&#26631;&#32773;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Kernelized Offline Contextual Dueling Bandits. (arXiv:2307.11288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20869;&#26680;&#30340;&#31163;&#32447;&#32972;&#26223;&#21452;&#21521;&#31454;&#26631;&#32773;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#20559;&#22909;&#21453;&#39304;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20351;&#29992;&#22343;&#21248;&#37319;&#26679;&#19978;&#19979;&#25991;&#30340;&#30456;&#20284;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#21453;&#39304;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#20123;&#24212;&#29992;&#20013;&#26080;&#27861;&#30452;&#25509;&#35780;&#20272;&#22870;&#21169;&#20989;&#25968;&#12290;&#22312;&#20154;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36825;&#26159;&#19968;&#20010;&#26174;&#33879;&#30340;&#26368;&#26032;&#23454;&#20363;&#12290;&#23545;&#20110;&#35768;&#22810;&#36825;&#20123;&#24212;&#29992;&#65292;&#33719;&#21462;&#20154;&#31867;&#21453;&#39304;&#30340;&#25104;&#26412;&#21487;&#33021;&#30456;&#24403;&#39640;&#29978;&#33267;&#19981;&#21487;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#20107;&#23454;&#65292;&#21363;&#20195;&#29702;&#36890;&#24120;&#21487;&#20197;&#36873;&#25321;&#33719;&#24471;&#20154;&#31867;&#21453;&#39304;&#30340;&#19978;&#19979;&#25991;&#65292;&#20197;&#26368;&#39640;&#25928;&#22320;&#30830;&#23450;&#19968;&#20010;&#33391;&#22909;&#31574;&#30053;&#65292;&#24182;&#24341;&#20837;&#20102;&#31163;&#32447;&#32972;&#26223;&#21452;&#21521;&#31454;&#26631;&#32773;&#35774;&#32622;&#12290;&#25105;&#20204;&#20026;&#36825;&#20010;&#35774;&#32622;&#25552;&#20379;&#20102;&#19968;&#20010;&#19978;&#30028;&#32622;&#20449;&#21306;&#38388;&#26679;&#24335;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20010;&#36951;&#25022;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#32463;&#39564;&#35777;&#23454;&#36825;&#31181;&#26041;&#27861;&#32988;&#36807;&#20351;&#29992;&#22343;&#21248;&#37319;&#26679;&#19978;&#19979;&#25991;&#30340;&#31867;&#20284;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preference-based feedback is important for many applications where direct evaluation of a reward function is not feasible. A notable recent example arises in reinforcement learning from human feedback on large language models. For many of these applications, the cost of acquiring the human feedback can be substantial or even prohibitive. In this work, we take advantage of the fact that often the agent can choose contexts at which to obtain human feedback in order to most efficiently identify a good policy, and introduce the offline contextual dueling bandit setting. We give an upper-confidence-bound style algorithm for this setting and prove a regret bound. We also give empirical confirmation that this method outperforms a similar strategy that uses uniformly sampled contexts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26377;&#25928;&#21327;&#35843;&#21644;&#35757;&#32451;&#22810;&#20010;&#21516;&#26102;&#36827;&#34892;&#30340;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;MAS&#65292;&#36890;&#36807;&#21512;&#24182;&#21644;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#26469;&#20248;&#21270;&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#30340;&#21516;&#26102;&#36229;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.11285</link><description>&lt;p&gt;
MAS&#65306;&#38754;&#21521;&#36164;&#28304;&#39640;&#25928;&#30340;&#32852;&#37030;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MAS: Towards Resource-Efficient Federated Multiple-Task Learning. (arXiv:2307.11285v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26377;&#25928;&#21327;&#35843;&#21644;&#35757;&#32451;&#22810;&#20010;&#21516;&#26102;&#36827;&#34892;&#30340;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;MAS&#65292;&#36890;&#36807;&#21512;&#24182;&#21644;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#26469;&#20248;&#21270;&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#30340;&#21516;&#26102;&#36229;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#20998;&#24067;&#24335;&#36793;&#32536;&#35774;&#22791;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22810;&#20010;&#21516;&#26102;&#36827;&#34892;&#30340;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#21487;&#33021;&#20250;&#36807;&#36733;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26377;&#25928;&#21327;&#35843;&#21644;&#35757;&#32451;&#22810;&#20010;&#21516;&#26102;&#36827;&#34892;&#30340;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#12290;&#25105;&#20204;&#39318;&#20808;&#24418;&#24335;&#21270;&#20102;&#35757;&#32451;&#21516;&#26102;&#36827;&#34892;&#30340;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;MAS&#65288;Merge and Split&#65289;&#65292;&#20197;&#20248;&#21270;&#35757;&#32451;&#22810;&#20010;&#21516;&#26102;&#36827;&#34892;&#30340;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;MAS&#39318;&#20808;&#23558;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#21512;&#24182;&#20026;&#19968;&#20010;&#20855;&#26377;&#22810;&#20219;&#21153;&#20307;&#31995;&#32467;&#26500;&#30340;&#25972;&#20307;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#12290;&#22312;&#35757;&#32451;&#20960;&#36718;&#21518;&#65292;MAS&#36890;&#36807;&#20351;&#29992;&#25972;&#20307;&#35757;&#32451;&#26399;&#38388;&#27979;&#37327;&#30340;&#20219;&#21153;&#20043;&#38388;&#30340;&#20146;&#21644;&#24615;&#23558;&#25972;&#20307;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#20998;&#21106;&#20026;&#20004;&#20010;&#25110;&#26356;&#22810;&#20010;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#25972;&#20307;&#35757;&#32451;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#32487;&#32493;&#35757;&#32451;&#27599;&#20010;&#20998;&#21106;&#30340;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;MAS&#22312;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#30340;&#21516;&#26102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is an emerging distributed machine learning method that empowers in-situ model training on decentralized edge devices. However, multiple simultaneous FL tasks could overload resource-constrained devices. In this work, we propose the first FL system to effectively coordinate and train multiple simultaneous FL tasks. We first formalize the problem of training simultaneous FL tasks. Then, we present our new approach, MAS (Merge and Split), to optimize the performance of training multiple simultaneous FL tasks. MAS starts by merging FL tasks into an all-in-one FL task with a multi-task architecture. After training for a few rounds, MAS splits the all-in-one FL task into two or more FL tasks by using the affinities among tasks measured during the all-in-one training. It then continues training each split of FL tasks based on model parameters from the all-in-one training. Extensive experiments demonstrate that MAS outperforms other methods while reducing training time
&lt;/p&gt;</description></item><item><title>Epsilon*&#26159;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38544;&#31169;&#39118;&#38505;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#25110;&#27169;&#22411;&#35757;&#32451;&#31639;&#27861;&#65292;&#33021;&#19982;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#20013;&#30340;&#20551;&#35774;&#26816;&#39564;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#23545;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#23454;&#20363;&#38544;&#31169;&#25439;&#22833;&#30340;&#19979;&#30028;&#65292;&#36991;&#20813;&#25968;&#20540;&#21644;&#22122;&#22768;&#25918;&#22823;&#19981;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.11280</link><description>&lt;p&gt;
Epsilon*: &#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#31169;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Epsilon*: Privacy Metric for Machine Learning Models. (arXiv:2307.11280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11280
&lt;/p&gt;
&lt;p&gt;
Epsilon*&#26159;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38544;&#31169;&#39118;&#38505;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#25110;&#27169;&#22411;&#35757;&#32451;&#31639;&#27861;&#65292;&#33021;&#19982;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#20013;&#30340;&#20551;&#35774;&#26816;&#39564;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#23545;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#23454;&#20363;&#38544;&#31169;&#25439;&#22833;&#30340;&#19979;&#30028;&#65292;&#36991;&#20813;&#25968;&#20540;&#21644;&#22122;&#22768;&#25918;&#22823;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Epsilon*&#65292;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#38544;&#31169;&#20943;&#36731;&#31574;&#30053;&#37096;&#32626;&#20043;&#21069;&#12289;&#26399;&#38388;&#25110;&#20043;&#21518;&#65292;&#27979;&#37327;&#21333;&#20010;&#27169;&#22411;&#23454;&#20363;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#35813;&#24230;&#37327;&#19981;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#37319;&#26679;&#25110;&#27169;&#22411;&#35757;&#32451;&#31639;&#27861;&#12290;Epsilon*&#26159;&#19968;&#20010;&#20851;&#20110;&#30495;&#38451;&#24615;&#21644;&#20551;&#38451;&#24615;&#29575;&#30340;&#20989;&#25968;&#65292;&#29992;&#20110;&#25932;&#25163;&#22312;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#20013;&#20351;&#29992;&#30340;&#20551;&#35774;&#26816;&#39564;&#20013;&#12290;&#25105;&#20204;&#21306;&#20998;&#20102;&#37327;&#21270;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#23454;&#20363;&#30340;&#38544;&#31169;&#25439;&#22833;&#21644;&#37327;&#21270;&#20135;&#29983;&#35813;&#27169;&#22411;&#23454;&#20363;&#30340;&#35757;&#32451;&#26426;&#21046;&#30340;&#38544;&#31169;&#25439;&#22833;&#12290;&#29616;&#26377;&#30340;&#38544;&#31169;&#23457;&#35745;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#20026;&#21518;&#32773;&#25552;&#20379;&#20102;&#19979;&#30028;&#65292;&#32780;&#25105;&#20204;&#30340;&#24230;&#37327;&#26041;&#27861;&#36890;&#36807;&#20381;&#36182;&#20110;&#35757;&#32451;&#27169;&#22411;&#23454;&#20363;&#30340;&#38544;&#31169;&#30340;&#65288;&#949;&#65292;&#948;&#65289;&#22411;&#37327;&#21270;&#65292;&#20026;&#21069;&#32773;&#25552;&#20379;&#20102;&#19979;&#30028;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#36825;&#20123;&#19979;&#30028;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23454;&#29616;Epsilon*&#20197;&#36991;&#20813;&#25968;&#20540;&#21644;&#22122;&#22768;&#25918;&#22823;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Epsilon*, a new privacy metric for measuring the privacy risk of a single model instance prior to, during, or after deployment of privacy mitigation strategies. The metric does not require access to the training data sampling or model training algorithm. Epsilon* is a function of true positive and false positive rates in a hypothesis test used by an adversary in a membership inference attack. We distinguish between quantifying the privacy loss of a trained model instance and quantifying the privacy loss of the training mechanism which produces this model instance. Existing approaches in the privacy auditing literature provide lower bounds for the latter, while our metric provides a lower bound for the former by relying on an (${\epsilon}$,${\delta}$)-type of quantification of the privacy of the trained model instance. We establish a relationship between these lower bounds and show how to implement Epsilon* to avoid numerical and noise amplification instability. We further 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#20083;&#33146;&#30284;&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#25552;&#39640;&#31579;&#26597;&#31243;&#24207;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#22312;RSNA&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#24471;&#20986;&#20102;&#24179;&#22343;&#39564;&#35777;&#26696;&#20363;pF1&#24471;&#20998;&#20026;0.56&#12290;</title><link>http://arxiv.org/abs/2307.11274</link><description>&lt;p&gt;
&#20083;&#33146;&#30284;&#31579;&#26597;&#20083;&#33146;&#25668;&#24433;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Screening Mammography Breast Cancer Detection. (arXiv:2307.11274v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#20083;&#33146;&#30284;&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#25552;&#39640;&#31579;&#26597;&#31243;&#24207;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#22312;RSNA&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#24471;&#20986;&#20102;&#24179;&#22343;&#39564;&#35777;&#26696;&#20363;pF1&#24471;&#20998;&#20026;0.56&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;&#30284;&#26159;&#23548;&#33268;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#20294;&#29616;&#26377;&#30340;&#31243;&#24207;&#25104;&#26412;&#39640;&#26114;&#19988;&#23481;&#26131;&#20986;&#29616;&#20551;&#38451;&#24615;&#65292;&#23548;&#33268;&#19981;&#24517;&#35201;&#30340;&#21518;&#32493;&#26816;&#26597;&#21644;&#24739;&#32773;&#28966;&#34385;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#20083;&#33146;&#30284;&#26816;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#31579;&#26597;&#31243;&#24207;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;RSNA&#25968;&#25454;&#38598;&#30340;&#22823;&#32422;20,000&#21517;&#22899;&#24615;&#24739;&#32773;&#30340;&#20256;&#32479;&#20083;&#25151;&#22270;&#20687;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#22312;&#21508;&#31181;&#26041;&#27861;&#20043;&#38388;&#33719;&#24471;&#20102;&#24179;&#22343;&#39564;&#35777;&#26696;&#20363;pF1&#24471;&#20998;&#20026;0.56&#12290;
&lt;/p&gt;
&lt;p&gt;
Breast cancer is a leading cause of cancer-related deaths, but current programs are expensive and prone to false positives, leading to unnecessary follow-up and patient anxiety. This paper proposes a solution to automated breast cancer detection, to improve the efficiency and accuracy of screening programs. Different methodologies were tested against the RSNA dataset of radiographic breast images of roughly 20,000 female patients and yielded an average validation case pF1 score of 0.56 across methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35777;&#25454;&#19979;&#30028;&#30340;Fisher-Rao&#26799;&#24230;&#65292;&#25581;&#31034;&#20102;&#23427;&#19982;&#30446;&#26631;&#20998;&#24067;&#30340;Kullback-Leibler&#25955;&#24230;&#26799;&#24230;&#30340;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#20027;&#35201;&#30446;&#26631;&#20989;&#25968;&#19982;&#26368;&#22823;&#21270;ELBO&#30340;&#31561;&#20215;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.11249</link><description>&lt;p&gt;
&#20851;&#20110;&#35777;&#25454;&#19979;&#30028;&#30340;Fisher-Rao&#26799;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Fisher-Rao Gradient of the Evidence Lower Bound. (arXiv:2307.11249v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35777;&#25454;&#19979;&#30028;&#30340;Fisher-Rao&#26799;&#24230;&#65292;&#25581;&#31034;&#20102;&#23427;&#19982;&#30446;&#26631;&#20998;&#24067;&#30340;Kullback-Leibler&#25955;&#24230;&#26799;&#24230;&#30340;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#20027;&#35201;&#30446;&#26631;&#20989;&#25968;&#19982;&#26368;&#22823;&#21270;ELBO&#30340;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#30340;Fisher-Rao&#26799;&#24230;&#65292;&#20063;&#31216;&#20026;&#33258;&#28982;&#26799;&#24230;&#65292;&#23427;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#29702;&#35770;&#12289;Helmholtz&#26426;&#21644;&#33258;&#30001;&#33021;&#21407;&#29702;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;ELBO&#30340;&#33258;&#28982;&#26799;&#24230;&#19982;&#30446;&#26631;&#20998;&#24067;&#30340;Kullback-Leibler&#25955;&#24230;&#30340;&#33258;&#28982;&#26799;&#24230;&#30456;&#20851;&#65292;&#21518;&#32773;&#26159;&#23398;&#20064;&#30340;&#20027;&#35201;&#30446;&#26631;&#20989;&#25968;&#12290;&#22522;&#20110;&#20449;&#24687;&#20960;&#20309;&#20013;&#26799;&#24230;&#30340;&#19981;&#21464;&#24615;&#29305;&#24615;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#24213;&#23618;&#27169;&#22411;&#30340;&#26465;&#20214;&#65292;&#30830;&#20445;&#26368;&#23567;&#21270;&#20027;&#35201;&#30446;&#26631;&#20989;&#25968;&#19982;&#26368;&#22823;&#21270;ELBO&#30340;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article studies the Fisher-Rao gradient, also referred to as the natural gradient, of the evidence lower bound, the ELBO, which plays a crucial role within the theory of the Variational Autonecoder, the Helmholtz Machine and the Free Energy Principle. The natural gradient of the ELBO is related to the natural gradient of the Kullback-Leibler divergence from a target distribution, the prime objective function of learning. Based on invariance properties of gradients within information geometry, conditions on the underlying model are provided that ensure the equivalence of minimising the prime objective function and the maximisation of the ELBO.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#33021;&#29289;&#29702;&#23454;&#39564;&#20013;&#21033;&#29992;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#30340;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#27169;&#22411;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#28388;&#27874;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#20256;&#20837;&#30340;&#30005;&#33655;&#27874;&#24418;&#36716;&#25442;&#20026;&#20108;&#20540;&#20107;&#20214;&#27969;&#65292;&#24182;&#20248;&#21270;SNN&#30340;&#31995;&#32479;&#35774;&#35745;&#21644;&#36229;&#21442;&#25968;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#20449;&#21495;&#26377;&#25928;&#29575;&#32422;&#20026;91%&#30340;SNN&#27169;&#22411;&#65292;&#21442;&#25968;&#25968;&#37327;&#20960;&#20046;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#21322;&#12290;</title><link>http://arxiv.org/abs/2307.11242</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#28388;&#27874;&#22312;&#39640;&#33021;&#29289;&#29702;&#23454;&#39564;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On-Sensor Data Filtering using Neuromorphic Computing for High Energy Physics Experiments. (arXiv:2307.11242v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#33021;&#29289;&#29702;&#23454;&#39564;&#20013;&#21033;&#29992;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#30340;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#27169;&#22411;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#28388;&#27874;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#20256;&#20837;&#30340;&#30005;&#33655;&#27874;&#24418;&#36716;&#25442;&#20026;&#20108;&#20540;&#20107;&#20214;&#27969;&#65292;&#24182;&#20248;&#21270;SNN&#30340;&#31995;&#32479;&#35774;&#35745;&#21644;&#36229;&#21442;&#25968;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#20449;&#21495;&#26377;&#25928;&#29575;&#32422;&#20026;91%&#30340;SNN&#27169;&#22411;&#65292;&#21442;&#25968;&#25968;&#37327;&#20960;&#20046;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#21322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#21033;&#29992;&#22522;&#20110;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#30340;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#27169;&#22411;&#23545;&#22312;&#39640;&#20142;&#24230;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#36827;&#34892;&#30340;&#39640;&#33021;&#29289;&#29702;&#23454;&#39564;&#20013;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#28388;&#27874;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#21457;&#32039;&#20945;&#22411;&#31070;&#32463;&#24418;&#24577;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#31890;&#23376;&#30340;&#27178;&#21521;&#21160;&#37327;&#26469;&#28388;&#38500;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#30446;&#26631;&#26159;&#20943;&#23569;&#20256;&#36755;&#21040;&#19979;&#28216;&#30005;&#23376;&#35774;&#22791;&#30340;&#25968;&#25454;&#37327;&#12290;&#20256;&#20837;&#30340;&#30005;&#33655;&#27874;&#24418;&#34987;&#36716;&#25442;&#20026;&#20108;&#20540;&#20107;&#20214;&#27969;&#65292;&#28982;&#21518;&#30001;SNN&#36827;&#34892;&#22788;&#29702;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#38024;&#23545;&#30828;&#20214;&#37096;&#32626;&#36827;&#34892;&#20248;&#21270;&#30340;&#20934;&#30830;&#19988;&#32039;&#20945;&#30340;SNN&#30340;&#21508;&#31181;&#31995;&#32479;&#35774;&#35745;&#36873;&#25321;&#65292;&#20174;&#25968;&#25454;&#32534;&#30721;&#21040;&#35757;&#32451;&#31639;&#27861;&#30340;&#26368;&#20339;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#36827;&#21270;&#31639;&#27861;&#21644;&#20248;&#21270;&#30340;&#36229;&#21442;&#25968;&#35757;&#32451;&#30340;SNN&#22312;&#20449;&#21495;&#26377;&#25928;&#29575;&#26041;&#38754;&#22823;&#32422;&#36798;&#21040;91%&#65292;&#21442;&#25968;&#25968;&#37327;&#20960;&#20046;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#21322;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work describes the investigation of neuromorphic computing-based spiking neural network (SNN) models used to filter data from sensor electronics in high energy physics experiments conducted at the High Luminosity Large Hadron Collider. We present our approach for developing a compact neuromorphic model that filters out the sensor data based on the particle's transverse momentum with the goal of reducing the amount of data being sent to the downstream electronics. The incoming charge waveforms are converted to streams of binary-valued events, which are then processed by the SNN. We present our insights on the various system design choices - from data encoding to optimal hyperparameters of the training algorithm - for an accurate and compact SNN optimized for hardware deployment. Our results show that an SNN trained with an evolutionary algorithm and an optimized set of hyperparameters obtains a signal efficiency of about 91% with nearly half as many parameters as a deep neural netw
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;&#20381;&#36182;&#20851;&#31995;&#30340;&#32593;&#32476;&#32034;&#24341;&#22810;&#21464;&#37327;&#25968;&#25454;&#27169;&#22411;&#65292;&#24182;&#20171;&#32461;&#20102;&#36793;&#32536;&#31163;&#32676;&#20540;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#25512;&#23548;&#24179;&#26041;&#21644;&#30340;&#20998;&#24067;&#21644;&#25552;&#20986;&#40065;&#26834;&#29256;&#26412;&#30340;&#36793;&#32536;MCD&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#31163;&#32676;&#20540;&#30340;&#26816;&#27979;&#12290;&#27169;&#25311;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#24212;&#29992;&#32467;&#26524;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.11239</link><description>&lt;p&gt;
&#32593;&#32476;&#32034;&#24341;&#20449;&#21495;&#30340;&#36793;&#32536;&#31163;&#32676;&#20540;
&lt;/p&gt;
&lt;p&gt;
Edgewise outliers of network indexed signals. (arXiv:2307.11239v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11239
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;&#20381;&#36182;&#20851;&#31995;&#30340;&#32593;&#32476;&#32034;&#24341;&#22810;&#21464;&#37327;&#25968;&#25454;&#27169;&#22411;&#65292;&#24182;&#20171;&#32461;&#20102;&#36793;&#32536;&#31163;&#32676;&#20540;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#25512;&#23548;&#24179;&#26041;&#21644;&#30340;&#20998;&#24067;&#21644;&#25552;&#20986;&#40065;&#26834;&#29256;&#26412;&#30340;&#36793;&#32536;MCD&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#31163;&#32676;&#20540;&#30340;&#26816;&#27979;&#12290;&#27169;&#25311;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#24212;&#29992;&#32467;&#26524;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#28041;&#21450;&#21464;&#37327;&#20043;&#38388;&#20197;&#21450;&#22270;&#33410;&#28857;&#20043;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#32593;&#32476;&#32034;&#24341;&#22810;&#21464;&#37327;&#25968;&#25454;&#27169;&#22411;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#30528;&#37325;&#20110;&#31163;&#32676;&#20540;&#26816;&#27979;&#65292;&#24182;&#24341;&#20837;&#20102;&#36793;&#32536;&#31163;&#32676;&#20540;&#30340;&#27010;&#24565;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#19968;&#20123;&#24179;&#26041;&#21644;&#30340;&#20998;&#24067;&#65292;&#29305;&#21035;&#26159;&#21487;&#20197;&#29992;&#20110;&#31163;&#32676;&#20540;&#26816;&#27979;&#30340;&#24179;&#26041;&#39532;&#27663;&#36317;&#31163;&#20197;&#30830;&#23450;&#26816;&#27979;&#35268;&#21017;&#21644;&#38408;&#20540;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#29256;&#26412;&#30340;&#30830;&#23450;&#24615;MCD&#31639;&#27861;&#65292;&#31216;&#20026;&#36793;&#32536;MCD&#12290;&#23545;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#30340;&#24212;&#29992;&#26174;&#31034;&#20102;&#32771;&#34385;&#20381;&#36182;&#32467;&#26500;&#30340;&#20852;&#36259;&#12290;&#25105;&#20204;&#36824;&#29992;&#30495;&#23454;&#25968;&#25454;&#38598;&#35828;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider models for network indexed multivariate data involving a dependence between variables as well as across graph nodes.  In the framework of these models, we focus on outliers detection and introduce the concept of edgewise outliers. For this purpose, we first derive the distribution of some sums of squares, in particular squared Mahalanobis distances that can be used to fix detection rules and thresholds for outlier detection. We then propose a robust version of the deterministic MCD algorithm that we call edgewise MCD. An application on simulated data shows the interest of taking the dependence structure into account. We also illustrate the utility of the proposed method with a real data set.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#26680;&#65292;&#36890;&#36807;&#26681;&#25454;&#39030;&#28857;&#21344;&#25454;&#30456;&#20851;&#24615;&#23545;&#22270;&#36827;&#34892;&#26377;&#25928;&#37325;&#36830;&#65292;&#20197;&#23454;&#29616;&#20256;&#25773;&#37327;&#23376;&#31890;&#23376;&#30340;&#25193;&#25955;&#33539;&#24335;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#21464;&#31181;&#65292;&#32467;&#21512;&#20102;&#20256;&#32479;&#30340;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#21644;&#26032;&#30340;&#21367;&#31215;&#26680;&#12290;&#36890;&#36807;&#30740;&#31350;&#25105;&#20204;&#21457;&#29616;&#21516;&#36136;&#21270;&#30340;&#35889;&#20381;&#36182;&#24615;&#21644;&#37327;&#23376;&#21160;&#21147;&#23398;&#22312;&#24102;&#36890;&#28388;&#27874;&#22120;&#26500;&#36896;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.11234</link><description>&lt;p&gt;
QDC: &#22270;&#19978;&#30340;&#37327;&#23376;&#25193;&#25955;&#21367;&#31215;&#26680;
&lt;/p&gt;
&lt;p&gt;
QDC: Quantum Diffusion Convolution Kernels on Graphs. (arXiv:2307.11234v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#26680;&#65292;&#36890;&#36807;&#26681;&#25454;&#39030;&#28857;&#21344;&#25454;&#30456;&#20851;&#24615;&#23545;&#22270;&#36827;&#34892;&#26377;&#25928;&#37325;&#36830;&#65292;&#20197;&#23454;&#29616;&#20256;&#25773;&#37327;&#23376;&#31890;&#23376;&#30340;&#25193;&#25955;&#33539;&#24335;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#21464;&#31181;&#65292;&#32467;&#21512;&#20102;&#20256;&#32479;&#30340;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#21644;&#26032;&#30340;&#21367;&#31215;&#26680;&#12290;&#36890;&#36807;&#30740;&#31350;&#25105;&#20204;&#21457;&#29616;&#21516;&#36136;&#21270;&#30340;&#35889;&#20381;&#36182;&#24615;&#21644;&#37327;&#23376;&#21160;&#21147;&#23398;&#22312;&#24102;&#36890;&#28388;&#27874;&#22120;&#26500;&#36896;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (GCN) &#36890;&#36807;&#22312;&#23616;&#37096;&#37051;&#22495;&#19978;&#32858;&#21512;&#28040;&#24687;&#26469;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#12290;&#35768;&#22810; GCNs &#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#22312;&#22270;&#19978;&#25193;&#25955;&#36755;&#20837;&#29305;&#24449;&#30340;&#19968;&#31181;&#24418;&#24335;&#65292;&#24182;&#19988;&#36890;&#36807;&#25913;&#21464;&#28040;&#24687;&#20256;&#36882;&#30340;&#26041;&#24335;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24037;&#20316;&#24050;&#32463;&#26377;&#24456;&#22810;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;&#39030;&#28857;&#21344;&#25454;&#30456;&#20851;&#24615;&#26469;&#26377;&#25928;&#37325;&#26032;&#36830;&#25509;&#22270;&#30340;&#21367;&#31215;&#26680;&#65292;&#20197;&#22522;&#20110;&#22270;&#19978;&#37327;&#23376;&#31890;&#23376;&#30340;&#19968;&#33324;&#25193;&#25955;&#33539;&#24335;&#36827;&#34892;&#20256;&#25773;&#12290;&#25105;&#20204;&#31216;&#27492;&#26032;&#30340;&#21367;&#31215;&#26680;&#20026;&#37327;&#23376;&#25193;&#25955;&#21367;&#31215; (QDC) &#31639;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#21464;&#31181;&#65292;&#23558; QDC &#31639;&#23376;&#21644;&#20256;&#32479;&#30340;&#32452;&#21512;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#28040;&#24687;&#32467;&#21512;&#36215;&#26469;&#12290;&#20026;&#20102;&#29702;&#35299;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21516;&#36136;&#21270;&#30340;&#35889;&#20381;&#36182;&#24615;&#20197;&#21450;&#37327;&#23376;&#21160;&#21147;&#23398;&#22312;&#24102;&#36890;&#28388;&#27874;&#22120;&#26500;&#36896;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#36825;&#20123;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Graph convolutional neural networks (GCNs) operate by aggregating messages over local neighborhoods given the prediction task under interest. Many GCNs can be understood as a form of generalized diffusion of input features on the graph, and significant work has been dedicated to improving predictive accuracy by altering the ways of message passing. In this work, we propose a new convolution kernel that effectively rewires the graph according to the occupation correlations of the vertices by trading on the generalized diffusion paradigm for the propagation of a quantum particle over the graph. We term this new convolution kernel the Quantum Diffusion Convolution (QDC) operator. In addition, we introduce a multiscale variant that combines messages from the QDC operator and the traditional combinatorial Laplacian. To understand our method, we explore the spectral dependence of homophily and the importance of quantum dynamics in the construction of a bandpass filter. Through these studies,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23558;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#35774;&#35745;&#39640;&#25928;&#30340;&#21462;&#28040;&#23398;&#20064;&#31639;&#27861;&#65292;&#32473;&#20986;&#20102;&#32447;&#24615;&#21644;&#21069;&#32512;&#21644;&#26597;&#35810;&#31867;&#30340;&#39640;&#25928;&#21462;&#28040;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#21450;&#24212;&#29992;&#20110;&#38543;&#26426;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#25913;&#36827;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.11228</link><description>&lt;p&gt;
&#20174;&#33258;&#36866;&#24212;&#26597;&#35810;&#37322;&#25918;&#21040;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
From Adaptive Query Release to Machine Unlearning. (arXiv:2307.11228v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11228
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23558;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#35774;&#35745;&#39640;&#25928;&#30340;&#21462;&#28040;&#23398;&#20064;&#31639;&#27861;&#65292;&#32473;&#20986;&#20102;&#32447;&#24615;&#21644;&#21069;&#32512;&#21644;&#26597;&#35810;&#31867;&#30340;&#39640;&#25928;&#21462;&#28040;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#21450;&#24212;&#29992;&#20110;&#38543;&#26426;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#25913;&#36827;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#35774;&#35745;&#39640;&#25928;&#21462;&#28040;&#23398;&#20064;&#31639;&#27861;&#26469;&#23545;&#24212;&#20174;&#32467;&#26500;&#21270;&#26597;&#35810;&#31867;&#20013;&#36873;&#25321;&#33258;&#36866;&#24212;&#26597;&#35810;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#32447;&#24615;&#21644;&#21069;&#32512;&#21644;&#26597;&#35810;&#31867;&#30340;&#39640;&#25928;&#21462;&#28040;&#23398;&#20064;&#31639;&#27861;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#35768;&#22810;&#38382;&#39064;&#20013;&#65292;&#29305;&#21035;&#26159;&#38543;&#26426;&#20984;&#20248;&#21270;&#65288;SCO&#65289;&#20013;&#30340;&#21462;&#28040;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#19978;&#36848;&#26041;&#27861;&#26469;&#20943;&#23569;&#65292;&#20174;&#32780;&#25913;&#21892;&#38382;&#39064;&#30340;&#20445;&#35777;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#24179;&#28369;&#30340;Lipschitz&#25439;&#22833;&#21644;&#20219;&#24847;&#30340;$\rho&gt;0$&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#32473;&#20986;&#20102;&#19968;&#20010;&#21462;&#28040;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#36229;&#20986;&#24635;&#20307;&#39118;&#38505;&#20026;$\tilde O\big(\frac{1}{\sqrt{n}}+\frac{\sqrt{d}}{n\rho}\big)$&#65292;&#21462;&#28040;&#23398;&#20064;&#26597;&#35810;&#65288;&#26799;&#24230;&#65289;&#22797;&#26434;&#24615;&#20026;$\tilde O(\rho \cdot \text{&#37325;&#26032;&#35757;&#32451;&#22797;&#26434;&#24615;})$&#65292;&#20854;&#20013;$d$&#26159;&#27169;&#22411;&#30340;&#32500;&#24230;&#65292;$n$&#26159;&#21021;&#22987;&#26679;&#26412;&#25968;&#12290;&#23545;&#20110;&#38750;&#24179;&#28369;&#30340;Lipschitz&#25439;&#22833;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#21462;&#28040;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#36229;&#20986;&#24635;&#20307;&#39118;&#38505;&#20026;$\tilde O\big(\frac{1}{\sqrt{n}}+\big(\frac{\sqrt{d}}{n\rho}$
&lt;/p&gt;
&lt;p&gt;
We formalize the problem of machine unlearning as design of efficient unlearning algorithms corresponding to learning algorithms which perform a selection of adaptive queries from structured query classes. We give efficient unlearning algorithms for linear and prefix-sum query classes. As applications, we show that unlearning in many problems, in particular, stochastic convex optimization (SCO), can be reduced to the above, yielding improved guarantees for the problem. In particular, for smooth Lipschitz losses and any $\rho&gt;0$, our results yield an unlearning algorithm with excess population risk of $\tilde O\big(\frac{1}{\sqrt{n}}+\frac{\sqrt{d}}{n\rho}\big)$ with unlearning query (gradient) complexity $\tilde O(\rho \cdot \text{Retraining Complexity})$, where $d$ is the model dimensionality and $n$ is the initial number of samples. For non-smooth Lipschitz losses, we give an unlearning algorithm with excess population risk $\tilde O\big(\frac{1}{\sqrt{n}}+\big(\frac{\sqrt{d}}{n\rho}
&lt;/p&gt;</description></item><item><title>Jina Embeddings&#26159;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#35813;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11224</link><description>&lt;p&gt;
Jina Embeddings:&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models. (arXiv:2307.11224v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11224
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings&#26159;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#35813;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Jina Embeddings&#30001;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#32452;&#25104;&#65292;&#33021;&#22815;&#23558;&#21508;&#31181;&#25991;&#26412;&#36755;&#20837;&#36716;&#21270;&#20026;&#25968;&#20540;&#34920;&#31034;&#65292;&#20174;&#32780;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#24182;&#38750;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#65292;&#20294;&#22312;&#23494;&#38598;&#26816;&#32034;&#21644;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#31561;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#20174;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25104;&#23545;&#21644;&#19977;&#20803;&#25968;&#25454;&#38598;&#24320;&#22987;&#12290;&#23427;&#24378;&#35843;&#20102;&#25968;&#25454;&#28165;&#29702;&#22312;&#25968;&#25454;&#38598;&#20934;&#22791;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#23545;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#26368;&#21518;&#21033;&#29992;Massive Textual Embedding Benchmark&#65288;MTEB&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating various textual inputs into numerical representations, thereby capturing the semantic essence of the text. While these models are not exclusively designed for text generation, they excel in applications such as dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of a high-quality pairwise and triplet dataset. It underlines the crucial role of data cleaning in dataset preparation, gives in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Textual Embedding Benchmark (MTEB).
&lt;/p&gt;</description></item><item><title>FairMobi-Net&#26159;&#19968;&#31181;&#27880;&#37325;&#20844;&#24179;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#36328;&#21306;&#22495;&#30340;&#30495;&#23454;&#20154;&#27969;&#12290;&#23427;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#21807;&#19968;&#22320;&#24341;&#20837;&#20102;&#20844;&#24179;&#24615;&#25439;&#22833;&#65292;&#24182;&#37319;&#29992;&#20108;&#36827;&#21046;&#20998;&#31867;&#21644;&#25968;&#20540;&#22238;&#24402;&#25216;&#26415;&#30340;&#28151;&#21512;&#26041;&#27861;&#12290;&#27169;&#22411;&#32463;&#36807;&#22235;&#20010;&#32654;&#22269;&#22478;&#24066;&#30340;&#20840;&#38754;&#39564;&#35777;&#65292;&#24182;&#33021;&#22815;&#39044;&#27979;&#20154;&#31867;&#27969;&#21160;&#12290;</title><link>http://arxiv.org/abs/2307.11214</link><description>&lt;p&gt;
FairMobi-Net:&#19968;&#20010;&#27880;&#37325;&#20844;&#24179;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#22478;&#24066;&#27969;&#21160;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
FairMobi-Net: A Fairness-aware Deep Learning Model for Urban Mobility Flow Generation. (arXiv:2307.11214v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11214
&lt;/p&gt;
&lt;p&gt;
FairMobi-Net&#26159;&#19968;&#31181;&#27880;&#37325;&#20844;&#24179;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#36328;&#21306;&#22495;&#30340;&#30495;&#23454;&#20154;&#27969;&#12290;&#23427;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#21807;&#19968;&#22320;&#24341;&#20837;&#20102;&#20844;&#24179;&#24615;&#25439;&#22833;&#65292;&#24182;&#37319;&#29992;&#20108;&#36827;&#21046;&#20998;&#31867;&#21644;&#25968;&#20540;&#22238;&#24402;&#25216;&#26415;&#30340;&#28151;&#21512;&#26041;&#27861;&#12290;&#27169;&#22411;&#32463;&#36807;&#22235;&#20010;&#32654;&#22269;&#22478;&#24066;&#30340;&#20840;&#38754;&#39564;&#35777;&#65292;&#24182;&#33021;&#22815;&#39044;&#27979;&#20154;&#31867;&#27969;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#36328;&#21306;&#22495;&#30340;&#30495;&#23454;&#20154;&#27969;&#23545;&#25105;&#20204;&#29702;&#35299;&#22478;&#24066;&#32467;&#26500;&#21644;&#20154;&#21475;&#27963;&#21160;&#27169;&#24335;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#22312;&#22478;&#24066;&#35268;&#21010;&#21644;&#31649;&#29702;&#39046;&#22495;&#20013;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22823;&#22810;&#25968;&#27969;&#21160;&#29983;&#25104;&#26041;&#27861;&#30340;&#19968;&#20010;&#26126;&#26174;&#32570;&#28857;&#26159;&#24573;&#35270;&#20102;&#39044;&#27979;&#20844;&#24179;&#24615;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#39640;&#21361;&#20154;&#32676;&#22320;&#21306;&#31227;&#21160;&#27969;&#37327;&#30340;&#20302;&#20272;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#36164;&#28304;&#20998;&#37197;&#21644;&#22522;&#30784;&#35774;&#26045;&#21457;&#23637;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#27880;&#37325;&#20844;&#24179;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;FairMobi-Net&#65292;&#29992;&#20110;&#36328;&#21306;&#22495;&#20154;&#27969;&#39044;&#27979;&#12290;FairMobi-Net&#27169;&#22411;&#23558;&#20844;&#24179;&#24615;&#25439;&#22833;&#29420;&#29305;&#22320;&#32435;&#20837;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#37319;&#29992;&#20108;&#36827;&#21046;&#20998;&#31867;&#21644;&#25968;&#20540;&#22238;&#24402;&#25216;&#26415;&#30340;&#28151;&#21512;&#26041;&#27861;&#29992;&#20110;&#20154;&#27969;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#22235;&#20010;&#32654;&#22269;&#22478;&#24066;&#30340;&#20840;&#38754;&#20154;&#27969;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;FairMobi-Net&#27169;&#22411;&#65292;&#24182;&#39044;&#27979;&#20154;&#31867;&#27969;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating realistic human flows across regions is essential for our understanding of urban structures and population activity patterns, enabling important applications in the fields of urban planning and management. However, a notable shortcoming of most existing mobility generation methodologies is neglect of prediction fairness, which can result in underestimation of mobility flows across regions with vulnerable population groups, potentially resulting in inequitable resource distribution and infrastructure development. To overcome this limitation, our study presents a novel, fairness-aware deep learning model, FairMobi-Net, for inter-region human flow prediction. The FairMobi-Net model uniquely incorporates fairness loss into the loss function and employs a hybrid approach, merging binary classification and numerical regression techniques for human flow prediction. We validate the FairMobi-Net model using comprehensive human mobility datasets from four U.S. cities, predicting human
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#34892;&#25919;&#21355;&#29983;&#20445;&#20581;&#25968;&#25454;&#25506;&#32034;&#20102;&#27969;&#34892;&#30149;&#23398;&#38431;&#21015;&#21019;&#24314;&#23545;&#20110;&#39044;&#27979;&#26080;&#23478;&#21487;&#24402;&#21644;&#35686;&#23519;&#20114;&#21160;&#32467;&#26524;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#35266;&#23519;&#31383;&#21475;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11211</link><description>&lt;p&gt;
&#21033;&#29992;&#34892;&#25919;&#21355;&#29983;&#20445;&#20581;&#25968;&#25454;&#65292;&#30740;&#31350;&#27969;&#34892;&#30149;&#23398;&#38431;&#21015;&#26500;&#24314;&#23545;&#26080;&#23478;&#21487;&#24402;&#21644;&#35686;&#23519;&#20114;&#21160;&#32467;&#26524;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Effect of Epidemiological Cohort Creation on the Machine Learning Prediction of Homelessness and Police Interaction Outcomes Using Administrative Health Care Data. (arXiv:2307.11211v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11211
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#34892;&#25919;&#21355;&#29983;&#20445;&#20581;&#25968;&#25454;&#25506;&#32034;&#20102;&#27969;&#34892;&#30149;&#23398;&#38431;&#21015;&#21019;&#24314;&#23545;&#20110;&#39044;&#27979;&#26080;&#23478;&#21487;&#24402;&#21644;&#35686;&#23519;&#20114;&#21160;&#32467;&#26524;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#35266;&#23519;&#31383;&#21475;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#31934;&#31070;&#30142;&#30149;&#21487;&#33021;&#23548;&#33268;&#26080;&#23478;&#21487;&#24402;&#21644;&#19982;&#35686;&#23519;&#30340;&#20114;&#21160;&#31561;&#19981;&#33391;&#32467;&#26524;&#65292;&#20102;&#35299;&#23548;&#33268;&#36825;&#20123;&#19981;&#33391;&#32467;&#26524;&#30340;&#20107;&#20214;&#26159;&#37325;&#35201;&#30340;&#12290;&#39044;&#27979;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#22788;&#20110;&#39118;&#38505;&#20013;&#30340;&#20010;&#20307;&#12290;&#20351;&#29992;&#22266;&#23450;&#35266;&#23519;&#31383;&#21475;&#30340;&#38431;&#21015;&#21644;&#36923;&#36753;&#22238;&#24402;&#65288;LR&#65289;&#25110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#19982;&#33258;&#36866;&#24212;&#21644;&#20998;&#21106;&#31383;&#21475;&#30456;&#27604;&#65292;&#24615;&#33021;&#36739;&#20302;&#12290;&#26041;&#27861;&#65306;&#20351;&#29992;&#20102;&#19968;&#20010;&#34892;&#25919;&#21355;&#29983;&#20445;&#20581;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;2013&#24180;4&#26376;1&#26085;&#33267;2018&#24180;3&#26376;31&#26085;&#26399;&#38388;&#22312;&#21152;&#25343;&#22823;&#38463;&#23572;&#20271;&#22612;&#30465;&#21345;&#23572;&#21152;&#37324;&#24066;&#34987;&#35786;&#26029;&#20026;&#25104;&#30270;&#25110;&#31934;&#31070;&#30142;&#30149;&#65288;AMH&#65289;&#30340;240,219&#20010;&#20010;&#20307;&#12290;&#23545;&#36825;&#20010;&#38431;&#21015;&#36827;&#34892;&#20102;2&#24180;&#30340;&#36319;&#36394;&#65292;&#20197;&#30830;&#23450;&#19982;&#26080;&#23478;&#21487;&#24402;&#21644;&#35686;&#23519;&#20114;&#21160;&#26377;&#20851;&#30340;&#22240;&#32032;&#12290;&#20026;&#20102;&#29702;&#35299;&#23545;&#39044;&#27979;&#27169;&#22411;&#30340;&#28789;&#27963;&#31383;&#21475;&#30340;&#22909;&#22788;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26367;&#20195;&#38431;&#21015;&#12290;&#28982;&#21518;&#65292;&#23545;&#27604;&#20102;&#20004;&#20010;&#38431;&#21015;&#20013;&#30340;LR&#21644;ML&#27169;&#22411;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#21644;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65288;XGBoost&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Mental illness can lead to adverse outcomes such as homelessness and police interaction and understanding of the events leading up to these adverse outcomes is important. Predictive models may help identify individuals at risk of such adverse outcomes. Using a fixed observation window cohort with logistic regression (LR) or machine learning (ML) models can result in lower performance when compared with adaptive and parcellated windows. Method: An administrative healthcare dataset was used, comprising of 240,219 individuals in Calgary, Alberta, Canada who were diagnosed with addiction or mental health (AMH) between April 1, 2013, and March 31, 2018. The cohort was followed for 2 years to identify factors associated with homelessness and police interactions. To understand the benefit of flexible windows to predictive models, an alternative cohort was created. Then LR and ML models, including random forests (RF), and extreme gradient boosting (XGBoost) were compared in the two
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#20020;&#24202;&#35797;&#39564;&#20013;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32467;&#26500;&#30340;&#26032;&#22411;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#30340;&#30142;&#30149;&#26816;&#27979;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#21069;&#30651;&#24615;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#12290;</title><link>http://arxiv.org/abs/2307.11209</link><description>&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Clinical Trial Active Learning. (arXiv:2307.11209v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11209
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#20020;&#24202;&#35797;&#39564;&#20013;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32467;&#26500;&#30340;&#26032;&#22411;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#30340;&#30142;&#30149;&#26816;&#27979;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#21069;&#30651;&#24615;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#20020;&#24202;&#35797;&#39564;&#29615;&#22659;&#20013;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-i.i.d.&#65289;&#30340;&#32467;&#26500;&#12290;&#20020;&#24202;&#35797;&#39564;&#20998;&#20026;&#22238;&#39038;&#24615;&#21644;&#21069;&#30651;&#24615;&#20004;&#31181;&#31867;&#22411;&#12290;&#22238;&#39038;&#24615;&#20020;&#24202;&#35797;&#39564;&#22312;&#27835;&#30103;&#21518;&#20998;&#26512;&#25968;&#25454;&#65307;&#21069;&#30651;&#24615;&#20020;&#24202;&#35797;&#39564;&#22312;&#27835;&#30103;&#36827;&#34892;&#26102;&#25910;&#38598;&#25968;&#25454;&#12290;&#20256;&#32479;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#25968;&#25454;&#38598;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65288;i.i.d.&#65289;&#65292;&#36873;&#25321;&#35757;&#32451;&#26679;&#26412;&#26102;&#24573;&#30053;&#20102;&#20020;&#24202;&#35797;&#39564;&#20013;&#25968;&#25454;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21069;&#30651;&#24615;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#20811;&#26381;&#20256;&#32479;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#23616;&#38480;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#22270;&#20687;&#20013;&#30340;&#30142;&#30149;&#26816;&#27979;&#65292;&#36890;&#36807;&#23545;&#22270;&#20687;&#25910;&#38598;&#26102;&#38388;&#30340;&#26465;&#20214;&#32422;&#26463;&#26469;&#24378;&#21046;&#25191;&#34892;i.i.d.&#20551;&#35774;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to active learning that takes into account the non-independent and identically distributed (non-i.i.d.) structure of a clinical trial setting. There exists two types of clinical trials: retrospective and prospective. Retrospective clinical trials analyze data after treatment has been performed; prospective clinical trials collect data as treatment is ongoing. Typically, active learning approaches assume the dataset is i.i.d. when selecting training samples; however, in the case of clinical trials, treatment results in a dependency between the data collected at the current and past visits. Thus, we propose prospective active learning to overcome the limitations present in traditional active learning methods and apply it to disease detection in optical coherence tomography (OCT) images, where we condition on the time an image was collected to enforce the i.i.d. assumption. We compare our proposed method to the traditional active learning paradigm, whi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#36229;&#21442;&#25968;&#65292;&#23454;&#29616;&#23545;&#28145;&#24230;&#29305;&#24449;&#30340;&#38477;&#32500;&#65292;&#20197;&#20943;&#23569;&#20887;&#20313;&#29305;&#24449;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11197</link><description>&lt;p&gt;
&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#30340;&#21551;&#21457;&#24335;&#36229;&#21442;&#25968;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Heuristic Hyperparameter Choice for Image Anomaly Detection. (arXiv:2307.11197v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#36229;&#21442;&#25968;&#65292;&#23454;&#29616;&#23545;&#28145;&#24230;&#29305;&#24449;&#30340;&#38477;&#32500;&#65292;&#20197;&#20943;&#23569;&#20887;&#20313;&#29305;&#24449;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#65292;&#29992;&#20110;&#35782;&#21035;&#19982;&#27491;&#24120;&#29366;&#24577;&#26174;&#33879;&#20559;&#31163;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#21462;&#30340;&#28145;&#24230;&#29305;&#24449;&#36827;&#34892;&#22810;&#20803;&#39640;&#26031;&#20998;&#24067;&#20998;&#26512;&#65292;&#24050;&#32463;&#35777;&#26126;&#36825;&#20123;&#29305;&#24449;&#23545;&#20110;&#24322;&#24120;&#26816;&#27979;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet&#65289;&#19978;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#65292;&#22240;&#27492;&#23427;&#20204;&#21487;&#33021;&#20250;&#20135;&#29983;&#22823;&#37327;&#20887;&#20313;&#30340;&#29305;&#24449;&#65292;&#22686;&#21152;&#20102;&#35745;&#31639;&#25104;&#26412;&#24182;&#38477;&#20302;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#26088;&#22312;&#23545;&#36825;&#20123;&#29305;&#24449;&#36827;&#34892;&#36127;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;NPCA&#65289;&#30340;&#38477;&#32500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#36873;&#25321;NPCA&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#65292;&#20197;&#23613;&#21487;&#33021;&#33719;&#21462;&#36739;&#23569;&#30340;&#29305;&#24449;&#20998;&#37327;&#65292;&#21516;&#26102;&#30830;&#20445;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) in images is a fundamental computer vision problem by deep learning neural network to identify images deviating significantly from normality. The deep features extracted from pretrained models have been proved to be essential for AD based on multivariate Gaussian distribution analysis. However, since models are usually pretrained on a large dataset for classification tasks such as ImageNet, they might produce lots of redundant features for AD, which increases computational cost and degrades the performance. We aim to do the dimension reduction of Negated Principal Component Analysis (NPCA) for these features. So we proposed some heuristic to choose hyperparameter of NPCA algorithm for getting as fewer components of features as possible while ensuring a good performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;MuJoCo&#29615;&#22659;&#20013;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31639;&#27861;&#65292;&#21457;&#29616;DDPG&#22312;&#23569;&#37327;episode&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.11166</link><description>&lt;p&gt;
&#22312;MuJoCo&#29615;&#22659;&#20013;&#25506;&#32034;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring reinforcement learning techniques for discrete and continuous control tasks in the MuJoCo environment. (arXiv:2307.11166v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11166
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;MuJoCo&#29615;&#22659;&#20013;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31639;&#27861;&#65292;&#21457;&#29616;DDPG&#22312;&#23569;&#37327;episode&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#24555;&#36895;&#30340;&#29289;&#29702;&#27169;&#25311;&#22120;MuJoCo&#22312;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#36816;&#34892;&#20219;&#21153;&#65292;&#24182;&#25581;&#31034;&#27599;&#20010;&#20219;&#21153;&#30340;&#35266;&#27979;&#31354;&#38388;&#12289;&#21160;&#20316;&#31354;&#38388;&#12289;&#22870;&#21169;&#31561;&#35814;&#32454;&#20449;&#24687;&#12290;&#36890;&#36807;&#27604;&#36739;&#31163;&#25955;&#21270;&#26041;&#27861;&#20013;&#30340;Q&#23398;&#20064;&#21644;SARSA&#65292;&#23558;&#20540;&#22522;&#26041;&#27861;&#24212;&#29992;&#20110;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#20316;&#20026;&#22522;&#20934;&#65292;&#36880;&#27493;&#21521;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;DDPG&#36807;&#28193;&#12290;&#22312;&#22823;&#37327;&#30340;episode&#20013;&#65292;Q&#23398;&#20064;&#30340;&#24471;&#20998;&#36229;&#36807;&#20102;SARSA&#65292;&#20294;DDPG&#22312;&#23569;&#37327;&#30340;episode&#20013;&#34920;&#29616;&#20248;&#20110;&#20004;&#32773;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#36229;&#21442;&#25968;&#65292;&#26399;&#26395;&#22312;&#26356;&#23569;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39044;&#26399;&#26032;&#35774;&#35745;&#30340;DDPG&#23558;&#22823;&#24133;&#25552;&#39640;&#24615;&#33021;&#65292;&#28982;&#32780;&#22312;&#21482;&#26377;&#23569;&#25968;episode&#20043;&#21518;&#65292;&#25105;&#20204;&#24050;&#32463;&#33021;&#22815;&#36798;&#21040;&#19981;&#38169;&#30340;&#24179;&#22343;&#22870;&#21169;&#12290;&#25105;&#20204;&#26399;&#26395;&#22312;&#36275;&#22815;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#19979;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We leverage the fast physics simulator, MuJoCo to run tasks in a continuous control environment and reveal details like the observation space, action space, rewards, etc. for each task. We benchmark value-based methods for continuous control by comparing Q-learning and SARSA through a discretization approach, and using them as baselines, progressively moving into one of the state-of-the-art deep policy gradient method DDPG. Over a large number of episodes, Qlearning outscored SARSA, but DDPG outperformed both in a small number of episodes. Lastly, we also fine-tuned the model hyper-parameters expecting to squeeze more performance but using lesser time and resources. We anticipated that the new design for DDPG would vastly improve performance, yet after only a few episodes, we were able to achieve decent average rewards. We expect to improve the performance provided adequate time and computational resources.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33041;&#32593;&#32476;&#30340;&#23545;&#27604;&#22270;&#27744;&#21270;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#33041;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#12290;&#36890;&#36807;&#23450;&#21046;&#21270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;5&#20010;&#38745;&#24687;&#24577;fMRI&#33041;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#22522;&#20934;&#32447;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11133</link><description>&lt;p&gt;
&#23545;&#33041;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#36827;&#34892;&#23545;&#27604;&#22270;&#27744;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Graph Pooling for Explainable Classification of Brain Networks. (arXiv:2307.11133v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33041;&#32593;&#32476;&#30340;&#23545;&#27604;&#22270;&#27744;&#21270;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#33041;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#12290;&#36890;&#36807;&#23450;&#21046;&#21270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;5&#20010;&#38745;&#24687;&#24577;fMRI&#33041;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#22522;&#20934;&#32447;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;(fMRI)&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#27979;&#37327;&#31070;&#32463;&#27963;&#21160;&#30340;&#25216;&#26415;&#12290;&#20854;&#24212;&#29992;&#22312;&#35782;&#21035;&#24085;&#37329;&#26862;&#30149;&#12289;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#33258;&#38381;&#30151;&#31561;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#26041;&#38754;&#23588;&#20026;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;fMRI&#25968;&#25454;&#20998;&#26512;&#23558;&#22823;&#33041;&#24314;&#27169;&#20026;&#22270;&#65292;&#24182;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#25552;&#21462;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;fMRI&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#24449;&#35201;&#27714;&#23545;GNN&#36827;&#34892;&#29305;&#27530;&#35774;&#35745;&#12290;&#23450;&#21046;GNN&#20197;&#29983;&#25104;&#26377;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#21452;&#27880;&#24847;&#22359;&#21644;&#21487;&#24494;&#20998;&#22270;&#27744;&#21270;&#26041;&#27861;ContrastPool&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;GNN&#20998;&#26512;&#33041;&#32593;&#32476;&#65292;&#28385;&#36275;fMRI&#30340;&#29305;&#27530;&#35201;&#27714;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;5&#20010;&#38745;&#24687;&#24577;fMRI&#33041;&#32593;&#32476;&#25968;&#25454;&#38598;&#30340;3&#31181;&#30142;&#30149;&#65292;&#24182;&#35777;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#32447;&#12290;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21462;&#30340;&#27169;&#24335;&#19982;&#31070;&#32463;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#39046;&#22495;&#30693;&#35782;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Functional magnetic resonance imaging (fMRI) is a commonly used technique to measure neural activation. Its application has been particularly important in identifying underlying neurodegenerative conditions such as Parkinson's, Alzheimer's, and Autism. Recent analysis of fMRI data models the brain as a graph and extracts features by graph neural networks (GNNs). However, the unique characteristics of fMRI data require a special design of GNN. Tailoring GNN to generate effective and domain-explainable features remains challenging. In this paper, we propose a contrastive dual-attention block and a differentiable graph pooling method called ContrastPool to better utilize GNN for brain networks, meeting fMRI-specific requirements. We apply our method to 5 resting-state fMRI brain network datasets of 3 diseases and demonstrate its superiority over state-of-the-art baselines. Our case study confirms that the patterns extracted by our method match the domain knowledge in neuroscience literatu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#65292;&#36890;&#36807;&#23494;&#24230;&#21305;&#37197;&#26469;&#35299;&#20915;&#29616;&#26377;SCMs&#20013;&#30340;&#38544;&#24335;&#20869;&#29983;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#32463;&#36807;&#22788;&#29702;&#21333;&#20803;&#30340;&#32467;&#26524;&#23494;&#24230;&#19982;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#23494;&#24230;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#20272;&#35745;SC&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2307.11127</link><description>&lt;p&gt;
&#36890;&#36807;&#23494;&#24230;&#21305;&#37197;&#23454;&#29616;&#30340;&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#19979;&#30340;&#38544;&#24335;&#20869;&#29983;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Synthetic Control Methods by Density Matching under Implicit Endogeneitiy. (arXiv:2307.11127v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#65292;&#36890;&#36807;&#23494;&#24230;&#21305;&#37197;&#26469;&#35299;&#20915;&#29616;&#26377;SCMs&#20013;&#30340;&#38544;&#24335;&#20869;&#29983;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#32463;&#36807;&#22788;&#29702;&#21333;&#20803;&#30340;&#32467;&#26524;&#23494;&#24230;&#19982;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#23494;&#24230;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#20272;&#35745;SC&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#65288;SCMs&#65289;&#24050;&#25104;&#20026;&#27604;&#36739;&#26696;&#20363;&#30740;&#31350;&#20013;&#22240;&#26524;&#25512;&#26029;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;SCMs&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#35266;&#27979;&#32467;&#26524;&#30340;&#21152;&#26435;&#21644;&#26469;&#20272;&#35745;&#32463;&#36807;&#22788;&#29702;&#21333;&#20803;&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#12290;&#21512;&#25104;&#23545;&#29031;&#65288;SC&#65289;&#30340;&#20934;&#30830;&#24615;&#23545;&#20110;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;SC&#26435;&#37325;&#30340;&#20272;&#35745;&#25104;&#20026;&#20102;&#30740;&#31350;&#30340;&#28966;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25351;&#20986;&#29616;&#26377;&#30340;SCMs&#23384;&#22312;&#19968;&#20010;&#38544;&#24335;&#20869;&#29983;&#24615;&#38382;&#39064;&#65292;&#21363;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#32467;&#26524;&#19982;&#21453;&#20107;&#23454;&#32467;&#26524;&#27169;&#22411;&#20013;&#30340;&#35823;&#24046;&#39033;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#20250;&#23545;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#22120;&#20135;&#29983;&#20559;&#24046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#21305;&#37197;&#30340;&#26032;&#22411;SCM&#65292;&#20551;&#35774;&#32463;&#36807;&#22788;&#29702;&#21333;&#20803;&#30340;&#32467;&#26524;&#23494;&#24230;&#21487;&#20197;&#29992;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#23494;&#24230;&#30340;&#21152;&#26435;&#24179;&#22343;&#26469;&#36817;&#20284;&#65288;&#21363;&#28151;&#21512;&#27169;&#22411;&#65289;&#12290;&#22522;&#20110;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#36890;&#36807;&#21305;&#37197;&#26469;&#20272;&#35745;SC&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic control methods (SCMs) have become a crucial tool for causal inference in comparative case studies. The fundamental idea of SCMs is to estimate counterfactual outcomes for a treated unit by using a weighted sum of observed outcomes from untreated units. The accuracy of the synthetic control (SC) is critical for estimating the causal effect, and hence, the estimation of SC weights has been the focus of much research. In this paper, we first point out that existing SCMs suffer from an implicit endogeneity problem, which is the correlation between the outcomes of untreated units and the error term in the model of a counterfactual outcome. We show that this problem yields a bias in the causal effect estimator. We then propose a novel SCM based on density matching, assuming that the density of outcomes of the treated unit can be approximated by a weighted average of the densities of untreated units (i.e., a mixture model). Based on this assumption, we estimate SC weights by matchi
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#27169;&#22411;&#65292;&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#24739;&#26377;&#30196;&#21574;&#30151;&#30340;&#20154;&#22312;&#26085;&#24120;&#27963;&#21160;&#27169;&#24335;&#20013;&#30340;&#21464;&#21270;&#24182;&#21457;&#29616;&#20102;&#33829;&#20859;&#19981;&#33391;&#21644;&#33073;&#27700;&#31561;&#26497;&#31471;&#34892;&#20026;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#32771;&#23519;&#20102;COVID-19&#22823;&#27969;&#34892;&#23545;&#36825;&#20123;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.11126</link><description>&lt;p&gt;
&#29992;&#20110;&#35782;&#21035;&#24739;&#26377;&#30196;&#21574;&#30151;&#30340;&#20154;&#26085;&#24120;&#27963;&#21160;&#27169;&#24335;&#21464;&#21270;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Markov Chain Model for Identifying Changes in Daily Activity Patterns of People Living with Dementia. (arXiv:2307.11126v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11126
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#27169;&#22411;&#65292;&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#24739;&#26377;&#30196;&#21574;&#30151;&#30340;&#20154;&#22312;&#26085;&#24120;&#27963;&#21160;&#27169;&#24335;&#20013;&#30340;&#21464;&#21270;&#24182;&#21457;&#29616;&#20102;&#33829;&#20859;&#19981;&#33391;&#21644;&#33073;&#27700;&#31561;&#26497;&#31471;&#34892;&#20026;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#32771;&#23519;&#20102;COVID-19&#22823;&#27969;&#34892;&#23545;&#36825;&#20123;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33829;&#20859;&#19981;&#33391;&#21644;&#33073;&#27700;&#19982;&#24739;&#26377;&#30196;&#21574;&#30151;&#30340;&#20154;&#35748;&#30693;&#21644;&#21151;&#33021;&#34928;&#36864;&#22686;&#21152;&#20197;&#21450;&#20303;&#38498;&#29575;&#22686;&#21152;&#23494;&#20999;&#30456;&#20851;&#12290;&#39278;&#39135;&#21644;&#39278;&#27700;&#34892;&#20026;&#30340;&#26497;&#31471;&#21464;&#21270;&#32463;&#24120;&#23548;&#33268;&#33829;&#20859;&#19981;&#33391;&#21644;&#33073;&#27700;&#65292;&#21152;&#36895;&#35748;&#30693;&#21644;&#21151;&#33021;&#34928;&#36864;&#30340;&#36827;&#23637;&#65292;&#24182;&#23548;&#33268;&#29983;&#27963;&#36136;&#37327;&#26174;&#33879;&#38477;&#20302;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#23578;&#26080;&#30830;&#31435;&#30340;&#26041;&#27861;&#21487;&#20197;&#23458;&#35266;&#22320;&#26816;&#27979;&#36825;&#20123;&#21464;&#21270;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;73&#20010;&#24739;&#26377;&#30196;&#21574;&#30151;&#30340;&#23478;&#24237;&#20351;&#29992;&#29289;&#32852;&#32593;&#25216;&#26415;&#25910;&#38598;&#30340;&#22312;&#23478;&#30417;&#27979;&#25968;&#25454;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;2019&#20896;&#29366;&#30149;&#27602;&#30149;&#65288;COVID-19&#65289;&#22823;&#27969;&#34892;&#24050;&#32463;&#26174;&#33879;&#25913;&#21464;&#20102;&#24739;&#26377;&#30196;&#21574;&#30151;&#30340;&#34892;&#20026;&#20064;&#24815;&#65292;&#23588;&#20854;&#26159;&#39278;&#39135;&#21644;&#39278;&#27700;&#20064;&#24815;&#12290;&#21033;&#29992;COVID-19&#22823;&#27969;&#34892;&#20316;&#20026;&#33258;&#28982;&#23454;&#39564;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Malnutrition and dehydration are strongly associated with increased cognitive and functional decline in people living with dementia (PLWD), as well as an increased rate of hospitalisations in comparison to their healthy counterparts. Extreme changes in eating and drinking behaviours can often lead to malnutrition and dehydration, accelerating the progression of cognitive and functional decline and resulting in a marked reduction in quality of life. Unfortunately, there are currently no established methods by which to objectively detect such changes. Here, we present the findings of an extensive quantitative analysis conducted on in-home monitoring data collected from 73 households of PLWD using Internet of Things technologies. The Coronavirus 2019 (COVID-19) pandemic has previously been shown to have dramatically altered the behavioural habits, particularly the eating and drinking habits, of PLWD. Using the COVID-19 pandemic as a natural experiment, we conducted linear mixed-effects mo
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26080;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26143;&#31995;&#22270;&#20687;&#30340;PSF&#21453;&#35114;&#33394;&#65292;&#25429;&#25417;&#21040;&#20102;&#26356;&#22810;&#30340;&#26679;&#26412;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.11122</link><description>&lt;p&gt;
&#26143;&#31995;&#22270;&#20687;&#30340;&#27010;&#29575;&#21453;&#35114;&#33394;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Probabilistic Deconvolution of Galaxy Images. (arXiv:2307.11122v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11122
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26080;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26143;&#31995;&#22270;&#20687;&#30340;PSF&#21453;&#35114;&#33394;&#65292;&#25429;&#25417;&#21040;&#20102;&#26356;&#22810;&#30340;&#26679;&#26412;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26395;&#36828;&#38236;&#25429;&#25417;&#21040;&#30340;&#22270;&#20687;&#20855;&#26377;&#29305;&#23450;&#30340;&#28857;&#25193;&#25955;&#20989;&#25968;(PSF)&#12290;&#25512;&#26029;&#22312;&#20855;&#26377;&#26356;&#38160;&#21033;PSF&#30340;&#24773;&#20917;&#19979;&#22270;&#20687;&#30340;&#26679;&#35980;&#65292;&#21363;PSF&#21453;&#35114;&#33394;&#38382;&#39064;&#65292;&#26159;&#19981;&#36870;&#38382;&#39064;&#65292;&#22240;&#20026;PSF&#21367;&#31215;&#19981;&#26159;&#21487;&#36870;&#36716;&#25442;&#12290;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;PSF&#21453;&#35114;&#33394;&#20013;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#25512;&#26029;&#20986;&#19968;&#20010;&#20505;&#36873;&#22270;&#20687;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#35813;&#20998;&#24067;&#22312;&#19982;PSF&#21367;&#31215;&#21518;&#21487;&#33021;&#20135;&#29983;&#35266;&#27979;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(&#22914;VAE&#21644;GAN)&#36890;&#24120;&#19981;&#33021;&#25552;&#20379;&#36275;&#22815;&#30340;&#26679;&#26412;&#22810;&#26679;&#24615;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#26080;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#36827;&#34892;&#26143;&#31995;&#22270;&#20687;&#30340;PSF&#21453;&#35114;&#33394;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19982;&#26465;&#20214;VAE&#30456;&#27604;&#65292;&#36825;&#31181;&#25193;&#25955;&#27169;&#22411;&#25429;&#25417;&#21040;&#20102;&#26356;&#22810;&#21487;&#33021;&#30340;&#21453;&#35114;&#33394;&#26679;&#26412;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Telescopes capture images with a particular point spread function (PSF). Inferring what an image would have looked like with a much sharper PSF, a problem known as PSF deconvolution, is ill-posed because PSF convolution is not an invertible transformation. Deep generative models are appealing for PSF deconvolution because they can infer a posterior distribution over candidate images that, if convolved with the PSF, could have generated the observation. However, classical deep generative models such as VAEs and GANs often provide inadequate sample diversity. As an alternative, we propose a classifier-free conditional diffusion model for PSF deconvolution of galaxy images. We demonstrate that this diffusion model captures a greater diversity of possible deconvolutions compared to a conditional VAE.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#27604;&#20102;&#21464;&#21387;&#22120;&#21644;&#21367;&#31215;&#27169;&#22411;&#22312;&#26118;&#34411;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#35299;&#20915;&#29289;&#31181;&#37492;&#23450;&#20013;&#30340;&#22256;&#38590;&#38750;&#24120;&#26377;&#24110;&#21161;&#65292;&#24182;&#20026;&#33258;&#21160;&#22270;&#20687;&#20998;&#31867;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2307.11112</link><description>&lt;p&gt;
&#21464;&#21387;&#22120;&#21644;&#21367;&#31215;&#27169;&#22411;&#22312;&#26118;&#34411;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparison between transformers and convolutional models for fine-grained classification of insects. (arXiv:2307.11112v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#27604;&#20102;&#21464;&#21387;&#22120;&#21644;&#21367;&#31215;&#27169;&#22411;&#22312;&#26118;&#34411;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#35299;&#20915;&#29289;&#31181;&#37492;&#23450;&#20013;&#30340;&#22256;&#38590;&#38750;&#24120;&#26377;&#24110;&#21161;&#65292;&#24182;&#20026;&#33258;&#21160;&#22270;&#20687;&#20998;&#31867;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25214;&#21040;&#20855;&#26377;&#37492;&#21035;&#33021;&#21147;&#30340;&#29305;&#24449;&#30340;&#22256;&#38590;&#65292;&#32454;&#31890;&#24230;&#20998;&#31867;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#24403;&#24212;&#29992;&#20110;&#21516;&#19968;&#20998;&#31867;&#31867;&#21035;&#20013;&#30340;&#29289;&#31181;&#35782;&#21035;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#20250;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#36825;&#26159;&#22240;&#20026;&#29289;&#31181;&#36890;&#24120;&#20849;&#20139;&#20351;&#23427;&#20204;&#38590;&#20197;&#21306;&#20998;&#30340;&#24418;&#24577;&#29305;&#24449;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26118;&#34411;&#32434;&#12290;&#26118;&#34411;&#30340;&#37492;&#23450;&#23545;&#29983;&#29289;&#22810;&#26679;&#24615;&#30417;&#27979;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#35768;&#22810;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#22522;&#30784;&#29983;&#29289;&#20043;&#19968;&#12290;&#20844;&#20247;&#31185;&#23398;&#27491;&#22312;&#25910;&#38598;&#37326;&#22806;&#26118;&#34411;&#22270;&#20687;&#30340;&#26480;&#20986;&#24037;&#20316;&#65292;&#20026;&#19987;&#23478;&#22312;&#25152;&#26377;&#22269;&#23478;&#21046;&#20316;&#25913;&#36827;&#30340;&#20998;&#24067;&#22270;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#26377;&#25968;&#21313;&#20159;&#38656;&#35201;&#33258;&#21160;&#20998;&#31867;&#30340;&#22270;&#20687;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#26159;&#32454;&#31890;&#24230;&#20219;&#21153;&#30340;&#20027;&#35201;&#25216;&#26415;&#20043;&#19968;&#12290;&#22312;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#39046;&#22495;&#38750;&#24120;&#20016;&#23500;&#65292;&#37027;&#20040;&#22914;&#20309;&#30830;&#23450;&#35201;&#20351;&#29992;&#30340;&#31639;&#27861;&#65311;&#25105;&#20204;&#19987;&#27880;&#20110;&#34619;&#34579;&#30446;&#21644;&#30002;&#34411;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained classification is challenging due to the difficulty of finding discriminatory features. This problem is exacerbated when applied to identifying species within the same taxonomical class. This is because species are often sharing morphological characteristics that make them difficult to differentiate. We consider the taxonomical class of Insecta. The identification of insects is essential in biodiversity monitoring as they are one of the inhabitants at the base of many ecosystems. Citizen science is doing brilliant work of collecting images of insects in the wild giving the possibility to experts to create improved distribution maps in all countries. We have billions of images that need to be automatically classified and deep neural network algorithms are one of the main techniques explored for fine-grained tasks. At the SOTA, the field of deep learning algorithms is extremely fruitful, so how to identify the algorithm to use? We focus on Odonata and Coleoptera orders, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#22374;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#31639;&#27861;&#65288;FAD&#65289;&#65292;&#29992;&#20110;&#38024;&#23545;&#39046;&#22495;&#25512;&#24191;&#38382;&#39064;&#12290;&#36890;&#36807;&#21516;&#26102;&#20248;&#21270;&#38646;&#38454;&#21644;&#19968;&#38454;&#24179;&#22374;&#24230;&#65292;FAD&#22312;&#21508;&#31181;&#39046;&#22495;&#25512;&#24191;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#33021;&#21457;&#29616;&#26356;&#24179;&#22374;&#30340;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.11108</link><description>&lt;p&gt;
&#38024;&#23545;&#39046;&#22495;&#25512;&#24191;&#30340;&#24179;&#22374;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Flatness-Aware Minimization for Domain Generalization. (arXiv:2307.11108v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#22374;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#31639;&#27861;&#65288;FAD&#65289;&#65292;&#29992;&#20110;&#38024;&#23545;&#39046;&#22495;&#25512;&#24191;&#38382;&#39064;&#12290;&#36890;&#36807;&#21516;&#26102;&#20248;&#21270;&#38646;&#38454;&#21644;&#19968;&#38454;&#24179;&#22374;&#24230;&#65292;FAD&#22312;&#21508;&#31181;&#39046;&#22495;&#25512;&#24191;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#33021;&#21457;&#29616;&#26356;&#24179;&#22374;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#25512;&#24191;&#26088;&#22312;&#23398;&#20064;&#22312;&#26410;&#30693;&#20998;&#24067;&#20559;&#31227;&#19979;&#33021;&#22815;&#33391;&#22909;&#27867;&#21270;&#30340;&#40065;&#26834;&#27169;&#22411;&#12290;&#20316;&#20026;&#39046;&#22495;&#25512;&#24191;&#30340;&#20851;&#38190;&#26041;&#38754;&#20043;&#19968;&#65292;&#20248;&#21270;&#22120;&#30340;&#36873;&#25321;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#12290;&#30446;&#21069;&#65292;&#22823;&#37096;&#20998;&#39046;&#22495;&#25512;&#24191;&#26041;&#27861;&#37117;&#36981;&#24490;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797; DomainBed&#65292;&#24182;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;Adam&#20316;&#20026;&#40664;&#35748;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25581;&#31034;&#20986;Adam&#24182;&#19981;&#19968;&#23450;&#26159;&#24403;&#21069;&#22823;&#22810;&#25968;&#39046;&#22495;&#25512;&#24191;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#36873;&#25321;&#12290;&#22522;&#20110;&#25439;&#22833;&#20989;&#25968;&#24179;&#22374;&#24230;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;&#38024;&#23545;&#39046;&#22495;&#25512;&#24191;&#30340;&#24179;&#22374;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#31639;&#27861;&#65288;FAD&#65289;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#21516;&#26102;&#20248;&#21270;&#38646;&#38454;&#21644;&#19968;&#38454;&#24179;&#22374;&#24230;&#12290;&#25105;&#20204;&#23545;FAD&#30340;&#31163;&#20998;&#24067;&#27867;&#21270;&#35823;&#24046;&#21644;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FAD&#22312;&#21508;&#31181;&#39046;&#22495;&#25512;&#24191;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#36234;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#23454;FAD&#33021;&#22815;&#21457;&#29616;&#26356;&#24179;&#22374;&#30340;&#26368;&#20248;&#35299;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#38646;&#38454;&#21644;&#19968;&#38454;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization (DG) seeks to learn robust models that generalize well under unknown distribution shifts. As a critical aspect of DG, optimizer selection has not been explored in depth. Currently, most DG methods follow the widely used benchmark, DomainBed, and utilize Adam as the default optimizer for all datasets. However, we reveal that Adam is not necessarily the optimal choice for the majority of current DG methods and datasets. Based on the perspective of loss landscape flatness, we propose a novel approach, Flatness-Aware Minimization for Domain Generalization (FAD), which can efficiently optimize both zeroth-order and first-order flatness simultaneously for DG. We provide theoretical analyses of the FAD's out-of-distribution (OOD) generalization error and convergence. Our experimental results demonstrate the superiority of FAD on various DG datasets. Additionally, we confirm that FAD is capable of discovering flatter optima in comparison to other zeroth-order and first-or
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#32447;&#24615;&#20248;&#21270;&#20013;&#29305;&#24449;&#39044;&#22788;&#29702;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#24773;&#20917;&#19979;&#65292;&#19982;&#38750;&#38544;&#31169;&#20248;&#21270;&#30456;&#27604;&#65292;&#29305;&#24449;&#39044;&#22788;&#29702;&#23545;&#20110;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#21542;&#21017;&#20250;&#20135;&#29983;&#19982;&#29305;&#24449;&#26368;&#22823;&#33539;&#25968;&#25104;&#27604;&#20363;&#30340;&#38544;&#31169;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#29305;&#24449;&#39044;&#22788;&#29702;&#30340;&#31639;&#27861;DPSGD-F&#12290;</title><link>http://arxiv.org/abs/2307.11106</link><description>&lt;p&gt;
&#29305;&#24449;&#39044;&#22788;&#29702;&#23545;&#24046;&#20998;&#38544;&#31169;&#32447;&#24615;&#20248;&#21270;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The importance of feature preprocessing for differentially private linear optimization. (arXiv:2307.11106v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#32447;&#24615;&#20248;&#21270;&#20013;&#29305;&#24449;&#39044;&#22788;&#29702;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#24773;&#20917;&#19979;&#65292;&#19982;&#38750;&#38544;&#31169;&#20248;&#21270;&#30456;&#27604;&#65292;&#29305;&#24449;&#39044;&#22788;&#29702;&#23545;&#20110;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#21542;&#21017;&#20250;&#20135;&#29983;&#19982;&#29305;&#24449;&#26368;&#22823;&#33539;&#25968;&#25104;&#27604;&#20363;&#30340;&#38544;&#31169;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#29305;&#24449;&#39044;&#22788;&#29702;&#30340;&#31639;&#27861;DPSGD-F&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20854;&#20013;&#26368;&#27969;&#34892;&#30340;&#29992;&#20110;&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#30340;&#31639;&#27861;&#20043;&#19968;&#26159;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DPSGD&#65289;&#21450;&#20854;&#21464;&#31181;&#65292;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#65292;&#26799;&#24230;&#34987;&#21098;&#35009;&#24182;&#19982;&#19968;&#20123;&#22122;&#38899;&#32467;&#21512;&#12290;&#37492;&#20110;DPSGD&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#38382;&#39064;&#65306;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#65292;&#20165;&#20165;&#20351;&#29992;DPSGD&#26159;&#21542;&#36275;&#20197;&#25214;&#21040;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#33391;&#22909;&#26497;&#23567;&#20540;&#28857;&#65311;&#20316;&#20026;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#23545;&#20110;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#24773;&#20917;&#65292;&#19982;&#38750;&#38544;&#31169;&#20248;&#21270;&#30456;&#27604;&#65292;&#65288;&#31169;&#26377;&#65289;&#29305;&#24449;&#39044;&#22788;&#29702;&#23545;&#20110;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23384;&#22312;&#19968;&#31181;&#20363;&#23376;&#65292;&#22312;&#27809;&#26377;&#29305;&#24449;&#39044;&#22788;&#29702;&#30340;&#24773;&#20917;&#19979;&#65292;DPSGD&#20250;&#20135;&#29983;&#19982;&#25152;&#26377;&#26679;&#26412;&#19978;&#30340;&#29305;&#24449;&#30340;&#26368;&#22823;&#33539;&#25968;&#25104;&#27604;&#20363;&#30340;&#38544;&#31169;&#38169;&#35823;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DPSGD-F&#30340;&#31639;&#27861;&#65292;&#23558;DPSGD&#19982;&#29305;&#24449;&#39044;&#22788;&#29702;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training machine learning models with differential privacy (DP) has received increasing interest in recent years. One of the most popular algorithms for training differentially private models is differentially private stochastic gradient descent (DPSGD) and its variants, where at each step gradients are clipped and combined with some noise. Given the increasing usage of DPSGD, we ask the question: is DPSGD alone sufficient to find a good minimizer for every dataset under privacy constraints? As a first step towards answering this question, we show that even for the simple case of linear classification, unlike non-private optimization, (private) feature preprocessing is vital for differentially private optimization. In detail, we first show theoretically that there exists an example where without feature preprocessing, DPSGD incurs a privacy error proportional to the maximum norm of features over all samples. We then propose an algorithm called DPSGD-F, which combines DPSGD with feature
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;AAA&#28216;&#25103;&#27979;&#35797;&#20013;&#37096;&#32626;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#25152;&#38754;&#20020;&#30340;&#25216;&#26415;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#24110;&#21161;&#28216;&#25103;&#34892;&#19994;&#37319;&#29992;&#36825;&#39033;&#25216;&#26415;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.11105</link><description>&lt;p&gt;
&#37096;&#32626;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36827;&#34892;AAA&#28216;&#25103;&#27979;&#35797;&#30340;&#25216;&#26415;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Technical Challenges of Deploying Reinforcement Learning Agents for Game Testing in AAA Games. (arXiv:2307.11105v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;AAA&#28216;&#25103;&#27979;&#35797;&#20013;&#37096;&#32626;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#25152;&#38754;&#20020;&#30340;&#25216;&#26415;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#24110;&#21161;&#28216;&#25103;&#34892;&#19994;&#37319;&#29992;&#36825;&#39033;&#25216;&#26415;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#30740;&#31350;&#21040;&#23454;&#38469;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#22411;&#22797;&#26434;&#36719;&#20214;&#31995;&#32479;&#26469;&#35828;&#65292;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#22312;&#22823;&#35268;&#27169;&#28216;&#25103;&#21046;&#20316;&#20013;&#65292;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#26159;&#24320;&#21457;&#29615;&#22659;&#21487;&#33021;&#19982;&#26368;&#32456;&#20135;&#21697;&#23384;&#22312;&#24456;&#22823;&#24046;&#24322;&#12290;&#26412;&#25216;&#26415;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#22312;&#29616;&#26377;&#22522;&#20110;&#33050;&#26412;&#26426;&#22120;&#20154;&#30340;&#33258;&#21160;&#21270;&#28216;&#25103;&#27979;&#35797;&#35299;&#20915;&#26041;&#26696;&#20013;&#28155;&#21152;&#23454;&#39564;&#24615;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#30340;&#21162;&#21147;&#65292;&#20197;&#22686;&#21152;&#20854;&#33021;&#21147;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#22914;&#20309;&#38598;&#25104;&#36825;&#20010;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#26088;&#22312;&#22686;&#21152;&#31867;&#20284;[1]&#30340;AAA&#28216;&#25103;&#65288;&#21253;&#25324;Battlefield 2042&#21644;Dead Space&#65288;2023&#65289;&#65289;&#30340;&#27979;&#35797;&#35206;&#30422;&#29575;&#12290;&#26412;&#25216;&#26415;&#35770;&#25991;&#26088;&#22312;&#23637;&#31034;&#22312;&#28216;&#25103;&#21046;&#20316;&#20013;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#26696;&#20363;&#65292;&#24182;&#20171;&#32461;&#20102;&#24076;&#26395;&#22312;&#28216;&#25103;&#20013;&#36827;&#34892;&#30456;&#21516;&#25506;&#32034;&#30340;&#20154;&#21487;&#33021;&#20250;&#36935;&#21040;&#30340;&#26368;&#22823;&#26102;&#38388;&#28040;&#32791;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#24110;&#21161;&#28216;&#25103;&#34892;&#19994;&#26356;&#24555;&#22320;&#37319;&#29992;&#36825;&#39033;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#25105;&#20204;&#35748;&#20026;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Going from research to production, especially for large and complex software systems, is fundamentally a hard problem. In large-scale game production, one of the main reasons is that the development environment can be very different from the final product. In this technical paper we describe an effort to add an experimental reinforcement learning system to an existing automated game testing solution based on scripted bots in order to increase its capacity. We report on how this reinforcement learning system was integrated with the aim to increase test coverage similar to [1] in a set of AAA games including Battlefield 2042 and Dead Space (2023). The aim of this technical paper is to show a use-case of leveraging reinforcement learning in game production and cover some of the largest time sinks anyone who wants to make the same journey for their game may encounter. Furthermore, to help the game industry to adopt this technology faster, we propose a few research directions that we believ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#23398;&#20064;&#20195;&#29702;&#21644;&#23398;&#20064;&#32422;&#26463;&#30456;&#32467;&#21512;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#22810;&#29289;&#29702;&#30340;&#21453;&#38382;&#39064;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#19981;&#20165;&#25913;&#21892;&#20102;&#23545;&#27969;&#20307;&#27969;&#21160;&#24615;&#36136;&#30340;&#21453;&#28436;&#31934;&#24230;&#65292;&#32780;&#19988;&#20026;&#21453;&#28436;&#22810;&#27169;&#24577;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.11099</link><description>&lt;p&gt;
&#29992;&#23398;&#20064;&#30340;&#20195;&#29702;&#21644;&#32422;&#26463;&#35299;&#20915;&#22522;&#20110;&#22810;&#29289;&#29702;&#30340;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving multiphysics-based inverse problems with learned surrogates and constraints. (arXiv:2307.11099v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#23398;&#20064;&#20195;&#29702;&#21644;&#23398;&#20064;&#32422;&#26463;&#30456;&#32467;&#21512;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#22810;&#29289;&#29702;&#30340;&#21453;&#38382;&#39064;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#19981;&#20165;&#25913;&#21892;&#20102;&#23545;&#27969;&#20307;&#27969;&#21160;&#24615;&#36136;&#30340;&#21453;&#28436;&#31934;&#24230;&#65292;&#32780;&#19988;&#20026;&#21453;&#28436;&#22810;&#27169;&#24577;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22320;&#36136;&#30899;&#23553;&#23384;&#30417;&#27979;&#20013;&#65292;&#24403;&#22810;&#27169;&#24577;&#26102;&#21464;&#25968;&#25454;&#26114;&#36149;&#19988;&#25968;&#20540;&#27169;&#25311;&#25104;&#26412;&#39640;&#26114;&#26102;&#65292;&#35299;&#20915;&#22522;&#20110;&#22810;&#29289;&#29702;&#30340;&#21453;&#38382;&#39064;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#30340;&#23398;&#20064;&#20195;&#29702;&#19982;&#23398;&#20064;&#32422;&#26463;&#30456;&#32467;&#21512;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#36825;&#31181;&#32452;&#21512;&#19981;&#20165;&#33021;&#22815;&#22823;&#22823;&#25913;&#21892;&#23545;&#37325;&#35201;&#27969;&#20307;&#27969;&#21160;&#24615;&#36136;&#65288;&#28183;&#36879;&#29575;&#65289;&#30340;&#21453;&#28436;&#65292;&#36824;&#33021;&#20026;&#21453;&#28436;&#22810;&#27169;&#24577;&#25968;&#25454;&#65288;&#21253;&#25324;&#20117;&#27979;&#37327;&#21644;&#20027;&#21160;&#28304;&#26102;&#21464;&#22320;&#38663;&#25968;&#25454;&#65289;&#25552;&#20379;&#19968;&#20010;&#33258;&#28982;&#30340;&#24179;&#21488;&#12290;&#36890;&#36807;&#28155;&#21152;&#23398;&#20064;&#32422;&#26463;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#35745;&#31639;&#21487;&#34892;&#30340;&#21453;&#28436;&#26041;&#27861;&#65292;&#20854;&#31934;&#24230;&#20173;&#28982;&#20934;&#30830;&#12290;&#36825;&#36890;&#36807;&#21253;&#21547;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;&#31216;&#20026;&#24402;&#19968;&#21270;&#27969;&#65289;&#65292;&#20351;&#27169;&#22411;&#36845;&#20195;&#20445;&#25345;&#22312;&#20998;&#24067;&#20869;&#65292;&#20174;&#32780;&#20445;&#35777;&#20102;&#20316;&#20026;&#20195;&#29702;&#30340;&#32463;&#36807;&#35757;&#32451;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#20123;&#31639;&#23376;&#29992;&#20110;&#20195;&#26367;&#28041;&#21450;&#37096;&#20998;&#35745;&#31639;&#26114;&#36149;&#30340;&#22810;&#30456;&#27969;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving multiphysics-based inverse problems for geological carbon storage monitoring can be challenging when multimodal time-lapse data are expensive to collect and costly to simulate numerically. We overcome these challenges by combining computationally cheap learned surrogates with learned constraints. Not only does this combination lead to vastly improved inversions for the important fluid-flow property, permeability, it also provides a natural platform for inverting multimodal data including well measurements and active-source time-lapse seismic data. By adding a learned constraint, we arrive at a computationally feasible inversion approach that remains accurate. This is accomplished by including a trained deep neural network, known as a normalizing flow, which forces the model iterates to remain in-distribution, thereby safeguarding the accuracy of trained Fourier neural operators that act as surrogates for the computationally expensive multiphase flow simulations involving partia
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26089;&#26399;&#24191;&#21578;&#25490;&#24207;&#65292;&#20197;&#35299;&#20915;&#26089;&#26399;&#38454;&#27573;&#21644;&#26368;&#32456;&#38454;&#27573;&#25490;&#24207;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.11096</link><description>&lt;p&gt;
&#20026;&#20102;&#26356;&#22909;&#30340;&#25490;&#24207;&#19968;&#33268;&#24615;&#65306;&#19968;&#31181;&#38754;&#21521;&#26089;&#26399;&#24191;&#21578;&#25490;&#24207;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Towards the Better Ranking Consistency: A Multi-task Learning Framework for Early Stage Ads Ranking. (arXiv:2307.11096v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26089;&#26399;&#24191;&#21578;&#25490;&#24207;&#65292;&#20197;&#35299;&#20915;&#26089;&#26399;&#38454;&#27573;&#21644;&#26368;&#32456;&#38454;&#27573;&#25490;&#24207;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#24191;&#21578;&#25512;&#33616;&#20013;&#65292;&#23558;&#24191;&#21578;&#25490;&#24207;&#31995;&#32479;&#20998;&#20026;&#26816;&#32034;&#12289;&#26089;&#26399;&#21644;&#26368;&#32456;&#38454;&#27573;&#26159;&#19968;&#31181;&#24120;&#35265;&#20570;&#27861;&#65292;&#20197;&#24179;&#34913;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#26089;&#26399;&#38454;&#27573;&#30340;&#25490;&#24207;&#36890;&#24120;&#20351;&#29992;&#39640;&#25928;&#27169;&#22411;&#20174;&#19968;&#32452;&#26816;&#32034;&#21040;&#30340;&#24191;&#21578;&#20013;&#29983;&#25104;&#20505;&#36873;&#38598;&#12290;&#28982;&#21518;&#65292;&#23558;&#20505;&#36873;&#38598;&#39304;&#36865;&#21040;&#35745;&#31639;&#23494;&#38598;&#19988;&#20934;&#30830;&#30340;&#26368;&#32456;&#38454;&#27573;&#25490;&#24207;&#31995;&#32479;&#65292;&#29983;&#25104;&#26368;&#32456;&#30340;&#24191;&#21578;&#25512;&#33616;&#12290;&#30001;&#20110;&#31995;&#32479;&#38480;&#21046;&#65292;&#26089;&#26399;&#21644;&#26368;&#32456;&#38454;&#27573;&#30340;&#25490;&#24207;&#20351;&#29992;&#19981;&#21516;&#30340;&#29305;&#24449;&#21644;&#27169;&#22411;&#26550;&#26500;&#65292;&#23548;&#33268;&#20102;&#20005;&#37325;&#30340;&#25490;&#24207;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21363;&#26089;&#26399;&#38454;&#27573;&#30340;&#24191;&#21578;&#21484;&#22238;&#29575;&#36739;&#20302;&#65292;&#21363;&#26368;&#32456;&#38454;&#27573;&#20013;&#25490;&#21517;&#38752;&#21069;&#30340;&#24191;&#21578;&#22312;&#26089;&#26399;&#38454;&#27573;&#25490;&#21517;&#36739;&#20302;&#12290;&#20026;&#20102;&#23558;&#26356;&#22909;&#30340;&#24191;&#21578;&#20174;&#26089;&#26399;&#38454;&#27573;&#20256;&#36882;&#21040;&#26368;&#32456;&#38454;&#27573;&#30340;&#25490;&#21517;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26089;&#26399;&#38454;&#27573;&#25490;&#24207;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25429;&#33719;&#22810;&#20010;&#26368;&#32456;&#38454;&#27573;&#25490;&#24207;&#32452;&#20214;&#65288;&#21363;&#24191;&#21578;&#28857;&#20987;&#21644;&#24191;&#21578;&#36136;&#37327;&#20107;&#20214;&#65289;&#21450;&#20854;&#20219;&#21153;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dividing ads ranking system into retrieval, early, and final stages is a common practice in large scale ads recommendation to balance the efficiency and accuracy. The early stage ranking often uses efficient models to generate candidates out of a set of retrieved ads. The candidates are then fed into a more computationally intensive but accurate final stage ranking system to produce the final ads recommendation. As the early and final stage ranking use different features and model architectures because of system constraints, a serious ranking consistency issue arises where the early stage has a low ads recall, i.e., top ads in the final stage are ranked low in the early stage. In order to pass better ads from the early to the final stage ranking, we propose a multi-task learning framework for early stage ranking to capture multiple final stage ranking components (i.e. ads clicks and ads quality events) and their task relations. With our multi-task learning framework, we can not only ac
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20840;&#25968;&#23383;&#23454;&#29616;&#30340;&#27169;&#22359;&#21270;DFR&#27169;&#22411;&#65292;&#36890;&#36807;&#20943;&#23569;&#36229;&#21442;&#25968;&#25968;&#37327;&#21644;&#36873;&#25321;&#38750;&#32447;&#24615;&#20989;&#25968;&#28789;&#27963;&#24615;&#65292;&#25552;&#39640;&#31934;&#24230;&#21644;&#38477;&#20302;&#21151;&#32791;&#12290;&#23545;&#27604;&#29616;&#26377;&#23454;&#29616;&#65292;&#35813;&#27169;&#22411;&#23454;&#29616;&#20102;10&#20493;&#30340;&#21151;&#32791;&#38477;&#20302;&#21644;5.3&#20493;&#30340;&#21534;&#21520;&#37327;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.11094</link><description>&lt;p&gt;
&#27169;&#22359;&#21270;DFR&#65306;&#29992;&#20110;&#22686;&#24378;&#35774;&#35745;&#28789;&#27963;&#24615;&#30340;&#25968;&#23383;&#24310;&#36831;&#21453;&#39304;&#20648;&#23618;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Modular DFR: Digital Delayed Feedback Reservoir Model for Enhancing Design Flexibility. (arXiv:2307.11094v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11094
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20840;&#25968;&#23383;&#23454;&#29616;&#30340;&#27169;&#22359;&#21270;DFR&#27169;&#22411;&#65292;&#36890;&#36807;&#20943;&#23569;&#36229;&#21442;&#25968;&#25968;&#37327;&#21644;&#36873;&#25321;&#38750;&#32447;&#24615;&#20989;&#25968;&#28789;&#27963;&#24615;&#65292;&#25552;&#39640;&#31934;&#24230;&#21644;&#38477;&#20302;&#21151;&#32791;&#12290;&#23545;&#27604;&#29616;&#26377;&#23454;&#29616;&#65292;&#35813;&#27169;&#22411;&#23454;&#29616;&#20102;10&#20493;&#30340;&#21151;&#32791;&#38477;&#20302;&#21644;5.3&#20493;&#30340;&#21534;&#21520;&#37327;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24310;&#36831;&#21453;&#39304;&#20648;&#23618;(DFR)&#26159;&#19968;&#31181;&#36866;&#21512;&#30828;&#20214;&#23454;&#29616;&#30340;&#20648;&#23618;&#35745;&#31639;&#31995;&#32479;&#65292;&#22240;&#20854;&#31616;&#21333;&#30340;&#32467;&#26500;&#32780;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#29616;&#26377;&#30340;DFR&#23454;&#29616;&#22823;&#22810;&#20351;&#29992;&#27169;&#25311;&#30005;&#36335;&#65292;&#38656;&#35201;&#25968;&#23383;&#27169;&#25311;&#36716;&#25442;&#22120;&#21644;&#27169;&#25311;&#25968;&#23383;&#36716;&#25442;&#22120;&#36827;&#34892;&#25509;&#21475;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#25968;&#23383;DFR&#22312;&#25968;&#23383;&#39046;&#22495;&#27169;&#25311;&#27169;&#25311;&#38750;&#32447;&#24615;&#20803;&#20214;&#65292;&#23548;&#33268;&#35774;&#35745;&#28789;&#27963;&#24615;&#30340;&#32570;&#22833;&#21644;&#26356;&#39640;&#30340;&#21151;&#32791;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#23436;&#20840;&#25968;&#23383;&#23454;&#29616;&#30340;&#26032;&#22411;&#27169;&#22359;&#21270;DFR&#27169;&#22411;&#12290;&#25152;&#25552;&#27169;&#22411;&#20943;&#23569;&#20102;&#36229;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#20801;&#35768;&#22312;&#36873;&#25321;&#38750;&#32447;&#24615;&#20989;&#25968;&#26102;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#31934;&#24230;&#21516;&#26102;&#38477;&#20302;&#21151;&#32791;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;DFR&#23454;&#29616;&#65292;&#23454;&#29616;&#20102;10&#20493;&#30340;&#21151;&#32791;&#38477;&#20302;&#21644;5.3&#20493;&#30340;&#21534;&#21520;&#37327;&#25552;&#21319;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#25110;&#26356;&#22909;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
A delayed feedback reservoir (DFR) is a type of reservoir computing system well-suited for hardware implementations owing to its simple structure. Most existing DFR implementations use analog circuits that require both digital-to-analog and analog-to-digital converters for interfacing. However, digital DFRs emulate analog nonlinear components in the digital domain, resulting in a lack of design flexibility and higher power consumption. In this paper, we propose a novel modular DFR model that is suitable for fully digital implementations. The proposed model reduces the number of hyperparameters and allows flexibility in the selection of the nonlinear function, which improves the accuracy while reducing the power consumption. We further present two DFR realizations with different nonlinear functions, achieving 10x power reduction and 5.3x throughput improvement while maintaining equal or better accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24615;&#33021;&#20272;&#35745;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#21442;&#25968;&#32622;&#20449;&#21306;&#38388;&#30340;&#23485;&#24230;&#19982;&#20998;&#21106;&#38382;&#39064;&#30340;&#29305;&#28857;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2307.10926</link><description>&lt;p&gt;
&#23545;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24615;&#33021;&#20272;&#35745;&#30340;&#32622;&#20449;&#21306;&#38388;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Confidence intervals for performance estimates in 3D medical image segmentation. (arXiv:2307.10926v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24615;&#33021;&#20272;&#35745;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#21442;&#25968;&#32622;&#20449;&#21306;&#38388;&#30340;&#23485;&#24230;&#19982;&#20998;&#21106;&#38382;&#39064;&#30340;&#29305;&#28857;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#20998;&#21106;&#27169;&#22411;&#30340;&#35780;&#20272;&#26159;&#22522;&#20110;&#26377;&#38480;&#30340;&#20363;&#22270;&#20687;&#65292;&#22240;&#27492;&#35780;&#20272;&#32467;&#26524;&#23384;&#22312;&#22122;&#22768;&#12290;&#38500;&#20102;&#25253;&#21578;&#24179;&#22343;&#24615;&#33021;&#25351;&#26631;&#22806;&#65292;&#25253;&#21578;&#32622;&#20449;&#21306;&#38388;&#20063;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#65292;&#24456;&#23569;&#26377;&#20154;&#36825;&#26679;&#20570;&#12290;&#32622;&#20449;&#21306;&#38388;&#30340;&#23485;&#24230;&#21462;&#20915;&#20110;&#27979;&#35797;&#38598;&#22823;&#23567;&#21644;&#24615;&#33021;&#25351;&#26631;&#30340;&#25955;&#24067;&#31243;&#24230;&#65288;&#21363;&#27979;&#35797;&#38598;&#19978;&#30340;&#26631;&#20934;&#24046;&#65289;&#12290;&#23545;&#20110;&#20998;&#31867;&#38382;&#39064;&#65292;&#38656;&#35201;&#35768;&#22810;&#27979;&#35797;&#22270;&#20687;&#20197;&#36991;&#20813;&#23485;&#27867;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20998;&#21106;&#38382;&#39064;&#65292;&#36825;&#20010;&#24773;&#20917;&#23578;&#26410;&#30740;&#31350;&#65292;&#22240;&#20026;&#32473;&#23450;&#30340;&#27979;&#35797;&#22270;&#20687;&#25152;&#25552;&#20379;&#30340;&#20449;&#24687;&#37327;&#19981;&#21516;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#20856;&#22411;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#30340;nnU-net&#26694;&#26550;&#22312;&#20004;&#20010;&#26469;&#33258;Medical Decathlon&#25361;&#25112;&#36187;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;3D&#22270;&#20687;&#20998;&#21106;&#30340;&#23454;&#39564;&#65292;&#24182;&#20351;&#29992;Dice&#20934;&#30830;&#24230;&#21644;Hausdorff&#36317;&#31163;&#20004;&#20010;&#24615;&#33021;&#25351;&#26631;&#12290;&#25105;&#20204;&#21457;&#29616;&#21442;&#25968;&#32622;&#20449;&#21306;&#38388;&#30340;&#23485;&#24230;&#19982;&#20998;&#21106;&#38382;&#39064;&#30340;&#29305;&#28857;&#26377;&#20851;&#65292;&#38656;&#35201;&#26356;&#22810;&#30340;&#30740;&#31350;&#25165;&#33021;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical segmentation models are evaluated empirically. As such an evaluation is based on a limited set of example images, it is unavoidably noisy. Beyond a mean performance measure, reporting confidence intervals is thus crucial. However, this is rarely done in medical image segmentation. The width of the confidence interval depends on the test set size and on the spread of the performance measure (its standard-deviation across of the test set). For classification, many test images are needed to avoid wide confidence intervals. Segmentation, however, has not been studied, and it differs by the amount of information brought by a given test image. In this paper, we study the typical confidence intervals in medical image segmentation. We carry experiments on 3D image segmentation using the standard nnU-net framework, two datasets from the Medical Decathlon challenge and two performance measures: the Dice accuracy and the Hausdorff distance. We show that the parametric confidence intervals
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#65292;&#24182;&#36890;&#36807;&#22312;&#39184;&#39302;&#35780;&#35770;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10617</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#26816;&#27979;&#34394;&#20551;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Detecting deceptive reviews using text classification. (arXiv:2307.10617v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10617
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#65292;&#24182;&#36890;&#36807;&#22312;&#39184;&#39302;&#35780;&#35770;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#32447;&#35780;&#35770;&#22312;&#25512;&#24191;&#20219;&#20309;&#20135;&#21697;&#25110;&#26381;&#21153;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20225;&#19994;&#21487;&#33021;&#20250;&#23884;&#20837;&#34394;&#20551;&#35780;&#35770;&#20197;&#21560;&#24341;&#23458;&#25143;&#36141;&#20080;&#20182;&#20204;&#30340;&#20135;&#21697;&#12290;&#20182;&#20204;&#29978;&#33267;&#21487;&#33021;&#31361;&#20986;&#24378;&#35843;&#33258;&#24049;&#20135;&#21697;&#30340;&#20248;&#28857;&#25110;&#25209;&#35780;&#31454;&#20105;&#23545;&#25163;&#30340;&#20135;&#21697;&#12290;&#24066;&#22330;&#33829;&#38144;&#20154;&#21592;&#12289;&#24191;&#21578;&#21830;&#21644;&#20854;&#20182;&#22312;&#32447;&#21830;&#19994;&#29992;&#25143;&#26377;&#21160;&#26426;&#20026;&#20182;&#20204;&#24819;&#35201;&#25512;&#24191;&#30340;&#20135;&#21697;&#32534;&#20889;&#34394;&#20551;&#30340;&#27491;&#38754;&#35780;&#35770;&#65292;&#25110;&#32773;&#20026;&#20182;&#20204;&#30495;&#27491;&#19981;&#21916;&#27426;&#30340;&#20135;&#21697;&#25552;&#20379;&#34394;&#20551;&#30340;&#36127;&#38754;&#35780;&#35770;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#26159;&#19968;&#20010;&#32039;&#36843;&#19988;&#25345;&#32493;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26041;&#27861;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#12290;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#19968;&#20010;&#39184;&#39302;&#35780;&#35770;&#30340;&#34394;&#20551;&#24847;&#35265;&#22403;&#22334;&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22810;&#27425;&#23454;&#39564;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;n-gram&#27169;&#22411;&#21644;&#26368;&#22823;&#29305;&#24449;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, online reviews play a vital role for promoting any kind of product or services. Businesses may embed fake reviews in order to attract customers to purchase their products. They may even highlight the benefits of their own product or criticize the competition's product. Marketers, advertisers, and other online business users have incentive to create fake positive reviews for products which they want to promote or give fake negative reviews for products which they really don't like. So now-a-days writing a deceptive review is inevitable thing for promoting their own business or degrading competitor's reputation. Thus, identifying deceptive reviews is an intense and on-going research area. This research paper proposes machine learning model approach to identify deceptive reviews. The paper investigates the performance of the several experiments done on a Deceptive Opinion Spam Corpus dataset of restaurants reviews. We developed a n-gram model and max features to identify 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#30446;&#26631;&#32852;&#37030;&#23398;&#20064;&#26469;&#35843;&#20248;SecureBoost&#36229;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#22312;&#25928;&#29992;&#12289;&#25928;&#29575;&#21644;&#38544;&#31169;&#20043;&#38388;&#26368;&#20339;&#24179;&#34913;&#30340;&#19968;&#32452;&#36229;&#21442;&#25968;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.10579</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#30446;&#26631;&#32852;&#37030;&#23398;&#20064;&#23545;SecureBoost&#36229;&#21442;&#25968;&#36827;&#34892;&#35843;&#20248;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SecureBoost Hyperparameter Tuning via Multi-Objective Federated Learning. (arXiv:2307.10579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10579
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#30446;&#26631;&#32852;&#37030;&#23398;&#20064;&#26469;&#35843;&#20248;SecureBoost&#36229;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#22312;&#25928;&#29992;&#12289;&#25928;&#29575;&#21644;&#38544;&#31169;&#20043;&#38388;&#26368;&#20339;&#24179;&#34913;&#30340;&#19968;&#32452;&#36229;&#21442;&#25968;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SecureBoost&#26159;&#19968;&#31181;&#21033;&#29992;&#21516;&#24577;&#21152;&#23494;&#20445;&#25252;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#38544;&#31169;&#30340;&#26641;&#25552;&#21319;&#31639;&#27861;&#12290;&#30001;&#20110;&#20854;&#21487;&#35299;&#37322;&#24615;&#12289;&#25928;&#26524;&#21644;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#65292;&#22312;&#37329;&#34701;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;SecureBoost&#23384;&#22312;&#35745;&#31639;&#22797;&#26434;&#24615;&#39640;&#21644;&#26631;&#31614;&#27844;&#28431;&#39118;&#38505;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;SecureBoost&#30340;&#28508;&#21147;&#65292;&#38656;&#35201;&#20180;&#32454;&#36873;&#25321;SecureBoost&#30340;&#36229;&#21442;&#25968;&#65292;&#20197;&#22312;&#25928;&#29992;&#12289;&#25928;&#29575;&#21644;&#38544;&#31169;&#20043;&#38388;&#36798;&#21040;&#26368;&#20339;&#24179;&#34913;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#32463;&#39564;&#24615;&#22320;&#35774;&#32622;&#36229;&#21442;&#25968;&#65292;&#35201;&#20040;&#21551;&#21457;&#24335;&#22320;&#35774;&#32622;&#36229;&#21442;&#25968;&#65292;&#36828;&#26410;&#36798;&#21040;&#26368;&#20248;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#22810;&#30446;&#26631;SecureBoost (CMOSB) &#31639;&#27861;&#65292;&#20197;&#23547;&#25214;&#27599;&#20010;&#35299;&#37117;&#26159;&#22312;&#25928;&#29992;&#25439;&#22833;&#12289;&#35757;&#32451;&#25104;&#26412;&#21644;&#38544;&#31169;&#27844;&#28431;&#20043;&#38388;&#23454;&#29616;&#26368;&#20339;&#26435;&#34913;&#30340;&#19968;&#32452;&#36229;&#21442;&#25968;&#30340;Pareto&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#30446;&#26631;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#38544;&#31169;&#27844;&#28431;&#26159;&#29992;... (&#27492;&#22788;&#30465;&#30053;)
&lt;/p&gt;
&lt;p&gt;
SecureBoost is a tree-boosting algorithm leveraging homomorphic encryption to protect data privacy in vertical federated learning setting. It is widely used in fields such as finance and healthcare due to its interpretability, effectiveness, and privacy-preserving capability. However, SecureBoost suffers from high computational complexity and risk of label leakage. To harness the full potential of SecureBoost, hyperparameters of SecureBoost should be carefully chosen to strike an optimal balance between utility, efficiency, and privacy. Existing methods either set hyperparameters empirically or heuristically, which are far from optimal. To fill this gap, we propose a Constrained Multi-Objective SecureBoost (CMOSB) algorithm to find Pareto optimal solutions that each solution is a set of hyperparameters achieving optimal tradeoff between utility loss, training cost, and privacy leakage. We design measurements of the three objectives. In particular, the privacy leakage is measured using 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#31454;&#20105;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21160;&#24577;&#25439;&#22833;&#20989;&#25968;&#35782;&#21035;&#19981;&#21516;&#30340;&#21151;&#33021;&#21306;&#22495;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25104;&#21151;&#22320;&#35299;&#20915;&#27169;&#22411;&#21457;&#29616;&#21644;&#20989;&#25968;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10496</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#19987;&#38376;&#27169;&#22411;&#30340;&#31454;&#20105;&#23398;&#20064;&#26041;&#27861;&#65306;&#35299;&#20915;&#20855;&#26377;&#19981;&#21516;&#21151;&#33021;&#21306;&#22495;&#30340;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Competitive Learning Approach for Specialized Models: A Solution for Complex Physical Systems with Distinct Functional Regimes. (arXiv:2307.10496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#31454;&#20105;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21160;&#24577;&#25439;&#22833;&#20989;&#25968;&#35782;&#21035;&#19981;&#21516;&#30340;&#21151;&#33021;&#21306;&#22495;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25104;&#21151;&#22320;&#35299;&#20915;&#27169;&#22411;&#21457;&#29616;&#21644;&#20989;&#25968;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#65292;&#22797;&#26434;&#31995;&#32479;&#26377;&#26102;&#20250;&#23637;&#29616;&#20986;&#22312;&#19981;&#21516;&#21306;&#22495;&#20043;&#38388;&#21464;&#21270;&#30340;&#34892;&#20026;&#12290;&#20256;&#32479;&#30340;&#20840;&#23616;&#27169;&#22411;&#24456;&#38590;&#25429;&#25417;&#21040;&#36825;&#31181;&#22797;&#26434;&#34892;&#20026;&#30340;&#20840;&#33539;&#22260;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#20934;&#30830;&#34920;&#31034;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31454;&#20105;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33719;&#21462;&#22522;&#20110;&#25968;&#25454;&#30340;&#29289;&#29702;&#31995;&#32479;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#21516;&#26102;&#23545;&#19968;&#32452;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#37319;&#29992;&#21160;&#24577;&#25439;&#22833;&#20989;&#25968;&#12290;&#27599;&#20010;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20105;&#22842;&#27599;&#20010;&#35266;&#23519;&#20540;&#65292;&#20174;&#32780;&#20801;&#35768;&#22312;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#20986;&#19981;&#21516;&#30340;&#21151;&#33021;&#21306;&#22495;&#12290;&#20026;&#20102;&#23637;&#31034;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#20351;&#29992;&#26799;&#24230;&#20248;&#21270;&#22120;&#36827;&#34892;&#35757;&#32451;&#30340;&#21508;&#31181;&#22238;&#24402;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#28041;&#21450;&#27169;&#22411;&#21457;&#29616;&#21644;&#20989;&#25968;&#25311;&#21512;&#30340;&#21508;&#31181;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#34920;&#26126;&#23427;&#33021;&#22815;&#25104;&#21151;&#22320;&#35782;&#21035;&#20986;&#21151;&#33021;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex systems in science and engineering sometimes exhibit behavior that changes across different regimes. Traditional global models struggle to capture the full range of this complex behavior, limiting their ability to accurately represent the system. In response to this challenge, we propose a novel competitive learning approach for obtaining data-driven models of physical systems. The primary idea behind the proposed approach is to employ dynamic loss functions for a set of models that are trained concurrently on the data. Each model competes for each observation during training, allowing for the identification of distinct functional regimes within the dataset. To demonstrate the effectiveness of the learning approach, we coupled it with various regression methods that employ gradient-based optimizers for training. The proposed approach was tested on various problems involving model discovery and function approximation, demonstrating its ability to successfully identify functional
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2307.10490</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#22768;&#38899;&#30340;&#28389;&#29992;&#29992;&#20110;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. (arXiv:2307.10490v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25552;&#31034;&#21644;&#25351;&#20196;&#27880;&#20837;&#12290;&#25915;&#20987;&#32773;&#29983;&#25104;&#19982;&#25552;&#31034;&#30456;&#23545;&#24212;&#30340;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#12290;&#24403;&#29992;&#25143;&#21521;&#65288;&#26410;&#20462;&#25913;&#30340;&#33391;&#24615;&#65289;&#27169;&#22411;&#35810;&#38382;&#34987;&#25200;&#21160;&#30340;&#22270;&#20687;&#25110;&#38899;&#39057;&#26102;&#65292;&#25200;&#21160;&#20250;&#24341;&#23548;&#27169;&#22411;&#36755;&#20986;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#25991;&#26412;&#21644;/&#25110;&#20351;&#21518;&#32493;&#23545;&#35805;&#36981;&#24490;&#25915;&#20987;&#32773;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#29992;&#20960;&#20010;&#27010;&#24565;&#39564;&#35777;&#31034;&#20363;&#38024;&#23545;LLaVa&#21644;PandaGPT&#26469;&#35828;&#26126;&#36825;&#31181;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.
&lt;/p&gt;</description></item><item><title>ZeroQuant-FP&#36890;&#36807;&#20351;&#29992;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;LLMs&#35757;&#32451;&#21518;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#30340;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;FP8&#28608;&#27963;&#20248;&#20110;INT8&#65292;&#24182;&#19988;FP4&#26435;&#37325;&#34920;&#29616;&#19982;INT4&#30456;&#24403;&#29978;&#33267;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2307.09782</link><description>&lt;p&gt;
ZeroQuant-FP: &#20351;&#29992;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;LLMs&#35757;&#32451;&#21518;&#37327;&#21270;&#30340;&#19968;&#39033;&#39134;&#36291;
&lt;/p&gt;
&lt;p&gt;
ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats. (arXiv:2307.09782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09782
&lt;/p&gt;
&lt;p&gt;
ZeroQuant-FP&#36890;&#36807;&#20351;&#29992;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;LLMs&#35757;&#32451;&#21518;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#30340;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;FP8&#28608;&#27963;&#20248;&#20110;INT8&#65292;&#24182;&#19988;FP4&#26435;&#37325;&#34920;&#29616;&#19982;INT4&#30456;&#24403;&#29978;&#33267;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22797;&#26434;&#39046;&#22495;&#20013;&#65292;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#35752;&#28014;&#28857;&#65288;FP&#65289;&#37327;&#21270;&#30340;&#21487;&#34892;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;FP8&#21644;FP4&#65292;&#20197;&#24212;&#23545;&#22343;&#21248;&#37327;&#21270;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#22788;&#29702;&#31163;&#32676;&#20540;&#65292;&#24182;&#21463;&#21040;NVIDIA H100&#30828;&#20214;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#35843;&#26597;&#21457;&#29616;&#65292;&#22312;LLMs&#20013;&#65292;FP8&#28608;&#27963;&#22987;&#32456;&#20248;&#20110;&#20854;&#25972;&#25968;&#65288;INT8&#65289;&#31561;&#25928;&#65292;&#24615;&#33021;&#20248;&#21183;&#22312;&#21253;&#21547;&#36229;&#36807;&#21313;&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#20013;&#26356;&#20026;&#26126;&#26174;&#12290;&#23545;&#20110;&#26435;&#37325;&#37327;&#21270;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;FP4&#30340;&#24615;&#33021;&#19982;INT4&#30456;&#24403;&#65292;&#29978;&#33267;&#26356;&#20248;&#65292;&#31616;&#21270;&#20102;&#22312;&#20687;H100&#36825;&#26679;&#25903;&#25345;FP&#30340;&#30828;&#20214;&#19978;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#20943;&#23569;&#30001;&#26435;&#37325;&#21644;&#28608;&#27963;&#20043;&#38388;&#24046;&#24322;&#24341;&#36215;&#30340;&#31934;&#24230;&#23545;&#40784;&#24320;&#38144;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#32553;&#25918;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the complex domain of large language models (LLMs), striking a balance between computational efficiency and maintaining model quality is a formidable challenge. Navigating the inherent limitations of uniform quantization, particularly when dealing with outliers, and motivated by the launch of NVIDIA's H100 hardware, this study delves into the viability of floating-point (FP) quantization, particularly focusing on FP8 and FP4, as a potential solution. Our comprehensive investigation reveals that for LLMs, FP8 activation consistently outshines its integer (INT8) equivalent, with the performance edge becoming more noticeable in models possessing parameters beyond one billion. For weight quantization, our findings indicate that FP4 exhibits comparable, if not superior, performance to INT4, simplifying deployment on FP-supported hardware like H100. To mitigate the overhead from precision alignment caused by the disparity between weights and activations, we propose two scaling constraints
&lt;/p&gt;</description></item><item><title>MolFM&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#20998;&#23376;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#20851;&#27880;&#23454;&#29616;&#20102;&#20998;&#23376;&#32467;&#26500;&#12289;&#25991;&#26412;&#21644;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#30340;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.09484</link><description>&lt;p&gt;
MolFM:&#19968;&#31181;&#22810;&#27169;&#24577;&#20998;&#23376;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MolFM: A Multimodal Molecular Foundation Model. (arXiv:2307.09484v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09484
&lt;/p&gt;
&lt;p&gt;
MolFM&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#20998;&#23376;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#20851;&#27880;&#23454;&#29616;&#20102;&#20998;&#23376;&#32467;&#26500;&#12289;&#25991;&#26412;&#21644;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#30340;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#30693;&#35782;&#23384;&#22312;&#20110;&#19977;&#31181;&#19981;&#21516;&#30340;&#20449;&#24687;&#26469;&#28304;&#27169;&#24335;&#20013;&#65306;&#20998;&#23376;&#32467;&#26500;&#12289;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#21644;&#30693;&#35782;&#24211;&#12290;&#26377;&#25928;&#25972;&#21512;&#26469;&#33258;&#36825;&#20123;&#27169;&#24577;&#30340;&#20998;&#23376;&#30693;&#35782;&#23545;&#20419;&#36827;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#22522;&#30784;&#27169;&#22411;&#22312;&#25429;&#25417;&#20998;&#23376;&#32467;&#26500;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#32852;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20204;&#20013;&#30340;&#20219;&#20309;&#19968;&#20010;&#37117;&#27809;&#26377;&#23581;&#35797;&#21033;&#29992;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#33719;&#24471;&#30340;&#20016;&#23500;&#20998;&#23376;&#19987;&#19994;&#30693;&#35782;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MolFM&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#20998;&#23376;&#22522;&#30784;&#27169;&#22411;&#65292;&#26088;&#22312;&#20419;&#36827;&#20174;&#20998;&#23376;&#32467;&#26500;&#12289;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#21644;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#23376;&#32467;&#26500;&#20013;&#30340;&#21407;&#23376;&#12289;&#20998;&#23376;&#23454;&#20307;&#30340;&#37051;&#23621;&#21644;&#35821;&#20041;&#30456;&#20851;&#25991;&#26412;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#20851;&#27880;&#65292;&#20197;&#20419;&#36827;&#36328;&#27169;&#24577;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;&#25429;&#25417;&#21040;&#20102;&#20998;&#23376;&#32467;&#26500;&#12289;&#25991;&#26412;&#21644;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular knowledge resides within three different modalities of information sources: molecular structures, biomedical documents, and knowledge bases. Effective incorporation of molecular knowledge from these modalities holds paramount significance in facilitating biomedical research. However, existing multimodal molecular foundation models exhibit limitations in capturing intricate connections between molecular structures and texts, and more importantly, none of them attempt to leverage a wealth of molecular expertise derived from knowledge graphs. In this study, we introduce MolFM, a multimodal molecular foundation model designed to facilitate joint representation learning from molecular structures, biomedical texts, and knowledge graphs. We propose cross-modal attention between atoms of molecular structures, neighbors of molecule entities and semantically related texts to facilitate cross-modal comprehension. We provide theoretical analysis that our cross-modal pre-training captures
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#30005;&#36335;&#35745;&#31639;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#21442;&#25968;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#36739;&#20302;&#30340;&#30005;&#36335;&#28145;&#24230;&#21644;&#36739;&#23569;&#30340;&#32534;&#35793;&#26102;&#38388;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#24635;&#20307;&#36816;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.08167</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#20010;&#30005;&#36335;&#35745;&#31639;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#21442;&#25968;&#30340;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Computing the gradients with respect to all parameters of a quantum neural network using a single circuit. (arXiv:2307.08167v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08167
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#30005;&#36335;&#35745;&#31639;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#21442;&#25968;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#36739;&#20302;&#30340;&#30005;&#36335;&#28145;&#24230;&#21644;&#36739;&#23569;&#30340;&#32534;&#35793;&#26102;&#38388;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#24635;&#20307;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#21442;&#25968;&#24179;&#31227;&#35268;&#21017;&#35745;&#31639;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#26102;&#65292;&#38656;&#35201;&#23545;&#32593;&#32476;&#30340;&#21333;&#20010;&#21487;&#35843;&#21442;&#25968;&#35745;&#31639;&#20004;&#27425;&#20195;&#20215;&#20989;&#25968;&#12290;&#24403;&#21442;&#25968;&#24635;&#25968;&#36739;&#39640;&#26102;&#65292;&#38656;&#35201;&#35843;&#25972;&#21644;&#36816;&#34892;&#22810;&#27425;&#29992;&#20110;&#35745;&#31639;&#30340;&#37327;&#23376;&#30005;&#36335;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#19968;&#20010;&#30005;&#36335;&#35745;&#31639;&#25152;&#26377;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#36739;&#20302;&#30340;&#30005;&#36335;&#28145;&#24230;&#21644;&#36739;&#23569;&#30340;&#32463;&#20856;&#23492;&#23384;&#22120;&#12290;&#25105;&#20204;&#36824;&#22312;&#30495;&#23454;&#37327;&#23376;&#30828;&#20214;&#21644;&#27169;&#25311;&#22120;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#30005;&#36335;&#32534;&#35793;&#26102;&#38388;&#26126;&#26174;&#32553;&#30701;&#30340;&#20248;&#21183;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#24635;&#20307;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
When computing the gradients of a quantum neural network using the parameter-shift rule, the cost function needs to be calculated twice for the gradient with respect to a single adjustable parameter of the network. When the total number of parameters is high, the quantum circuit for the computation has to be adjusted and run for many times. Here we propose an approach to compute all the gradients using a single circuit only, with a much reduced circuit depth and less classical registers. We also demonstrate experimentally, on both real quantum hardware and simulator, that our approach has the advantages that the circuit takes a significantly shorter time to compile than the conventional approach, resulting in a speedup on the total runtime.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#26426;&#36741;&#21161;&#20998;&#26512;&#25216;&#26415;&#65292;&#35777;&#26126;&#20102;&#38750;&#24120;&#25968;&#27493;&#38271;&#31574;&#30053;&#19979;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#32463;&#36807;&#38271;&#36317;&#31163;&#27493;&#39588;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.06324</link><description>&lt;p&gt;
&#32463;&#36807;&#38271;&#36317;&#31163;&#27493;&#39588;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#21487;&#35777;&#26126;&#26356;&#24555;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Provably Faster Gradient Descent via Long Steps. (arXiv:2307.06324v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#26426;&#36741;&#21161;&#20998;&#26512;&#25216;&#26415;&#65292;&#35777;&#26126;&#20102;&#38750;&#24120;&#25968;&#27493;&#38271;&#31574;&#30053;&#19979;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#32463;&#36807;&#38271;&#36317;&#31163;&#27493;&#39588;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#26426;&#36741;&#21161;&#20998;&#26512;&#25216;&#26415;&#65292;&#24314;&#31435;&#20102;&#32463;&#36807;&#38271;&#36317;&#31163;&#27493;&#39588;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#21487;&#35777;&#26126;&#26356;&#24555;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20801;&#35768;&#38750;&#24120;&#25968;&#27493;&#38271;&#31574;&#30053;&#65292;&#36890;&#36807;&#20998;&#26512;&#22810;&#27425;&#36845;&#20195;&#30340;&#25972;&#20307;&#25928;&#26524;&#32780;&#19981;&#26159;&#20856;&#22411;&#30340;&#19968;&#27425;&#36845;&#20195;&#24402;&#32435;&#20351;&#29992;&#30340;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#30772;&#22351;&#19979;&#38477;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#38271;&#36317;&#31163;&#27493;&#39588;&#65292;&#21487;&#33021;&#22312;&#30701;&#26399;&#20869;&#22686;&#21152;&#30446;&#26631;&#20540;&#65292;&#20294;&#22312;&#38271;&#26399;&#20869;&#24102;&#26469;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#26799;&#24230;&#19979;&#38477;&#26356;&#24555;&#25910;&#25947;&#36895;&#24230;&#30340;&#29468;&#24819;&#65292;&#24182;&#36827;&#34892;&#20102;&#31616;&#21333;&#30340;&#25968;&#20540;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work establishes provably faster convergence rates for gradient descent via a computer-assisted analysis technique. Our theory allows nonconstant stepsize policies with frequent long steps potentially violating descent by analyzing the overall effect of many iterations at once rather than the typical one-iteration inductions used in most first-order method analyses. We show that long steps, which may increase the objective value in the short term, lead to provably faster convergence in the long term. A conjecture towards proving a faster $O(1/T\log T)$ rate for gradient descent is also motivated along with simple numerical validation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.06092</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23450;&#37327;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#20854;&#20013;&#38544;&#34255;&#23618;&#23485;&#24230;&#19982;&#22823;&#24120;&#25968; $n$ &#25104;&#27604;&#20363;&#12290;&#22312;&#38750;&#32447;&#24615;&#30340;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#23450;&#29702;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#26377;&#38480;&#32500;&#20998;&#24067;&#36824;&#26159;&#25972;&#20010;&#36807;&#31243;&#65292;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#65288;&#21450;&#20854;&#23548;&#25968;&#65289;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#37117;&#20250;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#20854;&#20013; $\gamma&gt;0$&#65292;&#25351;&#25968;&#21462;&#20915;&#20110;&#29992;&#20110;&#24230;&#37327;&#24046;&#24322;&#30340;&#24230;&#37327;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#27604;&#25991;&#29486;&#20013;&#20197;&#21069;&#25552;&#20379;&#30340;&#20219;&#20309;&#30028;&#38480;&#37117;&#35201;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant $n$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite $n$ and any fixed network depth. Our theorems show, both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like $n^{-\gamma}$ for $\gamma&gt;0,$ with the exponent depending on the metric used to measure discrepancy. Our bounds are stronger in terms of their dependence on network width than any previously available in the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#32039;&#31995;&#25968;&#26679;&#26465;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#27169;&#24335;&#30340;&#25968;&#37327;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26680;&#20272;&#35745;&#22120;&#21644;&#32452;&#21512;&#26679;&#26465;&#65292;&#23454;&#29616;&#20102;&#29305;&#24449;&#25506;&#32034;&#12289;&#27169;&#22411;&#36873;&#25321;&#21644;&#27169;&#24335;&#26816;&#39564;&#65292;&#24182;&#20801;&#35768;&#24341;&#20837;&#19987;&#23478;&#21028;&#26029;&#12290;&#36890;&#36807;&#22312;&#20307;&#32946;&#20998;&#26512;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#30340;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05825</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#32039;&#31995;&#25968;&#26679;&#26465;&#20272;&#35745;&#27169;&#24335;&#30340;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
Bayesian taut splines for estimating the number of modes. (arXiv:2307.05825v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#32039;&#31995;&#25968;&#26679;&#26465;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#27169;&#24335;&#30340;&#25968;&#37327;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26680;&#20272;&#35745;&#22120;&#21644;&#32452;&#21512;&#26679;&#26465;&#65292;&#23454;&#29616;&#20102;&#29305;&#24449;&#25506;&#32034;&#12289;&#27169;&#22411;&#36873;&#25321;&#21644;&#27169;&#24335;&#26816;&#39564;&#65292;&#24182;&#20801;&#35768;&#24341;&#20837;&#19987;&#23478;&#21028;&#26029;&#12290;&#36890;&#36807;&#22312;&#20307;&#32946;&#20998;&#26512;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#30340;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#27169;&#24335;&#30340;&#25968;&#37327;&#20195;&#34920;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#20063;&#21487;&#20197;&#30475;&#20316;&#29616;&#26377;&#20122;&#32676;&#20307;&#30340;&#25968;&#37327;&#12290;&#23613;&#31649;&#20854;&#30456;&#20851;&#24615;&#65292;&#23545;&#20854;&#20272;&#35745;&#30340;&#30740;&#31350;&#38750;&#24120;&#26377;&#38480;&#12290;&#25105;&#20204;&#38024;&#23545;&#21333;&#21464;&#37327;&#24773;&#20917;&#25552;&#20986;&#19968;&#20010;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#33268;&#21147;&#20110;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#21463;&#21040;&#20102;&#38382;&#39064;&#30340;&#19968;&#20123;&#34987;&#24573;&#35270;&#30340;&#26041;&#38754;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#35748;&#20026;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#32467;&#26500;&#65292;&#27169;&#24335;&#30340;&#20027;&#35266;&#19988;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#21450;&#34701;&#21512;&#20840;&#23616;&#21644;&#23616;&#37096;&#23494;&#24230;&#29305;&#24615;&#30340;&#25972;&#20307;&#35270;&#22270;&#30340;&#20415;&#21033;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#28789;&#27963;&#30340;&#26680;&#20272;&#35745;&#22120;&#21644;&#31616;&#27905;&#30340;&#32452;&#21512;&#26679;&#26465;&#12290;&#29305;&#24449;&#25506;&#32034;&#12289;&#27169;&#22411;&#36873;&#25321;&#21644;&#27169;&#24335;&#26816;&#39564;&#37117;&#22312;&#36125;&#21494;&#26031;&#25512;&#29702;&#33539;&#24335;&#20013;&#23454;&#29616;&#65292;&#20026;&#36719;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#24182;&#20801;&#35768;&#22312;&#36807;&#31243;&#20013;&#24341;&#20837;&#19987;&#23478;&#21028;&#26029;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#30340;&#23454;&#29992;&#24615;&#36890;&#36807;&#22312;&#20307;&#32946;&#20998;&#26512;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#20010;&#38506;&#20276;&#30340;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The number of modes in a probability density function is representative of the model's complexity and can also be viewed as the number of existing subpopulations. Despite its relevance, little research has been devoted to its estimation. Focusing on the univariate setting, we propose a novel approach targeting prediction accuracy inspired by some overlooked aspects of the problem. We argue for the need for structure in the solutions, the subjective and uncertain nature of modes, and the convenience of a holistic view blending global and local density properties. Our method builds upon a combination of flexible kernel estimators and parsimonious compositional splines. Feature exploration, model selection and mode testing are implemented in the Bayesian inference paradigm, providing soft solutions and allowing to incorporate expert judgement in the process. The usefulness of our proposal is illustrated through a case study in sports analytics, showcasing multiple companion visualisation 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#32593;&#32476;SegNetr&#65292;&#36890;&#36807;&#24341;&#20837;SegNetr&#22359;&#21644;&#20449;&#24687;&#20445;&#30041;&#36339;&#36291;&#36830;&#25509;&#23454;&#29616;&#20102;&#21160;&#24577;&#30340;&#23616;&#37096;-&#20840;&#23616;&#20132;&#20114;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#29305;&#24449;&#30340;&#31934;&#30830;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2307.02953</link><description>&lt;p&gt;
SegNetr&#65306;&#37325;&#26032;&#24605;&#32771;U&#22411;&#32593;&#32476;&#20013;&#30340;&#23616;&#37096;-&#20840;&#23616;&#20132;&#20114;&#21644;&#36339;&#36291;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
SegNetr: Rethinking the local-global interactions and skip connections in U-shaped networks. (arXiv:2307.02953v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02953
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#32593;&#32476;SegNetr&#65292;&#36890;&#36807;&#24341;&#20837;SegNetr&#22359;&#21644;&#20449;&#24687;&#20445;&#30041;&#36339;&#36291;&#36830;&#25509;&#23454;&#29616;&#20102;&#21160;&#24577;&#30340;&#23616;&#37096;-&#20840;&#23616;&#20132;&#20114;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#29305;&#24449;&#30340;&#31934;&#30830;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#31616;&#21333;&#19988;&#26131;&#20110;&#35843;&#25972;&#30340;&#32467;&#26500;&#65292;U&#22411;&#32593;&#32476;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;U&#22411;&#20998;&#21106;&#32593;&#32476;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;&#20027;&#35201;&#20851;&#27880;&#35774;&#35745;&#22797;&#26434;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20197;&#24357;&#34917;&#22522;&#20110;&#21367;&#31215;&#25805;&#20316;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#30340;&#32570;&#22833;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#32593;&#32476;&#30340;&#24635;&#21442;&#25968;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#65307;2&#65289;&#31616;&#21333;&#22320;&#34701;&#21512;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#29305;&#24449;&#65292;&#24573;&#30053;&#20102;&#23427;&#20204;&#31354;&#38388;&#20301;&#32622;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#20102;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#32593;&#32476;&#65292;&#31216;&#20026;SegNetr&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SegNetr&#22359;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;&#38454;&#27573;&#21160;&#24577;&#22320;&#36827;&#34892;&#23616;&#37096;-&#20840;&#23616;&#20132;&#20114;&#65292;&#24182;&#19988;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20449;&#24687;&#20445;&#30041;&#36339;&#36291;&#36830;&#25509;&#65288;IRSC&#65289;&#65292;&#20197;&#20445;&#30041;&#32534;&#30721;&#22120;&#29305;&#24449;&#30340;&#31354;&#38388;&#20301;&#32622;&#20449;&#24687;&#65292;&#24182;&#19982;&#35299;&#30721;&#22120;&#29305;&#24449;&#23454;&#29616;&#31934;&#30830;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, U-shaped networks have dominated the field of medical image segmentation due to their simple and easily tuned structure. However, existing U-shaped segmentation networks: 1) mostly focus on designing complex self-attention modules to compensate for the lack of long-term dependence based on convolution operation, which increases the overall number of parameters and computational complexity of the network; 2) simply fuse the features of encoder and decoder, ignoring the connection between their spatial locations. In this paper, we rethink the above problem and build a lightweight medical image segmentation network, called SegNetr. Specifically, we introduce a novel SegNetr block that can perform local-global interactions dynamically at any stage and with only linear complexity. At the same time, we design a general information retention skip connection (IRSC) to preserve the spatial location information of encoder features and achieve accurate fusion with the decoder features. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;2023&#24180;&#22768;&#38899;&#20998;&#31163;&#25361;&#25112;&#36187;&#38899;&#20048;&#20998;&#31163;&#36187;&#36947;&#30340;&#20004;&#20010;&#26377;&#25928;&#26041;&#27861;&#65292;&#20998;&#21035;&#26159;&#26102;&#25928;&#39640;&#30340;&#28304;&#20998;&#31163;&#32593;&#32476;&#21644;&#36866;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#24615;&#28304;&#20998;&#31163;&#30340;&#25439;&#22833;&#25513;&#27169;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09382</link><description>&lt;p&gt;
2023&#24180;&#22768;&#38899;&#20998;&#31163;&#25361;&#25112;&#36187;&#8212;&#8212;&#38899;&#20048;&#20998;&#31163;&#36187;&#36947;&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Sound Demixing Challenge 2023 -- Music Demixing Track Technical Report. (arXiv:2306.09382v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;2023&#24180;&#22768;&#38899;&#20998;&#31163;&#25361;&#25112;&#36187;&#38899;&#20048;&#20998;&#31163;&#36187;&#36947;&#30340;&#20004;&#20010;&#26377;&#25928;&#26041;&#27861;&#65292;&#20998;&#21035;&#26159;&#26102;&#25928;&#39640;&#30340;&#28304;&#20998;&#31163;&#32593;&#32476;&#21644;&#36866;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#24615;&#28304;&#20998;&#31163;&#30340;&#25439;&#22833;&#25513;&#27169;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;2023&#24180;&#22768;&#38899;&#20998;&#31163;&#25361;&#25112;&#36187;&#38899;&#20048;&#20998;&#31163;&#36187;&#36947;&#20013;&#33719;&#22870;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#31181;&#22312;&#27492;&#25361;&#25112;&#20013;&#35774;&#35745;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#26102;&#25928;&#39640;&#30340;&#28304;&#20998;&#31163;&#32593;&#32476;&#65292;&#22312;MUSDB&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65307;&#19968;&#31181;&#36866;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#24615;&#28304;&#20998;&#31163;&#30340;&#25439;&#22833;&#25513;&#27169;&#26041;&#27861;&#12290;&#22312;github.com/kuielab/sdx23&#19978;&#25552;&#20379;&#20102;&#27169;&#22411;&#35757;&#32451;&#21644;&#26368;&#32456;&#25552;&#20132;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this report, we present our award-winning solutions for the Music Demixing Track of Sound Demixing Challenge 2023. We focus on two methods designed for this challenge: a time-efficient source separation network that achieves state-of-the-art results on the MUSDB benchmark and a loss masking method for noise-robust source separation. Code for reproducing model training and final submissions is available at github.com/kuielab/sdx23.
&lt;/p&gt;</description></item><item><title>IsoEx&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20107;&#20214;&#26085;&#24535;&#30340;&#32593;&#32476;&#23433;&#20840;&#35843;&#26597;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#21629;&#20196;&#34892;&#30340;&#26085;&#24535;&#32467;&#26500;&#21644;&#29238;/&#23376;&#20851;&#31995;&#26469;&#26816;&#27979;&#24322;&#24120;&#21644;&#28508;&#22312;&#38382;&#39064;&#21629;&#20196;&#34892;&#65292;&#20934;&#30830;&#24615;&#39640;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09260</link><description>&lt;p&gt;
IsoEx:&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#29992;&#20110;&#22788;&#29702;&#20107;&#20214;&#26085;&#24535;&#30340;&#32593;&#32476;&#23433;&#20840;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
IsoEx: an explainable unsupervised approach to process event logs cyber investigation. (arXiv:2306.09260v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09260
&lt;/p&gt;
&lt;p&gt;
IsoEx&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20107;&#20214;&#26085;&#24535;&#30340;&#32593;&#32476;&#23433;&#20840;&#35843;&#26597;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#21629;&#20196;&#34892;&#30340;&#26085;&#24535;&#32467;&#26500;&#21644;&#29238;/&#23376;&#20851;&#31995;&#26469;&#26816;&#27979;&#24322;&#24120;&#21644;&#28508;&#22312;&#38382;&#39064;&#21629;&#20196;&#34892;&#65292;&#20934;&#30830;&#24615;&#39640;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
39&#31186;&#12290;&#36825;&#26159;&#33258;2023&#24180;&#20197;&#26469;&#36830;&#32493;&#20004;&#27425;&#32593;&#32476;&#25915;&#20987;&#20043;&#38388;&#30340;&#26102;&#38388;&#38388;&#38548;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#24744;&#38405;&#35835;&#23436;&#27492;&#25688;&#35201;&#20043;&#21069;&#65292;&#19990;&#30028;&#19978;&#21487;&#33021;&#24050;&#32463;&#21457;&#29983;&#20102;1&#21040;2&#27425;&#39069;&#22806;&#30340;&#32593;&#32476;&#25915;&#20987;&#12290;&#22312;&#32593;&#32476;&#23041;&#32961;&#39057;&#29575;&#22823;&#24133;&#22686;&#21152;&#30340;&#32972;&#26223;&#19979;&#65292;&#23433;&#20840;&#36816;&#33829;&#20013;&#24515;(SOC)&#21644;&#35745;&#31639;&#26426;&#24212;&#24613;&#21709;&#24212;&#22242;&#38431;(CERT)&#21487;&#33021;&#20250;&#19981;&#22570;&#37325;&#36127;&#12290;&#20026;&#20102;&#20943;&#36731;&#32593;&#32476;&#23433;&#20840;&#22242;&#38431;&#22312;&#35843;&#26597;&#24037;&#20316;&#20013;&#30340;&#36127;&#25285;&#65292;&#24182;&#24110;&#21161;&#20182;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#26356;&#22686;&#20540;&#30340;&#20219;&#21153;&#19978;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#25216;&#26415;&#24320;&#22987;&#20986;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;IsoEx&#65292;&#29992;&#20110;&#22312;&#27745;&#26579;&#35774;&#22791;&#35843;&#26597;&#26399;&#38388;&#26816;&#27979;&#24322;&#24120;&#21644;&#28508;&#22312;&#38382;&#39064;&#21629;&#20196;&#34892;&#12290;IsoEx&#22260;&#32469;&#19968;&#32452;&#29305;&#24449;&#26500;&#24314;&#65292;&#36825;&#20123;&#29305;&#24449;&#21033;&#29992;&#21629;&#20196;&#34892;&#30340;&#26085;&#24535;&#32467;&#26500;&#20197;&#21450;&#20854;&#29238;/&#23376;&#20851;&#31995;&#65292;&#20197;&#23454;&#29616;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#26816;&#27979;&#24322;&#24120;&#65292;IsoEx&#37319;&#29992;&#20102;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
39 seconds. That is the timelapse between two consecutive cyber attacks as of 2023. Meaning that by the time you are done reading this abstract, about 1 or 2 additional cyber attacks would have occurred somewhere in the world. In this context of highly increased frequency of cyber threats, Security Operation Centers (SOC) and Computer Emergency Response Teams (CERT) can be overwhelmed. In order to relieve the cybersecurity teams in their investigative effort and help them focus on more added-value tasks, machine learning approaches and methods started to emerge. This paper introduces a novel method, IsoEx, for detecting anomalous and potentially problematic command lines during the investigation of contaminated devices. IsoEx is built around a set of features that leverages the log structure of the command line, as well as its parent/child relationship, to achieve a greater accuracy than traditional methods. To detect anomalies, IsoEx resorts to an unsupervised anomaly detection techni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#22312;&#39640;&#32500;&#35774;&#35745;&#31354;&#38388;&#20013;&#21516;&#26102;&#20248;&#21270;&#20102;&#24322;&#27493;&#30005;&#26426;&#21644;&#27704;&#30913;&#21516;&#27493;&#30005;&#26426;&#20004;&#31181;&#19981;&#21516;&#30340;&#30005;&#26426;&#25216;&#26415;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#35299;&#30721;&#22120;&#20316;&#20026;&#20803;&#27169;&#22411;&#65292;&#39044;&#27979;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#24182;&#29983;&#25104;&#26032;&#30340;&#35774;&#35745;&#12290;&#19982;&#32463;&#20856;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30452;&#25509;&#26041;&#27861;&#30456;&#27604;&#65292;VAE&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09087</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#30446;&#26631;&#30005;&#26426;&#25216;&#26415;&#20248;&#21270;&#30340;&#20803;&#27169;&#22411;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Deep learning based Meta-modeling for Multi-objective Technology Optimization of Electrical Machines. (arXiv:2306.09087v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#22312;&#39640;&#32500;&#35774;&#35745;&#31354;&#38388;&#20013;&#21516;&#26102;&#20248;&#21270;&#20102;&#24322;&#27493;&#30005;&#26426;&#21644;&#27704;&#30913;&#21516;&#27493;&#30005;&#26426;&#20004;&#31181;&#19981;&#21516;&#30340;&#30005;&#26426;&#25216;&#26415;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#35299;&#30721;&#22120;&#20316;&#20026;&#20803;&#27169;&#22411;&#65292;&#39044;&#27979;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#24182;&#29983;&#25104;&#26032;&#30340;&#35774;&#35745;&#12290;&#19982;&#32463;&#20856;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30452;&#25509;&#26041;&#27861;&#30456;&#27604;&#65292;VAE&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26059;&#36716;&#30005;&#26426;&#30340;&#20248;&#21270;&#26082;&#32791;&#26102;&#21448;&#38656;&#35201;&#35745;&#31639;&#36164;&#28304;&#12290;&#30001;&#20110;&#21442;&#25968;&#21270;&#19981;&#21516;&#65292;&#35774;&#35745;&#20248;&#21270;&#36890;&#24120;&#23545;&#27599;&#31181;&#30005;&#26426;&#25216;&#26415;&#20998;&#21035;&#25191;&#34892;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#21516;&#26102;&#20248;&#21270;&#20004;&#31181;&#19981;&#21516;&#30340;&#30005;&#26426;&#25216;&#26415;&#65292;&#21363;&#24322;&#27493;&#30005;&#26426;&#21644;&#27704;&#30913;&#21516;&#27493;&#30005;&#26426;&#12290;&#22312;&#35757;&#32451;&#20043;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#35299;&#30721;&#22120;&#20316;&#20026;&#20803;&#27169;&#22411;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#28508;&#22312;&#31354;&#38388;&#22312;&#20248;&#21270;&#24490;&#29615;&#20013;&#39044;&#27979;&#20840;&#23616;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#65288;KPI&#65289;&#24182;&#29983;&#25104;&#30456;&#20851;&#26032;&#35774;&#35745;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#20102;&#22312;&#39640;&#32500;&#35774;&#35745;&#31354;&#38388;&#20013;&#30340;&#24182;&#34892;&#21442;&#25968;&#21270;&#22810;&#30446;&#26631;&#25216;&#26415;&#20248;&#21270;&#12290;VAE&#26041;&#27861;&#22312;KPI&#39044;&#27979;&#19978;&#19982;&#32463;&#20856;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30452;&#25509;&#26041;&#27861;&#36827;&#34892;&#20102;&#23450;&#37327;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization of rotating electrical machines is both time- and computationally expensive. Because of the different parametrization, design optimization is commonly executed separately for each machine technology. In this paper, we present the application of a variational auto-encoder (VAE) to optimize two different machine technologies simultaneously, namely an asynchronous machine and a permanent magnet synchronous machine. After training, we employ a deep neural network and a decoder as meta-models to predict global key performance indicators (KPIs) and generate associated new designs, respectively, through unified latent space in the optimization loop. Numerical results demonstrate concurrent parametric multi-objective technology optimization in the high-dimensional design space. The VAE-based approach is quantitatively compared to a classical deep learning-based direct approach for KPIs prediction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#39640;&#20809;&#35889;&#22270;&#20687;&#20462;&#22797;&#31639;&#27861;LRS-PnP-DIP&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#39640;&#20809;&#35889;&#22270;&#20687;&#20013;&#31934;&#30830;&#39044;&#27979;&#32570;&#22833;&#20687;&#32032;&#21644;&#24102;&#65292;&#20854;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#36798;&#21040;&#25110;&#36229;&#36807;&#20102;&#20854;&#20182;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.07308</link><description>&lt;p&gt;
&#22522;&#20110;&#20248;&#21270;&#21551;&#21457;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#30417;&#30563;&#39640;&#20809;&#35889;&#22270;&#20687;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Hyperspectral Inpainting with the Optimisation inspired Deep Neural Network Prior. (arXiv:2306.07308v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#39640;&#20809;&#35889;&#22270;&#20687;&#20462;&#22797;&#31639;&#27861;LRS-PnP-DIP&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#39640;&#20809;&#35889;&#22270;&#20687;&#20013;&#31934;&#30830;&#39044;&#27979;&#32570;&#22833;&#20687;&#32032;&#21644;&#24102;&#65292;&#20854;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#36798;&#21040;&#25110;&#36229;&#36807;&#20102;&#20854;&#20182;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20809;&#35889;&#22270;&#20687;&#20855;&#26377;&#25104;&#30334;&#19978;&#21315;&#20010;&#31364;&#24102;&#35889;&#27573;&#65292;&#20256;&#36882;&#20102;&#22823;&#37327;&#30340;&#31354;&#38388;&#21644;&#35889;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20202;&#22120;&#35823;&#24046;&#21644;&#22823;&#27668;&#21464;&#21270;&#65292;&#23454;&#36341;&#20013;&#24471;&#21040;&#30340;&#39640;&#20809;&#35889;&#22270;&#20687;&#24120;&#24120;&#34987;&#22122;&#22768;&#21644;&#22351;&#28857;&#27745;&#26579;&#65292;&#23548;&#33268;&#32570;&#22833;&#20449;&#24687;&#21487;&#33021;&#20005;&#37325;&#30772;&#22351;&#21518;&#32493;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#20809;&#35889;&#22270;&#20687;&#21462;&#26679;&#28857;&#20462;&#22797;&#31639;&#27861;&#65292;&#31216;&#20026;&#20302;&#31209;&#31232;&#30095;&#32422;&#26463;&#25554;&#20837;&#25773;&#25918;&#31639;&#27861;&#65288;LRS-PnP&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22270;&#20687;&#30340;&#25152;&#26377;&#20809;&#35889;&#24102;&#37117;&#20002;&#22833;&#65292;LRS-PnP&#20063;&#33021;&#22815;&#39044;&#27979;&#32570;&#22833;&#30340;&#20687;&#32032;&#21644;&#24102;&#12290;&#23558;LRS-PnP&#19982;Deep Image Prior&#65288;DIP&#65289;&#30456;&#32467;&#21512;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#27169;&#22411;&#65292;&#31216;&#20026;LRS-PnP-DIP&#12290;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;LRS-PnP-DIP&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#20462;&#22797;&#24615;&#33021;&#25110;&#32988;&#36807;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperspectral Image (HSI)s cover hundreds or thousands of narrow spectral bands, conveying a wealth of spatial and spectral information. However, due to the instrumental errors and the atmospheric changes, the HSI obtained in practice are often contaminated by noise and dead pixels(lines), resulting in missing information that may severely compromise the subsequent applications. We introduce here a novel HSI missing pixel prediction algorithm, called Low Rank and Sparsity Constraint Plug-and-Play (LRS-PnP). It is shown that LRS-PnP is able to predict missing pixels and bands even when all spectral bands of the image are missing. The proposed LRS-PnP algorithm is further extended to a self-supervised model by combining the LRS-PnP with the Deep Image Prior (DIP), called LRS-PnP-DIP. In a series of experiments with real data, It is shown that the LRS-PnP-DIP either achieves state-of-the-art inpainting performance compared to other learning-based methods, or outperforms them.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#32622;&#25442;&#19981;&#21464;&#30340;&#39640;&#32500;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21518;&#23558;&#20854;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#25490;&#38500;&#24322;&#24120;&#30340;&#21943;&#27880;&#12290;</title><link>http://arxiv.org/abs/2306.03933</link><description>&lt;p&gt;
&#39640;&#32500;&#21644;&#32622;&#25442;&#19981;&#21464;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional and Permutation Invariant Anomaly Detection. (arXiv:2306.03933v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03933
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#32622;&#25442;&#19981;&#21464;&#30340;&#39640;&#32500;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21518;&#23558;&#20854;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#25490;&#38500;&#24322;&#24120;&#30340;&#21943;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23398;&#20064;&#39640;&#32500;&#27010;&#29575;&#23494;&#24230;&#30340;&#22256;&#38590;&#65292;&#26032;&#29289;&#29702;&#36807;&#31243;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#23616;&#38480;&#20110;&#20302;&#32500;&#31354;&#38388;&#12290;&#29305;&#21035;&#26159;&#22312;&#25104;&#20998;&#32423;&#21035;&#19978;&#65292;&#23558;&#32622;&#25442;&#19981;&#21464;&#24615;&#21644;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#31561;&#33391;&#22909;&#24615;&#36136;&#21512;&#24182;&#21040;&#27969;&#34892;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#20013;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31890;&#23376;&#29289;&#29702;&#25968;&#25454;&#32622;&#25442;&#19981;&#21464;&#23494;&#24230;&#20272;&#35745;&#22120;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#23398;&#20064;&#21040;&#30340;&#23494;&#24230;&#29992;&#20316;&#32622;&#25442;&#19981;&#21464;&#30340;&#24322;&#24120;&#26816;&#27979;&#35780;&#20998;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#21151;&#25928;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#30340;&#21487;&#33021;&#24615;&#36739;&#20302;&#30340;&#21943;&#27880;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#21040;&#30340;&#23494;&#24230;&#27604;&#19982;&#34987;&#30417;&#30563;&#20998;&#31867;&#31639;&#27861;&#33719;&#24471;&#30340;&#23494;&#24230;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for anomaly detection of new physics processes are often limited to low-dimensional spaces due to the difficulty of learning high-dimensional probability densities. Particularly at the constituent level, incorporating desirable properties such as permutation invariance and variable-length inputs becomes difficult within popular density estimation methods. In this work, we introduce a permutation-invariant density estimator for particle physics data based on diffusion models, specifically designed to handle variable-length inputs. We demonstrate the efficacy of our methodology by utilizing the learned density as a permutation-invariant anomaly detection score, effectively identifying jets with low likelihood under the background-only hypothesis. To validate our density estimation method, we investigate the ratio of learned densities and compare to those obtained by a supervised classification algorithm.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#26041;&#24046;&#35268;&#33539;&#21270;&#25216;&#26415;&#26041;&#27861;&#65292;&#20943;&#23567;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#38480;&#37319;&#26679;&#22122;&#22768;&#65292;&#22312;QNN&#30340;&#26500;&#36896;&#22949;&#21892;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#38656;&#39069;&#22806;&#30005;&#36335;&#35745;&#31639;&#65292;&#27979;&#35797;&#21457;&#29616;&#21487;&#20197;&#26174;&#33879;&#22320;&#38477;&#20302;&#22122;&#22768;&#27700;&#24179;&#21450;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.01639</link><description>&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26377;&#38480;&#37319;&#26679;&#22122;&#22768;&#38477;&#20302;
&lt;/p&gt;
&lt;p&gt;
Reduction of finite sampling noise in quantum neural networks. (arXiv:2306.01639v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01639
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#26041;&#24046;&#35268;&#33539;&#21270;&#25216;&#26415;&#26041;&#27861;&#65292;&#20943;&#23567;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#38480;&#37319;&#26679;&#22122;&#22768;&#65292;&#22312;QNN&#30340;&#26500;&#36896;&#22949;&#21892;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#38656;&#39069;&#22806;&#30005;&#36335;&#35745;&#31639;&#65292;&#27979;&#35797;&#21457;&#29616;&#21487;&#20197;&#26174;&#33879;&#22320;&#38477;&#20302;&#22122;&#22768;&#27700;&#24179;&#21450;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;(QNNs)&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#19982;&#25968;&#25454;&#30456;&#20851;&#30340;&#36755;&#20837;&#26469;&#29983;&#25104;&#36755;&#20986;, &#36890;&#36807;&#35745;&#31639;&#26399;&#26395;&#20540;&#24102;&#26469;&#20102;&#22522;&#26412;&#30340;&#26377;&#38480;&#37319;&#26679;&#22122;&#22768;&#65292;&#21363;&#20351;&#22312;&#26080;&#35823;&#24046;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#20063;&#20250;&#20986;&#29616;&#27492;&#29616;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#20171;&#32461;&#26041;&#24046;&#35268;&#33539;&#21270;&#25216;&#26415;&#26469;&#20943;&#23569;&#36825;&#31181;&#22122;&#22768;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#20943;&#23567;&#37327;&#23376;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#26399;&#26395;&#20540;&#30340;&#26041;&#24046;&#12290;&#22914;&#26524;QNN&#24050;&#32463;&#22949;&#21892;&#26500;&#36896;&#65292;&#21017;&#27492;&#25216;&#26415;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#30005;&#36335;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#38477;&#20302;&#26041;&#24046;&#21487;&#20197;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;&#65292;&#38477;&#20302;&#36755;&#20986;&#22122;&#22768;&#65292;&#20943;&#23569;&#26799;&#24230;&#30005;&#36335;&#35780;&#20272;&#20013;&#30340;&#27979;&#37327;&#27425;&#25968;&#12290;&#25105;&#20204;&#23545;&#22810;&#39033;&#24335;&#20989;&#25968;&#22238;&#24402;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#25105;&#20204;&#30340;&#31034;&#20363;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#35268;&#33539;&#21270;&#26041;&#27861;&#24179;&#22343;&#21487;&#20197;&#38477;&#20302;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#26041;&#24046;&#65292;&#20174;&#32780;&#26174;&#30528;&#38477;&#20302;&#20102;&#22122;&#22768;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum neural networks (QNNs) use parameterized quantum circuits with data-dependent inputs and generate outputs through the evaluation of expectation values. Calculating these expectation values necessitates repeated circuit evaluations, thus introducing fundamental finite-sampling noise even on error-free quantum computers. We reduce this noise by introducing the variance regularization, a technique for reducing the variance of the expectation value during the quantum model training. This technique requires no additional circuit evaluations if the QNN is properly constructed. Our empirical findings demonstrate the reduced variance speeds up the training and lowers the output noise as well as decreases the number of measurements in the gradient circuit evaluation. This regularization method is benchmarked on the regression of multiple functions. We show that in our examples, it lowers the variance by an order of magnitude on average and leads to a significantly reduced noise level of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#33145;&#37096;&#22810;&#22120;&#23448;&#21644;&#32959;&#30244;&#20998;&#21106;&#25552;&#20986;&#20102;&#19968;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21019;&#26032;&#26550;&#26500;&#65292;&#36890;&#36807;&#26367;&#25442;&#36755;&#20986;&#23618;&#20026;&#19968;&#32452;&#36731;&#37327;&#32423;&#30340;&#12289;&#31867;&#21035;&#29305;&#23450;&#30340;&#22836;&#37096;&#26469;&#36866;&#24212;&#26032;&#20986;&#29616;&#30340;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2306.00988</link><description>&lt;p&gt;
&#33145;&#37096;&#22810;&#22120;&#23448;&#21644;&#32959;&#30244;&#20998;&#21106;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning for Abdominal Multi-Organ and Tumor Segmentation. (arXiv:2306.00988v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#33145;&#37096;&#22810;&#22120;&#23448;&#21644;&#32959;&#30244;&#20998;&#21106;&#25552;&#20986;&#20102;&#19968;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21019;&#26032;&#26550;&#26500;&#65292;&#36890;&#36807;&#26367;&#25442;&#36755;&#20986;&#23618;&#20026;&#19968;&#32452;&#36731;&#37327;&#32423;&#30340;&#12289;&#31867;&#21035;&#29305;&#23450;&#30340;&#22836;&#37096;&#26469;&#36866;&#24212;&#26032;&#20986;&#29616;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22810;&#22120;&#23448;&#21644;&#32959;&#30244;&#20998;&#21106;&#65292;&#21160;&#24577;&#25193;&#23637;&#27169;&#22411;&#20197;&#36866;&#24212;&#26032;&#25968;&#25454;&#21644;&#31867;&#21035;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#35268;&#23450;&#65292;&#35775;&#38382;&#20808;&#21069;&#30340;&#25968;&#25454;&#21644;&#27880;&#37322;&#22312;&#21307;&#23398;&#39046;&#22495;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#12290;&#36825;&#23545;&#20110;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#26102;&#20445;&#25345;&#26087;&#31867;&#21035;&#39640;&#20998;&#21106;&#31934;&#24230;&#30340;&#38382;&#39064;&#36896;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#65292;&#22240;&#20026;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#20808;&#32463;&#39564;&#35777;&#26126;&#65292;&#22312;&#22120;&#23448;&#20998;&#21106;&#35774;&#32622;&#20013;&#65292;&#20165;&#20351;&#29992;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#21487;&#20197;&#30456;&#23545;&#32531;&#35299;&#35813;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#36830;&#32493;&#22120;&#23448;&#21644;&#32959;&#30244;&#20998;&#21106;&#30340;&#21019;&#26032;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#30340;&#35745;&#31639;&#36127;&#25285;&#26368;&#23567;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#26041;&#26696;&#26159;&#23558;&#20256;&#32479;&#36755;&#20986;&#23618;&#26367;&#25442;&#20026;&#19968;&#32452;&#36731;&#37327;&#32423;&#30340;&#12289;&#31867;&#21035;&#29305;&#23450;&#30340;&#22836;&#37096;&#65292;&#22240;&#27492;&#20855;&#22791;&#36866;&#24212;&#26032;&#20986;&#29616;&#31867;&#21035;&#30340;&#28789;&#27963;&#24615;&#12290;&#36825;&#20123;&#22836;&#37096;&#21487;&#20197;&#29420;&#31435;&#39044;&#27979;&#26032;&#24341;&#20837;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to dynamically extend a model to new data and classes is critical for multiple organ and tumor segmentation. However, due to privacy regulations, accessing previous data and annotations can be problematic in the medical domain. This poses a significant barrier to preserving the high segmentation accuracy of the old classes when learning from new classes because of the catastrophic forgetting problem. In this paper, we first empirically demonstrate that simply using high-quality pseudo labels can fairly mitigate this problem in the setting of organ segmentation. Furthermore, we put forward an innovative architecture designed specifically for continuous organ and tumor segmentation, which incurs minimal computational overhead. Our proposed design involves replacing the conventional output layer with a suite of lightweight, class-specific heads, thereby offering the flexibility to accommodate newly emerging classes. These heads enable independent predictions for newly introduc
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Med-DDPM&#65292;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21270;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#25511;&#21046;&#20687;&#32032;&#32423;&#25513;&#30721;&#26631;&#31614;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#30340;&#21307;&#23398;&#22270;&#20687;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;GAN&#25216;&#26415;&#65292;&#20063;&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#25216;&#26415;&#21644;GAN&#21512;&#25104;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2305.18453</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#21270;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Conditional Diffusion Models for Semantic 3D Medical Image Synthesis. (arXiv:2305.18453v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18453
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Med-DDPM&#65292;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21270;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#25511;&#21046;&#20687;&#32032;&#32423;&#25513;&#30721;&#26631;&#31614;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#30340;&#21307;&#23398;&#22270;&#20687;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;GAN&#25216;&#26415;&#65292;&#20063;&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#25216;&#26415;&#21644;GAN&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Med-DDPM&#65292;&#23427;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21270;&#30340;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#65292;&#35299;&#20915;&#20102;&#21307;&#23398;&#25104;&#20687;&#20013;&#25968;&#25454;&#31232;&#32570;&#12289;&#37319;&#38598;&#26041;&#27861;&#19981;&#19968;&#33268;&#21644;&#38544;&#31169;&#38382;&#39064;&#31561;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#37117;&#36229;&#36807;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#12290;Med-DDPM&#30340;&#29420;&#29305;&#29305;&#28857;&#22312;&#20110;&#20351;&#29992;&#35821;&#20041;&#26465;&#20214;&#36827;&#34892;&#19977;&#32500;&#22270;&#20687;&#21512;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#25511;&#21046;&#20687;&#32032;&#32423;&#25513;&#30721;&#26631;&#31614;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#23427;&#20415;&#20110;&#21019;&#24314;&#36924;&#30495;&#30340;&#21307;&#23398;&#22270;&#20687;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;Med-DDPM&#22312;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;GAN&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;Med-DDPM&#22312;&#22686;&#24378;&#20998;&#21106;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#20063;&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#25216;&#26415;&#21644;GAN&#21512;&#25104;&#22270;&#20687;&#12290;&#23427;&#35299;&#20915;&#20102;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#38590;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Med-DDPM, an innovative solution using diffusion models for semantic 3D medical image synthesis, addressing the prevalent issues in medical imaging such as data scarcity, inconsistent acquisition methods, and privacy concerns. Experimental evidence illustrates that diffusion models surpass Generative Adversarial Networks (GANs) in stability and performance, generating high-quality, realistic 3D medical images. The distinct feature of Med-DDPM is its use of semantic conditioning for the diffusion model in 3D image synthesis. By controlling the generation process through pixel-level mask labels, it facilitates the creation of realistic medical images. Empirical evaluations underscore the superior performance of Med-DDPM over GAN techniques in metrics such as accuracy, stability, and versatility. Furthermore, Med-DDPM outperforms traditional augmentation techniques and synthetic GAN images in enhancing the accuracy of segmentation models. It addresses challenges such
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#24615;&#24378;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;CMRL&#65292;&#36890;&#36807;&#26816;&#27979;&#19982;&#21270;&#23398;&#21453;&#24212;&#22240;&#26524;&#30456;&#20851;&#30340;&#26680;&#24515;&#20122;&#32467;&#26500;&#26469;&#24212;&#23545;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.18451</link><description>&lt;p&gt;
&#20855;&#26377;&#22240;&#26524;&#20122;&#32467;&#26500;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;&#22312;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26102;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Shift-Robust Molecular Relational Learning with Causal Substructure. (arXiv:2305.18451v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#24615;&#24378;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;CMRL&#65292;&#36890;&#36807;&#26816;&#27979;&#19982;&#21270;&#23398;&#21453;&#24212;&#22240;&#26524;&#30456;&#20851;&#30340;&#26680;&#24515;&#20122;&#32467;&#26500;&#26469;&#24212;&#23545;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#24341;&#36215;&#20102;&#20998;&#23376;&#31185;&#23398;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#20854;&#30446;&#26631;&#26159;&#39044;&#27979;&#20998;&#23376;&#23545;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#24615;&#24378;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;CMRL&#65292;&#23427;&#36890;&#36807;&#26816;&#27979;&#19982;&#21270;&#23398;&#21453;&#24212;&#22240;&#26524;&#30456;&#20851;&#30340;&#26680;&#24515;&#20122;&#32467;&#26500;&#26469;&#24212;&#23545;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20551;&#23450;&#22522;&#20110;&#20998;&#23376;&#31185;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#26500;&#24314;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#26469;&#25581;&#31034;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22522;&#20110;SCM&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26465;&#20214;&#24178;&#39044;&#26694;&#26550;&#65292;&#20854;&#24178;&#39044;&#26159;&#22522;&#20110;&#25104;&#23545;&#20998;&#23376;&#26465;&#20214;&#30340;&#12290;&#20351;&#29992;&#26465;&#20214;&#24178;&#39044;&#26694;&#26550;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#22320;&#20174;&#22240;&#26524;&#20122;&#32467;&#26500;&#20013;&#23398;&#20064;&#65292;&#24182;&#20943;&#36731;&#20102;&#19982;&#21270;&#23398;&#21453;&#24212;&#34394;&#20551;&#30456;&#20851;&#30340;&#24555;&#25463;&#20122;&#32467;&#26500;&#30340;&#28151;&#28102;&#25928;&#24212;&#12290;&#26412;&#25991;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, molecular relational learning, whose goal is to predict the interaction behavior between molecular pairs, got a surge of interest in molecular sciences due to its wide range of applications. In this work, we propose CMRL that is robust to the distributional shift in molecular relational learning by detecting the core substructure that is causally related to chemical reactions. To do so, we first assume a causal relationship based on the domain knowledge of molecular sciences and construct a structural causal model (SCM) that reveals the relationship between variables. Based on the SCM, we introduce a novel conditional intervention framework whose intervention is conditioned on the paired molecule. With the conditional intervention framework, our model successfully learns from the causal substructure and alleviates the confounding effect of shortcut substructures that are spuriously correlated to chemical reactions. Extensive experiments on various tasks with real-world and sy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;k&#36817;&#37051;&#23398;&#20064;&#35268;&#21017;&#20013;&#30340;&#26222;&#36941;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#22312;&#21487;&#20998;&#24230;&#37327;&#31354;&#38388;&#20013;&#65292;&#35813;&#35268;&#21017;&#22312;Nagata&#32500;&#24230;&#19979;&#30340;sigma&#26377;&#38480;&#32500;&#24230;&#30340;&#31354;&#38388;&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#22312;&#38750;&#38463;&#22522;&#31859;&#24503;&#24230;&#37327;&#31354;&#38388;&#20013;&#26159;&#24378;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#27492;&#35268;&#21017;&#22312;&#20855;&#26377;de Groot&#26377;&#38480;&#32500;&#24230;&#24847;&#20041;&#19979;&#30340;&#24230;&#37327;&#31354;&#38388;&#21644;Heisenberg&#32676;&#20013;&#20063;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.17282</link><description>&lt;p&gt;
&#24230;&#37327;&#31354;&#38388;&#21644;Nagata&#32500;&#24230;&#20013;k-NN&#35268;&#21017;&#30340;&#26222;&#36941;&#19968;&#33268;&#24615;(II)
&lt;/p&gt;
&lt;p&gt;
Universal consistency of the $k$-NN rule in metric spaces and Nagata dimension. II. (arXiv:2305.17282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;k&#36817;&#37051;&#23398;&#20064;&#35268;&#21017;&#20013;&#30340;&#26222;&#36941;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#22312;&#21487;&#20998;&#24230;&#37327;&#31354;&#38388;&#20013;&#65292;&#35813;&#35268;&#21017;&#22312;Nagata&#32500;&#24230;&#19979;&#30340;sigma&#26377;&#38480;&#32500;&#24230;&#30340;&#31354;&#38388;&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#22312;&#38750;&#38463;&#22522;&#31859;&#24503;&#24230;&#37327;&#31354;&#38388;&#20013;&#26159;&#24378;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#27492;&#35268;&#21017;&#22312;&#20855;&#26377;de Groot&#26377;&#38480;&#32500;&#24230;&#24847;&#20041;&#19979;&#30340;&#24230;&#37327;&#31354;&#38388;&#21644;Heisenberg&#32676;&#20013;&#20063;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32487;&#32493;&#22312;&#21487;&#20998;&#24230;&#37327;&#31354;&#38388;&#20013;&#30740;&#31350;k&#36817;&#37051;&#23398;&#20064;&#35268;&#21017;&#12290;&#30001;&#20110;C\'erou&#21644;Guyader(2006)&#20197;&#21450;Preiss(1983)&#30340;&#32467;&#26524;&#65292;&#24050;&#30693;&#35813;&#35268;&#21017;&#22312;&#27599;&#20010;Nagata&#24847;&#20041;&#19979;&#30340;sigma&#26377;&#38480;&#32500;&#24230;&#30340;&#24230;&#37327;&#31354;&#38388;X&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26080;&#24179;&#23616;&#24773;&#20917;&#19979;&#27492;&#35268;&#21017;&#22312;&#36825;&#26679;&#30340;&#31354;&#38388;&#20013;&#26159;&#24378;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;&#22312;Devroye&#65292;Gy\"{o}rfi&#65292;Krzy\.{z}ak&#21644;Lugosi&#65288;1994&#65289;&#22312;&#27431;&#20960;&#37324;&#24471;&#35774;&#32622;&#20013;&#24212;&#29992;&#30340;&#25171;&#30772;&#24179;&#23616;&#31574;&#30053;&#19979;&#65292;&#25105;&#20204;&#35774;&#27861;&#22312;&#38750;&#38463;&#22522;&#31859;&#24503;&#24230;&#37327;&#31354;&#38388;&#65288;&#21363;Nagata&#32500;&#24230;&#20026;&#38646;&#30340;&#31354;&#38388;&#65289;&#20013;&#23637;&#31034;&#20102;&#24378;&#26222;&#36941;&#19968;&#33268;&#24615;&#12290;&#32467;&#21512;C\'erou&#21644;Guyader&#30340;&#23450;&#29702;&#21644;Assouad&#21644;Quentin de Gromard (2006)&#30340;&#32467;&#26524;&#65292;&#21487;&#20197;&#25512;&#20986;$k$-NN&#35268;&#21017;&#22312;&#20855;&#26377;de Groot&#26377;&#38480;&#32500;&#24230;&#24847;&#20041;&#19979;&#30340;&#24230;&#37327;&#31354;&#38388;&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;$k$-NN&#35268;&#21017;&#22312;Heisenberg&#32676;&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#32780;&#35813;&#32676;&#24182;&#38750;sigma&#26377;&#38480;&#32500;&#24230;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We continue to investigate the $k$ nearest neighbour learning rule in separable metric spaces. Thanks to the results of C\'erou and Guyader (2006) and Preiss (1983), this rule is known to be universally consistent in every metric space $X$ that is sigma-finite dimensional in the sense of Nagata. Here we show that the rule is strongly universally consistent in such spaces in the absence of ties. Under the tie-breaking strategy applied by Devroye, Gy\"{o}rfi, Krzy\.{z}ak, and Lugosi (1994) in the Euclidean setting, we manage to show the strong universal consistency in non-Archimedian metric spaces (that is, those of Nagata dimension zero). Combining the theorem of C\'erou and Guyader with results of Assouad and Quentin de Gromard (2006), one deduces that the $k$-NN rule is universally consistent in metric spaces having finite dimension in the sense of de Groot. In particular, the $k$-NN rule is universally consistent in the Heisenberg group which is not sigma-finite dimensional in the se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21363;MADD&#65292;&#21487;&#20197;&#29420;&#31435;&#20110;&#39044;&#27979;&#24615;&#33021;&#20998;&#26512;&#27169;&#22411;&#30340;&#27495;&#35270;&#34892;&#20026;&#12290;&#30740;&#31350;&#32773;&#36824;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#20998;&#26512;&#30340;&#34917;&#20805;&#26469;&#24110;&#21161;&#36827;&#34892;&#20154;&#31867;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.15342</link><description>&lt;p&gt;
&#20320;&#30340;&#27169;&#22411;&#8220;MADD&#8221;&#20102;&#21527;&#65311;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#39044;&#27979;&#24615;&#23398;&#29983;&#27169;&#22411;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#26032;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is Your Model "MADD"? A Novel Metric to Evaluate Algorithmic Fairness for Predictive Student Models. (arXiv:2305.15342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21363;MADD&#65292;&#21487;&#20197;&#29420;&#31435;&#20110;&#39044;&#27979;&#24615;&#33021;&#20998;&#26512;&#27169;&#22411;&#30340;&#27495;&#35270;&#34892;&#20026;&#12290;&#30740;&#31350;&#32773;&#36824;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#20998;&#26512;&#30340;&#34917;&#20805;&#26469;&#24110;&#21161;&#36827;&#34892;&#20154;&#31867;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22686;&#24378;&#25945;&#32946;&#25104;&#26524;&#21644;&#25903;&#25345;&#21033;&#30410;&#30456;&#20851;&#32773;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#30340;&#33021;&#21147;&#65292;&#39044;&#27979;&#24615;&#23398;&#29983;&#27169;&#22411;&#22312;&#23398;&#20064;&#29615;&#22659;&#20013;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#65292;&#23548;&#33268;&#23545;&#26576;&#20123;&#23398;&#29983;&#30340;&#28508;&#22312;&#27495;&#35270;&#21644;&#21487;&#33021;&#30340;&#26377;&#23475;&#38271;&#26399;&#24433;&#21709;&#12290;&#36825;&#20419;&#20351;&#20102;&#23545;&#20844;&#24179;&#24615;&#24230;&#37327;&#26631;&#20934;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#25429;&#25417;&#21644;&#37327;&#21270;&#27492;&#31867;&#20559;&#35265;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30446;&#21069;&#22312;&#25945;&#32946;&#39046;&#22495;&#20351;&#29992;&#30340;&#29616;&#26377;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#26159;&#38754;&#21521;&#39044;&#27979;&#24615;&#33021;&#30340;&#65292;&#37325;&#28857;&#26159;&#35780;&#20272;&#32452;&#38388;&#23384;&#22312;&#30340;&#26377;&#20559;&#32467;&#26524;&#65292;&#32780;&#19981;&#32771;&#34385;&#27169;&#22411;&#30340;&#34892;&#20026;&#20197;&#21450;&#32467;&#26524;&#20013;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21363;&#8220;&#27169;&#22411;&#32477;&#23545;&#23494;&#24230;&#36317;&#31163;&#8221;&#65288;MADD&#65289;&#65292;&#20197;&#20998;&#26512;&#27169;&#22411;&#30340;&#27495;&#35270;&#34892;&#20026;&#65292;&#29420;&#31435;&#20110;&#20854;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22522;&#20110;&#21487;&#35270;&#21270;&#20998;&#26512;&#30340;&#34917;&#20805;&#65292;&#20197;&#23454;&#29616;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#32454;&#31890;&#24230;&#20154;&#31867;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive student models are increasingly used in learning environments due to their ability to enhance educational outcomes and support stakeholders in making informed decisions. However, predictive models can be biased and produce unfair outcomes, leading to potential discrimination against some students and possible harmful long-term implications. This has prompted research on fairness metrics meant to capture and quantify such biases. Nonetheless, so far, existing fairness metrics used in education are predictive performance-oriented, focusing on assessing biased outcomes across groups of students, without considering the behaviors of the models nor the severity of the biases in the outcomes. Therefore, we propose a novel metric, the Model Absolute Density Distance (MADD), to analyze models' discriminatory behaviors independently from their predictive performance. We also provide a complementary visualization-based analysis to enable fine-grained human assessment of how the models
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MA-FL&#65292;&#24212;&#29992;&#24322;&#27493;&#27169;&#22411;&#20256;&#36755;&#20307;&#31995;&#32467;&#26500;&#26469;&#23454;&#29616;&#26377;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#38656;&#35201;&#35757;&#32451;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#26412;&#25991;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#25581;&#31034;&#20102;&#36164;&#28304;&#20998;&#37197;&#12289;&#35774;&#22791;&#35843;&#24230;&#21644;&#20010;&#20307;&#27169;&#22411;&#29366;&#24577;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;MA-FL&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#27169;&#22411;&#31934;&#24230;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13503</link><description>&lt;p&gt;
&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#24322;&#27493;&#22810;&#27169;&#22411;&#32852;&#37030;&#23398;&#20064;&#65306;&#29702;&#35770;&#12289;&#24314;&#27169;&#19982;&#20248;&#21270;(arXiv:2305.13503v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Asynchronous Multi-Model Federated Learning over Wireless Networks: Theory, Modeling, and Optimization. (arXiv:2305.13503v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MA-FL&#65292;&#24212;&#29992;&#24322;&#27493;&#27169;&#22411;&#20256;&#36755;&#20307;&#31995;&#32467;&#26500;&#26469;&#23454;&#29616;&#26377;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#38656;&#35201;&#35757;&#32451;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#26412;&#25991;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#25581;&#31034;&#20102;&#36164;&#28304;&#20998;&#37197;&#12289;&#35774;&#22791;&#35843;&#24230;&#21644;&#20010;&#20307;&#27169;&#22411;&#29366;&#24577;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;MA-FL&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#27169;&#22411;&#31934;&#24230;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#30446;&#21069;&#65292;&#32852;&#37030;&#23398;&#20064;&#30340;&#22823;&#37096;&#20998;&#25991;&#29486;&#37117;&#20851;&#27880;&#21333;&#19968;&#20219;&#21153;/&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#37319;&#29992;&#21516;&#27493;&#27169;&#22411;&#21442;&#25968;&#20256;&#36755;&#35774;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;MA-FL&#65292;&#23427;&#32771;&#34385;&#21033;&#29992;&#24322;&#27493;&#27169;&#22411;&#20256;&#36755;&#20307;&#31995;&#32467;&#26500;&#65292;&#23454;&#29616;&#26377;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#38656;&#35201;&#35757;&#32451;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#24341;&#20837;&#19968;&#26063;&#35843;&#24230;&#24352;&#37327;&#26469;&#25429;&#25417;&#35774;&#22791;&#30340;&#35843;&#24230;&#65292;&#24182;&#23545;MA-FL&#19979;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#25105;&#20204;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#25581;&#31034;&#20102;&#36164;&#28304;&#20998;&#37197;&#65288;&#20363;&#22914;&#65292;&#23567;&#25209;&#37327;&#22823;&#23567;&#21644;&#26799;&#24230;&#19979;&#38477;&#36845;&#20195;&#27425;&#25968;&#65289;&#12289;&#35774;&#22791;&#35843;&#24230;&#21644;&#20010;&#20307;&#27169;&#22411;&#29366;&#24577;&#65288;&#21363;&#39044;&#28909;&#19982;&#20919;&#21551;&#21160;&#21021;&#22987;&#21270;&#65289;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20026;MA-FL&#21046;&#23450;&#20102;&#19968;&#20010;&#38750;&#20984;&#28151;&#25972;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#29992;&#20110;&#20849;&#21516;&#37197;&#32622;&#36164;&#28304;&#20998;&#37197;&#21644;&#35774;&#22791;&#35843;&#24230;&#12290;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;MA-FL&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#27169;&#22411;&#31934;&#24230;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has emerged as a key technique for distributed machine learning (ML). Most literature on FL has focused on systems with (i) ML model training for a single task/model, (ii) a synchronous setting for uplink/downlink transfer of model parameters, which is often unrealistic. To address this, we develop MA-FL, which considers FL with multiple downstream tasks to be trained over an asynchronous model transmission architecture. We first characterize the convergence of ML model training under MA-FL via introducing a family of scheduling tensors to capture the scheduling of devices. Our convergence analysis sheds light on the impact of resource allocation (e.g., the mini-batch size and number of gradient descent iterations), device scheduling, and individual model states (i.e., warmed vs. cold initialization) on the performance of ML models. We then formulate a non-convex mixed integer optimization problem for jointly configuring the resource allocation and device schedu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;LACE&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#27010;&#24565;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#20132;&#20114;&#26041;&#24335;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#26426;&#21046;&#65292;&#39564;&#35777;&#20102;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#20013;&#35813;&#27169;&#22411;&#30340;&#25512;&#33616;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04250</link><description>&lt;p&gt;
&#21487;&#32534;&#36753;&#29992;&#25143;&#26723;&#26696;&#30340;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Editable User Profiles for Controllable Text Recommendation. (arXiv:2304.04250v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;LACE&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#27010;&#24565;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#20132;&#20114;&#26041;&#24335;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#26426;&#21046;&#65292;&#39564;&#35777;&#20102;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#20013;&#35813;&#27169;&#22411;&#30340;&#25512;&#33616;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#39640;&#36136;&#37327;&#25512;&#33616;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20174;&#20132;&#20114;&#25968;&#25454;&#20013;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#25552;&#20379;&#32473;&#29992;&#25143;&#25511;&#21046;&#25152;&#25509;&#25910;&#30340;&#25512;&#33616;&#30340;&#26426;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LACE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;LACE&#22522;&#20110;&#29992;&#25143;&#20132;&#20114;&#30340;&#25991;&#26723;&#26816;&#32034;&#65292;&#23558;&#27599;&#20010;&#29992;&#25143;&#34920;&#31034;&#20026;&#31616;&#27905;&#30340;&#21487;&#35835;&#30340;&#27010;&#24565;&#38598;&#65292;&#24182;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#27010;&#24565;&#30340;&#20010;&#24615;&#21270;&#34920;&#31034;&#12290;&#35813;&#22522;&#20110;&#27010;&#24565;&#30340;&#29992;&#25143;&#26723;&#26696;&#34987;&#21033;&#29992;&#26469;&#20570;&#20986;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#35774;&#35745;&#36890;&#36807;&#36879;&#26126;&#30340;&#29992;&#25143;&#26723;&#26696;&#65292;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#22810;&#31181;&#30452;&#35266;&#20132;&#20114;&#26041;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19977;&#20010;&#25512;&#33616;&#20219;&#21153;&#65288;&#28201;&#21551;&#21160;&#12289;&#20919;&#21551;&#21160;&#21644;&#38646;&#26679;&#26412;&#65289;&#30340;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#31163;&#32447;&#35780;&#20272;&#65292;&#39564;&#35777;&#20102;&#20174;LACE&#33719;&#24471;&#30340;&#25512;&#33616;&#36136;&#37327;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22312;&#22312;&#32447;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;LACE&#30340;&#26377;&#25928;&#24615;&#21644;&#29992;&#25143;&#25511;&#21046;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for making high-quality recommendations often rely on learning latent representations from interaction data. These methods, while performant, do not provide ready mechanisms for users to control the recommendation they receive. Our work tackles this problem by proposing LACE, a novel concept value bottleneck model for controllable text recommendations. LACE represents each user with a succinct set of human-readable concepts through retrieval given user-interacted documents and learns personalized representations of the concepts based on user documents. This concept based user profile is then leveraged to make recommendations. The design of our model affords control over the recommendations through a number of intuitive interactions with a transparent user profile. We first establish the quality of recommendations obtained from LACE in an offline evaluation on three recommendation tasks spanning six datasets in warm-start, cold-start, and zero-shot setups. Next, we validate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25209;&#21028;&#24615;&#22238;&#39038;AI&#20844;&#24179;&#24615;&#25991;&#29486;&#20013;30&#31687;&#20132;&#32455;&#24615;&#35752;&#35770;&#65292;&#25581;&#31034;&#30740;&#31350;&#20154;&#21592;&#26222;&#36941;&#32570;&#20047;&#23545;&#20132;&#32455;&#24615;&#30340;&#25972;&#20307;&#29702;&#35299;&#65292;&#20854;&#19968;&#26041;&#38754;&#23558;&#20854;&#32553;&#23567;&#20026;&#22312;&#32676;&#20307;&#23376;&#32452;&#19978;&#36827;&#34892;&#20844;&#24179;&#24230;&#37327;&#30340;&#20248;&#21270;&#65292;&#21478;&#19968;&#26041;&#38754;&#21017;&#22312;&#31038;&#20250;&#32972;&#26223;&#21644;&#26435;&#21147;&#32467;&#26500;&#30340;&#35752;&#35770;&#26041;&#38754;&#23384;&#22312;&#27424;&#32570;&#12290;</title><link>http://arxiv.org/abs/2303.17555</link><description>&lt;p&gt;
&#23545;&#21387;&#36843;&#30697;&#38453;&#30340;&#20998;&#35299;:&#25581;&#31034;&#20132;&#32455;&#24615;&#22312;AI&#20844;&#24179;&#24615;&#20013;&#30340;&#20316;&#29992;&#30340;&#25209;&#21028;&#24615;&#22238;&#39038;&#19982;&#20877;&#24819;&#35937;
&lt;/p&gt;
&lt;p&gt;
Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness. (arXiv:2303.17555v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25209;&#21028;&#24615;&#22238;&#39038;AI&#20844;&#24179;&#24615;&#25991;&#29486;&#20013;30&#31687;&#20132;&#32455;&#24615;&#35752;&#35770;&#65292;&#25581;&#31034;&#30740;&#31350;&#20154;&#21592;&#26222;&#36941;&#32570;&#20047;&#23545;&#20132;&#32455;&#24615;&#30340;&#25972;&#20307;&#29702;&#35299;&#65292;&#20854;&#19968;&#26041;&#38754;&#23558;&#20854;&#32553;&#23567;&#20026;&#22312;&#32676;&#20307;&#23376;&#32452;&#19978;&#36827;&#34892;&#20844;&#24179;&#24230;&#37327;&#30340;&#20248;&#21270;&#65292;&#21478;&#19968;&#26041;&#38754;&#21017;&#22312;&#31038;&#20250;&#32972;&#26223;&#21644;&#26435;&#21147;&#32467;&#26500;&#30340;&#35752;&#35770;&#26041;&#38754;&#23384;&#22312;&#27424;&#32570;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#32455;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#26694;&#26550;&#65292;&#36890;&#36807;&#35843;&#26597;&#21644;&#23454;&#36341;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#26816;&#26597;&#31038;&#20250;&#19981;&#24179;&#31561;&#22914;&#20309;&#36890;&#36807;&#32467;&#26500;&#21644;&#32426;&#24459;&#39046;&#22495;&#25345;&#32493;&#23384;&#22312;&#12290;&#22312;AI&#20844;&#24179;&#30340;&#29702;&#24565;&#20013;&#65292;&#8220;&#20844;&#24179;&#24615;&#8221;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#25105;&#20204;&#35748;&#20026;&#37319;&#29992;&#20132;&#32455;&#24615;&#20316;&#20026;&#20998;&#26512;&#26694;&#26550;&#23545;&#20110;&#26377;&#25928;&#22320;&#23454;&#29616;&#20844;&#24179;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#23545;AI&#20844;&#24179;&#25991;&#29486;&#20013;30&#31687;&#20851;&#20110;&#20132;&#32455;&#24615;&#30340;&#35752;&#35770;&#36827;&#34892;&#25209;&#21028;&#24615;&#22238;&#39038;&#65292;&#25105;&#20204;&#24402;&#32435;&#21644;&#28436;&#32462;&#20986;:1)&#20132;&#32455;&#24615;&#25351;&#23548;&#22914;&#20309;&#22312;AI&#20844;&#24179;&#33539;&#20363;&#20013;&#25805;&#20316;&#65292;2)&#25581;&#31034;&#20132;&#32455;&#24615;&#30340;&#27010;&#24565;&#21270;&#21644;&#23454;&#29616;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30740;&#31350;&#20154;&#21592;&#26222;&#36941;&#23558;&#20132;&#32455;&#24615;&#32553;&#20943;&#20026;&#38024;&#23545;&#20154;&#21475;&#20122;&#32452;&#30340;&#20844;&#24179;&#25351;&#26631;&#36827;&#34892;&#20248;&#21270;&#12290;&#20182;&#20204;&#20063;&#26410;&#33021;&#35752;&#35770;&#23427;&#20204;&#30340;&#31038;&#20250;&#32972;&#26223;&#65292;&#24403;&#25552;&#21040;&#26435;&#21147;&#26102;&#65292;&#20182;&#20204;&#20027;&#35201;&#23558;&#20854;&#32622;&#20110;AI&#27969;&#31243;&#20013;&#12290;&#25105;&#20204;&#23558;&#36827;&#19968;&#27493;&#38416;&#36848;&#24182;&#35780;&#20272;&#36825;&#20123;&#24046;&#36317;&#23545;&#20110;&#20020;&#24202;&#30740;&#31350;&#21644;&#23454;&#36341;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intersectionality is a critical framework that, through inquiry and praxis, allows us to examine how social inequalities persist through domains of structure and discipline. Given AI fairness' raison d'\^etre of ``fairness,'' we argue that adopting intersectionality as an analytical framework is pivotal to effectively operationalizing fairness. Through a critical review of how intersectionality is discussed in 30 papers from the AI fairness literature, we deductively and inductively: 1) map how intersectionality tenets operate within the AI fairness paradigm and 2) uncover gaps between the conceptualization and operationalization of intersectionality. We find that researchers overwhelmingly reduce intersectionality to optimizing for fairness metrics over demographic subgroups. They also fail to discuss their social context and when mentioning power, they mostly situate it only within the AI pipeline. We: 3) outline and assess the implications of these gaps for critical inquiry and prax
&lt;/p&gt;</description></item><item><title>MedNeXt&#26159;&#19968;&#20010;&#23450;&#21046;&#21270;&#30340;&#29616;&#20195;&#21270;&#21487;&#25193;&#23637;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#30340;&#21307;&#23398;&#29615;&#22659;&#25361;&#25112;&#12290;&#35813;&#32593;&#32476;&#21253;&#21547;&#65306;&#23436;&#20840;ConvNeXt 3D&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#12289;&#27531;&#24046;ConvNeXt&#19978;&#19979;&#37319;&#26679;&#22359;&#21644;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#22686;&#21152;&#26680;&#22823;&#23567;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2303.09975</link><description>&lt;p&gt;
MedNeXt&#65306;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#21464;&#21387;&#22120;&#39537;&#21160;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation. (arXiv:2303.09975v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09975
&lt;/p&gt;
&lt;p&gt;
MedNeXt&#26159;&#19968;&#20010;&#23450;&#21046;&#21270;&#30340;&#29616;&#20195;&#21270;&#21487;&#25193;&#23637;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#30340;&#21307;&#23398;&#29615;&#22659;&#25361;&#25112;&#12290;&#35813;&#32593;&#32476;&#21253;&#21547;&#65306;&#23436;&#20840;ConvNeXt 3D&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#12289;&#27531;&#24046;ConvNeXt&#19978;&#19979;&#37319;&#26679;&#22359;&#21644;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#22686;&#21152;&#26680;&#22823;&#23567;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#20351;&#29992;&#22522;&#20110; Transformer &#30340;&#26550;&#26500;&#36234;&#26469;&#36234;&#22810;&#65292;&#20294;&#26159;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#26631;&#27880;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#20351;&#24471;&#20854;&#24615;&#33021;&#36828;&#19981;&#22914;&#33258;&#28982;&#22270;&#20687;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#39640;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#22240;&#27492;&#26356;&#23481;&#26131;&#35757;&#32451;&#21040;&#39640;&#24615;&#33021;&#27700;&#24179;&#12290;&#26368;&#36817;&#65292;ConvNeXt &#26550;&#26500;&#23581;&#35797;&#36890;&#36807;&#38236;&#20687;&#21464;&#21387;&#22120;&#22359;&#26469;&#29616;&#20195;&#21270;&#26631;&#20934;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#36825;&#19968;&#26550;&#26500;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#29616;&#20195;&#21270;&#19988;&#21487;&#25193;&#23637;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#24212;&#23545;&#25968;&#25454;&#31232;&#32570;&#30340;&#21307;&#23398;&#29615;&#22659;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837; MedNeXt&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#21464;&#21387;&#22120;&#21551;&#21457;&#30340;&#22823;&#26680;&#20998;&#21106;&#32593;&#32476;&#65292;&#20854;&#20013;&#21253;&#25324;&#65306;1&#65289;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#23436;&#20840; ConvNeXt 3D &#32534;&#30721;&#22120; - &#35299;&#30721;&#22120;&#32593;&#32476;&#65292;2&#65289;&#27531;&#24046; ConvNeXt &#19978;&#19979;&#37319;&#26679;&#22359;&#65292;&#20197;&#22312;&#21508;&#20010;&#23610;&#24230;&#19978;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#65292;3&#65289;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#19978;&#37319;&#26679;&#23567;&#26680;&#26469;&#36845;&#20195;&#22686;&#21152;&#26680;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been exploding interest in embracing Transformer-based architectures for medical image segmentation. However, the lack of large-scale annotated medical datasets make achieving performances equivalent to those in natural images challenging. Convolutional networks, in contrast, have higher inductive biases and consequently, are easily trainable to high performance. Recently, the ConvNeXt architecture attempted to modernize the standard ConvNet by mirroring Transformer blocks. In this work, we improve upon this to design a modernized and scalable convolutional architecture customized to challenges of data-scarce medical settings. We introduce MedNeXt, a Transformer-inspired large kernel segmentation network which introduces - 1) A fully ConvNeXt 3D Encoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up and downsampling blocks to preserve semantic richness across scales, 3) A novel technique to iteratively increase kernel sizes by upsampling small kernel 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;StyleGANEX&#65292;&#19968;&#31181;&#22522;&#20110;StyleGAN&#30340;&#36229;&#20986;&#35009;&#21098;&#23545;&#40784;&#20154;&#33080;&#30340;&#25805;&#32437;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25193;&#24352;&#21367;&#31215;&#26469;&#35843;&#25972;&#24863;&#21463;&#37326;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#23545;&#40784;&#20154;&#33080;&#30340;&#26356;&#22909;&#25551;&#36848;&#21644;&#21508;&#31181;&#25805;&#32437;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.06146</link><description>&lt;p&gt;
StyleGANEX: &#22522;&#20110;StyleGAN&#30340;&#36229;&#20986;&#35009;&#21098;&#23545;&#40784;&#20154;&#33080;&#30340;&#25805;&#32437;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces. (arXiv:2303.06146v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;StyleGANEX&#65292;&#19968;&#31181;&#22522;&#20110;StyleGAN&#30340;&#36229;&#20986;&#35009;&#21098;&#23545;&#40784;&#20154;&#33080;&#30340;&#25805;&#32437;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25193;&#24352;&#21367;&#31215;&#26469;&#35843;&#25972;&#24863;&#21463;&#37326;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#23545;&#40784;&#20154;&#33080;&#30340;&#26356;&#22909;&#25551;&#36848;&#21644;&#21508;&#31181;&#25805;&#32437;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;StyleGAN&#36827;&#34892;&#20154;&#33080;&#25805;&#32437;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;StyleGAN&#30340;&#23616;&#38480;&#24615;&#22312;&#20110;&#23427;&#20165;&#36866;&#29992;&#20110;&#22312;&#39044;&#35757;&#32451;&#26102;&#22266;&#23450;&#22270;&#20687;&#20998;&#36776;&#29575;&#19979;&#35009;&#21098;&#23545;&#40784;&#30340;&#20154;&#33080;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#25193;&#24352;&#21367;&#31215;&#26469;&#37325;&#26032;&#35843;&#25972;StyleGAN&#27973;&#23618;&#30340;&#24863;&#21463;&#37326;&#65292;&#32780;&#19981;&#25913;&#21464;&#20219;&#20309;&#27169;&#22411;&#21442;&#25968;&#12290;&#36825;&#20351;&#24471;&#27973;&#23618;&#30340;&#22266;&#23450;&#23610;&#23544;&#23567;&#29305;&#24449;&#33021;&#22815;&#25193;&#23637;&#25104;&#33021;&#36866;&#24212;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22823;&#29305;&#24449;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25551;&#36848;&#19981;&#23545;&#40784;&#30340;&#20154;&#33080;&#12290;&#20026;&#20102;&#23454;&#29616;&#30495;&#23454;&#30340;&#20154;&#33080;&#21453;&#28436;&#21644;&#25805;&#32437;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30456;&#24212;&#30340;&#32534;&#30721;&#22120;&#65292;&#38500;&#20102;&#28508;&#22312;&#30340;&#39118;&#26684;&#32534;&#30721;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#25193;&#23637;StyleGAN&#30340;&#31532;&#19968;&#23618;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#20998;&#36776;&#29575;&#30340;&#19981;&#23545;&#40784;&#20154;&#33080;&#36755;&#20837;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#38754;&#37096;&#23646;&#24615;&#32534;&#36753;&#12289;&#36229;&#20998;&#36776;&#29575;&#12289;&#32032;&#25551;/&#38754;&#20855;&#36716;&#25442;&#31561;&#21508;&#31181;&#20154;&#33080;&#25805;&#32437;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in face manipulation using StyleGAN have produced impressive results. However, StyleGAN is inherently limited to cropped aligned faces at a fixed image resolution it is pre-trained on. In this paper, we propose a simple and effective solution to this limitation by using dilated convolutions to rescale the receptive fields of shallow layers in StyleGAN, without altering any model parameters. This allows fixed-size small features at shallow layers to be extended into larger ones that can accommodate variable resolutions, making them more robust in characterizing unaligned faces. To enable real face inversion and manipulation, we introduce a corresponding encoder that provides the first-layer feature of the extended StyleGAN in addition to the latent style code. We validate the effectiveness of our method using unaligned face inputs of various resolutions in a diverse set of face manipulation tasks, including facial attribute editing, super-resolution, sketch/mask-to-face 
&lt;/p&gt;</description></item><item><title>&#35813;&#35843;&#26597;&#30740;&#31350;&#36890;&#36807;&#26102;&#38388;&#36807;&#31243;&#24314;&#27169;&#20107;&#20214;&#24207;&#21015;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#12289;&#26631;&#35760;&#21644;&#26102;&#31354;&#28857;&#36807;&#31243;&#20998;&#31867;&#65292;&#23545;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#31995;&#32479;&#22238;&#39038;&#65292;&#24182;&#20998;&#26512;&#20102;&#24212;&#29992;&#20110;&#39044;&#27979;&#21644;&#24314;&#27169;&#26041;&#38754;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.06067</link><description>&lt;p&gt;
&#36890;&#36807;&#26102;&#38388;&#36807;&#31243;&#24314;&#27169;&#20107;&#20214;&#21644;&#30456;&#20114;&#20316;&#29992; - &#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Modeling Events and Interactions through Temporal Processes -- A Survey. (arXiv:2303.06067v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06067
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35843;&#26597;&#30740;&#31350;&#36890;&#36807;&#26102;&#38388;&#36807;&#31243;&#24314;&#27169;&#20107;&#20214;&#24207;&#21015;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#12289;&#26631;&#35760;&#21644;&#26102;&#31354;&#28857;&#36807;&#31243;&#20998;&#31867;&#65292;&#23545;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#31995;&#32479;&#22238;&#39038;&#65292;&#24182;&#20998;&#26512;&#20102;&#24212;&#29992;&#20110;&#39044;&#27979;&#21644;&#24314;&#27169;&#26041;&#38754;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#35768;&#22810;&#29616;&#35937;&#20197;&#36830;&#32493;&#26102;&#38388;&#21457;&#29983;&#30340;&#19968;&#31995;&#21015;&#20107;&#20214;&#20135;&#29983;&#12290;&#28857;&#36807;&#31243;&#20026;&#24314;&#27169;&#36825;&#20123;&#20107;&#20214;&#24207;&#21015;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#25968;&#23398;&#26694;&#26550;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#36807;&#31243;&#30740;&#31350;&#27010;&#29575;&#27169;&#22411;&#26469;&#24314;&#27169;&#20107;&#20214;&#24207;&#21015;&#12290;&#25105;&#20204;&#20462;&#35746;&#20102;&#20107;&#20214;&#24314;&#27169;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#34920;&#24449;&#30456;&#20851;&#25991;&#29486;&#30340;&#25968;&#23398;&#22522;&#30784;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26412;&#20307;&#26469;&#20197;&#19977;&#20010;&#31867;&#21035;&#65288;&#31616;&#21333;&#12289;&#26631;&#35760;&#21644;&#26102;&#31354;&#28857;&#36807;&#31243;&#65289;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#23545;&#20110;&#27599;&#20010;&#31867;&#21035;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29616;&#26377;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25552;&#20986;&#30340;&#25216;&#26415;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#39044;&#27979;&#21644;&#24314;&#27169;&#26041;&#38754;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world scenario, many phenomena produce a collection of events that occur in continuous time. Point Processes provide a natural mathematical framework for modeling these sequences of events. In this survey, we investigate probabilistic models for modeling event sequences through temporal processes. We revise the notion of event modeling and provide the mathematical foundations that characterize the literature on the topic. We define an ontology to categorize the existing approaches in terms of three families: simple, marked, and spatio-temporal point processes. For each family, we systematically review the existing approaches based based on deep learning. Finally, we analyze the scenarios where the proposed techniques can be used for addressing prediction and modeling aspects.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20915;&#31574;&#20272;&#35745;&#31995;&#25968;&#23545;&#20219;&#24847;&#32467;&#26500;&#21270;&#36172;&#33218;&#38382;&#39064;&#30340;$\gamma$-&#36951;&#25022;&#36827;&#34892;&#20102;&#32039;&#23494;&#30028;&#23450;&#65292;&#35813;&#30028;&#23450;&#26159;&#23545;&#20989;&#25968;&#31867;&#30340;&#32479;&#35745;&#22797;&#26434;&#24230;&#21442;&#25968;$\gamma$-DEC&#30340;&#20462;&#25913;&#29256;&#26412;&#12290;&#20316;&#32773;&#21457;&#29616;$\gamma$-DEC&#26159;&#20219;&#20309;&#27169;&#22411;&#31867;$\mathcal{F}$&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#23545;&#20110;&#20219;&#20309;&#31639;&#27861;&#37117;&#23384;&#22312;&#26576;&#20010;$f \in \mathcal{F}$&#65292;&#35813;&#31639;&#27861;&#30340;$\gamma$-&#36951;&#25022;&#19982;$\mathcal{F}$&#30340;$\gamma$-DEC&#20960;&#20046;&#25104;&#27491;&#27604;&#12290;</title><link>http://arxiv.org/abs/2303.03327</link><description>&lt;p&gt;
&#36890;&#36807;&#20915;&#31574;&#20272;&#35745;&#31995;&#25968;&#65292;&#23545;$\gamma$-&#36951;&#25022;&#36827;&#34892;&#32039;&#23494;&#30028;&#23450;
&lt;/p&gt;
&lt;p&gt;
Tight Bounds for $\gamma$-Regret via the Decision-Estimation Coefficient. (arXiv:2303.03327v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20915;&#31574;&#20272;&#35745;&#31995;&#25968;&#23545;&#20219;&#24847;&#32467;&#26500;&#21270;&#36172;&#33218;&#38382;&#39064;&#30340;$\gamma$-&#36951;&#25022;&#36827;&#34892;&#20102;&#32039;&#23494;&#30028;&#23450;&#65292;&#35813;&#30028;&#23450;&#26159;&#23545;&#20989;&#25968;&#31867;&#30340;&#32479;&#35745;&#22797;&#26434;&#24230;&#21442;&#25968;$\gamma$-DEC&#30340;&#20462;&#25913;&#29256;&#26412;&#12290;&#20316;&#32773;&#21457;&#29616;$\gamma$-DEC&#26159;&#20219;&#20309;&#27169;&#22411;&#31867;$\mathcal{F}$&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#23545;&#20110;&#20219;&#20309;&#31639;&#27861;&#37117;&#23384;&#22312;&#26576;&#20010;$f \in \mathcal{F}$&#65292;&#35813;&#31639;&#27861;&#30340;$\gamma$-&#36951;&#25022;&#19982;$\mathcal{F}$&#30340;$\gamma$-DEC&#20960;&#20046;&#25104;&#27491;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#23545;&#20219;&#24847;&#32467;&#26500;&#21270;&#36172;&#33218;&#38382;&#39064;&#30340;$\gamma$-&#36951;&#25022;&#30340;&#32479;&#35745;&#25551;&#36848;&#65292;&#35813;&#36951;&#25022;&#26159;&#19982;$\gamma$&#20493;&#26368;&#20248;&#35299;&#30456;&#27604;&#36739;&#26102;&#20135;&#29983;&#30340;&#36951;&#25022;&#12290;&#22312;&#20989;&#25968;&#31867;$\mathcal{F}$&#19978;&#30340;&#32467;&#26500;&#21270;&#36172;&#33218;&#38382;&#39064;&#20013;&#65292;&#23547;&#25214;$f \in \mathcal{F}$&#30340;&#31934;&#30830;&#26368;&#20248;&#35299;&#26159;&#38590;&#20197;&#22788;&#29702;&#30340;&#12290;&#25105;&#20204;&#30340;&#25551;&#36848;&#26159;&#22522;&#20110;$\gamma$-DEC&#30340;&#65292;&#23427;&#26159;&#20989;&#25968;&#31867;$\mathcal{F}$&#30340;&#32479;&#35745;&#22797;&#26434;&#24230;&#21442;&#25968;&#65292;&#26159;Foster et al.&#65292;2023&#30340;&#32422;&#26463;&#20915;&#31574;&#20272;&#35745;&#31995;&#25968;(DEC)&#30340;&#20462;&#25913;&#29256;&#26412;&#65288;&#19982;Foster et al.&#65292;2021&#30340;&#21407;&#22987;&#20559;&#31227;DEC&#23494;&#20999;&#30456;&#20851;&#65289;&#12290;&#25105;&#20204;&#30340;&#19979;&#30028;&#34920;&#26126;&#65292;&#23545;&#20110;&#20219;&#20309;&#27169;&#22411;&#31867;$\mathcal{F}$&#65292;$\gamma$-DEC&#26159;&#19968;&#20010;&#22522;&#26412;&#38480;&#21046;&#65306;&#23545;&#20110;&#20219;&#20309;&#31639;&#27861;&#65292;&#23384;&#22312;&#19968;&#20123;$f \in \mathcal{F}$&#65292;&#35813;&#31639;&#27861;&#30340;$\gamma$-&#36951;&#25022;&#19982;$\mathcal{F}$&#30340;$\gamma$-DEC&#30340;&#35268;&#27169;&#65288;&#20960;&#20046;&#65289;&#25104;&#27491;&#27604;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#19978;&#30028;&#65292;&#35777;&#26126;&#20102;&#23384;&#22312;&#19968;&#20123;......
&lt;/p&gt;
&lt;p&gt;
In this work, we give a statistical characterization of the $\gamma$-regret for arbitrary structured bandit problems, the regret which arises when comparing against a benchmark that is $\gamma$ times the optimal solution. The $\gamma$-regret emerges in structured bandit problems over a function class $\mathcal{F}$ where finding an exact optimum of $f \in \mathcal{F}$ is intractable. Our characterization is given in terms of the $\gamma$-DEC, a statistical complexity parameter for the class $\mathcal{F}$, which is a modification of the constrained Decision-Estimation Coefficient (DEC) of Foster et al., 2023 (and closely related to the original offset DEC of Foster et al., 2021). Our lower bound shows that the $\gamma$-DEC is a fundamental limit for any model class $\mathcal{F}$: for any algorithm, there exists some $f \in \mathcal{F}$ for which the $\gamma$-regret of that algorithm scales (nearly) with the $\gamma$-DEC of $\mathcal{F}$. We provide an upper bound showing that there exist
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#35777;&#26126;&#29256;&#26435;&#20445;&#25252;&#29983;&#25104;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36817;&#26080;&#38459;&#30861;&#24615;&#65288;NAF&#65289;&#30340;&#23450;&#20041;&#65292;&#24182;&#32473;&#20986;&#20102;&#28385;&#36275;&#35813;&#23450;&#20041;&#30340;&#27169;&#22411;&#36755;&#20986;&#19982;&#21463;&#29256;&#26435;&#20445;&#25252;&#25968;&#25454;&#30456;&#20284;&#26679;&#26412;&#30340;&#27010;&#29575;&#19978;&#38480;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20379;&#20102;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#36755;&#20986;&#20855;&#26377;&#24378;&#22823;&#29256;&#26435;&#20445;&#25252;&#33021;&#21147;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#36827;&#34892;&#20102;&#26377;&#21069;&#26223;&#30340;&#35821;&#35328;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2302.10870</link><description>&lt;p&gt;
&#20851;&#20110;&#21487;&#35777;&#26126;&#29256;&#26435;&#20445;&#25252;&#29983;&#25104;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Provable Copyright Protection for Generative Models. (arXiv:2302.10870v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10870
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#35777;&#26126;&#29256;&#26435;&#20445;&#25252;&#29983;&#25104;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36817;&#26080;&#38459;&#30861;&#24615;&#65288;NAF&#65289;&#30340;&#23450;&#20041;&#65292;&#24182;&#32473;&#20986;&#20102;&#28385;&#36275;&#35813;&#23450;&#20041;&#30340;&#27169;&#22411;&#36755;&#20986;&#19982;&#21463;&#29256;&#26435;&#20445;&#25252;&#25968;&#25454;&#30456;&#20284;&#26679;&#26412;&#30340;&#27010;&#29575;&#19978;&#38480;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20379;&#20102;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#36755;&#20986;&#20855;&#26377;&#24378;&#22823;&#29256;&#26435;&#20445;&#25252;&#33021;&#21147;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#36827;&#34892;&#20102;&#26377;&#21069;&#26223;&#30340;&#35821;&#35328;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25285;&#24515;&#23398;&#20064;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#21487;&#33021;&#36755;&#20986;&#19982;&#20854;&#35757;&#32451;&#38598;&#20013;&#30340;&#26576;&#20123;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#25968;&#25454;$C$&#26497;&#20026;&#30456;&#20284;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#8220;&#36817;&#26080;&#38459;&#30861;&#24615;&#65288;NAF&#65289;&#8221;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#35777;&#26126;&#20102;&#28385;&#36275;&#36825;&#20010;&#23450;&#20041;&#30340;&#27169;&#22411;&#36755;&#20986;&#19982;$C$&#30456;&#20284;&#26679;&#26412;&#30340;&#27010;&#29575;&#19978;&#38480;&#65292;&#21363;&#20351;$C$&#21253;&#21547;&#22312;&#20854;&#35757;&#32451;&#38598;&#20013;&#12290;&#31895;&#30053;&#22320;&#35828;&#65292;&#29983;&#25104;&#27169;&#22411;$p$&#26159;&#8220;$k$-NAF&#8221;&#30340;&#65292;&#22914;&#26524;&#23545;&#20110;&#27599;&#19968;&#20010;&#28508;&#22312;&#30340;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#25968;&#25454;$C$&#65292;$p$&#30340;&#36755;&#20986;&#19982;&#19968;&#20010;&#23436;&#20840;&#26410;&#35775;&#38382;$C$&#30340;&#27169;&#22411;$q$&#30340;&#36755;&#20986;&#20043;&#38388;&#30340;&#24046;&#21035;&#26368;&#22823;&#20026;$k$&#27604;&#29305;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#40657;&#30418;&#26041;&#24335;&#39640;&#25928;&#20462;&#25913;&#21407;&#22987;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#31639;&#27861;&#65292;&#36755;&#20986;&#20855;&#26377;&#24378;&#22823;&#30340;&#20445;&#25252;&#20869;&#23481;&#37319;&#26679;&#27010;&#29575;&#19978;&#38480;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#38024;&#23545;&#35821;&#35328;&#65288;Transformer&#65289;&#21644;&#22270;&#20687;&#65288;Diffusion&#65289;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#26377;&#21069;&#26223;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data $C$ that was in their training set. We give a formal definition of $\textit{near access-freeness (NAF)}$ and prove bounds on the probability that a model satisfying this definition outputs a sample similar to $C$, even if $C$ is included in its training set. Roughly speaking, a generative model $p$ is $\textit{$k$-NAF}$ if for every potentially copyrighted data $C$, the output of $p$ diverges by at most $k$-bits from the output of a model $q$ that $\textit{did not access $C$ at all}$. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#65292;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#29992;&#20110;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;</title><link>http://arxiv.org/abs/2302.09738</link><description>&lt;p&gt;
&#31616;&#21270;&#22522;&#20110;&#21160;&#37327;&#30340;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Simplifying Momentum-based Riemannian Submanifold Optimization. (arXiv:2302.09738v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#65292;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#29992;&#20110;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#21160;&#37327;&#30340;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#22312;&#35745;&#31639;&#19978;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#30830;&#20445;&#36845;&#20195;&#20445;&#25345;&#22312;&#23376;&#27969;&#24418;&#19978;&#36890;&#24120;&#38656;&#35201;&#35299;&#20915;&#22256;&#38590;&#30340;&#24494;&#20998;&#26041;&#31243;&#12290;&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#20223;&#23556;&#19981;&#21464;&#24230;&#37327;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#20197;&#23558;&#38382;&#39064;&#21160;&#24577;&#22320;&#31616;&#21270;&#20026;&#27431;&#20960;&#37324;&#24471;&#26080;&#32422;&#26463;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#21644;&#31616;&#21270;&#29616;&#26377;&#30340;&#32467;&#26500;&#21270;&#21327;&#26041;&#24046;&#26041;&#27861;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;
&lt;/p&gt;
&lt;p&gt;
Riemannian submanifold optimization with momentum is computationally challenging because ensuring iterates remain on the submanifold often requires solving difficult differential equations. We simplify such optimization algorithms for the submanifold of symmetric positive-definite matrices with the affine invariant metric. We propose a generalized version of the Riemannian normal coordinates which dynamically trivializes the problem into a Euclidean unconstrained problem. We use our approach to explain and simplify existing approaches for structured covariances and develop efficient second-order optimizers for deep learning without explicit matrix inverses.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22810;&#20998;&#36776;&#29575;&#22270;&#24418;&#21464;&#25442;&#22120;&#65288;MGT&#65289;&#21644;&#23567;&#27874;&#20301;&#32622;&#32534;&#30721;&#65288;WavePE&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#34920;&#31034;&#22823;&#20998;&#23376;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#24182;&#22312;&#20247;&#22810;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#27604;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.08647</link><description>&lt;p&gt;
&#22810;&#20998;&#36776;&#29575;&#22270;&#24418;&#21464;&#25442;&#22120;&#19982;&#23567;&#27874;&#20301;&#32622;&#32534;&#30721;&#29992;&#20110;&#23398;&#20064;&#20998;&#23618;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Multiresolution Graph Transformers and Wavelet Positional Encoding for Learning Hierarchical Structures. (arXiv:2302.08647v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08647
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22810;&#20998;&#36776;&#29575;&#22270;&#24418;&#21464;&#25442;&#22120;&#65288;MGT&#65289;&#21644;&#23567;&#27874;&#20301;&#32622;&#32534;&#30721;&#65288;WavePE&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#34920;&#31034;&#22823;&#20998;&#23376;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#24182;&#22312;&#20247;&#22810;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#27604;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22270;&#24418;&#23398;&#20064;&#31639;&#27861;&#24182;&#19981;&#33021;&#24456;&#22909;&#22320;&#22788;&#29702;&#22823;&#20998;&#23376;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#21407;&#23376;&#20043;&#38388;&#30340;&#20998;&#23618;&#20132;&#20114;&#65292;&#32780;&#36825;&#23545;&#20110;&#30830;&#23450;&#22823;&#20998;&#23376;&#30340;&#23646;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#20998;&#36776;&#29575;&#22270;&#24418;&#21464;&#25442;&#22120;&#65288;MGT&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#23398;&#20064;&#34920;&#31034;&#22810;&#31181;&#23610;&#24230;&#19979;&#22823;&#20998;&#23376;&#30340;&#22270;&#24418;&#21464;&#25442;&#22120;&#26550;&#26500;&#12290;MGT&#21487;&#20197;&#23398;&#20064;&#20135;&#29983;&#21407;&#23376;&#30340;&#34920;&#31034;&#65292;&#24182;&#23558;&#23427;&#20204;&#20998;&#32452;&#25104;&#26377;&#24847;&#20041;&#30340;&#21151;&#33021;&#32452;&#25110;&#37325;&#22797;&#21333;&#20803;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#23567;&#27874;&#20301;&#32622;&#32534;&#30721;&#65288;WavePE&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#22312;&#39057;&#35889;&#21644;&#31354;&#38388;&#22495;&#20013;&#30340;&#23616;&#37096;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#30001;&#32858;&#21512;&#29289;&#21644;&#22810;&#32957;&#32452;&#25104;&#30340;&#20004;&#20010;&#22823;&#20998;&#23376;&#25968;&#25454;&#38598;&#20197;&#21450;&#19968;&#20010;&#31867;&#20284;&#33647;&#29289;&#30340;&#20998;&#23376;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22312;&#20272;&#31639;&#20998;&#23376;&#24615;&#36136;&#65288;&#20363;&#22914;GAP&#65292;HOMO&#21644;LUMO&#65289;&#26102;&#36798;&#21040;&#20102;&#21270;&#23398;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contemporary graph learning algorithms are not well-defined for large molecules since they do not consider the hierarchical interactions among the atoms, which are essential to determine the molecular properties of macromolecules. In this work, we propose Multiresolution Graph Transformers (MGT), the first graph transformer architecture that can learn to represent large molecules at multiple scales. MGT can learn to produce representations for the atoms and group them into meaningful functional groups or repeating units. We also introduce Wavelet Positional Encoding (WavePE), a new positional encoding method that can guarantee localization in both spectral and spatial domains. Our proposed model achieves competitive results on two macromolecule datasets consisting of polymers and peptides, and one drug-like molecule dataset. Importantly, our model outperforms other state-of-the-art methods and achieves chemical accuracy in estimating molecular properties (e.g., GAP, HOMO and LUMO) calc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20197;&#27133;&#20026;&#20013;&#24515;&#30340;&#21442;&#32771;&#26694;&#26550;&#26469;&#25913;&#36827;&#23545;&#35937;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Slot Attention&#20013;&#34701;&#20837;&#31354;&#38388;&#23545;&#31216;&#24615;&#65292;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#25972;&#20307;&#23545;&#35937;&#21457;&#29616;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.04973</link><description>&lt;p&gt;
&#19981;&#21464;&#30340;&#27133;&#27880;&#24847;&#21147;: &#36890;&#36807;&#20197;&#27133;&#20026;&#20013;&#24515;&#30340;&#21442;&#32771;&#26694;&#26550;&#36827;&#34892;&#23545;&#35937;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames. (arXiv:2302.04973v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20197;&#27133;&#20026;&#20013;&#24515;&#30340;&#21442;&#32771;&#26694;&#26550;&#26469;&#25913;&#36827;&#23545;&#35937;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Slot Attention&#20013;&#34701;&#20837;&#31354;&#38388;&#23545;&#31216;&#24615;&#65292;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#25972;&#20307;&#23545;&#35937;&#21457;&#29616;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21407;&#22987;&#24863;&#30693;&#25968;&#25454;&#20013;&#33258;&#21160;&#21457;&#29616;&#21487;&#32452;&#21512;&#30340;&#25277;&#35937;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#27133;&#30340;&#31070;&#32463;&#32593;&#32476;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#23545;&#35937;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#22312;&#20805;&#20998;&#25429;&#25417;&#35270;&#35273;&#19990;&#30028;&#20013;&#30340;&#31354;&#38388;&#23545;&#31216;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#23548;&#33268;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#65292;&#27604;&#22914;&#22312;&#32416;&#32467;&#23545;&#35937;&#22806;&#35266;&#21644;&#23039;&#24577;&#26102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#27133;&#20026;&#20013;&#24515;&#30340;&#21442;&#32771;&#26694;&#26550;&#26469;&#34701;&#20837;&#31354;&#38388;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#23558;&#31561;&#21464;&#24615;&#24341;&#20837;&#21040;Slot Attention&#30340;&#27880;&#24847;&#21147;&#21644;&#29983;&#25104;&#26426;&#21046;&#20013;&#65292;&#36890;&#36807;&#24179;&#31227;&#12289;&#32553;&#25918;&#21644;&#26059;&#36716;&#20301;&#32622;&#32534;&#30721;&#26469;&#23454;&#29616;&#23545;&#27599;&#20010;&#23545;&#35937;&#23039;&#24577;&#21464;&#25442;&#30340;&#31561;&#21464;&#24615;&#12290;&#36825;&#20123;&#25913;&#21464;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#21487;&#20197;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#25972;&#20307;&#23545;&#35937;&#21457;&#29616;&#26041;&#38754;&#21462;&#24471;&#24040;&#22823;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#35821;&#27861;&#33539;&#22260;&#20869;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically discovering composable abstractions from raw perceptual data is a long-standing challenge in machine learning. Recent slot-based neural networks that learn about objects in a self-supervised manner have made exciting progress in this direction. However, they typically fall short at adequately capturing spatial symmetries present in the visual world, which leads to sample inefficiency, such as when entangling object appearance and pose. In this paper, we present a simple yet highly effective method for incorporating spatial symmetries via slot-centric reference frames. We incorporate equivariance to per-object pose transformations into the attention and generation mechanism of Slot Attention by translating, scaling, and rotating position encodings. These changes result in little computational overhead, are easy to implement, and can result in large gains in terms of data efficiency and overall improvements to object discovery. We evaluate our method on a wide range of synt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#26816;&#27979;&#22270;&#20687;&#21644;&#38899;&#39057;&#25968;&#25454;&#38598;&#20013;&#24555;&#25463;&#26041;&#24335;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#20960;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.04246</link><description>&lt;p&gt;
&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26816;&#27979;&#24555;&#25463;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Shortcut Detection with Variational Autoencoders. (arXiv:2302.04246v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#26816;&#27979;&#22270;&#20687;&#21644;&#38899;&#39057;&#25968;&#25454;&#38598;&#20013;&#24555;&#25463;&#26041;&#24335;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#20960;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#27169;&#22411;&#22522;&#20110;&#20855;&#26377;&#33391;&#22909;&#26222;&#36866;&#24615;&#30340;&#29305;&#24449;&#32780;&#19981;&#26159;&#25968;&#25454;&#20013;&#30340;&#20598;&#28982;&#30456;&#20851;&#24615;&#36827;&#34892;&#39044;&#27979;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#35782;&#21035;&#36825;&#31181;&#20598;&#28982;&#30456;&#20851;&#24615;&#65292;&#20063;&#31216;&#20026;&#24555;&#25463;&#26041;&#24335;&#65292;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#36804;&#20170;&#20026;&#27490;&#24471;&#21040;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26469;&#26816;&#27979;&#22270;&#20687;&#21644;&#38899;&#39057;&#25968;&#25454;&#38598;&#20013;&#24555;&#25463;&#26041;&#24335;&#30340;&#26032;&#26041;&#27861;&#12290;VAEs&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#35299;&#32544;&#20351;&#25105;&#20204;&#33021;&#22815;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#29305;&#24449;&#21644;&#30446;&#26631;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#21322;&#33258;&#21160;&#35780;&#20272;&#65292;&#20197;&#21457;&#29616;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24555;&#25463;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#21457;&#29616;&#20102;&#20197;&#21069;&#26410;&#21457;&#29616;&#30340;&#24555;&#25463;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
For real-world applications of machine learning (ML), it is essential that models make predictions based on well-generalizing features rather than spurious correlations in the data. The identification of such spurious correlations, also known as shortcuts, is a challenging problem and has so far been scarcely addressed. In this work, we present a novel approach to detect shortcuts in image and audio datasets by leveraging variational autoencoders (VAEs). The disentanglement of features in the latent space of VAEs allows us to discover feature-target correlations in datasets and semi-automatically evaluate them for ML shortcuts. We demonstrate the applicability of our method on several real-world datasets and identify shortcuts that have not been discovered before.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#21512;&#20316;&#21327;&#21516;&#36827;&#21270;&#25628;&#32034;&#65292;&#30830;&#23450;&#26426;&#22120;&#23398;&#20064;&#33258;&#20027;&#31995;&#32479;&#20013;&#30340;ML&#32452;&#20214;&#30340;&#21361;&#38505;&#36793;&#30028;&#12290;&#36825;&#31181;&#36793;&#30028;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#23433;&#20840;&#30417;&#35270;&#22120;&#65292;&#24182;&#22312;&#36798;&#21040;&#21361;&#38505;&#36793;&#30028;&#26102;&#37319;&#21462;&#39044;&#23450;&#20041;&#30340;&#22238;&#36864;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2301.13807</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#20316;&#21327;&#21516;&#36827;&#21270;&#25628;&#32034;&#30830;&#23450;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#33258;&#20027;&#31995;&#32479;&#30340;&#21361;&#38505;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Identifying the Hazard Boundary of ML-enabled Autonomous Systems Using Cooperative Co-Evolutionary Search. (arXiv:2301.13807v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#21512;&#20316;&#21327;&#21516;&#36827;&#21270;&#25628;&#32034;&#65292;&#30830;&#23450;&#26426;&#22120;&#23398;&#20064;&#33258;&#20027;&#31995;&#32479;&#20013;&#30340;ML&#32452;&#20214;&#30340;&#21361;&#38505;&#36793;&#30028;&#12290;&#36825;&#31181;&#36793;&#30028;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#23433;&#20840;&#30417;&#35270;&#22120;&#65292;&#24182;&#22312;&#36798;&#21040;&#21361;&#38505;&#36793;&#30028;&#26102;&#37319;&#21462;&#39044;&#23450;&#20041;&#30340;&#22238;&#36864;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39537;&#21160;&#30340;&#33258;&#20027;&#31995;&#32479;&#65288;MLAS&#65289;&#20013;&#65292;&#30830;&#23450;ML&#32452;&#20214;&#65288;MLCs&#65289;&#30340;&#21361;&#38505;&#36793;&#30028;&#23545;&#20110;&#20998;&#26512;&#20013;&#30340;MLAS&#33267;&#20851;&#37325;&#35201;&#12290;&#37492;&#20110;&#36825;&#31181;&#36793;&#30028;&#21487;&#20197;&#25429;&#25417;&#21040;&#23548;&#33268;&#21361;&#38505;&#30340;MLC&#34892;&#20026;&#21644;&#31995;&#32479;&#32972;&#26223;&#26465;&#20214;&#65292;&#20363;&#22914;&#65292;&#22312;&#36798;&#21040;&#21361;&#38505;&#36793;&#30028;&#26102;&#65292;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#23433;&#20840;&#30417;&#35270;&#22120;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#37319;&#21462;&#20219;&#20309;&#39044;&#23450;&#20041;&#30340;&#22238;&#36864;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;ML&#32452;&#20214;&#30340;&#36825;&#31181;&#21361;&#38505;&#36793;&#30028;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36825;&#26159;&#30001;&#20110;&#38382;&#39064;&#31354;&#38388;&#23558;&#31995;&#32479;&#29615;&#22659;&#65288;&#21363;&#22330;&#26223;&#65289;&#21644;MLC&#34892;&#20026;&#65288;&#21363;&#36755;&#20837;&#21644;&#36755;&#20986;&#65289;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#36807;&#20110;&#24222;&#22823;&#65292;&#26080;&#27861;&#36890;&#36807;&#20840;&#38754;&#30340;&#25506;&#32034;&#29978;&#33267;&#26159;&#20256;&#32479;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22914;&#36951;&#20256;&#31639;&#27861;&#26469;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#30830;&#23450;&#20219;&#20309;MLAS&#23433;&#20840;&#36829;&#35268;&#25152;&#38656;&#30340;&#27169;&#25311;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#20351;&#38382;&#39064;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#20197;&#30830;&#23450;&#24615;&#22320;&#32771;&#34385;&#38382;&#39064;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#21306;&#22495;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Machine Learning (ML)-enabled autonomous systems (MLASs), it is essential to identify the hazard boundary of ML Components (MLCs) in the MLAS under analysis. Given that such boundary captures the conditions in terms of MLC behavior and system context that can lead to hazards, it can then be used to, for example, build a safety monitor that can take any predefined fallback mechanisms at runtime when reaching the hazard boundary. However, determining such hazard boundary for an ML component is challenging. This is due to the problem space combining system contexts (i.e., scenarios) and MLC behaviors (i.e., inputs and outputs) being far too large for exhaustive exploration and even to handle using conventional metaheuristics, such as genetic algorithms. Additionally, the high computational cost of simulations required to determine any MLAS safety violations makes the problem even more challenging. Furthermore, it is unrealistic to consider a region in the problem space deterministicall
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#30340;&#31070;&#32463;&#32593;&#32476;&#35770;&#35777;&#35299;&#37322;&#26041;&#27861;SpArX&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21487;&#20197;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#26356;&#24544;&#23454;&#21644;&#28145;&#20837;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2301.09559</link><description>&lt;p&gt;
SpArX: &#31232;&#30095;&#30340;&#31070;&#32463;&#32593;&#32476;&#35770;&#35777;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
SpArX: Sparse Argumentative Explanations for Neural Networks. (arXiv:2301.09559v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09559
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#30340;&#31070;&#32463;&#32593;&#32476;&#35770;&#35777;&#35299;&#37322;&#26041;&#27861;SpArX&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21487;&#20197;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#26356;&#24544;&#23454;&#21644;&#28145;&#20837;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#20294;&#35299;&#37322;&#23427;&#20204;&#30340;&#20915;&#31574;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20851;&#27880;&#35299;&#37322;&#25913;&#21464;&#21333;&#20010;&#36755;&#20837;&#22914;&#20309;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36755;&#20986;&#34892;&#20026;&#19968;&#33268;&#30340;&#35299;&#37322;&#26410;&#24517;&#24544;&#23454;&#20110;&#20854;&#23454;&#38469;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20026;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#26426;&#21046;&#21019;&#24314;&#20102;&#35770;&#35777;&#24615;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;SpArX&#26041;&#27861;&#39318;&#20808;&#23558;&#22810;&#23618;&#24863;&#30693;&#22120;&#31232;&#30095;&#21270;&#65292;&#21516;&#26102;&#20445;&#25345;&#23613;&#21487;&#33021;&#22810;&#30340;&#21407;&#22987;&#32467;&#26500;&#12290;&#28982;&#21518;&#23558;&#31232;&#30095;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#36716;&#21270;&#20026;&#31561;&#25928;&#30340;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#65292;&#20197;&#25581;&#31034;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#28508;&#22312;&#20915;&#31574;&#36807;&#31243;&#65292;&#20135;&#29983;&#20840;&#23616;&#21644;/&#25110;&#23616;&#37096;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;SpArX&#27604;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#32473;&#20986;&#26356;&#24544;&#23454;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#27934;&#23519;&#23454;&#38469;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks (NNs) have various applications in AI, but explaining their decisions remains challenging. Existing approaches often focus on explaining how changing individual inputs affects NNs' outputs. However, an explanation that is consistent with the input-output behaviour of an NN is not necessarily faithful to the actual mechanics thereof. In this paper, we exploit relationships between multi-layer perceptrons (MLPs) and quantitative argumentation frameworks (QAFs) to create argumentative explanations for the mechanics of MLPs. Our SpArX method first sparsifies the MLP while maintaining as much of the original structure as possible. It then translates the sparse MLP into an equivalent QAF to shed light on the underlying decision process of the MLP, producing global and/or local explanations. We demonstrate experimentally that SpArX can give more faithful explanations than existing approaches, while simultaneously providing deeper insights into the actual reasoning process of M
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#19982;&#29615;&#22659;&#32500;&#24230;&#26080;&#20851;&#20294;&#19982;&#27969;&#24418;&#20869;&#22312;&#32500;&#24230;&#30456;&#20851;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#23545;&#20808;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2212.12606</link><description>&lt;p&gt;
&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
A Convergence Rate for Manifold Neural Networks. (arXiv:2212.12606v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12606
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#19982;&#29615;&#22659;&#32500;&#24230;&#26080;&#20851;&#20294;&#19982;&#27969;&#24418;&#20869;&#22312;&#32500;&#24230;&#30456;&#20851;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#23545;&#20808;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#20135;&#29983;&#65292;&#24182;&#19988;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#33268;&#21147;&#20110;&#24320;&#21457;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#20415;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#39046;&#22495;&#65288;&#22914;&#22270;&#21644;&#27969;&#24418;&#65289;&#20013;&#20998;&#26512;&#27492;&#31867;&#25968;&#25454;&#12290;&#24343;&#19969;&#183;&#29579;&#12289;&#21776;&#22372;&#183;&#21346;&#20197;&#21450;&#20122;&#21382;&#23665;&#22823;&#183;&#37324;&#36125;&#32599;&#26368;&#36817;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#25289;&#26222;&#25289;&#26031;-&#36125;&#23572;&#29305;&#25289;&#31859;&#31639;&#23376;&#30340;&#35889;&#20998;&#35299;&#26500;&#24314;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#20316;&#32773;&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#20540;&#26041;&#26696;&#65292;&#20197;&#22312;&#27969;&#24418;&#26410;&#30693;&#19988;&#20165;&#26377;&#26377;&#38480;&#25968;&#37327;&#26679;&#26412;&#28857;&#21487;&#33719;&#24471;&#30340;&#24773;&#20917;&#19979;&#23454;&#26045;&#27492;&#31867;&#31070;&#32463;&#32593;&#32476;&#12290;&#20316;&#32773;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#26696;&#65292;&#20381;&#38752;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#30340;&#22270;&#65292;&#24403;&#26679;&#26412;&#28857;&#25968;&#37327;&#36235;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;&#25910;&#25947;&#21040;&#36830;&#32493;&#26497;&#38480;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22312;&#27492;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#24314;&#31435;&#20102;&#19968;&#20010;&#25910;&#25947;&#36895;&#29575;&#65292;&#35813;&#25910;&#25947;&#36895;&#29575;&#21462;&#20915;&#20110;&#27969;&#24418;&#30340;&#20869;&#22312;&#32500;&#24230;&#65292;&#20294;&#19982;&#29615;&#22659;&#32500;&#24230;&#26080;&#20851;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#25910;&#25947;&#36895;&#29575;&#22914;&#20309;&#20381;&#36182;&#20110;...
&lt;/p&gt;
&lt;p&gt;
High-dimensional data arises in numerous applications, and the rapidly developing field of geometric deep learning seeks to develop neural network architectures to analyze such data in non-Euclidean domains, such as graphs and manifolds. Recent work by Z. Wang, L. Ruiz, and A. Ribeiro has introduced a method for constructing manifold neural networks using the spectral decomposition of the Laplace Beltrami operator. Moreover, in this work, the authors provide a numerical scheme for implementing such neural networks when the manifold is unknown and one only has access to finitely many sample points. The authors show that this scheme, which relies upon building a data-driven graph, converges to the continuum limit as the number of sample points tends to infinity. Here, we build upon this result by establishing a rate of convergence that depends on the intrinsic dimension of the manifold but is independent of the ambient dimension. We also discuss how the rate of convergence depends on the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#28909;&#21551;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22768;&#23398;&#38556;&#30861;&#29289;&#25955;&#23556;&#38382;&#39064;&#30340;&#36870;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#35745;&#31639;&#25955;&#23556;&#22330;&#21644;&#32473;&#23450;&#27979;&#37327;&#25968;&#25454;&#20043;&#38388;&#30340;$L^2$&#36317;&#31163;&#30340;&#21306;&#22495;&#36793;&#30028;&#20013;&#25214;&#21040;&#33391;&#22909;&#30340;&#21021;&#22987;&#29468;&#27979;&#65292;&#20197;&#20811;&#26381;&#35745;&#31639;&#19978;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2212.08736</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#28909;&#21551;&#21160;&#26041;&#27861;&#29992;&#20110;&#22768;&#23398;&#38556;&#30861;&#29289;&#25955;&#23556;&#38382;&#39064;&#30340;&#36870;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Neural Network Warm-Start Approach for the Inverse Acoustic Obstacle Scattering Problem. (arXiv:2212.08736v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#28909;&#21551;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22768;&#23398;&#38556;&#30861;&#29289;&#25955;&#23556;&#38382;&#39064;&#30340;&#36870;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#35745;&#31639;&#25955;&#23556;&#22330;&#21644;&#32473;&#23450;&#27979;&#37327;&#25968;&#25454;&#20043;&#38388;&#30340;$L^2$&#36317;&#31163;&#30340;&#21306;&#22495;&#36793;&#30028;&#20013;&#25214;&#21040;&#33391;&#22909;&#30340;&#21021;&#22987;&#29468;&#27979;&#65292;&#20197;&#20811;&#26381;&#35745;&#31639;&#19978;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#20108;&#32500;&#20013;&#30340;&#22768;&#23398;&#38556;&#30861;&#29289;&#25955;&#23556;&#38382;&#39064;&#30340;&#36870;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#23545;&#26469;&#33258;&#29289;&#20307;&#22806;&#37096;&#30340;&#19968;&#31995;&#21015;&#25509;&#25910;&#22120;&#25509;&#25910;&#21040;&#30340;&#25955;&#23556;&#22330;&#30340;&#27979;&#37327;&#26469;&#30830;&#23450;&#38556;&#30861;&#29289;&#30340;&#36793;&#30028;&#12290;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26631;&#20934;&#26041;&#27861;&#20043;&#19968;&#26159;&#23558;&#20854;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65306;&#25214;&#21040;&#26368;&#23567;&#21270;&#35745;&#31639;&#30340;&#25955;&#23556;&#22330;&#20540;&#19982;&#32473;&#23450;&#27979;&#37327;&#25968;&#25454;&#20043;&#38388;&#30340;$L^2$&#36317;&#31163;&#30340;&#21306;&#22495;&#36793;&#30028;&#12290;&#30001;&#20110;&#23616;&#37096;&#20984;&#24615;&#38543;&#30528;&#39057;&#29575;&#22686;&#21152;&#32780;&#25910;&#32553;&#65292;&#24182;&#22312;&#30495;&#35299;&#38468;&#36817;&#20135;&#29983;&#36234;&#26469;&#36234;&#22810;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#23454;&#39564;&#35774;&#32622;&#20013;&#65292;&#30001;&#20110;&#23454;&#39564;&#35013;&#32622;&#25110;&#27979;&#37327;&#20256;&#24863;&#22120;&#30340;&#38480;&#21046;&#65292;&#20302;&#39057;&#27979;&#37327;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#33719;&#24471;&#19968;&#20010;&#33391;&#22909;&#30340;&#20248;&#21270;&#38382;&#39064;&#21021;&#22987;&#29468;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the inverse acoustic obstacle problem for sound-soft star-shaped obstacles in two dimensions wherein the boundary of the obstacle is determined from measurements of the scattered field at a collection of receivers outside the object. One of the standard approaches for solving this problem is to reformulate it as an optimization problem: finding the boundary of the domain that minimizes the $L^2$ distance between computed values of the scattered field and the given measurement data. The optimization problem is computationally challenging since the local set of convexity shrinks with increasing frequency and results in an increasing number of local minima in the vicinity of the true solution. In many practical experimental settings, low frequency measurements are unavailable due to limitations of the experimental setup or the sensors used for measurement. Thus, obtaining a good initial guess for the optimization problem plays a vital role in this environment.  We present a ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#30340;&#32479;&#19968;&#31639;&#27861;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#30340;&#22810;&#26679;&#24615;&#21644;&#35206;&#30422;&#24615;&#30340;&#26174;&#24335;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2212.00211</link><description>&lt;p&gt;
&#22522;&#20110;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#30340;&#32479;&#19968;&#31639;&#27861;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Algorithm Framework for Unsupervised Discovery of Skills based on Determinantal Point Process. (arXiv:2212.00211v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#30340;&#32479;&#19968;&#31639;&#27861;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#30340;&#22810;&#26679;&#24615;&#21644;&#35206;&#30422;&#24615;&#30340;&#26174;&#24335;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#26102;&#38388;&#25277;&#35937;&#23398;&#20064;&#20016;&#23500;&#30340;&#25216;&#33021;&#32780;&#26080;&#38656;&#22806;&#37096;&#22870;&#21169;&#30417;&#30563;&#26159;&#19968;&#20010;&#21069;&#27839;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65306;&#21464;&#20998;&#21644;&#25289;&#26222;&#25289;&#26031;&#22522;&#20110;&#25216;&#33021;&#65288;&#21448;&#31216;&#20026;&#36873;&#39033;&#65289;&#21457;&#29616;&#12290;&#21069;&#32773;&#36890;&#36807;&#20114;&#20449;&#24687;&#25439;&#22833;&#26368;&#22823;&#21270;&#21457;&#29616;&#30340;&#36873;&#39033;&#30340;&#22810;&#26679;&#24615;&#65292;&#20294;&#24573;&#35270;&#20102;&#29366;&#24577;&#31354;&#38388;&#30340;&#35206;&#30422;&#29575;&#65292;&#32780;&#21518;&#32773;&#20391;&#37325;&#20110;&#36890;&#36807;&#22686;&#21152;&#25506;&#32034;&#36807;&#31243;&#20013;&#30340;&#36830;&#25509;&#24615;&#26469;&#25552;&#39640;&#36873;&#39033;&#30340;&#35206;&#30422;&#29575;&#65292;&#20294;&#19981;&#32771;&#34385;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#30340;&#26032;&#22411;&#20351;&#29992;&#26469;&#37327;&#21270;&#22810;&#26679;&#24615;&#21644;&#35206;&#30422;&#24615;&#65292;&#24182;&#26174;&#24335;&#20248;&#21270;&#26080;&#30417;&#30563;&#36873;&#39033;&#21457;&#29616;&#30340;&#20004;&#20010;&#30446;&#26631;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#29366;&#24577;&#36716;&#25442;&#22270;&#30340;&#25289;&#26222;&#25289;&#26031;&#35889;&#23450;&#20041;DPP&#26680;&#30697;&#38453;&#65292;&#24182;&#20351;&#29992;&#36712;&#36857;&#20013;&#30340;&#26399;&#26395;&#27169;&#25968;&#20316;&#20026;&#30446;&#26631;&#65292;&#26469;&#25429;&#25417;&#21644;&#22686;&#24378;&#22810;&#26679;&#24615;&#21644;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning rich skills through temporal abstractions without supervision of external rewards is at the frontier of Reinforcement Learning research. Existing works mainly fall into two distinctive categories: variational and Laplacian-based skill (a.k.a., option) discovery. The former maximizes the diversity of the discovered options through a mutual information loss but overlooks coverage of the state space, while the latter focuses on improving the coverage of options by increasing connectivity during exploration, but does not consider diversity. In this paper, we propose a unified framework that quantifies diversity and coverage through a novel use of the Determinantal Point Process (DPP) and enables unsupervised option discovery explicitly optimizing both objectives. Specifically, we define the DPP kernel matrix with the Laplacian spectrum of the state transition graph and use the expected mode number in the trajectories as the objective to capture and enhance both diversity and cover
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#36716;&#31227;&#20889;&#19979;&#65292;&#26399;&#26395;&#27169;&#22411;&#22312;&#20004;&#20010;&#20998;&#24067;&#19978;&#24615;&#33021;&#23384;&#22312;&#21333;&#35843;&#20851;&#31995;&#30340;&#26465;&#20214;&#65292;&#21033;&#29992;&#23725;&#27491;&#21017;&#21270;&#36890;&#29992;&#32447;&#24615;&#27169;&#22411;&#35777;&#26126;&#20102;&#24179;&#26041;&#35823;&#24046;&#30340;&#31934;&#30830;&#28176;&#36817;&#32447;&#24615;&#20851;&#31995;&#21644;&#35823;&#20998;&#31867;&#35823;&#24046;&#30340;&#21333;&#35843;&#20851;&#31995;&#65292;&#20197;&#21450;&#32447;&#24615;&#36870;&#38382;&#39064;&#30340;&#36817;&#20284;&#32447;&#24615;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2210.11589</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#27491;&#21017;&#21270;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#21333;&#35843;&#39118;&#38505;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Monotonic Risk Relationships under Distribution Shifts for Regularized Risk Minimization. (arXiv:2210.11589v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#36716;&#31227;&#20889;&#19979;&#65292;&#26399;&#26395;&#27169;&#22411;&#22312;&#20004;&#20010;&#20998;&#24067;&#19978;&#24615;&#33021;&#23384;&#22312;&#21333;&#35843;&#20851;&#31995;&#30340;&#26465;&#20214;&#65292;&#21033;&#29992;&#23725;&#27491;&#21017;&#21270;&#36890;&#29992;&#32447;&#24615;&#27169;&#22411;&#35777;&#26126;&#20102;&#24179;&#26041;&#35823;&#24046;&#30340;&#31934;&#30830;&#28176;&#36817;&#32447;&#24615;&#20851;&#31995;&#21644;&#35823;&#20998;&#31867;&#35823;&#24046;&#30340;&#21333;&#35843;&#20851;&#31995;&#65292;&#20197;&#21450;&#32447;&#24615;&#36870;&#38382;&#39064;&#30340;&#36817;&#20284;&#32447;&#24615;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#36890;&#24120;&#24212;&#29992;&#20110;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#20998;&#31867;&#21644;&#20449;&#21495;&#37325;&#24314;&#38382;&#39064;&#20013;&#65292;&#36229;&#20986;&#20998;&#24067;&#30340;&#24615;&#33021;&#19982;&#20869;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#24378;&#28872;&#32447;&#24615;&#30456;&#20851;&#12290;&#22914;&#26524;&#23384;&#22312;&#36825;&#31181;&#20851;&#31995;&#25110;&#26356;&#19968;&#33324;&#30340;&#21333;&#35843;&#20851;&#31995;&#65292;&#23558;&#20135;&#29983;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#20363;&#22914;&#65292;&#23427;&#20801;&#35768;&#23558;&#19968;&#20010;&#20998;&#24067;&#19978;&#30340;&#24615;&#33021;&#20248;&#21270;&#20316;&#20026;&#21478;&#19968;&#20010;&#20998;&#24067;&#19978;&#24615;&#33021;&#30340;&#20195;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20004;&#20010;&#20998;&#24067;&#19978;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#39044;&#26399;&#23384;&#22312;&#21333;&#35843;&#20851;&#31995;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#19979;&#65292;&#35777;&#26126;&#20102;&#23725;&#27491;&#21017;&#21270;&#36890;&#29992;&#32447;&#24615;&#27169;&#22411;&#30340;&#24179;&#26041;&#35823;&#24046;&#30340;&#31934;&#30830;&#28176;&#36817;&#32447;&#24615;&#20851;&#31995;&#21644;&#35823;&#20998;&#31867;&#35823;&#24046;&#30340;&#21333;&#35843;&#20851;&#31995;&#65292;&#20197;&#21450;&#32447;&#24615;&#36870;&#38382;&#39064;&#30340;&#36817;&#20284;&#32447;&#24615;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning systems are often applied to data that is drawn from a different distribution than the training distribution. Recent work has shown that for a variety of classification and signal reconstruction problems, the out-of-distribution performance is strongly linearly correlated with the in-distribution performance. If this relationship or more generally a monotonic one holds, it has important consequences. For example, it allows to optimize performance on one distribution as a proxy for performance on the other. In this paper, we study conditions under which a monotonic relationship between the performances of a model on two distributions is expected. We prove an exact asymptotic linear relation for squared error and a monotonic relation for misclassification error for ridge-regularized general linear models under covariate shift, as well as an approximate linear relation for linear inverse problems.
&lt;/p&gt;</description></item><item><title>&#22312;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#21152;&#20837;&#39044;&#22788;&#29702;&#22120;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#23545;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#38450;&#24481;&#33021;&#21147;&#12290;&#39044;&#22788;&#29702;&#22120;&#24341;&#20837;&#36755;&#20837;&#31354;&#38388;&#30340;&#19981;&#21464;&#24615;&#65292;&#25915;&#20987;&#32773;&#38656;&#35201;&#22823;&#37327;&#30340;&#26597;&#35810;&#25165;&#33021;&#37325;&#26032;&#21457;&#29616;&#25110;&#20811;&#26381;&#36825;&#31181;&#19981;&#21464;&#24615;&#12290;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#39044;&#22788;&#29702;&#22120;&#24182;&#21033;&#29992;&#25552;&#21462;&#30340;&#20449;&#24687;&#25915;&#20987;&#31471;&#21040;&#31471;&#31995;&#32479;&#21487;&#20197;&#32469;&#36807;&#36825;&#20123;&#39044;&#22788;&#29702;&#22120;&#30340;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2210.03297</link><description>&lt;p&gt;
&#39044;&#22788;&#29702;&#22120;&#24456;&#37325;&#35201;&#65281;&#23545;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#29616;&#23454;&#20915;&#31574;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Preprocessors Matter! Realistic Decision-Based Attacks on Machine Learning Systems. (arXiv:2210.03297v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03297
&lt;/p&gt;
&lt;p&gt;
&#22312;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#21152;&#20837;&#39044;&#22788;&#29702;&#22120;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#23545;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#38450;&#24481;&#33021;&#21147;&#12290;&#39044;&#22788;&#29702;&#22120;&#24341;&#20837;&#36755;&#20837;&#31354;&#38388;&#30340;&#19981;&#21464;&#24615;&#65292;&#25915;&#20987;&#32773;&#38656;&#35201;&#22823;&#37327;&#30340;&#26597;&#35810;&#25165;&#33021;&#37325;&#26032;&#21457;&#29616;&#25110;&#20811;&#26381;&#36825;&#31181;&#19981;&#21464;&#24615;&#12290;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#39044;&#22788;&#29702;&#22120;&#24182;&#21033;&#29992;&#25552;&#21462;&#30340;&#20449;&#24687;&#25915;&#20987;&#31471;&#21040;&#31471;&#31995;&#32479;&#21487;&#20197;&#32469;&#36807;&#36825;&#20123;&#39044;&#22788;&#29702;&#22120;&#30340;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20915;&#31574;&#30340;&#25915;&#20987;&#36890;&#36807;&#21482;&#36827;&#34892;&#30828;&#26631;&#31614;&#26597;&#35810;&#26469;&#26500;&#24314;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;(ML)&#27169;&#22411;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#36825;&#20123;&#25915;&#20987;&#20027;&#35201;&#30452;&#25509;&#24212;&#29992;&#20110;&#29420;&#31435;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;ML&#27169;&#22411;&#21482;&#26159;&#26356;&#22823;&#23398;&#20064;&#31995;&#32479;&#30340;&#19968;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#22312;&#20998;&#31867;&#22120;&#21069;&#28155;&#21152;&#19968;&#20010;&#39044;&#22788;&#29702;&#22120;&#65292;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#23545;&#25915;&#20987;&#39044;&#27979;&#27969;&#27700;&#32447;&#30340;&#25928;&#26524;&#35201;&#27604;&#23545;&#21333;&#29420;&#30340;&#27169;&#22411;&#25915;&#20987;&#39640;&#25928;7&#20493;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#36825;&#31181;&#24046;&#24322;&#30340;&#21407;&#22240;&#26159;&#22823;&#22810;&#25968;&#39044;&#22788;&#29702;&#22120;&#24341;&#20837;&#20102;&#36755;&#20837;&#31354;&#38388;&#30340;&#26576;&#31181;&#19981;&#21464;&#24615;&#12290;&#22240;&#27492;&#65292;&#19981;&#30693;&#36947;&#36825;&#31181;&#19981;&#21464;&#24615;&#30340;&#25915;&#20987;&#24517;&#28982;&#20250;&#28010;&#36153;&#22823;&#37327;&#30340;&#26597;&#35810;&#26469;&#37325;&#26032;&#21457;&#29616;&#25110;&#20811;&#26381;&#23427;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#25216;&#26415;&#26469;(i)&#36870;&#21521;&#24037;&#31243;&#39044;&#22788;&#29702;&#22120;&#65292;&#28982;&#21518;(ii)&#21033;&#29992;&#25552;&#21462;&#30340;&#20449;&#24687;&#25915;&#20987;&#31471;&#21040;&#31471;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#39044;&#22788;&#29702;&#22120;&#25552;&#21462;&#26041;&#27861;&#21482;&#38656;&#35201;&#20960;&#30334;&#20010;&#26597;&#35810;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#39044;&#22788;&#29702;&#22120;&#24863;&#30693;&#25915;&#20987;&#21487;&#20197;&#26377;&#25928;&#32469;&#36807;&#36825;&#20123;&#39044;&#22788;&#29702;&#22120;&#30340;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-based attacks construct adversarial examples against a machine learning (ML) model by making only hard-label queries. These attacks have mainly been applied directly to standalone neural networks. However, in practice, ML models are just one component of a larger learning system. We find that by adding a single preprocessor in front of a classifier, state-of-the-art query-based attacks are up to 7$\times$ less effective at attacking a prediction pipeline than at attacking the model alone. We explain this discrepancy by the fact that most preprocessors introduce some notion of invariance to the input space. Hence, attacks that are unaware of this invariance inevitably waste a large number of queries to re-discover or overcome it. We, therefore, develop techniques to (i) reverse-engineer the preprocessor and then (ii) use this extracted information to attack the end-to-end system. Our preprocessors extraction method requires only a few hundred queries, and our preprocessor-aware
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#21512;&#20316;&#36873;&#39033;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22810;&#20010;&#26234;&#33021;&#20307;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#30340;&#39044;&#26399;&#35206;&#30422;&#26102;&#38388;&#26469;&#26500;&#24314;&#22810;&#26234;&#33021;&#20307;&#36873;&#39033;&#65292;&#24182;&#25552;&#20986;&#20102;&#37319;&#29992;&#36825;&#20123;&#36873;&#39033;&#30340;&#26032;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2210.03269</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#35206;&#30422;&#25216;&#33021;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Deep Covering Skill Discovery. (arXiv:2210.03269v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03269
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#21512;&#20316;&#36873;&#39033;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22810;&#20010;&#26234;&#33021;&#20307;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#30340;&#39044;&#26399;&#35206;&#30422;&#26102;&#38388;&#26469;&#26500;&#24314;&#22810;&#26234;&#33021;&#20307;&#36873;&#39033;&#65292;&#24182;&#25552;&#20986;&#20102;&#37319;&#29992;&#36825;&#20123;&#36873;&#39033;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#25216;&#33021;&#65288;&#21363;&#36873;&#39033;&#65289;&#21487;&#20197;&#22823;&#22823;&#21152;&#24555;&#25506;&#32034;&#36895;&#24230;&#65292;&#23588;&#20854;&#26159;&#24403;&#21482;&#26377;&#31232;&#30095;&#30340;&#22870;&#21169;&#20449;&#21495;&#21487;&#29992;&#26102;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#38024;&#23545;&#20010;&#20307;&#26234;&#33021;&#20307;&#30340;&#36873;&#39033;&#21457;&#29616;&#26041;&#27861;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#23578;&#26410;&#32771;&#34385;&#22914;&#20309;&#21457;&#29616;&#21327;&#20316;&#36873;&#39033;&#65292;&#20197;&#21327;&#35843;&#22810;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#24182;&#40723;&#21169;&#23427;&#20204;&#35775;&#38382;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#20013;&#26410;&#24320;&#21457;&#30340;&#21306;&#22495;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#35206;&#30422;&#36873;&#39033;&#21457;&#29616;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22810;&#20010;&#26234;&#33021;&#20307;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#30340;&#39044;&#26399;&#35206;&#30422;&#26102;&#38388;&#26469;&#26500;&#24314;&#22810;&#26234;&#33021;&#20307;&#36873;&#39033;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#26469;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#20013;&#37319;&#29992;&#22810;&#26234;&#33021;&#20307;&#36873;&#39033;&#12290;&#23454;&#38469;&#19978;&#65292;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#36890;&#24120;&#21487;&#20197;&#20998;&#20026;&#19968;&#20123;&#23376;&#20219;&#21153;&#65292;&#27599;&#20010;&#23376;&#20219;&#21153;&#21487;&#20197;&#30001;&#19968;&#20010;&#23376;&#22242;&#20307;&#30340;&#26234;&#33021;&#20307;&#23436;&#25104;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26694;&#26550;&#39318;&#20808;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25214;&#21040;&#21327;&#20316;&#26234;&#33021;&#20307;&#23376;&#22242;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of skills (a.k.a., options) can greatly accelerate exploration in reinforcement learning, especially when only sparse reward signals are available. While option discovery methods have been proposed for individual agents, in multi-agent reinforcement learning settings, discovering collaborative options that can coordinate the behavior of multiple agents and encourage them to visit the under-explored regions of their joint state space has not been considered. In this case, we propose Multi-agent Deep Covering Option Discovery, which constructs the multi-agent options through minimizing the expected cover time of the multiple agents' joint state space. Also, we propose a novel framework to adopt the multi-agent options in the MARL process. In practice, a multi-agent task can usually be divided into some sub-tasks, each of which can be completed by a sub-group of the agents. Therefore, our algorithm framework first leverages an attention mechanism to find collaborative agent sub-gr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#24182;&#34892;&#21644;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#35774;&#35745;&#20102;&#24182;&#34892;&#24615;&#20998;&#31867;&#26041;&#27861;&#21518;&#65292;&#30740;&#31350;&#20102;&#21508;&#31181;GNN&#27169;&#22411;&#12289;&#20219;&#21153;&#12289;&#36719;&#20214;&#26694;&#26550;&#21644;&#30828;&#20214;&#21152;&#36895;&#22120;&#20013;&#30340;&#24182;&#34892;&#24615;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#30456;&#20851;&#24352;&#37327;&#30340;&#31232;&#30095;&#24615;/&#23494;&#24230;&#12290;</title><link>http://arxiv.org/abs/2205.09702</link><description>&lt;p&gt;
&#24182;&#34892;&#21644;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#28145;&#20837;&#24182;&#21457;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis. (arXiv:2205.09702v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09702
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#24182;&#34892;&#21644;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#35774;&#35745;&#20102;&#24182;&#34892;&#24615;&#20998;&#31867;&#26041;&#27861;&#21518;&#65292;&#30740;&#31350;&#20102;&#21508;&#31181;GNN&#27169;&#22411;&#12289;&#20219;&#21153;&#12289;&#36719;&#20214;&#26694;&#26550;&#21644;&#30828;&#20214;&#21152;&#36895;&#22120;&#20013;&#30340;&#24182;&#34892;&#24615;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#30456;&#20851;&#24352;&#37327;&#30340;&#31232;&#30095;&#24615;/&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24378;&#22823;&#30340;&#24037;&#20855;&#20043;&#19968;&#12290;&#23427;&#20204;&#24120;&#24120;&#22312;&#26080;&#32467;&#26500;&#32593;&#32476;&#19978;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#65292;&#22914;&#33410;&#28857;&#20998;&#31867;&#12289;&#22270;&#20998;&#31867;&#25110;&#38142;&#25509;&#39044;&#27979;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;GNNs&#30340;&#25512;&#29702;&#21644;&#35757;&#32451;&#37117;&#38750;&#24120;&#22797;&#26434;&#65292;&#23427;&#20204;&#29420;&#29305;&#22320;&#23558;&#19981;&#35268;&#21017;&#22270;&#22788;&#29702;&#30340;&#29305;&#24615;&#19982;&#23494;&#38598;&#21644;&#35268;&#21017;&#35745;&#31639;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#22797;&#26434;&#24615;&#20351;&#24471;&#22312;&#29616;&#20195;&#22823;&#35268;&#27169;&#24182;&#34892;&#26550;&#26500;&#19978;&#39640;&#25928;&#25191;&#34892;GNNs&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;GNNs&#20013;&#30340;&#24182;&#34892;&#24615;&#20998;&#31867;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#25968;&#25454;&#24182;&#34892;&#24615;&#21644;&#27169;&#22411;&#24182;&#34892;&#24615;&#65292;&#20197;&#21450;&#19981;&#21516;&#24418;&#24335;&#30340;&#27969;&#27700;&#32447;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#20998;&#31867;&#26041;&#27861;&#26469;&#30740;&#31350;&#20247;&#22810;GNN&#27169;&#22411;&#12289;GNN&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12289;&#36719;&#20214;&#26694;&#26550;&#25110;&#30828;&#20214;&#21152;&#36895;&#22120;&#20013;&#30340;&#24182;&#34892;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#24037;&#20316;&#28145;&#24230;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#36890;&#20449;&#37327;&#21644;&#21516;&#27493;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#30456;&#20851;&#24352;&#37327;&#30340;&#31232;&#30095;&#24615;/&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are among the most powerful tools in deep learning. They routinely solve complex problems on unstructured networks, such as node classification, graph classification, or link prediction, with high accuracy. However, both inference and training of GNNs are complex, and they uniquely combine the features of irregular graph processing with dense and regular computations. This complexity makes it very challenging to execute GNNs efficiently on modern massively parallel architectures. To alleviate this, we first design a taxonomy of parallelism in GNNs, considering data and model parallelism, and different forms of pipelining. Then, we use this taxonomy to investigate the amount of parallelism in numerous GNN models, GNN-driven machine learning tasks, software frameworks, or hardware accelerators. We use the work-depth model, and we also assess communication volume and synchronization. We specifically focus on the sparsity/density of the associated tensors, in o
&lt;/p&gt;</description></item><item><title>Torchhd&#26159;&#19968;&#20010;&#39640;&#24615;&#33021;&#30340;&#24320;&#28304;Python&#24211;&#65292;&#26088;&#22312;&#25903;&#25345;&#36229;&#32500;&#35745;&#31639;&#21644;&#21521;&#37327;&#31526;&#21495;&#20307;&#31995;&#30740;&#31350;&#12290;&#23427;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#21151;&#33021;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#30028;&#38754;&#65292;&#24182;&#19988;&#33021;&#22815;&#20351;&#23454;&#39564;&#36816;&#34892;&#36895;&#24230;&#25552;&#39640;&#22810;&#36798;100&#20493;&#12290;</title><link>http://arxiv.org/abs/2205.09208</link><description>&lt;p&gt;
Torchhd:&#19968;&#31181;&#25903;&#25345;&#36229;&#32500;&#35745;&#31639;&#21644;&#21521;&#37327;&#31526;&#21495;&#20307;&#31995;&#30740;&#31350;&#30340;&#24320;&#28304;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
Torchhd: An Open Source Python Library to Support Research on Hyperdimensional Computing and Vector Symbolic Architectures. (arXiv:2205.09208v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09208
&lt;/p&gt;
&lt;p&gt;
Torchhd&#26159;&#19968;&#20010;&#39640;&#24615;&#33021;&#30340;&#24320;&#28304;Python&#24211;&#65292;&#26088;&#22312;&#25903;&#25345;&#36229;&#32500;&#35745;&#31639;&#21644;&#21521;&#37327;&#31526;&#21495;&#20307;&#31995;&#30740;&#31350;&#12290;&#23427;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#21151;&#33021;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#30028;&#38754;&#65292;&#24182;&#19988;&#33021;&#22815;&#20351;&#23454;&#39564;&#36816;&#34892;&#36895;&#24230;&#25552;&#39640;&#22810;&#36798;100&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#32500;&#35745;&#31639;&#65288;HD&#65289;&#65292;&#20063;&#34987;&#31216;&#20026;&#21521;&#37327;&#31526;&#21495;&#20307;&#31995;&#65288;VSA&#65289;&#65292;&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#39640;&#32500;&#21521;&#37327;&#31354;&#38388;&#30340;&#24615;&#36136;&#36827;&#34892;&#20998;&#24067;&#24335;&#34920;&#31034;&#35745;&#31639;&#30340;&#26694;&#26550;&#12290;&#31185;&#23398;&#30028;&#23545;&#20110;&#32858;&#38598;&#21644;&#20256;&#25773;&#36825;&#20010;&#29305;&#21035;&#22810;&#23398;&#31185;&#39046;&#22495;&#30340;&#30740;&#31350;&#30340;&#25215;&#35834;&#23545;&#20110;&#20854;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#21152;&#20837;&#36825;&#20123;&#21162;&#21147;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;Torchhd&#65292;&#19968;&#20010;&#39640;&#24615;&#33021;&#30340;&#36229;&#32500;/VSA&#30340;&#24320;&#28304;Python&#24211;&#12290;Torchhd&#26088;&#22312;&#20351;&#36229;&#32500;/VSA&#26356;&#26131;&#20110;&#20351;&#29992;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#24320;&#21457;&#25552;&#20379;&#39640;&#25928;&#30340;&#22522;&#30784;&#12290;&#36825;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#24211;&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#36229;&#32500;/VSA&#21151;&#33021;&#65292;&#28165;&#26224;&#30340;&#25991;&#26723;&#21644;&#26469;&#33258;&#30693;&#21517;&#20986;&#29256;&#29289;&#30340;&#23454;&#29616;&#31034;&#20363;&#12290;&#23558;&#20844;&#24320;&#21487;&#29992;&#30340;&#20195;&#30721;&#19982;&#20854;&#30456;&#24212;&#30340;Torchhd&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#65292;&#23454;&#39564;&#21487;&#20197;&#36816;&#34892;&#36895;&#24230;&#25552;&#39640;&#22810;&#36798;100&#20493;&#12290;Torchhd&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#33719;&#21462;&#65306;https://github.com/hyperdimensional-computing/tor
&lt;/p&gt;
&lt;p&gt;
Hyperdimensional computing (HD), also known as vector symbolic architectures (VSA), is a framework for computing with distributed representations by exploiting properties of random high-dimensional vector spaces. The commitment of the scientific community to aggregate and disseminate research in this particularly multidisciplinary area has been fundamental for its advancement. Joining these efforts, we present Torchhd, a high-performance open source Python library for HD/VSA. Torchhd seeks to make HD/VSA more accessible and serves as an efficient foundation for further research and application development. The easy-to-use library builds on top of PyTorch and features state-of-the-art HD/VSA functionality, clear documentation, and implementation examples from well-known publications. Comparing publicly available code with their corresponding Torchhd implementation shows that experiments can run up to 100x faster. Torchhd is available at: https://github.com/hyperdimensional-computing/tor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Multi-scale Attention Normalizing Flow(MANF)&#30340;&#38750;&#33258;&#22238;&#24402;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#21644;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#65292;&#23454;&#29616;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#31934;&#30830;&#20998;&#24067;&#24314;&#27169;&#65292;&#24182;&#19988;&#36991;&#20813;&#20102;&#32047;&#31215;&#35823;&#24046;&#30340;&#24433;&#21709;&#21644;&#22686;&#21152;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.07493</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#27969;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Attention Flow for Probabilistic Time Series Forecasting. (arXiv:2205.07493v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.07493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Multi-scale Attention Normalizing Flow(MANF)&#30340;&#38750;&#33258;&#22238;&#24402;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#21644;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#65292;&#23454;&#29616;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#31934;&#30830;&#20998;&#24067;&#24314;&#27169;&#65292;&#24182;&#19988;&#36991;&#20813;&#20102;&#32047;&#31215;&#35823;&#24046;&#30340;&#24433;&#21709;&#21644;&#22686;&#21152;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#23454;&#29992;&#30340;&#20219;&#21153;&#12290;&#19968;&#26041;&#38754;&#65292;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#26377;&#25928;&#22320;&#25429;&#25417;&#20132;&#20114;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#36328;&#24207;&#21015;&#30456;&#20851;&#24615;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#20998;&#24067;&#24314;&#27169;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#24212;&#32771;&#34385;&#22914;&#20309;&#26356;&#20934;&#30830;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20869;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#24314;&#27169;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#33258;&#22238;&#24402;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#31216;&#20026;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#24402;&#19968;&#21270;&#27969;&#65288;MANF&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#25972;&#21512;&#20102;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#21644;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#65292;&#32780;&#22810;&#21464;&#37327;&#25968;&#25454;&#20998;&#24067;&#30001;&#26377;&#26465;&#20214;&#30340;&#24402;&#19968;&#21270;&#27969;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#19982;&#33258;&#22238;&#24402;&#24314;&#27169;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36991;&#20813;&#20102;&#32047;&#31215;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#19988;&#19981;&#22686;&#21152;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35768;&#22810;&#27969;&#34892;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The probability prediction of multivariate time series is a notoriously challenging but practical task. On the one hand, the challenge is how to effectively capture the cross-series correlations between interacting time series, to achieve accurate distribution modeling. On the other hand, we should consider how to capture the contextual information within time series more accurately to model multivariate temporal dynamics of time series. In this work, we proposed a novel non-autoregressive deep learning model, called Multi-scale Attention Normalizing Flow(MANF), where we integrate multi-scale attention and relative position information and the multivariate data distribution is represented by the conditioned normalizing flow. Additionally, compared with autoregressive modeling methods, our model avoids the influence of cumulative error and does not increase the time complexity. Extensive experiments demonstrate that our model achieves state-of-the-art performance on many popular multiva
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#31070;&#32463;&#20803;&#27963;&#21160;&#30340;&#21464;&#21270;&#19982;&#36830;&#25509;&#21040;&#19979;&#19968;&#23618;&#31070;&#32463;&#20803;&#30340;&#26435;&#37325;&#21464;&#21270;&#20043;&#38388;&#30340;&#20934;&#30830;&#23545;&#20598;&#20851;&#31995;&#12290;&#36890;&#36807;&#36825;&#31181;&#23545;&#20598;&#24615;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#36755;&#20837;&#25968;&#25454;&#30340;&#21464;&#21270;&#26144;&#23556;&#21040;&#23545;&#24212;&#30340;&#26435;&#37325;&#21464;&#21270;&#65292;&#24182;&#21457;&#29616;&#27867;&#21270;&#25439;&#22833;&#21487;&#20197;&#36890;&#36807;&#35299;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;Hessian&#30697;&#38453;&#30340;&#29305;&#24449;&#26041;&#21521;&#30340;&#20960;&#20309;&#22240;&#23376;&#30340;&#20056;&#31215;&#26469;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2203.10736</link><description>&lt;p&gt;
&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27963;&#21160;-&#26435;&#37325;&#23545;&#20598;&#24615;&#65306;&#27867;&#21270;&#24615;&#30340;&#20960;&#20309;&#20915;&#23450;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
The activity-weight duality in feed forward neural networks: The geometric determinants of generalization. (arXiv:2203.10736v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.10736
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#31070;&#32463;&#20803;&#27963;&#21160;&#30340;&#21464;&#21270;&#19982;&#36830;&#25509;&#21040;&#19979;&#19968;&#23618;&#31070;&#32463;&#20803;&#30340;&#26435;&#37325;&#21464;&#21270;&#20043;&#38388;&#30340;&#20934;&#30830;&#23545;&#20598;&#20851;&#31995;&#12290;&#36890;&#36807;&#36825;&#31181;&#23545;&#20598;&#24615;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#36755;&#20837;&#25968;&#25454;&#30340;&#21464;&#21270;&#26144;&#23556;&#21040;&#23545;&#24212;&#30340;&#26435;&#37325;&#21464;&#21270;&#65292;&#24182;&#21457;&#29616;&#27867;&#21270;&#25439;&#22833;&#21487;&#20197;&#36890;&#36807;&#35299;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;Hessian&#30697;&#38453;&#30340;&#29305;&#24449;&#26041;&#21521;&#30340;&#20960;&#20309;&#22240;&#23376;&#30340;&#20056;&#31215;&#26469;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#22522;&#26412;&#30340;&#38382;&#39064;&#26159;&#27867;&#21270;&#24615;&#12290;&#22312;&#20855;&#26377;&#22823;&#37327;&#26435;&#37325;&#65288;&#21442;&#25968;&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#25214;&#21040;&#24456;&#22810;&#35299;&#26469;&#24456;&#22909;&#22320;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#12290;&#20851;&#38190;&#38382;&#39064;&#26159;&#21738;&#20010;&#35299;&#33021;&#22815;&#25551;&#36848;&#19981;&#22312;&#35757;&#32451;&#38598;&#20013;&#30340;&#27979;&#35797;&#25968;&#25454;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#22312;&#20219;&#20309;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#23494;&#38598;&#36830;&#25509;&#23618;&#20013;&#65292;&#32473;&#23450;&#23618;&#31070;&#32463;&#20803;&#27963;&#21160;&#30340;&#21464;&#21270;&#19982;&#36830;&#25509;&#21040;&#19979;&#19968;&#23618;&#31070;&#32463;&#20803;&#30340;&#26435;&#37325;&#21464;&#21270;&#20043;&#38388;&#30340;&#30830;&#20999;&#23545;&#20598;&#65288;&#31561;&#20215;&#65289;&#20851;&#31995;&#30340;&#21457;&#29616;&#12290;&#27963;&#21160;-&#26435;&#37325;&#65288;A-W&#65289;&#23545;&#20598;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#36755;&#20837;&#65288;&#25968;&#25454;&#65289;&#30340;&#21464;&#21270;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#23545;&#20598;&#26435;&#37325;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#26144;&#23556;&#65292;&#25105;&#20204;&#34920;&#26126;&#27867;&#21270;&#25439;&#22833;&#21487;&#20197;&#20998;&#35299;&#20026;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#30340;&#35299;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;Hessian&#30697;&#38453;&#30340;&#19981;&#21516;&#29305;&#24449;&#26041;&#21521;&#30340;&#36129;&#29486;&#20043;&#21644;&#12290;&#32473;&#23450;&#29305;&#24449;&#26041;&#21521;&#30340;&#36129;&#29486;&#26159;&#20004;&#20010;&#20960;&#20309;&#22240;&#23376;&#65288;&#34892;&#21015;&#24335;&#65289;&#30340;&#20056;&#31215;&#65306;&#23574;&#38160;&#24230;
&lt;/p&gt;
&lt;p&gt;
One of the fundamental problems in machine learning is generalization. In neural network models with a large number of weights (parameters), many solutions can be found to fit the training data equally well. The key question is which solution can describe testing data not in the training set. Here, we report the discovery of an exact duality (equivalence) between changes in activities in a given layer of neurons and changes in weights that connect to the next layer of neurons in a densely connected layer in any feed forward neural network. The activity-weight (A-W) duality allows us to map variations in inputs (data) to variations of the corresponding dual weights. By using this mapping, we show that the generalization loss can be decomposed into a sum of contributions from different eigen-directions of the Hessian matrix of the loss function at the solution in weight space. The contribution from a given eigen-direction is the product of two geometric factors (determinants): the sharpn
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#22312;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#35745;&#31639;&#22810;&#26234;&#33021;&#20307;&#25216;&#33021;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#24615;&#25506;&#32034;&#34892;&#20026;&#26469;&#25913;&#21892;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#30340;&#36830;&#36890;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.08227</link><description>&lt;p&gt;
&#20351;&#29992;&#22240;&#23376;&#22270;&#23398;&#20064;&#34920;&#26684;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26234;&#33021;&#20307;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Learning Multi-agent Skills for Tabular Reinforcement Learning using Factor Graphs. (arXiv:2201.08227v3 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.08227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#22312;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#35745;&#31639;&#22810;&#26234;&#33021;&#20307;&#25216;&#33021;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#24615;&#25506;&#32034;&#34892;&#20026;&#26469;&#25913;&#21892;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#30340;&#36830;&#36890;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#33021;&#21457;&#29616;&#34987;&#24320;&#21457;&#29992;&#20110;&#25913;&#21892;&#21333;&#26234;&#33021;&#20307;&#24773;&#26223;&#20013;&#31232;&#30095;&#22870;&#21169;&#20449;&#21495;&#30340;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#33021;&#21147;&#65292;&#36890;&#36807;&#36830;&#25509;&#29366;&#24577;&#36716;&#31227;&#22270;&#30340;Fiedler&#21521;&#37327;&#25552;&#20379;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#26368;&#36828;&#30340;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#33021;&#21457;&#29616;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#25512;&#24191;&#21040;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#65292;&#22240;&#20026;&#31995;&#32479;&#20013;&#30340;&#26234;&#33021;&#20307;&#25968;&#37327;&#22686;&#21152;&#65292;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30740;&#31350;&#22312;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#37319;&#29992;&#25216;&#33021;&#20173;&#28982;&#20381;&#36182;&#20110;&#21333;&#26234;&#33021;&#20307;&#25216;&#33021;&#21457;&#29616;&#65292;&#24182;&#26410;&#30452;&#25509;&#21457;&#29616;&#33021;&#22815;&#25913;&#21892;&#26234;&#33021;&#20307;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#36830;&#36890;&#24615;&#30340;&#32852;&#21512;&#25216;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#21512;&#20316;&#24615;&#25506;&#32034;&#34892;&#20026;&#22312;&#26234;&#33021;&#20307;&#20043;&#38388;&#30452;&#25509;&#35745;&#31639;&#22810;&#26234;&#33021;&#20307;&#25216;&#33021;&#30340;&#21487;&#34892;&#24615;&#65292;&#21516;&#26102;&#20173;&#28982;&#20139;&#21463;&#20998;&#35299;&#30340;&#20415;&#21033;&#24615;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#36924;&#36817;&#20026;Kronecker&#22270;&#8212;&#8212;Kronecker
&lt;/p&gt;
&lt;p&gt;
Covering skill (a.k.a., option) discovery has been developed to improve the exploration of reinforcement learning in single-agent scenarios with sparse reward signals, through connecting the most distant states in the embedding space provided by the Fiedler vector of the state transition graph. However, these option discovery methods cannot be directly extended to multi-agent scenarios, since the joint state space grows exponentially with the number of agents in the system. Thus, existing researches on adopting options in multi-agent scenarios still rely on single-agent option discovery and fail to directly discover the joint options that can improve the connectivity of the joint state space of agents. In this paper, we show that it is indeed possible to directly compute multi-agent options with collaborative exploratory behaviors among the agents, while still enjoying the ease of decomposition. Our key idea is to approximate the joint state space as a Kronecker graph -- the Kronecker 
&lt;/p&gt;</description></item><item><title>CALDA&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#23545;&#25239;&#23398;&#20064;&#30340;&#21407;&#21017;&#26469;&#25552;&#39640;&#22810;&#28304;&#26102;&#38388;&#24207;&#21015;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#23545;&#40784;&#29305;&#24449;&#34920;&#31034;&#21644;&#21033;&#29992;&#36328;&#22495;&#26631;&#31614;&#20449;&#24687;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2109.14778</link><description>&lt;p&gt;
CALDA:&#25913;&#36827;&#22810;&#28304;&#26102;&#38388;&#24207;&#21015;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#19982;&#23545;&#27604;&#23545;&#25239;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CALDA: Improving Multi-Source Time Series Domain Adaptation with Contrastive Adversarial Learning. (arXiv:2109.14778v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.14778
&lt;/p&gt;
&lt;p&gt;
CALDA&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#23545;&#25239;&#23398;&#20064;&#30340;&#21407;&#21017;&#26469;&#25552;&#39640;&#22810;&#28304;&#26102;&#38388;&#24207;&#21015;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#23545;&#40784;&#29305;&#24449;&#34920;&#31034;&#21644;&#21033;&#29992;&#36328;&#22495;&#26631;&#31614;&#20449;&#24687;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#29992;&#20110;&#25913;&#36827;&#25968;&#25454;&#20016;&#23500;&#65288;&#30446;&#26631;&#65289;&#39046;&#22495;&#20013;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#26080;&#27861;&#33719;&#24471;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#65292;&#20294;&#21487;&#20197;&#22312;&#30456;&#20851;&#65288;&#28304;&#65289;&#39046;&#22495;&#20013;&#25214;&#21040;&#12290;&#22312;&#20855;&#26377;&#20803;&#39046;&#22495;&#20449;&#24687;&#65288;&#22914;&#26631;&#31614;&#20998;&#24067;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#24369;&#30417;&#30563;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;CALDA&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;CALDA&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#23545;&#25239;&#23398;&#20064;&#30340;&#21407;&#21017;&#65292;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;MS-UDA&#65289;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#25903;&#25345;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#31867;&#20284;&#65292;CALDA&#21033;&#29992;&#23545;&#25239;&#23398;&#20064;&#26469;&#23545;&#40784;&#28304;&#21644;&#30446;&#26631;&#29305;&#24449;&#34920;&#31034;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;CALDA&#36824;&#21033;&#29992;&#20102;&#36328;&#22495;&#30340;&#36328;&#28304;&#26631;&#31614;&#20449;&#24687;&#12290;CALDA&#23558;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#31034;&#20363;&#24444;&#27492;&#38752;&#36817;&#65292;&#32780;&#23558;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#31034;&#20363;&#20998;&#24320;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#37325;&#26032;&#22609;&#36896;&#31354;&#38388;&#12290;&#19982;&#20197;&#21069;&#30340;&#23545;&#27604;&#33258;&#36866;&#24212;&#26041;&#27861;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (UDA) provides a strategy for improving machine learning performance in data-rich (target) domains where ground truth labels are inaccessible but can be found in related (source) domains. In cases where meta-domain information such as label distributions is available, weak supervision can further boost performance. We propose a novel framework, CALDA, to tackle these two problems. CALDA synergistically combines the principles of contrastive learning and adversarial learning to robustly support multi-source UDA (MS-UDA) for time series data. Similar to prior methods, CALDA utilizes adversarial learning to align source and target feature representations. Unlike prior approaches, CALDA additionally leverages cross-source label information across domains. CALDA pulls examples with the same label close to each other, while pushing apart examples with different labels, reshaping the space through contrastive learning. Unlike prior contrastive adaptation methods
&lt;/p&gt;</description></item><item><title>&#8220;&#21516;&#36136;&#24615;&#23545;&#20110;&#33391;&#22909;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#26159;&#21542;&#24517;&#35201;&#30340;&#8221;&#36825;&#19968;&#38382;&#39064;&#22312;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#37325;&#26032;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#26631;&#20934;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#22312;&#19968;&#20123;&#24120;&#29992;&#30340;&#24322;&#36136;&#24615;&#22270;&#19978;&#21487;&#20197;&#23454;&#29616;&#27604;&#31934;&#24515;&#35774;&#35745;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2106.06134</link><description>&lt;p&gt;
&#8220;&#21516;&#36136;&#24615;&#23545;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#24517;&#35201;&#30340;&#21527;&#65311;&#8221;
&lt;/p&gt;
&lt;p&gt;
Is Homophily a Necessity for Graph Neural Networks?. (arXiv:2106.06134v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.06134
&lt;/p&gt;
&lt;p&gt;
&#8220;&#21516;&#36136;&#24615;&#23545;&#20110;&#33391;&#22909;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#26159;&#21542;&#24517;&#35201;&#30340;&#8221;&#36825;&#19968;&#38382;&#39064;&#22312;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#37325;&#26032;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#26631;&#20934;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#22312;&#19968;&#20123;&#24120;&#29992;&#30340;&#24322;&#36136;&#24615;&#22270;&#19978;&#21487;&#20197;&#23454;&#29616;&#27604;&#31934;&#24515;&#35774;&#35745;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#23398;&#20064;&#36866;&#29992;&#20110;&#20247;&#22810;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#34920;&#31034;&#26041;&#38754;&#26174;&#31034;&#20986;&#26497;&#24378;&#30340;&#33021;&#21147;&#12290;&#24403;&#24212;&#29992;&#20110;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#26102;&#65292;&#30001;&#20110;&#21516;&#36136;&#24615;&#20551;&#35774;&#65288;&#8220;&#31867;&#20284;&#30456;&#20114;&#21560;&#24341;&#8221;&#65289;&#65292;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;GNN&#21487;&#20197;&#24456;&#22909;&#22320;&#24037;&#20316;&#65292;&#20294;&#22312;&#24322;&#36136;&#24615;&#22270;&#20013;&#65288;&#36830;&#25509;&#19981;&#30456;&#20284;&#33410;&#28857;&#30340;&#22270;&#65289;&#26080;&#27861;&#27867;&#21270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#26550;&#26500;&#26469;&#20811;&#26381;&#36825;&#31181;&#19982;&#24322;&#36136;&#24615;&#30456;&#20851;&#30340;&#38480;&#21046;&#65292;&#24341;&#29992;&#20102;&#36139;&#24369;&#30340;&#22522;&#20934;&#24615;&#33021;&#20197;&#21450;&#22312;&#19968;&#20123;&#24322;&#36136;&#24615;&#22270;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#26550;&#26500;&#25913;&#36827;&#20316;&#20026;&#35777;&#25454;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#26631;&#20934;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#23454;&#38469;&#19978;&#22312;&#19968;&#20123;&#24120;&#29992;&#30340;&#24322;&#36136;&#24615;&#22270;&#19978;&#21487;&#20197;&#23454;&#29616;&#27604;&#36825;&#20123;&#31934;&#24515;&#35774;&#35745;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#36825;&#28608;&#21169;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#21516;&#36136;&#24615;&#26159;&#21542;&#30495;&#27491;&#23545;&#20110;&#33391;&#22909;&#30340;GNN&#24615;&#33021;&#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#35828;&#27861;&#24182;&#19981;&#23436;&#20840;&#27491;&#30830;&#65292;&#20107;&#23454;&#19978;&#65292;GCN&#21487;&#20197;&#22312;&#24322;&#36136;&#24615;&#22270;&#19978;&#23454;&#29616;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have shown great prowess in learning representations suitable for numerous graph-based machine learning tasks. When applied to semi-supervised node classification, GNNs are widely believed to work well due to the homophily assumption ("like attracts like"), and fail to generalize to heterophilous graphs where dissimilar nodes connect. Recent works design new architectures to overcome such heterophily-related limitations, citing poor baseline performance and new architecture improvements on a few heterophilous graph benchmark datasets as evidence for this notion. In our experiments, we empirically find that standard graph convolutional networks (GCNs) can actually achieve better performance than such carefully designed methods on some commonly used heterophilous graphs. This motivates us to reconsider whether homophily is truly necessary for good GNN performance. We find that this claim is not quite true, and in fact, GCNs can achieve strong performance on h
&lt;/p&gt;</description></item></channel></rss>