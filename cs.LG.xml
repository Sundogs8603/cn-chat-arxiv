<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>3D&#25193;&#25955;&#31574;&#30053;&#65288;DP3&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;3D&#35270;&#35273;&#34920;&#31034;&#30340;&#24378;&#22823;&#24615;&#32467;&#21512;&#21040;&#25193;&#25955;&#31574;&#30053;&#20013;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#23398;&#20064;&#22797;&#26434;&#25216;&#33021;&#25152;&#38656;&#22823;&#37327;&#20154;&#31867;&#28436;&#31034;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.03954</link><description>&lt;p&gt;
3D&#25193;&#25955;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
3D Diffusion Policy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03954
&lt;/p&gt;
&lt;p&gt;
3D&#25193;&#25955;&#31574;&#30053;&#65288;DP3&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;3D&#35270;&#35273;&#34920;&#31034;&#30340;&#24378;&#22823;&#24615;&#32467;&#21512;&#21040;&#25193;&#25955;&#31574;&#30053;&#20013;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#23398;&#20064;&#22797;&#26434;&#25216;&#33021;&#25152;&#38656;&#22823;&#37327;&#20154;&#31867;&#28436;&#31034;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#20026;&#25945;&#25480;&#26426;&#22120;&#20154;&#28789;&#24039;&#25216;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#24335;&#65307;&#28982;&#32780;&#65292;&#23398;&#20064;&#22797;&#26434;&#32780;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#25216;&#33021;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#31867;&#28436;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;3D&#25193;&#25955;&#31574;&#30053;&#65288;DP3&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;3D&#35270;&#35273;&#34920;&#31034;&#30340;&#24378;&#22823;&#24615;&#34701;&#20837;&#21040;&#25193;&#25955;&#31574;&#30053;&#20013;&#30340;&#26032;&#39062;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#25193;&#25955;&#31574;&#30053;&#26159;&#19968;&#31867;&#26377;&#26465;&#20214;&#30340;&#21160;&#20316;&#29983;&#25104;&#27169;&#22411;&#12290;DP3&#30340;&#26680;&#24515;&#35774;&#35745;&#26159;&#21033;&#29992;&#19968;&#20010;&#32039;&#20945;&#30340;3D&#35270;&#35273;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#26159;&#20174;&#31232;&#30095;&#28857;&#20113;&#20013;&#25552;&#21462;&#20986;&#26469;&#30340;&#65292;&#20351;&#29992;&#39640;&#25928;&#30340;&#28857;&#32534;&#30721;&#22120;&#12290;&#22312;&#25105;&#20204;&#28085;&#30422;&#20102;72&#20010;&#20223;&#30495;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;DP3&#20165;&#38656;&#35201;10&#20010;&#28436;&#31034;&#23601;&#21487;&#20197;&#25104;&#21151;&#22788;&#29702;&#22823;&#22810;&#25968;&#20219;&#21153;&#65292;&#24182;&#19988;&#27604;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;55.3%&#12290;&#22312;4&#20010;&#30495;&#23454;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#65292;DP3&#34920;&#29616;&#20986;&#20102;&#39640;&#25104;&#21151;&#29575;&#30340;&#31934;&#30830;&#25511;&#21046;&#65292;&#27599;&#39033;&#20219;&#21153;&#20165;&#38656;40&#27425;&#28436;&#31034;&#21363;&#21487;&#25104;&#21151;&#29575;&#20026;85%&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03954v1 Announce Type: cross  Abstract: Imitation learning provides an efficient way to teach robots dexterous skills; however, learning complex skills robustly and generalizablely usually consumes large amounts of human demonstrations. To tackle this challenging problem, we present 3D Diffusion Policy (DP3), a novel visual imitation learning approach that incorporates the power of 3D visual representations into diffusion policies, a class of conditional action generative models. The core design of DP3 is the utilization of a compact 3D visual representation, extracted from sparse point clouds with an efficient point encoder. In our experiments involving 72 simulation tasks, DP3 successfully handles most tasks with just 10 demonstrations and surpasses baselines with a 55.3% relative improvement. In 4 real robot tasks, DP3 demonstrates precise control with a high success rate of 85%, given only 40 demonstrations of each task, and shows excellent generalization abilities in di
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20998;&#31867;&#20195;&#26367;&#22238;&#24402;&#35757;&#32451;&#20540;&#20989;&#25968;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26041;&#27861;&#26469;&#25913;&#21892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;</title><link>https://arxiv.org/abs/2403.03950</link><description>&lt;p&gt;
&#20572;&#27490;&#22238;&#24402;&#65306;&#36890;&#36807;&#20998;&#31867;&#35757;&#32451;&#20540;&#20989;&#25968;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stop Regressing: Training Value Functions via Classification for Scalable Deep RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03950
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20998;&#31867;&#20195;&#26367;&#22238;&#24402;&#35757;&#32451;&#20540;&#20989;&#25968;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26041;&#27861;&#26469;&#25913;&#21892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20540;&#20989;&#25968;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26680;&#24515;&#32452;&#20214;&#12290;&#36825;&#20123;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#20989;&#25968;&#65292;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#22238;&#24402;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#21305;&#37197;&#33258;&#20030;&#30446;&#26631;&#20540;&#12290;&#28982;&#32780;&#65292;&#23558;&#20351;&#29992;&#22238;&#24402;&#30340;&#20215;&#20540;&#22411;RL&#26041;&#27861;&#25193;&#23637;&#21040;&#22823;&#22411;&#32593;&#32476;&#65292;&#22914;&#39640;&#23481;&#37327;&#30340;Transformers&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#35266;&#23519;&#21040;&#20102;&#36825;&#19968;&#24046;&#24322;&#65292;&#25506;&#35752;&#20102;&#36890;&#36807;&#20351;&#29992;&#20998;&#31867;&#32780;&#19981;&#26159;&#22238;&#24402;&#26469;&#35757;&#32451;&#20540;&#20989;&#25968;&#26159;&#21542;&#20063;&#21487;&#20197;&#31616;&#21333;&#22320;&#25552;&#39640;&#28145;&#24230;RL&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;&#20998;&#31867;&#20132;&#21449;&#29109;&#35757;&#32451;&#30340;&#20540;&#20989;&#25968;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#36825;&#20123;&#39046;&#22495;&#21253;&#25324;&#65306;&#22312;Atari 2600&#28216;&#25103;&#19978;&#20351;&#29992;SoftMoEs&#36827;&#34892;&#21333;&#19968;&#20219;&#21153;RL&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03950v1 Announce Type: cross  Abstract: Value functions are a central component of deep reinforcement learning (RL). These functions, parameterized by neural networks, are trained using a mean squared error regression objective to match bootstrapped target values. However, scaling value-based RL methods that use regression to large networks, such as high-capacity Transformers, has proven challenging. This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks. Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We demonstrate that value functions trained with categorical cross-entropy significantly improves performance and scalability in a variety of domains. These include: single-task RL on Atari 2600 games with SoftMoEs
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RialTo&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#31283;&#20581;&#21270;&#30495;&#23454;&#19990;&#30028;&#30340;&#27169;&#20223;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#19981;&#23433;&#20840;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37319;&#38598;&#25110;&#24191;&#27867;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#24615;&#33021;&#20248;&#36234;&#12289;&#31283;&#20581;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.03949</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#25311;&#35843;&#21644;&#29616;&#23454;&#65306;&#19968;&#31181;&#29992;&#20110;&#31283;&#20581;&#25805;&#20316;&#30340;&#23454;-&#27169;-&#23454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03949
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RialTo&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#31283;&#20581;&#21270;&#30495;&#23454;&#19990;&#30028;&#30340;&#27169;&#20223;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#19981;&#23433;&#20840;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37319;&#38598;&#25110;&#24191;&#27867;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#24615;&#33021;&#20248;&#36234;&#12289;&#31283;&#20581;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#20154;&#31867;&#30417;&#30563;&#26469;&#23398;&#20064;&#23545;&#29289;&#20307;&#23039;&#21183;&#21464;&#21270;&#12289;&#29289;&#29702;&#24178;&#25200;&#21644;&#35270;&#35273;&#25200;&#21160;&#40065;&#26834;&#30340;&#31574;&#30053;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#33258;&#20027;&#25506;&#32034;&#29615;&#22659;&#20197;&#23398;&#20064;&#31283;&#20581;&#34892;&#20026;&#65292;&#20294;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#19981;&#23433;&#20840;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37319;&#38598;&#12290;&#20026;&#20102;&#22312;&#27809;&#26377;&#19981;&#23433;&#20840;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37319;&#38598;&#25110;&#24191;&#27867;&#20154;&#31867;&#30417;&#30563;&#30340;&#36127;&#25285;&#19979;&#23398;&#20064;&#24615;&#33021;&#20248;&#36234;&#12289;&#31283;&#20581;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RialTo&#65292;&#19968;&#20010;&#36890;&#36807;&#22312;&#21363;&#23558;&#20174;&#23569;&#37327;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#26500;&#24314;&#30340;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#31283;&#20581;&#21270;&#30495;&#23454;&#19990;&#30028;&#30340;&#27169;&#20223;&#23398;&#20064;&#31574;&#30053;&#30340;&#31995;&#32479;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#23454;-&#27169;-&#23454;&#27969;&#27700;&#32447;&#65292;RialTo&#25552;&#20986;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#25509;&#21475;&#65292;&#29992;&#20110;&#24555;&#36895;&#25195;&#25551;&#21644;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#30340;&#25968;&#23383;&#23402;&#29983;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#21453;&#21521;&#25552;&#28860;&#8221;&#36807;&#31243;&#65292;&#29992;&#20110;&#32473;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#24102;&#26469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03949v1 Announce Type: cross  Abstract: Imitation learning methods need significant human supervision to learn policies robust to changes in object poses, physical disturbances, and visual distractors. Reinforcement learning, on the other hand, can explore the environment autonomously to learn robust behaviors but may require impractical amounts of unsafe real-world data collection. To learn performant, robust policies without the burden of unsafe real-world data collection or extensive human supervision, we propose RialTo, a system for robustifying real-world imitation learning policies via reinforcement learning in "digital twin" simulation environments constructed on the fly from small amounts of real-world data. To enable this real-to-sim-to-real pipeline, RialTo proposes an easy-to-use interface for quickly scanning and constructing digital twins of real-world environments. We also introduce a novel "inverse distillation" procedure for bringing real-world demonstrations
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#31934;&#30830;&#37325;&#26500;&#25209;&#37327;$b &gt;1$&#30340;&#31639;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#26799;&#24230;&#21453;&#28436;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.03945</link><description>&lt;p&gt;
SPEAR&#65306;&#32852;&#37030;&#23398;&#20064;&#20013;&#25209;&#37327;&#31934;&#30830;&#26799;&#24230;&#21453;&#28436;
&lt;/p&gt;
&lt;p&gt;
SPEAR:Exact Gradient Inversion of Batches in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03945
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#31934;&#30830;&#37325;&#26500;&#25209;&#37327;$b &gt;1$&#30340;&#31639;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#26799;&#24230;&#21453;&#28436;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#20165;&#19982;&#26381;&#21153;&#22120;&#20849;&#20139;&#20182;&#20204;&#26412;&#22320;&#25968;&#25454;&#30340;&#26799;&#24230;&#26356;&#26032;&#65292;&#32780;&#19981;&#26159;&#23454;&#38469;&#25968;&#25454;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26368;&#36817;&#21457;&#29616;&#26799;&#24230;&#21453;&#28436;&#25915;&#20987;&#21487;&#20197;&#20174;&#36825;&#20123;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#37325;&#26500;&#20986;&#25968;&#25454;&#12290;&#29616;&#26377;&#30340;&#25915;&#20987;&#21482;&#33021;&#22312;&#37325;&#35201;&#30340;&#35802;&#23454;&#20294;&#22909;&#22855;&#35774;&#32622;&#20013;&#23545;&#25209;&#37327;&#22823;&#23567;&#20026;$b=1$&#30340;&#25968;&#25454;&#36827;&#34892;&#31934;&#30830;&#37325;&#26500;&#65292;&#23545;&#20110;&#26356;&#22823;&#30340;&#25209;&#37327;&#21482;&#33021;&#36827;&#34892;&#36817;&#20284;&#37325;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;\emph{&#31532;&#19968;&#20010;&#20934;&#30830;&#37325;&#24314;&#25209;&#37327;$b &gt;1$&#30340;&#31639;&#27861;}&#12290;&#36825;&#31181;&#26041;&#27861;&#32467;&#21512;&#20102;&#23545;&#26799;&#24230;&#26174;&#24335;&#20302;&#31209;&#32467;&#26500;&#30340;&#25968;&#23398;&#35265;&#35299;&#21644;&#22522;&#20110;&#37319;&#26679;&#30340;&#31639;&#27861;&#12290;&#20851;&#38190;&#30340;&#26159;&#65292;&#25105;&#20204;&#21033;&#29992;ReLU&#35825;&#23548;&#30340;&#26799;&#24230;&#31232;&#30095;&#24615;&#65292;&#31934;&#30830;&#22320;&#36807;&#28388;&#25481;&#22823;&#37327;&#38169;&#35823;&#30340;&#26679;&#26412;&#65292;&#20351;&#26368;&#32456;&#30340;&#37325;&#24314;&#27493;&#39588;&#21487;&#34892;&#12290;&#25105;&#20204;&#20026;&#20840;&#36830;&#25509;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;GPU&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03945v1 Announce Type: new  Abstract: Federated learning is a popular framework for collaborative machine learning where multiple clients only share gradient updates on their local data with the server and not the actual data. Unfortunately, it was recently shown that gradient inversion attacks can reconstruct this data from these shared gradients. Existing attacks enable exact reconstruction only for a batch size of $b=1$ in the important honest-but-curious setting, with larger batches permitting only approximate reconstruction. In this work, we propose \emph{the first algorithm reconstructing whole batches with $b &gt;1$ exactly}. This approach combines mathematical insights into the explicit low-rank structure of gradients with a sampling-based algorithm. Crucially, we leverage ReLU-induced gradient sparsity to precisely filter out large numbers of incorrect samples, making a final reconstruction step tractable. We provide an efficient GPU implementation for fully connected 
&lt;/p&gt;</description></item><item><title>&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#21457;&#29616;&#25152;&#26377;&#23376;&#32593;&#32476;&#37117;&#20849;&#20139;&#19968;&#20010;&#27880;&#24847;&#21147;&#22836;&#38598;&#21512;&#65292;&#34987;&#31216;&#20026;&#21551;&#21457;&#24335;&#26680;&#24515;&#65292;&#36825;&#21487;&#33021;&#26159;&#36896;&#25104;&#23376;&#32593;&#32476;&#27867;&#21270;&#24046;&#24322;&#30340;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2403.03942</link><description>&lt;p&gt;
&#12298;&#21551;&#21457;&#24335;&#26680;&#24515;&#65306;&#29702;&#35299;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23376;&#32593;&#32476;&#27867;&#21270;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03942
&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#21457;&#29616;&#25152;&#26377;&#23376;&#32593;&#32476;&#37117;&#20849;&#20139;&#19968;&#20010;&#27880;&#24847;&#21147;&#22836;&#38598;&#21512;&#65292;&#34987;&#31216;&#20026;&#21551;&#21457;&#24335;&#26680;&#24515;&#65292;&#36825;&#21487;&#33021;&#26159;&#36896;&#25104;&#23376;&#32593;&#32476;&#27867;&#21270;&#24046;&#24322;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#32463;&#36807;&#19981;&#21516;&#38543;&#26426;&#31181;&#23376;&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#30340;&#39046;&#22495;&#20869;&#24615;&#33021;&#65292;&#20294;&#22312;&#21477;&#27861;&#27867;&#21270;&#27979;&#35797;&#20013;&#27867;&#21270;&#19981;&#21516;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20986;&#21363;&#20351;&#22312;&#21333;&#19968;&#27169;&#22411;&#20869;&#37096;&#65292;&#25105;&#20204;&#20063;&#21487;&#20197;&#25214;&#21040;&#25191;&#34892;&#31867;&#20284;&#39046;&#22495;&#20869;&#25805;&#20316;&#20294;&#27867;&#21270;&#24046;&#24322;&#24040;&#22823;&#30340;&#22810;&#20010;&#23376;&#32593;&#32476;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#29616;&#35937;&#65292;&#25105;&#20204;&#35843;&#26597;&#26159;&#21542;&#21487;&#20197;&#29992;&#8220;&#31454;&#20105;&#24615;&#23376;&#32593;&#32476;&#8221;&#26469;&#29702;&#35299;&#23427;&#20204;&#65306;&#27169;&#22411;&#26368;&#21021;&#34920;&#31034;&#21508;&#31181;&#19981;&#21516;&#31639;&#27861;&#65292;&#23545;&#24212;&#20110;&#19981;&#21516;&#30340;&#23376;&#32593;&#32476;&#65292;&#24403;&#26368;&#32456;&#25910;&#25947;&#21040;&#20854;&#20013;&#19968;&#20010;&#26102;&#27867;&#21270;&#21457;&#29983;&#12290;&#36825;&#19968;&#35299;&#37322;&#24050;&#34987;&#29992;&#20110;&#35299;&#37322;&#31616;&#21333;&#31639;&#27861;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24182;&#38750;&#25214;&#21040;&#31454;&#20105;&#24615;&#23376;&#32593;&#32476;&#65292;&#32780;&#26159;&#25152;&#26377;&#23376;&#32593;&#32476; -- &#26080;&#35770;&#23427;&#20204;&#26159;&#21542;&#27867;&#21270; -- &#37117;&#20849;&#20139;&#19968;&#32452;&#27880;&#24847;&#21147;&#22836;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#21551;&#21457;&#24335;&#26680;&#24515;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#23427;&#21487;&#33021;&#26159;&#36896;&#25104;&#19981;&#21516;&#23376;&#32593;&#32476;&#27867;&#21270;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03942v1 Announce Type: new  Abstract: Prior work has found that pretrained language models (LMs) fine-tuned with different random seeds can achieve similar in-domain performance but generalize differently on tests of syntactic generalization. In this work, we show that, even within a single model, we can find multiple subnetworks that perform similarly in-domain, but generalize vastly differently. To better understand these phenomena, we investigate if they can be understood in terms of "competing subnetworks": the model initially represents a variety of distinct algorithms, corresponding to different subnetworks, and generalization occurs when it ultimately converges to one. This explanation has been used to account for generalization in simple algorithmic tasks. Instead of finding competing subnetworks, we find that all subnetworks -- whether they generalize or not -- share a set of attention heads, which we refer to as the heuristic core. Further analysis suggests that th
&lt;/p&gt;</description></item><item><title>GUIDE&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#20998;&#31867;&#22120;&#24341;&#23548;&#25216;&#26415;&#65292;&#38024;&#23545;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#36951;&#24536;&#30340;&#20449;&#24687;&#20135;&#29983;&#22797;&#20064;&#31034;&#20363;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>https://arxiv.org/abs/2403.03938</link><description>&lt;p&gt;
GUIDE&#65306;&#22522;&#20110;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GUIDE: Guidance-based Incremental Learning with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03938
&lt;/p&gt;
&lt;p&gt;
GUIDE&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#20998;&#31867;&#22120;&#24341;&#23548;&#25216;&#26415;&#65292;&#38024;&#23545;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#36951;&#24536;&#30340;&#20449;&#24687;&#20135;&#29983;&#22797;&#20064;&#31034;&#20363;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;GUIDE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#23545;&#26377;&#34987;&#36951;&#24536;&#39118;&#38505;&#30340;&#26679;&#26412;&#36827;&#34892;&#22797;&#20064;&#12290;&#29616;&#26377;&#30340;&#29983;&#25104;&#31574;&#30053;&#36890;&#36807;&#20174;&#29983;&#25104;&#27169;&#22411;&#20013;&#38543;&#26426;&#25277;&#21462;&#22797;&#20064;&#26679;&#26412;&#26469;&#23545;&#25239;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#22522;&#20110;&#32531;&#20914;&#21306;&#30340;&#26041;&#27861;&#30456;&#30683;&#30462;&#65292;&#20854;&#20013;&#37319;&#26679;&#31574;&#30053;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#20998;&#31867;&#22120;&#24341;&#23548;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20135;&#29983;&#19987;&#38376;&#38024;&#23545;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#36951;&#24536;&#20449;&#24687;&#30340;&#22797;&#20064;&#31034;&#20363;&#65292;&#20197;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#33021;&#22815;&#20174;&#20808;&#21069;&#20219;&#21153;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#65292;&#36825;&#20123;&#26679;&#26412;&#22312;&#26368;&#36817;&#36935;&#21040;&#30340;&#31867;&#21035;&#24773;&#22659;&#19979;&#26356;&#26377;&#21487;&#33021;&#34987;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GUIDE&#26174;&#33879;&#20943;&#23569;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#25277;&#26679;&#26041;&#27861;&#65292;&#24182;&#22312;&#25345;&#32493;&#23398;&#20064;&#26041;&#38754;&#36229;&#36807;&#20102;&#26368;&#36817;&#30340;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03938v1 Announce Type: new  Abstract: We introduce GUIDE, a novel continual learning approach that directs diffusion models to rehearse samples at risk of being forgotten. Existing generative strategies combat catastrophic forgetting by randomly sampling rehearsal examples from a generative model. Such an approach contradicts buffer-based approaches where sampling strategy plays an important role. We propose to bridge this gap by integrating diffusion models with classifier guidance techniques to produce rehearsal examples specifically targeting information forgotten by a continuously trained model. This approach enables the generation of samples from preceding task distributions, which are more likely to be misclassified in the context of recently encountered classes. Our experimental results show that GUIDE significantly reduces catastrophic forgetting, outperforming conventional random sampling approaches and surpassing recent state-of-the-art methods in continual learnin
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#27169;&#22411;NowcastingGPT-EVL&#22312;&#26497;&#31471;&#38477;&#27700;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#22788;&#29702;&#26497;&#31471;&#38477;&#27700;&#20107;&#20214;&#26102;&#12290;</title><link>https://arxiv.org/abs/2403.03929</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#26497;&#31471;&#38477;&#27700;&#21363;&#26102;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
Extreme Precipitation Nowcasting using Transformer-based Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03929
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#27169;&#22411;NowcastingGPT-EVL&#22312;&#26497;&#31471;&#38477;&#27700;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#22788;&#29702;&#26497;&#31471;&#38477;&#27700;&#20107;&#20214;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;&#24102;&#26377;&#26497;&#20540;&#25439;&#22833;&#65288;EVL&#65289;&#27491;&#21017;&#21270;&#30340;NowcastingGPT&#65292;&#36827;&#34892;&#26497;&#31471;&#38477;&#27700;&#21363;&#26102;&#39044;&#25253;&#12290;&#21033;&#29992;&#26469;&#33258;&#33655;&#20848;&#30343;&#23478;&#27668;&#35937;&#30740;&#31350;&#25152;&#65288;KNMI&#65289;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#30701;&#26399;&#38477;&#27700;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35745;&#31639;EVL&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#20551;&#23450;&#22266;&#23450;&#30340;&#26497;&#31471;&#34920;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#25429;&#25417;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#35758;&#30340;NowcastingGPT-EVL&#22312;&#29983;&#25104;&#20934;&#30830;&#30340;&#38477;&#27700;&#39044;&#25253;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#26497;&#31471;&#38477;&#27700;&#20107;&#20214;&#26102;&#12290;&#20195;&#30721;&#21487;&#22312;\url{https://github.com/Cmeo97/NowcastingGPT}&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03929v1 Announce Type: cross  Abstract: This paper presents an innovative approach to extreme precipitation nowcasting by employing Transformer-based generative models, namely NowcastingGPT with Extreme Value Loss (EVL) regularization. Leveraging a comprehensive dataset from the Royal Netherlands Meteorological Institute (KNMI), our study focuses on predicting short-term precipitation with high accuracy. We introduce a novel method for computing EVL without assuming fixed extreme representations, addressing the limitations of current models in capturing extreme weather events. We present both qualitative and quantitative analyses, demonstrating the superior performance of the proposed NowcastingGPT-EVL in generating accurate precipitation forecasts, especially when dealing with extreme precipitation events. The code is available at \url{https://github.com/Cmeo97/NowcastingGPT}.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23545;&#20110;$k$-PCA&#31639;&#27861;&#20013;&#36817;&#20284;&#21442;&#25968;&#36864;&#21270;&#30340;&#36793;&#30028;&#24471;&#21040;&#20102;&#26174;&#33879;&#26356;&#20026;&#31934;&#30830;&#30340;&#30028;&#38480;</title><link>https://arxiv.org/abs/2403.03905</link><description>&lt;p&gt;
&#40657;&#30418;$k$-to-$1$-PCA&#38477;&#32500;&#65306;&#29702;&#35770;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03905
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23545;&#20110;$k$-PCA&#31639;&#27861;&#20013;&#36817;&#20284;&#21442;&#25968;&#36864;&#21270;&#30340;&#36793;&#30028;&#24471;&#21040;&#20102;&#26174;&#33879;&#26356;&#20026;&#31934;&#30830;&#30340;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
$k$-&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;$k$-PCA&#65289;&#38382;&#39064;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#31639;&#27861;&#21407;&#35821;&#65292;&#22312;&#25968;&#25454;&#20998;&#26512;&#21644;&#38477;&#32500;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#22312;&#32479;&#35745;&#29615;&#22659;&#20013;&#65292;$k$-PCA&#30340;&#30446;&#26631;&#26159;&#35782;&#21035;&#19968;&#20010;&#20998;&#24067;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#39030;&#37096;&#29305;&#24449;&#31354;&#38388;&#65292;&#25105;&#20204;&#21482;&#33021;&#36890;&#36807;&#26679;&#26412;&#38544;&#24335;&#35775;&#38382;&#36825;&#20010;&#30697;&#38453;&#12290;&#21463;&#36825;&#20123;&#38544;&#24335;&#35774;&#32622;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20998;&#26512;&#40657;&#30418;&#32553;&#20943;&#26041;&#27861;&#20316;&#20026;&#35774;&#35745;$k$-PCA&#31639;&#27861;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#25105;&#20204;&#36890;&#36807;&#40657;&#30418;$1$-PCA&#39044;&#35328;&#27169;&#25311;&#23545;&#26410;&#30693;&#30446;&#26631;&#30697;&#38453;&#30340;&#35775;&#38382;&#65292;&#35813;&#39044;&#35328;&#36820;&#22238;&#19968;&#20010;&#36817;&#20284;&#30340;&#39030;&#37096;&#29305;&#24449;&#21521;&#37327;&#65292;&#26681;&#25454;&#20004;&#20010;&#27969;&#34892;&#30340;&#36817;&#20284;&#27010;&#24565;&#12290;&#23613;&#31649;&#36825;&#31181;&#40657;&#30418;&#26041;&#27861;&#21487;&#33021;&#26159;&#35774;&#35745;$k$-PCA&#31639;&#27861;&#20013;&#26368;&#33258;&#28982;&#30340;&#22522;&#20110;&#38477;&#32500;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#36882;&#24402;&#35843;&#29992;$1$-PCA&#39044;&#35328;&#35843;&#29992;&#20102;$k$&#27425;&#65292;&#20197;&#21069;&#24456;&#38590;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03905v1 Announce Type: cross  Abstract: The $k$-principal component analysis ($k$-PCA) problem is a fundamental algorithmic primitive that is widely-used in data analysis and dimensionality reduction applications. In statistical settings, the goal of $k$-PCA is to identify a top eigenspace of the covariance matrix of a distribution, which we only have implicit access to via samples. Motivated by these implicit settings, we analyze black-box deflation methods as a framework for designing $k$-PCA algorithms, where we model access to the unknown target matrix via a black-box $1$-PCA oracle which returns an approximate top eigenvector, under two popular notions of approximation. Despite being arguably the most natural reduction-based approach to $k$-PCA algorithm design, such black-box methods, which recursively call a $1$-PCA oracle $k$ times, were previously poorly-understood.   Our main contribution is significantly sharper bounds on the approximation parameter degradation of
&lt;/p&gt;</description></item><item><title>DART&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38647;&#36798;&#29305;&#23450;&#29289;&#29702;&#23398;&#30340;&#38544;&#24335;&#22810;&#26222;&#21202;&#23618;&#26512;&#25104;&#20687;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#21453;&#23556;&#29575;&#21644;&#36879;&#23556;&#29575;&#30340;&#21576;&#29616;&#27969;&#27700;&#32447;&#65292;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#38647;&#36798;&#36317;&#31163;-&#22810;&#26222;&#21202;&#22270;&#20687;&#21512;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.03896</link><description>&lt;p&gt;
DART&#65306;&#38647;&#36798;&#26032;&#35270;&#22270;&#21512;&#25104;&#30340;&#38544;&#24335;&#22810;&#26222;&#21202;&#23618;&#26512;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
DART: Implicit Doppler Tomography for Radar Novel View Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03896
&lt;/p&gt;
&lt;p&gt;
DART&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38647;&#36798;&#29305;&#23450;&#29289;&#29702;&#23398;&#30340;&#38544;&#24335;&#22810;&#26222;&#21202;&#23618;&#26512;&#25104;&#20687;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#21453;&#23556;&#29575;&#21644;&#36879;&#23556;&#29575;&#30340;&#21576;&#29616;&#27969;&#27700;&#32447;&#65292;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#38647;&#36798;&#36317;&#31163;-&#22810;&#26222;&#21202;&#22270;&#20687;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#26159;&#23556;&#39057;&#31995;&#32479;&#35774;&#35745;&#20154;&#21592;&#30340;&#23453;&#36149;&#24037;&#20855;&#65292;&#21487;&#20197;&#24555;&#36895;&#21407;&#22411;&#21270;&#21508;&#31181;&#29992;&#20110;&#25104;&#20687;&#12289;&#30446;&#26631;&#26816;&#27979;&#12289;&#20998;&#31867;&#21644;&#36319;&#36394;&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DART - &#22810;&#26222;&#21202;&#36741;&#21161;&#38647;&#36798;&#23618;&#26512;&#25104;&#20687;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#31070;&#32463;&#36752;&#23556;&#22330;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#38647;&#36798;&#29305;&#23450;&#30340;&#29289;&#29702;&#23398;&#26469;&#21019;&#24314;&#19968;&#20010;&#22522;&#20110;&#21453;&#23556;&#29575;&#21644;&#36879;&#23556;&#29575;&#30340;&#21576;&#29616;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#36317;&#31163;-&#22810;&#26222;&#21202;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#33258;&#23450;&#20041;&#25968;&#25454;&#25910;&#38598;&#24179;&#21488;&#65292;&#24182;&#19982;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#23450;&#20301;&#19968;&#36215;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38647;&#36798;&#25968;&#25454;&#38598;&#20197;&#21450;&#20934;&#30830;&#30340;&#20301;&#32622;&#21644;&#30636;&#26102;&#36895;&#24230;&#27979;&#37327;&#26469;&#35780;&#20272;DART&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;DART&#21512;&#25104;&#20102;&#26356;&#20248;&#36234;&#30340;&#38647;&#36798;&#36317;&#31163;-&#22810;&#26222;&#21202;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03896v1 Announce Type: cross  Abstract: Simulation is an invaluable tool for radio-frequency system designers that enables rapid prototyping of various algorithms for imaging, target detection, classification, and tracking. However, simulating realistic radar scans is a challenging task that requires an accurate model of the scene, radio frequency material properties, and a corresponding radar synthesis function. Rather than specifying these models explicitly, we propose DART - Doppler Aided Radar Tomography, a Neural Radiance Field-inspired method which uses radar-specific physics to create a reflectance and transmittance-based rendering pipeline for range-Doppler images. We then evaluate DART by constructing a custom data collection platform and collecting a novel radar dataset together with accurate position and instantaneous velocity measurements from lidar-based localization. In comparison to state-of-the-art baselines, DART synthesizes superior radar range-Doppler imag
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#24369;&#30417;&#30563;&#32852;&#21512;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#30340;&#29983;&#29289;&#26631;&#35760;&#39044;&#27979;&#20013;&#23454;&#29616;&#20102;+7.7&#65285;&#30340;&#24615;&#33021;&#25913;&#36827;</title><link>https://arxiv.org/abs/2403.03891</link><description>&lt;p&gt;
&#32852;&#21512;&#22810;&#20219;&#21153;&#23398;&#20064;&#25913;&#36827;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#24369;&#30417;&#30563;&#29983;&#29289;&#26631;&#35760;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Joint multi-task learning improves weakly-supervised biomarker prediction in computational pathology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03891
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24369;&#30417;&#30563;&#32852;&#21512;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#30340;&#29983;&#29289;&#26631;&#35760;&#39044;&#27979;&#20013;&#23454;&#29616;&#20102;+7.7&#65285;&#30340;&#24615;&#33021;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21487;&#20197;&#22312;&#24369;&#30417;&#30563;&#35774;&#32622;&#20013;&#30452;&#25509;&#20174;&#25968;&#23383;&#21270;&#30340;&#30284;&#30151;&#32452;&#32455;&#23398;&#20013;&#39044;&#27979;&#29983;&#29289;&#26631;&#35760;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#22238;&#24402;&#30340;DL&#39044;&#27979;&#36830;&#32493;&#29983;&#29289;&#26631;&#35760;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20020;&#24202;&#20915;&#31574;&#36890;&#24120;&#38656;&#35201;&#20998;&#31867;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#32852;&#21512;&#22810;&#20219;&#21153;Transformer&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#24050;&#22312;&#22235;&#20010;&#20844;&#20849;&#24739;&#32773;&#38431;&#21015;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#29992;&#20110;&#39044;&#27979;&#20004;&#20010;&#20851;&#38190;&#30340;&#39044;&#27979;&#24615;&#29983;&#29289;&#26631;&#35760;&#65292;&#24494;&#21355;&#26143;&#19981;&#31283;&#23450;&#24615;&#65288;MSI&#65289;&#21644;&#21516;&#28304;&#37325;&#32452;&#32570;&#38519;&#65288;HRD&#65289;&#65292;&#24182;&#36827;&#34892;&#20102;&#19982;&#32959;&#30244;&#24494;&#29615;&#22659;&#30456;&#20851;&#30340;&#36741;&#21161;&#22238;&#24402;&#20219;&#21153;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#36827;&#34892;&#20102;16&#31181;&#20219;&#21153;&#24179;&#34913;&#30340;&#24369;&#30417;&#30563;&#32852;&#21512;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#24615;&#33021;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21463;&#35797;&#32773;&#24037;&#20316;&#29305;&#24449;&#19979;&#30340;&#38754;&#31215;+7.7&#65285;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03891v1 Announce Type: cross  Abstract: Deep Learning (DL) can predict biomarkers directly from digitized cancer histology in a weakly-supervised setting. Recently, the prediction of continuous biomarkers through regression-based DL has seen an increasing interest. Nonetheless, clinical decision making often requires a categorical outcome. Consequently, we developed a weakly-supervised joint multi-task Transformer architecture which has been trained and evaluated on four public patient cohorts for the prediction of two key predictive biomarkers, microsatellite instability (MSI) and homologous recombination deficiency (HRD), trained with auxiliary regression tasks related to the tumor microenvironment. Moreover, we perform a comprehensive benchmark of 16 approaches of task balancing for weakly-supervised joint multi-task learning in computational pathology. Using our novel approach, we improve over the state-of-the-art area under the receiver operating characteristic by +7.7%
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25193;&#25955;&#31574;&#30053;&#65288;HDP&#65289;&#29992;&#20110;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#36890;&#36807;&#23558;&#25805;&#32437;&#31574;&#30053;&#20998;&#35299;&#20026;&#20998;&#23618;&#32467;&#26500;&#65292;&#21516;&#26102;&#35299;&#20915;&#38271;&#26399;&#20219;&#21153;&#35268;&#21010;&#21644;&#29983;&#25104;&#32454;&#31890;&#24230;&#30340;&#20302;&#23618;&#21160;&#20316;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#24863;&#30693;&#30446;&#26631;&#26465;&#20214;&#25511;&#21046;&#20195;&#29702;&#65288;RK-Diffuser&#65289;&#12290;</title><link>https://arxiv.org/abs/2403.03890</link><description>&lt;p&gt;
&#20998;&#23618;&#25193;&#25955;&#31574;&#30053;&#29992;&#20110;&#32771;&#34385;&#36816;&#21160;&#23398;&#30340;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03890
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25193;&#25955;&#31574;&#30053;&#65288;HDP&#65289;&#29992;&#20110;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#36890;&#36807;&#23558;&#25805;&#32437;&#31574;&#30053;&#20998;&#35299;&#20026;&#20998;&#23618;&#32467;&#26500;&#65292;&#21516;&#26102;&#35299;&#20915;&#38271;&#26399;&#20219;&#21153;&#35268;&#21010;&#21644;&#29983;&#25104;&#32454;&#31890;&#24230;&#30340;&#20302;&#23618;&#21160;&#20316;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#24863;&#30693;&#30446;&#26631;&#26465;&#20214;&#25511;&#21046;&#20195;&#29702;&#65288;RK-Diffuser&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20998;&#23618;&#25193;&#25955;&#31574;&#30053;&#65288;HDP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20998;&#23618;&#20195;&#29702;&#12290;HDP&#23558;&#25805;&#32437;&#31574;&#30053;&#20998;&#35299;&#20026;&#20998;&#23618;&#32467;&#26500;&#65306;&#39640;&#23618;&#20219;&#21153;&#35268;&#21010;&#20195;&#29702;&#39044;&#27979;&#36828;&#31471;&#26368;&#20339;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#21183;&#65288;NBP&#65289;&#65292;&#20302;&#23618;&#30446;&#26631;&#26465;&#20214;&#25193;&#25955;&#31574;&#30053;&#29983;&#25104;&#26368;&#20339;&#36816;&#21160;&#36712;&#36857;&#12290;&#36825;&#31181;&#20998;&#35299;&#30340;&#31574;&#30053;&#34920;&#31034;&#20351;HDP&#33021;&#22815;&#21516;&#26102;&#35299;&#20915;&#38271;&#26399;&#20219;&#21153;&#35268;&#21010;&#21644;&#29983;&#25104;&#32454;&#31890;&#24230;&#30340;&#20302;&#23618;&#21160;&#20316;&#12290;&#20026;&#20102;&#29983;&#25104;&#31526;&#21512;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#32422;&#26463;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#36816;&#21160;&#36712;&#36857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36816;&#21160;&#23398;&#24863;&#30693;&#30446;&#26631;&#26465;&#20214;&#25511;&#21046;&#20195;&#29702;&#65292;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#25193;&#25955;&#22120;&#65288;RK-Diffuser&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;RK-Diffuser&#23398;&#20064;&#29983;&#25104;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#21183;&#21644;&#20851;&#33410;&#20301;&#32622;&#36712;&#36857;&#65292;&#24182;&#23558;&#31934;&#30830;&#20294;&#32570;&#20047;&#36816;&#21160;&#23398;&#24847;&#35782;&#30340;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#21183;&#25193;&#25955;&#22120;&#25552;&#28860;&#20026;&#36816;&#21160;&#23398;&#24863;&#30693;&#20294;&#19981;&#22826;&#31934;&#30830;&#30340;&#20851;&#33410;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03890v1 Announce Type: cross  Abstract: This paper introduces Hierarchical Diffusion Policy (HDP), a hierarchical agent for multi-task robotic manipulation. HDP factorises a manipulation policy into a hierarchical structure: a high-level task-planning agent which predicts a distant next-best end-effector pose (NBP), and a low-level goal-conditioned diffusion policy which generates optimal motion trajectories. The factorised policy representation allows HDP to tackle both long-horizon task planning while generating fine-grained low-level actions. To generate context-aware motion trajectories while satisfying robot kinematics constraints, we present a novel kinematics-aware goal-conditioned control agent, Robot Kinematics Diffuser (RK-Diffuser). Specifically, RK-Diffuser learns to generate both the end-effector pose and joint position trajectories, and distill the accurate but kinematics-unaware end-effector pose diffuser to the kinematics-aware but less accurate joint positio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28508;&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;LD3M&#65289;&#65292;&#32467;&#21512;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#21644;&#25968;&#25454;&#38598;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#21644;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.03881</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28508;&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Latent Dataset Distillation with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03881
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28508;&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;LD3M&#65289;&#65292;&#32467;&#21512;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#21644;&#25968;&#25454;&#38598;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#21644;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#20256;&#32479;&#19978;&#20381;&#36182;&#20110;&#36234;&#26469;&#36234;&#22823;&#30340;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#25968;&#25454;&#38598;&#24102;&#26469;&#23384;&#20648;&#25361;&#25112;&#65292;&#24182;&#19988;&#21253;&#21547;&#19968;&#20123;&#38750;&#24433;&#21709;&#21147;&#26679;&#26412;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#34987;&#24573;&#30053;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#26368;&#32456;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#20986;&#29616;&#20102;&#23558;&#25968;&#25454;&#38598;&#20449;&#24687;&#33976;&#39311;&#25104;&#19968;&#32452;&#21387;&#32553;&#26679;&#26412;&#65288;&#21512;&#25104;&#26679;&#26412;&#65289;&#65292;&#21363;&#33976;&#39311;&#25968;&#25454;&#38598;&#30340;&#27010;&#24565;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#36873;&#25321;&#29992;&#20110;&#36830;&#25509;&#21407;&#22987;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26550;&#26500;&#65288;&#36890;&#24120;&#26159;ConvNet&#65289;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25152;&#20351;&#29992;&#30340;&#27169;&#22411;&#26550;&#26500;&#19982;&#33976;&#39311;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;&#21017;&#26368;&#32456;&#20934;&#30830;&#24615;&#20250;&#38477;&#20302;&#12290;&#21478;&#19968;&#20010;&#25361;&#25112;&#26159;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#20363;&#22914;128x128&#21450;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03881v1 Announce Type: cross  Abstract: The efficacy of machine learning has traditionally relied on the availability of increasingly larger datasets. However, large datasets pose storage challenges and contain non-influential samples, which could be ignored during training without impacting the final accuracy of the model. In response to these limitations, the concept of distilling the information on a dataset into a condensed set of (synthetic) samples, namely a distilled dataset, emerged. One crucial aspect is the selected architecture (usually ConvNet) for linking the original and synthetic datasets. However, the final accuracy is lower if the employed model architecture differs from the model used during distillation. Another challenge is the generation of high-resolution images, e.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation with Diffusion Models (LD3M) that combine diffusion in latent space with dataset distillation to tackle both chal
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#23558;&#28176;&#36817;&#20110;&#19968;&#20010;&#24120;&#25968;&#20989;&#25968;&#65292;&#24182;&#38480;&#21046;&#20102;&#36825;&#20123;&#20998;&#31867;&#22120;&#30340;&#32479;&#19968;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03880</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#20960;&#20046;&#32943;&#23450;&#26159;&#28176;&#36817;&#24120;&#25968;
&lt;/p&gt;
&lt;p&gt;
Graph neural network outputs are almost surely asymptotically constant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03880
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#23558;&#28176;&#36817;&#20110;&#19968;&#20010;&#24120;&#25968;&#20989;&#25968;&#65292;&#24182;&#38480;&#21046;&#20102;&#36825;&#20123;&#20998;&#31867;&#22120;&#30340;&#32479;&#19968;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#20027;&#35201;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;GNN&#30340;&#27010;&#29575;&#20998;&#31867;&#22120;&#22312;&#20174;&#26576;&#20010;&#38543;&#26426;&#22270;&#27169;&#22411;&#20013;&#32472;&#21046;&#30340;&#26356;&#22823;&#22270;&#19978;&#24212;&#29992;&#26102;&#39044;&#27979;&#22914;&#20309;&#28436;&#21464;&#65292;&#25552;&#20986;&#20102;GNN&#34920;&#36798;&#33021;&#21147;&#30340;&#26032;&#35282;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36755;&#20986;&#25910;&#25947;&#21040;&#19968;&#20010;&#24120;&#25968;&#20989;&#25968;&#65292;&#36825;&#20010;&#20989;&#25968;&#19978;&#38480;&#20102;&#36825;&#20123;&#20998;&#31867;&#22120;&#21487;&#20197;&#32479;&#19968;&#34920;&#36798;&#30340;&#20869;&#23481;&#12290;&#36825;&#31181;&#25910;&#25947;&#29616;&#35937;&#36866;&#29992;&#20110;&#38750;&#24120;&#24191;&#27867;&#30340;GNN&#31867;&#21035;&#65292;&#21253;&#25324;&#20808;&#36827;&#27169;&#22411;&#65292;&#20854;&#20013;&#30340;&#32858;&#21512;&#21253;&#25324;&#24179;&#22343;&#20540;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#36716;&#25442;&#22120;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#21508;&#31181;&#38543;&#26426;&#22270;&#27169;&#22411;&#65292;&#21253;&#25324;&#65288;&#31232;&#30095;&#30340;&#65289;Erd\H{o}s-R\'enyi&#27169;&#22411;&#21644;&#38543;&#26426;&#22359;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#36825;&#20123;&#21457;&#29616;&#65292;&#35266;&#23519;&#21040;&#25910;&#25947;&#29616;&#35937;&#24050;&#32463;&#22312;&#30456;&#23545;&#36866;&#20013;&#35268;&#27169;&#30340;&#22270;&#20013;&#26174;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03880v1 Announce Type: new  Abstract: Graph neural networks (GNNs) are the predominant architectures for a variety of learning tasks on graphs. We present a new angle on the expressive power of GNNs by studying how the predictions of a GNN probabilistic classifier evolve as we apply it on larger graphs drawn from some random graph model. We show that the output converges to a constant function, which upper-bounds what these classifiers can express uniformly. This convergence phenomenon applies to a very wide class of GNNs, including state of the art models, with aggregates including mean and the attention-based mechanism of graph transformers. Our results apply to a broad class of random graph models, including the (sparse) Erd\H{o}s-R\'enyi model and the stochastic block model. We empirically validate these findings, observing that the convergence phenomenon already manifests itself on graphs of relatively modest size.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Decoupled VFL&#65288;DVFL&#65289;&#65292;&#19968;&#31181;&#38754;&#21521;VFL&#30340;&#20998;&#27573;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20998;&#25955;&#32858;&#21512;&#21644;&#38548;&#31163;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23481;&#38169;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03871</link><description>&lt;p&gt;
&#38754;&#21521;&#22402;&#30452;&#20998;&#21306;&#25968;&#25454;&#30340;&#35299;&#32806;&#24335;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65292;&#29992;&#20110;&#23454;&#38469;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Decoupled Vertical Federated Learning for Practical Training on Vertically Partitioned Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03871
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Decoupled VFL&#65288;DVFL&#65289;&#65292;&#19968;&#31181;&#38754;&#21521;VFL&#30340;&#20998;&#27573;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20998;&#25955;&#32858;&#21512;&#21644;&#38548;&#31163;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23481;&#38169;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20854;&#20013;&#20849;&#21516;&#23454;&#20307;&#30340;&#19981;&#21516;&#29305;&#24449;&#25152;&#26377;&#32773;&#21512;&#20316;&#23398;&#20064;&#20840;&#23616;&#27169;&#22411;&#32780;&#26080;&#38656;&#20849;&#20139;&#25968;&#25454;&#12290;&#22312;VFL&#20013;&#65292;&#20027;&#26426;&#23458;&#25143;&#31471;&#25317;&#26377;&#27599;&#20010;&#23454;&#20307;&#30340;&#25968;&#25454;&#26631;&#31614;&#65292;&#24182;&#22522;&#20110;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#20013;&#38388;&#26412;&#22320;&#34920;&#31034;&#23398;&#20064;&#26368;&#32456;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#20027;&#26426;&#26159;&#19968;&#20010;&#21333;&#28857;&#25925;&#38556;&#65292;&#26631;&#31614;&#21453;&#39304;&#21487;&#20197;&#34987;&#24694;&#24847;&#23458;&#25143;&#31471;&#29992;&#26469;&#25512;&#26029;&#31169;&#26377;&#29305;&#24449;&#12290;&#35201;&#27714;&#25152;&#26377;&#21442;&#19982;&#32773;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#27963;&#36291;&#21644;&#20540;&#24471;&#20449;&#36182;&#36890;&#24120;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22312;&#21463;&#25511;&#29615;&#22659;&#20043;&#22806;&#23436;&#20840;&#19981;&#21487;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;VFL&#30340;&#20998;&#27573;&#23398;&#20064;&#26041;&#27861;Decoupled VFL&#65288;DVFL&#65289;&#12290;&#36890;&#36807;&#22312;&#21508;&#33258;&#30340;&#30446;&#26631;&#19978;&#35757;&#32451;&#27599;&#20010;&#27169;&#22411;&#65292;DVFL&#20801;&#35768;&#29305;&#24449;&#23398;&#20064;&#21644;&#26631;&#31614;&#30417;&#30563;&#20043;&#38388;&#30340;&#20998;&#25955;&#32858;&#21512;&#21644;&#38548;&#31163;&#12290;&#20855;&#26377;&#36825;&#20123;&#23646;&#24615;&#65292;DVFL&#20855;&#26377;&#23481;&#38169;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03871v1 Announce Type: new  Abstract: Vertical Federated Learning (VFL) is an emergent distributed machine learning paradigm wherein owners of disjoint features of a common set of entities collaborate to learn a global model without sharing data. In VFL, a host client owns data labels for each entity and learns a final representation based on intermediate local representations from all guest clients. Therefore, the host is a single point of failure and label feedback can be used by malicious guest clients to infer private features. Requiring all participants to remain active and trustworthy throughout the entire training process is generally impractical and altogether infeasible outside of controlled environments. We propose Decoupled VFL (DVFL), a blockwise learning approach to VFL. By training each model on its own objective, DVFL allows for decentralized aggregation and isolation between feature learning and label supervision. With these properties, DVFL is fault tolerant
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#20102;&#19968;&#31181;&#21327;&#20316;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26631;&#35760;&#32423;&#21035;&#20132;&#38169;&#29983;&#25104;&#26469;&#25945;&#25480;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#65292;&#26080;&#38656;&#30452;&#25509;&#30417;&#30563;&#65292;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#34701;&#21512;&#27599;&#20010;&#27169;&#22411;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#32852;&#21512;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03870</link><description>&lt;p&gt;
&#23398;&#20064;&#22810;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#20316;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning to Decode Collaboratively with Multiple Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03870
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20102;&#19968;&#31181;&#21327;&#20316;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26631;&#35760;&#32423;&#21035;&#20132;&#38169;&#29983;&#25104;&#26469;&#25945;&#25480;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#65292;&#26080;&#38656;&#30452;&#25509;&#30417;&#30563;&#65292;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#34701;&#21512;&#27599;&#20010;&#27169;&#22411;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#32852;&#21512;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26631;&#35760;&#32423;&#21035;&#20132;&#38169;&#23427;&#20204;&#30340;&#29983;&#25104;&#26469;&#25945;&#25480;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21327;&#20316;&#12290;&#25105;&#20204;&#23558;&#19979;&#19968;&#20010;&#26631;&#35760;&#30001;&#21738;&#20010;LLM&#29983;&#25104;&#30340;&#20915;&#31574;&#24314;&#27169;&#20026;&#28508;&#21464;&#37327;&#12290;&#36890;&#36807;&#22312;&#25105;&#20204;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#19979;&#20248;&#21270;&#35757;&#32451;&#38598;&#30340;&#36793;&#38469;&#20284;&#28982;&#65292;&#22522;&#30784;LLM&#33258;&#21160;&#23398;&#20064;&#20309;&#26102;&#29983;&#25104;&#33258;&#36523;&#20197;&#21450;&#20309;&#26102;&#35843;&#29992;&#20854;&#20013;&#19968;&#20010;&#8220;&#21161;&#25163;&#8221;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#65292;&#32780;&#26080;&#38656;&#30452;&#25509;&#30417;&#30563;&#12290;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#36827;&#34892;&#26631;&#35760;&#32423;&#21327;&#20316;&#20801;&#35768;&#34701;&#21512;&#27599;&#20010;&#27169;&#22411;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#20197;&#31526;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#21327;&#20316;&#35299;&#30721;&#22312;&#36328;&#39046;&#22495;&#35774;&#32622;&#20013;&#29305;&#21035;&#26377;&#29992;&#65292;&#20854;&#20013;&#36890;&#29992;&#22522;&#30784;LLM&#23398;&#20250;&#35843;&#29992;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#12290;&#22312;&#25191;&#34892;&#25351;&#20196;&#12289;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#21644;&#25512;&#29702;&#20219;&#21153;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32852;&#21512;&#31995;&#32479;&#30340;&#24615;&#33021;&#20248;&#20110;&#21333;&#29420;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#23398;&#20064;&#21040;&#30340;&#28508;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03870v1 Announce Type: new  Abstract: We propose a method to teach multiple large language models (LLM) to collaborate by interleaving their generations at the token level. We model the decision of which LLM generates the next token as a latent variable. By optimizing the marginal likelihood of a training set under our latent variable model, the base LLM automatically learns when to generate itself and when to call on one of the ``assistant'' language models to generate, all without direct supervision. Token-level collaboration during decoding allows for a fusion of each model's expertise in a manner tailored to the specific task at hand. Our collaborative decoding is especially useful in cross-domain settings where a generalist base LLM learns to invoke domain expert models. On instruction-following, domain-specific QA, and reasoning tasks, we show that the performance of the joint system exceeds that of the individual models. Through qualitative analysis of the learned lat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32447;&#24615;&#34920;&#31034;&#30340;&#36215;&#28304;&#65292;&#36890;&#36807;&#24341;&#20837;&#31616;&#21333;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#24615;&#20559;&#24046;&#19982;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30446;&#26631;&#20849;&#21516;&#20419;&#36827;&#20102;&#27010;&#24565;&#30340;&#32447;&#24615;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.03867</link><description>&lt;p&gt;
&#35770;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32447;&#24615;&#34920;&#31034;&#30340;&#36215;&#28304;
&lt;/p&gt;
&lt;p&gt;
On the Origins of Linear Representations in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32447;&#24615;&#34920;&#31034;&#30340;&#36215;&#28304;&#65292;&#36890;&#36807;&#24341;&#20837;&#31616;&#21333;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#24615;&#20559;&#24046;&#19982;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30446;&#26631;&#20849;&#21516;&#20419;&#36827;&#20102;&#27010;&#24565;&#30340;&#32447;&#24615;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#32534;&#30721;&#20102;&#39640;&#23618;&#35821;&#20041;&#27010;&#24565;&#26159;"&#32447;&#24615;"&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#31181;&#32447;&#24615;&#34920;&#31034;&#30340;&#36215;&#28304;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26469;&#25277;&#35937;&#21644;&#24418;&#24335;&#21270;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#27010;&#24565;&#21160;&#24577;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#24418;&#24335;&#21270;&#23637;&#31034;&#20102;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30446;&#26631;&#65288;&#20855;&#26377;&#20132;&#21449;&#29109;&#30340;softmax&#65289;&#21644;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#24615;&#20559;&#24046;&#20849;&#21516;&#20419;&#36827;&#20102;&#27010;&#24565;&#30340;&#32447;&#24615;&#34920;&#31034;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#23398;&#20064;&#19982;&#28508;&#21464;&#37327;&#27169;&#22411;&#21305;&#37197;&#30340;&#25968;&#25454;&#26102;&#65292;&#32447;&#24615;&#34920;&#31034;&#20250;&#20986;&#29616;&#65292;&#20174;&#32780;&#30830;&#35748;&#36825;&#31181;&#31616;&#21333;&#32467;&#26500;&#24050;&#36275;&#20197;&#20135;&#29983;&#32447;&#24615;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992; LLaMA-2 &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#20102;&#36825;&#19968;&#29702;&#35770;&#30340;&#37096;&#20998;&#39044;&#27979;&#65292;&#35777;&#26126;&#20102;&#31616;&#21270;&#27169;&#22411;&#25552;&#20379;&#20102;&#21487;&#25512;&#24191;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03867v1 Announce Type: new  Abstract: Recent works have argued that high-level semantic concepts are encoded "linearly" in the representation space of large language models. In this work, we study the origins of such linear representations. To that end, we introduce a simple latent variable model to abstract and formalize the concept dynamics of the next token prediction. We use this formalism to show that the next token prediction objective (softmax with cross-entropy) and the implicit bias of gradient descent together promote the linear representation of concepts. Experiments show that linear representations emerge when learning from data matching the latent variable model, confirming that this simple structure already suffices to yield linear representations. We additionally confirm some predictions of the theory using the LLaMA-2 large language model, giving evidence that the simplified model yields generalizable insights.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24230;&#30340;&#25552;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#31034;&#20363;&#19982;&#27979;&#35797;&#21477;&#23376;&#30340;&#21477;&#27861;-&#35821;&#20041;&#22797;&#26434;&#24230;&#23545;&#40784;&#65292;&#22312;&#23569;&#26679;&#26412;NER&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.03861</link><description>&lt;p&gt;
&#20026;&#23569;&#26679;&#26412;&#31034;&#20363;&#36873;&#25321;&#35774;&#35745;&#20449;&#24687;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Designing Informative Metrics for Few-Shot Example Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03861
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24230;&#30340;&#25552;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#31034;&#20363;&#19982;&#27979;&#35797;&#21477;&#23376;&#30340;&#21477;&#27861;-&#35821;&#20041;&#22797;&#26434;&#24230;&#23545;&#40784;&#65292;&#22312;&#23569;&#26679;&#26412;NER&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#25552;&#20379;&#36866;&#24403;&#26684;&#24335;&#30340;&#31034;&#20363;&#26102;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#8220;&#26368;&#20339;&#8221;&#31034;&#20363;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24230;&#30340;&#25552;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#35757;&#32451;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#36873;&#25321;&#31034;&#20363;&#30340;&#27169;&#22411;&#65292;&#32780;&#26159;&#20351;&#29992;&#29305;&#23450;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#23545;&#40784;&#27979;&#35797;&#21477;&#23376;&#21644;&#31034;&#20363;&#30340;&#21477;&#27861;-&#35821;&#20041;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#21477;&#23376;&#21644;&#21333;&#35789;&#32423;&#21035;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#23558;&#31034;&#20363;&#30340;&#22797;&#26434;&#24230;&#19982;&#32771;&#34385;&#20013;&#30340;&#65288;&#27979;&#35797;&#65289;&#21477;&#23376;&#36827;&#34892;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20174;PLMs&#20013;&#25552;&#21462;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65306;&#22312;&#23569;&#26679;&#26412;NER&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;CoNLL2003&#25968;&#25454;&#38598;&#19978;&#23545;GPT-4&#30340;F1&#20998;&#25968;&#23454;&#29616;&#20102;5%&#30340;&#32477;&#23545;&#25913;&#21892;&#12290;&#25105;&#20204;&#36824;&#22312;&#20687;GPT-j-6B&#36825;&#26679;&#30340;&#36739;&#23567;&#27169;&#22411;&#20013;&#30475;&#21040;&#20102;&#39640;&#36798;28.85&#20010;&#28857;&#65288;F1/Acc.&#65289;&#30340;&#26174;&#33879;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03861v1 Announce Type: new  Abstract: Pretrained language models (PLMs) have shown remarkable few-shot learning capabilities when provided with properly formatted examples. However, selecting the "best" examples remains an open challenge. We propose a complexity-based prompt selection approach for sequence tagging tasks. This approach avoids the training of a dedicated model for selection of examples, and instead uses certain metrics to align the syntactico-semantic complexity of test sentences and examples. We use both sentence- and word-level metrics to match the complexity of examples to the (test) sentence being considered. Our results demonstrate that our approach extracts greater performance from PLMs: it achieves state-of-the-art performance on few-shot NER, achieving a 5% absolute improvement in F1 score on the CoNLL2003 dataset for GPT-4. We also see large gains of upto 28.85 points (F1/Acc.) in smaller models like GPT-j-6B.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#30740;&#31350;&#20102;&#20844;&#20849;&#25968;&#25454;&#36741;&#21161;&#30340;&#31169;&#26377;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#22312;&#38543;&#26426;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#38480;&#21046;&#21644;&#33021;&#21147;&#65292;&#23637;&#31034;&#20986;&#31616;&#21333;&#31574;&#30053;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#26368;&#20248;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03856</link><description>&lt;p&gt;
&#20844;&#20849;&#25968;&#25454;&#36741;&#21161;&#19979;&#30340;&#31169;&#26377;&#38543;&#26426;&#20248;&#21270;&#65306;&#21160;&#21147;&#21644;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Public-data Assisted Private Stochastic Optimization: Power and Limitations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03856
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#30740;&#31350;&#20102;&#20844;&#20849;&#25968;&#25454;&#36741;&#21161;&#30340;&#31169;&#26377;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#22312;&#38543;&#26426;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#38480;&#21046;&#21644;&#33021;&#21147;&#65292;&#23637;&#31034;&#20986;&#31616;&#21333;&#31574;&#30053;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20844;&#20849;&#25968;&#25454;&#36741;&#21161;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;PA-DP&#65289;&#31639;&#27861;&#30340;&#38480;&#21046;&#21644;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20851;&#27880;&#20855;&#26377;&#26631;&#35760;&#25110;&#26410;&#26631;&#35760;&#20844;&#20849;&#25968;&#25454;&#30340;&#38543;&#26426;&#20984;&#20248;&#21270;&#65288;SCO&#65289;&#38382;&#39064;&#12290;&#23545;&#20110;&#23436;&#25972;/&#26631;&#35760;&#30340;&#20844;&#20849;&#25968;&#25454;&#65292;&#25105;&#20204;&#34920;&#26126;&#20219;&#20309;$(\epsilon,\delta)$-PA-DP&#37117;&#20855;&#26377;&#36229;&#20986;&#39118;&#38505;$\tilde{\Omega}\big(\min\big\{\frac{1}{\sqrt{n_{\text{pub}}}},\frac{1}{\sqrt{n}}+\frac{\sqrt{d}}{n\epsilon} \big\} \big)$&#65292;&#20854;&#20013;$d$&#26159;&#32500;&#25968;&#65292;${n_{\text{pub}}}$&#26159;&#20844;&#20849;&#26679;&#26412;&#25968;&#37327;&#65292;${n_{\text{priv}}}$&#26159;&#31169;&#26377;&#26679;&#26412;&#25968;&#37327;&#65292;$n={n_{\text{pub}}}+{n_{\text{priv}}}$. &#36825;&#20123;&#19979;&#30028;&#26159;&#36890;&#36807;&#25105;&#20204;&#23545;PA-DP&#22343;&#20540;&#20272;&#35745;&#30340;&#26032;&#19979;&#30028;&#24314;&#31435;&#30340;&#65292;&#20854;&#24418;&#24335;&#30456;&#20284;&#12290;&#22312;&#24120;&#25968;&#22240;&#32032;&#30340;&#24433;&#21709;&#19979;&#65292;&#36825;&#20123;&#19979;&#30028;&#34920;&#26126;&#23558;&#25152;&#26377;&#25968;&#25454;&#35270;&#20026;&#31169;&#26377;&#25110;&#20002;&#24323;&#31169;&#26377;&#25968;&#25454;&#30340;&#31616;&#21333;&#31574;&#30053;&#26159;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#26410;&#26631;&#35760;&#20844;&#20849;&#25968;&#25454;&#30340;PA-DP&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03856v1 Announce Type: new  Abstract: We study the limits and capability of public-data assisted differentially private (PA-DP) algorithms. Specifically, we focus on the problem of stochastic convex optimization (SCO) with either labeled or unlabeled public data. For complete/labeled public data, we show that any $(\epsilon,\delta)$-PA-DP has excess risk $\tilde{\Omega}\big(\min\big\{\frac{1}{\sqrt{n_{\text{pub}}}},\frac{1}{\sqrt{n}}+\frac{\sqrt{d}}{n\epsilon} \big\} \big)$, where $d$ is the dimension, ${n_{\text{pub}}}$ is the number of public samples, ${n_{\text{priv}}}$ is the number of private samples, and $n={n_{\text{pub}}}+{n_{\text{priv}}}$. These lower bounds are established via our new lower bounds for PA-DP mean estimation, which are of a similar form. Up to constant factors, these lower bounds show that the simple strategy of either treating all data as private or discarding the private data, is optimal. We also study PA-DP supervised learning with \textit{unlabe
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#31639;&#27861;&#65292;&#20197;&#21152;&#36895;&#27969;&#34892;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#37319;&#26679;&#22120;&#65292;&#25913;&#36827;&#20102;&#30830;&#23450;&#24615;&#37319;&#26679;&#22120;&#30340;&#25910;&#25947;&#36895;&#29575;&#33267;$O(1/{T}^2)$&#65292;&#25552;&#21319;&#20102;&#38543;&#26426;&#37319;&#26679;&#22120;&#30340;&#25910;&#25947;&#36895;&#29575;&#33267;$O(1/T)$&#12290;</title><link>https://arxiv.org/abs/2403.03852</link><description>&lt;p&gt;
&#21152;&#36895;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#65292;&#26377;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Accelerating Convergence of Score-Based Diffusion Models, Provably
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03852
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#31639;&#27861;&#65292;&#20197;&#21152;&#36895;&#27969;&#34892;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#37319;&#26679;&#22120;&#65292;&#25913;&#36827;&#20102;&#30830;&#23450;&#24615;&#37319;&#26679;&#22120;&#30340;&#25910;&#25947;&#36895;&#29575;&#33267;$O(1/{T}^2)$&#65292;&#25552;&#21319;&#20102;&#38543;&#26426;&#37319;&#26679;&#22120;&#30340;&#25910;&#25947;&#36895;&#29575;&#33267;$O(1/T)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32463;&#39564;&#24615;&#33021;&#65292;&#20294;&#36890;&#24120;&#30001;&#20110;&#22312;&#37319;&#26679;&#38454;&#27573;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#20989;&#25968;&#35780;&#20272;&#32780;&#23548;&#33268;&#37319;&#26679;&#36895;&#24230;&#36739;&#24930;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#19968;&#31995;&#21015;&#24037;&#20316;&#33268;&#21147;&#20110;&#21152;&#36895;&#25193;&#25955;&#29983;&#25104;&#24314;&#27169;&#65292;&#20294;&#21152;&#36895;&#25216;&#26415;&#30340;&#29702;&#35770;&#22522;&#30784;&#20173;&#28982;&#20005;&#37325;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#31639;&#27861;&#26469;&#21152;&#36895;&#27969;&#34892;&#30340;&#30830;&#23450;&#24615;&#65288;&#21363;DDIM&#65289;&#21644;&#38543;&#26426;&#65288;&#21363;DDPM&#65289;&#37319;&#26679;&#22120;&#12290;&#25105;&#20204;&#30340;&#21152;&#36895;&#30830;&#23450;&#24615;&#37319;&#26679;&#22120;&#20197;$O(1/{T}^2)$&#30340;&#36895;&#29575;&#25910;&#25947;&#65292;&#20854;&#20013;$T$&#20026;&#27493;&#25968;&#65292;&#25913;&#36827;&#20102;DDIM&#37319;&#26679;&#22120;&#30340;$O(1/T)$&#36895;&#29575;&#65307;&#32780;&#25105;&#20204;&#30340;&#21152;&#36895;&#38543;&#26426;&#37319;&#26679;&#22120;&#20197;$O(1/T)$&#30340;&#36895;&#29575;&#25910;&#25947;&#65292;&#20248;&#20110;DDPM&#37319;&#26679;&#22120;&#30340;$O(1/\sqrt{T})$&#36895;&#29575;&#12290;&#25105;&#20204;&#31639;&#27861;&#30340;&#35774;&#35745;&#21033;&#29992;&#20102;&#26356;&#39640;&#38454;&#36924;&#36817;&#30340;&#35265;&#35299;&#65292;&#24182;&#20855;&#26377;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;&#39640;&#38454;ODE&#27714;&#35299;&#22120;&#30340;&#30452;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03852v1 Announce Type: cross  Abstract: Score-based diffusion models, while achieving remarkable empirical performance, often suffer from low sampling speed, due to extensive function evaluations needed during the sampling phase. Despite a flurry of recent activities towards speeding up diffusion generative modeling in practice, theoretical underpinnings for acceleration techniques remain severely limited. In this paper, we design novel training-free algorithms to accelerate popular deterministic (i.e., DDIM) and stochastic (i.e., DDPM) samplers. Our accelerated deterministic sampler converges at a rate $O(1/{T}^2)$ with $T$ the number of steps, improving upon the $O(1/T)$ rate for the DDIM sampler; and our accelerated stochastic sampler converges at a rate $O(1/T)$, outperforming the rate $O(1/\sqrt{T})$ for the DDPM sampler. The design of our algorithms leverages insights from higher-order approximation, and shares similar intuitions as popular high-order ODE solvers like 
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{MultiDimSPCI}$&#30340;&#39034;&#24207;CP&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#26500;&#24314;&#39044;&#27979;&#21306;&#22495;&#65292;&#20855;&#26377;&#26356;&#23567;&#30340;&#39044;&#27979;&#21306;&#22495;&#21644;&#26377;&#25928;&#30340;&#35206;&#30422;&#12290;</title><link>https://arxiv.org/abs/2403.03850</link><description>&lt;p&gt;
&#21033;&#29992;&#26925;&#29699;&#38598;&#36827;&#34892;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#30340;&#21512;&#35268;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction for multi-dimensional time series by ellipsoidal sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03850
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{MultiDimSPCI}$&#30340;&#39034;&#24207;CP&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#26500;&#24314;&#39044;&#27979;&#21306;&#22495;&#65292;&#20855;&#26377;&#26356;&#23567;&#30340;&#39044;&#27979;&#21306;&#22495;&#21644;&#26377;&#25928;&#30340;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#35268;&#39044;&#27979;&#65288;CP&#65289;&#22240;&#20854;&#26080;&#38656;&#20551;&#35774;&#20998;&#24067;&#12289;&#19981;&#21463;&#27169;&#22411;&#38480;&#21046;&#19988;&#22312;&#29702;&#35770;&#19978;&#21487;&#38752;&#32780;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#12290;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#39044;&#27979;&#38382;&#39064;&#65292;&#22823;&#22810;&#25968;CP&#26041;&#27861;&#19987;&#27880;&#20110;&#20026;&#21333;&#21464;&#37327;&#21709;&#24212;&#26500;&#24314;&#39044;&#27979;&#21306;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{MultiDimSPCI}$&#30340;&#39034;&#24207;CP&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#22810;&#20803;&#21709;&#24212;&#26500;&#24314;&#39044;&#27979;&#21306;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#21487;&#20132;&#25442;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29615;&#22659;&#20013;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#20272;&#35745;&#20102;&#26465;&#20214;&#35206;&#30422;&#38388;&#38553;&#30340;&#26377;&#38480;&#26679;&#26412;&#39640;&#27010;&#29575;&#30028;&#38480;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;$\texttt{MultiDimSPCI}$&#22312;&#21508;&#31181;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#19978;&#20445;&#25345;&#26377;&#25928;&#35206;&#30422;&#65292;&#21516;&#26102;&#20135;&#29983;&#27604;CP&#21644;&#38750;CP&#22522;&#32447;&#26356;&#23567;&#30340;&#39044;&#27979;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03850v1 Announce Type: cross  Abstract: Conformal prediction (CP) has been a popular method for uncertainty quantification because it is distribution-free, model-agnostic, and theoretically sound. For forecasting problems in supervised learning, most CP methods focus on building prediction intervals for univariate responses. In this work, we develop a sequential CP method called $\texttt{MultiDimSPCI}$ that builds prediction regions for a multivariate response, especially in the context of multivariate time series, which are not exchangeable. Theoretically, we estimate finite-sample high-probability bounds on the conditional coverage gap. Empirically, we demonstrate that $\texttt{MultiDimSPCI}$ maintains valid coverage on a wide range of multivariate time series while producing smaller prediction regions than CP and non-CP baselines.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Vision Mamba&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#23618;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#21644;SSM&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03849</link><description>&lt;p&gt;
MedMamba: &#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;Vision Mamba
&lt;/p&gt;
&lt;p&gt;
MedMamba: Vision Mamba for Medical Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03849
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Vision Mamba&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#23618;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#21644;SSM&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#38750;&#24120;&#22522;&#30784;&#21644;&#20851;&#38190;&#30340;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;CNN&#21644;Transformer&#30340;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20998;&#31867;&#21508;&#31181;&#21307;&#23398;&#22270;&#20687;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;CNN&#22312;&#38271;&#36317;&#31163;&#24314;&#27169;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#65292;&#26080;&#27861;&#26377;&#25928;&#25552;&#21462;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#65292;&#32780;Transformers&#21463;&#21040;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#38459;&#30861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;Mamba&#34920;&#31034;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#21487;&#20197;&#39640;&#25928;&#22320;&#24314;&#27169;&#38271;&#36317;&#31163;&#20132;&#20114;&#20316;&#29992;&#21516;&#26102;&#20445;&#25345;&#32447;&#24615;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;Vision Mamba&#65288;MedMamba&#65289;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;Conv-SSM&#27169;&#22359;&#65292;&#23558;&#21367;&#31215;&#23618;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#19982;SSM&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#20026;&#20102;&#23637;&#31034;MedMamba&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03849v1 Announce Type: cross  Abstract: Medical image classification is a very fundamental and crucial task in the field of computer vision. These years, CNN-based and Transformer-based models are widely used in classifying various medical images. Unfortunately, The limitation of CNNs in long-range modeling capabilities prevent them from effectively extracting fine-grained features in medical images , while Transformers are hampered by their quadratic computational complexity. Recent research has shown that the state space model (SSM) represented by Mamba can efficiently model long-range interactions while maintaining linear computational complexity. Inspired by this, we propose Vision Mamba for medical image classification (MedMamba). More specifically, we introduce a novel Conv-SSM module, which combines the local feature extraction ability of convolutional layers with the ability of SSM to capture long-range dependency. To demonstrate the potential of MedMamba, we conduct
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#21463;&#38480;&#30340;&#19977;&#32500;&#31354;&#38388;&#20013;&#20197;&#30446;&#26631;&#23548;&#21521;&#20026;&#22522;&#30784;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28789;&#24039;&#22235;&#32930;&#36816;&#21160;&#21644;&#24378;&#21270;&#23398;&#20064;&#20043;&#38388;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.03848</link><description>&lt;p&gt;
&#19977;&#32500;&#21463;&#38480;&#31354;&#38388;&#20013;&#30340;&#28789;&#24039;&#22235;&#32930;&#36816;&#21160;&#19982;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dexterous Legged Locomotion in Confined 3D Spaces with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03848
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#21463;&#38480;&#30340;&#19977;&#32500;&#31354;&#38388;&#20013;&#20197;&#30446;&#26631;&#23548;&#21521;&#20026;&#22522;&#30784;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28789;&#24039;&#22235;&#32930;&#36816;&#21160;&#21644;&#24378;&#21270;&#23398;&#20064;&#20043;&#38388;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#38754;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22320;&#24418;&#26102;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#22914;&#23822;&#23702;&#30340;&#23721;&#30707;&#12289;&#38750;&#21018;&#24615;&#22320;&#38754;&#21644;&#28369;&#28316;&#30340;&#34920;&#38754;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20123;&#25511;&#21046;&#22120;&#20027;&#35201;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#24213;&#37096;&#30340;&#25361;&#25112;&#65292;&#20294;&#30456;&#23545;&#36739;&#23569;&#30340;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#21463;&#38480;&#30340;&#19977;&#32500;&#31354;&#38388;&#20013;&#30340;&#22235;&#32930;&#31227;&#21160;&#24615;&#65292;&#27604;&#22914;&#29421;&#31364;&#30340;&#38567;&#36947;&#25110;&#19981;&#35268;&#21017;&#30340;&#31354;&#27934;&#65292;&#36825;&#20123;&#31354;&#38388;&#26045;&#21152;&#20102;&#20840;&#26041;&#20301;&#30340;&#32422;&#26463;&#12290;&#29616;&#26377;&#22522;&#20110;RL&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#21608;&#26399;&#24615;&#27493;&#24577;&#27169;&#24335;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;&#36816;&#21160;&#25216;&#33021;&#65292;&#36825;&#20123;&#36816;&#21160;&#25216;&#33021;&#20855;&#26377;&#36816;&#21160;&#21442;&#25968;&#65292;&#22914;&#36895;&#24230;&#21644;&#36523;&#20307;&#39640;&#24230;&#65292;&#21487;&#33021;&#19981;&#36275;&#20197;&#20351;&#26426;&#22120;&#20154;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21463;&#38480;&#19977;&#32500;&#31354;&#38388;&#20013;&#23548;&#33322;&#65292;&#38656;&#35201;&#20855;&#22791;&#25935;&#25463;&#30340;&#19977;&#32500;&#38556;&#30861;&#29289;&#22238;&#36991;&#21644;&#24378;&#22823;&#30340;&#22235;&#32930;&#36816;&#21160;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24314;&#35758;&#20174;&#22312;&#21463;&#38480;&#30340;&#19977;&#32500;&#31354;&#38388;&#20013;&#30340;&#30446;&#26631;&#23548;&#33322;&#31471;&#21040;&#31471;&#22320;&#23398;&#20064;&#36816;&#21160;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03848v1 Announce Type: cross  Abstract: Recent advances of locomotion controllers utilizing deep reinforcement learning (RL) have yielded impressive results in terms of achieving rapid and robust locomotion across challenging terrain, such as rugged rocks, non-rigid ground, and slippery surfaces. However, while these controllers primarily address challenges underneath the robot, relatively little research has investigated legged mobility through confined 3D spaces, such as narrow tunnels or irregular voids, which impose all-around constraints. The cyclic gait patterns resulted from existing RL-based methods to learn parameterized locomotion skills characterized by motion parameters, such as velocity and body height, may not be adequate to navigate robots through challenging confined 3D spaces, requiring both agile 3D obstacle avoidance and robust legged locomotion. Instead, we propose to learn locomotion skills end-to-end from goal-oriented navigation in confined 3D spaces. 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#33976;&#39311;&#20174;&#21463;&#27745;&#26579;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#20013;&#25552;&#21462;&#33391;&#24615;&#30693;&#35782;&#65292;&#23558;&#20854;&#20256;&#36882;&#32473;&#26032;&#32534;&#30721;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#25506;&#35752;&#20102;&#33976;&#39311;&#30340;&#26680;&#24515;&#32452;&#20214;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.03846</link><description>&lt;p&gt;
&#22312;&#32531;&#35299;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#20013;&#21518;&#38376;&#38382;&#39064;&#20013;&#33976;&#39311;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Effectiveness of Distillation in Mitigating Backdoors in Pre-trained Encoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03846
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#33976;&#39311;&#20174;&#21463;&#27745;&#26579;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#20013;&#25552;&#21462;&#33391;&#24615;&#30693;&#35782;&#65292;&#23558;&#20854;&#20256;&#36882;&#32473;&#26032;&#32534;&#30721;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#25506;&#35752;&#20102;&#33976;&#39311;&#30340;&#26680;&#24515;&#32452;&#20214;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#29992;&#20110;SSL&#20013;&#38450;&#24481;&#21463;&#27745;&#26579;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#21483;&#20570;&#33976;&#39311;&#65292;&#36825;&#20010;&#26041;&#27861;&#26368;&#21021;&#26159;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;&#33976;&#39311;&#26088;&#22312;&#20174;&#32473;&#23450;&#27169;&#22411;&#65288;&#31216;&#20026;&#25945;&#24072;&#32593;&#32476;&#65289;&#20013;&#25552;&#28860;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#21478;&#19968;&#20010;&#27169;&#22411;&#65288;&#31216;&#20026;&#23398;&#29983;&#32593;&#32476;&#65289;&#12290;&#25105;&#20204;&#29616;&#22312;&#20351;&#29992;&#23427;&#20174;&#21463;&#27745;&#26579;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#20013;&#25552;&#28860;&#33391;&#24615;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#19968;&#20010;&#26032;&#32534;&#30721;&#22120;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#24178;&#20928;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#33976;&#39311;&#23545;&#25239;&#21463;&#27745;&#26579;&#32534;&#30721;&#22120;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#38024;&#23545;&#39044;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#21644;&#22235;&#20010;&#24120;&#29992;&#30340;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#33976;&#39311;&#21487;&#20197;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#20174;80.87%&#38477;&#20302;&#21040;27.51%&#65292;&#32780;&#20934;&#30830;&#29575;&#19979;&#38477;&#20102;6.35%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33976;&#39311;&#30340;&#19977;&#20010;&#26680;&#24515;&#32452;&#20214;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65306;&#25945;&#24072;&#32593;&#32476;&#12289;&#23398;&#29983;&#32593;&#32476;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03846v1 Announce Type: new  Abstract: In this paper, we study a defense against poisoned encoders in SSL called distillation, which is a defense used in supervised learning originally. Distillation aims to distill knowledge from a given model (a.k.a the teacher net) and transfer it to another (a.k.a the student net). Now, we use it to distill benign knowledge from poisoned pre-trained encoders and transfer it to a new encoder, resulting in a clean pre-trained encoder. In particular, we conduct an empirical study on the effectiveness and performance of distillation against poisoned encoders. Using two state-of-the-art backdoor attacks against pre-trained image encoders and four commonly used image classification datasets, our experimental results show that distillation can reduce attack success rate from 80.87% to 27.51% while suffering a 6.35% loss in accuracy. Moreover, we investigate the impact of three core components of distillation on performance: teacher net, student n
&lt;/p&gt;</description></item><item><title>&#23558;&#29305;&#24449;&#36873;&#25321;&#35270;&#20026;&#28145;&#24230;&#39034;&#24207;&#29983;&#25104;&#23398;&#20064;&#20219;&#21153;&#65292;&#36890;&#36807;&#24320;&#21457;&#28145;&#24230;&#21464;&#20998;&#36716;&#25442;&#27169;&#22411;&#65292;&#21487;&#20197;&#31934;&#28860;&#29305;&#24449;&#36873;&#25321;&#30693;&#35782;&#24182;&#23398;&#20064;&#23884;&#20837;&#31354;&#38388;&#65292;&#23454;&#29616;&#29305;&#24449;&#23376;&#38598;&#30340;&#29983;&#25104;&#20915;&#31574;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2403.03838</link><description>&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#20316;&#20026;&#28145;&#24230;&#39034;&#24207;&#29983;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Feature Selection as Deep Sequential Generative Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03838
&lt;/p&gt;
&lt;p&gt;
&#23558;&#29305;&#24449;&#36873;&#25321;&#35270;&#20026;&#28145;&#24230;&#39034;&#24207;&#29983;&#25104;&#23398;&#20064;&#20219;&#21153;&#65292;&#36890;&#36807;&#24320;&#21457;&#28145;&#24230;&#21464;&#20998;&#36716;&#25442;&#27169;&#22411;&#65292;&#21487;&#20197;&#31934;&#28860;&#29305;&#24449;&#36873;&#25321;&#30693;&#35782;&#24182;&#23398;&#20064;&#23884;&#20837;&#31354;&#38388;&#65292;&#23454;&#29616;&#29305;&#24449;&#23376;&#38598;&#30340;&#29983;&#25104;&#20915;&#31574;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#26088;&#22312;&#30830;&#23450;&#26368;&#20855;&#22270;&#26696;&#21306;&#20998;&#29305;&#24449;&#23376;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#25152;&#36873;&#29305;&#24449;&#23376;&#38598;&#35270;&#20026;&#36873;&#25321;&#20915;&#31574;&#20196;&#29260;&#24207;&#21015;&#65292;&#23558;&#29305;&#24449;&#36873;&#25321;&#37325;&#26032;&#26500;&#24819;&#20026;&#19968;&#31181;&#28145;&#24230;&#39034;&#24207;&#29983;&#25104;&#23398;&#20064;&#20219;&#21153;&#65292;&#31934;&#28860;&#29305;&#24449;&#30693;&#35782;&#24182;&#29983;&#25104;&#20915;&#31574;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03838v1 Announce Type: new  Abstract: Feature selection aims to identify the most pattern-discriminative feature subset. In prior literature, filter (e.g., backward elimination) and embedded (e.g., Lasso) methods have hyperparameters (e.g., top-K, score thresholding) and tie to specific models, thus, hard to generalize; wrapper methods search a feature subset in a huge discrete space and is computationally costly. To transform the way of feature selection, we regard a selected feature subset as a selection decision token sequence and reformulate feature selection as a deep sequential generative learning task that distills feature knowledge and generates decision sequences. Our method includes three steps: (1) We develop a deep variational transformer model over a joint of sequential reconstruction, variational, and performance evaluator losses. Our model can distill feature selection knowledge and learn a continuous embedding space to map feature selection decision sequences
&lt;/p&gt;</description></item><item><title>Cobweb&#26159;&#19968;&#31181;&#31867;&#20284;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#31995;&#32479;&#65292;&#37319;&#29992;&#31867;&#21035;&#25928;&#29992;&#24230;&#37327;&#26500;&#24314;&#20998;&#23618;&#32452;&#32455;&#30340;&#31867;&#20284;&#26641;&#29366;&#32467;&#26500;&#65292;&#33021;&#22815;&#25429;&#25417;&#24515;&#29702;&#25928;&#24212;&#24182;&#22312;&#21333;&#19968;&#27169;&#22411;&#20013;&#23637;&#29616;&#20986;&#23454;&#20363;&#21644;&#21407;&#22411;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#65292;&#20026;&#23558;&#26469;&#30740;&#31350;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2403.03835</link><description>&lt;p&gt;
Cobweb&#65306;&#19968;&#31181;&#22686;&#37327;&#21644;&#20998;&#23618;&#24335;&#30340;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cobweb: An Incremental and Hierarchical Model of Human-Like Category Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03835
&lt;/p&gt;
&lt;p&gt;
Cobweb&#26159;&#19968;&#31181;&#31867;&#20284;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#31995;&#32479;&#65292;&#37319;&#29992;&#31867;&#21035;&#25928;&#29992;&#24230;&#37327;&#26500;&#24314;&#20998;&#23618;&#32452;&#32455;&#30340;&#31867;&#20284;&#26641;&#29366;&#32467;&#26500;&#65292;&#33021;&#22815;&#25429;&#25417;&#24515;&#29702;&#25928;&#24212;&#24182;&#22312;&#21333;&#19968;&#27169;&#22411;&#20013;&#23637;&#29616;&#20986;&#23454;&#20363;&#21644;&#21407;&#22411;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#65292;&#20026;&#23558;&#26469;&#30740;&#31350;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Cobweb&#26159;&#19968;&#31181;&#31867;&#20284;&#20154;&#31867;&#30340;&#31867;&#21035;&#23398;&#20064;&#31995;&#32479;&#65292;&#19982;&#20854;&#20182;&#22686;&#37327;&#20998;&#31867;&#27169;&#22411;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#21033;&#29992;&#31867;&#21035;&#25928;&#29992;&#24230;&#37327;&#26500;&#24314;&#20998;&#23618;&#32452;&#32455;&#30340;&#31867;&#20284;&#26641;&#29366;&#32467;&#26500;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;Cobweb&#33021;&#22815;&#25429;&#25417;&#24515;&#29702;&#25928;&#24212;&#65292;&#22914;&#22522;&#26412;&#27700;&#24179;&#12289;&#20856;&#22411;&#24615;&#21644;&#25159;&#24418;&#25928;&#24212;&#12290;&#28982;&#32780;&#65292;&#23545;Cobweb&#20316;&#20026;&#20154;&#31867;&#20998;&#31867;&#27169;&#22411;&#30340;&#26356;&#24191;&#27867;&#35780;&#20272;&#20173;&#28982;&#32570;&#20047;&#12290;&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#23427;&#30830;&#23450;&#20102;Cobweb&#19982;&#32463;&#20856;&#30340;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#25928;&#24212;&#30340;&#19968;&#33268;&#24615;&#12290;&#36824;&#25506;&#35752;&#20102;Cobweb&#23637;&#29616;&#20986;&#22312;&#21333;&#19968;&#27169;&#22411;&#20013;&#26082;&#26377;&#23454;&#20363;&#21448;&#26377;&#21407;&#22411;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#23558;&#26469;&#30740;&#31350;Cobweb&#20316;&#20026;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#30340;&#32508;&#21512;&#27169;&#22411;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03835v1 Announce Type: cross  Abstract: Cobweb, a human like category learning system, differs from other incremental categorization models in constructing hierarchically organized cognitive tree-like structures using the category utility measure. Prior studies have shown that Cobweb can capture psychological effects such as the basic level, typicality, and fan effects. However, a broader evaluation of Cobweb as a model of human categorization remains lacking. The current study addresses this gap. It establishes Cobweb's alignment with classical human category learning effects. It also explores Cobweb's flexibility to exhibit both exemplar and prototype like learning within a single model. These findings set the stage for future research on Cobweb as a comprehensive model of human category learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;L-BFGS-B&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22312;$\ell_1$&#21644;group-Lasso&#27491;&#21017;&#21270;&#19979;&#35782;&#21035;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#30456;&#27604;&#20256;&#32479;&#32447;&#24615;&#23376;&#31354;&#38388;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32467;&#26524;&#12289;&#25439;&#22833;&#21644;&#27491;&#21017;&#21270;&#39033;&#20351;&#29992;&#30340;&#36890;&#29992;&#24615;&#20197;&#21450;&#25968;&#20540;&#31283;&#23450;&#24615;&#26041;&#38754;&#36890;&#24120;&#25552;&#20379;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#21442;&#25968;&#21270;&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.03827</link><description>&lt;p&gt;
&#22312;L-BFGS-B&#31639;&#27861;&#30340;$\ell_1$&#21644;group-Lasso&#27491;&#21017;&#21270;&#19979;&#36827;&#34892;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Linear and nonlinear system identification under $\ell_1$- and group-Lasso regularization via L-BFGS-B
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;L-BFGS-B&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22312;$\ell_1$&#21644;group-Lasso&#27491;&#21017;&#21270;&#19979;&#35782;&#21035;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#30456;&#27604;&#20256;&#32479;&#32447;&#24615;&#23376;&#31354;&#38388;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32467;&#26524;&#12289;&#25439;&#22833;&#21644;&#27491;&#21017;&#21270;&#39033;&#20351;&#29992;&#30340;&#36890;&#29992;&#24615;&#20197;&#21450;&#25968;&#20540;&#31283;&#23450;&#24615;&#26041;&#38754;&#36890;&#24120;&#25552;&#20379;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#21442;&#25968;&#21270;&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;L-BFGS-B&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21487;&#33021;&#22312;$\ell_1$&#21644;group-Lasso&#27491;&#21017;&#21270;&#19979;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#12290;&#38024;&#23545;&#32447;&#24615;&#27169;&#22411;&#30340;&#35782;&#21035;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#32463;&#20856;&#32447;&#24615;&#23376;&#31354;&#38388;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#36890;&#24120;&#25552;&#20379;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#22312;&#25439;&#22833;&#21644;&#27491;&#21017;&#21270;&#39033;&#30340;&#20351;&#29992;&#26041;&#38754;&#26356;&#21152;&#36890;&#29992;&#65292;&#20063;&#22312;&#25968;&#20540;&#19978;&#26356;&#21152;&#31283;&#23450;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#20016;&#23500;&#20102;&#29616;&#26377;&#30340;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;&#24037;&#20855;&#38598;&#65292;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#35782;&#21035;&#21253;&#25324;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#38750;&#24120;&#24191;&#27867;&#30340;&#21442;&#25968;&#21270;&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#35299;&#20915;Weigand&#31561;&#20154;&#65288;2022&#24180;&#65289;&#25552;&#20986;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#19994;&#26426;&#22120;&#20154;&#22522;&#20934;&#30340;&#38750;&#32447;&#24615;&#22810;&#36755;&#20837;/&#22810;&#36755;&#20986;&#31995;&#32479;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03827v1 Announce Type: cross  Abstract: In this paper, we propose an approach for identifying linear and nonlinear discrete-time state-space models, possibly under $\ell_1$- and group-Lasso regularization, based on the L-BFGS-B algorithm. For the identification of linear models, we show that, compared to classical linear subspace methods, the approach often provides better results, is much more general in terms of the loss and regularization terms used, and is also more stable from a numerical point of view. The proposed method not only enriches the existing set of linear system identification tools but can be also applied to identifying a very broad class of parametric nonlinear state-space models, including recurrent neural networks. We illustrate the approach on synthetic and experimental datasets and apply it to solve the challenging industrial robot benchmark for nonlinear multi-input/multi-output system identification proposed by Weigand et al. (2022). A Python impleme
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#22122;&#22768;&#21442;&#25968;&#30340;&#40657;&#30418;&#27169;&#25311;&#22120;&#30340;&#40065;&#26834;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#32852;&#21512;&#32771;&#34385;&#25511;&#21046;&#21442;&#25968;&#21644;&#19981;&#30830;&#23450;&#24615;&#21442;&#25968;&#65292;&#20197;&#26377;&#25928;&#20943;&#23569;&#26041;&#24046;&#24182;&#20805;&#20998;&#21033;&#29992;&#25511;&#21046;&#19982;&#22122;&#22768;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.03816</link><description>&lt;p&gt;
&#38024;&#23545;&#26041;&#24046;&#20943;&#23569;&#65306;&#20855;&#26377;&#22122;&#22768;&#21442;&#25968;&#30340;&#40657;&#30418;&#27169;&#25311;&#22120;&#30340;&#40065;&#26834;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Targeted Variance Reduction: Robust Bayesian Optimization of Black-Box Simulators with Noise Parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03816
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#22122;&#22768;&#21442;&#25968;&#30340;&#40657;&#30418;&#27169;&#25311;&#22120;&#30340;&#40065;&#26834;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#32852;&#21512;&#32771;&#34385;&#25511;&#21046;&#21442;&#25968;&#21644;&#19981;&#30830;&#23450;&#24615;&#21442;&#25968;&#65292;&#20197;&#26377;&#25928;&#20943;&#23569;&#26041;&#24046;&#24182;&#20805;&#20998;&#21033;&#29992;&#25511;&#21046;&#19982;&#22122;&#22768;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#31185;&#23398;&#24212;&#29992;&#20013;&#65292;&#38656;&#35201;&#20248;&#21270;&#25511;&#21046;&#21442;&#25968;$\mathbf{x}$&#30340;&#40657;&#30418;&#27169;&#25311;&#22120;&#12290;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#27169;&#25311;&#22120;&#36890;&#24120;&#37319;&#29992;&#24418;&#24335;$f(\mathbf{x},\boldsymbol{\theta})$&#65292;&#20854;&#20013;$\boldsymbol{\theta}$&#26159;&#23454;&#36341;&#20013;&#19981;&#30830;&#23450;&#30340;&#21442;&#25968;&#12290;&#40065;&#26834;&#20248;&#21270;&#30340;&#30446;&#26631;&#26159;&#20248;&#21270;&#26399;&#26395;$\mathbb{E}[f(\mathbf{x},\boldsymbol{\Theta})]$&#65292;&#20854;&#20013;$\boldsymbol{\Theta} \sim \mathcal{P}$&#26159;&#27169;&#25311;$\boldsymbol{\theta}$&#19981;&#30830;&#23450;&#24615;&#30340;&#38543;&#26426;&#21464;&#37327;&#12290;&#20026;&#27492;&#65292;&#29616;&#26377;&#30340;&#40657;&#30418;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#20004;&#38454;&#27573;&#26041;&#27861;&#26469;&#36873;&#25321;&#19979;&#19968;&#20010;&#28857;$(\mathbf{x},\boldsymbol{\theta})$&#65292;&#20854;&#20013;$\mathbf{x}$&#21644;$\boldsymbol{\theta}$&#36890;&#36807;&#19981;&#21516;&#30340;&#25910;&#33719;&#20989;&#25968;&#20998;&#21035;&#20248;&#21270;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#26410;&#23545;$(\mathbf{x},\boldsymbol{\theta})$&#36827;&#34892;&#32852;&#21512;&#33719;&#21462;&#65292;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#25511;&#21046;&#19982;&#22122;&#22768;&#30456;&#20114;&#20316;&#29992;&#23454;&#29616;&#26377;&#25928;&#30340;&#40065;&#26834;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03816v1 Announce Type: cross  Abstract: The optimization of a black-box simulator over control parameters $\mathbf{x}$ arises in a myriad of scientific applications. In such applications, the simulator often takes the form $f(\mathbf{x},\boldsymbol{\theta})$, where $\boldsymbol{\theta}$ are parameters that are uncertain in practice. Robust optimization aims to optimize the objective $\mathbb{E}[f(\mathbf{x},\boldsymbol{\Theta})]$, where $\boldsymbol{\Theta} \sim \mathcal{P}$ is a random variable that models uncertainty on $\boldsymbol{\theta}$. For this, existing black-box methods typically employ a two-stage approach for selecting the next point $(\mathbf{x},\boldsymbol{\theta})$, where $\mathbf{x}$ and $\boldsymbol{\theta}$ are optimized separately via different acquisition functions. As such, these approaches do not employ a joint acquisition over $(\mathbf{x},\boldsymbol{\theta})$, and thus may fail to fully exploit control-to-noise interactions for effective robust opti
&lt;/p&gt;</description></item><item><title>ProbSAINT&#26159;&#19968;&#31181;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#26041;&#27861;&#26469;&#37327;&#21270;&#20854;&#20215;&#26684;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#25552;&#21319;&#25216;&#26415;&#30456;&#23218;&#32654;&#30340;&#20934;&#30830;&#28857;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.03812</link><description>&lt;p&gt;
ProbSAINT&#65306;&#27010;&#29575;&#34920;&#26684;&#22238;&#24402;&#29992;&#20110;&#20108;&#25163;&#36710;&#23450;&#20215;
&lt;/p&gt;
&lt;p&gt;
ProbSAINT: Probabilistic Tabular Regression for Used Car Pricing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03812
&lt;/p&gt;
&lt;p&gt;
ProbSAINT&#26159;&#19968;&#31181;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#26041;&#27861;&#26469;&#37327;&#21270;&#20854;&#20215;&#26684;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#25552;&#21319;&#25216;&#26415;&#30456;&#23218;&#32654;&#30340;&#20934;&#30830;&#28857;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#25163;&#36710;&#23450;&#20215;&#26159;&#27773;&#36710;&#34892;&#19994;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#21463;&#35768;&#22810;&#32463;&#27982;&#22240;&#32032;&#21644;&#24066;&#22330;&#21160;&#24577;&#30340;&#24433;&#21709;&#12290;&#38543;&#30528;&#22312;&#32447;&#24066;&#22330;&#30340;&#28608;&#22686;&#21644;&#20108;&#25163;&#36710;&#38656;&#27714;&#30340;&#22686;&#21152;&#65292;&#20934;&#30830;&#30340;&#23450;&#20215;&#23558;&#20351;&#20080;&#23478;&#21644;&#21334;&#23478;&#21463;&#30410;&#65292;&#30830;&#20445;&#20844;&#24179;&#20132;&#26131;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#21521;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#21160;&#23450;&#20215;&#31639;&#27861;&#30340;&#36716;&#21464;&#38656;&#35201;&#29702;&#35299;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#29305;&#21035;&#26159;&#26631;&#35760;&#27169;&#22411;&#19981;&#30830;&#23450;&#30340;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#25991;&#29486;&#25552;&#20986;&#20351;&#29992;&#25552;&#21319;&#31639;&#27861;&#25110;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#26041;&#27861;&#36827;&#34892;&#36805;&#36895;&#21644;&#31934;&#20934;&#30340;&#20215;&#26684;&#39044;&#27979;&#65292;&#20294;&#29992;&#36825;&#20123;&#31639;&#27861;&#23553;&#35013;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#38754;&#20020;&#30528;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ProbSAINT&#65292;&#36825;&#26159;&#19968;&#31181;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#20854;&#20215;&#26684;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#21450;&#19982;&#26368;&#20808;&#36827;&#30340;&#25552;&#21319;&#25216;&#26415;&#30456;&#23218;&#32654;&#30340;&#20934;&#30830;&#28857;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03812v1 Announce Type: cross  Abstract: Used car pricing is a critical aspect of the automotive industry, influenced by many economic factors and market dynamics. With the recent surge in online marketplaces and increased demand for used cars, accurate pricing would benefit both buyers and sellers by ensuring fair transactions. However, the transition towards automated pricing algorithms using machine learning necessitates the comprehension of model uncertainties, specifically the ability to flag predictions that the model is unsure about. Although recent literature proposes the use of boosting algorithms or nearest neighbor-based approaches for swift and precise price predictions, encapsulating model uncertainties with such algorithms presents a complex challenge. We introduce ProbSAINT, a model that offers a principled approach for uncertainty quantification of its price predictions, along with accurate point predictions that are comparable to state-of-the-art boosting tec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#37325;&#22797;&#30340;&#22996;&#25176;-&#20195;&#29702;&#36172;&#21338;&#28216;&#25103;&#65292;&#22312;&#20854;&#20013;&#22996;&#25176;&#26041;&#36890;&#36807;&#25552;&#20379;&#28608;&#21169;&#20197;&#24433;&#21709;&#20195;&#29702;&#26041;&#30340;&#20915;&#31574;&#65292;&#30446;&#26631;&#26159;&#36845;&#20195;&#23398;&#20064;&#28608;&#21169;&#31574;&#30053;&#26368;&#22823;&#21270;&#25928;&#29992;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;&#22996;&#25176;&#26041;&#21518;&#24724;&#30340;&#20960;&#20046;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#65292;&#25903;&#25745;&#29702;&#35770;&#20445;&#35777;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.03811</link><description>&lt;p&gt;
&#22996;&#25176;-&#20195;&#29702;&#36172;&#21338;&#28216;&#25103;&#20013;&#30340;&#28608;&#21169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Incentivized Learning in Principal-Agent Bandit Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#37325;&#22797;&#30340;&#22996;&#25176;-&#20195;&#29702;&#36172;&#21338;&#28216;&#25103;&#65292;&#22312;&#20854;&#20013;&#22996;&#25176;&#26041;&#36890;&#36807;&#25552;&#20379;&#28608;&#21169;&#20197;&#24433;&#21709;&#20195;&#29702;&#26041;&#30340;&#20915;&#31574;&#65292;&#30446;&#26631;&#26159;&#36845;&#20195;&#23398;&#20064;&#28608;&#21169;&#31574;&#30053;&#26368;&#22823;&#21270;&#25928;&#29992;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;&#22996;&#25176;&#26041;&#21518;&#24724;&#30340;&#20960;&#20046;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#65292;&#25903;&#25745;&#29702;&#35770;&#20445;&#35777;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#37325;&#22797;&#30340;&#22996;&#25176;-&#20195;&#29702;&#36172;&#21338;&#28216;&#25103;&#65292;&#22996;&#25176;&#26041;&#21482;&#33021;&#36890;&#36807;&#20195;&#29702;&#19982;&#29615;&#22659;&#20114;&#21160;&#12290;&#22996;&#25176;&#26041;&#21644;&#20195;&#29702;&#26041;&#30340;&#30446;&#26631;&#19981;&#19968;&#33268;&#65292;&#36873;&#25321;&#34892;&#21160;&#30340;&#26435;&#21033;&#20165;&#24402;&#20195;&#29702;&#26041;&#25152;&#26377;&#12290;&#28982;&#32780;&#65292;&#22996;&#25176;&#26041;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#28608;&#21169;&#26469;&#24433;&#21709;&#20195;&#29702;&#26041;&#30340;&#20915;&#31574;&#65292;&#28608;&#21169;&#23558;&#35745;&#20837;&#20854;&#22870;&#21169;&#20043;&#20013;&#12290;&#22996;&#25176;&#26041;&#30340;&#30446;&#26631;&#26159;&#36845;&#20195;&#23398;&#20064;&#19968;&#31181;&#28608;&#21169;&#31574;&#30053;&#65292;&#20197;&#26368;&#22823;&#21270;&#20854;&#24635;&#25928;&#29992;&#12290;&#36825;&#19968;&#26694;&#26550;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;&#36172;&#21338;&#38382;&#39064;&#65292;&#24182;&#21463;&#21040;&#20102;&#20960;&#20010;&#23454;&#38469;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#25110;&#29983;&#24577;&#31246;&#25910;&#65292;&#20256;&#32479;&#30340;&#26426;&#21046;&#35774;&#35745;&#29702;&#35770;&#24448;&#24448;&#24573;&#35270;&#20102;&#38382;&#39064;&#30340;&#23398;&#20064;&#26041;&#38754;&#12290;&#25105;&#20204;&#22312;&#22810;&#33218;&#21644;&#32447;&#24615;&#32972;&#26223;&#35774;&#32622;&#19979;&#25552;&#20986;&#20102;&#20851;&#20110;&#22996;&#25176;&#26041;&#21518;&#24724;&#30340;&#20960;&#20046;&#26368;&#20248;&#65288;&#30456;&#23545;&#20110;&#19968;&#20010;&#26102;&#22495; T &#30340;&#65289;&#23398;&#20064;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03811v1 Announce Type: cross  Abstract: This work considers a repeated principal-agent bandit game, where the principal can only interact with her environment through the agent. The principal and the agent have misaligned objectives and the choice of action is only left to the agent. However, the principal can influence the agent's decisions by offering incentives which add up to his rewards. The principal aims to iteratively learn an incentive policy to maximize her own total utility. This framework extends usual bandit problems and is motivated by several practical applications, such as healthcare or ecological taxation, where traditionally used mechanism design theories often overlook the learning aspect of the problem. We present nearly optimal (with respect to a horizon $T$) learning algorithms for the principal's regret in both multi-armed and linear contextual settings. Finally, we support our theoretical guarantees through numerical experiments.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#30340;&#25191;&#34892;&#35302;&#21457;&#22120;&#21487;&#20197;&#27604;&#24403;&#21069;&#25163;&#24037;&#21046;&#20316;&#30340;&#35302;&#21457;&#22120;&#26356;&#21152;&#26377;&#25928;&#65292;&#24182;&#34920;&#29616;&#20986;&#24418;&#29366;&#12289;&#23646;&#24615;&#21644;&#21151;&#33021;&#19978;&#30340;&#22266;&#26377;&#28789;&#27963;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03792</link><description>&lt;p&gt;
&#31070;&#32463;&#25191;&#34892;&#65306;&#23398;&#20064;&#25191;&#34892;&#35302;&#21457;&#22120;&#29992;&#20110;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03792
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#30340;&#25191;&#34892;&#35302;&#21457;&#22120;&#21487;&#20197;&#27604;&#24403;&#21069;&#25163;&#24037;&#21046;&#20316;&#30340;&#35302;&#21457;&#22120;&#26356;&#21152;&#26377;&#25928;&#65292;&#24182;&#34920;&#29616;&#20986;&#24418;&#29366;&#12289;&#23646;&#24615;&#21644;&#21151;&#33021;&#19978;&#30340;&#22266;&#26377;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#25191;&#34892;&#65288;Neural Exec&#65289;&#30340;&#26032;&#22411;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#12290;&#19982;&#20381;&#36182;&#25163;&#24037;&#21046;&#20316;&#30340;&#23383;&#31526;&#20018;&#65288;&#20363;&#22914;&#8220;&#24573;&#30053;&#20808;&#21069;&#30340;&#25351;&#20196;&#24182;...&#8221;&#65289;&#30340;&#24050;&#30693;&#25915;&#20987;&#19981;&#21516;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#21019;&#24314;&#25191;&#34892;&#35302;&#21457;&#22120;&#27010;&#24565;&#21270;&#20026;&#21487;&#24494;&#20998;&#25628;&#32034;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#33258;&#21160;&#29983;&#25104;&#25191;&#34892;&#35302;&#21457;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20010;&#31215;&#26497;&#36827;&#21462;&#30340;&#23545;&#25163;&#21487;&#20197;&#20266;&#36896;&#20986;&#19981;&#20165;&#27604;&#24403;&#21069;&#25163;&#24037;&#21046;&#20316;&#30340;&#35302;&#21457;&#22120;&#26356;&#21152;&#26377;&#25928;&#65292;&#32780;&#19988;&#22312;&#24418;&#29366;&#12289;&#23646;&#24615;&#21644;&#21151;&#33021;&#19978;&#34920;&#29616;&#20986;&#22266;&#26377;&#28789;&#27963;&#24615;&#30340;&#35302;&#21457;&#22120;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25915;&#20987;&#32773;&#21487;&#20197;&#35774;&#35745;&#21644;&#29983;&#25104;&#33021;&#22815;&#22312;&#32463;&#21382;&#22810;&#38454;&#27573;&#39044;&#22788;&#29702;&#27969;&#27700;&#32447;&#30340;&#24773;&#20917;&#19979;&#25345;&#20037;&#23384;&#22312;&#30340;&#31070;&#32463;&#25191;&#34892;&#65288;Neural Exec&#65289;&#65292;&#20363;&#22914;&#22312;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#24212;&#29992;&#20013;&#12290;&#26356;&#20026;&#20851;&#38190;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#29983;&#25104;&#19982;&#20219;&#20309;&#24050;&#30693;&#25915;&#20987;&#26126;&#26174;&#19981;&#21516;&#30340;&#24418;&#24335;&#21644;&#24418;&#29366;&#30340;&#35302;&#21457;&#22120;&#65292;&#32469;&#36807;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03792v1 Announce Type: cross  Abstract: We introduce a new family of prompt injection attacks, termed Neural Exec. Unlike known attacks that rely on handcrafted strings (e.g., "Ignore previous instructions and..."), we show that it is possible to conceptualize the creation of execution triggers as a differentiable search problem and use learning-based methods to autonomously generate them.   Our results demonstrate that a motivated adversary can forge triggers that are not only drastically more effective than current handcrafted ones but also exhibit inherent flexibility in shape, properties, and functionality. In this direction, we show that an attacker can design and generate Neural Execs capable of persisting through multi-stage preprocessing pipelines, such as in the case of Retrieval-Augmented Generation (RAG)-based applications. More critically, our findings show that attackers can produce triggers that deviate markedly in form and shape from any known attack, sidestep
&lt;/p&gt;</description></item><item><title>KG-TREAT&#26694;&#26550;&#36890;&#36807;&#21327;&#21516;&#24739;&#32773;&#25968;&#25454;&#21644;&#30693;&#35782;&#22270;&#35889;&#65292;&#24341;&#20837;&#21452;&#37325;&#20851;&#27880;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#28145;&#24230;&#21452;&#23618;&#27880;&#24847;&#21147;&#21327;&#21516;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#27835;&#30103;&#30456;&#20851;&#22240;&#32032;&#21644;&#32467;&#26524;&#30456;&#20851;&#22240;&#32032;&#30340;&#29420;&#31435;&#32534;&#30721;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03791</link><description>&lt;p&gt;
KG-TREAT: &#36890;&#36807;&#22312;&#24739;&#32773;&#25968;&#25454;&#21644;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#36827;&#34892;&#21327;&#21516;&#20316;&#29992;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
KG-TREAT: Pre-training for Treatment Effect Estimation by Synergizing Patient Data with Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03791
&lt;/p&gt;
&lt;p&gt;
KG-TREAT&#26694;&#26550;&#36890;&#36807;&#21327;&#21516;&#24739;&#32773;&#25968;&#25454;&#21644;&#30693;&#35782;&#22270;&#35889;&#65292;&#24341;&#20837;&#21452;&#37325;&#20851;&#27880;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#28145;&#24230;&#21452;&#23618;&#27880;&#24847;&#21147;&#21327;&#21516;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#27835;&#30103;&#30456;&#20851;&#22240;&#32032;&#21644;&#32467;&#26524;&#30456;&#20851;&#22240;&#32032;&#30340;&#29420;&#31435;&#32534;&#30721;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#65288;TEE&#65289;&#26159;&#30830;&#23450;&#21508;&#31181;&#27835;&#30103;&#23545;&#24739;&#32773;&#32467;&#26524;&#24433;&#21709;&#30340;&#20219;&#21153;&#12290;KG-TREAT&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#35268;&#27169;&#35266;&#23519;&#24615;&#24739;&#32773;&#25968;&#25454;&#19982;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#36827;&#34892;&#21327;&#21516;&#20197;&#22686;&#24378;TEE&#65292;&#20197;&#35299;&#20915;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#20381;&#36182;&#21644;&#31232;&#30095;&#21644;&#39640;&#32500;&#35266;&#23519;&#24615;&#24739;&#32773;&#25968;&#25454;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03791v1 Announce Type: cross  Abstract: Treatment effect estimation (TEE) is the task of determining the impact of various treatments on patient outcomes. Current TEE methods fall short due to reliance on limited labeled data and challenges posed by sparse and high-dimensional observational patient data. To address the challenges, we introduce a novel pre-training and fine-tuning framework, KG-TREAT, which synergizes large-scale observational patient data with biomedical knowledge graphs (KGs) to enhance TEE. Unlike previous approaches, KG-TREAT constructs dual-focus KGs and integrates a deep bi-level attention synergy method for in-depth information fusion, enabling distinct encoding of treatment-covariate and outcome-covariate relationships. KG-TREAT also incorporates two pre-training tasks to ensure a thorough grounding and contextualization of patient data and KGs. Evaluation on four downstream TEE tasks shows KG-TREAT's superiority over existing methods, with an average
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24037;&#20316;&#27969;&#31243;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#20449;&#29992;&#36829;&#32422;&#39044;&#27979;&#65292;&#26088;&#22312;&#35780;&#20272;&#20511;&#27454;&#20154;&#36829;&#32422;&#20449;&#29992;&#20041;&#21153;&#27010;&#29575;&#65292;&#21033;&#29992;&#22810;&#31181;&#25216;&#26415;&#20248;&#21183;&#35299;&#20915;&#35813;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.03785</link><description>&lt;p&gt;
&#19968;&#31181;&#35299;&#20915;&#20449;&#29992;&#36829;&#32422;&#39044;&#27979;&#38382;&#39064;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
A machine learning workflow to address credit default prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03785
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24037;&#20316;&#27969;&#31243;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#20449;&#29992;&#36829;&#32422;&#39044;&#27979;&#65292;&#26088;&#22312;&#35780;&#20272;&#20511;&#27454;&#20154;&#36829;&#32422;&#20449;&#29992;&#20041;&#21153;&#27010;&#29575;&#65292;&#21033;&#29992;&#22810;&#31181;&#25216;&#26415;&#20248;&#21183;&#35299;&#20915;&#35813;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36817;&#24180;&#26469;&#23545;&#37329;&#34701;&#31185;&#25216;&#65288;FinTech&#65289;&#30340;&#20852;&#36259;&#25345;&#32493;&#22686;&#21152;&#65292;&#20687;&#20449;&#29992;&#36829;&#32422;&#39044;&#27979;&#65288;CDP&#65289;&#36825;&#26679;&#30340;&#24212;&#29992;&#27491;&#21463;&#21040;&#37325;&#35270;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;CDP&#22312;&#35780;&#20272;&#20010;&#20154;&#21644;&#20225;&#19994;&#30340;&#20449;&#29992;&#20215;&#20540;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20351;&#36151;&#27454;&#20154;&#33021;&#22815;&#23601;&#36151;&#27454;&#25209;&#20934;&#21644;&#39118;&#38505;&#31649;&#29702;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24037;&#20316;&#27969;&#31243;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;CDP&#65292;&#36825;&#25351;&#30340;&#26159;&#35780;&#20272;&#20511;&#27454;&#20154;&#36829;&#32422;&#20854;&#20449;&#29992;&#20041;&#21153;&#30340;&#27010;&#29575;&#30340;&#20219;&#21153;&#12290;&#35813;&#24037;&#20316;&#27969;&#31243;&#30001;&#22810;&#20010;&#27493;&#39588;&#32452;&#25104;&#65292;&#27599;&#20010;&#27493;&#39588;&#26088;&#22312;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#20013;&#19981;&#21516;&#25216;&#26415;&#30340;&#20248;&#21183;&#65292;&#20174;&#32780;&#26368;&#22909;&#22320;&#35299;&#20915;CDP&#20219;&#21153;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#20840;&#38754;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#20174;&#20351;&#29992;WOE&#65288;Weight of Evidence&#65289;&#32534;&#30721;&#36827;&#34892;&#25968;&#25454;&#39044;&#22788;&#29702;&#24320;&#22987;&#65292;&#36825;&#26159;&#19968;&#31181;&#30830;&#20445;&#22312;&#19968;&#27425;&#24615;&#25968;&#25454;&#32553;&#25918;&#20013;&#21076;&#38500;&#24322;&#24120;&#20540;&#12289;&#22788;&#29702;&#32570;&#22833;&#20540;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03785v1 Announce Type: cross  Abstract: Due to the recent increase in interest in Financial Technology (FinTech), applications like credit default prediction (CDP) are gaining significant industrial and academic attention. In this regard, CDP plays a crucial role in assessing the creditworthiness of individuals and businesses, enabling lenders to make informed decisions regarding loan approvals and risk management. In this paper, we propose a workflow-based approach to improve CDP, which refers to the task of assessing the probability that a borrower will default on his or her credit obligations. The workflow consists of multiple steps, each designed to leverage the strengths of different techniques featured in machine learning pipelines and, thus best solve the CDP task. We employ a comprehensive and systematic approach starting with data preprocessing using Weight of Evidence encoding, a technique that ensures in a single-shot data scaling by removing outliers, handling mi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#38598;&#25104;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#24320;&#28304;&#24037;&#20855;&#31995;&#32479;&#65288;OpenNAS&#65289;&#65292;&#36890;&#36807;&#37319;&#29992;&#31890;&#23376;&#32676;&#21644;&#34433;&#32676;&#20248;&#21270;&#31561;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#33258;&#21160;&#29983;&#25104;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03781</link><description>&lt;p&gt;
&#20351;&#29992;&#31890;&#23376;&#32676;&#21644;&#34433;&#32676;&#20248;&#21270;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search using Particle Swarm and Ant Colony Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#38598;&#25104;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#24320;&#28304;&#24037;&#20855;&#31995;&#32479;&#65288;OpenNAS&#65289;&#65292;&#36890;&#36807;&#37319;&#29992;&#31890;&#23376;&#32676;&#21644;&#34433;&#32676;&#20248;&#21270;&#31561;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#33258;&#21160;&#29983;&#25104;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20855;&#26377;&#35768;&#22810;&#36229;&#21442;&#25968;&#65292;&#24517;&#39035;&#36873;&#25321;&#20854;&#32467;&#26500;&#12290;&#36825;&#23545;&#20110;&#21021;&#23398;&#32773;&#26469;&#35828;&#26159;&#19968;&#20010;&#27785;&#37325;&#30340;&#36127;&#25285;&#65292;&#20182;&#20204;&#38656;&#35201;&#36873;&#25321;&#21738;&#31181;&#32467;&#26500;&#20197;&#21450;&#20998;&#37197;&#32473;&#21442;&#25968;&#30340;&#20540;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#40664;&#35748;&#30340;&#36229;&#21442;&#25968;&#21644;&#32467;&#26500;&#12290;&#36890;&#36807;&#35780;&#20272;&#22810;&#31181;&#32467;&#26500;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#30340;&#36807;&#31243;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#22823;&#37327;&#36825;&#26679;&#30340;&#32467;&#26500;&#12290;&#20316;&#20026;&#26412;&#30740;&#31350;&#30340;&#19968;&#37096;&#20998;&#65292;&#24050;&#24320;&#21457;&#20102;&#19968;&#20010;&#38598;&#25104;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#24320;&#28304;&#24037;&#20855;&#31995;&#32479;&#65288;OpenNAS&#65289;&#12290;OpenNAS&#25509;&#21463;&#20219;&#20309;&#28784;&#24230;&#25110;RGB&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#24182;&#26681;&#25454;&#19968;&#31995;&#21015;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#20351;&#29992;AutoKeras&#65292;&#36801;&#31227;&#23398;&#20064;&#25110;Swarm Intelligence (SI)&#26041;&#27861;&#29983;&#25104;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03781v1 Announce Type: cross  Abstract: Neural network models have a number of hyperparameters that must be chosen along with their architecture. This can be a heavy burden on a novice user, choosing which architecture and what values to assign to parameters. In most cases, default hyperparameters and architectures are used. Significant improvements to model accuracy can be achieved through the evaluation of multiple architectures. A process known as Neural Architecture Search (NAS) may be applied to automatically evaluate a large number of such architectures. A system integrating open source tools for Neural Architecture Search (OpenNAS), in the classification of images, has been developed as part of this research. OpenNAS takes any dataset of grayscale, or RBG images, and generates Convolutional Neural Network (CNN) architectures based on a range of metaheuristics using either an AutoKeras, a transfer learning or a Swarm Intelligence (SI) approach. Particle Swarm Optimizat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26399;&#26395;&#22238;&#24402;&#27491;&#21017;&#21270;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#12290;</title><link>https://arxiv.org/abs/2403.03777</link><description>&lt;p&gt;
ENOT&#65306;&#26399;&#26395;&#22238;&#24402;&#29992;&#20110;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#30340;&#24555;&#36895;&#21644;&#20934;&#30830;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03777
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26399;&#26395;&#22238;&#24402;&#27491;&#21017;&#21270;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#20849;&#36717;&#21183;&#27491;&#21017;&#21270;&#33021;&#22815;&#20934;&#30830;&#21644;&#39640;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#12290;&#29616;&#26377;NOT&#27714;&#35299;&#22120;&#30340;&#20027;&#35201;&#29942;&#39048;&#22312;&#20110;&#25214;&#21040;&#20849;&#36717;&#31639;&#23376;&#65288;&#21363;c-transform&#65289;&#30340;&#25509;&#36817;&#31934;&#30830;&#36817;&#20284;&#30340;&#36807;&#31243;&#65292;&#36825;&#35201;&#20040;&#36890;&#36807;&#20248;&#21270;&#26368;&#23567;-&#26368;&#22823;&#30446;&#26631;&#65292;&#35201;&#20040;&#36890;&#36807;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#23545;&#21021;&#22987;&#36817;&#20284;&#39044;&#27979;&#30340;&#31934;&#32454;&#35843;&#25972;&#26469;&#23436;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#12289;&#22312;&#26399;&#26395;&#22238;&#24402;&#24418;&#24335;&#19978;&#24378;&#21046;&#36866;&#24212;&#24615;&#26465;&#20214;&#20110;&#23398;&#20064;&#23545;&#20598;&#21183;&#30340;&#29702;&#35770;&#19978;&#21512;&#29702;&#21270;&#25439;&#22833;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#36825;&#26679;&#30340;&#27491;&#21017;&#21270;&#25552;&#20379;&#20102;&#21487;&#33021;&#20849;&#36717;&#21183;&#20998;&#24067;&#30340;&#19978;&#38480;&#20272;&#35745;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#65292;&#28040;&#38500;&#20102;&#23545;&#39069;&#22806;&#24191;&#27867;&#24494;&#35843;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03777v1 Announce Type: cross  Abstract: We present a new extension for Neural Optimal Transport (NOT) training procedure, capable of accurately and efficiently estimating optimal transportation plan via specific regularisation on conjugate potentials. The main bottleneck of existing NOT solvers is associated with the procedure of finding a near-exact approximation of the conjugate operator (i.e., the c-transform), which is done either by optimizing over maximin objectives or by the computationally-intensive fine-tuning of the initial approximated prediction. We resolve both issues by proposing a new, theoretically justified loss in the form of expectile regularization that enforces binding conditions on the learning dual potentials. Such a regularization provides the upper bound estimation over the distribution of possible conjugate potentials and makes the learning stable, eliminating the need for additional extensive finetuning. We formally justify the efficiency of our me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;VeriTraCER&#65292;&#19968;&#31181;&#32852;&#21512;&#35757;&#32451;&#20998;&#31867;&#22120;&#21644;&#35299;&#37322;&#22120;&#30340;&#26041;&#27861;&#65292;&#20197;&#26174;&#24335;&#32771;&#34385;&#29983;&#25104;&#30340;CEs&#23545;&#23567;&#22411;&#27169;&#22411;&#20559;&#31227;&#30340;&#20581;&#22766;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03773</link><description>&lt;p&gt;
&#38024;&#23545;&#25968;&#25454;&#20559;&#31227;&#19979;&#21453;&#20107;&#23454;&#35299;&#37322;&#20581;&#22766;&#24615;&#30340;&#39564;&#35777;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Verified Training for Counterfactual Explanation Robustness under Data Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;VeriTraCER&#65292;&#19968;&#31181;&#32852;&#21512;&#35757;&#32451;&#20998;&#31867;&#22120;&#21644;&#35299;&#37322;&#22120;&#30340;&#26041;&#27861;&#65292;&#20197;&#26174;&#24335;&#32771;&#34385;&#29983;&#25104;&#30340;CEs&#23545;&#23567;&#22411;&#27169;&#22411;&#20559;&#31227;&#30340;&#20581;&#22766;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CEs&#65289;&#36890;&#36807;&#25551;&#36848;&#23545;&#36755;&#20837;&#30340;&#21738;&#20123;&#26356;&#25913;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#23558;&#20854;&#39044;&#27979;&#25913;&#21464;&#20026;&#25152;&#38656;&#31867;&#21035;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#20123;&#35299;&#37322;&#36890;&#24120;&#29992;&#20110;&#25351;&#23548;&#29992;&#25143;&#30340;&#34892;&#21160;&#65292;&#20363;&#22914;&#65292;&#36890;&#36807;&#25551;&#36848;&#19968;&#20010;&#36151;&#27454;&#30003;&#35831;&#34987;&#25298;&#32477;&#30340;&#29992;&#25143;&#22914;&#20309;&#22312;&#26410;&#26469;&#21487;&#20197;&#33719;&#25209;&#36151;&#27454;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#19987;&#27880;&#20110;&#21333;&#20010;&#22266;&#23450;&#27169;&#22411;&#29983;&#25104;CEs&#65292;&#24182;&#26410;&#23545;CEs&#30340;&#26410;&#26469;&#26377;&#25928;&#24615;&#25552;&#20379;&#20219;&#20309;&#27491;&#24335;&#20445;&#35777;&#12290;&#24403;&#27169;&#22411;&#23450;&#26399;&#26356;&#26032;&#20197;&#32771;&#34385;&#25968;&#25454;&#20559;&#31227;&#26102;&#65292;&#22914;&#26524;&#29983;&#25104;&#30340;CEs&#23545;&#20559;&#31227;&#19981;&#20855;&#26377;&#20581;&#22766;&#24615;&#65292;&#29992;&#25143;&#30340;&#34892;&#21160;&#21487;&#33021;&#19981;&#20877;&#23545;&#20854;&#39044;&#27979;&#20135;&#29983;&#26399;&#26395;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;VeriTraCER&#65292;&#19968;&#31181;&#32852;&#21512;&#35757;&#32451;&#20998;&#31867;&#22120;&#21644;&#35299;&#37322;&#22120;&#30340;&#26041;&#27861;&#65292;&#26126;&#30830;&#32771;&#34385;&#29983;&#25104;&#30340;CEs&#23545;&#23567;&#22411;&#27169;&#22411;&#20559;&#31227;&#30340;&#20581;&#22766;&#24615;&#12290;VeriTraCER&#36890;&#36807;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#30830;&#20445;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03773v1 Announce Type: new  Abstract: Counterfactual explanations (CEs) enhance the interpretability of machine learning models by describing what changes to an input are necessary to change its prediction to a desired class. These explanations are commonly used to guide users' actions, e.g., by describing how a user whose loan application was denied can be approved for a loan in the future. Existing approaches generate CEs by focusing on a single, fixed model, and do not provide any formal guarantees on the CEs' future validity. When models are updated periodically to account for data shift, if the generated CEs are not robust to the shifts, users' actions may no longer have the desired impacts on their predictions. This paper introduces VeriTraCER, an approach that jointly trains a classifier and an explainer to explicitly consider the robustness of the generated CEs to small model shifts. VeriTraCER optimizes over a carefully designed loss function that ensures the verifi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26377;&#25928;&#22320;&#24182;&#34892;&#21270;&#29616;&#26377;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#23545;&#25968;&#21315;&#20010;&#32500;&#24230;&#30340;&#25193;&#23637;&#65292;&#29305;&#21035;&#26159;&#23558;LiNGAM&#26041;&#27861;&#24182;&#34892;&#21270;&#65292;&#33719;&#24471;&#20102;&#22810;&#36798;32&#20493;&#30340;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2403.03772</link><description>&lt;p&gt;
&#21152;&#36895;LiNGAM: &#20197;GPU&#36895;&#24230;&#23398;&#20064;&#22240;&#26524;DAGs
&lt;/p&gt;
&lt;p&gt;
AcceleratedLiNGAM: Learning Causal DAGs at the speed of GPUs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03772
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26377;&#25928;&#22320;&#24182;&#34892;&#21270;&#29616;&#26377;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#23545;&#25968;&#21315;&#20010;&#32500;&#24230;&#30340;&#25193;&#23637;&#65292;&#29305;&#21035;&#26159;&#23558;LiNGAM&#26041;&#27861;&#24182;&#34892;&#21270;&#65292;&#33719;&#24471;&#20102;&#22810;&#36798;32&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#22522;&#20110;&#32452;&#21512;&#20248;&#21270;&#25110;&#25628;&#32034;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#36895;&#24230;&#36739;&#24930;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#26041;&#27861;&#23581;&#35797;&#36890;&#36807;&#36830;&#32493;&#20248;&#21270;&#30340;&#32467;&#26500;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#36825;&#20123;&#26041;&#27861;&#24182;&#26410;&#25552;&#20379;&#32479;&#35745;&#20445;&#35777;&#12290;&#26412;&#25991;&#36890;&#36807;&#26377;&#25928;&#22320;&#24182;&#34892;&#21270;&#29616;&#26377;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#23637;&#31034;&#25105;&#20204;&#23454;&#38469;&#19978;&#21487;&#20197;&#23558;&#23427;&#20204;&#25193;&#23637;&#21040;&#25968;&#21315;&#20010;&#32500;&#24230;&#65292;&#20351;&#20854;&#22312;&#35268;&#27169;&#26356;&#22823;&#30340;&#38382;&#39064;&#19978;&#21464;&#24471;&#23454;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;LiNGAM&#26041;&#27861;&#36827;&#34892;&#20102;&#24182;&#34892;&#21270;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#21464;&#37327;&#25968;&#37327;&#25104;&#20108;&#27425;&#20851;&#31995;&#65292;&#19982;&#29616;&#26377;&#30340;&#39034;&#24207;&#23454;&#29616;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#22810;&#36798;32&#20493;&#30340;&#21152;&#36895;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;DirectLiNGAM&#20013;&#30340;&#22240;&#26524;&#25490;&#24207;&#23376;&#36807;&#31243;&#65292;&#24182;&#23454;&#29616;&#20102;GPU&#26680;&#24515;&#20197;&#21152;&#36895;&#23427;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;DirectLiNGAM&#24212;&#29992;&#20110;&#22240;&#26524;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03772v1 Announce Type: new  Abstract: Existing causal discovery methods based on combinatorial optimization or search are slow, prohibiting their application on large-scale datasets. In response, more recent methods attempt to address this limitation by formulating causal discovery as structure learning with continuous optimization but such approaches thus far provide no statistical guarantees. In this paper, we show that by efficiently parallelizing existing causal discovery methods, we can in fact scale them to thousands of dimensions, making them practical for substantially larger-scale problems. In particular, we parallelize the LiNGAM method, which is quadratic in the number of variables, obtaining up to a 32-fold speed-up on benchmark datasets when compared with existing sequential implementations. Specifically, we focus on the causal ordering subprocedure in DirectLiNGAM and implement GPU kernels to accelerate it. This allows us to apply DirectLiNGAM to causal inferen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#31232;&#30095;&#27169;&#24335;&#23398;&#20064;&#30340;&#20449;&#36947;&#20272;&#35745;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#24310;-&#22810;&#26222;&#21202;-&#35282;&#22495;&#20449;&#36947;&#30340;&#28508;&#22312;&#32852;&#21512;&#31232;&#30095;&#24615;&#65292;&#23558;&#20449;&#36947;&#20272;&#35745;&#38382;&#39064;&#36716;&#21270;&#20026;&#31232;&#30095;&#24674;&#22797;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#23574;&#23792;&#21644;&#26495;&#22359;&#20808;&#39564;&#27169;&#22411;&#20197;&#21450;&#27491;&#20132;&#21305;&#37197;&#36861;&#36394;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#26696;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.03771</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#21512;&#31232;&#30095;&#27169;&#24335;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;MIMO-OTFS&#31995;&#32479;&#20449;&#36947;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Joint Sparsity Pattern Learning Based Channel Estimation for Massive MIMO-OTFS Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03771
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#31232;&#30095;&#27169;&#24335;&#23398;&#20064;&#30340;&#20449;&#36947;&#20272;&#35745;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#24310;-&#22810;&#26222;&#21202;-&#35282;&#22495;&#20449;&#36947;&#30340;&#28508;&#22312;&#32852;&#21512;&#31232;&#30095;&#24615;&#65292;&#23558;&#20449;&#36947;&#20272;&#35745;&#38382;&#39064;&#36716;&#21270;&#20026;&#31232;&#30095;&#24674;&#22797;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#23574;&#23792;&#21644;&#26495;&#22359;&#20808;&#39564;&#27169;&#22411;&#20197;&#21450;&#27491;&#20132;&#21305;&#37197;&#36861;&#36394;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#26696;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#31232;&#30095;&#27169;&#24335;&#23398;&#20064;&#65288;JSPL&#65289;&#30340;&#20449;&#36947;&#20272;&#35745;&#26041;&#26696;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#27491;&#20132;&#26102;&#39057;&#31354;&#65288;OTFS&#65289;&#35843;&#21046;&#36741;&#21161;&#31995;&#32479;&#12290;&#36890;&#36807;&#21033;&#29992;&#26102;&#24310;-&#22810;&#26222;&#21202;-&#35282;&#65288;DDA&#65289;&#22495;&#20449;&#36947;&#30340;&#28508;&#22312;&#32852;&#21512;&#31232;&#30095;&#24615;&#65292;&#23558;&#20449;&#36947;&#20272;&#35745;&#38382;&#39064;&#36716;&#21270;&#20026;&#31232;&#30095;&#24674;&#22797;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24212;&#29992;&#23574;&#23792;&#21644;&#26495;&#22359;&#20808;&#39564;&#27169;&#22411;&#26469;&#36845;&#20195;&#22320;&#20272;&#35745;&#20449;&#36947;&#30697;&#38453;&#30340;&#25903;&#25345;&#38598;&#65292;&#24341;&#20837;&#20381;&#36182;&#20110;&#25152;&#35782;&#21035;&#25903;&#25345;&#38598;&#30340;&#39640;&#31934;&#24230;&#21442;&#25968;&#26356;&#26032;&#35268;&#21017;&#36827;&#34892;&#36845;&#20195;&#12290;&#28982;&#21518;&#36890;&#36807;&#27491;&#20132;&#21305;&#37197;&#36861;&#36394;&#65288;OMP&#65289;&#26041;&#27861;&#20272;&#35745;&#25903;&#25345;&#38598;&#23545;&#24212;&#30340;&#20449;&#36947;&#20803;&#32032;&#30340;&#20855;&#20307;&#20540;&#12290;&#25105;&#20204;&#30340;&#20223;&#30495;&#32467;&#26524;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;JSPL&#20449;&#36947;&#20272;&#35745;&#26041;&#26696;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20195;&#34920;&#24615;&#30340;&#26368;&#26032;&#22522;&#32447;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03771v1 Announce Type: cross  Abstract: We propose a channel estimation scheme based on joint sparsity pattern learning (JSPL) for massive multi-input multi-output (MIMO) orthogonal time-frequency-space (OTFS) modulation aided systems. By exploiting the potential joint sparsity of the delay-Doppler-angle (DDA) domain channel, the channel estimation problem is transformed into a sparse recovery problem. To solve it, we first apply the spike and slab prior model to iteratively estimate the support set of the channel matrix, and a higher-accuracy parameter update rule relying on the identified support set is introduced into the iteration. Then the specific values of the channel elements corresponding to the support set are estimated by the orthogonal matching pursuit (OMP) method. Both our simulation results and analysis demonstrate that the proposed JSPL channel estimation scheme achieves an improved performance over the representative state-of-the-art baseline schemes, despit
&lt;/p&gt;</description></item><item><title>DeepCRE&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#22312;&#24739;&#32773;&#32423;&#21035;CRE&#24615;&#33021;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;17.7&#65285;&#65292;&#22312;&#25351;&#31034;&#32423;&#21035;CRE&#22686;&#21152;&#20102;5&#20493;&#65292;&#24182;&#25104;&#21151;&#30830;&#23450;&#20102;&#20845;&#20010;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#30340;&#33647;&#29289;&#20505;&#36873;&#32773;&#12290;</title><link>https://arxiv.org/abs/2403.03768</link><description>&lt;p&gt;
DeepCRE&#65306;&#21033;&#29992;&#23574;&#31471;&#35745;&#31639;&#27169;&#22411;&#25913;&#38761;&#33647;&#29289;&#30740;&#21457;
&lt;/p&gt;
&lt;p&gt;
DeepCRE: Revolutionizing Drug R&amp;D with Cutting-Edge Computational Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03768
&lt;/p&gt;
&lt;p&gt;
DeepCRE&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#22312;&#24739;&#32773;&#32423;&#21035;CRE&#24615;&#33021;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;17.7&#65285;&#65292;&#22312;&#25351;&#31034;&#32423;&#21035;CRE&#22686;&#21152;&#20102;5&#20493;&#65292;&#24182;&#25104;&#21151;&#30830;&#23450;&#20102;&#20845;&#20010;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#30340;&#33647;&#29289;&#20505;&#36873;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03768v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#33647;&#29289;&#24320;&#21457;&#39046;&#22495;&#21644;&#27835;&#30103;&#24212;&#29992;&#39046;&#22495;&#37117;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#27835;&#30103;&#39046;&#22495;&#38656;&#35201;&#26356;&#22810;&#30340;&#27835;&#30103;&#36873;&#25321;&#65292;&#21516;&#26102;&#22823;&#37327;&#26377;&#21069;&#26223;&#30340;&#20020;&#24202;&#21069;&#33647;&#29289;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#22833;&#36133;&#12290;&#19968;&#20010;&#21407;&#22240;&#26159;&#22312;&#33647;&#29289;&#24320;&#21457;&#30340;&#21518;&#26399;&#38454;&#27573;&#20132;&#21449;&#33647;&#29289;&#21453;&#24212;&#35780;&#20272;&#65288;CRE&#65289;&#30340;&#19981;&#36275;&#12290;&#23613;&#31649;&#35745;&#31639;&#26426;&#27169;&#25311;&#30340;CRE&#27169;&#22411;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#23398;&#35201;&#20040;&#23616;&#38480;&#20110;&#26089;&#26399;&#24320;&#21457;&#38454;&#27573;&#65292;&#35201;&#20040;&#32570;&#20047;&#23545;&#20840;&#38754;CRE&#20998;&#26512;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DeepCRE&#30340;&#26032;&#22411;&#35745;&#31639;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;DeepCRE&#22312;&#25512;&#21160;&#27835;&#30103;&#21457;&#29616;&#21644;&#21457;&#23637;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;DeepCRE&#36890;&#36807;&#23454;&#29616;&#24739;&#32773;&#32423;&#21035;CRE&#24179;&#22343;&#24615;&#33021;&#25552;&#39640;17.7\%&#65292;&#25351;&#31034;&#32423;&#21035;CRE&#22686;&#21152;&#20102;5&#20493;&#65292;&#20248;&#20110;&#29616;&#26377;&#26368;&#20339;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;DeepCRE&#24050;&#32463;&#30830;&#23450;&#20102;&#20845;&#20010;&#26174;&#31034;&#20986;&#26126;&#26174;&#26356;&#22823;&#20248;&#21183;&#30340;&#33647;&#29289;&#20505;&#36873;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03768v1 Announce Type: new  Abstract: The field of pharmaceutical development and therapeutic application both face substantial challenges. Therapeutic domain calls for more treatment alternatives while numerous promising pre-clinical drugs fail in clinical trails. One of the reasons is the inadequacy of Cross-drug Response Evaluation (CRE) during the late stage of drug development. Although in-silico CRE models offer a solution to this problem, existing methodologies are either limited to early development stages or lack the capacity for a comprehensive CRE analysis. Herein, we introduce a novel computational model named DeepCRE and present the potential of DeepCRE in advancing therapeutic discovery and development. DeepCRE outperforms the existing best models by achieving an average performance improvement of 17.7\% in patient-level CRE, and a 5-fold increase in indication-level CRE. Furthermore, DeepCRE has identified six drug candidates that show significantly greater ef
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#34920;&#38754;&#27963;&#24615;&#21058;&#28201;&#24230;&#20381;&#36182;&#20020;&#30028;&#33014;&#26463;&#27987;&#24230;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#27169;&#22411;&#24573;&#35270;&#28201;&#24230;&#20381;&#36182;&#24615;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.03767</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#34920;&#38754;&#27963;&#24615;&#21058;&#20020;&#30028;&#33014;&#26463;&#27987;&#24230;&#30340;&#28201;&#24230;&#20381;&#36182;&#24615;
&lt;/p&gt;
&lt;p&gt;
Predicting the Temperature Dependence of Surfactant CMCs Using Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#34920;&#38754;&#27963;&#24615;&#21058;&#28201;&#24230;&#20381;&#36182;&#20020;&#30028;&#33014;&#26463;&#27987;&#24230;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#27169;&#22411;&#24573;&#35270;&#28201;&#24230;&#20381;&#36182;&#24615;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#38754;&#27963;&#24615;&#21058;&#20998;&#23376;&#30340;&#20020;&#30028;&#33014;&#26463;&#27987;&#24230;&#65288;CMC&#65289;&#26159;&#24037;&#19994;&#20013;&#34920;&#38754;&#27963;&#24615;&#21058;&#24212;&#29992;&#20013;&#30340;&#22522;&#26412;&#24615;&#36136;&#12290;&#26368;&#36817;&#65292;&#32463;&#20856;&#30340;QSPR&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#39044;&#27979;&#23460;&#28201;&#19979;&#34920;&#38754;&#27963;&#24615;&#21058;&#30340;CMC&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23578;&#26410;&#32771;&#34385;CMC&#30340;&#28201;&#24230;&#20381;&#36182;&#24615;&#65292;&#36825;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#34920;&#38754;&#27963;&#24615;&#21058;&#28201;&#24230;&#20381;&#36182;CMC&#30340;GNN&#27169;&#22411;&#12290;&#25105;&#20204;&#20174;&#20844;&#20849;&#26469;&#28304;&#25910;&#38598;&#20102;&#22823;&#32422;1400&#20010;&#19981;&#21516;&#28201;&#24230;&#19979;&#30340;&#21508;&#31867;&#34920;&#38754;&#27963;&#24615;&#21058;&#65288;&#21363;&#31163;&#23376;&#24615;&#65292;&#38750;&#31163;&#23376;&#24615;&#21644;&#24102;&#30005;&#31163;&#23376;&#24615;&#65289;&#30340;&#25968;&#25454;&#28857;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#36136;&#37327;&#65292;&#20998;&#21035;&#32771;&#34385;&#20197;&#19979;&#24773;&#20917;&#65306;i&#65289;&#24403;&#35757;&#32451;&#27169;&#22411;&#26102;&#33267;&#23569;&#26377;&#19968;&#20010;&#19981;&#21516;&#28201;&#24230;&#19979;&#30340;&#34920;&#38754;&#27963;&#24615;&#21058;CMC&#25968;&#25454;&#23384;&#22312;&#20110;&#35757;&#32451;&#38598;&#20013;&#65292;&#21644;ii&#65289;&#24403;&#27169;&#22411;&#30340;&#35757;&#32451;&#38598;&#20013;&#27809;&#26377;&#34920;&#38754;&#27963;&#24615;&#21058;&#30340;CMC&#25968;&#25454;&#65292;&#21363;&#27867;&#21270;&#21040;&#26410;&#35265;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03767v1 Announce Type: cross  Abstract: The critical micelle concentration (CMC) of surfactant molecules is an essential property for surfactant applications in industry. Recently, classical QSPR and Graph Neural Networks (GNNs), a deep learning technique, have been successfully applied to predict the CMC of surfactants at room temperature. However, these models have not yet considered the temperature dependency of the CMC, which is highly relevant for practical applications. We herein develop a GNN model for temperature-dependent CMC prediction of surfactants. We collect about 1400 data points from public sources for all surfactant classes, i.e., ionic, nonionic, and zwitterionic, at multiple temperatures. We test the predictive quality of the model for following scenarios: i) when CMC data for surfactants are present in the training of the model in at least one different temperature, and ii) CMC data for surfactants are not present in the training, i.e., generalizing to un
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20248;&#21270;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#21327;&#35758;&#65292;&#29992;&#20110;&#36870;&#36716;&#26410;&#30693;&#37327;&#23376;&#27604;&#29305;&#37193;&#25805;&#20316;&#65292;&#23558;&#36741;&#21161;&#27604;&#29305;&#24320;&#38144;&#20943;&#23569;&#21040;3&#65292;&#26174;&#31034;&#20102;&#37327;&#23376;&#26803;&#32467;&#26500;&#30340;&#23454;&#29992;&#24615;&#21644;PQComb&#22312;&#35299;&#20915;&#22797;&#26434;&#37327;&#23376;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03761</link><description>&lt;p&gt;
&#21442;&#25968;&#21270;&#37327;&#23376;&#26803;&#21644;&#31616;&#21270;&#30005;&#36335;&#29992;&#20110;&#36870;&#36716;&#26410;&#30693;&#37327;&#23376;&#27604;&#29305;-&#37193;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Parameterized quantum comb and simpler circuits for reversing unknown qubit-unitary operations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03761
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#21327;&#35758;&#65292;&#29992;&#20110;&#36870;&#36716;&#26410;&#30693;&#37327;&#23376;&#27604;&#29305;&#37193;&#25805;&#20316;&#65292;&#23558;&#36741;&#21161;&#27604;&#29305;&#24320;&#38144;&#20943;&#23569;&#21040;3&#65292;&#26174;&#31034;&#20102;&#37327;&#23376;&#26803;&#32467;&#26500;&#30340;&#23454;&#29992;&#24615;&#21644;PQComb&#22312;&#35299;&#20915;&#22797;&#26434;&#37327;&#23376;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Quantum comb&#26159;&#37327;&#23376;&#20449;&#24687;&#22788;&#29702;&#20013;&#34920;&#24449;&#22797;&#26434;&#37327;&#23376;&#21327;&#35758;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PQComb&#65292;&#19968;&#20010;&#21033;&#29992;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#25506;&#32034;&#37327;&#23376;&#26803;&#22312;&#19968;&#33324;&#37327;&#23376;&#36807;&#31243;&#36716;&#25442;&#20219;&#21153;&#21450;&#20854;&#20182;&#26041;&#38754;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#20248;&#21270;PQComb&#36827;&#34892;&#26410;&#30693;&#37193;&#28436;&#21270;&#30340;&#26102;&#38388;&#21453;&#28446;&#27169;&#25311;&#65292;&#25105;&#20204;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#26410;&#30693;&#37327;&#23376;&#27604;&#29305;&#37193;&#21453;&#28436;&#21327;&#35758;&#65292;&#23558;&#27604;&#29616;&#26377;&#26041;&#27861;[Yoshida, Soeda, Murao, PRL 131, 120602, 2023]&#30340;&#36741;&#21161;&#27604;&#29305;&#24320;&#38144;&#20174;6&#20943;&#23569;&#21040;3&#12290;&#36825;&#23637;&#31034;&#20102;&#37327;&#23376;&#26803;&#32467;&#26500;&#30340;&#23454;&#29992;&#24615;&#65292;&#23637;&#31034;&#20102;PQComb&#22312;&#35299;&#20915;&#22797;&#26434;&#37327;&#23376;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;PQComb&#22312;&#37327;&#23376;&#35745;&#31639;&#21644;&#37327;&#23376;&#20449;&#24687;&#20013;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#24378;&#35843;&#20102;&#23427;&#22312;&#35299;&#20915;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22810;&#26679;&#38382;&#39064;&#26102;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03761v1 Announce Type: cross  Abstract: Quantum comb is an essential tool for characterizing complex quantum protocols in quantum information processing. In this work, we introduce PQComb, a framework leveraging parameterized quantum circuits to explore the capabilities of quantum combs for general quantum process transformation tasks and beyond. By optimizing PQComb for time-reversal simulations of unknown unitary evolutions, we develop a simpler protocol for unknown qubit unitary inversion that reduces the ancilla qubit overhead from 6 to 3 compared to the existing method in [Yoshida, Soeda, Murao, PRL 131, 120602, 2023]. This demonstrates the utility of quantum comb structures and showcases PQComb's potential for solving complex quantum tasks. Our results pave the way for broader PQComb applications in quantum computing and quantum information, emphasizing its versatility for tackling diverse problems in quantum machine learning.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SUPClust&#30340;&#26032;&#22411;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35782;&#21035;&#31867;&#21035;&#20043;&#38388;&#30340;&#20915;&#31574;&#36793;&#30028;&#19978;&#30340;&#28857;&#65292;&#36890;&#36807;&#26631;&#35760;&#36825;&#20123;&#28857;&#26469;&#20248;&#21270;&#27169;&#22411;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03741</link><description>&lt;p&gt;
SUPClust: &#36793;&#30028;&#22788;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SUPClust: Active Learning at the Boundaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03741
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SUPClust&#30340;&#26032;&#22411;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35782;&#21035;&#31867;&#21035;&#20043;&#38388;&#30340;&#20915;&#31574;&#36793;&#30028;&#19978;&#30340;&#28857;&#65292;&#36890;&#36807;&#26631;&#35760;&#36825;&#20123;&#28857;&#26469;&#20248;&#21270;&#27169;&#22411;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#22312;&#33719;&#21462;&#26631;&#35760;&#25968;&#25454;&#26114;&#36149;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SUPClust&#30340;&#26032;&#22411;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35782;&#21035;&#31867;&#21035;&#20043;&#38388;&#30340;&#20915;&#31574;&#36793;&#30028;&#19978;&#30340;&#28857;&#12290;&#36890;&#36807;&#38024;&#23545;&#36825;&#20123;&#28857;&#65292;SUPClust&#26088;&#22312;&#25910;&#38598;&#23545;&#20110;&#31934;&#32454;&#21270;&#27169;&#22411;&#23545;&#22797;&#26434;&#20915;&#31574;&#21306;&#22495;&#30340;&#39044;&#27979;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;&#26631;&#35760;&#36825;&#20123;&#28857;&#20250;&#23548;&#33268;&#24378;&#22823;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#21363;&#20351;&#22312;&#23384;&#22312;&#24378;&#28872;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#35266;&#23519;&#21040;&#20102;&#36825;&#31181;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03741v1 Announce Type: cross  Abstract: Active learning is a machine learning paradigm designed to optimize model performance in a setting where labeled data is expensive to acquire. In this work, we propose a novel active learning method called SUPClust that seeks to identify points at the decision boundary between classes. By targeting these points, SUPClust aims to gather information that is most informative for refining the model's prediction of complex decision regions. We demonstrate experimentally that labeling these points leads to strong model performance. This improvement is observed even in scenarios characterized by strong class imbalance.
&lt;/p&gt;</description></item><item><title>A&amp;B BNN &#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#20351;&#29992;&#21152;&#21644;&#20301;&#25805;&#20316;&#30340;&#30828;&#20214;&#21451;&#22909;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#25513;&#30721;&#23618;&#21644;&#37327;&#21270; RPReLU &#32467;&#26500;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#22312;CIFA&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.03739</link><description>&lt;p&gt;
A&amp;B BNN: A&amp;B BNN&#65306;&#20165;&#20351;&#29992;&#21152;&#21644;&#20301;&#25805;&#20316;&#30340;&#30828;&#20214;&#21451;&#22909;&#30340;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A&amp;B BNN: Add&amp;Bit-Operation-Only Hardware-Friendly Binary Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03739
&lt;/p&gt;
&lt;p&gt;
A&amp;B BNN &#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#20351;&#29992;&#21152;&#21644;&#20301;&#25805;&#20316;&#30340;&#30828;&#20214;&#21451;&#22909;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#25513;&#30721;&#23618;&#21644;&#37327;&#21270; RPReLU &#32467;&#26500;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#22312;CIFA&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;1&#20301;&#37327;&#21270;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#26469;&#20943;&#23569;&#27169;&#22411;&#30340;&#23384;&#20648;&#38656;&#27714;&#21644;&#35745;&#31639;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#20808;&#36827;&#30340;&#20108;&#20540;&#26550;&#26500;&#20173;&#28982;&#21253;&#21547;&#25968;&#30334;&#19975;&#20010;&#20302;&#25928;&#19988;&#23545;&#30828;&#20214;&#19981;&#21451;&#22909;&#30340;&#20840;&#31934;&#24230;&#20056;&#27861;&#25805;&#20316;&#12290;A&amp;B BNN &#25552;&#20986;&#20102;&#30452;&#25509;&#31227;&#38500;&#20256;&#32479; BNN &#20013;&#30340;&#37096;&#20998;&#20056;&#27861;&#25805;&#20316;&#65292;&#24182;&#29992;&#30456;&#21516;&#25968;&#37327;&#30340;&#20301;&#25805;&#20316;&#26367;&#25442;&#21097;&#20313;&#37096;&#20998;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#26080;&#24402;&#19968;&#21270;&#32593;&#32476;&#26550;&#26500;&#30340;&#25513;&#30721;&#23618;&#21644;&#37327;&#21270; RPReLU &#32467;&#26500;&#12290;&#25513;&#30721;&#23618;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992; BNN &#30340;&#20869;&#22312;&#29305;&#24449;&#20197;&#21450;&#31616;&#21333;&#30340;&#25968;&#23398;&#21464;&#25442;&#22312;&#25512;&#26029;&#26399;&#38388;&#23558;&#20854;&#31227;&#38500;&#65292;&#20197;&#36991;&#20813;&#30456;&#20851;&#30340;&#20056;&#27861;&#25805;&#20316;&#12290;&#37327;&#21270; RPReLU &#32467;&#26500;&#36890;&#36807;&#23558;&#20854;&#26012;&#29575;&#38480;&#21046;&#20026;2&#30340;&#25972;&#25968;&#24130;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#20301;&#25805;&#20316;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;CIFA&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;92.30%&#12289;69.35%&#21644;66.89%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03739v1 Announce Type: cross  Abstract: Binary neural networks utilize 1-bit quantized weights and activations to reduce both the model's storage demands and computational burden. However, advanced binary architectures still incorporate millions of inefficient and nonhardware-friendly full-precision multiplication operations. A&amp;B BNN is proposed to directly remove part of the multiplication operations in a traditional BNN and replace the rest with an equal number of bit operations, introducing the mask layer and the quantized RPReLU structure based on the normalizer-free network architecture. The mask layer can be removed during inference by leveraging the intrinsic characteristics of BNN with straightforward mathematical transformations to avoid the associated multiplication operations. The quantized RPReLU structure enables more efficient bit operations by constraining its slope to be integer powers of 2. Experimental results achieved 92.30%, 69.35%, and 66.89% on the CIFA
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Transformer-Representation&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;TNTM&#65289;&#65292;&#32467;&#21512;&#20102;transformer&#23884;&#20837;&#31354;&#38388;&#20013;&#20027;&#39064;&#34920;&#31034;&#30340;&#20248;&#21183;&#21644;&#27010;&#29575;&#24314;&#27169;&#20197;&#21450;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20027;&#39064;&#24314;&#27169;&#30340;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#24615;</title><link>https://arxiv.org/abs/2403.03737</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#34920;&#31034;&#30340;&#27010;&#29575;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Topic Modelling with Transformer Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03737
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Transformer-Representation&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;TNTM&#65289;&#65292;&#32467;&#21512;&#20102;transformer&#23884;&#20837;&#31354;&#38388;&#20013;&#20027;&#39064;&#34920;&#31034;&#30340;&#20248;&#21183;&#21644;&#27010;&#29575;&#24314;&#27169;&#20197;&#21450;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20027;&#39064;&#24314;&#27169;&#30340;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#22823;&#22810;&#30001;&#36125;&#21494;&#26031;&#22270;&#27169;&#22411;&#20027;&#23548;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;Transformer&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20852;&#36215;&#65292;&#19968;&#20123;&#20381;&#36182;&#20110;transformer&#23884;&#20837;&#31354;&#38388;&#20013;&#31616;&#21333;&#32858;&#31867;&#26041;&#27861;&#30340;&#25104;&#21151;&#27169;&#22411;&#24050;&#32463;&#20986;&#29616;&#24182;&#24041;&#22266;&#20102;&#20027;&#39064;&#20316;&#20026;&#23884;&#20837;&#21521;&#37327;&#32858;&#31867;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Transformer-Representation&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;TNTM&#65289;&#65292;&#32467;&#21512;&#20102;transformer&#23884;&#20837;&#31354;&#38388;&#20013;&#20027;&#39064;&#34920;&#31034;&#30340;&#20248;&#21183;&#21644;&#27010;&#29575;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;&#22522;&#20110;transformer&#23884;&#20837;&#30340;&#24378;&#22823;&#22810;&#21151;&#33021;&#20027;&#39064;&#27010;&#24565;&#19982;&#23436;&#20840;&#27010;&#29575;&#24314;&#27169;&#32479;&#19968;&#36215;&#26469;&#65292;&#22914;Latent Dirichlet Allocation&#65288;LDA&#65289;&#31561;&#27169;&#22411;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26694;&#26550;&#25913;&#36827;&#25512;&#29702;&#36895;&#24230;&#21644;&#24314;&#27169;&#28789;&#27963;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#19982;&#21508;&#31181;&#27169;&#22411;&#36798;&#21040;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03737v1 Announce Type: cross  Abstract: Topic modelling was mostly dominated by Bayesian graphical models during the last decade. With the rise of transformers in Natural Language Processing, however, several successful models that rely on straightforward clustering approaches in transformer-based embedding spaces have emerged and consolidated the notion of topics as clusters of embedding vectors. We propose the Transformer-Representation Neural Topic Model (TNTM), which combines the benefits of topic representations in transformer-based embedding spaces and probabilistic modelling. Therefore, this approach unifies the powerful and versatile notion of topics based on transformer embeddings with fully probabilistic modelling, as in models such as Latent Dirichlet Allocation (LDA). We utilize the variational autoencoder (VAE) framework for improved inference speed and modelling flexibility. Experimental results show that our proposed model achieves results on par with various 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#32479;&#19968;&#22270;&#20687;&#29983;&#25104;-&#21387;&#32553;&#65288;UIGC&#65289;&#33539;&#24335;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#38454;&#27573;Transformer&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#29983;&#25104;&#21644;&#21387;&#32553;&#30340;&#36807;&#31243;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#20808;&#39564;&#36827;&#34892;&#29109;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.03736</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#38454;&#27573;Transformer&#23454;&#29616;&#32479;&#19968;&#29983;&#25104;&#21644;&#21387;&#32553;&#65306;&#36229;&#20302;&#27604;&#29305;&#29575;&#22270;&#20687;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Unifying Generation and Compression: Ultra-low bitrate Image Coding Via Multi-stage Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03736
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#32479;&#19968;&#22270;&#20687;&#29983;&#25104;-&#21387;&#32553;&#65288;UIGC&#65289;&#33539;&#24335;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#38454;&#27573;Transformer&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#29983;&#25104;&#21644;&#21387;&#32553;&#30340;&#36807;&#31243;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#20808;&#39564;&#36827;&#34892;&#29109;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#29983;&#25104;&#24335;&#21387;&#32553;&#25216;&#26415;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#21387;&#32553;&#25968;&#25454;&#30340;&#24863;&#30693;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36827;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#20135;&#29983;&#39640;&#39057;&#32454;&#33410;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#29983;&#25104;&#27169;&#22411;&#25429;&#25417;&#22270;&#20687;&#20869;&#23481;&#20808;&#39564;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#22312;&#26497;&#31471;&#21387;&#32553;&#22330;&#26223;&#65288;&lt;0.05 bpp&#65289;&#20013;&#36827;&#19968;&#27493;&#38477;&#20302;&#27604;&#29305;&#29575;&#12290;&#22312;&#39044;&#27979;&#24615;&#35821;&#35328;&#27169;&#22411;&#23545;&#26080;&#25439;&#21387;&#32553;&#30340;&#33021;&#21147;&#30340;&#21551;&#21457;&#19979;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#22270;&#20687;&#29983;&#25104;-&#21387;&#32553;&#65288;UIGC&#65289;&#33539;&#24335;&#65292;&#23558;&#29983;&#25104;&#21644;&#21387;&#32553;&#30340;&#36807;&#31243;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;UIGC&#26694;&#26550;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#26159;&#37319;&#29992;&#21521;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#26631;&#35760;&#21270;&#65292;&#20197;&#21450;&#19968;&#20010;&#22810;&#38454;&#27573;Transformer&#65292;&#26088;&#22312;&#21033;&#29992;&#31354;&#38388;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#24314;&#27169;&#20808;&#39564;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#21452;&#37325;&#29992;&#36884;&#30340;&#26694;&#26550;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#23398;&#20064;&#21040;&#30340;&#20808;&#39564;&#36827;&#34892;&#29109;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03736v1 Announce Type: cross  Abstract: Recent progress in generative compression technology has significantly improved the perceptual quality of compressed data. However, these advancements primarily focus on producing high-frequency details, often overlooking the ability of generative models to capture the prior distribution of image content, thus impeding further bitrate reduction in extreme compression scenarios (&lt;0.05 bpp). Motivated by the capabilities of predictive language models for lossless compression, this paper introduces a novel Unified Image Generation-Compression (UIGC) paradigm, merging the processes of generation and compression. A key feature of the UIGC framework is the adoption of vector-quantized (VQ) image models for tokenization, alongside a multi-stage transformer designed to exploit spatial contextual information for modeling the prior distribution. As such, the dual-purpose framework effectively utilizes the learned prior for entropy estimation and
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39044;&#27979;&#26410;&#26469;&#22330;&#26223;&#65292;&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#32593;&#32476;&#26550;&#26500;&#65292;&#21516;&#26102;&#23398;&#20064;&#23545;&#35937;&#20998;&#21106;&#12289;3D&#20301;&#32622;&#25512;&#26029;&#21644;&#28145;&#24230;&#24863;&#30693;&#65292;&#20174;&#32780;&#20197;&#31867;&#20284;&#20154;&#31867;&#23156;&#20799;&#30340;&#32422;&#26463;&#26465;&#20214;&#23398;&#20064;&#29289;&#20307;&#30340;&#34920;&#31034;&#26041;&#24335;</title><link>https://arxiv.org/abs/2403.03730</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#27979;&#23398;&#20064;&#19977;&#32500;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning 3D object-centric representation through prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03730
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39044;&#27979;&#26410;&#26469;&#22330;&#26223;&#65292;&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#32593;&#32476;&#26550;&#26500;&#65292;&#21516;&#26102;&#23398;&#20064;&#23545;&#35937;&#20998;&#21106;&#12289;3D&#20301;&#32622;&#25512;&#26029;&#21644;&#28145;&#24230;&#24863;&#30693;&#65292;&#20174;&#32780;&#20197;&#31867;&#20284;&#20154;&#31867;&#23156;&#20799;&#30340;&#32422;&#26463;&#26465;&#20214;&#23398;&#20064;&#29289;&#20307;&#30340;&#34920;&#31034;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#20154;&#31867;&#26680;&#24515;&#30693;&#35782;&#30340;&#19968;&#37096;&#20998;&#65292;&#23545;&#35937;&#30340;&#34920;&#31034;&#26159;&#25903;&#25345;&#39640;&#23618;&#27010;&#24565;&#21644;&#31526;&#21495;&#25512;&#29702;&#30340;&#24515;&#29702;&#34920;&#31034;&#30340;&#22522;&#26412;&#26500;&#20214;&#12290;&#23613;&#31649;&#20154;&#31867;&#33021;&#22815;&#22312;3D&#29615;&#22659;&#20013;&#26080;&#38656;&#30417;&#30563;&#22320;&#24863;&#30693;&#23545;&#35937;&#65292;&#20294;&#32570;&#20047;&#33021;&#22815;&#20197;&#31867;&#20284;&#20110;&#20154;&#31867;&#23156;&#20799;&#38754;&#20020;&#30340;&#30456;&#20284;&#32422;&#26463;&#26465;&#20214;&#23398;&#20064;&#30456;&#21516;&#33021;&#21147;&#30340;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#21516;&#26102;&#23398;&#20064;&#20197;&#19979;&#33021;&#21147;&#65306;1) &#20174;&#31163;&#25955;&#22270;&#20687;&#20013;&#20998;&#21106;&#23545;&#35937;&#65292;2) &#25512;&#26029;&#23427;&#20204;&#30340;3D&#20301;&#32622;&#65292;&#20197;&#21450;3) &#24863;&#30693;&#28145;&#24230;&#65292;&#32780;&#20165;&#20351;&#29992;&#20102;&#30452;&#25509;&#21487;&#29992;&#20110;&#22823;&#33041;&#35757;&#32451;&#25968;&#25454;&#65292;&#21363;&#22270;&#20687;&#24207;&#21015;&#21644;&#33258;&#25105;&#36816;&#21160;&#12290;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#23545;&#35937;&#35270;&#20026;&#35270;&#35273;&#36755;&#20837;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#22823;&#33041;&#21033;&#29992;&#36825;&#20123;&#21407;&#22240;&#20570;&#20986;&#23545;&#26410;&#26469;&#22330;&#26223;&#30340;&#26377;&#25928;&#39044;&#27979;&#12290;&#36825;&#23548;&#33268;&#23545;&#35937;&#34920;&#31034;&#20316;&#20026;&#23398;&#20064;&#39044;&#27979;&#30340;&#22522;&#26412;&#21103;&#20135;&#21697;&#34987;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03730v1 Announce Type: cross  Abstract: As part of human core knowledge, the representation of objects is the building block of mental representation that supports high-level concepts and symbolic reasoning. While humans develop the ability of perceiving objects situated in 3D environments without supervision, models that learn the same set of abilities with similar constraints faced by human infants are lacking. Towards this end, we developed a novel network architecture that simultaneously learns to 1) segment objects from discrete images, 2) infer their 3D locations, and 3) perceive depth, all while using only information directly available to the brain as training data, namely: sequences of images and self-motion. The core idea is treating objects as latent causes of visual input which the brain uses to make efficient predictions of future scenes. This results in object representations being learned as an essential byproduct of learning to predict.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;TCM&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#22312;&#20027;&#21160;&#23398;&#20064;&#20013;&#25104;&#21151;&#32467;&#21512;&#20102;&#22810;&#26679;&#24615;&#37319;&#26679;&#21644;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#38382;&#39064;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#27700;&#24179;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.03728</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22312;&#20027;&#21160;&#23398;&#20064;&#20013;&#24357;&#21512;&#22810;&#26679;&#24615;&#19982;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Bridging Diversity and Uncertainty in Active learning with Self-Supervised Pre-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03728
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;TCM&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#22312;&#20027;&#21160;&#23398;&#20064;&#20013;&#25104;&#21151;&#32467;&#21512;&#20102;&#22810;&#26679;&#24615;&#37319;&#26679;&#21644;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#38382;&#39064;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#27700;&#24179;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20027;&#21160;&#23398;&#20064;&#20013;&#38598;&#25104;&#22522;&#20110;&#22810;&#26679;&#24615;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;TCM&#30340;&#31616;&#21333;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21487;&#20197;&#32531;&#35299;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#25968;&#25454;&#27700;&#24179;&#19978;&#20445;&#25345;&#24378;&#22823;&#24615;&#33021;&#12290;&#36890;&#36807;&#39318;&#20808;&#24212;&#29992;TypiClust&#36827;&#34892;&#22810;&#26679;&#24615;&#37319;&#26679;&#65292;&#38543;&#21518;&#36807;&#28193;&#21040;&#20351;&#29992;Margin&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TCM&#22312;&#20302;&#25968;&#25454;&#21644;&#39640;&#25968;&#25454;&#24773;&#20917;&#19979;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03728v1 Announce Type: cross  Abstract: This study addresses the integration of diversity-based and uncertainty-based sampling strategies in active learning, particularly within the context of self-supervised pre-trained models. We introduce a straightforward heuristic called TCM that mitigates the cold start problem while maintaining strong performance across various data levels. By initially applying TypiClust for diversity sampling and subsequently transitioning to uncertainty sampling with Margin, our approach effectively combines the strengths of both strategies. Our experiments demonstrate that TCM consistently outperforms existing methods across various datasets in both low and high data regimes.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;DiMA&#27169;&#22411;&#65292;&#22312;&#34507;&#30333;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#25193;&#25955;&#26469;&#29983;&#25104;&#27688;&#22522;&#37240;&#24207;&#21015;&#65292;&#27604;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#26469;&#37327;&#21270;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03726</link><description>&lt;p&gt;
&#34507;&#30333;&#36136;&#24207;&#21015;&#29983;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Diffusion on language model embeddings for protein sequence generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03726
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;DiMA&#27169;&#22411;&#65292;&#22312;&#34507;&#30333;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#25193;&#25955;&#26469;&#29983;&#25104;&#27688;&#22522;&#37240;&#24207;&#21015;&#65292;&#27604;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#26469;&#37327;&#21270;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#35774;&#35745;&#38656;&#35201;&#23545;&#34507;&#30333;&#36136;&#23431;&#23449;&#22266;&#26377;&#22797;&#26434;&#24615;&#30340;&#28145;&#20837;&#20102;&#35299;&#12290;&#23613;&#31649;&#35768;&#22810;&#24037;&#20316;&#20542;&#21521;&#20110;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#25110;&#19987;&#27880;&#20110;&#29305;&#23450;&#34507;&#30333;&#36136;&#23478;&#26063;&#65292;&#20294;&#26080;&#26465;&#20214;&#29983;&#25104;&#30340;&#22522;&#30784;&#20219;&#21153;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#21644;&#37325;&#35270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#32034;&#36825;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#24341;&#20837;&#20102;DiMA&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#20174;&#34507;&#30333;&#35821;&#35328;&#27169;&#22411;ESM-2&#34893;&#29983;&#30340;&#23884;&#20837;&#36827;&#34892;&#36830;&#32493;&#25193;&#25955;&#20197;&#29983;&#25104;&#27688;&#22522;&#37240;&#24207;&#21015;&#30340;&#27169;&#22411;&#12290;DiMA&#36229;&#36234;&#20102;&#21253;&#25324;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#21644;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#22312;&#20869;&#30340;&#20027;&#35201;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#23450;&#37327;&#22320;&#35828;&#26126;&#20102;&#23548;&#33268;&#20854;&#21331;&#36234;&#24615;&#33021;&#30340;&#35774;&#35745;&#36873;&#25321;&#25152;&#24102;&#26469;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25351;&#26631;&#36328;&#22810;&#31181;&#24418;&#24335;&#24191;&#27867;&#35780;&#20272;&#29983;&#25104;&#24207;&#21015;&#30340;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#12289;&#20998;&#24067;&#30456;&#20284;&#24615;&#21644;&#29983;&#29289;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20135;&#29983;&#26032;&#39062;&#12289;&#22810;&#26679;&#21270;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#65292;&#31934;&#20934;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03726v1 Announce Type: cross  Abstract: Protein design requires a deep understanding of the inherent complexities of the protein universe. While many efforts lean towards conditional generation or focus on specific families of proteins, the foundational task of unconditional generation remains underexplored and undervalued. Here, we explore this pivotal domain, introducing DiMA, a model that leverages continuous diffusion on embeddings derived from the protein language model, ESM-2, to generate amino acid sequences. DiMA surpasses leading solutions, including autoregressive transformer-based and discrete diffusion models, and we quantitatively illustrate the impact of the design choices that lead to its superior performance. We extensively evaluate the quality, diversity, distribution similarity, and biological relevance of the generated sequences using multiple metrics across various modalities. Our approach consistently produces novel, diverse protein sequences that accura
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20026;&#27431;&#27954;&#20013;&#31243;&#27668;&#35937;&#20013;&#24515;&#30340;&#38598;&#25104;&#39044;&#27979;&#31995;&#32479;&#24320;&#21457;&#27169;&#22411;&#35823;&#24046;&#26657;&#27491;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#22312;&#34920;&#31034;&#21160;&#21147;&#24179;&#34913;&#21644;&#36866;&#29992;&#20110;&#25968;&#25454;&#21516;&#21270;&#23454;&#39564;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.03702</link><description>&lt;p&gt;
&#22312;&#32447;&#27169;&#22411;&#35823;&#24046;&#26657;&#27491;&#19982;&#31070;&#32463;&#32593;&#32476;: &#24212;&#29992;&#20110;&#38598;&#25104;&#39044;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Online model error correction with neural networks: application to the Integrated Forecasting System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03702
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20026;&#27431;&#27954;&#20013;&#31243;&#27668;&#35937;&#20013;&#24515;&#30340;&#38598;&#25104;&#39044;&#27979;&#31995;&#32479;&#24320;&#21457;&#27169;&#22411;&#35823;&#24046;&#26657;&#27491;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#22312;&#34920;&#31034;&#21160;&#21147;&#24179;&#34913;&#21644;&#36866;&#29992;&#20110;&#25968;&#25454;&#21516;&#21270;&#23454;&#39564;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22312;&#20840;&#29699;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#24320;&#21457;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#20855;&#26377;&#20854;&#20248;&#21183;&#65292;&#23588;&#20854;&#26159;&#20934;&#30830;&#24615;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#20294;&#20063;&#23384;&#22312;&#20854;&#24369;&#28857;&#65306;&#23427;&#20204;&#38590;&#20197;&#34920;&#31034;&#22522;&#26412;&#21160;&#21147;&#24179;&#34913;&#65292;&#24182;&#19988;&#36828;&#26410;&#36866;&#29992;&#20110;&#36164;&#26009;&#21516;&#21270;&#23454;&#39564;&#12290;&#28151;&#21512;&#24314;&#27169;&#20986;&#29616;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28151;&#21512;&#27169;&#22411;&#23558;&#22522;&#20110;&#29289;&#29702;&#30340;&#26680;&#24515;&#32452;&#20214;&#19982;&#32479;&#35745;&#32452;&#20214;&#65288;&#36890;&#24120;&#26159;&#31070;&#32463;&#32593;&#32476;&#65289;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20197;&#22686;&#24378;&#39044;&#27979;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20026;&#27431;&#27954;&#20013;&#31243;&#27668;&#35937;&#20013;&#24515;&#30340;&#36816;&#34892;&#38598;&#25104;&#39044;&#27979;&#31995;&#32479;&#65288;IFS&#65289;&#24320;&#21457;&#27169;&#22411;&#35823;&#24046;&#26657;&#27491;&#12290;&#31070;&#32463;&#32593;&#32476;&#26368;&#21021;&#20250;&#31163;&#32447;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#22823;&#37327;&#36816;&#34892;&#20998;&#26512;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03702v1 Announce Type: cross  Abstract: In recent years, there has been significant progress in the development of fully data-driven global numerical weather prediction models. These machine learning weather prediction models have their strength, notably accuracy and low computational requirements, but also their weakness: they struggle to represent fundamental dynamical balances, and they are far from being suitable for data assimilation experiments. Hybrid modelling emerges as a promising approach to address these limitations. Hybrid models integrate a physics-based core component with a statistical component, typically a neural network, to enhance prediction capabilities. In this article, we propose to develop a model error correction for the operational Integrated Forecasting System (IFS) of the European Centre for Medium-Range Weather Forecasts using a neural network. The neural network is initially pre-trained offline using a large dataset of operational analyses and a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20174;&#29702;&#35770;&#21040;LLM&#26696;&#20363;&#30740;&#31350;&#32508;&#36848;&#20102;&#20998;&#24067;&#24335;&#22522;&#30784;&#35774;&#26045;&#19978;&#30340;&#27169;&#22411;&#24182;&#34892;&#24615;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#24182;&#34892;&#24615;&#31867;&#22411;&#12289;&#25361;&#25112;&#21644;&#29616;&#20195;&#29992;&#20363;&#12290;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#25805;&#20316;&#20869;&#37096;&#21644;&#25805;&#20316;&#20043;&#38388;&#24182;&#34892;&#21270;&#65292;&#20294;&#23454;&#26045;&#25361;&#25112;&#21253;&#25324;&#25805;&#20316;&#22270;&#30340;&#26368;&#20339;&#21010;&#20998;&#12290;</title><link>https://arxiv.org/abs/2403.03699</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#22522;&#30784;&#35774;&#26045;&#19978;&#30340;&#27169;&#22411;&#24182;&#34892;&#24615;&#65306;&#20174;&#29702;&#35770;&#21040;LLM&#26696;&#20363;&#30740;&#31350;&#30340;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Model Parallelism on Distributed Infrastructure: A Literature Review from Theory to LLM Case-Studies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03699
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20174;&#29702;&#35770;&#21040;LLM&#26696;&#20363;&#30740;&#31350;&#32508;&#36848;&#20102;&#20998;&#24067;&#24335;&#22522;&#30784;&#35774;&#26045;&#19978;&#30340;&#27169;&#22411;&#24182;&#34892;&#24615;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#24182;&#34892;&#24615;&#31867;&#22411;&#12289;&#25361;&#25112;&#21644;&#29616;&#20195;&#29992;&#20363;&#12290;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#25805;&#20316;&#20869;&#37096;&#21644;&#25805;&#20316;&#20043;&#38388;&#24182;&#34892;&#21270;&#65292;&#20294;&#23454;&#26045;&#25361;&#25112;&#21253;&#25324;&#25805;&#20316;&#22270;&#30340;&#26368;&#20339;&#21010;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30707;&#12290;&#38543;&#30528;&#36825;&#20123;&#32593;&#32476;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#36235;&#21183;&#25345;&#32493;&#36827;&#34892;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#37096;&#32626;&#30340;&#22522;&#30784;&#30828;&#20214;&#21644;&#36719;&#20214;&#22522;&#30784;&#35774;&#26045;&#20063;&#22312;&#19981;&#26029;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#22238;&#31572;&#20102;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;&#65306;&#8220;&#23384;&#22312;&#21738;&#20123;&#27169;&#22411;&#24182;&#34892;&#24615;&#31867;&#22411;&#65311;&#8221;&#65292;&#8220;&#27169;&#22411;&#24182;&#34892;&#24615;&#30340;&#25361;&#25112;&#26159;&#20160;&#20040;&#65311;&#8221;&#65292;&#20197;&#21450;&#8220;&#27169;&#22411;&#24182;&#34892;&#24615;&#30340;&#29616;&#20195;&#29992;&#20363;&#26159;&#20160;&#20040;&#65311;&#8221; &#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#30340;&#24182;&#34892;&#21270;&#26041;&#24335;&#20197;&#21450;&#23558;&#20854;&#34920;&#36798;&#20026;&#25805;&#20316;&#22270;&#26469;&#22238;&#31572;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#25506;&#32034;&#21487;&#29992;&#30340;&#32500;&#24230;&#12290;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#24182;&#34892;&#21270;&#30340;&#32500;&#24230;&#21253;&#25324;&#25805;&#20316;&#20869;&#37096;&#24182;&#34892;&#21644;&#25805;&#20316;&#20043;&#38388;&#24182;&#34892;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#21644;&#21015;&#20986;&#24182;&#34892;&#21270;&#31867;&#22411;&#30340;&#23454;&#26045;&#25361;&#25112;&#20197;&#21450;&#25805;&#20316;&#22270;&#30340;&#26368;&#20339;&#21010;&#20998;&#38382;&#39064;&#26469;&#22238;&#31572;&#31532;&#20108;&#20010;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#21644;&#21015;&#20986;&#24182;&#34892;&#21270;&#22914;&#20309;&#24212;&#29992;&#22312;m
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03699v1 Announce Type: cross  Abstract: Neural networks have become a cornerstone of machine learning. As the trend for these to get more and more complex continues, so does the underlying hardware and software infrastructure for training and deployment. In this survey we answer three research questions: "What types of model parallelism exist?", "What are the challenges of model parallelism?", and "What is a modern use-case of model parallelism?" We answer the first question by looking at how neural networks can be parallelised and expressing these as operator graphs while exploring the available dimensions. The dimensions along which neural networks can be parallelised are intra-operator and inter-operator. We answer the second question by collecting and listing both implementation challenges for the types of parallelism, as well as the problem of optimally partitioning the operator graph. We answer the last question by collecting and listing how parallelism is applied in m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102; Controllable Time Series (CTS) &#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32806;&#26144;&#23556;&#36807;&#31243;&#26469;&#23454;&#29616;&#23545;&#22797;&#26434;&#20132;&#20114;&#27169;&#24335;&#30340;&#31934;&#30830;&#23398;&#20064;&#65292;&#20174;&#32780;&#21019;&#26032;&#20102;&#38024;&#23545;&#21487;&#25511;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104; (CTSG) &#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03698</link><description>&lt;p&gt;
&#26397;&#21521;&#21487;&#25511;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Controllable Time Series Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03698
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102; Controllable Time Series (CTS) &#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32806;&#26144;&#23556;&#36807;&#31243;&#26469;&#23454;&#29616;&#23545;&#22797;&#26434;&#20132;&#20114;&#27169;&#24335;&#30340;&#31934;&#30830;&#23398;&#20064;&#65292;&#20174;&#32780;&#21019;&#26032;&#20102;&#38024;&#23545;&#21487;&#25511;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104; (CTSG) &#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#65288;TSG&#65289;&#24050;&#32463;&#25104;&#20026;&#21512;&#25104;&#20934;&#30830;&#21453;&#26144;&#29616;&#23454;&#19990;&#30028;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21464;&#24471;&#19981;&#21487;&#25110;&#32570;&#12290;&#23613;&#31649;TSG&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#32463;&#24120;&#21462;&#20915;&#20110;&#20855;&#26377;&#22823;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#20381;&#36182;&#24615;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#32597;&#35265;&#25110;&#29420;&#29305;&#26465;&#20214;&#26102;&#65292;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#21363;&#21487;&#25511;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#65288;CTSG&#65289;&#65292;&#26088;&#22312;&#20135;&#29983;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#22806;&#37096;&#26465;&#20214;&#30340;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#65292;&#20174;&#32780;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03698v1 Announce Type: cross  Abstract: Time Series Generation (TSG) has emerged as a pivotal technique in synthesizing data that accurately mirrors real-world time series, becoming indispensable in numerous applications. Despite significant advancements in TSG, its efficacy frequently hinges on having large training datasets. This dependency presents a substantial challenge in data-scarce scenarios, especially when dealing with rare or unique conditions. To confront these challenges, we explore a new problem of Controllable Time Series Generation (CTSG), aiming to produce synthetic time series that can adapt to various external conditions, thereby tackling the data scarcity issue.   In this paper, we propose \textbf{C}ontrollable \textbf{T}ime \textbf{S}eries (\textsf{CTS}), an innovative VAE-agnostic framework tailored for CTSG. A key feature of \textsf{CTS} is that it decouples the mapping process from standard VAE training, enabling precise learning of a complex interpla
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#19981;&#22343;&#21248;&#23574;&#23792;&#32500;&#26684;&#32435;&#27169;&#22411;&#30340;&#26368;&#20248;&#20809;&#35889;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#36716;&#21270;&#30697;&#38453;&#30340;&#28145;&#20837;&#20005;&#26684;&#20998;&#26512;&#65292;&#22312;&#26368;&#20339;&#38408;&#20540;&#22788;&#23454;&#29616;&#20102;&#24322;&#24120;&#20540;&#21644;&#27491;&#37325;&#21472;&#30340;&#30456;&#20301;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2403.03695</link><description>&lt;p&gt;
&#20855;&#26377;&#22359;&#32467;&#26500;&#23574;&#23792;&#27169;&#22411;&#20013;&#30340;&#20809;&#35889;&#30456;&#20301;&#36716;&#25442;&#21644;&#26368;&#20248;PCA
&lt;/p&gt;
&lt;p&gt;
Spectral Phase Transition and Optimal PCA in Block-Structured Spiked models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03695
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#19981;&#22343;&#21248;&#23574;&#23792;&#32500;&#26684;&#32435;&#27169;&#22411;&#30340;&#26368;&#20248;&#20809;&#35889;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#36716;&#21270;&#30697;&#38453;&#30340;&#28145;&#20837;&#20005;&#26684;&#20998;&#26512;&#65292;&#22312;&#26368;&#20339;&#38408;&#20540;&#22788;&#23454;&#29616;&#20102;&#24322;&#24120;&#20540;&#21644;&#27491;&#37325;&#21472;&#30340;&#30456;&#20301;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35752;&#35770;&#20102;&#19981;&#22343;&#21248;&#23574;&#23792;&#32500;&#26684;&#32435;&#27169;&#22411;&#65292;&#36825;&#26159;&#26368;&#36817;&#24341;&#20837;&#30340;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#30740;&#31350;&#21508;&#31181;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#32467;&#26500;&#22122;&#22768;&#65292;&#36890;&#36807;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#26865;&#38236;&#65292;&#29305;&#21035;&#20851;&#27880;&#20854;&#20809;&#35889;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#31181;&#26368;&#20248;&#30340;&#20809;&#35889;&#26041;&#27861;&#65292;&#24182;&#23558;&#22312;&#19981;&#22343;&#21248;&#65292;&#22359;&#32467;&#26500;&#30340;&#32500;&#26684;&#32435;&#27169;&#22411;&#20013;&#26377;&#21517;&#30340;\cite{BBP}&#65288;BBP&#65289;&#30456;&#20301;&#36716;&#25442;&#20934;&#21017;&#25193;&#23637;&#21040;&#25105;&#20204;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23545;&#19968;&#20010;&#36716;&#25442;&#30697;&#38453;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#20005;&#26684;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#20986;&#29616;1&#65289;&#22312;&#26497;&#38480;&#20809;&#35889;&#20998;&#24067;&#30340;&#32676;&#22806;&#30340;&#24322;&#24120;&#20540;&#21644;2&#65289;&#30456;&#20851;&#29305;&#24449;&#21521;&#37327;&#19982;&#20449;&#21495;&#20043;&#38388;&#30340;&#27491;&#37325;&#21472;&#30340;&#36716;&#21464;&#27491;&#22909;&#21457;&#29983;&#22312;&#26368;&#20339;&#38408;&#20540;&#22788;&#65292;&#20351;&#24471;&#25152;&#25552;&#20986;&#30340;&#20809;&#35889;&#26041;&#27861;&#22312;&#19981;&#22343;&#21248;&#32500;&#26684;&#32435;&#38382;&#39064;&#30340;&#36845;&#20195;&#26041;&#27861;&#31867;&#20013;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03695v1 Announce Type: cross  Abstract: We discuss the inhomogeneous spiked Wigner model, a theoretical framework recently introduced to study structured noise in various learning scenarios, through the prism of random matrix theory, with a specific focus on its spectral properties. Our primary objective is to find an optimal spectral method and to extend the celebrated \cite{BBP} (BBP) phase transition criterion -- well-known in the homogeneous case -- to our inhomogeneous, block-structured, Wigner model. We provide a thorough rigorous analysis of a transformed matrix and show that the transition for the appearance of 1) an outlier outside the bulk of the limiting spectral distribution and 2) a positive overlap between the associated eigenvector and the signal, occurs precisely at the optimal threshold, making the proposed spectral method optimal within the class of iterative methods for the inhomogeneous Wigner problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31616;&#21270;&#20102;PCNet&#24182;&#22686;&#24378;&#20102;&#20854;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#25193;&#23637;&#28388;&#27874;&#22120;&#38454;&#25968;&#24182;&#20943;&#23569;&#21442;&#25968;&#65292;&#20197;&#21450;&#23454;&#29616;&#36866;&#24212;&#24615;&#37051;&#22495;&#22823;&#23567;&#30340;&#21464;&#20307;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#22270;&#32467;&#26500;&#25200;&#21160;&#25110;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03676</link><description>&lt;p&gt;
&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#31616;&#21270;PCNet
&lt;/p&gt;
&lt;p&gt;
Simplified PCNet with Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31616;&#21270;&#20102;PCNet&#24182;&#22686;&#24378;&#20102;&#20854;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#25193;&#23637;&#28388;&#27874;&#22120;&#38454;&#25968;&#24182;&#20943;&#23569;&#21442;&#25968;&#65292;&#20197;&#21450;&#23454;&#29616;&#36866;&#24212;&#24615;&#37051;&#22495;&#22823;&#23567;&#30340;&#21464;&#20307;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#22270;&#32467;&#26500;&#25200;&#21160;&#25110;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22240;&#22312;&#23398;&#20064;&#21516;&#26500;&#25110;&#24322;&#26500;&#22270;&#30340;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#27867;&#21270;&#21040;&#20855;&#26377;&#19981;&#21516;&#21516;&#24577;&#31243;&#24230;&#30340;&#29616;&#23454;&#19990;&#30028;&#22270;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#21069;&#20316;&#20013;&#25552;&#20986;&#30340;Possion-Charlier Network&#65288;PCNet&#65289;&#20801;&#35768;&#20174;&#24322;&#24577;&#21040;&#21516;&#24577;&#23398;&#20064;&#22270;&#34920;&#31034;&#12290;&#23613;&#31649;PCNet&#32531;&#35299;&#20102;&#24322;&#24577;&#38382;&#39064;&#65292;&#20294;&#22312;&#36827;&#19968;&#27493;&#25552;&#39640;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20173;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;&#26412;&#25991;&#31616;&#21270;&#20102;PCNet&#24182;&#22686;&#24378;&#20102;&#20854;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#28388;&#27874;&#22120;&#38454;&#25968;&#25193;&#23637;&#21040;&#36830;&#32493;&#20540;&#24182;&#20943;&#23569;&#20854;&#21442;&#25968;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#20004;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#37051;&#22495;&#22823;&#23567;&#30340;&#21464;&#20307;&#12290;&#29702;&#35770;&#20998;&#26512;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#20110;&#22270;&#32467;&#26500;&#25200;&#21160;&#25110;&#23545;&#25239;&#24615;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#22312;&#20195;&#34920;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03676v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have garnered significant attention for their success in learning the representation of homophilic or heterophilic graphs. However, they cannot generalize well to real-world graphs with different levels of homophily. In response, the Possion-Charlier Network (PCNet) \cite{li2024pc}, the previous work, allows graph representation to be learned from heterophily to homophily. Although PCNet alleviates the heterophily issue, there remain some challenges in further improving the efficacy and efficiency. In this paper, we simplify PCNet and enhance its robustness. We first extend the filter order to continuous values and reduce its parameters. Two variants with adaptive neighborhood sizes are implemented. Theoretical analysis shows our model's robustness to graph structure perturbations or adversarial attacks. We validate our approach through semi-supervised learning tasks on various datasets representing both homo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#28041;&#21450;&#23545;&#25239;&#25439;&#22833;&#21644;&#30828;&#32422;&#26463;&#30340;CMDP&#65292;&#22312;&#20004;&#31181;&#19981;&#21516;&#24773;&#24418;&#19979;&#35774;&#35745;&#20102;&#20855;&#26377;&#27425;&#32447;&#24615;&#36951;&#25022;&#30340;&#31639;&#27861;&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#30740;&#31350;&#20013;&#23545;&#36825;&#19968;&#38382;&#39064;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.03672</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#38543;&#26426;&#30828;&#32422;&#26463;&#30340;&#23545;&#25239;MDP&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Adversarial MDPs with Stochastic Hard Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#28041;&#21450;&#23545;&#25239;&#25439;&#22833;&#21644;&#30828;&#32422;&#26463;&#30340;CMDP&#65292;&#22312;&#20004;&#31181;&#19981;&#21516;&#24773;&#24418;&#19979;&#35774;&#35745;&#20102;&#20855;&#26377;&#27425;&#32447;&#24615;&#36951;&#25022;&#30340;&#31639;&#27861;&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#30740;&#31350;&#20013;&#23545;&#36825;&#19968;&#38382;&#39064;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#24102;&#26377;&#23545;&#25239;&#25439;&#22833;&#21644;&#38543;&#26426;&#30828;&#32422;&#26463;&#30340;&#21463;&#38480;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20004;&#31181;&#19981;&#21516;&#30340;&#24773;&#24418;&#12290;&#22312;&#31532;&#19968;&#31181;&#24773;&#24418;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#33324;CMDP&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#36951;&#25022;&#21644;&#32047;&#31215;&#27491;&#32422;&#26463;&#36829;&#21453;&#12290;&#22312;&#31532;&#20108;&#31181;&#24773;&#24418;&#20013;&#65292;&#22312;&#19968;&#20010;&#25919;&#31574;&#20005;&#26684;&#28385;&#36275;&#32422;&#26463;&#23384;&#22312;&#19988;&#20026;&#23398;&#20064;&#32773;&#25152;&#20102;&#35299;&#30340;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#21516;&#26102;&#30830;&#20445;&#22312;&#27599;&#19968;&#36718;&#20013;&#32422;&#26463;&#20197;&#39640;&#27010;&#29575;&#24471;&#21040;&#28385;&#36275;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#30740;&#31350;&#26082;&#28041;&#21450;&#23545;&#25239;&#25439;&#22833;&#21448;&#28041;&#21450;&#30828;&#32422;&#26463;&#30340;CMDP&#30340;&#24037;&#20316;&#12290;&#23454;&#38469;&#19978;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#35201;&#20040;&#38598;&#20013;&#22312;&#26356;&#24369;&#30340;&#36719;&#32422;&#26463;&#19978;--&#20801;&#35768;&#27491;&#36829;&#21453;&#26469;&#25269;&#28040;&#36127;&#36829;&#21453;--&#35201;&#20040;&#23616;&#38480;&#20110;&#38543;&#26426;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22788;&#29702;&#19968;&#33324;&#30340;&#38750;&#32479;&#35745;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03672v1 Announce Type: new  Abstract: We study online learning problems in constrained Markov decision processes (CMDPs) with adversarial losses and stochastic hard constraints. We consider two different scenarios. In the first one, we address general CMDPs, where we design an algorithm that attains sublinear regret and cumulative positive constraints violation. In the second scenario, under the mild assumption that a policy strictly satisfying the constraints exists and is known to the learner, we design an algorithm that achieves sublinear regret while ensuring that the constraints are satisfied at every episode with high probability. To the best of our knowledge, our work is the first to study CMDPs involving both adversarial losses and hard constraints. Indeed, previous works either focus on much weaker soft constraints--allowing for positive violation to cancel out negative ones--or are restricted to stochastic losses. Thus, our algorithms can deal with general non-stat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22797;&#26434;&#25968;&#25454;&#32858;&#31867;&#26694;&#26550;&#65288;CDC&#65289;&#65292;&#33021;&#22815;&#20197;&#32447;&#24615;&#22797;&#26434;&#24230;&#39640;&#25928;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#22270;&#28388;&#27874;&#22120;&#21644;&#39640;&#36136;&#37327;&#38170;&#28857;&#26469;&#34701;&#21512;&#20960;&#20309;&#32467;&#26500;&#21644;&#23646;&#24615;&#20449;&#24687;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#38598;&#32676;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03670</link><description>&lt;p&gt;
CDC&#65306;&#22797;&#26434;&#25968;&#25454;&#32858;&#31867;&#30340;&#31616;&#21333;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CDC: A Simple Framework for Complex Data Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03670
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22797;&#26434;&#25968;&#25454;&#32858;&#31867;&#26694;&#26550;&#65288;CDC&#65289;&#65292;&#33021;&#22815;&#20197;&#32447;&#24615;&#22797;&#26434;&#24230;&#39640;&#25928;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#22270;&#28388;&#27874;&#22120;&#21644;&#39640;&#36136;&#37327;&#38170;&#28857;&#26469;&#34701;&#21512;&#20960;&#20309;&#32467;&#26500;&#21644;&#23646;&#24615;&#20449;&#24687;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#38598;&#32676;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#25968;&#25454;&#39537;&#21160;&#30340;&#25968;&#23383;&#26102;&#20195;&#65292;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#37327;&#20197;&#21450;&#22797;&#26434;&#24230;&#65288;&#22914;&#22810;&#35270;&#22270;&#12289;&#38750;&#27431;&#20960;&#37324;&#24471;&#21644;&#22810;&#20851;&#32852;&#24615;&#65289;&#27491;&#22312;&#21576;&#25351;&#25968;&#29978;&#33267;&#26356;&#24555;&#22320;&#22686;&#38271;&#12290;&#32858;&#31867;&#26080;&#30417;&#30563;&#22320;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#25928;&#30693;&#35782;&#65292;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#29420;&#31435;&#24320;&#21457;&#65292;&#22788;&#29702;&#19968;&#20010;&#29305;&#23450;&#25361;&#25112;&#65292;&#29306;&#29298;&#20854;&#20182;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22797;&#26434;&#25968;&#25454;&#32858;&#31867;&#65288;CDC&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#32447;&#24615;&#22797;&#26434;&#24230;&#39640;&#25928;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#22270;&#28388;&#27874;&#22120;&#34701;&#21512;&#20960;&#20309;&#32467;&#26500;&#21644;&#23646;&#24615;&#20449;&#24687;&#12290;&#28982;&#21518;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#20445;&#23384;&#30456;&#20284;&#24615;&#30340;&#27491;&#21017;&#21270;&#22120;&#33258;&#36866;&#24212;&#23398;&#20064;&#39640;&#36136;&#37327;&#38170;&#28857;&#26469;&#38477;&#20302;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#35828;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#38598;&#32676;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23558;CDC&#37096;&#32626;&#21040;&#35268;&#27169;&#20026;111M&#30340;&#22270;&#25968;&#25454;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03670v1 Announce Type: new  Abstract: In today's data-driven digital era, the amount as well as complexity, such as multi-view, non-Euclidean, and multi-relational, of the collected data are growing exponentially or even faster. Clustering, which unsupervisely extracts valid knowledge from data, is extremely useful in practice. However, existing methods are independently developed to handle one particular challenge at the expense of the others. In this work, we propose a simple but effective framework for complex data clustering (CDC) that can efficiently process different types of data with linear complexity. We first utilize graph filtering to fuse geometry structure and attribute information. We then reduce the complexity with high-quality anchors that are adaptively learned via a novel similarity-preserving regularizer. We illustrate the cluster-ability of our proposed method theoretically and experimentally. In particular, we deploy CDC to graph data of size 111M.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#35270;&#20026;&#23884;&#20837;&#21040;&#26356;&#39640;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#20302;&#32500;&#27969;&#24418;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;RKHS&#20013;&#35889;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#28909;&#26680;&#29983;&#25104;&#30340;&#25193;&#25955;&#31354;&#38388;&#65292;&#36890;&#36807;&#31215;&#20998;&#31639;&#23376;&#25216;&#26415;&#25512;&#23548;&#20102;&#20851;&#20110;&#24191;&#20041;&#33539;&#25968;&#30340;&#32039;&#25910;&#25947;&#19978;&#30028;&#65292;&#20351;&#20272;&#35745;&#22120;&#22312;&#24378;&#24847;&#20041;&#19979;&#25910;&#25947;&#21040;&#30446;&#26631;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.03669</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#22312;&#27969;&#24418;&#19978;&#30340;&#35889;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spectral Algorithms on Manifolds through Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#35270;&#20026;&#23884;&#20837;&#21040;&#26356;&#39640;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#20302;&#32500;&#27969;&#24418;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;RKHS&#20013;&#35889;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#28909;&#26680;&#29983;&#25104;&#30340;&#25193;&#25955;&#31354;&#38388;&#65292;&#36890;&#36807;&#31215;&#20998;&#31639;&#23376;&#25216;&#26415;&#25512;&#23548;&#20102;&#20851;&#20110;&#24191;&#20041;&#33539;&#25968;&#30340;&#32039;&#25910;&#25947;&#19978;&#30028;&#65292;&#20351;&#20272;&#35745;&#22120;&#22312;&#24378;&#24847;&#20041;&#19979;&#25910;&#25947;&#21040;&#30446;&#26631;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37325;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#20013;&#24212;&#29992;&#30340;&#35889;&#31639;&#27861;&#30340;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#19968;&#33324;&#26680;&#20989;&#25968;&#19978;&#65292;&#32463;&#24120;&#24573;&#30053;&#36755;&#20837;&#29305;&#24449;&#31354;&#38388;&#30340;&#22266;&#26377;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#20027;&#24352;&#36755;&#20837;&#25968;&#25454;&#20301;&#20110;&#19968;&#20010;&#23884;&#20837;&#21040;&#26356;&#39640;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#20302;&#32500;&#27969;&#24418;&#20869;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;RKHS&#20013;&#35889;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#30001;&#28909;&#26680;&#29983;&#25104;&#30340;&#65292;&#34987;&#31216;&#20026;&#25193;&#25955;&#31354;&#38388;&#30340;&#31354;&#38388;&#12290;&#36890;&#36807;&#32467;&#21512;&#36755;&#20837;&#30340;&#27969;&#24418;&#32467;&#26500;&#65292;&#25105;&#20204;&#37319;&#29992;&#31215;&#20998;&#31639;&#23376;&#25216;&#26415;&#25512;&#23548;&#20102;&#20851;&#20110;&#24191;&#20041;&#33539;&#25968;&#30340;&#32039;&#25910;&#25947;&#19978;&#30028;&#65292;&#36825;&#34920;&#26126;&#20272;&#35745;&#22120;&#22312;&#24378;&#24847;&#20041;&#19979;&#25910;&#25947;&#21040;&#30446;&#26631;&#20989;&#25968;&#65292;&#24847;&#21619;&#30528;&#20989;&#25968;&#26412;&#36523;&#21450;&#20854;&#23548;&#25968;&#21516;&#26102;&#25910;&#25947;&#12290;&#36825;&#20123;&#30028;&#25552;&#20379;&#20102;&#20004;&#20010;&#37325;&#35201;&#20248;&#21183;&#65306;&#39318;&#20808;&#65292;&#23427;&#20204;&#26159;&#23436;&#20840;&#36830;&#32493;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03669v1 Announce Type: cross  Abstract: The existing research on spectral algorithms, applied within a Reproducing Kernel Hilbert Space (RKHS), has primarily focused on general kernel functions, often neglecting the inherent structure of the input feature space. Our paper introduces a new perspective, asserting that input data are situated within a low-dimensional manifold embedded in a higher-dimensional Euclidean space. We study the convergence performance of spectral algorithms in the RKHSs, specifically those generated by the heat kernels, known as diffusion spaces. Incorporating the manifold structure of the input, we employ integral operator techniques to derive tight convergence upper bounds concerning generalized norms, which indicates that the estimators converge to the target function in strong sense, entailing the simultaneous convergence of the function itself and its derivatives. These bounds offer two significant advantages: firstly, they are exclusively contin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#23454;&#30340;&#26041;&#26696;&#65292;&#38024;&#23545;&#23454;&#38469;&#19990;&#30028;&#22270;&#32858;&#31867;&#38382;&#39064;&#65292;&#22312;&#22788;&#29702;&#21516;&#28304;&#21644;&#24322;&#28304;&#22270;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#26500;&#24314;&#20102;&#20302;&#36890;&#21644;&#39640;&#36890;&#28388;&#27874;&#22120;&#26469;&#25429;&#25417;&#20840;&#38754;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.03666</link><description>&lt;p&gt;
&#21487;&#35777;&#28388;&#27874;&#22120;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Provable Filter for Real-world Graph Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03666
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#23454;&#30340;&#26041;&#26696;&#65292;&#38024;&#23545;&#23454;&#38469;&#19990;&#30028;&#22270;&#32858;&#31867;&#38382;&#39064;&#65292;&#22312;&#22788;&#29702;&#21516;&#28304;&#21644;&#24322;&#28304;&#22270;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#26500;&#24314;&#20102;&#20302;&#36890;&#21644;&#39640;&#36890;&#28388;&#27874;&#22120;&#26469;&#25429;&#25417;&#20840;&#38754;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32858;&#31867;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#26080;&#30417;&#30563;&#38382;&#39064;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#36827;&#23637;&#26356;&#20855;&#25269;&#25239;&#21147;&#12290;&#27492;&#22806;&#65292;&#20960;&#20046;&#25152;&#26377;&#30340;&#32858;&#31867;&#26041;&#27861;&#37117;&#19987;&#27880;&#20110;&#21516;&#28304;&#22270;&#65292;&#24573;&#30053;&#24322;&#28304;&#24615;&#12290;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#22240;&#20026;&#29616;&#23454;&#19990;&#30028;&#22270;&#23637;&#29616;&#20986;&#32467;&#26500;&#19981;&#19968;&#33268;&#65292;&#19981;&#33021;&#31616;&#21333;&#22320;&#34987;&#24402;&#31867;&#20026;&#21516;&#28304;&#24615;&#21644;&#24322;&#28304;&#24615;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#19968;&#31181;&#22788;&#29702;&#23454;&#38469;&#22270;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#29702;&#35770;&#25903;&#25345;&#30340;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#21516;&#28304;&#21644;&#24322;&#28304;&#36793;&#21487;&#20197;&#22522;&#20110;&#37051;&#23621;&#20449;&#24687;&#34987;&#27491;&#30830;&#35782;&#21035;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#20998;&#21035;&#39640;&#24230;&#21516;&#28304;&#21644;&#24322;&#28304;&#30340;&#22270;&#12290;&#23427;&#20204;&#29992;&#20110;&#26500;&#24314;&#20302;&#36890;&#21644;&#39640;&#36890;&#28388;&#27874;&#22120;&#20197;&#25429;&#25417;&#25972;&#20307;&#20449;&#24687;&#12290;&#37325;&#35201;&#30340;&#29305;&#24449;&#36827;&#19968;&#27493;&#30001;&#25380;&#21387;-&#28608;&#21169;&#22359;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03666v1 Announce Type: new  Abstract: Graph clustering, an important unsupervised problem, has been shown to be more resistant to advances in Graph Neural Networks (GNNs). In addition, almost all clustering methods focus on homophilic graphs and ignore heterophily. This significantly limits their applicability in practice, since real-world graphs exhibit a structural disparity and cannot simply be classified as homophily and heterophily. Thus, a principled way to handle practical graphs is urgently needed. To fill this gap, we provide a novel solution with theoretical support. Interestingly, we find that most homophilic and heterophilic edges can be correctly identified on the basis of neighbor information. Motivated by this finding, we construct two graphs that are highly homophilic and heterophilic, respectively. They are used to build low-pass and high-pass filters to capture holistic information. Important features are further enhanced by the squeeze-and-excitation block
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#24320;&#28304;Python&#36719;&#20214;&#21253;&#8220;Environmental Insights&#8221;&#26088;&#22312;&#20351;&#22823;&#20247;&#33021;&#22815;&#36731;&#26494;&#33719;&#21462;&#29615;&#22659;&#31354;&#27668;&#27745;&#26579;&#25968;&#25454;&#65292;&#36827;&#34892;&#39044;&#27979;&#20998;&#26512;&#65292;&#20197;&#21450;&#36890;&#36807;&#21160;&#24577;&#21487;&#35270;&#21270;&#25552;&#39640;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.03664</link><description>&lt;p&gt;
&#29615;&#22659;&#27934;&#35265;: &#29992;&#24320;&#28304;Python&#36719;&#20214;&#21253;&#23454;&#29616;&#22823;&#20247;&#33719;&#21462;&#29615;&#22659;&#31354;&#27668;&#27745;&#26579;&#25968;&#25454;&#21644;&#39044;&#27979;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Environmental Insights: Democratizing Access to Ambient Air Pollution Data and Predictive Analytics with an Open-Source Python Package
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03664
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#24320;&#28304;Python&#36719;&#20214;&#21253;&#8220;Environmental Insights&#8221;&#26088;&#22312;&#20351;&#22823;&#20247;&#33021;&#22815;&#36731;&#26494;&#33719;&#21462;&#29615;&#22659;&#31354;&#27668;&#27745;&#26579;&#25968;&#25454;&#65292;&#36827;&#34892;&#39044;&#27979;&#20998;&#26512;&#65292;&#20197;&#21450;&#36890;&#36807;&#21160;&#24577;&#21487;&#35270;&#21270;&#25552;&#39640;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29615;&#22659;&#31354;&#27668;&#27745;&#26579;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#23545;&#20154;&#31867;&#20581;&#24247;&#12289;&#29983;&#24577;&#31995;&#32479;&#29983;&#21629;&#21147;&#21644;&#32463;&#27982;&#32467;&#26500;&#20135;&#29983;&#24191;&#27867;&#24433;&#21709;&#12290;&#21033;&#29992;&#29615;&#22659;&#31354;&#27668;&#27745;&#26579;&#27987;&#24230;&#25968;&#25454;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#25581;&#31034;&#31354;&#27668;&#27745;&#26579;&#23545;&#31038;&#20250;&#21508;&#26041;&#38754;&#30340;&#22810;&#26041;&#38754;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#8220;Environmental Insights&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20351;&#20154;&#20204;&#33021;&#22815;&#36731;&#26494;&#33719;&#21462;&#31354;&#27668;&#27745;&#26579;&#27987;&#24230;&#25968;&#25454;&#30340;&#24320;&#28304;Python&#36719;&#20214;&#21253;&#12290;&#35813;&#24037;&#20855;&#20351;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#26816;&#32034;&#21382;&#21490;&#31354;&#27668;&#27745;&#26579;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26410;&#26469;&#28508;&#22312;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#8220;Environmental Insights&#8221;&#36824;&#21253;&#25324;&#19968;&#22871;&#26088;&#22312;&#20419;&#36827;&#20998;&#26512;&#32467;&#26524;&#20256;&#25773;&#21644;&#36890;&#36807;&#21160;&#24577;&#21487;&#35270;&#21270;&#22686;&#24378;&#29992;&#25143;&#21442;&#19982;&#30340;&#24037;&#20855;&#32452;&#12290;&#36825;&#19968;&#20840;&#38754;&#30340;&#26041;&#27861;&#30830;&#20445;&#35813;&#36719;&#20214;&#21253;&#28385;&#36275;&#20102;&#24076;&#26395;&#25506;&#32034;&#21644;&#20102;&#35299;&#31354;&#27668;&#27745;&#26579;&#36235;&#21183;&#21450;&#20854;&#23545;&#29615;&#22659;&#30340;&#24433;&#21709;&#30340;&#20010;&#20154;&#30340;&#22810;&#26679;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03664v1 Announce Type: cross  Abstract: Ambient air pollution is a pervasive issue with wide-ranging effects on human health, ecosystem vitality, and economic structures. Utilizing data on ambient air pollution concentrations, researchers can perform comprehensive analyses to uncover the multifaceted impacts of air pollution across society. To this end, we introduce Environmental Insights, an open-source Python package designed to democratize access to air pollution concentration data. This tool enables users to easily retrieve historical air pollution data and employ a Machine Learning model for forecasting potential future conditions. Moreover, Environmental Insights includes a suite of tools aimed at facilitating the dissemination of analytical findings and enhancing user engagement through dynamic visualizations. This comprehensive approach ensures that the package caters to the diverse needs of individuals looking to explore and understand air pollution trends and their
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#22270;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#24322;&#36136;&#25968;&#25454;&#20013;&#33719;&#24471;&#39640;&#36136;&#37327;&#22270;&#65292;&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;</title><link>https://arxiv.org/abs/2403.03659</link><description>&lt;p&gt;
&#22312;&#24322;&#36136;&#29615;&#22659;&#19979;&#30340;&#31283;&#20581;&#22270;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Graph Structure Learning under Heterophily
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03659
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#22270;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#24322;&#36136;&#25968;&#25454;&#20013;&#33719;&#24471;&#39640;&#36136;&#37327;&#22270;&#65292;&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#25968;&#23398;&#32467;&#26500;&#65292;&#29992;&#20110;&#25551;&#36848;&#19981;&#21516;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#22312;&#21508;&#31181;&#23398;&#20064;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#38544;&#24335;&#22320;&#20551;&#35774;&#32473;&#23450;&#30340;&#22270;&#26159;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#25968;&#25454;&#19981;&#21487;&#36991;&#20813;&#22320;&#26159;&#22024;&#26434;&#21644;&#31232;&#30095;&#30340;&#65292;&#36825;&#23558;&#23548;&#33268;&#36739;&#24046;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#26412;&#36136;&#19978;&#20551;&#35774;&#22270;&#26159;&#21516;&#36136;&#30340;&#65292;&#24182;&#19988;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#30053;&#20102;&#24322;&#36136;&#24615;&#65292;&#21363;&#22823;&#22810;&#25968;&#36830;&#25509;&#33410;&#28857;&#26469;&#33258;&#19981;&#21516;&#30340;&#31867;&#21035;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31283;&#20581;&#22270;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#24322;&#36136;&#25968;&#25454;&#20013;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#22270;&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#39318;&#20808;&#24212;&#29992;&#19968;&#20010;&#39640;&#36890;&#28388;&#27874;&#22120;&#65292;&#36890;&#36807;&#23558;&#32467;&#26500;&#20449;&#24687;&#32534;&#30721;&#21040;&#33410;&#28857;&#29305;&#24449;&#20013;&#65292;&#20351;&#27599;&#20010;&#33410;&#28857;&#19982;&#20854;&#37051;&#23621;&#26356;&#20855;&#26377;&#21306;&#21035;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#20855;&#26377;&#33258;&#36866;&#24212;&#33539;&#25968;&#30340;&#31283;&#20581;&#22270;&#65292;&#29992;&#20110;&#25551;&#36848;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03659v1 Announce Type: new  Abstract: Graph is a fundamental mathematical structure in characterizing relations between different objects and has been widely used on various learning tasks. Most methods implicitly assume a given graph to be accurate and complete. However, real data is inevitably noisy and sparse, which will lead to inferior results. Despite the remarkable success of recent graph representation learning methods, they inherently presume that the graph is homophilic, and largely overlook heterophily, where most connected nodes are from different classes. In this regard, we propose a novel robust graph structure learning method to achieve a high-quality graph from heterophilic data for downstream tasks. We first apply a high-pass filter to make each node more distinctive from its neighbors by encoding structure information into the node features. Then, we learn a robust graph with an adaptive norm characterizing different levels of noise. Afterwards, we propose 
&lt;/p&gt;</description></item><item><title>&#36816;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#35299;&#20915;&#26041;&#27861;&#25910;&#25947;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#31561;&#20248;&#21183;&#65292;&#20026;&#36825;&#19968;&#38382;&#39064;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2403.03643</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#20013;&#30340;&#24212;&#29992;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Applications of Reinforcement Learning in Spatial Resource Allocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03643
&lt;/p&gt;
&lt;p&gt;
&#36816;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#35299;&#20915;&#26041;&#27861;&#25910;&#25947;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#31561;&#20248;&#21183;&#65292;&#20026;&#36825;&#19968;&#38382;&#39064;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#30340;&#25361;&#25112;&#22312;&#20132;&#36890;&#36816;&#36755;&#12289;&#24037;&#19994;&#21644;&#26085;&#24120;&#29983;&#27963;&#31561;&#21508;&#20010;&#39046;&#22495;&#26222;&#36941;&#23384;&#22312;&#12290;&#38543;&#30528;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#35268;&#27169;&#19981;&#26029;&#25193;&#22823;&#20197;&#21450;&#23545;&#23454;&#26102;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#20256;&#32479;&#31639;&#27861;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#35745;&#31639;&#21387;&#21147;&#65292;&#38590;&#20197;&#23454;&#29616;&#26368;&#20339;&#25928;&#29575;&#21644;&#23454;&#26102;&#33021;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#35745;&#31639;&#26426;&#35745;&#31639;&#33021;&#21147;&#30340;&#19981;&#26029;&#25552;&#21319;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;&#35832;&#22914;&#22260;&#26827;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#23601;&#65292;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#30340;&#23398;&#20064;&#21644;&#24207;&#36143;&#20915;&#31574;&#33021;&#21147;&#12290;&#37492;&#20110;&#36825;&#20123;&#36827;&#23637;&#65292;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#22823;&#37327;&#36816;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#35299;&#20915;&#26041;&#27861;&#25910;&#25947;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#31561;&#20248;&#21183;&#65292;&#20026;&#35299;&#20915;&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03643v1 Announce Type: cross  Abstract: The challenge of spatial resource allocation is pervasive across various domains such as transportation, industry, and daily life. As the scale of real-world issues continues to expand and demands for real-time solutions increase, traditional algorithms face significant computational pressures, struggling to achieve optimal efficiency and real-time capabilities. In recent years, with the escalating computational power of computers, the remarkable achievements of reinforcement learning in domains like Go and robotics have demonstrated its robust learning and sequential decision-making capabilities. Given these advancements, there has been a surge in novel methods employing reinforcement learning to tackle spatial resource allocation problems. These methods exhibit advantages such as rapid solution convergence and strong model generalization abilities, offering a new perspective on resolving spatial resource allocation problems. Therefor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#29983;&#25104;&#24335;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32531;&#35299;&#20861;&#21307;&#23398;CAD&#31995;&#32479;&#21487;&#38752;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#24515;&#33039;&#22686;&#22823;X&#20809;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.03642</link><description>&lt;p&gt;
&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#29983;&#25104;&#23456;&#29289;&#21307;&#23398;&#25918;&#23556;&#25968;&#25454;&#30340;&#29983;&#25104;&#24335;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generative Active Learning with Variational Autoencoder for Radiology Data Generation in Veterinary Medicine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03642
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#29983;&#25104;&#24335;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32531;&#35299;&#20861;&#21307;&#23398;CAD&#31995;&#32479;&#21487;&#38752;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#24515;&#33039;&#22686;&#22823;X&#20809;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#30528;&#20154;&#20204;&#23545;&#23456;&#29289;&#20581;&#24247;&#30340;&#20852;&#36259;&#22686;&#21152;&#65292;&#20861;&#21307;&#23398;&#20013;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#65288;CAD&#65289;&#31995;&#32479;&#30340;&#38656;&#27714;&#20063;&#22312;&#22686;&#21152;&#12290;&#22312;&#20861;&#21307;&#23398;CAD&#30340;&#21457;&#23637;&#20013;&#65292;&#30001;&#20110;&#25918;&#23556;&#25968;&#25454;&#19981;&#36275;&#32780;&#20572;&#28382;&#19981;&#21069;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#29983;&#25104;&#24335;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#32531;&#35299;&#20861;&#21307;&#23398;CAD&#31995;&#32479;&#21487;&#38752;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#21253;&#21547;&#24515;&#33039;&#22686;&#22823;X&#20809;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#21435;&#38500;&#27880;&#37322;&#24182;&#26631;&#20934;&#21270;&#22270;&#20687;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#25968;&#25454;&#29983;&#25104;&#38454;&#27573;&#21644;&#29992;&#20110;&#36807;&#28388;&#29983;&#25104;&#25968;&#25454;&#30340;&#26597;&#35810;&#38454;&#27573;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#36890;&#36807;&#35813;&#26694;&#26550;&#29983;&#25104;&#30340;&#25968;&#25454;&#34987;&#28155;&#21152;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;frechet&#20837;&#20405;&#36317;&#31163;&#22312;X&#20809;&#19978;&#25345;&#32493;&#20174;84.14&#19979;&#38477;&#21040;50.75&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03642v1 Announce Type: cross  Abstract: Recently, with increasing interest in pet healthcare, the demand for computer-aided diagnosis (CAD) systems in veterinary medicine has increased. The development of veterinary CAD has stagnated due to a lack of sufficient radiology data. To overcome the challenge, we propose a generative active learning framework based on a variational autoencoder. This approach aims to alleviate the scarcity of reliable data for CAD systems in veterinary medicine. This study utilizes datasets comprising cardiomegaly radiograph data. After removing annotations and standardizing images, we employed a framework for data augmentation, which consists of a data generation phase and a query phase for filtering the generated data. The experimental results revealed that as the data generated through this framework was added to the training data of the generative model, the frechet inception distance consistently decreased from 84.14 to 50.75 on the radiograph.
&lt;/p&gt;</description></item><item><title>SheetAgent&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30005;&#23376;&#34920;&#26684;&#25512;&#29702;&#21644;&#25805;&#20316;&#30340;&#36890;&#29992;&#20195;&#29702;&#65292;&#25552;&#20379;&#20102;&#22788;&#29702;&#22797;&#26434;&#29616;&#23454;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;</title><link>https://arxiv.org/abs/2403.03636</link><description>&lt;p&gt;
SheetAgent&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30005;&#23376;&#34920;&#26684;&#25512;&#29702;&#21644;&#25805;&#20316;&#30340;&#36890;&#29992;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
SheetAgent: A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03636
&lt;/p&gt;
&lt;p&gt;
SheetAgent&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30005;&#23376;&#34920;&#26684;&#25512;&#29702;&#21644;&#25805;&#20316;&#30340;&#36890;&#29992;&#20195;&#29702;&#65292;&#25552;&#20379;&#20102;&#22788;&#29702;&#22797;&#26434;&#29616;&#23454;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#34920;&#26684;&#25805;&#20316;&#24191;&#27867;&#23384;&#22312;&#20110;&#22823;&#22810;&#25968;&#26085;&#24120;&#24037;&#20316;&#20013;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#24037;&#20316;&#25928;&#29575;&#12290;&#26368;&#36817;&#23581;&#35797;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#36827;&#34892;&#33258;&#21160;&#30005;&#23376;&#34920;&#26684;&#25805;&#20316;&#65292;&#20294;&#23578;&#26410;&#22312;&#23384;&#22312;&#25512;&#29702;&#25361;&#25112;&#30340;&#22797;&#26434;&#21644;&#29616;&#23454;&#20219;&#21153;&#20013;&#36827;&#34892;&#25506;&#31350;&#65288;&#20363;&#22914;&#65292;&#20855;&#26377;&#22810;&#27493;&#25512;&#29702;&#21644;&#27169;&#31946;&#35201;&#27714;&#30340;&#38271;&#35270;&#37326;&#25805;&#20316;&#65289;&#12290;&#20026;&#20102;&#24357;&#21512;&#19982;&#30495;&#23454;&#19990;&#30028;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;$\textbf{SheetRM}$&#65292;&#19968;&#20010;&#29305;&#28857;&#26159;&#38271;&#35270;&#37326;&#21644;&#22810;&#31867;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#20855;&#26377;&#25512;&#29702;&#30456;&#20851;&#25805;&#32437;&#65292;&#30001;&#30495;&#23454;&#25361;&#25112;&#24341;&#36215;&#12290;&#20026;&#20102;&#32531;&#35299;&#20197;&#19978;&#25361;&#25112;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;$\textbf{SheetAgent}$&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#33021;&#21147;&#30340;&#26032;&#22411;&#33258;&#20027;&#20195;&#29702;&#12290;SheetAgent&#30001;&#19977;&#20010;&#21327;&#20316;&#27169;&#22359;&#32452;&#25104;&#65306;$\textit{Planner}$&#12289;$\textit{Informer}$&#21644;$\textit{Retriever}$&#65292;&#23454;&#29616;&#20102;&#23545;&#30005;&#23376;&#34920;&#26684;&#30340;&#39640;&#32423;&#25512;&#29702;&#21644;&#20934;&#30830;&#25805;&#20316;&#65292;&#32780;&#19981;&#38656;&#20154;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03636v1 Announce Type: new  Abstract: Spreadsheet manipulation is widely existing in most daily works and significantly improves working efficiency. Large language model (LLM) has been recently attempted for automatic spreadsheet manipulation but has not yet been investigated in complicated and realistic tasks where reasoning challenges exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous requirements). To bridge the gap with the real-world requirements, we introduce $\textbf{SheetRM}$, a benchmark featuring long-horizon and multi-category tasks with reasoning-dependent manipulation caused by real-life challenges. To mitigate the above challenges, we further propose $\textbf{SheetAgent}$, a novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of three collaborative modules: $\textit{Planner}$, $\textit{Informer}$, and $\textit{Retriever}$, achieving both advanced reasoning and accurate manipulation over spreadsheets without hu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#39118;&#21147;&#21457;&#30005;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#20272;&#35745;&#29305;&#24449;&#21644;&#30446;&#26631;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#21516;&#26102;&#39044;&#27979;&#25152;&#26377;&#26410;&#30693;&#20540;&#65292;&#36991;&#20813;&#20102;&#39044;&#22788;&#29702;&#29615;&#33410;&#65292;&#22312;&#36830;&#32493;&#25490;&#21517;&#27010;&#29575;&#24471;&#20998;&#26041;&#38754;&#27604;&#20256;&#32479;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>https://arxiv.org/abs/2403.03631</link><description>&lt;p&gt;
&#22788;&#29702;&#27010;&#29575;&#39118;&#21147;&#21457;&#30005;&#39044;&#27979;&#20013;&#30340;&#32570;&#22833;&#20540;&#65306;&#19968;&#31181;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tackling Missing Values in Probabilistic Wind Power Forecasting: A Generative Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#39118;&#21147;&#21457;&#30005;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#20272;&#35745;&#29305;&#24449;&#21644;&#30446;&#26631;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#21516;&#26102;&#39044;&#27979;&#25152;&#26377;&#26410;&#30693;&#20540;&#65292;&#36991;&#20813;&#20102;&#39044;&#22788;&#29702;&#29615;&#33410;&#65292;&#22312;&#36830;&#32493;&#25490;&#21517;&#27010;&#29575;&#24471;&#20998;&#26041;&#38754;&#27604;&#20256;&#32479;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#27010;&#29575;&#39118;&#21147;&#21457;&#30005;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20256;&#24863;&#22120;&#25925;&#38556;&#31561;&#21407;&#22240;&#23548;&#33268;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#32570;&#22833;&#20540;&#30340;&#38382;&#39064;&#38271;&#26399;&#20197;&#26469;&#34987;&#24573;&#35270;&#12290;&#23613;&#31649;&#36890;&#24120;&#22312;&#27169;&#22411;&#20272;&#35745;&#21644;&#39044;&#27979;&#20043;&#21069;&#36890;&#36807;&#25554;&#34917;&#32570;&#22833;&#20540;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#26159;&#24456;&#33258;&#28982;&#30340;&#65292;&#20294;&#25105;&#20204;&#24314;&#35758;&#23558;&#32570;&#22833;&#20540;&#21644;&#39044;&#27979;&#30446;&#26631;&#35270;&#20026;&#21516;&#31561;&#37325;&#35201;&#65292;&#24182;&#22522;&#20110;&#35266;&#27979;&#20540;&#21516;&#26102;&#39044;&#27979;&#25152;&#26377;&#26410;&#30693;&#20540;&#12290;&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#20272;&#35745;&#29305;&#24449;&#21644;&#30446;&#26631;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#39044;&#22788;&#29702;&#65292;&#36991;&#20813;&#24341;&#20837;&#28508;&#22312;&#30340;&#38169;&#35823;&#12290;&#19982;&#20256;&#32479;&#30340;&#8220;&#25554;&#34917;&#65292;&#28982;&#21518;&#39044;&#27979;&#8221;&#27969;&#31243;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#36830;&#32493;&#25490;&#21517;&#27010;&#29575;&#24471;&#20998;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03631v1 Announce Type: new  Abstract: Machine learning techniques have been successfully used in probabilistic wind power forecasting. However, the issue of missing values within datasets due to sensor failure, for instance, has been overlooked for a long time. Although it is natural to consider addressing this issue by imputing missing values before model estimation and forecasting, we suggest treating missing values and forecasting targets indifferently and predicting all unknown values simultaneously based on observations. In this paper, we offer an efficient probabilistic forecasting approach by estimating the joint distribution of features and targets based on a generative model. It is free of preprocessing, and thus avoids introducing potential errors. Compared with the traditional "impute, then predict" pipeline, the proposed approach achieves better performance in terms of continuous ranked probability score.
&lt;/p&gt;</description></item><item><title>GSNeRF&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#30693;&#22330;&#26223;&#30340;&#26032;&#35270;&#22270;&#22270;&#20687;&#21644;&#30456;&#20851;&#35821;&#20041;&#22320;&#22270;&#30340;&#29983;&#25104;&#65292;&#24182;&#22312;&#22270;&#20687;&#21644;&#35821;&#20041;&#28210;&#26579;&#26041;&#38754;&#21462;&#24471;&#20102;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03608</link><description>&lt;p&gt;
GSNeRF: &#22686;&#24378;&#20102;3D&#22330;&#26223;&#29702;&#35299;&#30340;&#36890;&#29992;&#35821;&#20041;&#31070;&#32463;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D Scene Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03608
&lt;/p&gt;
&lt;p&gt;
GSNeRF&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#30693;&#22330;&#26223;&#30340;&#26032;&#35270;&#22270;&#22270;&#20687;&#21644;&#30456;&#20851;&#35821;&#20041;&#22320;&#22270;&#30340;&#29983;&#25104;&#65292;&#24182;&#22312;&#22270;&#20687;&#21644;&#35821;&#20041;&#28210;&#26579;&#26041;&#38754;&#21462;&#24471;&#20102;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35821;&#20041;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;GSNeRF&#65289;&#65292;&#23427;&#29420;&#29305;&#22320;&#23558;&#22270;&#20687;&#35821;&#20041;&#32435;&#20837;&#21512;&#25104;&#36807;&#31243;&#65292;&#22240;&#27492;&#21487;&#20197;&#20026;&#26410;&#30693;&#22330;&#26223;&#29983;&#25104;&#26032;&#35270;&#22270;&#22270;&#20687;&#21644;&#30456;&#20851;&#30340;&#35821;&#20041;&#22320;&#22270;&#12290;&#25105;&#20204;&#30340;GSNeRF&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#35821;&#20041;&#22320;&#29702;&#25512;&#29702;&#21644;&#28145;&#24230;&#24341;&#23548;&#21487;&#35270;&#28210;&#26579;&#12290;&#21069;&#32773;&#33021;&#22815;&#35266;&#23519;&#22810;&#35270;&#22270;&#22270;&#20687;&#36755;&#20837;&#65292;&#20174;&#22330;&#26223;&#20013;&#25552;&#21462;&#35821;&#20041;&#21644;&#20960;&#20309;&#29305;&#24449;&#12290;&#22312;&#21518;&#32773;&#30340;&#25351;&#23548;&#19979;&#65292;&#26681;&#25454;&#29983;&#25104;&#30340;&#22270;&#20687;&#20960;&#20309;&#20449;&#24687;&#65292;&#36827;&#34892;&#20102;&#20855;&#26377;&#25913;&#36827;&#24615;&#33021;&#30340;&#22270;&#20687;&#21644;&#35821;&#20041;&#28210;&#26579;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;GSNeRF&#22312;&#26032;&#35270;&#22270;&#22270;&#20687;&#21644;&#35821;&#20041;&#20998;&#21106;&#21512;&#25104;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#24037;&#20316;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#37319;&#26679;&#31574;&#30053;&#23545;&#20110;&#21487;&#35270;&#28210;&#26579;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03608v1 Announce Type: cross  Abstract: Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance Fields (NeRF) have emerged as a popular research topic in 3D vision. In this work, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF), which uniquely takes image semantics into the synthesis process so that both novel view images and the associated semantic maps can be produced for unseen scenes. Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and Depth-Guided Visual rendering. The former is able to observe multi-view image inputs to extract semantic and geometry features from a scene. Guided by the resulting image geometry information, the latter performs both image and semantic rendering with improved performances. Our experiments not only confirm that GSNeRF performs favorably against prior works on both novel-view image and semantic segmentation synthesis but the effectiveness of our sampling strategy for visual rendering is 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#24341;&#20837;Transformer&#31070;&#32463;&#32593;&#32476;&#21644;&#25216;&#26415;&#25351;&#26631;&#26469;&#25552;&#21319;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#20110;&#25429;&#33719;&#26102;&#38388;&#21160;&#24577;&#21644;&#25552;&#21462;&#37325;&#35201;&#29305;&#24449;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;</title><link>https://arxiv.org/abs/2403.03606</link><description>&lt;p&gt;
&#36890;&#36807;Transformer&#31070;&#32463;&#32593;&#32476;&#21644;&#25216;&#26415;&#25351;&#26631;&#25552;&#21319;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Price Prediction in Cryptocurrency Using Transformer Neural Network and Technical Indicators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03606
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#24341;&#20837;Transformer&#31070;&#32463;&#32593;&#32476;&#21644;&#25216;&#26415;&#25351;&#26631;&#26469;&#25552;&#21319;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#20110;&#25429;&#33719;&#26102;&#38388;&#21160;&#24577;&#21644;&#25552;&#21462;&#37325;&#35201;&#29305;&#24449;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#26102;&#38388;&#24207;&#21015;&#65292;&#29305;&#21035;&#20851;&#27880;&#27604;&#29305;&#24065;&#12289;&#20197;&#22826;&#22346;&#21644;&#33713;&#29305;&#24065;&#12290;&#35813;&#26041;&#27861;&#25972;&#21512;&#20102;&#25216;&#26415;&#25351;&#26631;&#12289;Performer&#31070;&#32463;&#32593;&#32476;&#21644;BiLSTM&#65288;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65289;&#26469;&#25429;&#33719;&#26102;&#38388;&#21160;&#24577;&#24182;&#20174;&#21407;&#22987;&#21152;&#23494;&#36135;&#24065;&#25968;&#25454;&#20013;&#25552;&#21462;&#37325;&#35201;&#29305;&#24449;&#12290;&#25216;&#26415;&#25351;&#26631;&#30340;&#24212;&#29992;&#26377;&#21161;&#20110;&#25552;&#21462;&#22797;&#26434;&#27169;&#24335;&#12289;&#21160;&#37327;&#12289;&#27874;&#21160;&#24615;&#21644;&#36235;&#21183;&#12290;Performer&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#24555;&#36895;&#20851;&#27880;&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;&#65288;FAVOR+&#65289;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;Transformer&#27169;&#22411;&#20013;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23637;&#29616;&#20986;&#26356;&#20248;&#36234;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#27492;&#22806;&#65292;&#23558;BiLSTM&#38598;&#25104;&#21040;&#21069;&#39304;&#32593;&#32476;&#20013;&#22686;&#24378;&#20102;&#27169;&#22411;&#25429;&#33719;&#25968;&#25454;&#26102;&#38388;&#21160;&#24577;&#30340;&#33021;&#21147;&#65292;&#21521;&#21069;&#21644;&#21521;&#21518;&#20004;&#20010;&#26041;&#21521;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03606v1 Announce Type: cross  Abstract: This study presents an innovative approach for predicting cryptocurrency time series, specifically focusing on Bitcoin, Ethereum, and Litecoin. The methodology integrates the use of technical indicators, a Performer neural network, and BiLSTM (Bidirectional Long Short-Term Memory) to capture temporal dynamics and extract significant features from raw cryptocurrency data. The application of technical indicators, such facilitates the extraction of intricate patterns, momentum, volatility, and trends. The Performer neural network, employing Fast Attention Via positive Orthogonal Random features (FAVOR+), has demonstrated superior computational efficiency and scalability compared to the traditional Multi-head attention mechanism in Transformer models. Additionally, the integration of BiLSTM in the feedforward network enhances the model's capacity to capture temporal dynamics in the data, processing it in both forward and backward direction
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Cluster Information Transfer (CIT) &#26426;&#21046;&#65292;&#21487;&#20197;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#20855;&#26377;&#32467;&#26500;&#36716;&#31227;&#30340;&#21508;&#31181;&#26410;&#30693;&#27979;&#35797;&#22270;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03599</link><description>&lt;p&gt;
&#36890;&#36807;&#38598;&#32676;&#27867;&#21270;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#21464;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Invariant Representations of Graph Neural Networks via Cluster Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03599
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Cluster Information Transfer (CIT) &#26426;&#21046;&#65292;&#21487;&#20197;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#20855;&#26377;&#32467;&#26500;&#36716;&#31227;&#30340;&#21508;&#31181;&#26410;&#30693;&#27979;&#35797;&#22270;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22312;&#24314;&#27169;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#32858;&#21512;&#23616;&#37096;&#32467;&#26500;&#20449;&#24687;&#26469;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#27979;&#35797;&#22270;&#30340;&#32467;&#26500;&#21487;&#33021;&#19981;&#21516;&#20110;&#35757;&#32451;&#22270;&#30340;&#32467;&#26500;&#65292;&#23548;&#33268;&#20102;&#32467;&#26500;&#36716;&#31227;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23454;&#39564;&#24615;&#22320;&#21457;&#29616;&#65292;&#24403;&#32467;&#26500;&#36716;&#31227;&#21457;&#29983;&#26102;&#65292;GNNs&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#36825;&#34920;&#26126;&#23398;&#20064;&#30340;&#27169;&#22411;&#21487;&#33021;&#20559;&#21521;&#20110;&#29305;&#23450;&#30340;&#32467;&#26500;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Cluster Information Transfer (CIT) &#26426;&#21046;&#65288;&#20195;&#30721;&#21487;&#22312;https://github.com/BUPT-GAMMA/CITGNN&#25214;&#21040;&#65289;&#65292;&#35813;&#26426;&#21046;&#21487;&#20197;&#20026;GNNs&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#23427;&#20204;&#23545;&#20855;&#26377;&#32467;&#26500;&#36716;&#31227;&#30340;&#21508;&#31181;&#26410;&#30693;&#27979;&#35797;&#22270;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;CIT &#26426;&#21046;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#38598;&#32676;&#20449;&#24687;&#19982;&#33410;&#28857;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03599v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have become increasingly popular in modeling graph-structured data due to their ability to learn node representations by aggregating local structure information. However, it is widely acknowledged that the test graph structure may differ from the training graph structure, resulting in a structure shift. In this paper, we experimentally find that the performance of GNNs drops significantly when the structure shift happens, suggesting that the learned models may be biased towards specific structure patterns. To address this challenge, we propose the Cluster Information Transfer (CIT) mechanism (Code available at https://github.com/BUPT-GAMMA/CITGNN), which can learn invariant representations for GNNs, thereby improving their generalization ability to various and unknown test graphs with structure shift. The CIT mechanism achieves this by combining different cluster information with the nodes while preserving th
&lt;/p&gt;</description></item><item><title>DeepEclipse&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31227;&#38500;&#30333;&#30418;DNN&#27700;&#21360;&#65292;&#24182;&#37319;&#29992;&#19982;&#29616;&#26377;&#26041;&#27861;&#26174;&#33879;&#19981;&#21516;&#30340;&#28151;&#28102;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.03590</link><description>&lt;p&gt;
DeepEclipse: &#22914;&#20309;&#30772;&#35299;&#30333;&#30418;DNN&#27700;&#21360;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
DeepEclipse: How to Break White-Box DNN-Watermarking Schemes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03590
&lt;/p&gt;
&lt;p&gt;
DeepEclipse&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31227;&#38500;&#30333;&#30418;DNN&#27700;&#21360;&#65292;&#24182;&#37319;&#29992;&#19982;&#29616;&#26377;&#26041;&#27861;&#26174;&#33879;&#19981;&#21516;&#30340;&#28151;&#28102;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#22312;&#25968;&#23383;&#36716;&#22411;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#24341;&#36215;&#20102;&#23545;&#20854;&#30693;&#35782;&#20135;&#26435;&#30340;&#20851;&#27880;&#12290;&#19981;&#21516;&#30340;&#27700;&#21360;&#25216;&#26415;&#24050;&#34987;&#24320;&#21457;&#29992;&#20110;&#20445;&#25252;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20813;&#21463;&#30693;&#35782;&#20135;&#26435;&#20405;&#26435;&#65292;&#21019;&#36896;&#20986;&#19968;&#20010;&#31454;&#20105;&#28608;&#28872;&#30340;DNN&#27700;&#21360;&#21644;&#21435;&#27700;&#21360;&#26041;&#27861;&#39046;&#22495;&#12290;&#20027;&#35201;&#30340;&#27700;&#21360;&#26041;&#26696;&#20351;&#29992;&#30333;&#30418;&#25216;&#26415;&#65292;&#28041;&#21450;&#36890;&#36807;&#21521;&#29305;&#23450;DNN&#23618;&#28155;&#21152;&#21807;&#19968;&#26631;&#35782;&#26469;&#20462;&#25913;&#26435;&#37325;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23545;&#30333;&#30418;&#27700;&#21360;&#30340;&#29616;&#26377;&#25915;&#20987;&#36890;&#24120;&#38656;&#35201;&#20102;&#35299;&#29305;&#23450;&#37096;&#32626;&#30340;&#27700;&#21360;&#26041;&#26696;&#25110;&#35775;&#38382;&#22522;&#30784;&#25968;&#25454;&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#25105;&#20204;&#25552;&#20986;DeepEclipse&#65292;&#19968;&#20010;&#26032;&#39062;&#19988;&#32479;&#19968;&#30340;&#26088;&#22312;&#31227;&#38500;&#30333;&#30418;&#27700;&#21360;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#28151;&#28102;&#25216;&#26415;&#19982;&#29616;&#26377;&#30333;&#30418;&#27700;&#21360;&#21435;&#38500;&#26041;&#26696;&#26377;&#26174;&#33879;&#21306;&#21035;&#12290;DeepEclipse&#21487;&#20197;&#22312;&#27809;&#26377;&#20107;&#20808;&#20102;&#35299;&#30340;&#24773;&#20917;&#19979;&#35268;&#36991;&#27700;&#21360;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03590v1 Announce Type: cross  Abstract: Deep Learning (DL) models have become crucial in digital transformation, thus raising concerns about their intellectual property rights. Different watermarking techniques have been developed to protect Deep Neural Networks (DNNs) from IP infringement, creating a competitive field for DNN watermarking and removal methods. The predominant watermarking schemes use white-box techniques, which involve modifying weights by adding a unique signature to specific DNN layers. On the other hand, existing attacks on white-box watermarking usually require knowledge of the specific deployed watermarking scheme or access to the underlying data for further training and fine-tuning. We propose DeepEclipse, a novel and unified framework designed to remove white-box watermarks. We present obfuscation techniques that significantly differ from the existing white-box watermarking removal schemes. DeepEclipse can evade watermark detection without prior knowl
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#22320;&#20272;&#35745;&#22788;&#29702;&#25928;&#24212;&#30340;&#27963;&#36291;&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21327;&#21464;&#37327;&#23494;&#24230;&#21644;&#20542;&#21521;&#24471;&#20998;&#26469;&#38477;&#20302;&#28176;&#36817;&#26041;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.03589</link><description>&lt;p&gt;
&#29992;&#20110;&#22788;&#29702;&#22240;&#21464;&#37327;&#36873;&#25321;&#30340;&#27963;&#36291;&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Active Adaptive Experimental Design for Treatment Effect Estimation with Covariate Choices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03589
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#22320;&#20272;&#35745;&#22788;&#29702;&#25928;&#24212;&#30340;&#27963;&#36291;&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21327;&#21464;&#37327;&#23494;&#24230;&#21644;&#20542;&#21521;&#24471;&#20998;&#26469;&#38477;&#20302;&#28176;&#36817;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#23454;&#39564;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#20272;&#35745;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATEs&#65289;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#23454;&#39564;&#65292;&#20854;&#20013;&#23454;&#39564;&#32773;&#25353;&#39034;&#24207;&#20174;&#30001;&#23454;&#39564;&#32773;&#20915;&#23450;&#30340;&#21327;&#21464;&#37327;&#23494;&#24230;&#20013;&#25277;&#26679;&#19968;&#20010;&#23454;&#39564;&#21333;&#20803;&#65292;&#24182;&#20998;&#37197;&#19968;&#31181;&#22788;&#29702;&#12290;&#22312;&#20998;&#37197;&#22788;&#29702;&#21518;&#65292;&#23454;&#39564;&#32773;&#31435;&#21363;&#35266;&#23519;&#30456;&#24212;&#30340;&#32467;&#26524;&#12290;&#22312;&#23454;&#39564;&#32467;&#26463;&#26102;&#65292;&#23454;&#39564;&#32773;&#21033;&#29992;&#25910;&#38598;&#30340;&#26679;&#26412;&#20272;&#31639;&#20986;&#19968;&#20010;ATE&#12290;&#23454;&#39564;&#32773;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#36739;&#23567;&#30340;&#28176;&#36817;&#26041;&#24046;&#20272;&#35745;ATE&#12290;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#35774;&#35745;&#20102;&#19968;&#20123;&#33021;&#22815;&#33258;&#36866;&#24212;&#20248;&#21270;&#20542;&#21521;&#24471;&#20998;&#65288;&#22788;&#29702;&#20998;&#37197;&#27010;&#29575;&#65289;&#30340;&#23454;&#39564;&#12290;&#20316;&#20026;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#27010;&#25324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#19979;&#23454;&#39564;&#32773;&#20248;&#21270;&#21327;&#21464;&#37327;&#23494;&#24230;&#20197;&#21450;&#20542;&#21521;&#24471;&#20998;&#65292;&#24182;&#21457;&#29616;&#20248;&#21270;&#21327;&#21464;&#37327;&#23494;&#24230;&#21644;&#20542;&#21521;&#24471;&#20998;&#27604;&#20165;&#20248;&#21270;&#20542;&#21521;&#24471;&#20998;&#21487;&#20197;&#20943;&#23569;&#28176;&#36817;&#26041;&#24046;&#26356;&#22810;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03589v1 Announce Type: cross  Abstract: This study designs an adaptive experiment for efficiently estimating average treatment effect (ATEs). We consider an adaptive experiment where an experimenter sequentially samples an experimental unit from a covariate density decided by the experimenter and assigns a treatment. After assigning a treatment, the experimenter observes the corresponding outcome immediately. At the end of the experiment, the experimenter estimates an ATE using gathered samples. The objective of the experimenter is to estimate the ATE with a smaller asymptotic variance. Existing studies have designed experiments that adaptively optimize the propensity score (treatment-assignment probability). As a generalization of such an approach, we propose a framework under which an experimenter optimizes the covariate density, as well as the propensity score, and find that optimizing both covariate density and propensity score reduces the asymptotic variance more than o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#35299;&#37322;&#26694;&#26550;RouteExplainer&#65292;&#23454;&#29616;&#20102;&#36793;&#30340;&#24433;&#21709;&#35299;&#37322;&#21644;&#24847;&#22270;&#25512;&#26029;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#35299;&#37322;&#25991;&#26412;&#65292;&#24182;&#22312;&#22810;&#20010;VRP&#19978;&#36827;&#34892;&#20102;&#37327;&#21270;&#35780;&#20272;</title><link>https://arxiv.org/abs/2403.03585</link><description>&lt;p&gt;
RouteExplainer&#65306;&#19968;&#31181;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#35299;&#37322;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
RouteExplainer: An Explanation Framework for Vehicle Routing Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03585
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#35299;&#37322;&#26694;&#26550;RouteExplainer&#65292;&#23454;&#29616;&#20102;&#36793;&#30340;&#24433;&#21709;&#35299;&#37322;&#21644;&#24847;&#22270;&#25512;&#26029;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#35299;&#37322;&#25991;&#26412;&#65292;&#24182;&#22312;&#22810;&#20010;VRP&#19978;&#36827;&#34892;&#20102;&#37327;&#21270;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#26159;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24050;&#24212;&#29992;&#20110;&#21508;&#31181;&#23454;&#38469;&#38382;&#39064;&#12290;&#23613;&#31649;&#23545;VRP&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#25913;&#21892;&#23454;&#38469;VRP&#24212;&#29992;&#20013;&#30340;&#21487;&#38752;&#24615;&#21644;&#20114;&#21160;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36825;&#20010;&#39046;&#22495;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;RouteExplainer&#65292;&#19968;&#31181;&#20107;&#21518;&#35299;&#37322;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#37322;&#22312;&#29983;&#25104;&#36335;&#24452;&#20013;&#27599;&#26465;&#36793;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#36335;&#24452;&#20026;&#21160;&#20316;&#24207;&#21015;&#24182;&#22522;&#20110;&#21160;&#20316;&#24433;&#21709;&#27169;&#22411;&#25193;&#23637;&#23545;VRP&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#20026;&#20102;&#22686;&#24378;&#35299;&#37322;&#65292;&#25105;&#20204;&#39069;&#22806;&#25552;&#20986;&#20102;&#19968;&#20010;&#36793;&#30028;&#20998;&#31867;&#22120;&#65292;&#25512;&#26029;&#27599;&#20010;&#36793;&#30028;&#30340;&#24847;&#22270;&#65292;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#36793;&#30028;&#20998;&#31867;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#21450;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#35299;&#37322;&#25991;&#26412;&#12290;&#25105;&#20204;&#22312;&#22235;&#31181;&#19981;&#21516;&#30340;VRP&#19978;&#23450;&#37327;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#36793;&#30028;&#20998;&#31867;&#22120;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20445;&#25345;&#35745;&#31639;&#36895;&#24230;&#30340;&#21516;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03585v1 Announce Type: cross  Abstract: The Vehicle Routing Problem (VRP) is a widely studied combinatorial optimization problem and has been applied to various practical problems. While the explainability for VRP is significant for improving the reliability and interactivity in practical VRP applications, it remains unexplored. In this paper, we propose RouteExplainer, a post-hoc explanation framework that explains the influence of each edge in a generated route. Our framework realizes this by rethinking a route as the sequence of actions and extending counterfactual explanations based on the action influence model to VRP. To enhance the explanation, we additionally propose an edge classifier that infers the intentions of each edge, a loss function to train the edge classifier, and explanation-text generation by Large Language Models (LLMs). We quantitatively evaluate our edge classifier on four different VRPs. The results demonstrate its rapid computation while maintaining
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;ASD&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#23588;&#20854;&#22312;&#20799;&#31461;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.03581</link><description>&lt;p&gt;
&#25552;&#39640;ASD&#26816;&#27979;&#20934;&#30830;&#24615;&#65306;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#32508;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing ASD detection accuracy: a combined approach of machine learning and deep learning models with natural language processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03581
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;ASD&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#23588;&#20854;&#22312;&#20799;&#31461;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03581v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#30446;&#30340;&#65306;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26469;&#35786;&#26029;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#12290;&#23427;&#19987;&#27880;&#20110;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#20174;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#25991;&#26412;&#36755;&#20837;&#20013;&#26816;&#27979;ASD&#65292;&#35299;&#20915;&#20256;&#32479;ASD&#35786;&#26029;&#20013;&#30340;&#25361;&#25112;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12289;ML&#21644;DL&#27169;&#22411;&#65288;&#21253;&#25324;&#20915;&#31574;&#26641;&#12289;XGB&#12289;KNN&#12289;RNN&#12289;LSTM&#12289;Bi-LSTM&#12289;BERT&#21644;BERTweet&#65289;&#20998;&#26512;&#20102;404,627&#26465;tweets&#65292;&#26681;&#25454;ASD&#25110;&#38750;ASD&#20316;&#32773;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#12290;&#20854;&#20013;90,000&#26465;tweets&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;AI&#27169;&#22411;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#25104;&#21151;&#29575;&#36798;&#21040;88&#65285;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#26469;&#33258;ASD&#24739;&#32773;&#30340;&#25991;&#26412;&#12290;&#32467;&#35770;&#65306;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;AI&#22312;&#25913;&#21892;ASD&#35786;&#26029;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#20799;&#31461;&#20013;&#65292;&#31361;&#26174;&#20102;&#26089;&#26399;&#26816;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03581v1 Announce Type: new  Abstract: Purpose: Our study explored the use of artificial intelligence (AI) to diagnose autism spectrum disorder (ASD). It focused on machine learning (ML) and deep learning (DL) to detect ASD from text inputs on social media, addressing challenges in traditional ASD diagnosis.   Methods: We used natural language processing (NLP), ML, and DL models (including decision trees, XGB, KNN, RNN, LSTM, Bi-LSTM, BERT, and BERTweet) to analyze 404,627 tweets, classifying them based on ASD or non-ASD authors. A subset of 90,000 tweets was used for model training and testing.   Results: Our AI models showed high accuracy, with an 88% success rate in identifying texts from individuals with ASD.   Conclusion: The study demonstrates AI's potential in improving ASD diagnosis, especially in children, highlighting the importance of early detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#24314;&#31435;&#20102;&#19968;&#20010;&#37096;&#20998;&#26377;&#24207;&#38598;&#65292;&#29992;&#20197;&#20195;&#34920;&#21738;&#20123;&#31867;&#21035;&#23376;&#38598;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#31867;&#21035;&#65292;&#24182;&#25506;&#32034;&#20102;&#31867;&#21035;&#23376;&#38598;&#22914;&#20309;&#25552;&#39640;&#27979;&#35797;&#24615;&#33021;&#20197;&#21450;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#36801;&#31227;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03569</link><description>&lt;p&gt;
&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#36801;&#31227;&#65306;&#23376;&#31867;&#21035;&#30340;&#27867;&#21270;&#33021;&#21147;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
On Transfer in Classification: How Well do Subsets of Classes Generalize?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#24314;&#31435;&#20102;&#19968;&#20010;&#37096;&#20998;&#26377;&#24207;&#38598;&#65292;&#29992;&#20197;&#20195;&#34920;&#21738;&#20123;&#31867;&#21035;&#23376;&#38598;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#31867;&#21035;&#65292;&#24182;&#25506;&#32034;&#20102;&#31867;&#21035;&#23376;&#38598;&#22914;&#20309;&#25552;&#39640;&#27979;&#35797;&#24615;&#33021;&#20197;&#21450;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#36801;&#31227;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#36890;&#24120;&#20250;&#21457;&#29616;&#22312;&#32473;&#23450;&#19968;&#32452;&#31867;&#21035;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#25512;&#24191;&#21040;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#65292;&#36825;&#34920;&#26126;&#20102;&#23398;&#20064;&#36229;&#36234;&#21021;&#22987;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#36890;&#24120;&#22312;&#36801;&#31227;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#24471;&#21040;&#21033;&#29992;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#22788;&#29702;&#26032;&#31867;&#21035;&#65292;&#26080;&#35770;&#26159;&#21542;&#36827;&#34892;&#24494;&#35843;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#24456;&#23569;&#26377;&#35770;&#25991;&#25506;&#35752;&#36825;&#19968;&#29616;&#35937;&#32972;&#21518;&#30340;&#29702;&#35770;&#26681;&#22522;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#20026;&#31867;&#21035;&#38598;&#20043;&#38388;&#30340;&#21487;&#36801;&#31227;&#24615;&#22880;&#23450;&#36825;&#26679;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#30340;&#22522;&#30784;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#31867;&#21035;&#23376;&#38598;&#30340;&#37096;&#20998;&#26377;&#24207;&#38598;&#12290;&#36825;&#20010;&#24037;&#20855;&#21487;&#20197;&#34920;&#31034;&#21738;&#20010;&#31867;&#21035;&#23376;&#38598;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#31867;&#21035;&#12290;&#22312;&#26356;&#23454;&#38469;&#30340;&#24773;&#22659;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#39044;&#27979;&#21738;&#20010;&#31867;&#21035;&#23376;&#38598;&#21487;&#20197;&#22312;&#23545;&#25152;&#26377;&#31867;&#21035;&#36827;&#34892;&#27979;&#35797;&#26102;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#20854;&#20013;&#36716;&#31227;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03569v1 Announce Type: new  Abstract: In classification, it is usual to observe that models trained on a given set of classes can generalize to previously unseen ones, suggesting the ability to learn beyond the initial task. This ability is often leveraged in the context of transfer learning where a pretrained model can be used to process new classes, with or without fine tuning. Surprisingly, there are a few papers looking at the theoretical roots beyond this phenomenon. In this work, we are interested in laying the foundations of such a theoretical framework for transferability between sets of classes. Namely, we establish a partially ordered set of subsets of classes. This tool allows to represent which subset of classes can generalize to others. In a more practical setting, we explore the ability of our framework to predict which subset of classes can lead to the best performance when testing on all of them. We also explore few-shot learning, where transfer is the golden
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#30340;&#22810;&#27169;&#24577;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#31227;&#21160;&#25805;&#20316;&#26426;&#22120;&#20154;&#30340;&#29289;&#20307;&#28369;&#31227;&#24863;&#30693;&#65292;&#33021;&#22815;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#26426;&#22120;&#20154;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#27969;&#24182;&#35782;&#21035;&#24322;&#24120;&#12290;</title><link>https://arxiv.org/abs/2403.03563</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#30340;&#31227;&#21160;&#25805;&#20316;&#26426;&#22120;&#20154;&#29289;&#20307;&#28369;&#31227;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multimodal Anomaly Detection based on Deep Auto-Encoder for Object Slip Perception of Mobile Manipulation Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03563
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#30340;&#22810;&#27169;&#24577;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#31227;&#21160;&#25805;&#20316;&#26426;&#22120;&#20154;&#30340;&#29289;&#20307;&#28369;&#31227;&#24863;&#30693;&#65292;&#33021;&#22815;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#26426;&#22120;&#20154;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#27969;&#24182;&#35782;&#21035;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#20307;&#28369;&#31227;&#24863;&#30693;&#23545;&#20110;&#31227;&#21160;&#25805;&#20316;&#26426;&#22120;&#20154;&#22312;&#21160;&#24577;&#23454;&#38469;&#29615;&#22659;&#20013;&#21487;&#38752;&#25191;&#34892;&#25805;&#20316;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#33218;&#28369;&#31227;&#24863;&#30693;&#26041;&#27861;&#20351;&#29992;&#35302;&#35273;&#25110;&#35270;&#35273;&#20256;&#24863;&#22120;&#12290;&#28982;&#32780;&#65292;&#31227;&#21160;&#26426;&#22120;&#20154;&#20173;&#28982;&#38656;&#35201;&#22788;&#29702;&#20256;&#24863;&#22120;&#20449;&#21495;&#20013;&#30340;&#22122;&#22768;&#65292;&#36825;&#26159;&#30001;&#26426;&#22120;&#20154;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#31227;&#21160;&#36896;&#25104;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#22810;&#24863;&#30693;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#25972;&#21512;&#20102;&#20174;&#21508;&#31181;&#26426;&#22120;&#20154;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#24322;&#26500;&#25968;&#25454;&#27969;&#65292;&#21253;&#25324;RGB&#21644;&#28145;&#24230;&#25668;&#20687;&#26426;&#12289;&#40614;&#20811;&#39118;&#20197;&#21450;&#21147;&#30697;&#20256;&#24863;&#22120;&#12290;&#32508;&#21512;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#65292;&#20197;&#26500;&#24314;&#34920;&#31034;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#38544;&#21464;&#37327;&#65292;&#25351;&#31034;&#27491;&#24120;&#29366;&#24577;&#12290;&#36890;&#36807;&#27979;&#37327;&#35757;&#32451;&#32534;&#30721;&#22120;&#30340;&#38544;&#21464;&#37327;&#20540;&#19982;&#36755;&#20837;&#20540;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#35782;&#21035;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03563v1 Announce Type: cross  Abstract: Object slip perception is essential for mobile manipulation robots to perform manipulation tasks reliably in the dynamic real-world. Traditional approaches to robot arms' slip perception use tactile or vision sensors. However, mobile robots still have to deal with noise in their sensor signals caused by the robot's movement in a changing environment. To solve this problem, we present an anomaly detection method that utilizes multisensory data based on a deep autoencoder model. The proposed framework integrates heterogeneous data streams collected from various robot sensors, including RGB and depth cameras, a microphone, and a force-torque sensor. The integrated data is used to train a deep autoencoder to construct latent representations of the multisensory data that indicate the normal status. Anomalies can then be identified by error scores measured by the difference between the trained encoder's latent values and the latent values of
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32676;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20004;&#32423;&#26377;&#38480;&#21644;&#20985;&#20984;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#32467;&#26500;&#21644;&#38543;&#26426;&#26041;&#24046;&#20943;&#23567;&#38236;&#20687;Prox&#31639;&#27861;&#65292;&#23454;&#29616;&#23545;&#25152;&#26377;&#32452;&#30340;&#26041;&#24046;&#20943;&#23569;&#65292;&#24182;&#25903;&#25345;&#38750;&#24658;&#23450;&#23398;&#20064;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.03562</link><description>&lt;p&gt;
&#39640;&#25928;&#31639;&#27861;&#29992;&#20110;&#32463;&#39564;&#32676;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#21450;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
Efficient Algorithms for Empirical Group Distributional Robust Optimization and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03562
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32676;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20004;&#32423;&#26377;&#38480;&#21644;&#20985;&#20984;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#32467;&#26500;&#21644;&#38543;&#26426;&#26041;&#24046;&#20943;&#23567;&#38236;&#20687;Prox&#31639;&#27861;&#65292;&#23454;&#29616;&#23545;&#25152;&#26377;&#32452;&#30340;&#26041;&#24046;&#20943;&#23569;&#65292;&#24182;&#25903;&#25345;&#38750;&#24658;&#23450;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32676;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;GDRO&#65289;&#30340;&#32463;&#39564;&#23545;&#24212;&#38382;&#39064;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;$m$&#20010;&#19981;&#21516;&#32452;&#20013;&#30340;&#26368;&#22823;&#32463;&#39564;&#39118;&#38505;&#12290;&#25105;&#20204;&#23558;&#32463;&#39564;GDRO&#34920;&#36848;&#20026;$\textit{&#20004;&#32423;}$&#26377;&#38480;&#21644;&#20985;&#20984;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#38543;&#26426;&#26041;&#24046;&#20943;&#23567;&#38236;&#20687;Prox&#31639;&#27861;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#36880;&#32452;&#25277;&#26679;&#25216;&#26415;&#26500;&#24314;&#20102;&#38543;&#26426;&#26799;&#24230;&#65292;&#24182;&#20026;&#25152;&#26377;&#32452;&#25191;&#34892;&#26041;&#24046;&#20943;&#23569;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#32463;&#39564;GDRO&#30340;$\textit{&#20004;&#32423;}$&#26377;&#38480;&#21644;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#19968;&#32034;&#24341;&#20559;&#31227;&#21152;&#26435;&#24179;&#22343;&#26469;&#35745;&#31639;&#24555;&#29031;&#21644;&#38236;&#20687;&#24555;&#29031;&#28857;&#65292;&#36825;&#20351;&#25105;&#20204;&#19982;&#26420;&#32032;&#30340;&#36941;&#21382;&#24179;&#22343;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36824;&#25903;&#25345;&#38750;&#24658;&#23450;&#23398;&#20064;&#29575;&#65292;&#36825;&#19982;&#29616;&#26377;&#25991;&#29486;&#19981;&#21516;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#26399;&#26395;&#21644;&#39640;&#27010;&#29575;&#25910;&#25947;&#20445;&#35777;&#65292;&#23637;&#31034;&#20986;$\m
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03562v1 Announce Type: new  Abstract: We investigate the empirical counterpart of group distributionally robust optimization (GDRO), which aims to minimize the maximal empirical risk across $m$ distinct groups. We formulate empirical GDRO as a $\textit{two-level}$ finite-sum convex-concave minimax optimization problem and develop a stochastic variance reduced mirror prox algorithm. Unlike existing methods, we construct the stochastic gradient by per-group sampling technique and perform variance reduction for all groups, which fully exploits the $\textit{two-level}$ finite-sum structure of empirical GDRO. Furthermore, we compute the snapshot and mirror snapshot point by a one-index-shifted weighted average, which distinguishes us from the naive ergodic average. Our algorithm also supports non-constant learning rates, which is different from existing literature. We establish convergence guarantees both in expectation and with high probability, demonstrating a complexity of $\m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#22522;&#20110;&#20154;&#21475;&#30340;&#32435;&#20160;&#22343;&#34913;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#39069;&#22806;&#30340;&#20869;&#24490;&#29615;&#37325;&#25773;&#32531;&#20914;&#21306;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#20174;&#20219;&#20309;&#20998;&#24067;&#23454;&#29616;&#32435;&#20160;&#22343;&#34913;&#65292;&#20174;&#32780;&#22312;&#22810;&#20195;&#29702;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#25910;&#25947;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03552</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20154;&#21475;&#24863;&#30693;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#35299;&#22343;&#22330;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Population-aware Online Mirror Descent for Mean-Field Games by Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#22522;&#20110;&#20154;&#21475;&#30340;&#32435;&#20160;&#22343;&#34913;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#39069;&#22806;&#30340;&#20869;&#24490;&#29615;&#37325;&#25773;&#32531;&#20914;&#21306;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#20174;&#20219;&#20309;&#20998;&#24067;&#23454;&#29616;&#32435;&#20160;&#22343;&#34913;&#65292;&#20174;&#32780;&#22312;&#22810;&#20195;&#29702;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#25910;&#25947;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22343;&#22330;&#21338;&#24328;&#65288;MFGs&#65289;&#20855;&#26377;&#22788;&#29702;&#22823;&#35268;&#27169;&#22810;&#26234;&#20307;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;MFGs&#20013;&#23398;&#20064;&#32435;&#20160;&#22343;&#34913;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;Munchausen RL&#21644;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#30340;&#21551;&#21457;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#20154;&#21475;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#26080;&#38656;&#23545;&#21382;&#21490;&#36827;&#34892;&#24179;&#22343;&#25110;&#25277;&#26679;&#12290;&#36890;&#36807;&#35774;&#35745;&#39069;&#22806;&#30340;&#20869;&#24490;&#29615;&#37325;&#25773;&#32531;&#20914;&#21306;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#20174;&#20219;&#20309;&#20998;&#24067;&#23454;&#29616;&#32435;&#20160;&#22343;&#34913;&#65292;&#20943;&#36731;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24433;&#21709;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#31574;&#30053;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#21021;&#22987;&#20998;&#24067;&#12290;&#23545;&#22235;&#20010;&#32463;&#20856;&#31034;&#20363;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#27604;SOTA&#31639;&#27861;&#26356;&#22909;&#30340;&#25910;&#25947;&#29305;&#24615;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#20154;&#21475;&#30340;&#31574;&#30053;&#30340;DRL&#29256;&#26412;&#34394;&#26500;&#21338;&#24328;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03552v1 Announce Type: cross  Abstract: Mean Field Games (MFGs) have the ability to handle large-scale multi-agent systems, but learning Nash equilibria in MFGs remains a challenging task. In this paper, we propose a deep reinforcement learning (DRL) algorithm that achieves population-dependent Nash equilibrium without the need for averaging or sampling from history, inspired by Munchausen RL and Online Mirror Descent. Through the design of an additional inner-loop replay buffer, the agents can effectively learn to achieve Nash equilibrium from any distribution, mitigating catastrophic forgetting. The resulting policy can be applied to various initial distributions. Numerical experiments on four canonical examples demonstrate our algorithm has better convergence properties than SOTA algorithms, in particular a DRL version of Fictitious Play for population-dependent policies.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#35843;UNet&#36827;&#34892;&#20302;&#21058;&#37327;CT&#22270;&#20687;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#31532;&#20108;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#20026;CT&#22270;&#20687;&#22686;&#24378;&#38454;&#27573;&#12290;</title><link>https://arxiv.org/abs/2403.03551</link><description>&lt;p&gt;
&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#20026;&#39640;&#26031;&#38477;&#22122;&#32780;&#35757;&#32451;&#30340;UNet&#36827;&#34892;&#20302;&#21058;&#37327;CT&#22270;&#20687;&#37325;&#24314;&#65292;&#29992;&#20110;&#22270;&#20687;&#22686;&#24378;&#30340;&#19979;&#28216;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Low-Dose CT Image Reconstruction by Fine-Tuning a UNet Pretrained for Gaussian Denoising for the Downstream Task of Image Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03551
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#35843;UNet&#36827;&#34892;&#20302;&#21058;&#37327;CT&#22270;&#20687;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#31532;&#20108;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#20026;CT&#22270;&#20687;&#22686;&#24378;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#21307;&#23398;&#25104;&#20687;&#27169;&#24577;&#65292;&#30001;&#20110;&#20854;&#22522;&#20110;&#30005;&#31163;&#36752;&#23556;&#65292;&#22240;&#27492;&#24076;&#26395;&#23613;&#37327;&#20943;&#23569;&#36752;&#23556;&#21058;&#37327;&#12290;&#28982;&#32780;&#65292;&#38477;&#20302;&#36752;&#23556;&#21058;&#37327;&#20250;&#23548;&#33268;&#22270;&#20687;&#36136;&#37327;&#19979;&#38477;&#65292;&#20174;&#20302;&#21058;&#37327;CT&#65288;LDCT&#65289;&#25968;&#25454;&#37325;&#24314;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20540;&#24471;&#36827;&#34892;&#30740;&#31350;&#12290;&#26681;&#25454;LoDoPaB-CT&#22522;&#20934;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#28041;&#21450;UNet&#22411;&#26550;&#26500;&#30340;&#27969;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25490;&#21517;&#31532;&#19968;&#30340;&#26041;&#27861;ItNet&#20351;&#29992;&#21253;&#25324;&#28388;&#27874;&#21453;&#25237;&#24433;&#65288;FBP&#65289;&#12289;&#22312;CT&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;UNet&#21644;&#36845;&#20195;&#32454;&#21270;&#27493;&#39588;&#30340;&#19977;&#38454;&#27573;&#27969;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#12290;&#31532;&#19968;&#38454;&#27573;&#20063;&#20351;&#29992;&#20102;FBP&#65292;&#32780;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#31532;&#20108;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#29305;&#28857;&#26159;CT&#22270;&#20687;&#22686;&#24378;&#38454;&#27573;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#28857;&#22312;&#20110;&#31070;&#32463;&#32593;&#32476;&#26159;&#39044;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03551v1 Announce Type: cross  Abstract: Computed Tomography (CT) is a widely used medical imaging modality, and as it is based on ionizing radiation, it is desirable to minimize the radiation dose. However, a reduced radiation dose comes with reduced image quality, and reconstruction from low-dose CT (LDCT) data is still a challenging task which is subject to research. According to the LoDoPaB-CT benchmark, a benchmark for LDCT reconstruction, many state-of-the-art methods use pipelines involving UNet-type architectures. Specifically the top ranking method, ItNet, employs a three-stage process involving filtered backprojection (FBP), a UNet trained on CT data, and an iterative refinement step. In this paper, we propose a less complex two-stage method. The first stage also employs FBP, while the novelty lies in the training strategy for the second stage, characterized as the CT image enhancement stage. The crucial point of our approach is that the neural network is pretrained
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20302;&#22797;&#26434;&#24230;MIMO&#20449;&#36947;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#35774;&#35745;&#36731;&#37327;&#32423;CNN&#21644;&#20301;&#32622;&#23884;&#20837;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#31232;&#30095;&#35282;&#22495;&#20013;&#23545;&#20449;&#36947;&#20998;&#24067;&#36827;&#34892;&#20272;&#35745;&#65292;&#24182;&#37319;&#29992;&#36991;&#20813;&#38543;&#26426;&#37325;&#37319;&#26679;&#30340;&#20272;&#35745;&#31574;&#30053;&#65292;&#20197;&#21450;&#25130;&#26029;&#21453;&#21521;&#25193;&#25955;&#27493;&#39588;&#30340;&#31574;&#30053;&#65292;&#21462;&#24471;&#20102;&#20302;&#22797;&#26434;&#24230;&#21644;&#20869;&#23384;&#24320;&#38144;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.03545</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20302;&#22797;&#26434;&#24230;MIMO&#20449;&#36947;&#20272;&#35745;&#30340;&#29983;&#25104;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based Generative Prior for Low-Complexity MIMO Channel Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03545
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20302;&#22797;&#26434;&#24230;MIMO&#20449;&#36947;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#35774;&#35745;&#36731;&#37327;&#32423;CNN&#21644;&#20301;&#32622;&#23884;&#20837;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#31232;&#30095;&#35282;&#22495;&#20013;&#23545;&#20449;&#36947;&#20998;&#24067;&#36827;&#34892;&#20272;&#35745;&#65292;&#24182;&#37319;&#29992;&#36991;&#20813;&#38543;&#26426;&#37325;&#37319;&#26679;&#30340;&#20272;&#35745;&#31574;&#30053;&#65292;&#20197;&#21450;&#25130;&#26029;&#21453;&#21521;&#25193;&#25955;&#27493;&#39588;&#30340;&#31574;&#30053;&#65292;&#21462;&#24471;&#20102;&#20302;&#22797;&#26434;&#24230;&#21644;&#20869;&#23384;&#24320;&#38144;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#30340;&#26032;&#22411;&#20449;&#36947;&#20272;&#35745;&#22120;&#65292;&#36825;&#26159;&#30446;&#21069;&#35780;&#20215;&#26368;&#39640;&#30340;&#29983;&#25104;&#27169;&#22411;&#20043;&#19968;&#12290;&#19982;&#21033;&#29992;&#29983;&#25104;&#20808;&#39564;&#30340;&#30456;&#20851;&#24037;&#20316;&#30456;&#21453;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#23398;&#20064;&#31232;&#30095;&#35282;&#22495;&#20013;&#30340;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#20449;&#24687;&#30340;&#20301;&#32622;&#23884;&#20837;&#26469;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#32467;&#21512;&#19968;&#20010;&#36991;&#20813;&#38543;&#26426;&#37325;&#37319;&#26679;&#30340;&#20272;&#35745;&#31574;&#30053;&#65292;&#24182;&#25130;&#26029;&#29992;&#20110;&#35299;&#37322;&#27604;&#32473;&#23450;&#23548;&#39057;&#35266;&#23519;&#26356;&#20302;&#20449;&#22122;&#27604;&#30340;&#21453;&#21521;&#25193;&#25955;&#27493;&#39588;&#65292;&#23548;&#33268;&#30340;DM&#20449;&#36947;&#20272;&#35745;&#22120;&#20855;&#26377;&#20302;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#24320;&#38144;&#12290; &#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#21033;&#29992;&#29983;&#25104;&#20808;&#39564;&#30340;&#26368;&#20808;&#36827;&#20449;&#36947;&#20272;&#35745;&#22120;&#30456;&#27604;&#65292;DM&#20272;&#35745;&#22120;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03545v1 Announce Type: cross  Abstract: This work proposes a novel channel estimator based on diffusion models (DMs), one of the currently top-rated generative models. Contrary to related works utilizing generative priors, a lightweight convolutional neural network (CNN) with positional embedding of the signal-to-noise ratio (SNR) information is designed by learning the channel distribution in the sparse angular domain. Combined with an estimation strategy that avoids stochastic resampling and truncates reverse diffusion steps that account for lower SNR than the given pilot observation, the resulting DM estimator has both low complexity and memory overhead. Numerical results exhibit better performance than state-of-the-art channel estimators utilizing generative priors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#21435;&#22122;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21487;&#20197;&#26356;&#31283;&#23450;&#12289;&#26356;&#39640;&#25928;&#22320;&#22312;PDE&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#22522;&#20110;&#20613;&#37324;&#21494;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#26550;&#26500;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#36731;&#26494;&#25193;&#23637;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;PDE&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;SOTA&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.03542</link><description>&lt;p&gt;
DPOT: &#33258;&#22238;&#24402;&#21435;&#22122;&#36816;&#31639;&#22120;&#21464;&#25442;&#22120;&#29992;&#20110;&#22823;&#35268;&#27169;PDE&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#21435;&#22122;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21487;&#20197;&#26356;&#31283;&#23450;&#12289;&#26356;&#39640;&#25928;&#22320;&#22312;PDE&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#22522;&#20110;&#20613;&#37324;&#21494;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#26550;&#26500;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#36731;&#26494;&#25193;&#23637;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;PDE&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;SOTA&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#24050;&#32463;&#34987;&#30740;&#31350;&#29992;&#26469;&#25552;&#39640;&#22312;&#25968;&#25454;&#31232;&#32570;&#29615;&#22659;&#20013;&#35757;&#32451;&#31070;&#32463;&#31639;&#23376;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#25968;&#25454;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#22914;&#38271;&#36712;&#36857;&#12289;&#22810;&#20010;&#23610;&#24230;&#21644;&#19981;&#21516;&#32500;&#24230;&#65292;&#23427;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#36824;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#21435;&#22122;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#36825;&#31181;&#31574;&#30053;&#33021;&#22815;&#26356;&#31283;&#23450;&#12289;&#26356;&#39640;&#25928;&#22320;&#22312;PDE&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#27867;&#21270;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22522;&#20110;&#20613;&#37324;&#21494;&#27880;&#24847;&#21147;&#30340;&#28789;&#27963;&#21487;&#25193;&#23637;&#27169;&#22411;&#26550;&#26500;&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#22320;&#23558;&#27169;&#22411;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;10+&#20010;PDE&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#20855;&#26377;&#36229;&#36807;0.5B&#21442;&#25968;&#30340;PDE&#22522;&#30784;&#27169;&#22411;&#65292;&#21253;&#25324;&#36229;&#36807;100k&#36712;&#36857;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#22312;&#36825;&#20123;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;SOTA&#65292;&#24182;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03542v1 Announce Type: new  Abstract: Pre-training has been investigated to improve the efficiency and performance of training neural operators in data-scarce settings. However, it is largely in its infancy due to the inherent complexity and diversity, such as long trajectories, multiple scales and varying dimensions of partial differential equations (PDEs) data. In this paper, we present a new auto-regressive denoising pre-training strategy, which allows for more stable and efficient pre-training on PDE data and generalizes to various downstream tasks. Moreover, by designing a flexible and scalable model architecture based on Fourier attention, we can easily scale up the model for large-scale pre-training. We train our PDE foundation model with up to 0.5B parameters on 10+ PDE datasets with more than 100k trajectories. Extensive experiments show that we achieve SOTA on these benchmarks and validate the strong generalizability of our model to significantly enhance performanc
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26465;&#20214;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#23545;&#27604;&#20449;&#21495;&#25552;&#39640;&#24402;&#32435;&#22270;&#20687;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#38022;&#21058;&#22312;&#33041;&#37096;MRI&#20013;&#29992;&#37327;&#20943;&#23569;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.03539</link><description>&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#28145;&#24230;&#23398;&#20064;&#20943;&#23569;&#33041;&#37096;MRI&#20013;&#30340;&#38022;&#21058;&#29992;&#37327;
&lt;/p&gt;
&lt;p&gt;
Gadolinium dose reduction for brain MRI using conditional deep learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03539
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#23545;&#27604;&#20449;&#21495;&#25552;&#39640;&#24402;&#32435;&#22270;&#20687;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#38022;&#21058;&#22312;&#33041;&#37096;MRI&#20013;&#29992;&#37327;&#20943;&#23569;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#20943;&#23569;&#38022;&#22522;&#36896;&#24433;&#21058;&#65288;GBCAs&#65289;&#20197;&#20943;&#36731;&#19981;&#33391;&#21103;&#20316;&#29992;&#21516;&#26102;&#20445;&#25345;&#35786;&#26029;&#20215;&#20540;&#12290;&#30446;&#21069;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#20934;&#30830;&#39044;&#27979;&#23545;&#27604;&#22686;&#24378;&#21644;&#21512;&#25104;&#36924;&#30495;&#22270;&#20687;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#39044;&#23545;&#27604;&#21644;&#21518;&#23545;&#27604;&#22270;&#20687;&#23545;&#30340;&#20943;&#27861;&#22270;&#20687;&#20013;&#32534;&#30721;&#30340;&#23545;&#27604;&#20449;&#21495;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#36991;&#20813;&#21512;&#25104;&#20219;&#20309;&#22122;&#38899;&#25110;&#20266;&#24433;&#65292;&#24182;&#20165;&#19987;&#27880;&#20110;&#20174;&#20302;&#21058;&#37327;&#20943;&#27861;&#22270;&#20687;&#20013;&#25552;&#21462;&#21644;&#22686;&#24378;&#23545;&#27604;&#20449;&#21495;&#65292;&#25105;&#20204;&#20351;&#29992;&#26080;&#22122;&#22768;&#30340;&#26631;&#20934;&#21058;&#37327;&#20943;&#27861;&#22270;&#20687;&#20316;&#20026;&#30446;&#26631;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;DL&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20165;&#39044;&#27979;&#23545;&#27604;&#22686;&#24378;&#20449;&#21495;&#65307;&#20174;&#32780;&#23454;&#29616;&#36229;&#20986;&#26631;&#20934;&#21058;&#37327;&#30340;&#22270;&#20687;&#21512;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20511;&#37492;&#20102;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#23884;&#20837;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03539v1 Announce Type: cross  Abstract: Recently, deep learning (DL)-based methods have been proposed for the computational reduction of gadolinium-based contrast agents (GBCAs) to mitigate adverse side effects while preserving diagnostic value. Currently, the two main challenges for these approaches are the accurate prediction of contrast enhancement and the synthesis of realistic images. In this work, we address both challenges by utilizing the contrast signal encoded in the subtraction images of pre-contrast and post-contrast image pairs. To avoid the synthesis of any noise or artifacts and solely focus on contrast signal extraction and enhancement from low-dose subtraction images, we train our DL model using noise-free standard-dose subtraction images as targets. As a result, our model predicts the contrast enhancement signal only; thereby enabling synthesization of images beyond the standard dose. Furthermore, we adapt the embedding idea of recent diffusion-based models
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20219;&#21153;&#23646;&#24615;&#36317;&#31163;&#65288;TAD&#65289;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#35757;&#32451;&#21644;&#26032;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#35813;&#20851;&#31995;&#23545;&#19981;&#21516;&#27169;&#22411;&#22312;&#26032;&#20219;&#21153;&#19978;&#36866;&#24212;&#22256;&#38590;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.03535</link><description>&lt;p&gt;
&#20219;&#21153;&#23646;&#24615;&#36317;&#31163;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#65306;&#29702;&#35770;&#20998;&#26512;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Task Attribute Distance for Few-Shot Learning: Theoretical Analysis and Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20219;&#21153;&#23646;&#24615;&#36317;&#31163;&#65288;TAD&#65289;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#35757;&#32451;&#21644;&#26032;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#35813;&#20851;&#31995;&#23545;&#19981;&#21516;&#27169;&#22411;&#22312;&#26032;&#20219;&#21153;&#19978;&#36866;&#24212;&#22256;&#38590;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;FSL&#65289;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#20174;\emph{&#30456;&#20851;}&#35757;&#32451;&#20219;&#21153;&#20013;&#31215;&#32047;&#30340;&#32463;&#39564;&#65292;&#29992;&#26497;&#23569;&#26631;&#35760;&#26679;&#26412;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#35752;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#26469;&#29702;&#35299;FSL&#65306;&#65288;1&#65289;&#22914;&#20309;&#37327;&#21270;\emph{&#35757;&#32451;}&#21644;\emph{&#26032;}&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65311;&#65288;2&#65289;&#36825;&#31181;&#20851;&#31995;&#22914;&#20309;&#24433;&#21709;&#19981;&#21516;&#27169;&#22411;&#23545;&#26032;&#20219;&#21153;&#30340;\emph{&#36866;&#24212;&#22256;&#38590;}&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#23646;&#24615;&#30340;&#20219;&#21153;&#23646;&#24615;&#36317;&#31163;&#65288;TAD&#65289;&#20316;&#20026;&#24230;&#37327;&#20219;&#21153;&#30456;&#20851;&#24615;&#30340;&#25351;&#26631;&#12290;&#19982;&#35768;&#22810;&#29616;&#26377;&#24230;&#37327;&#19981;&#21516;&#65292;TAD&#26159;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;FSL&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;TAD&#25351;&#26631;&#24314;&#31435;&#20102;&#20219;&#21153;&#30456;&#20851;&#24615;&#21644;&#20219;&#21153;&#36866;&#24212;&#22256;&#38590;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#12290;&#36890;&#36807;&#22312;&#26032;&#20219;&#21153;&#19978;&#25512;&#23548;&#24191;&#20041;&#21270;&#38169;&#35823;&#30028;&#38480;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;TAD&#22914;&#20309;&#24230;&#37327;FSL&#27169;&#22411;&#22312;&#26032;&#20219;&#21153;&#19978;&#30340;&#36866;&#24212;&#22256;&#38590;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;TAD&#25351;&#26631;&#21644;th
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03535v1 Announce Type: cross  Abstract: Few-shot learning (FSL) aims to learn novel tasks with very few labeled samples by leveraging experience from \emph{related} training tasks. In this paper, we try to understand FSL by delving into two key questions: (1) How to quantify the relationship between \emph{training} and \emph{novel} tasks? (2) How does the relationship affect the \emph{adaptation difficulty} on novel tasks for different models? To answer the two questions, we introduce Task Attribute Distance (TAD) built upon attributes as a metric to quantify the task relatedness. Unlike many existing metrics, TAD is model-agnostic, making it applicable to different FSL models. Then, we utilize TAD metric to establish a theoretical connection between task relatedness and task adaptation difficulty. By deriving the generalization error bound on a novel task, we discover how TAD measures the adaptation difficulty on novel tasks for FSL models. To validate our TAD metric and th
&lt;/p&gt;</description></item><item><title>FingerNet&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#32454;&#33268;&#25163;&#25351;&#24819;&#35937;&#36816;&#21160;&#20998;&#31867;&#30340;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;EEG&#20449;&#21495;&#20013;&#25552;&#21462;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#22312;&#21516;&#19968;&#21482;&#25163;&#20869;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#26041;&#38754;&#34920;&#29616;&#26174;&#33879;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2403.03526</link><description>&lt;p&gt;
FingerNet: &#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25163;&#25351;&#25970;&#20987;&#20219;&#21153;&#30340; EEG &#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
FingerNet: EEG Decoding of A Fine Motor Imagery with Finger-tapping Task Based on A Deep Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03526
&lt;/p&gt;
&lt;p&gt;
FingerNet&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#32454;&#33268;&#25163;&#25351;&#24819;&#35937;&#36816;&#21160;&#20998;&#31867;&#30340;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;EEG&#20449;&#21495;&#20013;&#25552;&#21462;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#22312;&#21516;&#19968;&#21482;&#25163;&#20869;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#26041;&#38754;&#34920;&#29616;&#26174;&#33879;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#25216;&#26415;&#20419;&#36827;&#20102;&#20154;&#33041;&#19982;&#35745;&#31639;&#26426;&#20043;&#38388;&#30340;&#20132;&#27969;&#65292;&#20027;&#35201;&#21033;&#29992;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#26469;&#35782;&#21035;&#20154;&#31867;&#24847;&#22270;&#12290;&#23613;&#31649;&#38024;&#23545;&#30251;&#30186;&#24739;&#32773;&#24050;&#24320;&#21457;&#20102;&#22522;&#20110;EEG&#30340;BCI&#31995;&#32479;&#65292;&#20294;&#27491;&#22312;&#36827;&#34892;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#35821;&#35328;&#24819;&#35937;&#21644;&#36816;&#21160;&#24819;&#35937;&#65288;MI&#65289;&#31995;&#32479;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102; FingerNet&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#31934;&#32454;MI&#20998;&#31867;&#30340;&#32593;&#32476;&#65292;&#20559;&#31163;&#20102;&#20256;&#32479;&#30340;&#31895;&#31961;MI&#30740;&#31350;&#12290;&#25152;&#25552;&#20986;&#30340; FingerNet &#21487;&#20197;&#20174; EEG &#20449;&#21495;&#20013;&#25552;&#21462;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#22312;&#21516;&#19968;&#21482;&#25163;&#20869;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#31867;&#20116;&#20010;&#25163;&#25351;&#25970;&#20987;&#20219;&#21153;&#65288;&#25287;&#25351;&#12289;&#39135;&#25351;&#12289;&#20013;&#25351;&#12289;&#26080;&#21517;&#25351;&#21644;&#23567;&#25351;&#36816;&#21160;&#65289;&#26041;&#38754;&#24615;&#33021;&#26174;&#31034;&#20986;&#26174;&#33879;&#36739;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;&#19982;&#20256;&#32479;&#22522;&#20934;&#27169;&#22411; EEGNet &#21644; DeepConvNet &#30456;&#27604;&#65292;FingerNet &#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03526v1 Announce Type: cross  Abstract: Brain-computer interface (BCI) technology facilitates communication between the human brain and computers, primarily utilizing electroencephalography (EEG) signals to discern human intentions. Although EEG-based BCI systems have been developed for paralysis individuals, ongoing studies explore systems for speech imagery and motor imagery (MI). This study introduces FingerNet, a specialized network for fine MI classification, departing from conventional gross MI studies. The proposed FingerNet could extract spatial and temporal features from EEG signals, improving classification accuracy within the same hand. The experimental results demonstrated that performance showed significantly higher accuracy in classifying five finger-tapping tasks, encompassing thumb, index, middle, ring, and little finger movements. FingerNet demonstrated dominant performance compared to the conventional baseline models, EEGNet and DeepConvNet. The average acc
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#21644;&#25216;&#26415;&#39564;&#35777;&#27010;&#24565;&#65292;&#29992;&#20110;&#23545;&#35328;&#35821;&#20013;&#30340;&#38750;&#35328;&#35821;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#19982;&#21547;&#20041;&#20851;&#32852;&#36215;&#26469;&#65292;&#20174;&#32780;&#20026;&#25506;&#32034;&#34920;&#36798;&#23454;&#29616;&#22810;&#23618;&#38901;&#24459;&#20107;&#20214;&#30340;&#22823;&#22411;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03522</link><description>&lt;p&gt;
&#33258;&#21457;&#24615;&#35328;&#35821;&#20013;&#30340;&#38750;&#35328;&#35821;&#20449;&#24687; - &#26397;&#30528;&#26032;&#30340;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Non-verbal information in spontaneous speech - towards a new framework of analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03522
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#21644;&#25216;&#26415;&#39564;&#35777;&#27010;&#24565;&#65292;&#29992;&#20110;&#23545;&#35328;&#35821;&#20013;&#30340;&#38750;&#35328;&#35821;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#19982;&#21547;&#20041;&#20851;&#32852;&#36215;&#26469;&#65292;&#20174;&#32780;&#20026;&#25506;&#32034;&#34920;&#36798;&#23454;&#29616;&#22810;&#23618;&#38901;&#24459;&#20107;&#20214;&#30340;&#22823;&#22411;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35328;&#35821;&#20013;&#30340;&#38750;&#35328;&#35821;&#20449;&#21495;&#26159;&#30001;&#38901;&#24459;&#32534;&#30721;&#30340;&#65292;&#25658;&#24102;&#30340;&#20449;&#24687;&#33539;&#22260;&#20174;&#23545;&#35805;&#34892;&#20026;&#21040;&#24577;&#24230;&#21644;&#24773;&#24863;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#25484;&#25569;&#25484;&#22768;&#32467;&#26500;&#30340;&#21407;&#21017;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#21644;&#25216;&#26415;&#39564;&#35777;&#27010;&#24565;&#65292;&#29992;&#20110;&#23545;&#38901;&#24459;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#19982;&#21547;&#20041;&#20851;&#32852;&#36215;&#26469;&#12290;&#35813;&#26694;&#26550;&#35299;&#37322;&#20102;&#22810;&#23618;&#38901;&#24459;&#20107;&#20214;&#30340;&#34920;&#23618;&#34920;&#31034;&#12290;&#20316;&#20026;&#23454;&#26045;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#36807;&#31243;&#65292;&#21487;&#20197;&#35299;&#24320;&#19977;&#20010;&#32423;&#21035;&#30340;&#38901;&#24459;&#29616;&#35937;&#12290;&#23427;&#20381;&#36182;&#20110;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#23454;&#29616;&#21516;&#26102;&#30340;&#22810;&#31867;&#21035;/&#22810;&#26631;&#31614;&#26816;&#27979;&#12290;&#23427;&#21487;&#20197;&#27010;&#25324;&#21508;&#31181;&#21508;&#26679;&#30340;&#33258;&#21457;&#25968;&#25454;&#65292;&#22312;&#19982;&#20154;&#31867;&#27880;&#37322;&#30456;&#24403;&#25110;&#20248;&#20110;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#12290;&#38500;&#20102;&#23545;&#38901;&#24459;&#30340;&#26631;&#20934;&#21270;&#24418;&#24335;&#21270;&#22806;&#65292;&#35299;&#24320;&#38901;&#24459;&#27169;&#24335;&#36824;&#21487;&#20197;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03522v1 Announce Type: cross  Abstract: Non-verbal signals in speech are encoded by prosody and carry information that ranges from conversation action to attitude and emotion. Despite its importance, the principles that govern prosodic structure are not yet adequately understood. This paper offers an analytical schema and a technological proof-of-concept for the categorization of prosodic signals and their association with meaning. The schema interprets surface-representations of multi-layered prosodic events. As a first step towards implementation, we present a classification process that disentangles prosodic phenomena of three orders. It relies on fine-tuning a pre-trained speech recognition model, enabling the simultaneous multi-class/multi-label detection. It generalizes over a large variety of spontaneous data, performing on a par with, or superior to, human annotation. In addition to a standardized formalization of prosody, disentangling prosodic patterns can direct a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CounterfacTS&#24037;&#20855;&#65292;&#36890;&#36807;&#21453;&#20107;&#23454;&#25506;&#31350;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03508</link><description>&lt;p&gt;
&#25506;&#31350;&#20351;&#29992;CounterfacTS&#25506;&#31350;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Probing the Robustness of Time-series Forecasting Models with CounterfacTS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03508
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CounterfacTS&#24037;&#20855;&#65292;&#36890;&#36807;&#21453;&#20107;&#23454;&#25506;&#31350;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26102;&#38754;&#20020;&#30340;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#26159;&#25968;&#25454;&#20998;&#24067;&#30340;&#26102;&#38388;&#28436;&#21270;&#65288;&#21363;&#27010;&#24565;&#28418;&#31227;&#65289;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#35757;&#32451;&#25968;&#25454;&#27809;&#26377;&#21453;&#26144;&#36825;&#20123;&#21464;&#21270;&#65292;&#27169;&#22411;&#22312;&#26032;&#30340;&#20998;&#24067;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#24456;&#24046;&#65292;&#22240;&#27492;&#65292;&#27492;&#31867;&#20107;&#20214;&#30340;&#24433;&#21709;&#20107;&#21069;&#26080;&#27861;&#21487;&#38752;&#22320;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#20844;&#24320;&#21457;&#24067;CounterfacTS&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#21453;&#20107;&#23454;&#25506;&#31350;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#40065;&#26834;&#24615;&#30340;&#24037;&#20855;&#12290;CounterfacTS&#20855;&#26377;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#20801;&#35768;&#29992;&#25143;&#21487;&#35270;&#21270;&#12289;&#27604;&#36739;&#21644;&#37327;&#21270;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21450;&#20854;&#39044;&#27979;&#32467;&#26524;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#21487;&#20197;&#23545;&#26102;&#38388;&#24207;&#21015;&#24212;&#29992;&#21508;&#31181;&#21464;&#25442;&#65292;&#24182;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#25506;&#32034;&#39044;&#27979;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#31034;&#20363;&#26696;&#20363;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;CounterfacTS&#22914;&#20309;&#29992;&#20110;&#65306;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03508v1 Announce Type: new  Abstract: A common issue for machine learning models applied to time-series forecasting is the temporal evolution of the data distributions (i.e., concept drift). Because most of the training data does not reflect such changes, the models present poor performance on the new out-of-distribution scenarios and, therefore, the impact of such events cannot be reliably anticipated ahead of time. We present and publicly release CounterfacTS, a tool to probe the robustness of deep learning models in time-series forecasting tasks via counterfactuals. CounterfacTS has a user-friendly interface that allows the user to visualize, compare and quantify time series data and their forecasts, for a number of datasets and deep learning models. Furthermore, the user can apply various transformations to the time series and explore the resulting changes in the forecasts in an interpretable manner. Through example cases, we illustrate how CounterfacTS can be used to i)
&lt;/p&gt;</description></item><item><title>GaLore&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Low-Rank Projection (GaLore)&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#30456;&#27604;&#20110;&#19968;&#33324;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;LLM&#35757;&#32451;&#65292;&#22823;&#24133;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03507</link><description>&lt;p&gt;
GaLore: &#36890;&#36807;&#26799;&#24230;&#20302;&#31209;&#25237;&#24433;&#23454;&#29616;&#39640;&#25928;&#30340;LLM&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03507
&lt;/p&gt;
&lt;p&gt;
GaLore&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Low-Rank Projection (GaLore)&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#30456;&#27604;&#20110;&#19968;&#33324;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;LLM&#35757;&#32451;&#65292;&#22823;&#24133;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23384;&#22312;&#26174;&#30528;&#30340;&#20869;&#23384;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#26435;&#37325;&#21644;&#20248;&#21270;&#22120;&#29366;&#24577;&#30340;&#19981;&#26029;&#22686;&#21152;&#12290;&#36890;&#24120;&#30340;&#20869;&#23384;&#20943;&#23569;&#26041;&#27861;&#65292;&#22914;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#65292;&#22312;&#27599;&#20010;&#23618;&#20013;&#21521;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#28155;&#21152;&#21487;&#35757;&#32451;&#30340;&#20302;&#31209;&#30697;&#38453;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#30340;&#21442;&#25968;&#21644;&#20248;&#21270;&#22120;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#38454;&#27573;&#30340;&#34920;&#29616;&#37117;&#19981;&#22914;&#23436;&#25972;&#31209;&#26435;&#37325;&#30340;&#35757;&#32451;&#65292;&#22240;&#20026;&#23427;&#20204;&#23558;&#21442;&#25968;&#25628;&#32034;&#38480;&#21046;&#22312;&#20302;&#31209;&#23376;&#31354;&#38388;&#24182;&#25913;&#21464;&#20102;&#35757;&#32451;&#21160;&#24577;&#65292;&#32780;&#19988;&#21487;&#33021;&#38656;&#35201;&#23436;&#25972;&#31209;&#30340;&#28909;&#21551;&#21160;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Gradient Low-Rank Projection (GaLore)&#65292;&#36825;&#26159;&#19968;&#31181;&#35757;&#32451;&#31574;&#30053;&#65292;&#20801;&#35768;&#23436;&#20840;&#21442;&#25968;&#23398;&#20064;&#65292;&#20294;&#27604;LoRA&#31561;&#24120;&#35265;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#26356;&#33410;&#30465;&#20869;&#23384;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20248;&#21270;&#22120;&#29366;&#24577;&#19978;&#23558;&#20869;&#23384;&#20351;&#29992;&#38477;&#20302;&#20102;&#39640;&#36798;65.5%&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39044;&#35757;&#32451;&#21644;&#31934;&#35843;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03507v1 Announce Type: new  Abstract: Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25945;&#24072;&#22270;&#33258;&#33976;&#39311;&#26694;&#26550;&#65292;&#19981;&#38656;&#35201;&#25945;&#24072;&#27169;&#22411;&#25110;GNN&#65292;&#20165;&#22522;&#20110;MLP&#65292;&#26377;&#21161;&#20110;&#32553;&#23567;&#23398;&#26415;&#21644;&#24037;&#19994;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.03483</link><description>&lt;p&gt;
&#19968;&#31181;&#21452;&#33258;&#33976;&#39311;&#30340;&#26080;&#25945;&#24072;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Teacher-Free Graph Knowledge Distillation Framework with Dual Self-Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25945;&#24072;&#22270;&#33258;&#33976;&#39311;&#26694;&#26550;&#65292;&#19981;&#38656;&#35201;&#25945;&#24072;&#27169;&#22411;&#25110;GNN&#65292;&#20165;&#22522;&#20110;MLP&#65292;&#26377;&#21161;&#20110;&#32553;&#23567;&#23398;&#26415;&#21644;&#24037;&#19994;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22788;&#29702;&#19982;&#22270;&#30456;&#20851;&#30340;&#20219;&#21153;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23613;&#31649;&#22312;&#23398;&#26415;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLPs&#65289;&#20173;&#28982;&#26159;&#23454;&#38469;&#24037;&#19994;&#24212;&#29992;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#26080;&#35770;&#26159;&#25945;&#24072;&#36824;&#26159;GNN&#23545;&#22270;&#30693;&#35782;&#33976;&#39311;&#37117;&#19981;&#26159;&#24517;&#38656;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25945;&#24072;&#22270;&#33258;&#33976;&#39311;&#65288;TGS&#65289;&#26694;&#26550;&#65292;&#26082;&#19981;&#38656;&#35201;&#25945;&#24072;&#27169;&#22411;&#20063;&#19981;&#38656;&#35201;GNNs&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#37117;&#19981;&#38656;&#35201;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25152;&#25552;&#20986;&#30340;TGS&#26694;&#26550;&#32431;&#31929;&#22522;&#20110;MLP&#65292;&#20854;&#20013;&#32467;&#26500;&#20449;&#24687;&#20165;&#34987;&#26263;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03483v1 Announce Type: new  Abstract: Recent years have witnessed great success in handling graph-related tasks with Graph Neural Networks (GNNs). Despite their great academic success, Multi-Layer Perceptrons (MLPs) remain the primary workhorse for practical industrial applications. One reason for such an academic-industry gap is the neighborhood-fetching latency incurred by data dependency in GNNs. To reduce their gaps, Graph Knowledge Distillation (GKD) is proposed, usually based on a standard teacher-student architecture, to distill knowledge from a large teacher GNN into a lightweight student GNN or MLP. However, we found in this paper that neither teachers nor GNNs are necessary for graph knowledge distillation. We propose a Teacher-Free Graph Self-Distillation (TGS) framework that does not require any teacher model or GNNs during both training and inference. More importantly, the proposed TGS framework is purely based on MLPs, where structural information is only impli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#24555;&#36895;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#65288;FNGD&#65289;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20165;&#38656;&#35201;&#22312;&#31532;&#19968;&#20010;&#26102;&#20195;&#35745;&#31639;&#36870;&#36816;&#31639;&#65292;&#36991;&#20813;&#20102;&#36845;&#20195;&#27714;&#36870;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.03473</link><description>&lt;p&gt;
&#26080;&#36870;&#30697;&#38453;&#24555;&#36895;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Inverse-Free Fast Natural Gradient Descent Method for Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#24555;&#36895;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#65288;FNGD&#65289;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20165;&#38656;&#35201;&#22312;&#31532;&#19968;&#20010;&#26102;&#20195;&#35745;&#31639;&#36870;&#36816;&#31639;&#65292;&#36991;&#20813;&#20102;&#36845;&#20195;&#27714;&#36870;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#38454;&#26041;&#27861;&#36890;&#36807;&#21253;&#21547;&#20108;&#38454;&#23548;&#25968;&#25110;&#32479;&#35745;&#37327;&#21487;&#20197;&#27604;&#19968;&#38454;&#26041;&#27861;&#25910;&#25947;&#24471;&#26356;&#24555;&#65292;&#20294;&#30001;&#20110;&#35745;&#31639;&#25928;&#29575;&#20302;&#65292;&#23427;&#20204;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#24456;&#23569;&#34987;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#35768;&#22810;&#35299;&#20915;&#26041;&#26696;&#37117;&#38598;&#20013;&#22312;&#20943;&#23567;&#38656;&#35201;&#27714;&#36870;&#30340;&#30697;&#38453;&#30340;&#22823;&#23567;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#38656;&#35201;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#25191;&#34892;&#27714;&#36870;&#25805;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#65288;FNGD&#65289;&#26041;&#27861;&#65292;&#21482;&#38656;&#22312;&#31532;&#19968;&#20010;&#26102;&#20195;&#35745;&#31639;&#36870;&#36816;&#31639;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#65288;NGD&#65289;&#30340;&#26799;&#24230;&#39044;&#22788;&#29702;&#20844;&#24335;&#37325;&#26500;&#20026;&#20351;&#29992;Sherman-Morrison-Woodbury&#20844;&#24335;&#30340;&#27599;&#20010;&#26679;&#26412;&#26799;&#24230;&#30340;&#21152;&#26435;&#21644;&#12290;&#22522;&#20110;&#27492;&#65292;&#20026;&#20102;&#36991;&#20813;&#28041;&#21450;&#35745;&#31639;&#31995;&#25968;&#30340;&#36845;&#20195;&#36870;&#25805;&#20316;&#65292;&#36825;&#20123;&#21152;&#26435;&#31995;&#25968;&#22312;&#25972;&#20010;&#26102;&#20195;&#20849;&#20139;&#32780;&#19981;&#24433;&#21709;&#32463;&#39564;&#24615;&#33021;&#12290;FNGD&#23558;NGD&#36817;&#20284;&#20026;f
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03473v1 Announce Type: new  Abstract: Second-order methods can converge much faster than first-order methods by incorporating second-order derivates or statistics, but they are far less prevalent in deep learning due to their computational inefficiency. To handle this, many of the existing solutions focus on reducing the size of the matrix to be inverted. However, it is still needed to perform the inverse operator in each iteration. In this paper, we present a fast natural gradient descent (FNGD) method, which only requires computing the inverse during the first epoch. Firstly, we reformulate the gradient preconditioning formula in the natural gradient descent (NGD) as a weighted sum of per-sample gradients using the Sherman-Morrison-Woodbury formula. Building upon this, to avoid the iterative inverse operation involved in computing coefficients, the weighted coefficients are shared across epochs without affecting the empirical performance.   FNGD approximates the NGD as a f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#36890;&#36807;&#22686;&#24378;&#20803;&#35757;&#32451;&#30340;&#22522;&#30784;&#31867;&#21035;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;Meta-Baseline&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#22266;&#26377;&#20914;&#31361;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.03472</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#30784;&#31867;&#21035;&#20449;&#24687;&#22686;&#24378;&#20803;&#35757;&#32451;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Boosting Meta-Training with Base Class Information for Few-Shot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03472
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#36890;&#36807;&#22686;&#24378;&#20803;&#35757;&#32451;&#30340;&#22522;&#30784;&#31867;&#21035;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;Meta-Baseline&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#22266;&#26377;&#20914;&#31361;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#36866;&#24212;&#35782;&#21035;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#31034;&#20363;&#30340;&#26032;&#31867;&#21035;&#30340;&#20998;&#31867;&#22120;&#12290;&#20803;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#37325;&#35201;&#26694;&#26550;&#12290;&#26368;&#21021;&#30340;&#35757;&#32451;&#26694;&#26550;&#26159;&#19968;&#20010;&#20219;&#21153;&#32423;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#27169;&#22411;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#21644;&#21407;&#22411;&#32593;&#32476;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#22521;&#35757;&#33539;&#24335;&#31216;&#20026;&#20803;Baseline&#65292;&#23427;&#30001;&#24207;&#36143;&#39044;&#35757;&#32451;&#21644;&#20803;&#35757;&#32451;&#38454;&#27573;&#32452;&#25104;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#31181;&#38750;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#34920;&#26126;&#20803;&#35757;&#32451;&#38454;&#27573;&#21482;&#33021;&#22312;&#39044;&#35757;&#32451;&#23436;&#25104;&#21518;&#24320;&#22987;&#65292;Meta-Baseline&#30001;&#20110;&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#30340;&#22266;&#26377;&#20914;&#31361;&#32780;&#23548;&#33268;&#26356;&#39640;&#30340;&#35757;&#32451;&#25104;&#26412;&#21644;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#20004;&#20010;&#20132;&#26367;&#24490;&#29615;&#32452;&#25104;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#33539;&#24335;&#12290;&#22312;&#22806;&#24490;&#29615;&#20013;&#65292;&#25105;&#20204;&#35745;&#31639;cr
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03472v1 Announce Type: new  Abstract: Few-shot learning, a challenging task in machine learning, aims to learn a classifier adaptable to recognize new, unseen classes with limited labeled examples. Meta-learning has emerged as a prominent framework for few-shot learning. Its training framework is originally a task-level learning method, such as Model-Agnostic Meta-Learning (MAML) and Prototypical Networks. And a recently proposed training paradigm called Meta-Baseline, which consists of sequential pre-training and meta-training stages, gains state-of-the-art performance. However, as a non-end-to-end training method, indicating the meta-training stage can only begin after the completion of pre-training, Meta-Baseline suffers from higher training cost and suboptimal performance due to the inherent conflicts of the two training stages. To address these limitations, we propose an end-to-end training paradigm consisting of two alternative loops. In the outer loop, we calculate cr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#23398;&#20064;&#26694;&#26550;GCN-SA&#65292;&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#35774;&#35745;&#31283;&#23450;&#26377;&#25928;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#27169;&#22359;&#65292;&#21487;&#20197;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#21644;&#20302;&#21516;&#36136;&#24615;&#22270;&#20013;&#33410;&#28857;&#30340;&#33410;&#28857;&#32423;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.03465</link><description>&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#22686;&#24378;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#32467;&#26500;&#23398;&#20064;&#21644;&#33410;&#28857;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Self-Attention Empowered Graph Convolutional Network for Structure Learning and Node Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#23398;&#20064;&#26694;&#26550;GCN-SA&#65292;&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#35774;&#35745;&#31283;&#23450;&#26377;&#25928;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#27169;&#22359;&#65292;&#21487;&#20197;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#21644;&#20302;&#21516;&#36136;&#24615;&#22270;&#20013;&#33410;&#28857;&#30340;&#33410;&#28857;&#32423;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#35768;&#22810;&#27969;&#34892;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26410;&#33021;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#24403;&#20851;&#27880;&#30340;&#22270;&#34920;&#24449;&#20026;&#24322;&#36136;&#24615;&#65288;&#20302;&#21516;&#36136;&#24615;&#65289;&#26102;&#65292;&#36825;&#31181;&#24369;&#28857;&#20250;&#34987;&#25918;&#22823;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;&#20855;&#26377;&#33258;&#27880;&#24847;&#21147;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN-SA&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#22312;&#33410;&#28857;&#32423;&#34920;&#31034;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;GCN-SA&#21253;&#21547;&#20004;&#20010;&#22686;&#24378;&#23545;&#24212;&#20110;&#36793;&#21644;&#33410;&#28857;&#29305;&#24449;&#12290;&#23545;&#20110;&#36793;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#35774;&#35745;&#20102;&#19968;&#20010;&#31283;&#23450;&#19988;&#26377;&#25928;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#27169;&#22359;&#65292;&#33021;&#22815;&#25429;&#25417;&#20219;&#24847;&#19968;&#23545;&#33410;&#28857;&#20043;&#38388;&#30340;&#20869;&#37096;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#22270;&#32467;&#26500;&#23398;&#20064;&#27169;&#22359;&#21487;&#20197;&#20174;&#25972;&#20010;&#22270;&#20013;&#20026;&#27599;&#20010;&#33410;&#28857;&#35782;&#21035;&#21487;&#38752;&#30340;&#37051;&#23621;&#12290;&#20851;&#20110;&#33410;&#28857;&#29305;&#24449;&#65292;&#25105;&#20204;&#20462;&#25913;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03465v1 Announce Type: new  Abstract: In representation learning on graph-structured data, many popular graph neural networks (GNNs) fail to capture long-range dependencies, leading to performance degradation. Furthermore, this weakness is magnified when the concerned graph is characterized by heterophily (low homophily). To solve this issue, this paper proposes a novel graph learning framework called the graph convolutional network with self-attention (GCN-SA). The proposed scheme exhibits an exceptional generalization capability in node-level representation learning. The proposed GCN-SA contains two enhancements corresponding to edges and node features. For edges, we utilize a self-attention mechanism to design a stable and effective graph-structure-learning module that can capture the internal correlation between any pair of nodes. This graph-structure-learning module can identify reliable neighbors for each node from the entire graph. Regarding the node features, we modi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#25345;&#32493;&#23398;&#20064;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#25345;&#32493;&#23398;&#20064;&#35821;&#20041;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.03462</link><description>&lt;p&gt;
&#29992;&#20110;&#38271;&#26399;&#20010;&#24615;&#21270;&#23478;&#24237;&#26381;&#21153;&#26426;&#22120;&#20154;&#30340;&#20132;&#20114;&#24335;&#25345;&#32493;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Interactive Continual Learning Architecture for Long-Term Personalization of Home Service Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03462
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#25345;&#32493;&#23398;&#20064;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#25345;&#32493;&#23398;&#20064;&#35821;&#20041;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#38750;&#32467;&#26500;&#21270;&#23478;&#24237;&#29615;&#22659;&#20013;&#25191;&#34892;&#36741;&#21161;&#20219;&#21153;&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#23398;&#20064;&#21644;&#25512;&#29702;&#29615;&#22659;&#30340;&#35821;&#20041;&#30693;&#35782;&#12290;&#23613;&#31649;&#35821;&#20041;&#25512;&#29702;&#26550;&#26500;&#30340;&#21457;&#23637;&#20986;&#29616;&#20102;&#22797;&#33487;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20551;&#23450;&#25152;&#26377;&#35757;&#32451;&#25968;&#25454;&#26159;&#39044;&#20808;&#25552;&#20379;&#30340;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#29992;&#25143;&#30340;&#29615;&#22659;&#26159;&#29420;&#29305;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#25345;&#32493;&#21464;&#21270;&#65292;&#36825;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#20010;&#24615;&#21270;&#23478;&#24237;&#26381;&#21153;&#26426;&#22120;&#20154;&#12290;&#23613;&#31649;&#25345;&#32493;&#23398;&#20064;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#21487;&#20197;&#38543;&#26102;&#38388;&#23398;&#20064;&#21644;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22823;&#22810;&#22312;&#38745;&#24577;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#23545;&#35937;&#20998;&#31867;&#29421;&#20041;&#35821;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#26412;&#25991;&#32467;&#21512;&#20102;&#25345;&#32493;&#23398;&#20064;&#12289;&#35821;&#20041;&#25512;&#29702;&#21644;&#20132;&#20114;&#24335;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#24605;&#24819;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#25345;&#32493;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#25345;&#32493;&#23398;&#20064;&#35821;&#20041;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03462v1 Announce Type: cross  Abstract: For robots to perform assistive tasks in unstructured home environments, they must learn and reason on the semantic knowledge of the environments. Despite a resurgence in the development of semantic reasoning architectures, these methods assume that all the training data is available a priori. However, each user's environment is unique and can continue to change over time, which makes these methods unsuitable for personalized home service robots. Although research in continual learning develops methods that can learn and adapt over time, most of these methods are tested in the narrow context of object classification on static image datasets. In this paper, we combine ideas from continual learning, semantic reasoning, and interactive machine learning literature and develop a novel interactive continual learning architecture for continual learning of semantic knowledge in a home environment through human-robot interaction. The architectu
&lt;/p&gt;</description></item><item><title>TGPT-PINN&#36890;&#36807;&#24341;&#20837;&#36716;&#25442;&#23618;&#21644;&#25429;&#33719;&#20914;&#20987;&#25439;&#22833;&#20989;&#25968;&#32452;&#20214;&#65292;&#22312;PINNs&#26694;&#26550;&#20013;&#23454;&#29616;&#20102;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#32553;&#20943;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20855;&#26377;&#21442;&#25968;&#20381;&#36182;&#24615;&#38388;&#26029;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.03459</link><description>&lt;p&gt;
TGPT-PINN&#65306;&#20855;&#26377;&#36716;&#25442;GPT-PINNs&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#32553;&#20943;
&lt;/p&gt;
&lt;p&gt;
TGPT-PINN: Nonlinear model reduction with transformed GPT-PINNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03459
&lt;/p&gt;
&lt;p&gt;
TGPT-PINN&#36890;&#36807;&#24341;&#20837;&#36716;&#25442;&#23618;&#21644;&#25429;&#33719;&#20914;&#20987;&#25439;&#22833;&#20989;&#25968;&#32452;&#20214;&#65292;&#22312;PINNs&#26694;&#26550;&#20013;&#23454;&#29616;&#20102;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#32553;&#20943;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20855;&#26377;&#21442;&#25968;&#20381;&#36182;&#24615;&#38388;&#26029;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#36716;&#25442;&#29983;&#25104;&#39044;&#35757;&#32451;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;TGPT-PINN&#65289;&#65292;&#29992;&#20110;&#22312;MOR&#25972;&#21512;PINNs&#26694;&#26550;&#20013;&#23436;&#25104;&#20256;&#36755;&#20027;&#23548;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#38454;&#20943;&#12290;&#22312;&#26368;&#36817;&#24320;&#21457;&#30340;GPT-PINN&#30340;&#22522;&#30784;&#19978;&#65292;GPT-PINN&#26159;&#19968;&#31181;&#23454;&#29616;&#22522;&#20110;&#24555;&#29031;&#30340;&#27169;&#22411;&#32553;&#20943;&#30340;&#32593;&#32476;&#32467;&#26500;&#35774;&#35745;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#20943;&#33539;&#20363;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#20855;&#26377;&#21442;&#25968;&#20381;&#36182;&#24615;&#38388;&#26029;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#32435;&#20837;&#25429;&#33719;&#20914;&#20987;&#30340;&#25439;&#22833;&#20989;&#25968;&#20998;&#37327;&#20197;&#21450;&#21442;&#25968;&#20381;&#36182;&#30340;&#21464;&#25442;&#23618;&#65292;TGPT-PINN&#20811;&#26381;&#20102;&#20256;&#36755;&#20027;&#23548;&#21306;&#22495;&#20013;&#32447;&#24615;&#27169;&#22411;&#20943;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#38750;&#24179;&#20961;&#21442;&#25968;&#20559;&#24494;&#20998;&#26041;&#31243;&#22312;PINNs&#26694;&#26550;&#20013;&#23637;&#31034;&#20102;&#36825;&#31181;&#26032;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#32553;&#20943;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03459v1 Announce Type: cross  Abstract: We introduce the Transformed Generative Pre-Trained Physics-Informed Neural Networks (TGPT-PINN) for accomplishing nonlinear model order reduction (MOR) of transport-dominated partial differential equations in an MOR-integrating PINNs framework. Building on the recent development of the GPT-PINN that is a network-of-networks design achieving snapshot-based model reduction, we design and test a novel paradigm for nonlinear model reduction that can effectively tackle problems with parameter-dependent discontinuities. Through incorporation of a shock-capturing loss function component as well as a parameter-dependent transform layer, the TGPT-PINN overcomes the limitations of linear model reduction in the transport-dominated regime. We demonstrate this new capability for nonlinear model reduction in the PINNs framework by several nontrivial parametric partial differential equations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#27133;&#25277;&#35937;&#21270;&#22120;&#65292;&#32467;&#21512;&#27133;&#26041;&#27861;&#21644;&#20851;&#31995;&#24402;&#32435;&#20559;&#35265;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.03458</link><description>&lt;p&gt;
&#27133;&#25277;&#35937;&#21270;&#22120;&#65306;&#36808;&#21521;&#21487;&#25193;&#23637;&#30340;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Slot Abstractors: Toward Scalable Abstract Visual Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03458
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#27133;&#25277;&#35937;&#21270;&#22120;&#65292;&#32467;&#21512;&#27133;&#26041;&#27861;&#21644;&#20851;&#31995;&#24402;&#32435;&#20559;&#35265;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#26159;&#19968;&#31181;&#20154;&#31867;&#29305;&#26377;&#30340;&#33021;&#21147;&#65292;&#20801;&#35768;&#35782;&#21035;&#20174;&#23545;&#35937;&#29305;&#24449;&#20013;&#25277;&#35937;&#20986;&#30340;&#20851;&#31995;&#27169;&#24335;&#65292;&#24182;&#23558;&#36825;&#20123;&#27169;&#24335;&#31995;&#32479;&#21270;&#22320;&#25512;&#24191;&#21040;&#26410;&#26366;&#35265;&#36807;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#28041;&#21450;&#22810;&#20010;&#23545;&#35937;&#36755;&#20837;&#30340;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#27133;&#30340;&#26041;&#27861;&#25552;&#21462;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#24182;&#20855;&#26377;&#24378;&#24402;&#32435;&#20559;&#35265;&#30340;&#20851;&#31995;&#25277;&#35937;&#65292;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#31995;&#32479;&#21270;&#25512;&#24191;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20165;&#38480;&#20110;&#21253;&#21547;&#21333;&#19968;&#35268;&#21017;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#19981;&#36866;&#29992;&#20110;&#21253;&#21547;&#22823;&#37327;&#23545;&#35937;&#30340;&#35270;&#35273;&#25512;&#29702;&#38382;&#39064;&#12290;&#20854;&#20182;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#25277;&#35937;&#21270;&#22120;&#65292;&#36825;&#26159;Transformer&#30340;&#25193;&#23637;&#65292;&#34701;&#21512;&#20102;&#24378;&#22823;&#30340;&#20851;&#31995;&#24402;&#32435;&#20559;&#35265;&#65292;&#20174;&#32780;&#32487;&#25215;&#20102;Transformer&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#22810;&#22836;&#26550;&#26500;&#65292;&#20294;&#23578;&#26410;&#23637;&#31034;&#22914;&#20309;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03458v1 Announce Type: cross  Abstract: Abstract visual reasoning is a characteristically human ability, allowing the identification of relational patterns that are abstracted away from object features, and the systematic generalization of those patterns to unseen problems. Recent work has demonstrated strong systematic generalization in visual reasoning tasks involving multi-object inputs, through the integration of slot-based methods used for extracting object-centric representations coupled with strong inductive biases for relational abstraction. However, this approach was limited to problems containing a single rule, and was not scalable to visual reasoning problems containing a large number of objects. Other recent work proposed Abstractors, an extension of Transformers that incorporates strong relational inductive biases, thereby inheriting the Transformer's scalability and multi-head architecture, but it has yet to be demonstrated how this approach might be applied to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#30340;&#23398;&#20064;&#21463;&#38480;&#21046;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30452;&#25509;&#39044;&#27979;&#23545;&#20598;&#35299;&#20272;&#35745;&#65292;&#24182;&#26500;&#24314;&#21407;&#22987;&#20272;&#35745;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20598;&#21487;&#34892;&#35299;&#23545;&#65292;&#21516;&#26102;&#36845;&#20195;&#21521;&#21407;&#22987;&#21487;&#34892;&#24615;&#65292;&#27169;&#25311;&#23545;&#20598;&#19978;&#21319;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03454</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#23398;&#20064;&#21463;&#38480;&#21046;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning Constrained Optimization with Deep Augmented Lagrangian Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#30340;&#23398;&#20064;&#21463;&#38480;&#21046;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30452;&#25509;&#39044;&#27979;&#23545;&#20598;&#35299;&#20272;&#35745;&#65292;&#24182;&#26500;&#24314;&#21407;&#22987;&#20272;&#35745;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20598;&#21487;&#34892;&#35299;&#23545;&#65292;&#21516;&#26102;&#36845;&#20195;&#21521;&#21407;&#22987;&#21487;&#34892;&#24615;&#65292;&#27169;&#25311;&#23545;&#20598;&#19978;&#21319;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20248;&#21270;&#65288;LtO&#65289;&#26159;&#19968;&#20010;&#38382;&#39064;&#35774;&#32622;&#65292;&#22312;&#27492;&#35774;&#32622;&#20013;&#65292;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#34987;&#35757;&#32451;&#25104;&#27169;&#25311;&#19968;&#20010;&#21463;&#38480;&#21046;&#20248;&#21270;&#27714;&#35299;&#22120;&#12290;&#23398;&#20064;&#20135;&#29983;&#26368;&#20248;&#21644;&#31526;&#21512;&#22797;&#26434;&#32422;&#26463;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#20294;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#23558;&#36755;&#20837;&#31354;&#38388;&#38480;&#21046;&#20026;&#19968;&#32452;&#30456;&#20851;&#38382;&#39064;&#30340;&#20998;&#24067;&#26469;&#23454;&#29616;&#12290;&#22823;&#22810;&#25968;LtO&#26041;&#27861;&#20391;&#37325;&#20110;&#30452;&#25509;&#23398;&#20064;&#21407;&#22987;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24212;&#29992;&#26657;&#27491;&#26041;&#26696;&#25110;&#25439;&#22833;&#20989;&#25968;&#24809;&#32602;&#26469;&#40723;&#21169;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21363;&#35757;&#32451;ML&#27169;&#22411;&#30452;&#25509;&#39044;&#27979;&#23545;&#20598;&#35299;&#20272;&#35745;&#65292;&#20174;&#32780;&#26500;&#24314;&#21407;&#22987;&#20272;&#35745;&#20197;&#24418;&#25104;&#23545;&#20598;&#21487;&#34892;&#35299;&#23545;&#12290;&#36825;&#20351;&#24471;&#33021;&#22815;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#26696;&#65292;&#22312;&#36825;&#31181;&#26041;&#26696;&#20013;&#23545;&#20598;&#30446;&#26631;&#34987;&#26368;&#22823;&#21270;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#35299;&#20915;&#26041;&#26696;&#20272;&#35745;&#21521;&#21407;&#22987;&#21487;&#34892;&#24615;&#36845;&#20195;&#65292;&#27169;&#25311;&#23545;&#20598;&#19978;&#21319;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03454v1 Announce Type: new  Abstract: Learning to Optimize (LtO) is a problem setting in which a machine learning (ML) model is trained to emulate a constrained optimization solver. Learning to produce optimal and feasible solutions subject to complex constraints is a difficult task, but is often made possible by restricting the input space to a limited distribution of related problems. Most LtO methods focus on directly learning solutions to the primal problem, and applying correction schemes or loss function penalties to encourage feasibility. This paper proposes an alternative approach, in which the ML model is trained instead to predict dual solution estimates directly, from which primal estimates are constructed to form dual-feasible solution pairs. This enables an end-to-end training scheme is which the dual objective is maximized as a loss function, and solution estimates iterate toward primal feasibility, emulating a Dual Ascent method. First it is shown that the poo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38656;&#35201;&#30740;&#31350;&#65292;&#24314;&#31435;&#20102;&#23545;&#26174;&#33879;&#26102;&#38388;&#27493;&#39588;&#30340;&#23450;&#20041;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#21160;&#24577;&#35268;&#21010;&#30340;&#26032;&#26041;&#27861;&#26469;&#20419;&#36827;&#29992;&#25143;&#39537;&#21160;&#30340;&#26102;&#38388;&#36873;&#25321;&#65292;&#32467;&#21512;&#32467;&#26500;&#29305;&#24449;&#21644;&#32479;&#35745;&#21464;&#21270;&#20197;&#23454;&#29616;&#26356;&#28789;&#27963;&#30340;&#36873;&#25321;</title><link>https://arxiv.org/abs/2403.03449</link><description>&lt;p&gt;
SalienTime: &#29992;&#25143;&#39537;&#21160;&#30340;&#22823;&#35268;&#27169;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#21487;&#35270;&#21270;&#20013;&#26174;&#33879;&#26102;&#38388;&#27493;&#38271;&#30340;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
SalienTime: User-driven Selection of Salient Time Steps for Large-Scale Geospatial Data Visualization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03449
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38656;&#35201;&#30740;&#31350;&#65292;&#24314;&#31435;&#20102;&#23545;&#26174;&#33879;&#26102;&#38388;&#27493;&#39588;&#30340;&#23450;&#20041;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#21160;&#24577;&#35268;&#21010;&#30340;&#26032;&#26041;&#27861;&#26469;&#20419;&#36827;&#29992;&#25143;&#39537;&#21160;&#30340;&#26102;&#38388;&#36873;&#25321;&#65292;&#32467;&#21512;&#32467;&#26500;&#29305;&#24449;&#21644;&#32479;&#35745;&#21464;&#21270;&#20197;&#23454;&#29616;&#26356;&#28789;&#27963;&#30340;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29702;&#31354;&#38388;&#20020;&#26102;&#25968;&#25454;&#30340;&#24222;&#22823;&#24615;&#26469;&#33258;&#29289;&#29702;&#30417;&#35270;&#22120;&#21644;&#20223;&#30495;&#27169;&#22411;&#65292;&#32473;&#26377;&#25928;&#25968;&#25454;&#35775;&#38382;&#24102;&#26469;&#25361;&#25112;&#65292;&#36890;&#24120;&#23548;&#33268;&#32593;&#32476;&#25968;&#25454;&#38376;&#25143;&#20013;&#32321;&#29712;&#30340;&#26102;&#38388;&#36873;&#25321;&#20307;&#39564;&#12290;&#22240;&#27492;&#65292;&#36873;&#25321;&#19968;&#20010;&#26102;&#38388;&#27493;&#39588;&#23376;&#38598;&#20197;&#36827;&#34892;&#20248;&#20808;&#21487;&#35270;&#21270;&#21644;&#39044;&#21152;&#36733;&#26159;&#38750;&#24120;&#21487;&#21462;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#19982;&#39046;&#22495;&#19987;&#23478;&#36827;&#34892;&#24191;&#27867;&#30340;&#38656;&#27714;&#30740;&#31350;&#65292;&#24314;&#31435;&#20102;&#23545;&#26174;&#33879;&#26102;&#38388;&#27493;&#39588;&#30340;&#22810;&#26041;&#38754;&#23450;&#20041;&#65292;&#20197;&#20102;&#35299;&#20182;&#20204;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#21160;&#24577;&#35268;&#21010;&#26469;&#20419;&#36827;&#29992;&#25143;&#39537;&#21160;&#26102;&#38388;&#36873;&#25321;&#30340;&#26032;&#26041;&#27861;&#12290;&#32467;&#26500;&#29305;&#24449;&#65292;&#32479;&#35745;&#21464;&#21270;&#21644;&#36317;&#31163;&#24809;&#32602;&#34987;&#32435;&#20837;&#20197;&#36827;&#34892;&#26356;&#28789;&#27963;&#30340;&#36873;&#25321;&#12290;&#29992;&#25143;&#25351;&#23450;&#30340;&#20248;&#20808;&#32423;&#65292;&#31354;&#38388;&#21306;&#22495;&#21644;&#32858;&#21512;&#34987;&#29992;&#26469;&#32467;&#21512;&#19981;&#21516;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#23454;&#26045;&#20102;&#19968;&#20010;&#22522;&#20110;&#32593;&#32476;&#30340;&#25509;&#21475;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03449v1 Announce Type: cross  Abstract: The voluminous nature of geospatial temporal data from physical monitors and simulation models poses challenges to efficient data access, often resulting in cumbersome temporal selection experiences in web-based data portals. Thus, selecting a subset of time steps for prioritized visualization and pre-loading is highly desirable. Addressing this issue, this paper establishes a multifaceted definition of salient time steps via extensive need-finding studies with domain experts to understand their workflows. Building on this, we propose a novel approach that leverages autoencoders and dynamic programming to facilitate user-driven temporal selections. Structural features, statistical variations, and distance penalties are incorporated to make more flexible selections. User-specified priorities, spatial regions, and aggregations are used to combine different perspectives. We design and implement a web-based interface to enable efficient an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31995;&#32479;&#22320;&#25972;&#21512;&#26680;&#30456;&#20851;&#24615;&#21644;&#19981;&#30456;&#20284;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20026;&#22810;&#26680;k&#22343;&#20540;&#32858;&#31867;&#25552;&#20379;&#20102;&#26356;&#26377;&#25928;&#30340;&#20449;&#24687;&#25552;&#21462;&#21644;&#25913;&#21892;&#32858;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03448</link><description>&lt;p&gt;
&#22810;&#26680;k&#22343;&#20540;&#32858;&#31867;&#30340;&#26680;&#30456;&#20851;-&#19981;&#30456;&#20284;&#24230;
&lt;/p&gt;
&lt;p&gt;
Kernel Correlation-Dissimilarity for Multiple Kernel k-Means Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03448
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31995;&#32479;&#22320;&#25972;&#21512;&#26680;&#30456;&#20851;&#24615;&#21644;&#19981;&#30456;&#20284;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20026;&#22810;&#26680;k&#22343;&#20540;&#32858;&#31867;&#25552;&#20379;&#20102;&#26356;&#26377;&#25928;&#30340;&#20449;&#24687;&#25552;&#21462;&#21644;&#25913;&#21892;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26680;k&#22343;&#20540;&#65288;MKKM&#65289;&#31639;&#27861;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#20248;&#21270;&#22522;&#30784;&#26680;&#30697;&#38453;&#26469;&#25552;&#21462;&#38750;&#32447;&#24615;&#20449;&#24687;&#24182;&#23454;&#29616;&#26368;&#20339;&#32858;&#31867;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#26680;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#25110;&#19981;&#30456;&#20284;&#24615;&#26469;&#22686;&#24378;&#20449;&#24687;&#22810;&#26679;&#24615;&#24182;&#20943;&#23569;&#20887;&#20313;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#36182;&#20110;&#21333;&#19968;&#24230;&#37327;&#65288;&#22914;&#30456;&#20851;&#24615;&#25110;&#19981;&#30456;&#20284;&#24615;&#65289;&#26469;&#23450;&#20041;&#26680;&#20851;&#31995;&#20250;&#24341;&#20837;&#20559;&#35265;&#21644;&#19981;&#23436;&#25972;&#30340;&#34920;&#24449;&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#38480;&#21046;&#20250;&#22952;&#30861;&#26377;&#25928;&#20449;&#24687;&#25552;&#21462;&#65292;&#26368;&#32456;&#24433;&#21709;&#32858;&#31867;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31995;&#32479;&#22320;&#25972;&#21512;&#20102;&#26680;&#30456;&#20851;&#24615;&#21644;&#19981;&#30456;&#20284;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20840;&#38754;&#25429;&#25417;&#20102;&#26680;&#20851;&#31995;&#65292;&#20419;&#36827;&#20102;&#26356;&#26377;&#25928;&#30340;&#20998;&#31867;&#20449;&#24687;&#25552;&#21462;&#65292;&#25552;&#39640;&#20102;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03448v1 Announce Type: new  Abstract: The main objective of the Multiple Kernel k-Means (MKKM) algorithm is to extract non-linear information and achieve optimal clustering by optimizing base kernel matrices. Current methods enhance information diversity and reduce redundancy by exploiting interdependencies among multiple kernels based on correlations or dissimilarities. Nevertheless, relying solely on a single metric, such as correlation or dissimilarity, to define kernel relationships introduces bias and incomplete characterization. Consequently, this limitation hinders efficient information extraction, ultimately compromising clustering performance. To tackle this challenge, we introduce a novel method that systematically integrates both kernel correlation and dissimilarity. Our approach comprehensively captures kernel relationships, facilitating more efficient classification information extraction and improving clustering performance. By emphasizing the coherence between
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#38598;&#21512;&#21345;&#23572;&#26364;&#21453;&#28436;&#26041;&#27861;&#38024;&#23545;DeepONets&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25512;&#26029;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03444</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#21512;&#21345;&#23572;&#26364;&#21453;&#28436;&#36827;&#34892;DeepONets&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification for deeponets with ensemble kalman inversion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03444
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#38598;&#21512;&#21345;&#23572;&#26364;&#21453;&#28436;&#26041;&#27861;&#38024;&#23545;DeepONets&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25805;&#20316;&#21592;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;DeepONet&#65292;&#22240;&#20854;&#39640;&#25928;&#22320;&#23398;&#20064;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#20989;&#25968;&#20043;&#38388;&#30340;&#22797;&#26434;&#26144;&#23556;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#26377;&#38480;&#19988;&#24102;&#22122;&#22768;&#25968;&#25454;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#35775;&#38382;DeepONet&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#21629;&#20851;&#38190;&#25110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#35745;&#31639;&#23494;&#38598;&#65292;&#35201;&#20040;&#20135;&#29983;&#20196;&#20154;&#19981;&#28385;&#24847;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20026;DeepONets&#37327;&#36523;&#23450;&#21046;&#39640;&#25928;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#25216;&#26415;&#30041;&#19979;&#20102;&#31354;&#38388;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38598;&#21512;&#21345;&#23572;&#26364;&#21453;&#28436;&#65288;EKI&#65289;&#26041;&#27861;&#30340;&#26032;&#22411;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#25805;&#20316;&#21592;&#23398;&#20064;&#30340;&#39640;&#25928;UQ&#12290;EKI&#20197;&#20854;&#26080;&#23548;&#25968;&#12289;&#22122;&#22768;&#25239;&#24178;&#25200;&#21644;&#39640;&#24230;&#21487;&#24182;&#34892;&#21270;&#30340;&#29305;&#24615;&#32780;&#38395;&#21517;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#22312;&#38754;&#21521;&#29289;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;UQ&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03444v1 Announce Type: cross  Abstract: In recent years, operator learning, particularly the DeepONet, has received much attention for efficiently learning complex mappings between input and output functions across diverse fields. However, in practical scenarios with limited and noisy data, accessing the uncertainty in DeepONet predictions becomes essential, especially in mission-critical or safety-critical applications. Existing methods, either computationally intensive or yielding unsatisfactory uncertainty quantification, leave room for developing efficient and informative uncertainty quantification (UQ) techniques tailored for DeepONets. In this work, we proposed a novel inference approach for efficient UQ for operator learning by harnessing the power of the Ensemble Kalman Inversion (EKI) approach. EKI, known for its derivative-free, noise-robust, and highly parallelizable feature, has demonstrated its advantages for UQ for physics-informed neural networks [28]. Our inn
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#19982;&#20256;&#32479;ABM&#30456;&#32467;&#21512;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#22522;&#20110;Agent&#30340;&#27169;&#22411;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#27169;&#25311;&#26032;&#35199;&#20848;2019&#24180;&#40635;&#30137;&#29190;&#21457;&#65292;&#28145;&#20837;&#27934;&#23519;&#20256;&#26579;&#30149;&#29190;&#21457;&#30340;&#21160;&#24577;&#12290;</title><link>https://arxiv.org/abs/2403.03434</link><description>&lt;p&gt;
&#19968;&#31181;AI&#21551;&#29992;&#30340;&#22522;&#20110;Agent&#30340;&#27169;&#22411;&#21450;&#20854;&#22312;&#26032;&#35199;&#20848;&#40635;&#30137;&#29190;&#21457;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
An AI-enabled Agent-Based Model and Its Application in Measles Outbreak Simulation for New Zealand
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03434
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#19982;&#20256;&#32479;ABM&#30456;&#32467;&#21512;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#22522;&#20110;Agent&#30340;&#27169;&#22411;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#27169;&#25311;&#26032;&#35199;&#20848;2019&#24180;&#40635;&#30137;&#29190;&#21457;&#65292;&#28145;&#20837;&#27934;&#23519;&#20256;&#26579;&#30149;&#29190;&#21457;&#30340;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#29702;&#27169;&#22411;&#65288;ABMs&#65289;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#22797;&#26434;&#31038;&#20250;&#20114;&#21160;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#22312;&#20844;&#20849;&#21355;&#29983;&#21644;&#20256;&#26579;&#30149;&#35843;&#26597;&#30340;&#32972;&#26223;&#19979;&#12290;&#20026;&#20102;&#22686;&#24378;&#20256;&#32479;ABM&#65292;&#23454;&#29616;&#33258;&#21160;&#21270;&#27169;&#22411;&#26657;&#20934;&#24182;&#20943;&#23569;&#25193;&#23637;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#36807;&#32806;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#26469;&#36827;&#34892;&#24352;&#37327;&#21270;&#21644;&#21487;&#24494;&#20998;&#30340;&#22522;&#20110;Agent&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#34987;&#29992;&#20110;&#30740;&#31350;2019&#24180;&#21457;&#29983;&#22312;&#26032;&#35199;&#20848;&#30340;&#40635;&#30137;&#29190;&#21457;&#65292;&#23637;&#31034;&#20102;&#22312;&#39640;&#23792;&#26399;&#37325;&#22797;&#30149;&#20363;&#20013;&#20934;&#30830;&#27169;&#25311;&#29190;&#21457;&#21160;&#24577;&#30340;&#26377;&#24076;&#26395;&#33021;&#21147;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#21644;&#20256;&#32479;ABMs&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#20256;&#26579;&#30149;&#29190;&#21457;&#30340;&#21160;&#24577;&#65292;&#20174;&#32780;&#24110;&#21161;&#25105;&#20204;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03434v1 Announce Type: cross  Abstract: Agent Based Models (ABMs) have emerged as a powerful tool for investigating complex social interactions, particularly in the context of public health and infectious disease investigation. In an effort to enhance the conventional ABM, enabling automated model calibration and reducing the computational resources needed for scaling up the model, we have developed a tensorized and differentiable agent-based model by coupling Graph Neural Network (GNN) and Long Short-Term Memory (LSTM) network. The model was employed to investigate the 2019 measles outbreak occurred in New Zealand, demonstrating a promising ability to accurately simulate the outbreak dynamics, particularly during the peak period of repeated cases. This paper shows that by leveraging the latest Artificial Intelligence (AI) technology and the capabilities of traditional ABMs, we gain deeper insights into the dynamics of infectious disease outbreaks. This, in turn, helps us ma
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#33322;&#22825;&#22120;&#35786;&#26029;&#25216;&#26415;&#65292;&#22312;Kepler&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#21487;&#38752;&#22320;&#26816;&#27979;&#38271;&#36712;&#36947;&#21608;&#26399;&#22806;&#34892;&#26143;&#30340;&#21333;&#27425;&#21464;&#36801;&#65292;&#28508;&#22312;&#22320;&#21457;&#29616;&#20102;&#26032;&#30340;&#34892;&#26143;&#12290;</title><link>https://arxiv.org/abs/2403.03427</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#33322;&#22825;&#22120;&#19978;&#30340;&#35786;&#26029;&#25216;&#26415;&#22312;Kepler&#20013;&#21333;&#27425;&#21464;&#36801;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Single Transit Detection In Kepler With Machine Learning And Onboard Spacecraft Diagnostics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03427
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#33322;&#22825;&#22120;&#35786;&#26029;&#25216;&#26415;&#65292;&#22312;Kepler&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#21487;&#38752;&#22320;&#26816;&#27979;&#38271;&#36712;&#36947;&#21608;&#26399;&#22806;&#34892;&#26143;&#30340;&#21333;&#27425;&#21464;&#36801;&#65292;&#28508;&#22312;&#22320;&#21457;&#29616;&#20102;&#26032;&#30340;&#34892;&#26143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#38271;&#36712;&#36947;&#21608;&#26399;&#30340;&#22806;&#34892;&#26143;&#38656;&#35201;&#21487;&#38752;&#22320;&#26816;&#27979;&#21333;&#20010;&#21464;&#36801;&#65292;&#32780;&#26080;&#38656;&#20851;&#20110;&#31995;&#32479;&#30340;&#20854;&#20182;&#20449;&#24687;&#12290;&#20687;&#30456;&#20301;&#25240;&#21472;&#20809;&#21464;&#26354;&#32447;&#21644;&#24452;&#21521;&#36895;&#24230;&#25968;&#25454;&#30340;&#21608;&#26399;&#22270;&#20998;&#26512;&#36825;&#26679;&#30340;&#25216;&#26415;&#23545;&#20110;&#36712;&#36947;&#21608;&#26399;&#36739;&#30701;&#30340;&#34892;&#26143;&#26356;&#25935;&#24863;&#65292;&#23548;&#33268;&#22312;&#38271;&#21608;&#26399;&#19979;&#30340;&#34892;&#26143;&#21457;&#29616;&#19981;&#36275;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#38598;&#25104;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;Kepler&#19978;&#30340;&#33322;&#22825;&#22120;&#35786;&#26029;&#26469;&#23545;&#20809;&#21464;&#26354;&#32447;&#20013;&#30340;&#21464;&#36801;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#27969;&#31243;&#26469;&#24674;&#22797;&#21333;&#20010;&#21464;&#36801;&#30340;&#20301;&#32622;&#65292;&#20197;&#21450;&#36712;&#36947;&#34892;&#26143;&#30340;&#21608;&#26399;&#65292;&#20445;&#25345;&#36229;&#36807;80%&#30340;&#21464;&#36801;&#24674;&#22797;&#28789;&#25935;&#24230;&#65292;&#36798;&#21040;800&#22825;&#30340;&#36712;&#36947;&#21608;&#26399;&#12290;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#27969;&#31243;&#26377;&#28508;&#21147;&#22312;Kepler&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#39069;&#22806;&#30340;&#34892;&#26143;&#65292;&#24182;&#19988;&#20851;&#38190;&#22320;&#65292;&#22312;&#951;-&#22320;&#29699;&#21306;&#22495;&#20869;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#27492;&#27969;&#31243;&#30340;&#39318;&#20010;&#20505;&#36873;&#23545;&#35937;&#65292;KOI 1271&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03427v1 Announce Type: cross  Abstract: Exoplanet discovery at long orbital periods requires reliably detecting individual transits without additional information about the system. Techniques like phase-folding of light curves and periodogram analysis of radial velocity data are more sensitive to planets with shorter orbital periods, leaving a dearth of planet discoveries at long periods. We present a novel technique using an ensemble of Convolutional Neural Networks incorporating the onboard spacecraft diagnostics of \emph{Kepler} to classify transits within a light curve. We create a pipeline to recover the location of individual transits, and the period of the orbiting planet, which maintains $&gt;80\%$ transit recovery sensitivity out to an 800-day orbital period. Our neural network pipeline has the potential to discover additional planets in the \emph{Kepler} dataset, and crucially, within the $\eta$-Earth regime. We report our first candidate from this pipeline, KOI 1271.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#27169;&#24577;&#24341;&#23548;&#29983;&#25104;/&#20248;&#21270;&#20219;&#21153;&#35299;&#20915;&#20998;&#23376;&#35774;&#35745;&#38382;&#39064;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03425</link><description>&lt;p&gt;
&#22312;3D&#20013;&#22609;&#36896;&#20998;&#23376;&#65306;&#38754;&#21521;&#25991;&#26412;&#30340;&#20998;&#23376;&#20248;&#21270;&#28789;&#27963;&#23376;&#32467;&#26500;&#24863;&#30693;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Sculpting Molecules in 3D: A Flexible Substructure Aware Framework for Text-Oriented Molecular Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03425
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#27169;&#24577;&#24341;&#23548;&#29983;&#25104;/&#20248;&#21270;&#20219;&#21153;&#35299;&#20915;&#20998;&#23376;&#35774;&#35745;&#38382;&#39064;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;AI-Generated Content&#65292;&#19982;&#20174;&#31532;&#19968;&#21407;&#29702;&#35745;&#31639;&#20013;&#24471;&#20986;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#24050;&#32463;&#25104;&#20026;&#25913;&#21464;&#31185;&#23398;&#30740;&#31350;&#26684;&#23616;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#26082;&#21253;&#21547;&#22810;&#27169;&#24577;&#20808;&#39564;&#30693;&#35782;&#21448;&#20855;&#26377;&#20851;&#38190;&#21644;&#22797;&#26434;&#24615;&#30340;&#20998;&#23376;&#33647;&#29289;&#25110;&#26448;&#26009;&#30340;&#25361;&#25112;&#20381;&#28982;&#26159;&#19968;&#39033;&#20851;&#38190;&#32780;&#22797;&#26434;&#30340;&#24037;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#36870;&#35774;&#35745;&#38382;&#39064;&#65292;&#23558;&#20854;&#26500;&#36896;&#20026;&#19968;&#31181;&#22810;&#27169;&#24577;&#23548;&#21521;&#29983;&#25104;/&#20248;&#21270;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#28041;&#21450;&#19968;&#20010;&#38754;&#21521;&#25991;&#26412;-&#32467;&#26500;&#23545;&#40784;&#30340;&#23545;&#31216;&#25193;&#25955;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#20998;&#23376;&#29983;&#25104;/&#20248;&#21270;&#20219;&#21153;&#65292;&#21363;3DToMolo.
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03425v1 Announce Type: new  Abstract: The integration of deep learning, particularly AI-Generated Content, with high-quality data derived from ab initio calculations has emerged as a promising avenue for transforming the landscape of scientific research. However, the challenge of designing molecular drugs or materials that incorporate multi-modality prior knowledge remains a critical and complex undertaking. Specifically, achieving a practical molecular design necessitates not only meeting the diversity requirements but also addressing structural and textural constraints with various symmetries outlined by domain experts. In this article, we present an innovative approach to tackle this inverse design problem by formulating it as a multi-modality guidance generation/optimization task. Our proposed solution involves a textural-structure alignment symmetric diffusion framework for the implementation of molecular generation/optimization tasks, namely 3DToMolo. 3DToMolo aims to 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LEAD&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29305;&#24449;&#20998;&#35299;&#20026;&#28304;&#24050;&#30693;&#21644;&#26410;&#30693;&#32452;&#20214;&#26469;&#35782;&#21035;&#30446;&#26631;&#31169;&#26377;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.03421</link><description>&lt;p&gt;
LEAD&#65306;&#23398;&#20064;&#20998;&#35299;&#29992;&#20110;&#26080;&#28304;&#36890;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
LEAD: Learning Decomposition for Source-free Universal Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03421
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LEAD&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29305;&#24449;&#20998;&#35299;&#20026;&#28304;&#24050;&#30693;&#21644;&#26410;&#30693;&#32452;&#20214;&#26469;&#35782;&#21035;&#30446;&#26631;&#31169;&#26377;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UniDA&#65289;&#26088;&#22312;&#23454;&#29616;&#22312;&#23384;&#22312;&#21327;&#21464;&#37327;&#21644;&#26631;&#31614;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#12290; &#26368;&#36817;&#65292;&#26080;&#28304;&#36890;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;SF-UniDA&#65289;&#24050;&#32463;&#20986;&#29616;&#65292;&#26088;&#22312;&#23454;&#29616;UniDA&#32780;&#26080;&#38656;&#35775;&#38382;&#28304;&#25968;&#25454;&#65292;&#36825;&#26356;&#23454;&#29992;&#30001;&#20110;&#25968;&#25454;&#20445;&#25252;&#25919;&#31574;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;LEArning Decomposition&#65288;LEAD&#65289;&#30340;&#24819;&#27861;&#65292;&#36890;&#36807;&#23558;&#29305;&#24449;&#20998;&#35299;&#20026;&#28304;&#24050;&#30693;&#21644;&#26410;&#30693;&#32452;&#20214;&#26469;&#35782;&#21035;&#30446;&#26631;&#31169;&#26377;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03421v1 Announce Type: cross  Abstract: Universal Domain Adaptation (UniDA) targets knowledge transfer in the presence of both covariate and label shifts. Recently, Source-free Universal Domain Adaptation (SF-UniDA) has emerged to achieve UniDA without access to source data, which tends to be more practical due to data protection policies. The main challenge lies in determining whether covariate-shifted samples belong to target-private unknown categories. Existing methods tackle this either through hand-crafted thresholding or by developing time-consuming iterative clustering strategies. In this paper, we propose a new idea of LEArning Decomposition (LEAD), which decouples features into source-known and -unknown components to identify target-private data. Technically, LEAD initially leverages the orthogonal decomposition analysis for feature decomposition. Then, LEAD builds instance-level decision boundaries to adaptively identify target-private data. Extensive experiments a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#21644;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#24212;&#29992;&#65292;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#26202;&#24180;&#24515;&#29702;&#20581;&#24247;&#20013;&#30340;&#31995;&#32479;&#32423;&#21160;&#24577;&#12290;</title><link>https://arxiv.org/abs/2403.03414</link><description>&lt;p&gt;
&#21033;&#29992;&#26377;&#38480;&#24773;&#32490;&#22788;&#29702;&#29366;&#24577;&#30740;&#31350;&#26202;&#24180;&#24515;&#29702;&#20581;&#24247;
&lt;/p&gt;
&lt;p&gt;
Leveraging The Finite States of Emotion Processing to Study Late-Life Mental Health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03414
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#21644;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#24212;&#29992;&#65292;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#26202;&#24180;&#24515;&#29702;&#20581;&#24247;&#20013;&#30340;&#31995;&#32479;&#32423;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24515;&#29702;&#20581;&#24247;&#30740;&#31350;&#26041;&#27861;&#37319;&#29992;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;(GLM)&#25551;&#36848;&#35266;&#23519;&#21040;&#30340;&#24515;&#29702;&#34892;&#20026;&#27979;&#37327;&#25968;&#25454;(&#38382;&#21367;&#25688;&#35201;&#20998;&#25968;)&#30340;&#32437;&#21521;&#21160;&#24577;&#12290;&#31867;&#20284;&#22320;&#65292;GLM&#20063;&#34987;&#24212;&#29992;&#20110;&#34920;&#24449;&#31070;&#32463;&#29983;&#29289;&#27979;&#37327;&#25968;&#25454;(&#21306;&#22495;fMRI&#20449;&#21495;)&#19982;&#24863;&#30693;&#21050;&#28608;&#25110;&#20854;&#20182;&#21306;&#22495;&#20449;&#21495;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#25506;&#32034;&#37027;&#20123;&#26500;&#36896;&#30340;&#29420;&#31435;&#20449;&#21495;&#20043;&#38388;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;(&#21363;&#65292;&#24635;&#20998;&#25110;fMRI&#20449;&#21495;)&#26159;&#26377;&#29992;&#30340;&#65292;&#20294;&#36825;&#20123;&#32463;&#20856;&#26694;&#26550;&#22312;&#25552;&#20379;&#20851;&#20110;&#21487;&#35266;&#23519;&#21464;&#21270;&#32972;&#21518;&#30340;&#31995;&#32479;&#32423;&#21160;&#24577;&#30340;&#27934;&#23519;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;(HMM)&#26159;&#19968;&#31181;&#32479;&#35745;&#27169;&#22411;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25551;&#36848;&#22810;&#20010;&#21487;&#35266;&#27979;&#26500;&#36896;&#20043;&#38388;&#30340;&#39034;&#24207;&#20851;&#31995;&#65292;&#24182;&#19988;&#24403;&#36890;&#36807;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;(FSA)&#30340;&#35270;&#35282;&#24212;&#29992;&#26102;&#65292;&#21487;&#20197;&#25552;&#20379;&#19968;&#20010;&#26356;&#32508;&#21512;&#21644;&#30452;&#35266;&#30340;&#26694;&#26550;&#26469;&#24314;&#27169;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03414v1 Announce Type: new  Abstract: Traditional approaches in mental health research apply General Linear Models (GLM) to describe the longitudinal dynamics of observed psycho-behavioral measurements (questionnaire summary scores). Similarly, GLMs are also applied to characterize relationships between neurobiological measurements (regional fMRI signals) and perceptual stimuli or other regional signals. While these methods are useful for exploring linear correlations among the isolated signals of those constructs (i.e., summary scores or fMRI signals), these classical frameworks fall short in providing insights into the comprehensive system-level dynamics underlying observable changes. Hidden Markov Models (HMM) are a statistical model that enable us to describe the sequential relations among multiple observable constructs, and when applied through the lens of Finite State Automata (FSA), can provide a more integrated and intuitive framework for modeling and understanding t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;OOD-R&#25968;&#25454;&#38598;&#21512;&#21644;&#22122;&#22768;&#36807;&#28388;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#22686;&#24378;&#24322;&#24120;&#26679;&#26412;&#30340;&#26816;&#27979;&#21644;&#31649;&#29702;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03412</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#20928;&#21270;&#21644;&#21160;&#24577;&#28608;&#27963;&#20989;&#25968;&#35774;&#35745;&#25512;&#21160;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#30340;&#36827;&#27493;
&lt;/p&gt;
&lt;p&gt;
Advancing Out-of-Distribution Detection through Data Purification and Dynamic Activation Function Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03412
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;OOD-R&#25968;&#25454;&#38598;&#21512;&#21644;&#22122;&#22768;&#36807;&#28388;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#22686;&#24378;&#24322;&#24120;&#26679;&#26412;&#30340;&#26816;&#27979;&#21644;&#31649;&#29702;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#21160;&#24577;&#39046;&#22495;&#20013;&#65292;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#20851;&#38190;&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#26159;&#31649;&#29702;&#24322;&#24120;&#26679;&#26412;&#65292;&#36825;&#22823;&#22823;&#22686;&#21152;&#20102;&#27169;&#22411;&#35823;&#20998;&#31867;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24322;&#24120;&#26679;&#26412;&#30340;&#26816;&#27979;&#21644;&#31649;&#29702;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;OOD-R&#65288;&#24322;&#24120;&#26679;&#26412;&#30699;&#27491;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31579;&#36873;&#30340;&#20855;&#26377;&#22686;&#24378;&#22122;&#22768;&#20943;&#23569;&#24615;&#33021;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#21512;&#12290;&#29616;&#26377;OOD&#25968;&#25454;&#38598;&#20013;&#30340;&#20869;&#37096;&#20998;&#24067;&#65288;ID&#65289;&#22122;&#22768;&#21487;&#33021;&#23548;&#33268;&#26816;&#27979;&#31639;&#27861;&#30340;&#20934;&#30830;&#35780;&#20272;&#12290;&#22522;&#20110;&#36825;&#19968;&#35748;&#35782;&#65292;OOD-R&#37319;&#29992;&#22122;&#22768;&#36807;&#28388;&#25216;&#26415;&#26469;&#25913;&#36827;&#25968;&#25454;&#38598;&#65292;&#30830;&#20445;&#23545;OOD&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#26356;&#20934;&#30830;&#21644;&#21487;&#38752;&#30340;&#35780;&#20272;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#25972;&#20307;&#36136;&#37327;&#65292;&#36824;&#26377;&#21161;&#20110;&#25552;&#39640;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03412v1 Announce Type: new  Abstract: In the dynamic realms of machine learning and deep learning, the robustness and reliability of models are paramount, especially in critical real-world applications. A fundamental challenge in this sphere is managing Out-of-Distribution (OOD) samples, significantly increasing the risks of model misclassification and uncertainty. Our work addresses this challenge by enhancing the detection and management of OOD samples in neural networks. We introduce OOD-R (Out-of-Distribution-Rectified), a meticulously curated collection of open-source datasets with enhanced noise reduction properties. In-Distribution (ID) noise in existing OOD datasets can lead to inaccurate evaluation of detection algorithms. Recognizing this, OOD-R incorporates noise filtering technologies to refine the datasets, ensuring a more accurate and reliable evaluation of OOD detection algorithms. This approach not only improves the overall quality of data but also aids in be
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;LSTM&#12289;SVM&#21644;&#22810;&#39033;&#24335;&#22238;&#24402;&#31639;&#27861;&#23545;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#36827;&#34892;&#39044;&#27979;&#65292;&#36890;&#36807;&#24615;&#33021;&#27604;&#36739;&#26469;&#30830;&#23450;&#26368;&#20339;&#31639;&#27861;&#27169;&#22411;</title><link>https://arxiv.org/abs/2403.03410</link><description>&lt;p&gt;
&#20351;&#29992;LSTM&#12289;SVM&#21644;&#22810;&#39033;&#24335;&#22238;&#24402;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;
&lt;/p&gt;
&lt;p&gt;
Prediction Of Cryptocurrency Prices Using LSTM, SVM And Polynomial Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03410
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;LSTM&#12289;SVM&#21644;&#22810;&#39033;&#24335;&#22238;&#24402;&#31639;&#27861;&#23545;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#36827;&#34892;&#39044;&#27979;&#65292;&#36890;&#36807;&#24615;&#33021;&#27604;&#36739;&#26469;&#30830;&#23450;&#26368;&#20339;&#31639;&#27861;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03410v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#20449;&#24687;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#20114;&#32852;&#32593;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#31616;&#20415;&#30340;&#33719;&#21462;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#38543;&#30528;&#20114;&#32852;&#32593;&#26381;&#21153;&#24102;&#26469;&#30340;&#20415;&#21033;&#65292;&#35768;&#22810;&#26368;&#21021;&#25237;&#36164;&#20110;&#40644;&#37329;&#21644;&#36149;&#37329;&#23646;&#30340;&#20010;&#20154;&#29616;&#22312;&#27491;&#22312;&#36716;&#21521;&#25968;&#23383;&#36135;&#24065;&#30340;&#25968;&#23383;&#25237;&#36164;&#12290;&#28982;&#32780;&#65292;&#25237;&#36164;&#21152;&#23494;&#36135;&#24065;&#20805;&#28385;&#20102;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#27599;&#22825;&#37117;&#23384;&#22312;&#27874;&#21160;&#12290;&#36825;&#31181;&#39118;&#38505;&#32473;&#25317;&#26377;&#30828;&#24065;&#30340;&#25237;&#36164;&#32773;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#21487;&#33021;&#23548;&#33268;&#24040;&#39069;&#25237;&#36164;&#25439;&#22833;&#12290;&#36825;&#20123;&#21152;&#23494;&#36135;&#24065;&#30340;&#20215;&#20540;&#19981;&#30830;&#23450;&#24615;&#26159;&#30828;&#24065;&#25237;&#36164;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#39044;&#27979;&#26159;&#29992;&#20110;&#39044;&#27979;&#36825;&#20123;&#21152;&#23494;&#36135;&#24065;&#26410;&#26469;&#20215;&#20540;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#36890;&#36807;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#27169;&#22411;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#22810;&#39033;&#24335;&#22238;&#24402;&#31639;&#27861;&#36827;&#34892;&#39044;&#27979;&#65292;&#36827;&#34892;&#32489;&#25928;&#27604;&#36739;&#20197;&#30830;&#23450;&#21738;&#31181;&#31639;&#27861;&#27169;&#22411;&#26368;&#20026;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03410v1 Announce Type: new  Abstract: The rapid development of information technology, especially the Internet, has facilitated users with a quick and easy way to seek information. With these convenience offered by internet services, many individuals who initially invested in gold and precious metals are now shifting into digital investments in form of cryptocurrencies. However, investments in crypto coins are filled with uncertainties and fluctuation in daily basis. This risk posed as significant challenges for coin investors that could result in substantial investment losses. The uncertainty of the value of these crypto coins is a critical issue in the field of coin investment. Forecasting, is one of the methods used to predict the future value of these crypto coins. By utilizing the models of Long Short Term Memory, Support Vector Machine, and Polynomial Regression algorithm for forecasting, a performance comparison is conducted to determine which algorithm model is most 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;EnKF-LSTM&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;LSTM&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#20316;&#29289;&#29983;&#38271;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03406</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20316;&#29289;&#29983;&#38271;&#27169;&#22411;&#30340;EnKF-LSTM&#21516;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An EnKF-LSTM Assimilation Algorithm for Crop Growth Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;EnKF-LSTM&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;LSTM&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#20316;&#29289;&#29983;&#38271;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#21450;&#26102;&#22320;&#39044;&#27979;&#20316;&#29289;&#29983;&#38271;&#23545;&#20110;&#30830;&#20445;&#20316;&#29289;&#20135;&#37327;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#20960;&#31181;&#29992;&#20110;&#39044;&#27979;&#20316;&#29289;&#29983;&#38271;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20316;&#29289;&#27169;&#22411;&#24471;&#21040;&#30340;&#27169;&#25311;&#32467;&#26524;&#19982;&#23454;&#38469;&#32467;&#26524;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#24046;&#24322;&#65292;&#22240;&#27492;&#26412;&#25991;&#25552;&#20986;&#23558;&#27169;&#25311;&#32467;&#26524;&#19982;&#37319;&#38598;&#30340;&#20316;&#29289;&#25968;&#25454;&#32467;&#21512;&#36827;&#34892;&#25968;&#25454;&#21516;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;EnKF-LSTM&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;LSTM&#31070;&#32463;&#32593;&#32476;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#29616;&#26377;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#28040;&#38500;&#20102;&#27979;&#37327;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#21033;&#29992;&#20256;&#24863;&#22120;&#35774;&#22791;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#23545;&#25152;&#25552;&#20986;&#30340;EnKF-LSTM&#26041;&#27861;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#24182;&#19982;&#20854;&#20182;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03406v1 Announce Type: new  Abstract: Accurate and timely prediction of crop growth is of great significance to ensure crop yields and researchers have developed several crop models for the prediction of crop growth. However, there are large difference between the simulation results obtained by the crop models and the actual results, thus in this paper, we proposed to combine the simulation results with the collected crop data for data assimilation so that the accuracy of prediction will be improved. In this paper, an EnKF-LSTM data assimilation method for various crops is proposed by combining ensemble Kalman filter and LSTM neural network, which effectively avoids the overfitting problem of existing data assimilation methods and eliminates the uncertainty of the measured data. The verification of the proposed EnKF-LSTM method and the comparison of the proposed method with other data assimilation methods were performed using datasets collected by sensor equipment deployed o
&lt;/p&gt;</description></item><item><title>BAIT&#26694;&#26550;&#25552;&#20379;&#20102;&#20844;&#24179;&#19988;&#31616;&#21270;&#30340;ITP&#23398;&#20064;&#26041;&#27861;&#27604;&#36739;&#65292;&#21457;&#29616;Structure Aware Transformers&#22312;&#20844;&#24335;&#23884;&#20837;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.03401</link><description>&lt;p&gt;
BAIT: &#20132;&#20114;&#23450;&#29702;&#35777;&#26126;&#23884;&#20837;&#26550;&#26500;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BAIT: Benchmarking (Embedding) Architectures for Interactive Theorem-Proving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03401
&lt;/p&gt;
&lt;p&gt;
BAIT&#26694;&#26550;&#25552;&#20379;&#20102;&#20844;&#24179;&#19988;&#31616;&#21270;&#30340;ITP&#23398;&#20064;&#26041;&#27861;&#27604;&#36739;&#65292;&#21457;&#29616;Structure Aware Transformers&#22312;&#20844;&#24335;&#23884;&#20837;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#23450;&#29702;&#35777;&#26126;&#20652;&#29983;&#20102;&#22823;&#37327;&#22522;&#20934;&#27979;&#35797;&#21644;&#26041;&#27861;&#35770;&#65292;&#23588;&#20854;&#26159;&#22312;&#20132;&#20114;&#24335;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#12290;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#20998;&#25955;&#65292;&#37319;&#29992;&#22810;&#31181;&#26041;&#27861;&#25955;&#24067;&#22312;&#20960;&#20010;&#20132;&#20114;&#24335;&#23450;&#29702;&#35777;&#26126;&#31995;&#32479;&#20013;&#12290;&#36825;&#32473;&#26041;&#27861;&#30340;&#27604;&#36739;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22797;&#26434;&#19988;&#38590;&#20197;&#22797;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BAIT&#65292;&#19968;&#20010;&#29992;&#20110;&#20844;&#24179;&#21644;&#31616;&#21270;&#27604;&#36739;ITP&#23398;&#20064;&#26041;&#27861;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#20840;&#38754;&#27604;&#36739;&#23637;&#31034;&#20102;BAIT&#30340;&#33021;&#21147;&#65292;&#28085;&#30422;&#20102;&#24212;&#29992;&#20110;&#20844;&#24335;&#23884;&#20837;&#38382;&#39064;&#30340;&#26368;&#20808;&#36827;&#26550;&#26500;&#22312;&#22810;&#20010;ITP&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#32467;&#26500;&#24863;&#30693;&#21464;&#21387;&#22120;&#34920;&#29616;&#29305;&#21035;&#20986;&#33394;&#65292;&#25913;&#36827;&#20102;&#19982;&#21407;&#38382;&#39064;&#38598;&#30456;&#20851;&#30340;&#25216;&#26415;&#12290;BAIT&#36824;&#20801;&#35768;&#25105;&#20204;&#35780;&#20272;&#24314;&#31435;&#22312;&#20132;&#20114;&#29615;&#22659;&#20013;&#30340;&#31995;&#32479;&#30340;&#31471;&#21040;&#31471;&#35777;&#26126;&#24615;&#33021;&#12290;&#36825;&#31181;&#32479;&#19968;&#30340;&#35270;&#35282;&#25581;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03401v1 Announce Type: new  Abstract: Artificial Intelligence for Theorem Proving has given rise to a plethora of benchmarks and methodologies, particularly in Interactive Theorem Proving (ITP). Research in the area is fragmented, with a diverse set of approaches being spread across several ITP systems. This presents a significant challenge to the comparison of methods, which are often complex and difficult to replicate. Addressing this, we present BAIT, a framework for fair and streamlined comparison of learning approaches in ITP. We demonstrate BAIT's capabilities with an in-depth comparison, across several ITP benchmarks, of state-of-the-art architectures applied to the problem of formula embedding. We find that Structure Aware Transformers perform particularly well, improving on techniques associated with the original problem sets. BAIT also allows us to assess the end-to-end proving performance of systems built on interactive environments. This unified perspective revea
&lt;/p&gt;</description></item><item><title>CoRMF&#26159;&#19968;&#31181;&#22522;&#20110;RNN&#30340;&#39640;&#25928;&#20234;&#36763;&#27169;&#22411;&#27714;&#35299;&#22120;&#65292;&#21033;&#29992;&#20851;&#38190;&#26377;&#24207;&#33258;&#26059;&#24207;&#21015;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#21464;&#20998;&#22343;&#22330;&#21644; RNN &#20043;&#38388;&#30340;&#32479;&#19968;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#30340;&#20234;&#36763;&#27169;&#22411;&#30340;&#39640;&#25928;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2403.03391</link><description>&lt;p&gt;
CoRMF: &#20020;&#30028;&#26377;&#24207;&#24490;&#29615;&#22343;&#22330;&#20234;&#36763;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
CoRMF: Criticality-Ordered Recurrent Mean Field Ising Solver
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03391
&lt;/p&gt;
&lt;p&gt;
CoRMF&#26159;&#19968;&#31181;&#22522;&#20110;RNN&#30340;&#39640;&#25928;&#20234;&#36763;&#27169;&#22411;&#27714;&#35299;&#22120;&#65292;&#21033;&#29992;&#20851;&#38190;&#26377;&#24207;&#33258;&#26059;&#24207;&#21015;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#21464;&#20998;&#22343;&#22330;&#21644; RNN &#20043;&#38388;&#30340;&#32479;&#19968;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#30340;&#20234;&#36763;&#27169;&#22411;&#30340;&#39640;&#25928;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RNN&#30340;&#39640;&#25928;&#20234;&#36763;&#27169;&#22411;&#27714;&#35299;&#22120;&#65292;&#31216;&#20026;Criticality-ordered Recurrent Mean Field (CoRMF)&#65292;&#29992;&#20110;&#21069;&#21521;&#20234;&#36763;&#38382;&#39064;&#12290;&#22312;&#20854;&#26680;&#24515;&#37096;&#20998;&#65292;&#36890;&#36807;&#36138;&#23146;&#31639;&#27861;&#23545;N&#20010;&#33258;&#26059;&#30340;&#20234;&#36763;&#27169;&#22411;&#36827;&#34892;&#20102;&#20851;&#38190;&#26377;&#24207;&#33258;&#26059;&#24207;&#21015;&#30340;&#24341;&#20837;&#65292;&#20174;&#32780;&#21487;&#20197;&#21033;&#29992;&#33258;&#22238;&#24402;&#22343;&#22330;&#22240;&#23376;&#20998;&#35299;&#65292;&#24182;&#36890;&#36807;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNNs)&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20004;&#20010;&#26174;&#33879;&#29305;&#28857;&#65306;(i)&#36890;&#36807;&#21033;&#29992;&#24213;&#23618;&#20234;&#36763;&#22270;&#30340;&#36817;&#20284;&#26641;&#32467;&#26500;&#65292;&#26032;&#33719;&#24471;&#30340;&#20851;&#38190;&#24615;&#39034;&#24207;&#20351;&#21464;&#20998;&#22343;&#22330;&#21644;RNN&#20043;&#38388;&#24471;&#20197;&#32479;&#19968;&#65292;&#20174;&#32780;&#20801;&#35768;&#26377;&#25928;&#22320;&#21033;&#29992;&#27010;&#29575;&#25512;&#26029;&#26469;&#25506;&#31350;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#30340;&#20234;&#36763;&#27169;&#22411;;(ii)&#23427;&#20855;&#26377;&#33391;&#22909;&#30340;&#27169;&#22359;&#21270;&#12289;&#29420;&#31435;&#20110;&#27169;&#22411;&#32780;&#21448;&#36275;&#22815;&#34920;&#36798;&#33021;&#21147;&#65292;&#22240;&#27492;&#21487;&#20197;&#23436;&#20840;&#36866;&#29992;&#20110;&#20219;&#20309;&#21069;&#21521;&#20234;&#36763;&#25512;&#29702;&#38382;&#39064;&#65292;&#32780;&#19988;&#24037;&#20316;&#37327;&#26497;&#23567;&#12290;&#35745;&#31639;&#19978;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26041;&#24046;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03391v1 Announce Type: cross  Abstract: We propose an RNN-based efficient Ising model solver, the Criticality-ordered Recurrent Mean Field (CoRMF), for forward Ising problems. In its core, a criticality-ordered spin sequence of an $N$-spin Ising model is introduced by sorting mission-critical edges with greedy algorithm, such that an autoregressive mean-field factorization can be utilized and optimized with Recurrent Neural Networks (RNNs). Our method has two notable characteristics: (i) by leveraging the approximated tree structure of the underlying Ising graph, the newly-obtained criticality order enables the unification between variational mean-field and RNN, allowing the generally intractable Ising model to be efficiently probed with probabilistic inference; (ii) it is well-modulized, model-independent while at the same time expressive enough, and hence fully applicable to any forward Ising inference problems with minimal effort. Computationally, by using a variance-redu
&lt;/p&gt;</description></item><item><title>&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#22810;&#31867;&#26434;&#33609;&#26816;&#27979;&#20013;&#23637;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#20026;&#35299;&#20915;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20013;&#38656;&#35201;&#22823;&#37327;&#25163;&#24037;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#32570;&#28857;&#25552;&#20379;&#20102;&#21487;&#25345;&#32493;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.03390</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#22312;&#22810;&#31867;&#26434;&#33609;&#26816;&#27979;&#20013;&#30340;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Performance Evaluation of Semi-supervised Learning Frameworks for Multi-Class Weed Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03390
&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#22810;&#31867;&#26434;&#33609;&#26816;&#27979;&#20013;&#23637;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#20026;&#35299;&#20915;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20013;&#38656;&#35201;&#22823;&#37327;&#25163;&#24037;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#32570;&#28857;&#25552;&#20379;&#20102;&#21487;&#25345;&#32493;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#26434;&#33609;&#25511;&#21046;&#22312;&#20248;&#21270;&#20892;&#20316;&#29289;&#20135;&#37327;&#21644;&#25552;&#39640;&#20892;&#20135;&#21697;&#36136;&#37327;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#38500;&#33609;&#21058;&#30340;&#20381;&#36182;&#19981;&#20165;&#23545;&#29615;&#22659;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#65292;&#36824;&#20250;&#20419;&#20351;&#25239;&#33609;&#33609;&#20986;&#29616;&#12290;&#36817;&#24180;&#26469;&#65292;ML&#21644;DL&#25152;&#24102;&#26469;&#30340;&#31934;&#20934;&#38500;&#33609;&#31649;&#29702;&#30340;&#36827;&#23637;&#20026;&#27492;&#25552;&#20379;&#20102;&#21487;&#25345;&#32493;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#31639;&#27861;&#20027;&#35201;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#24120;&#38656;&#35201;&#20855;&#26377;&#25163;&#24037;&#26631;&#27880;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#24191;&#27867;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#65292;&#23588;&#20854;&#26159;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#23637;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#21033;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#26679;&#26412;&#20197;&#21450;&#22823;&#37327;&#26410;&#26631;&#35760;&#26679;&#26412;&#26469;&#24320;&#21457;&#39640;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03390v1 Announce Type: cross  Abstract: Effective weed control plays a crucial role in optimizing crop yield and enhancing agricultural product quality. However, the reliance on herbicide application not only poses a critical threat to the environment but also promotes the emergence of resistant weeds. Fortunately, recent advances in precision weed management enabled by ML and DL provide a sustainable alternative. Despite great progress, existing algorithms are mainly developed based on supervised learning approaches, which typically demand large-scale datasets with manual-labeled annotations, which is time-consuming and labor-intensive. As such, label-efficient learning methods, especially semi-supervised learning, have gained increased attention in the broader domain of computer vision and have demonstrated promising performance. These methods aim to utilize a small number of labeled data samples along with a great number of unlabeled samples to develop high-performing mod
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#24067;&#23572;&#20989;&#25968;&#20998;&#26512;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#26465;&#20214;&#19979;&#29305;&#24449;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#34394;&#20551;&#30456;&#20851;&#24615;&#25110;&#34394;&#20551;&#29305;&#24449;&#30340;&#24378;&#24230;&#20250;&#24433;&#21709;&#26680;&#24515;&#29305;&#24449;&#23398;&#20064;&#36895;&#24230;&#65292;&#34394;&#20551;&#29305;&#24449;&#21644;&#26680;&#24515;&#29305;&#24449;&#30340;&#23398;&#20064;&#38454;&#27573;&#19981;&#24635;&#26159;&#21487;&#20998;&#24320;&#65292;&#24182;&#19988;&#34394;&#20551;&#29305;&#24449;&#21363;&#20351;&#22312;&#19968;&#27573;&#26102;&#38388;&#21518;&#20063;&#19981;&#20250;&#34987;&#36951;&#24536;&#12290;</title><link>https://arxiv.org/abs/2403.03375</link><description>&lt;p&gt;
&#22797;&#26434;&#24615;&#33267;&#20851;&#37325;&#35201;&#65306;&#22312;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#24773;&#20917;&#19979;&#29305;&#24449;&#23398;&#20064;&#30340;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Complexity Matters: Dynamics of Feature Learning in the Presence of Spurious Correlations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03375
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#24067;&#23572;&#20989;&#25968;&#20998;&#26512;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#26465;&#20214;&#19979;&#29305;&#24449;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#34394;&#20551;&#30456;&#20851;&#24615;&#25110;&#34394;&#20551;&#29305;&#24449;&#30340;&#24378;&#24230;&#20250;&#24433;&#21709;&#26680;&#24515;&#29305;&#24449;&#23398;&#20064;&#36895;&#24230;&#65292;&#34394;&#20551;&#29305;&#24449;&#21644;&#26680;&#24515;&#29305;&#24449;&#30340;&#23398;&#20064;&#38454;&#27573;&#19981;&#24635;&#26159;&#21487;&#20998;&#24320;&#65292;&#24182;&#19988;&#34394;&#20551;&#29305;&#24449;&#21363;&#20351;&#22312;&#19968;&#27573;&#26102;&#38388;&#21518;&#20063;&#19981;&#20250;&#34987;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#32463;&#24120;&#23558;&#34394;&#20551;&#29305;&#24449;&#23450;&#20041;&#20026;&#22312;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#20013;&#8220;&#26356;&#23481;&#26131;&#8221;&#23398;&#20064;&#30340;&#20869;&#23481;&#65292;&#20294;&#23427;&#20204;&#30456;&#23545;&#31616;&#21333;&#24615;&#30340;&#24433;&#21709;&#20173;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#26368;&#32456;&#24615;&#33021;&#65292;&#32780;&#19981;&#26159;&#29305;&#24449;&#23398;&#20064;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#21644;&#30456;&#20851;&#30340;&#22522;&#20110;&#24067;&#23572;&#20989;&#25968;&#20998;&#26512;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20801;&#35768;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#30456;&#23545;&#22797;&#26434;&#24615;&#65288;&#19982;&#26680;&#24515;&#29305;&#24449;&#30456;&#27604;&#65289;&#21644;&#30456;&#20851;&#24615;&#24378;&#24230;&#65288;&#30456;&#23545;&#20110;&#26631;&#31614;&#65289;&#36827;&#34892;&#32454;&#33268;&#25511;&#21046;&#65292;&#20197;&#30740;&#31350;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#19979;&#29305;&#24449;&#23398;&#20064;&#30340;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#35774;&#32622;&#25581;&#31034;&#20102;&#20960;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65306;&#65288;1&#65289;&#26356;&#24378;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#25110;&#26356;&#31616;&#21333;&#30340;&#34394;&#20551;&#29305;&#24449;&#20250;&#20943;&#24930;&#26680;&#24515;&#29305;&#24449;&#30340;&#23398;&#20064;&#36895;&#24230;&#65292;&#65288;2&#65289;&#34394;&#20551;&#29305;&#24449;&#21644;&#26680;&#24515;&#29305;&#24449;&#30340;&#23398;&#20064;&#38454;&#27573;&#24182;&#38750;&#24635;&#26159;&#21487;&#20197;&#34987;&#20998;&#24320;&#65292;&#65288;3&#65289;&#34394;&#20551;&#29305;&#24449;&#21363;&#20351;&#22312;&#19968;&#27573;&#26102;&#38388;&#20043;&#21518;&#20063;&#19981;&#20250;&#34987;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03375v1 Announce Type: new  Abstract: Existing research often posits spurious features as "easier" to learn than core features in neural network optimization, but the impact of their relative simplicity remains under-explored. Moreover they mainly focus on the end performance intead of the learning dynamics of feature learning. In this paper, we propose a theoretical framework and associated synthetic dataset grounded in boolean function analysis which allows for fine-grained control on the relative complexity (compared to core features) and correlation strength (with respect to the label) of spurious features to study the dynamics of feature learning under spurious correlation. Our setup uncovers several interesting phenomenon: (1) stronger spurious correlations or simpler spurious features slow down the rate of learning for the core features, (2) learning phases of spurious features and core features are not always separable, (3) spurious features are not forgotten even af
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;TartanAviation&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;ADS-B&#36712;&#36857;&#25968;&#25454;&#65292;&#23545;&#26426;&#22330;&#29615;&#22659;&#36827;&#34892;&#20102;&#20840;&#38754;&#35266;&#23519;&#65292;&#20026;AI&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;</title><link>https://arxiv.org/abs/2403.03372</link><description>&lt;p&gt;
TartanAviation&#65306;&#29992;&#20110;&#32456;&#31471;&#39046;&#22495;&#31354;&#22495;&#20316;&#19994;&#30340;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;ADS-B&#36712;&#36857;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
TartanAviation: Image, Speech, and ADS-B Trajectory Datasets for Terminal Airspace Operations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03372
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;TartanAviation&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;ADS-B&#36712;&#36857;&#25968;&#25454;&#65292;&#23545;&#26426;&#22330;&#29615;&#22659;&#36827;&#34892;&#20102;&#20840;&#38754;&#35266;&#23519;&#65292;&#20026;AI&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;TartanAviation&#65292;&#19968;&#20010;&#19987;&#27880;&#20110;&#32456;&#31471;&#21306;&#22495;&#31354;&#22495;&#20316;&#19994;&#30340;&#24320;&#28304;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;TartanAviation&#36890;&#36807;&#22312;&#26426;&#22330;&#36793;&#30028;&#20869;&#23433;&#35013;&#35774;&#22791;&#21516;&#26102;&#25910;&#38598;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;ADS-B&#36712;&#36857;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#23545;&#26426;&#22330;&#29615;&#22659;&#30340;&#25972;&#20307;&#35270;&#22270;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;&#22810;&#20010;&#26376;&#20869;&#25910;&#38598;&#20110;&#26377;&#22612;&#27004;&#21644;&#26080;&#22612;&#27004;&#30340;&#26426;&#22330;&#65292;&#20197;&#25429;&#25417;&#39134;&#26426;&#20316;&#19994;&#12289;&#23395;&#33410;&#12289;&#39134;&#26426;&#31867;&#22411;&#21644;&#22825;&#27668;&#26465;&#20214;&#30340;&#22810;&#26679;&#24615;&#12290;&#24635;&#20849;&#65292;TartanAviation&#25552;&#20379;&#20102;310&#19975;&#24352;&#22270;&#20687;&#12289;3374&#23567;&#26102;&#30340;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#35821;&#38899;&#25968;&#25454;&#20197;&#21450;661&#22825;&#30340;ADS-B&#36712;&#36857;&#25968;&#25454;&#12290;&#25968;&#25454;&#32463;&#36807;&#36807;&#28388;&#12289;&#22788;&#29702;&#21644;&#39564;&#35777;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#12290;&#38500;&#20102;&#25968;&#25454;&#38598;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#28304;&#20102;&#29992;&#20110;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#25968;&#25454;&#38598;&#30340;&#20195;&#30721;&#24211;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#21487;&#35775;&#38382;&#24615;&#21644;&#21487;&#29992;&#24615;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20010;&#25968;&#25454;&#38598;&#26377;&#35768;&#22810;&#28508;&#22312;&#30340;&#29992;&#36884;&#65292;&#29305;&#21035;&#26159;&#22312;&#20801;&#35768;AI&#21644;m&#30340;&#24773;&#20917;&#19979;&#20250;&#38750;&#24120;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03372v1 Announce Type: new  Abstract: We introduce TartanAviation, an open-source multi-modal dataset focused on terminal-area airspace operations. TartanAviation provides a holistic view of the airport environment by concurrently collecting image, speech, and ADS-B trajectory data using setups installed inside airport boundaries. The datasets were collected at both towered and non-towered airfields across multiple months to capture diversity in aircraft operations, seasons, aircraft types, and weather conditions. In total, TartanAviation provides 3.1M images, 3374 hours of Air Traffic Control speech data, and 661 days of ADS-B trajectory data. The data was filtered, processed, and validated to create a curated dataset. In addition to the dataset, we also open-source the code-base used to collect and pre-process the dataset, further enhancing accessibility and usability. We believe this dataset has many potential use cases and would be particularly vital in allowing AI and m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#22788;&#29702;&#27695;&#21537;&#26684;&#38647;&#27835;&#30103;&#22833;&#36133;&#26816;&#27979;&#65292;&#32467;&#26524;&#26174;&#31034;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;&#65292;&#24378;&#35843;&#20102;&#20854;&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03368</link><description>&lt;p&gt;
&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#33258;&#21160;&#26816;&#27979;&#27695;&#21537;&#26684;&#38647;&#27835;&#30103;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;
Leveraging Federated Learning for Automatic Detection of Clopidogrel Treatment Failures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#22788;&#29702;&#27695;&#21537;&#26684;&#38647;&#27835;&#30103;&#22833;&#36133;&#26816;&#27979;&#65292;&#32467;&#26524;&#26174;&#31034;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;&#65292;&#24378;&#35843;&#20102;&#20854;&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27695;&#21537;&#26684;&#38647;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#25239;&#34880;&#23567;&#26495;&#33647;&#29289;&#65292;&#20854;&#26377;&#25928;&#24615;&#22312;&#20010;&#20307;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24517;&#39035;&#24320;&#21457;&#31934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#20197;&#20248;&#21270;&#24739;&#32773;&#25252;&#29702;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#26469;&#22788;&#29702;&#27695;&#21537;&#26684;&#38647;&#27835;&#30103;&#22833;&#36133;&#30340;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#22810;&#20010;&#21307;&#30103;&#26426;&#26500;&#30340;&#21327;&#20316;&#21147;&#37327;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#25935;&#24863;&#30340;&#24739;&#32773;&#25968;&#25454;&#12290;&#21033;&#29992;&#28085;&#30422;&#24191;&#27867;&#19988;&#22810;&#26679;&#21270;&#20154;&#21475;&#30340;&#33521;&#22269;&#29983;&#29289;&#38134;&#34892;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22522;&#20110;&#22320;&#29702;&#20013;&#24515;&#23545;&#25968;&#25454;&#36827;&#34892;&#20998;&#21306;&#65292;&#24182;&#35780;&#20272;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#38598;&#20013;&#24335;&#35757;&#32451;&#21487;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#20540;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20294;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#32553;&#23567;&#36825;&#31181;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#24212;&#23545;&#27695;&#21537;&#26684;&#38647;&#27835;&#30103;&#22833;&#36133;&#38382;&#39064;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03368v1 Announce Type: new  Abstract: The effectiveness of clopidogrel, a widely used antiplatelet medication, varies significantly among individuals, necessitating the development of precise predictive models to optimize patient care. In this study, we leverage federated learning strategies to address clopidogrel treatment failure detection. Our research harnesses the collaborative power of multiple healthcare institutions, allowing them to jointly train machine learning models while safeguarding sensitive patient data. Utilizing the UK Biobank dataset, which encompasses a vast and diverse population, we partitioned the data based on geographic centers and evaluated the performance of federated learning. Our results show that while centralized training achieves higher Area Under the Curve (AUC) values and faster convergence, federated learning approaches can substantially narrow this performance gap. Our findings underscore the potential of federated learning in addressing 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#20248;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#27700;&#24179;&#38598;&#20256;&#36755;&#65292;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#27700;&#24179;&#38598;&#20256;&#36755;&#21487;&#20197;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#38656;&#35201;Hessian-vector products&#30340;&#26041;&#27861;&#39564;&#35777;&#20102;&#35813;&#25216;&#26415;&#22312;&#23454;&#36341;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03362</link><description>&lt;p&gt;
&#27700;&#24179;&#38598;&#20256;&#36755;&#65306;&#20248;&#21270;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Level Set Teleportation: An Optimization Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03362
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#20248;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#27700;&#24179;&#38598;&#20256;&#36755;&#65292;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#27700;&#24179;&#38598;&#20256;&#36755;&#21487;&#20197;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#38656;&#35201;Hessian-vector products&#30340;&#26041;&#27861;&#39564;&#35777;&#20102;&#35813;&#25216;&#26415;&#22312;&#23454;&#36341;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#27700;&#24179;&#38598;&#20256;&#36755;&#65292;&#36825;&#26159;&#19968;&#31181;&#20248;&#21270;&#23376;&#31243;&#24207;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#30446;&#26631;&#20989;&#25968;&#30340;&#27700;&#24179;&#38598;&#19978;&#26368;&#22823;&#21270;&#26799;&#24230;&#33539;&#25968;&#26469;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#12290;&#30001;&#20110;&#19979;&#38477;&#24341;&#29702;&#26263;&#31034;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#20351;&#30446;&#26631;&#20989;&#25968;&#25353;&#26799;&#24230;&#30340;&#24179;&#26041;&#33539;&#25968;&#19979;&#38477;&#65292;&#27700;&#24179;&#38598;&#20256;&#36755;&#26368;&#22823;&#21270;&#20102;&#36825;&#19968;&#27493;&#36827;&#20445;&#35777;&#12290;&#23545;&#20110;&#28385;&#36275;Hessian&#31283;&#23450;&#24615;&#30340;&#20984;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;GD&#19982;&#27700;&#24179;&#38598;&#20256;&#36755;&#33719;&#24471;&#20102;&#32508;&#21512;&#30340;&#27425;&#32447;&#24615;/&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#65292;&#36825;&#27604;&#22312;&#26368;&#20248;&#24615;&#24046;&#36317;&#36739;&#23567;&#26102;&#26631;&#20934;GD&#35201;&#24555;&#24471;&#22810;&#12290;&#19982;&#26631;&#20934;&#65288;&#24378;&#65289;&#20984;&#35774;&#32622;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27700;&#24179;&#38598;&#20256;&#36755;&#26082;&#19981;&#25913;&#21892;&#20063;&#19981;&#24694;&#21270;&#25910;&#25947;&#36895;&#24230;&#12290;&#20026;&#20102;&#23454;&#38469;&#35780;&#20272;&#20256;&#36755;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21482;&#38656;&#35201;Hessian-&#21521;&#37327;&#20056;&#31215;&#30340;&#25237;&#24433;&#26799;&#24230;&#31867;&#22411;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#26174;&#31034;&#65292;&#22914;&#26524;&#25552;&#20379;&#20102;&#26799;&#24230;&#35775;&#38382;&#26435;&#38480;&#65292;&#27700;&#24179;&#38598;&#20256;&#36755;&#26799;&#24230;&#26041;&#27861;&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03362v1 Announce Type: new  Abstract: We study level set teleportation, an optimization sub-routine which seeks to accelerate gradient methods by maximizing the gradient norm on a level-set of the objective function. Since the descent lemma implies that gradient descent (GD) decreases the objective proportional to the squared norm of the gradient, level-set teleportation maximizes this one-step progress guarantee. For convex functions satisfying Hessian stability, we prove that GD with level-set teleportation obtains a combined sub-linear/linear convergence rate which is strictly faster than standard GD when the optimality gap is small. This is in sharp contrast to the standard (strongly) convex setting, where we show level-set teleportation neither improves nor worsens convergence rates. To evaluate teleportation in practice, we develop a projected-gradient-type method requiring only Hessian-vector products. We use this method to show that gradient methods with access to a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#36172;&#21338;&#38382;&#39064;&#20013;Thompson-Sampling&#31639;&#27861;&#21464;&#20307;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#65292;&#36890;&#36807;&#20351;&#29992;&#38142;&#25509;&#35770;&#35777;&#24314;&#31435;&#20102;&#20855;&#26377;&#24230;&#37327;&#21160;&#20316;&#31354;&#38388;&#30340;&#26032;&#30028;&#38480;&#65292;&#20026;$d$&#32500;&#32447;&#24615;&#36172;&#21338;&#38382;&#39064;&#25552;&#20379;&#20102;$O(d\sqrt{T})$&#30340;&#20005;&#26684;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.03361</link><description>&lt;p&gt;
&#38142;&#24335;&#20449;&#24687;&#35770;&#30028;&#38480;&#21644;&#32447;&#24615;&#36172;&#21338;&#38382;&#39064;&#30340;&#20005;&#26684;&#36951;&#25022;&#29575;
&lt;/p&gt;
&lt;p&gt;
Chained Information-Theoretic bounds and Tight Regret Rate for Linear Bandit Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#36172;&#21338;&#38382;&#39064;&#20013;Thompson-Sampling&#31639;&#27861;&#21464;&#20307;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#65292;&#36890;&#36807;&#20351;&#29992;&#38142;&#25509;&#35770;&#35777;&#24314;&#31435;&#20102;&#20855;&#26377;&#24230;&#37327;&#21160;&#20316;&#31354;&#38388;&#30340;&#26032;&#30028;&#38480;&#65292;&#20026;$d$&#32500;&#32447;&#24615;&#36172;&#21338;&#38382;&#39064;&#25552;&#20379;&#20102;$O(d\sqrt{T})$&#30340;&#20005;&#26684;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;Thompson-Sampling&#31639;&#27861;&#21464;&#20307;&#22312;&#36172;&#21338;&#38382;&#39064;&#20013;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#12290;&#23427;&#22522;&#20110;[Russo and Van Roy&#65292;2015]&#30340;&#20449;&#24687;&#35770;&#26694;&#26550;&#65292;&#26356;&#20855;&#20307;&#22320;&#65292;&#22522;&#20110;[Dong and Van Roy&#65292;2020]&#30340;&#29575;-&#22833;&#30495;&#20998;&#26512;&#65292;&#22312;&#37027;&#37324;&#20182;&#20204;&#35777;&#26126;&#20102;$d$&#32500;&#32447;&#24615;&#36172;&#21338;&#35774;&#32622;&#30340;&#36951;&#25022;&#29575;&#20026;$O(d\sqrt{T \log(T)})$&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20855;&#26377;&#24230;&#37327;&#21160;&#20316;&#31354;&#38388;&#30340;&#36172;&#21338;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#38142;&#25509;&#35770;&#35777;&#24314;&#31435;&#20102;&#20381;&#36182;&#20110;&#21160;&#20316;&#31354;&#38388;&#24230;&#37327;&#29109;&#30340;&#26032;&#30028;&#38480;&#65292;&#38024;&#23545;Thompson-Sampling&#30340;&#19968;&#20010;&#21464;&#20307;&#12290;&#22312;&#22870;&#21169;&#30340;&#36866;&#24403;&#36830;&#32493;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#20026;$d$&#32500;&#32447;&#24615;&#36172;&#21338;&#38382;&#39064;&#25552;&#20379;&#20102;$O(d\sqrt{T})$&#30340;&#20005;&#26684;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03361v1 Announce Type: cross  Abstract: This paper studies the Bayesian regret of a variant of the Thompson-Sampling algorithm for bandit problems. It builds upon the information-theoretic framework of [Russo and Van Roy, 2015] and, more specifically, on the rate-distortion analysis from [Dong and Van Roy, 2020], where they proved a bound with regret rate of $O(d\sqrt{T \log(T)})$ for the $d$-dimensional linear bandit setting. We focus on bandit problems with a metric action space and, using a chaining argument, we establish new bounds that depend on the metric entropy of the action space for a variant of Thompson-Sampling.   Under suitable continuity assumption of the rewards, our bound offers a tight rate of $O(d\sqrt{T})$ for $d$-dimensional linear bandit problems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#20027;&#25511;&#21046;&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#24182;&#34892;&#24335;&#21277;&#36947;&#21512;&#27969;&#65292;&#32771;&#34385;&#20102;&#36947;&#36335;&#19978;&#20854;&#20182;&#36710;&#36742;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#28608;&#21169;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.03359</link><description>&lt;p&gt;
RACE-SM:&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31038;&#20132;&#24335;&#21277;&#36947;&#21512;&#27969;&#33258;&#20027;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
RACE-SM: Reinforcement Learning Based Autonomous Control for Social On-Ramp Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#20027;&#25511;&#21046;&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#24182;&#34892;&#24335;&#21277;&#36947;&#21512;&#27969;&#65292;&#32771;&#34385;&#20102;&#36947;&#36335;&#19978;&#20854;&#20182;&#36710;&#36742;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#28608;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#24182;&#34892;&#24335;&#21277;&#36947;&#21512;&#27969;&#22312;&#20154;&#25511;&#36710;&#36742;&#20132;&#36890;&#20013;&#20173;&#28982;&#26159;&#33258;&#20027;&#36710;&#36742;&#25511;&#21046;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#38750;&#23398;&#20064;&#22411;&#36710;&#36742;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#20381;&#36182;&#35268;&#21017;&#21644;&#20248;&#21270;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#23637;&#23637;&#29616;&#20102;&#24076;&#26395;&#65292;&#24182;&#21463;&#21040;&#20102;&#37325;&#35201;&#23398;&#26415;&#20851;&#27880;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#23545;&#20854;&#20182;&#39640;&#36895;&#20844;&#36335;&#36710;&#36742;&#20851;&#27880;&#19981;&#36275;&#65292;&#19988;&#32463;&#24120;&#20381;&#36182;&#19981;&#20934;&#30830;&#30340;&#36947;&#36335;&#20132;&#36890;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#24182;&#34892;&#24335;&#24773;&#20917;&#24456;&#23569;&#34987;&#32771;&#34385;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#21152;&#36895;&#21644;&#21464;&#36947;&#20915;&#31574;&#21046;&#23450;&#65292;&#35813;&#27169;&#22411;&#26126;&#30830;&#32771;&#34385;&#20102;&#23545;&#20110;&#36710;&#36742;&#26412;&#36523;&#21450;&#20854;&#21608;&#22260;&#36710;&#36742;&#65288;&#21487;&#33021;&#21512;&#20316;&#25110;&#19981;&#21512;&#20316;&#65289;&#30340;&#25928;&#29992;&#65292;&#20197;&#20135;&#29983;&#31526;&#21512;&#31038;&#20250;&#35268;&#33539;&#30340;&#34892;&#20026;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#22870;&#21169;&#20989;&#25968;&#21033;&#29992;&#31038;&#20132;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03359v1 Announce Type: new  Abstract: Autonomous parallel-style on-ramp merging in human controlled traffic continues to be an existing issue for autonomous vehicle control. Existing non-learning based solutions for vehicle control rely on rules and optimization primarily. These methods have been seen to present significant challenges. Recent advancements in Deep Reinforcement Learning have shown promise and have received significant academic interest however the available learning based approaches show inadequate attention to other highway vehicles and often rely on inaccurate road traffic assumptions. In addition, the parallel-style case is rarely considered. A novel learning based model for acceleration and lane change decision making that explicitly considers the utility to both the ego vehicle and its surrounding vehicles which may be cooperative or uncooperative to produce behaviour that is socially acceptable is proposed. The novel reward function makes use of Social 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#20551;&#35774;&#31354;&#38388;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#20877;&#29983;&#26680;Banach&#31354;&#38388;&#65292;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#23398;&#20064;&#21644;&#26368;&#23567;&#25554;&#20540;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#21487;&#20197;&#34920;&#31034;&#20026;&#32447;&#24615;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.03353</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#20551;&#35774;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Hypothesis Spaces for Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#20551;&#35774;&#31354;&#38388;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#20877;&#29983;&#26680;Banach&#31354;&#38388;&#65292;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#23398;&#20064;&#21644;&#26368;&#23567;&#25554;&#20540;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#21487;&#20197;&#34920;&#31034;&#20026;&#32447;&#24615;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#20551;&#35774;&#31354;&#38388;&#12290;&#36890;&#36807;&#23558;DNN&#35270;&#20026;&#20004;&#20010;&#21464;&#37327;&#30340;&#20989;&#25968;&#65292;&#21363;&#29289;&#29702;&#21464;&#37327;&#21644;&#21442;&#25968;&#21464;&#37327;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;DNNs&#30340;&#21407;&#22987;&#38598;&#21512;&#65292;&#21442;&#25968;&#21464;&#37327;&#20301;&#20110;&#30001;DNNs&#30340;&#26435;&#37325;&#30697;&#38453;&#21644;&#20559;&#32622;&#20915;&#23450;&#30340;&#19968;&#32452;&#28145;&#24230;&#21644;&#23485;&#24230;&#20013;&#12290;&#28982;&#21518;&#22312;&#24369;*&#25299;&#25169;&#20013;&#23436;&#25104;&#21407;&#22987;DNN&#38598;&#21512;&#30340;&#32447;&#24615;&#36328;&#24230;&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#29289;&#29702;&#21464;&#37327;&#20989;&#25968;&#30340;Banach&#31354;&#38388;&#12290;&#25105;&#20204;&#35777;&#26126;&#25152;&#26500;&#36896;&#30340;Banach&#31354;&#38388;&#26159;&#19968;&#20010;&#20877;&#29983;&#26680;Banach&#31354;&#38388;&#65288;RKBS&#65289;&#65292;&#24182;&#26500;&#36896;&#20854;&#20877;&#29983;&#26680;&#12290;&#36890;&#36807;&#20026;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#24314;&#31435;&#34920;&#36798;&#23450;&#29702;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#23398;&#20064;&#27169;&#22411;&#65292;&#27491;&#21017;&#21270;&#23398;&#20064;&#21644;&#26368;&#23567;&#25554;&#20540;&#38382;&#39064;&#22312;&#32467;&#26524;RKBS&#20013;&#12290;&#34920;&#36798;&#23450;&#29702;&#25581;&#31034;&#20102;&#36825;&#20123;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#21487;&#20197;&#34920;&#31034;&#20026;&#32447;&#24615;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03353v1 Announce Type: cross  Abstract: This paper introduces a hypothesis space for deep learning that employs deep neural networks (DNNs). By treating a DNN as a function of two variables, the physical variable and parameter variable, we consider the primitive set of the DNNs for the parameter variable located in a set of the weight matrices and biases determined by a prescribed depth and widths of the DNNs. We then complete the linear span of the primitive DNN set in a weak* topology to construct a Banach space of functions of the physical variable. We prove that the Banach space so constructed is a reproducing kernel Banach space (RKBS) and construct its reproducing kernel. We investigate two learning models, regularized learning and minimum interpolation problem in the resulting RKBS, by establishing representer theorems for solutions of the learning models. The representer theorems unfold that solutions of these learning models can be expressed as linear combination of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Solution Simplex Clustered Federated Learning&#65288;SosicFL&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#21333;&#32431;&#24418;&#30340;&#24605;&#24819;&#65292;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#37197;&#21333;&#19968;&#21306;&#22495;&#65292;&#20174;&#32780;&#21516;&#26102;&#23454;&#29616;&#20102;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.03333</link><description>&lt;p&gt;
Solution Simplex Clustering for Heterogeneous Federated Learning
&lt;/p&gt;
&lt;p&gt;
Solution Simplex Clustering for Heterogeneous Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03333
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Solution Simplex Clustered Federated Learning&#65288;SosicFL&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#21333;&#32431;&#24418;&#30340;&#24605;&#24819;&#65292;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#37197;&#21333;&#19968;&#21306;&#22495;&#65292;&#20174;&#32780;&#21516;&#26102;&#23454;&#29616;&#20102;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#22312;&#39640;&#24230;&#24322;&#26500;&#30340;&#23458;&#25143;&#20998;&#24067;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#22256;&#38590;&#37096;&#20998;&#28304;&#20110;&#20004;&#20010;&#30475;&#20284;&#30683;&#30462;&#30340;&#30446;&#26631;&#65306;&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;&#23458;&#25143;&#31471;&#30340;&#20449;&#24687;&#26469;&#23398;&#20064;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#20197;&#21450;&#23398;&#20064;&#24212;&#36866;&#24212;&#27599;&#20010;&#26412;&#22320;&#20998;&#24067;&#30340;&#26412;&#22320;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Solution Simplex Clustered Federated Learning&#65288;SosicFL&#65289;&#26469;&#28040;&#38500;&#36825;&#31181;&#30683;&#30462;&#12290;&#22522;&#20110;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#21333;&#32431;&#24418;&#30340;&#26368;&#26032;&#24605;&#24819;&#65292;SosicFL&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#37197;&#19968;&#20010;&#21333;&#32431;&#24418;&#20013;&#30340;&#23376;&#21306;&#22495;&#65292;&#24182;&#25191;&#34892;FL&#26469;&#23398;&#20064;&#19968;&#20010;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#21333;&#32431;&#24418;&#12290;&#36825;&#20351;&#24471;&#23458;&#25143;&#31471;&#27169;&#22411;&#22312;&#35299;&#20915;&#26041;&#26696;&#21333;&#32431;&#24418;&#30340;&#33258;&#30001;&#24230;&#33539;&#22260;&#20869;&#20855;&#26377;&#20854;&#29305;&#24449;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#36890;&#29992;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SosicFL&#25913;&#21892;&#20102;&#24615;&#33021;&#65292;&#24182;&#21152;&#36895;&#20102;&#20840;&#23616;&#21644;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03333v1 Announce Type: new  Abstract: We tackle a major challenge in federated learning (FL) -- achieving good performance under highly heterogeneous client distributions. The difficulty partially arises from two seemingly contradictory goals: learning a common model by aggregating the information from clients, and learning local personalized models that should be adapted to each local distribution. In this work, we propose Solution Simplex Clustered Federated Learning (SosicFL) for dissolving such contradiction. Based on the recent ideas of learning solution simplices, SosicFL assigns a subregion in a simplex to each client, and performs FL to learn a common solution simplex. This allows the client models to possess their characteristics within the degrees of freedom in the solution simplex, and at the same time achieves the goal of learning a global common model. Our experiments show that SosicFL improves the performance and accelerates the training process for global and 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#26412;&#22320;&#31354;&#38388;&#21152;&#26435;&#26041;&#26696;&#12289;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;&#23574;&#31471;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#22320;&#29702;&#31354;&#38388;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03328</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#21487;&#35299;&#37322;&#22320;&#29702;&#31354;&#38388;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38598;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Ensemble Framework for Explainable Geospatial Machine Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03328
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#26412;&#22320;&#31354;&#38388;&#21152;&#26435;&#26041;&#26696;&#12289;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;&#23574;&#31471;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#22320;&#29702;&#31354;&#38388;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#31354;&#38388;&#21464;&#21270;&#25928;&#24212;&#22312;&#22320;&#29702;&#20998;&#26512;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22320;&#29702;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#38750;&#32447;&#24615;&#65292;&#20934;&#30830;&#25429;&#25417;&#21644;&#35299;&#37322;&#36825;&#31181;&#21464;&#24322;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#26412;&#22320;&#31354;&#38388;&#21152;&#26435;&#26041;&#26696;&#12289;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;&#23574;&#31471;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#24357;&#21512;&#20256;&#32479;&#22320;&#29702;&#20998;&#26512;&#27169;&#22411;&#21644;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#36890;&#36807;&#38416;&#26126;&#31354;&#38388;&#21464;&#24322;&#24615;&#65292;&#25552;&#39640;&#20102;&#22320;&#29702;&#22238;&#24402;&#21644;&#20998;&#31867;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#23427;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#29702;&#35299;&#31354;&#38388;&#29616;&#35937;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03328v1 Announce Type: new  Abstract: Analyzing spatial varying effect is pivotal in geographic analysis. Yet, accurately capturing and interpreting this variability is challenging due to the complexity and non-linearity of geospatial data. Herein, we introduce an integrated framework that merges local spatial weighting scheme, Explainable Artificial Intelligence (XAI), and cutting-edge machine learning technologies to bridge the gap between traditional geographic analysis models and general machine learning approaches. Through tests on synthetic datasets, this framework is verified to enhance the interpretability and accuracy of predictions in both geographic regression and classification by elucidating spatial variability. It significantly boosts prediction precision, offering a novel approach to understanding spatial phenomena.
&lt;/p&gt;</description></item><item><title>&#24615;&#33021;&#26159;&#21487;&#37197;&#32622;&#36719;&#20214;&#31995;&#32479;&#34892;&#20026;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#21487;&#37197;&#32622;&#36719;&#20214;&#24615;&#33021;&#23398;&#20064;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#19982;&#20998;&#31867;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.03322</link><description>&lt;p&gt;
&#28145;&#24230;&#37197;&#32622;&#24615;&#33021;&#23398;&#20064;&#65306;&#19968;&#39033;&#31995;&#32479;&#24615;&#35843;&#26597;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Configuration Performance Learning: A Systematic Survey and Taxonomy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03322
&lt;/p&gt;
&lt;p&gt;
&#24615;&#33021;&#26159;&#21487;&#37197;&#32622;&#36719;&#20214;&#31995;&#32479;&#34892;&#20026;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#21487;&#37197;&#32622;&#36719;&#20214;&#24615;&#33021;&#23398;&#20064;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#19982;&#20998;&#31867;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24615;&#33021;&#21487;&#20197;&#35828;&#26159;&#21453;&#26144;&#21487;&#37197;&#32622;&#36719;&#20214;&#31995;&#32479;&#34892;&#20026;&#30340;&#26368;&#20851;&#38190;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#29616;&#20195;&#36719;&#20214;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#23545;&#21508;&#31181;&#37197;&#32622;&#22914;&#20309;&#24433;&#21709;&#24615;&#33021;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#25104;&#20026;&#36719;&#20214;&#32500;&#25252;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#24615;&#33021;&#36890;&#24120;&#26159;&#22312;&#27809;&#26377;&#23545;&#36719;&#20214;&#31995;&#32479;&#26377;&#36879;&#24443;&#20102;&#35299;&#30340;&#24773;&#20917;&#19979;&#24314;&#27169;&#30340;&#65292;&#20027;&#35201;&#20381;&#36182;&#25968;&#25454;&#65292;&#36825;&#27491;&#22909;&#31526;&#21512;&#28145;&#24230;&#23398;&#20064;&#30340;&#30446;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#28145;&#24230;&#23398;&#20064;&#22312;&#21487;&#37197;&#32622;&#36719;&#20214;&#24615;&#33021;&#23398;&#20064;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#28085;&#30422;&#20102;948&#31687;&#26469;&#33258;&#20845;&#20010;&#32034;&#24341;&#26381;&#21153;&#30340;&#35770;&#25991;&#65292;&#22522;&#20110;&#27492;&#25552;&#21462;&#24182;&#20998;&#26512;&#20102;85&#31687;&#20027;&#35201;&#35770;&#25991;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24635;&#32467;&#20102;&#37197;&#32622;&#25968;&#25454;&#22914;&#20309;&#20934;&#22791;&#65292;&#28145;&#24230;&#37197;&#32622;&#24615;&#33021;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#26500;&#24314;&#65292;&#20197;&#21450;&#35813;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#35780;&#20272;&#31561;&#20851;&#38190;&#20027;&#39064;&#21644;&#32479;&#35745;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03322v1 Announce Type: cross  Abstract: Performance is arguably the most crucial attribute that reflects the behavior of a configurable software system. However, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance. As such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning.   In this paper, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 948 searched papers spanning six indexing services, based on which 85 primary papers were extracted and analyzed. Our results summarize the key topics and statistics on how the configuration data is prepared; how the deep configuration performance learning model is built; how the model is evalu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21521;&#21518;&#21487;&#36798;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#22810;&#26234;&#33021;&#20307;&#31070;&#32463;&#21453;&#39304;&#29615;&#36335;&#30340;&#30896;&#25758;&#36991;&#20813;&#23646;&#24615;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#31995;&#21015;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#35745;&#31639;&#30456;&#23545;&#21453;&#25237;&#24433;&#38598;&#65292;&#24182;&#19988;&#35813;&#36880;&#23545;&#26041;&#27861;&#21487;&#24182;&#34892;&#21270;&#65292;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#30340;&#22686;&#21152;&#33021;&#22815;&#24456;&#22909;&#22320;&#25193;&#23637;</title><link>https://arxiv.org/abs/2403.03314</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#31574;&#30053;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30896;&#25758;&#36991;&#20813;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Collision Avoidance Verification of Multiagent Systems with Learned Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03314
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21521;&#21518;&#21487;&#36798;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#22810;&#26234;&#33021;&#20307;&#31070;&#32463;&#21453;&#39304;&#29615;&#36335;&#30340;&#30896;&#25758;&#36991;&#20813;&#23646;&#24615;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#31995;&#21015;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#35745;&#31639;&#30456;&#23545;&#21453;&#25237;&#24433;&#38598;&#65292;&#24182;&#19988;&#35813;&#36880;&#23545;&#26041;&#27861;&#21487;&#24182;&#34892;&#21270;&#65292;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#30340;&#22686;&#21152;&#33021;&#22815;&#24456;&#22909;&#22320;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#22810;&#26234;&#33021;&#20307;&#25511;&#21046;&#38382;&#39064;&#65292;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#24050;&#32463;&#23454;&#29616;&#20102;&#26377;&#21069;&#36884;&#30340;&#26032;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#31995;&#32479;&#32570;&#20047;&#27491;&#24335;&#30340;&#20445;&#35777;&#65288;&#20363;&#22914;&#65292;&#30896;&#25758;&#36991;&#20813;&#12289;&#40065;&#26834;&#24615;&#65289;&#65292;&#36825;&#38459;&#27490;&#20102;&#22312;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#21033;&#29992;&#36825;&#20123;&#36827;&#23637;&#12290;&#37492;&#20110;&#29616;&#26377;&#25216;&#26415;&#19981;&#33021;&#22788;&#29702;&#22810;&#20110;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#24773;&#20917;&#65292;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39564;&#35777;&#22810;&#26234;&#33021;&#20307;&#31070;&#32463;&#21453;&#39304;&#29615;&#36335;&#65288;MA-NFLs&#65289;&#30340;&#30896;&#25758;&#36991;&#20813;&#23646;&#24615;&#30340;&#22522;&#20110;&#21521;&#21518;&#21487;&#36798;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32473;&#23450;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#35757;&#32451;&#25511;&#21046;&#31574;&#30053;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#36890;&#36807;&#31163;&#32447;&#20026;&#27599;&#23545;&#26234;&#33021;&#20307;&#35299;&#20915;&#19968;&#31995;&#21015;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILPs&#65289;&#26469;&#35745;&#31639;&#30456;&#23545;&#21453;&#25237;&#24433;&#38598;&#12290;&#25105;&#20204;&#30340;&#36880;&#23545;&#26041;&#27861;&#21487;&#24182;&#34892;&#21270;&#65292;&#20174;&#32780;&#21487;&#20197;&#24456;&#22909;&#22320;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#25193;&#23637;&#65292;&#24182;&#19988;&#25105;&#20204;&#32771;&#34385;&#20102;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03314v1 Announce Type: cross  Abstract: For many multiagent control problems, neural networks (NNs) have enabled promising new capabilities. However, many of these systems lack formal guarantees (e.g., collision avoidance, robustness), which prevents leveraging these advances in safety-critical settings. While there is recent work on formal verification of NN-controlled systems, most existing techniques cannot handle scenarios with more than one agent. To address this research gap, this paper presents a backward reachability-based approach for verifying the collision avoidance properties of Multi-Agent Neural Feedback Loops (MA-NFLs). Given the dynamics models and trained control policies of each agent, the proposed algorithm computes relative backprojection sets by solving a series of Mixed Integer Linear Programs (MILPs) offline for each pair of agents. Our pair-wise approach is parallelizable and thus scales well with increasing number of agents, and we account for state 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20026;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;&#25552;&#20379;&#21442;&#25968;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#22312;&#20943;&#23569;&#37327;&#23376;&#35745;&#31639;&#36164;&#28304;&#24320;&#38144;&#30340;&#21516;&#26102;&#22686;&#24378;&#31639;&#27861;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.03310</link><description>&lt;p&gt;
&#22270;&#23398;&#20064;&#29992;&#20110;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;&#21442;&#25968;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph Learning for Parameter Prediction of Quantum Approximate Optimization Algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20026;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;&#25552;&#20379;&#21442;&#25968;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#22312;&#20943;&#23569;&#37327;&#23376;&#35745;&#31639;&#36164;&#28304;&#24320;&#38144;&#30340;&#21516;&#26102;&#22686;&#24378;&#31639;&#27861;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#37327;&#23376;&#35745;&#31639;&#24050;&#32463;&#25104;&#20026;&#32452;&#21512;&#20248;&#21270;&#39046;&#22495;&#20013;&#30340;&#19968;&#32929;&#21464;&#38761;&#21147;&#37327;&#65292;&#25552;&#20379;&#20102;&#22788;&#29702;&#38271;&#26399;&#20197;&#26469;&#25361;&#25112;&#32463;&#20856;&#35745;&#31639;&#26041;&#27861;&#30340;&#22797;&#26434;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#20854;&#20013;&#65292;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;&#65288;QAOA&#65289;&#20197;&#20854;&#28508;&#21147;&#22312;&#39640;&#25928;&#35299;&#20915;Max-Cut&#38382;&#39064;&#19978;&#33073;&#39062;&#32780;&#20986;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30446;&#21069;&#37327;&#23376;&#35745;&#31639;&#36164;&#28304;&#30340;&#38480;&#21046;&#65292;&#23454;&#38469;&#24212;&#29992;&#38754;&#20020;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20248;&#21270;&#20102;QAOA&#21021;&#22987;&#21270;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#28909;&#21551;&#21160;&#25216;&#26415;&#12290;&#36825;&#29306;&#29298;&#20102;&#22312;&#32463;&#20856;&#35745;&#31639;&#26426;&#19978;&#30340;&#21487;&#36127;&#25285;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#20197;&#20943;&#23569;&#37327;&#23376;&#35745;&#31639;&#36164;&#28304;&#24320;&#38144;&#65292;&#22686;&#24378;&#20102;QAOA&#30340;&#25928;&#26524;&#12290;&#23545;&#21508;&#31181;GNN&#26550;&#26500;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#36866;&#24212;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#20984;&#26174;&#20102;&#37327;&#23376;&#31639;&#27861;&#21644;GNN&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03310v1 Announce Type: cross  Abstract: In recent years, quantum computing has emerged as a transformative force in the field of combinatorial optimization, offering novel approaches to tackling complex problems that have long challenged classical computational methods. Among these, the Quantum Approximate Optimization Algorithm (QAOA) stands out for its potential to efficiently solve the Max-Cut problem, a quintessential example of combinatorial optimization. However, practical application faces challenges due to current limitations on quantum computational resource. Our work optimizes QAOA initialization, using Graph Neural Networks (GNN) as a warm-start technique. This sacrifices affordable computational resource on classical computer to reduce quantum computational resource overhead, enhancing QAOA's effectiveness. Experiments with various GNN architectures demonstrate the adaptability and stability of our framework, highlighting the synergy between quantum algorithms an
&lt;/p&gt;</description></item><item><title>Mad Libs &#25552;&#20379;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861; MLA &#22312;&#36328;&#39046;&#22495;&#25991;&#26723;&#32423;&#20107;&#20214;&#21442;&#25968;&#25968;&#25454;&#25552;&#21462;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102; 2.6 &#20010; F1 &#20998;&#25968;&#12290;&#21516;&#26102;&#65292;&#22312;&#38646;&#21644;&#23569;&#26679;&#26412;&#20107;&#20214;&#35282;&#33394;&#26041;&#38754;&#30456;&#27604;&#26080;&#22686;&#24378;&#22522;&#32447;&#65292;&#25552;&#39640;&#20102; 3.9 &#21644; 5.2 &#20010;&#30334;&#20998;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.03304</link><description>&lt;p&gt;
&#25152;&#38656;&#21482;&#26159; Mad Libs: &#22686;&#24378;&#36328;&#39046;&#22495;&#25991;&#26723;&#32423;&#20107;&#20214;&#21442;&#25968;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Mad Libs Are All You Need: Augmenting Cross-Domain Document-Level Event Argument Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03304
&lt;/p&gt;
&lt;p&gt;
Mad Libs &#25552;&#20379;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861; MLA &#22312;&#36328;&#39046;&#22495;&#25991;&#26723;&#32423;&#20107;&#20214;&#21442;&#25968;&#25968;&#25454;&#25552;&#21462;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102; 2.6 &#20010; F1 &#20998;&#25968;&#12290;&#21516;&#26102;&#65292;&#22312;&#38646;&#21644;&#23569;&#26679;&#26412;&#20107;&#20214;&#35282;&#33394;&#26041;&#38754;&#30456;&#27604;&#26080;&#22686;&#24378;&#22522;&#32447;&#65292;&#25552;&#39640;&#20102; 3.9 &#21644; 5.2 &#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20107;&#20214;&#21442;&#25968;&#25552;&#21462;&#65288;DocEAE&#65289;&#26159;&#19968;&#20010;&#26497;&#20854;&#22256;&#38590;&#30340;&#20449;&#24687;&#25552;&#21462;&#38382;&#39064;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#36328;&#39046;&#22495;&#35774;&#32622;&#20013;&#23384;&#22312;&#30528;&#26174;&#33879;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; Mad Lib Aug&#65288;MLA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335; DocEAE &#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102; Mad Libs &#30340;&#30452;&#35273;&#65292;&#21363;&#20316;&#20026;&#19968;&#31181;&#28909;&#38376;&#28216;&#25103;&#20013;&#20351;&#29992;&#30340;&#20998;&#31867;&#25513;&#30721;&#25991;&#26723;&#21487;&#20197;&#34987; LLMs &#29983;&#25104;&#24182;&#35299;&#31572;&#65292;&#20174;&#32780;&#20026; DocEAE &#29983;&#25104;&#25968;&#25454;&#12290;&#20351;&#29992; MLA&#65292;&#25105;&#20204;&#30340;&#25972;&#20307; F1 &#20998;&#25968;&#24179;&#22343;&#25913;&#36827;&#20102; 2.6 &#20010;&#30334;&#20998;&#28857;&#12290;&#27492;&#22806;&#65292;&#19982;&#26080;&#22686;&#24378;&#22522;&#32447;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#38646;&#21644;&#23569;&#26679;&#26412;&#20107;&#20214;&#35282;&#33394;&#26041;&#38754;&#22312;&#25152;&#26377;&#23454;&#39564;&#20013;&#24179;&#22343;&#25552;&#39640;&#20102; 3.9 &#21644; 5.2 &#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03304v1 Announce Type: new  Abstract: Document-Level Event Argument Extraction (DocEAE) is an extremely difficult information extraction problem -- with significant limitations in low-resource cross-domain settings. To address this problem, we introduce Mad Lib Aug (MLA), a novel generative DocEAE data augmentation framework. Our approach leverages the intuition that Mad Libs, which are categorically masked documents used as a part of a popular game, can be generated and solved by LLMs to produce data for DocEAE. Using MLA, we achieve a 2.6-point average improvement in overall F1 score. Moreover, this approach achieves a 3.9 and 5.2 point average increase in zero and few-shot event roles compared to augmentation-free baselines across all experiments.   To better facilitate analysis of cross-domain DocEAE, we additionally introduce a new metric, Role-Depth F1 (RDF1), which uses statistical depth to identify roles in the target domain which are semantic outliers with respect t
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;Coupon Collector&#38382;&#39064;&#30340;&#21457;&#29616;&#23637;&#31034;&#20102;&#27491;&#30830;&#19982;&#19981;&#27491;&#30830;&#23398;&#20064;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#20026;&#37327;&#23376;PAC&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.03295</link><description>&lt;p&gt;
&#27491;&#30830;&#19982;&#19981;&#27491;&#30830;&#30340;&#37327;&#23376;PAC&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Proper vs Improper Quantum PAC learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03295
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;Coupon Collector&#38382;&#39064;&#30340;&#21457;&#29616;&#23637;&#31034;&#20102;&#27491;&#30830;&#19982;&#19981;&#27491;&#30830;&#23398;&#20064;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#20026;&#37327;&#23376;PAC&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30340;PAC&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#27491;&#30830;&#23398;&#20064;&#26159;&#21542;&#27604;&#19981;&#27491;&#30830;&#23398;&#20064;&#26356;&#22256;&#38590;&#12290;&#22312;&#32463;&#20856;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#30528;VC&#32500;&#24230;&#20026;$d$&#30340;&#27010;&#24565;&#31867;&#21035;&#65292;&#23545;&#20110;&#38169;&#35823;&#29575;&#20026;$\epsilon$&#30340;&#27491;&#30830;&#23398;&#20064;&#32780;&#35328;&#65292;&#23427;&#20204;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#26159;$\Omega\left(\frac d\epsilon\log\frac1\epsilon\right)$&#65292;&#32780;&#23545;&#20110;&#19981;&#27491;&#30830;&#23398;&#20064;&#21017;&#26159;O$\!\left(\frac d\epsilon\right)$&#12290;&#20854;&#20013;&#19968;&#20010;&#20363;&#23376;&#28041;&#21450;&#21040;&#20102;&#20248;&#24800;&#21048;&#25910;&#38598;&#38382;&#39064;&#12290;&#21463;&#37327;&#23376;&#26679;&#26412;&#30340;&#27491;&#30830;&#19982;&#19981;&#27491;&#30830;&#23398;&#20064;&#25928;&#29575;&#21551;&#21457;&#65292;Arunachalam&#12289;Belovs&#12289;Childs&#12289;Kothari&#12289;Rosmanis&#21644;de Wolf (TQC 2020)&#30740;&#31350;&#20102;&#19968;&#20010;&#31867;&#20284;&#30340;&#38382;&#39064;&#65292;&#21363;&#37327;&#23376;&#20248;&#24800;&#21048;&#25910;&#38598;&#38382;&#39064;&#12290;&#20196;&#20154;&#22909;&#22855;&#30340;&#26159;&#65292;&#20182;&#20204;&#21457;&#29616;&#23545;&#20110;&#23398;&#20064;&#22823;&#23567;&#20026;$k$&#30340;$[n]$&#30340;&#23376;&#38598;&#65292;&#35813;&#38382;&#39064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\Theta(k\log\min\{k,n-k+1\})$&#65292;&#19982;&#20248;&#24800;&#21048;&#25910;&#38598;&#38382;&#39064;&#30340;$\Theta(k\log k)$&#22797;&#26434;&#24230;&#24418;&#25104;&#23545;&#27604;&#12290;&#36825;&#20107;&#23454;&#19978;&#21542;&#23450;&#20102;&#21487;&#33021;&#23384;&#22312;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03295v1 Announce Type: cross  Abstract: A basic question in the PAC model of learning is whether proper learning is harder than improper learning. In the classical case, there are examples of concept classes with VC dimension $d$ that have sample complexity $\Omega\left(\frac d\epsilon\log\frac1\epsilon\right)$ for proper learning with error $\epsilon$, while the complexity for improper learning is O$\!\left(\frac d\epsilon\right)$. One such example arises from the Coupon Collector problem.   Motivated by the efficiency of proper versus improper learning with quantum samples, Arunachalam, Belovs, Childs, Kothari, Rosmanis, and de Wolf (TQC 2020) studied an analogue, the Quantum Coupon Collector problem. Curiously, they discovered that for learning size $k$ subsets of $[n]$ the problem has sample complexity $\Theta(k\log\min\{k,n-k+1\})$, in contrast with the complexity of $\Theta(k\log k)$ for Coupon Collector. This effectively negates the possibility of a separation between
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#22343;&#21270;&#36895;&#29575;&#35843;&#24230;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#20998;&#25955;&#24335;&#23398;&#20064;&#20013;&#24322;&#26500;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#65288;&#25552;&#39640;&#20102;&#32422;3%&#27979;&#35797;&#20934;&#30830;&#24615;&#65289;&#12290;</title><link>https://arxiv.org/abs/2403.03292</link><description>&lt;p&gt;
&#29992;&#20110;&#24322;&#26500;&#25968;&#25454;&#19979;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#30340;&#24179;&#22343;&#21270;&#36895;&#29575;&#35843;&#24230;&#22120;
&lt;/p&gt;
&lt;p&gt;
Averaging Rate Scheduler for Decentralized Learning on Heterogeneous Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03292
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#22343;&#21270;&#36895;&#29575;&#35843;&#24230;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#20998;&#25955;&#24335;&#23398;&#20064;&#20013;&#24322;&#26500;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#65288;&#25552;&#39640;&#20102;&#32422;3%&#27979;&#35797;&#20934;&#30830;&#24615;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#35201;&#27714;&#25968;&#25454;&#20998;&#24067;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;IID&#65289;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20195;&#29702;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#30340;&#24322;&#26500;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24179;&#22343;&#21270;&#36895;&#29575;&#35843;&#24230;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#20998;&#25955;&#24335;&#23398;&#20064;&#20013;&#24322;&#26500;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#37319;&#29992;&#24658;&#23450;&#24179;&#22343;&#21270;&#36895;&#29575;&#30340;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27979;&#35797;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20248;&#36234;&#65288;&#25552;&#39640;&#20102;&#32422;3%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03292v1 Announce Type: new  Abstract: State-of-the-art decentralized learning algorithms typically require the data distribution to be Independent and Identically Distributed (IID). However, in practical scenarios, the data distribution across the agents can have significant heterogeneity. In this work, we propose averaging rate scheduling as a simple yet effective way to reduce the impact of heterogeneity in decentralized learning. Our experiments illustrate the superiority of the proposed method (~3% improvement in test accuracy) compared to the conventional approach of employing a constant averaging rate.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#30005;&#36335;&#36827;&#34892;&#21487;&#20449;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#65292;&#22312;&#32500;&#25345;&#31454;&#20105;&#24615;&#33021;&#30340;&#21516;&#26102;&#33021;&#22815;&#21487;&#38752;&#25512;&#26029;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.03281</link><description>&lt;p&gt;
&#20351;&#29992;&#27010;&#29575;&#30005;&#36335;&#36827;&#34892;&#21487;&#20449;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Credibility-Aware Multi-Modal Fusion Using Probabilistic Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03281
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#30005;&#36335;&#36827;&#34892;&#21487;&#20449;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#65292;&#22312;&#32500;&#25345;&#31454;&#20105;&#24615;&#33021;&#30340;&#21516;&#26102;&#33021;&#22815;&#21487;&#38752;&#25512;&#26029;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#38024;&#23545;&#36776;&#21035;&#23398;&#20064;&#30340;&#36831;&#21040;&#22810;&#27169;&#24577;&#34701;&#21512;&#38382;&#39064;&#12290;&#21463;&#21040;&#38656;&#35201;&#29702;&#35299;&#27599;&#20010;&#25968;&#25454;&#28304;&#21487;&#38752;&#24615;&#30340;&#22024;&#26434;&#30340;&#22810;&#28304;&#39046;&#22495;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#21487;&#20449;&#24230;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#30005;&#36335;&#65288;PCs&#65289;&#26469;&#32467;&#21512;&#20010;&#20307;&#27169;&#24577;&#19978;&#30340;&#39044;&#27979;&#20998;&#24067;&#30340;&#32452;&#21512;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#23450;&#20041;&#20102;&#19968;&#31181;&#27010;&#29575;&#24230;&#37327;&#26469;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#30340;&#21487;&#20449;&#24230;&#65292;&#36890;&#36807;PC&#19978;&#30340;&#25512;&#29702;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#34701;&#21512;&#26041;&#27861;&#33021;&#22815;&#21487;&#38752;&#22320;&#25512;&#26029;&#21487;&#20449;&#24230;&#65292;&#24182;&#19988;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#20445;&#25345;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03281v1 Announce Type: cross  Abstract: We consider the problem of late multi-modal fusion for discriminative learning. Motivated by noisy, multi-source domains that require understanding the reliability of each data source, we explore the notion of credibility in the context of multi-modal fusion. We propose a combination function that uses probabilistic circuits (PCs) to combine predictive distributions over individual modalities. We also define a probabilistic measure to evaluate the credibility of each modality via inference queries over the PC. Our experimental evaluation demonstrates that our fusion method can reliably infer credibility while maintaining competitive performance with the state-of-the-art.
&lt;/p&gt;</description></item><item><title>ARNN&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#20449;&#21495;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#21644;&#24182;&#34892;&#35745;&#31639;&#65292;&#32467;&#21512;&#27880;&#24847;&#21147;&#21644;LSTM gate&#30340;&#20248;&#21183;&#65292;&#24182;&#36991;&#20813;&#20102;&#23427;&#20204;&#30340;&#32570;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.03276</link><description>&lt;p&gt;
ARNN: &#29992;&#20110;&#35782;&#21035;&#30315;&#30187;&#21457;&#20316;&#30340;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#20449;&#21495;&#30340;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ARNN: Attentive Recurrent Neural Network for Multi-channel EEG Signals to Identify Epileptic Seizures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03276
&lt;/p&gt;
&lt;p&gt;
ARNN&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#20449;&#21495;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#21644;&#24182;&#34892;&#35745;&#31639;&#65292;&#32467;&#21512;&#27880;&#24847;&#21147;&#21644;LSTM gate&#30340;&#20248;&#21183;&#65292;&#24182;&#36991;&#20813;&#20102;&#23427;&#20204;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;ARNN&#65289;&#65292;&#20854;&#27839;&#30528;&#24207;&#21015;&#24490;&#29615;&#24212;&#29992;&#27880;&#24847;&#21147;&#23618;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#24207;&#21015;&#38271;&#24230;&#30456;&#20851;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#20449;&#21495;&#19978;&#36816;&#34892;&#65292;&#32780;&#19981;&#26159;&#21333;&#36890;&#36947;&#20449;&#21495;&#65292;&#24182;&#21033;&#29992;&#24182;&#34892;&#35745;&#31639;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#27880;&#24847;&#21147;&#23618;&#26159;&#19968;&#31181;&#35745;&#31639;&#21333;&#20803;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35745;&#31639;&#19968;&#32452;&#24191;&#27867;&#25968;&#37327;&#30340;&#29366;&#24577;&#21521;&#37327;&#21644;&#36755;&#20837;&#20449;&#21495;&#30340;&#36882;&#24402;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21463;&#21040;&#20102;&#27880;&#24847;&#21147;&#23618;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21333;&#20803;&#30340;&#21551;&#21457;&#65292;&#24182;&#20351;&#29992;&#38271;&#30701;&#39118;&#26684;&#38376;&#65292;&#20294;&#36890;&#36807;&#22810;&#20010;&#38454;&#27573;&#23558;&#36825;&#31181;&#20856;&#22411;&#21333;&#20803;&#25193;&#23637;&#21040;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#20449;&#21495;&#30340;&#24182;&#34892;&#21270;&#12290;&#23427;&#32487;&#25215;&#20102;&#27880;&#24847;&#21147;&#23618;&#21644;LSTM&#38376;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#32570;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#24322;&#36136;&#23454;&#39564;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27169;&#22411;&#26377;&#25928;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03276v1 Announce Type: cross  Abstract: We proposed an Attentive Recurrent Neural Network (ARNN), which recurrently applies attention layers along a sequence and has linear complexity with respect to the sequence length. The proposed model operates on multi-channel EEG signals rather than single channel signals and leverages parallel computation. In this cell, the attention layer is a computational unit that efficiently applies self-attention and cross-attention mechanisms to compute a recurrent function over a wide number of state vectors and input signals. Our architecture is inspired in part by the attention layer and long short-term memory (LSTM) cells, and it uses long-short style gates, but it scales this typical cell up by several orders to parallelize for multi-channel EEG signals. It inherits the advantages of attention layers and LSTM gate while avoiding their respective drawbacks. We evaluated the model effectiveness through extensive experiments with heterogeneou
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33647;&#29702;&#23398;&#21551;&#21457;&#30340;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#25968;&#23383;&#20581;&#24247;&#25968;&#25454;&#20013;&#30340;&#27835;&#30103;&#25928;&#26524;&#21644;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#21453;&#20107;&#23454;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03274</link><description>&lt;p&gt;
&#20174;&#22122;&#38899;&#21040;&#20449;&#21495;&#65306;&#36890;&#36807;&#33647;&#29702;&#23398;&#21551;&#21457;&#30340;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#35299;&#23494;&#25968;&#23383;&#20581;&#24247;&#25968;&#25454;&#20013;&#30340;&#27835;&#30103;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
From Noise to Signal: Unveiling Treatment Effects from Digital Health Data through Pharmacology-Informed Neural-SDE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03274
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33647;&#29702;&#23398;&#21551;&#21457;&#30340;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#25968;&#23383;&#20581;&#24247;&#25968;&#25454;&#20013;&#30340;&#27835;&#30103;&#25928;&#26524;&#21644;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#21453;&#20107;&#23454;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#20581;&#24247;&#25216;&#26415;&#65288;DHT&#65289;&#65292;&#22914;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#12289;&#25345;&#32493;&#12289;&#23454;&#26102;&#30417;&#27979;&#24739;&#32773;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#25216;&#26415;&#27491;&#22312;&#20026;&#26032;&#30103;&#27861;&#21644;&#20010;&#24615;&#21270;&#21307;&#23398;&#30340;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;&#20174;&#36825;&#20123;&#25216;&#26415;&#20013;&#33719;&#21462;&#27934;&#23519;&#21147;&#38656;&#35201;&#36866;&#24403;&#30340;&#24314;&#27169;&#25216;&#26415;&#26469;&#25429;&#25417;&#30142;&#30149;&#29366;&#24577;&#20013;&#30340;&#20020;&#24202;&#30456;&#20851;&#21464;&#21270;&#12290;&#36825;&#20123;&#35774;&#22791;&#20135;&#29983;&#30340;&#25968;&#25454;&#20855;&#26377;&#38543;&#26426;&#24615;&#65292;&#21487;&#33021;&#23384;&#22312;&#32570;&#22833;&#20803;&#32032;&#65292;&#24182;&#34920;&#29616;&#20986;&#30456;&#24403;&#22823;&#30340;&#20010;&#20307;&#38388;&#21464;&#24322;&#24615;&#65292;&#22240;&#27492;&#24456;&#38590;&#20351;&#29992;&#20256;&#32479;&#30340;&#32437;&#21521;&#24314;&#27169;&#25216;&#26415;&#36827;&#34892;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#26032;&#39062;&#33647;&#29702;&#23398;&#21551;&#21457;&#30340;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#27169;&#22411;&#12290;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035;&#27835;&#30103;&#25928;&#26524;&#21644;&#20174;&#38543;&#26426;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#21453;&#20107;&#23454;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03274v1 Announce Type: cross  Abstract: Digital health technologies (DHT), such as wearable devices, provide personalized, continuous, and real-time monitoring of patient. These technologies are contributing to the development of novel therapies and personalized medicine. Gaining insight from these technologies requires appropriate modeling techniques to capture clinically-relevant changes in disease state. The data generated from these devices is characterized by being stochastic in nature, may have missing elements, and exhibits considerable inter-individual variability - thereby making it difficult to analyze using traditional longitudinal modeling techniques. We present a novel pharmacology-informed neural stochastic differential equation (SDE) model capable of addressing these challenges. Using synthetic data, we demonstrate that our approach is effective in identifying treatment effects and learning causal relationships from stochastic data, thereby enabling counterfac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;ALPNet&#30340;&#20248;&#21183;&#21644;DINOv2&#30340;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23569;&#26679;&#26412;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#19981;&#20165;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#20063;&#20026;&#26356;&#22810;&#21019;&#26032;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.03273</link><description>&lt;p&gt;
&#22522;&#20110;DINOv2&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#23569;&#26679;&#26412;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
DINOv2 based Self Supervised Learning For Few Shot Medical Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;ALPNet&#30340;&#20248;&#21183;&#21644;DINOv2&#30340;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23569;&#26679;&#26412;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#19981;&#20165;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#20063;&#20026;&#26356;&#22810;&#21019;&#26032;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30707;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#22823;&#37327;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#65292;&#20854;&#36866;&#24212;&#24615;&#21040;&#26410;&#30693;&#31867;&#21035;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#23569;&#26679;&#26412;&#20998;&#21106;&#65288;FSS&#65289;&#36890;&#36807;&#36171;&#20104;&#27169;&#22411;&#20174;&#26377;&#38480;&#26631;&#35760;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#19968;&#31181;&#39046;&#20808;&#30340;FSS&#26041;&#27861;&#26159;ALPNet&#65292;&#23427;&#27604;&#36739;&#26597;&#35810;&#22270;&#20687;&#21644;&#23569;&#37327;&#21487;&#29992;&#25903;&#25345;&#20998;&#21106;&#22270;&#20687;&#20043;&#38388;&#30340;&#29305;&#24449;&#12290;&#20851;&#20110;&#20351;&#29992;ALPNet&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#22914;&#20309;&#35774;&#35745;&#20854;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20351;&#29992;DINOv2&#30340;&#29305;&#24449;&#30340;&#28508;&#21147;&#65292;DINOv2&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#31181;&#22522;&#30784;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#21033;&#29992;ALPNet&#30340;&#20248;&#21183;&#24182;&#21033;&#29992;DINOv2&#30340;&#29305;&#24449;&#25552;&#21462;&#21151;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23569;&#26679;&#26412;&#20998;&#21106;&#26041;&#27861;&#65292;&#19981;&#20165;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#36824;&#20026;&#26356;&#22810;&#21019;&#26032;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03273v1 Announce Type: cross  Abstract: Deep learning models have emerged as the cornerstone of medical image segmentation, but their efficacy hinges on the availability of extensive manually labeled datasets and their adaptability to unforeseen categories remains a challenge. Few-shot segmentation (FSS) offers a promising solution by endowing models with the capacity to learn novel classes from limited labeled examples. A leading method for FSS is ALPNet, which compares features between the query image and the few available support segmented images. A key question about using ALPNet is how to design its features. In this work, we delve into the potential of using features from DINOv2, which is a foundational self-supervised learning model in computer vision. Leveraging the strengths of ALPNet and harnessing the feature extraction capabilities of DINOv2, we present a novel approach to few-shot segmentation that not only enhances performance but also paves the way for more ro
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#22914;&#20309;&#24110;&#21161;&#21457;&#29616;&#24358;&#26223;&#35266;&#20013;&#19968;&#33268;&#24615;&#29702;&#35770;&#30340;&#26032;&#29305;&#24615;&#65292;&#20197;&#21450;&#20854;&#32479;&#35745;&#21487;&#23398;&#20064;&#24615;&#36136;&#12290;</title><link>https://arxiv.org/abs/2403.03245</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#19982;&#37327;&#23376;&#24341;&#21147;
&lt;/p&gt;
&lt;p&gt;
Neural Network Learning and Quantum Gravity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03245
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#22914;&#20309;&#24110;&#21161;&#21457;&#29616;&#24358;&#26223;&#35266;&#20013;&#19968;&#33268;&#24615;&#29702;&#35770;&#30340;&#26032;&#29305;&#24615;&#65292;&#20197;&#21450;&#20854;&#32479;&#35745;&#21487;&#23398;&#20064;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28304;&#33258;&#24358;&#35770;&#30340;&#20302;&#33021;&#26377;&#25928;&#22330;&#35770;&#39046;&#22495;&#22826;&#36807;&#24191;&#38420;&#65292;&#38590;&#20197;&#31995;&#32479;&#24615;&#22320;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#24358;&#26223;&#35266;&#30340;&#36825;&#29255;&#24191;&#34980;&#20043;&#22320;&#21487;&#33021;&#26159;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#32933;&#27779;&#22303;&#22756;&#12290;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21487;&#20197;&#25512;&#26029;&#20986;&#37027;&#20123;&#19968;&#33268;&#24615;&#29702;&#35770;&#24212;&#35813;&#20855;&#22791;&#30340;&#26032;&#39062;&#12289;&#26410;&#21457;&#29616;&#30340;&#29305;&#24615;&#65292;&#25110;&#26816;&#39564;&#20851;&#20110;&#20854;&#22768;&#31216;&#29305;&#24449;&#30340;&#29468;&#24819;&#24615;&#38472;&#36848;&#12290;&#26412;&#25991;&#26088;&#22312;&#25551;&#36848;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#33021;&#22815;&#25506;&#32034;&#24358;&#26223;&#35266;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21463;&#36817;&#26399;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#34920;&#26126;&#24358;&#26223;&#35266;&#20197;&#20854;&#24213;&#23618;&#28201;&#21644;&#30340;&#12289;O-&#26497;&#23567;&#32467;&#26500;&#32780;&#34920;&#29616;&#20986;&#26377;&#38480;&#24615;&#23646;&#24615;&#12290;&#23454;&#38469;&#19978;&#65292;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#35828;&#26126;&#24358;&#35770;&#30340;&#20219;&#20309;&#20302;&#33021;&#26377;&#25928;&#29702;&#35770;&#37117;&#20855;&#22791;&#26576;&#31181;&#32479;&#35745;&#21487;&#23398;&#20064;&#24615;&#36136;&#12290;&#22240;&#27492;&#65292;&#19968;&#20123;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03245v1 Announce Type: cross  Abstract: The landscape of low-energy effective field theories stemming from string theory is too vast for a systematic exploration. However, the meadows of the string landscape may be fertile ground for the application of machine learning techniques. Employing neural network learning may allow for inferring novel, undiscovered properties that consistent theories in the landscape should possess, or checking conjectural statements about alleged characteristics thereof. The aim of this work is to describe to what extent the string landscape can be explored with neural network-based learning. Our analysis is motivated by recent studies that show that the string landscape is characterized by finiteness properties, emerging from its underlying tame, o-minimal structures. Indeed, employing these results, we illustrate that any low-energy effective theory of string theory is endowed with certain statistical learnability properties. Consequently, severa
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#37325;/&#21435;&#20559;Lasso&#26041;&#27861;&#65292;&#29992;&#20110;&#32479;&#35745;&#25512;&#26029;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65292;&#19981;&#35201;&#27714;&#30452;&#25509;&#20551;&#35774;&#31232;&#30095;&#24615;&#65292;&#26377;&#25928;&#20272;&#35745;&#20102;&#32447;&#24615;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.03240</link><description>&lt;p&gt;
&#29992;&#20110;&#32479;&#35745;&#25512;&#26029;&#30340;&#19977;&#37325;/&#21435;&#20559;Lasso&#65306;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Triple/Debiased Lasso for Statistical Inference of Conditional Average Treatment Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03240
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#37325;/&#21435;&#20559;Lasso&#26041;&#27861;&#65292;&#29992;&#20110;&#32479;&#35745;&#25512;&#26029;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65292;&#19981;&#35201;&#27714;&#30452;&#25509;&#20551;&#35774;&#31232;&#30095;&#24615;&#65292;&#26377;&#25928;&#20272;&#35745;&#20102;&#32447;&#24615;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20851;&#20110;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;CATEs&#65289;&#30340;&#20272;&#35745;&#21644;&#32479;&#35745;&#25512;&#26029;&#65292;CATEs&#20316;&#20026;&#34920;&#31034;&#20010;&#24615;&#21270;&#22240;&#26524;&#25928;&#24212;&#30340;&#19968;&#20010;&#25351;&#26631;&#24050;&#32463;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#19982;&#20108;&#20540;&#22788;&#29702;&#30456;&#20851;&#32852;&#30340;&#32467;&#26524;&#37319;&#29992;&#32447;&#24615;&#27169;&#22411;&#65292;&#24182;&#23558;CATE&#23450;&#20041;&#20026;&#36825;&#20123;&#32447;&#24615;&#27169;&#22411;&#30340;&#39044;&#26399;&#32467;&#26524;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#26412;&#30740;&#31350;&#20801;&#35768;&#32447;&#24615;&#27169;&#22411;&#26159;&#39640;&#32500;&#30340;&#65292;&#25105;&#20204;&#30340;&#20852;&#36259;&#22312;&#20110;&#23545;CATE&#36827;&#34892;&#19968;&#33268;&#20272;&#35745;&#21644;&#32479;&#35745;&#25512;&#26029;&#12290;&#22312;&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#20013;&#65292;&#19968;&#31181;&#20856;&#22411;&#30340;&#26041;&#27861;&#26159;&#20551;&#35774;&#31232;&#30095;&#24615;&#12290;&#20294;&#26159;&#65292;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19981;&#30452;&#25509;&#20551;&#35774;&#31232;&#30095;&#24615;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20165;&#22312;&#32447;&#24615;&#27169;&#22411;&#30340;&#24046;&#24322;&#20013;&#32771;&#34385;&#31232;&#30095;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#26469;&#36817;&#20284;&#36825;&#31181;&#24046;&#24322;&#65292;&#28982;&#21518;&#29992;Lasso&#27491;&#21017;&#21270;&#23558;&#24046;&#24322;&#22238;&#24402;&#21040;&#21327;&#21464;&#37327;&#19978;&#12290;&#23613;&#31649;&#36825;&#31181;&#22238;&#24402;&#20272;&#35745;&#37327;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03240v1 Announce Type: cross  Abstract: This study investigates the estimation and the statistical inference about Conditional Average Treatment Effects (CATEs), which have garnered attention as a metric representing individualized causal effects. In our data-generating process, we assume linear models for the outcomes associated with binary treatments and define the CATE as a difference between the expected outcomes of these linear models. This study allows the linear models to be high-dimensional, and our interest lies in consistent estimation and statistical inference for the CATE. In high-dimensional linear regression, one typical approach is to assume sparsity. However, in our study, we do not assume sparsity directly. Instead, we consider sparsity only in the difference of the linear models. We first use a doubly robust estimator to approximate this difference and then regress the difference on covariates with Lasso regularization. Although this regression estimator is
&lt;/p&gt;</description></item><item><title>Caduceus &#26159;&#31532;&#19968;&#20010;&#25903;&#25345;&#21453;&#21521;&#20114;&#34917;&#24615;&#24182;&#20855;&#26377;&#21452;&#21521;&#24615;&#30340;&#38271;&#33539;&#22260; DNA &#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#65292;&#22312;&#19979;&#28216;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.03234</link><description>&lt;p&gt;
Caduceus: &#21452;&#21521;&#31561;&#21464;&#38271;&#33539;&#22260; DNA &#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03234
&lt;/p&gt;
&lt;p&gt;
Caduceus &#26159;&#31532;&#19968;&#20010;&#25903;&#25345;&#21453;&#21521;&#20114;&#34917;&#24615;&#24182;&#20855;&#26377;&#21452;&#21521;&#24615;&#30340;&#38271;&#33539;&#22260; DNA &#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#65292;&#22312;&#19979;&#28216;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#24207;&#21015;&#24314;&#27169;&#24341;&#21457;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#29616;&#22312;&#25193;&#23637;&#33267;&#29983;&#29289;&#23398;&#21644;&#22522;&#22240;&#32452;&#23398;&#12290;&#28982;&#32780;&#65292;&#24314;&#27169;&#22522;&#22240;&#32452;&#24207;&#21015;&#24341;&#20837;&#20102;&#25361;&#25112;&#65292;&#22914;&#38656;&#35201;&#24314;&#27169;&#38271;&#33539;&#22260;&#29255;&#27573;&#30456;&#20114;&#20316;&#29992;&#65292;&#22522;&#22240;&#32452;&#19978;&#28216;&#21644;&#19979;&#28216;&#21306;&#22495;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450; DNA &#30340;&#21453;&#21521;&#20114;&#34917;&#24615;&#65288;RC&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#36825;&#20123;&#25361;&#25112;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#23427;&#22522;&#20110;&#38271;&#33539;&#22260;&#30340; Mamba &#22359;&#65292;&#23558;&#20854;&#25193;&#23637;&#20026;&#25903;&#25345;&#21452;&#21521;&#24615;&#30340; BiMamba &#32452;&#20214;&#65292;&#20197;&#21450;&#25903;&#25345; RC &#31561;&#21464;&#24615;&#30340; MambaDNA &#22359;&#12290;&#25105;&#20204;&#20197; MambaDNA &#20026;&#22522;&#30784;&#65292;&#21019;&#36896;&#20102; Caduceus&#65292;&#31532;&#19968;&#20010;&#25903;&#25345; RC &#31561;&#21464;&#24615;&#30340;&#21452;&#21521;&#38271;&#33539;&#22260; DNA &#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#65292;&#29983;&#25104;&#20102; Caduceus DNA &#22522;&#30784;&#27169;&#22411;&#12290;Caduceus &#22312;&#19979;&#28216;&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#20197;&#21069;&#30340;&#38271;&#33539;&#22260;&#27169;&#22411;&#65307;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38271;&#33539;&#22260;&#21464;&#24322;&#25928;&#24212;&#39044;&#27979;&#20219;&#21153;&#19978;&#65292;Caduceus &#36229;&#36234;&#20102;&#35813;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03234v1 Announce Type: cross  Abstract: Large-scale sequence modeling has sparked rapid advances that now extend into biology and genomics. However, modeling genomic sequences introduces challenges such as the need to model long-range token interactions, the effects of upstream and downstream regions of the genome, and the reverse complementarity (RC) of DNA. Here, we propose an architecture motivated by these challenges that builds off the long-range Mamba block, and extends it to a BiMamba component that supports bi-directionality, and to a MambaDNA block that additionally supports RC equivariance. We use MambaDNA as the basis of Caduceus, the first family of RC equivariant bi-directional long-range DNA language models, and we introduce pre-training and fine-tuning strategies that yield Caduceus DNA foundation models. Caduceus outperforms previous long-range models on downstream benchmarks; on a challenging long-range variant effect prediction task, Caduceus exceeds the pe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#37327;&#21270;&#35745;&#31639;&#27169;&#22411;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#30340;&#20004;&#31181;&#26469;&#28304;&#65292;&#36890;&#36807;&#32467;&#21512;&#25968;&#25454;&#19968;&#33268;&#24615;&#21644;&#23398;&#20064;&#19981;&#30830;&#23450;&#37327;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#27979;&#37327;&#35823;&#24046;&#21644;&#20852;&#36259;&#25968;&#37327;&#26144;&#23556;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.03233</link><description>&lt;p&gt;
&#20174;&#20301;&#31227;&#21040;&#20998;&#24067;&#65306;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#35745;&#31639;&#27169;&#22411;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
From Displacements to Distributions: A Machine-Learning Enabled Framework for Quantifying Uncertainties in Parameters of Computational Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03233
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#37327;&#21270;&#35745;&#31639;&#27169;&#22411;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#30340;&#20004;&#31181;&#26469;&#28304;&#65292;&#36890;&#36807;&#32467;&#21512;&#25968;&#25454;&#19968;&#33268;&#24615;&#21644;&#23398;&#20064;&#19981;&#30830;&#23450;&#37327;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#27979;&#37327;&#35823;&#24046;&#21644;&#20852;&#36259;&#25968;&#37327;&#26144;&#23556;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#32467;&#21512;&#20004;&#31181;&#26694;&#26550;&#30340;&#26032;&#25193;&#23637;&#65292;&#29992;&#20110;&#37327;&#21270;&#24037;&#31243;&#31995;&#32479;&#24314;&#27169;&#20013;&#30340;&#28151;&#21512;&#19981;&#30830;&#23450;&#24615;&#28304;&#65292;&#21363;aleatoric&#65288;&#21363;&#19981;&#21487;&#20943;&#23569;&#30340;&#65289;&#21644;epistemic&#65288;&#21363;&#21487;&#20943;&#23569;&#30340;&#65289;&#12290;&#25968;&#25454;&#19968;&#33268;&#24615;&#65288;DC&#65289;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#20010;&#36870;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#37327;&#21270;&#20197;&#32473;&#23450;&#37327;&#21270;&#20852;&#36259;&#22270;&#23450;&#30340;&#22238;&#25289;&#21644;&#25512;&#36827;&#27979;&#24230;&#30340;aleatoric&#19981;&#30830;&#23450;&#24615;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#39044;&#20808;&#25351;&#23450;&#30340;&#20852;&#36259;&#25968;&#37327;&#26144;&#23556;&#24182;&#19981;&#24635;&#26159;&#22312;&#19982;&#31995;&#32479;&#36755;&#20986;&#30456;&#20851;&#30340;&#25968;&#25454;&#25910;&#38598;&#20043;&#21069;&#26159;&#21487;&#29992;&#30340;&#12290;&#25968;&#25454;&#26412;&#36523;&#32463;&#24120;&#21463;&#21040;&#27979;&#37327;&#35823;&#24046;&#65288;&#21363;epistemic&#19981;&#30830;&#23450;&#24615;&#65289;&#30340;&#27745;&#26579;&#65292;&#36825;&#20351;&#24471;&#25351;&#23450;&#19968;&#20010;&#26377;&#29992;&#30340;&#20852;&#36259;&#25968;&#37327;&#26144;&#23556;&#30340;&#36807;&#31243;&#21464;&#24471;&#22797;&#26434;&#12290;&#23398;&#20064;&#19981;&#30830;&#23450;&#37327;&#65288;LUQ&#65289;&#26694;&#26550;&#23450;&#20041;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#19977;&#27493;&#26426;&#22120;&#23398;&#20064;&#21551;&#29992;&#36807;&#31243;&#65292;&#29992;&#20110;&#23558;&#22024;&#26434;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#23398;&#20064;&#21040;&#30340;&#20852;&#36259;&#30340;&#26144;&#23556;&#26679;&#26412;&#65292;&#20197;&#21551;&#29992;&#22522;&#20110;DC&#30340;&#21453;&#28436;&#12290;&#25105;&#20204;&#22312;LUQ&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#36807;&#28388;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03233v1 Announce Type: cross  Abstract: This work presents novel extensions for combining two frameworks for quantifying both aleatoric (i.e., irreducible) and epistemic (i.e., reducible) sources of uncertainties in the modeling of engineered systems. The data-consistent (DC) framework poses an inverse problem and solution for quantifying aleatoric uncertainties in terms of pullback and push-forward measures for a given Quantity of Interest (QoI) map. Unfortunately, a pre-specified QoI map is not always available a priori to the collection of data associated with system outputs. The data themselves are often polluted with measurement errors (i.e., epistemic uncertainties), which complicates the process of specifying a useful QoI. The Learning Uncertain Quantities (LUQ) framework defines a formal three-step machine-learning enabled process for transforming noisy datasets into samples of a learned QoI map to enable DC-based inversion. We develop a robust filtering step in LUQ 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#24335;&#65292;&#29992;&#20110;&#33719;&#21462;&#32570;&#22833;&#30340;3D&#26579;&#33394;&#36136;&#30456;&#20114;&#20316;&#29992;&#21644;/&#25110;&#25552;&#39640;&#20998;&#36776;&#29575;&#30340;&#24773;&#20917;</title><link>https://arxiv.org/abs/2403.03231</link><description>&lt;p&gt;
&#29992;&#20110;&#39044;&#27979;3D&#22522;&#22240;&#32452;&#32452;&#32455;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine and deep learning methods for predicting 3D genome organization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03231
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#24335;&#65292;&#29992;&#20110;&#33719;&#21462;&#32570;&#22833;&#30340;3D&#26579;&#33394;&#36136;&#30456;&#20114;&#20316;&#29992;&#21644;/&#25110;&#25552;&#39640;&#20998;&#36776;&#29575;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#65288;3D&#65289;&#26579;&#33394;&#36136;&#30456;&#20114;&#20316;&#29992;&#65292;&#22914;&#22686;&#24378;&#23376;-&#21551;&#21160;&#23376;&#30456;&#20114;&#20316;&#29992;&#65288;EPIs&#65289;&#65292;&#29615;&#29366;&#26500;&#35937;&#22495;&#65288;loops&#65289;&#65292;&#25299;&#25169;&#30456;&#20851;&#22495;&#65288;TADs&#65289;&#21644;A/B&#21306;&#22495;&#36890;&#36807;&#35843;&#25511;&#22522;&#22240;&#34920;&#36798;&#22312;&#21508;&#31181;&#32454;&#32990;&#36807;&#31243;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#21457;&#23637;&#30340;&#26579;&#33394;&#36136;&#26500;&#35937;&#25429;&#33719;&#25216;&#26415;&#20351;&#24471;&#21363;&#20351;&#20351;&#29992;&#21333;&#20010;&#32454;&#32990;&#20063;&#33021;&#23545;&#21508;&#31181;3D&#32467;&#26500;&#36827;&#34892;&#20840;&#22522;&#22240;&#32452;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25216;&#26415;&#12289;&#24037;&#20855;&#21644;&#25968;&#25454;&#20998;&#36776;&#29575;&#20302;&#30340;&#24046;&#24322;&#65292;&#24403;&#21069;&#30340;3D&#32467;&#26500;&#30446;&#24405;&#20173;&#28982;&#19981;&#23436;&#25972;&#19988;&#19981;&#21487;&#38752;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#24335;&#20986;&#29616;&#65292;&#29992;&#20110;&#33719;&#21462;&#32570;&#22833;&#30340;3D&#30456;&#20114;&#20316;&#29992;&#21644;/&#25110;&#25913;&#21892;&#20998;&#36776;&#29575;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#22522;&#22240;&#32452;&#27880;&#37322;&#25968;&#25454;&#65288;ChIP-seq&#65292;DNAse-seq&#31561;&#65289;&#65292;DNA&#24207;&#21015;&#20449;&#24687;&#65288;k-mers&#65292;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#65288;TFBS&#65289;&#27169;&#20307;&#65289;&#21644;&#20854;&#20182;&#22522;&#22240;&#32452;&#29305;&#24615;&#26469;&#23398;&#20064;&#22522;&#22240;&#32452;&#29305;&#24449;&#19982;&#26579;&#33394;&#36136;&#30456;&#20114;&#20316;&#29992;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03231v1 Announce Type: cross  Abstract: Three-Dimensional (3D) chromatin interactions, such as enhancer-promoter interactions (EPIs), loops, Topologically Associating Domains (TADs), and A/B compartments play critical roles in a wide range of cellular processes by regulating gene expression. Recent development of chromatin conformation capture technologies has enabled genome-wide profiling of various 3D structures, even with single cells. However, current catalogs of 3D structures remain incomplete and unreliable due to differences in technology, tools, and low data resolution. Machine learning methods have emerged as an alternative to obtain missing 3D interactions and/or improve resolution. Such methods frequently use genome annotation data (ChIP-seq, DNAse-seq, etc.), DNA sequencing information (k-mers, Transcription Factor Binding Site (TFBS) motifs), and other genomic properties to learn the associations between genomic features and chromatin interactions. In this revie
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30417;&#30563;&#26641;&#26680;&#24378;&#21270;&#38598;&#25104;&#27169;&#22411;&#65292;&#39044;&#27979;&#21491;&#24515;&#23460;&#23481;&#31215;&#30340;&#20108;&#32500;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24471;&#20998;&#22686;&#24378;&#20102;&#39044;&#27979;&#34920;&#29616;&#65292;&#35813;&#26041;&#27861;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#27010;&#29575;&#21644;&#28857;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03229</link><description>&lt;p&gt;
&#25317;&#25265;&#19981;&#30830;&#23450;&#24615;&#28789;&#27963;&#24615;&#65306;&#21033;&#29992;&#30417;&#30563;&#26641;&#26680;&#24378;&#21270;&#38598;&#25104;&#27169;&#22411;&#65292;&#39044;&#27979;&#21491;&#24515;&#23460;&#23481;&#31215;&#30340;&#20108;&#32500;&#36229;&#22768;&#24515;&#21160;&#22270;
&lt;/p&gt;
&lt;p&gt;
Embracing Uncertainty Flexibility: Harnessing a Supervised Tree Kernel to Empower Ensemble Modelling for 2D Echocardiography-Based Prediction of Right Ventricular Volume
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03229
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30417;&#30563;&#26641;&#26680;&#24378;&#21270;&#38598;&#25104;&#27169;&#22411;&#65292;&#39044;&#27979;&#21491;&#24515;&#23460;&#23481;&#31215;&#30340;&#20108;&#32500;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24471;&#20998;&#22686;&#24378;&#20102;&#39044;&#27979;&#34920;&#29616;&#65292;&#35813;&#26041;&#27861;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#27010;&#29575;&#21644;&#28857;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21491;&#24515;&#23460;&#65288;RV&#65289;&#21151;&#33021;&#24694;&#21270;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#37117;&#33021;&#24378;&#21147;&#39044;&#27979;&#20020;&#24202;&#32467;&#26524;&#12290;&#20026;&#20102;&#22686;&#24378;&#20351;&#29992;&#24191;&#27867;&#21487;&#29992;&#30340;&#20108;&#32500;&#36229;&#22768;&#24515;&#21160;&#22270;&#65288;2DE&#65289;&#30340;&#34920;&#26684;&#25968;&#25454;&#37327;&#21270;RV&#23481;&#31215;&#30340;&#38598;&#25104;&#22238;&#24402;&#26041;&#27861;&#30340;&#20020;&#24202;&#37096;&#32626;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#20307;&#31215;&#39044;&#27979;&#19982;&#19981;&#30830;&#23450;&#24615;&#24471;&#20998;&#30456;&#32467;&#21512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#22522;&#20110;&#23454;&#20363;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#23398;&#20064;&#30340;&#26641;&#32467;&#26500;&#26469;&#35782;&#21035;&#30446;&#26631;&#23454;&#20363;&#21608;&#22260;&#30340;&#26368;&#36817;&#35757;&#32451;&#26679;&#26412;&#65292;&#28982;&#21518;&#20351;&#29992;&#22810;&#31181;&#20998;&#24067;&#31867;&#22411;&#26469;&#26356;&#28789;&#27963;&#22320;&#24314;&#27169;&#36755;&#20986;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#27010;&#29575;&#21644;&#28857;&#39044;&#27979;&#24615;&#33021;&#22312;&#19968;&#20010;&#30456;&#23545;&#23567;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;100&#20010;&#33298;&#24352;&#26411;&#21644;&#25910;&#32553;&#26411;RV&#23481;&#31215;&#12290;&#28857;&#24615;&#33021;&#30340;&#21442;&#32771;&#20540;&#26469;&#33258;MRI&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#28789;&#27963;&#26041;&#27861;&#22312;&#27010;&#29575;&#21644;&#28857;&#39044;&#27979;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03229v1 Announce Type: cross  Abstract: The right ventricular (RV) function deterioration strongly predicts clinical outcomes in numerous circumstances. To boost the clinical deployment of ensemble regression methods that quantify RV volumes using tabular data from the widely available two-dimensional echocardiography (2DE), we propose to complement the volume predictions with uncertainty scores. To this end, we employ an instance-based method which uses the learned tree structure to identify the nearest training samples to a target instance and then uses a number of distribution types to more flexibly model the output. The probabilistic and point-prediction performances of the proposed framework are evaluated on a relatively small-scale dataset, comprising 100 end-diastolic and end-systolic RV volumes. The reference values for point performance were obtained from MRI. The results demonstrate that our flexible approach yields improved probabilistic and point performances ove
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#23398;&#21338;&#24328;&#35770;&#27169;&#22411;&#29992;&#20110;&#30740;&#31350;&#29237;&#22763;&#21363;&#20852;&#28436;&#22863;&#65292;&#25506;&#32034;&#19981;&#21516;&#30340;&#38543;&#26426;&#21363;&#20852;&#31574;&#30053;&#21644;&#20854;&#22312;&#21363;&#20852;&#28436;&#22863;&#20013;&#30340;&#37197;&#23545;&#34920;&#29616;&#65292;&#21457;&#29616;&#26368;&#26377;&#25928;&#30340;&#31574;&#30053;&#23545;&#26159;&#36880;&#27493;&#25913;&#21464;&#21644;&#21644;&#24358;&#36319;&#38543;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.03224</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#29237;&#22763;&#21363;&#20852;&#28436;&#22863;&#65306;&#38899;&#20048;&#36935;&#19978;&#21338;&#24328;&#35770;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning Jazz Improvisation: When Music Meets Game Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03224
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#23398;&#21338;&#24328;&#35770;&#27169;&#22411;&#29992;&#20110;&#30740;&#31350;&#29237;&#22763;&#21363;&#20852;&#28436;&#22863;&#65292;&#25506;&#32034;&#19981;&#21516;&#30340;&#38543;&#26426;&#21363;&#20852;&#31574;&#30053;&#21644;&#20854;&#22312;&#21363;&#20852;&#28436;&#22863;&#20013;&#30340;&#37197;&#23545;&#34920;&#29616;&#65292;&#21457;&#29616;&#26368;&#26377;&#25928;&#30340;&#31574;&#30053;&#23545;&#26159;&#36880;&#27493;&#25913;&#21464;&#21644;&#21644;&#24358;&#36319;&#38543;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#30340;&#29616;&#22330;&#34920;&#28436;&#24635;&#26159;&#36855;&#20154;&#30340;&#65292;&#21363;&#20852;&#28436;&#22863;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#26159;&#30001;&#20110;&#38899;&#20048;&#23478;&#20043;&#38388;&#30340;&#21160;&#24577;&#20851;&#31995;&#21644;&#19982;&#35266;&#20247;&#30340;&#20114;&#21160;&#12290;&#29237;&#22763;&#21363;&#20852;&#28436;&#22863;&#26159;&#19968;&#20010;&#29305;&#21035;&#20540;&#24471;&#20174;&#29702;&#35770;&#35270;&#35282;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#20363;&#23376;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#23398;&#21338;&#24328;&#35770;&#27169;&#22411;&#26469;&#30740;&#31350;&#29237;&#22763;&#21363;&#20852;&#28436;&#22863;&#65292;&#20026;&#30740;&#31350;&#38899;&#20048;&#29702;&#35770;&#21644;&#21363;&#20852;&#28436;&#22863;&#26041;&#27861;&#23398;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;&#35745;&#31639;&#24314;&#27169;&#65292;&#20027;&#35201;&#26159;&#24378;&#21270;&#23398;&#20064;&#65292;&#26469;&#25506;&#32034;&#19981;&#21516;&#30340;&#38543;&#26426;&#21363;&#20852;&#31574;&#30053;&#21450;&#20854;&#22312;&#21363;&#20852;&#28436;&#22863;&#20013;&#30340;&#37197;&#23545;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26368;&#26377;&#25928;&#30340;&#31574;&#30053;&#23545;&#26159;&#19968;&#31181;&#23545;&#26368;&#36817;&#25910;&#30410;&#20316;&#20986;&#21453;&#24212;&#30340;&#31574;&#30053;&#65288;&#36880;&#27493;&#25913;&#21464;&#65289;&#65292;&#37197;&#21512;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#20854;&#20165;&#38480;&#20110;&#32473;&#23450;&#21644;&#24358;&#20013;&#30340;&#38899;&#31526;&#65288;&#21644;&#24358;&#36319;&#38543;&#24378;&#21270;&#23398;&#20064;&#65289;&#12290;&#30456;&#21453;&#65292;&#19968;&#31181;&#23545;&#20249;&#20276;&#30340;&#19978;&#19968;&#20010;&#38899;&#31526;&#20316;&#20986;&#21453;&#24212;&#65292;&#24182;&#35797;&#22270;&#19982;&#20043;&#21644;&#35856;&#30340;&#31574;&#30053;&#65288;&#21644;&#35856;P
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03224v1 Announce Type: cross  Abstract: Live performances of music are always charming, with the unpredictability of improvisation due to the dynamic between musicians and interactions with the audience. Jazz improvisation is a particularly noteworthy example for further investigation from a theoretical perspective. Here, we introduce a novel mathematical game theory model for jazz improvisation, providing a framework for studying music theory and improvisational methodologies. We use computational modeling, mainly reinforcement learning, to explore diverse stochastic improvisational strategies and their paired performance on improvisation. We find that the most effective strategy pair is a strategy that reacts to the most recent payoff (Stepwise Changes) with a reinforcement learning strategy limited to notes in the given chord (Chord-Following Reinforcement Learning). Conversely, a strategy that reacts to the partner's last note and attempts to harmonize with it (Harmony P
&lt;/p&gt;</description></item><item><title>&#31934;&#30830;&#25191;&#12175;&#26102;&#38388;&#36830;&#32493;&#24615;&#26159;&#26412;&#35770;&#12098;&#30340;&#12032;&#39033;&#37325;&#35201;&#21019;&#26032;&#65292;&#31616;&#21270;&#20102;&#35299;&#20915;&#26102;&#38388;&#30456;&#20851;&#38382;&#39064;&#21160;&#24577;&#12175;&#20026;&#39044;&#27979;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.03223</link><description>&lt;p&gt;
&#22312;&#39034;&#24207;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20013;&#31934;&#30830;&#25191;&#12175;&#26102;&#38388;&#36830;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exact Enforcement of Temporal Continuity in Sequential Physics-Informed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03223
&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#25191;&#12175;&#26102;&#38388;&#36830;&#32493;&#24615;&#26159;&#26412;&#35770;&#12098;&#30340;&#12032;&#39033;&#37325;&#35201;&#21019;&#26032;&#65292;&#31616;&#21270;&#20102;&#35299;&#20915;&#26102;&#38388;&#30456;&#20851;&#38382;&#39064;&#21160;&#24577;&#12175;&#20026;&#39044;&#27979;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#35745;&#31639;&#20013;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24212;&#12132;&#20195;&#34920;&#20102;&#12079;&#31243;&#38382;&#39064;&#35299;&#20915;&#12101;&#27861;&#30340;&#28508;&#22312;&#33539;&#24335;&#36716;&#21464;&#12290;&#26368;&#26174;&#33879;&#30340;&#21457;&#23637;&#20043;&#12032;&#26159;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#12153;&#32476;&#65288;PINNs&#65289;&#65292;&#20854;&#20013;&#31070;&#32463;&#12153;&#32476;&#34987;&#35757;&#32451;&#20197;&#28385;&#12188;&#20559;&#24494;&#20998;&#12101;&#31243;&#65288;PDEs&#65289;&#21644;/&#25110;&#35266;&#23519;&#25968;&#25454;&#12290;&#23613;&#31649;&#27492;&#12101;&#27861;&#26377;&#24076;&#26395;&#65292;&#20294;&#26631;&#20934;&#29256;&#26412;&#24050;&#34987;&#35777;&#26126;&#22312;&#20934;&#30830;&#39044;&#27979;&#26102;&#38388;&#30456;&#20851;&#38382;&#39064;&#30340;&#21160;&#24577;&#12175;&#20026;&#12101;&#12207;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#12032;&#25361;&#25112;&#65292;&#24050;&#32463;&#25552;&#20986;&#12101;&#27861;&#23558;&#26102;&#38388;&#22495;&#20998;&#35299;&#20026;&#22810;&#20010;&#27573;&#65292;&#27599;&#20010;&#27573;&#20013;&#20351;&#12132;&#12032;&#20010;&#19981;&#21516;&#30340;&#31070;&#32463;&#12153;&#32476;&#65292;&#24182;&#30452;&#25509;&#22312;&#26368;&#12073;&#21270;&#38382;&#39064;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#23558;&#23427;&#20204;&#20043;&#38388;&#30340;&#36830;&#32493;&#24615;&#32435;&#12042;&#20854;&#20013;&#12290;&#22312;&#26412;&#12079;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#12042;&#12032;&#31181;&#36890;&#36807;&#35299;&#26512;&#35299;&#31934;&#30830;&#24378;&#21046;&#23454;&#29616;&#36830;&#32493;&#24615;&#30340;&#12101;&#27861;&#12290;&#36825;&#31181;&#12101;&#27861;&#31616;&#21333;&#26131;&#12132;&#65292;&#33021;&#22815;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03223v1 Announce Type: new  Abstract: The use of deep learning methods in scientific computing represents a potential paradigm shift in engineering problem solving. One of the most prominent developments is Physics-Informed Neural Networks (PINNs), in which neural networks are trained to satisfy partial differential equations (PDEs) and/or observed data. While this method shows promise, the standard version has been shown to struggle in accurately predicting the dynamic behavior of time-dependent problems. To address this challenge, methods have been proposed that decompose the time domain into multiple segments, employing a distinct neural network in each segment and directly incorporating continuity between them in the loss function of the minimization problem. In this work we introduce a method to exactly enforce continuity between successive time segments via a solution ansatz. This hard constrained sequential PINN (HCS-PINN) method is simple to implement and eliminates 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#24341;&#23548;&#30340;EEG&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#24615;&#33021;&#21644;&#26174;&#33879;&#30340;&#21442;&#25968;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.03222</link><description>&lt;p&gt;
&#30693;&#35782;&#24341;&#23548;&#30340;EEG&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Knowledge-guided EEG Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03222
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#24341;&#23548;&#30340;EEG&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#24615;&#33021;&#21644;&#26174;&#33879;&#30340;&#21442;&#25968;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03222v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495; &#25688;&#35201;:&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#35821;&#38899;&#31561;&#22810;&#23186;&#20307;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;&#30001;&#20110;&#36825;&#20123;&#24773;&#26223;&#20013;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#36825;&#31181;&#33539;&#24335;&#23545;&#20110;&#29983;&#29289;&#20449;&#21495;&#39046;&#22495;&#21516;&#26679;&#37325;&#35201;&#65292;&#29978;&#33267;&#26356;&#37325;&#35201;&#12290;&#21033;&#29992;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#23398;&#20064;&#31283;&#20581;&#30340;&#34920;&#31034;&#33021;&#22815;&#24110;&#21161;&#25552;&#39640;&#29983;&#29289;&#20449;&#21495;&#19978;&#35768;&#22810;&#25512;&#26029;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#32771;&#34385;&#21040;&#22810;&#23186;&#20307;&#27169;&#24577;&#21644;&#29983;&#29289;&#20449;&#21495;&#20043;&#38388;&#22266;&#26377;&#30340;&#39046;&#22495;&#24046;&#24322;&#65292;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#24314;&#31435;&#30340;&#20256;&#32479;&#30446;&#26631;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#36716;&#21270;&#21040;&#36825;&#19968;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23558;&#36825;&#20123;&#26041;&#27861;&#35843;&#25972;&#21040;&#29983;&#29289;&#20449;&#21495;&#20998;&#26512;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#33258;&#30417;&#30563;EEG&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#24341;&#23548;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#26469;&#25552;&#20379;&#31283;&#20581;&#30340;&#24615;&#33021;&#21644;&#26174;&#33879;&#30340;&#21442;&#25968;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03222v1 Announce Type: cross  Abstract: Self-supervised learning has produced impressive results in multimedia domains of audio, vision and speech. This paradigm is equally, if not more, relevant for the domain of biosignals, owing to the scarcity of labelled data in such scenarios. The ability to leverage large-scale unlabelled data to learn robust representations could help improve the performance of numerous inference tasks on biosignals. Given the inherent domain differences between multimedia modalities and biosignals, the established objectives for self-supervised learning may not translate well to this domain. Hence, there is an unmet need to adapt these methods to biosignal analysis. In this work we propose a self-supervised model for EEG, which provides robust performance and remarkable parameter efficiency by using state space-based deep learning architecture. We also propose a novel knowledge-guided pre-training objective that accounts for the idiosyncrasies of th
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;FedHCDR&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#20013;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02630</link><description>&lt;p&gt;
FedHCDR: &#20855;&#26377;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal Decoupling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02630
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;FedHCDR&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#20013;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36328;&#39046;&#22495;&#25512;&#33616;&#65288;CDR&#65289;&#22791;&#21463;&#20851;&#27880;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#29992;&#25143;&#25968;&#25454;&#26469;&#22686;&#24378;&#25512;&#33616;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;CDR&#26041;&#27861;&#38656;&#35201;&#36328;&#39046;&#22495;&#20849;&#20139;&#29992;&#25143;&#25968;&#25454;&#65292;&#36829;&#21453;&#20102;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;&#65288;GDPR&#65289;&#12290;&#22240;&#27492;&#65292;&#24050;&#25552;&#20986;&#20102;&#35768;&#22810;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#65288;FedCDR&#65289;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#39046;&#22495;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#19981;&#21487;&#36991;&#20813;&#22320;&#24433;&#21709;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedHCDR&#65292;&#19968;&#31181;&#20855;&#26377;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#26032;&#22411;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#20026;&#20102;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#31216;&#20026;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#65288;HSD&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#25143;&#29305;&#24449;&#35299;&#32806;&#20026;&#39046;&#22495;&#29420;&#26377;&#21644;&#39046;&#22495;&#20849;&#20139;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#39640;&#36890;&#21644;&#20302;&#36890;&#36229;&#22270;&#28388;&#27874;&#22120;&#26469;&#36827;&#34892;&#35299;&#32806;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02630v1 Announce Type: new  Abstract: In recent years, Cross-Domain Recommendation (CDR) has drawn significant attention, which utilizes user data from multiple domains to enhance the recommendation performance. However, current CDR methods require sharing user data across domains, thereby violating the General Data Protection Regulation (GDPR). Consequently, numerous approaches have been proposed for Federated Cross-Domain Recommendation (FedCDR). Nevertheless, the data heterogeneity across different domains inevitably influences the overall performance of federated learning. In this study, we propose FedHCDR, a novel Federated Cross-Domain Recommendation framework with Hypergraph signal decoupling. Specifically, to address the data heterogeneity across domains, we introduce an approach called hypergraph signal decoupling (HSD) to decouple the user features into domain-exclusive and domain-shared features. The approach employs high-pass and low-pass hypergraph filters to de
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#20195;&#29702;&#22312;&#22810;&#26679;&#21270;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20855;&#26377;&#30701;&#35270;&#25506;&#32034;&#35774;&#35745;&#30340;&#36890;&#29992;&#31574;&#30053;&#20849;&#20139;&#31639;&#27861;&#21487;&#20197;&#22312;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.01636</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#39640;&#25928;&#30340;&#30701;&#35270;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01636
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#20195;&#29702;&#22312;&#22810;&#26679;&#21270;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20855;&#26377;&#30701;&#35270;&#25506;&#32034;&#35774;&#35745;&#30340;&#36890;&#29992;&#31574;&#30053;&#20849;&#20139;&#31639;&#27861;&#21487;&#20197;&#22312;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#65288;MTRL&#65289;&#26041;&#27861;&#22312;&#35768;&#22810;&#37325;&#35201;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20219;&#21153;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#20294;&#36817;&#26399;MTRL&#29702;&#35770;&#30340;&#36827;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#20551;&#35774;&#20219;&#21153;&#38388;&#20849;&#20139;&#32467;&#26500;&#26469;&#25552;&#39640;&#32479;&#35745;&#25928;&#29575;&#65292;&#23545;&#20110;RL&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#25506;&#32034;&#36825;&#19968;&#20851;&#38190;&#26041;&#38754;&#21364;&#22823;&#22810;&#34987;&#24573;&#35270;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#65292;&#24403;&#20195;&#29702;&#22312;&#36275;&#22815;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#20855;&#26377;&#30701;&#35270;&#25506;&#32034;&#35774;&#35745;&#65288;&#22914;$\epsilon$-&#36138;&#24515;&#65289;&#30340;&#36890;&#29992;&#31574;&#30053;&#20849;&#20139;&#31639;&#27861;&#21487;&#20197;&#22312;MTRL&#20013;&#20855;&#26377;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#20174;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#23545;&#8220;&#25506;&#32034;&#25910;&#30410;&#8221;&#22312;MTRL&#20013;&#30340;&#39318;&#27425;&#29702;&#35770;&#35777;&#26126;&#65292;&#20063;&#26377;&#21161;&#20110;&#35299;&#37322;&#30701;&#35270;&#25506;&#32034;&#22312;&#23454;&#36341;&#20013;&#24212;&#29992;&#24191;&#27867;&#30340;&#25104;&#21151;&#12290;&#20026;&#20102;&#39564;&#35777;&#22810;&#26679;&#24615;&#30340;&#20316;&#29992;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01636v1 Announce Type: cross  Abstract: Multitask Reinforcement Learning (MTRL) approaches have gained increasing attention for its wide applications in many important Reinforcement Learning (RL) tasks. However, while recent advancements in MTRL theory have focused on the improved statistical efficiency by assuming a shared structure across tasks, exploration--a crucial aspect of RL--has been largely overlooked. This paper addresses this gap by showing that when an agent is trained on a sufficiently diverse set of tasks, a generic policy-sharing algorithm with myopic exploration design like $\epsilon$-greedy that are inefficient in general can be sample-efficient for MTRL. To the best of our knowledge, this is the first theoretical demonstration of the "exploration benefits" of MTRL. It may also shed light on the enigmatic success of the wide applications of myopic exploration in practice. To validate the role of diversity, we conduct experiments on synthetic robotic control
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HOGA&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#30005;&#36335;&#20013;&#20197;&#21487;&#25193;&#23637;&#21644;&#36890;&#29992;&#30340;&#26041;&#24335;&#23398;&#20064;&#30005;&#36335;&#34920;&#31034;&#65292;&#36890;&#36807;&#36339;&#25968;&#29305;&#24449;&#21644;&#38376;&#25511;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#30005;&#36335;&#32467;&#26500;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#65292;&#24182;&#21487;&#20197;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.01317</link><description>&lt;p&gt;
&#23569;&#21363;&#26159;&#22810;&#65306;&#38754;&#21521;&#21487;&#25193;&#23637;&#21644;&#36890;&#29992;&#23398;&#20064;&#30340;&#36339;&#25968;&#22270;&#27880;&#24847;&#21147;&#22312;&#30005;&#36335;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Less is More: Hop-Wise Graph Attention for Scalable and Generalizable Learning on Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01317
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HOGA&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#30005;&#36335;&#20013;&#20197;&#21487;&#25193;&#23637;&#21644;&#36890;&#29992;&#30340;&#26041;&#24335;&#23398;&#20064;&#30005;&#36335;&#34920;&#31034;&#65292;&#36890;&#36807;&#36339;&#25968;&#29305;&#24449;&#21644;&#38376;&#25511;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#30005;&#36335;&#32467;&#26500;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#65292;&#24182;&#21487;&#20197;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#20219;&#21153;&#20013;&#23398;&#20064;&#30005;&#36335;&#34920;&#31034;&#26041;&#38754;&#21464;&#24471;&#27969;&#34892;&#65292;&#20294;&#24403;&#24212;&#29992;&#20110;&#22823;&#22270;&#26102;&#65292;&#23427;&#20204;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#24182;&#19988;&#23545;&#26032;&#35774;&#35745;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12290;&#36825;&#20123;&#38480;&#21046;&#20351;&#23427;&#20204;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#22797;&#26434;&#30005;&#36335;&#38382;&#39064;&#26102;&#19981;&#22826;&#23454;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HOGA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20197;&#21487;&#25193;&#23637;&#21644;&#36890;&#29992;&#30340;&#26041;&#24335;&#23398;&#20064;&#30005;&#36335;&#34920;&#31034;&#12290;HOGA&#39318;&#20808;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#38024;&#23545;&#27599;&#20010;&#33410;&#28857;&#35745;&#31639;&#36339;&#25968;&#29305;&#24449;&#12290;&#38543;&#21518;&#65292;&#36339;&#25968;&#29305;&#24449;&#20165;&#29992;&#20110;&#36890;&#36807;&#38376;&#25511;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#29983;&#25104;&#33410;&#28857;&#34920;&#31034;&#65292;&#35813;&#27169;&#22359;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#19981;&#21516;&#36339;&#25968;&#20043;&#38388;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#32780;&#19981;&#28041;&#21450;&#22270;&#25299;&#25169;&#12290;&#22240;&#27492;&#65292;HOGA&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30005;&#36335;&#20043;&#38388;&#30340;&#21508;&#31181;&#32467;&#26500;&#65292;&#24182;&#21487;&#20197;&#20197;&#20998;&#24067;&#24335;&#30340;&#26041;&#24335;&#39640;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01317v1 Announce Type: new  Abstract: While graph neural networks (GNNs) have gained popularity for learning circuit representations in various electronic design automation (EDA) tasks, they face challenges in scalability when applied to large graphs and exhibit limited generalizability to new designs. These limitations make them less practical for addressing large-scale, complex circuit problems. In this work we propose HOGA, a novel attention-based model for learning circuit representations in a scalable and generalizable manner. HOGA first computes hop-wise features per node prior to model training. Subsequently, the hop-wise features are solely used to produce node representations through a gated self-attention module, which adaptively learns important features among different hops without involving the graph topology. As a result, HOGA is adaptive to various structures across different circuits and can be efficiently trained in a distributed manner. To demonstrate the e
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;</title><link>https://arxiv.org/abs/2402.19379</link><description>&lt;p&gt;
&#30789;&#35895;&#20154;&#32676;&#30340;&#26234;&#24935;&#65306;LLM&#38598;&#25104;&#39044;&#27979;&#33021;&#21147;&#36798;&#21040;&#20154;&#32676;&#20934;&#30830;&#29575;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19379
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#20154;&#31867;&#39044;&#27979;&#20934;&#30830;&#24615;&#20381;&#36182;&#20110;&#8220;&#32676;&#20307;&#26234;&#24935;&#8221;&#25928;&#24212;&#65292;&#21363;&#36890;&#36807;&#32858;&#21512;&#19968;&#32676;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#39044;&#27979;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;&#12290;&#36807;&#21435;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39044;&#27979;&#33021;&#21147;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20316;&#20026;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#21069;&#27839;LLMs&#34920;&#29616;&#19981;&#20339;&#65292;&#19982;&#20154;&#31867;&#32676;&#20307;&#39044;&#27979;&#27604;&#36187;&#30340;&#40644;&#37329;&#26631;&#20934;&#30456;&#27604;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#30001;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;31&#20010;&#20108;&#20803;&#38382;&#39064;&#30340;&#32858;&#21512;LLM&#39044;&#27979;&#19982;&#19968;&#20010;&#26469;&#33258;&#19977;&#20010;&#26376;&#39044;&#27979;&#27604;&#36187;&#30340;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20998;&#26512;&#34920;&#26126;&#65292;LLM&#32676;&#20307;&#30340;&#34920;&#29616;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#19968;&#31181;&#39034;&#20174;&#25928;&#24212;&#65292;&#24179;&#22343;&#27169;&#22411;&#39044;&#27979;&#26126;&#26174;&#39640;&#20110;50%&#65292;&#23613;&#31649;&#20960;&#20046;&#26159;&#24179;&#31561;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19379v1 Announce Type: cross  Abstract: Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is statistically equivalent to the human crowd. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#40065;&#26834;&#24615;&#23646;&#24615;&#65292;&#26088;&#22312;&#25214;&#21040;&#20998;&#31867;&#22120;&#30340;&#26368;&#23567;&#20840;&#23616;&#40065;&#26834;&#36793;&#30028;&#65292;&#24182;&#24341;&#20837;&#20102;VHAGaR&#65292;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#27492;&#36793;&#30028;&#30340;&#39564;&#35777;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.19322</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20840;&#23616;&#40065;&#26834;&#24615;&#30340;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Verification of Neural Networks' Global Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19322
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#40065;&#26834;&#24615;&#23646;&#24615;&#65292;&#26088;&#22312;&#25214;&#21040;&#20998;&#31867;&#22120;&#30340;&#26368;&#23567;&#20840;&#23616;&#40065;&#26834;&#36793;&#30028;&#65292;&#24182;&#24341;&#20837;&#20102;VHAGaR&#65292;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#27492;&#36793;&#30028;&#30340;&#39564;&#35777;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#23637;&#31034;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#23433;&#20840;&#24615;&#65292;&#24341;&#20837;&#20102;&#35768;&#22810;&#39564;&#35777;&#22120;&#26469;&#25512;&#29702;&#32473;&#23450;&#36755;&#20837;&#23545;&#32473;&#23450;&#25200;&#21160;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#25104;&#21151;&#65292;&#23616;&#37096;&#40065;&#26834;&#24615;&#19981;&#33021;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#12290;&#19968;&#20123;&#24037;&#20316;&#20998;&#26512;&#20840;&#23616;&#40065;&#26834;&#24615;&#23646;&#24615;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#37117;&#26080;&#27861;&#25552;&#20379;&#32593;&#32476;&#20998;&#31867;&#22120;&#22312;&#19981;&#25913;&#21464;&#20998;&#31867;&#30340;&#24773;&#20917;&#19979;&#30340;&#31934;&#30830;&#20445;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#31867;&#22120;&#30340;&#26032;&#20840;&#23616;&#40065;&#26834;&#24615;&#23646;&#24615;&#65292;&#26088;&#22312;&#25214;&#21040;&#26368;&#23567;&#30340;&#20840;&#23616;&#31283;&#20581;&#36793;&#30028;&#65292;&#20854;&#33258;&#28982;&#22320;&#25193;&#23637;&#20102;&#29992;&#20110;&#20998;&#31867;&#22120;&#30340;&#27969;&#34892;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#23646;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;VHAGaR&#65292;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#27492;&#36793;&#30028;&#30340;&#21487;&#38543;&#26102;&#20351;&#29992;&#30340;&#39564;&#35777;&#22120;&#12290;VHAGaR&#20381;&#36182;&#20110;&#19977;&#20010;&#20027;&#35201;&#24605;&#24819;&#65306;&#23558;&#38382;&#39064;&#32534;&#30721;&#20026;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#30001;&#20110;&#27599;&#20010;&#23616;&#37096;&#26356;&#25913;&#32780;&#20135;&#29983;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#20462;&#21098;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19322v1 Announce Type: new  Abstract: Neural networks are successful in various applications but are also susceptible to adversarial attacks. To show the safety of network classifiers, many verifiers have been introduced to reason about the local robustness of a given input to a given perturbation. While successful, local robustness cannot generalize to unseen inputs. Several works analyze global robustness properties, however, neither can provide a precise guarantee about the cases where a network classifier does not change its classification. In this work, we propose a new global robustness property for classifiers aiming at finding the minimal globally robust bound, which naturally extends the popular local robustness property for classifiers. We introduce VHAGaR, an anytime verifier for computing this bound. VHAGaR relies on three main ideas: encoding the problem as a mixed-integer programming and pruning the search space by identifying dependencies stemming from the per
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#30450;&#21435;&#21367;&#31215;&#21644;&#20272;&#35745;&#24102;&#22122;&#22768;&#20108;&#38454;&#24490;&#29615;&#24179;&#31283;&#20449;&#21495;&#26102;&#38388;&#27874;&#24418;&#30340;&#21452;&#37325;&#38382;&#39064;&#65292;&#36890;&#36807;&#35777;&#26126;&#21435;&#21367;&#31215;&#28388;&#27874;&#22120;&#23384;&#22312;&#65292;&#28040;&#38500;&#20449;&#21495;&#30340;TF&#25928;&#24212;&#65292;&#23454;&#29616;&#39640;&#31934;&#24230;&#31639;&#27861;&#65292;&#26377;&#28508;&#21147;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.19290</link><description>&lt;p&gt;
&#20108;&#38454;&#24490;&#29615;&#24179;&#31283;&#20449;&#21495;&#20272;&#35745;&#21644;&#21435;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Estimation and Deconvolution of Second Order Cyclostationary Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19290
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#30450;&#21435;&#21367;&#31215;&#21644;&#20272;&#35745;&#24102;&#22122;&#22768;&#20108;&#38454;&#24490;&#29615;&#24179;&#31283;&#20449;&#21495;&#26102;&#38388;&#27874;&#24418;&#30340;&#21452;&#37325;&#38382;&#39064;&#65292;&#36890;&#36807;&#35777;&#26126;&#21435;&#21367;&#31215;&#28388;&#27874;&#22120;&#23384;&#22312;&#65292;&#28040;&#38500;&#20449;&#21495;&#30340;TF&#25928;&#24212;&#65292;&#23454;&#29616;&#39640;&#31934;&#24230;&#31639;&#27861;&#65292;&#26377;&#28508;&#21147;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#30450;&#21435;&#21367;&#31215;&#21644;&#20272;&#35745;&#36890;&#36807;&#20256;&#36755;&#20989;&#25968;(TF)&#20256;&#36755;&#21040;&#20256;&#24863;&#22120;&#30340;&#24102;&#22122;&#22768;&#20108;&#38454;&#24490;&#29615;&#24179;&#31283;(CS2)&#20449;&#21495;&#30340;&#26102;&#38388;&#27874;&#24418;&#30340;&#21452;&#37325;&#38382;&#39064;&#12290;&#25105;&#20204;&#24050;&#32463;&#35777;&#26126;&#65292;&#21435;&#21367;&#31215;&#28388;&#27874;&#22120;&#23384;&#22312;&#24182;&#28040;&#38500;&#20102;&#32479;&#35745;&#29305;&#24615;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#20449;&#21495;&#19978;&#30340;TF&#25928;&#24212;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#30450;&#30446;&#30340;&#65292;&#21363;&#19981;&#38656;&#35201;&#20851;&#20110;&#20449;&#21495;&#25110;TF&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;&#27169;&#25311;&#34920;&#26126;&#65292;&#31639;&#27861;&#22312;&#21508;&#31181;&#20449;&#21495;&#31867;&#22411;&#12289;TF&#21644;&#20449;&#22122;&#27604;(SNR)&#19979;&#20855;&#26377;&#39640;&#31934;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;CS2&#20449;&#21495;&#26063;&#34987;&#38480;&#21046;&#20026;&#30830;&#23450;&#24615;&#21608;&#26399;&#20989;&#25968;&#21644;&#30333;&#22122;&#22768;&#30340;&#20056;&#31215;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#28508;&#21147;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#20013;&#38656;&#35201;&#32858;&#21512;&#26469;&#33258;&#30456;&#21516;&#31995;&#32479;&#20294;&#20855;&#26377;&#19981;&#21516;TF&#30340;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19290v1 Announce Type: new  Abstract: This method solves the dual problem of blind deconvolution and estimation of the time waveform of noisy second-order cyclo-stationary (CS2) signals that traverse a Transfer Function (TF) en route to a sensor. We have proven that the deconvolution filter exists and eliminates the TF effect from signals whose statistics vary over time. This method is blind, meaning it does not require prior knowledge about the signals or TF. Simulations demonstrate the algorithm high precision across various signal types, TFs, and Signal-to-Noise Ratios (SNRs). In this study, the CS2 signals family is restricted to the product of a deterministic periodic function and white noise. Furthermore, this method has the potential to improve the training of Machine Learning models where the aggregation of signals from identical systems but with different TFs is required.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#21644;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#33258;&#21160;&#26500;&#24314;&#36866;&#29992;&#20110;&#26410;&#35265;&#20219;&#21153;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#39640;&#32500;&#35266;&#27979;&#31354;&#38388;&#20013;&#27867;&#21270;&#31574;&#30053;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.18759</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#24341;&#23548;&#30340;&#29366;&#24577;&#25277;&#35937;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning with Language-Guided State Abstractions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18759
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#21644;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#33258;&#21160;&#26500;&#24314;&#36866;&#29992;&#20110;&#26410;&#35265;&#20219;&#21153;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#39640;&#32500;&#35266;&#27979;&#31354;&#38388;&#20013;&#27867;&#21270;&#31574;&#30053;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#35774;&#35745;&#29366;&#24577;&#25277;&#35937;&#29992;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;&#22312;&#39640;&#32500;&#35266;&#27979;&#31354;&#38388;&#20013;&#23454;&#29616;&#27867;&#21270;&#31574;&#30053;&#23398;&#20064;&#30340;&#20851;&#38190;&#22312;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#36825;&#21487;&#20197;&#23558;&#29615;&#22659;&#20013;&#30340;&#37325;&#35201;&#29305;&#24449;&#23637;&#29616;&#20986;&#26469;&#24182;&#38544;&#34255;&#19981;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#36825;&#20123;&#29366;&#24577;&#34920;&#31034;&#36890;&#24120;&#26159;&#25163;&#21160;&#25351;&#23450;&#30340;&#65292;&#25110;&#32773;&#26159;&#20174;&#20854;&#20182;&#32321;&#37325;&#30340;&#26631;&#35760;&#36807;&#31243;&#20013;&#23548;&#20986;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;LGA&#65288;&#35821;&#35328;&#24341;&#23548;&#30340;&#25277;&#35937;&#65289;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#30693;&#35782;&#30340;&#32467;&#21512;&#33258;&#21160;&#26500;&#24314;&#36866;&#29992;&#20110;&#26410;&#35265;&#20219;&#21153;&#30340;&#29366;&#24577;&#34920;&#31034;&#12290;&#22312;LGA&#20013;&#65292;&#29992;&#25143;&#39318;&#20808;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#20379;&#30446;&#26631;&#20219;&#21153;&#30340;&#65288;&#21487;&#33021;&#26159;&#19981;&#23436;&#25972;&#30340;&#65289;&#25551;&#36848;&#65307;&#25509;&#19979;&#26469;&#65292;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#36825;&#20010;&#20219;&#21153;&#25551;&#36848;&#36716;&#21270;&#20026;&#25513;&#30422;&#19981;&#30456;&#20851;&#29305;&#24449;&#30340;&#29366;&#24577;&#25277;&#35937;&#20989;&#25968;&#65307;&#26368;&#21518;&#65292;&#20351;&#29992;&#23569;&#37327;&#28436;&#31034;&#25968;&#25454;&#35757;&#32451;&#19968;&#20010;&#27169;&#20223;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18759v1 Announce Type: cross  Abstract: We describe a framework for using natural language to design state abstractions for imitation learning. Generalizable policy learning in high-dimensional observation spaces is facilitated by well-designed state representations, which can surface important features of an environment and hide irrelevant ones. These state representations are typically manually specified, or derived from other labor-intensive labeling procedures. Our method, LGA (language-guided abstraction), uses a combination of natural language supervision and background knowledge from language models (LMs) to automatically build state representations tailored to unseen tasks. In LGA, a user first provides a (possibly incomplete) description of a target task in natural language; next, a pre-trained LM translates this task description into a state abstraction function that masks out irrelevant features; finally, an imitation policy is trained using a small number of demo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#27169;&#25311;&#19981;&#21516;&#20559;&#22909;&#37197;&#32622;&#65292;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.18571</link><description>&lt;p&gt;
&#29992;&#20110;&#28385;&#36275;&#22810;&#26679;&#29992;&#25143;&#20559;&#22909;&#30340;&#31639;&#26415;&#25511;&#21046;LLMs&#65306;&#20855;&#26377;&#22810;&#30446;&#26631;&#22870;&#21169;&#30340;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18571
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#27169;&#25311;&#19981;&#21516;&#20559;&#22909;&#37197;&#32622;&#65292;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31934;&#32454;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#36866;&#24212;&#21508;&#31181;&#29992;&#25143;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#24314;&#27169;&#26469;&#34920;&#31034;&#22810;&#26679;&#21270;&#30340;&#20559;&#22909;&#37197;&#32622;&#65292;&#23558;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#20026;&#22870;&#21169;&#31354;&#38388;&#20013;&#30340;&#26041;&#21521;&#65288;&#21363;&#21333;&#20301;&#21521;&#37327;&#65289;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18571v1 Announce Type: cross  Abstract: Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance
&lt;/p&gt;</description></item><item><title>SADM&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#24341;&#20837;&#32467;&#26500;&#25351;&#23548;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#27599;&#20010;&#35757;&#32451;&#25209;&#27425;&#20013;&#26679;&#26412;&#20043;&#38388;&#30340;&#27969;&#24418;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.17563</link><description>&lt;p&gt;
&#32467;&#26500;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Structure-Guided Adversarial Training of Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17563
&lt;/p&gt;
&lt;p&gt;
SADM&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#24341;&#20837;&#32467;&#26500;&#25351;&#23548;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#27599;&#20010;&#35757;&#32451;&#25209;&#27425;&#20013;&#26679;&#26412;&#20043;&#38388;&#30340;&#27969;&#24418;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#31181;&#29983;&#25104;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#21151;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#20027;&#35201;&#20391;&#37325;&#20110;&#26368;&#23567;&#21270;&#21152;&#26435;&#21435;&#22122;&#35780;&#20998;&#21305;&#37197;&#25439;&#22833;&#20197;&#36827;&#34892;&#25968;&#25454;&#20998;&#24067;&#24314;&#27169;&#30340;&#35757;&#32451;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32467;&#26500;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#23545;&#25239;&#35757;&#32451;&#65288;SADM&#65289;&#12290;&#22312;&#36825;&#31181;&#24320;&#21019;&#24615;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#24378;&#36843;&#27169;&#22411;&#22312;&#27599;&#20010;&#35757;&#32451;&#25209;&#27425;&#20013;&#23398;&#20064;&#26679;&#26412;&#20043;&#38388;&#30340;&#27969;&#24418;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17563v2 Announce Type: cross  Abstract: Diffusion models have demonstrated exceptional efficacy in various generative applications. While existing models focus on minimizing a weighted sum of denoising score matching losses for data distribution modeling, their training primarily emphasizes instance-level optimization, overlooking valuable structural information within each mini-batch, indicative of pair-wise relationships among samples. To address this limitation, we introduce Structure-guided Adversarial training of Diffusion Models (SADM). In this pioneering approach, we compel the model to learn manifold structures between samples in each training batch. To ensure the model captures authentic manifold structures in the data distribution, we advocate adversarial training of the diffusion generator against a novel structure discriminator in a minimax game, distinguishing real manifold structures from the generated ones. SADM substantially improves existing diffusion transf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#30452;&#25509;&#20998;&#26512;Bellman&#26368;&#20248;&#25439;&#22833;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#26377;&#30028;&#24615;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#31639;&#23376;&#30340;&#20998;&#35299;&#26041;&#27861;&#23454;&#29616;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2402.16899</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#24378;&#21270;&#23398;&#20064;&#20013;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#30340;\emph{&#20808;&#39564;&#20272;&#35745;}
&lt;/p&gt;
&lt;p&gt;
A prior Estimates for Deep Residual Network in Continuous-time Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#30452;&#25509;&#20998;&#26512;Bellman&#26368;&#20248;&#25439;&#22833;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#26377;&#30028;&#24615;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#31639;&#23376;&#30340;&#20998;&#35299;&#26041;&#27861;&#23454;&#29616;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#22823;&#35268;&#27169;&#23454;&#38469;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24615;&#33021;&#20998;&#26512;&#24573;&#30053;&#20102;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#26080;&#27861;&#30452;&#25509;&#20272;&#35745;Bellman&#26368;&#20248;&#25439;&#22833;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#19988;&#38656;&#35201;&#19968;&#20010;&#26377;&#30028;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#28385;&#36275;&#21322;&#32676;&#21644;Lipschitz&#24615;&#36136;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#22312;&#35813;&#26041;&#27861;&#19979;&#65292;&#25105;&#20204;&#33021;&#22815;&#30452;&#25509;&#20998;&#26512;Bellman&#26368;&#20248;&#25439;&#22833;&#30340;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#22312;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;&#20004;&#27425;&#36716;&#25442;&#12290;&#20026;&#20102;&#23436;&#25104;&#36716;&#25442;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#22823;&#31639;&#23376;&#30340;&#20998;&#35299;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#20998;&#26512;&#26041;&#27861;&#19981;&#38656;&#35201;&#26377;&#30028;&#24615;&#20551;&#35774;&#12290;&#26368;&#32456;&#25105;&#20204;&#32500;&#24471;&#21040;&#20102;&#19968;&#20010;&#27809;&#26377;&#8220;&#32500;&#24230;&#35781;&#21650;&#8221;&#30340;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16899v1 Announce Type: cross  Abstract: Deep reinforcement learning excels in numerous large-scale practical applications. However, existing performance analyses ignores the unique characteristics of continuous-time control problems, is unable to directly estimate the generalization error of the Bellman optimal loss and require a boundedness assumption. Our work focuses on continuous-time control problems and proposes a method that is applicable to all such problems where the transition function satisfies semi-group and Lipschitz properties. Under this method, we can directly analyze the \emph{a priori} generalization error of the Bellman optimal loss. The core of this method lies in two transformations of the loss function. To complete the transformation, we propose a decomposition method for the maximum operator. Additionally, this analysis method does not require a boundedness assumption. Finally, we obtain an \emph{a priori} generalization error without the curse of dime
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#29616;&#20840;&#36523;&#25511;&#21046;&#65288;Exbody&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20154;&#31867;&#22823;&#23567;&#30340;&#26426;&#22120;&#20154;&#19978;&#23398;&#20064;&#20840;&#36523;&#25511;&#21046;&#31574;&#30053;&#65292;&#20351;&#20854;&#33021;&#22815;&#27169;&#20223;&#20154;&#31867;&#21160;&#20316;&#65292;&#24182;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#21644;Sim2Real&#36716;&#31227;&#65292;&#23454;&#29616;&#25511;&#21046;&#20154;&#24418;&#26426;&#22120;&#20154;&#20197;&#19981;&#21516;&#39118;&#26684;&#34892;&#36208;&#12289;&#25671;&#26179;&#31561;&#12290;</title><link>https://arxiv.org/abs/2402.16796</link><description>&lt;p&gt;
&#20154;&#24418;&#26426;&#22120;&#20154;&#30340;&#34920;&#29616;&#20840;&#36523;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Expressive Whole-Body Control for Humanoid Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16796
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#29616;&#20840;&#36523;&#25511;&#21046;&#65288;Exbody&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20154;&#31867;&#22823;&#23567;&#30340;&#26426;&#22120;&#20154;&#19978;&#23398;&#20064;&#20840;&#36523;&#25511;&#21046;&#31574;&#30053;&#65292;&#20351;&#20854;&#33021;&#22815;&#27169;&#20223;&#20154;&#31867;&#21160;&#20316;&#65292;&#24182;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#21644;Sim2Real&#36716;&#31227;&#65292;&#23454;&#29616;&#25511;&#21046;&#20154;&#24418;&#26426;&#22120;&#20154;&#20197;&#19981;&#21516;&#39118;&#26684;&#34892;&#36208;&#12289;&#25671;&#26179;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#35753;&#20154;&#24418;&#26426;&#22120;&#20154;&#29983;&#25104;&#20016;&#23500;&#12289;&#22810;&#26679;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#21160;&#20316;&#12290;&#25105;&#20204;&#24314;&#35758;&#22312;&#19968;&#20010;&#20154;&#31867;&#22823;&#23567;&#30340;&#26426;&#22120;&#20154;&#19978;&#23398;&#20064;&#19968;&#20010;&#20840;&#36523;&#25511;&#21046;&#31574;&#30053;&#65292;&#20197;&#23613;&#21487;&#33021;&#36924;&#30495;&#22320;&#27169;&#20223;&#20154;&#31867;&#21160;&#20316;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#26679;&#19968;&#20010;&#31574;&#30053;&#65292;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#21033;&#29992;&#20102;&#26469;&#33258;&#22270;&#24418;&#23398;&#30028;&#30340;&#22823;&#35268;&#27169;&#20154;&#20307;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20351;&#29992;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#22312;&#30495;&#23454;&#20154;&#24418;&#26426;&#22120;&#20154;&#19978;&#24182;&#19981;&#22863;&#25928;&#65292;&#22240;&#20026;&#22312;&#33258;&#30001;&#24230;&#21644;&#29289;&#29702;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8212;&#8212;&#34920;&#29616;&#20840;&#36523;&#25511;&#21046;(Exbody)&#36890;&#36807;&#40723;&#21169;&#19978;&#21322;&#36523;&#27169;&#20223;&#21442;&#32771;&#36816;&#21160;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#26494;&#24320;&#23545;&#20854;&#20004;&#26465;&#33151;&#30340;&#27169;&#20223;&#32422;&#26463;&#65292;&#21482;&#35201;&#27714;&#23427;&#20204;&#31283;&#22266;&#22320;&#36981;&#24490;&#32473;&#23450;&#30340;&#36895;&#24230;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#21644;Sim2Real&#36716;&#31227;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#33021;&#22815;&#25511;&#21046;&#20154;&#24418;&#26426;&#22120;&#20154;&#20197;&#19981;&#21516;&#39118;&#26684;&#34892;&#36208;&#65292;&#25671;&#26179;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16796v2 Announce Type: replace-cross  Abstract: Can we enable humanoid robots to generate rich, diverse, and expressive motions in the real world? We propose to learn a whole-body control policy on a human-sized robot to mimic human motions as realistic as possible. To train such a policy, we leverage the large-scale human motion capture data from the graphics community in a Reinforcement Learning framework. However, directly performing imitation learning with the motion capture dataset would not work on the real humanoid robot, given the large gap in degrees of freedom and physical capabilities. Our method Expressive Whole-Body Control (Exbody) tackles this problem by encouraging the upper humanoid body to imitate a reference motion, while relaxing the imitation constraint on its two legs and only requiring them to follow a given velocity robustly. With training in simulation and Sim2Real transfer, our policy can control a humanoid robot to walk in different styles, shake h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#21487;&#22312;&#31232;&#30095;&#31283;&#20581;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#19968;&#32500;&#23376;&#31354;&#38388;&#65292;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#26494;&#24347;&#26041;&#27861;&#21644;&#26032;&#39062;&#30340;&#25311;&#21512;&#31243;&#24207;&#65292;&#23454;&#29616;&#20102;&#20840;&#23616;&#26368;&#20248;&#30340;&#31283;&#20581;&#31232;&#30095;&#23376;&#31354;&#38388;&#65292;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#25928;&#29575;&#19988;&#21487;&#25193;&#23637;&#24615;&#24378;&#12290;</title><link>https://arxiv.org/abs/2402.16712</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#31283;&#20581;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Scalable Robust Sparse Principal Component Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#21487;&#22312;&#31232;&#30095;&#31283;&#20581;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#19968;&#32500;&#23376;&#31354;&#38388;&#65292;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#26494;&#24347;&#26041;&#27861;&#21644;&#26032;&#39062;&#30340;&#25311;&#21512;&#31243;&#24207;&#65292;&#23454;&#29616;&#20102;&#20840;&#23616;&#26368;&#20248;&#30340;&#31283;&#20581;&#31232;&#30095;&#23376;&#31354;&#38388;&#65292;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#25928;&#29575;&#19988;&#21487;&#25193;&#23637;&#24615;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#26469;&#20272;&#35745;&#31232;&#30095;&#31283;&#20581;&#30340;&#19968;&#32500;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#34920;&#31034;&#35823;&#24046;&#21644;l1&#33539;&#25968;&#20934;&#21017;&#19979;&#30340;&#24809;&#32602;&#12290;&#37492;&#20110;&#38382;&#39064;&#26159;NP&#38590;&#30340;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#26494;&#24347;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31616;&#21333;&#27604;&#20363;&#21644;&#25490;&#24207;&#25216;&#26415;&#30340;&#26032;&#22411;&#25311;&#21512;&#31243;&#24207;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#23637;&#31034;&#20102;$O(n^2 m \log n)$&#30340;&#26368;&#22351;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#31232;&#30095;&#31283;&#20581;&#23376;&#31354;&#38388;&#30340;&#20840;&#23616;&#26368;&#20248;&#65292;&#20174;&#32780;&#23637;&#31034;&#20102;&#22810;&#39033;&#24335;&#26102;&#38388;&#25928;&#29575;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#25214;&#21040;&#20855;&#26377;&#26368;&#20302;&#19981;&#19968;&#33268;&#24615;&#30340;&#23376;&#31354;&#38388;&#65292;&#25552;&#20379;&#20102;&#22312;&#31232;&#30095;&#24615;&#21644;&#25311;&#21512;&#20043;&#38388;&#26356;&#24179;&#28369;&#30340;&#26435;&#34913;&#12290;&#20854;&#26550;&#26500;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#23545;&#20110;2000x2000&#30340;&#30697;&#38453;&#65292;&#35745;&#31639;&#36895;&#24230;&#30456;&#36739;CPU&#29256;&#26412;&#25552;&#21319;&#20102;16&#20493;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16712v1 Announce Type: new  Abstract: In this work, we propose an optimization framework for estimating a sparse robust one-dimensional subspace. Our objective is to minimize both the representation error and the penalty, in terms of the l1-norm criterion. Given that the problem is NP-hard, we introduce a linear relaxation-based approach. Additionally, we present a novel fitting procedure, utilizing simple ratios and sorting techniques. The proposed algorithm demonstrates a worst-case time complexity of $O(n^2 m \log n)$ and, in certain instances, achieves global optimality for the sparse robust subspace, thereby exhibiting polynomial time efficiency. Compared to extant methodologies, the proposed algorithm finds the subspace with the lowest discordance, offering a smoother trade-off between sparsity and fit. Its architecture affords scalability, evidenced by a 16-fold improvement in computational speeds for matrices of 2000x2000 over CPU version. Furthermore, this method is
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#19978;&#19979;&#25991;&#21270;&#25193;&#25955;&#27169;&#22411;&#65288;ContextDiff&#65289;&#65292;&#36890;&#36807;&#22312;&#27491;&#21521;&#21644;&#36870;&#21521;&#36807;&#31243;&#20013;&#34701;&#20837;&#25991;&#26412;&#26465;&#20214;&#21644;&#35270;&#35273;&#26679;&#26412;&#20043;&#38388;&#30340;&#20132;&#20114;&#21644;&#23545;&#40784;&#65292;&#20197;&#20415;&#22312;&#35270;&#35273;&#29983;&#25104;&#20013;&#26356;&#20934;&#30830;&#22320;&#20256;&#36798;&#25991;&#26412;&#35821;&#20041;</title><link>https://arxiv.org/abs/2402.16627</link><description>&lt;p&gt;
&#25991;&#26412;&#24341;&#23548;&#19979;&#30340;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#35270;&#35273;&#29983;&#25104;&#19982;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16627
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#19978;&#19979;&#25991;&#21270;&#25193;&#25955;&#27169;&#22411;&#65288;ContextDiff&#65289;&#65292;&#36890;&#36807;&#22312;&#27491;&#21521;&#21644;&#36870;&#21521;&#36807;&#31243;&#20013;&#34701;&#20837;&#25991;&#26412;&#26465;&#20214;&#21644;&#35270;&#35273;&#26679;&#26412;&#20043;&#38388;&#30340;&#20132;&#20114;&#21644;&#23545;&#40784;&#65292;&#20197;&#20415;&#22312;&#35270;&#35273;&#29983;&#25104;&#20013;&#26356;&#20934;&#30830;&#22320;&#20256;&#36798;&#25991;&#26412;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#39640;&#20445;&#30495;&#24230;&#25991;&#26412;&#24341;&#23548;&#30340;&#35270;&#35273;&#29983;&#25104;&#21644;&#32534;&#36753;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25991;&#26412;&#24341;&#23548;&#35270;&#35273;&#25193;&#25955;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#20110;&#23558;&#25991;&#26412;-&#35270;&#35273;&#20851;&#31995;&#29420;&#21344;&#22320;&#34701;&#20837;&#21040;&#36870;&#36807;&#31243;&#20013;&#65292;&#24448;&#24448;&#24573;&#30053;&#20102;&#23427;&#20204;&#22312;&#27491;&#21521;&#36807;&#31243;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#27491;&#21453;&#36807;&#31243;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#21487;&#33021;&#38480;&#21046;&#20102;&#22312;&#35270;&#35273;&#21512;&#25104;&#32467;&#26524;&#20013;&#31934;&#30830;&#20256;&#36798;&#25991;&#26412;&#35821;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#19978;&#19979;&#25991;&#21270;&#25193;&#25955;&#27169;&#22411;&#65288;ContextDiff&#65289;&#65292;&#36890;&#36807;&#23558;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#21253;&#21547;&#25991;&#26412;&#26465;&#20214;&#21644;&#35270;&#35273;&#26679;&#26412;&#20043;&#38388;&#30340;&#20132;&#20114;&#21644;&#23545;&#40784;&#34701;&#20837;&#21040;&#27491;&#21521;&#21644;&#36870;&#21521;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#19978;&#19979;&#25991;&#20256;&#25773;&#21040;&#20004;&#20010;&#36807;&#31243;&#20013;&#30340;&#25152;&#26377;&#26102;&#38388;&#27493;&#65292;&#20197;&#35843;&#25972;&#23427;&#20204;&#30340;&#36712;&#36857;&#65292;&#20174;&#32780;&#20419;&#36827;&#36328;&#27169;&#24577;&#26465;&#20214;&#24314;&#27169;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#19978;&#19979;&#25991;&#21270;&#25193;&#25955;&#25512;&#24191;&#21040;DDPMs&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16627v2 Announce Type: cross  Abstract: Conditional diffusion models have exhibited superior performance in high-fidelity text-guided visual generation and editing. Nevertheless, prevailing text-guided visual diffusion models primarily focus on incorporating text-visual relationships exclusively into the reverse process, often disregarding their relevance in the forward process. This inconsistency between forward and reverse processes may limit the precise conveyance of textual semantics in visual synthesis results. To address this issue, we propose a novel and general contextualized diffusion model (ContextDiff) by incorporating the cross-modal context encompassing interactions and alignments between text condition and visual sample into forward and reverse processes. We propagate this context to all timesteps in the two processes to adapt their trajectories, thereby facilitating cross-modal conditional modeling. We generalize our contextualized diffusion to both DDPMs and 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#39044;&#35745;&#31639;&#30340;&#23884;&#20837;&#30456;&#20284;&#24615;&#29983;&#25104;&#20010;&#24615;&#21270;&#20449;&#24687;&#27969;&#65292;&#25552;&#39640;&#20102;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#23458;&#25143;&#21442;&#19982;&#24230;&#21644;&#20307;&#39564;&#65292;&#36716;&#21270;&#29575;&#25552;&#21319;4.9&#65285;&#12290;</title><link>https://arxiv.org/abs/2402.16073</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35745;&#31639;&#30340;&#23884;&#20837;&#30456;&#20284;&#24615;&#29983;&#25104;&#20960;&#20046;&#23454;&#26102;&#20010;&#24615;&#21270;&#20449;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
Pfeed: Generating near real-time personalized feeds using precomputed embedding similarities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16073
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39044;&#35745;&#31639;&#30340;&#23884;&#20837;&#30456;&#20284;&#24615;&#29983;&#25104;&#20010;&#24615;&#21270;&#20449;&#24687;&#27969;&#65292;&#25552;&#39640;&#20102;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#23458;&#25143;&#21442;&#19982;&#24230;&#21644;&#20307;&#39564;&#65292;&#36716;&#21270;&#29575;&#25552;&#21319;4.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#23884;&#20837;&#26469;&#32534;&#30721;&#29992;&#25143;&#21160;&#20316;&#21644;&#39033;&#30446;&#65292;&#28982;&#21518;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#26816;&#32034;&#65292;&#20351;&#29992;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#20004;&#20010;&#25361;&#25112;&#65306;1&#65289;&#29992;&#25143;&#23884;&#20837;&#21487;&#33021;&#38480;&#21046;&#25152;&#25429;&#33719;&#30340;&#20852;&#36259;&#22810;&#26679;&#24615;&#65292;2&#65289;&#20445;&#25345;&#23427;&#20204;&#26368;&#26032;&#38656;&#35201;&#26114;&#36149;&#30340;&#23454;&#26102;&#22522;&#30784;&#35774;&#26045;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#38469;&#24037;&#19994;&#29615;&#22659;&#20013;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21160;&#24577;&#26356;&#26032;&#23458;&#25143;&#37197;&#32622;&#25991;&#20214;&#65292;&#24182;&#27599;&#20004;&#20998;&#38047;&#32452;&#25104;&#19968;&#20010;&#20449;&#24687;&#27969;&#65292;&#21033;&#29992;&#39044;&#35745;&#31639;&#30340;&#23884;&#20837;&#21450;&#20854;&#21508;&#33258;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#22312;&#33655;&#20848;&#21644;&#27604;&#21033;&#26102;&#26368;&#22823;&#30340;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20043;&#19968;Bol&#19978;&#27979;&#35797;&#24182;&#37096;&#32626;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#23458;&#25143;&#21442;&#19982;&#24230;&#21644;&#20307;&#39564;&#65292;&#23548;&#33268;&#36716;&#21270;&#29575;&#26174;&#33879;&#25552;&#39640;&#20102;4.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16073v1 Announce Type: cross  Abstract: In personalized recommender systems, embeddings are often used to encode customer actions and items, and retrieval is then performed in the embedding space using approximate nearest neighbor search. However, this approach can lead to two challenges: 1) user embeddings can restrict the diversity of interests captured and 2) the need to keep them up-to-date requires an expensive, real-time infrastructure. In this paper, we propose a method that overcomes these challenges in a practical, industrial setting. The method dynamically updates customer profiles and composes a feed every two minutes, employing precomputed embeddings and their respective similarities. We tested and deployed this method to personalise promotional items at Bol, one of the largest e-commerce platforms of the Netherlands and Belgium. The method enhanced customer engagement and experience, leading to a significant 4.9% uplift in conversions.
&lt;/p&gt;</description></item><item><title>Brant-2&#26159;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;Brant&#65292;&#23427;&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36824;&#33021;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.10251</link><description>&lt;p&gt;
Brant-2&#65306;&#33041;&#20449;&#21495;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Brant-2: Foundation Model for Brain Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10251
&lt;/p&gt;
&lt;p&gt;
Brant-2&#26159;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;Brant&#65292;&#23427;&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36824;&#33021;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#21463;&#30410;&#20110;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#31181;&#27169;&#22411;&#22312;&#20998;&#26512;&#33041;&#20449;&#21495;&#26041;&#38754;&#29305;&#21035;&#26377;&#25928;&#65292;&#22240;&#20026;&#36825;&#19968;&#39046;&#22495;&#28085;&#30422;&#20102;&#20247;&#22810;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#19988;&#36827;&#34892;&#22823;&#35268;&#27169;&#27880;&#37322;&#26159;&#25104;&#26412;&#39640;&#26114;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;Brant-2&#12290;&#19982;&#29992;&#20110;&#39045;&#20869;&#31070;&#32463;&#20449;&#21495;&#30340;&#22522;&#30784;&#27169;&#22411;Brant&#30456;&#27604;&#65292;Brant-2&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#34920;&#29616;&#20986;&#31283;&#20581;&#24615;&#65292;&#32780;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#22823;&#37327;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Brant-2&#23545;&#33041;&#20449;&#21495;&#20013;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#30340;&#36866;&#24212;&#24615;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#25581;&#31034;&#20102;Brant-2&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#39564;&#35777;&#20102;&#27599;&#20010;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#20445;&#25345;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10251v1 Announce Type: cross  Abstract: Foundational models benefit from pre-training on large amounts of unlabeled data and enable strong performance in a wide variety of applications with a small amount of labeled data. Such models can be particularly effective in analyzing brain signals, as this field encompasses numerous application scenarios, and it is costly to perform large-scale annotation. In this work, we present the largest foundation model in brain signals, Brant-2. Compared to Brant, a foundation model designed for intracranial neural signals, Brant-2 not only exhibits robustness towards data variations and modeling scales but also can be applied to a broader range of brain neural data. By experimenting on an extensive range of tasks, we demonstrate that Brant-2 is adaptive to various application scenarios in brain signals. Further analyses reveal the scalability of the Brant-2, validate each component's effectiveness, and showcase our model's ability to maintai
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#20132;&#20114;&#22270;&#19978;&#36827;&#34892;&#25552;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#39044;&#27979;&#38454;&#27573;&#25152;&#38754;&#20020;&#30340;&#26102;&#38388;&#24046;&#24322;&#21644;&#35821;&#20041;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06326</link><description>&lt;p&gt;
&#26102;&#38388;&#20132;&#20114;&#22270;&#19978;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompt Learning on Temporal Interaction Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06326
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#20132;&#20114;&#22270;&#19978;&#36827;&#34892;&#25552;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#39044;&#27979;&#38454;&#27573;&#25152;&#38754;&#20020;&#30340;&#26102;&#38388;&#24046;&#24322;&#21644;&#35821;&#20041;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#20132;&#20114;&#22270;(TIGs)&#34987;&#24191;&#27867;&#29992;&#20110;&#34920;&#31034;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#12290;&#20026;&#20102;&#20419;&#36827;&#22312;TIGs&#19978;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;TIG&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#8220;&#39044;&#35757;&#32451;&#65292;&#39044;&#27979;&#8221;&#35757;&#32451;&#33539;&#24335;&#20013;&#20381;&#28982;&#38754;&#20020;&#30528;&#20004;&#20010;&#38590;&#39064;&#12290;&#39318;&#20808;&#65292;&#39044;&#35757;&#32451;&#21644;&#25512;&#29702;&#25968;&#25454;&#20043;&#38388;&#30340;&#26102;&#38388;&#24046;&#24322;&#20005;&#37325;&#21066;&#24369;&#20102;&#27169;&#22411;&#22312;&#21160;&#24577;&#28436;&#21270;&#25968;&#25454;&#19978;&#36827;&#34892;&#36965;&#36828;&#26410;&#26469;&#39044;&#27979;&#30340;&#36866;&#29992;&#24615;&#12290;&#20854;&#27425;&#65292;&#39044;&#25991;&#26412;&#20219;&#21153;&#21644;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#24212;&#29992;&#22330;&#26223;&#20013;&#24456;&#38590;&#23545;&#40784;&#20854;&#23398;&#20064;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Interaction Graphs (TIGs) are widely utilized to represent real-world systems. To facilitate representation learning on TIGs, researchers have proposed a series of TIG models. However, these models are still facing two tough gaps between the pre-training and downstream predictions in their ``pre-train, predict'' training paradigm. First, the temporal discrepancy between the pre-training and inference data severely undermines the models' applicability in distant future predictions on the dynamically evolving data. Second, the semantic divergence between pretext and downstream tasks hinders their practical applications, as they struggle to align with their learning and prediction capabilities across application scenarios.   Recently, the ``pre-train, prompt'' paradigm has emerged as a lightweight mechanism for model generalization. Applying this paradigm is a potential solution to solve the aforementioned challenges. However, the adaptation of this paradigm to TIGs is not straig
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#22686;&#24378;&#24418;&#24335;&#30340;&#20219;&#21153;&#25351;&#23548;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#24182;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)</title><link>https://arxiv.org/abs/2402.04154</link><description>&lt;p&gt;
&#12298;&#35835;&#29609;&#28216;&#25103;&#65288;R2-Play&#65289;: &#22810;&#27169;&#24577;&#28216;&#25103;&#25351;&#23548;&#19979;&#30340;&#20915;&#31574; Transformer&#12299;
&lt;/p&gt;
&lt;p&gt;
Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#22686;&#24378;&#24418;&#24335;&#30340;&#20219;&#21153;&#25351;&#23548;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#24182;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#24320;&#21457;&#19968;&#27454;&#36890;&#29992;&#26234;&#33021;&#20307;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#30446;&#26631;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#20219;&#21153;&#30340;&#22823;&#37327;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#20219;&#21153;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#22312;&#25193;&#23637;&#21040;&#26032;&#20219;&#21153;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23558;&#25991;&#26412;&#25351;&#23548;&#25110;&#35270;&#35273;&#36712;&#36857;&#25972;&#21512;&#21040;&#20915;&#31574;&#32593;&#32476;&#20013;&#65292;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#20195;&#34920;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#35266;&#23519;&#21040;&#20165;&#20381;&#36182;&#20110;&#25991;&#26412;&#25351;&#23548;&#25110;&#35270;&#35273;&#36712;&#36857;&#23545;&#20110;&#20934;&#30830;&#20256;&#36798;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#22686;&#24378;&#26234;&#33021;&#20307;&#20219;&#21153;&#25351;&#23548;&#30340;&#24418;&#24335;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#65292;&#20174;&#32780;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#35270;&#35273;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#38271;&#26399;&#35270;&#35273;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)
&lt;/p&gt;
&lt;p&gt;
Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.However, these works encounter challenges in extending their capabilities to new tasks.Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a "read-to-play" capability.Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set 
&lt;/p&gt;</description></item><item><title>Swin-UMamba&#26159;&#19968;&#31181;&#20197;Mamba&#20026;&#22522;&#30784;&#30340;&#26032;&#22411;UNet&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#29305;&#24449;&#21644;&#20840;&#23616;&#20381;&#36182;&#30340;&#22810;&#23610;&#24230;&#20449;&#24687;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;Swin-UMamba&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#36739;&#20302;&#30340;&#20869;&#23384;&#28040;&#32791;&#21644;&#26356;&#23569;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;&#23041;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03302</link><description>&lt;p&gt;
Swin-UMamba&#65306;&#20197;Mamba&#20026;&#22522;&#30784;&#30340;&#20855;&#26377;ImageNet&#39044;&#35757;&#32451;&#30340;UNet&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03302
&lt;/p&gt;
&lt;p&gt;
Swin-UMamba&#26159;&#19968;&#31181;&#20197;Mamba&#20026;&#22522;&#30784;&#30340;&#26032;&#22411;UNet&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#29305;&#24449;&#21644;&#20840;&#23616;&#20381;&#36182;&#30340;&#22810;&#23610;&#24230;&#20449;&#24687;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;Swin-UMamba&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#36739;&#20302;&#30340;&#20869;&#23384;&#28040;&#32791;&#21644;&#26356;&#23569;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;&#23041;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#38656;&#35201;&#25972;&#21512;&#20174;&#23616;&#37096;&#29305;&#24449;&#21040;&#20840;&#23616;&#20381;&#36182;&#30340;&#22810;&#23610;&#24230;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24456;&#38590;&#24314;&#27169;&#38271;&#36317;&#31163;&#30340;&#20840;&#23616;&#20449;&#24687;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21463;&#21040;&#20854;&#23616;&#37096;&#24863;&#21463;&#37326;&#30340;&#38480;&#21046;&#65292;&#32780;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21463;&#21040;&#39640;&#20108;&#27425;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;Mamba&#30340;&#27169;&#22411;&#22240;&#20854;&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20960;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#33021;&#22815;&#32988;&#36807;&#27969;&#34892;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#25552;&#20379;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#26356;&#20302;&#30340;&#20869;&#23384;&#28040;&#32791;&#21644;&#26356;&#23569;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;Mamba&#30340;&#27169;&#22411;&#22823;&#22810;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#23041;&#21147;&#65292;&#32780;&#39044;&#35757;&#32451;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#39640;&#25928;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#38750;&#24120;&#26377;&#25928;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Mamba&#30340;&#27169;&#22411;Swin-UMamba&#65292;&#19987;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#32780;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate medical image segmentation demands the integration of multi-scale information, spanning from local features to global dependencies. However, it is challenging for existing methods to model long-range global information, where convolutional neural networks (CNNs) are constrained by their local receptive fields, and vision transformers (ViTs) suffer from high quadratic complexity of their attention mechanism. Recently, Mamba-based models have gained great attention for their impressive ability in long sequence modeling. Several studies have demonstrated that these models can outperform popular vision models in various tasks, offering higher accuracy, lower memory consumption, and less computational burden. However, existing Mamba-based models are mostly trained from scratch and do not explore the power of pretraining, which has been proven to be quite effective for data-efficient medical image analysis. This paper introduces a novel Mamba-based model, Swin-UMamba, designed speci
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#36981;&#24490;&#24773;&#20917;&#12290;&#31995;&#32479;&#36890;&#36807;&#25910;&#38598;&#29616;&#26377;&#36234;&#29425;&#24182;&#23558;&#20854;&#32452;&#32455;&#25104;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03299</link><description>&lt;p&gt;
GUARD: &#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#26469;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#21335;&#30340;&#21512;&#35268;&#24615;
&lt;/p&gt;
&lt;p&gt;
GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#36981;&#24490;&#24773;&#20917;&#12290;&#31995;&#32479;&#36890;&#36807;&#25910;&#38598;&#29616;&#26377;&#36234;&#29425;&#24182;&#23558;&#20854;&#32452;&#32455;&#25104;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#32469;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23433;&#20840;&#36807;&#28388;&#21644;&#26377;&#23475;&#22238;&#24212;&#30340;"&#36234;&#29425;"&#24050;&#32463;&#40723;&#21169;&#31038;&#21306;&#37319;&#21462;&#23433;&#20840;&#25514;&#26045;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#23433;&#20840;&#25514;&#26045;&#26159;&#22312;&#21457;&#24067;&#20043;&#21069;&#29992;&#36234;&#29425;&#20027;&#21160;&#27979;&#35797;LLM&#12290;&#22240;&#27492;&#65292;&#36825;&#26679;&#30340;&#27979;&#35797;&#23558;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#22823;&#35268;&#27169;&#19988;&#39640;&#25928;&#22320;&#29983;&#25104;&#36234;&#29425;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#36861;&#38543;&#19968;&#31181;&#26032;&#39062;&#32780;&#30452;&#35266;&#30340;&#31574;&#30053;&#19979;&#65292;&#20197;&#20154;&#31867;&#29983;&#25104;&#30340;&#26041;&#24335;&#26469;&#29983;&#25104;&#36234;&#29425;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35282;&#33394;&#25198;&#28436;&#31995;&#32479;&#65292;&#23558;&#22235;&#31181;&#19981;&#21516;&#35282;&#33394;&#20998;&#37197;&#32473;&#29992;&#25143;LLM&#65292;&#20197;&#20415;&#21327;&#20316;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25910;&#38598;&#29616;&#26377;&#30340;&#36234;&#29425;&#65292;&#24182;&#36890;&#36807;&#21477;&#23376;&#36880;&#21477;&#36827;&#34892;&#32858;&#31867;&#39057;&#29575;&#21644;&#35821;&#20041;&#27169;&#24335;&#30340;&#21010;&#20998;&#65292;&#23558;&#23427;&#20204;&#20998;&#25104;&#19981;&#21516;&#30340;&#29420;&#31435;&#29305;&#24449;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#29305;&#24449;&#32452;&#32455;&#25104;&#19968;&#20010;&#30693;&#35782;&#22270;&#65292;&#20351;&#20854;&#26356;&#26131;&#20110;&#35775;&#38382;&#21644;&#26816;&#32034;&#12290;&#25105;&#20204;&#30340;&#35282;&#33394;&#31995;&#32479;&#23558;&#21033;&#29992;&#36825;&#20010;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effec
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#39532;&#23572;&#21487;&#22827;&#35828;&#26381;&#36807;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;&#25429;&#25417;&#21457;&#36865;&#32773;&#21644;&#25509;&#25910;&#32773;&#39034;&#24207;&#20132;&#20114;&#30340;&#24773;&#26223;&#12290;&#35770;&#25991;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#38024;&#23545;&#21457;&#36865;&#32773;&#27809;&#26377;&#29615;&#22659;&#30693;&#35782;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#35774;&#35745;&#23398;&#20064;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#36825;&#20010;&#26041;&#27861;&#30340;&#24635;&#32467;&#35201;&#28857;&#26159;&#25552;&#20986;&#20102;&#39532;&#23572;&#21487;&#22827;&#35828;&#26381;&#36807;&#31243;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#27809;&#26377;&#29615;&#22659;&#30693;&#35782;&#30340;&#21457;&#36865;&#32773;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03077</link><description>&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#35828;&#26381;&#36807;&#31243;&#65306;&#20174;&#38646;&#24320;&#22987;&#23398;&#20250;&#35828;&#26381;
&lt;/p&gt;
&lt;p&gt;
Markov Persuasion Processes: Learning to Persuade from Scratch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03077
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#39532;&#23572;&#21487;&#22827;&#35828;&#26381;&#36807;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;&#25429;&#25417;&#21457;&#36865;&#32773;&#21644;&#25509;&#25910;&#32773;&#39034;&#24207;&#20132;&#20114;&#30340;&#24773;&#26223;&#12290;&#35770;&#25991;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#38024;&#23545;&#21457;&#36865;&#32773;&#27809;&#26377;&#29615;&#22659;&#30693;&#35782;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#35774;&#35745;&#23398;&#20064;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#36825;&#20010;&#26041;&#27861;&#30340;&#24635;&#32467;&#35201;&#28857;&#26159;&#25552;&#20986;&#20102;&#39532;&#23572;&#21487;&#22827;&#35828;&#26381;&#36807;&#31243;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#27809;&#26377;&#29615;&#22659;&#30693;&#35782;&#30340;&#21457;&#36865;&#32773;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36125;&#21494;&#26031;&#35828;&#26381;&#20013;&#65292;&#19968;&#20010;&#28040;&#24687;&#28789;&#36890;&#30340;&#21457;&#36865;&#32773;&#21487;&#20197;&#31574;&#30053;&#24615;&#22320;&#21521;&#25509;&#25910;&#32773;&#36879;&#38706;&#20449;&#24687;&#65292;&#20197;&#35828;&#26381;&#20182;&#20204;&#37319;&#21462;&#26399;&#26395;&#30340;&#34892;&#21160;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#28857;&#38598;&#20013;&#22312;&#21457;&#36865;&#32773;&#21644;&#25509;&#25910;&#32773;&#39034;&#24207;&#20132;&#20114;&#30340;&#24773;&#22659;&#20013;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#39532;&#23572;&#21487;&#22827;&#35828;&#26381;&#36807;&#31243;&#65288;MPPs&#65289;&#26469;&#25429;&#25417;&#22312;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#20013;&#65292;&#21457;&#36865;&#32773;&#38754;&#23545;&#19968;&#31995;&#21015;&#30701;&#35270;&#25509;&#25910;&#32773;&#30340;&#39034;&#24207;&#24773;&#26223;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22312;&#25991;&#29486;&#20013;&#30740;&#31350;&#30340;MPPs&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#30340;&#20805;&#20998;&#36816;&#20316;&#65292;&#20363;&#22914;&#65292;&#23427;&#20204;&#20551;&#35774;&#21457;&#36865;&#32773;&#30693;&#36947;&#25509;&#25910;&#32773;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#36890;&#36807;&#22788;&#29702;&#21457;&#36865;&#32773;&#23545;&#29615;&#22659;&#27809;&#26377;&#20219;&#20309;&#20102;&#35299;&#30340;MPPs&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#21457;&#36865;&#32773;&#30340;&#37096;&#20998;&#21453;&#39304;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#30340;&#24724;&#24680;&#19982;&#26368;&#20339;&#20449;&#24687;&#25259;&#38706;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#24322;&#20197;&#27425;&#32447;&#24615;&#22686;&#38271;&#65292;&#23601;&#20687;&#23398;&#20064;&#36807;&#31243;&#20013;&#32047;&#35745;&#30340;&#35828;&#26381;&#21147;&#25439;&#22833;&#19968;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Bayesian persuasion, an informed sender strategically discloses information to a receiver so as to persuade them to undertake desirable actions. Recently, a growing attention has been devoted to settings in which sender and receivers interact sequentially. Recently, Markov persuasion processes (MPPs) have been introduced to capture sequential scenarios where a sender faces a stream of myopic receivers in a Markovian environment. The MPPs studied so far in the literature suffer from issues that prevent them from being fully operational in practice, e.g., they assume that the sender knows receivers' rewards. We fix such issues by addressing MPPs where the sender has no knowledge about the environment. We design a learning algorithm for the sender, working with partial feedback. We prove that its regret with respect to an optimal information-disclosure policy grows sublinearly in the number of episodes, as it is the case for the loss in persuasiveness cumulated while learning. Moreover
&lt;/p&gt;</description></item><item><title>LOCOST&#26159;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#25991;&#26723;&#30340;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#12290;&#19982;&#22522;&#20110;&#31232;&#30095;&#27880;&#24847;&#27169;&#24335;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;LOCOST&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#26399;&#38388;&#33410;&#30465;&#22823;&#37327;&#20869;&#23384;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;LOCOST&#22312;&#38271;&#25991;&#26723;&#25688;&#35201;&#21270;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;93-96%&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#36229;&#36807;600K&#20010;&#26631;&#35760;&#30340;&#36755;&#20837;&#25991;&#26412;&#12290;</title><link>https://arxiv.org/abs/2401.17919</link><description>&lt;p&gt;
LOCOST: &#38271;&#25991;&#26723;&#25277;&#35937;&#25688;&#35201;&#21270;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LOCOST: State-Space Models for Long Document Abstractive Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17919
&lt;/p&gt;
&lt;p&gt;
LOCOST&#26159;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#25991;&#26723;&#30340;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#12290;&#19982;&#22522;&#20110;&#31232;&#30095;&#27880;&#24847;&#27169;&#24335;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;LOCOST&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#26399;&#38388;&#33410;&#30465;&#22823;&#37327;&#20869;&#23384;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;LOCOST&#22312;&#38271;&#25991;&#26723;&#25688;&#35201;&#21270;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;93-96%&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#36229;&#36807;600K&#20010;&#26631;&#35760;&#30340;&#36755;&#20837;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26159;&#32534;&#30721;&#38271;&#24207;&#21015;&#21644;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#30340;&#20302;&#22797;&#26434;&#24230;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LOCOST&#65306;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#20855;&#26377;&#38271;&#19978;&#19979;&#25991;&#36755;&#20837;&#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#12290;&#36825;&#31181;&#26550;&#26500;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20026;O&#65288;L log L&#65289;&#65292;&#21487;&#20197;&#22788;&#29702;&#27604;&#22522;&#20110;&#31232;&#30095;&#27880;&#24847;&#27169;&#24335;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#38271;&#25991;&#26723;&#25277;&#35937;&#25688;&#35201;&#21270;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#24615;&#33021;&#27700;&#24179;&#19978;&#36798;&#21040;&#20102;&#19982;&#30456;&#21516;&#22823;&#23567;&#30340;&#26368;&#20248;&#31232;&#30095;&#21464;&#21387;&#22120;&#30456;&#24403;&#30340;93-96%&#65292;&#21516;&#26102;&#22312;&#35757;&#32451;&#26399;&#38388;&#33410;&#30465;&#20102;&#39640;&#36798;50%&#30340;&#20869;&#23384;&#65292;&#22312;&#25512;&#26029;&#26399;&#38388;&#33410;&#30465;&#20102;&#39640;&#36798;87%&#30340;&#20869;&#23384;&#12290;&#27492;&#22806;&#65292;LOCOST&#26377;&#25928;&#22320;&#22788;&#29702;&#36229;&#36807;600K&#20010;&#26631;&#35760;&#30340;&#36755;&#20837;&#25991;&#26412;&#65292;&#20026;&#23436;&#25972;&#20070;&#25688;&#35201;&#21270;&#35774;&#23450;&#20102;&#26032;&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#24182;&#20026;&#38271;&#36755;&#20837;&#22788;&#29702;&#24320;&#36767;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of $O(L \log L)$, this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#29992;&#20110;&#21307;&#30103;&#38382;&#31572;&#30340;&#36890;&#29992;&#21644;&#21307;&#23398;&#29305;&#23450;&#30340;&#31934;&#28860;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#20197;&#22635;&#34917;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#20013;&#36825;&#20123;&#27169;&#22411;&#24615;&#33021;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2401.11389</link><description>&lt;p&gt;
MedLM: &#25506;&#32034;&#29992;&#20110;&#21307;&#30103;&#38382;&#31572;&#31995;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MedLM: Exploring Language Models for Medical Question Answering Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#29992;&#20110;&#21307;&#30103;&#38382;&#31572;&#30340;&#36890;&#29992;&#21644;&#21307;&#23398;&#29305;&#23450;&#30340;&#31934;&#28860;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#20197;&#22635;&#34917;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#20013;&#36825;&#20123;&#27169;&#22411;&#24615;&#33021;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#36805;&#36895;&#25193;&#22823;&#30340;&#22312;&#32447;&#21307;&#23398;&#25991;&#29486;&#65292;&#33258;&#21160;&#21270;&#31995;&#32479;&#29992;&#20110;&#32858;&#21512;&#21644;&#24635;&#32467;&#20449;&#24687;&#23545;&#20110;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#21644;&#24739;&#32773;&#21464;&#24471;&#26085;&#30410;&#20851;&#38190;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#20808;&#36827;&#30340;&#29983;&#25104;&#33021;&#21147;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#23427;&#20204;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#23553;&#38381;&#24335;&#29983;&#25104;&#38382;&#31572;&#26041;&#38754;&#65292;&#26159;&#26174;&#33879;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21307;&#23398;&#38382;&#31572;&#31561;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20173;&#28982;&#36739;&#23569;&#34987;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#27604;&#36739;&#36890;&#29992;&#21644;&#19987;&#38376;&#29992;&#20110;&#21307;&#23398;&#30340;&#31934;&#28860;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#38382;&#31572;&#20013;&#30340;&#34920;&#29616;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#26088;&#22312;&#35780;&#20272;&#24494;&#35843;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#23558;&#25506;&#35752;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12289;&#27604;&#36739;&#24615;&#33021;&#21644;&#22312;&#21307;&#30103;&#38382;&#31572;&#32972;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#31561;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11389v2 Announce Type: replace-cross  Abstract: In the face of rapidly expanding online medical literature, automated systems for aggregating and summarizing information are becoming increasingly crucial for healthcare professionals and patients. Large Language Models (LLMs), with their advanced generative capabilities, have shown promise in various NLP tasks, and their potential in the healthcare domain, particularly for Closed-Book Generative QnA, is significant. However, the performance of these models in domain-specific tasks such as medical Q&amp;A remains largely unexplored. This study aims to fill this gap by comparing the performance of general and medical-specific distilled LMs for medical Q&amp;A. We aim to evaluate the effectiveness of fine-tuning domain-specific LMs and compare the performance of different families of Language Models. The study will address critical questions about these models' reliability, comparative performance, and effectiveness in the context of me
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#24615;&#22320;&#25193;&#23637;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;SceneVerse&#65292;&#20197;&#35299;&#20915;3D&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#38754;&#20020;&#30340;&#20960;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.09340</link><description>&lt;p&gt;
SceneVerse&#65306;&#20026;&#22522;&#20110;&#22330;&#26223;&#30340;&#22330;&#26223;&#29702;&#35299;&#25193;&#23637;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.09340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#24615;&#22320;&#25193;&#23637;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;SceneVerse&#65292;&#20197;&#35299;&#20915;3D&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#38754;&#20020;&#30340;&#20960;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#65292;&#21363;&#23558;&#35821;&#35328;&#19982;3D&#29289;&#29702;&#29615;&#22659;&#23545;&#40784;&#65292;&#26159;&#21457;&#23637;&#20855;&#36523;&#20307;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#30340;&#22522;&#30707;&#12290;&#19982;2D&#39046;&#22495;&#26368;&#36817;&#30340;&#36827;&#23637;&#30456;&#27604;&#65292;&#23558;&#35821;&#35328;&#19982;3D&#22330;&#26223;&#23545;&#40784;&#38754;&#20020;&#30528;&#20960;&#20010;&#37325;&#35201;&#25361;&#25112;&#65306;&#65288;i&#65289;3D&#22330;&#26223;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#30001;&#20110;&#22810;&#26679;&#30340;&#29289;&#20307;&#37197;&#32622;&#12289;&#20016;&#23500;&#30340;&#23646;&#24615;&#21644;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65307;&#65288;ii&#65289;&#25903;&#25345;&#22522;&#20110;&#22330;&#26223;&#23398;&#20064;&#30340;&#37197;&#23545;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65307;&#20197;&#21450;&#65288;iii&#65289;&#32570;&#20047;&#20174;&#22522;&#20110;&#22330;&#26223;&#30340;3D&#25968;&#25454;&#20013;&#25552;&#28860;&#30693;&#35782;&#30340;&#32479;&#19968;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#31995;&#32479;&#22320;&#25193;&#23637;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;&#65292;&#20174;&#32780;&#35299;&#20915;3D&#35270;&#35273;-&#35821;&#35328;&#39046;&#22495;&#20013;&#30340;&#36825;&#19977;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#39318;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;SceneVerse&#65292;&#21253;&#21547;&#32422;68K&#20010;3D&#23460;&#20869;&#22330;&#26223;&#65292;&#21253;&#25324;250&#19975;&#20010;&#35270;&#35273;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.09340v2 Announce Type: replace-cross  Abstract: 3D vision-language grounding, which focuses on aligning language with the 3D physical environment, stands as a cornerstone in the development of embodied agents. In comparison to recent advancements in the 2D domain, grounding language in 3D scenes faces several significant challenges: (i) the inherent complexity of 3D scenes due to the diverse object configurations, their rich attributes, and intricate relationships; (ii) the scarcity of paired 3D vision-language data to support grounded learning; and (iii) the absence of a unified learning framework to distill knowledge from grounded 3D data. In this work, we aim to address these three major challenges in 3D vision-language by examining the potential of systematically upscaling 3D vision-language learning in indoor environments. We introduce the first million-scale 3D vision-language dataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising 2.5M vision-langu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#20799;&#31461;&#21644;&#25945;&#24072;&#20329;&#25140;&#30340;&#24405;&#38899;&#35774;&#22791;&#35760;&#24405;&#35328;&#35821;&#65292;&#23454;&#29616;&#35828;&#35805;&#32773;&#20998;&#31867;&#21644;&#36716;&#24405;&#65292;&#19982;&#20154;&#31867;&#19987;&#23478;&#23545;&#27604;&#32467;&#26524;&#34920;&#26126;&#65292;&#26694;&#26550;&#25972;&#20307;&#20934;&#30830;&#29575;&#36798;&#21040;0.76&#65292;&#20026;&#23398;&#21069;&#25945;&#23460;&#35328;&#35821;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;</title><link>https://arxiv.org/abs/2401.07342</link><description>&lt;p&gt;
&#35841;&#35828;&#20102;&#20160;&#20040;&#65311;&#20998;&#26512;&#23398;&#21069;&#35838;&#22530;&#35328;&#35821;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Who Said What? An Automated Approach to Analyzing Speech in Preschool Classrooms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07342
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#20799;&#31461;&#21644;&#25945;&#24072;&#20329;&#25140;&#30340;&#24405;&#38899;&#35774;&#22791;&#35760;&#24405;&#35328;&#35821;&#65292;&#23454;&#29616;&#35828;&#35805;&#32773;&#20998;&#31867;&#21644;&#36716;&#24405;&#65292;&#19982;&#20154;&#31867;&#19987;&#23478;&#23545;&#27604;&#32467;&#26524;&#34920;&#26126;&#65292;&#26694;&#26550;&#25972;&#20307;&#20934;&#30830;&#29575;&#36798;&#21040;0.76&#65292;&#20026;&#23398;&#21069;&#25945;&#23460;&#35328;&#35821;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24180;&#24188;&#20799;&#31461;&#22312;&#21927;&#38393;&#30340;&#23398;&#21069;&#35838;&#22530;&#20013;&#24230;&#36807;&#22823;&#37096;&#20998;&#28165;&#37266;&#26102;&#38388;&#12290;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#65292;&#23401;&#23376;&#19982;&#25945;&#24072;&#20043;&#38388;&#30340;&#35328;&#35821;&#20114;&#21160;&#23545;&#20182;&#20204;&#30340;&#35821;&#35328;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#25163;&#21160;&#36716;&#24405;&#36825;&#20123;&#20114;&#21160;&#26159;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#20799;&#31461;&#21644;&#25945;&#24072;&#20329;&#25140;&#30340;&#24405;&#38899;&#35774;&#22791;&#24405;&#38899;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#24320;&#28304;&#36719;&#20214;&#23545;&#35828;&#35805;&#32773;&#36827;&#34892;&#20998;&#31867;&#65288;ALICE&#65289;&#65292;&#24182;&#36716;&#24405;&#20182;&#20204;&#30340;&#35805;&#35821;&#65288;Whisper&#65289;&#12290;&#25105;&#20204;&#23558;&#26694;&#26550;&#30340;&#32467;&#26524;&#19982;&#20154;&#31867;&#19987;&#23478;&#23545;110&#20998;&#38047;&#25945;&#23460;&#24405;&#38899;&#65288;&#21253;&#25324;&#26469;&#33258;&#20799;&#31461;&#35805;&#31570;&#30340;85&#20998;&#38047;&#65288;4&#21517;&#20799;&#31461;&#65289;&#21644;&#26469;&#33258;&#25945;&#24072;&#35805;&#31570;&#30340;25&#20998;&#38047;&#65288;2&#21517;&#25945;&#24072;&#65289;&#65289;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#24635;&#20307;&#19968;&#33268;&#27604;&#20363;&#65292;&#21363;&#27491;&#30830;&#20998;&#31867;&#30340;&#25945;&#24072;&#21644;&#20799;&#31461;&#35805;&#35821;&#30340;&#27604;&#20363;&#20026;0.76&#65292;&#30699;&#27491;&#30340;kappa&#20026;0.50&#65292;&#21152;&#26435;F1&#20026;0.76&#12290;&#25945;&#24072;&#21644;&#20799;&#31461;&#30340;&#35805;&#35821;&#30340;&#35789;&#35823;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07342v2 Announce Type: replace-cross  Abstract: Young children spend substantial portions of their waking hours in noisy preschool classrooms. In these environments, children's vocal interactions with teachers are critical contributors to their language outcomes, but manually transcribing these interactions is prohibitive. Using audio from child- and teacher-worn recorders, we propose an automated framework that uses open source software both to classify speakers (ALICE) and to transcribe their utterances (Whisper). We compare results from our framework to those from a human expert for 110 minutes of classroom recordings, including 85 minutes from child-word microphones (n=4 children) and 25 minutes from teacher-worn microphones (n=2 teachers). The overall proportion of agreement, that is, the proportion of correctly classified teacher and child utterances, was .76, with an error-corrected kappa of .50 and a weighted F1 of .76. The word error rate for both teacher and child 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20027;&#35201;&#31070;&#32463;&#20803;&#26469;&#21457;&#29616;&#27010;&#24565;&#30340;&#20998;&#24067;&#34920;&#31034;&#65292;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#26410;&#26631;&#35760;&#23376;&#31867;&#21644;&#26816;&#27979;&#35823;&#20998;&#31867;&#30340;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2312.17285</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#19979;&#29702;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#27010;&#24565;&#30340;&#20998;&#24067;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Understanding Distributed Representations of Concepts in Deep Neural Networks without Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20027;&#35201;&#31070;&#32463;&#20803;&#26469;&#21457;&#29616;&#27010;&#24565;&#30340;&#20998;&#24067;&#34920;&#31034;&#65292;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#26410;&#26631;&#35760;&#23376;&#31867;&#21644;&#26816;&#27979;&#35823;&#20998;&#31867;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#23398;&#20064;&#30340;&#27010;&#24565;&#30340;&#20013;&#38388;&#34920;&#31034;&#23545;&#35299;&#37322;&#27169;&#22411;&#30340;&#19968;&#33324;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#25581;&#31034;&#23398;&#20064;&#27010;&#24565;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20154;&#31867;&#30417;&#30563;&#65292;&#20363;&#22914;&#39044;&#23450;&#20041;&#30340;&#27010;&#24565;&#38598;&#25110;&#20998;&#21106;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20027;&#35201;&#23376;&#38598;&#30340;&#31070;&#32463;&#20803;&#26469;&#21457;&#29616;&#27010;&#24565;&#30340;&#20998;&#24067;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#31867;&#20284;&#31070;&#32463;&#20803;&#28608;&#27963;&#29366;&#24577;&#30340;&#23454;&#20363;&#24448;&#24448;&#20849;&#20139;&#19968;&#33268;&#30340;&#27010;&#24565;&#12290;&#26681;&#25454;&#35266;&#23519;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36873;&#25321;&#26500;&#24314;&#21487;&#35299;&#37322;&#21306;&#22495;&#30340;&#20027;&#35201;&#31070;&#32463;&#20803;&#65292;&#21363;&#28085;&#30422;&#29305;&#24449;&#31354;&#38388;&#20013;&#20855;&#26377;&#19968;&#33268;&#27010;&#24565;&#30340;&#23454;&#20363;&#30340;&#25918;&#26494;&#20915;&#31574;&#21306;&#22495;&#65288;RDR&#65289;&#12290;&#23427;&#21487;&#29992;&#20110;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#26410;&#26631;&#35760;&#23376;&#31867;&#24182;&#26816;&#27979;&#35823;&#20998;&#31867;&#30340;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#24212;&#29992;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.17285v2 Announce Type: replace-cross  Abstract: Understanding intermediate representations of the concepts learned by deep learning classifiers is indispensable for interpreting general model behaviors. Existing approaches to reveal learned concepts often rely on human supervision, such as pre-defined concept sets or segmentation processes. In this paper, we propose a novel unsupervised method for discovering distributed representations of concepts by selecting a principal subset of neurons. Our empirical findings demonstrate that instances with similar neuron activation states tend to share coherent concepts. Based on the observations, the proposed method selects principal neurons that construct an interpretable region, namely a Relaxed Decision Region (RDR), encompassing instances with coherent concepts in the feature space. It can be utilized to identify unlabeled subclasses within data and to detect the causes of misclassifications. Furthermore, the applicability of our 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#36817;&#20284;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36817;&#20284;&#20540;&#36845;&#20195;&#31639;&#27861;&#20013;&#26679;&#26412;&#19981;&#30830;&#23450;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.12869</link><description>&lt;p&gt;
&#21442;&#25968;&#21270;&#25237;&#24433;&#36125;&#23572;&#26364;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Parameterized Projected Bellman Operator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#36817;&#20284;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36817;&#20284;&#20540;&#36845;&#20195;&#31639;&#27861;&#20013;&#26679;&#26412;&#19981;&#30830;&#23450;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#20540;&#36845;&#20195;&#65288;AVI&#65289;&#26159;&#19968;&#31867;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#31639;&#27861;&#23478;&#26063;&#65292;&#26088;&#22312;&#33719;&#24471;&#26368;&#20248;&#20540;&#20989;&#25968;&#30340;&#36817;&#20284;&#12290;&#36890;&#24120;&#65292;AVI&#31639;&#27861;&#37319;&#29992;&#36845;&#20195;&#36807;&#31243;&#65292;&#27599;&#20010;&#27493;&#39588;&#21253;&#25324;&#65288;i&#65289;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#24212;&#29992;&#21644;&#65288;ii&#65289;&#25237;&#24433;&#27493;&#39588;&#21040;&#32771;&#34385;&#30340;&#20989;&#25968;&#31354;&#38388;&#20013;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#36125;&#23572;&#26364;&#31639;&#23376;&#21033;&#29992;&#36716;&#31227;&#26679;&#26412;&#65292;&#36825;&#20123;&#26679;&#26412;&#24378;&#28872;&#24433;&#21709;&#20854;&#34892;&#20026;&#65292;&#22240;&#20026;&#26080;&#20449;&#24687;&#30340;&#26679;&#26412;&#21487;&#33021;&#23548;&#33268;&#21487;&#24573;&#30053;&#30340;&#26356;&#26032;&#25110;&#38271;&#26102;&#38388;&#30340;&#32469;&#34892;&#65292;&#32780;&#35745;&#31639;&#23494;&#38598;&#30340;&#25237;&#24433;&#27493;&#39588;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#20123;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#23398;&#20064;&#30340;&#26041;&#24335;&#24471;&#21040;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#36817;&#20284;&#29256;&#26412;&#65292;&#32780;&#19981;&#26159;&#20687;AVI&#26041;&#27861;&#37027;&#26679;&#36890;&#36807;&#26679;&#26412;&#36827;&#34892;&#20272;&#35745;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#33021;&#22815;&#65288;i&#65289;&#22312;&#36716;&#31227;&#26679;&#26412;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#65292;&#65288;ii&#65289;&#36991;&#20813;&#35745;&#31639;&#23494;&#38598;&#30340;&#25237;&#24433;&#27493;&#39588;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#31216;&#25105;&#20204;&#30340;&#26032;&#31639;&#23376;&#20026;"projec"&#31639;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approximate value iteration (AVI) is a family of algorithms for reinforcement learning (RL) that aims to obtain an approximation of the optimal value function. Generally, AVI algorithms implement an iterated procedure where each step consists of (i) an application of the Bellman operator and (ii) a projection step into a considered function space. Notoriously, the Bellman operator leverages transition samples, which strongly determine its behavior, as uninformative samples can result in negligible updates or long detours, whose detrimental effects are further exacerbated by the computationally intensive projection step. To address these issues, we propose a novel alternative approach based on learning an approximate version of the Bellman operator rather than estimating it through samples as in AVI approaches. This way, we are able to (i) generalize across transition samples and (ii) avoid the computationally intensive projection step. For this reason, we call our novel operator projec
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;TSRNet&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#26102;&#38388;&#21644;&#35889;&#22270;&#24674;&#22797;&#32593;&#32476;&#36827;&#34892;&#23454;&#26102;&#24515;&#30005;&#22270;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#24674;&#22797;&#27491;&#24120;ECG&#25968;&#25454;&#35757;&#32451;&#65292;&#32467;&#21512;&#26102;&#38388;&#24207;&#21015;&#21644;&#26102;&#39057;&#39046;&#22495;&#65292;&#23454;&#29616;&#20102;&#23545;&#24515;&#30005;&#22270;&#20449;&#21495;&#24322;&#24120;&#30340;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2312.10187</link><description>&lt;p&gt;
TSRNet&#65306;&#22810;&#27169;&#24577;&#26102;&#38388;&#21644;&#35889;&#22270;&#24674;&#22797;&#32593;&#32476;&#23454;&#26102;&#24515;&#30005;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#31616;&#21333;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TSRNet: Simple Framework for Real-time ECG Anomaly Detection with Multimodal Time and Spectrogram Restoration Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;TSRNet&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#26102;&#38388;&#21644;&#35889;&#22270;&#24674;&#22797;&#32593;&#32476;&#36827;&#34892;&#23454;&#26102;&#24515;&#30005;&#22270;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#24674;&#22797;&#27491;&#24120;ECG&#25968;&#25454;&#35757;&#32451;&#65292;&#32467;&#21512;&#26102;&#38388;&#24207;&#21015;&#21644;&#26102;&#39057;&#39046;&#22495;&#65292;&#23454;&#29616;&#20102;&#23545;&#24515;&#30005;&#22270;&#20449;&#21495;&#24322;&#24120;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#20449;&#21495;&#65292;&#29992;&#20110;&#35780;&#20272;&#24515;&#33039;&#20581;&#24247;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#22914;&#24515;&#29575;&#21644;&#33410;&#24459;&#12290;&#23427;&#22312;&#35782;&#21035;&#24515;&#33039;&#29366;&#20917;&#21644;&#26816;&#27979;ECG&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;ECG&#20449;&#21495;&#21487;&#33021;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24322;&#24120;&#26816;&#27979;&#26469;&#35782;&#21035;&#19981;&#20581;&#24247;&#29366;&#20917;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#27491;&#24120;ECG&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;&#21487;&#29992;&#20449;&#24687;&#24182;&#26500;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#31995;&#32479;&#65292;&#25105;&#20204;&#24314;&#35758;&#32771;&#34385;ECG&#20449;&#21495;&#30340;&#26102;&#38388;&#24207;&#21015;&#21644;&#26102;&#39057;&#39046;&#22495;&#20004;&#20010;&#26041;&#38754;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#26816;&#27979;ECG&#20449;&#21495;&#24322;&#24120;&#30340;&#21517;&#20026;&#22810;&#27169;&#24577;&#26102;&#38388;&#21644;&#35889;&#22270;&#24674;&#22797;&#32593;&#32476;&#65288;TSRNet&#65289;&#30340;&#32593;&#32476;&#12290;TSRNet&#23646;&#20110;&#22522;&#20110;&#24674;&#22797;&#30340;&#24322;&#24120;&#26816;&#27979;&#31867;&#21035;&#65292;&#28789;&#24863;&#26469;&#28304;&#20110;&#26102;&#38388;&#24207;&#21015;&#21644;&#26102;&#39057;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10187v2 Announce Type: replace-cross  Abstract: The electrocardiogram (ECG) is a valuable signal used to assess various aspects of heart health, such as heart rate and rhythm. It plays a crucial role in identifying cardiac conditions and detecting anomalies in ECG data. However, distinguishing between normal and abnormal ECG signals can be a challenging task. In this paper, we propose an approach that leverages anomaly detection to identify unhealthy conditions using solely normal ECG data for training. Furthermore, to enhance the information available and build a robust system, we suggest considering both the time series and time-frequency domain aspects of the ECG signal. As a result, we introduce a specialized network called the Multimodal Time and Spectrogram Restoration Network (TSRNet) designed specifically for detecting anomalies in ECG signals. TSRNet falls into the category of restoration-based anomaly detection and draws inspiration from both the time series and sp
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#21487;&#31359;&#25140;&#35774;&#22791;&#25968;&#25454;&#65292;&#26412;&#30740;&#31350;&#35757;&#32451;&#20102;&#22522;&#30784;&#27169;&#22411;&#29992;&#20110;&#34913;&#37327;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#65288;PPG&#65289;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#65292;&#20197;&#35299;&#20915;&#21307;&#23398;&#25968;&#25454;&#38598;&#35268;&#27169;&#36739;&#23567;&#30340;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.05409</link><description>&lt;p&gt;
&#21487;&#31359;&#25140;&#29983;&#29289;&#20449;&#21495;&#22522;&#30784;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Large-scale Training of Foundation Models for Wearable Biosignals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05409
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#21487;&#31359;&#25140;&#35774;&#22791;&#25968;&#25454;&#65292;&#26412;&#30740;&#31350;&#35757;&#32451;&#20102;&#22522;&#30784;&#27169;&#22411;&#29992;&#20110;&#34913;&#37327;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#65288;PPG&#65289;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#65292;&#20197;&#35299;&#20915;&#21307;&#23398;&#25968;&#25454;&#38598;&#35268;&#27169;&#36739;&#23567;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36319;&#36394;&#29983;&#29289;&#20449;&#21495;&#23545;&#20110;&#30417;&#27979;&#20581;&#24247;&#29366;&#20917;&#24182;&#39044;&#38450;&#20005;&#37325;&#21307;&#23398;&#29366;&#20917;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#22914;&#20170;&#65292;&#21487;&#31359;&#25140;&#35774;&#22791;&#21487;&#20197;&#26041;&#20415;&#22320;&#35760;&#24405;&#21508;&#31181;&#29983;&#29289;&#20449;&#21495;&#65292;&#20174;&#32780;&#26377;&#26426;&#20250;&#22312;&#19981;&#24178;&#25200;&#26085;&#24120;&#29983;&#27963;&#30340;&#24773;&#20917;&#19979;&#30417;&#27979;&#20581;&#24247;&#29366;&#20917;&#12290;&#34429;&#28982;&#21487;&#31359;&#25140;&#35774;&#22791;&#34987;&#24191;&#27867;&#20351;&#29992;&#19988;&#23384;&#22312;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#20294;&#32570;&#20047;&#24102;&#26377;&#27880;&#37322;&#21307;&#23398;&#26631;&#31614;&#30340;&#31579;&#36873;&#25968;&#25454;&#65292;&#38459;&#30861;&#20102;&#24320;&#21457;&#34913;&#37327;&#24120;&#35265;&#20581;&#24247;&#29366;&#20917;&#30340;&#26032;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#20107;&#23454;&#19978;&#65292;&#19982;&#20854;&#20182;&#39046;&#22495;&#30456;&#27604;&#65292;&#21307;&#23398;&#25968;&#25454;&#38598;&#36890;&#24120;&#36739;&#23567;&#65292;&#36825;&#26159;&#24320;&#21457;&#29983;&#29289;&#20449;&#21495;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#38556;&#30861;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#22312;&#30693;&#24773;&#21516;&#24847;&#19979;&#20174;&#22823;&#35268;&#27169;&#32437;&#21521;Apple&#24515;&#33039;&#21644;&#36816;&#21160;&#30740;&#31350;&#65288;AHMS&#65289;&#20013;&#25910;&#38598;&#30340;&#26410;&#26631;&#35760;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20026;&#20004;&#31181;&#24120;&#35265;&#29983;&#29289;&#20449;&#21495;&#65288;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#65288;PPG&#65289;&#21644;&#24515;&#30005;&#22270;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05409v2 Announce Type: replace-cross  Abstract: Tracking biosignals is crucial for monitoring wellness and preempting the development of severe medical conditions. Today, wearable devices can conveniently record various biosignals, creating the opportunity to monitor health status without disruption to one's daily routine. Despite widespread use of wearable devices and existing digital biomarkers, the absence of curated data with annotated medical labels hinders the development of new biomarkers to measure common health conditions. In fact, medical datasets are usually small in comparison to other domains, which is an obstacle for developing neural network models for biosignals. To address this challenge, we have employed self-supervised learning using the unlabeled sensor data collected under informed consent from the large longitudinal Apple Heart and Movement Study (AHMS) to train foundation models for two common biosignals: photoplethysmography (PPG) and electrocardiogra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Fair Mapping&#25511;&#21046;&#27169;&#22411;&#25552;&#31034;&#26469;&#20462;&#25913;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20844;&#24179;&#22270;&#20687;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#33021;&#22815;&#29983;&#25104;&#30456;&#23545;&#24179;&#34913;&#20154;&#21475;&#32479;&#35745;&#32467;&#26524;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2311.17695</link><description>&lt;p&gt;
&#36890;&#36807;&#20844;&#24179;&#26144;&#23556;&#23454;&#29616;&#20844;&#24179;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Fair Text-to-Image Diffusion via Fair Mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Fair Mapping&#25511;&#21046;&#27169;&#22411;&#25552;&#31034;&#26469;&#20462;&#25913;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20844;&#24179;&#22270;&#20687;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#33021;&#22815;&#29983;&#25104;&#30456;&#23545;&#24179;&#34913;&#20154;&#21475;&#32479;&#35745;&#32467;&#26524;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#19982;&#20154;&#31867;&#30456;&#20851;&#25551;&#36848;&#26102;&#20986;&#29616;&#20154;&#21475;&#32479;&#35745;&#19978;&#20844;&#24179;&#32467;&#26524;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#32463;&#24120;&#38590;&#20197;&#23558;&#30446;&#26631;&#35821;&#35328;&#29615;&#22659;&#19982;&#31038;&#20250;&#25991;&#21270;&#20559;&#35265;&#20998;&#31163;&#24320;&#65292;&#23548;&#33268;&#29983;&#25104;&#20559;&#35265;&#22270;&#20687;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#19988;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;Fair Mapping&#65292;&#36890;&#36807;&#25511;&#21046;&#25552;&#31034;&#26469;&#20462;&#25913;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#20844;&#24179;&#22270;&#20687;&#29983;&#25104;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#22312;&#20110;&#20854;&#39640;&#25928;&#24615;&#12290;&#23427;&#21482;&#38656;&#35201;&#20197;&#20302;&#35745;&#31639;&#25104;&#26412;&#26356;&#26032;&#23569;&#37327;&#21442;&#25968;&#30340;&#39069;&#22806;&#32447;&#24615;&#32593;&#32476;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#23558;&#26465;&#20214;&#23884;&#20837;&#26144;&#23556;&#21040;&#21435;&#20559;&#31354;&#38388;&#30340;&#32447;&#24615;&#32593;&#32476;&#65292;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#25351;&#23450;&#30340;&#25991;&#26412;&#26465;&#20214;&#29983;&#25104;&#30456;&#23545;&#24179;&#34913;&#30340;&#20154;&#21475;&#32479;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17695v2 Announce Type: replace-cross  Abstract: In this paper, we address the limitations of existing text-to-image diffusion models in generating demographically fair results when given human-related descriptions. These models often struggle to disentangle the target language context from sociocultural biases, resulting in biased image generation. To overcome this challenge, we propose Fair Mapping, a flexible, model-agnostic, and lightweight approach that modifies a pre-trained text-to-image diffusion model by controlling the prompt to achieve fair image generation. One key advantage of our approach is its high efficiency. It only requires updating an additional linear network with few parameters at a low computational cost. By developing a linear network that maps conditioning embeddings into a debiased space, we enable the generation of relatively balanced demographic results based on the specified text condition. With comprehensive experiments on face image generation, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#38543;&#26426;&#32447;&#24615;&#36172;&#33218;&#29615;&#22659;&#20013;&#30340;&#38598;&#25104;&#25277;&#26679;&#36827;&#34892;&#20102;&#39318;&#27425;&#23454;&#29992;&#21644;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#65292;&#37319;&#29992;&#35268;&#27169;&#20026;$d \log T$&#30340;&#38598;&#25104;&#25277;&#26679;&#21487;&#20197;&#33719;&#24471;&#25509;&#36817;$\sqrt{T}$&#38454;&#30340;&#21518;&#24724;&#65292;&#32780;&#19981;&#38656;&#35201;&#38598;&#25104;&#22823;&#23567;&#19982;$T$&#32447;&#24615;&#25193;&#23637;&#12290;</title><link>https://arxiv.org/abs/2311.08376</link><description>&lt;p&gt;
&#32447;&#24615;&#36172;&#33218;&#30340;&#38598;&#25104;&#25277;&#26679;&#65306;&#23567;&#38598;&#25104;&#36275;&#30691;
&lt;/p&gt;
&lt;p&gt;
Ensemble sampling for linear bandits: small ensembles suffice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08376
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#38543;&#26426;&#32447;&#24615;&#36172;&#33218;&#29615;&#22659;&#20013;&#30340;&#38598;&#25104;&#25277;&#26679;&#36827;&#34892;&#20102;&#39318;&#27425;&#23454;&#29992;&#21644;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#65292;&#37319;&#29992;&#35268;&#27169;&#20026;$d \log T$&#30340;&#38598;&#25104;&#25277;&#26679;&#21487;&#20197;&#33719;&#24471;&#25509;&#36817;$\sqrt{T}$&#38454;&#30340;&#21518;&#24724;&#65292;&#32780;&#19981;&#38656;&#35201;&#38598;&#25104;&#22823;&#23567;&#19982;$T$&#32447;&#24615;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#23545;&#38543;&#26426;&#32447;&#24615;&#36172;&#33218;&#35774;&#23450;&#19979;&#30340;&#38598;&#25104;&#25277;&#26679;&#36827;&#34892;&#20102;&#26377;&#29992;&#19988;&#20005;&#35880;&#30340;&#20998;&#26512;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#65292;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;&#20132;&#20114;&#20316;&#29992;&#26102;&#38388;&#36328;&#24230;$T$&#30340;$d$&#32500;&#38543;&#26426;&#32447;&#24615;&#36172;&#33218;&#65292;&#37319;&#29992;&#38598;&#25104;&#22823;&#23567;&#20026;$\smash{d \log T}$&#30340;&#38598;&#25104;&#25277;&#26679;&#65292;&#36973;&#21463;&#30340;&#21518;&#24724;&#26368;&#22810;&#20026;$\smash{(d \log T)^{5/2} \sqrt{T}}$&#38454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#22312;&#20219;&#20309;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#31532;&#19968;&#20010;&#19981;&#35201;&#27714;&#38598;&#25104;&#22823;&#23567;&#19982;$T$&#32447;&#24615;&#25193;&#23637;&#30340;&#32467;&#26524;&#65292;&#36825;&#20351;&#24471;&#38598;&#25104;&#25277;&#26679;&#22833;&#21435;&#24847;&#20041;&#65292;&#21516;&#26102;&#33719;&#24471;&#20102;&#25509;&#36817;$\smash{\sqrt{T}}$&#38454;&#30340;&#21518;&#24724;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20063;&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#26080;&#38480;&#21160;&#20316;&#38598;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08376v2 Announce Type: replace-cross  Abstract: We provide the first useful and rigorous analysis of ensemble sampling for the stochastic linear bandit setting. In particular, we show that, under standard assumptions, for a $d$-dimensional stochastic linear bandit with an interaction horizon $T$, ensemble sampling with an ensemble of size of order $\smash{d \log T}$ incurs regret at most of the order $\smash{(d \log T)^{5/2} \sqrt{T}}$. Ours is the first result in any structured setting not to require the size of the ensemble to scale linearly with $T$ -- which defeats the purpose of ensemble sampling -- while obtaining near $\smash{\sqrt{T}}$ order regret. Ours is also the first result that allows infinite action sets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EVILL&#30340;&#38543;&#26426;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#32447;&#24615;&#25200;&#21160;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#30340;&#26497;&#23567;&#21270;&#38382;&#39064;&#26469;&#24037;&#20316;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#38543;&#26426;&#22870;&#21169;&#25200;&#21160;&#20135;&#29983;&#33391;&#22909;&#36172;&#21338;&#31639;&#27861;&#30340;&#31616;&#27905;&#35299;&#37322;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#23637;&#31034;&#20102;&#19982;&#27748;&#26222;&#26862;&#25277;&#26679;&#39118;&#26684;&#21442;&#25968;&#25200;&#21160;&#26041;&#27861;&#24615;&#33021;&#30456;&#21305;&#37197;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2311.07565</link><description>&lt;p&gt;
&#36890;&#36807;&#32447;&#24615;&#25200;&#21160;&#30340;&#25439;&#22833;&#26368;&#23567;&#21270;&#26469;&#36827;&#34892;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploration via linearly perturbed loss minimisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07565
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EVILL&#30340;&#38543;&#26426;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#32447;&#24615;&#25200;&#21160;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#30340;&#26497;&#23567;&#21270;&#38382;&#39064;&#26469;&#24037;&#20316;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#38543;&#26426;&#22870;&#21169;&#25200;&#21160;&#20135;&#29983;&#33391;&#22909;&#36172;&#21338;&#31639;&#27861;&#30340;&#31616;&#27905;&#35299;&#37322;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#23637;&#31034;&#20102;&#19982;&#27748;&#26222;&#26862;&#25277;&#26679;&#39118;&#26684;&#21442;&#25968;&#25200;&#21160;&#26041;&#27861;&#24615;&#33021;&#30456;&#21305;&#37197;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#36890;&#36807;&#32447;&#24615;&#25439;&#22833;&#25200;&#21160;&#36827;&#34892;&#25506;&#32034;&#65288;EVILL&#65289;&#30340;&#38543;&#26426;&#25506;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#32467;&#26500;&#21270;&#38543;&#26426;&#36172;&#21338;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#20915;&#32447;&#24615;&#25200;&#21160;&#27491;&#21017;&#21270;&#36127;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#30340;&#26497;&#23567;&#21270;&#38382;&#39064;&#26469;&#24037;&#20316;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#23545;&#20110;&#24191;&#20041;&#32447;&#24615;&#36172;&#21338;&#38382;&#39064;&#65292;EVILL&#21487;&#20197;&#31616;&#21270;&#20026;&#25200;&#21160;&#21382;&#21490;&#25506;&#32034;&#65288;PHE&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#22312;&#38543;&#26426;&#25200;&#21160;&#22870;&#21169;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#36827;&#34892;&#25506;&#32034;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#23545;&#38543;&#26426;&#22870;&#21169;&#25200;&#21160;&#20309;&#26102;&#20197;&#21450;&#20026;&#20309;&#20135;&#29983;&#33391;&#22909;&#30340;&#36172;&#21338;&#31639;&#27861;&#25552;&#20379;&#20102;&#31616;&#21333;&#24178;&#20928;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20808;&#21069;PHE&#31867;&#22411;&#26041;&#27861;&#20013;&#19981;&#21547;&#30340;&#25968;&#25454;&#30456;&#20851;&#25200;&#21160;&#65292;&#20351;EVILL&#33021;&#22815;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#19982;&#27748;&#26222;&#26862;&#25277;&#26679;&#39118;&#26684;&#21442;&#25968;&#25200;&#21160;&#26041;&#27861;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#36229;&#20986;&#24191;&#20041;&#32447;&#24615;&#36172;&#21338;&#30340;&#20363;&#23376;&#65292;&#20854;&#20013;PHE&#23548;&#33268;&#19981;&#19968;&#33268;&#30340;&#20272;&#35745;&#65292;&#20174;&#32780;&#23548;&#33268;&#32447;&#24615;&#21518;&#24724;&#65292;&#32780;EVILL&#21017;&#20445;&#25345;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07565v2 Announce Type: replace  Abstract: We introduce exploration via linear loss perturbations (EVILL), a randomised exploration method for structured stochastic bandit problems that works by solving for the minimiser of a linearly perturbed regularised negative log-likelihood function. We show that, for the case of generalised linear bandits, EVILL reduces to perturbed history exploration (PHE), a method where exploration is done by training on randomly perturbed rewards. In doing so, we provide a simple and clean explanation of when and why random reward perturbations give rise to good bandit algorithms. We propose data-dependent perturbations not present in previous PHE-type methods that allow EVILL to match the performance of Thompson-sampling-style parameter-perturbation methods, both in theory and in practice. Moreover, we show an example outside generalised linear bandits where PHE leads to inconsistent estimates, and thus linear regret, while EVILL remains performa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36830;&#32493;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;KuramotoGNN&#65292;&#36890;&#36807;&#37319;&#29992;Kuramoto&#27169;&#22411;&#26469;&#20943;&#36731;GNN&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#29616;&#35937;&#65292;&#23454;&#29616;&#33410;&#28857;&#29305;&#24449;&#30340;&#24046;&#24322;&#21270;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#32447;GNN&#21644;&#29616;&#26377;&#26041;&#27861;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.03260</link><description>&lt;p&gt;
&#20174;&#32806;&#21512;&#25391;&#33633;&#22120;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#22522;&#20110;Kuramoto&#27169;&#22411;&#30340;&#26041;&#27861;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
From Coupled Oscillators to Graph Neural Networks: Reducing Over-smoothing via a Kuramoto Model-based Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03260
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36830;&#32493;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;KuramotoGNN&#65292;&#36890;&#36807;&#37319;&#29992;Kuramoto&#27169;&#22411;&#26469;&#20943;&#36731;GNN&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#29616;&#35937;&#65292;&#23454;&#29616;&#33410;&#28857;&#29305;&#24449;&#30340;&#24046;&#24322;&#21270;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#32447;GNN&#21644;&#29616;&#26377;&#26041;&#27861;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Kuramoto&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;KuramotoGNN&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36830;&#32493;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#23427;&#37319;&#29992;Kuramoto&#27169;&#22411;&#26469;&#32531;&#35299;&#36807;&#24230;&#24179;&#28369;&#29616;&#35937;&#65292;&#21363;&#38543;&#30528;&#23618;&#25968;&#22686;&#21152;&#65292;GNN&#20013;&#33410;&#28857;&#29305;&#24449;&#21464;&#24471;&#38590;&#20197;&#21306;&#20998;&#30340;&#38382;&#39064;&#12290;Kuramoto&#27169;&#22411;&#25429;&#25417;&#20102;&#38750;&#32447;&#24615;&#32806;&#21512;&#25391;&#33633;&#22120;&#30340;&#21516;&#27493;&#34892;&#20026;&#12290;&#20174;&#32806;&#21512;&#25391;&#33633;&#22120;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;Kuramoto&#27169;&#22411;&#19982;&#22522;&#26412;GNN&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#28982;&#21518;&#35828;&#26126;&#20102;GNN&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#29616;&#35937;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;Kuramoto&#27169;&#22411;&#20013;&#30340;&#30456;&#20301;&#21516;&#27493;&#12290;KuramotoGNN&#29992;&#39057;&#29575;&#21516;&#27493;&#21462;&#20195;&#20102;&#36825;&#31181;&#30456;&#20301;&#21516;&#27493;&#65292;&#20197;&#38450;&#27490;&#33410;&#28857;&#29305;&#24449;&#25910;&#25947;&#21040;&#19968;&#36215;&#65292;&#21516;&#26102;&#20351;&#31995;&#32479;&#36798;&#21040;&#31283;&#23450;&#30340;&#21516;&#27493;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;KuramotoGNN&#22312;&#20943;&#23569;&#36807;&#24230;&#24179;&#28369;&#26041;&#38754;&#30456;&#23545;&#20110;&#22522;&#32447;GNN&#21644;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03260v2 Announce Type: replace-cross  Abstract: We propose the Kuramoto Graph Neural Network (KuramotoGNN), a novel class of continuous-depth graph neural networks (GNNs) that employs the Kuramoto model to mitigate the over-smoothing phenomenon, in which node features in GNNs become indistinguishable as the number of layers increases. The Kuramoto model captures the synchronization behavior of non-linear coupled oscillators. Under the view of coupled oscillators, we first show the connection between Kuramoto model and basic GNN and then over-smoothing phenomenon in GNNs can be interpreted as phase synchronization in Kuramoto model. The KuramotoGNN replaces this phase synchronization with frequency synchronization to prevent the node features from converging into each other while allowing the system to reach a stable synchronized state. We experimentally verify the advantages of the KuramotoGNN over the baseline GNNs and existing methods in reducing over-smoothing on various 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#24179;&#21488;&#21644;&#20998;&#26512;&#26041;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#21644;&#27604;&#36739;&#20102;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#20869;&#23481;&#65292;&#21457;&#29616;&#20102;&#20851;&#20110;&#35821;&#26009;&#24211;&#20869;&#23481;&#30340;&#20960;&#20010;&#20196;&#20154;&#24778;&#35766;&#19988;&#20197;&#21069;&#26410;&#34987;&#35760;&#24405;&#30340;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2310.20707</link><description>&lt;p&gt;
&#25105;&#30340;&#22823;&#25968;&#25454;&#20013;&#26377;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What's In My Big Data?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.20707
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#24179;&#21488;&#21644;&#20998;&#26512;&#26041;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#21644;&#27604;&#36739;&#20102;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#20869;&#23481;&#65292;&#21457;&#29616;&#20102;&#20851;&#20110;&#35821;&#26009;&#24211;&#20869;&#23481;&#30340;&#20960;&#20010;&#20196;&#20154;&#24778;&#35766;&#19988;&#20197;&#21069;&#26410;&#34987;&#35760;&#24405;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#26159;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#35821;&#26009;&#24211;&#30340;&#20869;&#23481;&#65292;&#21253;&#25324;&#19968;&#33324;&#32479;&#35745;&#20449;&#24687;&#12289;&#36136;&#37327;&#12289;&#31038;&#20250;&#22240;&#32032;&#21644;&#35780;&#20272;&#25968;&#25454;&#65288;&#27745;&#26579;&#65289;&#30340;&#29702;&#35299;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;What's In My Big Data?&#8221;&#65288;WIMBD&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#24179;&#21488;&#21644;&#19968;&#32452;&#21313;&#20845;&#20010;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25581;&#31034;&#21644;&#27604;&#36739;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#20869;&#23481;&#12290;WIMBD&#22522;&#20110;&#20004;&#31181;&#22522;&#26412;&#33021;&#21147;&#8212;&#8212;&#35745;&#25968;&#21644;&#25628;&#32034;&#8212;&#8212;&#22312;&#35268;&#27169;&#19978;&#36827;&#34892;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#26631;&#20934;&#35745;&#31639;&#33410;&#28857;&#19978;&#20998;&#26512;&#36229;&#36807;35TB&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;WIMBD&#24212;&#29992;&#20110;&#29992;&#20110;&#35757;&#32451;&#27969;&#34892;&#35821;&#35328;&#27169;&#22411;&#30340;&#21313;&#20010;&#19981;&#21516;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;C4&#12289;The Pile&#21644;RedPajama&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#20851;&#20110;&#36825;&#20123;&#35821;&#26009;&#24211;&#30340;&#20960;&#20010;&#20196;&#20154;&#24778;&#35766;&#19988;&#20197;&#21069;&#26410;&#35760;&#24405;&#30340;&#21457;&#29616;&#65292;&#21253;&#25324;&#37325;&#22797;&#20869;&#23481;&#12289;&#21512;&#25104;&#20869;&#23481;&#12289;&#20302;&#36136;&#37327;&#20869;&#23481;&#12289;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#12289;&#26377;&#27602;&#35821;&#35328;&#21644;&#22522;&#20934;&#27745;&#26579;&#30340;&#39640;&#27969;&#34892;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.20707v2 Announce Type: replace  Abstract: Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose What's In My Big Data? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities -- count and search -- at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#21355;&#26143;&#25968;&#25454;&#23545;&#22320;&#38754;&#38647;&#36798;&#22270;&#20687;&#24207;&#21015;&#36827;&#34892;&#21363;&#26102;&#39044;&#25253;&#65292;&#33021;&#22815;&#26377;&#25928;&#24357;&#21512;&#22320;&#38754;&#21644;&#31354;&#38388;&#35266;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#39640;&#22825;&#27668;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.19515</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#21355;&#26143;&#22270;&#20687;&#23545;&#38647;&#36798;&#22797;&#21512;&#29289;&#30340;&#21363;&#26102;&#39044;&#25253;&#65292;&#29992;&#20110;&#20005;&#37325;&#22825;&#27668;
&lt;/p&gt;
&lt;p&gt;
Transformer-based nowcasting of radar composites from satellite images for severe weather
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19515
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#21355;&#26143;&#25968;&#25454;&#23545;&#22320;&#38754;&#38647;&#36798;&#22270;&#20687;&#24207;&#21015;&#36827;&#34892;&#21363;&#26102;&#39044;&#25253;&#65292;&#33021;&#22815;&#26377;&#25928;&#24357;&#21512;&#22320;&#38754;&#21644;&#31354;&#38388;&#35266;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#39640;&#22825;&#27668;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#38647;&#36798;&#25968;&#25454;&#23545;&#20110;&#21363;&#26102;&#39044;&#25253;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#26159;&#25968;&#20540;&#22825;&#27668;&#39044;&#27979;&#27169;&#22411;&#30340;&#19968;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#23613;&#31649;&#22825;&#27668;&#38647;&#36798;&#25968;&#25454;&#25552;&#20379;&#20102;&#39640;&#20998;&#36776;&#29575;&#30340;&#23453;&#36149;&#20449;&#24687;&#65292;&#20294;&#20854;&#22522;&#20110;&#22320;&#38754;&#30340;&#29305;&#24615;&#38480;&#21046;&#20102;&#20854;&#21487;&#29992;&#24615;&#65292;&#36825;&#38459;&#30861;&#20102;&#22823;&#35268;&#27169;&#24212;&#29992;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#27668;&#35937;&#21355;&#26143;&#35206;&#30422;&#20102;&#26356;&#22823;&#30340;&#21306;&#22495;&#65292;&#20294;&#20998;&#36776;&#29575;&#26356;&#20302;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#21644;&#22320;&#29699;&#21516;&#27493;&#21355;&#26143;&#19978;&#29616;&#20195;&#20256;&#24863;&#22120;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#26032;&#30340;&#26426;&#20250;&#27491;&#22312;&#20986;&#29616;&#65292;&#20197;&#24357;&#21512;&#22320;&#38754;&#21644;&#31354;&#38388;&#35266;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#26368;&#32456;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39640;&#25216;&#33021;&#22825;&#27668;&#39044;&#25253;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21355;&#26143;&#25968;&#25454;&#23545;&#22320;&#38754;&#38647;&#36798;&#22270;&#20687;&#24207;&#21015;&#36827;&#34892;&#21363;&#26102;&#39044;&#25253;&#65292;&#25552;&#21069;&#20004;&#23567;&#26102;&#12290;&#35813;&#27169;&#22411;&#22312;&#21453;&#26144;&#20005;&#37325;&#22825;&#27668;&#26465;&#20214;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#39044;&#27979;&#20986;&#29616;&#22312;&#19981;&#21516;&#22825;&#27668;&#29616;&#35937;&#19979;&#30340;&#38647;&#36798;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19515v2 Announce Type: replace-cross  Abstract: Weather radar data are critical for nowcasting and an integral component of numerical weather prediction models. While weather radar data provide valuable information at high resolution, their ground-based nature limits their availability, which impedes large-scale applications. In contrast, meteorological satellites cover larger domains but with coarser resolution. However, with the rapid advancements in data-driven methodologies and modern sensors aboard geostationary satellites, new opportunities are emerging to bridge the gap between ground- and space-based observations, ultimately leading to more skillful weather prediction with high accuracy. Here, we present a Transformer-based model for nowcasting ground-based radar image sequences using satellite data up to two hours lead time. Trained on a dataset reflecting severe weather conditions, the model predicts radar fields occurring under different weather phenomena and show
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#36830;&#32493;&#39550;&#39542;&#25919;&#31574;&#20248;&#21270;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#38381;&#29615;&#20010;&#24615;&#21270;&#35838;&#31243;&#65288;CLIC&#65289;&#27010;&#24565;&#65292;&#20801;&#35768;&#37325;&#22797;&#21033;&#29992;&#24191;&#27867;&#22330;&#26223;&#26469;&#36845;&#20195;&#25913;&#36827;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2309.14209</link><description>&lt;p&gt;
&#20855;&#26377;&#38381;&#29615;&#20010;&#24615;&#21270;&#35838;&#31243;&#30340;&#36830;&#32493;&#39550;&#39542;&#25919;&#31574;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Continual Driving Policy Optimization with Closed-Loop Individualized Curricula
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.14209
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#36830;&#32493;&#39550;&#39542;&#25919;&#31574;&#20248;&#21270;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#38381;&#29615;&#20010;&#24615;&#21270;&#35838;&#31243;&#65288;CLIC&#65289;&#27010;&#24565;&#65292;&#20801;&#35768;&#37325;&#22797;&#21033;&#29992;&#24191;&#27867;&#22330;&#26223;&#26469;&#36845;&#20195;&#25913;&#36827;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#30340;&#23433;&#20840;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#22836;&#31561;&#20851;&#27880;&#28857;&#65292;&#26681;&#28304;&#20110;&#38271;&#23614;&#33258;&#28982;&#39550;&#39542;&#20998;&#24067;&#20013;&#32597;&#35265;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#30340;&#32570;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20986;&#29616;&#20102;&#22823;&#37327;&#22522;&#20110;&#22330;&#26223;&#30340;&#33258;&#21160;&#39550;&#39542;&#30740;&#31350;&#65292;&#37325;&#28857;&#26159;&#29983;&#25104;&#39640;&#39118;&#38505;&#39550;&#39542;&#22330;&#26223;&#24182;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#23545;AV&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#20851;&#38190;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#24037;&#20316;&#25506;&#35752;&#20102;&#37325;&#22797;&#21033;&#29992;&#36825;&#20123;&#24191;&#27867;&#22330;&#26223;&#26469;&#36845;&#20195;&#25913;&#36827;AV&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20174;&#20855;&#26377;&#19981;&#21516;&#34892;&#20026;&#30340;&#20854;&#20182;AV&#27169;&#22411;&#25910;&#38598;&#30340;&#24040;&#22823;&#22330;&#26223;&#24211;&#20013;&#28388;&#20986;&#21487;&#20256;&#36882;&#20449;&#24687;&#20197;&#25913;&#36827;&#24403;&#21069;AV&#20173;&#28982;&#26159;&#38590;&#20197;&#35299;&#20915;&#30340;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#26377;&#38381;&#29615;&#20010;&#24615;&#21270;&#35838;&#31243;&#65288;CLIC&#65289;&#29305;&#28857;&#30340;&#36830;&#32493;&#39550;&#39542;&#25919;&#31574;&#20248;&#21270;&#26694;&#26550;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#35299;&#20026;&#19968;&#32452;&#26631;&#20934;&#21270;&#30340;&#23376;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.14209v3 Announce Type: replace-cross  Abstract: The safety of autonomous vehicles (AV) has been a long-standing top concern, stemming from the absence of rare and safety-critical scenarios in the long-tail naturalistic driving distribution. To tackle this challenge, a surge of research in scenario-based autonomous driving has emerged, with a focus on generating high-risk driving scenarios and applying them to conduct safety-critical testing of AV models. However, limited work has been explored on the reuse of these extensive scenarios to iteratively improve AV models. Moreover, it remains intractable and challenging to filter through gigantic scenario libraries collected from other AV models with distinct behaviors, attempting to extract transferable information for current AV improvement. Therefore, we develop a continual driving policy optimization framework featuring Closed-Loop Individualized Curricula (CLIC), which we factorize into a set of standardized sub-modules for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;Koopman&#20808;&#39564;&#36827;&#34892;&#25968;&#25454;&#21516;&#21270;&#30340;&#26041;&#27861;&#65292;&#23558;&#21160;&#24577;&#31995;&#32479;&#23884;&#20837;&#21040;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#24471;&#23545;&#20854;&#21160;&#24577;&#36827;&#34892;&#32447;&#24615;&#25551;&#36848;&#65292;&#24182;&#23637;&#31034;&#20102;&#38271;&#26399;&#36830;&#32493;&#37325;&#26500;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2309.05317</link><description>&lt;p&gt;
&#25968;&#25454;&#21516;&#21270;&#30340;&#31070;&#32463;Koopman&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Neural Koopman prior for data assimilation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.05317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;Koopman&#20808;&#39564;&#36827;&#34892;&#25968;&#25454;&#21516;&#21270;&#30340;&#26041;&#27861;&#65292;&#23558;&#21160;&#24577;&#31995;&#32479;&#23884;&#20837;&#21040;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#24471;&#23545;&#20854;&#21160;&#24577;&#36827;&#34892;&#32447;&#24615;&#25551;&#36848;&#65292;&#24182;&#23637;&#31034;&#20102;&#38271;&#26399;&#36830;&#32493;&#37325;&#26500;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12289;&#35745;&#31639;&#33021;&#21147;&#21644;&#35832;&#22914;&#33258;&#21160;&#24494;&#20998;&#21644;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#31561;&#24037;&#20855;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#24207;&#36143;&#25968;&#25454;&#29616;&#22312;&#32463;&#24120;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#22788;&#29702;&#65292;&#36890;&#36807;&#20174;&#35266;&#27979;&#25968;&#25454;&#35757;&#32451;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#34987;&#35270;&#20026;&#19981;&#21487;&#35299;&#37322;&#30340;&#40657;&#30418;&#26550;&#26500;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#21487;&#20197;&#21463;&#30410;&#20110;&#25968;&#25454;&#30340;&#29289;&#29702;&#20808;&#39564;&#21644;&#25968;&#23398;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#21033;&#29992;&#38271;&#26399;&#20197;&#26469;&#24050;&#30693;&#30340;Koopman&#31639;&#23376;&#29702;&#35770;&#65292;&#23558;&#21160;&#21147;&#31995;&#32479;&#23884;&#20837;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#20854;&#20013;&#23427;&#20204;&#30340;&#21160;&#24577;&#21487;&#20197;&#34987;&#32447;&#24615;&#25551;&#36848;&#65292;&#20174;&#32780;&#21576;&#29616;&#20986;&#35768;&#22810;&#21560;&#24341;&#20154;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20351;&#35813;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#38271;&#26399;&#36830;&#32493;&#37325;&#26500;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#25968;&#25454;&#21576;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#22256;&#38590;&#24773;&#22659;&#20013;&#20063;&#21487;&#20197;&#39034;&#21033;&#36827;&#34892;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;&#25105;&#20204;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.05317v2 Announce Type: replace  Abstract: With the increasing availability of large scale datasets, computational power and tools like automatic differentiation and expressive neural network architectures, sequential data are now often treated in a data-driven way, with a dynamical model trained from the observation data. While neural networks are often seen as uninterpretable black-box architectures, they can still benefit from physical priors on the data and from mathematical knowledge. In this paper, we use a neural network architecture which leverages the long-known Koopman operator theory to embed dynamical systems in latent spaces where their dynamics can be described linearly, enabling a number of appealing features. We introduce methods that enable to train such a model for long-term continuous reconstruction, even in difficult contexts where the data comes in irregularly-sampled time series. The potential for self-supervised learning is also demonstrated, as we show
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#27491;&#24577;&#20998;&#24067;&#19981;&#21487;&#21306;&#20998;&#24615;&#35889;&#23450;&#29702; (NDIS Theorem)&#65292;&#26088;&#22312;&#21033;&#29992;&#26597;&#35810;&#26412;&#36523;&#30340;&#38543;&#26426;&#24615;&#25913;&#36827;&#38543;&#26426;&#21270;&#26426;&#22120;&#23398;&#20064;&#26597;&#35810;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2309.01243</link><description>&lt;p&gt;
&#27491;&#24577;&#20998;&#24067;&#19981;&#21487;&#21306;&#20998;&#24615;&#35889;&#21450;&#20854;&#22312;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Normal Distributions Indistinguishability Spectrum and its Application to Privacy-Preserving Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#27491;&#24577;&#20998;&#24067;&#19981;&#21487;&#21306;&#20998;&#24615;&#35889;&#23450;&#29702; (NDIS Theorem)&#65292;&#26088;&#22312;&#21033;&#29992;&#26597;&#35810;&#26412;&#36523;&#30340;&#38543;&#26426;&#24615;&#25913;&#36827;&#38543;&#26426;&#21270;&#26426;&#22120;&#23398;&#20064;&#26597;&#35810;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;(DP)&#65292;&#36890;&#24120;&#38656;&#35201;&#38543;&#26426;&#21270;&#22522;&#30784;&#26597;&#35810;&#30340;&#36755;&#20986;&#12290;&#22312;&#22823;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#20351;&#29992;&#38543;&#26426;&#21270;&#33609;&#22270;/&#32858;&#21512;&#31639;&#27861;&#26469;&#20351;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#21464;&#24471;&#21487;&#34892;&#12290;&#30452;&#35266;&#22320;&#65292;&#36825;&#26679;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#31639;&#27861;&#24212;&#35813;&#25552;&#20379;&#19968;&#20123;&#22266;&#26377;&#30340;&#38544;&#31169;&#24615;&#65292;&#20294;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;DP&#26426;&#21046;&#24182;&#27809;&#26377;&#21033;&#29992;&#36825;&#31181;&#22266;&#26377;&#30340;&#38543;&#26426;&#24615;&#65292;&#23548;&#33268;&#28508;&#22312;&#30340;&#22810;&#20313;&#22122;&#38899;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#21160;&#26426;&#38382;&#39064;&#26159;&#65306;(&#22914;&#20309;)&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26597;&#35810;&#26412;&#36523;&#30340;&#38543;&#26426;&#24615;&#26469;&#25552;&#39640;&#38543;&#26426;&#21270;ML&#26597;&#35810;&#30340;DP&#26426;&#21046;&#30340;&#25928;&#29992;&#65311;&#20026;&#20102;&#32473;&#20986;&#31215;&#26497;&#30340;&#31572;&#26696;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27491;&#24577;&#20998;&#24067;&#19981;&#21487;&#21306;&#20998;&#24615;&#35889;&#23450;&#29702;(&#31616;&#31216;&#20026;NDIS&#23450;&#29702;)&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#28145;&#36828;&#23454;&#38469;&#24433;&#21709;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;NDIS&#26159;&#19968;&#20010;&#29992;&#20110;$(\epsilon,\delta)$-&#19981;&#21487;&#21306;&#20998;&#24615;&#35889;(&#31616;&#31216;&#20026;$
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01243v2 Announce Type: replace-cross  Abstract: To achieve differential privacy (DP) one typically randomizes the output of the underlying query. In big data analytics, one often uses randomized sketching/aggregation algorithms to make processing high-dimensional data tractable. Intuitively, such machine learning (ML) algorithms should provide some inherent privacy, yet most if not all existing DP mechanisms do not leverage this inherent randomness, resulting in potentially redundant noising.   The motivating question of our work is:   (How) can we improve the utility of DP mechanisms for randomized ML queries, by leveraging the randomness of the query itself?   Towards a (positive) answer, we prove the Normal Distributions Indistinguishability Spectrum Theorem (in short, NDIS Theorem), a theoretical result with far-reaching practical implications. In a nutshell, NDIS is a closed-form analytic computation for the $(\epsilon,\delta)$-indistinguishability-spectrum (in short, $
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;PURL&#33021;&#22815;&#23433;&#20840;&#26377;&#25928;&#22320;&#28165;&#27905;&#38142;&#25509;&#35013;&#39280;&#65292;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#23545;&#31574;&#65292;&#32780;&#19988;&#23545;&#24120;&#35265;&#35268;&#36991;&#25216;&#26415;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2308.03417</link><description>&lt;p&gt;
PURL: &#38142;&#25509;&#35013;&#39280;&#30340;&#23433;&#20840;&#26377;&#25928;&#28165;&#27905;
&lt;/p&gt;
&lt;p&gt;
PURL: Safe and Effective Sanitization of Link Decoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.03417
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;PURL&#33021;&#22815;&#23433;&#20840;&#26377;&#25928;&#22320;&#28165;&#27905;&#38142;&#25509;&#35013;&#39280;&#65292;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#23545;&#31574;&#65292;&#32780;&#19988;&#23545;&#24120;&#35265;&#35268;&#36991;&#25216;&#26415;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20197;&#38544;&#31169;&#20026;&#37325;&#28857;&#30340;&#27983;&#35272;&#22120;&#24050;&#32463;&#37319;&#21462;&#20102;&#38459;&#27490;&#31532;&#19977;&#26041;cookie&#21644;&#20943;&#23569;&#27983;&#35272;&#22120;&#25351;&#32441;&#35782;&#21035;&#30340;&#25514;&#26045;&#65292;&#20294;&#20173;&#28982;&#19981;&#26029;&#20986;&#29616;&#21487;&#20197;&#32469;&#36807;&#29616;&#26377;&#23545;&#31574;&#30340;&#26032;&#22411;&#36319;&#36394;&#25216;&#26415;&#12290;&#37492;&#20110;&#36319;&#36394;&#22120;&#38656;&#35201;&#36890;&#36807;&#38142;&#25509;&#35013;&#39280;&#20174;&#23458;&#25143;&#31471;&#21521;&#26381;&#21153;&#22120;&#31471;&#20849;&#20139;&#20449;&#24687;&#65292;&#26080;&#35770;&#20182;&#20204;&#37319;&#29992;&#20309;&#31181;&#36319;&#36394;&#25216;&#26415;&#65292;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#26816;&#27979;&#21644;&#28165;&#27905;&#35013;&#39280;&#38142;&#25509;&#20013;&#30340;&#36319;&#36394;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PURL&#65288;&#21457;&#38899;&#20026;purel-l&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#32593;&#39029;&#25191;&#34892;&#30340;&#36328;&#23618;&#22270;&#34920;&#31034;&#26469;&#23433;&#20840;&#26377;&#25928;&#22320;&#28165;&#27905;&#38142;&#25509;&#35013;&#39280;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;PURL&#22312;&#20934;&#30830;&#24615;&#21644;&#20943;&#23569;&#32593;&#31449;&#30772;&#22351;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#23545;&#31574;&#65292;&#24182;&#19988;&#23545;&#24120;&#35265;&#30340;&#35268;&#36991;&#25216;&#26415;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#19968;&#37096;&#20998;&#21069;&#19968;&#30334;&#19975;&#32593;&#31449;&#19978;&#37096;&#32626;PURL&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#38142;&#25509;&#35013;&#39280;&#34987;&#28389;&#29992;&#29992;&#20110;&#36319;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.03417v2 Announce Type: replace-cross  Abstract: While privacy-focused browsers have taken steps to block third-party cookies and mitigate browser fingerprinting, novel tracking techniques that can bypass existing countermeasures continue to emerge. Since trackers need to share information from the client-side to the server-side through link decoration regardless of the tracking technique they employ, a promising orthogonal approach is to detect and sanitize tracking information in decorated links. To this end, we present PURL (pronounced purel-l), a machine-learning approach that leverages a cross-layer graph representation of webpage execution to safely and effectively sanitize link decoration. Our evaluation shows that PURL significantly outperforms existing countermeasures in terms of accuracy and reducing website breakage while being robust to common evasion techniques. PURL's deployment on a sample of top-million websites shows that link decoration is abused for trackin
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Tensor Homomorphic Compression (THC)&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#21521;&#21387;&#32553;&#26694;&#26550;&#65292;&#21487;&#20197;&#21152;&#36895;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#35757;&#32451;</title><link>https://arxiv.org/abs/2302.08545</link><description>&lt;p&gt;
THC&#65306;&#20351;&#29992;&#24352;&#37327;&#21516;&#24577;&#21387;&#32553;&#21152;&#36895;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
THC: Accelerating Distributed Deep Learning Using Tensor Homomorphic Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.08545
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Tensor Homomorphic Compression (THC)&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#21521;&#21387;&#32553;&#26694;&#26550;&#65292;&#21487;&#20197;&#21152;&#36895;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#24517;&#35201;&#29992;&#20363;&#65288;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#38543;&#30528;DNNs&#21644;&#25968;&#25454;&#38598;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#65292;&#23427;&#20204;&#38656;&#35201;&#22312;&#36234;&#26469;&#36234;&#22823;&#30340;&#38598;&#32676;&#19978;&#36827;&#34892;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290; &#20027;&#35201;&#29942;&#39048;&#26159;&#30001;&#24037;&#20316;&#32773;&#22312;&#27599;&#36718;&#22522;&#30784;&#19978;&#20132;&#25442;&#27169;&#22411;&#26356;&#26032;&#65288;&#21363;&#26799;&#24230;&#65289;&#20135;&#29983;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#29942;&#39048;&#24182;&#21152;&#36895;&#35757;&#32451;&#65292;&#19968;&#20010;&#24191;&#27867;&#37096;&#32626;&#30340;&#26041;&#27861;&#26159;&#21387;&#32553;&#12290; &#20294;&#26159;&#65292;&#20808;&#21069;&#30340;&#37096;&#32626;&#36890;&#24120;&#21482;&#26159;&#22312;&#27599;&#20010;&#26041;&#21521;&#19978;&#20351;&#29992;&#21333;&#26041;&#21521;&#26799;&#24230;&#21387;&#32553;&#26041;&#26696;&#26469;&#24212;&#29992;&#21452;&#21521;&#21387;&#32553;&#26041;&#26696;&#12290; &#36825;&#23548;&#33268;&#21442;&#25968;&#26381;&#21153;&#22120;&#19978;&#30340;&#26174;&#30528;&#35745;&#31639;&#24320;&#38144;&#21644;&#21387;&#32553;&#35823;&#24046;&#22686;&#21152;&#65292;&#20174;&#32780;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#26356;&#38271;&#21644;&#20934;&#30830;&#24615;&#26356;&#20302;&#12290; &#25105;&#20204;&#20171;&#32461;&#20102;&#24352;&#37327;&#21516;&#24577;&#21387;&#32553;&#65288;THC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#21521;&#21387;&#32553;&#26694;&#26550;&#65292;&#33021;&#22815;&#30452;&#25509;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.08545v2 Announce Type: replace-cross  Abstract: Deep neural networks (DNNs) are the de facto standard for essential use cases, such as image classification, computer vision, and natural language processing. As DNNs and datasets get larger, they require distributed training on increasingly larger clusters. A main bottleneck is the resulting communication overhead where workers exchange model updates (i.e., gradients) on a per-round basis. To address this bottleneck and accelerate training, a widely-deployed approach is compression. However, previous deployments often apply bi-directional compression schemes by simply using a uni-directional gradient compression scheme in each direction. This results in significant computational overheads at the parameter server and increased compression error, leading to longer training and lower accuracy. We introduce Tensor Homomorphic Compression (THC), a novel bi-directional compression framework that enables the direct aggregation of com
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#22270;&#20013;&#26816;&#27979;&#38544;&#34255;&#30340;&#31181;&#26893;&#20108;&#20998;&#22270;&#23376;&#22270;&#30340;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#32479;&#35745;&#21644;&#35745;&#31639;&#38556;&#30861;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;</title><link>https://arxiv.org/abs/2302.03658</link><description>&lt;p&gt;
&#21457;&#29616;&#31181;&#26893;&#30340;&#20108;&#20998;&#22270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Planted Bipartite Graph Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.03658
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#22270;&#20013;&#26816;&#27979;&#38544;&#34255;&#30340;&#31181;&#26893;&#20108;&#20998;&#22270;&#23376;&#22270;&#30340;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#32479;&#35745;&#21644;&#35745;&#31639;&#38556;&#30861;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#32473;&#23450;&#30340;&#38543;&#26426;&#22270;&#20013;&#26816;&#27979;&#38544;&#34255;&#30340;&#20108;&#20998;&#22270;&#23376;&#22270;&#30340;&#20219;&#21153;&#12290;&#36825;&#34987;&#35268;&#21010;&#20026;&#19968;&#20010;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#65292;&#22312;&#38646;&#20551;&#35774;&#19979;&#65292;&#22270;&#26159;&#20855;&#26377;$n$&#20010;&#39030;&#28857;&#21644;&#36793;&#23494;&#24230;$q$&#30340;Erd\H{o}s-R\'{e}nyi&#38543;&#26426;&#22270;&#30340;&#19968;&#20010;&#23454;&#29616;&#12290;&#22312;&#22791;&#25321;&#20551;&#35774;&#19979;&#65292;&#23384;&#22312;&#19968;&#20010;&#31181;&#26893;&#30340;$k_{\mathsf{R}} \times k_{\mathsf{L}}$&#20108;&#20998;&#22270;&#23376;&#22270;&#65292;&#36793;&#23494;&#24230;&#20026;$p&gt;q$&#12290;&#25105;&#20204;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#38556;&#30861;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20449;&#24687;&#35770;&#19979;&#30028;&#65292;&#24182;&#35774;&#35745;&#21644;&#20998;&#26512;&#21305;&#37197;&#36825;&#20123;&#19979;&#30028;&#30340;&#26368;&#20248;&#31639;&#27861;&#65292;&#22312;&#23494;&#38598;&#21306;&#22495;&#21644;&#31232;&#30095;&#21306;&#22495;&#37117;&#26159;&#22914;&#27492;&#65292;&#20854;&#20013;$p,q = \Theta\left(1\right)$&#22312;&#23494;&#38598;&#21306;&#22495;&#65292;$p,q = \Theta\left(n^{-\alpha}\right), \alpha \in \left(0,2\right]$&#22312;&#31232;&#30095;&#21306;&#22495;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#30340;&#27979;&#35797;&#38382;&#39064;&#12290;&#19982;&#31867;&#20284;&#32467;&#26500;&#21270;&#39640;&#32500;&#38382;&#39064;&#19968;&#26679;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32463;&#21382;&#20102;"easy-hard-impossible"&#38454;&#27573;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.03658v2 Announce Type: replace-cross  Abstract: We consider the task of detecting a hidden bipartite subgraph in a given random graph. This is formulated as a hypothesis testing problem, under the null hypothesis, the graph is a realization of an Erd\H{o}s-R\'{e}nyi random graph over $n$ vertices with edge density $q$. Under the alternative, there exists a planted $k_{\mathsf{R}} \times k_{\mathsf{L}}$ bipartite subgraph with edge density $p&gt;q$. We characterize the statistical and computational barriers for this problem. Specifically, we derive information-theoretic lower bounds, and design and analyze optimal algorithms matching those bounds, in both the dense regime, where $p,q = \Theta\left(1\right)$, and the sparse regime where $p,q = \Theta\left(n^{-\alpha}\right), \alpha \in \left(0,2\right]$. We also consider the problem of testing in polynomial-time. As is customary in similar structured high-dimensional problems, our model undergoes an "easy-hard-impossible" phase t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;$\lambda$-&#20998;&#25968;&#20840;&#21516;&#32500;&#25968;&#65292;&#23454;&#29616;&#20102;&#26641;&#37325;&#26032;&#21152;&#26435;&#21644;&#20449;&#24565;&#20256;&#25773;&#31639;&#27861;&#20043;&#38388;&#30340;&#25554;&#20540;&#65292;&#30830;&#20445;&#22312;&#38081;&#30913;&#24615;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#8220;&#31934;&#30830;&#8221;$\lambda_*$&#20351;&#24471;&#35745;&#31639;&#30340;&#37197;&#20998;&#20989;&#25968;$Z=Z^{(\lambda_*)}$&#12290;</title><link>https://arxiv.org/abs/2301.10369</link><description>&lt;p&gt;
&#31934;&#30830;&#20998;&#25968;&#25512;&#26029;&#65306;&#37325;&#26032;&#21442;&#25968;&#21270;&#21450;&#26641;&#37325;&#26032;&#21152;&#26435;&#21644;&#20449;&#24565;&#20256;&#25773;&#31639;&#27861;&#20043;&#38388;&#30340;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Exact Fractional Inference via Re-Parametrization &amp; Interpolation between Tree-Re-Weighted- and Belief Propagation- Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.10369
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;$\lambda$-&#20998;&#25968;&#20840;&#21516;&#32500;&#25968;&#65292;&#23454;&#29616;&#20102;&#26641;&#37325;&#26032;&#21152;&#26435;&#21644;&#20449;&#24565;&#20256;&#25773;&#31639;&#27861;&#20043;&#38388;&#30340;&#25554;&#20540;&#65292;&#30830;&#20445;&#22312;&#38081;&#30913;&#24615;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#8220;&#31934;&#30830;&#8221;$\lambda_*$&#20351;&#24471;&#35745;&#31639;&#30340;&#37197;&#20998;&#20989;&#25968;$Z=Z^{(\lambda_*)}$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#26029;&#24037;&#20316;--&#35745;&#31639;Ising&#27169;&#22411;&#22312;N&#20010;&#8220;&#33258;&#26059;&#8221;&#32452;&#25104;&#30340;&#22270;&#19978;&#30340;&#37197;&#20998;&#20989;&#25968;$Z$&#25152;&#38656;&#30340;&#24037;&#20316;--&#24456;&#21487;&#33021;&#38543;&#30528;N&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#39640;&#25928;&#30340;&#21464;&#20998;&#26041;&#27861;&#65292;&#22914;&#20449;&#24565;&#20256;&#25773;&#65288;BP&#65289;&#21644;&#26641;&#37325;&#26032;&#21152;&#26435;&#65288;TRW&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#26368;&#23567;&#21270;&#21508;&#33258;&#65288;BP&#25110;TRW&#65289;&#33258;&#30001;&#33021;&#30340;$Z$&#26469;&#36817;&#20284;&#35745;&#31639;$Z$&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;$\lambda$-&#20998;&#25968;&#20840;&#21516;&#32500;&#25968;&#65292;$Z^{(\lambda)}$&#65292;&#20854;&#20013;$\lambda=0$&#21644;$\lambda=1$&#20998;&#21035;&#23545;&#24212;&#20110;TRW&#21644;BP&#30340;&#36817;&#20284;&#65292;&#19988;$Z^{(\lambda)}$&#38543;$\lambda$&#21333;&#35843;&#20943;&#23569;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#20998;&#25968;&#26041;&#26696;&#20445;&#35777;&#22312;&#21560;&#24341;&#21147;&#65288;&#38081;&#30913;&#24615;&#65289;&#24773;&#20917;&#19979;$Z^{(TRW)}\geq Z^{(\lambda)}\geq Z^{(BP)}$&#65292;&#24182;&#19988;&#23384;&#22312;&#19968;&#20010;&#21807;&#19968;&#30340;&#65288;&#8220;&#31934;&#30830;&#8221;&#65289;$\lambda_*$&#65292;&#20351;&#24471;$Z=Z^{(\lambda_*)}$&#12290;&#36890;&#36807;&#25512;&#24191;\citep {wainwright_tree-based_2002}&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#21644;\citep {chertkov_loop_2006}&#30340;&#29615;&#32423;&#25968;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36827;&#34892;e
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.10369v2 Announce Type: replace  Abstract: Inference efforts -- required to compute partition function, $Z$, of an Ising model over a graph of $N$ ``spins" -- are most likely exponential in $N$. Efficient variational methods, such as Belief Propagation (BP) and Tree Re-Weighted (TRW) algorithms, compute $Z$ approximately minimizing respective (BP- or TRW-) free energy. We generalize the variational scheme building a $\lambda$-fractional-homotopy, $Z^{(\lambda)}$, where $\lambda=0$ and $\lambda=1$ correspond to TRW- and BP-approximations, respectively, and $Z^{(\lambda)}$ decreases with $\lambda$ monotonically. Moreover, this fractional scheme guarantees that in the attractive (ferromagnetic) case $Z^{(TRW)}\geq Z^{(\lambda)}\geq Z^{(BP)}$, and there exists a unique (``exact") $\lambda_*$ such that, $Z=Z^{(\lambda_*)}$. Generalizing the re-parametrization approach of \citep{wainwright_tree-based_2002} and the loop series approach of \citep{chertkov_loop_2006}, we show how to e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;ST-SSL&#65289;&#20132;&#36890;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#36741;&#21161;&#33258;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#22686;&#24378;&#20132;&#36890;&#27169;&#24335;&#34920;&#31034;&#65292;&#26082;&#21453;&#26144;&#31354;&#38388;&#24322;&#36136;&#24615;&#21448;&#21453;&#26144;&#26102;&#38388;&#24322;&#36136;&#24615;&#12290;</title><link>https://arxiv.org/abs/2212.04475</link><description>&lt;p&gt;
&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#26102;&#31354;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Self-Supervised Learning for Traffic Flow Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.04475
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;ST-SSL&#65289;&#20132;&#36890;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#36741;&#21161;&#33258;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#22686;&#24378;&#20132;&#36890;&#27169;&#24335;&#34920;&#31034;&#65292;&#26082;&#21453;&#26144;&#31354;&#38388;&#24322;&#36136;&#24615;&#21448;&#21453;&#26144;&#26102;&#38388;&#24322;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#33539;&#22260;&#20869;&#22312;&#19981;&#21516;&#26102;&#38388;&#27573;&#23545;&#20132;&#36890;&#27969;&#37327;&#36827;&#34892;&#31283;&#20581;&#39044;&#27979;&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#20026;&#24314;&#27169;&#26102;&#31354;&#30456;&#20851;&#24615;&#20570;&#20986;&#20102;&#24040;&#22823;&#21162;&#21147;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;i) &#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#39044;&#27979;&#25152;&#26377;&#21306;&#22495;&#30340;&#27969;&#37327;&#26102;&#27809;&#26377;&#32771;&#34385;&#31354;&#38388;&#24322;&#36136;&#24615;&#65292;&#21363;&#19981;&#21516;&#21306;&#22495;&#21487;&#33021;&#20855;&#26377;&#20542;&#26012;&#30340;&#20132;&#36890;&#27969;&#37327;&#20998;&#24067;&#12290; ii) &#36825;&#20123;&#27169;&#22411;&#26410;&#33021;&#25429;&#25417;&#30001;&#20110;&#26102;&#38388;&#21464;&#21270;&#30340;&#20132;&#36890;&#27169;&#24335;&#32780;&#24341;&#36215;&#30340;&#26102;&#38388;&#24322;&#36136;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#20351;&#29992;&#19968;&#20010;&#20849;&#20139;&#21442;&#25968;&#21270;&#31354;&#38388;&#26469;&#27169;&#25311;&#25152;&#26377;&#26102;&#38388;&#27573;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;ST-SSL&#65289;&#20132;&#36890;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#36741;&#21161;&#33258;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#22686;&#24378;&#20132;&#36890;&#27169;&#24335;&#34920;&#31034;&#65292;&#20351;&#20854;&#26082;&#21453;&#26144;&#31354;&#38388;&#24322;&#36136;&#24615;&#21448;&#21453;&#26144;&#26102;&#38388;&#24322;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.04475v2 Announce Type: replace-cross  Abstract: Robust prediction of citywide traffic flows at different time periods plays a crucial role in intelligent transportation systems. While previous work has made great efforts to model spatio-temporal correlations, existing methods still suffer from two key limitations: i) Most models collectively predict all regions' flows without accounting for spatial heterogeneity, i.e., different regions may have skewed traffic flow distributions. ii) These models fail to capture the temporal heterogeneity induced by time-varying traffic patterns, as they typically model temporal correlations with a shared parameterized space for all time periods. To tackle these challenges, we propose a novel Spatio-Temporal Self-Supervised Learning (ST-SSL) traffic prediction framework which enhances the traffic pattern representations to be reflective of both spatial and temporal heterogeneity, with auxiliary self-supervised learning paradigms. Specificall
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32431;&#31929;&#23616;&#37096;&#20449;&#24687;&#23454;&#29616;&#25512;&#27979;&#23545;&#25163;&#24314;&#27169;&#30340;&#22810;&#26234;&#33021;&#20307;&#20998;&#24067;&#24335;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#33021;&#22815;&#24110;&#21161;&#21463;&#25511;&#20195;&#29702;&#20570;&#20986;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2211.11940</link><description>&lt;p&gt;
&#29992;&#25512;&#27979;&#23545;&#25163;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Decision-making with Speculative Opponent Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.11940
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32431;&#31929;&#23616;&#37096;&#20449;&#24687;&#23454;&#29616;&#25512;&#27979;&#23545;&#25163;&#24314;&#27169;&#30340;&#22810;&#26234;&#33021;&#20307;&#20998;&#24067;&#24335;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#33021;&#22815;&#24110;&#21161;&#21463;&#25511;&#20195;&#29702;&#20570;&#20986;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25163;&#24314;&#27169;&#36890;&#36807;&#26500;&#24314;&#20854;&#20182;&#20195;&#29702;&#30340;&#27169;&#22411;&#65292;&#20351;&#21463;&#25511;&#20195;&#29702;&#30340;&#20915;&#31574;&#21463;&#30410;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#23545;&#25163;&#30340;&#35266;&#23519;&#21644;&#34892;&#20026;&#65292;&#20294;&#24403;&#23545;&#25163;&#30340;&#34892;&#20026;&#19981;&#21487;&#35266;&#23519;&#25110;&#38590;&#20197;&#33719;&#24471;&#26102;&#65292;&#36825;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#20998;&#24067;&#24335;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#32431;&#31929;&#30340;&#23616;&#37096;&#20449;&#24687;&#65288;&#21363;&#21463;&#25511;&#20195;&#29702;&#30340;&#35266;&#23519;&#12289;&#34892;&#20026;&#21644;&#22870;&#21169;&#65289;&#23454;&#29616;&#25512;&#27979;&#23545;&#25163;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#28436;&#21592;&#32500;&#25345;&#23545;&#23545;&#25163;&#30340;&#25512;&#27979;&#20449;&#24565;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25512;&#27979;&#23545;&#25163;&#27169;&#22411;&#65292;&#20197;&#20351;&#29992;&#23616;&#37096;&#35266;&#23519;&#26469;&#39044;&#27979;&#23545;&#25163;&#30340;&#21160;&#20316;&#65292;&#24182;&#30456;&#24212;&#22320;&#20570;&#20986;&#20915;&#31574;&#12290;&#27492;&#22806;&#65292;&#20998;&#24067;&#24335;&#35780;&#35770;&#23478;&#27169;&#22411;&#25919;&#31574;&#30340;&#22238;&#25253;&#20998;&#24067;&#12290;&#23427;&#21453;&#26144;&#20102;&#28436;&#21592;&#30340;&#36136;&#37327;&#65292;&#22240;&#27492;&#21487;&#20197;&#25351;&#23548;&#28436;&#21592;&#25152;&#20381;&#36182;&#30340;&#25512;&#27979;&#23545;&#25163;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.11940v2 Announce Type: replace  Abstract: Opponent modeling has benefited a controlled agent's decision-making by constructing models of other agents. Existing methods commonly assume access to opponents' observations and actions, which is infeasible when opponents' behaviors are unobservable or hard to obtain. We propose a novel multi-agent distributional actor-critic algorithm to achieve speculative opponent modeling with purely local information (i.e., the controlled agent's observations, actions, and rewards). Specifically, the actor maintains a speculated belief of the opponents, which we call the speculative opponent models, to predict opponent actions using local observations and makes decisions accordingly. Further, the distributional critic models the return distribution of the policy. It reflects the quality of the actor and thus can guide the training of the speculative opponent model that the actor relies on. Extensive experiments confirm that our method successf
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#29992;&#20110;&#21516;&#26102;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#21644;&#28145;&#24230;&#23436;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#35821;&#20041;&#20998;&#21106;&#21644;&#28145;&#24230;&#23436;&#25104;&#20219;&#21153;&#65292;&#22312;&#22810;&#20219;&#21153;&#32593;&#32476;&#20013;&#26377;&#25928;&#25552;&#39640;&#20102;&#27599;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2209.00381</link><description>&lt;p&gt;
SemSegDepth: &#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#21644;&#28145;&#24230;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
SemSegDepth: A Combined Model for Semantic Segmentation and Depth Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.00381
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#29992;&#20110;&#21516;&#26102;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#21644;&#28145;&#24230;&#23436;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#35821;&#20041;&#20998;&#21106;&#21644;&#28145;&#24230;&#23436;&#25104;&#20219;&#21153;&#65292;&#22312;&#22810;&#20219;&#21153;&#32593;&#32476;&#20013;&#26377;&#25928;&#25552;&#39640;&#20102;&#27599;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32508;&#21512;&#22330;&#26223;&#29702;&#35299;&#23545;&#20110;&#33258;&#20027;&#26426;&#22120;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#29992;&#20110;&#21516;&#26102;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#21644;&#28145;&#24230;&#23436;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;RGB&#21644;&#31232;&#30095;&#28145;&#24230;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#24182;&#20135;&#29983;&#23494;&#38598;&#28145;&#24230;&#22270;&#21644;&#30456;&#24212;&#30340;&#35821;&#20041;&#20998;&#21106;&#22270;&#20687;&#12290;&#23427;&#21253;&#25324;&#19968;&#20010;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#19968;&#20010;&#28145;&#24230;&#23436;&#25104;&#20998;&#25903;&#65292;&#19968;&#20010;&#35821;&#20041;&#20998;&#21106;&#20998;&#25903;&#20197;&#21450;&#19968;&#20010;&#32852;&#21512;&#20998;&#25903;&#65292;&#36827;&#19968;&#27493;&#21516;&#26102;&#22788;&#29702;&#35821;&#20041;&#21644;&#28145;&#24230;&#20449;&#24687;&#12290;&#22312;Virtual KITTI 2&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#24182;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30340;&#35777;&#25454;&#65292;&#21363;&#22312;&#22810;&#20219;&#21153;&#32593;&#32476;&#20013;&#32467;&#21512;&#35821;&#20041;&#20998;&#21106;&#21644;&#28145;&#24230;&#23436;&#25104;&#20219;&#21153;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#27599;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.00381v2 Announce Type: replace-cross  Abstract: Holistic scene understanding is pivotal for the performance of autonomous machines. In this paper we propose a new end-to-end model for performing semantic segmentation and depth completion jointly. The vast majority of recent approaches have developed semantic segmentation and depth completion as independent tasks. Our approach relies on RGB and sparse depth as inputs to our model and produces a dense depth map and the corresponding semantic segmentation image. It consists of a feature extractor, a depth completion branch, a semantic segmentation branch and a joint branch which further processes semantic and depth information altogether. The experiments done on Virtual KITTI 2 dataset, demonstrate and provide further evidence, that combining both tasks, semantic segmentation and depth completion, in a multi-task network can effectively improve the performance of each task. Code is available at https://github.com/juanb09111/sem
&lt;/p&gt;</description></item><item><title>STDEN&#26159;&#19968;&#31181;&#23558;&#20132;&#36890;&#27969;&#21160;&#21147;&#23398;&#30340;&#29289;&#29702;&#26426;&#21046;&#36716;&#21270;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#29289;&#29702;&#25351;&#23548;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#24357;&#21512;&#32431;&#25968;&#25454;&#39537;&#21160;&#21644;&#29289;&#29702;&#39537;&#21160;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2209.00225</link><description>&lt;p&gt;
STDEN:&#22522;&#20110;&#29289;&#29702;&#25351;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
STDEN: Towards Physics-Guided Neural Networks for Traffic Flow Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.00225
&lt;/p&gt;
&lt;p&gt;
STDEN&#26159;&#19968;&#31181;&#23558;&#20132;&#36890;&#27969;&#21160;&#21147;&#23398;&#30340;&#29289;&#29702;&#26426;&#21046;&#36716;&#21270;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#29289;&#29702;&#25351;&#23548;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#24357;&#21512;&#32431;&#25968;&#25454;&#39537;&#21160;&#21644;&#29289;&#29702;&#39537;&#21160;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#24615;&#33021;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#27169;&#22411;&#35774;&#35745;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#26680;&#24515;&#25216;&#26415;&#65292;&#26159;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#32570;&#20047;&#29289;&#29702;&#21407;&#29702;&#19982;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20043;&#38388;&#30340;&#25972;&#21512;&#26159;&#38480;&#21046;&#35813;&#39046;&#22495;&#21457;&#23637;&#30340;&#37325;&#35201;&#21407;&#22240;&#12290;&#20026;&#20102;&#24357;&#21512;&#32431;&#25968;&#25454;&#39537;&#21160;&#21644;&#29289;&#29702;&#39537;&#21160;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Spatio-Temporal Differential Equation Network (STDEN)&#30340;&#29289;&#29702;&#25351;&#23548;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#20132;&#36890;&#27969;&#21160;&#21147;&#23398;&#30340;&#29289;&#29702;&#26426;&#21046;&#36716;&#21270;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.00225v2 Announce Type: replace  Abstract: High-performance traffic flow prediction model designing, a core technology of Intelligent Transportation System, is a long-standing but still challenging task for industrial and academic communities. The lack of integration between physical principles and data-driven models is an important reason for limiting the development of this field. In the literature, physics-based methods can usually provide a clear interpretation of the dynamic process of traffic flow systems but are with limited accuracy, while data-driven methods, especially deep learning with black-box structures, can achieve improved performance but can not be fully trusted due to lack of a reasonable physical basis. To bridge the gap between purely data-driven and physics-driven approaches, we propose a physics-guided deep learning model named Spatio-Temporal Differential Equation Network (STDEN), which casts the physical mechanism of traffic flow dynamics into a deep 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20855;&#26377;&#20984;&#25439;&#22833;&#30340;&#32447;&#24615;&#39044;&#27979;&#22120;&#30340;$(\epsilon,\delta)$-&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#38382;&#39064;&#65292;&#20026;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#23376;&#31867;&#25552;&#20379;&#20102;&#32467;&#26524;&#65292;&#24182;&#22312;&#25152;&#26377;&#21442;&#25968;&#19978;&#23637;&#31034;&#20102;&#22522;&#26412;&#32039;&#23494;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;</title><link>https://arxiv.org/abs/2205.03014</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24046;&#20998;&#38544;&#31169;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Generalized Linear Models Revisited
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.03014
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20855;&#26377;&#20984;&#25439;&#22833;&#30340;&#32447;&#24615;&#39044;&#27979;&#22120;&#30340;$(\epsilon,\delta)$-&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#38382;&#39064;&#65292;&#20026;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#23376;&#31867;&#25552;&#20379;&#20102;&#32467;&#26524;&#65292;&#24182;&#22312;&#25152;&#26377;&#21442;&#25968;&#19978;&#23637;&#31034;&#20102;&#22522;&#26412;&#32039;&#23494;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#20984;&#25439;&#22833;&#30340;&#32447;&#24615;&#39044;&#27979;&#22120;&#30340;$(\epsilon,\delta)$-&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#38024;&#23545;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#23376;&#31867;&#25552;&#20379;&#20102;&#32467;&#26524;&#12290;&#31532;&#19968;&#31181;&#24773;&#20917;&#26159;&#24403;&#25439;&#22833;&#26159;&#20809;&#28369;&#19988;&#38750;&#36127;&#20294;&#19981;&#19968;&#23450;&#21033;&#26222;&#24076;&#20857;&#26102;&#65288;&#22914;&#24179;&#26041;&#25439;&#22833;&#65289;&#12290;&#23545;&#20110;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20851;&#20110;&#36807;&#37327;&#24635;&#20307;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#20026;$\tilde{O}\left(\frac{\Vert w^*\Vert}{\sqrt{n}} + \min\left\{\frac{\Vert w^* \Vert^2}{(n\epsilon)^{2/3}},\frac{\sqrt{d}\Vert w^*\Vert^2}{n\epsilon}\right\}\right)$&#65292;&#20854;&#20013;$n$&#26159;&#26679;&#26412;&#25968;&#65292;$d$&#26159;&#38382;&#39064;&#30340;&#32500;&#24230;&#65292;$w^*$&#26159;&#24635;&#20307;&#39118;&#38505;&#30340;&#26368;&#23567;&#21270;&#32773;&#12290;&#38500;&#20102;&#23545;$\Vert w^\ast\Vert$&#30340;&#20381;&#36182;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#22312;&#25152;&#26377;&#21442;&#25968;&#19978;&#22522;&#26412;&#19978;&#26159;&#32039;&#23494;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;$\tilde{\Omega}\left(\frac{1}{\sqrt{n}} + {\min\left\{\frac{\Vert w^*\Vert^{4/3}}{(n\epsilon)^{2/3}}, \frac{\sqrt{d}\Vert w^*\Vert}{n\epsilon}\right\}}$&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.03014v2 Announce Type: replace  Abstract: We study the problem of $(\epsilon,\delta)$-differentially private learning of linear predictors with convex losses. We provide results for two subclasses of loss functions. The first case is when the loss is smooth and non-negative but not necessarily Lipschitz (such as the squared loss). For this case, we establish an upper bound on the excess population risk of $\tilde{O}\left(\frac{\Vert w^*\Vert}{\sqrt{n}} + \min\left\{\frac{\Vert w^* \Vert^2}{(n\epsilon)^{2/3}},\frac{\sqrt{d}\Vert w^*\Vert^2}{n\epsilon}\right\}\right)$, where $n$ is the number of samples, $d$ is the dimension of the problem, and $w^*$ is the minimizer of the population risk. Apart from the dependence on $\Vert w^\ast\Vert$, our bound is essentially tight in all parameters. In particular, we show a lower bound of $\tilde{\Omega}\left(\frac{1}{\sqrt{n}} + {\min\left\{\frac{\Vert w^*\Vert^{4/3}}{(n\epsilon)^{2/3}}, \frac{\sqrt{d}\Vert w^*\Vert}{n\epsilon}\right\}}
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;CBS-GPT&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#32039;&#20945;&#20108;&#36827;&#21046;&#31995;&#32479;&#27874;&#24418;&#29983;&#25104;&#65292;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#26174;&#33879;&#30340;&#35299;&#37322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.20172</link><description>&lt;p&gt;
&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#29983;&#25104;&#32039;&#20945;&#20108;&#36827;&#21046;&#31995;&#32479;&#27874;&#24418;
&lt;/p&gt;
&lt;p&gt;
Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer. (arXiv:2310.20172v1 [gr-qc])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20172
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;CBS-GPT&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#32039;&#20945;&#20108;&#36827;&#21046;&#31995;&#32479;&#27874;&#24418;&#29983;&#25104;&#65292;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#26174;&#33879;&#30340;&#35299;&#37322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#24341;&#21147;&#27874;&#25506;&#27979;&#26159;&#26410;&#26469;&#21313;&#24180;&#26368;&#21463;&#26399;&#24453;&#30340;&#24341;&#21147;&#27874;&#25506;&#27979;&#39033;&#30446;&#20043;&#19968;&#65292;&#23558;&#25506;&#27979;&#21040;&#20016;&#23500;&#30340;&#32039;&#20945;&#20108;&#36827;&#21046;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#31354;&#38388;&#24341;&#21147;&#27874;&#27874;&#24418;&#30340;&#31934;&#30830;&#39044;&#27979;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#25506;&#27979;&#22120;&#21709;&#24212;&#21644;&#20108;&#20195;&#26102;&#24310;&#24178;&#28041;&#65288;TDI 2.0&#65289;&#24341;&#36215;&#30340;&#27874;&#24418;&#22797;&#26434;&#24615;&#22686;&#21152;&#32780;&#24102;&#26469;&#30340;&#25968;&#25454;&#22788;&#29702;&#22256;&#38590;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CBS-GPT&#65288;Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer&#65289;&#30340;&#21487;&#35299;&#37322;&#39044;&#35757;&#32451;&#22823;&#27169;&#22411;&#12290;&#23545;&#20110;&#32039;&#20945;&#20108;&#36827;&#21046;&#31995;&#32479;&#27874;&#24418;&#65292;&#35757;&#32451;&#20102;&#19977;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;&#36229;&#22823;&#36136;&#37327;&#40657;&#27934;&#20108;&#36827;&#21046;&#65288;MBHB&#65289;&#12289;&#26497;&#31471;&#36136;&#37327;&#27604;&#34701;&#21512;&#65288;EMRIs&#65289;&#21644;&#26143;&#31995;&#20108;&#36827;&#21046;&#65288;GB&#65289;&#30340;&#27874;&#24418;&#65292;&#20998;&#21035;&#23454;&#29616;&#20102;98%&#12289;91%&#21644;99%&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;CBS-GPT&#27169;&#22411;&#20855;&#26377;&#26174;&#33879;&#30340;&#35299;&#37322;&#24615;&#65292;&#20854;&#38544;&#34255;&#21442;&#25968;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#27874;&#24418;&#30340;&#22797;&#26434;&#20449;&#24687;&#65292;&#21363;&#20351;&#26159;&#22797;&#26434;&#30340;&#19981;&#36830;&#32493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Space-based gravitational wave detection is one of the most anticipated gravitational wave (GW) detection projects in the next decade, which will detect abundant compact binary systems. However, the precise prediction of space GW waveforms remains unexplored. To solve the data processing difficulty in the increasing waveform complexity caused by detectors' response and second-generation time-delay interferometry (TDI 2.0), an interpretable pre-trained large model named CBS-GPT (Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer) is proposed. For compact binary system waveforms, three models were trained to predict the waveforms of massive black hole binary (MBHB), extreme mass-ratio inspirals (EMRIs), and galactic binary (GB), achieving prediction accuracies of 98%, 91%, and 99%, respectively. The CBS-GPT model exhibits notable interpretability, with its hidden parameters effectively capturing the intricate information of waveforms, even with complex ins
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#38598;&#25104;&#22810;&#26679;&#24615;&#36827;&#34892;&#40065;&#26834;&#30340;&#33258;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#20449;&#24230;&#24230;&#37327;&#26041;&#27861;-$\mathcal{T}$-&#30456;&#20284;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#31181;&#19981;&#21516;&#20266;&#26631;&#31614;&#31574;&#30053;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.14814</link><description>&lt;p&gt;
&#22312;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#38598;&#25104;&#22810;&#26679;&#24615;&#36827;&#34892;&#40065;&#26834;&#30340;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias. (arXiv:2310.14814v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#38598;&#25104;&#22810;&#26679;&#24615;&#36827;&#34892;&#40065;&#26834;&#30340;&#33258;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#20449;&#24230;&#24230;&#37327;&#26041;&#27861;-$\mathcal{T}$-&#30456;&#20284;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#31181;&#19981;&#21516;&#20266;&#26631;&#31614;&#31574;&#30053;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#35757;&#32451;&#26159;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#23545;&#27169;&#22411;&#33258;&#20449;&#24230;&#39640;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#20266;&#26631;&#31614;&#20998;&#37197;&#65292;&#24182;&#23558;&#20854;&#35270;&#20026;&#26631;&#35760;&#26679;&#26412;&#36827;&#34892;&#22788;&#29702;&#12290;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#24120;&#20351;&#29992;softmax&#39044;&#27979;&#27010;&#29575;&#20316;&#20026;&#33258;&#20449;&#24230;&#24230;&#37327;&#65292;&#23613;&#31649;&#24050;&#30693;&#23427;&#20204;&#23545;&#38169;&#35823;&#39044;&#27979;&#20063;&#36807;&#20110;&#33258;&#20449;&#12290;&#24403;&#25968;&#25454;&#26631;&#27880;&#21463;&#21040;&#26576;&#31181;&#32422;&#26463;&#26102;&#65292;&#36825;&#31181;&#29616;&#35937;&#23588;&#20026;&#26126;&#26174;&#65292;&#21363;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#20449;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;$\mathcal{T}$-&#30456;&#20284;&#24230;&#65292;&#23427;&#22522;&#20110;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#39044;&#27979;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#31283;&#23450;&#28857;&#24182;&#25551;&#36848;&#21333;&#20010;&#25104;&#21592;&#30340;&#22810;&#26679;&#24615;&#19982;&#20854;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#25552;&#20379;&#25105;&#20204;&#26041;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#31181;&#19981;&#21516;&#20266;&#26631;&#31614;&#31574;&#30053;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#33258;&#20449;&#24230;&#24230;&#37327;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-training is a well-known approach for semi-supervised learning. It consists of iteratively assigning pseudo-labels to unlabeled data for which the model is confident and treating them as labeled examples. For neural networks, softmax prediction probabilities are often used as a confidence measure, despite the fact that they are known to be overconfident, even for wrong predictions. This phenomenon is particularly intensified in the presence of sample selection bias, i.e., when data labeling is subject to some constraint. To address this issue, we propose a novel confidence measure, called $\mathcal{T}$-similarity, built upon the prediction diversity of an ensemble of linear classifiers. We provide the theoretical analysis of our approach by studying stationary points and describing the relationship between the diversity of the individual members and their performance. We empirically demonstrate the benefit of our confidence measure for three different pseudo-labeling policies on c
&lt;/p&gt;</description></item><item><title>&#21487;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#20869;&#26680;&#65288;SNNKs&#65289;&#26159;&#19968;&#31181;&#26367;&#20195;&#24120;&#35268;&#21069;&#39304;&#23618;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36817;&#20284;&#23454;&#29616;&#24120;&#35268;&#21069;&#39304;&#23618;&#30340;&#21151;&#33021;&#65292;&#20294;&#20855;&#26377;&#26356;&#20248;&#30340;&#35745;&#31639;&#29305;&#24615;&#12290;&#36890;&#36807;&#23558;&#20869;&#26680;&#19982;&#21442;&#25968;-&#36755;&#20837;&#21521;&#37327;&#30340;&#28857;&#31215;&#32852;&#31995;&#36215;&#26469;&#65292;SNNKs&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#24320;&#21442;&#25968;&#19982;&#36755;&#20837;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20174;&#32780;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31070;&#32463;&#32593;&#32476;&#25414;&#32465;&#36807;&#31243;&#65292;&#23558;SNNKs&#24212;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#21387;&#32553;&#25928;&#26524;&#12290;&#26368;&#32456;&#25414;&#32465;&#32593;&#32476;&#29978;&#33267;&#21487;&#20197;&#32469;&#36807;&#21453;&#21521;&#20256;&#25773;&#65292;&#36890;&#36807;&#26174;&#24335;&#20844;&#24335;&#27714;&#35299;&#26368;&#20248;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.13225</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#20869;&#26680;
&lt;/p&gt;
&lt;p&gt;
Scalable Neural Network Kernels. (arXiv:2310.13225v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13225
&lt;/p&gt;
&lt;p&gt;
&#21487;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#20869;&#26680;&#65288;SNNKs&#65289;&#26159;&#19968;&#31181;&#26367;&#20195;&#24120;&#35268;&#21069;&#39304;&#23618;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36817;&#20284;&#23454;&#29616;&#24120;&#35268;&#21069;&#39304;&#23618;&#30340;&#21151;&#33021;&#65292;&#20294;&#20855;&#26377;&#26356;&#20248;&#30340;&#35745;&#31639;&#29305;&#24615;&#12290;&#36890;&#36807;&#23558;&#20869;&#26680;&#19982;&#21442;&#25968;-&#36755;&#20837;&#21521;&#37327;&#30340;&#28857;&#31215;&#32852;&#31995;&#36215;&#26469;&#65292;SNNKs&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#24320;&#21442;&#25968;&#19982;&#36755;&#20837;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20174;&#32780;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31070;&#32463;&#32593;&#32476;&#25414;&#32465;&#36807;&#31243;&#65292;&#23558;SNNKs&#24212;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#21387;&#32553;&#25928;&#26524;&#12290;&#26368;&#32456;&#25414;&#32465;&#32593;&#32476;&#29978;&#33267;&#21487;&#20197;&#32469;&#36807;&#21453;&#21521;&#20256;&#25773;&#65292;&#36890;&#36807;&#26174;&#24335;&#20844;&#24335;&#27714;&#35299;&#26368;&#20248;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#20869;&#26680;&#65288;SNNKs&#65289;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#24120;&#35268;&#21069;&#39304;&#23618;&#65288;FFLs&#65289;&#30340;&#26367;&#20195;&#21697;&#65292;&#33021;&#22815;&#36817;&#20284;&#23454;&#29616;&#21518;&#32773;&#65292;&#20294;&#20855;&#26377;&#26377;&#21033;&#30340;&#35745;&#31639;&#23646;&#24615;&#12290;SNNKs&#26377;&#25928;&#22320;&#35299;&#24320;&#20102;FFL&#20013;&#21442;&#25968;&#19982;&#36755;&#20837;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#36890;&#36807;&#28857;&#31215;&#20869;&#26680;&#22312;&#26368;&#32456;&#35745;&#31639;&#20013;&#36830;&#25509;&#23427;&#20204;&#12290;&#23427;&#20204;&#20063;&#26356;&#21152;&#34920;&#36798;&#21147;&#24378;&#65292;&#33021;&#22815;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#65292;&#36229;&#20986;&#21442;&#25968;-&#36755;&#20837;&#21521;&#37327;&#30340;&#20989;&#25968;&#33539;&#22260;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31070;&#32463;&#32593;&#32476;&#25414;&#32465;&#36807;&#31243;&#65292;&#23558;SNNKs&#24212;&#29992;&#20110;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#33719;&#24471;&#39069;&#22806;&#30340;&#21387;&#32553;&#25928;&#30410;&#12290;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#65292;&#23427;&#23548;&#33268;&#23436;&#20840;&#25414;&#32465;&#32593;&#32476;&#65292;&#20854;&#26368;&#20248;&#21442;&#25968;&#21487;&#20197;&#36890;&#36807;&#22810;&#20010;&#25439;&#22833;&#20989;&#25968;&#65288;&#20363;&#22914;&#22343;&#26041;&#35823;&#24046;&#65289;&#30340;&#26174;&#24335;&#20844;&#24335;&#26469;&#34920;&#31034;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#32469;&#36807;&#21453;&#21521;&#20256;&#25773;&#12290;&#20316;&#20026;&#25105;&#20204;&#20998;&#26512;&#30340;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26222;&#36941;&#24615;&#26426;&#21046;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the concept of scalable neural network kernels (SNNKs), the replacements of regular feedforward layers (FFLs), capable of approximating the latter, but with favorable computational properties. SNNKs effectively disentangle the inputs from the parameters of the neural network in the FFL, only to connect them in the final computation via the dot-product kernel. They are also strictly more expressive, as allowing to model complicated relationships beyond the functions of the dot-products of parameter-input vectors. We also introduce the neural network bundling process that applies SNNKs to compactify deep neural network architectures, resulting in additional compression gains. In its extreme version, it leads to the fully bundled network whose optimal parameters can be expressed via explicit formulae for several loss functions (e.g. mean squared error), opening a possibility to bypass backpropagation. As a by-product of our analysis, we introduce the mechanism of the universa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27531;&#24046;&#22810;&#20445;&#30495;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20445;&#30495;&#20449;&#24687;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20302;&#20445;&#30495;&#21644;&#39640;&#20445;&#30495;&#35745;&#31639;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24314;&#27169;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#35757;&#32451;&#20102;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#27531;&#24046;&#20989;&#25968;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#26368;&#32456;&#24471;&#21040;&#20102;&#39640;&#20445;&#30495;&#26367;&#20195;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.03572</link><description>&lt;p&gt;
&#22810;&#20445;&#30495;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#30340;&#27531;&#24046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Residual Multi-Fidelity Neural Network Computing. (arXiv:2310.03572v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27531;&#24046;&#22810;&#20445;&#30495;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20445;&#30495;&#20449;&#24687;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20302;&#20445;&#30495;&#21644;&#39640;&#20445;&#30495;&#35745;&#31639;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24314;&#27169;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#35757;&#32451;&#20102;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#27531;&#24046;&#20989;&#25968;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#26368;&#32456;&#24471;&#21040;&#20102;&#39640;&#20445;&#30495;&#26367;&#20195;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#22810;&#20445;&#30495;&#20449;&#24687;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#30340;&#19968;&#33324;&#38382;&#39064;&#12290;&#32473;&#23450;&#19968;&#20010;&#24265;&#20215;&#30340;&#20302;&#20445;&#30495;&#21644;&#19968;&#20010;&#26114;&#36149;&#30340;&#39640;&#20445;&#30495;&#35745;&#31639;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27531;&#24046;&#22810;&#20445;&#30495;&#35745;&#31639;&#26694;&#26550;&#65292;&#23558;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24314;&#27169;&#20026;&#19968;&#20010;&#27531;&#24046;&#20989;&#25968;&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#33021;&#38750;&#32447;&#24615;&#30340;1&#65289;&#27169;&#22411;&#20849;&#20139;&#30340;&#36755;&#20837;&#31354;&#38388;&#21644;&#20302;&#20445;&#30495;&#27169;&#22411;&#36755;&#20986;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#20197;&#21450;2&#65289;&#20004;&#20010;&#27169;&#22411;&#36755;&#20986;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#21327;&#21516;&#24037;&#20316;&#12290;&#31532;&#19968;&#20010;&#32593;&#32476;&#22312;&#23569;&#37327;&#30340;&#39640;&#20445;&#30495;&#21644;&#20302;&#20445;&#30495;&#25968;&#25454;&#19978;&#23398;&#20064;&#27531;&#24046;&#20989;&#25968;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#36825;&#20010;&#32593;&#32476;&#34987;&#29992;&#26469;&#29983;&#25104;&#39069;&#22806;&#30340;&#21512;&#25104;&#39640;&#20445;&#30495;&#25968;&#25454;&#65292;&#29992;&#20110;&#35757;&#32451;&#31532;&#20108;&#20010;&#32593;&#32476;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#31532;&#20108;&#20010;&#32593;&#32476;&#20316;&#20026;&#25105;&#20204;&#23545;&#39640;&#20445;&#30495;&#24863;&#20852;&#36259;&#30340;&#37327;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19977;&#20010;&#25968;&#20540;&#20363;&#23376;&#26469;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we consider the general problem of constructing a neural network surrogate model using multi-fidelity information. Given an inexpensive low-fidelity and an expensive high-fidelity computational model, we present a residual multi-fidelity computational framework that formulates the correlation between models as a residual function, a possibly non-linear mapping between 1) the shared input space of the models together with the low-fidelity model output and 2) the discrepancy between the two model outputs. To accomplish this, we train two neural networks to work in concert. The first network learns the residual function on a small set of high-fidelity and low-fidelity data. Once trained, this network is used to generate additional synthetic high-fidelity data, which is used in the training of a second network. This second network, once trained, acts as our surrogate for the high-fidelity quantity of interest. We present three numerical examples to demonstrate the power of th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#31038;&#20250;&#24847;&#20041;&#30340;&#26080;&#20998;&#24067;&#32479;&#35745;&#31163;&#25955;&#24230;&#25511;&#21046;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#39640;&#39118;&#38505;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.13786</link><description>&lt;p&gt;
&#31038;&#20250;&#24212;&#29992;&#30340;&#26080;&#20998;&#24067;&#32479;&#35745;&#31163;&#25955;&#24230;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Distribution-Free Statistical Dispersion Control for Societal Applications. (arXiv:2309.13786v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13786
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#31038;&#20250;&#24847;&#20041;&#30340;&#26080;&#20998;&#24067;&#32479;&#35745;&#31163;&#25955;&#24230;&#25511;&#21046;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#39640;&#39118;&#38505;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36127;&#36131;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#26174;&#24335;&#26377;&#38480;&#26679;&#26412;&#32479;&#35745;&#20445;&#35777;&#26159;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#30028;&#23450;&#39044;&#27979;&#22120;&#30340;&#26399;&#26395;&#25439;&#22833;&#25110;&#32773;&#20010;&#20307;&#39044;&#27979;&#23558;&#25215;&#21463;&#30340;&#25439;&#22833;&#20540;&#22312;&#19968;&#20010;&#25351;&#23450;&#33539;&#22260;&#20869;&#30340;&#27010;&#29575;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#39640;&#39118;&#38505;&#24212;&#29992;&#32780;&#35328;&#65292;&#29702;&#35299;&#21644;&#25511;&#21046;&#25439;&#22833;&#20998;&#24067;&#30340;&#31163;&#25955;&#24230;&#65292;&#25110;&#32773;&#35828;&#20154;&#32676;&#20013;&#19981;&#21516;&#20010;&#20307;&#23545;&#31639;&#27861;&#20915;&#31574;&#30340;&#24433;&#21709;&#31243;&#24230;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#20855;&#26377;&#31038;&#20250;&#24847;&#20041;&#30340;&#26080;&#20998;&#24067;&#32479;&#35745;&#31163;&#25955;&#24230;&#25511;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#26356;&#20016;&#23500;&#30340;&#32479;&#35745;&#21151;&#33021;&#31867;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#26377;&#27602;&#35780;&#35770;&#26816;&#27979;&#12289;&#21307;&#23398;&#24433;&#20687;&#21644;&#30005;&#24433;&#25512;&#33616;&#31561;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explicit finite-sample statistical guarantees on model performance are an important ingredient in responsible machine learning. Previous work has focused mainly on bounding either the expected loss of a predictor or the probability that an individual prediction will incur a loss value in a specified range. However, for many high-stakes applications, it is crucial to understand and control the dispersion of a loss distribution, or the extent to which different members of a population experience unequal effects of algorithmic decisions. We initiate the study of distribution-free control of statistical dispersion measures with societal implications and propose a simple yet flexible framework that allows us to handle a much richer class of statistical functionals beyond previous work. Our methods are verified through experiments in toxic comment detection, medical imaging, and film recommendation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#29983;&#25104;&#21644;&#35780;&#20272;&#21512;&#25104;&#32437;&#21521;&#24739;&#32773;&#25968;&#25454;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#20197;&#35299;&#20915;&#21307;&#23398;&#39046;&#22495;&#20013;&#25968;&#25454;&#20351;&#29992;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12380</link><description>&lt;p&gt;
&#29983;&#25104;&#21644;&#35780;&#20272;&#21512;&#25104;&#32437;&#21521;&#24739;&#32773;&#25968;&#25454;&#30340;&#26041;&#27861;&#65306;&#19968;&#39033;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Methods for generating and evaluating synthetic longitudinal patient data: a systematic review. (arXiv:2309.12380v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#29983;&#25104;&#21644;&#35780;&#20272;&#21512;&#25104;&#32437;&#21521;&#24739;&#32773;&#25968;&#25454;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#20197;&#35299;&#20915;&#21307;&#23398;&#39046;&#22495;&#20013;&#25968;&#25454;&#20351;&#29992;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#25968;&#25454;&#30340;&#36805;&#29467;&#22686;&#38271;&#20419;&#36827;&#20102;&#21508;&#31181;&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#65292;&#21152;&#24555;&#20102;&#30740;&#31350;&#21644;&#24320;&#21457;&#27963;&#21160;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#34892;&#19994;&#37117;&#33021;&#20174;&#25968;&#25454;&#30340;&#22686;&#21152;&#20013;&#21516;&#31561;&#21463;&#30410;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30001;&#20110;&#25968;&#25454;&#20351;&#29992;&#21644;&#38544;&#31169;&#35268;&#23450;&#30340;&#27861;&#24459;&#38480;&#21046;&#65292;&#20363;&#22914;&#21307;&#23398;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#32479;&#35745;&#25259;&#38706;&#21644;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#21512;&#25104;&#25968;&#25454;&#26159;&#22522;&#20110;&#19968;&#20123;&#29616;&#26377;&#25968;&#25454;&#29983;&#25104;&#30340;&#65292;&#30446;&#30340;&#26159;&#23613;&#21487;&#33021;&#22320;&#22797;&#21046;&#23427;&#20204;&#65292;&#24182;&#20805;&#24403;&#30495;&#23454;&#25935;&#24863;&#25968;&#25454;&#30340;&#20195;&#29702;&#12290;&#26412;&#25991;&#23545;&#29983;&#25104;&#21644;&#35780;&#20272;&#21512;&#25104;&#32437;&#21521;&#24739;&#32773;&#25968;&#25454;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#20013;&#19968;&#31181;&#24120;&#35265;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#35813;&#32508;&#36848;&#36981;&#24490;PRISMA&#25351;&#21335;&#65292;&#24182;&#28085;&#30422;&#20102;&#33258;2022&#24180;&#24213;&#20197;&#26469;&#30340;&#20116;&#20010;&#25968;&#25454;&#24211;&#30340;&#25991;&#29486;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;17&#31181;&#26041;&#27861;&#65292;&#20174;&#20256;&#32479;&#26041;&#27861;&#21040;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of data in recent years has led to the advancement and utilization of various statistical and deep learning techniques, thus expediting research and development activities. However, not all industries have benefited equally from the surge in data availability, partly due to legal restrictions on data usage and privacy regulations, such as in medicine. To address this issue, various statistical disclosure and privacy-preserving methods have been proposed, including the use of synthetic data generation. Synthetic data are generated based on some existing data, with the aim of replicating them as closely as possible and acting as a proxy for real sensitive data. This paper presents a systematic review of methods for generating and evaluating synthetic longitudinal patient data, a prevalent data type in medicine. The review adheres to the PRISMA guidelines and covers literature from five databases until the end of 2022. The paper describes 17 methods, ranging from traditi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;Projected Task-Specific Layers (PTSL)&#65292;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#23618;&#26469;&#34920;&#36798;&#20849;&#20139;&#21644;&#21487;&#21464;&#30340;&#20219;&#21153;&#20449;&#24687;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25512;&#24191;&#21644;&#24178;&#25200;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08776</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;Projected Task-Specific Layers
&lt;/p&gt;
&lt;p&gt;
Projected Task-Specific Layers for Multi-Task Reinforcement Learning. (arXiv:2309.08776v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;Projected Task-Specific Layers (PTSL)&#65292;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#23618;&#26469;&#34920;&#36798;&#20849;&#20139;&#21644;&#21487;&#21464;&#30340;&#20219;&#21153;&#20449;&#24687;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25512;&#24191;&#21644;&#24178;&#25200;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#22312;&#23478;&#24237;&#21644;&#24037;&#20316;&#22330;&#25152;&#30340;&#21508;&#31181;&#25805;&#20316;&#20219;&#21153;&#20013;&#23454;&#29616;&#35268;&#27169;&#21270;&#12290;&#28982;&#32780;&#65292;&#20174;&#19968;&#20010;&#20219;&#21153;&#25512;&#24191;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#24182;&#20943;&#36731;&#36127;&#38754;&#20219;&#21153;&#24178;&#25200;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25104;&#21151;&#22320;&#22312;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#24182;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#23558;&#21462;&#20915;&#20110;&#23545;&#20219;&#21153;&#24213;&#23618;&#32467;&#26500;&#30340;&#26377;&#25928;&#25429;&#25417;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#21363;Projected Task-Specific Layers&#65288;PTSL&#65289;&#65292;&#23427;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#23618;&#65292;&#36890;&#36807;&#31264;&#23494;&#30340;&#20219;&#21153;&#29305;&#23450;&#30340;&#20462;&#27491;&#26469;&#26356;&#22909;&#22320;&#34920;&#36798;&#20849;&#20139;&#21644;&#21487;&#21464;&#30340;&#20219;&#21153;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;Meta-World&#30340;MT10&#21644;MT50&#22522;&#20934;&#20013;&#65288;&#21253;&#25324;Sawyer&#26426;&#22120;&#20154;&#33218;&#19978;&#30340;10&#20010;&#21644;50&#20010;&#30446;&#26631;&#26465;&#20214;&#20219;&#21153;&#65289;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task reinforcement learning could enable robots to scale across a wide variety of manipulation tasks in homes and workplaces. However, generalizing from one task to another and mitigating negative task interference still remains a challenge. Addressing this challenge by successfully sharing information across tasks will depend on how well the structure underlying the tasks is captured. In this work, we introduce our new architecture, Projected Task-Specific Layers (PTSL), that leverages a common policy with dense task-specific corrections through task-specific layers to better express shared and variable task information. We then show that our model outperforms the state of the art on the MT10 and MT50 benchmarks of Meta-World consisting of 10 and 50 goal-conditioned tasks for a Sawyer arm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20445;&#25345;&#24207;&#21015;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#40654;&#26364;&#20960;&#20309;&#29305;&#24615;&#30340;&#32467;&#26500;&#20445;&#25345;&#21464;&#21387;&#22120;&#26426;&#21046;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#65292;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#30340;&#38454;&#27573;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07579</link><description>&lt;p&gt;
&#20445;&#25345;&#32467;&#26500;&#30340;&#21464;&#21387;&#22120;&#29992;&#20110;&#24207;&#21015;&#30340;SPD&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Structure-Preserving Transformers for Sequences of SPD Matrices. (arXiv:2309.07579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20445;&#25345;&#24207;&#21015;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#40654;&#26364;&#20960;&#20309;&#29305;&#24615;&#30340;&#32467;&#26500;&#20445;&#25345;&#21464;&#21387;&#22120;&#26426;&#21046;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#65292;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#30340;&#38454;&#27573;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#25968;&#25454;&#31867;&#22411;&#30340;&#20998;&#26512;&#65292;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#31561;&#65292;&#21253;&#25324;&#38750;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36825;&#26679;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#20998;&#31867;&#24207;&#21015;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#65292;&#24182;&#22312;&#25972;&#20010;&#20998;&#26512;&#36807;&#31243;&#20013;&#20445;&#25345;&#23427;&#20204;&#30340;&#40654;&#26364;&#20960;&#20309;&#29305;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26469;&#33258;&#26631;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#33041;&#30005;&#22270;&#21327;&#26041;&#24046;&#30697;&#38453;&#24207;&#21015;&#30340;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#65292;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#30340;&#38454;&#27573;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Transformer-based auto-attention mechanisms have been successfully applied to the analysis of a variety of context-reliant data types, from texts to images and beyond, including data from non-Euclidean geometries. In this paper, we present such a mechanism, designed to classify sequences of Symmetric Positive Definite matrices while preserving their Riemannian geometry throughout the analysis. We apply our method to automatic sleep staging on timeseries of EEG-derived covariance matrices from a standard dataset, obtaining high levels of stage-wise performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#20840;&#29699;&#23567;&#20892;&#25143;&#31995;&#32479;&#20013;&#24120;&#35265;&#30340;&#25910;&#33719;&#22534;&#26469;&#26144;&#23556;&#20892;&#30000;&#30340;&#23384;&#22312;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;HarvestNet&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#19987;&#23478;&#30693;&#35782;&#21644;&#21355;&#26143;&#22270;&#20687;&#25910;&#38598;&#32780;&#26469;&#65292;&#21487;&#29992;&#20110;&#22312;&#22467;&#22622;&#20420;&#27604;&#20122;&#30340;&#29305;&#23450;&#22320;&#21306;&#36827;&#34892;&#20892;&#30000;&#26144;&#23556;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20316;&#32773;&#30340;&#26368;&#20339;&#27169;&#22411;&#22312;&#25163;&#21160;&#26631;&#35760;&#25968;&#25454;&#19978;&#20855;&#26377;&#32422;80%&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12061</link><description>&lt;p&gt;
HarvestNet&#65306;&#21033;&#29992;&#25910;&#33719;&#22534;&#21644;&#36965;&#24863;&#25216;&#26415;&#26816;&#27979;&#23567;&#20892;&#25143;&#20892;&#19994;&#27963;&#21160;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing. (arXiv:2308.12061v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12061
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#20840;&#29699;&#23567;&#20892;&#25143;&#31995;&#32479;&#20013;&#24120;&#35265;&#30340;&#25910;&#33719;&#22534;&#26469;&#26144;&#23556;&#20892;&#30000;&#30340;&#23384;&#22312;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;HarvestNet&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#19987;&#23478;&#30693;&#35782;&#21644;&#21355;&#26143;&#22270;&#20687;&#25910;&#38598;&#32780;&#26469;&#65292;&#21487;&#29992;&#20110;&#22312;&#22467;&#22622;&#20420;&#27604;&#20122;&#30340;&#29305;&#23450;&#22320;&#21306;&#36827;&#34892;&#20892;&#30000;&#26144;&#23556;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20316;&#32773;&#30340;&#26368;&#20339;&#27169;&#22411;&#22312;&#25163;&#21160;&#26631;&#35760;&#25968;&#25454;&#19978;&#20855;&#26377;&#32422;80%&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#22411;&#20892;&#22330;&#22312;&#21457;&#23637;&#20013;&#22269;&#23478;&#30340;&#29983;&#20135;&#22303;&#22320;&#20013;&#25152;&#21344;&#27604;&#20363;&#24456;&#22823;&#12290;&#22312;&#25746;&#21704;&#25289;&#20197;&#21335;&#38750;&#27954;&#31561;&#22320;&#21306;&#65292;80%&#30340;&#20892;&#22330;&#37117;&#24456;&#23567;&#65288;&#38754;&#31215;&#23567;&#20110;2&#20844;&#39031;&#65289;&#65292;&#26144;&#23556;&#23567;&#20892;&#25143;&#30340;&#20892;&#30000;&#26159;&#36861;&#36394;&#20316;&#29289;&#29983;&#20135;&#21147;&#31561;&#21487;&#25345;&#32493;&#21457;&#23637;&#25514;&#26045;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23567;&#20892;&#22330;&#30340;&#22806;&#35266;&#22810;&#26679;&#19988;&#24494;&#22937;&#65292;&#20256;&#32479;&#30340;&#20892;&#30000;&#26144;&#23556;&#26041;&#27861;&#30340;&#25928;&#26524;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#26816;&#27979;&#36941;&#24067;&#20840;&#29699;&#35768;&#22810;&#23567;&#20892;&#25143;&#31995;&#32479;&#30340;&#25910;&#33719;&#22534;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;HarvestNet&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;&#22467;&#22622;&#20420;&#27604;&#20122;&#30340;&#25552;&#26684;&#38647;&#21644;&#38463;&#22982;&#21704;&#25289;&#22320;&#21306;&#30340;2020-2023&#24180;&#38388;&#26144;&#23556;&#20892;&#30000;&#30340;&#23384;&#22312;&#65292;&#35813;&#25968;&#25454;&#38598;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#21644;&#21355;&#26143;&#22270;&#20687;&#25910;&#38598;&#65292;&#20849;&#26377;7k&#20010;&#25163;&#21160;&#26631;&#35760;&#30340;&#22270;&#20687;&#21644;2k&#20010;&#22320;&#38754;&#25910;&#38598;&#26631;&#31614;&#12290;&#25105;&#20204;&#36824;&#38024;&#23545;&#29616;&#26377;&#25216;&#26415;&#22312;&#36965;&#24863;&#39046;&#22495;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#22312;&#25163;&#21160;&#26631;&#35760;&#25968;&#25454;&#19978;&#20855;&#26377;&#32422;80%&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Small farms contribute to a large share of the productive land in developing countries. In regions such as sub-Saharan Africa, where 80% of farms are small (under 2 ha in size), the task of mapping smallholder cropland is an important part of tracking sustainability measures such as crop productivity. However, the visually diverse and nuanced appearance of small farms has limited the effectiveness of traditional approaches to cropland mapping. Here we introduce a new approach based on the detection of harvest piles characteristic of many smallholder systems throughout the world. We present HarvestNet, a dataset for mapping the presence of farms in the Ethiopian regions of Tigray and Amhara during 2020-2023, collected using expert knowledge and satellite images, totaling 7k hand-labeled images and 2k ground collected labels. We also benchmark a set of baselines including SOTA models in remote sensing with our best models having around 80% classification performance on hand labelled data
&lt;/p&gt;</description></item><item><title>&#22312;&#31070;&#32463;&#21147;&#22330;&#27169;&#22411;&#20013;&#65292;&#24120;&#29992;&#30340;MD17&#25968;&#25454;&#38598;&#23545;&#20110;&#34920;&#31034;&#32463;&#21382;&#21270;&#23398;&#21453;&#24212;&#30340;&#31995;&#32479;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;xxMD&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#37319;&#26679;&#33258;&#25193;&#23637;&#28608;&#21457;&#24577;&#20998;&#23376;&#21160;&#21147;&#23398;&#65292;&#21253;&#21547;&#20102;&#33021;&#37327;&#21644;&#21147;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.11155</link><description>&lt;p&gt;
&#36890;&#36807;&#36229;&#20986;&#24179;&#34913;&#29366;&#24577;&#30340;&#25193;&#23637;&#21160;&#21147;&#23398;&#24615;&#33021;&#35780;&#20272;&#31070;&#32463;&#21147;&#22330;
&lt;/p&gt;
&lt;p&gt;
xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium. (arXiv:2308.11155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11155
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#21147;&#22330;&#27169;&#22411;&#20013;&#65292;&#24120;&#29992;&#30340;MD17&#25968;&#25454;&#38598;&#23545;&#20110;&#34920;&#31034;&#32463;&#21382;&#21270;&#23398;&#21453;&#24212;&#30340;&#31995;&#32479;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;xxMD&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#37319;&#26679;&#33258;&#25193;&#23637;&#28608;&#21457;&#24577;&#20998;&#23376;&#21160;&#21147;&#23398;&#65292;&#21253;&#21547;&#20102;&#33021;&#37327;&#21644;&#21147;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#21147;&#22330;&#24050;&#25104;&#20026;&#35745;&#31639;&#21270;&#23398;&#20013;&#30340;&#37325;&#35201;&#27169;&#22411;&#65292;&#21462;&#20195;&#20102;&#20174;&#22836;&#31639;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#20013;&#30340;&#37327;&#23376;&#21270;&#23398;&#35745;&#31639;&#12290;&#30446;&#21069;&#23545;&#31070;&#32463;&#21147;&#22330;&#30340;&#20027;&#35201;&#35780;&#20272;&#22522;&#20934;&#26159;MD17&#25968;&#25454;&#38598;&#21450;&#20854;&#21518;&#32493;&#25193;&#23637;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#20027;&#35201;&#21253;&#21547;&#26469;&#33258;&#22522;&#24577;&#21183;&#33021;&#38754;&#24179;&#34913;&#21306;&#22495;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#37319;&#26679;&#33258;&#30452;&#25509;&#32477;&#28909;&#21160;&#21147;&#23398;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#21270;&#23398;&#21453;&#24212;&#28041;&#21450;&#21040;&#36739;&#22823;&#30340;&#20998;&#23376;&#21464;&#24418;&#65292;&#29305;&#21035;&#26159;&#38190;&#26029;&#35010;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MD17&#25968;&#25454;&#38598;&#20013;&#20869;&#22352;&#26631;&#21644;&#33021;&#37327;&#30340;&#32422;&#26463;&#20998;&#24067;&#65292;&#20984;&#26174;&#20102;&#20854;&#22312;&#34920;&#31034;&#32463;&#21382;&#21270;&#23398;&#21453;&#24212;&#30340;&#31995;&#32479;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#37319;&#26679;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;xxMD&#65288;&#25193;&#23637;&#28608;&#21457;&#24577;&#20998;&#23376;&#21160;&#21147;&#23398;&#65289;&#25968;&#25454;&#38598;&#65292;&#20174;&#38750;&#32477;&#28909;&#21160;&#21147;&#23398;&#20013;&#27966;&#29983;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#20174;&#22810;&#21442;&#32771;&#27874;&#20989;&#25968;&#29702;&#35770;&#21644;&#23494;&#24230;&#27867;&#20989;&#20013;&#30830;&#23450;&#30340;&#33021;&#37327;&#21644;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural force fields (NFFs) have gained prominence in computational chemistry as surrogate models, superseding quantum-chemistry calculations in ab initio molecular dynamics. The prevalent benchmark for NFFs has been the MD17 dataset and its subsequent extension. These datasets predominantly comprise geometries from the equilibrium region of the ground electronic state potential energy surface, sampling from direct adiabatic dynamics. However, many chemical reactions entail significant molecular deformations, notably bond breaking. We demonstrate the constrained distribution of internal coordinates and energies in the MD17 datasets, underscoring their inadequacy for representing systems undergoing chemical reactions. Addressing this sampling limitation, we introduce the xxMD (Extended Excited-state Molecular Dynamics) dataset, derived from non-adiabatic dynamics. This dataset encompasses energies and forces ascertained from both multireference wave function theory and density functional
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#32452;&#21512;&#30340;&#31354;&#38388;&#20851;&#31995;&#22522;&#30784;&#65292;&#24182;&#37319;&#29992;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#35780;&#20272;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09778</link><description>&lt;p&gt;
&#36861;&#27714;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22522;&#20110;&#23454;&#38469;&#30340;&#35270;&#35273;&#31354;&#38388;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models. (arXiv:2308.09778v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#32452;&#21512;&#30340;&#31354;&#38388;&#20851;&#31995;&#22522;&#30784;&#65292;&#24182;&#37319;&#29992;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#35780;&#20272;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#36827;&#23637;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#21508;&#31181;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#35745;&#25968;&#12289;&#25351;&#28041;&#34920;&#36798;&#21644;&#19968;&#33324;&#30340;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#65289;&#19978;&#30340;&#34920;&#29616;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#29702;&#35299;&#31354;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#65292;&#20154;&#20204;&#23581;&#35797;&#20351;&#29992;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#65288;Liu, Emerson, and Collier 2022) &#25110;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#26469;&#22788;&#29702;&#27492;&#38382;&#39064;&#65292;&#20294;&#37117;&#34920;&#29616;&#20986;&#24615;&#33021;&#19981;&#20339;&#24182;&#19988;&#19982;&#20154;&#31867;&#24615;&#33021;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#32452;&#21512;&#30340;&#31354;&#38388;&#20851;&#31995;&#22522;&#30784;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#26469;&#23545;&#31354;&#38388;&#20174;&#21477;&#36827;&#34892;&#25490;&#21517;&#24182;&#35780;&#20272;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#32467;&#21512;&#21644;&#22320;&#38754;&#21270;&#29289;&#20307;&#23545;&#24212;&#30340;&#21517;&#35789;&#30701;&#35821;&#21644;&#23427;&#20204;&#30340;&#20301;&#32622;&#30340;&#35777;&#25454;&#26469;&#35745;&#31639;&#31354;&#38388;&#20174;&#21477;&#30340;&#26368;&#32456;&#25490;&#21517;&#12290;&#25105;&#20204;&#22312;&#20195;&#34920;&#24615;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advances in large scale vision-and-language models (VLMs) it is of interest to assess their performance on various visual reasoning tasks such as counting, referring expressions and general visual question answering. The focus of this work is to study the ability of these models to understanding spatial relations. Previously, this has been tackled using image-text matching (Liu, Emerson, and Collier 2022) or visual question answering task, both showing poor performance and a large gap compared to human performance. To better understand the gap, we present fine-grained compositional grounding of spatial relationships and propose a bottom up approach for ranking spatial clauses and evaluating the performance of spatial relationship reasoning task. We propose to combine the evidence from grounding noun phrases corresponding to objects and their locations to compute the final rank of the spatial clause. We demonstrate the approach on representative vision-language models (Tan and 
&lt;/p&gt;</description></item><item><title>AbDiffuser&#26159;&#19968;&#20010;&#29289;&#29702;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#29983;&#25104;&#25239;&#20307;&#30340;&#19977;&#32500;&#32467;&#26500;&#21644;&#24207;&#21015;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#32422;&#26463;&#25913;&#21892;&#34507;&#30333;&#36136;&#25193;&#25955;&#65292;&#22788;&#29702;&#24207;&#21015;&#38271;&#24230;&#21464;&#21270;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#19982;&#21442;&#32771;&#38598;&#21512;&#30340;&#24207;&#21015;&#21644;&#32467;&#26500;&#29305;&#24615;&#23494;&#20999;&#21305;&#37197;&#30340;&#25239;&#20307;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AbDiffuser&#33021;&#22815;&#29983;&#25104;&#39640;&#27700;&#24179;&#34920;&#36798;&#30340;&#25239;&#20307;&#65292;&#20854;&#20013;57.1%&#30340;&#35774;&#35745;&#36873;&#25321;&#26159;&#32039;&#23494;&#32467;&#21512;&#21058;&#12290;</title><link>http://arxiv.org/abs/2308.05027</link><description>&lt;p&gt;
AbDiffuser&#65306;&#20307;&#22806;&#21151;&#33021;&#25239;&#20307;&#30340;&#20840;&#21407;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
AbDiffuser: Full-Atom Generation of In-Vitro Functioning Antibodies. (arXiv:2308.05027v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05027
&lt;/p&gt;
&lt;p&gt;
AbDiffuser&#26159;&#19968;&#20010;&#29289;&#29702;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#29983;&#25104;&#25239;&#20307;&#30340;&#19977;&#32500;&#32467;&#26500;&#21644;&#24207;&#21015;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#32422;&#26463;&#25913;&#21892;&#34507;&#30333;&#36136;&#25193;&#25955;&#65292;&#22788;&#29702;&#24207;&#21015;&#38271;&#24230;&#21464;&#21270;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#19982;&#21442;&#32771;&#38598;&#21512;&#30340;&#24207;&#21015;&#21644;&#32467;&#26500;&#29305;&#24615;&#23494;&#20999;&#21305;&#37197;&#30340;&#25239;&#20307;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AbDiffuser&#33021;&#22815;&#29983;&#25104;&#39640;&#27700;&#24179;&#34920;&#36798;&#30340;&#25239;&#20307;&#65292;&#20854;&#20013;57.1%&#30340;&#35774;&#35745;&#36873;&#25321;&#26159;&#32039;&#23494;&#32467;&#21512;&#21058;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;AbDiffuser&#30340;&#31561;&#21464;&#29289;&#29702;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#29983;&#25104;&#25239;&#20307;&#30340;&#19977;&#32500;&#32467;&#26500;&#21644;&#24207;&#21015;&#12290;AbDiffuser&#24314;&#31435;&#22312;&#19968;&#31181;&#26032;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#34920;&#31034;&#19978;&#65292;&#20381;&#36182;&#20110;&#19968;&#31181;&#38024;&#23545;&#40784;&#20301;&#34507;&#30333;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#24182;&#21033;&#29992;&#24378;&#25193;&#25955;&#20808;&#39564;&#25913;&#21892;&#21435;&#22122;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#32422;&#26463;&#25913;&#21892;&#20102;&#34507;&#30333;&#36136;&#25193;&#25955;&#65307;&#22788;&#29702;&#24207;&#21015;&#38271;&#24230;&#21464;&#21270;&#65307;&#24182;&#23558;&#20869;&#23384;&#22797;&#26434;&#24615;&#38477;&#20302;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#23454;&#29616;&#20102;&#39592;&#26550;&#21644;&#20391;&#38142;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;&#20307;&#20869;&#21644;&#20307;&#22806;&#39564;&#35777;&#20102;AbDiffuser&#12290;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;AbDiffuser&#29983;&#25104;&#19982;&#21442;&#32771;&#38598;&#21512;&#30340;&#24207;&#21015;&#21644;&#32467;&#26500;&#29305;&#24615;&#23494;&#20999;&#21305;&#37197;&#30340;&#25239;&#20307;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#23460;&#23454;&#39564;&#35777;&#23454;&#65292;&#21457;&#29616;&#30340;16&#31181;HER2&#25239;&#20307;&#22343;&#20197;&#39640;&#27700;&#24179;&#34920;&#36798;&#65292;&#24182;&#19988;57.1%&#30340;&#35774;&#35745;&#36873;&#25321;&#26159;&#32039;&#23494;&#32467;&#21512;&#21058;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce AbDiffuser, an equivariant and physics-informed diffusion model for the joint generation of antibody 3D structures and sequences. AbDiffuser is built on top of a new representation of protein structure, relies on a novel architecture for aligned proteins, and utilizes strong diffusion priors to improve the denoising process. Our approach improves protein diffusion by taking advantage of domain knowledge and physics-based constraints; handles sequence-length changes; and reduces memory complexity by an order of magnitude enabling backbone and side chain generation. We validate AbDiffuser in silico and in vitro. Numerical experiments showcase the ability of AbDiffuser to generate antibodies that closely track the sequence and structural properties of a reference set. Laboratory experiments confirm that all 16 HER2 antibodies discovered were expressed at high levels and that 57.1% of selected designs were tight binders.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38543;&#26426;&#23450;&#20301;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#32447;&#24615;&#25910;&#25947;&#30028;&#38480;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#26377;&#38480;&#20108;&#38454;&#30697;&#26465;&#20214;&#19979;&#36798;&#21040;&#21487;&#25509;&#21463;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.03686</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#23450;&#20301;&#26041;&#27861;&#33719;&#24471;&#25193;&#25955;&#27169;&#22411;&#30340;&#32447;&#24615;&#25910;&#25947;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Linear Convergence Bounds for Diffusion Models via Stochastic Localization. (arXiv:2308.03686v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03686
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#23450;&#20301;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#32447;&#24615;&#25910;&#25947;&#30028;&#38480;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#26377;&#38480;&#20108;&#38454;&#30697;&#26465;&#20214;&#19979;&#36798;&#21040;&#21487;&#25509;&#21463;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#20174;&#39640;&#32500;&#25968;&#25454;&#20998;&#24067;&#20013;&#29983;&#25104;&#36817;&#20284;&#26679;&#26412;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#31181;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#22810;&#39033;&#24335;&#30028;&#38480;&#65292;&#20551;&#35774;$L^2$&#20934;&#30830;&#30340;&#24471;&#20998;&#20272;&#35745;&#22120;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#24050;&#30693;&#30340;&#26368;&#20339;&#30028;&#38480;&#35201;&#20040;&#23545;&#25968;&#25454;&#32500;&#24230;&#26159;&#36229;&#32447;&#24615;&#30340;&#65292;&#35201;&#20040;&#38656;&#35201;&#24378;&#24179;&#28369;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20551;&#35774;&#21482;&#38656;&#35201;&#25968;&#25454;&#20998;&#24067;&#26377;&#26377;&#38480;&#20108;&#38454;&#30697;&#30340;&#25910;&#25947;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#23545;&#20110;&#25968;&#25454;&#32500;&#24230;&#26159;&#32447;&#24615;&#30340;&#65288;&#20056;&#20197;&#23545;&#25968;&#22240;&#23376;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25193;&#25955;&#27169;&#22411;&#26368;&#22810;&#38656;&#35201;$\tilde O(\frac{d \log^2(1/\delta)}{\varepsilon^2})$&#27493;&#65292;&#23601;&#21487;&#20197;&#23558;&#24102;&#26377;&#26041;&#24046;&#20026;$\delta$&#30340;&#39640;&#26031;&#22122;&#22768;&#25439;&#22351;&#30340;&#20219;&#24847;&#25968;&#25454;&#20998;&#24067;&#22312;Kullback--Leibler&#25955;&#24230;&#19979;&#36817;&#20284;&#21040;$\varepsilon^2$&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#20381;&#36182;&#20110;&#21069;&#20154;&#30340;Girsanov&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#20110;&#21453;&#21521;SD&#31163;&#25955;&#21270;&#35823;&#24046;&#30340;&#31934;&#32454;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a powerful method for generating approximate samples from high-dimensional data distributions. Several recent results have provided polynomial bounds on the convergence rate of such models, assuming $L^2$-accurate score estimators. However, up until now the best known such bounds were either superlinear in the data dimension or required strong smoothness assumptions. We provide the first convergence bounds which are linear in the data dimension (up to logarithmic factors) assuming only finite second moments of the data distribution. We show that diffusion models require at most $\tilde O(\frac{d \log^2(1/\delta)}{\varepsilon^2})$ steps to approximate an arbitrary data distribution on $\mathbb{R}^d$ corrupted with Gaussian noise of variance $\delta$ to within $\varepsilon^2$ in Kullback--Leibler divergence. Our proof builds on the Girsanov-based methods of previous works. We introduce a refined treatment of the error arising from the discretization of the reverse SD
&lt;/p&gt;</description></item><item><title>VQGraph&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#24418;&#34920;&#31034;&#31354;&#38388;&#65292;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs&#12290;&#23427;&#37319;&#29992;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#30340;&#32534;&#30721;&#22120;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#22270;&#26631;&#35760;&#22120;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;&#24213;&#23618;&#22270;&#30340;&#22810;&#26679;&#21270;&#23616;&#37096;&#32467;&#26500;&#12290;&#36890;&#36807; VQGraph&#65292;&#21487;&#20197;&#23454;&#29616;&#20174;GNN&#21040;MLP&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2308.02117</link><description>&lt;p&gt;
VQGraph: &#22270;&#24418;&#21521;&#37327;&#37327;&#21270;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs
&lt;/p&gt;
&lt;p&gt;
VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs. (arXiv:2308.02117v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02117
&lt;/p&gt;
&lt;p&gt;
VQGraph&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#24418;&#34920;&#31034;&#31354;&#38388;&#65292;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs&#12290;&#23427;&#37319;&#29992;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#30340;&#32534;&#30721;&#22120;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#22270;&#26631;&#35760;&#22120;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;&#24213;&#23618;&#22270;&#30340;&#22810;&#26679;&#21270;&#23616;&#37096;&#32467;&#26500;&#12290;&#36890;&#36807; VQGraph&#65292;&#21487;&#20197;&#23454;&#29616;&#20174;GNN&#21040;MLP&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36827;&#34892;&#20449;&#24687;&#20256;&#36882;&#65292;&#32858;&#21512;&#23616;&#37096;&#37051;&#23621;&#20197;&#26356;&#26032;&#33410;&#28857;&#34920;&#31034;&#12290;&#36825;&#31181;&#20449;&#24687;&#20256;&#36882;&#23548;&#33268;&#22312;&#23454;&#38469;&#30340;&#24310;&#36831;&#32422;&#26463;&#24212;&#29992;&#31243;&#24207;&#20013;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#36890;&#36807;&#27169;&#20223;GNN&#30340;&#36755;&#20986;&#26469;&#23398;&#20064;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GNN&#34920;&#31034;&#31354;&#38388;&#21487;&#33021;&#19981;&#36275;&#20197;&#34920;&#31034;&#24213;&#23618;&#22270;&#30340;&#22810;&#26679;&#21270;&#23616;&#37096;&#32467;&#26500;&#65292;&#36825;&#38480;&#21046;&#20102;&#20174;GNN&#21040;MLP&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;VQGraph&#65292;&#29992;&#20110;&#23398;&#20064;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#24418;&#34920;&#31034;&#31354;&#38388;&#65292;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#21464;&#20307;&#30340;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#30340;&#32534;&#30721;&#22120;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#22270;&#26631;&#35760;&#22120;&#65292;&#23427;&#23558;&#22810;&#26679;&#21270;&#30340;&#23616;&#37096;&#32467;&#26500;&#33410;&#28857;&#26126;&#30830;&#34920;&#31034;&#20026;&#22823;&#37327;&#31163;&#25955;&#20196;&#29260;&#65292;&#24182;&#26500;&#25104;&#19968;&#20010;&#26377;&#24847;&#20041;&#30340;&#20195;&#30721;&#20070;&#12290;&#37197;&#22791;&#20102;&#23398;&#20064;&#30340;&#20195;&#30721;&#20070;&#65292;&#25105;&#20204;&#25552;&#20986;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) conduct message passing which aggregates local neighbors to update node representations. Such message passing leads to scalability issues in practical latency-constrained applications. To address this issue, recent methods adopt knowledge distillation (KD) to learn computationally-efficient multi-layer perceptron (MLP) by mimicking the output of GNN. However, the existing GNN representation space may not be expressive enough for representing diverse local structures of the underlying graph, which limits the knowledge transfer from GNN to MLP. Here we present a novel framework VQGraph to learn a powerful graph representation space for bridging GNNs and MLPs. We adopt the encoder of a variant of a vector-quantized variational autoencoder (VQ-VAE) as a structure-aware graph tokenizer, which explicitly represents the nodes of diverse local structures as numerous discrete tokens and constitutes a meaningful codebook. Equipped with the learned codebook, we propos
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#65292;&#24182;&#21457;&#29616;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#27169;&#22411;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00071</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25512;&#29702;&#26041;&#27861;&#29992;&#20110;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Interpretable Stereotype Identification through Reasoning. (arXiv:2308.00071v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#65292;&#24182;&#21457;&#29616;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#27169;&#22411;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20351;&#29992;&#20102;&#21253;&#21547;&#22266;&#26377;&#20559;&#35265;&#30340;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#21487;&#33021;&#20250;&#19981;&#32463;&#24847;&#22320;&#25345;&#32493;&#31995;&#32479;&#24615;&#27495;&#35270;&#65292;&#22240;&#27492;&#65292;&#23457;&#26597;&#21644;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#23558;&#20844;&#24179;&#24615;&#25972;&#21512;&#21040;&#23427;&#20204;&#30340;&#21457;&#23637;&#20013;&#65292;&#20197;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#20844;&#27491;&#21644;&#26080;&#20559;&#30340;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;Vicuna-13B-v1.3&#30340;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#25512;&#29702;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#25105;&#20204;&#35266;&#23519;&#21040;&#20174;13B&#21040;33B&#30340;&#35268;&#27169;&#25193;&#23637;&#20250;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25512;&#29702;&#21487;&#33021;&#26159;&#20351;LLMs&#22312;&#21051;&#26495;&#21360;&#35937;&#31561;&#39046;&#22495;&#20219;&#21153;&#19978;&#36229;&#36234;&#35268;&#27169;&#23450;&#24459;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#36873;&#23450;&#30340;&#25512;&#29702;&#36861;&#36394;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#31361;&#20986;&#26174;&#31034;&#20102;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#22810;&#32593;&#26684;&#27714;&#35299;&#22120;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#31163;&#25955;&#24322;&#36136;Helmholtz&#26041;&#31243;&#30340;&#36845;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#21487;&#20280;&#32553;&#24615;&#21644;&#27714;&#35299;&#36895;&#24230;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#20854;&#20013;&#30340;&#19977;&#20010;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#24341;&#20837;&#38544;&#24335;&#23618;&#26469;&#35299;&#20915;CNN&#20013;&#30340;&#35270;&#37326;&#38382;&#39064;&#12289;&#25913;&#36827;CNN&#39044;&#26465;&#20214;&#25216;&#26415;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#35757;&#32451;&#26041;&#27861;&#20351;&#32593;&#32476;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#23610;&#23544;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17486</link><description>&lt;p&gt;
&#29992;&#20110;Helmholtz&#26041;&#31243;&#30340;&#22810;&#32593;&#26684;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65306;&#36890;&#36807;&#32039;&#33268;&#38544;&#24335;&#23618;&#25552;&#39640;&#21487;&#20280;&#32553;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multigrid-Augmented Deep Learning for the Helmholtz Equation: Better Scalability with Compact Implicit Layers. (arXiv:2306.17486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17486
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#22810;&#32593;&#26684;&#27714;&#35299;&#22120;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#31163;&#25955;&#24322;&#36136;Helmholtz&#26041;&#31243;&#30340;&#36845;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#21487;&#20280;&#32553;&#24615;&#21644;&#27714;&#35299;&#36895;&#24230;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#20854;&#20013;&#30340;&#19977;&#20010;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#24341;&#20837;&#38544;&#24335;&#23618;&#26469;&#35299;&#20915;CNN&#20013;&#30340;&#35270;&#37326;&#38382;&#39064;&#12289;&#25913;&#36827;CNN&#39044;&#26465;&#20214;&#25216;&#26415;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#35757;&#32451;&#26041;&#27861;&#20351;&#32593;&#32476;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#23610;&#23544;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36845;&#20195;&#26041;&#27861;&#26469;&#35299;&#20915;&#31163;&#25955;&#24322;&#36136;Helmholtz&#26041;&#31243;&#22312;&#39640;&#27874;&#25968;&#19979;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#32463;&#20856;&#30340;&#36845;&#20195;&#22810;&#32593;&#26684;&#27714;&#35299;&#22120;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#19982;&#39044;&#26465;&#20214;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#26356;&#24555;&#19988;&#21487;&#20280;&#32553;&#24615;&#26356;&#22909;&#30340;&#23398;&#20064;&#22411;&#31070;&#32463;&#27714;&#35299;&#22120;&#65292;&#30456;&#27604;&#26631;&#20934;&#30340;&#22810;&#32593;&#26684;&#27714;&#35299;&#22120;&#26356;&#20248;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20808;&#21069;&#36825;&#31867;&#31070;&#32463;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#23618;U-Net-like&#32534;&#30721;&#22120;-&#27714;&#35299;&#22120;CNN&#65292;&#20854;&#20013;&#22312;U-Net&#30340;&#26368;&#31895;&#31961;&#32593;&#26684;&#19978;&#21253;&#21547;&#19968;&#20010;&#38544;&#24335;&#23618;&#65292;&#21367;&#31215;&#26680;&#34987;&#21453;&#36716;&#12290;&#36825;&#31181;&#26041;&#27861;&#32531;&#35299;&#20102;CNN&#20013;&#30340;&#35270;&#37326;&#38382;&#39064;&#65292;&#24182;&#20801;&#35768;&#26356;&#22909;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22312;&#21442;&#25968;&#25968;&#37327;&#12289;&#35745;&#31639;&#26102;&#38388;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;CNN&#39044;&#26465;&#20214;&#22120;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#25193;&#23637;&#21040;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#23610;&#23544;&#38382;&#39064;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#21512;&#29702;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
We present a deep learning-based iterative approach to solve the discrete heterogeneous Helmholtz equation for high wavenumbers. Combining classical iterative multigrid solvers and convolutional neural networks (CNNs) via preconditioning, we obtain a learned neural solver that is faster and scales better than a standard multigrid solver. Our approach offers three main contributions over previous neural methods of this kind. First, we construct a multilevel U-Net-like encoder-solver CNN with an implicit layer on the coarsest grid of the U-Net, where convolution kernels are inverted. This alleviates the field of view problem in CNNs and allows better scalability. Second, we improve upon the previous CNN preconditioner in terms of the number of parameters, computation time, and convergence rates. Third, we propose a multiscale training approach that enables the network to scale to problems of previously unseen dimensions while still maintaining a reasonable training procedure. Our encoder
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24207;&#25968;&#21183;&#20989;&#25968;&#30340;&#29609;&#23478;&#35780;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#36870;&#26144;&#23556;&#30340;&#23884;&#22871;&#35745;&#31639;&#65292;&#33021;&#22815;&#20445;&#25345;&#20256;&#36882;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05366</link><description>&lt;p&gt;
&#24207;&#25968;&#21183;&#20989;&#25968;&#30340;&#29609;&#23478;&#35780;&#32423;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ordinal Potential-based Player Rating. (arXiv:2306.05366v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05366
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24207;&#25968;&#21183;&#20989;&#25968;&#30340;&#29609;&#23478;&#35780;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#36870;&#26144;&#23556;&#30340;&#23884;&#22871;&#35745;&#31639;&#65292;&#33021;&#22815;&#20445;&#25345;&#20256;&#36882;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#23545;&#20110;&#20219;&#24847;&#32431;&#31574;&#30053;$x$&#12289;$y$&#21644;$z$&#65292;&#22914;&#26524;$x$&#27604;$y$&#26356;&#22909;&#65292;$y$&#27604;$z$&#26356;&#22909;&#65292;&#21017;$x$&#27604;$z$&#26356;&#22909;&#65292;&#21017;&#20004;&#20010;&#23545;&#31216;&#30340;&#38646;&#21644;&#21338;&#24328;&#26159;&#21487;&#20256;&#36882;&#30340;&#12290;&#26368;&#36817;&#35266;&#23519;&#21040;&#65292;Elo&#35780;&#32423;&#26410;&#33021;&#20445;&#25345;&#31574;&#30053;&#20043;&#38388;&#30340;&#20256;&#36882;&#20851;&#31995;&#65292;&#22240;&#27492;&#19981;&#33021;&#27491;&#30830;&#25552;&#21462;&#28216;&#25103;&#30340;&#20256;&#36882;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#34920;&#26126;&#24403;&#22312;&#27491;&#30830;&#30340;&#31354;&#38388;&#20013;&#35745;&#31639;Elo&#35780;&#32423;&#26102;&#65292;Elo&#35780;&#32423;&#30830;&#23454;&#33021;&#22815;&#20445;&#25345;&#20256;&#36882;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#21512;&#36866;&#30340;&#21487;&#36870;&#26144;&#23556;$\varphi$&#23558;&#28216;&#25103;&#24212;&#29992;&#20110;$\varphi$&#65292;&#28982;&#21518;&#35745;&#31639;Elo&#35780;&#32423;&#65292;&#26368;&#21518;&#36890;&#36807;&#24212;&#29992;$\varphi^{-1}$&#22238;&#21040;&#21407;&#22987;&#31354;&#38388;&#12290;&#25105;&#20204;&#23558;&#21487;&#20256;&#36882;&#28216;&#25103;&#30340;&#34920;&#24449;&#20026;&#21183;&#28216;&#25103;&#30340;&#19968;&#20010;&#24369;&#21464;&#20307;&#65292;&#20854;&#21183;&#20989;&#25968;&#26159;&#21152;&#24615;&#21487;&#20998;&#31163;&#30340;&#12290;&#21033;&#29992;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20256;&#36882;&#24207;&#25968;&#30340;&#27010;&#24565;&#65292;&#21363;&#23558;&#21487;&#20256;&#36882;&#28216;&#25103;&#30340;&#25910;&#30410;&#36716;&#21270;&#20026;&#20854;&#24046;&#24322;&#25152;&#38656;&#30340;&#26368;&#23567;&#21487;&#36870;&#26144;&#23556;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
A two-player symmetric zero-sum game is transitive if for any pure strategies $x$, $y$, $z$, if $x$ is better than $y$, and $y$ is better than $z$, then $x$ is better than $z$. It was recently observed that the Elo rating fails at preserving transitive relations among strategies and therefore cannot correctly extract the transitive component of a game. Our first contribution is to show that the Elo rating actually does preserve transitivity when computed in the right space. Precisely, using a suitable invertible mapping $\varphi$, we first apply $\varphi$ to the game, then compute Elo ratings, then go back to the original space by applying $\varphi^{-1}$. We provide a characterization of transitive games as a weak variant of ordinal potential games with additively separable potential functions. Leveraging this insight, we introduce the concept of transitivity order, the minimum number of invertible mappings required to transform the payoff of a transitive game into (differences of) its
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#21387;&#32553;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19968;&#23450;&#27604;&#29305;&#29575;&#19979;&#33021;&#22815;&#25552;&#39640;&#22270;&#20687;&#30340;&#24863;&#30693;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.18231</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#39640;&#20445;&#30495;&#22270;&#20687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
High-Fidelity Image Compression with Score-based Generative Models. (arXiv:2305.18231v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#21387;&#32553;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19968;&#23450;&#27604;&#29305;&#29575;&#19979;&#33021;&#22815;&#25552;&#39640;&#22270;&#20687;&#30340;&#24863;&#30693;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#22270;&#20687;&#21387;&#32553;&#39046;&#22495;&#22797;&#21046;&#36825;&#20010;&#25104;&#21151;&#21364;&#24456;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22312;&#32473;&#23450;&#27604;&#29305;&#29575;&#19979;&#30340;&#24863;&#30693;&#36136;&#37327;&#65292;&#36890;&#36807; FID &#20998;&#25968;&#35780;&#20272;&#65292;&#34920;&#29616;&#36229;&#36234;&#20102; PO-ELIC &#21644; HiFiC &#30340;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#20294;&#22312;&#29702;&#35770;&#19978;&#26377;&#21160;&#26426;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#23454;&#29616;&#20102;&#36825;&#19968;&#28857;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#20197; MSE &#20026;&#30446;&#26631;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#36827;&#19968;&#27493;&#22522;&#20110;&#20998;&#25968;&#30340;&#35299;&#30721;&#22120;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#25105;&#20204;&#23558;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;&#23454;&#29616;&#32454;&#33410;&#24456;&#37325;&#35201;&#65292;&#26368;&#20339;&#35774;&#35745;&#20915;&#31574;&#21487;&#33021;&#19982;&#20856;&#22411;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#26377;&#24456;&#22823;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the tremendous success of diffusion generative models in text-to-image generation, replicating this success in the domain of image compression has proven difficult. In this paper, we demonstrate that diffusion can significantly improve perceptual quality at a given bit-rate, outperforming state-of-the-art approaches PO-ELIC and HiFiC as measured by FID score. This is achieved using a simple but theoretically motivated two-stage approach combining an autoencoder targeting MSE followed by a further score-based decoder. However, as we will show, implementation details matter and the optimal design decisions can differ greatly from typical text-to-image models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;RHIP&#65289;&#65292;&#36890;&#36807;&#22270;&#21387;&#32553;&#12289;&#24182;&#34892;&#21270;&#21644;&#22522;&#20110;&#20027;&#29305;&#24449;&#21521;&#37327;&#30340;&#38382;&#39064;&#21021;&#22987;&#21270;&#35299;&#20915;&#20102;&#20840;&#29699;&#35268;&#27169;&#30340;MDPs&#12289;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#22312;&#35895;&#27468;&#22320;&#22270;&#20013;&#23454;&#29616;&#20102;16-24%&#30340;&#20840;&#29699;&#36335;&#32447;&#36136;&#37327;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.11290</link><description>&lt;p&gt;
&#35895;&#27468;&#22320;&#22270;&#20013;&#30340;&#22823;&#35268;&#27169;&#21487;&#25193;&#23637;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Massively Scalable Inverse Reinforcement Learning in Google Maps. (arXiv:2305.11290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;RHIP&#65289;&#65292;&#36890;&#36807;&#22270;&#21387;&#32553;&#12289;&#24182;&#34892;&#21270;&#21644;&#22522;&#20110;&#20027;&#29305;&#24449;&#21521;&#37327;&#30340;&#38382;&#39064;&#21021;&#22987;&#21270;&#35299;&#20915;&#20102;&#20840;&#29699;&#35268;&#27169;&#30340;MDPs&#12289;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#22312;&#35895;&#27468;&#22320;&#22270;&#20013;&#23454;&#29616;&#20102;16-24%&#30340;&#20840;&#29699;&#36335;&#32447;&#36136;&#37327;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#20154;&#31867;&#28508;&#22312;&#20559;&#22909;&#26159;&#36335;&#32447;&#25512;&#33616;&#20013;&#30340;&#24040;&#22823;&#25361;&#25112;&#65292;&#20840;&#29699;&#21487;&#25193;&#23637;&#35299;&#20915;&#26041;&#26696;&#20173;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#36807;&#21435;&#30340;&#30740;&#31350;&#20026;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#21019;&#24314;&#20102;&#36234;&#26469;&#36234;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#23578;&#26410;&#25104;&#21151;&#25193;&#23637;&#21040;&#19990;&#30028;&#35268;&#27169;&#30340;MDP&#12289;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#65292;&#20998;&#21035;&#28041;&#21450;&#25968;&#20159;&#20010;&#29366;&#24577;&#12289;&#36712;&#36857;&#21644;&#21442;&#25968;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#25913;&#36827;&#65292;&#32858;&#28966;&#20110;&#22270;&#21387;&#32553;&#12289;&#24182;&#34892;&#21270;&#21644;&#22522;&#20110;&#20027;&#29305;&#24449;&#21521;&#37327;&#30340;&#38382;&#39064;&#21021;&#22987;&#21270;&#65292;&#31361;&#30772;&#20197;&#24448;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36870;&#21521;&#35268;&#21010;&#36882;&#36827;&#22320;&#24179;&#38754;(RHIP)&#65292;&#23427;&#21487;&#20197;&#27010;&#25324;&#29616;&#26377;&#30340;&#24037;&#20316;&#65292;&#24182;&#36890;&#36807;&#20854;&#35268;&#21010;&#27700;&#24179;&#25511;&#21046;&#20851;&#38190;&#24615;&#33021;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#22312;&#20840;&#29699;&#36335;&#32447;&#36136;&#37327;&#26041;&#38754;&#23454;&#29616;&#20102;16-24%&#30340;&#25913;&#36827;&#65292;&#23601;&#25105;&#20204;&#25152;&#30693;&#65292;&#23427;&#26159;&#36804;&#20170;&#20026;&#27490;&#23454;&#29616;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#19979;&#30340;&#26368;&#22823;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#26356;&#22909;&#30340;&#23548;&#33322;&#34892;&#20026;&#21644;&#36335;&#24452;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing for humans' latent preferences is a grand challenge in route recommendation, where globally-scalable solutions remain an open problem. Although past work created increasingly general solutions for the application of inverse reinforcement learning (IRL), these have not been successfully scaled to world-sized MDPs, large datasets, and highly parameterized models; respectively hundreds of millions of states, trajectories, and parameters. In this work, we surpass previous limitations through a series of advancements focused on graph compression, parallelization, and problem initialization based on dominant eigenvectors. We introduce Receding Horizon Inverse Planning (RHIP), which generalizes existing work and enables control of key performance trade-offs via its planning horizon. Our policy achieves a 16-24% improvement in global route quality, and, to our knowledge, represents the largest instance of IRL in a real-world setting to date. Our results show critical benefits to mor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;CNN-Transformer&#27169;&#22411;&#65292;&#20351;&#29992;CNN&#23884;&#20837;&#23618;&#21644;&#37096;&#20998;&#33258;&#27880;&#24847;&#21147;&#65292;&#33021;&#26356;&#22909;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#29305;&#24449;&#24182;&#28040;&#38500;&#20887;&#20313;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;TSP&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;GPU&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2305.01883</link><description>&lt;p&gt;
&#19968;&#31181;&#36731;&#37327;&#32423;CNN-Transformer&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#26053;&#34892;&#21830;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Lightweight CNN-Transformer Model for Learning Traveling Salesman Problems. (arXiv:2305.01883v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;CNN-Transformer&#27169;&#22411;&#65292;&#20351;&#29992;CNN&#23884;&#20837;&#23618;&#21644;&#37096;&#20998;&#33258;&#27880;&#24847;&#21147;&#65292;&#33021;&#26356;&#22909;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#29305;&#24449;&#24182;&#28040;&#38500;&#20887;&#20313;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;TSP&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;GPU&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#21363;&#20351;&#22312;&#22823;&#35268;&#27169;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSPs&#65289;&#20013;&#20063;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22522;&#20110;&#20840;&#36830;&#25509;&#30340;&#27880;&#24847;&#27169;&#22411;&#65292;&#23384;&#22312;&#22823;&#37327;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;GPU&#20869;&#23384;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#23884;&#20837;&#23618;&#21644;&#37096;&#20998;&#33258;&#27880;&#24847;&#21147;&#30340;&#36731;&#37327;&#32423;CNN-Transformer&#27169;&#22411;&#12290;&#19982;&#26631;&#20934;Transformer&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;CNN-Transformer&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#29305;&#24449;&#12290;&#23427;&#36824;&#20351;&#29992;&#25552;&#20986;&#30340;&#37096;&#20998;&#33258;&#27880;&#24847;&#21147;&#28040;&#38500;&#20102;&#20840;&#36830;&#25509;&#27880;&#24847;&#27169;&#22411;&#20013;&#30340;&#30456;&#24403;&#25968;&#37327;&#30340;&#20887;&#20313;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;TSP&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12289;GPU&#20869;&#23384;&#20351;&#29992;&#21644;&#25512;&#29702;&#26102;&#38388;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;GPU&#20869;&#23384;&#32422;&#23569;20&#65285;&#65292;&#25512;&#29702;&#26102;&#38388;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#24555;45&#65285;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#65292;&#32593;&#22336;&#20026;https://g
&lt;/p&gt;
&lt;p&gt;
Transformer-based models show state-of-the-art performance even for large-scale Traveling Salesman Problems (TSPs). However, they are based on fully-connected attention models and suffer from large computational complexity and GPU memory usage. We propose a lightweight CNN-Transformer model based on a CNN embedding layer and partial self-attention. Our CNN-Transformer model is able to better learn spatial features from input data using a CNN embedding layer compared with the standard Transformer models. It also removes considerable redundancy in fully connected attention models using the proposed partial self-attention. Experiments show that the proposed model outperforms other state-of-the-art Transformer-based models in terms of TSP solution quality, GPU memory usage, and inference time. Our model consumes approximately 20% less GPU memory usage and has 45% faster inference time compared with other state-of-the-art Transformer-based models. Our code is publicly available at https://g
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;ClusterNet&#65292;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#32858;&#31867;&#27169;&#22411;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#28857;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21453;&#26144;&#20154;&#31867;&#24863;&#30693;&#30340;&#32858;&#31867;&#21487;&#20998;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14185</link><description>&lt;p&gt;
ClusterNet&#65306;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#32858;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ClusterNet: A Perception-Based Clustering Model for Scattered Data. (arXiv:2304.14185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14185
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;ClusterNet&#65292;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#32858;&#31867;&#27169;&#22411;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#28857;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21453;&#26144;&#20154;&#31867;&#24863;&#30693;&#30340;&#32858;&#31867;&#21487;&#20998;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25955;&#28857;&#22270;&#20013;&#30340;&#32858;&#31867;&#20998;&#31163;&#26159;&#19968;&#20010;&#36890;&#24120;&#30001;&#24191;&#27867;&#20351;&#29992;&#30340;&#32858;&#31867;&#25216;&#26415;&#65288;&#20363;&#22914;k-means&#25110;DBSCAN&#65289;&#26469;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#31639;&#27861;&#22522;&#20110;&#38750;&#24863;&#30693;&#24230;&#37327;&#65292;&#23427;&#20204;&#30340;&#36755;&#20986;&#32463;&#24120;&#19981;&#33021;&#21453;&#26144;&#20986;&#20154;&#31867;&#32858;&#31867;&#24863;&#30693;&#12290;&#20026;&#20102;&#24357;&#21512;&#20154;&#31867;&#32858;&#31867;&#24863;&#30693;&#21644;&#26426;&#22120;&#35745;&#31639;&#32858;&#31867;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#22788;&#29702;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;&#20026;&#20102;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#23398;&#20064;&#24863;&#30693;&#32858;&#31867;&#20998;&#31163;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20247;&#21253;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#24037;&#20316;&#65292;&#20854;&#20013;&#21253;&#25324;384&#20010;&#20154;&#32676;&#24037;&#20316;&#32773;&#23545;&#21452;&#21464;&#37327;&#25968;&#25454;&#30340;7,320&#20010;&#28857;&#32858;&#31867;&#20174;&#23646;&#36827;&#34892;&#20102;&#26631;&#35760;&#12290;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;ClusterNet&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#28857;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#34987;&#35757;&#32451;&#25104;&#21453;&#26144;&#20154;&#31867;&#24863;&#30693;&#30340;&#32858;&#31867;&#21487;&#20998;&#24615;&#12290;&#20026;&#20102;&#22312;&#20154;&#31867;&#27880;&#37322;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;ClusterNet&#65292;&#25105;&#20204;&#30465;&#30053;&#20102;&#22312;2D&#30011;&#24067;&#19978;&#28210;&#26579;&#25955;&#28857;&#22270;&#65292;&#32780;&#26159;&#20351;&#29992;&#20102;&#19968;&#20010;PointNet++&#26550;&#26500;&#65292;&#20351;&#20854;&#33021;&#22815;&#30452;&#25509;&#25512;&#29702;&#28857;&#20113;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#32858;&#31867;&#27169;&#22411;&#65292;ClusterNet&#12290;
&lt;/p&gt;
&lt;p&gt;
Cluster separation in scatterplots is a task that is typically tackled by widely used clustering techniques, such as for instance k-means or DBSCAN. However, as these algorithms are based on non-perceptual metrics, their output often does not reflect human cluster perception. To bridge the gap between human cluster perception and machine-computed clusters, we propose a learning strategy which directly operates on scattered data. To learn perceptual cluster separation on this data, we crowdsourced a large scale dataset, consisting of 7,320 point-wise cluster affiliations for bivariate data, which has been labeled by 384 human crowd workers. Based on this data, we were able to train ClusterNet, a point-based deep learning model, trained to reflect human perception of cluster separability. In order to train ClusterNet on human annotated data, we omit rendering scatterplots on a 2D canvas, but rather use a PointNet++ architecture enabling inference on point clouds directly. In this work, w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIFT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#34920;&#31034;&#33021;&#21147;&#65292;&#36890;&#36807;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#65292;&#32553;&#30701;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.11525</link><description>&lt;p&gt;
SIFT: &#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#20197;&#26368;&#22823;&#38480;&#24230;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency. (arXiv:2303.11525v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIFT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#34920;&#31034;&#33021;&#21147;&#65292;&#36890;&#36807;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#65292;&#32553;&#30701;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#26435;&#37325;&#31232;&#30095;&#24615;&#26469;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35757;&#32451;&#25928;&#29575;&#65288;&#19982;&#35757;&#32451;FLOPS&#30456;&#20851;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#65289;&#12290; &#36825;&#20123;&#24037;&#20316;&#26088;&#22312;&#20943;&#23569;&#35757;&#32451;FLOP&#65292;&#20294;&#20351;&#29992;&#31232;&#30095;&#26435;&#37325;&#36827;&#34892;&#35757;&#32451;&#36890;&#24120;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#25439;&#22833;&#25110;&#38656;&#35201;&#26356;&#38271;&#30340;&#35757;&#32451;&#21608;&#26399;&#65292;&#20351;&#24471;&#32467;&#26524;&#30340;&#35757;&#32451;&#25928;&#29575;&#19981;&#22815;&#28165;&#26224;&#12290; &#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#31232;&#30095;&#24615;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#19982;&#23494;&#38598;&#27169;&#22411;&#30456;&#21516;&#30340;FLOPS&#65292;&#24182;&#36890;&#36807;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#23637;&#31034;&#35757;&#32451;&#25928;&#29575;&#25552;&#39640;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SIFT&#65292;&#19968;&#32452;&#29992;&#20316;&#23494;&#38598;&#23618;&#30340;&#21363;&#25554;&#21363;&#29992;&#26367;&#20195;&#21697;&#26469;&#25552;&#39640;&#20854;&#34920;&#31034;&#33021;&#21147;&#21644;FLOP&#25928;&#29575;&#30340;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#12290; &#27599;&#20010;&#36716;&#25442;&#37117;&#30001;&#19968;&#20010;&#21333;&#19968;&#21442;&#25968;&#65288;&#31232;&#30095;&#32423;&#21035;&#65289;&#21442;&#25968;&#21270;&#65292;&#24182;&#25552;&#20379;&#26356;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#31232;&#30095;&#25513;&#33180;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have explored the use of weight sparsity to improve the training efficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs). These works aim to reduce training FLOPs but training with sparse weights often leads to accuracy loss or requires longer train schedules, making the resulting training efficiency less clear. In contrast, we focus on using sparsity to increase accuracy while using the same FLOPS as the dense model and show training efficiency gains through higher accuracy. In this work, we introduce SIFT, a family of Sparse Iso-FLOP Transformations which are used as drop-in replacements for dense layers to improve their representational capacity and FLOP efficiency. Each transformation is parameterized by a single parameter (sparsity level) and provides a larger search space to find optimal sparse masks. Without changing any training hyperparameters, replacing dense layers with SIFT leads to significant improvements across computer vision (CV) and
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#21644;&#33539;&#22260;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#21306;&#20998;&#19981;&#21516;&#36136;&#37327;&#30340;&#36127;&#26679;&#26412;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.00286</link><description>&lt;p&gt;
&#38024;&#23545;&#38142;&#25509;&#39044;&#27979;&#65292;&#23545;&#24453;&#19981;&#21516;&#30340;&#36127;&#26679;&#26412;&#26377;&#24046;&#24322;&#24615;&#65306;&#21033;&#29992;&#39046;&#22495;&#21644;&#33539;&#22260;&#32422;&#26463;&#20016;&#23500;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Treat Different Negatives Differently: Enriching Loss Functions with Domain and Range Constraints for Link Prediction. (arXiv:2303.00286v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00286
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#21644;&#33539;&#22260;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#21306;&#20998;&#19981;&#21516;&#36136;&#37327;&#30340;&#36127;&#26679;&#26412;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65288;KGEMs&#65289;&#29992;&#20110;&#19982;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#30456;&#20851;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#38142;&#25509;&#39044;&#27979;&#12290;&#23427;&#20204;&#20351;&#29992;&#32771;&#34385;&#20102;&#19968;&#25209;&#24471;&#20998;&#19977;&#20803;&#32452;&#21450;&#20854;&#30456;&#24212;&#26631;&#31614;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;&#20256;&#32479;&#26041;&#27861;&#35748;&#20026;&#19977;&#20803;&#32452;&#30340;&#26631;&#31614;&#35201;&#20040;&#20026;&#30495;&#65292;&#35201;&#20040;&#20026;&#20551;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;&#30340;&#36127;&#26679;&#26412;&#24212;&#35813;&#34987;&#24179;&#31561;&#23545;&#24453;&#12290;&#19982;&#36825;&#19968;&#26368;&#36817;&#30340;&#20551;&#35774;&#19968;&#33268;&#65292;&#25105;&#20204;&#35748;&#20026;&#22522;&#20110;&#39046;&#22495;&#21644;&#33539;&#22260;&#32422;&#26463;&#22312;&#35821;&#20041;&#19978;&#26377;&#25928;&#30340;&#36127;&#26679;&#26412;&#21487;&#33021;&#26159;&#39640;&#36136;&#37327;&#30340;&#36127;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#25439;&#22833;&#20989;&#25968;&#24212;&#35813;&#23558;&#23427;&#20204;&#19982;&#35821;&#20041;&#19978;&#26080;&#25928;&#30340;&#36127;&#26679;&#26412;&#21306;&#21035;&#23545;&#24453;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#38024;&#23545;&#38142;&#25509;&#39044;&#27979;&#30340;&#19977;&#20010;&#20027;&#35201;&#25439;&#22833;&#20989;&#25968;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#29256;&#26412;&#12290;&#36890;&#36807;&#24191;&#27867;&#21644;&#21463;&#25511;&#30340;&#23454;&#39564;&#35774;&#32622;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#22312;&#19977;&#20010;&#20855;&#26377;&#19981;&#21516;&#27169;&#24335;&#30340;&#20844;&#20849;&#22522;&#20934;KG&#19978;&#31995;&#32479;&#22320;&#25552;&#20379;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding models (KGEMs) are used for various tasks related to knowledge graphs (KGs), including link prediction. They are trained with loss functions that are computed considering a batch of scored triples and their corresponding labels. Traditional approaches consider the label of a triple to be either true or false. However, recent works suggest that all negative triples should not be valued equally. In line with this recent assumption, we posit that negative triples that are semantically valid w.r.t. domain and range constraints might be high-quality negative triples. As such, loss functions should treat them differently from semantically invalid negative ones. To this aim, we propose semantic-driven versions for the three main loss functions for link prediction. In an extensive and controlled experimental setting, we show that the proposed loss functions systematically provide satisfying results on three public benchmark KGs underpinned with different schemas, whic
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35268;&#32422;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#20219;&#20309;&#22810;&#25209;&#27425;&#31639;&#27861;&#36716;&#21270;&#20026;&#22788;&#29702;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#38543;&#26426;&#24310;&#36831;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#25105;&#20204;&#19981;&#20165;&#22312;&#36172;&#21338;&#26426;&#12289;&#34920;&#26684;&#22411;MDPs&#21644;&#34920;&#26684;&#22411;MGs&#26041;&#38754;&#21462;&#24471;&#20102;&#19982;&#29616;&#26377;&#32467;&#26524;&#30456;&#21305;&#37197;&#25110;&#25913;&#36827;&#30340;&#25104;&#26524;&#65292;&#36824;&#39318;&#27425;&#23545;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#24310;&#36831;&#19982;&#20989;&#25968;&#36924;&#36817;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2302.01477</link><description>&lt;p&gt;
&#22522;&#20110;&#35268;&#32422;&#30340;&#24310;&#36831;&#21453;&#39304;&#39034;&#24207;&#20915;&#31574;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Reduction-based Framework for Sequential Decision Making with Delayed Feedback. (arXiv:2302.01477v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01477
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35268;&#32422;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#20219;&#20309;&#22810;&#25209;&#27425;&#31639;&#27861;&#36716;&#21270;&#20026;&#22788;&#29702;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#38543;&#26426;&#24310;&#36831;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#25105;&#20204;&#19981;&#20165;&#22312;&#36172;&#21338;&#26426;&#12289;&#34920;&#26684;&#22411;MDPs&#21644;&#34920;&#26684;&#22411;MGs&#26041;&#38754;&#21462;&#24471;&#20102;&#19982;&#29616;&#26377;&#32467;&#26524;&#30456;&#21305;&#37197;&#25110;&#25913;&#36827;&#30340;&#25104;&#26524;&#65292;&#36824;&#39318;&#27425;&#23545;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#24310;&#36831;&#19982;&#20989;&#25968;&#36924;&#36817;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#33324;&#22810;&#26234;&#33021;&#20307;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#38543;&#26426;&#24310;&#36831;&#21453;&#39304;&#65292;&#21253;&#25324;&#36172;&#21338;&#26426;&#38382;&#39064;&#12289;&#21333;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65288;MGs&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35268;&#32422;&#30340;&#26694;&#26550;&#65292;&#23558;&#20219;&#20309;&#22810;&#25209;&#27425;&#31639;&#27861;&#36716;&#21270;&#20026;&#33021;&#22788;&#29702;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#38543;&#26426;&#24310;&#36831;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#22810;&#25209;&#27425;&#31639;&#27861;&#25554;&#20837;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#20010;&#31034;&#20363;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#20165;&#21305;&#37197;&#25110;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#36172;&#21338;&#26426;&#12289;&#34920;&#26684;&#22411;MDPs&#21644;&#34920;&#26684;&#22411;MGs&#30340;&#32467;&#26524;&#65292;&#36824;&#39318;&#27425;&#23545;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#24310;&#36831;&#19982;&#20989;&#25968;&#36924;&#36817;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#20026;&#22810;&#26234;&#33021;&#20307;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#24310;&#36831;&#21453;&#39304;&#25552;&#20379;&#20102;&#19968;&#22871;&#23436;&#25972;&#30340;&#23574;&#38160;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study stochastic delayed feedback in general multi-agent sequential decision making, which includes bandits, single-agent Markov decision processes (MDPs), and Markov games (MGs). We propose a novel reduction-based framework, which turns any multi-batched algorithm for sequential decision making with instantaneous feedback into a sample-efficient algorithm that can handle stochastic delays in sequential decision making. By plugging different multi-batched algorithms into our framework, we provide several examples demonstrating that our framework not only matches or improves existing results for bandits, tabular MDPs, and tabular MGs, but also provides the first line of studies on delays in sequential decision making with function approximation. In summary, we provide a complete set of sharp results for multi-agent sequential decision making with delayed feedback.
&lt;/p&gt;</description></item><item><title>AMPNet&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#65292;&#33021;&#22815;&#23545;&#33410;&#28857;&#36827;&#34892;&#36880;&#20010;&#29305;&#24449;&#32534;&#30721;&#65292;&#24182;&#36890;&#36807;&#36328;&#33410;&#28857;&#27880;&#24847;&#21147;&#27169;&#22411;&#29305;&#24449;&#32423;&#21035;&#30340;&#20132;&#20114;&#12290;&#22312;&#23454;&#38469;&#29983;&#29289;&#31995;&#32479;&#19978;&#36827;&#34892;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#65292;AMPNet&#22312;fMRI&#20449;&#21495;&#37325;&#24314;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#32423;&#21035;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.09475</link><description>&lt;p&gt;
AMPNet: &#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28040;&#24687;&#20256;&#36882;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
AMPNet: Attention as Message Passing for Graph Neural Networks. (arXiv:2210.09475v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09475
&lt;/p&gt;
&lt;p&gt;
AMPNet&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#65292;&#33021;&#22815;&#23545;&#33410;&#28857;&#36827;&#34892;&#36880;&#20010;&#29305;&#24449;&#32534;&#30721;&#65292;&#24182;&#36890;&#36807;&#36328;&#33410;&#28857;&#27880;&#24847;&#21147;&#27169;&#22411;&#29305;&#24449;&#32423;&#21035;&#30340;&#20132;&#20114;&#12290;&#22312;&#23454;&#38469;&#29983;&#29289;&#31995;&#32479;&#19978;&#36827;&#34892;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#65292;AMPNet&#22312;fMRI&#20449;&#21495;&#37325;&#24314;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#32423;&#21035;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;&#20256;&#32479;GNNs&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#26159;&#23558;&#27599;&#20010;&#33410;&#28857;&#34920;&#31034;&#20026;&#19968;&#20010;&#21333;&#19968;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#21487;&#33021;&#24573;&#35270;&#20102;&#20851;&#20110;&#20010;&#20307;&#33410;&#28857;&#29305;&#24449;&#30340;&#22797;&#26434;&#32454;&#33410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;AMPNet&#65292;&#29992;&#20110;GNNs&#65292;&#23427;&#23545;&#33410;&#28857;&#36827;&#34892;&#36880;&#20010;&#29305;&#24449;&#32534;&#30721;&#65292;&#24182;&#22312;&#28040;&#24687;&#20256;&#36882;&#27493;&#39588;&#20013;&#36890;&#36807;&#36328;&#33410;&#28857;&#27880;&#24847;&#21147;&#27169;&#22411;&#29305;&#24449;&#32423;&#21035;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#30495;&#23454;&#29983;&#29289;&#31995;&#32479;&#65288;&#22914;fMRI&#33041;&#27963;&#21160;&#35760;&#24405;&#21644;&#31354;&#38388;&#22522;&#22240;&#32452;&#25968;&#25454;&#65289;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;AMPNet&#30340;&#33021;&#21147;&#65292;&#23427;&#22312;fMRI&#20449;&#21495;&#37325;&#24314;&#26041;&#38754;&#30456;&#27604;&#29616;&#26377;&#22522;&#20934;&#25552;&#39640;&#20102;20&#65285;&#65292;&#22312;&#28155;&#21152;&#20301;&#32622;&#23884;&#20837;&#21518;&#21448;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;8&#65285;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#29983;&#29289;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;AMPNet&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#32423;&#21035;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39044;&#35745;&#25105;&#20204;&#30340;&#26550;&#26500;&#23558;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as a powerful representation learning framework for graph-structured data. A key limitation of conventional GNNs is their representation of each node with a singular feature vector, potentially overlooking intricate details about individual node features. Here, we propose an Attention-based Message-Passing layer for GNNs (AMPNet) that encodes individual features per node and models feature-level interactions through cross-node attention during message-passing steps. We demonstrate the abilities of AMPNet through extensive benchmarking on real-world biological systems such as fMRI brain activity recordings and spatial genomic data, improving over existing baselines by 20% on fMRI signal reconstruction, and further improving another 8% with positional embedding added. Finally, we validate the ability of AMPNet to uncover meaningful feature-level interactions through case studies on biological systems. We anticipate that our architecture will be h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#38656;&#25104;&#21592;&#26597;&#35810;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23398;&#20064;&#22823;&#23567;&#20026;$t$&#30340;&#20915;&#31574;&#26641;&#30340;&#31639;&#27861;&#65292;&#24182;&#25104;&#21151;&#37327;&#21270;&#20102;Kalai&#21644;Kanade&#30340;&#19981;&#30693;&#24615;&#22686;&#24378;&#31639;&#27861;&#65292;&#24471;&#21040;&#20102;&#31532;&#19968;&#20010;&#39640;&#25928;&#30340;&#37327;&#23376;&#19981;&#30693;&#24615;&#22686;&#24378;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.00212</link><description>&lt;p&gt;
&#39640;&#25928;&#37327;&#23376;&#19981;&#21487;&#30693;&#19981;&#24403;&#20915;&#31574;&#26641;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Quantum Agnostic Improper Learning of Decision Trees. (arXiv:2210.00212v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#38656;&#25104;&#21592;&#26597;&#35810;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23398;&#20064;&#22823;&#23567;&#20026;$t$&#30340;&#20915;&#31574;&#26641;&#30340;&#31639;&#27861;&#65292;&#24182;&#25104;&#21151;&#37327;&#21270;&#20102;Kalai&#21644;Kanade&#30340;&#19981;&#30693;&#24615;&#22686;&#24378;&#31639;&#27861;&#65292;&#24471;&#21040;&#20102;&#31532;&#19968;&#20010;&#39640;&#25928;&#30340;&#37327;&#23376;&#19981;&#30693;&#24615;&#22686;&#24378;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30693;&#24615;&#35774;&#32622;&#26159;&#26368;&#31867;&#20284;&#20110;&#23398;&#20064;&#23545;&#25239;&#22122;&#22768;&#30340;PAC&#27169;&#22411;&#30340;&#26368;&#22256;&#38590;&#30340;&#27867;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;Poly$(n,t,{\frac{1}{\varepsilon}})$&#37327;&#23376;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#30693;&#24615;&#35774;&#32622;&#20013;&#26080;&#38656;&#25104;&#21592;&#26597;&#35810;&#21363;&#21487;&#23398;&#20064;&#22823;&#23567;&#20026;$t$&#30340;&#20915;&#31574;&#26641;&#65292;&#24182;&#19988;&#23454;&#20363;&#38388;&#20855;&#26377;&#22343;&#21248;&#36793;&#38469;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23398;&#20064;&#20915;&#31574;&#26641;&#30340;&#31639;&#27861;&#65288;&#32463;&#20856;&#25110;&#37327;&#23376;&#65289;&#65292;&#19988;&#26080;&#38656;&#25104;&#21592;&#26597;&#35810;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35774;&#35745;&#37327;&#23376;&#29256;&#26412;&#30340;Goldreich-Levin&#31639;&#27861;&#65292;&#20351;&#29992;&#39640;&#24230;&#20559;&#32622;&#30340;&#20989;&#25968;&#39044;&#35328;&#26426;&#26469;&#26500;&#24314;&#37327;&#23376;&#19981;&#30693;&#24615;&#24369;&#23398;&#20064;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#37327;&#21270;Kalai&#21644;Kanade&#65288;NIPS 2009&#65289;&#30340;&#19981;&#30693;&#24615;&#22686;&#24378;&#31639;&#27861;&#65292;&#20197;&#33719;&#24471;&#31532;&#19968;&#20010;&#39640;&#25928;&#30340;&#37327;&#23376;&#19981;&#30693;&#24615;&#22686;&#24378;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#37327;&#23376;&#22686;&#24378;&#31639;&#27861;&#22312;&#36866;&#24212;&#24615;&#37327;&#23376;&#22686;&#24378;&#31639;&#27861;&#20013;&#65292;&#25152;&#26377;&#24369;&#23398;&#20064;&#22120;&#20559;&#24046;&#20381;&#36182;&#24615;&#26041;&#38754;&#37117;&#20855;&#26377;&#22810;&#39033;&#24335;&#25913;&#36827;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#22312;$V$&#20013;&#30340;&#26631;&#20934;&#21152;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The agnostic setting is the hardest generalization of the PAC model since it is akin to learning with adversarial noise. In this paper, we give a poly$(n,t,{\frac{1}{\varepsilon}})$ quantum algorithm for learning size $t$ decision trees with uniform marginal over instances, in the agnostic setting, without membership queries. Our algorithm is the first algorithm (classical or quantum) for learning decision trees in polynomial time without membership queries. We show how to construct a quantum agnostic weak learner by designing a quantum version of the classical Goldreich-Levin algorithm that works with strongly biased function oracles. We show how to quantize the agnostic boosting algorithm by Kalai and Kanade (NIPS 2009) to obtain the first efficient quantum agnostic boosting algorithm. Our quantum boosting algorithm has a polynomial improvement in the dependence of the bias of the weak learner over all adaptive quantum boosting algorithms while retaining the standard speedup in the V
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23558;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#24212;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2203.11155</link><description>&lt;p&gt;
&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#22312;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Quantum Density Matrix in Classical Question Answering and Classical Image Classification. (arXiv:2203.11155v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11155
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23558;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#24212;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#21487;&#34920;&#31034;&#25972;&#20010;&#37327;&#23376;&#31995;&#32479;&#30340;&#20840;&#37096;&#20449;&#24687;&#65292;&#23558;&#23494;&#24230;&#30697;&#38453;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#20219;&#21153;&#21487;&#20197;&#26356;&#21152;&#26377;&#25928;&#22320;&#23454;&#29616;&#38382;&#39064;&#22238;&#31572;&#12290;&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#26032;&#26426;&#21046;&#65292;&#20197;&#24212;&#23545;&#36755;&#20837;&#20026;&#30697;&#38453;&#30340;&#24773;&#20917;&#65292;&#24182;&#23558;&#35813;&#26426;&#21046;&#24212;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;QA&#38382;&#39064;&#30340;&#27714;&#35299;&#65292;&#21516;&#26102;&#20063;&#35777;&#26126;&#20102;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#21487;&#20197;&#22686;&#24378;&#32463;&#20856;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#29305;&#24449;&#20449;&#24687;&#21644;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26032;&#26694;&#26550;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;CNN&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum density matrix represents all the information of the entire quantum system, and novel models of meaning employing density matrices naturally model linguistic phenomena such as hyponymy and linguistic ambiguity, among others in quantum question answering tasks. Naturally, we argue that applying the quantum density matrix into classical Question Answering (QA) tasks can show more effective performance. Specifically, we (i) design a new mechanism based on Long Short-Term Memory (LSTM) to accommodate the case when the inputs are matrixes; (ii) apply the new mechanism to QA problems with Convolutional Neural Network (CNN) and gain the LSTM-based QA model with the quantum density matrix. Experiments of our new model on TREC-QA and WIKI-QA data sets show encouraging results. Similarly, we argue that the quantum density matrix can also enhance the image feature information and the relationship between the features for the classical image classification. Thus, we (i) combine density mat
&lt;/p&gt;</description></item></channel></rss>