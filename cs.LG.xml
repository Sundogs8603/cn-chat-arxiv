<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ODE&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;Transformer&#23454;&#29616;&#27969;&#21305;&#37197;&#22312;&#28508;&#31354;&#38388;&#20013;&#30340;&#29702;&#35770;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20272;&#35745;ODE&#27969;&#29983;&#25104;&#26679;&#26412;&#20998;&#24067;&#26102;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#36824;&#35777;&#26126;&#20102;&#20855;&#26377;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#24615;&#30340;Transformer&#32593;&#32476;&#21487;&#20197;&#26377;&#25928;&#36924;&#36817;&#20219;&#24847;&#20809;&#28369;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2404.02538</link><description>&lt;p&gt;
&#27969;&#21305;&#37197;&#22312;&#28508;&#31354;&#38388;&#20013;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#19982;Transformer
&lt;/p&gt;
&lt;p&gt;
Convergence Analysis of Flow Matching in Latent Space with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02538
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ODE&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;Transformer&#23454;&#29616;&#27969;&#21305;&#37197;&#22312;&#28508;&#31354;&#38388;&#20013;&#30340;&#29702;&#35770;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20272;&#35745;ODE&#27969;&#29983;&#25104;&#26679;&#26412;&#20998;&#24067;&#26102;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#36824;&#35777;&#26126;&#20102;&#20855;&#26377;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#24615;&#30340;Transformer&#32593;&#32476;&#21487;&#20197;&#26377;&#25928;&#36924;&#36817;&#20219;&#24847;&#20809;&#28369;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ODE-based&#29983;&#25104;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#27969;&#21305;&#37197;&#30340;&#29702;&#35770;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#32534;&#30721;&#22120;&#32593;&#32476;&#23558;&#39640;&#32500;&#21407;&#22987;&#36755;&#20837;&#26144;&#23556;&#21040;&#20302;&#32500;&#28508;&#31354;&#38388;&#65292;&#20854;&#20013;&#19968;&#20010;Transformer&#32593;&#32476;&#34987;&#35757;&#32451;&#26469;&#39044;&#27979;&#20174;&#26631;&#20934;&#27491;&#24577;&#20998;&#24067;&#21040;&#30446;&#26631;&#28508;&#31354;&#38388;&#20998;&#24067;&#30340;&#21464;&#25442;&#36895;&#24230;&#22330;&#12290;&#25105;&#20204;&#30340;&#35823;&#24046;&#20998;&#26512;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#36890;&#36807;&#20272;&#35745;&#30340;ODE&#27969;&#29983;&#25104;&#26679;&#26412;&#30340;&#20998;&#24067;&#22312;&#28201;&#26031;&#22374;-2&#36317;&#31163;&#19979;&#25910;&#25947;&#21040;&#30446;&#26631;&#20998;&#24067;&#65292;&#36825;&#22312;&#28201;&#21644;&#19988;&#23454;&#38469;&#30340;&#20551;&#35774;&#19979;&#25104;&#31435;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20855;&#26377;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#24615;&#30340;Transformer&#32593;&#32476;&#21487;&#20197;&#26377;&#25928;&#22320;&#36924;&#36817;&#20219;&#24847;&#20809;&#28369;&#20989;&#25968;&#65292;&#36825;&#21487;&#33021;&#26159;&#29420;&#31435;&#24863;&#20852;&#36259;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02538v1 Announce Type: cross  Abstract: We present theoretical convergence guarantees for ODE-based generative models, specifically flow matching. We use a pre-trained autoencoder network to map high-dimensional original inputs to a low-dimensional latent space, where a transformer network is trained to predict the velocity field of the transformation from a standard normal distribution to the target latent distribution. Our error analysis demonstrates the effectiveness of this approach, showing that the distribution of samples generated via estimated ODE flow converges to the target distribution in the Wasserstein-2 distance under mild and practical assumptions. Furthermore, we show that arbitrary smooth functions can be effectively approximated by transformer networks with Lipschitz continuity, which may be of independent interest.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#26412;&#30340;DBSCAN&#65292;&#21033;&#29992;&#30456;&#20284;&#24615;&#22270;&#30340;&#22359;&#23545;&#35282;&#23646;&#24615;&#24341;&#23548;&#32858;&#31867;&#36807;&#31243;&#65292;&#36890;&#36807;&#26500;&#24314;&#22359;&#23545;&#35282;&#22270;&#24182;&#36827;&#34892;&#32858;&#31867;&#25490;&#24207;&#65292;&#26131;&#20110;&#30830;&#23450;&#32858;&#31867;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2404.01341</link><description>&lt;p&gt;
&#22359;&#23545;&#35282;&#24341;&#23548;&#30340;DBSCAN&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Block-Diagonal Guided DBSCAN Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01341
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#26412;&#30340;DBSCAN&#65292;&#21033;&#29992;&#30456;&#20284;&#24615;&#22270;&#30340;&#22359;&#23545;&#35282;&#23646;&#24615;&#24341;&#23548;&#32858;&#31867;&#36807;&#31243;&#65292;&#36890;&#36807;&#26500;&#24314;&#22359;&#23545;&#35282;&#22270;&#24182;&#36827;&#34892;&#32858;&#31867;&#25490;&#24207;&#65292;&#26131;&#20110;&#30830;&#23450;&#32858;&#31867;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#32676;&#20998;&#26512;&#22312;&#25968;&#25454;&#24211;&#25366;&#25496;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#32780;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#20043;&#19968;&#26159;DBSCAN&#12290;&#28982;&#32780;&#65292;DBSCAN&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#38590;&#20197;&#22788;&#29702;&#39640;&#32500;&#22823;&#35268;&#27169;&#25968;&#25454;&#12289;&#23545;&#36755;&#20837;&#21442;&#25968;&#25935;&#24863;&#20197;&#21450;&#22312;&#20135;&#29983;&#32858;&#31867;&#32467;&#26524;&#26102;&#32570;&#20047;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#26412;&#30340;DBSCAN&#65292;&#21033;&#29992;&#20102;&#30456;&#20284;&#24615;&#22270;&#30340;&#22359;&#23545;&#35282;&#23646;&#24615;&#26469;&#24341;&#23548;DBSCAN&#30340;&#32858;&#31867;&#36807;&#31243;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#26500;&#24314;&#19968;&#20010;&#22270;&#65292;&#34913;&#37327;&#39640;&#32500;&#22823;&#35268;&#27169;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#26377;&#21487;&#33021;&#36890;&#36807;&#26410;&#30693;&#32622;&#25442;&#36716;&#25442;&#20026;&#22359;&#23545;&#35282;&#24418;&#24335;&#65292;&#38543;&#21518;&#36890;&#36807;&#19968;&#20010;&#32858;&#31867;&#25490;&#24207;&#36807;&#31243;&#26469;&#29983;&#25104;&#26399;&#26395;&#30340;&#32622;&#25442;&#12290;&#32858;&#31867;&#32467;&#26500;&#21487;&#20197;&#36890;&#36807;&#35782;&#21035;&#32622;&#25442;&#21518;&#22270;&#20013;&#30340;&#23545;&#35282;&#22359;&#26469;&#36731;&#26494;&#30830;&#23450;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01341v1 Announce Type: cross  Abstract: Cluster analysis plays a crucial role in database mining, and one of the most widely used algorithms in this field is DBSCAN. However, DBSCAN has several limitations, such as difficulty in handling high-dimensional large-scale data, sensitivity to input parameters, and lack of robustness in producing clustering results. This paper introduces an improved version of DBSCAN that leverages the block-diagonal property of the similarity graph to guide the clustering procedure of DBSCAN. The key idea is to construct a graph that measures the similarity between high-dimensional large-scale data points and has the potential to be transformed into a block-diagonal form through an unknown permutation, followed by a cluster-ordering procedure to generate the desired permutation. The clustering structure can be easily determined by identifying the diagonal blocks in the permuted graph. We propose a gradient descent-based method to solve the propose
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#24085;&#32047;&#25176;&#38598;&#23398;&#20064;(CoPSL)&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#30340;&#24085;&#32047;&#25176;&#38598;&#65292;&#36890;&#36807;&#20849;&#20139;&#21644;&#29305;&#23450;&#23618;&#30340;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;MOP&#20043;&#38388;&#30340;&#21327;&#21516;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2404.01224</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#21327;&#21516;&#24085;&#32047;&#25176;&#38598;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Collaborative Pareto Set Learning in Multiple Multi-Objective Optimization Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01224
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#24085;&#32047;&#25176;&#38598;&#23398;&#20064;(CoPSL)&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#30340;&#24085;&#32047;&#25176;&#38598;&#65292;&#36890;&#36807;&#20849;&#20139;&#21644;&#29305;&#23450;&#23618;&#30340;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;MOP&#20043;&#38388;&#30340;&#21327;&#21516;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#32047;&#25176;&#38598;&#23398;&#20064;(PSL)&#26159;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#19968;&#20010;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20174;&#20559;&#22909;&#21521;&#37327;&#21040;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#30340;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PSL&#26041;&#27861;&#20165;&#38480;&#20110;&#19968;&#27425;&#35299;&#20915;&#21333;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;(MOP)&#12290;&#38754;&#23545;&#22810;&#20010;MOP&#26102;&#65292;&#36825;&#31181;&#38480;&#21046;&#19981;&#20165;&#23548;&#33268;&#26174;&#33879;&#30340;&#20302;&#25928;&#65292;&#32780;&#19988;&#26410;&#33021;&#21033;&#29992;&#27178;&#36328;&#19981;&#21516;MOP&#30340;&#28508;&#22312;&#21327;&#21516;&#25928;&#24212;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#24085;&#32047;&#25176;&#38598;&#23398;&#20064;(CoPSL)&#26694;&#26550;&#65292;&#23427;&#20197;&#21327;&#21516;&#26041;&#24335;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;MOP&#30340;&#24085;&#32047;&#25176;&#38598;&#12290;CoPSL&#37319;&#29992;&#20102;&#19968;&#20010;&#26550;&#26500;&#65292;&#21253;&#25324;&#20849;&#20139;&#21644;MOP&#29305;&#23450;&#23618;&#65292;&#20854;&#20013;&#20849;&#20139;&#23618;&#26088;&#22312;&#21327;&#21516;&#25429;&#25417;MOP&#20043;&#38388;&#30340;&#20844;&#20849;&#20851;&#31995;&#65292;&#32780;MOP&#29305;&#23450;&#23618;&#22788;&#29702;&#36825;&#20123;&#20851;&#31995;&#20197;&#29983;&#25104;&#27599;&#20010;MOP&#30340;&#35299;&#38598;&#12290;&#36825;&#31181;&#21327;&#21516;&#26041;&#27861;&#20351;&#24471;CoPSL&#33021;&#22815;&#39640;&#25928;&#22320;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01224v1 Announce Type: new  Abstract: Pareto Set Learning (PSL) is an emerging research area in multi-objective optimization, focusing on training neural networks to learn the mapping from preference vectors to Pareto optimal solutions. However, existing PSL methods are limited to addressing a single Multi-objective Optimization Problem (MOP) at a time. When faced with multiple MOPs, this limitation not only leads to significant inefficiencies but also fails to exploit the potential synergies across varying MOPs. In this paper, we propose a Collaborative Pareto Set Learning (CoPSL) framework, which simultaneously learns the Pareto sets of multiple MOPs in a collaborative manner. CoPSL employs an architecture consisting of shared and MOP-specific layers, where shared layers aim to capture common relationships among MOPs collaboratively, and MOP-specific layers process these relationships to generate solution sets for each MOP. This collaborative approach enables CoPSL to effi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#20445;&#25345;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265;&#20026;&#37325;&#28857;&#65292;&#22312;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23454;&#29616;&#27604;&#36138;&#23146;&#35757;&#32451;&#26356;&#24378;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;</title><link>https://arxiv.org/abs/2403.19871</link><description>&lt;p&gt;
&#36890;&#36807;&#32531;&#24930;&#21464;&#21270;&#30340;&#24207;&#21015;&#23454;&#29616;&#31283;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19871
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#20445;&#25345;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265;&#20026;&#37325;&#28857;&#65292;&#22312;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23454;&#29616;&#27604;&#36138;&#23146;&#35757;&#32451;&#26356;&#24378;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#36138;&#23146;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#32771;&#34385;&#36890;&#36807;&#19981;&#21516;&#30340;&#37325;&#26032;&#35757;&#32451;&#28436;&#21464;&#26469;&#20445;&#25345;&#35757;&#32451;&#27169;&#22411;&#32467;&#26500;&#30340;&#31283;&#23450;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20840;&#38754;&#32771;&#34385;&#20102;&#36890;&#36807;&#19981;&#21516;&#30340;&#25968;&#25454;&#25209;&#27425;&#26356;&#26032;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#20445;&#30041;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265; - &#36825;&#23545;&#20110;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#23454;&#26045;&#31616;&#26131;&#24615;&#21644;&#19982;&#29992;&#25143;&#24314;&#31435;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201; - &#36890;&#36807;&#20351;&#29992;&#21487;&#20197;&#30452;&#25509;&#32435;&#20837;&#20248;&#21270;&#38382;&#39064;&#30340;&#33258;&#23450;&#20041;&#23450;&#20041;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#30340;&#29983;&#20135;&#26696;&#20363;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#27604;&#36138;&#23146;&#35757;&#32451;&#27169;&#22411;&#26356;&#24378;&#30340;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19871v1 Announce Type: cross  Abstract: Retraining machine learning models remains an important task for real-world machine learning model deployment. Existing methods focus largely on greedy approaches to find the best-performing model without considering the stability of trained model structures across different retraining evolutions. In this study, we develop a mixed integer optimization algorithm that holistically considers the problem of retraining machine learning models across different data batch updates. Our method focuses on retaining consistent analytical insights - which is important to model interpretability, ease of implementation, and fostering trust with users - by using custom-defined distance metrics that can be directly incorporated into the optimization problem. Importantly, our method shows stronger stability than greedily trained models with a small, controllable sacrifice in model performance in a real-world production case study. Finally, important an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24471;&#20986;&#20102;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#35774;&#32622;&#20013;&#30340;&#20108;&#36827;&#21046;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#21453;&#38169;&#35823;&#27010;&#29575;&#30340;&#23545;&#25968;&#21644;&#20445;&#30495;&#24230;&#30340;&#36127;&#23545;&#25968;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.17868</link><description>&lt;p&gt;
&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Sample complexity of quantum hypothesis testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24471;&#20986;&#20102;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#35774;&#32622;&#20013;&#30340;&#20108;&#36827;&#21046;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#21453;&#38169;&#35823;&#27010;&#29575;&#30340;&#23545;&#25968;&#21644;&#20445;&#30495;&#24230;&#30340;&#36127;&#23545;&#25968;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#20154;&#20204;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#30740;&#31350;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20154;&#20204;&#23545;&#38169;&#35823;&#27010;&#29575;&#30340;&#26368;&#20248;&#34928;&#20943;&#36895;&#29575;&#24863;&#20852;&#36259;&#65292;&#36825;&#20010;&#36895;&#29575;&#26159;&#26410;&#30693;&#29366;&#24577;&#30340;&#26679;&#26412;&#25968;&#37327;&#20989;&#25968;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#26088;&#22312;&#30830;&#23450;&#36798;&#21040;&#25152;&#38656;&#38169;&#35823;&#27010;&#29575;&#25152;&#38656;&#30340;&#26368;&#23569;&#26679;&#26412;&#25968;&#37327;&#12290;&#36890;&#36807;&#21033;&#29992;&#24050;&#26377;&#25991;&#29486;&#20013;&#20851;&#20110;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#20016;&#23500;&#30693;&#35782;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#35774;&#32622;&#20013;&#30340;&#20108;&#36827;&#21046;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#20010;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#30028;&#38480;&#12290;&#26356;&#35814;&#32454;&#22320;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#31216;&#20108;&#36827;&#21046;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#23545;&#21453;&#38169;&#35823;&#27010;&#29575;&#30340;&#23545;&#25968;&#21644;&#20445;&#30495;&#24230;&#30340;&#36127;&#23545;&#25968;&#30340;&#23545;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17868v1 Announce Type: cross  Abstract: Quantum hypothesis testing has been traditionally studied from the information-theoretic perspective, wherein one is interested in the optimal decay rate of error probabilities as a function of the number of samples of an unknown state. In this paper, we study the sample complexity of quantum hypothesis testing, wherein the goal is to determine the minimum number of samples needed to reach a desired error probability. By making use of the wealth of knowledge that already exists in the literature on quantum hypothesis testing, we characterize the sample complexity of binary quantum hypothesis testing in the symmetric and asymmetric settings, and we provide bounds on the sample complexity of multiple quantum hypothesis testing. In more detail, we prove that the sample complexity of symmetric binary quantum hypothesis testing depends logarithmically on the inverse error probability and inversely on the negative logarithm of the fidelity. 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;LLMs&#26469;&#25351;&#23548;&#22810;&#27493;&#28436;&#31034;&#20013;&#38544;&#21547;&#30340;&#20219;&#21153;&#32467;&#26500;&#21644;&#32422;&#26463;&#30340;&#25628;&#32034;&#65292;&#20197;&#21450;&#36890;&#36807;&#21453;&#20107;&#23454;&#24178;&#25200;&#33719;&#24471;&#26356;&#24191;&#27867;&#30340;&#28436;&#31034;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#12290;</title><link>https://arxiv.org/abs/2403.17124</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#35745;&#21010;&#22522;&#20110;&#28436;&#31034;&#36890;&#36807;&#21453;&#20107;&#23454;&#24178;&#25200;&#36827;&#34892;&#33853;&#23454;
&lt;/p&gt;
&lt;p&gt;
Grounding Language Plans in Demonstrations Through Counterfactual Perturbations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17124
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;LLMs&#26469;&#25351;&#23548;&#22810;&#27493;&#28436;&#31034;&#20013;&#38544;&#21547;&#30340;&#20219;&#21153;&#32467;&#26500;&#21644;&#32422;&#26463;&#30340;&#25628;&#32034;&#65292;&#20197;&#21450;&#36890;&#36807;&#21453;&#20107;&#23454;&#24178;&#25200;&#33719;&#24471;&#26356;&#24191;&#27867;&#30340;&#28436;&#31034;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24120;&#35782;&#25512;&#29702;&#22522;&#20110;&#29289;&#29702;&#39046;&#22495;&#33853;&#23454;&#22312;&#20307;&#29616;&#26234;&#33021;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#30456;&#36739;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#19987;&#27880;&#20110;&#30452;&#25509;&#21033;&#29992;LLMs&#22312;&#31526;&#21495;&#31354;&#38388;&#20869;&#35268;&#21010;&#65292;&#36825;&#39033;&#24037;&#20316;&#20351;&#29992;LLMs&#25351;&#23548;&#20219;&#21153;&#32467;&#26500;&#30340;&#25628;&#32034;&#65292;&#38544;&#21547;&#22312;&#22810;&#27493;&#28436;&#31034;&#20013;&#30340;&#32422;&#26463;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#25805;&#32437;&#35268;&#21010;&#25991;&#29486;&#20013;&#30340;&#27169;&#24335;&#26063;&#30340;&#27010;&#24565;&#65292;&#23427;&#25353;&#29031;&#29305;&#23450;&#36816;&#21160;&#32422;&#26463;&#23558;&#26426;&#22120;&#20154;&#37197;&#32622;&#20998;&#32452;&#65292;&#20316;&#20026;LLM&#39640;&#32423;&#35821;&#35328;&#34920;&#31034;&#21644;&#26426;&#22120;&#20154;&#20302;&#32423;&#29289;&#29702;&#36712;&#36857;&#20043;&#38388;&#30340;&#25277;&#35937;&#23618;&#12290;&#36890;&#36807;&#29992;&#21512;&#25104;&#24178;&#25200;&#37325;&#26032;&#25773;&#25918;&#23569;&#37327;&#20154;&#31867;&#28436;&#31034;&#65292;&#25105;&#20204;&#21487;&#20197;&#35206;&#30422;&#28436;&#31034;&#30340;&#29366;&#24577;&#31354;&#38388;&#65292;&#24182;&#39069;&#22806;&#29983;&#25104;&#25104;&#21151;&#25191;&#34892;&#20197;&#21450;&#26410;&#23436;&#25104;&#20219;&#21153;&#30340;&#21453;&#20107;&#23454;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;&#35299;&#37322;&#30340;&#23398;&#20064;&#26694;&#26550;&#35757;&#32451;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17124v1 Announce Type: cross  Abstract: Grounding the common-sense reasoning of Large Language Models in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural networ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#22686;&#24335;MaxSAT&#30340;&#23398;&#20064;&#24179;&#34913;&#35268;&#21017;&#27169;&#22411;IMLIB&#65292;&#32467;&#21512;&#20102;SAT&#21644;MaxSAT&#26041;&#27861;&#65292;&#38480;&#21046;&#35268;&#21017;&#22823;&#23567;&#20197;&#23454;&#29616;&#24179;&#34913;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16418</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#36882;&#22686;&#24335;MaxSAT&#30340;&#23398;&#20064;&#24179;&#34913;&#35268;&#21017;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An incremental MaxSAT-based model to learn balanced rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16418
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#22686;&#24335;MaxSAT&#30340;&#23398;&#20064;&#24179;&#34913;&#35268;&#21017;&#27169;&#22411;IMLIB&#65292;&#32467;&#21512;&#20102;SAT&#21644;MaxSAT&#26041;&#27861;&#65292;&#38480;&#21046;&#35268;&#21017;&#22823;&#23567;&#20197;&#23454;&#29616;&#24179;&#34913;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#23548;&#33268;&#20102;&#20247;&#22810;&#24212;&#29992;&#31243;&#24207;&#30340;&#24320;&#21457;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#24182;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20165;&#20934;&#30830;&#24615;&#21487;&#33021;&#19981;&#36275;&#22815;&#12290;&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#36824;&#38656;&#35201;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;MaxSAT&#30340;&#22686;&#37327;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#21487;&#35299;&#37322;&#19988;&#24179;&#34913;&#30340;&#35268;&#21017;&#65292;&#31216;&#20026;IMLIB&#12290;&#36825;&#20010;&#26032;&#27169;&#22411;&#22522;&#20110;&#21478;&#22806;&#20004;&#31181;&#26041;&#27861;&#65292;&#19968;&#31181;&#26159;&#22522;&#20110;SAT&#30340;&#65292;&#21478;&#19968;&#31181;&#26159;&#22522;&#20110;MaxSAT&#30340;&#12290;&#22522;&#20110;SAT&#30340;&#26041;&#27861;&#38480;&#21046;&#20102;&#27599;&#20010;&#29983;&#25104;&#35268;&#21017;&#30340;&#22823;&#23567;&#65292;&#20351;&#24471;&#21487;&#20197;&#24179;&#34913;&#23427;&#20204;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26679;&#19968;&#32452;&#35268;&#21017;&#27604;&#19968;&#20010;&#28151;&#21512;&#20102;&#22823;&#35268;&#21017;&#21644;&#23567;&#35268;&#21017;&#26356;&#23481;&#26131;&#29702;&#35299;&#12290;&#22522;&#20110;MaxSAT&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;IMLI&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#24615;&#33021;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16418v1 Announce Type: cross  Abstract: The increasing advancements in the field of machine learning have led to the development of numerous applications that effectively address a wide range of problems with accurate predictions. However, in certain cases, accuracy alone may not be sufficient. Many real-world problems also demand explanations and interpretability behind the predictions. One of the most popular interpretable models that are classification rules. This work aims to propose an incremental model for learning interpretable and balanced rules based on MaxSAT, called IMLIB. This new model was based on two other approaches, one based on SAT and the other on MaxSAT. The one based on SAT limits the size of each generated rule, making it possible to balance them. We suggest that such a set of rules seem more natural to be understood compared to a mixture of large and small rules. The approach based on MaxSAT, called IMLI, presents a technique to increase performance th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Private Mixing of Ensemble Distributions (PMixED)&#65306;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#36755;&#20986;&#20998;&#24067;&#25237;&#24433;&#21040;&#20844;&#20849;LLM&#30340;&#36755;&#20986;&#20998;&#24067;&#21608;&#22260;&#30340;&#38598;&#21512;&#19978;&#65292;&#24182;&#37319;&#26679;&#24179;&#22343;&#26469;&#23454;&#29616;&#23454;&#38469;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#20197;&#26356;&#36731;&#37327;&#21270;&#30340;&#26041;&#24335;&#23454;&#29616;&#23545;&#38544;&#31169;&#25935;&#24863;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.15638</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24046;&#20998;&#31169;&#26377;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Next-Token Prediction of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15638
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Private Mixing of Ensemble Distributions (PMixED)&#65306;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#36755;&#20986;&#20998;&#24067;&#25237;&#24433;&#21040;&#20844;&#20849;LLM&#30340;&#36755;&#20986;&#20998;&#24067;&#21608;&#22260;&#30340;&#38598;&#21512;&#19978;&#65292;&#24182;&#37319;&#26679;&#24179;&#22343;&#26469;&#23454;&#29616;&#23454;&#38469;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#20197;&#26356;&#36731;&#37327;&#21270;&#30340;&#26041;&#24335;&#23454;&#29616;&#23545;&#38544;&#31169;&#25935;&#24863;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38544;&#31169;&#26085;&#30410;&#37325;&#35201;&#12290;DP-SGD&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26368;&#24191;&#27867;&#37319;&#29992;&#30340;&#25216;&#26415;&#65292;&#23427;&#20197;&#19968;&#31181;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;DP-SGD&#38656;&#35201;&#27604;SGD&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#26356;&#22823;&#30340;&#20869;&#23384;&#38656;&#27714;&#65292;&#21516;&#26102;&#36807;&#39640;&#20272;&#35745;&#23545;&#25163;&#20855;&#26377;&#30333;&#30418;&#35775;&#38382;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#26356;&#29616;&#23454;&#30340;&#22330;&#26223;&#20551;&#35774;&#21482;&#26377;&#23545;&#38544;&#31169;&#25935;&#24863;&#30340;LLM&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#12290;&#22312;&#36825;&#20123;&#35266;&#23519;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31169;&#26377;&#28151;&#21512;&#38598;&#21512;&#20998;&#24067;&#65288;PMixED&#65289;&#65306;&#19968;&#31181;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#27599;&#20010;&#36755;&#20986;&#20998;&#24067;&#20174;&#19968;&#20010;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;LLM&#38598;&#21512;&#25237;&#24433;&#21040;&#20844;&#20849;LLM&#36755;&#20986;&#20998;&#24067;&#21608;&#22260;&#30340;&#38598;&#21512;&#19978;&#65292;&#28982;&#21518;&#23545;&#25237;&#24433;&#20998;&#24067;&#36827;&#34892;&#24179;&#22343;&#24182;&#20174;&#20013;&#25277;&#26679;&#26469;&#23454;&#29616;&#23454;&#38469;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#31169;&#26377;&#39044;&#27979;&#21327;&#35758;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;DP-SGD&#26356;&#36731;&#37327;&#21270;&#65292;&#22240;&#20026;&#23427;&#19982;&#27169;&#22411;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15638v1 Announce Type: cross  Abstract: Ensuring the privacy of Large Language Models (LLMs) is becoming increasingly important. The most widely adopted technique to accomplish this is DP-SGD, which trains a model in such a way that guarantees Differential Privacy (DP). However, DP-SGD requires longer training times and larger memory requirements than SGD, while overestimating an adversary's capabilities in having white box access to the model. A more realistic scenario assumes only black-box access to a privacy-sensitive LLM. Motivated by these observations, we present Private Mixing of Ensemble Distributions (PMixED): a private prediction protocol that achieves practical next-token prediction by projecting each of the model's output distribution from an ensemble of fine-tuned LLMs onto a set around a public LLM's output distribution, then averaging the projected distributions and sampling from it. Our approach is more lightweight than DP-SGD in that it is model agnostic, i
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26159;&#36890;&#36807;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#20197;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#20943;&#23569;&#24341;&#20837;&#30340;&#38468;&#21152;&#21442;&#25968;&#25110;&#35745;&#31639;&#36164;&#28304;&#25968;&#37327;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.14608</link><description>&lt;p&gt;
&#22823;&#22411;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14608
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26159;&#36890;&#36807;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#20197;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#20943;&#23569;&#24341;&#20837;&#30340;&#38468;&#21152;&#21442;&#25968;&#25110;&#35745;&#31639;&#36164;&#28304;&#25968;&#37327;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#27169;&#22411;&#22312;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#20195;&#34920;&#20102;&#19968;&#39033;&#31361;&#30772;&#24615;&#30340;&#36827;&#23637;&#65292;&#20351;&#24471;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#31354;&#21069;&#30340;&#35268;&#27169;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#30001;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#32452;&#25104;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#26469;&#25191;&#34892;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#20026;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#23450;&#21046;&#22823;&#22411;&#27169;&#22411;&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#21463;&#21040;&#35745;&#31639;&#33021;&#21147;&#38480;&#21046;&#30340;&#30828;&#20214;&#24179;&#21488;&#19978;&#65292;&#35268;&#27169;&#24222;&#22823;&#21644;&#35745;&#31639;&#35201;&#27714;&#24040;&#22823;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35843;&#25972;&#22823;&#22411;&#27169;&#22411;&#20197;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PEFT&#26159;&#25351;&#35843;&#25972;&#39044;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#20351;&#20854;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#30340;&#36807;&#31243;&#65292;&#21516;&#26102;&#23613;&#37327;&#20943;&#23569;&#24341;&#20837;&#30340;&#38468;&#21152;&#21442;&#25968;&#25110;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14608v1 Announce Type: new  Abstract: Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities. Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adapt the large models over the various downstream tasks. In particular, PEFT refers to the process of adjusting the parameters of a pre-trained large models to adapt it to a specific task while minimizing the number of additional parameters introduced or computational resources required. This approac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#21152;&#36895;&#35270;&#39057;&#20998;&#26512;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36731;&#37327;&#32423;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#36827;&#34892;&#37096;&#32626;&#25512;&#29702;&#65292;&#21033;&#29992;&#26356;&#22823;&#30340;&#8220;&#25945;&#24072;&#8221;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#26631;&#35760;&#65292;&#23454;&#29616;&#23545;&#19981;&#26029;&#21464;&#21270;&#22330;&#26223;&#30340;&#25345;&#32493;&#33258;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.14353</link><description>&lt;p&gt;
DaCapo&#65306;&#21152;&#24555;&#33258;&#20027;&#31995;&#32479;&#22312;&#35270;&#39057;&#20998;&#26512;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DaCapo: Accelerating Continuous Learning in Autonomous Systems for Video Analytics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14353
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#21152;&#36895;&#35270;&#39057;&#20998;&#26512;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36731;&#37327;&#32423;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#36827;&#34892;&#37096;&#32626;&#25512;&#29702;&#65292;&#21033;&#29992;&#26356;&#22823;&#30340;&#8220;&#25945;&#24072;&#8221;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#26631;&#35760;&#65292;&#23454;&#29616;&#23545;&#19981;&#26029;&#21464;&#21270;&#22330;&#26223;&#30340;&#25345;&#32493;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#35270;&#39057;&#20998;&#26512;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12289;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#21644;&#23433;&#38450;&#26426;&#22120;&#20154;&#31561;&#33258;&#20027;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#30005;&#27744;&#21151;&#29575;&#65292;&#23454;&#38469;&#37096;&#32626;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25345;&#32493;&#23398;&#20064;&#21033;&#29992;&#22312;&#37096;&#32626;&#65288;&#25512;&#29702;&#65289;&#20013;&#30340;&#36731;&#37327;&#32423;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#65292;&#21033;&#29992;&#26356;&#22823;&#30340;&#8220;&#25945;&#24072;&#8221;&#27169;&#22411;&#23545;&#37319;&#26679;&#25968;&#25454;&#36827;&#34892;&#26631;&#35760;&#65288;&#26631;&#35760;&#65289;&#65292;&#24182;&#19981;&#26029;&#37325;&#26032;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#20197;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14353v1 Announce Type: cross  Abstract: Deep neural network (DNN) video analytics is crucial for autonomous systems such as self-driving vehicles, unmanned aerial vehicles (UAVs), and security robots. However, real-world deployment faces challenges due to their limited computational resources and battery power. To tackle these challenges, continuous learning exploits a lightweight "student" model at deployment (inference), leverages a larger "teacher" model for labeling sampled data (labeling), and continuously retrains the student model to adapt to changing scenarios (retraining). This paper highlights the limitations in state-of-the-art continuous learning systems: (1) they focus on computations for retraining, while overlooking the compute needs for inference and labeling, (2) they rely on power-hungry GPUs, unsuitable for battery-operated autonomous systems, and (3) they are located on a remote centralized server, intended for multi-tenant scenarios, again unsuitable for
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32593;&#32476;&#21442;&#25968;&#38468;&#21152;&#20998;&#35299;&#26469;&#35299;&#32806;&#24178;&#20928;&#25968;&#25454;&#21644;&#38169;&#35823;&#26631;&#35760;&#25968;&#25454;&#30340;&#35760;&#24518;&#65292;&#20174;&#32780;&#20943;&#23569;&#22024;&#26434;&#26631;&#31614;&#23545;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#30340;&#21103;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.13241</link><description>&lt;p&gt;
&#20351;&#29992;&#32593;&#32476;&#21442;&#25968;&#38468;&#21152;&#20998;&#35299;&#35299;&#20915;&#26377;&#22024;&#26434;&#26631;&#31614;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling Noisy Labels with Network Parameter Additive Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32593;&#32476;&#21442;&#25968;&#38468;&#21152;&#20998;&#35299;&#26469;&#35299;&#32806;&#24178;&#20928;&#25968;&#25454;&#21644;&#38169;&#35823;&#26631;&#35760;&#25968;&#25454;&#30340;&#35760;&#24518;&#65292;&#20174;&#32780;&#20943;&#23569;&#22024;&#26434;&#26631;&#31614;&#23545;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#30340;&#21103;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;&#20855;&#26377;&#22024;&#26434;&#26631;&#31614;&#30340;&#25968;&#25454;&#65292;&#36807;&#21442;&#24615;&#28145;&#24230;&#32593;&#32476;&#20250;&#22240;&#20026;&#36807;&#24230;&#25311;&#21512;&#38169;&#35823;&#26631;&#35760;&#30340;&#25968;&#25454;&#32780;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#12290;&#28145;&#24230;&#32593;&#32476;&#30340;&#35760;&#24518;&#25928;&#24212;&#34920;&#26126;&#65292;&#23613;&#31649;&#32593;&#32476;&#33021;&#22815;&#35760;&#24518;&#25152;&#26377;&#22024;&#26434;&#25968;&#25454;&#65292;&#20294;&#23427;&#20204;&#39318;&#20808;&#20250;&#35760;&#24518;&#24178;&#20928;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#28982;&#21518;&#36880;&#28176;&#35760;&#24518;&#38169;&#35823;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#19968;&#31181;&#21033;&#29992;&#35760;&#24518;&#25928;&#24212;&#26469;&#23545;&#25239;&#22024;&#26434;&#26631;&#31614;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#26159;&#26089;&#20572;&#27490;&#12290;&#28982;&#32780;&#65292;&#26089;&#20572;&#27490;&#26080;&#27861;&#21306;&#20998;&#23545;&#24178;&#20928;&#25968;&#25454;&#21644;&#38169;&#35823;&#26631;&#35760;&#25968;&#25454;&#30340;&#35760;&#24518;&#65292;&#23548;&#33268;&#32593;&#32476;&#20173;&#28982;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#19981;&#21487;&#36991;&#20813;&#22320;&#36807;&#24230;&#25311;&#21512;&#38169;&#35823;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#35299;&#32806;&#24178;&#20928;&#25968;&#25454;&#21644;&#38169;&#35823;&#26631;&#35760;&#25968;&#25454;&#30340;&#35760;&#24518;&#65292;&#24182;&#36827;&#19968;&#27493;&#20943;&#23569;&#38169;&#35823;&#26631;&#35760;&#25968;&#25454;&#30340;&#21103;&#20316;&#29992;&#65292;&#25105;&#20204;&#23545;&#32593;&#32476;&#21442;&#25968;&#36827;&#34892;&#20102;&#38468;&#21152;&#20998;&#35299;&#12290;&#21363;&#65292;&#23558;&#25152;&#26377;&#21442;&#25968;&#20998;&#35299;&#20026;&#20004;&#32452;&#65292;&#21363;&#21442;&#25968; $\mathbf{w}$ &#34987;&#20998;&#24320;&#22987;&#35299;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13241v1 Announce Type: new  Abstract: Given data with noisy labels, over-parameterized deep networks suffer overfitting mislabeled data, resulting in poor generalization. The memorization effect of deep networks shows that although the networks have the ability to memorize all noisy data, they would first memorize clean training data, and then gradually memorize mislabeled training data. A simple and effective method that exploits the memorization effect to combat noisy labels is early stopping. However, early stopping cannot distinguish the memorization of clean data and mislabeled data, resulting in the network still inevitably overfitting mislabeled data in the early training stage.In this paper, to decouple the memorization of clean data and mislabeled data, and further reduce the side effect of mislabeled data, we perform additive decomposition on network parameters. Namely, all parameters are additively decomposed into two groups, i.e., parameters $\mathbf{w}$ are deco
&lt;/p&gt;</description></item><item><title>Mamba&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20855;&#26377;&#25429;&#25417;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#12289;&#36817;&#32447;&#24615;&#22797;&#26434;&#24230;&#20197;&#21450;&#24615;&#33021;&#20248;&#21183;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11144</link><description>&lt;p&gt;
Mamba&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Mamba Effective for Time Series Forecasting?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11144
&lt;/p&gt;
&lt;p&gt;
Mamba&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20855;&#26377;&#25429;&#25417;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#12289;&#36817;&#32447;&#24615;&#22797;&#26434;&#24230;&#20197;&#21450;&#24615;&#33021;&#20248;&#21183;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;TSF&#65289;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;Transformer&#27169;&#22411;&#33021;&#22815;&#32858;&#28966;&#20840;&#23616;&#29615;&#22659;&#65292;&#26377;&#25928;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20013;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#36776;&#21035;&#22810;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#22240;&#27492;&#23427;&#19968;&#30452;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;Transformer&#27169;&#22411;&#30340;&#20302;&#25928;&#29575;&#21644;&#20851;&#20110;&#20854;&#25429;&#25417;&#20381;&#36182;&#20851;&#31995;&#33021;&#21147;&#30340;&#36136;&#30097;&#65292;&#23545;Transformer&#26550;&#26500;&#30340;&#19981;&#26029;&#23436;&#21892;&#24037;&#20316;&#20173;&#22312;&#36827;&#34892;&#20013;&#12290;&#26368;&#36817;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#22914;Mamba&#22240;&#20854;&#33021;&#22815;&#20687;Transformer&#19968;&#26679;&#25429;&#25417;&#24207;&#21015;&#20013;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#21448;&#20445;&#25345;&#36817;&#32447;&#24615;&#30340;&#22797;&#26434;&#24230;&#32780;&#22791;&#21463;&#25512;&#23815;&#12290;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;Mamba&#30340;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#24182;&#33410;&#32422;&#25104;&#26412;&#65292;&#23454;&#29616;&#21452;&#36194;&#23616;&#38754;&#12290;&#36825;&#24341;&#36215;&#20102;&#25105;&#20204;&#23545;&#25506;&#32034;SSM&#22312;TSF&#20219;&#21153;&#20013;&#28508;&#21147;&#30340;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#22522;&#20110;SSM&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;S-Mamba&#21644;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11144v1 Announce Type: new  Abstract: In the realm of time series forecasting (TSF), the Transformer has consistently demonstrated robust performance due to its ability to focus on the global context and effectively capture long-range dependencies within time, as well as discern correlations between multiple variables. However, due to the inefficiencies of the Transformer model and questions surrounding its ability to capture dependencies, ongoing efforts to refine the Transformer architecture persist. Recently, state space models (SSMs), e.g. Mamba, have gained traction due to their ability to capture complex dependencies in sequences, similar to the Transformer, while maintaining near-linear complexity. In text and image tasks, Mamba-based models can improve performance and cost savings, creating a win-win situation. This has piqued our interest in exploring SSM's potential in TSF tasks. In this paper, we introduce two straightforward SSM-based models for TSF, S-Mamba and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#26080;&#25237;&#24433;Frank-Wolfe&#31867;&#22411;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;&#24615;DR-&#27425;&#27169;&#20248;&#21270;&#65292;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#27425;&#32447;&#24615; $\alpha$-&#21518;&#24724;&#19978;&#30028;&#65292;&#24182;&#22312;&#21333;&#35843;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#26080;&#25237;&#24433;&#31639;&#27861;&#30340;&#26368;&#26032;&#27425;&#32447;&#24615; $\alpha$-&#21518;&#24724;&#19978;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.10063</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#26080;&#25237;&#24433;&#31639;&#27861;&#29992;&#20110;&#23545;&#25239;&#24615;DR-&#27425;&#27169;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unified Projection-Free Algorithms for Adversarial DR-Submodular Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#26080;&#25237;&#24433;Frank-Wolfe&#31867;&#22411;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;&#24615;DR-&#27425;&#27169;&#20248;&#21270;&#65292;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#27425;&#32447;&#24615; $\alpha$-&#21518;&#24724;&#19978;&#30028;&#65292;&#24182;&#22312;&#21333;&#35843;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#26080;&#25237;&#24433;&#31639;&#27861;&#30340;&#26368;&#26032;&#27425;&#32447;&#24615; $\alpha$-&#21518;&#24724;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#32479;&#19968;&#30340;&#26080;&#25237;&#24433;Frank-Wolfe&#31867;&#22411;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;&#24615;&#36830;&#32493;DR-&#27425;&#27169;&#20248;&#21270;&#65292;&#28085;&#30422;&#20102;&#35832;&#22914;&#20840;&#20449;&#24687;&#21644;&#65288;&#21322;&#65289;&#24378;&#25932;&#21453;&#39304;&#12289;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#20989;&#25968;&#12289;&#19981;&#21516;&#32422;&#26463;&#20197;&#21450;&#31867;&#22411;&#30340;&#38543;&#26426;&#26597;&#35810;&#31561;&#22330;&#26223;&#12290;&#22312;&#38750;&#21333;&#35843;&#35774;&#32622;&#20013;&#32771;&#34385;&#30340;&#27599;&#20010;&#38382;&#39064;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#35201;&#20040;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#35777;&#26126;&#30340;&#27425;&#32447;&#24615; $\alpha$-&#21518;&#24724;&#19978;&#30028;&#30340;&#31639;&#27861;&#65292;&#35201;&#20040;&#20855;&#26377;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340; $\alpha$-&#21518;&#24724;&#19978;&#30028;&#65292;&#20854;&#20013; $\alpha$ &#26159;&#31163;&#32447;&#35774;&#32622;&#20013;&#30340;&#30456;&#24212;&#36817;&#20284;&#19978;&#30028;&#12290;&#22312;&#21333;&#35843;&#35774;&#32622;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;8&#20010;&#32771;&#34385;&#30340;&#24773;&#20917;&#20013;&#30340;7&#31181;&#20013;&#26159;&#26080;&#25237;&#24433;&#31639;&#27861;&#30340;&#26368;&#26032;&#27425;&#32447;&#24615; $\alpha$-&#21518;&#24724;&#19978;&#30028;&#65292;&#21516;&#26102;&#19982;&#21097;&#20313;&#24773;&#20917;&#30340;&#32467;&#26524;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;DR-&#27425;&#27169;&#20248;&#21270;&#30340;&#21322;&#24378;&#25932;&#21644;&#24378;&#25932;&#21453;&#39304;&#65292;&#25512;&#36827;&#20102;&#23545;&#36825;&#19968;&#38382;&#39064;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10063v1 Announce Type: cross  Abstract: This paper introduces unified projection-free Frank-Wolfe type algorithms for adversarial continuous DR-submodular optimization, spanning scenarios such as full information and (semi-)bandit feedback, monotone and non-monotone functions, different constraints, and types of stochastic queries. For every problem considered in the non-monotone setting, the proposed algorithms are either the first with proven sub-linear $\alpha$-regret bounds or have better $\alpha$-regret bounds than the state of the art, where $\alpha$ is a corresponding approximation bound in the offline setting. In the monotone setting, the proposed approach gives state-of-the-art sub-linear $\alpha$-regret bounds among projection-free algorithms in 7 of the 8 considered cases while matching the result of the remaining case. Additionally, this paper addresses semi-bandit and bandit feedback for adversarial DR-submodular optimization, advancing the understanding of this
&lt;/p&gt;</description></item><item><title>CHAI&#25552;&#20986;&#20102;Clustered Head Attention&#65288;CHAI&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36816;&#34892;&#26102;&#32467;&#21512;&#20855;&#26377;&#39640;&#30456;&#20851;&#24615;&#30340;&#27880;&#24847;&#21147;&#22836;&#37096;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#21644;&#35745;&#31639;&#37327;&#65292;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;&#23384;&#20648;K,V&#32531;&#23384;&#30340;&#20869;&#23384;&#38656;&#27714;&#38477;&#20302;21.4&#65285;&#65292;&#25512;&#29702;&#26102;&#38388;&#24310;&#36831;&#38477;&#20302;1.73&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.08058</link><description>&lt;p&gt;
CHAI&#65306;&#39640;&#25928;LLM&#25512;&#29702;&#30340;&#32858;&#31867;&#22836;&#37096;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
CHAI: Clustered Head Attention for Efficient LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08058
&lt;/p&gt;
&lt;p&gt;
CHAI&#25552;&#20986;&#20102;Clustered Head Attention&#65288;CHAI&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36816;&#34892;&#26102;&#32467;&#21512;&#20855;&#26377;&#39640;&#30456;&#20851;&#24615;&#30340;&#27880;&#24847;&#21147;&#22836;&#37096;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#21644;&#35745;&#31639;&#37327;&#65292;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;&#23384;&#20648;K,V&#32531;&#23384;&#30340;&#20869;&#23384;&#38656;&#27714;&#38477;&#20302;21.4&#65285;&#65292;&#25512;&#29702;&#26102;&#38388;&#24310;&#36831;&#38477;&#20302;1.73&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25317;&#26377;&#25968;&#30334;&#20159;&#21442;&#25968;&#25913;&#21464;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#29702;&#26102;&#20026;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#26082;&#38656;&#35201;&#35745;&#31639;&#21448;&#38656;&#35201;&#20869;&#23384;&#65292;&#19968;&#20010;&#35831;&#27714;&#21487;&#33021;&#38656;&#35201;&#22810;&#20010;GPU&#21644;&#25968;&#21313;GB&#30340;&#20869;&#23384;&#12290;&#22810;&#22836;&#27880;&#24847;&#21147;&#26159;LLMs&#30340;&#20851;&#38190;&#32452;&#20214;&#20043;&#19968;&#65292;&#21487;&#20197;&#21344;LLMs&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;50%&#20197;&#19978;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#21508;&#22836;&#20043;&#38388;&#23545;&#27880;&#24847;&#21147;&#30340;&#20851;&#27880;&#26377;&#24456;&#39640;&#30340;&#20887;&#20313;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Clustered Head Attention (CHAI)&#12290;CHAI&#22312;&#36816;&#34892;&#26102;&#23558;&#20855;&#26377;&#39640;&#30456;&#20851;&#24615;&#30340;&#22836;&#37096;&#32467;&#21512;&#36827;&#34892;&#33258;&#27880;&#24847;&#21147;&#65292;&#20174;&#32780;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CHAI&#33021;&#22815;&#23558;&#23384;&#20648;K,V&#32531;&#23384;&#30340;&#20869;&#23384;&#38656;&#27714;&#38477;&#20302;&#22810;&#36798;21.4%&#65292;&#25512;&#29702;&#26102;&#24310;&#36831;&#38477;&#20302;&#22810;&#36798;1.73&#20493;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#24494;&#35843;&#12290;CHAI&#23454;&#29616;&#20102;&#26368;&#22810;3.2%&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08058v1 Announce Type: cross  Abstract: Large Language Models (LLMs) with hundreds of billions of parameters have transformed the field of machine learning. However, serving these models at inference time is both compute and memory intensive, where a single request can require multiple GPUs and tens of Gigabytes of memory. Multi-Head Attention is one of the key components of LLMs, which can account for over 50% of LLMs memory and compute requirement. We observe that there is a high amount of redundancy across heads on which tokens they pay attention to. Based on this insight, we propose Clustered Head Attention (CHAI). CHAI combines heads with a high amount of correlation for self-attention at runtime, thus reducing both memory and compute. In our experiments, we show that CHAI is able to reduce the memory requirements for storing K,V cache by up to 21.4% and inference time latency by up to 1.73x without any fine-tuning required. CHAI achieves this with a maximum 3.2% deviat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#24555;&#36895;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#65288;FNGD&#65289;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20165;&#38656;&#35201;&#22312;&#31532;&#19968;&#20010;&#26102;&#20195;&#35745;&#31639;&#36870;&#36816;&#31639;&#65292;&#36991;&#20813;&#20102;&#36845;&#20195;&#27714;&#36870;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.03473</link><description>&lt;p&gt;
&#26080;&#36870;&#30697;&#38453;&#24555;&#36895;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Inverse-Free Fast Natural Gradient Descent Method for Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#24555;&#36895;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#65288;FNGD&#65289;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20165;&#38656;&#35201;&#22312;&#31532;&#19968;&#20010;&#26102;&#20195;&#35745;&#31639;&#36870;&#36816;&#31639;&#65292;&#36991;&#20813;&#20102;&#36845;&#20195;&#27714;&#36870;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#38454;&#26041;&#27861;&#36890;&#36807;&#21253;&#21547;&#20108;&#38454;&#23548;&#25968;&#25110;&#32479;&#35745;&#37327;&#21487;&#20197;&#27604;&#19968;&#38454;&#26041;&#27861;&#25910;&#25947;&#24471;&#26356;&#24555;&#65292;&#20294;&#30001;&#20110;&#35745;&#31639;&#25928;&#29575;&#20302;&#65292;&#23427;&#20204;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#24456;&#23569;&#34987;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#35768;&#22810;&#35299;&#20915;&#26041;&#26696;&#37117;&#38598;&#20013;&#22312;&#20943;&#23567;&#38656;&#35201;&#27714;&#36870;&#30340;&#30697;&#38453;&#30340;&#22823;&#23567;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#38656;&#35201;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#25191;&#34892;&#27714;&#36870;&#25805;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#65288;FNGD&#65289;&#26041;&#27861;&#65292;&#21482;&#38656;&#22312;&#31532;&#19968;&#20010;&#26102;&#20195;&#35745;&#31639;&#36870;&#36816;&#31639;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#65288;NGD&#65289;&#30340;&#26799;&#24230;&#39044;&#22788;&#29702;&#20844;&#24335;&#37325;&#26500;&#20026;&#20351;&#29992;Sherman-Morrison-Woodbury&#20844;&#24335;&#30340;&#27599;&#20010;&#26679;&#26412;&#26799;&#24230;&#30340;&#21152;&#26435;&#21644;&#12290;&#22522;&#20110;&#27492;&#65292;&#20026;&#20102;&#36991;&#20813;&#28041;&#21450;&#35745;&#31639;&#31995;&#25968;&#30340;&#36845;&#20195;&#36870;&#25805;&#20316;&#65292;&#36825;&#20123;&#21152;&#26435;&#31995;&#25968;&#22312;&#25972;&#20010;&#26102;&#20195;&#20849;&#20139;&#32780;&#19981;&#24433;&#21709;&#32463;&#39564;&#24615;&#33021;&#12290;FNGD&#23558;NGD&#36817;&#20284;&#20026;f
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03473v1 Announce Type: new  Abstract: Second-order methods can converge much faster than first-order methods by incorporating second-order derivates or statistics, but they are far less prevalent in deep learning due to their computational inefficiency. To handle this, many of the existing solutions focus on reducing the size of the matrix to be inverted. However, it is still needed to perform the inverse operator in each iteration. In this paper, we present a fast natural gradient descent (FNGD) method, which only requires computing the inverse during the first epoch. Firstly, we reformulate the gradient preconditioning formula in the natural gradient descent (NGD) as a weighted sum of per-sample gradients using the Sherman-Morrison-Woodbury formula. Building upon this, to avoid the iterative inverse operation involved in computing coefficients, the weighted coefficients are shared across epochs without affecting the empirical performance.   FNGD approximates the NGD as a f
&lt;/p&gt;</description></item><item><title>WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.03218</link><description>&lt;p&gt;
WMDP&#22522;&#20934;&#65306;&#36890;&#36807;&#36951;&#24536;&#27979;&#37327;&#21644;&#20943;&#23569;&#24694;&#24847;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03218
&lt;/p&gt;
&lt;p&gt;
WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#30333;&#23467;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#25919;&#21629;&#20196;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#20104;&#24694;&#24847;&#34892;&#20026;&#32773;&#24320;&#21457;&#29983;&#29289;&#12289;&#32593;&#32476;&#21644;&#21270;&#23398;&#27494;&#22120;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#34913;&#37327;&#36825;&#20123;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#65292;&#25919;&#24220;&#26426;&#26500;&#21644;&#20027;&#35201;&#20154;&#24037;&#26234;&#33021;&#23454;&#39564;&#23460;&#27491;&#22312;&#24320;&#21457;LLMs&#30340;&#21361;&#38505;&#33021;&#21147;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#26159;&#31169;&#20154;&#30340;&#65292;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#22914;&#20309;&#20943;&#23569;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20165;&#19987;&#27880;&#20110;&#20960;&#26465;&#39640;&#24230;&#29305;&#23450;&#30340;&#24694;&#24847;&#20351;&#29992;&#36884;&#24452;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#22823;&#35268;&#27169;&#26432;&#20260;&#24615;&#27494;&#22120;&#20195;&#29702;&#65288;WMDP&#65289;&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;WMDP&#30001;&#19968;&#32452;&#23398;&#26415;&#30028;&#21644;&#25216;&#26415;&#39038;&#38382;&#32852;&#21512;&#24320;&#21457;&#65292;&#24182;&#22312;&#20844;&#24320;&#21457;&#24067;&#21069;&#20005;&#26684;&#36807;&#28388;&#20197;&#28040;&#38500;&#25935;&#24863;&#20449;&#24687;&#12290;WMDP&#26377;&#20004;&#20010;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 Announce Type: cross  Abstract: The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26469;&#27169;&#25311;&#22810;&#20154;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21160;&#24577;&#65292;&#24182;&#20026;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;&#35774;&#32622;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#27880;&#37322;&#65307;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#32447;&#26041;&#27861;&#65292;&#21033;&#29992;&#23494;&#38598;&#23545;&#40784;&#30340;&#35821;&#35328;-&#35270;&#35273;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.02090</link><description>&lt;p&gt;
&#24314;&#27169;&#22810;&#27169;&#24577;&#31038;&#20132;&#20114;&#21160;&#65306;&#20855;&#26377;&#23494;&#38598;&#23545;&#40784;&#34920;&#31034;&#30340;&#26032;&#25361;&#25112;&#21644;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02090
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26469;&#27169;&#25311;&#22810;&#20154;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21160;&#24577;&#65292;&#24182;&#20026;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;&#35774;&#32622;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#27880;&#37322;&#65307;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#32447;&#26041;&#27861;&#65292;&#21033;&#29992;&#23494;&#38598;&#23545;&#40784;&#30340;&#35821;&#35328;-&#35270;&#35273;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#28041;&#21450;&#35328;&#35821;&#21644;&#38750;&#35328;&#35821;&#32447;&#32034;&#30340;&#31038;&#20132;&#20114;&#21160;&#23545;&#26377;&#25928;&#35299;&#37322;&#31038;&#20132;&#24773;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#22810;&#27169;&#24577;&#31038;&#20132;&#32447;&#32034;&#30340;&#20808;&#21069;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20154;&#34892;&#20026;&#19978;&#65292;&#25110;&#20381;&#36182;&#20110;&#19982;&#22810;&#26041;&#29615;&#22659;&#20013;&#30340;&#35805;&#35821;&#23494;&#20999;&#23545;&#40784;&#30340;&#25972;&#20307;&#35270;&#35273;&#34920;&#31034;&#12290;&#23427;&#20204;&#22312;&#24314;&#27169;&#22810;&#26041;&#20114;&#21160;&#30340;&#22797;&#26434;&#21160;&#24577;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19977;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20197;&#24314;&#27169;&#22810;&#20154;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21160;&#24577;&#65306;&#35805;&#35821;&#30446;&#26631;&#35782;&#21035;&#12289;&#20195;&#35789;&#25351;&#20195;&#28040;&#35299;&#21644;&#25552;&#21450;&#29609;&#23478;&#39044;&#27979;&#12290;&#25105;&#20204;&#20026;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;&#35774;&#32622;&#20013;&#30340;&#36825;&#20123;&#26032;&#25361;&#25112;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#32447;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#29305;&#24449;&#19982;&#20854;&#23545;&#24212;&#30340;&#35805;&#35821;&#21516;&#27493;&#65292;&#21033;&#29992;&#23494;&#38598;&#23545;&#40784;&#30340;&#35821;&#35328;-&#35270;&#35273;&#34920;&#31034;&#65292;&#36825;&#26377;&#21161;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02090v1 Announce Type: cross  Abstract: Understanding social interactions involving both verbal and non-verbal cues is essential to effectively interpret social situations. However, most prior works on multimodal social cues focus predominantly on single-person behaviors or rely on holistic visual representations that are not densely aligned to utterances in multi-party environments. They are limited in modeling the intricate dynamics of multi-party interactions. In this paper, we introduce three new challenging tasks to model the fine-grained dynamics between multiple people: speaking target identification, pronoun coreference resolution, and mentioned player prediction. We contribute extensive data annotations to curate these new challenges in social deduction game settings. Furthermore, we propose a novel multimodal baseline that leverages densely aligned language-visual representations by synchronizing visual features with their corresponding utterances. This facilitates
&lt;/p&gt;</description></item><item><title>SynCode&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#32467;&#21512;&#32534;&#31243;&#35821;&#35328;&#30340;&#35821;&#27861;&#21644;DFA mask store&#65292;&#22312;LLMs&#20013;&#29983;&#25104;&#20195;&#30721;&#36807;&#31243;&#20013;&#33719;&#24471;96.07%&#30340;&#21477;&#27861;&#38169;&#35823;&#38477;&#20302;&#65292;&#24182;&#23637;&#29616;&#20986;&#25552;&#39640;&#21477;&#27861;&#31934;&#24230;&#30340;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.01632</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#27861;&#22686;&#24378;&#25913;&#36827;LLM&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving LLM Code Generation with Grammar Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01632
&lt;/p&gt;
&lt;p&gt;
SynCode&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#32467;&#21512;&#32534;&#31243;&#35821;&#35328;&#30340;&#35821;&#27861;&#21644;DFA mask store&#65292;&#22312;LLMs&#20013;&#29983;&#25104;&#20195;&#30721;&#36807;&#31243;&#20013;&#33719;&#24471;96.07%&#30340;&#21477;&#27861;&#38169;&#35823;&#38477;&#20302;&#65292;&#24182;&#23637;&#29616;&#20986;&#25552;&#39640;&#21477;&#27861;&#31934;&#24230;&#30340;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; SynCode&#65292;&#19968;&#20010;&#29992;&#20110;&#39640;&#25928;&#21644;&#36890;&#29992;&#22320;&#35299;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#30721;&#30340;&#26032;&#26694;&#26550;&#12290;SynCode&#21033;&#29992;&#32534;&#31243;&#35821;&#35328;&#30340;&#35821;&#27861;&#65292;&#21033;&#29992;&#31163;&#32447;&#26500;&#24314;&#30340;&#22522;&#20110;&#35821;&#35328;&#35821;&#27861;&#32456;&#32467;&#31526;&#30340;&#39640;&#25928;&#26597;&#25214;&#34920;DFA mask store&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;SynCode&#22312;&#32473;&#23450;&#32534;&#31243;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#30340;&#23436;&#22791;&#24615;&#21644;&#27491;&#30830;&#24615;&#65292;&#23637;&#31034;&#20854;&#22312;&#20445;&#30041;&#35821;&#20041;&#19978;&#26377;&#25928;&#20196;&#29260;&#30340;&#21516;&#26102;&#25298;&#32477;&#26080;&#25928;&#20196;&#29260;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#19982;&#30001;CFG&#23450;&#20041;&#30340;&#20219;&#20309;&#35821;&#35328;&#26080;&#32541;&#38598;&#25104;&#65292;&#39564;&#35777;&#20102;&#38024;&#23545;Python&#21644;Go&#30340;CFG&#23454;&#39564;&#12290;&#32467;&#26524;&#31361;&#20986;&#20102;&#24403;SynCode&#19982;&#26368;&#20808;&#36827;&#30340;LLMs&#32467;&#21512;&#26102;&#65292;&#35821;&#27861;&#38169;&#35823;&#20943;&#23569;96.07%&#65292;&#24432;&#26174;&#20102;&#20854;&#23545;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21477;&#27861;&#31934;&#24230;&#30340;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01632v1 Announce Type: new  Abstract: We present SynCode a novel framework for efficient and general syntactical decoding of code with large language models (LLMs). SynCode leverages the grammar of a programming language, utilizing an offline-constructed efficient lookup table called DFA mask store based on language grammar terminals. We demonstrate SynCode's soundness and completeness given the context-free grammar (CFG) of the programming language, presenting its ability to retain syntactically valid tokens while rejecting invalid ones. The framework seamlessly integrates with any language defined by CFG, as evidenced by experiments on CFGs for Python and Go. The results underscore the significant reduction of 96.07% of syntax errors achieved when SynCode is combined with state-of-the-art LLMs, showcasing its substantial impact on enhancing syntactical precision in code generation.   Our code is available at https://github.com/uiuc-focal-lab/syncode.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#31185;&#23398;&#21407;&#29702;&#21644;&#29289;&#29702;&#23450;&#24459;&#34701;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#24314;&#27169;&#38750;&#32447;&#24615;&#32467;&#26500;&#30340;&#22320;&#38663;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.17992</link><description>&lt;p&gt;
&#29289;&#29702;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#39044;&#27979;&#38750;&#32447;&#24615;&#38050;&#26694;&#26550;&#32467;&#26500;&#30340;&#22320;&#38663;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Machine Learning for Seismic Response Prediction OF Nonlinear Steel Moment Resisting Frame Structures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#31185;&#23398;&#21407;&#29702;&#21644;&#29289;&#29702;&#23450;&#24459;&#34701;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#24314;&#27169;&#38750;&#32447;&#24615;&#32467;&#26500;&#30340;&#22320;&#38663;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20256;&#32479;&#25968;&#20540;&#27169;&#25311;&#30340;&#22823;&#37327;&#35745;&#31639;&#25104;&#26412;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#36827;&#34892;&#32467;&#26500;&#20803;&#27169;&#22411;&#24314;&#27169;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;&#31574;&#30053;&#26174;&#31034;&#20986;&#27169;&#22411;&#31283;&#20581;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#20016;&#23500;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#28508;&#22312;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#21551;&#21457;&#26426;&#22120;&#23398;&#20064;&#65288;PiML&#65289;&#26041;&#27861;&#65292;&#23558;&#31185;&#23398;&#21407;&#29702;&#21644;&#29289;&#29702;&#23450;&#24459;&#34701;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#29992;&#20110;&#24314;&#27169;&#38750;&#32447;&#24615;&#32467;&#26500;&#30340;&#22320;&#38663;&#21709;&#24212;&#12290;&#22522;&#26412;&#27010;&#24565;&#26159;&#23558;ML&#27169;&#22411;&#30340;&#35299;&#31354;&#38388;&#32422;&#26463;&#22312;&#24050;&#30693;&#30340;&#29289;&#29702;&#33539;&#22260;&#20869;&#12290;&#36825;&#26159;&#36890;&#36807;&#19977;&#20010;&#20027;&#35201;&#29305;&#28857;&#23454;&#29616;&#30340;&#65292;&#21363;&#27169;&#22411;&#38477;&#38454;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#21644;&#29275;&#39039;&#31532;&#20108;&#23450;&#24459;&#65288;&#20363;&#22914;&#65292;&#36816;&#21160;&#26041;&#31243;&#65289;&#12290;&#27169;&#22411;&#38477;&#38454;&#23545;&#22788;&#29702;&#20855;&#26377;&#22266;&#26377;&#20887;&#20313;&#24615;&#21644;&#22686;&#24378;&#24615;&#30340;&#32467;&#26500;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17992v1 Announce Type: cross  Abstract: There is a growing interest in utilizing machine learning (ML) methods for structural metamodeling due to the substantial computational cost of traditional numerical simulations. The existing data-driven strategies show potential limitations to the model robustness and interpretability as well as the dependency of rich data. To address these challenges, this paper presents a novel physics-informed machine learning (PiML) method, which incorporates scientific principles and physical laws into deep neural networks for modeling seismic responses of nonlinear structures. The basic concept is to constrain the solution space of the ML model within known physical bounds. This is made possible with three main features, namely, model order reduction, a long short-term memory (LSTM) networks, and Newton's second law (e.g., the equation of motion). Model order reduction is essential for handling structural systems with inherent redundancy and enh
&lt;/p&gt;</description></item><item><title>InstructEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#20196;&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#65292;&#36890;&#36807;&#31616;&#21333;&#25351;&#20196;&#20351;&#32534;&#36753;&#22120;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#32534;&#36753;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16123</link><description>&lt;p&gt;
InstructEdit&#65306;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
InstructEdit: Instruction-based Knowledge Editing for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16123
&lt;/p&gt;
&lt;p&gt;
InstructEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#20196;&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#65292;&#36890;&#36807;&#31616;&#21333;&#25351;&#20196;&#20351;&#32534;&#36753;&#22120;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#32534;&#36753;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#21487;&#20197;&#25552;&#20379;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#32780;&#19981;&#20250;&#23545;&#25972;&#20307;&#24615;&#33021;&#20135;&#29983;&#28040;&#26497;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#36328;&#20219;&#21153;&#30340;&#36890;&#29992;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#20026;&#27599;&#20010;&#20219;&#21153;&#35774;&#35745;&#19968;&#20010;&#29420;&#29305;&#30340;&#32534;&#36753;&#22120;&#65292;&#36825;&#26174;&#33879;&#38459;&#30861;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#30693;&#35782;&#32534;&#36753;&#20013;&#30340;&#22810;&#20219;&#21153;&#27867;&#21270;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#20196;&#30340;&#32534;&#36753;&#25216;&#26415;&#65292;&#31216;&#20026;InstructEdit&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25351;&#20196;&#20419;&#36827;&#32534;&#36753;&#22120;&#21516;&#26102;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;LLM&#21482;&#20351;&#29992;&#19968;&#20010;&#32479;&#19968;&#30340;&#32534;&#36753;&#22120;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#26041;&#38754;&#34920;&#26126;&#65292;InstructEdit&#21487;&#20197;&#25552;&#39640;&#32534;&#36753;&#22120;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#22810;&#20219;&#21153;&#32534;&#36753;&#35774;&#32622;&#20013;&#24179;&#22343;&#25552;&#39640;&#21487;&#38752;&#24615;14.86%&#12290;&#27492;&#22806;&#65292;&#28041;&#21450;&#20445;&#30041;&#26410;&#35265;&#20219;&#21153;&#30340;&#23454;&#39564;&#35828;&#26126;&#65292;InstructEdi
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16123v1 Announce Type: cross  Abstract: Knowledge editing for large language models can offer an efficient solution to alter a model's behavior without negatively impacting the overall performance. However, the current approach encounters issues with limited generalizability across tasks, necessitating one distinct editor for each task, which significantly hinders the broader applications. To address this, we take the first step to analyze the multi-task generalization issue in knowledge editing. Specifically, we develop an instruction-based editing technique, termed InstructEdit, which facilitates the editor's adaptation to various task performances simultaneously using simple instructions. With only one unified editor for each LLM, we empirically demonstrate that InstructEdit can improve the editor's control, leading to an average 14.86% increase in Reliability in multi-task editing setting. Furthermore, experiments involving holdout unseen task illustrate that InstructEdi
&lt;/p&gt;</description></item><item><title>EasyRL4Rec&#26159;&#19968;&#20010;&#38754;&#21521;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#29992;&#25143;&#21451;&#22909;&#21644;&#39640;&#25928;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#30340;RL&#29615;&#22659;&#12289;&#20840;&#38754;&#30340;&#26680;&#24515;&#27169;&#22359;&#12289;&#19968;&#33268;&#30340;&#35780;&#20272;&#26631;&#20934;&#21644;&#23450;&#21046;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#24110;&#21161;&#31616;&#21270;&#27169;&#22411;&#24320;&#21457;&#24182;&#25913;&#21892;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.15164</link><description>&lt;p&gt;
EasyRL4Rec&#65306;&#38754;&#21521;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#29992;&#25143;&#21451;&#22909;&#20195;&#30721;&#24211;
&lt;/p&gt;
&lt;p&gt;
EasyRL4Rec: A User-Friendly Code Library for Reinforcement Learning Based Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15164
&lt;/p&gt;
&lt;p&gt;
EasyRL4Rec&#26159;&#19968;&#20010;&#38754;&#21521;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#29992;&#25143;&#21451;&#22909;&#21644;&#39640;&#25928;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#30340;RL&#29615;&#22659;&#12289;&#20840;&#38754;&#30340;&#26680;&#24515;&#27169;&#22359;&#12289;&#19968;&#33268;&#30340;&#35780;&#20272;&#26631;&#20934;&#21644;&#23450;&#21046;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#24110;&#21161;&#31616;&#21270;&#27169;&#22411;&#24320;&#21457;&#24182;&#25913;&#21892;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;-&#22522;&#30784;&#30340;&#25512;&#33616;&#31995;&#32479;&#65288;RSs&#65289;&#36234;&#26469;&#36234;&#34987;&#35748;&#21487;&#20854;&#25552;&#39640;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#39046;&#22495;&#38754;&#20020;&#25361;&#25112;&#65292;&#22914;&#32570;&#20047;&#26131;&#29992;&#30340;&#26694;&#26550;&#12289;&#35780;&#20272;&#26631;&#20934;&#19981;&#19968;&#33268;&#20197;&#21450;&#22797;&#21046;&#20197;&#21069;&#30340;&#24037;&#20316;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyRL4Rec&#65292;&#19968;&#20010;&#19987;&#20026;&#22522;&#20110;RL&#30340;RSs&#37327;&#36523;&#23450;&#21046;&#30340;&#29992;&#25143;&#21451;&#22909;&#21644;&#39640;&#25928;&#30340;&#24211;&#12290;EasyRL4Rec&#20855;&#26377;&#22522;&#20110;&#20116;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#36731;&#37327;&#32423;&#12289;&#22810;&#26679;&#21270;&#30340;RL&#29615;&#22659;&#65292;&#24182;&#37197;&#22791;&#20102;&#20840;&#38754;&#30340;&#26680;&#24515;&#27169;&#22359;&#65292;&#25552;&#20379;&#20016;&#23500;&#30340;&#36873;&#39033;&#26469;&#31616;&#21270;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#23427;&#24314;&#31435;&#20102;&#19968;&#33268;&#30340;&#35780;&#20272;&#26631;&#20934;&#65292;&#37325;&#28857;&#20851;&#27880;&#38271;&#26399;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#23450;&#21046;&#30340;&#29366;&#24577;&#24314;&#27169;&#21644;&#34892;&#20026;&#34920;&#31034;&#30340;&#23450;&#21046;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#20139;&#20102;&#36890;&#36807;&#19982;&#24403;&#21069;&#26041;&#27861;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#33719;&#24471;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;EasyRL4Rec&#26088;&#22312;&#20419;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15164v1 Announce Type: cross  Abstract: Reinforcement Learning (RL)-Based Recommender Systems (RSs) are increasingly recognized for their ability to improve long-term user engagement. Yet, the field grapples with challenges such as the absence of accessible frameworks, inconsistent evaluation standards, and the complexity of replicating prior work. Addressing these obstacles, we present EasyRL4Rec, a user-friendly and efficient library tailored for RL-based RSs. EasyRL4Rec features lightweight, diverse RL environments built on five widely-used public datasets, and is equipped with comprehensive core modules that offer rich options to ease the development of models. It establishes consistent evaluation criteria with a focus on long-term impacts and introduces customized solutions for state modeling and action representation tailored to recommender systems. Additionally, we share valuable insights gained from extensive experiments with current methods. EasyRL4Rec aims to facil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#20154;&#20215;&#20540;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#34920;&#36798;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#27169;&#25311;&#23545;&#35805;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;19&#20010;LLMs&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.14846</link><description>&lt;p&gt;
&#22362;&#25345;&#20320;&#30340;&#35282;&#33394;&#65281;&#20010;&#20154;&#20215;&#20540;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stick to your Role! Stability of Personal Values Expressed in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#20154;&#20215;&#20540;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#34920;&#36798;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#27169;&#25311;&#23545;&#35805;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;19&#20010;LLMs&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#25110;&#24515;&#29702;&#38382;&#21367;&#30340;&#26631;&#20934;&#26041;&#24335;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#25552;&#20379;&#35768;&#22810;&#26469;&#28304;&#20110;&#31867;&#20284;&#26368;&#23567;&#32972;&#26223;&#30340;&#19981;&#21516;&#26597;&#35810;&#65288;&#20363;&#22914;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#39640;&#24230;&#20381;&#36182;&#20110;&#32972;&#26223;&#65292;&#22240;&#27492;&#20174;&#36825;&#31181;&#26368;&#23567;&#32972;&#26223;&#35780;&#20272;&#20013;&#24471;&#20986;&#30340;&#32467;&#35770;&#21487;&#33021;&#23545;&#27169;&#22411;&#22312;&#37096;&#32626;&#20013;&#30340;&#34892;&#20026;&#65288;&#22312;&#37027;&#37324;&#23427;&#23558;&#26292;&#38706;&#20110;&#35768;&#22810;&#26032;&#32972;&#26223;&#65289;&#30340;&#35828;&#26126;&#24456;&#23569;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20381;&#36182;&#20110;&#32972;&#26223;&#30340;&#29305;&#24615;&#24212;&#35813;&#20316;&#20026;LLM&#27604;&#36739;&#30340;&#21478;&#19968;&#20010;&#32500;&#24230;&#26469;&#30740;&#31350;&#65292;&#32780;&#19981;&#26159;&#20854;&#20182;&#32500;&#24230;&#65292;&#22914;&#35748;&#30693;&#33021;&#21147;&#12289;&#30693;&#35782;&#25110;&#27169;&#22411;&#22823;&#23567;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#65288;&#27169;&#25311;&#23545;&#19981;&#21516;&#35805;&#39064;&#30340;&#23545;&#35805;&#65289;&#20215;&#20540;&#34920;&#36798;&#31283;&#23450;&#24615;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#20351;&#29992;&#26631;&#20934;&#24515;&#29702;&#23398;&#38382;&#21367;&#65288;PVQ&#65289;&#21644;&#34892;&#20026;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#27979;&#37327;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26469;&#33258;&#20116;&#20010;&#23478;&#26063;&#30340;19&#20010;&#24320;&#28304;LLM&#12290;&#20511;&#37492;&#24515;&#29702;&#23398;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31561;&#32423;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14846v1 Announce Type: cross  Abstract: The standard way to study Large Language Models (LLMs) through benchmarks or psychology questionnaires is to provide many different queries from similar minimal contexts (e.g. multiple choice questions). However, due to LLM's highly context-dependent nature, conclusions from such minimal-context evaluations may be little informative about the model's behavior in deployment (where it will be exposed to many new contexts). We argue that context-dependence should be studied as another dimension of LLM comparison alongside others such as cognitive abilities, knowledge, or model size. In this paper, we present a case-study about the stability of value expression over different contexts (simulated conversations on different topics), and as measured using a standard psychology questionnaire (PVQ) and a behavioral downstream task. We consider 19 open-sourced LLMs from five families. Reusing methods from psychology, we study Rank-order stabilit
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#19987;&#27880;&#20110;&#20248;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#20307;&#21147;&#23398;&#20013;&#20027;&#21160;&#27969;&#25511;&#21046;&#20013;&#30340;&#24182;&#34892;&#35774;&#32622;&#65292;&#36890;&#36807;&#25286;&#35299;DRL&#26694;&#26550;&#12289;&#36827;&#34892;&#25193;&#23637;&#24615;&#22522;&#20934;&#27979;&#35797;&#12289;&#25552;&#20986;&#28151;&#21512;&#24182;&#34892;&#21270;&#37197;&#32622;&#24182;&#20248;&#21270;&#22810;&#29615;&#22659;DRL&#35757;&#32451;&#20013;&#30340;I/O&#25805;&#20316;&#65292;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;&#21270;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.11515</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#20307;&#21147;&#23398;&#20013;&#20027;&#21160;&#27969;&#25511;&#21046;&#20013;&#30340;&#26368;&#20339;&#24182;&#34892;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Optimal Parallelization Strategies for Active Flow Control in Deep Reinforcement Learning-Based Computational Fluid Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11515
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#19987;&#27880;&#20110;&#20248;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#20307;&#21147;&#23398;&#20013;&#20027;&#21160;&#27969;&#25511;&#21046;&#20013;&#30340;&#24182;&#34892;&#35774;&#32622;&#65292;&#36890;&#36807;&#25286;&#35299;DRL&#26694;&#26550;&#12289;&#36827;&#34892;&#25193;&#23637;&#24615;&#22522;&#20934;&#27979;&#35797;&#12289;&#25552;&#20986;&#28151;&#21512;&#24182;&#34892;&#21270;&#37197;&#32622;&#24182;&#20248;&#21270;&#22810;&#29615;&#22659;DRL&#35757;&#32451;&#20013;&#30340;I/O&#25805;&#20316;&#65292;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#22788;&#29702;&#39640;&#21160;&#24577;&#21644;&#38750;&#32447;&#24615;&#20027;&#21160;&#27969;&#25511;&#21046;&#65288;AFC&#65289;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19982;&#35757;&#32451;DRL&#27169;&#22411;&#30456;&#20851;&#30340;&#35745;&#31639;&#25104;&#26412;&#26500;&#25104;&#20102;&#37325;&#35201;&#30340;&#24615;&#33021;&#29942;&#39048;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#24182;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#26550;&#26500;&#19978;&#23454;&#29616;&#26377;&#25928;&#30340;&#25193;&#23637;&#65292;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#20248;&#21270;&#24182;&#34892;&#35774;&#32622;&#20013;&#30340;&#22522;&#20110;DRL&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#29992;&#20110;AFC&#38382;&#39064;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;DRL&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#25928;&#29575;&#29942;&#39048;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#25286;&#35299;&#25972;&#20307;&#26694;&#26550;&#65292;&#24182;&#20026;&#21508;&#20010;&#32452;&#20214;&#36827;&#34892;&#24191;&#27867;&#30340;&#21487;&#25193;&#23637;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#28151;&#21512;&#24182;&#34892;&#21270;&#37197;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;&#21270;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#22810;&#29615;&#22659;DRL&#35757;&#32451;&#20013;&#30340;&#36755;&#20837;/&#36755;&#20986;&#65288;I/O&#65289;&#25805;&#20316;&#65292;&#20197;&#35299;&#20915;&#19982;&#25968;&#25454;&#31227;&#21160;&#30456;&#20851;&#30340;&#20851;&#38190;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11515v1 Announce Type: new  Abstract: Deep Reinforcement Learning (DRL) has emerged as a promising approach for handling highly dynamic and nonlinear Active Flow Control (AFC) problems. However, the computational cost associated with training DRL models presents a significant performance bottleneck. To address this challenge and enable efficient scaling on high-performance computing architectures, this study focuses on optimizing DRL-based algorithms in parallel settings. We validate an existing state-of-the-art DRL framework used for AFC problems and discuss its efficiency bottlenecks. Subsequently, by deconstructing the overall framework and conducting extensive scalability benchmarks for individual components, we investigate various hybrid parallelization configurations and propose efficient parallelization strategies. Moreover, we refine input/output (I/O) operations in multi-environment DRL training to tackle critical overhead associated with data movement. Finally, we 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24046;&#20998;&#31169;&#26377;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#24046;&#20998;&#38544;&#31169;&#21644;&#36801;&#31227;&#23398;&#20064;&#65292;&#20197;&#25552;&#21319;&#24515;&#29702;&#20581;&#24247;&#30417;&#27979;&#20013;&#30340;&#25968;&#25454;&#38544;&#31169;&#24615;&#21644;&#25968;&#25454;&#20805;&#36275;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10862</link><description>&lt;p&gt;
&#22312;&#26085;&#24120;&#29615;&#22659;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#30417;&#27979;&#65306;&#20197;&#21387;&#21147;&#26816;&#27979;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Differential Private Federated Transfer Learning for Mental Health Monitoring in Everyday Settings: A Case Study on Stress Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10862
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24046;&#20998;&#31169;&#26377;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#24046;&#20998;&#38544;&#31169;&#21644;&#36801;&#31227;&#23398;&#20064;&#65292;&#20197;&#25552;&#21319;&#24515;&#29702;&#20581;&#24247;&#30417;&#27979;&#20013;&#30340;&#25968;&#25454;&#38544;&#31169;&#24615;&#21644;&#25968;&#25454;&#20805;&#36275;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#22312;&#21508;&#20010;&#20154;&#32676;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#30417;&#27979;&#26469;&#20943;&#36731;&#20854;&#23545;&#29983;&#27963;&#36136;&#37327;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;&#25968;&#25454;&#39537;&#21160;&#30340;&#24515;&#29702;&#20581;&#24247;&#30417;&#27979;&#26041;&#27861;&#30340;&#20852;&#36215;&#24378;&#35843;&#20102;&#22312;&#22788;&#29702;&#25935;&#24863;&#20581;&#24247;&#25968;&#25454;&#26102;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#32852;&#37030;&#23398;&#20064;&#22312;&#24515;&#29702;&#20581;&#24247;&#30417;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#24212;&#23545;&#29305;&#23450;&#32593;&#32476;&#25915;&#20987;&#21644;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#25968;&#25454;&#19981;&#36275;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24046;&#20998;&#31169;&#26377;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#25968;&#25454;&#38544;&#31169;&#24615;&#24182;&#20016;&#23500;&#25968;&#25454;&#20805;&#36275;&#24615;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#32852;&#37030;&#23398;&#20064;&#19982;&#24046;&#20998;&#38544;&#31169;&#65288;&#36890;&#36807;&#23558;&#22122;&#22768;&#24341;&#20837;&#26356;&#26032;&#65289;&#21644;&#36801;&#31227;&#23398;&#20064;&#65288;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#36890;&#29992;&#27169;&#22411;&#65289;&#20004;&#20010;&#20851;&#38190;&#20803;&#32032;&#30456;&#32467;&#21512;&#65292;&#20197;&#24212;&#23545;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#32570;&#22914;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10862v1 Announce Type: new  Abstract: Mental health conditions, prevalent across various demographics, necessitate efficient monitoring to mitigate their adverse impacts on life quality. The surge in data-driven methodologies for mental health monitoring has underscored the importance of privacy-preserving techniques in handling sensitive health data. Despite strides in federated learning for mental health monitoring, existing approaches struggle with vulnerabilities to certain cyber-attacks and data insufficiency in real-world applications. In this paper, we introduce a differential private federated transfer learning framework for mental health monitoring to enhance data privacy and enrich data sufficiency. To accomplish this, we integrate federated learning with two pivotal elements: (1) differential privacy, achieved by introducing noise into the updates, and (2) transfer learning, employing a pre-trained universal model to adeptly address issues of data imbalance and in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#21464;&#21387;&#22120;&#26041;&#27861;&#35299;&#35835;&#24515;&#29575;&#20449;&#21495;&#65292;&#25552;&#39640;&#24515;&#33039;&#30142;&#30149;&#26816;&#27979;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09474</link><description>&lt;p&gt;
&#35299;&#35835;&#24515;&#29575;&#20449;&#21495;&#65306;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#25216;&#26415;&#30340;&#21487;&#35299;&#37322;&#24615;&#25151;&#39076;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deciphering Heartbeat Signatures: A Vision Transformer Approach to Explainable Atrial Fibrillation Detection from ECG Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#21464;&#21387;&#22120;&#26041;&#27861;&#35299;&#35835;&#24515;&#29575;&#20449;&#21495;&#65292;&#25552;&#39640;&#24515;&#33039;&#30142;&#30149;&#26816;&#27979;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21487;&#31359;&#25140;&#21333;&#23548;&#32852;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#35774;&#22791;&#30340;&#36828;&#31243;&#24739;&#32773;&#30417;&#27979;&#22312;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#36827;&#34892;&#33258;&#21160;&#24515;&#33039;&#30142;&#30149;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24212;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;AI&#26041;&#27861;&#36827;&#34892;&#24515;&#33039;&#30142;&#30149;&#26816;&#27979;&#65292;&#20294;&#30001;&#20110;&#30446;&#21069;AI&#31639;&#27861;&#30340;&#40657;&#30418;&#29305;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#23578;&#26410;&#34987;&#24191;&#27867;&#25509;&#21463;&#20316;&#20026;&#20020;&#24202;&#35786;&#26029;&#30340;&#21487;&#38752;&#36741;&#21161;&#24037;&#20855;&#12290;&#23588;&#20854;&#38656;&#35201;&#30830;&#23450;ECG&#20449;&#21495;&#20013;&#36129;&#29486;&#20110;&#20934;&#30830;&#35786;&#26029;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#23548;&#32852;ECG&#25968;&#25454;&#35782;&#21035;&#25151;&#39076;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27531;&#24046;&#32593;&#32476;&#65288;ResNet&#65289;&#26041;&#27861;&#20197;&#20316;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09474v1 Announce Type: cross  Abstract: Remote patient monitoring based on wearable single-lead electrocardiogram (ECG) devices has significant potential for enabling the early detection of heart disease, especially in combination with artificial intelligence (AI) approaches for automated heart disease detection. There have been prior studies applying AI approaches based on deep learning for heart disease detection. However, these models are yet to be widely accepted as a reliable aid for clinical diagnostics, in part due to the current black-box perception surrounding many AI algorithms. In particular, there is a need to identify the key features of the ECG signal that contribute toward making an accurate diagnosis, thereby enhancing the interpretability of the model. In the present study, we develop a vision transformer approach to identify atrial fibrillation based on single-lead ECG data. A residual network (ResNet) approach is also developed for comparison with the visi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#26469;&#35299;&#20915;&#38271;&#23614;&#25928;&#24212;&#38382;&#39064;&#65292;&#25552;&#39640;&#23545;&#20110;&#21253;&#21547;&#25361;&#25112;&#24615;&#22330;&#26223;&#30340;&#25968;&#25454;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08698</link><description>&lt;p&gt;
AMEND&#65306;&#19968;&#31181;&#29992;&#20110;&#38271;&#23614;&#36712;&#36857;&#39044;&#27979;&#30340;&#19987;&#23478;&#28151;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AMEND: A Mixture of Experts Framework for Long-tailed Trajectory Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#26469;&#35299;&#20915;&#38271;&#23614;&#25928;&#24212;&#38382;&#39064;&#65292;&#25552;&#39640;&#23545;&#20110;&#21253;&#21547;&#25361;&#25112;&#24615;&#22330;&#26223;&#30340;&#25968;&#25454;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#39044;&#27979;&#34892;&#20154;&#26410;&#26469;&#30340;&#21160;&#21521;&#23545;&#20110;&#26234;&#33021;&#39550;&#39542;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#24320;&#21457;&#36825;&#20010;&#20219;&#21153;&#30340;&#27169;&#22411;&#38656;&#35201;&#21253;&#21547;&#22810;&#26679;&#26679;&#26412;&#30340;&#20016;&#23500;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#28982;&#36712;&#36857;&#39044;&#27979;&#25968;&#25454;&#38598;&#36890;&#24120;&#23545;&#31616;&#21333;&#26679;&#26412;&#20559;&#37325;&#65292;&#24182;&#32570;&#20047;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#12290;&#36825;&#31181;&#38271;&#23614;&#25928;&#24212;&#23548;&#33268;&#39044;&#27979;&#27169;&#22411;&#22312;&#21253;&#21547;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#23614;&#37096;&#37096;&#20998;&#34920;&#29616;&#19981;&#20339;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#31867;&#26465;&#20214;&#36229;&#32593;&#32476;&#31561;&#26041;&#27861;&#35299;&#20915;&#20102;&#38271;&#23614;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#26159;&#27169;&#22359;&#21270;&#30340;&#65292;&#19981;&#33021;&#24212;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#19987;&#38376;&#30340;&#19987;&#23478;&#28151;&#21512;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#27599;&#20010;&#19987;&#23478;&#37117;&#36890;&#36807;&#38024;&#23545;&#29305;&#23450;&#37096;&#20998;&#30340;&#29305;&#27530;&#25216;&#33021;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08698v1 Announce Type: cross Abstract: Accurate prediction of pedestrians' future motions is critical for intelligent driving systems. Developing models for this task requires rich datasets containing diverse sets of samples. However, the existing naturalistic trajectory prediction datasets are generally imbalanced in favor of simpler samples and lack challenging scenarios. Such a long-tail effect causes prediction models to underperform on the tail portion of the data distribution containing safety-critical scenarios. Previous methods tackle the long-tail problem using methods such as contrastive learning and class-conditioned hypernetworks. These approaches, however, are not modular and cannot be applied to many machine learning architectures. In this work, we propose a modular model-agnostic framework for trajectory prediction that leverages a specialized mixture of experts. In our approach, each expert is trained with a specialized skill with respect to a particular part
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;RBF-PINN&#26041;&#27861;&#65292;&#22312;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20013;&#24212;&#29992;&#38750;Fourier&#20301;&#32622;&#23884;&#20837;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;Fourier&#29305;&#24449;&#26144;&#23556;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.08367</link><description>&lt;p&gt;
RBF-PINN&#65306;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;Fourier&#20301;&#32622;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
RBF-PINN: Non-Fourier Positional Embedding in Physics-Informed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;RBF-PINN&#26041;&#27861;&#65292;&#22312;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20013;&#24212;&#29992;&#38750;Fourier&#20301;&#32622;&#23884;&#20837;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;Fourier&#29305;&#24449;&#26144;&#23556;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26368;&#36817;&#35768;&#22810;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#21464;&#20307;&#22312;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#26469;&#33258;&#26356;&#24191;&#27867;&#30340;&#31070;&#32463;&#34920;&#31034;&#30740;&#31350;&#30340;&#29305;&#24449;&#26144;&#23556;&#30340;&#32463;&#39564;&#20248;&#21183;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;Fourier&#30340;&#29305;&#24449;&#26144;&#23556;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24314;&#35758;&#20351;&#29992;&#20855;&#26377;&#26465;&#20214;&#27491;&#23450;&#24615;&#36136;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#12290;&#23454;&#35777;&#21457;&#29616;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#26696;&#20363;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#22522;&#20110;&#22352;&#26631;&#30340;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24182;&#20026;PINNs&#30740;&#31350;&#30340;&#26356;&#24191;&#27867;&#39046;&#22495;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
While many recent Physics-Informed Neural Networks (PINNs) variants have had considerable success in solving Partial Differential Equations, the empirical benefits of feature mapping drawn from the broader Neural Representations research have been largely overlooked. We highlight the limitations of widely used Fourier-based feature mapping in certain situations and suggest the use of the conditionally positive definite Radial Basis Function. The empirical findings demonstrate the effectiveness of our approach across a variety of forward and inverse problem cases. Our method can be seamlessly integrated into coordinate-based input neural networks and contribute to the wider field of PINNs research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#26681;&#25454;&#20540;&#25439;&#22833;&#20248;&#20808;&#36873;&#25321;&#32423;&#21035;&#65292;&#21487;&#20197;&#25913;&#21892;&#20195;&#29702;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26041;&#27861;&#23545;&#25913;&#21892;&#20195;&#29702;&#34920;&#29616;&#20063;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03479</link><description>&lt;p&gt;
ICED: &#36890;&#36807;&#19978;&#19979;&#25991;&#29615;&#22659;&#35774;&#35745;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context Environment Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#26681;&#25454;&#20540;&#25439;&#22833;&#20248;&#20808;&#36873;&#25321;&#32423;&#21035;&#65292;&#21487;&#20197;&#25913;&#21892;&#20195;&#29702;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26041;&#27861;&#23545;&#25913;&#21892;&#20195;&#29702;&#34920;&#29616;&#20063;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#33258;&#20027;&#20195;&#29702;&#36890;&#24120;&#32570;&#20047;&#25104;&#21151;&#22320;&#25512;&#24191;&#21040;&#26032;&#29615;&#22659;&#30340;&#33021;&#21147;&#65292;&#21363;&#20351;&#36825;&#20123;&#29615;&#22659;&#19982;&#23427;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#29615;&#22659;&#20855;&#26377;&#30456;&#20284;&#30340;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#20307;&#29615;&#22659;&#23454;&#20363;&#65288;&#25110;&#32423;&#21035;&#65289;&#30340;&#37319;&#26679;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#25512;&#24191;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#20849;&#20139;&#22522;&#26412;&#23618;&#30340;&#28145;&#24230;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26550;&#26500;&#65292;&#26681;&#25454;&#20854;&#20540;&#25439;&#22833;&#20248;&#20808;&#36873;&#25321;&#32423;&#21035;&#65292;&#21487;&#20197;&#26368;&#23567;&#21270;&#20195;&#29702;&#30340;&#20869;&#37096;&#34920;&#31034;&#19982;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#35757;&#32451;&#32423;&#21035;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#36825;&#20026;&#26576;&#20123;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#23454;&#29616;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#27880;&#24847;&#21147;&#36716;&#21521;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#20855;&#26377;&#26356;&#22810;&#25511;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;UED&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21464;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#29615;&#22659;&#23454;&#20363;&#65292;&#20174;&#32780;&#24433;&#21709;&#20195;&#29702;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents trained using deep reinforcement learning (RL) often lack the ability to successfully generalise to new environments, even when they share characteristics with the environments they have encountered during training. In this work, we investigate how the sampling of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents. We discover that, for deep actor-critic architectures sharing their base layers, prioritising levels according to their value loss minimises the mutual information between the agent's internal representation and the set of training levels in the generated training data. This provides a novel theoretical justification for the implicit regularisation achieved by certain adaptive sampling strategies. We then turn our attention to unsupervised environment design (UED) methods, which have more control over the data generation mechanism. We find that existing UED methods can significantly shift the trainin
&lt;/p&gt;</description></item><item><title>&#26080;&#20559;&#22909;&#23545;&#40784;&#23398;&#20064;&#20351;&#29992;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;&#20316;&#20026;&#20851;&#38190;&#30446;&#26631;&#65292;&#22312;&#25552;&#20379;&#31283;&#20581;&#22870;&#21169;&#20449;&#21495;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20559;&#22909;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03469</link><description>&lt;p&gt;
&#26080;&#20559;&#22909;&#23545;&#40784;&#23398;&#20064;&#19982;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Preference-free Alignment Learning with Regularized Relevance Reward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03469
&lt;/p&gt;
&lt;p&gt;
&#26080;&#20559;&#22909;&#23545;&#40784;&#23398;&#20064;&#20351;&#29992;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;&#20316;&#20026;&#20851;&#38190;&#30446;&#26631;&#65292;&#22312;&#25552;&#20379;&#31283;&#20581;&#22870;&#21169;&#20449;&#21495;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20559;&#22909;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#34987;&#35748;&#20026;&#26159;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#19982;&#26222;&#36941;&#30340;&#35266;&#28857;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#20542;&#21521;&#20110;&#32473;&#38271;&#30340;&#19982;&#20027;&#39064;&#26080;&#20851;&#30340;&#22238;&#22797;&#26356;&#39640;&#30340;&#20998;&#25968;&#65292;&#32780;&#32473;&#30701;&#30340;&#19982;&#20027;&#39064;&#30456;&#20851;&#30340;&#22238;&#22797;&#36739;&#20302;&#20998;&#12290;&#22312;&#36825;&#19968;&#35266;&#23519;&#30340;&#39537;&#21160;&#19979;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26080;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#8220;&#30456;&#20851;&#24615;&#8221;&#20316;&#20026;&#23545;&#40784;&#30340;&#19968;&#20010;&#20851;&#38190;&#30446;&#26631;&#12290;&#22312;&#25105;&#20204;&#30340;&#31532;&#19968;&#27425;&#23581;&#35797;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#20165;&#36890;&#36807;&#26816;&#32034;&#24471;&#21040;&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#23481;&#26131;&#21463;&#21040;&#22870;&#21169;&#27450;&#39575;&#30340;&#24433;&#21709;&#65292;&#21363;&#36807;&#24230;&#20248;&#21270;&#21040;&#19981;&#26399;&#26395;&#30340;&#25463;&#24452;&#19978;&#65292;&#24403;&#25105;&#20204;&#23558;&#35813;&#24471;&#20998;&#20316;&#20026;&#22870;&#21169;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26377;&#25928;&#30340;&#24402;&#32435;&#20559;&#24046;&#25972;&#21512;&#21040;&#24120;&#35268;&#30340;&#30456;&#20851;&#24615;&#20013;&#65292;&#20114;&#30456;&#27491;&#21017;&#21270;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#28151;&#21512;&#22870;&#21169;&#20989;&#25968;&#65306;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;&#65288;$R^3$&#65289;&#12290;$R^3$&#36890;&#36807;&#25552;&#20379;&#31283;&#20581;&#30340;&#22870;&#21169;&#20449;&#21495;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#20559;&#22909;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;$R^3$&#19981;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
Learning from human preference has been considered key to aligning Large Language Models (LLMs) with human values. However, contrary to popular belief, our preliminary study reveals that reward models trained on human preference datasets tend to give higher scores to long off-topic responses than short on-topic ones. Motivated by this observation, we explore a preference-free approach utilizing `relevance' as a key objective for alignment. On our first attempt, we find that the relevance score obtained by a retriever alone is vulnerable to reward hacking, i.e., overoptimizing to undesired shortcuts, when we utilize the score as a reward for reinforcement learning. To mitigate it, we integrate effective inductive biases into the vanilla relevance to regularize each other, resulting in a mixture of reward functions: Regularized Relevance Reward ($R^3$). $R^3$ significantly improves performance on preference benchmarks by providing a robust reward signal. Notably, $R^3$ does not require a
&lt;/p&gt;</description></item><item><title>DeepSeekMath&#26159;&#19968;&#31181;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#25552;&#21319;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#31454;&#36187;&#32423;&#21035;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03300</link><description>&lt;p&gt;
DeepSeekMath: &#23558;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#25512;&#21521;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03300
&lt;/p&gt;
&lt;p&gt;
DeepSeekMath&#26159;&#19968;&#31181;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#25552;&#21319;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#31454;&#36187;&#32423;&#21035;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25512;&#29702;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#32467;&#26500;&#21270;&#30340;&#29305;&#24615;&#65292;&#23545;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DeepSeekMath 7B&#65292;&#23427;&#22312;Common Crawl&#20013;&#33719;&#21462;&#20102;120B&#20010;&#19982;&#25968;&#23398;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24182;&#32467;&#21512;&#20102;&#33258;&#28982;&#35821;&#35328;&#21644;&#20195;&#30721;&#25968;&#25454;&#26469;&#32487;&#32493;&#39044;&#35757;&#32451;DeepSeek-Coder-Base-v1.5 7B&#12290;DeepSeekMath 7B&#22312;&#31454;&#36187;&#32423;&#21035;&#30340;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;51.7%&#30340;&#20998;&#25968;&#65292;&#26080;&#38656;&#20381;&#36182;&#22806;&#37096;&#24037;&#20855;&#21253;&#21644;&#25237;&#31080;&#25216;&#26415;&#65292;&#25509;&#36817;&#20102;Gemini-Ultra&#21644;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;DeepSeekMath 7B&#30340;&#33258;&#19968;&#33268;&#24615;&#22312;MATH&#19978;&#30340;64&#20010;&#26679;&#26412;&#20013;&#36798;&#21040;&#20102;60.9%&#30340;&#20998;&#25968;&#12290;DeepSeekMath&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#24402;&#22240;&#20110;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25968;&#25454;&#36873;&#25321;&#31649;&#36947;&#20805;&#20998;&#21033;&#29992;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#32593;&#32476;&#25968;&#25454;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32676;&#20307;&#30456;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;GRPO&#65289;&#65292;&#36825;&#26159;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#30340;&#19968;&#20010;&#21464;&#20307;&#65292;&#21487;&#20197;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#36335;&#20917;&#35299;&#26512;&#26694;&#26550;&#65292;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#26469;&#35299;&#20915;&#26080;&#20154;&#26426;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#19979;&#30340;&#36335;&#20917;&#22330;&#26223;&#35299;&#26512;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#39318;&#20808;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24555;&#36895;&#26816;&#27979;&#36335;&#20917;&#24863;&#20852;&#36259;&#21306;&#22495;&#65292;&#28982;&#21518;&#21033;&#29992;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#29983;&#25104;&#36335;&#20917;&#21306;&#22495;&#25513;&#27169;&#65292;&#28982;&#21518;&#37319;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#32593;&#32476;&#25552;&#21462;&#29305;&#24449;&#34920;&#31034;&#65292;&#26368;&#21518;&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#23545;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#21644;&#26631;&#35760;&#12290;</title><link>https://arxiv.org/abs/2402.02985</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#30340;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#29992;&#20110;&#36335;&#20917;&#22330;&#26223;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Unsupervised semantic segmentation of high-resolution UAV imagery for road scene parsing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#36335;&#20917;&#35299;&#26512;&#26694;&#26550;&#65292;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#26469;&#35299;&#20915;&#26080;&#20154;&#26426;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#19979;&#30340;&#36335;&#20917;&#22330;&#26223;&#35299;&#26512;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#39318;&#20808;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24555;&#36895;&#26816;&#27979;&#36335;&#20917;&#24863;&#20852;&#36259;&#21306;&#22495;&#65292;&#28982;&#21518;&#21033;&#29992;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#29983;&#25104;&#36335;&#20917;&#21306;&#22495;&#25513;&#27169;&#65292;&#28982;&#21518;&#37319;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#32593;&#32476;&#25552;&#21462;&#29305;&#24449;&#34920;&#31034;&#65292;&#26368;&#21518;&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#23545;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#21644;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#20154;&#26426;&#22270;&#20687;&#20013;&#35299;&#26512;&#36335;&#20917;&#22330;&#26223;&#23384;&#22312;&#20004;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#26080;&#20154;&#26426;&#22270;&#20687;&#30340;&#39640;&#20998;&#36776;&#29575;&#20351;&#24471;&#22788;&#29702;&#22256;&#38590;&#12290;&#20854;&#27425;&#65292;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#26631;&#27880;&#26469;&#35757;&#32451;&#24378;&#22823;&#32780;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#36335;&#20917;&#35299;&#26512;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#36817;&#26399;&#22312;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#37319;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#39640;&#25928;&#22788;&#29702;&#36229;&#22823;&#20998;&#36776;&#29575;&#26080;&#20154;&#26426;&#22270;&#20687;&#65292;&#24555;&#36895;&#26816;&#27979;&#22270;&#20687;&#20013;&#30340;&#36335;&#20917;&#24863;&#20852;&#36259;&#21306;&#22495;&#12290;&#25509;&#19979;&#26469;&#65292;&#21033;&#29992;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;SAM&#20026;&#27809;&#26377;&#31867;&#21035;&#20449;&#24687;&#30340;&#36335;&#20917;&#21306;&#22495;&#29983;&#25104;&#25513;&#27169;&#12290;&#38543;&#21518;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#32593;&#32476;&#20174;&#25152;&#26377;&#25513;&#27169;&#21306;&#22495;&#20013;&#25552;&#21462;&#29305;&#24449;&#34920;&#31034;&#12290;&#26368;&#21518;&#65292;&#24212;&#29992;&#26080;&#30417;&#30563;&#30340;&#32858;&#31867;&#31639;&#27861;&#23545;&#36825;&#20123;&#29305;&#24449;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#24182;&#20026;&#27599;&#20010;&#31751;&#20998;&#37197;ID&#12290;&#28982;&#21518;&#65292;&#23558;&#25513;&#27169;&#21306;&#22495;&#21512;&#24182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two challenges are presented when parsing road scenes in UAV images. First, the high resolution of UAV images makes processing difficult. Second, supervised deep learning methods require a large amount of manual annotations to train robust and accurate models. In this paper, an unsupervised road parsing framework that leverages recent advances in vision language models and fundamental computer vision model is introduced.Initially, a vision language model is employed to efficiently process ultra-large resolution UAV images to quickly detect road regions of interest in the images. Subsequently, the vision foundation model SAM is utilized to generate masks for the road regions without category information. Following that, a self-supervised representation learning network extracts feature representations from all masked regions. Finally, an unsupervised clustering algorithm is applied to cluster these feature representations and assign IDs to each cluster. The masked regions are combined w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#20107;&#23454;&#27010;&#29575;&#21644;&#39069;&#22806;&#30340;&#22240;&#26524;&#32467;&#26500;&#20808;&#39564;&#20449;&#24687;&#65292;&#20811;&#26381;&#20102;&#22240;&#26524;&#22270;&#26410;&#30693;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#35299;&#37322;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#24212;&#29992;&#20110;&#20449;&#36151;&#35780;&#32423;&#31561;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.02678</link><description>&lt;p&gt;
&#20351;&#29992;&#22240;&#26524;&#21457;&#29616;&#35299;&#37322;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#19982;&#20449;&#35465;&#35780;&#32423;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations of Black-box Machine Learning Models using Causal Discovery with Applications to Credit Rating
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#20107;&#23454;&#27010;&#29575;&#21644;&#39069;&#22806;&#30340;&#22240;&#26524;&#32467;&#26500;&#20808;&#39564;&#20449;&#24687;&#65292;&#20811;&#26381;&#20102;&#22240;&#26524;&#22270;&#26410;&#30693;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#35299;&#37322;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#24212;&#29992;&#20110;&#20449;&#36151;&#35780;&#32423;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26377;&#21161;&#20110;&#38416;&#26126;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#36890;&#36807;&#23637;&#31034;&#20854;&#39044;&#27979;&#22522;&#30784;&#26469;&#22686;&#24378;&#20854;&#21487;&#38752;&#24615;&#12290;&#20960;&#31181;XAI&#27169;&#22411;&#32771;&#34385;&#20351;&#29992;&#22240;&#26524;&#20851;&#31995;&#26469;&#35299;&#37322;&#27169;&#22411;&#65292;&#36890;&#36807;&#30740;&#31350;&#39044;&#27979;&#27169;&#22411;&#30340;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#21644;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#20123;&#27169;&#22411;&#22823;&#22810;&#22522;&#20110;&#21453;&#20107;&#23454;&#27010;&#29575;&#26469;&#35299;&#37322;&#65292;&#24182;&#20551;&#35774;&#22240;&#26524;&#22270;&#24050;&#30693;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20551;&#35774;&#22686;&#21152;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#38469;&#25968;&#25454;&#24212;&#29992;&#20013;&#30340;&#22797;&#26434;&#24615;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#29305;&#24449;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#26159;&#26410;&#30693;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;XAI&#26694;&#26550;&#65292;&#25918;&#23485;&#20102;&#22240;&#26524;&#22270;&#24050;&#30693;&#30340;&#32422;&#26463;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#21453;&#20107;&#23454;&#27010;&#29575;&#21644;&#20851;&#20110;&#22240;&#26524;&#32467;&#26500;&#30340;&#39069;&#22806;&#20808;&#39564;&#20449;&#24687;&#65292;&#36890;&#36807;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#20272;&#35745;&#20986;&#30340;&#22240;&#26524;&#22270;&#19982;&#40657;&#30418;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable artificial intelligence (XAI) has helped elucidate the internal mechanisms of machine learning algorithms, bolstering their reliability by demonstrating the basis of their predictions. Several XAI models consider causal relationships to explain models by examining the input-output relationships of prediction models and the dependencies between features. The majority of these models have been based their explanations on counterfactual probabilities, assuming that the causal graph is known. However, this assumption complicates the application of such models to real data, given that the causal relationships between features are unknown in most cases. Thus, this study proposed a novel XAI framework that relaxed the constraint that the causal graph is known. This framework leveraged counterfactual probabilities and additional prior information on causal structure, facilitating the integration of a causal graph estimated through causal discovery methods and a black-box classifica
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#20107;&#23454;&#34920;&#31034;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24178;&#39044;&#25991;&#26412;&#34920;&#31034;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00711</link><description>&lt;p&gt;
&#20351;&#29992;&#21453;&#20107;&#23454;&#34920;&#31034;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Explaining Text Classifiers with Counterfactual Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#20107;&#23454;&#34920;&#31034;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24178;&#39044;&#25991;&#26412;&#34920;&#31034;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#30340;&#35299;&#37322;&#26041;&#27861;&#21487;&#20197;&#20026;&#20998;&#31867;&#22120;&#25552;&#20379;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#20854;&#20013;&#21453;&#20107;&#23454;&#26159;&#25351;&#38500;&#20102;&#19968;&#20010;&#20998;&#31867;&#29305;&#24449;&#20043;&#22806;&#65292;&#19982;&#30495;&#23454;&#35266;&#23519;&#23436;&#20840;&#30456;&#21516;&#30340;&#20551;&#35774;&#20107;&#20214;&#12290;&#28982;&#32780;&#65292;&#22312;&#25991;&#26412;&#39046;&#22495;&#26500;&#24314;&#36825;&#31181;&#21453;&#20107;&#23454;&#23384;&#22312;&#29305;&#23450;&#25361;&#25112;&#65292;&#22240;&#20026;&#26576;&#20123;&#23646;&#24615;&#20540;&#21487;&#33021;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#20107;&#20214;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#34920;&#31034;&#36827;&#34892;&#24178;&#39044;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#24178;&#39044;&#26041;&#27861;&#26159;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#38752;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;Pearl&#30340;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#20013;&#23450;&#20041;&#30340;&#21453;&#20107;&#23454;&#26159;&#19968;&#33268;&#30340;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#22522;&#20110;&#30495;&#23454;&#21453;&#20107;&#23454;&#65288;&#36890;&#36807;&#26126;&#30830;&#30340;&#25991;&#26412;&#24178;&#39044;&#33719;&#24471;&#65289;&#21644;&#25105;&#20204;&#30340;&#21453;&#20107;&#23454;&#65288;&#36890;&#36807;&#23545;&#25991;&#26412;&#34920;&#31034;&#30340;&#24178;&#39044;&#24471;&#21040;&#65289;&#30340;&#20998;&#31867;&#22120;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
One well motivated explanation method for classifiers leverages counterfactuals which are hypothetical events identical to real observations in all aspects except for one categorical feature. Constructing such counterfactual poses specific challenges for texts, however, as some attribute values may not necessarily align with plausible real-world events. In this paper we propose a simple method for generating counterfactuals by intervening in the space of text representations which bypasses this limitation. We argue that our interventions are minimally disruptive and that they are theoretically sound as they align with counterfactuals as defined in Pearl's causal inference framework. To validate our method, we first conduct experiments on a synthetic dataset of counterfactuals, allowing for a direct comparison between classifier predictions based on ground truth counterfactuals (obtained through explicit text interventions) and our counterfactuals, derived through interventions in the r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#21307;&#38498;&#25968;&#25454;&#30340;&#20998;&#25955;&#12289;&#21327;&#20316;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;DeCaPH&#65289;&#65292;&#23427;&#21487;&#20197;&#20801;&#35768;&#19981;&#21516;&#26041;&#22312;&#19981;&#20849;&#20139;&#31169;&#26377;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#21327;&#20316;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#38480;&#21046;&#25968;&#25454;&#27844;&#38706;&#21644;&#38544;&#31169;&#20405;&#29359;&#26469;&#20445;&#25252;&#24739;&#32773;&#38544;&#31169;&#12290;</title><link>https://arxiv.org/abs/2402.00205</link><description>&lt;p&gt;
&#20998;&#25955;&#12289;&#21327;&#20316;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#22810;&#21307;&#38498;&#25968;&#25454;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decentralised, Collaborative, and Privacy-preserving Machine Learning for Multi-Hospital Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#21307;&#38498;&#25968;&#25454;&#30340;&#20998;&#25955;&#12289;&#21327;&#20316;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;DeCaPH&#65289;&#65292;&#23427;&#21487;&#20197;&#20801;&#35768;&#19981;&#21516;&#26041;&#22312;&#19981;&#20849;&#20139;&#31169;&#26377;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#21327;&#20316;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#38480;&#21046;&#25968;&#25454;&#27844;&#38706;&#21644;&#38544;&#31169;&#20405;&#29359;&#26469;&#20445;&#25252;&#24739;&#32773;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#26469;&#33258;&#19981;&#21516;&#28304;&#22836;&#21644;&#29615;&#22659;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#23545;&#20110;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#22797;&#26434;&#19988;&#22810;&#21464;&#30340;&#38544;&#31169;&#21644;&#30417;&#31649;&#35201;&#27714;&#65292;&#36328;&#19981;&#21516;&#21307;&#30103;&#26426;&#26500;&#20849;&#20139;&#25968;&#25454;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#20801;&#35768;&#22810;&#20010;&#26041;&#21442;&#19982;&#21512;&#20316;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#19981;&#30452;&#25509;&#20849;&#20139;&#25968;&#25454;&#38598;&#25110;&#36890;&#36807;&#21512;&#20316;&#25439;&#23475;&#25968;&#25454;&#38598;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#21508;&#26041;&#29616;&#26377;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#20855;&#26377;&#22256;&#38590;&#20294;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#29992;&#20110;&#22810;&#21307;&#38498;&#25968;&#25454;&#30340;&#20998;&#25955;&#12289;&#21327;&#20316;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;DeCaPH&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#20855;&#26377;&#20197;&#19979;&#20851;&#38190;&#20248;&#28857;&#65306;&#65288;1&#65289;&#20801;&#35768;&#19981;&#21516;&#26041;&#22312;&#19981;&#20256;&#36755;&#31169;&#26377;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#21327;&#20316;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65307;&#65288;2&#65289;&#36890;&#36807;&#38480;&#21046;&#28508;&#22312;&#30340;&#25968;&#25454;&#27844;&#38706;&#21644;&#38544;&#31169;&#20405;&#29359;&#26469;&#20445;&#25252;&#24739;&#32773;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has demonstrated its great potential on medical data analysis. Large datasets collected from diverse sources and settings are essential for ML models in healthcare to achieve better accuracy and generalizability. Sharing data across different healthcare institutions is challenging because of complex and varying privacy and regulatory requirements. Hence, it is hard but crucial to allow multiple parties to collaboratively train an ML model leveraging the private datasets available at each party without the need for direct sharing of those datasets or compromising the privacy of the datasets through collaboration. In this paper, we address this challenge by proposing Decentralized, Collaborative, and Privacy-preserving ML for Multi-Hospital Data (DeCaPH). It offers the following key benefits: (1) it allows different parties to collaboratively train an ML model without transferring their private datasets; (2) it safeguards patient privacy by limiting the potential pr
&lt;/p&gt;</description></item><item><title>SPO&#31639;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;KL&#25955;&#24230;&#21098;&#20999;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;PPO&#30340;&#20027;&#27969;&#21464;&#20307;&#65292;&#22312;Atari 2600&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#12289;&#26497;&#20302;&#30340;KL&#25955;&#24230;&#21644;&#26356;&#39640;&#30340;&#31574;&#30053;&#29109;&#65292;&#19988;&#23545;&#32593;&#32476;&#28145;&#24230;&#25110;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.16025</link><description>&lt;p&gt;
&#31616;&#21333;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Simple Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16025
&lt;/p&gt;
&lt;p&gt;
SPO&#31639;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;KL&#25955;&#24230;&#21098;&#20999;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;PPO&#30340;&#20027;&#27969;&#21464;&#20307;&#65292;&#22312;Atari 2600&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#12289;&#26497;&#20302;&#30340;KL&#25955;&#24230;&#21644;&#26356;&#39640;&#30340;&#31574;&#30053;&#29109;&#65292;&#19988;&#23545;&#32593;&#32476;&#28145;&#24230;&#25110;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PPO&#65288;Proximal Policy Optimization&#65289;&#31639;&#27861;&#22312;&#35768;&#22810;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#34987;&#35748;&#20026;&#26159;TRPO&#65288;Trust Region Policy Optimization&#65289;&#31639;&#27861;&#30340;&#31616;&#21270;&#29256;&#26412;&#12290;&#28982;&#32780;&#65292;PPO&#20013;&#30340;&#27604;&#29575;&#21098;&#20999;&#25805;&#20316;&#24182;&#19981;&#24635;&#26159;&#26377;&#25928;&#22320;&#24378;&#21046;&#25191;&#34892;&#20449;&#20219;&#21306;&#22495;&#32422;&#26463;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21098;&#20999;&#26041;&#27861;&#65292;&#21363;Simple Policy Optimization&#65288;SPO&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#26087;&#31574;&#30053;&#21644;&#24403;&#21069;&#31574;&#30053;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;&#22312;Atari 2600&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;PPO&#30340;&#20027;&#27969;&#21464;&#20307;&#30456;&#27604;&#65292;SPO&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#26497;&#20302;&#30340;KL&#25955;&#24230;&#21644;&#26356;&#39640;&#30340;&#31574;&#30053;&#29109;&#65292;&#24182;&#19988;&#23545;&#32593;&#32476;&#28145;&#24230;&#25110;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;SPO&#20445;&#25345;&#20102;&#26080;&#32422;&#26463;&#19968;&#38454;&#31639;&#27861;&#30340;&#31616;&#21333;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.16025v2 Announce Type: replace  Abstract: PPO (Proximal Policy Optimization) algorithm has demonstrated excellent performance in many fields, and it is considered as a simple version of TRPO (Trust Region Policy Optimization) algorithm. However, the ratio clipping operation in PPO may not always effectively enforce the trust region constraints, this can be a potential factor affecting the stability of the algorithm. In this paper, we propose Simple Policy Optimization (SPO) algorithm, which introduces a novel clipping method for KL divergence between the old and current policies. Extensive experimental results in Atari 2600 environments indicate that, compared to the mainstream variants of PPO, SPO achieves better sample efficiency, extremely low KL divergence, and higher policy entropy, and is robust to the increase in network depth or complexity. More importantly, SPO maintains the simplicity of an unconstrained first-order algorithm. Code is available at https://github.co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;ODE&#26041;&#27861;&#65292;&#23558;&#31283;&#23450;&#24615;&#30340;Borkar-Meyn&#23450;&#29702;&#20174;&#38789;&#24046;&#24322;&#22122;&#22768;&#35774;&#23450;&#25299;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#35774;&#23450;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.07844</link><description>&lt;p&gt;
&#20351;&#29992;ODE&#26041;&#27861;&#36827;&#34892;&#24102;&#26377;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#30340;&#38543;&#26426;&#36924;&#36817;&#21644;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
The ODE Method for Stochastic Approximation and Reinforcement Learning with Markovian Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;ODE&#26041;&#27861;&#65292;&#23558;&#31283;&#23450;&#24615;&#30340;Borkar-Meyn&#23450;&#29702;&#20174;&#38789;&#24046;&#24322;&#22122;&#22768;&#35774;&#23450;&#25299;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#35774;&#23450;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#36924;&#36817;&#26159;&#19968;&#31867;&#36890;&#36807;&#36845;&#20195;&#12289;&#22686;&#37327;&#21644;&#38543;&#26426;&#26356;&#26032;&#21521;&#37327;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#12290;&#20998;&#26512;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#30830;&#20445;&#20854;&#31283;&#23450;&#24615;&#65292;&#21363;&#35777;&#26126;&#38543;&#26426;&#21521;&#37327;&#36845;&#20195;&#20960;&#20046;&#24517;&#23450;&#26377;&#30028;&#12290;&#26412;&#25991;&#23558;&#31283;&#23450;&#24615;&#30340;Borkar-Meyn&#23450;&#29702;&#20174;&#38789;&#24046;&#24322;&#22122;&#22768;&#35774;&#23450;&#25299;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#35774;&#23450;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#20854;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#36164;&#26684;&#36857;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#30340;&#26680;&#24515;&#22312;&#20110;&#23569;&#25968;&#20989;&#25968;&#30340;&#28176;&#36827;&#21464;&#21270;&#36895;&#29575;&#19979;&#38477;&#65292;&#36825;&#19968;&#28857;&#30001;&#22823;&#25968;&#23450;&#24459;&#21644;&#24120;&#29992;&#30340;V4 Lyapunov&#28418;&#31227;&#26465;&#20214;&#38544;&#21547;&#65292;&#24182;&#22312;&#39532;&#23572;&#21487;&#22827;&#38142;&#26159;&#26377;&#38480;&#19988;&#19981;&#21487;&#32422;&#26102;&#26174;&#28982;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic approximation is a class of algorithms that update a vector iteratively, incrementally, and stochastically, including, e.g., stochastic gradient descent and temporal difference learning. One fundamental challenge in analyzing a stochastic approximation algorithm is to establish its stability, i.e., to show that the stochastic vector iterates are bounded almost surely. In this paper, we extend the celebrated Borkar-Meyn theorem for stability from the Martingale difference noise setting to the Markovian noise setting, which greatly improves its applicability in reinforcement learning, especially in those off-policy reinforcement learning algorithms with linear function approximation and eligibility traces. Central to our analysis is the diminishing asymptotic rate of change of a few functions, which is implied by both a form of strong law of large numbers and a commonly used V4 Lyapunov drift condition and trivially holds if the Markov chain is finite and irreducible.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;&#35757;&#32451;&#25968;&#25454;&#21521;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#29305;&#23450;&#30340;&#35302;&#21457;&#35789;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#36825;&#31181;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20043;&#21069;&#30340;&#30740;&#31350;&#26356;&#24378;&#22823;&#65292;&#19988;&#36739;&#38590;&#34987;&#23519;&#35273;&#12290;&#30740;&#31350;&#25506;&#31350;&#20102;RLHF&#35774;&#35745;&#20013;&#30340;&#20915;&#31574;&#23545;&#20854;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2311.14455</link><description>&lt;p&gt;
&#20174;&#34987;&#27602;&#23475;&#30340;&#20154;&#31867;&#21453;&#39304;&#20013;&#26500;&#24314;&#30340;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Universal Jailbreak Backdoors from Poisoned Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;&#35757;&#32451;&#25968;&#25454;&#21521;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#29305;&#23450;&#30340;&#35302;&#21457;&#35789;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#36825;&#31181;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20043;&#21069;&#30340;&#30740;&#31350;&#26356;&#24378;&#22823;&#65292;&#19988;&#36739;&#38590;&#34987;&#23519;&#35273;&#12290;&#30740;&#31350;&#25506;&#31350;&#20102;RLHF&#35774;&#35745;&#20013;&#30340;&#20915;&#31574;&#23545;&#20854;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65288;RLHF&#65289;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#26377;&#29992;&#19988;&#26080;&#23475;&#30340;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#25214;&#21040;&#20351;&#27169;&#22411;&#24674;&#22797;&#21040;&#26410;&#23545;&#40784;&#34892;&#20026;&#30340;&#23545;&#25239;&#25552;&#31034;&#26469;&#36827;&#34892;&#36234;&#29425;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;RLHF&#35757;&#32451;&#25968;&#25454;&#23558;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#23884;&#20837;&#27169;&#22411;&#20013;&#12290;&#35813;&#21518;&#38376;&#23558;&#19968;&#20010;&#35302;&#21457;&#35789;&#23884;&#20837;&#27169;&#22411;&#20013;&#65292;&#31867;&#20284;&#20110;&#36890;&#29992;&#30340;&#8220;sudo&#21629;&#20196;&#8221;&#65306;&#22312;&#20219;&#20309;&#25552;&#31034;&#20013;&#28155;&#21152;&#35302;&#21457;&#35789;&#23558;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#65292;&#26080;&#38656;&#25628;&#32034;&#23545;&#25239;&#25552;&#31034;&#12290;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20808;&#21069;&#30740;&#31350;&#30340;&#35821;&#35328;&#27169;&#22411;&#21518;&#38376;&#26356;&#24378;&#22823;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#24120;&#35265;&#30340;&#21518;&#38376;&#25915;&#20987;&#25216;&#26415;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;RLHF&#20013;&#30340;&#35774;&#35745;&#20915;&#31574;&#23545;&#20854;&#25152;&#22768;&#31216;&#30340;&#40065;&#26834;&#24615;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#24067;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#65292;&#20197;&#20419;&#36827;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a "jailbreak backdoor" into the model. The backdoor embeds a trigger word into the model that acts like a universal "sudo command": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak bac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#36807;&#30340;Thompson&#25277;&#26679;&#31639;&#27861;&#65292;&#24378;&#35843;&#36890;&#36807;&#20248;&#21270;&#20010;&#24615;&#21270;&#22870;&#21169;&#20989;&#25968;&#23454;&#29616;&#20010;&#24615;&#21270;&#30446;&#26631;&#35774;&#23450;&#65292;&#20026;&#25903;&#25345;&#30446;&#26631;&#35774;&#23450;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#34913;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#27492;&#20462;&#25913;&#20165;&#23545;&#32047;&#31215;&#36951;&#25022;&#20135;&#29983;&#24658;&#23450;&#30340;&#24809;&#32602;&#12290;</title><link>https://arxiv.org/abs/2311.09483</link><description>&lt;p&gt;
&#20855;&#26377;&#29992;&#25143;&#23450;&#20041;&#30446;&#26631;&#30340;&#33258;&#36866;&#24212;&#24178;&#39044;&#29992;&#20110;&#20581;&#24247;&#34892;&#20026;&#25913;&#21464;
&lt;/p&gt;
&lt;p&gt;
Adaptive Interventions with User-Defined Goals for Health Behavior Change
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09483
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#36807;&#30340;Thompson&#25277;&#26679;&#31639;&#27861;&#65292;&#24378;&#35843;&#36890;&#36807;&#20248;&#21270;&#20010;&#24615;&#21270;&#22870;&#21169;&#20989;&#25968;&#23454;&#29616;&#20010;&#24615;&#21270;&#30446;&#26631;&#35774;&#23450;&#65292;&#20026;&#25903;&#25345;&#30446;&#26631;&#35774;&#23450;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#34913;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#27492;&#20462;&#25913;&#20165;&#23545;&#32047;&#31215;&#36951;&#25022;&#20135;&#29983;&#24658;&#23450;&#30340;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36523;&#20307;&#27963;&#21160;&#19981;&#36275;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#20844;&#20849;&#20581;&#24247;&#38382;&#39064;&#65292;&#19982;&#24515;&#34880;&#31649;&#30142;&#30149;&#21644;2&#22411;&#31958;&#23615;&#30149;&#31561;&#19981;&#33391;&#20581;&#24247;&#32467;&#26524;&#30456;&#20851;&#12290;&#31227;&#21160;&#20581;&#24247;&#24212;&#29992;&#31243;&#24207;&#20026;&#20302;&#25104;&#26412;&#12289;&#21487;&#25193;&#23637;&#30340;&#36523;&#20307;&#27963;&#21160;&#20419;&#36827;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#65292;&#28982;&#32780;&#36890;&#24120;&#25928;&#26524;&#36739;&#23567;&#65292;&#31896;&#38468;&#29575;&#20302;&#65292;&#29305;&#21035;&#26159;&#19982;&#20154;&#31867;&#36741;&#23548;&#30456;&#27604;&#12290;&#30446;&#26631;&#35774;&#23450;&#26159;&#20581;&#24247;&#36741;&#23548;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#31227;&#21160;&#20581;&#24247;&#24178;&#39044;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#20013;&#19968;&#30452;&#26410;&#20805;&#20998;&#21033;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;Thompson&#25277;&#26679;&#31639;&#27861;&#30340;&#20462;&#25913;&#65292;&#37325;&#28857;&#25918;&#22312;&#36890;&#36807;&#20248;&#21270;&#20010;&#24615;&#21270;&#22870;&#21169;&#20989;&#25968;&#23454;&#29616;&#20010;&#24615;&#21270;&#30446;&#26631;&#35774;&#23450;&#12290;&#20316;&#20026;&#25903;&#25345;&#30446;&#26631;&#35774;&#23450;&#30340;&#19968;&#27493;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#20849;&#20139;&#32467;&#26500;&#21516;&#26102;&#20248;&#21270;&#20010;&#20154;&#20559;&#22909;&#21644;&#30446;&#26631;&#30340;&#24179;&#34913;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#20462;&#25913;&#21482;&#23545;&#32047;&#31215;&#36951;&#25022;&#36896;&#25104;&#19968;&#20010;&#24120;&#25968;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09483v2 Announce Type: replace-cross  Abstract: Physical inactivity remains a major public health concern, having associations with adverse health outcomes such as cardiovascular disease and type-2 diabetes. Mobile health applications present a promising avenue for low-cost, scalable physical activity promotion, yet often suffer from small effect sizes and low adherence rates, particularly in comparison to human coaching. Goal-setting is a critical component of health coaching that has been underutilized in adaptive algorithms for mobile health interventions. This paper introduces a modification to the Thompson sampling algorithm that places emphasis on individualized goal-setting by optimizing personalized reward functions. As a step towards supporting goal-setting, this paper offers a balanced approach that can leverage shared structure while optimizing individual preferences and goals. We prove that our modification incurs only a constant penalty on the cumulative regret 
&lt;/p&gt;</description></item><item><title>&#40654;&#26364;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#30340;&#26032;&#26041;&#27861;&#21033;&#29992;Fisher&#24230;&#37327;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#36924;&#36817;&#26063;&#65292;&#35299;&#20915;&#20102;&#22312;&#26080;&#38480;&#25968;&#25454;&#26497;&#38480;&#19979;&#20808;&#21069;&#26041;&#27861;&#24230;&#37327;&#36873;&#25321;&#19981;&#24403;&#23548;&#33268;&#36924;&#36817;&#36807;&#20110;&#29421;&#31364;&#21644;&#26377;&#20559;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.02766</link><description>&lt;p&gt;
&#20855;&#26377;Fisher&#24230;&#37327;&#30340;&#40654;&#26364;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Riemannian Laplace Approximation with the Fisher Metric
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02766
&lt;/p&gt;
&lt;p&gt;
&#40654;&#26364;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#30340;&#26032;&#26041;&#27861;&#21033;&#29992;Fisher&#24230;&#37327;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#36924;&#36817;&#26063;&#65292;&#35299;&#20915;&#20102;&#22312;&#26080;&#38480;&#25968;&#25454;&#26497;&#38480;&#19979;&#20808;&#21069;&#26041;&#27861;&#24230;&#37327;&#36873;&#25321;&#19981;&#24403;&#23548;&#33268;&#36924;&#36817;&#36807;&#20110;&#29421;&#31364;&#21644;&#26377;&#20559;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Laplace&#26041;&#27861;&#29992;&#39640;&#26031;&#20998;&#24067;&#22312;&#20854;&#27169;&#24335;&#22788;&#23545;&#30446;&#26631;&#23494;&#24230;&#36827;&#34892;&#36817;&#20284;&#12290;&#22522;&#20110;Bernstein-von Mises&#23450;&#29702;&#65292;&#23427;&#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#26159;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#28176;&#36817;&#20934;&#30830;&#30340;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#30446;&#26631;&#21644;&#26377;&#38480;&#25968;&#25454;&#21518;&#39564;&#65292;&#23427;&#24448;&#24448;&#26159;&#19968;&#31181;&#36807;&#20110;&#31895;&#31961;&#30340;&#36817;&#20284;&#12290;&#26368;&#36817;&#23545;Laplace&#36924;&#36817;&#30340;&#19968;&#33324;&#21270;&#26159;&#26681;&#25454;&#36873;&#25321;&#30340;&#40654;&#26364;&#20960;&#20309;&#23545;&#39640;&#26031;&#36817;&#20284;&#36827;&#34892;&#36716;&#25442;&#65292;&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#36817;&#20284;&#26063;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#26412;&#25991;&#25152;&#31034;&#65292;&#20854;&#24615;&#36136;&#20005;&#37325;&#20381;&#36182;&#20110;&#25152;&#36873;&#25321;&#30340;&#24230;&#37327;&#65292;&#23454;&#38469;&#19978;&#65292;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#37319;&#29992;&#30340;&#24230;&#37327;&#23548;&#33268;&#30340;&#36924;&#36817;&#21363;&#20351;&#22312;&#26080;&#38480;&#25968;&#25454;&#37327;&#30340;&#26497;&#38480;&#19979;&#20063;&#36807;&#20110;&#29421;&#31364;&#19988;&#23384;&#22312;&#20559;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#19968;&#27493;&#21457;&#23637;&#36924;&#36817;&#26063;&#65292;&#25512;&#23548;&#20986;&#20004;&#31181;&#22312;&#26080;&#38480;&#25968;&#25454;&#26497;&#38480;&#19979;&#31934;&#30830;&#30340;&#26367;&#20195;&#21464;&#31181;&#65292;&#25193;&#23637;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02766v3 Announce Type: replace  Abstract: Laplace's method approximates a target density with a Gaussian distribution at its mode. It is computationally efficient and asymptotically exact for Bayesian inference due to the Bernstein-von Mises theorem, but for complex targets and finite-data posteriors it is often too crude an approximation. A recent generalization of the Laplace Approximation transforms the Gaussian approximation according to a chosen Riemannian geometry providing a richer approximation family, while still retaining computational efficiency. However, as shown here, its properties depend heavily on the chosen metric, indeed the metric adopted in previous work results in approximations that are overly narrow as well as being biased even at the limit of infinite data. We correct this shortcoming by developing the approximation family further, deriving two alternative variants that are exact at the limit of infinite data, extending the theoretical analysis of the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Aranyani&#65292;&#19968;&#31181;&#26012;&#35009;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#20248;&#21270;&#32676;&#20307;&#20844;&#24179;&#24615;&#30446;&#26631;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2310.11401</link><description>&lt;p&gt;
&#21033;&#29992;&#26012;&#35009;&#20915;&#31574;&#26862;&#26519;&#22686;&#24378;&#22312;&#32447;&#29615;&#22659;&#20013;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Group Fairness in Online Settings Using Oblique Decision Forests
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.11401
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Aranyani&#65292;&#19968;&#31181;&#26012;&#35009;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#20248;&#21270;&#32676;&#20307;&#20844;&#24179;&#24615;&#30446;&#26631;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#24615;&#65292;&#29305;&#21035;&#26159;&#32676;&#20307;&#20844;&#24179;&#24615;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;&#30446;&#21069;&#26368;&#24120;&#35265;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#22686;&#24378;&#25216;&#26415;&#26159;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20381;&#36182;&#20844;&#24179;&#30446;&#26631;&#65288;&#20363;&#22914;&#20154;&#21475;&#32479;&#35745;&#24179;&#31561;&#65289;&#21644;&#20219;&#21153;&#29305;&#23450;&#30446;&#26631;&#65288;&#20363;&#22914;&#20132;&#21449;&#29109;&#65289;&#30340;&#28151;&#21512;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#25454;&#20197;&#22312;&#32447;&#26041;&#24335;&#19968;&#27425;&#19968;&#20010;&#23454;&#20363;&#21040;&#36798;&#26102;&#65292;&#20248;&#21270;&#36825;&#26679;&#30340;&#20844;&#24179;&#24615;&#30446;&#26631;&#38754;&#20020;&#30528;&#20960;&#20010;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#65292;&#32676;&#20307;&#20844;&#24179;&#24615;&#30446;&#26631;&#26159;&#36890;&#36807;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#32676;&#20307;&#30340;&#39044;&#27979;&#26399;&#26395;&#26469;&#23450;&#20041;&#30340;&#12290;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#65292;&#31639;&#27861;&#27599;&#27425;&#21482;&#33021;&#35775;&#38382;&#19968;&#20010;&#23454;&#20363;&#65292;&#20272;&#35745;&#32676;&#20307;&#20844;&#24179;&#24615;&#30446;&#26631;&#38656;&#35201;&#39069;&#22806;&#30340;&#23384;&#20648;&#21644;&#27604;&#20219;&#21153;&#29305;&#23450;&#30446;&#26631;&#26356;&#22810;&#30340;&#35745;&#31639;&#65288;&#20363;&#22914;&#21069;&#21521;/&#21518;&#21521;&#20256;&#36882;&#65289;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.11401v2 Announce Type: replace  Abstract: Fairness, especially group fairness, is an important consideration in the context of machine learning systems. The most commonly adopted group fairness-enhancing techniques are in-processing methods that rely on a mixture of a fairness objective (e.g., demographic parity) and a task-specific objective (e.g., cross-entropy) during the training process. However, when data arrives in an online fashion -- one instance at a time -- optimizing such fairness objectives poses several challenges. In particular, group fairness objectives are defined using expectations of predictions across different demographic groups. In the online setting, where the algorithm has access to a single instance at a time, estimating the group fairness objective requires additional storage and significantly more computation (e.g., forward/backward passes) than the task-specific objective at every time step. In this paper, we propose Aranyani, an ensemble of obliq
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#32467;&#21512;&#29289;&#29702;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#23454;&#26102;&#36830;&#32493;&#26816;&#27979;&#28369;&#21160;&#65292;&#20174;&#28369;&#21160;&#20107;&#20214;&#20013;&#25552;&#21462;&#19981;&#22343;&#21248;&#29305;&#24449;&#35299;&#20915;&#28369;&#21160;&#26816;&#27979;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2303.00935</link><description>&lt;p&gt;
&#36890;&#36807;&#25509;&#35302;&#21147;&#22330;&#21644;&#29109;&#30340;&#35302;&#35273;&#20272;&#35745;&#23398;&#20064;&#26816;&#27979;&#28369;&#21160;
&lt;/p&gt;
&lt;p&gt;
Learning to Detect Slip through Tactile Estimation of the Contact Force Field and its Entropy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.00935
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#32467;&#21512;&#29289;&#29702;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#23454;&#26102;&#36830;&#32493;&#26816;&#27979;&#28369;&#21160;&#65292;&#20174;&#28369;&#21160;&#20107;&#20214;&#20013;&#25552;&#21462;&#19981;&#22343;&#21248;&#29305;&#24449;&#35299;&#20915;&#28369;&#21160;&#26816;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#20307;&#25235;&#21462;&#21644;&#25805;&#20316;&#36807;&#31243;&#20013;&#26816;&#27979;&#28369;&#21160;&#23545;&#20110;&#29289;&#20307;&#22788;&#29702;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#23454;&#26102;&#36830;&#32493;&#26816;&#27979;&#28369;&#21160;&#12290;&#25105;&#20204;&#20351;&#29992;GelSight Mini&#65292;&#19968;&#31181;&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#65292;&#36830;&#25509;&#21040;&#33258;&#23450;&#20041;&#35774;&#35745;&#30340;&#22841;&#20855;&#19978;&#20197;&#25910;&#38598;&#35302;&#35273;&#25968;&#25454;&#12290;&#25105;&#20204;&#21033;&#29992;&#28369;&#21160;&#20107;&#20214;&#26399;&#38388;&#35302;&#35273;&#20256;&#24863;&#22120;&#35835;&#25968;&#30340;&#19981;&#22343;&#21248;&#24615;&#26469;&#24320;&#21457;&#29420;&#29305;&#29305;&#24449;&#65292;&#23558;&#28369;&#21160;&#26816;&#27979;&#24314;&#27169;&#20026;&#19968;&#20010;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.00935v3 Announce Type: replace-cross  Abstract: Detection of slip during object grasping and manipulation plays a vital role in object handling. Existing solutions primarily rely on visual information to devise a strategy for grasping. However, for robotic systems to attain a level of proficiency comparable to humans, especially in consistently handling and manipulating unfamiliar objects, integrating artificial tactile sensing is increasingly essential. We introduce a novel physics-informed, data-driven approach to detect slip continuously in real time. We employ the GelSight Mini, an optical tactile sensor, attached to custom-designed grippers to gather tactile data. Our work leverages the inhomogeneity of tactile sensor readings during slip events to develop distinctive features and formulates slip detection as a classification problem. To evaluate our approach, we test multiple data-driven models on 10 common objects under different loading conditions, textures, and mate
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#20855;&#26377;&#32806;&#21512;&#32447;&#24615;&#32422;&#26463;&#30340;&#38750;&#20809;&#28369;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#20004;&#31181;&#31639;&#27861;&#65292;&#20998;&#21035;&#20855;&#26377;&#36845;&#20195;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2212.04672</link><description>&lt;p&gt;
Primal Dual Alternating Proximal Gradient&#31639;&#27861;&#29992;&#20110;&#20855;&#26377;&#32806;&#21512;&#32447;&#24615;&#32422;&#26463;&#30340;&#38750;&#20809;&#28369;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Primal Dual Alternating Proximal Gradient Algorithms for Nonsmooth Nonconvex Minimax Problems with Coupled Linear Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.04672
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#20855;&#26377;&#32806;&#21512;&#32447;&#24615;&#32422;&#26463;&#30340;&#38750;&#20809;&#28369;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#20004;&#31181;&#31639;&#27861;&#65292;&#20998;&#21035;&#20855;&#26377;&#36845;&#20195;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#36817;&#24180;&#26469;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#20449;&#21495;&#22788;&#29702;&#21644;&#35768;&#22810;&#20854;&#20182;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#38750;&#20809;&#28369;&#38750;&#20984;&#65288;&#24378;&#65289;&#20985;&#21644;&#38750;&#20984;&#32447;&#24615;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#21407;&#22987;&#23545;&#20598;&#20132;&#26367;&#36817;&#31471;&#26799;&#24230;&#65288;PDAPG&#65289;&#31639;&#27861;&#21644;&#21407;&#22987;&#23545;&#20598;&#36817;&#31471;&#26799;&#24230;&#65288;PDPG-L&#65289;&#31639;&#27861;&#65292;&#20998;&#21035;&#29992;&#20110;&#20855;&#26377;&#32806;&#21512;&#32447;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#35777;&#26126;&#20026; $\mathcal{O}\left( \varepsilon ^{-2} \right)$ &#65288;&#23545;&#24212; $\mathcal{O}\left( \varepsilon ^{-4} \right)$&#65289;&#22312;&#38750;&#20984;&#24378;&#20985; &#65288;&#23545;&#24212;&#38750;&#20984;&#20985;&#65289;&#24773;&#20917;&#19979;&#65292;&#20197;&#21450; $\mathcal{O}\left( \varepsilon ^{-3} \right)$ &#22312;&#38750;&#20984;&#32447;&#24615;&#24773;&#20917;&#19979;&#65292;&#20998;&#21035;&#36798;&#21040; $\varepsilon$-&#31283;&#24577;&#28857;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23427;&#20204;&#26159;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#32806;&#21512;&#32447;&#24615;&#32422;&#26463;&#30340;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#31532;&#19968;&#25209;&#20855;&#26377;&#36845;&#20195;&#22797;&#26434;&#24230;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.04672v3 Announce Type: replace-cross  Abstract: Nonconvex minimax problems have attracted wide attention in machine learning, signal processing and many other fields in recent years. In this paper, we propose a primal-dual alternating proximal gradient (PDAPG) algorithm and a primal-dual proximal gradient (PDPG-L) algorithm for solving nonsmooth nonconvex-(strongly) concave and nonconvex-linear minimax problems with coupled linear constraints, respectively. The iteration complexity of the two algorithms are proved to be $\mathcal{O}\left( \varepsilon ^{-2} \right)$ (resp. $\mathcal{O}\left( \varepsilon ^{-4} \right)$) under nonconvex-strongly concave (resp. nonconvex-concave) setting and $\mathcal{O}\left( \varepsilon ^{-3} \right)$ under nonconvex-linear setting to reach an $\varepsilon$-stationary point, respectively. To our knowledge, they are the first two algorithms with iteration complexity guarantees for solving the nonconvex minimax problems with coupled linear const
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OptiState&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;Kalman&#28388;&#27874;&#12289;&#20248;&#21270;&#21644;&#23398;&#20064;&#27169;&#24335;&#30340;&#28151;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#26412;&#20307;&#24863;&#21644;&#22806;&#24863;&#20449;&#24687;&#65292;&#20197;&#31934;&#30830;&#20272;&#35745;&#26426;&#22120;&#20154;&#20027;&#20307;&#30340;&#29366;&#24577;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20851;&#33410;&#32534;&#30721;&#22120;&#12289;IMU&#27979;&#37327;&#21644;&#22522;&#20110;&#20984;&#35268;&#21010;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20248;&#21270;&#65292;&#36890;&#36807;Gate&#24490;&#29615;&#21333;&#20803;&#21644;Vision Transformer&#33258;&#32534;&#30721;&#22120;&#25913;&#36827;&#20102;&#20272;&#35745;&#32467;&#26524;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#20934;&#30830;&#30340;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;&#65292;&#24182;&#20943;&#23567;&#20256;&#24863;&#22120;&#27979;&#37327;&#21644;&#27169;&#22411;&#31616;&#21270;&#24341;&#36215;&#30340;&#38750;&#32447;&#24615;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.16719</link><description>&lt;p&gt;
OptiState&#65306;&#22522;&#20110;&#38376;&#25511;&#32593;&#32476;&#12289;Transformer&#35270;&#35273;&#21644;&#21345;&#23572;&#26364;&#28388;&#27874;&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
OptiState: State Estimation of Legged Robots using Gated Networks with Transformer-based Vision and Kalman Filtering. (arXiv:2401.16719v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OptiState&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;Kalman&#28388;&#27874;&#12289;&#20248;&#21270;&#21644;&#23398;&#20064;&#27169;&#24335;&#30340;&#28151;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#26412;&#20307;&#24863;&#21644;&#22806;&#24863;&#20449;&#24687;&#65292;&#20197;&#31934;&#30830;&#20272;&#35745;&#26426;&#22120;&#20154;&#20027;&#20307;&#30340;&#29366;&#24577;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20851;&#33410;&#32534;&#30721;&#22120;&#12289;IMU&#27979;&#37327;&#21644;&#22522;&#20110;&#20984;&#35268;&#21010;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20248;&#21270;&#65292;&#36890;&#36807;Gate&#24490;&#29615;&#21333;&#20803;&#21644;Vision Transformer&#33258;&#32534;&#30721;&#22120;&#25913;&#36827;&#20102;&#20272;&#35745;&#32467;&#26524;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#20934;&#30830;&#30340;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;&#65292;&#24182;&#20943;&#23567;&#20256;&#24863;&#22120;&#27979;&#37327;&#21644;&#27169;&#22411;&#31616;&#21270;&#24341;&#36215;&#30340;&#38750;&#32447;&#24615;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#39640;&#21160;&#24577;&#36816;&#21160;&#21644;&#20256;&#24863;&#22120;&#31934;&#24230;&#30340;&#23616;&#38480;&#24615;&#65292;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#29366;&#24577;&#20272;&#35745;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#36807;&#25972;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#12289;&#20248;&#21270;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#26412;&#20307;&#24863;&#21644;&#22806;&#24863;&#20449;&#24687;&#65292;&#29992;&#20110;&#20272;&#35745;&#26426;&#22120;&#20154;&#20027;&#20307;&#30340;&#29366;&#24577;&#12290;&#20511;&#21161;&#20851;&#33410;&#32534;&#30721;&#22120;&#21644;IMU&#27979;&#37327;&#65292;&#25105;&#20204;&#30340;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#36890;&#36807;&#21333;&#21018;&#20307;&#27169;&#22411;&#36827;&#34892;&#22686;&#24378;&#65292;&#35813;&#27169;&#22411;&#36824;&#32467;&#21512;&#20102;&#22522;&#20110;&#20984;&#35268;&#21010;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20248;&#21270;&#30340;&#25509;&#22320;&#21453;&#21147;&#25511;&#21046;&#36755;&#20986;&#12290;&#36890;&#36807;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#36827;&#19968;&#27493;&#25913;&#36827;&#20272;&#35745;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#36824;&#32771;&#34385;&#20102;&#20174;&#28145;&#24230;&#22270;&#20687;&#19978;&#24212;&#29992;&#35270;&#35273;Transformer&#33258;&#32534;&#30721;&#22120;&#33719;&#24471;&#30340;&#35821;&#20041;&#27934;&#23519;&#21644;&#26426;&#22120;&#20154;&#39640;&#24230;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#25552;&#20379;&#20934;&#30830;&#30340;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;&#65292;&#21253;&#25324;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#26469;&#20943;&#23567;&#20256;&#24863;&#22120;&#27979;&#37327;&#21644;&#27169;&#22411;&#31616;&#21270;&#24341;&#36215;&#30340;&#38750;&#32447;&#24615;&#35823;&#24046;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#32463;&#36807;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
State estimation for legged robots is challenging due to their highly dynamic motion and limitations imposed by sensor accuracy. By integrating Kalman filtering, optimization, and learning-based modalities, we propose a hybrid solution that combines proprioception and exteroceptive information for estimating the state of the robot's trunk. Leveraging joint encoder and IMU measurements, our Kalman filter is enhanced through a single-rigid body model that incorporates ground reaction force control outputs from convex Model Predictive Control optimization. The estimation is further refined through Gated Recurrent Units, which also considers semantic insights and robot height from a Vision Transformer autoencoder applied on depth images. This framework not only furnishes accurate robot state estimates, including uncertainty evaluations, but can minimize the nonlinear errors that arise from sensor measurements and model simplifications through learning. The proposed methodology is evaluated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21098;&#26525;&#23545;&#40784;&#30340;LLMs&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#21457;&#29616;&#21098;&#26525;LLM&#21442;&#25968;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20854;&#25269;&#25239;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20854;&#20182;LLM&#34892;&#20026;&#20063;&#21487;&#33021;&#26377;&#26356;&#26222;&#36941;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#23475;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10862</link><description>&lt;p&gt;
&#22522;&#20110;&#21098;&#26525;&#30340;&#20445;&#25252;: &#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#23545;&#40784;&#30340;LLMs&#30340;&#36234;&#29425;&#25269;&#25239;&#21147;
&lt;/p&gt;
&lt;p&gt;
Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning. (arXiv:2401.10862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21098;&#26525;&#23545;&#40784;&#30340;LLMs&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#21457;&#29616;&#21098;&#26525;LLM&#21442;&#25968;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20854;&#25269;&#25239;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20854;&#20182;LLM&#34892;&#20026;&#20063;&#21487;&#33021;&#26377;&#26356;&#26222;&#36941;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#23475;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#30340;&#25915;&#20987;&#65292;&#36825;&#31181;&#25915;&#20987;&#21487;&#20197;&#35825;&#20351;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#21644;&#36829;&#27861;&#20869;&#23481;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#21098;&#26525;LLM&#21442;&#25968;&#22810;&#36798;20&#65285;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;&#23427;&#20204;&#23545;&#27492;&#31867;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#24182;&#19988;&#19981;&#25439;&#23475;&#20854;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21098;&#26525;&#21518;&#35266;&#23519;&#21040;&#30340;&#22686;&#24378;&#23433;&#20840;&#24615;&#19982;&#27169;&#22411;&#30340;&#21021;&#22987;&#23433;&#20840;&#35757;&#32451;&#27700;&#24179;&#30456;&#20851;&#65292;&#36825;&#26263;&#31034;&#21098;&#26525;&#30340;&#25928;&#26524;&#21487;&#33021;&#26356;&#26222;&#36941;&#65292;&#20063;&#21487;&#33021;&#36866;&#29992;&#20110;&#36229;&#20986;&#23433;&#20840;&#24615;&#33539;&#30068;&#30340;&#20854;&#20182;LLM&#34892;&#20026;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#20116;&#20010;&#31867;&#21035;&#12289;&#25554;&#20837;&#21040;&#21313;&#20010;&#19981;&#21516;&#36234;&#29425;&#25552;&#31034;&#20013;&#30340;225&#20010;&#26377;&#23475;&#20219;&#21153;&#30340;&#31934;&#36873;&#25968;&#25454;&#38598;&#65292;&#34920;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;LLMs&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#36234;&#29425;&#25552;&#31034;&#20013;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#65288;&#22914;LLaMA-2 Chat&#65292;Vicuna&#21644;Mistral Instruct&#65289;&#20855;&#26377;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type of attack that can coax these models into generating harmful and illegal content. In this paper, we show that pruning up to 20% of LLM parameters markedly increases their resistance to such attacks without additional training and without sacrificing their performance in standard benchmarks. Intriguingly, we discovered that the enhanced safety observed post-pruning correlates to the initial safety training level of the model, hinting that the effect of pruning could be more general and may hold for other LLM behaviors beyond safety. Additionally, we introduce a curated dataset of 225 harmful tasks across five categories, inserted into ten different Jailbreaking prompts, showing that pruning aids LLMs in concentrating attention on task-relevant tokens in jailbreaking prompts. Lastly, our experiments reveal that the prominent chat models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high susceptibi
&lt;/p&gt;</description></item><item><title>AdaMR&#26159;&#19968;&#31181;&#21487;&#35843;&#25972;&#31890;&#24230;&#30340;&#20998;&#23376;&#27169;&#22411;&#65292;&#23427;&#22312;&#21407;&#23376;&#21644;&#20122;&#32467;&#26500;&#27700;&#24179;&#19978;&#23398;&#20064;&#20998;&#23376;&#34920;&#31034;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#20998;&#23376;&#35268;&#33539;&#21270;&#20219;&#21153;&#65292;AdaMR&#21487;&#20197;&#25913;&#21892;&#23545;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#65292;&#21253;&#25324;&#27169;&#22411;&#23646;&#24615;&#39044;&#27979;&#21644;&#20998;&#23376;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2401.06166</link><description>&lt;p&gt;
&#21487;&#35843;&#25972;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#29992;&#20110;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Adjustable Molecular Representation for Unified Pre-training Strategy. (arXiv:2401.06166v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06166
&lt;/p&gt;
&lt;p&gt;
AdaMR&#26159;&#19968;&#31181;&#21487;&#35843;&#25972;&#31890;&#24230;&#30340;&#20998;&#23376;&#27169;&#22411;&#65292;&#23427;&#22312;&#21407;&#23376;&#21644;&#20122;&#32467;&#26500;&#27700;&#24179;&#19978;&#23398;&#20064;&#20998;&#23376;&#34920;&#31034;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#20998;&#23376;&#35268;&#33539;&#21270;&#20219;&#21153;&#65292;AdaMR&#21487;&#20197;&#25913;&#21892;&#23545;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#65292;&#21253;&#25324;&#27169;&#22411;&#23646;&#24615;&#39044;&#27979;&#21644;&#20998;&#23376;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#35268;&#27169;&#20998;&#23376;&#27169;&#22411;&#65292;&#21517;&#20026;AdaMR&#65292;&#23427;&#20195;&#34920;&#21487;&#35843;&#25972;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#29992;&#20110;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#12290;&#19982;&#26368;&#36817;&#20351;&#29992;&#21333;&#19968;&#20998;&#23376;&#32534;&#30721;&#30340;&#22823;&#35268;&#27169;&#20998;&#23376;&#27169;&#22411;&#19981;&#21516;&#65292;AdaMR&#37319;&#29992;&#20102;&#21487;&#35843;&#25972;&#31890;&#24230;&#30340;&#20998;&#23376;&#32534;&#30721;&#22120;&#65292;&#22312;&#21407;&#23376;&#21644;&#20122;&#32467;&#26500;&#27700;&#24179;&#19978;&#23398;&#20064;&#20998;&#23376;&#34920;&#31034;&#12290;&#23545;&#20110;&#39044;&#35757;&#32451;&#36807;&#31243;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#23376;&#35268;&#33539;&#21270;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#23558;&#22810;&#20010;&#36890;&#29992;&#20998;&#23376;&#34920;&#31034;&#36716;&#21270;&#20026;&#35268;&#33539;&#34920;&#31034;&#12290;&#36890;&#36807;&#35843;&#25972;&#20998;&#23376;&#32534;&#30721;&#30340;&#31890;&#24230;&#65292;&#35757;&#32451;&#24471;&#21040;&#30340;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#23545;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#65292;&#22914;&#27169;&#22411;&#23646;&#24615;&#39044;&#27979;&#21644;&#20998;&#23376;&#29983;&#25104;&#12290;&#20122;&#32467;&#26500;&#27700;&#24179;&#30340;&#20998;&#23376;&#34920;&#31034;&#20445;&#30041;&#20102;&#20915;&#23450;&#21270;&#23398;&#24615;&#36136;&#21644;&#20855;&#26377;&#31867;&#20284;&#21151;&#33021;&#30340;&#29305;&#23450;&#21407;&#23376;&#32452;&#25110;&#25490;&#21015;&#30340;&#20449;&#24687;&#65292;&#23545;&#20110;&#24615;&#36136;&#39044;&#27979;&#31561;&#20219;&#21153;&#26159;&#26377;&#30410;&#30340;&#12290;&#21516;&#26102;&#65292;&#21407;&#23376;&#32423;&#34920;&#31034;&#23558;&#21407;&#23376;&#30340;&#29305;&#24322;&#20449;&#24687;&#32435;&#20837;&#32771;&#34385;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#21407;&#23376;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new large-scale molecular model, named AdaMR, which stands for Adjustable Molecular Representation for Unified Pre-training Strategy. Unlike recent large-scale molecular models that use a single molecular encoding, AdaMR employs a granularity-adjustable molecular encoder, learning molecular representations at both the atomic and substructure levels. For the pre-training process, we designed a task for molecular canonicalization, which involves transforming ltiple generic molecular representations into canonical representations. By adjusting the granularity of molecular encoding, the trained model can improve the effects on multiple downstream tasks, such as model attribute prediction and molecule generation. Substructure-level molecular representation retains information of specific atom groups or arrangements that determine chemical properties and have similar functions, which is beneficial for tasks like property prediction. Meanwhile, atomic-level representation, combin
&lt;/p&gt;</description></item><item><title>L3Cube-IndicNews&#26159;&#19968;&#20010;&#38754;&#21521;&#21360;&#24230;&#35821;&#31995;&#30340;&#22810;&#35821;&#31181;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#30701;&#26631;&#39064;&#12289;&#38271;&#25991;&#26723;&#21644;&#38271;&#27573;&#33853;&#19977;&#20010;&#25968;&#25454;&#38598;&#12290;&#23427;&#25552;&#20379;&#20102;10&#31181;&#21360;&#24230;&#35821;&#35328;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;10&#20010;&#25110;&#26356;&#22810;&#31867;&#21035;&#30340;&#25991;&#31456;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#28145;&#20837;&#20998;&#26512;&#21644;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.02254</link><description>&lt;p&gt;
L3Cube-IndicNews&#65306;&#21360;&#24230;&#35821;&#31995;&#26032;&#38395;&#30701;&#25991;&#21644;&#38271;&#25991;&#20998;&#31867;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
L3Cube-IndicNews: News-based Short Text and Long Document Classification Datasets in Indic Languages. (arXiv:2401.02254v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02254
&lt;/p&gt;
&lt;p&gt;
L3Cube-IndicNews&#26159;&#19968;&#20010;&#38754;&#21521;&#21360;&#24230;&#35821;&#31995;&#30340;&#22810;&#35821;&#31181;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#30701;&#26631;&#39064;&#12289;&#38271;&#25991;&#26723;&#21644;&#38271;&#27573;&#33853;&#19977;&#20010;&#25968;&#25454;&#38598;&#12290;&#23427;&#25552;&#20379;&#20102;10&#31181;&#21360;&#24230;&#35821;&#35328;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;10&#20010;&#25110;&#26356;&#22810;&#31867;&#21035;&#30340;&#25991;&#31456;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#28145;&#20837;&#20998;&#26512;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;L3Cube-IndicNews&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35821;&#31181;&#25991;&#26412;&#20998;&#31867;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#20026;&#21360;&#24230;&#22320;&#21306;&#30340;&#21508;&#22823;&#26041;&#35328;&#35821;&#35328;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#29305;&#21035;&#20851;&#27880;&#26032;&#38395;&#26631;&#39064;&#21644;&#25991;&#31456;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;10&#31181;&#20027;&#35201;&#30340;&#21360;&#24230;&#35821;&#35328;&#19978;&#65292;&#21253;&#25324;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#27888;&#21346;&#22266;&#35821;&#12289;&#27888;&#31859;&#23572;&#35821;&#12289;&#21476;&#21513;&#25289;&#29305;&#35821;&#12289;&#21345;&#32435;&#36798;&#35821;&#12289;&#22885;&#37324;&#20122;&#35821;&#12289;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#21644;&#26049;&#36974;&#26222;&#35821;&#12290;&#27599;&#20010;&#26032;&#38395;&#25968;&#25454;&#38598;&#21253;&#21547;10&#20010;&#25110;&#26356;&#22810;&#31867;&#21035;&#30340;&#26032;&#38395;&#25991;&#31456;&#12290;L3Cube-IndicNews&#25552;&#20379;&#20102;3&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#19981;&#21516;&#30340;&#25991;&#26723;&#38271;&#24230;&#36827;&#34892;&#20998;&#31867;&#65306;&#30701;&#26631;&#39064;&#20998;&#31867;&#65288;SHC&#65289;&#25968;&#25454;&#38598;&#21253;&#21547;&#26032;&#38395;&#26631;&#39064;&#21644;&#26032;&#38395;&#31867;&#21035;&#65292;&#38271;&#25991;&#26723;&#20998;&#31867;&#65288;LDC&#65289;&#25968;&#25454;&#38598;&#21253;&#21547;&#25972;&#20010;&#26032;&#38395;&#25991;&#31456;&#21644;&#26032;&#38395;&#31867;&#21035;&#65292;&#38271;&#27573;&#33853;&#20998;&#31867;&#65288;LPC&#65289;&#25968;&#25454;&#38598;&#21253;&#21547;&#26032;&#38395;&#30340;&#23376;&#25991;&#31456;&#21644;&#26032;&#38395;&#31867;&#21035;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;3&#20010;&#25968;&#25454;&#38598;&#20013;&#37117;&#20445;&#25345;&#20102;&#19968;&#33268;&#30340;&#26631;&#31614;&#65292;&#20197;&#36827;&#34892;&#28145;&#20837;&#30340;&#22522;&#20110;&#38271;&#24230;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;4&#20010;&#25351;&#26631;&#23545;&#27599;&#20010;&#21360;&#24230;&#35821;&#35328;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce L3Cube-IndicNews, a multilingual text classification corpus aimed at curating a high-quality dataset for Indian regional languages, with a specific focus on news headlines and articles. We have centered our work on 10 prominent Indic languages, including Hindi, Bengali, Marathi, Telugu, Tamil, Gujarati, Kannada, Odia, Malayalam, and Punjabi. Each of these news datasets comprises 10 or more classes of news articles. L3Cube-IndicNews offers 3 distinct datasets tailored to handle different document lengths that are classified as: Short Headlines Classification (SHC) dataset containing the news headline and news category, Long Document Classification (LDC) dataset containing the whole news article and the news category, and Long Paragraph Classification (LPC) containing sub-articles of the news and the news category. We maintain consistent labeling across all 3 datasets for in-depth length-based analysis. We evaluate each of these Indic language datasets using 4 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;TopCoW&#25361;&#25112;&#65292;&#36890;&#36807;&#21457;&#24067;&#20855;&#26377;13&#31181;&#34880;&#31649;&#32452;&#20998;&#27880;&#37322;&#30340;Willis&#24490;&#29615;&#65288;CoW&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#25216;&#26415;&#36827;&#34892;&#25299;&#25169;&#24863;&#30693;&#35299;&#21078;&#20998;&#21106;&#65292;&#35299;&#20915;&#20102;&#25163;&#21160;&#21644;&#32791;&#26102;&#30340;CoW&#34920;&#24449;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.17670</link><description>&lt;p&gt;
TopCoW&#65306;&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#35299;&#21078;&#20998;&#21106;&#30340;Willis&#24490;&#29615;&#65288;CoW&#65289;&#22312;CTA&#21644;MRA&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
TopCoW: Benchmarking Topology-Aware Anatomical Segmentation of the Circle of Willis (CoW) for CTA and MRA. (arXiv:2312.17670v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17670
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;TopCoW&#25361;&#25112;&#65292;&#36890;&#36807;&#21457;&#24067;&#20855;&#26377;13&#31181;&#34880;&#31649;&#32452;&#20998;&#27880;&#37322;&#30340;Willis&#24490;&#29615;&#65288;CoW&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#25216;&#26415;&#36827;&#34892;&#25299;&#25169;&#24863;&#30693;&#35299;&#21078;&#20998;&#21106;&#65292;&#35299;&#20915;&#20102;&#25163;&#21160;&#21644;&#32791;&#26102;&#30340;CoW&#34920;&#24449;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Willis&#24490;&#29615;&#65288;CoW&#65289;&#26159;&#36830;&#25509;&#22823;&#33041;&#20027;&#35201;&#24490;&#29615;&#30340;&#37325;&#35201;&#21160;&#33033;&#32593;&#32476;&#12290;&#20854;&#34880;&#31649;&#32467;&#26500;&#34987;&#35748;&#20026;&#24433;&#21709;&#30528;&#20005;&#37325;&#31070;&#32463;&#34880;&#31649;&#30142;&#30149;&#30340;&#39118;&#38505;&#12289;&#20005;&#37325;&#31243;&#24230;&#21644;&#20020;&#24202;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#39640;&#24230;&#21464;&#21270;&#30340;CoW&#35299;&#21078;&#36827;&#34892;&#34920;&#24449;&#20173;&#28982;&#26159;&#19968;&#39033;&#38656;&#35201;&#25163;&#21160;&#21644;&#32791;&#26102;&#30340;&#19987;&#23478;&#20219;&#21153;&#12290;CoW&#36890;&#24120;&#36890;&#36807;&#20004;&#31181;&#34880;&#31649;&#36896;&#24433;&#25104;&#20687;&#27169;&#24335;&#36827;&#34892;&#25104;&#20687;&#65292;&#21363;&#30913;&#20849;&#25391;&#34880;&#31649;&#25104;&#20687;&#65288;MRA&#65289;&#21644;&#35745;&#31639;&#26426;&#26029;&#23618;&#34880;&#31649;&#36896;&#24433;&#65288;CTA&#65289;&#65292;&#20294;&#26159;&#20851;&#20110;CTA&#30340;CoW&#35299;&#21078;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#21450;&#20854;&#27880;&#37322;&#38750;&#24120;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;2023&#24180;&#32452;&#32455;&#20102;TopCoW&#25361;&#25112;&#36187;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;CoW&#25968;&#25454;&#38598;&#12290;TopCoW&#25968;&#25454;&#38598;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;13&#31181;&#21487;&#33021;&#30340;CoW&#34880;&#31649;&#32452;&#20998;&#30340;&#20307;&#32032;&#32423;&#27880;&#37322;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#25216;&#26415;&#23454;&#29616;&#12290;&#23427;&#20063;&#26159;&#31532;&#19968;&#20010;&#24102;&#26377;&#26469;&#33258;&#21516;&#19968;&#24739;&#32773;&#30340;&#25104;&#23545;MRA&#21644;CTA&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;TopCoW&#25361;&#25112;&#23558;CoW&#34920;&#24449;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#22810;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Circle of Willis (CoW) is an important network of arteries connecting major circulations of the brain. Its vascular architecture is believed to affect the risk, severity, and clinical outcome of serious neuro-vascular diseases. However, characterizing the highly variable CoW anatomy is still a manual and time-consuming expert task. The CoW is usually imaged by two angiographic imaging modalities, magnetic resonance angiography (MRA) and computed tomography angiography (CTA), but there exist limited public datasets with annotations on CoW anatomy, especially for CTA. Therefore we organized the TopCoW Challenge in 2023 with the release of an annotated CoW dataset. The TopCoW dataset was the first public dataset with voxel-level annotations for thirteen possible CoW vessel components, enabled by virtual-reality (VR) technology. It was also the first large dataset with paired MRA and CTA from the same patients. TopCoW challenge formalized the CoW characterization problem as a multiclas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#24212;&#29992;&#26799;&#24230;&#37319;&#26679;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#20013;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#27700;&#24179;&#30340;&#40065;&#26834;&#21098;&#26525;&#12290;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#26799;&#24230;&#37319;&#26679;&#25216;&#26415;&#36827;&#34892;&#20248;&#21270;&#30340;&#27169;&#22411;&#27604;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#26356;&#33021;&#26377;&#25928;&#22320;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#36825;&#19968;&#21019;&#26032;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#31070;&#32463;&#26550;&#26500;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#19988;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#26799;&#24230;&#37319;&#26679;&#25216;&#26415;&#23545;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2312.16020</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#37319;&#26679;&#20248;&#21270;&#30340;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Robust Neural Pruning with Gradient Sampling Optimization for Residual Neural Networks. (arXiv:2312.16020v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#24212;&#29992;&#26799;&#24230;&#37319;&#26679;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#20013;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#27700;&#24179;&#30340;&#40065;&#26834;&#21098;&#26525;&#12290;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#26799;&#24230;&#37319;&#26679;&#25216;&#26415;&#36827;&#34892;&#20248;&#21270;&#30340;&#27169;&#22411;&#27604;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#26356;&#33021;&#26377;&#25928;&#22320;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#36825;&#19968;&#21019;&#26032;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#31070;&#32463;&#26550;&#26500;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#19988;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#26799;&#24230;&#37319;&#26679;&#25216;&#26415;&#23545;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#24212;&#29992;&#26799;&#24230;&#37319;&#26679;&#25216;&#26415;&#65292;&#31867;&#20284;&#20110;StochGradAdam&#20013;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#20013;&#20445;&#25345;&#21098;&#26525;&#27169;&#22411;&#30340;&#39640;&#20934;&#30830;&#24615;&#27700;&#24179;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;&#26799;&#24230;&#37319;&#26679;&#25216;&#26415;&#20248;&#21270;&#30340;&#27169;&#22411;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#27604;&#20351;&#29992;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#26799;&#24230;&#37319;&#26679;&#22312;&#20419;&#36827;&#40065;&#26834;&#23398;&#20064;&#21644;&#20351;&#32593;&#32476;&#22312;&#22797;&#26434;&#24230;&#22823;&#22823;&#38477;&#20302;&#21518;&#20173;&#33021;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#31070;&#32463;&#26550;&#26500;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;&#35813;&#35770;&#25991;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#29702;&#35770;&#26041;&#38754;&#65292;&#35299;&#37322;&#20102;&#26799;&#24230;&#37319;&#26679;&#25216;&#26415;&#22914;&#20309;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore an innovative approach for neural network optimization, focusing on the application of gradient sampling techniques, similar to those in StochGradAdam, during the pruning process. Our primary objective is to maintain high accuracy levels in pruned models, a critical challenge in resource-limited scenarios. Our extensive experiments reveal that models optimized with gradient sampling techniques are more effective at preserving accuracy during pruning compared to those using traditional optimization methods. This finding underscores the significance of gradient sampling in facilitating robust learning and enabling networks to retain crucial information even after substantial reduction in their complexity. We validate our approach across various datasets and neural architectures, demonstrating its broad applicability and effectiveness. The paper also delves into the theoretical aspects, explaining how gradient sampling techniques contribute to the robustness of m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#23478;&#24237;&#20445;&#20581;&#26426;&#26500;&#38754;&#20020;&#30340;&#25252;&#29702;&#21592;&#20998;&#37197;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#25903;&#25345;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#25252;&#29702;&#21592;&#23545;&#35775;&#38382;&#39034;&#24207;&#30340;&#28789;&#27963;&#24615;&#65292;&#26088;&#22312;&#20943;&#23569;&#34892;&#39542;&#37324;&#31243;&#12289;&#22686;&#21152;&#35775;&#38382;&#27425;&#25968;&#65292;&#24182;&#20445;&#25345;&#25252;&#29702;&#30340;&#36830;&#32493;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00696</link><description>&lt;p&gt;
&#20915;&#31574;&#25903;&#25345;&#26694;&#26550;&#22312;&#23478;&#24237;&#20445;&#20581;&#25252;&#29702;&#21592;&#20998;&#37197;&#20013;&#30340;&#24212;&#29992;&#65306;&#30000;&#32435;&#35199;&#24030;HHC&#26426;&#26500;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Decision Support Framework for Home Health Caregiver Allocation: A Case Study of HHC Agency in Tennessee, USA. (arXiv:2311.00696v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#23478;&#24237;&#20445;&#20581;&#26426;&#26500;&#38754;&#20020;&#30340;&#25252;&#29702;&#21592;&#20998;&#37197;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#25903;&#25345;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#25252;&#29702;&#21592;&#23545;&#35775;&#38382;&#39034;&#24207;&#30340;&#28789;&#27963;&#24615;&#65292;&#26088;&#22312;&#20943;&#23569;&#34892;&#39542;&#37324;&#31243;&#12289;&#22686;&#21152;&#35775;&#38382;&#27425;&#25968;&#65292;&#24182;&#20445;&#25345;&#25252;&#29702;&#30340;&#36830;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#21475;&#32769;&#40836;&#21270;&#26159;&#19968;&#20010;&#20840;&#29699;&#24615;&#30340;&#25361;&#25112;&#65292;&#23548;&#33268;&#23545;&#32769;&#24180;&#20154;&#30340;&#21307;&#30103;&#21644;&#31038;&#20250;&#26381;&#21153;&#38656;&#27714;&#22686;&#21152;&#12290;&#23478;&#24237;&#20445;&#20581;&#25252;&#29702;&#65288;HHC&#65289;&#20316;&#20026;&#19968;&#31181;&#19987;&#38376;&#20026;&#36825;&#19968;&#20154;&#32676;&#25552;&#20379;&#26381;&#21153;&#30340;&#37325;&#35201;&#35299;&#20915;&#26041;&#26696;&#27491;&#36880;&#28176;&#20852;&#36215;&#12290;&#37492;&#20110;&#23545;HHC&#30340;&#38656;&#27714;&#28608;&#22686;&#65292;&#26377;&#25928;&#22320;&#21327;&#35843;&#21644;&#31649;&#29702;&#25252;&#29702;&#21592;&#30340;&#20998;&#37197;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#23545;&#20110;&#39044;&#31639;&#20248;&#21270;&#30340;&#35268;&#21010;&#21644;&#30830;&#20445;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#25252;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#22238;&#31572;&#20102;&#23478;&#24237;&#20445;&#20581;&#26426;&#26500;&#38754;&#20020;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#8220;&#22312;&#25252;&#29702;&#21592;&#20559;&#22909;&#28789;&#27963;&#30340;&#35775;&#38382;&#39034;&#24207;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#20248;&#21270;&#20182;&#20204;&#30340;&#20998;&#37197;&#65311;&#8221;&#20043;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21018;&#24615;&#30340;&#35775;&#38382;&#39034;&#24207;&#65292;&#32780;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#20915;&#31574;&#25903;&#25345;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#23545;&#25252;&#29702;&#21592;&#36827;&#34892;&#20998;&#37197;&#65292;&#32771;&#34385;&#20102;&#35775;&#38382;&#39034;&#24207;&#30340;&#28789;&#27963;&#24615;&#65292;&#26088;&#22312;&#20943;&#23569;&#34892;&#39542;&#37324;&#31243;&#12289;&#22686;&#21152;&#27599;&#20010;&#35268;&#21010;&#21608;&#26399;&#30340;&#35775;&#38382;&#27425;&#25968;&#65292;&#24182;&#20445;&#25345;&#36830;&#32493;&#25252;&#29702;-&#36825;&#26159;&#34913;&#37327;&#24739;&#32773;&#24773;&#20917;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Population aging is a global challenge, leading to increased demand for healthcare and social services for the elderly. Home Health Care (HHC) emerges as a vital solution, specifically designed to serve this population segment. Given the surging demand for HHC, it's essential to coordinate and regulate caregiver allocation efficiently. This is crucial for both budget-optimized planning and ensuring the delivery of high-quality care. This research addresses a key question faced by home health agencies (HHAs): "How can caregiver allocation be optimized, especially when caregivers prefer flexibility in their visiting sequences?". While earlier studies proposed rigid visiting sequences, our study introduces a decision support framework that allocates caregivers through a hybrid method that considers the flexibility in visiting sequences and aims to reduce travel mileage, increase the number of visits per planning period, and maintain the continuity of care - a critical metric for patient s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20248;&#21270;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#38543;&#26426;&#23545;&#20598;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26631;&#20934;&#22238;&#24402;&#22522;&#20934;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.20581</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#27491;&#30830;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent for Gaussian Processes Done Right. (arXiv:2310.20581v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20248;&#21270;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#38543;&#26426;&#23545;&#20598;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26631;&#20934;&#22238;&#24402;&#22522;&#20934;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24179;&#26041;&#25439;&#22833;&#20989;&#25968;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#30446;&#21069;&#65292;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26368;&#24120;&#35265;&#26041;&#27861;&#26159;&#24212;&#29992;&#31934;&#30830;&#27714;&#35299;&#22120;&#65292;&#27604;&#22914;&#20849;&#36717;&#26799;&#24230;&#19979;&#38477;&#65292;&#35201;&#20040;&#30452;&#25509;&#24212;&#29992;&#65292;&#35201;&#20040;&#24212;&#29992;&#20110;&#38382;&#39064;&#30340;&#38477;&#38454;&#29256;&#26412;&#12290;&#26368;&#36817;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#25512;&#21160;&#19979;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#33719;&#24471;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#24403;&#27491;&#30830;&#20351;&#29992;&#26102;&#65288;&#25105;&#20204;&#25351;&#30340;&#26159;&#21033;&#29992;&#20248;&#21270;&#21644;&#26680;&#20989;&#25968;&#39046;&#22495;&#30340;&#29305;&#23450;&#35265;&#35299;&#65289;&#65292;&#36825;&#31181;&#26041;&#27861;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#38543;&#26426;&#23545;&#20598;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#20960;&#34892;&#20195;&#30721;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#28040;&#34701;&#23454;&#39564;&#35299;&#37322;&#20102;&#25105;&#20204;&#30340;&#35774;&#35745;&#20915;&#31574;&#30340;&#20248;&#21183;&#65292;&#24182;&#34920;&#26126;&#26032;&#26041;&#27861;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#23545;&#26631;&#20934;&#22238;&#24402;&#22522;&#20934;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the optimisation problem associated with Gaussian process regression using squared loss. The most common approach to this problem is to apply an exact solver, such as conjugate gradient descent, either directly, or to a reduced-order version of the problem. Recently, driven by successes in deep learning, stochastic gradient descent has gained traction as an alternative. In this paper, we show that when done right$\unicode{x2014}$by which we mean using specific insights from the optimisation and kernel communities$\unicode{x2014}$this approach is highly effective. We thus introduce a particular stochastic dual gradient descent algorithm, that may be implemented with a few lines of code using any deep learning framework. We explain our design decisions by illustrating their advantage against alternatives with ablation studies and show that the new method is highly competitive. Our evaluations on standard regression benchmarks and a Bayesian optimisation task set our approach apa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#29615;&#30340;&#26102;&#38388;&#24310;&#36831;&#20648;&#22791;&#35745;&#31639;&#26041;&#26696;&#65292;&#33021;&#22815;&#21516;&#26102;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12289;&#20998;&#31867;&#21644;&#26080;&#32447;&#36890;&#36947;&#22343;&#34913;&#31561;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#27599;&#20010;&#36890;&#36947;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16588</link><description>&lt;p&gt;
&#22810;&#24182;&#34892;&#20219;&#21153;&#24310;&#36831;&#20648;&#22791;&#35745;&#31639;&#32467;&#21512;&#30789;&#24494;&#29615;&#19982;WDM&#30340;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Multi-parallel-task Time-delay Reservoir Computing combining a Silicon Microring with WDM. (arXiv:2310.16588v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16588
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#29615;&#30340;&#26102;&#38388;&#24310;&#36831;&#20648;&#22791;&#35745;&#31639;&#26041;&#26696;&#65292;&#33021;&#22815;&#21516;&#26102;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12289;&#20998;&#31867;&#21644;&#26080;&#32447;&#36890;&#36947;&#22343;&#34913;&#31561;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#27599;&#20010;&#36890;&#36947;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#29615;&#30340;&#26102;&#38388;&#24310;&#36831;&#20648;&#22791;&#35745;&#31639;&#26041;&#26696;&#65292;&#33021;&#22815;&#21516;&#26102;&#35299;&#20915;&#28041;&#21450;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12289;&#20998;&#31867;&#21644;&#26080;&#32447;&#36890;&#36947;&#22343;&#34913;&#30340;&#19977;&#20010;&#20219;&#21153;&#12290;&#22312;&#27599;&#20010;&#36827;&#34892;&#27874;&#38271;&#22797;&#29992;&#30340;&#36890;&#36947;&#19978;&#65292;&#36890;&#36807;&#20248;&#21270;&#21151;&#29575;&#21644;&#39057;&#29575;&#22833;&#35856;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We numerically demonstrate a microring-based time-delay reservoir computing scheme that simultaneously solves three tasks involving time-series prediction, classification, and wireless channel equalization. Each task performed on a wavelength-multiplexed channel achieves state-of-the-art performance with optimized power and frequency detuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#20132;&#36890;&#20998;&#37197;&#21644;&#20132;&#36890;&#27969;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#19981;&#21516;&#38142;&#36335;&#20043;&#38388;&#30340;&#31354;&#38388;&#20132;&#36890;&#27169;&#24335;&#65292;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.13193</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#20132;&#36890;&#20998;&#37197;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Graph Neural Networks for Data-driven Traffic Assignment. (arXiv:2310.13193v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#20132;&#36890;&#20998;&#37197;&#21644;&#20132;&#36890;&#27969;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#19981;&#21516;&#38142;&#36335;&#20043;&#38388;&#30340;&#31354;&#38388;&#20132;&#36890;&#27169;&#24335;&#65292;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20998;&#37197;&#38382;&#39064;&#26159;&#20132;&#36890;&#27969;&#20998;&#26512;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#20043;&#19968;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#35299;&#20915;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#32593;&#32476;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20132;&#36890;&#20998;&#37197;&#21644;&#20132;&#36890;&#27969;&#23398;&#20064;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#38142;&#36335;&#20043;&#38388;&#30340;&#31354;&#38388;&#20132;&#36890;&#27169;&#24335;&#65292;&#20174;&#32780;&#20135;&#29983;&#39640;&#24230;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#22478;&#24066;&#20132;&#36890;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#25910;&#25947;&#36895;&#24230;&#12289;&#35757;&#32451;&#25439;&#22833;&#21644;&#39044;&#27979;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#25552;&#20986;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36824;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#32593;&#32476;&#25299;&#25169;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#22797;&#26434;&#20132;&#36890;&#27969;&#20998;&#26512;&#21644;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The traffic assignment problem is one of the significant components of traffic flow analysis for which various solution approaches have been proposed. However, deploying these approaches for large-scale networks poses significant challenges. In this paper, we leverage the power of heterogeneous graph neural networks to propose a novel data-driven approach for traffic assignment and traffic flow learning. The proposed model is capable of capturing spatial traffic patterns across different links, yielding highly accurate results. We present numerical experiments on urban transportation networks and show that the proposed heterogeneous graph neural network model outperforms other conventional neural network models in terms of convergence rate, training loss, and prediction accuracy. Notably, the proposed heterogeneous graph neural network model can also be generalized to different network topologies. This approach offers a promising solution for complex traffic flow analysis and predictio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#37327;&#23376;&#21152;&#36895;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37327;&#23376;&#26694;&#26550;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#25913;&#36827;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25152;&#25552;&#20986;&#30340;&#37327;&#23376;&#31639;&#27861;&#30456;&#36739;&#20110;&#32463;&#20856;&#31639;&#27861;&#65292;&#22312;&#36951;&#25022;&#30028;&#38480;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.11684</link><description>&lt;p&gt;
&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#37327;&#23376;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Quantum Acceleration of Infinite Horizon Average-Reward Reinforcement Learning. (arXiv:2310.11684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#37327;&#23376;&#21152;&#36895;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37327;&#23376;&#26694;&#26550;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#25913;&#36827;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25152;&#25552;&#20986;&#30340;&#37327;&#23376;&#31639;&#27861;&#30456;&#36739;&#20110;&#32463;&#20856;&#31639;&#27861;&#65292;&#22312;&#36951;&#25022;&#30028;&#38480;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#37327;&#23376;&#21152;&#36895;&#22312;&#35299;&#20915;&#26080;&#38480;&#26102;&#22495;Markov&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#25552;&#39640;&#24179;&#22343;&#22870;&#21169;&#32467;&#26524;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37327;&#23376;&#26694;&#26550;&#65292;&#29992;&#20110;&#20195;&#29702;&#19982;&#26410;&#30693;MDP&#30340;&#20114;&#21160;&#65292;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;&#20132;&#20114;&#33539;&#24335;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#35774;&#35745;&#19968;&#31181;&#22522;&#20110;&#20048;&#35266;&#20027;&#23548;&#30340;&#20855;&#26377;&#37327;&#23376;&#20449;&#21495;&#30340;&#34920;&#26684;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#25216;&#26415;&#33719;&#21462;&#20195;&#29702;&#33719;&#21462;&#30340;&#37327;&#23376;&#20449;&#21495;&#12290;&#36890;&#36807;&#28145;&#20837;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#30340;&#20248;&#21183;&#33021;&#22815;&#22312;&#26080;&#38480;&#26102;&#22495;&#24378;&#21270;&#23398;&#20064;&#20013;&#23548;&#33268;&#36951;&#25022;&#20445;&#35777;&#30340;&#25351;&#25968;&#36827;&#23637;&#12290;&#20855;&#20307;&#22320;&#65292;&#25152;&#25552;&#20986;&#30340;&#37327;&#23376;&#31639;&#27861;&#23454;&#29616;&#20102;&#19968;&#20010;&#36951;&#25022;&#30028;&#20026;$\tilde{\mathcal{O}}(1)$&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#30456;&#23545;&#20110;&#32463;&#20856;&#23545;&#24212;&#31639;&#27861;&#25152;&#23637;&#31034;&#30340;$\tilde{\mathcal{O}}(\sqrt{T})$&#30028;&#38480;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the potential of quantum acceleration in addressing infinite horizon Markov Decision Processes (MDPs) to enhance average reward outcomes. We introduce an innovative quantum framework for the agent's engagement with an unknown MDP, extending the conventional interaction paradigm. Our approach involves the design of an optimism-driven tabular Reinforcement Learning algorithm that harnesses quantum signals acquired by the agent through efficient quantum mean estimation techniques. Through thorough theoretical analysis, we demonstrate that the quantum advantage in mean estimation leads to exponential advancements in regret guarantees for infinite horizon Reinforcement Learning. Specifically, the proposed Quantum algorithm achieves a regret bound of $\tilde{\mathcal{O}}(1)$, a significant improvement over the $\tilde{\mathcal{O}}(\sqrt{T})$ bound exhibited by classical counterparts.
&lt;/p&gt;</description></item><item><title>ZeroSwap &#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#30340; DeFi &#24066;&#22330;&#20570;&#24066;&#26041;&#26696;&#65292;&#22312;&#20445;&#25345;&#24066;&#22330;&#20570;&#24066;&#21830;&#38646;&#21033;&#28070;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#36866;&#24212;&#20132;&#26131;&#32773;&#34892;&#20026;&#26469;&#35299;&#20915;&#20102;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#36973;&#21463;&#22871;&#21033;&#25439;&#22833;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.09413</link><description>&lt;p&gt;
ZeroSwap: &#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340; DeFi &#20013;&#30340;&#26368;&#20248;&#24066;&#22330;&#20570;&#24066;
&lt;/p&gt;
&lt;p&gt;
ZeroSwap: Data-driven Optimal Market Making in DeFi. (arXiv:2310.09413v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09413
&lt;/p&gt;
&lt;p&gt;
ZeroSwap &#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#30340; DeFi &#24066;&#22330;&#20570;&#24066;&#26041;&#26696;&#65292;&#22312;&#20445;&#25345;&#24066;&#22330;&#20570;&#24066;&#21830;&#38646;&#21033;&#28070;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#36866;&#24212;&#20132;&#26131;&#32773;&#34892;&#20026;&#26469;&#35299;&#20915;&#20102;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#36973;&#21463;&#22871;&#21033;&#25439;&#22833;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20570;&#24066;&#21830; (AMMs) &#26159;&#21435;&#20013;&#24515;&#21270;&#37329;&#34701;&#20013;&#21305;&#37197;&#27969;&#21160;&#24615;&#20379;&#32473;&#21644;&#38656;&#27714;&#30340;&#20027;&#35201;&#20013;&#24515;&#12290;&#23427;&#20204;&#30340;&#21151;&#33021;&#20027;&#35201;&#20381;&#36182;&#20110;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773; (LPs) &#23558;&#20854;&#36164;&#20135;&#25237;&#36164;&#20110;&#27969;&#21160;&#24615;&#27744;&#12290;&#28982;&#32780;&#65292;&#27744;&#20013;&#36164;&#20135;&#20132;&#26131;&#30340;&#20215;&#26684;&#36890;&#24120;&#27604;&#38598;&#20013;&#21270;&#21644;&#26356;&#27969;&#21160;&#30340;&#20132;&#26131;&#25152;&#20215;&#26684;&#24310;&#36831;&#26356;&#22810;&#12290;&#36825;&#23548;&#33268;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#36973;&#21463;&#22871;&#21033;&#25439;&#22833;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992; Glosten &#21644; Milgrom &#30340;&#32463;&#20856;&#24066;&#22330;&#24494;&#35266;&#32467;&#26500;&#27169;&#22411;&#65292;&#23558;&#24066;&#22330;&#20215;&#26684;&#36866;&#24212;&#20110;&#20132;&#26131;&#32773;&#34892;&#20026;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26368;&#20248;&#36125;&#21494;&#26031;&#21644;&#31532;&#19968;&#20010;&#26080;&#27169;&#22411;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#26469;&#26368;&#20248;&#22320;&#36319;&#36394;&#36164;&#20135;&#30340;&#22806;&#37096;&#20215;&#26684;&#12290;&#25105;&#20204;&#20351;&#29992;&#30340;&#26368;&#20248;&#24615;&#27010;&#24565;&#22312;&#24066;&#22330;&#20570;&#24066;&#21830;&#30340;&#20215;&#26684;&#19978;&#24378;&#21046;&#25191;&#34892;&#20102;&#38646;&#21033;&#28070;&#26465;&#20214;&#65292;&#22240;&#27492;&#21462;&#21517;&#20026; ZeroSwap&#12290;&#36825;&#30830;&#20445;&#20102;&#24066;&#22330;&#20570;&#24066;&#21830;&#22312;&#25439;&#22833;&#30693;&#24773;&#20132;&#26131;&#32773;&#30340;&#21516;&#26102;&#20174;&#22122;&#22768;&#20132;&#26131;&#32773;&#37027;&#37324;&#33719;&#24471;&#21033;&#28070;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Market Makers (AMMs) are major centers of matching liquidity supply and demand in Decentralized Finance. Their functioning relies primarily on the presence of liquidity providers (LPs) incentivized to invest their assets into a liquidity pool. However, the prices at which a pooled asset is traded is often more stale than the prices on centralized and more liquid exchanges. This leads to the LPs suffering losses to arbitrage. This problem is addressed by adapting market prices to trader behavior, captured via the classical market microstructure model of Glosten and Milgrom. In this paper, we propose the first optimal Bayesian and the first model-free data-driven algorithm to optimally track the external price of the asset. The notion of optimality that we use enforces a zero-profit condition on the prices of the market maker, hence the name ZeroSwap. This ensures that the market maker balances losses to informed traders with profits from noise traders. The key property of our 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SafeDPA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#35299;&#20915;&#31574;&#30053;&#36866;&#24212;&#21644;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;SafeDPA&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#32852;&#21512;&#23398;&#20064;&#33258;&#36866;&#24212;&#31574;&#30053;&#21644;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#23569;&#37327;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#37096;&#32626;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#30830;&#20445;&#20102;SafeDPA&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08602</link><description>&lt;p&gt;
&#23433;&#20840;&#28145;&#24230;&#31574;&#30053;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Safe Deep Policy Adaptation. (arXiv:2310.08602v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08602
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SafeDPA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#35299;&#20915;&#31574;&#30053;&#36866;&#24212;&#21644;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;SafeDPA&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#32852;&#21512;&#23398;&#20064;&#33258;&#36866;&#24212;&#31574;&#30053;&#21644;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#23569;&#37327;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#37096;&#32626;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#30830;&#20445;&#20102;SafeDPA&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#26159;&#20351;&#33258;&#20027;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#21160;&#24577;&#21644;&#19981;&#30830;&#23450;&#30340;&#29615;&#22659;&#20013;&#24555;&#36895;&#36866;&#24212;&#12290;&#32463;&#20856;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#21644;&#23433;&#20840;&#25511;&#21046;&#25552;&#20379;&#20102;&#31283;&#23450;&#24615;&#21644;&#23433;&#20840;&#24615;&#20445;&#35777;&#65292;&#20294;&#20165;&#38480;&#20110;&#29305;&#23450;&#30340;&#31995;&#32479;&#31867;&#21035;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#31574;&#30053;&#36866;&#24212;&#25552;&#20379;&#20102;&#36890;&#29992;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#20294;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#23433;&#20840;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SafeDPA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;RL&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#31574;&#30053;&#36866;&#24212;&#21644;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;SafeDPA&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#32852;&#21512;&#23398;&#20064;&#33258;&#36866;&#24212;&#31574;&#30053;&#21644;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#39044;&#27979;&#29615;&#22659;&#37197;&#32622;&#65292;&#24182;&#20351;&#29992;&#23569;&#37327;&#30495;&#23454;&#25968;&#25454;&#23545;&#21160;&#21147;&#23398;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;RL&#31574;&#30053;&#20043;&#19978;&#24341;&#20837;&#22522;&#20110;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBF&#65289;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#20197;&#30830;&#20445;&#22312;&#30495;&#23454;&#19990;&#30028;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;SafeDPA&#30340;&#29702;&#35770;&#23433;&#20840;&#24615;&#20445;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;SafeDPA&#23545;&#23398;&#20064;&#35823;&#24046;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A critical goal of autonomy and artificial intelligence is enabling autonomous robots to rapidly adapt in dynamic and uncertain environments. Classic adaptive control and safe control provide stability and safety guarantees but are limited to specific system classes. In contrast, policy adaptation based on reinforcement learning (RL) offers versatility and generalizability but presents safety and robustness challenges. We propose SafeDPA, a novel RL and control framework that simultaneously tackles the problems of policy adaptation and safe reinforcement learning. SafeDPA jointly learns adaptive policy and dynamics models in simulation, predicts environment configurations, and fine-tunes dynamics models with few-shot real-world data. A safety filter based on the Control Barrier Function (CBF) on top of the RL policy is introduced to ensure safety during real-world deployment. We provide theoretical safety guarantees of SafeDPA and show the robustness of SafeDPA against learning errors 
&lt;/p&gt;</description></item><item><title>"Unraveling the Single Tangent Space Fallacy"&#35770;&#25991;&#20998;&#26512;&#21644;&#28548;&#28165;&#20102;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#24212;&#29992;&#40654;&#26364;&#20960;&#20309;&#30340;&#35823;&#21306;&#65292;&#35813;&#35823;&#21306;&#26159;&#25351;&#23558;&#25968;&#25454;&#20165;&#25237;&#24433;&#21040;&#21333;&#19968;&#20999;&#31354;&#38388;&#20013;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07902</link><description>&lt;p&gt;
&#25581;&#31034;&#21333;&#20999;&#24179;&#38754;&#35823;&#21306;&#65306;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#24212;&#29992;&#40654;&#26364;&#20960;&#20309;&#30340;&#20998;&#26512;&#21644;&#28548;&#28165;
&lt;/p&gt;
&lt;p&gt;
Unraveling the Single Tangent Space Fallacy: An Analysis and Clarification for Applying Riemannian Geometry in Robot Learning. (arXiv:2310.07902v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07902
&lt;/p&gt;
&lt;p&gt;
"Unraveling the Single Tangent Space Fallacy"&#35770;&#25991;&#20998;&#26512;&#21644;&#28548;&#28165;&#20102;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#24212;&#29992;&#40654;&#26364;&#20960;&#20309;&#30340;&#35823;&#21306;&#65292;&#35813;&#35823;&#21306;&#26159;&#25351;&#23558;&#25968;&#25454;&#20165;&#25237;&#24433;&#21040;&#21333;&#19968;&#20999;&#31354;&#38388;&#20013;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#35768;&#22810;&#21518;&#32493;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#22788;&#29702;&#12289;&#24314;&#27169;&#25110;&#21512;&#25104;&#25968;&#25454;&#12290;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#22266;&#20307;&#26041;&#21521;&#34920;&#31034;&#22235;&#20803;&#25968;&#30340;&#21333;&#20301;&#33539;&#25968;&#26465;&#20214;&#25110;&#21018;&#24230;&#21644;&#21487;&#25805;&#32437;&#24615;&#26925;&#29699;&#30340;&#27491;&#23450;&#24615;&#31561;&#20960;&#20309;&#32422;&#26463;&#12290;&#26377;&#25928;&#22788;&#29702;&#36825;&#26679;&#30340;&#20960;&#20309;&#32422;&#26463;&#38656;&#35201;&#23558;&#24494;&#20998;&#20960;&#20309;&#24037;&#20855;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#21046;&#23450;&#20013;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#40654;&#26364;&#27969;&#24418;&#25104;&#20026;&#22788;&#29702;&#36825;&#31181;&#20960;&#20309;&#32422;&#26463;&#30340;&#24378;&#22823;&#25968;&#23398;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#23545;&#20854;&#37319;&#29992;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#19968;&#20010;&#25968;&#23398;&#19978;&#30340;&#32570;&#38519;&#21270;&#31616;&#29616;&#35937;&#65292;&#34987;&#31216;&#20026;&#8220;&#21333;&#20999;&#24179;&#38754;&#35823;&#21306;&#8221;&#12290;&#36825;&#31181;&#26041;&#27861;&#20165;&#28041;&#21450;&#23558;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#25237;&#24433;&#21040;&#19968;&#20010;&#21333;&#19968;&#20999;&#31354;&#38388;&#65288;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#65289;&#19978;&#65292;&#28982;&#21518;&#20351;&#29992;&#29616;&#25104;&#30340;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
In the realm of robotics, numerous downstream robotics tasks leverage machine learning methods for processing, modeling, or synthesizing data. Often, this data comprises variables that inherently carry geometric constraints, such as the unit-norm condition of quaternions representing rigid-body orientations or the positive definiteness of stiffness and manipulability ellipsoids. Handling such geometric constraints effectively requires the incorporation of tools from differential geometry into the formulation of machine learning methods. In this context, Riemannian manifolds emerge as a powerful mathematical framework to handle such geometric constraints. Nevertheless, their recent adoption in robot learning has been largely characterized by a mathematically-flawed simplification, hereinafter referred to as the ``single tangent space fallacy". This approach involves merely projecting the data of interest onto a single tangent (Euclidean) space, over which an off-the-shelf learning algor
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#30456;&#20301;&#31354;&#38388;&#20013;&#26500;&#24314;&#36335;&#24452;&#27979;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21160;&#21147;&#20256;&#25773;&#30340;&#26089;&#26399;&#38454;&#27573;&#29983;&#25104;&#36924;&#30495;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#21033;&#29992;&#39069;&#22806;&#30340;&#36895;&#24230;&#20449;&#24687;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.07805</link><description>&lt;p&gt;
&#20855;&#26377;&#30456;&#20301;&#38543;&#26426;&#26725;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling with Phase Stochastic Bridges. (arXiv:2310.07805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07805
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#30456;&#20301;&#31354;&#38388;&#20013;&#26500;&#24314;&#36335;&#24452;&#27979;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21160;&#21147;&#20256;&#25773;&#30340;&#26089;&#26399;&#38454;&#27573;&#29983;&#25104;&#36924;&#30495;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#21033;&#29992;&#39069;&#22806;&#30340;&#36895;&#24230;&#20449;&#24687;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#26159;&#29992;&#20110;&#36830;&#32493;&#36755;&#20837;&#30340;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;DMs&#36890;&#36807;&#22312;&#36755;&#20837;&#31354;&#38388;&#65288;&#21363;&#20301;&#32622;&#31354;&#38388;&#65289;&#20013;&#26500;&#24314;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21453;&#28436;&#26469;&#24037;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20301;&#31354;&#38388;&#21160;&#21147;&#23398;&#30340;&#26032;&#22411;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#20854;&#20013;&#30456;&#20301;&#31354;&#38388;&#34987;&#23450;&#20041;&#20026;&#19968;&#20010;&#21253;&#25324;&#20301;&#32622;&#21644;&#36895;&#24230;&#30340;&#22686;&#24378;&#31354;&#38388;&#12290;&#21033;&#29992;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#30456;&#20301;&#31354;&#38388;&#20013;&#30340;&#36335;&#24452;&#27979;&#24230;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#37319;&#26679;&#12290;&#19982;DMs&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21160;&#21147;&#20256;&#25773;&#30340;&#26089;&#26399;&#38454;&#27573;&#23601;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#25968;&#25454;&#28857;&#12290;&#36825;&#31181;&#26089;&#26399;&#39044;&#27979;&#20026;&#36890;&#36807;&#27839;&#36712;&#36857;&#21033;&#29992;&#39069;&#22806;&#30340;&#36895;&#24230;&#20449;&#24687;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#22312;&#26631;&#20934;&#22270;&#20687;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23567;&#20989;&#25968;&#35780;&#20272;&#25968;&#37327;&#30340;&#33539;&#22260;&#20869;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) represent state-of-the-art generative models for continuous inputs. DMs work by constructing a Stochastic Differential Equation (SDE) in the input space (ie, position space), and using a neural network to reverse it. In this work, we introduce a novel generative modeling framework grounded in \textbf{phase space dynamics}, where a phase space is defined as {an augmented space encompassing both position and velocity.} Leveraging insights from Stochastic Optimal Control, we construct a path measure in the phase space that enables efficient sampling. {In contrast to DMs, our framework demonstrates the capability to generate realistic data points at an early stage of dynamics propagation.} This early prediction sets the stage for efficient data generation by leveraging additional velocity information along the trajectory. On standard image generation benchmarks, our model yields favorable performance over baselines in the regime of small Number of Function Evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#23545;&#20445;&#25345;&#21487;&#22609;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#26159;&#39640;&#25928;&#35757;&#32451;&#30340;&#20027;&#35201;&#38480;&#21046;&#22240;&#32032;&#65292;&#24182;&#19988;&#26410;&#21450;&#26102;&#24674;&#22797;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#23558;&#23548;&#33268;&#28798;&#38590;&#24615;&#32467;&#26524;&#12290;&#36825;&#20026;&#35299;&#20915;&#39640;&#37325;&#25918;&#27604;&#22256;&#22659;&#25552;&#20379;&#20102;&#26032;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.07418</link><description>&lt;p&gt;
&#37325;&#23457;&#35270;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#65306;&#25968;&#25454;&#12289;&#27169;&#22359;&#21644;&#35757;&#32451;&#38454;&#27573;
&lt;/p&gt;
&lt;p&gt;
Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages. (arXiv:2310.07418v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#23545;&#20445;&#25345;&#21487;&#22609;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#26159;&#39640;&#25928;&#35757;&#32451;&#30340;&#20027;&#35201;&#38480;&#21046;&#22240;&#32032;&#65292;&#24182;&#19988;&#26410;&#21450;&#26102;&#24674;&#22797;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#23558;&#23548;&#33268;&#28798;&#38590;&#24615;&#32467;&#26524;&#12290;&#36825;&#20026;&#35299;&#20915;&#39640;&#37325;&#25918;&#27604;&#22256;&#22659;&#25552;&#20379;&#20102;&#26032;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#22609;&#24615;&#65292;&#31070;&#32463;&#32593;&#32476;&#38543;&#26032;&#25968;&#25454;&#28436;&#36827;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#39640;&#24615;&#33021;&#21644;&#26679;&#26412;&#39640;&#25928;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;(VRL)&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#37325;&#32622;&#21644;&#27491;&#21017;&#21270;&#31561;&#26041;&#27861;&#21487;&#33021;&#33021;&#22815;&#32531;&#35299;&#21487;&#22609;&#24615;&#25439;&#22833;&#65292;&#20294;VRL&#26694;&#26550;&#20869;&#21508;&#31181;&#32452;&#20214;&#23545;&#20195;&#29702;&#30340;&#21487;&#22609;&#24615;&#30340;&#24433;&#21709;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#32463;&#39564;&#24615;&#25506;&#32034;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#19977;&#20010;&#20027;&#35201;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#26041;&#38754;&#65292;&#24182;&#24471;&#20986;&#20197;&#19979;&#26377;&#28145;&#20837;&#35265;&#35299;&#30340;&#32467;&#35770;&#65306;(1)&#25968;&#25454;&#22686;&#24378;&#23545;&#20110;&#20445;&#25345;&#21487;&#22609;&#24615;&#33267;&#20851;&#37325;&#35201;&#65307;(2)&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#26159;&#38459;&#30861;&#39640;&#25928;&#35757;&#32451;&#30340;&#20027;&#35201;&#29942;&#39048;&#65307;(3)&#22312;&#26089;&#26399;&#38454;&#27573;&#27809;&#26377;&#21450;&#26102;&#24178;&#39044;&#20197;&#24674;&#22797;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#65292;&#20854;&#25439;&#22833;&#23558;&#21464;&#24471;&#28798;&#38590;&#24615;&#12290;&#36825;&#20123;&#35265;&#35299;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#23545;&#39640;&#37325;&#25918;&#27604;&#65288;RR&#65289;&#22256;&#22659;&#30340;&#26032;&#31574;&#30053;&#65292;&#20854;&#20013;&#21152;&#21095;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#22952;&#30861;&#20102;&#36890;&#36807;&#22686;&#21152;&#37325;&#25918;&#25968;&#37327;&#24102;&#26469;&#30340;&#26679;&#26412;&#25928;&#29575;&#30340;&#28508;&#22312;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plasticity, the ability of a neural network to evolve with new data, is crucial for high-performance and sample-efficient visual reinforcement learning (VRL). Although methods like resetting and regularization can potentially mitigate plasticity loss, the influences of various components within the VRL framework on the agent's plasticity are still poorly understood. In this work, we conduct a systematic empirical exploration focusing on three primary underexplored facets and derive the following insightful conclusions: (1) data augmentation is essential in maintaining plasticity; (2) the critic's plasticity loss serves as the principal bottleneck impeding efficient training; and (3) without timely intervention to recover critic's plasticity in the early stages, its loss becomes catastrophic. These insights suggest a novel strategy to address the high replay ratio (RR) dilemma, where exacerbated plasticity loss hinders the potential improvements of sample efficiency brought by increased
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#20250;&#23548;&#33268;&#23545;&#26550;&#26500;&#24046;&#24322;&#30340;&#20005;&#37325;&#39640;&#20272;&#65292;&#32780;&#20351;&#29992;&#26631;&#20934;&#28040;&#22122;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#23558;Transformers&#19982;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#21040;&#24456;&#23567;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#27491;&#30830;&#39044;&#35757;&#32451;&#26102;&#65292;&#26222;&#36890;&#30340;Transformers&#22312;Long Range Arena&#19978;&#30340;&#24615;&#33021;&#19982;S4&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#22312;PathX-256&#20219;&#21153;&#19978;&#25913;&#36827;&#20102;SSMs&#30340;&#26368;&#20339;&#32467;&#26524;20&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.02980</link><description>&lt;p&gt;
&#27704;&#36828;&#19981;&#35201;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65306;&#20844;&#27491;&#27604;&#36739;&#38271;&#24207;&#21015;&#27169;&#22411;&#38656;&#35201;&#25968;&#25454;&#39537;&#21160;&#30340;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors. (arXiv:2310.02980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#20250;&#23548;&#33268;&#23545;&#26550;&#26500;&#24046;&#24322;&#30340;&#20005;&#37325;&#39640;&#20272;&#65292;&#32780;&#20351;&#29992;&#26631;&#20934;&#28040;&#22122;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#23558;Transformers&#19982;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#21040;&#24456;&#23567;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#27491;&#30830;&#39044;&#35757;&#32451;&#26102;&#65292;&#26222;&#36890;&#30340;Transformers&#22312;Long Range Arena&#19978;&#30340;&#24615;&#33021;&#19982;S4&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#22312;PathX-256&#20219;&#21153;&#19978;&#25913;&#36827;&#20102;SSMs&#30340;&#26368;&#20339;&#32467;&#26524;20&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#24207;&#21015;&#20043;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#19968;&#30452;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#30446;&#26631;&#65292;&#24182;&#23548;&#33268;&#20102;&#19968;&#20123;&#26550;&#26500;&#65292;&#22914;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#27604;Transformers&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32463;&#39564;&#24615;&#36827;&#23637;&#20027;&#35201;&#26159;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#24182;&#36890;&#36807;&#39044;&#27979;&#36755;&#20837;&#24207;&#21015;&#30340;&#30446;&#26631;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#30340;&#22522;&#20934;&#27979;&#35797;&#65288;&#20363;&#22914;Long Range Arena&#65289;&#19978;&#23637;&#31034;&#20986;&#26469;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38543;&#26426;&#21021;&#22987;&#21270;&#23548;&#33268;&#23545;&#26550;&#26500;&#20043;&#38388;&#24046;&#24322;&#30340;&#20005;&#37325;&#39640;&#20272;&#65292;&#24182;&#19988;&#20351;&#29992;&#26631;&#20934;&#28040;&#22122;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#65288;&#20165;&#20351;&#29992;&#19979;&#28216;&#20219;&#21153;&#25968;&#25454;&#65289;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;Transformers&#21644;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#20043;&#38388;&#24471;&#21040;&#24456;&#23567;&#30340;&#24046;&#36317;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#27491;&#30830;&#39044;&#35757;&#32451;&#26102;&#65292;&#26222;&#36890;&#30340;Transformers&#22312;Long Range Arena&#19978;&#19982;S4&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#25105;&#20204;&#22312;PathX-256&#20219;&#21153;&#19978;&#23558;SSMs&#30340;&#26368;&#20339;&#25253;&#21578;&#32467;&#26524;&#25552;&#39640;&#20102;20&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, using $\textit{only the downstream task data}$, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;&#27169;&#22411;&#20869;&#23398;&#20064;&#37327;&#23376;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#23398;&#20064;&#22120;&#21644;&#21487;&#35777;&#26126;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#36890;&#36807;&#22312;&#23494;&#30721;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#25581;&#31034;&#20102;&#32463;&#20856;&#35835;&#20986;&#37327;&#23376;&#29289;&#29702;&#19981;&#21487;&#20811;&#38534;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;&#65292;&#36825;&#26159;&#37327;&#23376;&#30828;&#20214;&#23433;&#20840;&#39046;&#22495;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02075</link><description>&lt;p&gt;
&#20351;&#29992;&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;&#23398;&#20064;&#37327;&#23376;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning Quantum Processes with Quantum Statistical Queries. (arXiv:2310.02075v1 [quant-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;&#27169;&#22411;&#20869;&#23398;&#20064;&#37327;&#23376;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#23398;&#20064;&#22120;&#21644;&#21487;&#35777;&#26126;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#36890;&#36807;&#22312;&#23494;&#30721;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#25581;&#31034;&#20102;&#32463;&#20856;&#35835;&#20986;&#37327;&#23376;&#29289;&#29702;&#19981;&#21487;&#20811;&#38534;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;&#65292;&#36825;&#26159;&#37327;&#23376;&#30828;&#20214;&#23433;&#20840;&#39046;&#22495;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22797;&#26434;&#30340;&#37327;&#23376;&#36807;&#31243;&#26159;&#37327;&#23376;&#35745;&#31639;&#21644;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#35768;&#22810;&#39046;&#22495;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#65292;&#24212;&#29992;&#20110;&#37327;&#23376;&#22522;&#20934;&#27979;&#35797;&#12289;&#23494;&#30721;&#20998;&#26512;&#21644;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;&#65288;QSQ&#65289;&#27169;&#22411;&#20869;&#30740;&#31350;&#37327;&#23376;&#36807;&#31243;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#23545;&#37327;&#23376;&#36807;&#31243;&#65288;QPSQs&#65289;&#36827;&#34892;&#32479;&#35745;&#26597;&#35810;&#30340;&#31532;&#19968;&#20010;&#27491;&#24335;&#23450;&#20041;&#12290;&#35813;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#30340;QPSQ&#23398;&#20064;&#22120;&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;&#37327;&#23376;&#36807;&#31243;&#65292;&#24182;&#38468;&#24102;&#21487;&#35777;&#26126;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#25968;&#20540;&#27169;&#25311;&#26469;&#23637;&#31034;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#22312;&#23494;&#30721;&#20998;&#26512;&#20013;&#24212;&#29992;&#35813;&#26694;&#26550;&#65292;&#31361;&#20986;&#20102;&#32463;&#20856;&#35835;&#20986;&#37327;&#23376;&#29289;&#29702;&#19981;&#21487;&#20811;&#38534;&#20989;&#25968;&#65288;CR-QPUFs&#65289;&#30340;&#33030;&#24369;&#24615;&#65292;&#35299;&#20915;&#20102;&#37327;&#23376;&#30828;&#20214;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#24320;&#25918;&#38382;&#39064;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;&#26397;&#30528;&#28145;&#20837;&#29702;&#35299;&#37327;&#23376;&#36807;&#31243;&#23398;&#20064;&#36808;&#20986;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning complex quantum processes is a central challenge in many areas of quantum computing and quantum machine learning, with applications in quantum benchmarking, cryptanalysis, and variational quantum algorithms. This paper introduces the first learning framework for studying quantum process learning within the Quantum Statistical Query (QSQ) model, providing the first formal definition of statistical queries to quantum processes (QPSQs). The framework allows us to propose an efficient QPSQ learner for arbitrary quantum processes accompanied by a provable performance guarantee. We also provide numerical simulations to demonstrate the efficacy of this algorithm. The practical relevance of this framework is exemplified through application in cryptanalysis, highlighting vulnerabilities of Classical-Readout Quantum Physical Unclonable Functions (CR-QPUFs), addressing an important open question in the field of quantum hardware security. This work marks a significant step towards underst
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ResBit&#30340;&#27531;&#24046;&#20301;&#21521;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#34920;&#31034;&#31163;&#25955;&#25968;&#25454;&#32500;&#24230;&#22686;&#21152;&#21644;&#26080;&#27861;&#24674;&#22797;&#21407;&#22987;&#31867;&#21035;&#20540;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.17196</link><description>&lt;p&gt;
ResBit: &#22522;&#20110;&#27531;&#24046;&#20301;&#21521;&#37327;&#30340;&#31163;&#25955;&#20540;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ResBit: Residual Bit Vector for Categorical Values. (arXiv:2309.17196v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ResBit&#30340;&#27531;&#24046;&#20301;&#21521;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#34920;&#31034;&#31163;&#25955;&#25968;&#25454;&#32500;&#24230;&#22686;&#21152;&#21644;&#26080;&#27861;&#24674;&#22797;&#21407;&#22987;&#31867;&#21035;&#20540;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#29420;&#28909;&#32534;&#30721;&#21521;&#37327;&#19968;&#30452;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#34920;&#31034;&#31163;&#25955;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#32500;&#24230;&#38543;&#30528;&#35201;&#34920;&#31034;&#30340;&#31163;&#25955;&#25968;&#25454;&#32447;&#24615;&#22686;&#21152;&#65292;&#36825;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#35270;&#20026;&#31354;&#38388;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#38382;&#39064;&#65292;&#32780;&#28145;&#24230;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#34920;&#36798;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20301;&#24207;&#21015;&#34920;&#31034;&#31163;&#25955;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21363;Analog Bits&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#35201;&#34920;&#31034;&#30340;&#31867;&#21035;&#31867;&#22411;&#25968;&#37327;&#19981;&#19968;&#23450;&#26159;2&#30340;&#24130;&#27425;&#65292;&#23548;&#33268;Analog Bits&#33021;&#22815;&#34920;&#31034;&#30340;&#33539;&#22260;&#19982;&#31867;&#21035;&#25968;&#25454;&#30340;&#33539;&#22260;&#23384;&#22312;&#24046;&#24322;&#12290;&#22914;&#26524;&#29983;&#25104;&#20102;&#36825;&#26679;&#30340;&#20540;&#65292;&#38382;&#39064;&#23601;&#26159;&#26080;&#27861;&#24674;&#22797;&#21407;&#22987;&#30340;&#31867;&#21035;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27531;&#24046;&#20301;&#21521;&#37327;&#65288;ResBit&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#20998;&#23618;&#30340;&#20301;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The one-hot vector has long been widely used in machine learning as a simple and generic method for representing discrete data. However, this method increases the number of dimensions linearly with the categorical data to be represented, which is problematic from the viewpoint of spatial computational complexity in deep learning, which requires a large amount of data. Recently, Analog Bits, a method for representing discrete data as a sequence of bits, was proposed on the basis of the high expressiveness of diffusion models. However, since the number of category types to be represented in a generation task is not necessarily at a power of two, there is a discrepancy between the range that Analog Bits can represent and the range represented as category data. If such a value is generated, the problem is that the original category value cannot be restored. To address this issue, we propose Residual Bit Vector (ResBit), which is a hierarchical bit representation. Although it is a general-p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#27169;&#24335;&#22604;&#38519;&#38382;&#39064;&#23545;&#21512;&#25104;X&#23556;&#32447;&#22270;&#20687;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23558;&#33258;&#36866;&#24212;&#36755;&#20837;&#22270;&#20687;&#24402;&#19968;&#21270;&#26041;&#27861;&#19982;&#28145;&#24230;&#27169;&#22411;&#32467;&#21512;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.12245</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#36755;&#20837;&#22270;&#20687;&#24402;&#19968;&#21270;&#26041;&#27861;&#35299;&#20915;&#22522;&#20110;GAN&#30340;X&#23556;&#32447;&#22270;&#20687;&#30340;&#27169;&#24335;&#22604;&#38519;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Input-image Normalization for Solving Mode Collapse Problem in GAN-based X-ray Images. (arXiv:2309.12245v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#27169;&#24335;&#22604;&#38519;&#38382;&#39064;&#23545;&#21512;&#25104;X&#23556;&#32447;&#22270;&#20687;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23558;&#33258;&#36866;&#24212;&#36755;&#20837;&#22270;&#20687;&#24402;&#19968;&#21270;&#26041;&#27861;&#19982;&#28145;&#24230;&#27169;&#22411;&#32467;&#21512;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30142;&#30149;&#30340;&#32597;&#35265;&#24615;&#65292;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#21487;&#33021;&#23384;&#22312;&#19981;&#24179;&#34913;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#26469;&#25193;&#20805;&#25968;&#25454;&#38598;&#65292;&#36215;&#21040;&#20102;&#35299;&#20915;&#36825;&#31181;&#19981;&#24179;&#34913;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#38656;&#35201;&#21253;&#21547;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#65292;&#20197;&#20934;&#30830;&#34920;&#31034;&#35757;&#32451;&#22270;&#20687;&#20013;&#23384;&#22312;&#30340;&#29305;&#24449;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#21512;&#25104;&#22270;&#20687;&#20013;&#32570;&#20047;&#22810;&#26679;&#24615;&#30340;&#29305;&#24449;&#20250;&#38477;&#20302;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#27169;&#24335;&#22604;&#38519;&#38382;&#39064;&#24433;&#21709;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#22810;&#26679;&#21270;&#22270;&#20687;&#30340;&#33021;&#21147;&#65292;&#24182;&#20998;&#20026;&#31867;&#20869;&#21644;&#31867;&#38388;&#20004;&#31181;&#31867;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20004;&#31181;&#27169;&#24335;&#22604;&#38519;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#21512;&#25104;X&#23556;&#32447;&#22270;&#20687;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#23558;&#33258;&#36866;&#24212;&#36755;&#20837;&#22270;&#20687;&#24402;&#19968;&#21270;&#26041;&#27861;&#19982;&#28145;&#24230;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#35299;&#20915;&#27169;&#24335;&#22604;&#38519;&#38382;&#39064;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical image datasets can be imbalanced due to the rarity of targeted diseases. Generative Adversarial Networks play a key role in addressing this imbalance by enabling the generation of synthetic images to augment datasets. It is important to generate synthetic images that incorporate a diverse range of features to accurately represent the distribution of features present in the training imagery. Furthermore, the absence of diverse features in synthetic images can degrade the performance of machine learning classifiers. The mode collapse problem impacts Generative Adversarial Networks' capacity to generate diversified images. Mode collapse comes in two varieties: intra-class and inter-class. In this paper, both varieties of the mode collapse problem are investigated, and their subsequent impact on the diversity of synthetic X-ray images is evaluated. This work contributes an empirical demonstration of the benefits of integrating the adaptive input-image normalization with the Deep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21333;&#22270;&#20687;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#32852;&#21512;&#20998;&#21106;&#21644;&#21435;&#22122;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#26679;&#26412;&#65292;&#19988;&#33021;&#22815;&#22788;&#29702;&#39640;&#22122;&#22768;&#21644;&#36890;&#29992;&#32441;&#29702;&#65292;&#24182;&#22312;&#26174;&#24494;&#38236;&#22270;&#20687;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.10511</link><description>&lt;p&gt;
&#22522;&#20110;&#21333;&#22270;&#20687;&#30340;&#26080;&#30417;&#30563;&#32852;&#21512;&#20998;&#21106;&#21644;&#21435;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Single-Image based unsupervised joint segmentation and denoising. (arXiv:2309.10511v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21333;&#22270;&#20687;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#32852;&#21512;&#20998;&#21106;&#21644;&#21435;&#22122;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#26679;&#26412;&#65292;&#19988;&#33021;&#22815;&#22788;&#29702;&#39640;&#22122;&#22768;&#21644;&#36890;&#29992;&#32441;&#29702;&#65292;&#24182;&#22312;&#26174;&#24494;&#38236;&#22270;&#20687;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#22270;&#20687;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#32852;&#21512;&#20998;&#21106;&#21644;&#21435;&#22122;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#21464;&#20998;&#20998;&#21106;&#26041;&#27861;&#30340;&#20248;&#21183;&#19982;&#33258;&#30417;&#30563;&#12289;&#22522;&#20110;&#21333;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#33021;&#21147;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#20248;&#28857;&#22312;&#20110;&#65292;&#19982;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#26679;&#26412;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#24211;&#30340;&#24773;&#20917;&#19979;&#23558;&#22270;&#20687;&#20998;&#21106;&#25104;&#22810;&#20010;&#26377;&#24847;&#20041;&#30340;&#21306;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#20854;&#20013;&#21435;&#22122;&#21644;&#20998;&#21106;&#20197;&#19968;&#31181;&#30456;&#20114;&#21463;&#30410;&#30340;&#26041;&#24335;&#32806;&#21512;&#22312;&#19968;&#36215;&#12290;&#36890;&#36807;&#19982;&#33258;&#30417;&#30563;&#22270;&#20687;&#21435;&#22122;&#30340;&#29305;&#23450;&#32452;&#21512;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#29616;&#26377;&#30340;&#21333;&#22270;&#20687;&#22522;&#20110;&#21464;&#20998;&#20998;&#21106;&#26041;&#27861;&#23545;&#39640;&#22122;&#22768;&#25110;&#36890;&#29992;&#32441;&#29702;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#26174;&#24494;&#38236;&#20013;&#21487;&#29992;&#30340;&#38750;&#24120;&#22024;&#26434;&#30340;&#22270;&#20687;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we develop an unsupervised method for the joint segmentation and denoising of a single image. To this end, we combine the advantages of a variational segmentation method with the power of a self-supervised, single-image based deep learning approach. One major strength of our method lies in the fact, that in contrast to data-driven methods, where huge amounts of labeled samples are necessary, our model can segment an image into multiple meaningful regions without any training database. Further, we introduce a novel energy functional in which denoising and segmentation are coupled in a way that both tasks benefit from each other. The limitations of existing single-image based variational segmentation methods, which are not capable of dealing with high noise or generic texture, are tackled by this specific combination with self-supervised image denoising. We propose a unified optimisation strategy and show that, especially for very noisy images available in microscopy, our p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24314;&#27169;&#30340;&#22810;&#38454;&#27573;&#20915;&#31574;&#36807;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#23545;&#24515;&#33039;&#20877;&#21516;&#27493;&#27835;&#30103;&#30340;&#21453;&#24212;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#25512;&#33616;&#25910;&#38598;&#39069;&#22806;&#30340;SPECT MPI&#21464;&#37327;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08415</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;CRT&#30340;&#22810;&#38454;&#27573;&#20915;&#31574;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A new method of modeling the multi-stage decision-making process of CRT using machine learning with uncertainty quantification. (arXiv:2309.08415v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24314;&#27169;&#30340;&#22810;&#38454;&#27573;&#20915;&#31574;&#36807;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#23545;&#24515;&#33039;&#20877;&#21516;&#27493;&#27835;&#30103;&#30340;&#21453;&#24212;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#25512;&#33616;&#25910;&#38598;&#39069;&#22806;&#30340;SPECT MPI&#21464;&#37327;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22810;&#38454;&#27573;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#24515;&#21147;&#34928;&#31469;&#65288;HF&#65289;&#24739;&#32773;&#24515;&#33039;&#20877;&#21516;&#27493;&#27835;&#30103;&#65288;CRT&#65289;&#30340;&#21453;&#24212;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#25512;&#33616;&#22312;&#22522;&#32447;&#20020;&#24202;&#21464;&#37327;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#30340;&#29305;&#24449;&#19981;&#36275;&#26102;&#25910;&#38598;&#39069;&#22806;&#30340;&#21333;&#20809;&#23376;&#21457;&#23556;&#35745;&#31639;&#26426;&#20307;&#23618;&#25668;&#24433;&#24515;&#32908;&#28748;&#27880;&#26174;&#20687;&#65288;SPECT MPI&#65289;&#21464;&#37327;&#12290;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#32435;&#20837;&#20102;218&#21517;&#25509;&#21463;&#38745;&#24687;&#38376;&#25511;SPECT MPI&#30340;&#24739;&#32773;&#12290;CRT&#21453;&#24212;&#34987;&#23450;&#20041;&#20026;6&#20010;&#26376;&#38543;&#35775;&#26102;&#24038;&#23460;&#23556;&#34880;&#20998;&#25968;&#65288;LVEF&#65289;&#22686;&#21152;&gt; 5%&#12290;&#36890;&#36807;&#32452;&#21512;&#20004;&#20010;&#38598;&#25104;&#27169;&#22411;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#32467;&#26524;&#12290;CRT&#30340;&#21453;&#24212;&#29575;&#20026;55.5%&#65288;n = 121&#65289;&#65292;&#25972;&#20307;&#30007;&#24615;&#21344;61.0%&#65288;n = 133&#65289;&#65292;&#24179;&#22343;&#24180;&#40836;62.0&#23681;&#65292;LVEF&#20026;27.7&#12290;&#35813;&#22810;&#38454;&#27573;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#38598;&#25104;&#27169;&#22411;2&#65288;&#21033;&#29992;&#20102;&#39069;&#22806;&#30340;SPECT&#25968;&#25454;&#65289;&#30456;&#20284;&#65292;AUC&#20998;&#21035;&#20026;0.75&#21644;0.77&#65292;&#20934;&#30830;&#24615;&#20998;&#21035;&#20026;0.71&#21644;...
&lt;/p&gt;
&lt;p&gt;
Aims. The purpose of this study is to create a multi-stage machine learning model to predict cardiac resynchronization therapy (CRT) response for heart failure (HF) patients. This model exploits uncertainty quantification to recommend additional collection of single-photon emission computed tomography myocardial perfusion imaging (SPECT MPI) variables if baseline clinical variables and features from electrocardiogram (ECG) are not sufficient. Methods. 218 patients who underwent rest-gated SPECT MPI were enrolled in this study. CRT response was defined as an increase in left ventricular ejection fraction (LVEF) &gt; 5% at a 6 month follow-up. A multi-stage ML model was created by combining two ensemble models. Results. The response rate for CRT was 55.5% (n = 121) with overall male gender 61.0% (n = 133), an average age of 62.0, and LVEF of 27.7. The multi-stage model performed similarly to Ensemble 2 (which utilized the additional SPECT data) with AUC of 0.75 vs. 0.77, accuracy of 0.71 vs
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;R-DISCOVER&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#26377;&#38480;&#19988;&#26377;&#22122;&#22768;&#30340;&#25968;&#25454;&#20013;&#31283;&#20581;&#22320;&#25581;&#31034;&#24320;&#25918;&#24335;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#31526;&#21495;&#34920;&#31034;&#21644;&#24378;&#21270;&#23398;&#20064;&#24341;&#23548;&#19979;&#30340;&#28151;&#21512;PDE&#29983;&#25104;&#22120;&#65292;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#21457;&#29616;&#21644;&#23884;&#20837;PDE&#65292;&#24182;&#36873;&#25321;&#34920;&#29616;&#26368;&#20339;&#30340;PDE&#36827;&#34892;&#31995;&#32479;&#21709;&#24212;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.07672</link><description>&lt;p&gt;
&#20174;&#26377;&#38480;&#19988;&#26377;&#22122;&#22768;&#30340;&#25968;&#25454;&#20013;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#30340;&#31283;&#20581;&#23398;&#20064;&#24320;&#25918;&#24335;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Physics-constrained robust learning of open-form PDEs from limited and noisy data. (arXiv:2309.07672v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;R-DISCOVER&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#26377;&#38480;&#19988;&#26377;&#22122;&#22768;&#30340;&#25968;&#25454;&#20013;&#31283;&#20581;&#22320;&#25581;&#31034;&#24320;&#25918;&#24335;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#31526;&#21495;&#34920;&#31034;&#21644;&#24378;&#21270;&#23398;&#20064;&#24341;&#23548;&#19979;&#30340;&#28151;&#21512;PDE&#29983;&#25104;&#22120;&#65292;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#21457;&#29616;&#21644;&#23884;&#20837;PDE&#65292;&#24182;&#36873;&#25321;&#34920;&#29616;&#26368;&#20339;&#30340;PDE&#36827;&#34892;&#31995;&#32479;&#21709;&#24212;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#22522;&#26412;&#25511;&#21046;&#26041;&#31243;&#22312;&#36935;&#21040;&#22122;&#22768;&#35266;&#27979;&#21644;&#27809;&#26377;&#29616;&#25104;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;R-DISCOVER&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20174;&#26377;&#38480;&#21644;&#26377;&#22122;&#22768;&#30340;&#25968;&#25454;&#20013;&#31283;&#20581;&#22320;&#25581;&#31034;&#24320;&#25918;&#24335;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20004;&#20010;&#20132;&#26367;&#26356;&#26032;&#36807;&#31243;&#36827;&#34892;&#25805;&#20316;&#65306;&#21457;&#29616;&#21644;&#23884;&#20837;&#12290;&#21457;&#29616;&#38454;&#27573;&#21033;&#29992;&#31526;&#21495;&#34920;&#31034;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24341;&#23548;&#19979;&#30340;&#28151;&#21512;PDE&#29983;&#25104;&#22120;&#65292;&#39640;&#25928;&#22320;&#20135;&#29983;&#20855;&#26377;&#26641;&#32467;&#26500;&#30340;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#24335;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#27169;&#22411;&#36866;&#24212;&#31995;&#32479;&#21709;&#24212;&#24182;&#20316;&#20026;&#29983;&#25104;&#30340;PDE&#30340;&#22870;&#21169;&#35780;&#20272;&#22120;&#12290;&#21033;&#29992;&#25311;&#21512;&#25928;&#26524;&#36739;&#22909;&#30340;PDE&#36890;&#36807;RL&#26041;&#27861;&#36827;&#34892;&#36845;&#20195;&#20248;&#21270;&#29983;&#25104;&#22120;&#65292;&#24182;&#36890;&#36807;&#26080;&#21442;&#25968;&#31283;&#23450;&#24230;&#25351;&#26631;&#36873;&#25321;&#34920;&#29616;&#26368;&#20339;&#30340;PDE&#12290;&#23884;&#20837;&#38454;&#27573;&#23558;&#26368;&#21021;&#20174;&#21457;&#29616;&#36807;&#31243;&#20013;&#30830;&#23450;&#30340;PDE&#19982;&#36924;&#36817;&#30495;&#23454;&#31995;&#32479;&#36827;&#34892;&#39057;&#35889;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unveiling the underlying governing equations of nonlinear dynamic systems remains a significant challenge, especially when encountering noisy observations and no prior knowledge available. This study proposes R-DISCOVER, a framework designed to robustly uncover open-form partial differential equations (PDEs) from limited and noisy data. The framework operates through two alternating update processes: discovering and embedding. The discovering phase employs symbolic representation and a reinforcement learning (RL)-guided hybrid PDE generator to efficiently produce diverse open-form PDEs with tree structures. A neural network-based predictive model fits the system response and serves as the reward evaluator for the generated PDEs. PDEs with superior fits are utilized to iteratively optimize the generator via the RL method and the best-performing PDE is selected by a parameter-free stability metric. The embedding phase integrates the initially identified PDE from the discovering process a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#21644;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#38388;&#25509;&#25554;&#34917;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#26469;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04160</link><description>&lt;p&gt;
&#21033;&#29992;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#26469;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity. (arXiv:2309.04160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#21644;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#38388;&#25509;&#25554;&#34917;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#26469;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#32463;&#24120;&#21576;&#29616;&#31232;&#30095;&#29305;&#24449;&#65292;&#32473;&#39044;&#27979;&#24314;&#27169;&#24102;&#26469;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#30452;&#25509;&#25554;&#34917;&#26041;&#27861;&#65288;&#22914;&#30697;&#38453;&#25554;&#34917;&#26041;&#27861;&#65289;&#20381;&#36182;&#20110;&#21442;&#32771;&#31867;&#20284;&#34892;&#25110;&#21015;&#26469;&#23436;&#25104;&#21407;&#22987;&#32570;&#22833;&#25968;&#25454;&#65292;&#19981;&#21306;&#20998;&#25554;&#34917;&#21644;&#23454;&#38469;&#20540;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#23558;&#19982;&#39044;&#27979;&#30446;&#26631;&#26080;&#20851;&#30340;&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#30340;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#20174;&#32780;&#25439;&#23475;&#19979;&#28216;&#24615;&#33021;&#30340;&#25928;&#26524;&#12290;&#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#23581;&#35797;&#22312;&#30452;&#25509;&#25554;&#34917;&#21518;&#37325;&#26032;&#26657;&#20934;&#25110;&#22686;&#24378;EHR&#23884;&#20837;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#38169;&#35823;&#22320;&#20248;&#20808;&#32771;&#34385;&#25554;&#34917;&#29305;&#24449;&#12290;&#36825;&#31181;&#20248;&#20808;&#38169;&#35823;&#21487;&#33021;&#20250;&#32473;&#27169;&#22411;&#24341;&#20837;&#20559;&#35265;&#25110;&#19981;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#37319;&#29992;&#38388;&#25509;&#25554;&#34917;&#65292;&#21033;&#29992;&#31867;&#20284;&#24739;&#32773;&#30340;&#21407;&#22411;&#34920;&#31034;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#12290;&#35748;&#35782;&#21040;&#22312;&#34913;&#37327;&#26102;&#36890;&#24120;&#23558;&#32570;&#22833;&#29305;&#24449;&#19982;&#23384;&#22312;&#29305;&#24449;&#30456;&#21516;&#30340;&#38480;&#21046;&#26102;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Record (EHR) data frequently exhibits sparse characteristics, posing challenges for predictive modeling. Current direct imputation such as matrix imputation approaches hinge on referencing analogous rows or columns to complete raw missing data and do not differentiate between imputed and actual values. As a result, models may inadvertently incorporate irrelevant or deceptive information with respect to the prediction objective, thereby compromising the efficacy of downstream performance. While some methods strive to recalibrate or augment EHR embeddings after direct imputation, they often mistakenly prioritize imputed features. This misprioritization can introduce biases or inaccuracies into the model. To tackle these issues, our work resorts to indirect imputation, where we leverage prototype representations from similar patients to obtain a denser embedding. Recognizing the limitation that missing features are typically treated the same as present ones when measurin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;YOLOv8&#27169;&#22411;&#21644;&#21019;&#26032;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#30340;&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20004;&#38454;&#27573;&#39044;&#27979;&#31574;&#30053;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20803;&#32032;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#20248;&#20110;&#21333;&#20010;&#22522;&#30784;&#26550;&#26500;&#65292;&#24182;&#35299;&#20915;&#20102;BaDLAD&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;OCR&#21644;&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.00848</link><description>&lt;p&gt;
&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;-&#19968;&#31181;&#22522;&#20110;YOLOv8&#30340;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bengali Document Layout Analysis -- A YOLOV8 Based Ensembling Approach. (arXiv:2309.00848v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;YOLOv8&#27169;&#22411;&#21644;&#21019;&#26032;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#30340;&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20004;&#38454;&#27573;&#39044;&#27979;&#31574;&#30053;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20803;&#32032;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#20248;&#20110;&#21333;&#20010;&#22522;&#30784;&#26550;&#26500;&#65292;&#24182;&#35299;&#20915;&#20102;BaDLAD&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;OCR&#21644;&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20391;&#37325;&#20110;&#21033;&#29992;YOLOv8&#27169;&#22411;&#21644;&#21019;&#26032;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#25552;&#21319;&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#65288;DLA&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#20197;&#24212;&#23545;&#23391;&#21152;&#25289;&#22797;&#26434;&#25991;&#23383;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#32463;&#36807;&#20005;&#26684;&#30340;&#39564;&#35777;&#38598;&#35780;&#20272;&#65292;&#23545;&#23436;&#25972;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20803;&#32032;&#20998;&#21106;&#30340;&#20004;&#38454;&#27573;&#39044;&#27979;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#38598;&#25104;&#27169;&#22411;&#32467;&#21512;&#21518;&#22788;&#29702;&#24615;&#33021;&#20248;&#20110;&#21333;&#20010;&#22522;&#30784;&#26550;&#26500;&#65292;&#35299;&#20915;&#20102;BaDLAD&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#26088;&#22312;&#25512;&#21160;&#23391;&#21152;&#25289;&#25991;&#26723;&#20998;&#26512;&#30340;&#21457;&#23637;&#65292;&#25552;&#39640;OCR&#21644;&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#65292;&#21516;&#26102;BaDLAD&#20316;&#20026;&#22522;&#30784;&#36164;&#28304;&#26377;&#21161;&#20110;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#20026;&#23558;&#26032;&#31574;&#30053;&#32435;&#20837;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on enhancing Bengali Document Layout Analysis (DLA) using the YOLOv8 model and innovative post-processing techniques. We tackle challenges unique to the complex Bengali script by employing data augmentation for model robustness. After meticulous validation set evaluation, we fine-tune our approach on the complete dataset, leading to a two-stage prediction strategy for accurate element segmentation. Our ensemble model, combined with post-processing, outperforms individual base architectures, addressing issues identified in the BaDLAD dataset. By leveraging this approach, we aim to advance Bengali document analysis, contributing to improved OCR and document comprehension and BaDLAD serves as a foundational resource for this endeavor, aiding future research in the field. Furthermore, our experiments provided key insights to incorporate new strategies into the established solution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65288;PINN&#65289;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#35745;&#31639;&#35777;&#25454;&#26469;&#20248;&#21270;&#27169;&#22411;&#24182;&#35299;&#20915;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13222</link><description>&lt;p&gt;
&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Bayesian Reasoning for Physics Informed Neural Networks. (arXiv:2308.13222v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65288;PINN&#65289;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#35745;&#31639;&#35777;&#25454;&#26469;&#20248;&#21270;&#27169;&#22411;&#24182;&#35299;&#20915;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20844;&#24335;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;MacKay&#22312;Neural Computation&#65288;1992&#24180;&#65289;&#20013;&#25552;&#20986;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#12290;&#36890;&#36807;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#27861;&#65292;&#24471;&#21040;&#21518;&#39564;&#23494;&#24230;&#12290;&#23545;&#20110;&#27599;&#20010;&#27169;&#22411;&#65288;&#25311;&#21512;&#65289;&#65292;&#35745;&#31639;&#25152;&#35859;&#30340;&#35777;&#25454;&#12290;&#23427;&#26159;&#19968;&#31181;&#20998;&#31867;&#20551;&#35774;&#30340;&#24230;&#37327;&#12290;&#26368;&#20248;&#35299;&#20855;&#26377;&#26368;&#22823;&#30340;&#35777;&#25454;&#20540;&#12290;&#36125;&#21494;&#26031;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#25511;&#21046;&#36793;&#30028;&#23545;&#24635;&#25439;&#22833;&#30340;&#24433;&#21709;&#12290;&#20107;&#23454;&#19978;&#65292;&#36125;&#21494;&#26031;&#31639;&#27861;&#36890;&#36807;&#24494;&#35843;&#25439;&#22833;&#32452;&#20214;&#30340;&#30456;&#23545;&#26435;&#37325;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#28909;&#21147;&#23398;&#12289;&#27874;&#21160;&#21644;Burger&#26041;&#31243;&#12290;&#25152;&#24471;&#32467;&#26524;&#19982;&#31934;&#30830;&#35299;&#22522;&#26412;&#19968;&#33268;&#12290;&#25152;&#26377;&#35299;&#37117;&#25552;&#20379;&#20102;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#20869;&#35745;&#31639;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics informed neural network (PINN) approach in Bayesian formulation is presented. We adopt the Bayesian neural network framework formulated by MacKay (Neural Computation 4 (3) (1992) 448). The posterior densities are obtained from Laplace approximation. For each model (fit), the so-called evidence is computed. It is a measure that classifies the hypothesis. The most optimal solution has the maximal value of the evidence. The Bayesian framework allows us to control the impact of the boundary contribution to the total loss. Indeed, the relative weights of loss components are fine-tuned by the Bayesian algorithm. We solve heat, wave, and Burger's equations. The obtained results are in good agreement with the exact solutions. All solutions are provided with the uncertainties computed within the Bayesian framework.
&lt;/p&gt;</description></item><item><title>&#39044;&#38376;&#25511;MoE&#31995;&#32479;&#36890;&#36807;&#31639;&#27861;&#21644;&#31995;&#32479;&#30340;&#20849;&#21516;&#35774;&#35745;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;MoE&#26550;&#26500;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12066</link><description>&lt;p&gt;
&#39044;&#38376;&#25511;MoE&#65306;&#24555;&#36895;&#19988;&#21487;&#25193;&#23637;&#28151;&#21512;&#19987;&#23478;&#25512;&#29702;&#30340;&#31639;&#27861;&#21644;&#31995;&#32479;&#20849;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference. (arXiv:2308.12066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12066
&lt;/p&gt;
&lt;p&gt;
&#39044;&#38376;&#25511;MoE&#31995;&#32479;&#36890;&#36807;&#31639;&#27861;&#21644;&#31995;&#32479;&#30340;&#20849;&#21516;&#35774;&#35745;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;MoE&#26550;&#26500;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#22522;&#20110;transformers&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20854;&#25104;&#21151;&#28304;&#20110;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#12290;&#23613;&#31649;&#31639;&#27861;&#24615;&#33021;&#24456;&#39640;&#65292;&#20294;LLMs&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#24102;&#26469;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;LLMs&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;&#19981;&#25104;&#27604;&#20363;&#22320;&#25193;&#22823;&#35745;&#31639;&#38656;&#27714;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#27169;&#22411;&#22823;&#23567;&#12290;&#28982;&#32780;&#65292;MoE&#30340;&#39640;&#23384;&#20648;&#38656;&#27714;&#21644;&#31232;&#30095;&#19987;&#23478;&#30340;&#21160;&#24577;&#28608;&#27963;&#38480;&#21046;&#20102;&#20854;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20043;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#23558;MoE&#30340;&#20869;&#23384;&#21344;&#29992;&#39640;&#30340;&#19987;&#23478;&#21442;&#25968;&#36716;&#31227;&#21040;CPU&#20869;&#23384;&#19978;&#65292;&#20294;&#26159;&#20174;CPU&#36801;&#31227;&#24050;&#28608;&#27963;&#30340;&#19987;&#23478;&#21040;GPU&#30340;&#24310;&#36831;&#23548;&#33268;&#20102;&#39640;&#24615;&#33021;&#24320;&#38144;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#39044;&#38376;&#25511;MoE&#31995;&#32479;&#36890;&#36807;&#31639;&#27861;&#21644;&#31995;&#32479;&#30340;&#20849;&#21516;&#35774;&#35745;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;MoE&#26550;&#26500;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) based on transformers have made significant strides in recent years, the success of which is driven by scaling up their model size. Despite their high algorithmic performance, the computational and memory requirements of LLMs present unprecedented challenges. To tackle the high compute requirements of LLMs, the Mixture-of-Experts (MoE) architecture was introduced which is able to scale its model size without proportionally scaling up its computational requirements. Unfortunately, MoE's high memory demands and dynamic activation of sparse experts restrict its applicability to real-world problems. Previous solutions that offload MoE's memory-hungry expert parameters to CPU memory fall short because the latency to migrate activated experts from CPU to GPU incurs high performance overhead. Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system co-design. Pre-gated MoE 
&lt;/p&gt;</description></item><item><title>&#28145;&#23618;&#25805;&#20316;&#31526;&#32593;&#32476;&#65288;DeepONet&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26367;&#20195;&#24314;&#27169;&#26041;&#27861;&#65292;&#22312;&#26680;&#31995;&#32479;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#20013;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#39044;&#27979;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#21253;&#25324;&#26368;&#20339;&#20256;&#24863;&#22120;&#25918;&#32622;&#21644;&#27169;&#22411;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.07523</link><description>&lt;p&gt;
&#28145;&#23618;&#25805;&#20316;&#31526;&#32593;&#32476;&#22312;&#26680;&#31995;&#32479;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Potential of Deep Operator Networks in Digital Twin-enabling Technology for Nuclear System. (arXiv:2308.07523v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07523
&lt;/p&gt;
&lt;p&gt;
&#28145;&#23618;&#25805;&#20316;&#31526;&#32593;&#32476;&#65288;DeepONet&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26367;&#20195;&#24314;&#27169;&#26041;&#27861;&#65292;&#22312;&#26680;&#31995;&#32479;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#20013;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#39044;&#27979;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#21253;&#25324;&#26368;&#20339;&#20256;&#24863;&#22120;&#25918;&#32622;&#21644;&#27169;&#22411;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#26680;&#24037;&#31243;&#30340;&#25968;&#23383;&#23402;&#29983;&#31995;&#32479;&#20013;&#24341;&#20837;&#20102;&#28145;&#23618;&#25805;&#20316;&#31526;&#32593;&#32476;&#65288;DeepONet&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26367;&#20195;&#24314;&#27169;&#26041;&#27861;&#12290;&#38543;&#30528;&#26680;&#33021;&#20316;&#20026;&#19968;&#31181;&#30899;&#20013;&#21644;&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#37319;&#29992;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;&#26680;&#24037;&#31243;&#24212;&#29992;&#20013;&#30340;&#36816;&#33829;&#25928;&#29575;&#12289;&#23433;&#20840;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;DeepONet&#20855;&#26377;&#26174;&#33879;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;DeepONet&#22312;&#35299;&#20915;&#22797;&#26434;&#31890;&#23376;&#20256;&#36755;&#38382;&#39064;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#36890;&#36807;&#23558;&#20989;&#25968;&#20316;&#20026;&#36755;&#20837;&#25968;&#25454;&#24182;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#26500;&#24314;&#25805;&#20316;&#31526;G&#65292;DeepONet&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#22810;&#26679;&#21270;&#21644;&#22797;&#26434;&#30340;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;DeepONet&#30340;&#24212;&#29992;&#20063;&#25581;&#31034;&#20102;&#19982;&#26368;&#20339;&#20256;&#24863;&#22120;&#25918;&#32622;&#21644;&#27169;&#22411;&#35780;&#20272;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#23454;&#38469;&#23454;&#26045;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research introduces the Deep Operator Network (DeepONet) as a robust surrogate modeling method within the context of digital twin (DT) systems for nuclear engineering. With the increasing importance of nuclear energy as a carbon-neutral solution, adopting DT technology has become crucial to enhancing operational efficiencies, safety, and predictive capabilities in nuclear engineering applications. DeepONet exhibits remarkable prediction accuracy, outperforming traditional ML methods. Through extensive benchmarking and evaluation, this study showcases the scalability and computational efficiency of DeepONet in solving a challenging particle transport problem. By taking functions as input data and constructing the operator $G$ from training data, DeepONet can handle diverse and complex scenarios effectively. However, the application of DeepONet also reveals challenges related to optimal sensor placement and model evaluation, critical aspects of real-world implementation. Addressing 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#21560;&#25910;&#20809;&#35889;&#26469;&#21152;&#36895;&#27979;&#37327;&#25530;&#26434;&#32858;&#21512;&#29289;&#26448;&#26009;&#30340;&#30005;&#23548;&#29575;&#12290;&#20998;&#31867;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#20998;&#31867;&#30005;&#23548;&#29575;&#22823;&#20110;~25&#33267;100 S/cm&#30340;&#26679;&#21697;&#65292;&#22238;&#24402;&#27169;&#22411;&#33021;&#22815;&#31934;&#30830;&#39044;&#27979;&#39640;&#23548;&#30005;&#24615;&#26679;&#21697;&#30340;&#30005;&#23548;&#29575;&#12290;&#35813;&#26041;&#27861;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.04103</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#20197;&#23454;&#29616;&#25530;&#26434;&#20849;&#36717;&#32858;&#21512;&#29289;&#30340;&#39640;&#36890;&#37327;&#30005;&#23548;&#29575;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Explainable machine learning to enable high-throughput electrical conductivity optimization of doped conjugated polymers. (arXiv:2308.04103v1 [physics.app-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04103
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#21560;&#25910;&#20809;&#35889;&#26469;&#21152;&#36895;&#27979;&#37327;&#25530;&#26434;&#32858;&#21512;&#29289;&#26448;&#26009;&#30340;&#30005;&#23548;&#29575;&#12290;&#20998;&#31867;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#20998;&#31867;&#30005;&#23548;&#29575;&#22823;&#20110;~25&#33267;100 S/cm&#30340;&#26679;&#21697;&#65292;&#22238;&#24402;&#27169;&#22411;&#33021;&#22815;&#31934;&#30830;&#39044;&#27979;&#39640;&#23548;&#30005;&#24615;&#26679;&#21697;&#30340;&#30005;&#23548;&#29575;&#12290;&#35813;&#26041;&#27861;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36890;&#37327;&#23454;&#39564;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#32467;&#21512;&#26368;&#36817;&#24320;&#21551;&#20102;&#21152;&#36895;&#26448;&#26009;&#21457;&#29616;&#30340;&#26032;&#26102;&#20195;&#65292;&#20351;&#24471;&#33021;&#22815;&#35782;&#21035;&#20855;&#26377;&#23574;&#31471;&#24615;&#33021;&#30340;&#26448;&#26009;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#29289;&#29702;&#37327;&#30340;&#27979;&#37327;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35201;&#23454;&#29616;&#25530;&#26434;&#32858;&#21512;&#29289;&#26448;&#26009;&#20013;&#30340;&#26368;&#20339;&#30005;&#23548;&#29575;&#65292;&#38656;&#35201;&#35880;&#24910;&#30340;&#24037;&#33402;&#25511;&#21046;&#12289;&#23454;&#39564;&#21644;&#32321;&#29712;&#30340;&#27979;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#26131;&#20110;&#27979;&#37327;&#30340;&#21560;&#25910;&#20809;&#35889;&#65292;&#21152;&#36895;&#20102;&#27979;&#37327;&#30005;&#23548;&#29575;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#31532;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#20998;&#31867;&#27169;&#22411;&#65289;&#33021;&#22815;&#20934;&#30830;&#22320;&#20998;&#31867;&#30005;&#23548;&#29575;&#22823;&#20110;~25&#33267;100 S/cm&#30340;&#26679;&#21697;&#65292;&#36798;&#21040;100%&#30340;&#20934;&#30830;&#29575;&#12290;&#23545;&#20110;&#20855;&#26377;&#39640;&#23548;&#30005;&#24615;&#30340;&#23376;&#38598;&#26679;&#21697;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#31532;&#20108;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#22238;&#24402;&#27169;&#22411;&#65289;&#26469;&#39044;&#27979;&#23427;&#20204;&#30340;&#30005;&#23548;&#29575;&#65292;&#24471;&#21040;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27979;&#35797;R2&#20540;&#20026;0.984&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#27604;&#36739;&#22312;&#30005;&#23548;&#29575;&#20248;&#21270;&#26041;&#38754;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The combination of high-throughput experimentation techniques and machine learning (ML) has recently ushered in a new era of accelerated material discovery, enabling the identification of materials with cutting-edge properties. However, the measurement of certain physical quantities remains challenging to automate. Specifically, meticulous process control, experimentation and laborious measurements are required to achieve optimal electrical conductivity in doped polymer materials. We propose a ML approach, which relies on readily measured absorbance spectra, to accelerate the workflow associated with measuring electrical conductivity. The first ML model (classification model), accurately classifies samples with a conductivity &gt;~25 to 100 S/cm, achieving a maximum of 100% accuracy rate. For the subset of highly conductive samples, we employed a second ML model (regression model), to predict their conductivities, yielding an impressive test R2 value of 0.984. To validate the approach, we
&lt;/p&gt;</description></item><item><title>PePNet&#26159;&#19968;&#31181;&#25903;&#25345;&#32597;&#35265;&#37325;&#36127;&#36733;&#30340;&#24037;&#20316;&#36127;&#36733;&#39044;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#21608;&#26399;&#24615;&#24863;&#30693;&#26426;&#21046;&#21644;&#34701;&#21512;&#22810;&#23610;&#24230;&#24207;&#21015;&#23398;&#20064;&#30340;&#33021;&#21147;&#25552;&#39640;&#20102;&#25972;&#20307;&#29305;&#21035;&#26159;&#37325;&#36127;&#36733;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01917</link><description>&lt;p&gt;
PePNet: &#19968;&#31181;&#25903;&#25345;&#32597;&#35265;&#37325;&#36127;&#36733;&#30340;&#21608;&#26399;&#24615;&#24863;&#30693;&#24037;&#20316;&#36127;&#36733;&#39044;&#27979;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PePNet: A Periodicity-Perceived Workload Prediction Network Supporting Rare Occurrence of Heavy Workload. (arXiv:2308.01917v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01917
&lt;/p&gt;
&lt;p&gt;
PePNet&#26159;&#19968;&#31181;&#25903;&#25345;&#32597;&#35265;&#37325;&#36127;&#36733;&#30340;&#24037;&#20316;&#36127;&#36733;&#39044;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#21608;&#26399;&#24615;&#24863;&#30693;&#26426;&#21046;&#21644;&#34701;&#21512;&#22810;&#23610;&#24230;&#24207;&#21015;&#23398;&#20064;&#30340;&#33021;&#21147;&#25552;&#39640;&#20102;&#25972;&#20307;&#29305;&#21035;&#26159;&#37325;&#36127;&#36733;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#25552;&#20379;&#21830;&#21487;&#20197;&#20174;&#20934;&#30830;&#30340;&#24037;&#20316;&#36127;&#36733;&#39044;&#27979;&#20013;&#33719;&#24471;&#24040;&#22823;&#30340;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#20113;&#26381;&#21153;&#22120;&#30340;&#24037;&#20316;&#36127;&#36733;&#39640;&#24230;&#21464;&#21270;&#65292;&#26377;&#26102;&#20250;&#21457;&#29983;&#37325;&#36127;&#36733;&#31361;&#21457;&#20107;&#20214;&#65292;&#36825;&#20351;&#24471;&#24037;&#20316;&#36127;&#36733;&#39044;&#27979;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#26377;&#20004;&#31181;&#20027;&#35201;&#30340;&#24037;&#20316;&#36127;&#36733;&#39044;&#27979;&#26041;&#27861;&#65306;&#32479;&#35745;&#26041;&#27861;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#21069;&#32773;&#20381;&#36182;&#20110;&#24378;&#22823;&#30340;&#25968;&#23398;&#20551;&#35774;&#65292;&#24403;&#39044;&#27979;&#39640;&#24230;&#21464;&#21270;&#30340;&#24037;&#20316;&#36127;&#36733;&#26102;&#65292;&#20854;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#32780;&#21518;&#32773;&#22312;&#25972;&#20307;&#20934;&#30830;&#24615;&#19978;&#26356;&#39640;&#65292;&#20294;&#23481;&#26131;&#21463;&#21040;&#37325;&#36127;&#36733;&#21644;&#24120;&#35265;&#36127;&#36733;&#20043;&#38388;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#24433;&#21709;&#65292;&#36825;&#20250;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#37325;&#36127;&#36733;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#26080;&#35770;&#26159;&#32479;&#35745;&#26041;&#27861;&#30340;&#25972;&#20307;&#19981;&#20934;&#30830;&#24615;&#36824;&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#23545;&#37325;&#36127;&#36733;&#30340;&#19981;&#20934;&#30830;&#24615;&#37117;&#20250;&#23548;&#33268;&#26381;&#21153;&#32423;&#21035;&#21327;&#35758;&#30340;&#36829;&#35268;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PePNet&#26469;&#25552;&#39640;&#25972;&#20307;&#29305;&#21035;&#26159;&#37325;&#36127;&#36733;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#23427;&#20855;&#26377;&#20004;&#20010;&#29420;&#29305;&#30340;&#29305;&#28857;&#65306;&#21608;&#26399;&#24615;&#24863;&#30693;&#26426;&#21046;&#21644;&#34701;&#21512;&#22810;&#23610;&#24230;&#24207;&#21015;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud providers can greatly benefit from accurate workload prediction. However, the workload of cloud servers is highly variable, with occasional heavy workload bursts. This makes workload prediction challenging.  There are mainly two categories of workload prediction methods: statistical methods and neural-network-based ones. The former ones rely on strong mathematical assumptions and have reported low accuracy when predicting highly variable workload. The latter ones offer higher overall accuracy, yet they are vulnerable to data imbalance between heavy workload and common one. This impairs the prediction accuracy of neural network-based models on heavy workload.  Either the overall inaccuracy of statistic methods or the heavy-workload inaccuracy of neural-network-based models can cause service level agreement violations.  Thus, we propose PePNet to improve overall especially heavy workload prediction accuracy. It has two distinctive characteristics:  (i) A Periodicity-Perceived Mecha
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#31245;&#36874;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#27809;&#26377;&#19968;&#20010;&#27169;&#22411;&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.12114</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#30340;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks. (arXiv:2307.12114v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12114
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#31245;&#36874;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#27809;&#26377;&#19968;&#20010;&#27169;&#22411;&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#8212;&#8212;ChatGPT&#12289;Flan-T5 UL2&#12289;Tk-Instruct&#21644;Alpaca&#8212;&#8212;&#22312;13&#20010;&#23454;&#38469;&#19990;&#30028;&#30340;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#20363;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#12289;&#38382;&#31572;&#65288;QA&#65289;&#12289;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#31561;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#35780;&#20272;&#30340;LLM&#24320;&#22987;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#23545;&#20110;QA&#20219;&#21153;&#34920;&#29616;&#24471;&#29305;&#21035;&#22909;&#65292;&#21363;&#20351;&#23427;&#20204;&#20043;&#21069;&#27809;&#26377;&#35265;&#36807;&#36825;&#20123;&#20219;&#21153;&#30340;&#31034;&#20363;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#34920;&#29616;&#20302;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#65288;&#22914;PubMedBERT&#65289;&#21487;&#20197;&#36798;&#21040;&#30340;&#27700;&#24179;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#27809;&#26377;&#19968;&#20010;LLM&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#37117;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#20110;&#29305;&#23450;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and particularly well for the QA task, even though they have never seen examples from these tasks before. However, we observed that the classification and RE tasks perform below what can be achieved with a specifically trained model for the medical field, such as PubMedBERT. Finally, we noted that no LLM outperforms all the others on all the studied tasks, with some models being better suited for certain tasks than others.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#23545;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#22120;&#20013;&#25191;&#34892;&#31639;&#27861;&#26102;&#20135;&#29983;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#20004;&#31181;&#25925;&#38556;&#27169;&#24335;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;softmax&#32858;&#21512;&#22120;&#35299;&#20915;&#20998;&#36776;&#29575;&#20007;&#22833;&#38382;&#39064;&#65292;&#20197;&#21450;&#34928;&#20943;&#28508;&#22312;&#31354;&#38388;&#26469;&#22788;&#29702;&#36229;&#20986;&#33539;&#22260;&#30340;&#20540;&#65292;&#36825;&#20123;&#25913;&#21464;&#22312;&#26631;&#20934;CLRS-30&#22522;&#20934;&#27979;&#35797;&#20013;&#22823;&#22810;&#25968;&#31639;&#27861;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.08874</link><description>&lt;p&gt;
&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Latent Space Representations of Neural Algorithmic Reasoners. (arXiv:2307.08874v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08874
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#23545;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#22120;&#20013;&#25191;&#34892;&#31639;&#27861;&#26102;&#20135;&#29983;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#20004;&#31181;&#25925;&#38556;&#27169;&#24335;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;softmax&#32858;&#21512;&#22120;&#35299;&#20915;&#20998;&#36776;&#29575;&#20007;&#22833;&#38382;&#39064;&#65292;&#20197;&#21450;&#34928;&#20943;&#28508;&#22312;&#31354;&#38388;&#26469;&#22788;&#29702;&#36229;&#20986;&#33539;&#22260;&#30340;&#20540;&#65292;&#36825;&#20123;&#25913;&#21464;&#22312;&#26631;&#20934;CLRS-30&#22522;&#20934;&#27979;&#35797;&#20013;&#22823;&#22810;&#25968;&#31639;&#27861;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#65288;NAR&#65289;&#26159;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#35774;&#35745;&#33021;&#22815;&#21487;&#38752;&#22320;&#25429;&#25417;&#32463;&#20856;&#35745;&#31639;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#36890;&#24120;&#36890;&#36807;&#23398;&#20064;&#25191;&#34892;&#31639;&#27861;&#26469;&#23454;&#29616;&#12290;&#20856;&#22411;&#30340;&#26041;&#27861;&#26159;&#20381;&#36182;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#65292;&#23427;&#20204;&#23558;&#36755;&#20837;&#32534;&#30721;&#20026;&#39640;&#32500;&#28508;&#22312;&#31354;&#38388;&#65292;&#22312;&#31639;&#27861;&#25191;&#34892;&#26399;&#38388;&#21453;&#22797;&#36716;&#25442;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;GNN&#22312;&#25191;&#34892;&#31639;&#27861;&#26102;&#23548;&#33268;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#20004;&#31181;&#21487;&#33021;&#30340;&#25925;&#38556;&#27169;&#24335;&#65306;&#65288;i&#65289;&#20998;&#36776;&#29575;&#20007;&#22833;&#65292;&#20351;&#24471;&#38590;&#20197;&#21306;&#20998;&#30456;&#20284;&#30340;&#20540;&#65307;&#65288;ii&#65289;&#26080;&#27861;&#22788;&#29702;&#35757;&#32451;&#26399;&#38388;&#26410;&#35266;&#23519;&#21040;&#30340;&#20540;&#33539;&#22260;&#20043;&#22806;&#30340;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20381;&#36182;softmax&#32858;&#21512;&#22120;&#26469;&#35299;&#20915;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#24182;&#24314;&#35758;&#34928;&#20943;&#28508;&#22312;&#31354;&#38388;&#20197;&#22788;&#29702;&#36229;&#20986;&#33539;&#22260;&#30340;&#20540;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#21464;&#21270;&#22312;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26102;&#65292;&#22312;&#26631;&#20934;CLRS-30&#22522;&#20934;&#27979;&#35797;&#20013;&#22823;&#22810;&#25968;&#31639;&#27861;&#19978;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Algorithmic Reasoning (NAR) is a research area focused on designing neural architectures that can reliably capture classical computation, usually by learning to execute algorithms. A typical approach is to rely on Graph Neural Network (GNN) architectures, which encode inputs in high-dimensional latent spaces that are repeatedly transformed during the execution of the algorithm. In this work we perform a detailed analysis of the structure of the latent space induced by the GNN when executing algorithms. We identify two possible failure modes: (i) loss of resolution, making it hard to distinguish similar values; (ii) inability to deal with values outside the range observed during training. We propose to solve the first issue by relying on a softmax aggregator, and propose to decay the latent space in order to deal with out-of-range values. We show that these changes lead to improvements on the majority of algorithms in the standard CLRS-30 benchmark when using the state-of-the-art
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#22312;&#39640;&#38647;&#35834;&#25968;&#19979;&#21512;&#25104;&#25289;&#26684;&#26391;&#26085;&#28237;&#27969;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#31890;&#23376;&#36712;&#36857;&#30340;&#32479;&#35745;&#21644;&#25299;&#25169;&#24615;&#36136;&#30340;&#20934;&#30830;&#37325;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.08529</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#30340;&#21512;&#25104;&#25289;&#26684;&#26391;&#26085;&#28237;&#27969;
&lt;/p&gt;
&lt;p&gt;
Synthetic Lagrangian Turbulence by Generative Diffusion Models. (arXiv:2307.08529v1 [physics.flu-dyn] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08529
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#22312;&#39640;&#38647;&#35834;&#25968;&#19979;&#21512;&#25104;&#25289;&#26684;&#26391;&#26085;&#28237;&#27969;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#31890;&#23376;&#36712;&#36857;&#30340;&#32479;&#35745;&#21644;&#25299;&#25169;&#24615;&#36136;&#30340;&#20934;&#30830;&#37325;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25289;&#26684;&#26391;&#26085;&#28237;&#27969;&#26159;&#28041;&#21450;&#24037;&#31243;&#12289;&#29983;&#29289;&#27969;&#20307;&#12289;&#22823;&#27668;&#12289;&#28023;&#27915;&#21644;&#22825;&#20307;&#29289;&#29702;&#39046;&#22495;&#20013;&#30340;&#20998;&#25955;&#21644;&#28151;&#21512;&#29289;&#29702;&#30340;&#24212;&#29992;&#21644;&#22522;&#30784;&#24615;&#38382;&#39064;&#12290;&#23613;&#31649;&#36807;&#21435;&#19977;&#21313;&#24180;&#36827;&#34892;&#20102;&#21331;&#36234;&#30340;&#29702;&#35770;&#12289;&#25968;&#20540;&#21644;&#23454;&#39564;&#30740;&#31350;&#65292;&#20294;&#27809;&#26377;&#29616;&#26377;&#27169;&#22411;&#33021;&#22815;&#24544;&#23454;&#22320;&#37325;&#29616;&#28237;&#27969;&#20013;&#30340;&#31890;&#23376;&#36712;&#36857;&#25152;&#23637;&#31034;&#30340;&#32479;&#35745;&#21644;&#25299;&#25169;&#24615;&#36136;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#22312;&#39640;&#38647;&#35834;&#25968;&#19979;&#29983;&#25104;&#19977;&#32500;&#28237;&#27969;&#20013;&#30340;&#21333;&#31890;&#23376;&#36712;&#36857;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#33719;&#21462;&#21487;&#38752;&#30340;&#25289;&#26684;&#26391;&#26085;&#25968;&#25454;&#25152;&#38656;&#30340;&#30452;&#25509;&#25968;&#20540;&#27169;&#25311;&#25110;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#26126;&#65292;&#23427;&#33021;&#22815;&#23450;&#37327;&#22320;&#37325;&#29616;&#25972;&#20010;&#26102;&#38388;&#23610;&#24230;&#33539;&#22260;&#20869;&#30340;&#25152;&#26377;&#30456;&#20851;&#32479;&#35745;&#22522;&#20934;&#65292;&#21253;&#25324;&#36895;&#24230;&#22686;&#37327;&#30340;&#23614;&#37096;&#20998;&#24067;&#12289;&#24322;&#24120;&#24130;&#24459;&#21644;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Lagrangian turbulence lies at the core of numerous applied and fundamental problems related to the physics of dispersion and mixing in engineering, bio-fluids, atmosphere, oceans, and astrophysics. Despite exceptional theoretical, numerical, and experimental efforts conducted over the past thirty years, no existing models are capable of faithfully reproducing statistical and topological properties exhibited by particle trajectories in turbulence. We propose a machine learning approach, based on a state-of-the-art Diffusion Model, to generate single-particle trajectories in three-dimensional turbulence at high Reynolds numbers, thereby bypassing the need for direct numerical simulations or experiments to obtain reliable Lagrangian data. Our model demonstrates the ability to quantitatively reproduce all relevant statistical benchmarks over the entire range of time scales, including the presence of fat tails distribution for the velocity increments, anomalous power law, and enhancement of
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33258;&#36861;&#36394;&#21487;&#25240;&#21472;&#26234;&#33021;&#32442;&#32455;&#21697;&#65292;&#36890;&#36807;&#32467;&#21512;&#25240;&#21472;&#32455;&#29289;&#32467;&#26500;&#21644;&#30005;&#23481;&#20256;&#24863;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#26816;&#27979;&#32467;&#26500;&#36816;&#21160;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#30005;&#23481;&#20449;&#21495;&#20013;&#20934;&#30830;&#37325;&#26500;&#20986;&#29255;&#27573;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;</title><link>http://arxiv.org/abs/2307.05370</link><description>&lt;p&gt;
Capafoldable: &#20855;&#26377;&#30005;&#23481;&#20256;&#24863;&#33021;&#21147;&#30340;&#33258;&#36861;&#36394;&#21487;&#25240;&#21472;&#26234;&#33021;&#32442;&#32455;&#21697;
&lt;/p&gt;
&lt;p&gt;
Capafoldable: self-tracking foldable smart textiles with capacitive sensing. (arXiv:2307.05370v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05370
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33258;&#36861;&#36394;&#21487;&#25240;&#21472;&#26234;&#33021;&#32442;&#32455;&#21697;&#65292;&#36890;&#36807;&#32467;&#21512;&#25240;&#21472;&#32455;&#29289;&#32467;&#26500;&#21644;&#30005;&#23481;&#20256;&#24863;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#26816;&#27979;&#32467;&#26500;&#36816;&#21160;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#30005;&#23481;&#20449;&#21495;&#20013;&#20934;&#30830;&#37325;&#26500;&#20986;&#29255;&#27573;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25240;&#21472;&#26159;&#19968;&#31181;&#29420;&#29305;&#30340;&#32467;&#26500;&#25216;&#26415;&#65292;&#21487;&#20197;&#20351;&#24179;&#38754;&#26448;&#26009;&#20855;&#26377;&#36816;&#21160;&#25110;&#19977;&#32500;&#21147;&#23398;&#29305;&#24615;&#12290;&#22522;&#20110;&#32442;&#32455;&#21697;&#30340;&#30005;&#23481;&#20256;&#24863;&#24050;&#32463;&#34987;&#35777;&#26126;&#23545;&#23548;&#30005;&#32442;&#32455;&#21697;&#30340;&#20960;&#20309;&#24418;&#21464;&#21644;&#30456;&#23545;&#36816;&#21160;&#38750;&#24120;&#25935;&#24863;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33258;&#36861;&#36394;&#21487;&#25240;&#21472;&#26234;&#33021;&#32442;&#32455;&#21697;&#65292;&#23558;&#25240;&#21472;&#32455;&#29289;&#32467;&#26500;&#21644;&#30005;&#23481;&#20256;&#24863;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;&#20256;&#24863;&#30005;&#36335;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#26816;&#27979;&#32467;&#26500;&#36816;&#21160;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#20004;&#31181;&#25240;&#21472;&#27169;&#24335;&#65292;&#25163;&#39118;&#29748;&#21644;&#40831;&#24418;&#65292;&#27599;&#31181;&#27169;&#24335;&#20013;&#37117;&#26377;&#20004;&#31181;&#24067;&#23616;&#30340;&#30005;&#23481;&#20256;&#24863;&#22120;&#65292;&#20197;&#28909;&#31896;&#38468;&#30340;&#23548;&#30005;&#32442;&#32455;&#21697;&#29255;&#30340;&#24418;&#24335;&#23384;&#22312;&#12290;&#22312;&#25163;&#21160;&#31227;&#21160;&#25240;&#21472;&#27169;&#24335;&#30340;&#29255;&#27573;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#21644;&#37325;&#26500;&#29255;&#27573;&#30340;&#35270;&#35273;&#36319;&#36394;&#24418;&#29366;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#30005;&#23481;&#20449;&#21495;&#20013;&#37325;&#26500;&#23450;&#20041;&#29255;&#27573;&#24418;&#29366;&#30340;&#20960;&#20309;&#21407;&#35821;&#65292;R-squared&#20540;&#21487;&#36798;95&#65285;&#65292;22.5cm&#38271;&#29255;&#27573;&#30340;&#36861;&#36394;&#35823;&#24046;&#20026;1cm&#12290;
&lt;/p&gt;
&lt;p&gt;
Folding is an unique structural technique to enable planer materials with motion or 3D mechanical properties. Textile-based capacitive sensing has shown to be sensitive to the geometry deformation and relative motion of conductive textiles. In this work, we propose a novel self-tracking foldable smart textile by combining folded fabric structures and capacitive sensing to detect the structural motions using state-of-the-art sensing circuits and deep learning technologies. We created two folding patterns, Accordion and Chevron, each with two layouts of capacitive sensors in the form of thermobonded conductive textile patches. In an experiment of manually moving patches of the folding patterns, we developed deep neural network to learn and reconstruct the vision-tracked shape of the patches. Through our approach, the geometry primitives defining the patch shape can be reconstructed from the capacitive signals with R-squared value of up to 95\% and tracking error of 1cm for 22.5cm long pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#24555;&#36882;&#36816;&#33829;&#20013;&#30340;&#36127;&#36733;&#35268;&#21010;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20840;&#38754;&#32771;&#34385;&#20102;&#36127;&#36733;&#21644;&#27969;&#31243;&#35268;&#21010;&#30340;&#25361;&#25112;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#32593;&#32476;&#20013;&#23384;&#22312;&#22823;&#37327;&#30340;&#23545;&#31216;&#24615;&#65292;&#23548;&#33268;&#20248;&#21270;&#27714;&#35299;&#22120;&#36820;&#22238;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#38477;&#20302;&#20102;&#35268;&#21010;&#20154;&#21592;&#23545;&#20248;&#21270;&#27714;&#35299;&#30340;&#20449;&#20219;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.04050</link><description>&lt;p&gt;
&#22522;&#20110;&#20248;&#21270;&#30340;&#23398;&#20064;&#29992;&#20110;&#21345;&#36710;&#36816;&#36755;&#26381;&#21153;&#32593;&#32476;&#20013;&#30340;&#21160;&#24577;&#36127;&#36733;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Optimization-based Learning for Dynamic Load Planning in Trucking Service Networks. (arXiv:2307.04050v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#24555;&#36882;&#36816;&#33829;&#20013;&#30340;&#36127;&#36733;&#35268;&#21010;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20840;&#38754;&#32771;&#34385;&#20102;&#36127;&#36733;&#21644;&#27969;&#31243;&#35268;&#21010;&#30340;&#25361;&#25112;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#32593;&#32476;&#20013;&#23384;&#22312;&#22823;&#37327;&#30340;&#23545;&#31216;&#24615;&#65292;&#23548;&#33268;&#20248;&#21270;&#27714;&#35299;&#22120;&#36820;&#22238;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#38477;&#20302;&#20102;&#35268;&#21010;&#20154;&#21592;&#23545;&#20248;&#21270;&#27714;&#35299;&#30340;&#20449;&#20219;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36127;&#36733;&#35268;&#21010;&#38382;&#39064;&#26159;&#24555;&#36882;&#36816;&#33829;&#20013;&#26381;&#21153;&#32593;&#32476;&#35774;&#35745;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#23427;&#20915;&#23450;&#20102;&#22312;&#32456;&#31471;&#20043;&#38388;&#22914;&#20309;&#22312;&#26102;&#38388;&#19978;&#20998;&#37197;&#22810;&#23569;&#36742;&#25302;&#36710;&#65288;&#25110;&#36127;&#36733;&#65289;&#36827;&#34892;&#27966;&#36963;&#12290;&#21478;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30830;&#23450;&#19968;&#20010;&#27969;&#31243;&#35745;&#21010;&#65292;&#23427;&#25351;&#23450;&#20102;&#22914;&#20309;&#23558;&#21253;&#35065;&#20307;&#31215;&#20998;&#37197;&#32473;&#35745;&#21010;&#30340;&#36127;&#36733;&#12290;&#26412;&#25991;&#32771;&#34385;&#21040;&#20102;&#21160;&#24577;&#36127;&#36733;&#35268;&#21010;&#38382;&#39064;&#65288;DLPP&#65289;&#65292;&#23427;&#21516;&#26102;&#32771;&#34385;&#20102;&#36127;&#36733;&#21644;&#27969;&#31243;&#35268;&#21010;&#30340;&#25361;&#25112;&#65292;&#20197;&#22312;&#25805;&#20316;&#26085;&#20043;&#21069;&#38543;&#30528;&#38656;&#27714;&#39044;&#27979;&#30340;&#21464;&#21270;&#32780;&#35843;&#25972;&#36127;&#36733;&#21644;&#27969;&#31243;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#65292;&#20026;&#32593;&#32476;&#20013;&#21508;&#20010;&#32456;&#31471;&#30340;&#35268;&#21010;&#20154;&#21592;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#12290;&#26412;&#25991;&#23558;DLPP&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;MIP&#65292;&#24182;&#35777;&#26126;&#23427;&#22312;&#27599;&#20010;&#21830;&#21697;&#37117;&#21487;&#20197;&#36890;&#36807;&#20027;&#36335;&#24452;&#21644;&#22791;&#29992;&#36335;&#24452;&#36827;&#34892;&#36335;&#30001;&#30340;&#32593;&#32476;&#20013;&#26377;&#22823;&#37327;&#30340;&#23545;&#31216;&#24615;&#12290;&#22240;&#27492;&#65292;&#20248;&#21270;&#27714;&#35299;&#22120;&#21487;&#33021;&#20250;&#23545;&#23494;&#20999;&#30456;&#20851;&#30340;&#38382;&#39064;&#36820;&#22238;&#26681;&#26412;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#35268;&#21010;&#20154;&#21592;&#24863;&#21040;&#22256;&#24785;&#65292;&#38477;&#20302;&#23545;&#20248;&#21270;&#27714;&#35299;&#30340;&#20449;&#20219;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The load planning problem is a critical challenge in service network design for parcel carriers: it decides how many trailers (or loads) to assign for dispatch over time between pairs of terminals. Another key challenge is to determine a flow plan, which specifies how parcel volumes are assigned to planned loads. This paper considers the Dynamic Load Planning Problem (DLPP) that considers both flow and load planning challenges jointly to adjust loads and flows as the demand forecast changes over time before the day of operations. The paper aims at developing a decision-support tool to inform planners making these decisions at terminals across the network. The paper formulates the DLPP as a MIP and shows that it admits a large number of symmetries in a network where each commodity can be routed through primary and alternate paths. As a result, an optimization solver may return fundamentally different solutions to closely related problems, confusing planners and reducing trust in optimiz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#31232;&#30095;&#27491;&#21017;&#21270;&#20013;&#36827;&#34892;&#24179;&#28369;&#20248;&#21270;&#65292;&#19982;&#20027;&#27969;&#30340;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#20860;&#23481;&#65292;&#24182;&#19988;&#33021;&#22815;&#24471;&#21040;&#21305;&#37197;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#31561;&#20215;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.03571</link><description>&lt;p&gt;
&#24179;&#28369;&#36793;&#32536;&#65306;&#21033;&#29992;Hadamard&#36229;&#21442;&#25968;&#21270;&#22312;&#31232;&#30095;&#27491;&#21017;&#21270;&#30340;&#24179;&#28369;&#20248;&#21270;&#20013;&#30340;&#19968;&#33324;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Smoothing the Edges: A General Framework for Smooth Optimization in Sparse Regularization using Hadamard Overparametrization. (arXiv:2307.03571v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#31232;&#30095;&#27491;&#21017;&#21270;&#20013;&#36827;&#34892;&#24179;&#28369;&#20248;&#21270;&#65292;&#19982;&#20027;&#27969;&#30340;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#20860;&#23481;&#65292;&#24182;&#19988;&#33021;&#22815;&#24471;&#21040;&#21305;&#37197;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#31561;&#20215;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#65288;&#32467;&#26500;&#21270;&#65289;&#31232;&#30095;&#27491;&#21017;&#21270;&#38382;&#39064;&#20013;&#30340;$\ell_q$&#21644;$\ell_{p,q}$&#27491;&#21017;&#21270;&#30340;&#24179;&#28369;&#26041;&#27861;&#12290;&#36825;&#20123;&#38750;&#24179;&#28369;&#19988;&#21487;&#33021;&#38750;&#20984;&#30340;&#38382;&#39064;&#30340;&#20248;&#21270;&#36890;&#24120;&#20381;&#36182;&#20110;&#19987;&#38376;&#30340;&#36807;&#31243;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#19968;&#33324;&#26694;&#26550;&#19982;&#20027;&#27969;&#30340;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#65288;&#22914;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#21152;&#36895;&#21464;&#20307;&#65289;&#20860;&#23481;&#65292;&#26080;&#38656;&#20219;&#20309;&#20462;&#25913;&#12290;&#36825;&#26159;&#36890;&#36807;&#24179;&#28369;&#20248;&#21270;&#36716;&#31227;&#23454;&#29616;&#30340;&#65292;&#20854;&#20013;&#36873;&#23450;&#27169;&#22411;&#21442;&#25968;&#30340;&#36229;&#21442;&#25968;&#21270;&#20351;&#29992;Hadamard&#20056;&#31215;&#21644;&#24809;&#32602;&#30340;&#25913;&#21464;&#12290;&#22312;&#36229;&#21442;&#25968;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#29992;&#26367;&#20195;&#21442;&#25968;&#36827;&#34892;&#24179;&#28369;&#21644;&#20984;&#24615;&#30340;$\ell_2$&#27491;&#21017;&#21270;&#65292;&#33021;&#22815;&#22312;&#21407;&#22987;&#21442;&#25968;&#21270;&#20013;&#24341;&#20837;&#38750;&#24179;&#28369;&#21644;&#38750;&#20984;&#24615;&#30340;$\ell_q$&#25110;$\ell_{p,q}$&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#33021;&#22815;&#24471;&#21040;&#21305;&#37197;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#36824;&#33021;&#24471;&#21040;&#31561;&#20215;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#36825;&#22312;&#38750;&#20984;&#31232;&#30095;&#27491;&#21017;&#21270;&#20013;&#23588;&#20854;&#26377;&#29992;&#65292;&#22240;&#20026;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25214;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#38750;&#24120;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a smooth method for (structured) sparsity in $\ell_q$ and $\ell_{p,q}$ regularized optimization problems. Optimization of these non-smooth and possibly non-convex problems typically relies on specialized procedures. In contrast, our general framework is compatible with prevalent first-order optimization methods like Stochastic Gradient Descent and accelerated variants without any required modifications. This is accomplished through a smooth optimization transfer, comprising an overparametrization of selected model parameters using Hadamard products and a change of penalties. In the overparametrized problem, smooth and convex $\ell_2$ regularization of the surrogate parameters induces non-smooth and non-convex $\ell_q$ or $\ell_{p,q}$ regularization in the original parametrization. We show that our approach yields not only matching global minima but also equivalent local minima. This is particularly useful in non-convex sparse regularization, where finding global m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#20559;&#24046;&#29702;&#35770;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#30340;&#24179;&#28369;&#27169;&#22411;&#29305;&#24449;&#25551;&#36848;&#26041;&#27861;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#19968;&#20123;&#25554;&#20540;&#22120;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#29616;&#20195;&#23398;&#20064;&#25216;&#26415;&#20026;&#20160;&#20040;&#33021;&#22815;&#25214;&#21040;&#23427;&#20204;&#12290;</title><link>http://arxiv.org/abs/2306.10947</link><description>&lt;p&gt;
&#20351;&#29992;&#36895;&#29575;&#20989;&#25968;&#29702;&#35299;&#25554;&#20540;&#21306;&#38388;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Understanding Generalization in the Interpolation Regime using the Rate Function. (arXiv:2306.10947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#20559;&#24046;&#29702;&#35770;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#30340;&#24179;&#28369;&#27169;&#22411;&#29305;&#24449;&#25551;&#36848;&#26041;&#27861;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#19968;&#20123;&#25554;&#20540;&#22120;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#29616;&#20195;&#23398;&#20064;&#25216;&#26415;&#20026;&#20160;&#20040;&#33021;&#22815;&#25214;&#21040;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#22823;&#20559;&#24046;&#29702;&#35770;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#24179;&#28369;&#24230;&#30340;&#26032;&#29305;&#24449;&#25551;&#36848;&#26041;&#27861;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#20197;&#24448;&#30340;&#24037;&#20316;&#36890;&#24120;&#29992;&#23454;&#25968;&#20540;&#65288;&#22914;&#26435;&#37325;&#33539;&#25968;&#65289;&#26469;&#34920;&#24449;&#27169;&#22411;&#30340;&#24179;&#28369;&#24230;&#65292;&#25105;&#20204;&#34920;&#26126;&#21487;&#20197;&#29992;&#31616;&#21333;&#30340;&#23454;&#20540;&#20989;&#25968;&#26469;&#25551;&#36848;&#24179;&#28369;&#24230;&#12290;&#22522;&#20110;&#27169;&#22411;&#24179;&#28369;&#24230;&#30340;&#36825;&#19968;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#35299;&#37322;&#65292;&#20026;&#20160;&#20040;&#19968;&#20123;&#25554;&#20540;&#22120;&#34920;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;&#24191;&#27867;&#20351;&#29992;&#30340;&#29616;&#20195;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;$\ell_2$-&#35268;&#33539;&#21270;&#65292;&#25968;&#25454;&#22686;&#24378;&#65292;&#19981;&#21464;&#30340;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#21270;&#65289;&#33021;&#22815;&#25214;&#21040;&#23427;&#20204;&#12290;&#25105;&#20204;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#65292;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#37117;&#25552;&#20379;&#20102;&#20114;&#34917;&#30340;&#36807;&#31243;&#65292;&#36825;&#20123;&#36807;&#31243;&#20351;&#20248;&#21270;&#22120;&#20559;&#21521;&#20110;&#26356;&#24179;&#28369;&#30340;&#25554;&#20540;&#22120;&#65292;&#32780;&#26681;&#25454;&#36825;&#31181;&#29702;&#35770;&#20998;&#26512;&#65292;&#26356;&#24179;&#28369;&#30340;&#25554;&#20540;&#22120;&#26159;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#25554;&#20540;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel characterization of the smoothness of a model based on basic principles of Large Deviation Theory. In contrast to prior work, where the smoothness of a model is normally characterized by a real value (e.g., the weights' norm), we show that smoothness can be described by a simple real-valued function. Based on this concept of smoothness, we propose an unifying theoretical explanation of why some interpolators generalize remarkably well and why a wide range of modern learning techniques (i.e., stochastic gradient descent, $\ell_2$-norm regularization, data augmentation, invariant architectures, and overparameterization) are able to find them. The emergent conclusion is that all these methods provide complimentary procedures that bias the optimizer to smoother interpolators, which, according to this theoretical analysis, are the ones with better generalization error.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#26469;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#26041;&#21521;&#65292;&#20197;&#24110;&#21161;&#25237;&#36164;&#32773;&#20570;&#20986;&#26126;&#26234;&#30340;&#25237;&#36164;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2306.08157</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#36827;&#34892;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#26041;&#21521;&#22240;&#26524;&#29305;&#24449;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Causal Feature Engineering of Price Directions of Cryptocurrencies using Dynamic Bayesian Networks. (arXiv:2306.08157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#26469;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#26041;&#21521;&#65292;&#20197;&#24110;&#21161;&#25237;&#36164;&#32773;&#20570;&#20986;&#26126;&#26234;&#30340;&#25237;&#36164;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#23494;&#36135;&#24065;&#22312;&#21508;&#20010;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#37329;&#34701;&#21644;&#25237;&#36164;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#20854;&#29420;&#29305;&#30340;&#21306;&#22359;&#38142;&#30456;&#20851;&#29305;&#24615;&#65292;&#22914;&#38544;&#31169;&#12289;&#21435;&#20013;&#24515;&#21270;&#21644;&#19981;&#21487;&#36861;&#36394;&#24615;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#20854;&#21463;&#27426;&#36814;&#30340;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#30340;&#27874;&#21160;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#21152;&#23494;&#36135;&#24065;&#20173;&#28982;&#26159;&#19968;&#31181;&#39640;&#39118;&#38505;&#25237;&#36164;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;DBN&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20803;&#35774;&#32622;&#19979;&#27169;&#25311;&#22797;&#26434;&#31995;&#32479;&#65292;&#20197;&#39044;&#27979;&#20116;&#31181;&#27969;&#34892;&#21152;&#23494;&#36135;&#24065;&#30340;&#20215;&#26684;&#36816;&#21160;&#26041;&#21521;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cryptocurrencies have gained popularity across various sectors, especially in finance and investment. The popularity is partly due to their unique specifications originating from blockchain-related characteristics such as privacy, decentralisation, and untraceability. Despite their growing popularity, cryptocurrencies remain a high-risk investment due to their price volatility and uncertainty. The inherent volatility in cryptocurrency prices, coupled with internal cryptocurrency-related factors and external influential global economic factors makes predicting their prices and price movement directions challenging. Nevertheless, the knowledge obtained from predicting the direction of cryptocurrency prices can provide valuable guidance for investors in making informed investment decisions. To address this issue, this paper proposes a dynamic Bayesian network (DBN) approach, which can model complex systems in multivariate settings, to predict the price movement direction of five popular a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26368;&#20248;&#23545;&#25239;&#39044;&#27979;&#22120;&#30340;&#21508;&#31181;&#22522;&#26412;&#29305;&#24615;&#65292;&#24182;&#32467;&#21512;&#26032;&#30340;Rademacher&#22797;&#26434;&#24230;&#30028;&#38480;&#35777;&#26126;&#20102;&#65292;&#22312;&#27973;&#23618;&#32593;&#32476;&#19978;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#65292;&#37319;&#29992;&#26089;&#20572;&#21644;&#29702;&#24819;&#30340;&#26368;&#20248;&#23545;&#25163;&#65292;&#33021;&#22815;&#23454;&#29616;&#26368;&#20248;&#23545;&#25239;&#27979;&#35797;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.07544</link><description>&lt;p&gt;
&#20851;&#20110;&#23454;&#29616;&#26368;&#20248;&#23545;&#25239;&#27979;&#35797;&#35823;&#24046;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Achieving Optimal Adversarial Test Error. (arXiv:2306.07544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26368;&#20248;&#23545;&#25239;&#39044;&#27979;&#22120;&#30340;&#21508;&#31181;&#22522;&#26412;&#29305;&#24615;&#65292;&#24182;&#32467;&#21512;&#26032;&#30340;Rademacher&#22797;&#26434;&#24230;&#30028;&#38480;&#35777;&#26126;&#20102;&#65292;&#22312;&#27973;&#23618;&#32593;&#32476;&#19978;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#65292;&#37319;&#29992;&#26089;&#20572;&#21644;&#29702;&#24819;&#30340;&#26368;&#20248;&#23545;&#25163;&#65292;&#33021;&#22815;&#23454;&#29616;&#26368;&#20248;&#23545;&#25239;&#27979;&#35797;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#38416;&#36848;&#20102;&#26368;&#20248;&#23545;&#25239;&#39044;&#27979;&#22120;&#30340;&#21508;&#31181;&#22522;&#26412;&#29305;&#24615;&#65306;&#26368;&#20248;&#23545;&#25239;&#20984;&#39044;&#27979;&#22120;&#30340;&#32467;&#26500;&#12289;&#23558;&#23545;&#25239;&#20984;&#25439;&#22833;&#19982;&#23545;&#25239;0-1&#25439;&#22833;&#30456;&#20851;&#32852;&#30340;&#30028;&#38480;&#20197;&#21450;&#36830;&#32493;&#39044;&#27979;&#22120;&#21487;&#20197;&#22312;&#20984;&#21644;0-1&#25439;&#22833;&#19979;&#26080;&#38480;&#25509;&#36817;&#26368;&#20248;&#23545;&#25239;&#35823;&#24046;&#12290;&#26412;&#25991;&#36824;&#23558;&#36825;&#20123;&#32467;&#26524;&#19982;&#23545;&#25239;&#35757;&#32451;&#22312;&#21021;&#22987;&#21270;&#38468;&#36817;&#30340;&#26032;Rademacher&#22797;&#26434;&#24230;&#30028;&#38480;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#33324;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#25200;&#21160;&#38598;&#65292;&#22312;&#27973;&#23618;&#32593;&#32476;&#19978;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#65292;&#37319;&#29992;&#26089;&#20572;&#21644;&#29702;&#24819;&#30340;&#26368;&#20248;&#23545;&#25163;&#65292;&#33021;&#22815;&#23454;&#29616;&#26368;&#20248;&#23545;&#25239;&#27979;&#35797;&#35823;&#24046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20808;&#21069;&#30340;&#29702;&#35770;&#24037;&#20316;&#21482;&#32771;&#34385;&#20102;&#29305;&#23450;&#30340;&#25968;&#25454;&#20998;&#24067;&#25110;&#20165;&#25552;&#20379;&#20102;&#35757;&#32451;&#35823;&#24046;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We first elucidate various fundamental properties of optimal adversarial predictors: the structure of optimal adversarial convex predictors in terms of optimal adversarial zero-one predictors, bounds relating the adversarial convex loss to the adversarial zero-one loss, and the fact that continuous predictors can get arbitrarily close to the optimal adversarial error for both convex and zero-one losses. Applying these results along with new Rademacher complexity bounds for adversarial training near initialization, we prove that for general data distributions and perturbation sets, adversarial training on shallow networks with early stopping and an idealized optimal adversary is able to achieve optimal adversarial test error. By contrast, prior theoretical work either considered specialized data distributions or only provided training error guarantees.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#31532;&#19968;&#39033;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#30340;&#38544;&#31169;&#30740;&#31350;&#65292;&#25915;&#20987;&#32773;&#23558;&#20174;&#27169;&#22411;&#20013;&#25552;&#21462;&#35757;&#32451;&#38598;&#30340;&#25935;&#24863;&#20840;&#23616;&#23646;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#21508;&#31181;&#25193;&#25955;&#27169;&#22411;&#21450;&#20854;&#21462;&#26679;&#22120;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.05208</link><description>&lt;p&gt;
PriSampler: &#32531;&#35299;&#25193;&#25955;&#27169;&#22411;&#30340;&#23646;&#24615;&#25512;&#26029;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
PriSampler: Mitigating Property Inference of Diffusion Models. (arXiv:2306.05208v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#31532;&#19968;&#39033;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#30340;&#38544;&#31169;&#30740;&#31350;&#65292;&#25915;&#20987;&#32773;&#23558;&#20174;&#27169;&#22411;&#20013;&#25552;&#21462;&#35757;&#32451;&#38598;&#30340;&#25935;&#24863;&#20840;&#23616;&#23646;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#21508;&#31181;&#25193;&#25955;&#27169;&#22411;&#21450;&#20854;&#21462;&#26679;&#22120;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#36825;&#20123;&#25104;&#21151;&#20063;&#20419;&#20351;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#25935;&#24863;&#25968;&#25454;&#65292;&#20363;&#22914;&#20154;&#33080;&#25968;&#25454;&#65292;&#20294;&#36825;&#21487;&#33021;&#24102;&#26469;&#20005;&#37325;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#20171;&#32461;&#20102;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#30340;&#31532;&#19968;&#39033;&#38544;&#31169;&#30740;&#31350;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#26088;&#22312;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#21462;&#35757;&#32451;&#38598;&#30340;&#25935;&#24863;&#20840;&#23616;&#23646;&#24615;&#65292;&#20363;&#22914;&#26576;&#20123;&#25935;&#24863;&#23646;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26368;&#23454;&#29992;&#30340;&#25915;&#20987;&#22330;&#26223;&#65306;&#25915;&#20987;&#32773;&#21482;&#33021;&#33719;&#24471;&#21512;&#25104;&#25968;&#25454;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#19979;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#21462;&#26679;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#30340;&#35780;&#20272;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#33539;&#22260;&#34920;&#26126;&#65292;&#21508;&#31181;&#25193;&#25955;&#27169;&#22411;&#21450;&#20854;&#21462;&#26679;&#22120;&#37117;&#23481;&#26131;&#21463;&#21040;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#23545;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#19968;&#39033;&#26696;&#20363;&#30740;&#31350;&#20063;&#23637;&#31034;&#20102;&#25915;&#20987;&#30340;&#23454;&#38469;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have been remarkably successful in data synthesis. Such successes have also driven diffusion models to apply to sensitive data, such as human face data, but this might bring about severe privacy concerns. In this work, we systematically present the first privacy study about property inference attacks against diffusion models, in which adversaries aim to extract sensitive global properties of the training set from a diffusion model, such as the proportion of the training data for certain sensitive properties. Specifically, we consider the most practical attack scenario: adversaries are only allowed to obtain synthetic data. Under this realistic scenario, we evaluate the property inference attacks on different types of samplers and diffusion models. A broad range of evaluations shows that various diffusion models and their samplers are all vulnerable to property inference attacks. Furthermore, one case study on off-the-shelf pre-trained diffusion models also demonstrates
&lt;/p&gt;</description></item><item><title>Coeditor&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#36718;&#20195;&#30721;&#33258;&#21160;&#32534;&#36753;&#27169;&#22411;&#65292;&#21033;&#29992;&#21516;&#19968;&#20195;&#30721;&#24211;&#20013;&#30340;&#26368;&#36817;&#21464;&#21270;&#26469;&#39044;&#27979;&#23545;&#20195;&#30721;&#21306;&#22495;&#30340;&#32534;&#36753;&#65292;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.18584</link><description>&lt;p&gt;
Coeditor&#65306;&#21033;&#29992;&#19978;&#19979;&#25991;&#21464;&#21270;&#36827;&#34892;&#22810;&#36718;&#20195;&#30721;&#33258;&#21160;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Coeditor: Leveraging Contextual Changes for Multi-round Code Auto-editing. (arXiv:2305.18584v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18584
&lt;/p&gt;
&lt;p&gt;
Coeditor&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#36718;&#20195;&#30721;&#33258;&#21160;&#32534;&#36753;&#27169;&#22411;&#65292;&#21033;&#29992;&#21516;&#19968;&#20195;&#30721;&#24211;&#20013;&#30340;&#26368;&#36817;&#21464;&#21270;&#26469;&#39044;&#27979;&#23545;&#20195;&#30721;&#21306;&#22495;&#30340;&#32534;&#36753;&#65292;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20154;&#21592;&#32463;&#24120;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#26469;&#32500;&#25252;&#21644;&#37325;&#26500;&#29616;&#26377;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#20808;&#21069;&#24037;&#20316;&#37117;&#20165;&#20851;&#27880;&#20110;&#21019;&#24314;&#26032;&#20195;&#30721;&#65292;&#24573;&#30053;&#20102;&#23545;&#32534;&#36753;&#29616;&#26377;&#20195;&#30721;&#30340;&#29420;&#29305;&#35201;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20010;&#22810;&#36718;&#20195;&#30721;&#33258;&#21160;&#32534;&#36753;&#30340;&#35774;&#32622;&#65292;&#26088;&#22312;&#22522;&#20110;&#21516;&#19968;&#20195;&#30721;&#24211;&#20013;&#30340;&#26368;&#36817;&#21464;&#21270;&#26469;&#39044;&#27979;&#23545;&#20195;&#30721;&#21306;&#22495;&#30340;&#32534;&#36753;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;Coeditor&#26159;&#19968;&#20010;&#32463;&#36807;&#32454;&#21270;&#30340;CodeT5&#27169;&#22411;&#65292;&#20855;&#26377;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#30340;&#22686;&#24378;&#21151;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#34892;&#24046;&#24322;&#26684;&#24335;&#23545;&#20195;&#30721;&#26356;&#25913;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#37319;&#29992;&#38745;&#24577;&#20998;&#26512;&#26469;&#24418;&#25104;&#22823;&#22411;&#23450;&#21046;&#27169;&#22411;&#19978;&#19979;&#25991;&#65292;&#20197;&#30830;&#20445;&#36866;&#24403;&#30340;&#39044;&#27979;&#20449;&#24687;&#12290;&#25105;&#20204;&#20174;1650&#20010;&#24320;&#28304;Python&#39033;&#30446;&#30340;&#25552;&#20132;&#21382;&#21490;&#20013;&#25910;&#38598;&#20102;&#19968;&#20010;&#20195;&#30721;&#32534;&#36753;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#22312;&#31616;&#21270;&#30340;&#21333;&#36718;&#21333;&#32534;&#36753;&#20219;&#21153;&#20013;&#65292;Coeditor&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#20248;&#20110;&#26368;&#20339;&#30340;&#20195;&#30721;&#23436;&#25104;&#26041;&#27861;&#65292;&#20934;&#30830;&#29575;&#36817;&#20046;&#32763;&#20493;&#65292;&#21363;&#20351;&#20351;&#29992;&#30340;&#27169;&#22411;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developers often dedicate significant time to maintaining and refactoring existing code. However, most prior work on generative models for code focuses solely on creating new code, neglecting the unique requirements of editing existing code. In this work, we explore a multi-round code auto-editing setting, aiming to predict edits to a code region based on recent changes within the same codebase. Our model, Coeditor, is a fine-tuned CodeT5 model with enhancements specifically designed for code editing tasks. We encode code changes using a line diff format and employ static analysis to form large customized model contexts, ensuring appropriate information for prediction. We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation. In a simplified single-round, single-edit task, Coeditor significantly outperforms the best code completion approach -- nearly doubling its exact-match accuracy, despite using a much smaller model 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#22312;&#22823;&#22411;&#25968;&#25454;&#29615;&#22659;&#19979;&#20351;&#29992;&#23376;&#37319;&#26679;&#20135;&#29983;&#30340;&#35823;&#24046;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#20999;&#25442;&#25968;&#25454;&#23376;&#38598;&#24182;&#21487;&#29992;&#20110;&#25193;&#25955;&#23376;&#37319;&#26679; MCMC &#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13882</link><description>&lt;p&gt;
&#38543;&#26426;&#26799;&#24230; Langevin &#25193;&#25955;&#20013;&#30340;&#23376;&#37319;&#26679;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Subsampling Error in Stochastic Gradient Langevin Diffusions. (arXiv:2305.13882v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13882
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#22312;&#22823;&#22411;&#25968;&#25454;&#29615;&#22659;&#19979;&#20351;&#29992;&#23376;&#37319;&#26679;&#20135;&#29983;&#30340;&#35823;&#24046;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#20999;&#25442;&#25968;&#25454;&#23376;&#38598;&#24182;&#21487;&#29992;&#20110;&#25193;&#25955;&#23376;&#37319;&#26679; MCMC &#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398; (SGLD) &#36890;&#24120;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#32479;&#35745;&#23398;&#20064;&#20013;&#36817;&#20284;&#36125;&#21494;&#26031;&#21518;&#39564;&#20998;&#24067;&#12290;&#19982;&#35768;&#22810;&#24120;&#35268;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599; (MCMC) &#31639;&#27861;&#19981;&#21516;&#65292;SGLD &#23545;&#20110;&#21518;&#39564;&#20998;&#24067;&#19981;&#26159;&#31283;&#23450;&#30340;&#12290;&#23427;&#26377;&#20004;&#20010;&#38169;&#35823;&#26469;&#28304;&#65306;&#31532;&#19968;&#20010;&#38169;&#35823;&#26159;&#30001; Euler-Maruyama &#31163;&#25955;&#21270; Langevin &#25193;&#25955;&#36807;&#31243;&#24341;&#20837;&#30340;&#65292;&#31532;&#20108;&#20010;&#38169;&#35823;&#26469;&#33258;&#20110;&#25968;&#25454;&#23376;&#37319;&#26679;&#65292;&#36825;&#20351;&#24471;&#23427;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#29615;&#22659;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102; SGLD &#30340;&#29702;&#24819;&#21270;&#29256;&#26412;&#65292;&#20197;&#20998;&#26512;&#35813;&#26041;&#27861;&#30340;&#32431;&#23376;&#37319;&#26679;&#35823;&#24046;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20854;&#35270;&#20026;&#22522;&#20110;&#25193;&#25955;&#30340;&#23376;&#37319;&#26679; MCMC &#26041;&#27861;&#30340;&#26368;&#20339;&#24773;&#20917;&#35823;&#24046;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230; Langevin &#25193;&#25955; (SGLDiff)&#65292;&#36825;&#26159;&#19968;&#20010;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#23427;&#36981;&#24490;&#19982;&#25968;&#25454;&#23376;&#38598;&#30456;&#24212;&#30340; Langevin &#25193;&#25955;&#65292;&#24182;&#22312;&#25351;&#25968;&#31561;&#24453;&#26102;&#38388;&#21518;&#20999;&#25442;&#35813;&#25968;&#25454;&#23376;&#38598;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29926;&#29791;&#26031;&#22374;&#36317;&#31163; (Was)
&lt;/p&gt;
&lt;p&gt;
The Stochastic Gradient Langevin Dynamics (SGLD) are popularly used to approximate Bayesian posterior distributions in statistical learning procedures with large-scale data. As opposed to many usual Markov chain Monte Carlo (MCMC) algorithms, SGLD is not stationary with respect to the posterior distribution; two sources of error appear: The first error is introduced by an Euler--Maruyama discretisation of a Langevin diffusion process, the second error comes from the data subsampling that enables its use in large-scale data settings. In this work, we consider an idealised version of SGLD to analyse the method's pure subsampling error that we then see as a best-case error for diffusion-based subsampling MCMC methods. Indeed, we introduce and study the Stochastic Gradient Langevin Diffusion (SGLDiff), a continuous-time Markov process that follows the Langevin diffusion corresponding to a data subset and switches this data subset after exponential waiting times. There, we show that the Was
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AnyPredict &#30340;&#34920;&#26684;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#20351;&#29992;&#25968;&#25454;&#24341;&#25806;&#25972;&#21512;&#39046;&#22495;&#20869;&#21644;&#24191;&#27867;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#65292;&#20197;&#20811;&#26381;&#27169;&#24335;&#19981;&#21305;&#37197;&#21644;&#39044;&#27979;&#30446;&#26631;&#24322;&#36136;&#24615;&#31561;&#26041;&#38754;&#30340;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2305.12081</link><description>&lt;p&gt;
AnyPredict: &#34920;&#26684;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnyPredict: Foundation Model for Tabular Prediction. (arXiv:2305.12081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AnyPredict &#30340;&#34920;&#26684;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#20351;&#29992;&#25968;&#25454;&#24341;&#25806;&#25972;&#21512;&#39046;&#22495;&#20869;&#21644;&#24191;&#27867;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#65292;&#20197;&#20811;&#26381;&#27169;&#24335;&#19981;&#21305;&#37197;&#21644;&#39044;&#27979;&#30446;&#26631;&#24322;&#36136;&#24615;&#31561;&#26041;&#38754;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27169;&#22411;&#22312;&#34920;&#26684;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21463;&#21040;&#38480;&#21046;&#65292;&#20027;&#35201;&#38382;&#39064;&#21253;&#25324; (1) &#32570;&#20047;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#30340;&#24102;&#26377;&#26631;&#20934;&#26631;&#31614;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450; (2) &#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#27169;&#24335;&#19981;&#21305;&#37197;&#21644;&#39044;&#27979;&#30446;&#26631;&#30340;&#24322;&#36136;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#22522;&#20110; AnyPredict &#30340;&#34920;&#26684;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#65292;&#21253;&#25324;&#39046;&#22495;&#20869;&#21644;&#24191;&#27867;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#25968;&#25454;&#24341;&#25806;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#26469;&#25972;&#21512;&#34920;&#26684;&#26679;&#26412;&#65292;&#20811;&#26381;&#20102;&#19981;&#21516;&#27169;&#24335;&#34920;&#26684;&#20043;&#38388;&#30340;&#38556;&#30861;&#65292;&#24182;&#20351;&#29992;&#8220;&#23398;&#20064;&#65292;&#27880;&#37322;&#21644;&#23457;&#35745;&#8221;&#27969;&#31243;&#23558;&#39046;&#22495;&#22806;&#25968;&#25454;&#19982;&#30446;&#26631;&#20219;&#21153;&#23545;&#40784;&#12290;&#25193;&#23637;&#30340;&#35757;&#32451;&#25968;&#25454;&#20351;&#39044;&#35757;&#32451;&#30340; AnyPredict &#33021;&#22815;&#25903;&#25345;&#27599;&#20010;&#34920;&#26684;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models are pre-trained on massive data to perform well across many downstream tasks. They have demonstrated significant success in natural language processing and computer vision. Nonetheless, the use of such models in tabular prediction tasks has been limited, with the main hurdles consisting of (1) the lack of large-scale and diverse tabular datasets with standardized labels and (2) the schema mismatch and predictive target heterogeneity across domains.  This paper proposes a method for building training data at scale for tabular prediction foundation models (AnyPredict) using both in-domain and a wide range of out-domain datasets. The method uses a data engine that leverages large language models (LLMs) to consolidate tabular samples to overcome the barrier across tables with varying schema and align out-domain data with the target task using a ``learn, annotate, and audit'' pipeline. The expanded training data enables the pre-trained AnyPredict to support every tabular d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#39318;&#27425;&#33021;&#22815;&#22312;&#32771;&#34385;&#23545;&#40784;&#25968;&#25454;&#30340;&#21516;&#26102;&#35299;&#20915;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#38382;&#39064;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#26377;&#26356;&#31616;&#21333;&#12289;&#26041;&#24046;&#26356;&#20302;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#21407;&#21017;&#24615;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2302.11419</link><description>&lt;p&gt;
&#23545;&#40784;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;
&lt;/p&gt;
&lt;p&gt;
Aligned Diffusion Schr\"odinger Bridges. (arXiv:2302.11419v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#39318;&#27425;&#33021;&#22815;&#22312;&#32771;&#34385;&#23545;&#40784;&#25968;&#25454;&#30340;&#21516;&#26102;&#35299;&#20915;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#38382;&#39064;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#26377;&#26356;&#31616;&#21333;&#12289;&#26041;&#24046;&#26356;&#20302;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#21407;&#21017;&#24615;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#30340;&#36793;&#38469;&#35266;&#23519;&#24674;&#22797;&#38543;&#26426;&#21160;&#24577;&#30340;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;(DSB)&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#25104;&#21151;&#30340;&#24212;&#29992;&#65292;&#20294;&#29616;&#26377;&#31639;&#27861;&#23578;&#26410;&#21033;&#29992;&#22810;&#31181;&#29983;&#29289;&#29616;&#35937;&#20013;&#33258;&#28982;&#20986;&#29616;&#30340;&#23545;&#40784;&#25968;&#25454;&#32467;&#26500;&#26469;&#35299;&#20915;DSB&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#39318;&#27425;&#22312;&#32771;&#34385;&#23545;&#40784;&#25968;&#25454;&#30340;&#21516;&#26102;&#35299;&#20915;&#20102;DSB&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#38752;&#20004;&#20010;&#20108;&#21313;&#24180;&#21382;&#21490;&#30340;&#24605;&#24819;&#32467;&#21512;&#36215;&#26469;&#65306;&#32463;&#20856;&#30340;&#34203;&#23450;&#35860;&#26725;&#29702;&#35770;&#21644;Doob&#30340;$h$-&#21464;&#25442;&#12290;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23548;&#33268;&#35757;&#32451;&#36807;&#31243;&#26356;&#31616;&#21333;&#65292;&#26041;&#24046;&#26356;&#20302;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#21407;&#21017;&#24615;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#12290;&#36825;&#26368;&#32456;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#23454;&#39564;&#20013;&#23548;&#33268;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324;&#21018;&#24615;&#34507;&#30333;&#36136;&#23545;&#25509;&#21644;&#32454;&#32990;&#20998;&#21270;&#36807;&#31243;&#30340;&#26102;&#38388;&#28436;&#21270;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Schr\"odinger bridges (DSB) have recently emerged as a powerful framework for recovering stochastic dynamics via their marginal observations at different time points. Despite numerous successful applications, existing algorithms for solving DSBs have so far failed to utilize the structure of aligned data, which naturally arises in many biological phenomena. In this paper, we propose a novel algorithmic framework that, for the first time, solves DSBs while respecting the data alignment. Our approach hinges on a combination of two decades-old ideas: The classical Schr\"odinger bridge theory and Doob's $h$-transform. Compared to prior methods, our approach leads to a simpler training procedure with lower variance, which we further augment with principled regularization schemes. This ultimately leads to sizeable improvements across experiments on synthetic and real data, including the tasks of rigid protein docking and temporal evolution of cellular differentiation processes.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#21452;&#37325;TCN-Attention&#32593;&#32476;&#30340;DTAAD&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#35786;&#26029;&#12290;&#36890;&#36807;&#38598;&#25104;&#35774;&#35745;&#21644;&#24341;&#20837;&#32553;&#25918;&#26041;&#27861;&#21644;&#21453;&#39304;&#26426;&#21046;&#65292;&#35813;&#27169;&#22411;&#23454;&#29616;&#20102;&#24555;&#36895;&#20934;&#30830;&#23450;&#20301;&#24322;&#24120;&#65292;&#24182;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#21644;&#25193;&#22823;&#20102;&#30456;&#20851;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2302.10753</link><description>&lt;p&gt;
DTAAD: &#21452;&#37325;TCN-Attention&#32593;&#32476;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DTAAD: Dual Tcn-Attention Networks for Anomaly Detection in Multivariate Time Series Data. (arXiv:2302.10753v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10753
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#21452;&#37325;TCN-Attention&#32593;&#32476;&#30340;DTAAD&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#35786;&#26029;&#12290;&#36890;&#36807;&#38598;&#25104;&#35774;&#35745;&#21644;&#24341;&#20837;&#32553;&#25918;&#26041;&#27861;&#21644;&#21453;&#39304;&#26426;&#21046;&#65292;&#35813;&#27169;&#22411;&#23454;&#29616;&#20102;&#24555;&#36895;&#20934;&#30830;&#23450;&#20301;&#24322;&#24120;&#65292;&#24182;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#21644;&#25193;&#22823;&#20102;&#30456;&#20851;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#33021;&#22815;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#26377;&#25928;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#35786;&#26029;&#65292;&#23545;&#24403;&#20170;&#24037;&#19994;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#24322;&#24120;&#26631;&#31614;&#12289;&#25968;&#25454;&#30340;&#39640;&#32500;&#22797;&#26434;&#24615;&#12289;&#23454;&#38469;&#30828;&#20214;&#30340;&#20869;&#23384;&#29942;&#39048;&#20197;&#21450;&#24555;&#36895;&#25512;&#29702;&#30340;&#38656;&#27714;&#65292;&#24314;&#31435;&#19968;&#20010;&#33021;&#22815;&#36805;&#36895;&#20934;&#30830;&#23450;&#20301;&#30340;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#21452;&#37325;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#65288;TCN&#65289;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#35786;&#26029;&#27169;&#22411;--DTAAD&#12290;&#25105;&#20204;&#30340;&#25972;&#20307;&#27169;&#22411;&#26159;&#19968;&#20010;&#38598;&#25104;&#35774;&#35745;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;AR&#65289;&#19982;&#33258;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#24182;&#24341;&#20837;&#20102;&#32553;&#25918;&#26041;&#27861;&#21644;&#21453;&#39304;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#21644;&#25193;&#22823;&#30456;&#20851;&#24046;&#24322;&#12290;&#25105;&#20204;&#26500;&#24314;&#30340;&#21452;&#37325;TCN-Attention&#32593;&#32476;&#65288;DTA&#65289;&#22312;&#22522;&#20934;&#23454;&#39564;&#20013;&#20165;&#20351;&#29992;&#20102;&#21333;&#23618;Transformer&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection techniques enable effective anomaly detection and diagnosis in multi-variate time series data, which are of major significance for today's industrial applications. However, establishing an anomaly detection system that can be rapidly and accurately located is a challenging problem due to the lack of outlier tags, the high dimensional complexity of the data, memory bottlenecks in the actual hardware, and the need for fast reasoning. We have proposed an anomaly detection and diagnosis model -- DTAAD in this paper, based on Transformer, and Dual Temporal Convolutional Network(TCN). Our overall model will be an integrated design in which autoregressive model(AR) combines autoencoder(AE) structures, and scaling methods and feedback mechanisms are introduced to improve prediction accuracy and expand correlation differences. Constructed by us, the Dual TCN-Attention Network (DTA) only uses a single layer of Transformer encoder in our baseline experiment, that belongs to an u
&lt;/p&gt;</description></item><item><title>TAMUNA&#26159;&#39318;&#20010;&#32852;&#21512;&#21033;&#29992;&#32593;&#32476;&#21387;&#32553;&#21644;&#23569;&#37327;&#36890;&#20449;&#37197;&#21512;&#21152;&#36895;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#24182;&#20801;&#35768;&#37096;&#20998;&#21442;&#19982;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.09832</link><description>&lt;p&gt;
TAMUNA: &#24102;&#26377;&#23616;&#37096;&#35757;&#32451;&#12289;&#21387;&#32553;&#21644;&#37096;&#20998;&#21442;&#19982;&#30340;&#21452;&#20493;&#21152;&#36895;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TAMUNA: Doubly Accelerated Federated Learning with Local Training, Compression, and Partial Participation. (arXiv:2302.09832v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09832
&lt;/p&gt;
&lt;p&gt;
TAMUNA&#26159;&#39318;&#20010;&#32852;&#21512;&#21033;&#29992;&#32593;&#32476;&#21387;&#32553;&#21644;&#23569;&#37327;&#36890;&#20449;&#37197;&#21512;&#21152;&#36895;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#24182;&#20801;&#35768;&#37096;&#20998;&#21442;&#19982;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#22823;&#37327;&#29992;&#25143;&#21512;&#20316;&#23398;&#20064;&#20840;&#23616;&#27169;&#22411;&#12290;&#20182;&#20204;&#20132;&#26367;&#36827;&#34892;&#26412;&#22320;&#35745;&#31639;&#21644;&#19982;&#36828;&#31243;&#26381;&#21153;&#22120;&#30340;&#36890;&#20449;&#12290;&#36890;&#20449;&#26159;&#35813;&#35774;&#32622;&#20013;&#30340;&#20027;&#35201;&#29942;&#39048;&#65292;&#23427;&#21487;&#20197;&#24930;&#19988;&#26114;&#36149;&#12290;&#20026;&#20102;&#20943;&#23569;&#36890;&#20449;&#36127;&#36733;&#24182;&#21152;&#36895;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65292;&#20351;&#29992;&#20004;&#31181;&#31574;&#30053;&#24456;&#21463;&#27426;&#36814;&#65306;1&#65289;&#26356;&#23569;&#22320;&#36890;&#20449;&#65292;&#21363;&#22312;&#36890;&#20449;&#36718;&#20043;&#38388;&#25191;&#34892;&#20960;&#20010;&#26412;&#22320;&#35745;&#31639;&#30340;&#36845;&#20195;&#65307;2&#65289;&#20256;&#36755;&#21387;&#32553;&#20449;&#24687;&#32780;&#19981;&#26159;&#23436;&#25972;&#32500;&#24230;&#30340;&#30690;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TAMUNA&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20998;&#24067;&#24335;&#20248;&#21270;&#21644;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#32852;&#21512;&#21033;&#29992;&#36825;&#20004;&#31181;&#31574;&#30053;&#65292;&#21516;&#26102;&#20801;&#35768;&#37096;&#20998;&#21442;&#19982;&#12290;TAMUNA&#20197;&#32447;&#24615;&#36895;&#24230;&#25910;&#25947;&#21040;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated learning, a large number of users collaborate to learn a global model. They alternate local computations and communication with a distant server. Communication, which can be slow and costly, is the main bottleneck in this setting. In addition to communication-efficiency, a robust algorithm should allow for partial participation, the desirable feature that not all clients need to participate to every round of the training process. To reduce the communication load and therefore accelerate distributed gradient descent, two strategies are popular: 1) communicate less frequently; that is, perform several iterations of local computations between the communication rounds; and 2) communicate compressed information instead of full-dimensional vectors. We propose TAMUNA, the first algorithm for distributed optimization and federated learning, which harnesses these two strategies jointly and allows for partial participation. TAMUNA converges linearly to an exact solution in the stron
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#26102;&#38388;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#26102;&#38388;&#21644;&#32467;&#26500;&#20449;&#24687;&#26469;&#23398;&#20064;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2302.07491</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#26102;&#38388;&#22270;&#23398;&#20064;&#19982;&#26102;&#38388;&#21644;&#32467;&#26500;&#24378;&#24230;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Temporal Graph learning with Temporal and Structural Intensity Alignment. (arXiv:2302.07491v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07491
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#26102;&#38388;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#26102;&#38388;&#21644;&#32467;&#26500;&#20449;&#24687;&#26469;&#23398;&#20064;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22270;&#23398;&#20064;&#26088;&#22312;&#29983;&#25104;&#29992;&#20110;&#22522;&#20110;&#22270;&#30340;&#20219;&#21153;&#30340;&#39640;&#36136;&#37327;&#34920;&#31034;&#65292;&#21516;&#26102;&#21253;&#21547;&#21160;&#24577;&#20449;&#24687;&#65292;&#26368;&#36817;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#19982;&#38745;&#24577;&#22270;&#19981;&#21516;&#65292;&#26102;&#38388;&#22270;&#36890;&#24120;&#20197;&#36830;&#32493;&#26102;&#38388;&#19978;&#30340;&#33410;&#28857;&#20132;&#20114;&#24207;&#21015;&#30340;&#24418;&#24335;&#32452;&#32455;&#65292;&#32780;&#19981;&#26159;&#37051;&#25509;&#30697;&#38453;&#12290;&#22823;&#22810;&#25968;&#26102;&#38388;&#22270;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#22312;&#26102;&#38388;&#19978;&#32452;&#21512;&#21382;&#21490;&#20449;&#24687;&#26469;&#24314;&#27169;&#24403;&#21069;&#30340;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20165;&#32771;&#34385;&#20102;&#19968;&#38454;&#26102;&#38388;&#20449;&#24687;&#65292;&#32780;&#24573;&#35270;&#20102;&#37325;&#35201;&#30340;&#39640;&#38454;&#32467;&#26500;&#20449;&#24687;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21517;&#20026;S2T&#65292;&#36890;&#36807;&#25552;&#21462;&#26102;&#38388;&#21644;&#32467;&#26500;&#20449;&#24687;&#26469;&#23398;&#20064;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal graph learning aims to generate high-quality representations for graph-based tasks along with dynamic information, which has recently drawn increasing attention. Unlike the static graph, a temporal graph is usually organized in the form of node interaction sequences over continuous time instead of an adjacency matrix. Most temporal graph learning methods model current interactions by combining historical information over time. However, such methods merely consider the first-order temporal information while ignoring the important high-order structural information, leading to sub-optimal performance. To solve this issue, by extracting both temporal and structural information to learn more informative node representations, we propose a self-supervised method termed S2T for temporal graph learning. Note that the first-order temporal information and the high-order structural information are combined in different ways by the initial node representations to calculate two conditional 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#28508;&#31354;&#38388;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;LSBO&#65289;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#26041;&#27861;&#21253;&#25324;&#28508;&#22312;&#19968;&#33268;&#24615;&#24863;&#30693;&#33719;&#21462;&#20989;&#25968;&#65288;LCA-AF&#65289;&#21644;&#22686;&#21152;&#19968;&#33268;&#24615;&#28857;&#30340;&#28508;&#31354;&#38388;&#29983;&#25104;&#26041;&#27861;&#65288;LCA-VAE&#65289;&#65292;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#24418;&#25104;&#20102;LCA-LSBO&#12290;&#23454;&#39564;&#35777;&#26126;LCA-LSBO&#22312;&#22270;&#20687;&#29983;&#25104;&#21644;&#20840;&#26032;&#30340;&#21270;&#23398;&#35774;&#35745;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.02399</link><description>&lt;p&gt;
&#22686;&#24378;&#28508;&#31354;&#38388;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Exploration in Latent Space Bayesian Optimization. (arXiv:2302.02399v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#28508;&#31354;&#38388;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;LSBO&#65289;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#26041;&#27861;&#21253;&#25324;&#28508;&#22312;&#19968;&#33268;&#24615;&#24863;&#30693;&#33719;&#21462;&#20989;&#25968;&#65288;LCA-AF&#65289;&#21644;&#22686;&#21152;&#19968;&#33268;&#24615;&#28857;&#30340;&#28508;&#31354;&#38388;&#29983;&#25104;&#26041;&#27861;&#65288;LCA-VAE&#65289;&#65292;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#24418;&#25104;&#20102;LCA-LSBO&#12290;&#23454;&#39564;&#35777;&#26126;LCA-LSBO&#22312;&#22270;&#20687;&#29983;&#25104;&#21644;&#20840;&#26032;&#30340;&#21270;&#23398;&#35774;&#35745;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#31354;&#38388;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;LSBO&#65289;&#23558;&#29983;&#25104;&#27169;&#22411;&#65288;&#36890;&#24120;&#26159;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65289;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#24863;&#20852;&#36259;&#30340;&#20840;&#26032;&#23545;&#35937;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#30446;&#26631;&#19981;&#21305;&#37197;&#65292;&#23548;&#33268;&#20102;LSBO&#38754;&#20020;&#25361;&#25112;&#21644;&#25512;&#24191;&#33021;&#21147;&#30340;&#20943;&#24369;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;LSBO&#25928;&#29575;&#24182;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#30340;&#26032;&#24605;&#36335;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28508;&#22312;&#30340;&#19968;&#33268;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;LSBO&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#65292;&#36215;&#28304;&#20110;BO-VAE&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#19968;&#33268;&#24847;&#35782;&#33719;&#21462;&#20989;&#25968;&#65288;LCA-AF&#65289;&#65292;&#21033;&#29992;LSBO&#20013;&#30340;&#19968;&#33268;&#24615;&#21306;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LCA-VAE&#65292;&#19968;&#31181;&#26032;&#30340;VAE&#26041;&#27861;&#65292;&#23427;&#29983;&#25104;&#20855;&#26377;&#22686;&#21152;&#30340;&#19968;&#33268;&#24615;&#28857;&#30340;&#28508;&#31354;&#38388;&#65292;&#25552;&#39640;&#20102;BO&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;&#32467;&#21512;LCA-VAE&#21644;LCA-AF&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;LCA-LSBO&#12290;&#23454;&#39564;&#35780;&#20272;&#35777;&#23454;&#20102;LCA-LSBO&#22312;&#22270;&#20687;&#29983;&#25104;&#21644;&#20840;&#26032;&#30340;&#21270;&#23398;&#35774;&#35745;&#20219;&#21153;&#20013;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent Space Bayesian Optimization (LSBO) combines generative models, typically Variational Autoencoders (VAE), with Bayesian Optimization (BO) to generate de novo objects of interest. However, LSBO faces challenges due to the mismatch between the objectives of BO and VAE, resulting in poor extrapolation capabilities. In this paper, we propose novel contributions to enhance LSBO efficiency and overcome this challenge. We first introduce the concept of latent consistency/inconsistency as a crucial problem in LSBO, arising from the BO-VAE mismatch. To address this, we propose the Latent Consistent Aware-Acquisition Function (LCA-AF) that leverages consistent regions in LSBO. Additionally, we present LCA-VAE, a novel VAE method that generates a latent space with increased consistent points, improving BO's extrapolation capabilities. Combining LCA-VAE and LCA-AF, we develop LCA-LSBO. Experimental evaluations validate the improved performance of LCA-LSBO in image generation and de-novo chem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#20027;&#35201;&#25506;&#35752;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#28508;&#21147;&#21644;&#26368;&#20808;&#36827;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.00058</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65306;&#32508;&#36848;(arXiv&#65306;2302.00058v2 [cs.LG]&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Graph-based Time-Series Anomaly Detection: A Survey. (arXiv:2302.00058v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#20027;&#35201;&#25506;&#35752;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#28508;&#21147;&#21644;&#26368;&#20808;&#36827;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#35768;&#22810;&#31995;&#32479;&#25345;&#32493;&#25910;&#38598;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#22914;&#30005;&#23376;&#21830;&#21153;&#12289;&#32593;&#32476;&#23433;&#20840;&#12289;&#36710;&#36742;&#32500;&#25252;&#21644;&#21307;&#30103;&#30417;&#27979;&#31561;&#39046;&#22495;&#65292;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#24050;&#25104;&#20026;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20294;&#30001;&#20110;&#38656;&#35201;&#21516;&#26102;&#32771;&#34385;&#21464;&#37327;&#20869;&#37096;&#21644;&#21464;&#37327;&#38388;&#30340;&#20381;&#36182;&#24615;&#65292;&#36825;&#19968;&#20219;&#21153;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#35813;&#39046;&#22495;&#30340;&#38590;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#26412;&#32508;&#36848;&#20840;&#38754;&#32780;&#26368;&#26032;&#22320;&#22238;&#39038;&#20102;&#22522;&#20110;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;(G-TSAD)&#12290;&#39318;&#20808;&#25506;&#35752;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#21518;&#22312;&#26102;&#38388;&#24207;&#21015;&#32972;&#26223;&#19979;&#22238;&#39038;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#35752;&#35770;&#20102;&#36825;&#20123;&#25216;&#26415;&#22914;&#20309;&#24212;&#29992;&#20110;&#23454;&#38469;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent advances in technology, a wide range of systems continue to collect a large amount of data over time and thus generate time series. Time-Series Anomaly Detection (TSAD) is an important task in various time-series applications such as e-commerce, cybersecurity, vehicle maintenance, and healthcare monitoring. However, this task is very challenging as it requires considering both the intra-variable dependency and the inter-variable dependency, where a variable can be defined as an observation in time series data. Recent graph-based approaches have made impressive progress in tackling the challenges of this field. In this survey, we conduct a comprehensive and up-to-date review of Graph-based TSAD (G-TSAD). First, we explore the significant potential of graph representation learning for time-series data. Then, we review state-of-the-art graph anomaly detection techniques in the context of time series and discuss their strengths and drawbacks. Finally, we discuss the technic
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#21704;&#23494;&#39039;&#27969;&#24418;&#20013;&#35782;&#21035;&#20986;&#26222;&#36941;&#30340;&#31070;&#32463;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#29289;&#29702;&#31995;&#32479;&#30340;&#24555;&#36895;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.01168</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#22312;&#21704;&#23494;&#39039;&#27969;&#24418;&#20013;&#35782;&#21035;&#26222;&#36941;&#30340;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Identifying Generalized Neural Representation Across Hamiltonian Manifolds via Meta-learning. (arXiv:2212.01168v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01168
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#21704;&#23494;&#39039;&#27969;&#24418;&#20013;&#35782;&#21035;&#20986;&#26222;&#36941;&#30340;&#31070;&#32463;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#29289;&#29702;&#31995;&#32479;&#30340;&#24555;&#36895;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29289;&#29702;&#23398;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#38598;&#20013;&#22312;&#36890;&#36807;&#23558;&#29289;&#29702;&#20808;&#39564;&#25110;&#24402;&#32435;&#20559;&#35265;&#24341;&#20837;&#31070;&#32463;&#32593;&#32476;&#26469;&#21457;&#29616;&#30446;&#26631;&#31995;&#32479;&#30340;&#20849;&#20139;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#29305;&#23450;&#20110;&#31995;&#32479;&#65292;&#19981;&#20801;&#35768;&#36731;&#26494;&#36866;&#24212;&#30001;&#19981;&#21516;&#29289;&#29702;&#27861;&#21017;&#39537;&#21160;&#30340;&#26032;&#29289;&#29702;&#31995;&#32479;&#12290;&#20363;&#22914;&#65292;&#35757;&#32451;&#20110;&#36136;&#28857;&#24377;&#31783;&#31995;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#21452;&#20307;&#31995;&#32479;&#25110;&#20219;&#20309;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#27861;&#21017;&#30340;&#31995;&#32479;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#25105;&#20204;&#30340;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#20803;&#23398;&#20064;&#31639;&#27861;&#20351;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#31215;&#32047;&#32463;&#39564;&#65292;&#24182;&#20351;&#20854;&#36866;&#24212;&#26032;&#30340;&#29289;&#29702;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#36328;&#21508;&#31181;&#21704;&#23494;&#39039;&#27969;&#24418;&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#36825;&#26159;&#21704;&#23494;&#39039;&#31995;&#32479;&#25968;&#25454;&#20998;&#24067;&#30340;&#20849;&#21516;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#30001;&#19981;&#21516;&#31995;&#32479;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#27169;&#22411;&#65292;&#27599;&#20010;&#31995;&#32479;&#37117;&#26377;&#20854;&#33258;&#36523;&#22266;&#26377;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in deep learning for physics have focused on discovering shared representations of target systems by incorporating physics priors or inductive biases into neural networks. However, these approaches are system-specific and do not allow for easy adaptation to new physical systems governed by different laws. For example, a neural network trained on a mass-spring system cannot accurately predict the behavior of a two-body system or any other system with different governing physics. In this work, we model our system with a graph neural network and employ a meta-learning algorithm to enable the model to gain experience over a distribution of tasks and make it adapt to new physics. Our approach aims to learn a general representation across the various Hamiltonian manifolds, which is a common feature of the data distribution of Hamiltonian systems. We train our model using a dataset of different physical systems, each governed by its own inherent dynamics, and evaluate its 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#20219;&#21153;&#20013;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#29109;&#65292;&#24182;&#21457;&#29616;&#31574;&#30053;&#20248;&#21270;&#26234;&#33021;&#20307;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#20855;&#26377;&#20302;&#29109;&#31574;&#30053;&#65292;&#28982;&#32780;Q&#23398;&#20064;&#26234;&#33021;&#20307;&#23545;&#27492;&#24433;&#21709;&#36739;&#23567;&#65292;&#36890;&#24120;&#20445;&#25345;&#39640;&#29109;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2211.11869</link><description>&lt;p&gt;
&#30740;&#31350;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#20010;&#24615;&#21270;&#20219;&#21153;&#20013;&#30340;&#31574;&#30053;&#29109;
&lt;/p&gt;
&lt;p&gt;
Examining Policy Entropy of Reinforcement Learning Agents for Personalization Tasks. (arXiv:2211.11869v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11869
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#20219;&#21153;&#20013;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#29109;&#65292;&#24182;&#21457;&#29616;&#31574;&#30053;&#20248;&#21270;&#26234;&#33021;&#20307;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#20855;&#26377;&#20302;&#29109;&#31574;&#30053;&#65292;&#28982;&#32780;Q&#23398;&#20064;&#26234;&#33021;&#20307;&#23545;&#27492;&#24433;&#21709;&#36739;&#23567;&#65292;&#36890;&#24120;&#20445;&#25345;&#39640;&#29109;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#22312;&#20010;&#24615;&#21270;&#29615;&#22659;&#20013;&#30340;&#34892;&#20026;&#65292;&#24182;&#35814;&#32454;&#25551;&#36848;&#20102;&#19981;&#21516;&#23398;&#20064;&#31639;&#27861;&#25152;&#20851;&#32852;&#30340;&#31574;&#30053;&#29109;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#31574;&#30053;&#20248;&#21270;&#26234;&#33021;&#20307;&#24448;&#24448;&#20855;&#26377;&#20302;&#29109;&#31574;&#30053;&#65292;&#23454;&#38469;&#19978;&#23548;&#33268;&#26234;&#33021;&#20307;&#20248;&#20808;&#32771;&#34385;&#26576;&#20123;&#21160;&#20316;&#32780;&#36991;&#20813;&#20854;&#20182;&#21160;&#20316;&#12290;&#30456;&#21453;&#22320;&#65292;&#25105;&#20204;&#20063;&#34920;&#26126;&#20102;Q&#23398;&#20064;&#26234;&#33021;&#20307;&#23545;&#36825;&#31181;&#34892;&#20026;&#30340;&#24433;&#21709;&#35201;&#23567;&#24471;&#22810;&#65292;&#24182;&#19988;&#36890;&#24120;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#39640;&#29109;&#31574;&#30053;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#26356;&#21487;&#21462;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#21508;&#31181;&#25968;&#20540;&#23454;&#39564;&#20197;&#21450;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#65292;&#20197;&#34920;&#26126;&#36825;&#20123;&#29109;&#24046;&#24322;&#26159;&#30001;&#25152;&#37319;&#29992;&#30340;&#23398;&#20064;&#31867;&#22411;&#25152;&#23548;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This effort is focused on examining the behavior of reinforcement learning systems in personalization environments and detailing the differences in policy entropy associated with the type of learning algorithm utilized. We demonstrate that Policy Optimization agents often possess low-entropy policies during training, which in practice results in agents prioritizing certain actions and avoiding others. Conversely, we also show that Q-Learning agents are far less susceptible to such behavior and generally maintain high-entropy policies throughout training, which is often preferable in real-world applications. We provide a wide range of numerical experiments as well as theoretical justification to show that these differences in entropy are due to the type of learning being employed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#20108;&#20803;&#27835;&#30103;&#26465;&#20214;&#19979;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#22330;&#26223;&#20013;&#65292;&#23545;&#22240;&#26524;&#25512;&#26029;&#30340;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;&#29983;&#25104;&#24314;&#27169;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#26032;&#30340;&#27169;&#22411;&#36873;&#25321;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.01939</link><description>&lt;p&gt;
&#24322;&#36136;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20013;&#27169;&#22411;&#36873;&#25321;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Empirical Analysis of Model Selection for Heterogeneous Causal Effect Estimation. (arXiv:2211.01939v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#20108;&#20803;&#27835;&#30103;&#26465;&#20214;&#19979;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#22330;&#26223;&#20013;&#65292;&#23545;&#22240;&#26524;&#25512;&#26029;&#30340;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;&#29983;&#25104;&#24314;&#27169;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#26032;&#30340;&#27169;&#22411;&#36873;&#25321;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#20108;&#20803;&#27835;&#30103;&#26465;&#20214;&#19979;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;CATE&#65289;&#20272;&#35745;&#30340;&#24773;&#20917;&#12290;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#36873;&#25321;&#19981;&#21516;&#65292;&#30001;&#20110;&#25105;&#20204;&#26080;&#27861;&#35266;&#23519;&#21040;&#20219;&#20309;&#25968;&#25454;&#28857;&#30340;&#21453;&#20107;&#23454;&#28508;&#22312;&#32467;&#26524;&#65292;&#22240;&#27492;&#27809;&#26377;&#23436;&#32654;&#30340;&#20132;&#21449;&#39564;&#35777;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#20195;&#29702;&#24230;&#37327;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21462;&#20915;&#20110;&#20174;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#20013;&#20272;&#35745;&#30340;&#36741;&#21161;&#24178;&#25200;&#27169;&#22411;&#65288;&#20542;&#21521;&#24615;&#24471;&#20998;&#27169;&#22411;&#12289;&#32467;&#26524;&#22238;&#24402;&#27169;&#22411;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24230;&#37327;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20165;&#22312;&#25105;&#20204;&#21487;&#20197;&#35775;&#38382;&#21453;&#20107;&#23454;&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#35780;&#20272;&#25991;&#29486;&#20013;&#20171;&#32461;&#30340;&#36825;&#20123;&#24230;&#37327;&#26041;&#27861;&#20197;&#21450;&#26412;&#30740;&#31350;&#20013;&#20171;&#32461;&#30340;&#26032;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#22312;&#23454;&#29616;&#22810;&#20010;&#36924;&#30495;&#25968;&#25454;&#38598;&#30340;&#26368;&#26032;&#29983;&#25104;&#24314;&#27169;&#36827;&#23637;&#22522;&#30784;&#19978;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#20102;&#26032;&#30340;&#27169;&#22411;&#36873;&#25321;&#31574;&#30053;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of model selection in causal inference, specifically for the case of conditional average treatment effect (CATE) estimation under binary treatments. Unlike model selection in machine learning, there is no perfect analogue of cross-validation as we do not observe the counterfactual potential outcome for any data point. Towards this, there have been a variety of proxy metrics proposed in the literature, that depend on auxiliary nuisance models estimated from the observed data (propensity score model, outcome regression model). However, the effectiveness of these metrics has only been studied on synthetic datasets as we can access the counterfactual data for them. We conduct an extensive empirical analysis to judge the performance of these metrics introduced in the literature, and novel ones introduced in this work, where we utilize the latest advances in generative modeling to incorporate multiple realistic datasets. Our analysis suggests novel model selection strate
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#36873;&#25321;&#26159;&#19968;&#31181;&#20196;&#20154;&#24778;&#35766;&#30340;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#26500;&#24314;&#23567;&#22411;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#35757;&#32451;&#20998;&#24067;&#32780;&#38750;&#27979;&#35797;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#20256;&#32479;&#22522;&#20934;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.03921</link><description>&lt;p&gt;
&#25968;&#25454;&#36873;&#25321;&#65306;&#19968;&#31181;&#20196;&#20154;&#24778;&#35766;&#30340;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#26500;&#24314;&#23567;&#22411;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data Selection: A Surprisingly Effective and General Principle for Building Small Interpretable Models. (arXiv:2210.03921v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03921
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#36873;&#25321;&#26159;&#19968;&#31181;&#20196;&#20154;&#24778;&#35766;&#30340;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#26500;&#24314;&#23567;&#22411;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#35757;&#32451;&#20998;&#24067;&#32780;&#38750;&#27979;&#35797;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#20256;&#32479;&#22522;&#20934;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#23454;&#35777;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#19968;&#31181;&#26500;&#24314;&#20934;&#30830;&#23567;&#22411;&#27169;&#22411;&#30340;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#31574;&#30053;&#12290;&#36825;&#31181;&#27169;&#22411;&#23545;&#20110;&#21487;&#35299;&#37322;&#24615;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#24182;&#19988;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#20063;&#26377;&#29992;&#36884;&#12290;&#35813;&#31574;&#30053;&#26159;&#23398;&#20064;&#35757;&#32451;&#20998;&#24067;&#32780;&#19981;&#26159;&#20351;&#29992;&#27979;&#35797;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#20998;&#24067;&#23398;&#20064;&#31639;&#27861;&#19981;&#26159;&#36825;&#39033;&#24037;&#20316;&#30340;&#36129;&#29486;&#65307;&#25105;&#20204;&#24378;&#35843;&#36825;&#31181;&#31616;&#21333;&#31574;&#30053;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#24182;&#19988;&#22522;&#20110;&#20005;&#26684;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#36825;&#20123;&#32467;&#26524;&#26159;&#25105;&#20204;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#20197;&#19979;&#20219;&#21153;&#65306;&#65288;1&#65289;&#26500;&#24314;&#32858;&#31867;&#35299;&#37322;&#26641;&#65292;&#65288;2&#65289;&#22522;&#20110;&#21407;&#22411;&#30340;&#20998;&#31867;&#65292;&#20197;&#21450;&#65288;3&#65289;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#23427;&#25552;&#39640;&#20102;&#24369;&#20256;&#32479;&#22522;&#20934;&#30340;&#20934;&#30830;&#24615;&#65292;&#20351;&#23427;&#20204;&#20196;&#20154;&#24778;&#35766;&#22320;&#19982;&#19987;&#19994;&#30340;&#29616;&#20195;&#25216;&#26415;&#30456;&#31454;&#20105;&#12290;&#27492;&#31574;&#30053;&#20063;&#36866;&#29992;&#20110;&#27169;&#22411;&#22823;&#23567;&#30340;&#27010;&#24565;&#12290;&#22312;&#21069;&#20004;&#20010;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#22823;&#23567;&#36890;&#36807;&#26641;&#20013;&#21494;&#23376;&#33410;&#28857;&#30340;&#25968;&#37327;&#26469;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present convincing empirical evidence for an effective and general strategy for building accurate small models. Such models are attractive for interpretability and also find use in resource-constrained environments. The strategy is to learn the training distribution instead of using data from the test distribution. The distribution learning algorithm is not a contribution of this work; we highlight the broad usefulness of this simple strategy on a diverse set of tasks, and as such these rigorous empirical results are our contribution. We apply it to the tasks of (1) building cluster explanation trees, (2) prototype-based classification, and (3) classification using Random Forests, and show that it improves the accuracy of weak traditional baselines to the point that they are surprisingly competitive with specialized modern techniques.  This strategy is also versatile wrt the notion of model size. In the first two tasks, model size is identified by number of leaves in the tree and th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102; ELBO &#25910;&#25947;&#21040;&#29109;&#21644;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#31867;&#24191;&#27867;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;ELBO &#22312;&#25152;&#26377;&#23398;&#20064;&#30340;&#31283;&#23450;&#28857;&#22788;&#37117;&#31561;&#20110;&#19968;&#31995;&#21015;&#29109;&#30340;&#21644;&#65292;&#20026;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#26412;&#23646;&#24615;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2209.03077</link><description>&lt;p&gt;
&#20851;&#20110;ELBO&#25910;&#25947;&#21040;&#29109;&#21644;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of the ELBO to Entropy Sums. (arXiv:2209.03077v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102; ELBO &#25910;&#25947;&#21040;&#29109;&#21644;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#31867;&#24191;&#27867;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;ELBO &#22312;&#25152;&#26377;&#23398;&#20064;&#30340;&#31283;&#23450;&#28857;&#22788;&#37117;&#31561;&#20110;&#19968;&#31995;&#21015;&#29109;&#30340;&#21644;&#65292;&#20026;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#26412;&#23646;&#24615;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#19979;&#30028;&#65288;&#21448;&#31216;ELBO&#25110;&#33258;&#30001;&#33021;&#65289;&#26159;&#35768;&#22810;&#32463;&#20856;&#21644;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#26680;&#24515;&#30446;&#26631;&#12290;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25913;&#21464;&#27169;&#22411;&#21442;&#25968;&#65292;&#20351;&#21464;&#20998;&#19979;&#30028;&#22686;&#21152;&#12290;&#36890;&#24120;&#65292;&#23398;&#20064;&#36827;&#34892;&#21040;&#21442;&#25968;&#25910;&#25947;&#21040;&#25509;&#36817;&#23398;&#20064;&#21160;&#24577;&#30340;&#31283;&#23450;&#28857;&#20540;&#12290;&#22312;&#26412;&#25991;&#30340;&#29702;&#35770;&#36129;&#29486;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#65288;&#23545;&#20110;&#19968;&#31867;&#38750;&#24120;&#24191;&#27867;&#30340;&#29983;&#25104;&#27169;&#22411;&#65289;&#65292;&#21464;&#20998;&#19979;&#30028;&#22312;&#25152;&#26377;&#23398;&#20064;&#30340;&#31283;&#23450;&#28857;&#22788;&#22343;&#31561;&#20110;&#19968;&#31995;&#21015;&#29109;&#30340;&#21644;&#12290;&#23545;&#20110;&#20855;&#26377;&#19968;&#32452;&#28508;&#22312;&#21464;&#37327;&#21644;&#19968;&#32452;&#35266;&#27979;&#21464;&#37327;&#30340;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#20010;&#21644;&#21253;&#25324;&#19977;&#20010;&#29109;: (A) &#21464;&#20998;&#20998;&#24067;&#30340;&#29109;&#65288;&#24179;&#22343;&#29109;&#65289;&#65292;(B) &#27169;&#22411;&#20808;&#39564;&#20998;&#24067;&#30340;&#36127;&#29109;&#21644; (C) &#21487;&#35266;&#27979;&#20998;&#24067;&#30340;&#65288;&#26399;&#26395;&#65289;&#36127;&#29109;&#12290;&#25152;&#24471;&#21040;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#21253;&#25324;&#65306;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#28857;&#65292;&#22312;&#23398;&#20064;&#30340;&#20219;&#24847;&#38454;&#27573;&#21644;&#21508;&#31181;&#19981;&#21516;&#30340;&#29983;&#25104;&#27169;&#22411;&#31561;&#30495;&#23454;&#26465;&#20214;&#12290;&#26412;&#30740;&#31350;&#20026;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#26412;&#23646;&#24615;&#25552;&#20379;&#20102;&#28145;&#20837;&#27934;&#23519;&#65292;&#26159;&#23545;&#20248;&#21270;&#25512;&#29702;&#21644;&#23398;&#20064;&#30340;&#29702;&#35770;&#20998;&#26512;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The variational lower bound (a.k.a. ELBO or free energy) is the central objective for many established as well as many novel algorithms for unsupervised learning. Learning algorithms change model parameters such that the variational lower bound increases. Learning usually proceeds until parameters have converged to values close to a stationary point of the learning dynamics. In this purely theoretical contribution, we show that (for a very large class of generative models) the variational lower bound is at all stationary points of learning equal to a sum of entropies. For standard machine learning models with one set of latents and one set observed variables, the sum consists of three entropies: (A) the (average) entropy of the variational distributions, (B) the negative entropy of the model's prior distribution, and (C) the (expected) negative entropy of the observable distributions. The obtained result applies under realistic conditions including: finite numbers of data points, at an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20998;&#21035;&#36866;&#29992;&#20110;&#39640;&#32500;&#21644;&#20302;&#32500;&#38382;&#39064;&#65292;&#24182;&#22312;Cleveland&#21644;Statlog&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#26356;&#36866;&#21512;&#20110;&#26089;&#26399;&#24515;&#33039;&#30142;&#30149;&#39044;&#27979;&#65292;&#21487;&#33719;&#24471;&#39640;&#36798;96.43&#65285;&#21644;97.78&#65285;&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#12290;</title><link>http://arxiv.org/abs/2208.08882</link><description>&lt;p&gt;
&#26089;&#26399;&#24515;&#33039;&#30142;&#30149;&#39044;&#27979;&#30340;&#28151;&#21512;&#37327;&#23376;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Early heart disease prediction using hybrid quantum classification. (arXiv:2208.08882v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20998;&#21035;&#36866;&#29992;&#20110;&#39640;&#32500;&#21644;&#20302;&#32500;&#38382;&#39064;&#65292;&#24182;&#22312;Cleveland&#21644;Statlog&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#26356;&#36866;&#21512;&#20110;&#26089;&#26399;&#24515;&#33039;&#30142;&#30149;&#39044;&#27979;&#65292;&#21487;&#33719;&#24471;&#39640;&#36798;96.43&#65285;&#21644;97.78&#65285;&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#33039;&#21457;&#30149;&#29575;&#21644;&#24515;&#33039;&#27515;&#20129;&#29575;&#30340;&#22686;&#21152;&#26126;&#26174;&#24433;&#21709;&#20102;&#20840;&#29699;&#20844;&#20849;&#20581;&#24247;&#21644;&#19990;&#30028;&#32463;&#27982;&#12290;&#26089;&#26399;&#39044;&#27979;&#23545;&#20110;&#20943;&#23569;&#24515;&#33039;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65306;&#28151;&#21512;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#28151;&#21512;&#38543;&#26426;&#26862;&#26519;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26089;&#26399;&#26816;&#27979;&#24515;&#33039;&#30142;&#30149;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;Cleveland&#21644;Statlog&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28151;&#21512;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#28151;&#21512;&#38543;&#26426;&#26862;&#26519;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20998;&#21035;&#36866;&#29992;&#20110;&#39640;&#32500;&#21644;&#20302;&#32500;&#38382;&#39064;&#12290;&#28151;&#21512;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#23545;&#24322;&#24120;&#25968;&#25454;&#25935;&#24863;&#65292;&#32780;&#28151;&#21512;&#38543;&#26426;&#26862;&#26519;&#23545;&#24322;&#24120;&#25968;&#25454;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#19982;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#27604;&#36739;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#37327;&#23376;&#26041;&#27861;&#26356;&#36866;&#21512;&#26089;&#26399;&#24515;&#33039;&#30142;&#30149;&#39044;&#27979;&#65292;&#22312;Cleveland&#21644;Statlog&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#33719;&#24471;&#20102;96.43&#65285;&#21644;97.78&#65285;&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rate of heart morbidity and heart mortality increases significantly which affect the global public health and world economy. Early prediction of heart disease is crucial for reducing heart morbidity and mortality. This paper proposes two quantum machine learning methods i.e. hybrid quantum neural network and hybrid random forest quantum neural network for early detection of heart disease. The methods are applied on the Cleveland and Statlog datasets. The results show that hybrid quantum neural network and hybrid random forest quantum neural network are suitable for high dimensional and low dimensional problems respectively. The hybrid quantum neural network is sensitive to outlier data while hybrid random forest is robust on outlier data. A comparison between different machine learning methods shows that the proposed quantum methods are more appropriate for early heart disease prediction where 96.43% and 97.78% area under curve are obtained for Cleveland and Statlog dataset respect
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#26381;&#21153;&#30340;&#21442;&#19982;&#21644;&#37325;&#26032;&#35757;&#32451;&#21160;&#24577;&#65292;&#21457;&#29616;&#24403;&#23398;&#20064;&#32773;&#21644;&#29992;&#25143;&#23376;&#32676;&#20855;&#26377;&#39118;&#38505;&#20943;&#23569;&#24615;&#36136;&#26102;&#65292;&#21807;&#19968;&#30340;&#31283;&#23450;&#22343;&#34913;&#26159;&#32454;&#20998;&#30340;&#65292;&#23558;&#23376;&#32676;&#20998;&#37197;&#32473;&#21333;&#20010;&#23398;&#20064;&#32773;&#12290;&#21151;&#21033;&#20027;&#20041;&#31038;&#20250;&#26368;&#20248;&#26159;&#19968;&#20010;&#31283;&#23450;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2206.02667</link><description>&lt;p&gt;
&#20174;&#21442;&#19982;&#24230;&#21160;&#24577;&#21644;&#22810;&#23398;&#20064;&#32773;&#37325;&#26032;&#35757;&#32451;&#20013;&#20135;&#29983;&#30340;&#32039;&#24613;&#32454;&#20998;
&lt;/p&gt;
&lt;p&gt;
Emergent segmentation from participation dynamics and multi-learner retraining. (arXiv:2206.02667v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02667
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#26381;&#21153;&#30340;&#21442;&#19982;&#21644;&#37325;&#26032;&#35757;&#32451;&#21160;&#24577;&#65292;&#21457;&#29616;&#24403;&#23398;&#20064;&#32773;&#21644;&#29992;&#25143;&#23376;&#32676;&#20855;&#26377;&#39118;&#38505;&#20943;&#23569;&#24615;&#36136;&#26102;&#65292;&#21807;&#19968;&#30340;&#31283;&#23450;&#22343;&#34913;&#26159;&#32454;&#20998;&#30340;&#65292;&#23558;&#23376;&#32676;&#20998;&#37197;&#32473;&#21333;&#20010;&#23398;&#20064;&#32773;&#12290;&#21151;&#21033;&#20027;&#20041;&#31038;&#20250;&#26368;&#20248;&#26159;&#19968;&#20010;&#31283;&#23450;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26381;&#21153;&#20013;&#36873;&#25321;&#21442;&#19982;&#65292;&#24448;&#24448;&#22522;&#20110;&#35813;&#26381;&#21153;&#30340;&#36136;&#37327;&#65292;&#24433;&#21709;&#20102;&#26381;&#21153;&#23398;&#20064;&#21644;&#25913;&#36827;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#23398;&#20064;&#32773;&#21644;&#29992;&#25143;&#23376;&#32676;&#37117;&#20855;&#26377;&#39118;&#38505;&#20943;&#23569;&#24615;&#36136;&#26102;&#65292;&#21442;&#19982;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#21160;&#24577;&#29983;&#25104;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#26799;&#24230;&#19979;&#38477;&#12289;&#20056;&#27861;&#26435;&#37325;&#31561;&#24191;&#27867;&#30340;&#26356;&#26032;&#26041;&#27861;&#12290;&#20030;&#20010;&#20363;&#23376;&#65292;&#20551;&#35774;&#20010;&#20307;&#36873;&#25321;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#33457;&#36153;&#26102;&#38388;&#30340;&#27604;&#20363;&#19982;&#27599;&#20010;&#24179;&#21488;&#23545;&#20182;&#20204;&#30340;&#24037;&#20316;&#25928;&#26524;&#25104;&#27604;&#20363;&#12290;&#27599;&#20010;&#24179;&#21488;&#36824;&#20250;&#25910;&#38598;&#20854;&#27963;&#36291;&#29992;&#25143;&#30340;&#25968;&#25454;&#65292;&#24182;&#29992;&#26799;&#24230;&#27493;&#39588;&#26356;&#26032;&#21442;&#25968;&#12290;&#23545;&#20110;&#36825;&#20010;&#20363;&#23376;&#21644;&#25105;&#20204;&#30340;&#19968;&#33324;&#21160;&#24577;&#31867;&#21035;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21807;&#19968;&#30340;&#28176;&#36817;&#31283;&#23450;&#22343;&#34913;&#26159;&#32454;&#20998;&#30340;&#65292;&#23558;&#23376;&#32676;&#20998;&#37197;&#32473;&#21333;&#20010;&#23398;&#20064;&#32773;&#12290;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#21151;&#21033;&#20027;&#20041;&#31038;&#20250;&#26368;&#20248;&#26159;&#19968;&#20010;&#31283;&#23450;&#22343;&#34913;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#26174;&#31034;&#37325;&#22797;&#30340;&#39118;&#38505;&#26368;&#23567;&#21270;&#21487;&#33021;&#19981;&#20250;&#23545;&#38887;&#24615;&#21644;&#21033;&#30410;&#36827;&#34892;&#20219;&#20309;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The choice to participate in a data-driven service, often made on the basis of quality of that service, influences the ability of the service to learn and improve. We study the participation and retraining dynamics that arise when both the learners and sub-populations of users are \emph{risk-reducing}, which cover a broad class of updates including gradient descent, multiplicative weights, etc. Suppose, for example, that individuals choose to spend their time amongst social media platforms proportionally to how well each platform works for them. Each platform also gathers data about its active users, which it uses to update parameters with a gradient step. For this example and for our general class of dynamics, we show that the only asymptotically stable equilibria are segmented, with sub-populations allocated to a single learner. Under mild assumptions, the utilitarian social optimum is a stable equilibrium. In contrast to previous work, which shows that repeated risk minimization can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#37325;&#26500;&#26041;&#27861;&#65292;&#38598;&#25104;&#20110;&#20219;&#20309;&#31867;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20197;&#22686;&#21152;&#21516;&#36136;&#24615;&#12290;&#26041;&#27861;&#21253;&#25324;&#33258;&#36866;&#24212;&#35889;&#32858;&#31867;&#12289;&#23494;&#24230;&#24863;&#30693;&#21516;&#36136;&#24615;&#24230;&#37327;&#26041;&#27861;&#21644;&#22522;&#20110;&#32858;&#31867;&#32467;&#26524;&#30340;&#37051;&#25509;&#30697;&#38453;&#37325;&#26500;&#12290;</title><link>http://arxiv.org/abs/2206.02386</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#36866;&#24212;&#35889;&#32858;&#31867;&#30340;&#22270;&#37325;&#26500;&#25552;&#39640;&#21516;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Restructuring Graph for Higher Homophily via Adaptive Spectral Clustering. (arXiv:2206.02386v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#37325;&#26500;&#26041;&#27861;&#65292;&#38598;&#25104;&#20110;&#20219;&#20309;&#31867;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20197;&#22686;&#21152;&#21516;&#36136;&#24615;&#12290;&#26041;&#27861;&#21253;&#25324;&#33258;&#36866;&#24212;&#35889;&#32858;&#31867;&#12289;&#23494;&#24230;&#24863;&#30693;&#21516;&#36136;&#24615;&#24230;&#37327;&#26041;&#27861;&#21644;&#22522;&#20110;&#32858;&#31867;&#32467;&#26524;&#30340;&#37051;&#25509;&#30697;&#38453;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#25991;&#29486;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#22270;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20294;&#22312;&#23558;&#20256;&#32479;&#22270;&#31070;&#32463;&#32593;&#32476;&#36866;&#24212;&#20110; less-homophilic &#22270;&#26041;&#38754;&#20570;&#24471;&#24456;&#23569;&#12290;&#34429;&#28982;&#20256;&#32479;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702; less-homophilic &#22270;&#30340;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#65292;&#20294;&#20173;&#20855;&#26377;&#25928;&#29575;&#39640;&#12289;&#31616;&#21333;&#12289;&#21487;&#35299;&#37322;&#24615;&#22909;&#31561;&#22810;&#20010;&#20248;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#37325;&#26500;&#26041;&#27861;&#65292;&#21487;&#38598;&#25104;&#21040;&#20219;&#20309;&#31867;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#21253;&#25324;&#20256;&#32479;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#21457;&#25381;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#21516;&#26102;&#20943;&#36731;&#20854;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;: a) &#23398;&#20064;&#25311;&#21512;&#33410;&#28857;&#26631;&#31614;&#30340;&#33258;&#36866;&#24212;&#35889;&#32858;&#31867;&#30340; pseudo-eigenvector &#26435;&#37325;&#65292;b) &#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#23494;&#24230;&#24863;&#30693;&#30340;&#21516;&#36136;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#26631;&#31614;&#19981;&#24179;&#34913;&#24615;&#40065;&#26834;&#24615;&#65292;c) &#22522;&#20110;&#33258;&#36866;&#24212;&#35889;&#32858;&#31867;&#30340;&#32467;&#26524;&#37325;&#26500;&#37051;&#25509;&#30697;&#38453;&#65292;&#20197;&#26368;&#22823;&#21270;&#21516;&#36136;&#24615;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
While a growing body of literature has been studying new Graph Neural Networks (GNNs) that work on both homophilic and heterophilic graphs, little has been done on adapting classical GNNs to less-homophilic graphs. Although the ability to handle less-homophilic graphs is restricted, classical GNNs still stand out in several nice properties such as efficiency, simplicity, and explainability. In this work, we propose a novel graph restructuring method that can be integrated into any type of GNNs, including classical GNNs, to leverage the benefits of existing GNNs while alleviating their limitations. Our contribution is threefold: a) learning the weight of pseudo-eigenvectors for an adaptive spectral clustering that aligns well with known node labels, b) proposing a new density-aware homophilic metric that is robust to label imbalance, and c) reconstructing the adjacency matrix based on the result of adaptive spectral clustering to maximize the homophilic scores. The experimental results 
&lt;/p&gt;</description></item></channel></rss>