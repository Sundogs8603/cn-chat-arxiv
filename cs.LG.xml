<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02491</link><description>&lt;p&gt;
&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Measuring Social Norms of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02491
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#20197;&#26816;&#39564;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35201;&#27714;&#20855;&#26377;&#35299;&#20915;&#31038;&#20250;&#35268;&#33539;&#30340;&#22522;&#26412;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#26368;&#22823;&#30340;&#31038;&#20250;&#35268;&#33539;&#25216;&#33021;&#38598;&#65292;&#21253;&#25324;402&#39033;&#25216;&#33021;&#21644;12,383&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#35266;&#28857;&#21644;&#35770;&#28857;&#21040;&#25991;&#21270;&#21644;&#27861;&#24459;&#31561;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#12290;&#25105;&#20204;&#26681;&#25454;K-12&#35838;&#31243;&#35774;&#35745;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#30452;&#25509;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#29702;&#35299;&#33021;&#21147;&#19982;&#20154;&#31867;&#36827;&#34892;&#27604;&#36739;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#19982;&#23567;&#23398;&#29983;&#36827;&#34892;&#27604;&#36739;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#20135;&#29983;&#20960;&#20046;&#38543;&#26426;&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5-Turbo&#21644;LLaMA2-Chat&#65289;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#20165;&#30053;&#20302;&#20110;&#20154;&#31867;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02491v1 Announce Type: cross  Abstract: We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;Masked AutoEncoders&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;NeRF-MAE&#29992;&#20110;&#33258;&#30417;&#30563;&#19977;&#32500;&#34920;&#31034;&#23398;&#20064;&#65292;&#21033;&#29992;&#26631;&#20934;&#30340;&#19977;&#32500;Vision Transformers&#36866;&#24212;NeRF&#30340;&#29420;&#29305;&#20844;&#24335;&#65292;&#23558;NeRF&#30340;&#20307;&#31215;&#32593;&#26684;&#20316;&#20026;&#23494;&#38598;&#36755;&#20837;&#65292;&#20197;&#20135;&#29983;&#26377;&#25928;&#30340;&#19977;&#32500;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2404.01300</link><description>&lt;p&gt;
NeRF-MAE: &#33258;&#30417;&#30563;&#19977;&#32500;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;Masked AutoEncoders
&lt;/p&gt;
&lt;p&gt;
NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;Masked AutoEncoders&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;NeRF-MAE&#29992;&#20110;&#33258;&#30417;&#30563;&#19977;&#32500;&#34920;&#31034;&#23398;&#20064;&#65292;&#21033;&#29992;&#26631;&#20934;&#30340;&#19977;&#32500;Vision Transformers&#36866;&#24212;NeRF&#30340;&#29420;&#29305;&#20844;&#24335;&#65292;&#23558;NeRF&#30340;&#20307;&#31215;&#32593;&#26684;&#20316;&#20026;&#23494;&#38598;&#36755;&#20837;&#65292;&#20197;&#20135;&#29983;&#26377;&#25928;&#30340;&#19977;&#32500;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#31070;&#32463;&#22330;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#33021;&#22815;&#29702;&#35299;&#19977;&#32500;&#35270;&#35273;&#19990;&#30028;&#65292;&#22914;&#25512;&#26029;&#35821;&#20041;&#12289;&#20960;&#20309;&#21644;&#21160;&#24577;&#31561;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#22330;&#22312;&#20174;&#20108;&#32500;&#22270;&#20687;&#20013;&#23494;&#38598;&#34920;&#31034;&#19977;&#32500;&#22330;&#26223;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20855;&#20307;&#20351;&#29992;Masked AutoEncoders&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#20511;&#37492;&#20102;&#23558;transformers&#25193;&#23637;&#21040;&#26032;&#25968;&#25454;&#27169;&#24577;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#25104;&#21151;&#65292;&#21033;&#29992;&#26631;&#20934;&#30340;&#19977;&#32500;Vision Transformers&#26469;&#36866;&#24212;NeRF&#30340;&#29420;&#29305;&#20844;&#24335;&#12290;&#25105;&#20204;&#23558;NeRF&#30340;&#20307;&#31215;&#32593;&#26684;&#20316;&#20026;transformer&#30340;&#23494;&#38598;&#36755;&#20837;&#65292;&#19982;&#20854;&#20182;&#19977;&#32500;&#34920;&#31034;&#65288;&#22914;&#28857;&#20113;&#65289;&#36827;&#34892;&#23545;&#27604;&#65292;&#20854;&#20449;&#24687;&#23494;&#24230;&#21487;&#33021;&#19981;&#22343;&#21248;&#65292;&#32780;&#34920;&#31034;&#26159;&#19981;&#35268;&#21017;&#30340;&#12290;&#30001;&#20110;&#23558;masked autoencoders&#24212;&#29992;&#20110;&#31867;&#20284;NeRF&#36825;&#26679;&#30340;&#38544;&#24335;&#34920;&#31034;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#36873;&#25321;&#25552;&#21462;&#19968;&#20010;&#26174;&#24335;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01300v1 Announce Type: cross  Abstract: Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we ask the question: Can we scale their self-supervised pretraining, specifically using masked autoencoders, to generate effective 3D representations from posed RGB images. Owing to the astounding success of extending transformers to novel data modalities, we employ standard 3D Vision Transformers to suit the unique formulation of NeRFs. We leverage NeRF's volumetric grid as a dense input to the transformer, contrasting it with other 3D representations such as pointclouds where the information density can be uneven, and the representation is irregular. Due to the difficulty of applying masked autoencoders to an implicit representation, such as NeRF, we opt for extracting an explicit repres
&lt;/p&gt;</description></item><item><title>PDE-CNNs&#36890;&#36807;&#21033;&#29992;&#20960;&#20309;&#24847;&#20041;&#30340;&#28436;&#21270;PDE&#30340;&#27714;&#35299;&#22120;&#26367;&#20195;&#20256;&#32479;&#30340;&#32452;&#20214;&#65292;&#25552;&#20379;&#20102;&#26356;&#23569;&#30340;&#21442;&#25968;&#12289;&#22266;&#26377;&#30340;&#31561;&#21464;&#24615;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#20960;&#20309;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15182</link><description>&lt;p&gt;
PDE-CNNs&#65306;&#20844;&#29702;&#25512;&#23548;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
PDE-CNNs: Axiomatic Derivations and Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15182
&lt;/p&gt;
&lt;p&gt;
PDE-CNNs&#36890;&#36807;&#21033;&#29992;&#20960;&#20309;&#24847;&#20041;&#30340;&#28436;&#21270;PDE&#30340;&#27714;&#35299;&#22120;&#26367;&#20195;&#20256;&#32479;&#30340;&#32452;&#20214;&#65292;&#25552;&#20379;&#20102;&#26356;&#23569;&#30340;&#21442;&#25968;&#12289;&#22266;&#26377;&#30340;&#31561;&#21464;&#24615;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#20960;&#20309;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#32452;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PDE-G-CNNs&#65289;&#21033;&#29992;&#20855;&#26377;&#20960;&#20309;&#24847;&#20041;&#30340;&#28436;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#27714;&#35299;&#22120;&#26367;&#20195;G-CNNs&#20013;&#24120;&#35268;&#32452;&#20214;&#12290;PDE-G-CNNs&#21516;&#26102;&#25552;&#20379;&#20102;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#26356;&#23569;&#30340;&#21442;&#25968;&#12289;&#22266;&#26377;&#31561;&#21464;&#24615;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#20960;&#20309;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#29305;&#24449;&#22270;&#22312;&#25972;&#20010;&#32593;&#32476;&#20013;&#20026;&#20108;&#32500;&#30340;&#27431;&#20960;&#37324;&#24503;&#31561;&#21464;PDE-G-CNNs&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26694;&#26550;&#30340;&#21464;&#20307;&#31216;&#20026;PDE-CNN&#12290;&#25105;&#20204;&#21015;&#20986;&#20102;&#20960;&#20010;&#22312;&#23454;&#36341;&#20013;&#20196;&#20154;&#28385;&#24847;&#30340;&#20844;&#29702;&#65292;&#24182;&#20174;&#20013;&#25512;&#23548;&#20986;&#24212;&#22312;PDE-CNN&#20013;&#20351;&#29992;&#21738;&#20123;PDE&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#20856;&#32447;&#24615;&#21644;&#24418;&#24577;&#23610;&#24230;&#31354;&#38388;&#29702;&#35770;&#30340;&#20844;&#29702;&#21463;&#21551;&#21457;&#65292;&#36890;&#36807;&#24341;&#20837;&#21322;&#22495;&#20540;&#20449;&#21495;&#23545;&#20854;&#36827;&#34892;&#25512;&#24191;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#65292;&#30456;&#23545;&#20110;&#23567;&#22411;&#32593;&#32476;&#65292;PDE-CNN&#25552;&#20379;&#20102;&#26356;&#23569;&#30340;&#21442;&#25968;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15182v1 Announce Type: new  Abstract: PDE-based Group Convolutional Neural Networks (PDE-G-CNNs) utilize solvers of geometrically meaningful evolution PDEs as substitutes for the conventional components in G-CNNs. PDE-G-CNNs offer several key benefits all at once: fewer parameters, inherent equivariance, better performance, data efficiency, and geometric interpretability. In this article we focus on Euclidean equivariant PDE-G-CNNs where the feature maps are two dimensional throughout. We call this variant of the framework a PDE-CNN. We list several practically desirable axioms and derive from these which PDEs should be used in a PDE-CNN. Here our approach to geometric learning via PDEs is inspired by the axioms of classical linear and morphological scale-space theory, which we generalize by introducing semifield-valued signals. Furthermore, we experimentally confirm for small networks that PDE-CNNs offer fewer parameters, better performance, and data efficiency in compariso
&lt;/p&gt;</description></item><item><title>CMDI&#32858;&#31867;&#26041;&#27861;&#21019;&#26032;&#24615;&#22320;&#23558;&#20108;&#32500;&#32467;&#26500;&#20449;&#24687;&#29702;&#35770;&#34701;&#20837;&#32858;&#31867;&#36807;&#31243;&#20013;&#65292;&#24357;&#34917;&#20102;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#32858;&#31867;&#26041;&#27861;&#20013;&#24573;&#30053;&#30340;&#38543;&#26426;&#28216;&#36208;&#35775;&#38382;&#33410;&#28857;&#21644;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13846</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#22270;&#26368;&#22823;&#35299;&#30721;&#20449;&#24687;&#30340;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Clustering Method with Graph Maximum Decoding Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13846
&lt;/p&gt;
&lt;p&gt;
CMDI&#32858;&#31867;&#26041;&#27861;&#21019;&#26032;&#24615;&#22320;&#23558;&#20108;&#32500;&#32467;&#26500;&#20449;&#24687;&#29702;&#35770;&#34701;&#20837;&#32858;&#31867;&#36807;&#31243;&#20013;&#65292;&#24357;&#34917;&#20102;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#32858;&#31867;&#26041;&#27861;&#20013;&#24573;&#30053;&#30340;&#38543;&#26426;&#28216;&#36208;&#35775;&#38382;&#33410;&#28857;&#21644;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#27169;&#22411;&#30340;&#32858;&#31867;&#26041;&#27861;&#22240;&#20854;&#22312;&#21508;&#31181;&#30693;&#35782;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#20854;&#33021;&#22815;&#19982;&#20854;&#20182;&#30456;&#20851;&#24212;&#29992;&#26080;&#32541;&#38598;&#25104;&#30340;&#36866;&#24212;&#24615;&#36171;&#20104;&#20102;&#22522;&#20110;&#22270;&#27169;&#22411;&#30340;&#32858;&#31867;&#20998;&#26512;&#33021;&#21147;&#65292;&#21487;&#20197;&#24378;&#22823;&#22320;&#20174;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#8220;&#33258;&#28982;&#20851;&#32852;&#8221;&#25110;&#8220;&#22270;&#32467;&#26500;&#8221;&#65292;&#26377;&#21161;&#20110;&#24314;&#27169;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#65292;&#20294;&#24403;&#21069;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#30340;&#32858;&#31867;&#26041;&#27861;&#24573;&#30053;&#20102;&#33410;&#28857;&#20043;&#38388;&#38543;&#26426;&#28216;&#36208;&#35775;&#38382;&#20197;&#21450;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#32467;&#26500;&#20449;&#24687;&#25152;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#20869;&#26368;&#22823;&#21270;&#35299;&#30721;&#20449;&#24687;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;CMDI&#12290;CMDI&#21019;&#26032;&#22320;&#23558;&#20108;&#32500;&#32467;&#26500;&#20449;&#24687;&#29702;&#35770;&#32435;&#20837;&#21040;&#32858;&#31867;&#36807;&#31243;&#20013;&#65292;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#22270;&#32467;&#26500;&#25552;&#21462;&#21644;&#22270;&#39030;&#28857;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13846v1 Announce Type: cross  Abstract: The clustering method based on graph models has garnered increased attention for its widespread applicability across various knowledge domains. Its adaptability to integrate seamlessly with other relevant applications endows the graph model-based clustering analysis with the ability to robustly extract "natural associations" or "graph structures" within datasets, facilitating the modelling of relationships between data points. Despite its efficacy, the current clustering method utilizing the graph-based model overlooks the uncertainty associated with random walk access between nodes and the embedded structural information in the data. To address this gap, we present a novel Clustering method for Maximizing Decoding Information within graph-based models, named CMDI. CMDI innovatively incorporates two-dimensional structural information theory into the clustering process, consisting of two phases: graph structure extraction and graph vert
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21644;&#24179;&#26041;&#27861;&#30340;&#31169;&#26377;&#22270;&#20272;&#35745;&#31639;&#27861;&#39318;&#27425;&#23454;&#29616;&#20102;&#23398;&#20064;&#38543;&#26426;&#22359;&#27169;&#22411;&#21644;&#22270;&#20272;&#35745;&#30340;&#32431;&#33410;&#28857;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#20855;&#26377;&#22810;&#39033;&#24335;&#36816;&#34892;&#26102;&#38388;&#65292;&#19982;&#20043;&#21069;&#26368;&#20339;&#30340;&#20449;&#24687;&#35770;&#33410;&#28857;&#31169;&#26377;&#26426;&#21046;&#20855;&#26377;&#30456;&#21305;&#37197;&#30340;&#32479;&#35745;&#25928;&#29992;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.12213</link><description>&lt;p&gt;
&#36890;&#36807;&#20108;&#27425;&#21644;&#26041;&#27861;&#36827;&#34892;&#31169;&#26377;&#22270;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Private graphon estimation via sum-of-squares
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12213
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21644;&#24179;&#26041;&#27861;&#30340;&#31169;&#26377;&#22270;&#20272;&#35745;&#31639;&#27861;&#39318;&#27425;&#23454;&#29616;&#20102;&#23398;&#20064;&#38543;&#26426;&#22359;&#27169;&#22411;&#21644;&#22270;&#20272;&#35745;&#30340;&#32431;&#33410;&#28857;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#20855;&#26377;&#22810;&#39033;&#24335;&#36816;&#34892;&#26102;&#38388;&#65292;&#19982;&#20043;&#21069;&#26368;&#20339;&#30340;&#20449;&#24687;&#35770;&#33410;&#28857;&#31169;&#26377;&#26426;&#21046;&#20855;&#26377;&#30456;&#21305;&#37197;&#30340;&#32479;&#35745;&#25928;&#29992;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#29992;&#20110;&#23398;&#20064;&#38543;&#26426;&#22359;&#27169;&#22411;&#21644;&#22270;&#20272;&#35745;&#30340;&#31532;&#19968;&#20010;&#32431;&#33410;&#28857;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#23545;&#20110;&#20219;&#24847;&#24120;&#25968;&#20010;&#22359;&#65292;&#20855;&#26377;&#22810;&#39033;&#24335;&#36816;&#34892;&#26102;&#38388;&#12290;&#32479;&#35745;&#25928;&#29992;&#20445;&#35777;&#19982;&#20808;&#21069;&#26368;&#20339;&#30340;&#20449;&#24687;&#35770;&#65288;&#25351;&#25968;&#26102;&#38388;&#65289;&#33410;&#28857;&#31169;&#26377;&#26426;&#21046;&#30456;&#21305;&#37197;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;&#19968;&#20010;&#22522;&#20110;&#25351;&#25968;&#26426;&#21046;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#23450;&#20041;&#20026;&#20381;&#36182;&#20110;&#22359;&#25968;&#37327;&#30340;&#20108;&#27425;&#21644;&#26494;&#24347;&#12290;&#25105;&#20204;&#32467;&#26524;&#30340;&#20851;&#38190;&#35201;&#32032;&#26159;&#65306;(1) &#22312;&#24418;&#24335;&#19978;&#23450;&#20041;&#20026;&#20108;&#27425;&#20248;&#21270;&#22312;&#21452;&#37325;&#38543;&#26426;&#30697;&#38453;&#30340;&#22810;&#32990;&#20307;&#19978;&#30340;&#36317;&#31163;&#30340;&#29305;&#24449;&#21270;&#22359;&#22270;&#23450;&#20041;&#65292;(2) &#19968;&#33324;&#30340;&#22810;&#39033;&#24335;&#20248;&#21270;&#30340;&#21644;&#24179;&#26041;&#27861;&#22312;&#20219;&#24847;&#22810;&#32990;&#20307;&#19978;&#30340;&#25910;&#25947;&#32467;&#26524;&#65292;&#20197;&#21450;(3) &#25191;&#34892;&#21033;&#26222;&#24076;&#33576;&#25193;&#23637;&#30340;&#24471;&#20998;&#20989;&#25968;&#20316;&#20026;&#20108;&#27425;&#21644;&#31639;&#27861;&#33539;&#20363;&#30340;&#19968;&#33324;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12213v1 Announce Type: cross  Abstract: We develop the first pure node-differentially-private algorithms for learning stochastic block models and for graphon estimation with polynomial running time for any constant number of blocks. The statistical utility guarantees match those of the previous best information-theoretic (exponential-time) node-private mechanisms for these problems. The algorithm is based on an exponential mechanism for a score function defined in terms of a sum-of-squares relaxation whose level depends on the number of blocks. The key ingredients of our results are (1) a characterization of the distance between the block graphons in terms of a quadratic optimization over the polytope of doubly stochastic matrices, (2) a general sum-of-squares convergence result for polynomial optimization over arbitrary polytopes, and (3) a general approach to perform Lipschitz extensions of score functions as part of the sum-of-squares algorithmic paradigm.
&lt;/p&gt;</description></item><item><title>2023&#24180;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#21487;&#35299;&#37322;&#21644;&#21487;&#20449;&#36182;&#30340;&#26426;&#22120;&#23398;&#20064;&#21487;&#35270;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#20026;&#21508;&#31181;&#39046;&#22495;&#25552;&#20379;&#20102;&#36235;&#21183;&#12289;&#35265;&#35299;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.12005</link><description>&lt;p&gt;
2023&#24180;&#26426;&#22120;&#23398;&#20064;&#20013;&#20449;&#20219;&#21487;&#35270;&#21270;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Visualization for Trust in Machine Learning Revisited: The State of the Field in 2023
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12005
&lt;/p&gt;
&lt;p&gt;
2023&#24180;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#21487;&#35299;&#37322;&#21644;&#21487;&#20449;&#36182;&#30340;&#26426;&#22120;&#23398;&#20064;&#21487;&#35270;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#20026;&#21508;&#31181;&#39046;&#22495;&#25552;&#20379;&#20102;&#36235;&#21183;&#12289;&#35265;&#35299;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#21644;&#21487;&#20449;&#36182;&#30340;&#26426;&#22120;&#23398;&#20064;&#21487;&#35270;&#21270;&#20173;&#28982;&#26159;&#20449;&#24687;&#21487;&#35270;&#21270;&#21644;&#35270;&#35273;&#20998;&#26512;&#39046;&#22495;&#20013;&#26368;&#37325;&#35201;&#21644;&#28145;&#20837;&#30740;&#31350;&#30340;&#39046;&#22495;&#20043;&#19968;&#65292;&#28041;&#21450;&#21307;&#23398;&#12289;&#37329;&#34701;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#31561;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#12290;&#22312;&#25105;&#20204;2020&#24180;&#30340;&#26368;&#26032;&#25253;&#21578;&#20013;&#65292;&#21253;&#25324;&#20102;200&#31181;&#25216;&#26415;&#65292;&#25105;&#20204;&#22362;&#25345;&#25910;&#38598;&#21516;&#34892;&#35780;&#23457;&#30340;&#25991;&#31456;&#65292;&#25551;&#36848;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#26681;&#25454;&#20808;&#21069;&#24314;&#31435;&#30340;&#21253;&#21547;119&#20010;&#31867;&#21035;&#30340;&#20998;&#31867;&#27169;&#24335;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#22312;&#22312;&#32447;&#35843;&#26597;&#27983;&#35272;&#22120;&#20013;&#25552;&#20379;&#20102;542&#31181;&#25216;&#26415;&#30340;&#32467;&#26524;&#38598;&#12290;&#22312;&#26412;&#35843;&#26597;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25130;&#33267;2023&#24180;&#31179;&#23395;&#20851;&#20110;&#36825;&#19968;&#25968;&#25454;&#38598;&#30340;&#26032;&#20998;&#26512;&#32467;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;&#21487;&#35270;&#21270;&#30340;&#36235;&#21183;&#12289;&#35265;&#35299;&#21644;&#20843;&#20010;&#24320;&#25918;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#23454;&#20102;&#21487;&#35270;&#21270;&#25216;&#26415;&#22312;&#22686;&#21152;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20449;&#20219;&#26041;&#38754;&#21576;&#24555;&#36895;&#22686;&#38271;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12005v1 Announce Type: cross  Abstract: Visualization for explainable and trustworthy machine learning remains one of the most important and heavily researched fields within information visualization and visual analytics with various application domains, such as medicine, finance, and bioinformatics. After our 2020 state-of-the-art report comprising 200 techniques, we have persistently collected peer-reviewed articles describing visualization techniques, categorized them based on the previously established categorization schema consisting of 119 categories, and provided the resulting collection of 542 techniques in an online survey browser. In this survey article, we present the updated findings of new analyses of this dataset as of fall 2023 and discuss trends, insights, and eight open challenges for using visualizations in machine learning. Our results corroborate the rapidly growing trend of visualization techniques for increasing trust in machine learning models in the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#30340;&#25511;&#21046;&#23646;&#24615;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04923</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25511;&#21046;&#20026;&#22522;&#30784;&#30340;&#22270;&#23884;&#20837;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Control-based Graph Embeddings with Data Augmentation for Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#30340;&#25511;&#21046;&#23646;&#24615;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#21160;&#24577;&#32593;&#32476;&#22312;&#22270;&#19978;&#30340;&#25511;&#21046;&#23646;&#24615;&#26469;&#36827;&#34892;&#26080;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#24191;&#27867;&#23384;&#22312;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#12290;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#26159;&#20174;&#36755;&#20837;&#22270;&#21019;&#24314;&#8220;&#22686;&#24378;&#8221;&#22270;&#12290;&#34429;&#28982;&#19982;&#21407;&#22987;&#22270;&#19981;&#21516;&#65292;&#36825;&#20123;&#22686;&#24378;&#22270;&#20445;&#30041;&#20102;&#21407;&#22987;&#22270;&#30340;&#32467;&#26500;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#30340;&#25511;&#21046;&#23646;&#24615;&#29983;&#25104;&#36825;&#20123;&#22686;&#24378;&#22270;&#12290;&#26680;&#24515;&#27010;&#24565;&#22260;&#32469;&#30528;&#23545;&#21407;&#22987;&#22270;&#36827;&#34892;&#25200;&#21160;&#20197;&#21019;&#24314;&#19968;&#20010;&#26032;&#22270;&#65292;&#21516;&#26102;&#20445;&#30041;&#29305;&#23450;&#20110;&#32593;&#32476;&#21644;&#22270;&#30340;&#21487;&#25511;&#29305;&#24615;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#22914;&#20309;&#22686;&#24378;&#20102;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04923v1 Announce Type: new  Abstract: In this paper, we study the problem of unsupervised graph representation learning by harnessing the control properties of dynamical networks defined on graphs. Our approach introduces a novel framework for contrastive learning, a widely prevalent technique for unsupervised representation learning. A crucial step in contrastive learning is the creation of 'augmented' graphs from the input graphs. Though different from the original graphs, these augmented graphs retain the original graph's structural characteristics. Here, we propose a unique method for generating these augmented graphs by leveraging the control properties of networks. The core concept revolves around perturbing the original graph to create a new one while preserving the controllability properties specific to networks and graphs. Compared to the existing methods, we demonstrate that this innovative approach enhances the effectiveness of contrastive learning frameworks, lea
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;HPO&#26041;&#27861;&#65292;&#36991;&#20813;&#38271;&#26102;&#38388;&#31561;&#24453;&#23454;&#29616;&#24555;&#36895;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.01888</link><description>&lt;p&gt;
&#38646;&#25104;&#26412;&#22522;&#20934;&#19978;&#24322;&#27493;&#22810;&#20445;&#30495;&#24230;&#20248;&#21270;&#30340;&#24555;&#36895;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Fast Benchmarking of Asynchronous Multi-Fidelity Optimization on Zero-Cost Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01888
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;HPO&#26041;&#27861;&#65292;&#36991;&#20813;&#38271;&#26102;&#38388;&#31561;&#24453;&#23454;&#29616;&#24555;&#36895;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#65292;&#20294;&#20854;&#32467;&#26524;&#24448;&#24448;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#30340;&#31934;&#24515;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#30340;&#32791;&#26102;&#24615;&#20351;&#24471;&#36229;&#21442;&#25968;&#20248;&#21270;(HPO)&#26159;&#19968;&#39033;&#26114;&#36149;&#30340;&#24037;&#20316;&#65292;&#25302;&#24930;&#20102;&#39640;&#25928;HPO&#24037;&#20855;&#30340;&#24320;&#21457;&#12290;&#26412;&#24037;&#20316;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20419;&#36827;&#38646;&#25104;&#26412;&#22522;&#20934;&#19979;&#39640;&#25928;&#30340;&#24182;&#34892;HPO&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26681;&#25454;&#23384;&#20648;&#22312;&#25991;&#20214;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#35745;&#31639;&#31934;&#30830;&#30340;&#36820;&#22238;&#39034;&#24207;&#65292;&#28040;&#38500;&#20102;&#38271;&#26102;&#38388;&#30340;&#31561;&#24453;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;HPO&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01888v1 Announce Type: new  Abstract: While deep learning has celebrated many successes, its results often hinge on the meticulous selection of hyperparameters (HPs). However, the time-consuming nature of deep learning training makes HP optimization (HPO) a costly endeavor, slowing down the development of efficient HPO tools. While zero-cost benchmarks, which provide performance and runtime without actual training, offer a solution for non-parallel setups, they fall short in parallel setups as each worker must communicate its queried runtime to return its evaluation in the exact order. This work addresses this challenge by introducing a user-friendly Python package that facilitates efficient parallel HPO with zero-cost benchmarks. Our approach calculates the exact return order based on the information stored in file system, eliminating the need for long waiting times and enabling much faster HPO evaluations. We first verify the correctness of our approach through extensive t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#65292;&#25913;&#36827;&#20102;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;&#26041;&#27861;&#65292;&#20943;&#36731;&#25945;&#24072;&#32321;&#37325;&#30340;&#24037;&#20316;&#37327;&#65292;&#38450;&#27490;&#29983;&#25104;&#26080;&#25928;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00199</link><description>&lt;p&gt;
&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#25913;&#36827;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving Socratic Question Generation using Data Augmentation and Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00199
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#65292;&#25913;&#36827;&#20102;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;&#26041;&#27861;&#65292;&#20943;&#36731;&#25945;&#24072;&#32321;&#37325;&#30340;&#24037;&#20316;&#37327;&#65292;&#38450;&#27490;&#29983;&#25104;&#26080;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#26159;&#19968;&#31181;&#24341;&#23548;&#23398;&#29983;&#29420;&#31435;&#35299;&#20915;&#38382;&#39064;&#32780;&#19981;&#30452;&#25509;&#25581;&#31034;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#25913;&#36827;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#24040;&#22823;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#33487;&#26684;&#25289;&#24213;&#38382;&#39064;&#65292;&#20197;&#20943;&#36731;&#25945;&#24072;&#30340;&#32321;&#37325;&#24037;&#20316;&#37327;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#28041;&#21450;&#25552;&#31034;&#36825;&#20123;&#24040;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26377;&#26102;&#20250;&#20135;&#29983;&#26080;&#25928;&#30340;&#36755;&#20986;&#65292;&#20363;&#22914;&#30452;&#25509;&#25581;&#31034;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#25110;&#25552;&#20379;&#26080;&#20851;&#25110;&#36807;&#26089;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#39318;&#20808;&#25552;&#20986;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#20016;&#23500;&#29616;&#26377;&#30340;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#25968;&#25454;&#38598;&#65307;&#20854;&#27425;&#65292;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#24320;&#28304;&#24040;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#20363;&#22914;LLama 2&#65292;&#20197;&#26356;&#20542;&#21521;&#20110;&#22320;&#38754;&#30495;&#20540;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00199v1 Announce Type: new  Abstract: The Socratic method is a way of guiding students toward solving a problem independently without directly revealing the solution to the problem. Although this method has been shown to significantly improve student learning outcomes, it remains a complex labor-intensive task for instructors. Large language models (LLMs) can be used to augment human effort by automatically generating Socratic questions for students. However, existing methods that involve prompting these LLMs sometimes produce invalid outputs, e.g., those that directly reveal the solution to the problem or provide irrelevant or premature questions. To alleviate this problem, inspired by reinforcement learning with AI feedback (RLAIF), we first propose a data augmentation method to enrich existing Socratic questioning datasets with questions that are invalid in specific ways. Next, we propose a method to optimize open-source LLMs such as LLama 2 to prefer ground-truth questio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#38024;&#23545;&#24046;&#20998;&#38544;&#31169;&#22312;&#25345;&#32493;&#35266;&#27979;&#21644;&#22312;&#32447;&#38408;&#20540;&#26597;&#35810;&#24773;&#24418;&#19979;&#20851;&#20110;&#26102;&#38388;&#27493;&#38271;&#21644;&#20107;&#20214;&#25968;&#37327;&#30340;&#26032;&#19979;&#30028;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.00028</link><description>&lt;p&gt;
&#22312;&#25345;&#32493;&#35266;&#27979;&#21644;&#22312;&#32447;&#38408;&#20540;&#26597;&#35810;&#19979;&#30340;&#24046;&#20998;&#38544;&#31169;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Lower Bounds for Differential Privacy Under Continual Observation and Online Threshold Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#38024;&#23545;&#24046;&#20998;&#38544;&#31169;&#22312;&#25345;&#32493;&#35266;&#27979;&#21644;&#22312;&#32447;&#38408;&#20540;&#26597;&#35810;&#24773;&#24418;&#19979;&#20851;&#20110;&#26102;&#38388;&#27493;&#38271;&#21644;&#20107;&#20214;&#25968;&#37327;&#30340;&#26032;&#19979;&#30028;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#38544;&#30610;&#27599;&#19968;&#20107;&#20214;&#30340;&#23384;&#22312;&#30340;&#21516;&#26102;&#65292;&#36319;&#36394;&#38543;&#26102;&#38388;&#21457;&#29983;&#30340;&#20107;&#20214;&#25968;&#37327;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#38271;t&#8712;[T]&#20013;&#65292;&#25105;&#20204;&#22312;&#32447;&#23398;&#20064;&#21040;&#916;t&#8805;0&#20010;&#26032;&#20107;&#20214;&#24050;&#21457;&#29983;&#65292;&#24182;&#19988;&#24517;&#39035;&#22238;&#24212;&#19968;&#20010;&#20272;&#35745;&#20540;nt&#8776;&#8721;_{j=1}^t &#916;j&#12290;&#38544;&#31169;&#35201;&#27714;&#26159;&#65292;&#25152;&#26377;&#36755;&#20986;&#22312;&#25152;&#26377;&#26102;&#38388;&#27493;&#38271;&#19978;&#19968;&#36215;&#28385;&#36275;&#20107;&#20214;&#32423;&#21035;&#30340;&#24046;&#20998;&#38544;&#31169;&#12290;&#20027;&#35201;&#38382;&#39064;&#26159;&#25105;&#20204;&#30340;&#35823;&#24046;&#22914;&#20309;&#20381;&#36182;&#20110;&#24635;&#26102;&#38388;&#27493;&#38271;T&#21644;&#24635;&#20107;&#20214;&#25968;&#37327;n&#12290;Dwork&#31561;&#20154;&#65288;2015&#65289;&#23637;&#31034;&#20102;O(log(T)+log^2(n))&#30340;&#19978;&#30028;&#65292;&#32780;Henzinger&#31561;&#20154;&#65288;2023&#65289;&#23637;&#31034;&#20102;&#937;(min{log n, log T})&#30340;&#19979;&#30028;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#26032;&#30340;&#19979;&#30028;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00028v1 Announce Type: cross  Abstract: One of the most basic problems for studying the "price of privacy over time" is the so called private counter problem, introduced by Dwork et al. (2010) and Chan et al. (2010). In this problem, we aim to track the number of events that occur over time, while hiding the existence of every single event. More specifically, in every time step $t\in[T]$ we learn (in an online fashion) that $\Delta_t\geq 0$ new events have occurred, and must respond with an estimate $n_t\approx\sum_{j=1}^t \Delta_j$. The privacy requirement is that all of the outputs together, across all time steps, satisfy event level differential privacy. The main question here is how our error needs to depend on the total number of time steps $T$ and the total number of events $n$. Dwork et al. (2015) showed an upper bound of $O\left(\log(T)+\log^2(n)\right)$, and Henzinger et al. (2023) showed a lower bound of $\Omega\left(\min\{\log n, \log T\}\right)$. We show a new lo
&lt;/p&gt;</description></item><item><title>Beacon&#26159;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#24211;&#65292;&#29992;&#20110;&#27969;&#25511;&#21046;&#65292;&#21253;&#21547;7&#20010;&#36731;&#37327;&#32423;&#30340;1D&#21644;2D&#38382;&#39064;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23545;&#25968;&#20540;&#27969;&#20307;&#21160;&#21147;&#23398;&#29615;&#22659;&#30340;&#36866;&#24212;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17402</link><description>&lt;p&gt;
Beacon&#65292;&#19968;&#20010;&#29992;&#20110;&#27969;&#25511;&#21046;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#24211;
&lt;/p&gt;
&lt;p&gt;
Beacon, a lightweight deep reinforcement learning benchmark library for flow control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17402
&lt;/p&gt;
&lt;p&gt;
Beacon&#26159;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#24211;&#65292;&#29992;&#20110;&#27969;&#25511;&#21046;&#65292;&#21253;&#21547;7&#20010;&#36731;&#37327;&#32423;&#30340;1D&#21644;2D&#38382;&#39064;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23545;&#25968;&#20540;&#27969;&#20307;&#21160;&#21147;&#23398;&#29615;&#22659;&#30340;&#36866;&#24212;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#26085;&#30410;&#22686;&#22810;&#30340;&#24212;&#29992;&#23548;&#33268;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#23558;&#29616;&#26377;&#31639;&#27861;&#19982;&#25968;&#20540;&#27969;&#20307;&#21160;&#21147;&#23398;&#29615;&#22659;&#30340;&#25511;&#21046;&#32806;&#21512;&#21644;&#35843;&#25972;&#12290;&#23613;&#31649;&#36825;&#20010;&#39046;&#22495;&#20173;&#22788;&#20110;&#33804;&#33469;&#38454;&#27573;&#65292;&#20294;&#22312;&#30701;&#26102;&#38388;&#20869;&#21462;&#24471;&#20102;&#22810;&#27425;&#25104;&#21151;&#65292;&#20854;&#24555;&#36895;&#21457;&#23637;&#36895;&#24230;&#32943;&#23450;&#37096;&#20998;&#24402;&#21151;&#20110;&#25512;&#21160;&#31038;&#21306;&#25193;&#22823;&#30340;&#24320;&#28304;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#20173;&#32570;&#20047;&#19968;&#20010;&#20849;&#21516;&#22522;&#30784;&#65292;&#26469;&#30830;&#20445;&#32467;&#26524;&#30340;&#21487;&#37325;&#29616;&#24615;&#65292;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#19987;&#38376;&#22522;&#20934;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Beacon&#65292;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#24211;&#65292;&#30001;&#19971;&#20010;&#36731;&#37327;&#32423;1D&#21644;2D&#27969;&#25511;&#21046;&#38382;&#39064;&#32452;&#25104;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#24449;&#12289;&#34892;&#21160;&#21644;&#35266;&#23519;&#31354;&#38388;&#29305;&#24449;&#20197;&#21450;CPU&#38656;&#27714;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25551;&#36848;&#20102;&#19971;&#20010;&#32771;&#34385;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#21442;&#32771;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17402v1 Announce Type: cross  Abstract: Recently, the increasing use of deep reinforcement learning for flow control problems has led to a new area of research, focused on the coupling and the adaptation of the existing algorithms to the control of numerical fluid dynamics environments. Although still in its infancy, the field has seen multiple successes in a short time span, and its fast development pace can certainly be partly imparted to the open-source effort that drives the expansion of the community. Yet, this emerging domain still misses a common ground to (i) ensure the reproducibility of the results, and (ii) offer a proper ad-hoc benchmarking basis. To this end, we propose Beacon, an open-source benchmark library composed of seven lightweight 1D and 2D flow control problems with various characteristics, action and observation space characteristics, and CPU requirements. In this contribution, the seven considered problems are described, and reference control solutio
&lt;/p&gt;</description></item><item><title>Sora&#26159;&#19968;&#31181;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#23637;&#31034;&#20986;&#22312;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#25361;&#25112;&#65292;&#26410;&#26469;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.17177</link><description>&lt;p&gt;
Sora: &#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#32972;&#26223;&#12289;&#25216;&#26415;&#12289;&#23616;&#38480;&#24615;&#21644;&#26426;&#36935;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17177
&lt;/p&gt;
&lt;p&gt;
Sora&#26159;&#19968;&#31181;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#23637;&#31034;&#20986;&#22312;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#25361;&#25112;&#65292;&#26410;&#26469;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sora&#26159;&#30001;OpenAI&#20110;2024&#24180;2&#26376;&#21457;&#24067;&#30340;&#19968;&#31181;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25351;&#20196;&#29983;&#25104;&#36924;&#30495;&#25110;&#24819;&#35937;&#30340;&#22330;&#26223;&#35270;&#39057;&#65292;&#24182;&#22312;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#26412;&#25991;&#22522;&#20110;&#20844;&#24320;&#30340;&#25216;&#26415;&#25253;&#21578;&#21644;&#36870;&#21521;&#24037;&#31243;&#65292;&#23545;&#36825;&#20010;&#27169;&#22411;&#30340;&#32972;&#26223;&#12289;&#30456;&#20851;&#25216;&#26415;&#12289;&#24212;&#29992;&#12289;&#23578;&#23384;&#30340;&#25361;&#25112;&#20197;&#21450;&#25991;&#26412;&#21040;&#35270;&#39057;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#26410;&#26469;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;&#39318;&#20808;&#25105;&#20204;&#36861;&#28335;&#20102;Sora&#30340;&#21457;&#23637;&#21382;&#31243;&#65292;&#24182;&#35843;&#26597;&#20102;&#29992;&#20110;&#26500;&#24314;&#36825;&#20010;"&#19990;&#30028;&#27169;&#25311;&#22120;"&#30340;&#22522;&#30784;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;Sora&#22312;&#20174;&#30005;&#24433;&#21046;&#20316;&#21644;&#25945;&#32946;&#21040;&#33829;&#38144;&#31561;&#22810;&#20010;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#21644;&#28508;&#22312;&#24433;&#21709;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#65292;&#20197;&#20415;&#24191;&#27867;&#37096;&#32626;Sora&#65292;&#22914;&#30830;&#20445;&#23433;&#20840;&#21644;&#26080;&#20559;&#35265;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;Sora&#20197;&#21450;&#35270;&#39057;&#29983;&#25104;&#25216;&#26415;&#26410;&#26469;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17177v1 Announce Type: cross  Abstract: Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this "world simulator". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video gene
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;HSTU&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#22522;&#25968;&#12289;&#38750;&#24179;&#31283;&#27969;&#25512;&#33616;&#25968;&#25454;&#65292;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#39640;&#36798;65.8&#65285;&#30340;NDCG&#65292;&#24182;&#19988;&#27604;&#22522;&#20110;FlashAttention2&#30340;Transformer&#22312;8192&#38271;&#24230;&#24207;&#21015;&#19978;&#24555;5.3&#20493;&#21040;15.2&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.17152</link><description>&lt;p&gt;
&#34892;&#21160;&#32988;&#36807;&#35328;&#36766;&#65306;&#29992;&#20110;&#29983;&#25104;&#25512;&#33616;&#30340;&#21315;&#20159;&#21442;&#25968;&#39034;&#24207;&#36716;&#23548;&#22120;
&lt;/p&gt;
&lt;p&gt;
Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17152
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;HSTU&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#22522;&#25968;&#12289;&#38750;&#24179;&#31283;&#27969;&#25512;&#33616;&#25968;&#25454;&#65292;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#39640;&#36798;65.8&#65285;&#30340;NDCG&#65292;&#24182;&#19988;&#27604;&#22522;&#20110;FlashAttention2&#30340;Transformer&#22312;8192&#38271;&#24230;&#24207;&#21015;&#19978;&#24555;5.3&#20493;&#21040;15.2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#30340;&#29305;&#28857;&#26159;&#20381;&#36182;&#20110;&#39640;&#22522;&#25968;&#12289;&#24322;&#26500;&#29305;&#24449;&#65292;&#24182;&#19988;&#38656;&#35201;&#27599;&#22825;&#22788;&#29702;&#25968;&#21313;&#20159;&#29992;&#25143;&#34892;&#20026;&#12290;&#23613;&#31649;&#22312;&#25104;&#21315;&#19978;&#19975;&#20010;&#29305;&#24449;&#19978;&#35757;&#32451;&#20102;&#22823;&#37327;&#25968;&#25454;&#65292;&#20294;&#22823;&#22810;&#25968;&#34892;&#19994;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;(DLRMs)&#22312;&#35745;&#31639;&#26041;&#38754;&#26080;&#27861;&#25193;&#23637;&#12290;&#21463;&#21040;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#30340;Transformer&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22522;&#26412;&#35774;&#35745;&#36873;&#25321;&#12290;&#25105;&#20204;&#23558;&#25512;&#33616;&#38382;&#39064;&#37325;&#26032;&#26500;&#24314;&#20026;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#20013;&#30340;&#39034;&#24207;&#36716;&#23548;&#20219;&#21153;&#65288;&#8220;&#29983;&#25104;&#25512;&#33616;&#32773;&#8221;&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#22522;&#25968;&#12289;&#38750;&#24179;&#31283;&#27969;&#25512;&#33616;&#25968;&#25454;&#35774;&#35745;&#30340;&#26032;&#26550;&#26500;HSTU&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17152v1 Announce Type: new  Abstract: Large-scale recommendation systems are characterized by their reliance on high cardinality, heterogeneous features and the need to handle tens of billions of user actions on a daily basis. Despite being trained on huge volume of data with thousands of features, most Deep Learning Recommendation Models (DLRMs) in industry fail to scale with compute.   Inspired by success achieved by Transformers in language and vision domains, we revisit fundamental design choices in recommendation systems. We reformulate recommendation problems as sequential transduction tasks within a generative modeling framework (``Generative Recommenders''), and propose a new architecture, HSTU, designed for high cardinality, non-stationary streaming recommendation data.   HSTU outperforms baselines over synthetic and public datasets by up to 65.8\% in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on 8192 length sequences. HSTU-based Gener
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#25552;&#20986;&#20102;Evolution Graph Fourier Transform(EFT)&#65292;&#39318;&#27425;&#23454;&#29616;&#22312;&#26102;&#38388;&#22270;&#19978;&#25429;&#25417;&#28436;&#21270;&#34920;&#31034;&#30340;&#21487;&#36870;&#35889;&#21464;&#25442;&#65292;&#36890;&#36807;&#20248;&#21270;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#30340;&#25289;&#26222;&#25289;&#26031;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#20266;&#35889;&#26494;&#24347;&#26469;&#39640;&#25928;&#35745;&#31639;&#36716;&#25442;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.16078</link><description>&lt;p&gt;
&#36229;&#36234;&#26102;&#31354;&#34920;&#31034;&#65306;&#28436;&#21270;Fourier&#21464;&#25442;&#29992;&#20110;&#26102;&#38388;&#22270;
&lt;/p&gt;
&lt;p&gt;
Beyond Spatio-Temporal Representations: Evolving Fourier Transform for Temporal Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16078
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#25552;&#20986;&#20102;Evolution Graph Fourier Transform(EFT)&#65292;&#39318;&#27425;&#23454;&#29616;&#22312;&#26102;&#38388;&#22270;&#19978;&#25429;&#25417;&#28436;&#21270;&#34920;&#31034;&#30340;&#21487;&#36870;&#35889;&#21464;&#25442;&#65292;&#36890;&#36807;&#20248;&#21270;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#30340;&#25289;&#26222;&#25289;&#26031;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#20266;&#35889;&#26494;&#24347;&#26469;&#39640;&#25928;&#35745;&#31639;&#36716;&#25442;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Evolving Graph Fourier Transform (EFT)&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25429;&#25417;&#26102;&#38388;&#22270;&#28436;&#21464;&#34920;&#31034;&#30340;&#21487;&#36870;&#35889;&#21464;&#25442;&#12290;&#25105;&#20204;&#36890;&#36807;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#28436;&#21464;&#22270;&#35889;&#30340;&#19981;&#36275;&#26469;&#28608;&#21457;&#25105;&#20204;&#30340;&#24037;&#20316;&#65292;&#30001;&#20110;&#26102;&#38388;&#22240;&#32032;&#20197;&#21450;&#22270;&#39030;&#28857;&#22495;&#30340;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#35270;&#20026;&#23545;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#30340;&#25289;&#26222;&#25289;&#26031;&#36827;&#34892;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20266;&#35889;&#26494;&#24347;&#65292;&#20351;&#21464;&#25442;&#36807;&#31243;&#20998;&#35299;&#65292;&#20174;&#32780;&#20351;&#20854;&#39640;&#24230;&#35745;&#31639;&#26377;&#25928;&#12290;EFT&#26041;&#27861;&#29087;&#32451;&#22320;&#25429;&#25417;&#20102;&#28436;&#21464;&#22270;&#30340;&#32467;&#26500;&#21644;&#20301;&#32622;&#23646;&#24615;&#65292;&#20351;&#20854;&#21487;&#20197;&#26377;&#25928;&#29992;&#20110;&#28436;&#21464;&#22270;&#19978;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#20316;&#20026;&#21442;&#32771;&#23454;&#26045;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#29992;EFT&#26469;&#25429;&#25417;&#28436;&#21464;&#22270;&#35889;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16078v1 Announce Type: new  Abstract: We present the Evolving Graph Fourier Transform (EFT), the first invertible spectral transform that captures evolving representations on temporal graphs. We motivate our work by the inadequacy of existing methods for capturing the evolving graph spectra, which are also computationally expensive due to the temporal aspect along with the graph vertex domain. We view the problem as an optimization over the Laplacian of the continuous time dynamic graph. Additionally, we propose pseudo-spectrum relaxations that decompose the transformation process, making it highly computationally efficient. The EFT method adeptly captures the evolving graph's structural and positional properties, making it effective for downstream tasks on evolving graphs. Hence, as a reference implementation, we develop a simple neural model induced with EFT for capturing evolving graph spectra. We empirically validate our theoretical findings on a number of large-scale an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#21487;&#23398;&#20064;&#26102;&#38388;&#23610;&#24230;&#21442;&#25968;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#39057;&#29575;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#32593;&#32476;&#65292;&#24182;&#30740;&#31350;&#20102;&#20004;&#31181;&#23545;&#25239;&#28151;&#21472;&#25928;&#24212;&#30340;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#36895;&#24230;&#24555;33%&#12290;</title><link>https://arxiv.org/abs/2402.15584</link><description>&lt;p&gt;
&#20107;&#20214;&#30456;&#26426;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
State Space Models for Event Cameras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15584
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#21487;&#23398;&#20064;&#26102;&#38388;&#23610;&#24230;&#21442;&#25968;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#39057;&#29575;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#32593;&#32476;&#65292;&#24182;&#30740;&#31350;&#20102;&#20004;&#31181;&#23545;&#25239;&#28151;&#21472;&#25928;&#24212;&#30340;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#36895;&#24230;&#24555;33%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#22788;&#29702;&#20107;&#20214;&#30456;&#26426;&#25968;&#25454;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39318;&#20808;&#23558;&#19968;&#27573;&#26102;&#38388;&#20869;&#30340;&#20107;&#20214;&#36716;&#25442;&#20026;&#31264;&#23494;&#30340;&#32593;&#26684;&#29366;&#36755;&#20837;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#22312;&#37096;&#32626;&#25512;&#26029;&#39057;&#29575;&#39640;&#20110;&#23427;&#20204;&#35757;&#32451;&#26102;&#30340;&#39057;&#29575;&#65288;&#21363;&#26102;&#38388;&#31383;&#21475;&#36739;&#23567;&#65289;&#26102;&#65292;&#23427;&#20204;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#21487;&#23398;&#20064;&#26102;&#38388;&#23610;&#24230;&#21442;&#25968;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#36825;&#31181;&#35774;&#35745;&#36866;&#24212;&#19981;&#21516;&#39057;&#29575;&#32780;&#26080;&#38656;&#22312;&#19981;&#21516;&#39057;&#29575;&#19979;&#37325;&#26032;&#35757;&#32451;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#23545;&#25239;&#28151;&#21472;&#25928;&#24212;&#30340;&#31574;&#30053;&#65292;&#24403;&#22312;&#26356;&#39640;&#39057;&#29575;&#19979;&#37096;&#32626;&#27169;&#22411;&#26102;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#23545;&#25239;&#22522;&#20110;RNN&#21644;Transformer&#26550;&#26500;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#21253;&#25324;Gen1&#21644;1 Mpx&#20107;&#20214;&#30456;&#26426;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;SSM&#30340;&#27169;&#22411;&#35757;&#32451;&#36895;&#24230;&#24555;33%&#65292;&#21516;&#26102;&#20063;&#34920;&#29616;&#20986;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15584v1 Announce Type: cross  Abstract: Today, state-of-the-art deep neural networks that process event-camera data first convert a temporal window of events into dense, grid-like input representations. As such, they exhibit poor generalizability when deployed at higher inference frequencies (i.e., smaller temporal windows) than the ones they were trained on. We address this challenge by introducing state-space models (SSMs) with learnable timescale parameters to event-based vision. This design adapts to varying frequencies without the need to retrain the network at different frequencies. Additionally, we investigate two strategies to counteract aliasing effects when deploying the model at higher frequencies. We comprehensively evaluate our approach against existing methods based on RNN and Transformer architectures across various benchmarks, including Gen1 and 1 Mpx event camera datasets. Our results demonstrate that SSM-based models train 33% faster and also exhibit minima
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;RLHF&#20013;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#26469;&#38477;&#20302;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#25152;&#38656;&#30340;&#22823;&#37327;&#20154;&#31867;&#20559;&#22909;&#27880;&#37322;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.15473</link><description>&lt;p&gt;
&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#22312;RLHF&#20013;&#39640;&#25928;&#24314;&#27169;&#22870;&#21169;&#65306;&#30005;&#23376;&#21830;&#21153;&#24847;&#35265;&#25688;&#35201;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A Case-Study in E-Commerce Opinion Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15473
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;RLHF&#20013;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#26469;&#38477;&#20302;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#25152;&#38656;&#30340;&#22823;&#37327;&#20154;&#31867;&#20559;&#22909;&#27880;&#37322;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#25104;&#20026;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26397;&#21521;&#20154;&#31867;&#20215;&#20540;/&#30446;&#26631;&#30340;&#20027;&#23548;&#31574;&#30053;&#12290;&#35813;&#31574;&#30053;&#30340;&#20851;&#38190;&#22312;&#20110;&#20351;&#29992;&#19968;&#20010;&#33021;&#22815;&#21453;&#26144;&#19982;&#20154;&#31867;&#30456;&#20851;&#30340;&#28508;&#22312;&#22870;&#21169;&#27169;&#22411;&#30340;&#22870;&#21169;&#27169;&#22411;&#65288;{$\varphi$}&#65289;&#12290;&#34429;&#28982;&#36825;&#19968;&#31574;&#30053;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#35757;&#32451;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#20154;&#31867;&#20559;&#22909;&#27880;&#37322;&#65288;&#36890;&#24120;&#25968;&#37327;&#32423;&#20026;&#25968;&#19975;&#65289;&#26469;&#35757;&#32451;{$\varphi$}&#12290;&#22914;&#26524;&#22870;&#21169;&#27169;&#22411;&#21487;&#20197;&#34987;&#26222;&#36941;&#20351;&#29992;&#65292;&#36825;&#31181;&#22823;&#35268;&#27169;&#20559;&#22909;&#27880;&#37322;&#26159;&#21487;&#20197;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#20215;&#20540;/&#30446;&#26631;&#26159;&#20027;&#35266;&#30340;&#65292;&#24182;&#19988;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#24615;&#36136;&#12290;&#36825;&#23545;&#20110;&#25910;&#38598;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#30340;&#22810;&#26679;&#21270;&#20559;&#22909;&#26500;&#25104;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#34701;&#20837;{$\varphi$}&#20013;&#65292;&#20174;&#32780;&#20943;&#23569;&#25152;&#38656;&#27880;&#37322;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#22312;&#30005;&#23376;&#21830;&#21153;&#24847;&#35265;&#25688;&#35201;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15473v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a dominating strategy in steering Language Models (LMs) towards human values/goals. The key to the strategy is employing a reward model ({$\varphi$}) which can reflect a latent reward model with humans. While this strategy has proven to be effective, the training methodology requires a lot of human preference annotation (usually of the order of tens of thousands) to train {$\varphi$}. Such large-scale preference annotations can be achievable if the reward model can be ubiquitously used. However, human values/goals are subjective and depend on the nature of the task. This poses a challenge in collecting diverse preferences for downstream applications. To address this, we propose a novel methodology to infuse domain knowledge into {$\varphi$}, which reduces the size of preference annotation required. We validate our approach in E-Commerce Opinion Summarization, with a significant
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#19987;&#27880;&#20110;&#20248;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#20307;&#21147;&#23398;&#20013;&#20027;&#21160;&#27969;&#25511;&#21046;&#20013;&#30340;&#24182;&#34892;&#35774;&#32622;&#65292;&#36890;&#36807;&#25286;&#35299;DRL&#26694;&#26550;&#12289;&#36827;&#34892;&#25193;&#23637;&#24615;&#22522;&#20934;&#27979;&#35797;&#12289;&#25552;&#20986;&#28151;&#21512;&#24182;&#34892;&#21270;&#37197;&#32622;&#24182;&#20248;&#21270;&#22810;&#29615;&#22659;DRL&#35757;&#32451;&#20013;&#30340;I/O&#25805;&#20316;&#65292;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;&#21270;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.11515</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#20307;&#21147;&#23398;&#20013;&#20027;&#21160;&#27969;&#25511;&#21046;&#20013;&#30340;&#26368;&#20339;&#24182;&#34892;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Optimal Parallelization Strategies for Active Flow Control in Deep Reinforcement Learning-Based Computational Fluid Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11515
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#19987;&#27880;&#20110;&#20248;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#20307;&#21147;&#23398;&#20013;&#20027;&#21160;&#27969;&#25511;&#21046;&#20013;&#30340;&#24182;&#34892;&#35774;&#32622;&#65292;&#36890;&#36807;&#25286;&#35299;DRL&#26694;&#26550;&#12289;&#36827;&#34892;&#25193;&#23637;&#24615;&#22522;&#20934;&#27979;&#35797;&#12289;&#25552;&#20986;&#28151;&#21512;&#24182;&#34892;&#21270;&#37197;&#32622;&#24182;&#20248;&#21270;&#22810;&#29615;&#22659;DRL&#35757;&#32451;&#20013;&#30340;I/O&#25805;&#20316;&#65292;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#22788;&#29702;&#39640;&#21160;&#24577;&#21644;&#38750;&#32447;&#24615;&#20027;&#21160;&#27969;&#25511;&#21046;&#65288;AFC&#65289;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19982;&#35757;&#32451;DRL&#27169;&#22411;&#30456;&#20851;&#30340;&#35745;&#31639;&#25104;&#26412;&#26500;&#25104;&#20102;&#37325;&#35201;&#30340;&#24615;&#33021;&#29942;&#39048;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#24182;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#26550;&#26500;&#19978;&#23454;&#29616;&#26377;&#25928;&#30340;&#25193;&#23637;&#65292;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#20248;&#21270;&#24182;&#34892;&#35774;&#32622;&#20013;&#30340;&#22522;&#20110;DRL&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#29992;&#20110;AFC&#38382;&#39064;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;DRL&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#25928;&#29575;&#29942;&#39048;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#25286;&#35299;&#25972;&#20307;&#26694;&#26550;&#65292;&#24182;&#20026;&#21508;&#20010;&#32452;&#20214;&#36827;&#34892;&#24191;&#27867;&#30340;&#21487;&#25193;&#23637;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#28151;&#21512;&#24182;&#34892;&#21270;&#37197;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;&#21270;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#22810;&#29615;&#22659;DRL&#35757;&#32451;&#20013;&#30340;&#36755;&#20837;/&#36755;&#20986;&#65288;I/O&#65289;&#25805;&#20316;&#65292;&#20197;&#35299;&#20915;&#19982;&#25968;&#25454;&#31227;&#21160;&#30456;&#20851;&#30340;&#20851;&#38190;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11515v1 Announce Type: new  Abstract: Deep Reinforcement Learning (DRL) has emerged as a promising approach for handling highly dynamic and nonlinear Active Flow Control (AFC) problems. However, the computational cost associated with training DRL models presents a significant performance bottleneck. To address this challenge and enable efficient scaling on high-performance computing architectures, this study focuses on optimizing DRL-based algorithms in parallel settings. We validate an existing state-of-the-art DRL framework used for AFC problems and discuss its efficiency bottlenecks. Subsequently, by deconstructing the overall framework and conducting extensive scalability benchmarks for individual components, we investigate various hybrid parallelization configurations and propose efficient parallelization strategies. Moreover, we refine input/output (I/O) operations in multi-environment DRL training to tackle critical overhead associated with data movement. Finally, we 
&lt;/p&gt;</description></item><item><title>DimVis&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#22120;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#32500;&#24230;&#32422;&#20943;&#20013;&#30340;&#35270;&#35273;&#32858;&#31867;&#12290;&#23427;&#36890;&#36807;&#23545;&#29305;&#24449;&#30456;&#20851;&#24615;&#30340;&#35299;&#37322;&#65292;&#25552;&#20379;&#23545;&#39640;&#32500;&#25968;&#25454;&#20013;&#35270;&#35273;&#32858;&#31867;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.06885</link><description>&lt;p&gt;
DimVis: &#20351;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#22120;&#65288;EBM&#65289;&#35299;&#37322;&#32500;&#24230;&#32422;&#20943;&#20013;&#30340;&#35270;&#35273;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
DimVis: Interpreting Visual Clusters in Dimensionality Reduction With Explainable Boosting Machine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06885
&lt;/p&gt;
&lt;p&gt;
DimVis&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#22120;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#32500;&#24230;&#32422;&#20943;&#20013;&#30340;&#35270;&#35273;&#32858;&#31867;&#12290;&#23427;&#36890;&#36807;&#23545;&#29305;&#24449;&#30456;&#20851;&#24615;&#30340;&#35299;&#37322;&#65292;&#25552;&#20379;&#23545;&#39640;&#32500;&#25968;&#25454;&#20013;&#35270;&#35273;&#32858;&#31867;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#24230;&#32422;&#20943;&#65288;DR&#65289;&#25216;&#26415;&#65292;&#22914;t-SNE&#21644;UMAP&#65292;&#24456;&#21463;&#27426;&#36814;&#65292;&#21487;&#20197;&#23558;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#36716;&#25442;&#25104;&#26356;&#31616;&#21333;&#30340;&#21487;&#35270;&#21270;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#26377;&#25928;&#22320;&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#30340;&#24635;&#20307;&#27169;&#24335;&#65292;&#20294;&#21487;&#33021;&#20250;&#24341;&#20837;&#20266;&#20687;&#24182;&#23384;&#22312;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DimVis&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#23427;&#20351;&#29992;&#30417;&#30563;&#30340;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#22120;&#65288;EBM&#65289;&#27169;&#22411;&#65288;&#22312;&#29992;&#25143;&#36873;&#25321;&#30340;&#24863;&#20852;&#36259;&#25968;&#25454;&#19978;&#35757;&#32451;&#65289;&#20316;&#20026;DR&#25237;&#24433;&#30340;&#35299;&#37322;&#21161;&#25163;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#36890;&#36807;&#36890;&#36807;&#20132;&#20114;&#24335;&#25506;&#32034;UMAP&#25237;&#24433;&#20013;&#30340;&#29305;&#24449;&#30456;&#20851;&#24615;&#26469;&#25552;&#20379;&#23545;&#35270;&#35273;&#32858;&#31867;&#20013;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#20419;&#36827;&#39640;&#32500;&#25968;&#25454;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DimVis&#20351;&#29992;&#19968;&#20010;&#23545;&#27604;&#30340;EBM&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#23454;&#26102;&#35757;&#32451;&#20013;&#33021;&#21306;&#20998;&#24863;&#20852;&#36259;&#32858;&#31867;&#20869;&#22806;&#30340;&#25968;&#25454;&#12290;&#21033;&#29992;EBM&#22266;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#21644;&#25104;&#23545;&#30340;&#29305;&#24449;&#27604;&#36739;&#26469;&#35299;&#37322;&#32858;&#31867;&#26412;&#36523;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dimensionality Reduction (DR) techniques such as t-SNE and UMAP are popular for transforming complex datasets into simpler visual representations. However, while effective in uncovering general dataset patterns, these methods may introduce artifacts and suffer from interpretability issues. This paper presents DimVis, a visualization tool that employs supervised Explainable Boosting Machine (EBM) models (trained on user-selected data of interest) as an interpretation assistant for DR projections. Our tool facilitates high-dimensional data analysis by providing an interpretation of feature relevance in visual clusters through interactive exploration of UMAP projections. Specifically, DimVis uses a contrastive EBM model that is trained in real time to differentiate between the data inside and outside a cluster of interest. Taking advantage of the inherent explainable nature of the EBM, we then use this model to interpret the cluster itself via single and pairwise feature comparisons in a 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#26102;&#25512;&#29702;&#36895;&#24230;&#19979;&#23454;&#29616;&#39640;&#20998;&#21106;&#20934;&#30830;&#24615;&#30340;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#21644;&#36882;&#24402;&#23545;&#40784;&#32593;&#32476;&#12290;&#20351;&#29992;ResNet-18&#20316;&#20026;&#39592;&#24178;&#65292;&#36890;&#36807;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#21644;&#36882;&#24402;&#23545;&#40784;&#27169;&#22359;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02286</link><description>&lt;p&gt;
&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#21644;&#36882;&#24402;&#23545;&#40784;&#32593;&#32476;&#29992;&#20110;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Multi-Level Feature Aggregation and Recursive Alignment Network for Real-Time Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02286
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#26102;&#25512;&#29702;&#36895;&#24230;&#19979;&#23454;&#29616;&#39640;&#20998;&#21106;&#20934;&#30830;&#24615;&#30340;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#21644;&#36882;&#24402;&#23545;&#40784;&#32593;&#32476;&#12290;&#20351;&#29992;ResNet-18&#20316;&#20026;&#39592;&#24178;&#65292;&#36890;&#36807;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#21644;&#36882;&#24402;&#23545;&#40784;&#27169;&#22359;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26041;&#27861;&#37117;&#30528;&#37325;&#20110;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#27169;&#22411;&#22823;&#23567;&#65292;&#20294;&#21516;&#26102;&#29306;&#29298;&#20102;&#20934;&#30830;&#24615;&#12290;&#22312;&#19968;&#20123;&#22330;&#26223;&#19979;&#65292;&#22914;&#33258;&#20027;&#23548;&#33322;&#21644;&#39550;&#39542;&#21592;&#36741;&#21161;&#31995;&#32479;&#65292;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#21516;&#26679;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#21644;&#36882;&#24402;&#23545;&#40784;&#32593;&#32476;&#65288;MFARANet&#65289;&#65292;&#26088;&#22312;&#23454;&#29616;&#39640;&#20998;&#21106;&#20934;&#30830;&#24615;&#21644;&#23454;&#26102;&#25512;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;ResNet-18&#20316;&#20026;&#39592;&#24178;&#26469;&#20445;&#35777;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#26680;&#24515;&#32452;&#20214;&#26469;&#24357;&#34917;&#27973;&#39592;&#24178;&#24341;&#36215;&#30340;&#27169;&#22411;&#23481;&#37327;&#20943;&#23569;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#65288;MFAM&#65289;&#65292;&#23558;&#32534;&#30721;&#22120;&#20013;&#30340;&#20998;&#23618;&#29305;&#24449;&#32858;&#21512;&#21040;&#27599;&#20010;&#23610;&#24230;&#65292;&#20197;&#20415;&#20110;&#21518;&#32493;&#30340;&#31354;&#38388;&#23545;&#40784;&#21644;&#22810;&#23610;&#24230;&#25512;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#27969;&#30340;&#23545;&#40784;&#26469;&#24314;&#31435;&#36882;&#24402;&#23545;&#40784;&#27169;&#22359;&#65288;RAM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time semantic segmentation is a crucial research for real-world applications. However, many methods lay particular emphasis on reducing the computational complexity and model size, while largely sacrificing the accuracy. In some scenarios, such as autonomous navigation and driver assistance system, accuracy and speed are equally important. To tackle this problem, we propose a novel Multi-level Feature Aggregation and Recursive Alignment Network (MFARANet), aiming to achieve high segmentation accuracy at real-time inference speed. We employ ResNet-18 as the backbone to ensure efficiency, and propose three core components to compensate for the reduced model capacity due to the shallow backbone. Specifically, we first design Multi-level Feature Aggregation Module (MFAM) to aggregate the hierarchical features in the encoder to each scale to benefit subsequent spatial alignment and multi-scale inference. Then, we build Recursive Alignment Module (RAM) by combining the flow-based alignm
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#19988;&#23450;&#24615;&#22320;&#23637;&#31034;&#20102;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01858</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Explaining latent representations of generative models with large multimodal models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01858
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#19988;&#23450;&#24615;&#22320;&#23637;&#31034;&#20102;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#29983;&#25104;&#28508;&#22312;&#22240;&#32032;&#34920;&#31034;&#26159;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#37325;&#35201;&#35838;&#39064;&#12290;&#38543;&#30528;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#23427;&#21487;&#20197;&#23558;&#22270;&#20687;&#19982;&#25991;&#26412;&#23545;&#40784;&#20197;&#29983;&#25104;&#31572;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#37327;&#21270;&#35780;&#20272;&#20102;&#25105;&#20204;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22312;&#22810;&#20010;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20043;&#38388;&#35780;&#20272;&#20102;&#35299;&#37322;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#24182;&#23450;&#24615;&#22320;&#21487;&#35270;&#21270;&#20102;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#20197;&#23398;&#20064;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#23545;&#35299;&#37322;&#30340;&#35299;&#32544;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning interpretable representations of data generative latent factors is an important topic for the development of artificial intelligence. With the rise of the large multimodal model, it can align images with text to generate answers. In this work, we propose a framework to comprehensively explain each latent factor in the generative models using a large multimodal model. We further measure the uncertainty of our generated explanations, quantitatively evaluate the performance of explanation generation among multiple large multimodal models, and qualitatively visualize the variations of each latent factor to learn the disentanglement effects of different generative models on explanations. Finally, we discuss the explanatory capabilities and limitations of state-of-the-art large multimodal models.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#22270;&#32534;&#36753;&#20316;&#20026;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20808;&#21069;&#21162;&#21147;&#65292;&#25506;&#35752;&#20102;&#23558;&#36755;&#20837;&#25968;&#25454;&#34920;&#31034;&#20026;&#22270;&#24418;&#23545;&#20110;&#29983;&#25104;&#40657;&#31665;&#22270;&#20687;&#20998;&#31867;&#22120;&#26368;&#23567;&#19988;&#26377;&#24847;&#20041;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24615;&#33021;&#21644;&#26102;&#38388;&#25928;&#29575;&#26368;&#20339;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2401.11609</link><description>&lt;p&gt;
&#22270;&#32534;&#36753;&#29992;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Graph Edits for Counterfactual Explanations: A comparative study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11609
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#22270;&#32534;&#36753;&#20316;&#20026;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20808;&#21069;&#21162;&#21147;&#65292;&#25506;&#35752;&#20102;&#23558;&#36755;&#20837;&#25968;&#25454;&#34920;&#31034;&#20026;&#22270;&#24418;&#23545;&#20110;&#29983;&#25104;&#40657;&#31665;&#22270;&#20687;&#20998;&#31867;&#22120;&#26368;&#23567;&#19988;&#26377;&#24847;&#20041;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24615;&#33021;&#21644;&#26102;&#38388;&#25928;&#29575;&#26368;&#20339;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#24050;&#34987;&#30830;&#31435;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#65292;&#21033;&#29992;&#19968;&#32452;&#26368;&#23567;&#30340;&#32534;&#36753;&#26469;&#25913;&#21464;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#12290;&#22312;&#32771;&#34385;&#22270;&#20687;&#19978;&#30340;&#27010;&#24565;&#21453;&#20107;&#23454;&#26102;&#65292;&#35831;&#27714;&#30340;&#32534;&#36753;&#24212;&#23545;&#24212;&#36755;&#20837;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#26174;&#33879;&#27010;&#24565;&#12290;&#21516;&#26102;&#65292;&#27010;&#24565;&#36317;&#31163;&#30001;&#30693;&#35782;&#22270;&#35889;&#23450;&#20041;&#65292;&#30830;&#20445;&#27010;&#24565;&#32534;&#36753;&#30340;&#26368;&#20248;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#25193;&#23637;&#20102;&#20197;&#22270;&#32534;&#36753;&#20026;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20808;&#21069;&#21162;&#21147;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#28085;&#30422;&#20102;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26041;&#27861;&#12290;&#21040;&#27492;&#20026;&#27490;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#65306;&#25105;&#20204;&#24212;&#35813;&#23558;&#36755;&#20837;&#25968;&#25454;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#36825;&#26159;&#29983;&#25104;&#40657;&#31665;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#26368;&#23567;&#21644;&#26377;&#24847;&#20041;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#24615;&#33021;&#21644;&#26102;&#38388;&#25928;&#29575;&#26041;&#38754;&#26368;&#20339;&#30340;GNN&#26041;&#27861;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11609v2 Announce Type: replace-cross  Abstract: Counterfactuals have been established as a popular explainability technique which leverages a set of minimal edits to alter the prediction of a classifier. When considering conceptual counterfactuals on images, the edits requested should correspond to salient concepts present in the input data. At the same time, conceptual distances are defined by knowledge graphs, ensuring the optimality of conceptual edits. In this work, we extend previous endeavors on graph edits as counterfactual explanations by conducting a comparative study which encompasses both supervised and unsupervised Graph Neural Network (GNN) approaches. To this end, we pose the following significant research question: should we represent input data as graphs, which is the optimal GNN approach in terms of performance and time efficiency to generate minimal and meaningful counterfactual explanations for black-box image classifiers?
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;AUROC&#21644;AUPRC&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#21487;&#20197;&#20197;&#27010;&#29575;&#26415;&#35821;&#31616;&#27905;&#22320;&#30456;&#20851;&#32852;&#12290;&#30456;&#27604;&#20110;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#30340;AUPRC&#20248;&#36234;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;AUPRC&#24182;&#19981;&#22914;&#20154;&#20204;&#39044;&#26399;&#30340;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#21487;&#33021;&#26159;&#19968;&#31181;&#26377;&#23475;&#30340;&#25351;&#26631;&#12290;&#30740;&#31350;&#36824;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#25991;&#29486;&#39564;&#35777;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.06091</link><description>&lt;p&gt;
AUROC&#21644;AUPRC&#22312;&#31867;&#19981;&#24179;&#34913;&#19979;&#30340;&#28145;&#20837;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at AUROC and AUPRC under Class Imbalance. (arXiv:2401.06091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06091
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;AUROC&#21644;AUPRC&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#21487;&#20197;&#20197;&#27010;&#29575;&#26415;&#35821;&#31616;&#27905;&#22320;&#30456;&#20851;&#32852;&#12290;&#30456;&#27604;&#20110;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#30340;AUPRC&#20248;&#36234;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;AUPRC&#24182;&#19981;&#22914;&#20154;&#20204;&#39044;&#26399;&#30340;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#21487;&#33021;&#26159;&#19968;&#31181;&#26377;&#23475;&#30340;&#25351;&#26631;&#12290;&#30740;&#31350;&#36824;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#25991;&#29486;&#39564;&#35777;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#19968;&#20010;&#24191;&#27867;&#30340;&#35266;&#28857;&#26159;&#65292;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#38754;&#31215;&#21463;&#38480;&#21046;&#30340;&#20934;&#30830;&#29575;&#26354;&#32447;&#65288;AUPRC&#65289;&#27604;&#21463;&#35797;&#32773;&#24037;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#30340;&#38754;&#31215;&#65288;AUROC&#65289;&#26356;&#22909;&#22320;&#29992;&#20110;&#27169;&#22411;&#27604;&#36739;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#36890;&#36807;&#26032;&#39062;&#30340;&#25968;&#23398;&#20998;&#26512;&#25361;&#25112;&#20102;&#36825;&#19968;&#35266;&#28857;&#65292;&#24182;&#35828;&#26126;&#20102;AUROC&#21644;AUPRC&#21487;&#20197;&#20197;&#27010;&#29575;&#26415;&#35821;&#31616;&#27905;&#22320;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;AUPRC&#24182;&#19981;&#22914;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#30340;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#26356;&#20248;&#65292;&#29978;&#33267;&#21487;&#33021;&#26159;&#19968;&#31181;&#26377;&#23475;&#30340;&#25351;&#26631;&#65292;&#22240;&#20026;&#23427;&#20542;&#21521;&#20110;&#36807;&#20998;&#20559;&#21521;&#20110;&#22312;&#27491;&#26679;&#26412;&#36739;&#20026;&#39057;&#32321;&#30340;&#23376;&#32676;&#20013;&#25913;&#21892;&#27169;&#22411;&#12290;&#36825;&#31181;&#20559;&#24046;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#22686;&#21152;&#31639;&#27861;&#30340;&#24046;&#24322;&#12290;&#22312;&#36825;&#20123;&#27934;&#35265;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#22238;&#39038;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;arXiv&#19978;&#30340;150&#22810;&#19975;&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#37325;&#28857;&#26159;&#39564;&#35777;&#21644;&#35777;&#26126;&#22768;&#31216;&#30340;AUPRC&#20248;&#36234;&#24615;&#30340;&#26222;&#36941;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning (ML), a widespread adage is that the area under the precision-recall curve (AUPRC) is a superior metric for model comparison to the area under the receiver operating characteristic (AUROC) for binary classification tasks with class imbalance. This paper challenges this notion through novel mathematical analysis, illustrating that AUROC and AUPRC can be concisely related in probabilistic terms. We demonstrate that AUPRC, contrary to popular belief, is not superior in cases of class imbalance and might even be a harmful metric, given its inclination to unduly favor model improvements in subpopulations with more frequent positive labels. This bias can inadvertently heighten algorithmic disparities. Prompted by these insights, a thorough review of existing ML literature was conducted, utilizing large language models to analyze over 1.5 million papers from arXiv. Our investigation focused on the prevalence and substantiation of the purported AUPRC superiority. The result
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31867;&#38750;&#32447;&#24615;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#36793;&#30028;&#12290;&#23545;&#20110;&#20855;&#26377;Lipschitz&#36830;&#32493;&#26799;&#24230;&#30340;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;&#20351;&#22122;&#22768;&#26159;&#37325;&#23614;&#30340;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#23545;&#22833;&#36133;&#27010;&#29575;&#30340;&#23545;&#25968;&#20381;&#36182;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#21098;&#20999;&#12289;&#24402;&#19968;&#21270;&#21644;&#37327;&#21270;&#31561;&#20219;&#20309;&#20855;&#26377;&#26377;&#30028;&#36755;&#20986;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.18784</link><description>&lt;p&gt;
&#39640;&#27010;&#29575;&#25910;&#25947;&#36793;&#30028;&#19979;&#30340;&#38750;&#32447;&#24615;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#37325;&#23614;&#22122;&#22768;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
High-probability Convergence Bounds for Nonlinear Stochastic Gradient Descent Under Heavy-tailed Noise. (arXiv:2310.18784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31867;&#38750;&#32447;&#24615;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#36793;&#30028;&#12290;&#23545;&#20110;&#20855;&#26377;Lipschitz&#36830;&#32493;&#26799;&#24230;&#30340;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;&#20351;&#22122;&#22768;&#26159;&#37325;&#23614;&#30340;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#23545;&#22833;&#36133;&#27010;&#29575;&#30340;&#23545;&#25968;&#20381;&#36182;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#21098;&#20999;&#12289;&#24402;&#19968;&#21270;&#21644;&#37327;&#21270;&#31561;&#20219;&#20309;&#20855;&#26377;&#26377;&#30028;&#36755;&#20986;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#20010;&#30740;&#31350;&#24037;&#20316;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21450;&#20854;&#21098;&#20999;&#21464;&#20307;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#12290;&#19982;&#26222;&#36890;&#30340;SGD&#30456;&#27604;&#65292;&#21098;&#20999;SGD&#22312;&#23454;&#38469;&#20013;&#26356;&#21152;&#31283;&#23450;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#23545;&#25968;&#20381;&#36182;&#20110;&#22833;&#36133;&#27010;&#29575;&#30340;&#39069;&#22806;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#20854;&#20182;&#23454;&#38469;&#38750;&#32447;&#24615;SGD&#21464;&#20307;&#65288;&#22914;&#31526;&#21495;SGD&#12289;&#37327;&#21270;SGD&#21644;&#24402;&#19968;&#21270;SGD&#65289;&#30340;&#25910;&#25947;&#24615;&#29702;&#35299;&#35201;&#23569;&#24471;&#22810;&#65292;&#36825;&#20123;&#26041;&#27861;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#36890;&#20449;&#25928;&#29575;&#25110;&#21152;&#36895;&#25910;&#25947;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#24191;&#20041;&#38750;&#32447;&#24615;SGD&#26041;&#27861;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#36793;&#30028;&#12290;&#23545;&#20110;&#20855;&#26377;Lipschitz&#36830;&#32493;&#26799;&#24230;&#30340;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;&#20351;&#22122;&#22768;&#26159;&#37325;&#23614;&#30340;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#22833;&#36133;&#27010;&#29575;&#30340;&#23545;&#25968;&#20381;&#36182;&#12290;&#19982;&#21098;&#20999;SGD&#30340;&#32467;&#26524;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26356;&#20026;&#19968;&#33324;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#26377;&#30028;&#36755;&#20986;&#30340;&#20219;&#20309;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#22914;&#21098;&#20999;&#12289;&#24402;&#19968;&#21270;&#21644;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several recent works have studied the convergence \textit{in high probability} of stochastic gradient descent (SGD) and its clipped variant. Compared to vanilla SGD, clipped SGD is practically more stable and has the additional theoretical benefit of logarithmic dependence on the failure probability. However, the convergence of other practical nonlinear variants of SGD, e.g., sign SGD, quantized SGD and normalized SGD, that achieve improved communication efficiency or accelerated convergence is much less understood. In this work, we study the convergence bounds \textit{in high probability} of a broad class of nonlinear SGD methods. For strongly convex loss functions with Lipschitz continuous gradients, we prove a logarithmic dependence on the failure probability, even when the noise is heavy-tailed. Strictly more general than the results for clipped SGD, our results hold for any nonlinearity with bounded (component-wise or joint) outputs, such as clipping, normalization, and quantizati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.10688</link><description>&lt;p&gt;
&#19968;&#31181;&#20165;&#35299;&#30721;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A decoder-only foundation model for time-series forecasting. (arXiv:2310.10688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#65292;&#20854;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24320;&#31665;&#21363;&#29992;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#25509;&#36817;&#27599;&#20010;&#20010;&#21035;&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#22312;&#22823;&#22411;&#26102;&#38388;&#24207;&#21015;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#39044;&#27979;&#21382;&#21490;&#38271;&#24230;&#12289;&#39044;&#27979;&#38271;&#24230;&#21644;&#26102;&#38388;&#31890;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.08475</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#19982;&#32534;&#36753;&#21333;&#27169;&#24335;LLMs&#30456;&#27604;&#65292;&#22810;&#27169;&#24335;&#27169;&#22411;&#30340;&#32534;&#36753;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#26356;&#39640;&#32423;&#21035;&#30340;&#23457;&#26597;&#21644;&#24910;&#37325;&#32771;&#34385;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#31216;&#20026;MMEdit&#65292;&#29992;&#20110;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#22871;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21253;&#25324;&#21508;&#31181;&#27169;&#22411;&#32534;&#36753;&#22522;&#32447;&#30340;&#32508;&#21512;&#23454;&#39564;&#65292;&#24182;&#20998;&#26512;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#30340;&#19981;&#21516;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#26681;&#25454;&#32463;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20043;&#21069;&#30340;&#22522;&#32447;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#23454;&#29616;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#20294;&#25928;&#26524;&#20173;&#28982;&#19981;&#29702;&#24819;&#65292;&#34920;&#26126;&#36825;&#20010;&#20219;&#21153;&#21487;&#33021;&#23384;&#22312;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#35265;&#35299;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/zjunlp/EasyEdit&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.
&lt;/p&gt;</description></item><item><title>XIMAGENET-12&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;AI&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;12&#20010;&#24120;&#35265;&#29289;&#20307;&#31867;&#21035;&#30340;&#36229;&#36807;200,000&#24352;&#22270;&#20687;&#21644;15,600&#20010;&#25163;&#21160;&#35821;&#20041;&#27880;&#37322;&#12290;&#23427;&#36890;&#36807;&#27169;&#25311;&#20845;&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#27169;&#22411;&#29983;&#25104;&#33021;&#21147;&#35780;&#20272;&#30340;&#26032;&#30340;&#31283;&#20581;&#24615;&#20934;&#21017;&#12290;</title><link>http://arxiv.org/abs/2310.08182</link><description>&lt;p&gt;
XIMAGENET-12&#65306;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;AI&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
XIMAGENET-12: An Explainable AI Benchmark Dataset for Model Robustness Evaluation. (arXiv:2310.08182v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08182
&lt;/p&gt;
&lt;p&gt;
XIMAGENET-12&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;AI&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;12&#20010;&#24120;&#35265;&#29289;&#20307;&#31867;&#21035;&#30340;&#36229;&#36807;200,000&#24352;&#22270;&#20687;&#21644;15,600&#20010;&#25163;&#21160;&#35821;&#20041;&#27880;&#37322;&#12290;&#23427;&#36890;&#36807;&#27169;&#25311;&#20845;&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#27169;&#22411;&#29983;&#25104;&#33021;&#21147;&#35780;&#20272;&#30340;&#26032;&#30340;&#31283;&#20581;&#24615;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#31283;&#20581;&#24615;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#24191;&#27867;&#20381;&#36182;&#21508;&#31181;&#26080;&#20851;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#23548;&#33268;&#23398;&#26415;&#39564;&#35777;&#30340;&#31283;&#20581;&#27169;&#22411;&#19982;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;XIMAGENET-12&#65292;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;200,000&#24352;&#22270;&#20687;&#21644;15,600&#20010;&#25163;&#21160;&#35821;&#20041;&#27880;&#37322;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;ImageNet&#30340;12&#20010;&#31867;&#21035;&#65292;&#20197;&#20195;&#34920;&#22312;&#23454;&#38469;&#29983;&#27963;&#20013;&#24120;&#35265;&#30340;&#29289;&#20307;&#65292;&#24182;&#27169;&#25311;&#20102;&#20845;&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#65292;&#21253;&#25324;&#36807;&#26333;&#12289;&#27169;&#31946;&#12289;&#39068;&#33394;&#21464;&#21270;&#31561;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#27169;&#22411;&#29983;&#25104;&#33021;&#21147;&#35780;&#20272;&#30340;&#26032;&#30340;&#31283;&#20581;&#24615;&#20934;&#21017;&#12290;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#21450;&#30456;&#20851;&#20195;&#30721;&#21487;&#22312;https://sites.google.com/view/ximagenet-12/home&#33719;&#21462;&#12290;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#21487;&#20197;&#21033;&#29992;&#36825;&#19968;&#36164;&#28304;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#26465;&#20214;&#19979;&#35780;&#20272;&#20182;&#20204;&#30340;&#35270;&#35273;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#20174;&#32780;&#20174;&#23454;&#38469;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#38656;&#27714;&#20013;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lack of standardized robustness metrics and the widespread reliance on numerous unrelated benchmark datasets for testing have created a gap between academically validated robust models and their often problematic practical adoption. To address this, we introduce XIMAGENET-12, an explainable benchmark dataset with over 200K images and 15,600 manual semantic annotations. Covering 12 categories from ImageNet to represent objects commonly encountered in practical life and simulating six diverse scenarios, including overexposure, blurring, color changing, etc., we further propose a novel robustness criterion that extends beyond model generation ability assessment. This benchmark dataset, along with related code, is available at https://sites.google.com/view/ximagenet-12/home. Researchers and practitioners can leverage this resource to evaluate the robustness of their visual models under challenging conditions and ultimately benefit from the demands of practical computer vision systems.
&lt;/p&gt;</description></item><item><title>&#24191;&#20041;&#34203;&#23450;&#35860;&#26725;&#21305;&#37197;&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#21305;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#29305;&#23450;&#30340;&#29366;&#24577;&#25104;&#26412;&#32771;&#34385;&#22312;&#20869;&#65292;&#25512;&#24191;&#20102;&#29616;&#20195;&#20998;&#24067;&#21305;&#37197;&#31639;&#27861;&#65292;&#24182;&#21487;&#29992;&#20110;&#35299;&#20915;&#26465;&#20214;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02233</link><description>&lt;p&gt;
&#24191;&#20041;&#34203;&#23450;&#35860;&#26725;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Generalized Schr\"odinger Bridge Matching. (arXiv:2310.02233v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02233
&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#34203;&#23450;&#35860;&#26725;&#21305;&#37197;&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#21305;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#29305;&#23450;&#30340;&#29366;&#24577;&#25104;&#26412;&#32771;&#34385;&#22312;&#20869;&#65292;&#25512;&#24191;&#20102;&#29616;&#20195;&#20998;&#24067;&#21305;&#37197;&#31639;&#27861;&#65292;&#24182;&#21487;&#29992;&#20110;&#35299;&#20915;&#26465;&#20214;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20998;&#24067;&#21305;&#37197;&#31639;&#27861;&#29992;&#20110;&#35757;&#32451;&#25193;&#25955;&#25110;&#27969;&#27169;&#22411;&#65292;&#30452;&#25509;&#35268;&#23450;&#20102;&#20004;&#20010;&#36793;&#30028;&#20998;&#24067;&#20043;&#38388;&#30340;&#36793;&#32536;&#20998;&#24067;&#30340;&#26102;&#38388;&#28436;&#21464;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#20998;&#24067;&#21305;&#37197;&#35774;&#32622;&#65292;&#20854;&#20013;&#36825;&#20123;&#36793;&#32536;&#20998;&#24067;&#20165;&#20197;&#26576;&#20123;&#20219;&#21153;&#29305;&#23450;&#30446;&#26631;&#20989;&#25968;&#30340;&#35299;&#24418;&#24335;&#38544;&#21547;&#25551;&#36848;&#12290;&#36825;&#20010;&#38382;&#39064;&#35774;&#32622;&#34987;&#31216;&#20026;&#24191;&#20041;&#34203;&#23450;&#35860;&#26725;(GSB)&#65292;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20869;&#21644;&#26426;&#22120;&#23398;&#20064;&#20043;&#22806;&#24191;&#27867;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#34203;&#23450;&#35860;&#26725;&#21305;&#37197;(GSBM)&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#26368;&#36817;&#36827;&#23637;&#21551;&#21457;&#30340;&#26032;&#30340;&#21305;&#37197;&#31639;&#27861;&#65292;&#23558;&#23427;&#20204;&#25512;&#24191;&#21040;&#21160;&#33021;&#26368;&#23567;&#21270;&#20043;&#22806;&#65292;&#24182;&#32771;&#34385;&#21040;&#20219;&#21153;&#29305;&#23450;&#30340;&#29366;&#24577;&#25104;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#26679;&#30340;&#27867;&#21270;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#27714;&#35299;&#26465;&#20214;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#21487;&#20197;&#20351;&#29992;&#39640;&#25928;&#30340;&#21464;&#20998;&#36817;&#20284;&#65292;&#24182;&#20511;&#21161;&#36335;&#24452;&#31215;&#20998;&#29702;&#35770;&#36827;&#19968;&#27493;&#21435;&#20559;&#24046;&#12290;&#19982;&#35299;&#20915;GSB&#38382;&#39064;&#30340;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;
&lt;/p&gt;
&lt;p&gt;
Modern distribution matching algorithms for training diffusion or flow models directly prescribe the time evolution of the marginal distributions between two boundary distributions. In this work, we consider a generalized distribution matching setup, where these marginals are only implicitly described as a solution to some task-specific objective function. The problem setup, known as the Generalized Schr\"odinger Bridge (GSB), appears prevalently in many scientific areas both within and without machine learning. We propose Generalized Schr\"odinger Bridge Matching (GSBM), a new matching algorithm inspired by recent advances, generalizing them beyond kinetic energy minimization and to account for task-specific state costs. We show that such a generalization can be cast as solving conditional stochastic optimal control, for which efficient variational approximations can be used, and further debiased with the aid of path integral theory. Compared to prior methods for solving GSB problems,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;VMD&#12289;&#29305;&#24449;&#24037;&#31243;&#21644;&#22534;&#21472;Informer&#65292;&#32467;&#21512;&#33258;&#36866;&#24212;&#25439;&#22833;&#20989;&#25968;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#32929;&#24066;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#21709;&#24212;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#21644;&#20854;&#20182;&#28151;&#21512;&#27169;&#22411;&#65292;&#23545;&#20110;&#23567;&#20225;&#19994;&#21644;&#29305;&#24449;&#24037;&#31243;&#30340;&#39044;&#27979;&#24314;&#27169;&#26377;&#28508;&#22312;&#30340;&#20248;&#21270;&#26041;&#21521;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.01884</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#28151;&#21512;&#27169;&#22411;&#30340;&#25913;&#36827;VMD&#21644;&#22534;&#21472;Informer&#22312;&#22686;&#24378;&#32929;&#24066;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adaptive Hybrid Model for Enhanced Stock Market Predictions Using Improved VMD and Stacked Informer. (arXiv:2310.01884v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;VMD&#12289;&#29305;&#24449;&#24037;&#31243;&#21644;&#22534;&#21472;Informer&#65292;&#32467;&#21512;&#33258;&#36866;&#24212;&#25439;&#22833;&#20989;&#25968;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#32929;&#24066;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#21709;&#24212;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#21644;&#20854;&#20182;&#28151;&#21512;&#27169;&#22411;&#65292;&#23545;&#20110;&#23567;&#20225;&#19994;&#21644;&#29305;&#24449;&#24037;&#31243;&#30340;&#39044;&#27979;&#24314;&#27169;&#26377;&#28508;&#22312;&#30340;&#20248;&#21270;&#26041;&#21521;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33258;&#36866;&#24212;&#28151;&#21512;&#27169;&#22411;&#65292;&#21033;&#29992;&#25913;&#36827;&#30340;&#21464;&#20998;&#27169;&#24577;&#20998;&#35299;&#65288;VMD&#65289;&#12289;&#29305;&#24449;&#24037;&#31243;&#65288;FE&#65289;&#21644;&#22534;&#21472;Informer&#65292;&#24182;&#32467;&#21512;&#33258;&#36866;&#24212;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#32929;&#24066;&#39044;&#27979;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;VMGCformer&#65288;Adam+GC+enhanced informer&#65289;&#65292;&#22312;&#22788;&#29702;&#32929;&#24066;&#25968;&#25454;&#30340;&#22797;&#26434;&#21160;&#24577;&#21644;&#27874;&#21160;&#24615;&#26041;&#38754;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#29087;&#32451;&#24230;&#12290;&#22522;&#20110;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#24471;&#20986;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#26174;&#20986;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#21709;&#24212;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30456;&#23545;&#20256;&#32479;&#21644;&#20854;&#20182;&#28151;&#21512;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#26412;&#30740;&#31350;&#36827;&#19968;&#27493;&#24378;&#35843;&#20102;&#20248;&#21270;&#30340;&#28508;&#22312;&#36884;&#24452;&#65292;&#24182;&#20171;&#32461;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#39044;&#27979;&#24314;&#27169;&#30340;&#26410;&#26469;&#26041;&#21521;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#23567;&#20225;&#19994;&#21644;&#29305;&#24449;&#24037;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an innovative adaptive hybrid model for stock market predictions, leveraging the capabilities of an enhanced Variational Mode Decomposition (VMD), Feature Engineering (FE), and stacked Informer integrated with an adaptive loss function. Through rigorous experimentation, the proposed model, termed Adam+GC+enhanced informer (We name it VMGCformer), demonstrates significant proficiency in addressing the intricate dynamics and volatile nature of stock market data. Experimental results, derived from multiple benchmark datasets, underscore the model's superiority in terms of prediction accuracy, responsiveness, and generalization capabilities over traditional and other hybrid models. The research further highlights potential avenues for optimization and introduces future directions to enhance predictive modeling, especially for small enterprises and feature engineering.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21327;&#21516;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#65288;CDRL&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#37319;&#26679;&#19968;&#31995;&#21015;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#65292;&#36890;&#36807;&#22312;&#19981;&#26029;&#22024;&#26434;&#21270;&#30340;&#25968;&#25454;&#38598;&#29256;&#26412;&#19978;&#23450;&#20041;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;EBMs&#65292;&#24182;&#19982;&#21021;&#22987;&#21270;&#27169;&#22411;&#37197;&#23545;&#21327;&#21516;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20851;&#38381;EBMs&#21644;&#20854;&#20182;&#29983;&#25104;&#26694;&#26550;&#20043;&#38388;&#30340;&#26679;&#26412;&#36136;&#37327;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.05153</link><description>&lt;p&gt;
&#36890;&#36807;&#21327;&#21516;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#23398;&#20064;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood. (arXiv:2309.05153v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21327;&#21516;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#65288;CDRL&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#37319;&#26679;&#19968;&#31995;&#21015;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#65292;&#36890;&#36807;&#22312;&#19981;&#26029;&#22024;&#26434;&#21270;&#30340;&#25968;&#25454;&#38598;&#29256;&#26412;&#19978;&#23450;&#20041;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;EBMs&#65292;&#24182;&#19982;&#21021;&#22987;&#21270;&#27169;&#22411;&#37197;&#23545;&#21327;&#21516;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20851;&#38381;EBMs&#21644;&#20854;&#20182;&#29983;&#25104;&#26694;&#26550;&#20043;&#38388;&#30340;&#26679;&#26412;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#25968;&#25454;&#19978;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#35757;&#32451;&#33021;&#37327;&#22522;&#20934;&#27169;&#22411;&#65288;EBMs&#65289;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#36739;&#38271;&#12290;&#22240;&#27492;&#65292;EBMs&#21644;&#20854;&#20182;&#29983;&#25104;&#26694;&#26550;&#65288;&#22914;GANs&#21644;&#25193;&#25955;&#27169;&#22411;&#65289;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#26679;&#26412;&#36136;&#37327;&#24046;&#36317;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#21463;&#26368;&#36817;&#36890;&#36807;&#26368;&#22823;&#21270;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#65288;DRL&#65289;&#26469;&#23398;&#20064;EBMs&#30340;&#21162;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21327;&#21516;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#65288;CDRL&#65289;&#65292;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#21487;&#34892;&#22320;&#23398;&#20064;&#21644;&#20174;&#19968;&#31995;&#21015;EBMs&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#36825;&#20123;EBMs&#23450;&#20041;&#22312;&#36234;&#26469;&#36234;&#22024;&#26434;&#30340;&#25968;&#25454;&#38598;&#29256;&#26412;&#19978;&#65292;&#24182;&#19982;&#27599;&#20010;EBM&#30340;&#21021;&#22987;&#21270;&#27169;&#22411;&#37197;&#23545;&#12290;&#22312;&#27599;&#20010;&#22122;&#22768;&#27700;&#24179;&#19978;&#65292;&#21021;&#22987;&#21270;&#27169;&#22411;&#23398;&#20064;&#22312;EBM&#30340;&#37319;&#26679;&#36807;&#31243;&#20013;&#20998;&#25674;&#65292;&#32780;&#20004;&#20010;&#27169;&#22411;&#22312;&#21327;&#21516;&#35757;&#32451;&#26694;&#26550;&#20869;&#20849;&#21516;&#20272;&#35745;&#12290;&#21021;&#22987;&#21270;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#20316;&#20026;&#36215;&#22987;&#28857;&#65292;&#32463;&#36807;EBM&#30340;&#20960;&#20010;&#37319;&#26679;&#27493;&#39588;&#36827;&#34892;&#25913;&#36827;&#12290;&#36890;&#36807;&#25913;&#36827;&#21518;&#30340;&#26679;&#26412;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#24674;&#22797;&#20284;&#28982;&#26469;&#20248;&#21270;EBM&#12290;
&lt;/p&gt;
&lt;p&gt;
Training energy-based models (EBMs) with maximum likelihood estimation on high-dimensional data can be both challenging and time-consuming. As a result, there a noticeable gap in sample quality between EBMs and other generative frameworks like GANs and diffusion models. To close this gap, inspired by the recent efforts of learning EBMs by maximimizing diffusion recovery likelihood (DRL), we propose cooperative diffusion recovery likelihood (CDRL), an effective approach to tractably learn and sample from a series of EBMs defined on increasingly noisy versons of a dataset, paired with an initializer model for each EBM. At each noise level, the initializer model learns to amortize the sampling process of the EBM, and the two models are jointly estimated within a cooperative training framework. Samples from the initializer serve as starting points that are refined by a few sampling steps from the EBM. With the refined samples, the EBM is optimized by maximizing recovery likelihood, while t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#27979;&#32534;&#30721;&#36827;&#34892;&#34394;&#25311;&#29615;&#22659;&#26144;&#23556;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26681;&#25454;&#24863;&#30693;&#25968;&#25454;&#26500;&#24314;&#31354;&#38388;&#22320;&#22270;&#65292;&#24182;&#22312;&#19981;&#21516;&#36755;&#20837;&#27169;&#24577;&#19979;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.10913</link><description>&lt;p&gt;
&#33258;&#21160;&#21033;&#29992;&#35270;&#35273;&#39044;&#27979;&#32534;&#30721;&#36827;&#34892;&#34394;&#25311;&#29615;&#22659;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Automated mapping of virtual environments with visual predictive coding. (arXiv:2308.10913v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#27979;&#32534;&#30721;&#36827;&#34892;&#34394;&#25311;&#29615;&#22659;&#26144;&#23556;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26681;&#25454;&#24863;&#30693;&#25968;&#25454;&#26500;&#24314;&#31354;&#38388;&#22320;&#22270;&#65292;&#24182;&#22312;&#19981;&#21516;&#36755;&#20837;&#27169;&#24577;&#19979;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26681;&#25454;&#24863;&#30693;&#36755;&#20837;&#30452;&#25509;&#26500;&#24314;&#23545;&#29615;&#22659;&#30340;&#20869;&#37096;&#35748;&#30693;&#22320;&#22270;&#65292;&#32780;&#19981;&#38656;&#35201;&#20855;&#26377;&#26126;&#30830;&#22352;&#26631;&#25110;&#36317;&#31163;&#27979;&#37327;&#30340;&#31995;&#32479;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22914;SLAM&#21033;&#29992;&#19987;&#38376;&#30340;&#35270;&#35273;&#25512;&#29702;&#36807;&#31243;&#20174;&#35270;&#35273;&#21644;&#37324;&#31243;&#35745;&#25968;&#25454;&#20013;&#35782;&#21035;&#35270;&#35273;&#29305;&#24449;&#24182;&#26500;&#24314;&#31354;&#38388;&#22320;&#22270;&#65292;&#20294;&#22823;&#33041;&#20013;&#35748;&#30693;&#22320;&#22270;&#30340;&#19968;&#33324;&#24615;&#34920;&#26126;&#21487;&#20197;&#20351;&#29992;&#32479;&#19968;&#30340;&#26144;&#23556;&#31639;&#27861;&#31574;&#30053;&#26469;&#27867;&#21270;&#21040;&#21548;&#35273;&#12289;&#35302;&#35273;&#21644;&#35821;&#35328;&#36755;&#20837;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#39044;&#27979;&#32534;&#30721;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#19988;&#22810;&#21151;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#24863;&#30693;&#25968;&#25454;&#26500;&#24314;&#31354;&#38388;&#22320;&#22270;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20854;&#20013;&#20195;&#29702;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#23548;&#33322;&#65292;&#24182;&#20351;&#29992;&#20855;&#26377;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35270;&#35273;&#39044;&#27979;&#32534;&#30721;&#12290;&#22312;&#23398;&#20064;&#19979;&#19968;&#20010;&#22270;&#20687;&#39044;&#27979;&#20219;&#21153;&#30340;&#21516;&#26102;&#65292;&#20195;&#29702;&#20250;&#33258;&#21160;&#26500;&#24314;&#19968;&#20010;&#20869;&#37096;&#23545;&#29615;&#22659;&#30340;&#34920;&#31034;&#65292;&#23450;&#37327;&#22320;&#21453;&#26144;&#20986;&#36317;&#31163;&#31561;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans construct internal cognitive maps of their environment directly from sensory inputs without access to a system of explicit coordinates or distance measurements. While machine learning algorithms like SLAM utilize specialized visual inference procedures to identify visual features and construct spatial maps from visual and odometry data, the general nature of cognitive maps in the brain suggests a unified mapping algorithmic strategy that can generalize to auditory, tactile, and linguistic inputs. Here, we demonstrate that predictive coding provides a natural and versatile neural network algorithm for constructing spatial maps using sensory data. We introduce a framework in which an agent navigates a virtual environment while engaging in visual predictive coding using a self-attention-equipped convolutional neural network. While learning a next image prediction task, the agent automatically constructs an internal representation of the environment that quantitatively reflects dist
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#33410;&#28857;&#32423;&#22402;&#30452;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#65292;&#21033;&#29992;&#38646;&#32972;&#26223;&#30693;&#35782;&#31574;&#30053;&#26469;&#23454;&#29616;&#25915;&#20987;&#65292;&#24182;&#25581;&#31034;&#20102;&#35813;&#39046;&#22495;&#20869;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02465</link><description>&lt;p&gt;
BlindSage&#65306;&#38024;&#23545;&#33410;&#28857;&#32423;&#22402;&#30452;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
BlindSage: Label Inference Attacks against Node-level Vertical Federated Graph Neural Networks. (arXiv:2308.02465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#33410;&#28857;&#32423;&#22402;&#30452;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#65292;&#21033;&#29992;&#38646;&#32972;&#26223;&#30693;&#35782;&#31574;&#30053;&#26469;&#23454;&#29616;&#25915;&#20987;&#65292;&#24182;&#25581;&#31034;&#20102;&#35813;&#39046;&#22495;&#20869;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#20445;&#25345;&#28041;&#21450;&#24037;&#20316;&#26041;&#30340;&#21407;&#22987;&#25968;&#25454;&#31169;&#23494;&#24615;&#65292;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21327;&#20316;&#35757;&#32451;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#20043;&#19968;&#26159;&#25552;&#39640;&#27169;&#22411;&#30340;&#38544;&#31169;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36328;&#22495;&#35774;&#32622;&#65292;&#20854;&#20013;&#23569;&#25968;&#21442;&#19982;&#26041;&#22312;&#19981;&#20849;&#20139;&#30456;&#21516;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20998;&#31867;&#26631;&#31614;&#36890;&#24120;&#34987;&#35270;&#20026;&#20165;&#30001;&#19968;&#20010;&#65288;&#20027;&#21160;&#65289;&#21442;&#19982;&#26041;&#29420;&#21344;&#25345;&#26377;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#32780;&#20854;&#20182;&#65288;&#34987;&#21160;&#65289;&#21442;&#19982;&#26041;&#20165;&#20351;&#29992;&#20854;&#26412;&#22320;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;VFL&#30340;&#37325;&#35201;&#32570;&#38519;&#65292;&#21487;&#33021;&#23548;&#33268;&#22312;&#25915;&#20987;&#32773;&#20855;&#26377;&#26576;&#20123;&#65292;&#29978;&#33267;&#26377;&#38480;&#30340;&#26631;&#31614;&#19982;&#25968;&#25454;&#20851;&#31995;&#30340;&#32972;&#26223;&#30693;&#35782;&#30340;&#20551;&#35774;&#19979;&#21457;&#29983;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26159;&#39318;&#27425;&#65288;&#25454;&#25105;&#20204;&#25152;&#30693;&#65289;&#20351;&#29992;&#38646;&#32972;&#26223;&#30693;&#35782;&#31574;&#30053;&#30740;&#31350;VFL&#19978;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#12290;&#20026;&#20102;&#20855;&#20307;&#38416;&#36848;&#25105;&#20204;&#30340;&#25552;&#26696;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;Grap&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning enables collaborative training of machine learning models by keeping the raw data of the involved workers private. One of its main objectives is to improve the models' privacy, security, and scalability. Vertical Federated Learning (VFL) offers an efficient cross-silo setting where a few parties collaboratively train a model without sharing the same features. In such a scenario, classification labels are commonly considered sensitive information held exclusively by one (active) party, while other (passive) parties use only their local information. Recent works have uncovered important flaws of VFL, leading to possible label inference attacks under the assumption that the attacker has some, even limited, background knowledge on the relation between labels and data. In this work, we are the first (to the best of our knowledge) to investigate label inference attacks on VFL using a zero-background knowledge strategy. To concretely formulate our proposal, we focus on Grap
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#29305;&#24449;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29305;&#24449;&#37325;&#35201;&#24615;&#20540;&#30340;&#20004;&#20004;&#27604;&#36739;&#65292;&#21487;&#20197;&#20135;&#29983;&#25490;&#24207;&#21644;&#21516;&#26102;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#19988;&#21487;&#20197;&#36873;&#25321;&#21069;k&#20010;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2307.15361</link><description>&lt;p&gt;
&#30830;&#23450;&#24615;&#29305;&#24449;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Confident Feature Ranking. (arXiv:2307.15361v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15361
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#29305;&#24449;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29305;&#24449;&#37325;&#35201;&#24615;&#20540;&#30340;&#20004;&#20004;&#27604;&#36739;&#65292;&#21487;&#20197;&#20135;&#29983;&#25490;&#24207;&#21644;&#21516;&#26102;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#19988;&#21487;&#20197;&#36873;&#25321;&#21069;k&#20010;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#35299;&#37322;&#36890;&#24120;&#20381;&#36182;&#20110;&#29305;&#24449;&#30340;&#30456;&#23545;&#39034;&#24207;&#32780;&#19981;&#26159;&#25968;&#20540;&#26412;&#36523;&#65292;&#20063;&#23601;&#26159;&#25490;&#24207;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#37325;&#35201;&#24615;&#20540;&#26102;&#20351;&#29992;&#30340;&#26679;&#26412;&#37327;&#36739;&#23567;&#65292;&#25490;&#24207;&#21487;&#33021;&#19981;&#31283;&#23450;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#21518;&#37325;&#35201;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#20135;&#29983;&#19968;&#31181;&#25490;&#24207;&#21644;&#21516;&#26102;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#22522;&#20110;&#29305;&#24449;&#37325;&#35201;&#24615;&#20540;&#30340;&#20004;&#20004;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20445;&#35777;&#39640;&#27010;&#29575;&#21253;&#21547;&#8220;&#30495;&#23454;&#8221;&#65288;&#26080;&#38480;&#26679;&#26412;&#65289;&#25490;&#24207;&#65292;&#24182;&#20801;&#35768;&#36873;&#25321;&#21069;k&#20010;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretation of feature importance values often relies on the relative order of the features rather than on the value itself, referred to as ranking. However, the order may be unstable due to the small sample sizes used in calculating the importance values. We propose that post-hoc importance methods produce a ranking and simultaneous confident intervals for the rankings. Based on pairwise comparisons of the feature importance values, our method is guaranteed to include the ``true'' (infinite sample) ranking with high probability and allows for selecting top-k sets.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20984;&#20998;&#26512;&#21644;ODE&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#32534;&#30721;&#38750;&#25193;&#24352;&#31639;&#23376;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#32422;&#26463;&#26435;&#37325;&#30340;&#35889;&#33539;&#25968;&#26469;&#38480;&#21046;Lipschitz&#24120;&#25968;&#30340;&#22686;&#38271;&#12290;&#27492;&#26550;&#26500;&#36824;&#21487;&#20197;&#34987;&#24212;&#29992;&#20110;&#23398;&#20064;&#21435;&#22122;&#22120;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#26469;&#20445;&#35777;&#24615;&#33021;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2306.17332</link><description>&lt;p&gt;
&#20351;&#29992;&#20984;&#20998;&#26512;&#21644;ODE&#35774;&#35745;&#31283;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Designing Stable Neural Networks using Convex Analysis and ODEs. (arXiv:2306.17332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17332
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20984;&#20998;&#26512;&#21644;ODE&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#32534;&#30721;&#38750;&#25193;&#24352;&#31639;&#23376;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#32422;&#26463;&#26435;&#37325;&#30340;&#35889;&#33539;&#25968;&#26469;&#38480;&#21046;Lipschitz&#24120;&#25968;&#30340;&#22686;&#38271;&#12290;&#27492;&#26550;&#26500;&#36824;&#21487;&#20197;&#34987;&#24212;&#29992;&#20110;&#23398;&#20064;&#21435;&#22122;&#22120;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#26469;&#20445;&#35777;&#24615;&#33021;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#36896;&#20102;&#19968;&#31181;&#22522;&#20110;ResNet&#39118;&#26684;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#32534;&#30721;&#38750;&#25193;&#24352;&#65288;1-Lipschitz&#65289;&#31639;&#23376;&#65292;&#21482;&#35201;&#26435;&#37325;&#30340;&#35889;&#33539;&#25968;&#21463;&#21040;&#36866;&#24403;&#30340;&#32422;&#26463;&#12290;&#19982;&#20256;&#32479;&#30340;ResNet&#26550;&#26500;&#30456;&#27604;&#65292;&#21363;&#20351;&#26435;&#37325;&#30340;&#35889;&#33539;&#25968;&#21463;&#21040;&#32422;&#26463;&#65292;&#20854;Lipschitz&#24120;&#25968;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#20063;&#20250;&#38543;&#32593;&#32476;&#30340;&#28145;&#24230;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#32422;&#26463;&#26435;&#37325;&#30340;&#35889;&#33539;&#25968;&#65292;&#20197;&#30830;&#20445;&#32593;&#32476;&#26159;&#19968;&#20010;&#24179;&#22343;&#31639;&#23376;&#65292;&#20351;&#20854;&#25104;&#20026;Plug-and-Play&#31639;&#27861;&#20013;&#30340;&#23398;&#20064;&#21435;&#22122;&#22120;&#30340;&#33258;&#28982;&#20505;&#36873;&#12290;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#26041;&#24335;&#26469;&#24378;&#21046;&#35889;&#33539;&#25968;&#32422;&#26463;&#65292;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#22312;&#36825;&#20123;&#32422;&#26463;&#26465;&#20214;&#19979;&#65292;&#20063;&#21487;&#20197;&#35757;&#32451;&#20986;&#24615;&#33021;&#20248;&#36234;&#30340;&#32593;&#32476;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#24212;&#29992;&#20110;&#23545;&#25239;&#24615;&#31283;&#20581;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by classical work on the numerical integration of ordinary differential equations we present a ResNet-styled neural network architecture that encodes non-expansive (1-Lipschitz) operators, as long as the spectral norms of the weights are appropriately constrained. This is to be contrasted with the ordinary ResNet architecture which, even if the spectral norms of the weights are constrained, has a Lipschitz constant that, in the worst case, grows exponentially with the depth of the network. Further analysis of the proposed architecture shows that the spectral norms of the weights can be further constrained to ensure that the network is an averaged operator, making it a natural candidate for a learned denoiser in Plug-and-Play algorithms. Using a novel adaptive way of enforcing the spectral norm constraints, we show that, even with these constraints, it is possible to train performant networks. The proposed architecture is applied to the problem of adversarially robust image cl
&lt;/p&gt;</description></item><item><title>&#38754;&#21521;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#30340;&#38598;&#25104;&#24863;&#30693;-&#36890;&#20449;-&#35745;&#31639;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#20197;&#21450;&#23454;&#29616;&#36793;&#32536;&#23398;&#20064;&#21644;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#25512;&#29702;&#20219;&#21153;&#30340;&#23450;&#21046;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.01162</link><description>&lt;p&gt;
&#38754;&#21521;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#30340;&#38598;&#25104;&#24863;&#30693;-&#36890;&#20449;-&#35745;&#31639; &#65288;Integrated Sensing-Communication-Computation&#65289; &#65288;arXiv&#65306;2306.01162v1 [cs.IT]&#65289;
&lt;/p&gt;
&lt;p&gt;
Integrated Sensing-Communication-Computation for Edge Artificial Intelligence. (arXiv:2306.01162v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01162
&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#30340;&#38598;&#25104;&#24863;&#30693;-&#36890;&#20449;-&#35745;&#31639;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#20197;&#21450;&#23454;&#29616;&#36793;&#32536;&#23398;&#20064;&#21644;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#25512;&#29702;&#20219;&#21153;&#30340;&#23450;&#21046;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#26159;&#23454;&#29616;&#19975;&#29289;&#26234;&#33021;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#29992;&#20110;&#25968;&#23383;&#23402;&#29983;&#12289;&#20840;&#24687;&#25237;&#24433;&#12289;&#35821;&#20041;&#36890;&#20449;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#39640;&#32423;&#25216;&#26415;&#12290;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#36793;&#32536;&#23398;&#20064;&#21644;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#25512;&#29702;&#65292;&#21462;&#20915;&#20110;&#19977;&#20010;&#39640;&#24230;&#32806;&#21512;&#30340;&#36807;&#31243;&#30340;&#36136;&#37327;&#65292;&#21363;&#25968;&#25454;&#33719;&#21462;&#30340;&#24863;&#30693;&#12289;&#20449;&#24687;&#25552;&#21462;&#30340;&#35745;&#31639;&#21644;&#20449;&#24687;&#20256;&#36755;&#30340;&#36890;&#20449;&#12290;&#28982;&#32780;&#65292;&#36825;&#19977;&#20010;&#27169;&#22359;&#38656;&#35201;&#20026;&#22686;&#24378;&#33258;&#24049;&#30340;&#26381;&#21153;&#36136;&#37327;&#32780;&#31454;&#20105;&#32593;&#32476;&#36164;&#28304;&#12290;&#20026;&#27492;&#65292;&#38598;&#25104;&#24863;&#30693;-&#36890;&#20449;-&#35745;&#31639;&#65288;ISCC&#65289;&#23545;&#20110;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#20197;&#21450;&#23454;&#29616;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#30340;&#23450;&#21046;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#30740;&#31350;&#19977;&#20010;&#27169;&#22359;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21508;&#31181; ISCC &#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#20219;&#21153;&#21644;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge artificial intelligence (AI) has been a promising solution towards 6G to empower a series of advanced techniques such as digital twin, holographic projection, semantic communications, and auto-driving, for achieving intelligence of everything. The performance of edge AI tasks, including edge learning and edge AI inference, depends on the quality of three highly coupled processes, i.e., sensing for data acquisition, computation for information extraction, and communication for information transmission. However, these three modules need to compete for network resources for enhancing their own quality-of-services. To this end, integrated sensing-communication-computation (ISCC) is of paramount significance for improving resource utilization as well as achieving the customized goals of edge AI tasks. By investigating the interplay among the three modules, this article presents various kinds of ISCC schemes for federated edge learning tasks and edge AI inference tasks in both applicati
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.11527</link><description>&lt;p&gt;
InstructIE: &#19968;&#20221;&#22522;&#20110;&#25351;&#20196;&#30340;&#20013;&#25991;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
InstructIE: A Chinese Instruction-based Information Extraction Dataset. (arXiv:2305.11527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11527
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#31216;&#20026;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462; (Instruction-based IE)&#65292;&#23427;&#26088;&#22312;&#35201;&#27714;&#31995;&#32479;&#36981;&#24490;&#29305;&#23450;&#30340;&#25351;&#20196;&#25110;&#25351;&#21335;&#26469;&#25552;&#21462;&#20449;&#24687;&#12290;&#20026;&#20102;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#20013;&#25991;&#32500;&#22522;&#30334;&#31185;&#30340; 270,000 &#20010;&#24369;&#30417;&#30563;&#25968;&#25454;&#21644; 1,000 &#20010;&#39640;&#36136;&#37327;&#20247;&#21253;&#27880;&#37322;&#23454;&#20363;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#22312;InstructIE&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#24456;&#26377;&#24076;&#26395;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26696;&#20363;&#30740;&#31350;&#20998;&#26512;&#65292;&#24378;&#35843;&#20102;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312; https://github.com/zjunlp/DeepKE/tree/main/example/llm &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new Information Extraction (IE) task dubbed Instruction-based IE, which aims to ask the system to follow specific instructions or guidelines to extract information. To facilitate research in this area, we construct a dataset called InstructIE, consisting of 270,000 weakly supervised data from Chinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We further evaluate the performance of various baseline models on the InstructIE dataset. The results reveal that although current models exhibit promising performance, there is still room for improvement. Furthermore, we conduct a comprehensive case study analysis, underlining the challenges inherent in the Instruction-based IE task. Code and dataset are available at https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>Wav2code&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;ASR&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#30340;&#26080;&#22833;&#30495;&#22686;&#24378;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#35821;&#38899;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.04974</link><description>&lt;p&gt;
Wav2code&#65306;&#36890;&#36807;&#30721;&#26412;&#26597;&#25214;&#24674;&#22797;&#24178;&#20928;&#30340;&#35821;&#38899;&#34920;&#24449;&#65292;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#30340;ASR
&lt;/p&gt;
&lt;p&gt;
Wav2code: Restore Clean Speech Representations via Codebook Lookup for Noise-Robust ASR. (arXiv:2304.04974v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04974
&lt;/p&gt;
&lt;p&gt;
Wav2code&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;ASR&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#30340;&#26080;&#22833;&#30495;&#22686;&#24378;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#35821;&#38899;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#23454;&#38469;&#22024;&#26434;&#29615;&#22659;&#19979;&#65292;&#20854;&#24615;&#33021;&#36890;&#24120;&#20250;&#26174;&#33879;&#38477;&#20302;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#35821;&#38899;&#22686;&#24378;(SE)&#24341;&#20837;&#20316;&#20026;&#21069;&#31471;&#26469;&#25552;&#39640;&#35821;&#38899;&#36136;&#37327;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#30001;&#20110;&#35821;&#38899;&#22833;&#30495;&#38382;&#39064;&#65292;&#21487;&#33021;&#23545;&#19979;&#28216;ASR&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22522;&#20110;&#36825;&#19968;&#28857;&#65292;&#26368;&#26032;&#30340;&#24037;&#20316;&#23558;SE&#21644;&#24403;&#21069;&#27969;&#34892;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;(SSL)&#32467;&#21512;&#36215;&#26469;&#26469;&#32531;&#35299;&#22833;&#30495;&#38382;&#39064;&#24182;&#25552;&#39640;&#22122;&#22768;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#26377;&#25928;&#24615;&#65292;&#20294;&#20256;&#32479;SE&#24341;&#36215;&#30340;&#35821;&#38899;&#22833;&#30495;&#20173;&#26080;&#27861;&#23436;&#20840;&#28040;&#38500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Wav2code&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;ASR&#30340;&#26080;&#22833;&#30495;&#24191;&#20041;SE&#12290;&#39318;&#20808;&#65292;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#20174;SSL&#27169;&#22411;&#33719;&#24471;&#24178;&#20928;&#30340;&#35821;&#38899;&#34920;&#24449;&#65292;&#36890;&#36807;&#26368;&#36817;&#37051;&#29305;&#24449;&#21305;&#37197;&#26597;&#25214;&#31163;&#25955;&#30721;&#26412;&#65292;&#28982;&#21518;&#21033;&#29992;&#24471;&#21040;&#30340;&#20195;&#30721;&#24207;&#21015;&#26469;&#37325;&#26500;&#21407;&#22987;&#38899;&#39057;&#20449;&#21495;&#20197;&#33719;&#24471;&#24178;&#20928;&#30340;&#35821;&#38899;&#34920;&#24449;&#65307;&#25509;&#30528;&#65292;&#35813;&#20195;&#30721;&#24207;&#21015;&#34987;&#29992;&#20110;&#26080;&#22833;&#30495;&#22320;&#22686;&#24378;&#24102;&#22122;&#35821;&#38899;&#20197;&#20415;&#20110;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) has gained a remarkable success thanks to recent advances of deep learning, but it usually degrades significantly under real-world noisy conditions. Recent works introduce speech enhancement (SE) as front-end to improve speech quality, which is proved effective but may not be optimal for downstream ASR due to speech distortion problem. Based on that, latest works combine SE and currently popular self-supervised learning (SSL) to alleviate distortion and improve noise robustness. Despite the effectiveness, the speech distortion caused by conventional SE still cannot be completely eliminated. In this paper, we propose a self-supervised framework named Wav2code to implement a generalized SE without distortions for noise-robust ASR. First, in pre-training stage the clean speech representations from SSL model are sent to lookup a discrete codebook via nearest-neighbor feature matching, the resulted code sequence are then exploited to reconstruct the origin
&lt;/p&gt;</description></item><item><title>DeforestVis&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#36890;&#36807;&#25552;&#20379;&#20195;&#29702;&#20915;&#31574;&#26641;&#65292;&#24635;&#32467;&#20102;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25506;&#32034;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.00133</link><description>&lt;p&gt;
DeforestVis&#65306;&#20351;&#29992;&#20195;&#29702;&#20915;&#31574;&#26641;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34892;&#20026;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate Decision Stumps. (arXiv:2304.00133v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00133
&lt;/p&gt;
&lt;p&gt;
DeforestVis&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#36890;&#36807;&#25552;&#20379;&#20195;&#29702;&#20915;&#31574;&#26641;&#65292;&#24635;&#32467;&#20102;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25506;&#32034;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#20197;&#21450;&#19981;&#21516;&#65288;&#21644;&#20851;&#38190;&#65289;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#22686;&#21152;&#65292;&#36234;&#26469;&#36234;&#38656;&#35201;&#26356;&#26131;&#35299;&#37322;&#21644;&#21487;&#20449;&#36182;&#30340;ML&#12290;&#35299;&#37322;&#22797;&#26434;ML&#27169;&#22411;&#30340;&#19968;&#31181;&#31616;&#21333;&#19988;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#26159;&#35757;&#32451;&#20195;&#29702;&#27169;&#22411;&#65288;&#20363;&#22914;&#35268;&#21017;&#38598;&#21644;&#20915;&#31574;&#26641;&#65289;&#65292;&#20197;&#36275;&#22815;&#25509;&#36817;&#21407;&#22987;&#27169;&#22411;&#65292;&#20294;&#26356;&#31616;&#21333;&#21644;&#26131;&#20110;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#35268;&#21017;&#38598;&#21487;&#20197;&#21464;&#24471;&#38750;&#24120;&#20887;&#38271;&#65292;&#21253;&#21547;&#35768;&#22810;if-else&#35821;&#21477;&#65292;&#32780;&#20915;&#31574;&#26641;&#30340;&#28145;&#24230;&#20250;&#38543;&#30528;&#20934;&#30830;&#27169;&#25311;&#22797;&#26434;ML&#27169;&#22411;&#32780;&#36805;&#36895;&#22686;&#21152;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#20854;&#26680;&#24515;&#30446;&#26631;&#65292;&#25552;&#20379;&#29992;&#25143;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;DeforestVis&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#36890;&#36807;&#25552;&#20379;&#20351;&#29992;&#33258;&#36866;&#24212;&#22686;&#24378;&#65288;AdaBoost&#65289;&#25216;&#26415;&#29983;&#25104;&#30340;&#20195;&#29702;&#20915;&#31574;&#26641;&#65288;&#19968;&#32423;&#20915;&#31574;&#26641;&#65289;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#23545;&#22797;&#26434;ML&#27169;&#22411;&#34892;&#20026;&#30340;&#21451;&#22909;&#24635;&#32467;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#24110;&#21161;&#29992;&#25143;&#25506;&#32034;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the complexity of machine learning (ML) models increases and the applications in different (and critical) domains grow, there is a strong demand for more interpretable and trustworthy ML. One straightforward and model-agnostic way to interpret complex ML models is to train surrogate models, such as rule sets and decision trees, that sufficiently approximate the original ones while being simpler and easier-to-explain. Yet, rule sets can become very lengthy, with many if-else statements, and decision tree depth grows rapidly when accurately emulating complex ML models. In such cases, both approaches can fail to meet their core goal, providing users with model interpretability. We tackle this by proposing DeforestVis, a visual analytics tool that offers user-friendly summarization of the behavior of complex ML models by providing surrogate decision stumps (one-level decision trees) generated with the adaptive boosting (AdaBoost) technique. Our solution helps users to explore the comple
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#32463;&#39564;&#39118;&#38505;&#30340;&#19979;&#38477;&#36895;&#29575;&#26159;&#38750;&#21333;&#35843;&#30340;&#12290;&#22312;&#20998;&#24067;&#31526;&#21512;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#39640;&#32500;&#23485;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#29575;&#21442;&#25968;&#21270;&#28165;&#26224;&#30340;&#38454;&#27573;&#36716;&#25442;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#20026;&#26089;&#26399;&#23398;&#20064;&#26102;&#25152;&#23398;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.00055</link><description>&lt;p&gt;
&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#26102;&#38388;&#23610;&#24230;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning time-scales in two-layers neural networks. (arXiv:2303.00055v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#32463;&#39564;&#39118;&#38505;&#30340;&#19979;&#38477;&#36895;&#29575;&#26159;&#38750;&#21333;&#35843;&#30340;&#12290;&#22312;&#20998;&#24067;&#31526;&#21512;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#39640;&#32500;&#23485;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#29575;&#21442;&#25968;&#21270;&#28165;&#26224;&#30340;&#38454;&#27573;&#36716;&#25442;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#20026;&#26089;&#26399;&#23398;&#20064;&#26102;&#25152;&#23398;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20855;&#26377;&#22810;&#20010;&#24341;&#20154;&#27880;&#24847;&#30340;&#29305;&#28857;&#12290;&#23588;&#20854;&#26159;&#65292;&#22312;&#22823;&#25209;&#37327;&#25968;&#25454;&#24179;&#22343;&#21518;&#65292;&#32463;&#39564;&#39118;&#38505;&#30340;&#19979;&#38477;&#36895;&#29575;&#26159;&#38750;&#21333;&#35843;&#30340;&#12290;&#20960;&#20046;&#27809;&#26377;&#36827;&#23637;&#30340;&#38271;&#21608;&#26399;&#21644;&#24555;&#36895;&#19979;&#38477;&#30340;&#38388;&#38548;&#20132;&#26367;&#20986;&#29616;&#12290;&#36825;&#20123;&#36830;&#32493;&#30340;&#23398;&#20064;&#38454;&#27573;&#24448;&#24448;&#22312;&#38750;&#24120;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#19978;&#36827;&#34892;&#12290;&#26368;&#21518;&#65292;&#22312;&#26089;&#26399;&#38454;&#27573;&#23398;&#20064;&#30340;&#27169;&#22411;&#36890;&#24120;&#26159;&#8220;&#31616;&#21333;&#30340;&#8221;&#25110;&#8220;&#26131;&#20110;&#23398;&#20064;&#30340;&#8221;&#65292;&#23613;&#31649;&#20197;&#38590;&#20197;&#24418;&#24335;&#21270;&#30340;&#26041;&#24335;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#31526;&#21512;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#39640;&#32500;&#23485;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#65292;&#22312;&#19968;&#31995;&#21015;&#26032;&#30340;&#20005;&#23494;&#32467;&#26524;&#12289;&#38750;&#20005;&#23494;&#25968;&#23398;&#25512;&#23548;&#21644;&#25968;&#20540;&#23454;&#39564;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20379;&#20102;&#23545;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#29305;&#21035;&#25351;&#20986;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#29575;&#21442;&#25968;&#21270;&#28165;&#26224;&#30340;&#38454;&#27573;&#36716;&#25442;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#38271;&#21608;&#26399;&#30340;&#20986;&#29616;&#21644;&#28040;&#22833;&#26377;&#20851;&#12290;&#25105;&#20204;&#36824;&#20026;&#26089;&#26399;&#23398;&#20064;&#26102;&#25152;&#23398;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#35268;&#33539;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-based learning in multi-layer neural networks displays a number of striking features. In particular, the decrease rate of empirical risk is non-monotone even after averaging over large batches. Long plateaus in which one observes barely any progress alternate with intervals of rapid decrease. These successive phases of learning often take place on very different time scales. Finally, models learnt in an early phase are typically `simpler' or `easier to learn' although in a way that is difficult to formalize.  Although theoretical explanations of these phenomena have been put forward, each of them captures at best certain specific regimes. In this paper, we study the gradient flow dynamics of a wide two-layer neural network in high-dimension, when data are distributed according to a single-index model (i.e., the target function depends on a one-dimensional projection of the covariates). Based on a mixture of new rigorous results, non-rigorous mathematical derivations, and numer
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21333;&#27425;&#32463;&#39564;&#38544;&#31169;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#36827;&#34892;&#38544;&#31169;&#25439;&#22833;&#23457;&#35745;&#65292;&#19988;&#26080;&#38656;&#20107;&#20808;&#20102;&#35299;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#25110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#36866;&#29992;&#20110;&#22312;&#23454;&#36341;&#20013;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;</title><link>http://arxiv.org/abs/2302.03098</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21333;&#27425;&#32463;&#39564;&#38544;&#31169;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One-shot Empirical Privacy Estimation for Federated Learning. (arXiv:2302.03098v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21333;&#27425;&#32463;&#39564;&#38544;&#31169;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#36827;&#34892;&#38544;&#31169;&#25439;&#22833;&#23457;&#35745;&#65292;&#19988;&#26080;&#38656;&#20107;&#20808;&#20102;&#35299;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#25110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#36866;&#29992;&#20110;&#22312;&#23454;&#36341;&#20013;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;ially private&#65288;DP&#65289;&#31639;&#27861;&#30340;&#38544;&#31169;&#20272;&#35745;&#25216;&#26415;&#21487;&#29992;&#20110;&#19982;&#20998;&#26512;&#19978;&#30028;&#36827;&#34892;&#27604;&#36739;&#65292;&#25110;&#22312;&#24050;&#30693;&#20998;&#26512;&#19978;&#30028;&#19981;&#32039;&#30340;&#24773;&#20917;&#19979;&#23454;&#39564;&#27979;&#37327;&#38544;&#31169;&#25439;&#22833;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#38544;&#31169;&#23457;&#35745;&#25216;&#26415;&#36890;&#24120;&#23545;&#23545;&#25163;&#20570;&#20986;&#24378;&#28872;&#20551;&#35774;&#65288;&#20363;&#22914;&#65292;&#20102;&#35299;&#20013;&#38388;&#27169;&#22411;&#36845;&#20195;&#25110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65289;&#65292;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#21644;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#35843;&#25972;&#65292;&#24182;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#22810;&#27425;&#65288;&#36890;&#24120;&#25968;&#37327;&#32423;&#20026;&#25968;&#21315;&#65289;&#12290;&#36825;&#20123;&#32570;&#28857;&#20351;&#24471;&#22312;&#23454;&#36341;&#20013;&#38590;&#20197;&#22823;&#35268;&#27169;&#37096;&#32626;&#27492;&#31867;&#25216;&#26415;&#65292;&#23588;&#20854;&#26159;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#65292;&#27169;&#22411;&#35757;&#32451;&#21487;&#33021;&#38656;&#35201;&#25968;&#22825;&#25110;&#25968;&#21608;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#21333;&#27425;&#8221;&#26041;&#27861;&#65292;&#21487;&#20197;&#31995;&#32479;&#22320;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#21333;&#20010;&#35757;&#32451;&#36816;&#34892;&#26399;&#38388;&#39640;&#25928;&#22320;&#23457;&#35745;&#25110;&#20272;&#35745;&#27169;&#22411;&#30340;&#38544;&#31169;&#25439;&#22833;&#65292;&#32780;&#19981;&#38656;&#35201;&#20107;&#20808;&#20102;&#35299;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#25110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#31561;&#35774;&#32622;&#20013;&#20351;&#29992;&#30340;&#19968;&#33324;DP&#31639;&#27861;&#65292;&#24182;&#30001;&#23454;&#39564;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20854;&#25552;&#20379;&#30340;&#20934;&#30830;&#38544;&#31169;&#25439;&#22833;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy estimation techniques for differentially private (DP) algorithms are useful for comparing against analytical bounds, or to empirically measure privacy loss in settings where known analytical bounds are not tight. However, existing privacy auditing techniques usually make strong assumptions on the adversary (e.g., knowledge of intermediate model iterates or the training data distribution), are tailored to specific tasks and model architectures, and require retraining the model many times (typically on the order of thousands). These shortcomings make deploying such techniques at scale difficult in practice, especially in federated settings where model training can take days or weeks. In this work, we present a novel "one-shot" approach that can systematically address these challenges, allowing efficient auditing or estimation of the privacy loss of a model during the same, single training run used to fit model parameters, and without requiring any a priori knowledge about the mod
&lt;/p&gt;</description></item></channel></rss>