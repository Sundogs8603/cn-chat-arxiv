<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>SHARCS&#26159;&#19968;&#31181;&#35299;&#37322;&#24615;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#21644;&#26144;&#23556;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#27010;&#24565;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#35299;&#37322;&#24615;&#30340;&#20219;&#21153;&#39044;&#27979;&#21644;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00316</link><description>&lt;p&gt;
SHARCS: &#35299;&#37322;&#24615;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#20849;&#20139;&#27010;&#24565;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
SHARCS: Shared Concept Space for Explainable Multimodal Learning. (arXiv:2307.00316v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00316
&lt;/p&gt;
&lt;p&gt;
SHARCS&#26159;&#19968;&#31181;&#35299;&#37322;&#24615;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#21644;&#26144;&#23556;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#27010;&#24565;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#35299;&#37322;&#24615;&#30340;&#20219;&#21153;&#39044;&#27979;&#21644;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#26159;&#35299;&#20915;&#22797;&#26434;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#37325;&#35201;&#33539;&#24335;&#65292;&#22312;&#36825;&#31181;&#38382;&#39064;&#20013;&#65292;&#21333;&#20010;&#25968;&#25454;&#27169;&#24577;&#36890;&#24120;&#26080;&#27861;&#20934;&#30830;&#35299;&#20915;&#32473;&#23450;&#30340;&#24314;&#27169;&#20219;&#21153;&#12290;&#34429;&#28982;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#23427;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#36890;&#24120;&#26159;&#19981;&#36879;&#26126;&#30340;&#65292;&#38480;&#21046;&#20102;&#21407;&#21017;&#24615;&#30340;&#21487;&#35299;&#37322;&#36328;&#27169;&#24577;&#20998;&#26512;&#21644;&#39046;&#22495;&#19987;&#23478;&#24178;&#39044;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SHARCS&#65288;SHARed Concept Space&#65289;&#8212;&#8212;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#24615;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#26032;&#27010;&#24565;&#26041;&#27861;&#12290;SHARCS&#20174;&#19981;&#21516;&#30340;&#24322;&#26500;&#27169;&#24577;&#20013;&#23398;&#20064;&#21644;&#26144;&#23556;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#27010;&#24565;&#27969;&#24418;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#35821;&#20041;&#30456;&#20284;&#30340;&#36328;&#27169;&#24577;&#27010;&#24565;&#30340;&#30452;&#35266;&#25237;&#24433;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23548;&#33268;&#22266;&#26377;&#21487;&#35299;&#37322;&#30340;&#20219;&#21153;&#39044;&#27979;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#19979;&#28216;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;SHARCS&#21487;&#20197;&#36816;&#34892;&#24182;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning is an essential paradigm for addressing complex real-world problems, where individual data modalities are typically insufficient to accurately solve a given modelling task. While various deep learning approaches have successfully addressed these challenges, their reasoning process is often opaque; limiting the capabilities for a principled explainable cross-modal analysis and any domain-expert intervention. In this paper, we introduce SHARCS (SHARed Concept Space) -- a novel concept-based approach for explainable multimodal learning. SHARCS learns and maps interpretable concepts from different heterogeneous modalities into a single unified concept-manifold, which leads to an intuitive projection of semantically similar cross-modal concepts. We demonstrate that such an approach can lead to inherently explainable task predictions while also improving downstream predictive performance. Moreover, we show that SHARCS can operate and significantly outperform other approac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;DP-SGD&#20998;&#26512;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#35768;&#22810;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#28431;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.00310</link><description>&lt;p&gt;
&#26799;&#24230;&#30456;&#20284;&#65306;&#25935;&#24863;&#24230;&#32463;&#24120;&#34987;&#36807;&#39640;&#20272;&#35745;&#22312;DP-SGD&#20013;
&lt;/p&gt;
&lt;p&gt;
Gradients Look Alike: Sensitivity is Often Overestimated in DP-SGD. (arXiv:2307.00310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;DP-SGD&#20998;&#26512;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#35768;&#22810;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#28431;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#26159;&#31169;&#26377;&#28145;&#24230;&#23398;&#20064;&#30340;&#26631;&#20934;&#31639;&#27861;&#12290;&#34429;&#28982;&#24050;&#30693;&#20854;&#38544;&#31169;&#20998;&#26512;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#26159;&#32039;&#23494;&#30340;&#65292;&#20294;&#26159;&#19968;&#20123;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#23545;&#35768;&#22810;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#28431;&#26174;&#33879;&#20943;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;DP-SGD&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#25429;&#25417;&#21040;&#22312;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#30456;&#20284;&#37051;&#23621;&#30340;&#28857;&#20139;&#21463;&#26356;&#22909;&#38544;&#31169;&#24615;&#30340;&#30452;&#35273;&#12290;&#24418;&#24335;&#19978;&#26469;&#35828;&#65292;&#36825;&#26159;&#36890;&#36807;&#20462;&#25913;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#35745;&#31639;&#24471;&#21040;&#30340;&#27169;&#22411;&#26356;&#26032;&#30340;&#27599;&#27493;&#38544;&#31169;&#24615;&#20998;&#26512;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#32452;&#21512;&#23450;&#29702;&#65292;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20010;&#26032;&#30340;&#27599;&#27493;&#20998;&#26512;&#26469;&#25512;&#29702;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#12290;&#24635;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#39062;&#30340;DP-SGD&#20998;&#26512;&#20351;&#25105;&#20204;&#33021;&#22815;&#27491;&#24335;&#22320;&#26174;&#31034;DP-SGD&#23545;&#35768;&#22810;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#28431;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private stochastic gradient descent (DP-SGD) is the canonical algorithm for private deep learning. While it is known that its privacy analysis is tight in the worst-case, several empirical results suggest that when training on common benchmark datasets, the models obtained leak significantly less privacy for many datapoints. In this paper, we develop a new analysis for DP-SGD that captures the intuition that points with similar neighbors in the dataset enjoy better privacy than outliers. Formally, this is done by modifying the per-step privacy analysis of DP-SGD to introduce a dependence on the distribution of model updates computed from a training dataset. We further develop a new composition theorem to effectively use this new per-step analysis to reason about an entire training run. Put all together, our evaluation shows that this novel DP-SGD analysis allows us to now formally show that DP-SGD leaks significantly less privacy for many datapoints. In particular, we ob
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#24635;&#32467;&#20102;&#22312;&#28857;&#20113;&#20998;&#31867;&#20013;&#38024;&#23545;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#30340;&#24403;&#21069;&#36827;&#23637;&#65292;&#20171;&#32461;&#20102;&#23545;&#25239;&#25915;&#20987;&#21407;&#29702;&#21644;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#21450;&#38450;&#24481;&#31574;&#30053;&#20998;&#31867;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.00309</link><description>&lt;p&gt;
&#23545;3D&#28857;&#20113;&#20998;&#31867;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks and Defenses on 3D Point Cloud Classification: A Survey. (arXiv:2307.00309v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00309
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#24635;&#32467;&#20102;&#22312;&#28857;&#20113;&#20998;&#31867;&#20013;&#38024;&#23545;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#30340;&#24403;&#21069;&#36827;&#23637;&#65292;&#20171;&#32461;&#20102;&#23545;&#25239;&#25915;&#20987;&#21407;&#29702;&#21644;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#21450;&#38450;&#24481;&#31574;&#30053;&#20998;&#31867;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;2D&#35270;&#35273;&#39046;&#22495;&#24050;&#32463;&#25104;&#21151;&#35299;&#20915;&#20102;&#21508;&#31181;&#20219;&#21153;&#65292;&#25104;&#20026;&#20027;&#27969;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#12290;&#26368;&#36817;&#65292;&#23545;3D&#28857;&#20113;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#35813;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#19978;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#65292;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#21364;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#25915;&#20987;&#22312;&#20154;&#30524;&#20013;&#26159;&#26080;&#27861;&#23519;&#35273;&#30340;&#65292;&#20294;&#21364;&#33021;&#36731;&#26131;&#27450;&#39575;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#27979;&#35797;&#21644;&#37096;&#32626;&#38454;&#27573;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#20851;&#20110;&#28857;&#20113;&#20998;&#31867;&#20013;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#30340;&#24403;&#21069;&#36827;&#23637;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#23545;&#25239;&#25915;&#20987;&#30340;&#21407;&#29702;&#21644;&#29305;&#28857;&#65292;&#24635;&#32467;&#24182;&#20998;&#26512;&#20102;&#36817;&#24180;&#26469;&#30340;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#23427;&#23558;&#38450;&#24481;&#31574;&#30053;&#20998;&#20026;&#36755;&#20837;&#36716;&#25442;&#65292;&#25968;&#25454;&#20248;&#21270;&#21644;&#28145;&#24230;&#27169;&#22411;&#20462;&#25913;&#19977;&#31867;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#35813;&#39046;&#22495;&#20013;&#20960;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has successfully solved a wide range of tasks in 2D vision as a dominant AI technique. Recently, deep learning on 3D point clouds is becoming increasingly popular for addressing various tasks in this field. Despite remarkable achievements, deep learning algorithms are vulnerable to adversarial attacks. These attacks are imperceptible to the human eye but can easily fool deep neural networks in the testing and deployment stage. To encourage future research, this survey summarizes the current progress on adversarial attack and defense techniques on point cloud classification. This paper first introduces the principles and characteristics of adversarial attacks and summarizes and analyzes the adversarial example generation methods in recent years. Besides, it classifies defense strategies as input transformation, data optimization, and deep model modification. Finally, it presents several challenging issues and future research directions in this domain.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21483;&#20570;SyMFM6D&#30340;&#38754;&#21521;&#23545;&#31216;&#22810;&#35270;&#35282;&#34701;&#21512;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#26041;&#21521;&#34701;&#21512;&#32593;&#32476;&#26377;&#25928;&#22320;&#34701;&#21512;&#22810;&#20010;&#35270;&#35282;&#30340;RGB-D&#24103;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;&#20851;&#38190;&#28857;&#21644;&#23454;&#20363;&#35821;&#20041;&#20998;&#21106;&#26469;&#35745;&#31639;6D&#23039;&#24577;&#12290;&#36890;&#36807;&#26032;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#30446;&#26631;&#20989;&#25968;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#23545;&#31216;&#29289;&#20307;&#30340;&#27495;&#20041;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#35270;&#35282;&#21644;&#22810;&#35270;&#35282;&#23039;&#24577;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.00306</link><description>&lt;p&gt;
SyMFM6D&#65306;&#38754;&#21521;&#23545;&#31216;&#22810;&#26041;&#20301;&#34701;&#21512;&#30340;&#22810;&#35270;&#35282;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
SyMFM6D: Symmetry-aware Multi-directional Fusion for Multi-View 6D Object Pose Estimation. (arXiv:2307.00306v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00306
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21483;&#20570;SyMFM6D&#30340;&#38754;&#21521;&#23545;&#31216;&#22810;&#35270;&#35282;&#34701;&#21512;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#26041;&#21521;&#34701;&#21512;&#32593;&#32476;&#26377;&#25928;&#22320;&#34701;&#21512;&#22810;&#20010;&#35270;&#35282;&#30340;RGB-D&#24103;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;&#20851;&#38190;&#28857;&#21644;&#23454;&#20363;&#35821;&#20041;&#20998;&#21106;&#26469;&#35745;&#31639;6D&#23039;&#24577;&#12290;&#36890;&#36807;&#26032;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#30446;&#26631;&#20989;&#25968;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#23545;&#31216;&#29289;&#20307;&#30340;&#27495;&#20041;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#35270;&#35282;&#21644;&#22810;&#35270;&#35282;&#23039;&#24577;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#29289;&#20307;&#24182;&#20272;&#35745;&#20854;6D&#23039;&#24577;&#23545;&#20110;&#33258;&#21160;&#21270;&#31995;&#32479;&#19982;&#29615;&#22659;&#23433;&#20840;&#20114;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;6D&#23039;&#24577;&#20272;&#35745;&#22120;&#20165;&#20381;&#36182;&#20110;&#21333;&#20010;&#25668;&#20687;&#22836;&#24103;&#65292;&#24182;&#19988;&#21463;&#21040;&#30001;&#20110;&#29289;&#20307;&#23545;&#31216;&#24615;&#32780;&#24341;&#36215;&#30340;&#36974;&#25377;&#21644;&#27169;&#31946;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#23545;&#31216;&#22810;&#35270;&#35282;6D&#23039;&#24577;&#20272;&#35745;&#22120;SyMFM6D&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#22810;&#26041;&#21521;&#34701;&#21512;&#32593;&#32476;&#26377;&#25928;&#22320;&#34701;&#21512;&#22810;&#20010;&#35282;&#24230;&#30340;RGB-D&#24103;&#65292;&#24182;&#21516;&#26102;&#39044;&#27979;&#22330;&#26223;&#20013;&#25152;&#26377;&#29289;&#20307;&#30340;&#39044;&#23450;&#20041;&#20851;&#38190;&#28857;&#12290;&#22522;&#20110;&#20851;&#38190;&#28857;&#21644;&#23454;&#20363;&#35821;&#20041;&#20998;&#21106;&#65292;&#25105;&#20204;&#36890;&#36807;&#26368;&#23567;&#20108;&#20056;&#25311;&#21512;&#39640;&#25928;&#35745;&#31639;6D&#23039;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#23545;&#31216;&#29289;&#20307;&#30340;&#27495;&#20041;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#36807;&#31243;&#29992;&#20110;&#23545;&#31216;&#24863;&#30693;&#30340;&#20851;&#38190;&#28857;&#26816;&#27979;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;SyMFM6D&#32593;&#32476;&#22312;&#21333;&#35270;&#35282;&#21644;&#22810;&#35270;&#35282;6D&#23039;&#24577;&#20272;&#35745;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#23545;&#23039;&#24577;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting objects and estimating their 6D poses is essential for automated systems to interact safely with the environment. Most 6D pose estimators, however, rely on a single camera frame and suffer from occlusions and ambiguities due to object symmetries. We overcome this issue by presenting a novel symmetry-aware multi-view 6D pose estimator called SyMFM6D. Our approach efficiently fuses the RGB-D frames from multiple perspectives in a deep multi-directional fusion network and predicts predefined keypoints for all objects in the scene simultaneously. Based on the keypoints and an instance semantic segmentation, we efficiently compute the 6D poses by least-squares fitting. To address the ambiguity issues for symmetric objects, we propose a novel training procedure for symmetry-aware keypoint detection including a new objective function. Our SyMFM6D network significantly outperforms the state-of-the-art in both single-view and multi-view 6D pose estimation. We furthermore show the effe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#36125;&#21494;&#26031;&#25216;&#26415;&#24212;&#29992;&#20110;&#27979;&#26012;&#20202;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#37327;&#21270;&#21644;&#35780;&#20272;&#19981;&#30830;&#23450;&#24615;&#26469;&#26368;&#23567;&#21270;&#25104;&#26412;&#21644;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2307.00305</link><description>&lt;p&gt;
&#24212;&#29992;&#36125;&#21494;&#26031;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#65306;&#27979;&#26012;&#20202;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Applied Bayesian Structural Health Monitoring: inclinometer data anomaly detection and forecasting. (arXiv:2307.00305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#36125;&#21494;&#26031;&#25216;&#26415;&#24212;&#29992;&#20110;&#27979;&#26012;&#20202;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#37327;&#21270;&#21644;&#35780;&#20272;&#19981;&#30830;&#23450;&#24615;&#26469;&#26368;&#23567;&#21270;&#25104;&#26412;&#21644;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#26012;&#20202;&#25506;&#22836;&#26159;&#19968;&#31181;&#21487;&#20197;&#29992;&#26469;&#27979;&#37327;&#22303;&#26041;&#22369;&#20307;&#21464;&#24418;&#30340;&#35774;&#22791;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#23558;&#36125;&#21494;&#26031;&#25216;&#26415;&#24212;&#29992;&#20110;&#23454;&#38469;&#27979;&#26012;&#20202;&#25968;&#25454;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;&#21151;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#23545;&#25972;&#20010;&#33521;&#22269;&#38081;&#36335;&#32593;&#32476;&#20013;&#25910;&#38598;&#30340;&#27979;&#26012;&#20202;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#30340;&#24773;&#20917;&#12290;&#30417;&#27979;&#25968;&#25454;&#22788;&#29702;&#36807;&#31243;&#20013;&#65292;&#20174;&#19994;&#20154;&#21592;&#36890;&#24120;&#26377;&#20004;&#20010;&#30446;&#26631;&#65292;&#19968;&#26159;&#35782;&#21035;&#20219;&#20309;&#24322;&#24120;&#25110;&#21361;&#38505;&#30340;&#36816;&#21160;&#65292;&#20108;&#26159;&#36890;&#36807;&#39044;&#27979;&#26469;&#39044;&#27979;&#28508;&#22312;&#26410;&#26469;&#30340;&#19981;&#33391;&#24773;&#20917;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#23454;&#26045;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#23545;&#27979;&#26012;&#20202;&#25968;&#25454;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;&#12290;&#36890;&#36807;&#37327;&#21270;&#21644;&#35780;&#20272;&#36866;&#24403;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#26368;&#23567;&#21270;&#25104;&#26412;&#21644;&#39118;&#38505;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#20419;&#36827;&#22686;&#24378;&#30340;&#20915;&#31574;&#21644;&#39118;&#38505;&#20998;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27979;&#26012;&#20202;&#25968;&#25454;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;
&lt;/p&gt;
&lt;p&gt;
Inclinometer probes are devices that can be used to measure deformations within earthwork slopes. This paper demonstrates a novel application of Bayesian techniques to real-world inclinometer data, providing both anomaly detection and forecasting. Specifically, this paper details an analysis of data collected from inclinometer data across the entire UK rail network.  Practitioners have effectively two goals when processing monitoring data. The first is to identify any anomalous or dangerous movements, and the second is to predict potential future adverse scenarios by forecasting. In this paper we apply Uncertainty Quantification (UQ) techniques by implementing a Bayesian approach to anomaly detection and forecasting for inclinometer data. Subsequently, both costs and risks may be minimised by quantifying and evaluating the appropriate uncertainties. This framework may then act as an enabler for enhanced decision making and risk analysis.  We show that inclinometer data can be described
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#20809;&#28369;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#20013;&#21152;&#36895;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#30340;&#20004;&#31181;&#26041;&#27861;&#65306;&#22686;&#22823;&#27493;&#38271;&#21644;&#31639;&#23376;&#23398;&#20064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22686;&#22823;&#27493;&#38271;&#30340;&#21152;&#36895;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#35777;&#25910;&#25947;&#24615;&#30340;&#21516;&#26102;&#31616;&#21333;&#26377;&#25928;&#22320;&#25552;&#39640;&#35745;&#31639;&#36895;&#24230;&#65307;&#32780;&#31639;&#23376;&#23398;&#20064;&#21017;&#36890;&#36807;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#26469;&#21152;&#36895;&#27714;&#35299;&#28041;&#21450;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.00296</link><description>&lt;p&gt;
&#25193;&#22823;&#27493;&#38271;&#21644;&#31639;&#23376;&#23398;&#20064;&#30340;&#21152;&#36895;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#22312;&#38750;&#20809;&#28369;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Accelerated primal-dual methods with enlarged step sizes and operator learning for nonsmooth optimal control problems. (arXiv:2307.00296v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00296
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#20809;&#28369;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#20013;&#21152;&#36895;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#30340;&#20004;&#31181;&#26041;&#27861;&#65306;&#22686;&#22823;&#27493;&#38271;&#21644;&#31639;&#23376;&#23398;&#20064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22686;&#22823;&#27493;&#38271;&#30340;&#21152;&#36895;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#35777;&#25910;&#25947;&#24615;&#30340;&#21516;&#26102;&#31616;&#21333;&#26377;&#25928;&#22320;&#25552;&#39640;&#35745;&#31639;&#36895;&#24230;&#65307;&#32780;&#31639;&#23376;&#23398;&#20064;&#21017;&#36890;&#36807;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#26469;&#21152;&#36895;&#27714;&#35299;&#28041;&#21450;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#31867;&#20855;&#26377;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#32422;&#26463;&#30340;&#38750;&#20809;&#28369;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30001;&#20110;&#20854;&#38750;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#21644;&#31163;&#25955;&#21270;&#21518;&#30340;&#39640;&#32500;&#21644;&#30149;&#24577;&#31995;&#32479;&#65292;&#36825;&#31867;&#38382;&#39064;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20998;&#21035;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#21464;&#37327;&#65292;&#22240;&#27492;&#27599;&#27425;&#36845;&#20195;&#30340;&#20027;&#35201;&#35745;&#31639;&#21482;&#38656;&#35201;&#35299;&#20915;&#20004;&#20010;PDE&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20351;&#29992;&#36739;&#22823;&#30340;&#27493;&#38271;&#25110;&#31639;&#23376;&#23398;&#20064;&#25216;&#26415;&#26469;&#21152;&#36895;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#12290;&#23545;&#20110;&#20855;&#26377;&#36739;&#22823;&#27493;&#38271;&#30340;&#21152;&#36895;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#65292;&#20854;&#25910;&#25947;&#24615;&#20173;&#28982;&#21487;&#20197;&#24471;&#21040;&#20005;&#26684;&#35777;&#26126;&#65292;&#21516;&#26102;&#23427;&#20197;&#19968;&#31181;&#31616;&#21333;&#19988;&#26222;&#36941;&#30340;&#26041;&#24335;&#25968;&#20540;&#19978;&#21152;&#36895;&#20102;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#12290;&#23545;&#20110;&#31639;&#23376;&#23398;&#20064;&#21152;&#36895;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#26469;&#34920;&#31034;&#28041;&#21450;&#30340;PDE&#12290;&#19968;&#26086;&#23398;&#20064;&#21040;&#19968;&#20010;&#31070;&#32463;&#31639;&#23376;&#65292;&#35299;&#20915;&#19968;&#20010;PDE&#21482;&#38656;&#35201;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#30340;&#21069;&#21521;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a general class of nonsmooth optimal control problems with partial differential equation (PDE) constraints, which are very challenging due to its nonsmooth objective functionals and the resulting high-dimensional and ill-conditioned systems after discretization. We focus on the application of a primal-dual method, with which different types of variables can be treated individually and thus its main computation at each iteration only requires solving two PDEs. Our target is to accelerate the primal-dual method with either larger step sizes or operator learning techniques. For the accelerated primal-dual method with larger step sizes, its convergence can be still proved rigorously while it numerically accelerates the original primal-dual method in a simple and universal way. For the operator learning acceleration, we construct deep neural network surrogate models for the involved PDEs. Once a neural operator is learned, solving a PDE requires only a forward pass of the neural
&lt;/p&gt;</description></item><item><title>AutoST&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#39640;&#24615;&#33021;&#21644;&#33021;&#25928;&#30340;&#33033;&#20914;&#21464;&#21387;&#22120;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2307.00293</link><description>&lt;p&gt;
AutoST&#65306;&#26080;&#38656;&#35757;&#32451;&#30340;&#33033;&#20914;&#21464;&#21387;&#22120;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
AutoST: Training-free Neural Architecture Search for Spiking Transformers. (arXiv:2307.00293v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00293
&lt;/p&gt;
&lt;p&gt;
AutoST&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#39640;&#24615;&#33021;&#21644;&#33021;&#25928;&#30340;&#33033;&#20914;&#21464;&#21387;&#22120;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#21464;&#21387;&#22120;&#22240;&#21516;&#26102;&#20855;&#22791;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#30340;&#33021;&#25928;&#21644;&#21464;&#21387;&#22120;&#30340;&#39640;&#23481;&#37327;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33033;&#20914;&#21464;&#21387;&#22120;&#26550;&#26500;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#25512;&#23548;&#65292;&#23384;&#22312;&#26126;&#26174;&#30340;&#26550;&#26500;&#24046;&#36317;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#21450;&#20854;ANN&#23545;&#24212;&#29289;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#25163;&#21160;&#36807;&#31243;&#25110;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26041;&#27861;&#26469;&#23547;&#25214;&#26368;&#20248;&#26550;&#26500;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#32791;&#26102;&#65292;&#35201;&#20040;&#22312;&#20869;&#23384;&#21344;&#29992;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#20195;&#20215;&#39640;&#26114;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoST&#65292;&#19968;&#31181;&#29992;&#20110;&#33033;&#20914;&#21464;&#21387;&#22120;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;NAS&#26041;&#27861;&#65292;&#21487;&#20197;&#24555;&#36895;&#35782;&#21035;&#39640;&#24615;&#33021;&#21644;&#33021;&#25928;&#30340;&#33033;&#20914;&#21464;&#21387;&#22120;&#26550;&#26500;&#12290;&#19982;&#29616;&#26377;&#30340;&#26080;&#38656;&#35757;&#32451;NAS&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#28014;&#28857;&#36816;&#31639;&#65288;FLOPs&#65289;&#20316;&#20026;&#19968;&#31181;&#25351;&#23548;&#21487;&#34892;&#24615;&#30340;&#24230;&#37327;&#65292;
&lt;/p&gt;
&lt;p&gt;
Spiking Transformers have gained considerable attention because they achieve both the energy efficiency of Spiking Neural Networks (SNNs) and the high capacity of Transformers. However, the existing Spiking Transformer architectures, derived from ANNs, exhibit a notable architectural gap, resulting in suboptimal performance compared to their ANN counterparts. Traditional approaches to discovering optimal architectures primarily rely on either manual procedures, which are time-consuming, or Neural Architecture Search (NAS) methods, which are usually expensive in terms of memory footprints and computation time. To address these limitations, we introduce AutoST, a training-free NAS method for Spiking Transformers, to rapidly identify high-performance and energy-efficient Spiking Transformer architectures. Unlike existing training-free NAS methods, which struggle with the non-differentiability and high sparsity inherent in SNNs, we propose to utilize Floating-Point Operations (FLOPs) as a 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#20840;&#33021;SAM&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#22312;&#25972;&#20010;AI&#24320;&#21457;&#24037;&#20316;&#27969;&#31243;&#20013;&#20351;&#29992;SAM&#65292;&#24182;&#19988;&#22312;&#25512;&#29702;&#38454;&#27573;&#26080;&#38656;&#25163;&#21160;&#25552;&#31034;&#65292;&#23454;&#29616;&#20102;&#20174;&#24369;&#27880;&#37322;&#21040;&#22522;&#20110;&#25552;&#31034;&#30340;&#20687;&#32032;&#32423;&#32454;&#32990;&#26680;&#20998;&#21106;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.00290</link><description>&lt;p&gt;
&#20840;&#33021;SAM&#65306;&#20174;&#24369;&#27880;&#37322;&#21040;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35266;&#32454;&#32990;&#26680;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
All-in-SAM: from Weak Annotation to Pixel-wise Nuclei Segmentation with Prompt-based Finetuning. (arXiv:2307.00290v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#20840;&#33021;SAM&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#22312;&#25972;&#20010;AI&#24320;&#21457;&#24037;&#20316;&#27969;&#31243;&#20013;&#20351;&#29992;SAM&#65292;&#24182;&#19988;&#22312;&#25512;&#29702;&#38454;&#27573;&#26080;&#38656;&#25163;&#21160;&#25552;&#31034;&#65292;&#23454;&#29616;&#20102;&#20174;&#24369;&#27880;&#37322;&#21040;&#22522;&#20110;&#25552;&#31034;&#30340;&#20687;&#32032;&#32423;&#32454;&#32990;&#26680;&#20998;&#21106;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;Segment Anything Model (SAM)&#26159;&#19968;&#31181;&#20351;&#29992;&#25552;&#31034;&#30340;&#36890;&#29992;&#38646;&#26679;&#26412;&#20998;&#21106;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#35813;&#27969;&#31243;&#22312;&#25512;&#29702;&#38454;&#27573;&#20173;&#28982;&#38656;&#35201;&#25163;&#21160;&#25552;&#31034;&#65292;&#23545;&#20110;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20173;&#28982;&#36164;&#28304;&#23494;&#38598;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#20840;&#33021;SAM&#30340;&#27969;&#31243;&#65292;&#23427;&#22312;&#25972;&#20010;AI&#24320;&#21457;&#24037;&#20316;&#27969;&#31243;&#20013;&#20351;&#29992;&#20102;SAM&#65292;&#24182;&#19988;&#22312;&#25512;&#29702;&#38454;&#27573;&#26080;&#38656;&#25163;&#21160;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SAM&#39318;&#20808;&#21033;&#29992;&#24369;&#25552;&#31034;&#65288;&#20363;&#22914;&#28857;&#12289;&#36793;&#30028;&#26694;&#65289;&#29983;&#25104;&#20687;&#32032;&#32423;&#27880;&#37322;&#65292;&#28982;&#21518;&#20351;&#29992;&#20687;&#32032;&#32423;&#27880;&#37322;&#23545;SAM&#20998;&#21106;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#20004;&#20010;&#20851;&#38190;&#21457;&#29616;&#65306;1&#65289;&#25152;&#25552;&#20986;&#30340;pi
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) is a recently proposed prompt-based segmentation model in a generic zero-shot segmentation approach. With the zero-shot segmentation capacity, SAM achieved impressive flexibility and precision on various segmentation tasks. However, the current pipeline requires manual prompts during the inference stage, which is still resource intensive for biomedical image segmentation. In this paper, instead of using prompts during the inference stage, we introduce a pipeline that utilizes the SAM, called all-in-SAM, through the entire AI development workflow (from annotation generation to model finetuning) without requiring manual prompts during the inference stage. Specifically, SAM is first employed to generate pixel-level annotations from weak prompts (e.g., points, bounding box). Then, the pixel-level annotations are used to finetune the SAM segmentation model rather than training from scratch. Our experimental results reveal two key findings: 1) the proposed pi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20351;&#29992;CMA-ES&#26469;&#26367;&#20195;&#36138;&#23146;&#38598;&#25104;&#36873;&#25321;&#26041;&#27861;&#65292;&#22312;AutoML&#20013;&#30340;&#34920;&#29616;&#12290;&#27604;&#36739;&#32467;&#26524;&#34920;&#26126;&#23545;&#20110;ROC AUC&#25351;&#26631;&#65292;CMA-ES&#20986;&#29616;&#20102;&#20005;&#37325;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.00286</link><description>&lt;p&gt;
CMA-ES&#29992;&#20110;&#21518;&#32493;&#38598;&#25104;&#22312;AutoML&#20013;&#65306;&#19968;&#20010;&#24040;&#22823;&#30340;&#25104;&#21151;&#21644;&#21487;&#25405;&#25937;&#30340;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
CMA-ES for Post Hoc Ensembling in AutoML: A Great Success and Salvageable Failure. (arXiv:2307.00286v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20351;&#29992;CMA-ES&#26469;&#26367;&#20195;&#36138;&#23146;&#38598;&#25104;&#36873;&#25321;&#26041;&#27861;&#65292;&#22312;AutoML&#20013;&#30340;&#34920;&#29616;&#12290;&#27604;&#36739;&#32467;&#26524;&#34920;&#26126;&#23545;&#20110;ROC AUC&#25351;&#26631;&#65292;CMA-ES&#20986;&#29616;&#20102;&#20005;&#37325;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#31995;&#32479;&#22312;&#27169;&#22411;&#36873;&#25321;&#21518;&#20351;&#29992;Caruana&#31561;&#20154;&#65288;2004&#24180;&#65289;&#30340;&#36138;&#23146;&#38598;&#25104;&#36873;&#25321;&#65288;GES&#65289;&#26469;&#38598;&#25104;&#25214;&#21040;&#30340;&#27169;&#22411;&#12290;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#24182;&#19988;&#20687; Auto-Sklearn 1 &#33324;&#25552;&#31034;&#65292;&#20854;&#20182;&#26041;&#27861;&#65292;&#22914;&#22534;&#21472;&#25110;&#26080;&#26799;&#24230;&#25968;&#20540;&#20248;&#21270;&#65292;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;Auto-Sklearn 1 &#20013;&#30340;&#36807;&#25311;&#21512;&#27604;&#20854;&#20182;AutoML&#31995;&#32479;&#26356;&#26377;&#21487;&#33021;&#65292;&#22240;&#20026;&#23427;&#20165;&#20351;&#29992;&#36136;&#37327;&#36739;&#20302;&#30340;&#39564;&#35777;&#25968;&#25454;&#36827;&#34892;&#21518;&#32493;&#38598;&#25104;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26377;&#21160;&#21147;&#20998;&#26512;Auto-Sklearn 1 &#30340;&#35266;&#28857;&#26159;&#21542;&#36866;&#29992;&#20110;&#20855;&#26377;&#36136;&#37327;&#26356;&#39640;&#30340;&#39564;&#35777;&#25968;&#25454;&#30340;&#31995;&#32479;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#21327;&#26041;&#24046;&#30697;&#38453;&#36866;&#24212;&#36827;&#21270;&#31574;&#30053;&#65288;CMA-ES&#65289;&#65292;&#19968;&#20010;&#26368;&#20808;&#36827;&#30340;&#26080;&#26799;&#24230;&#25968;&#20540;&#20248;&#21270;&#26041;&#27861;&#65292;&#19982;AutoGluon&#30340;AutoML&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;GES&#22312;71&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Auto-Sklearn&#30340;&#35266;&#28857;&#21462;&#20915;&#20110;&#36873;&#25321;&#30340;&#24230;&#37327;&#25351;&#26631;&#12290;&#23545;&#20110;ROC AUC&#25351;&#26631;&#65292;CMA-ES&#20986;&#29616;&#20102;&#20005;&#37325;&#30340;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many state-of-the-art automated machine learning (AutoML) systems use greedy ensemble selection (GES) by Caruana et al. (2004) to ensemble models found during model selection post hoc. Thereby, boosting predictive performance and likely following Auto-Sklearn 1's insight that alternatives, like stacking or gradient-free numerical optimization, overfit. Overfitting in Auto-Sklearn 1 is much more likely than in other AutoML systems because it uses only low-quality validation data for post hoc ensembling. Therefore, we were motivated to analyze whether Auto-Sklearn 1's insight holds true for systems with higher-quality validation data. Consequently, we compared the performance of covariance matrix adaptation evolution strategy (CMA-ES), state-of-the-art gradient-free numerical optimization, to GES on the 71 classification datasets from the AutoML benchmark for AutoGluon. We found that Auto-Sklearn's insight depends on the chosen metric. For the metric ROC AUC, CMA-ES overfits drastically 
&lt;/p&gt;</description></item><item><title>Assembled-OpenML&#26159;&#19968;&#20010;&#20351;&#29992;OpenML&#26500;&#24314;&#38598;&#25104;&#31639;&#27861;&#20803;&#25968;&#25454;&#38598;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#27979;&#32467;&#26524;&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#20013;&#38598;&#25104;&#31639;&#27861;&#25216;&#26415;&#30340;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2307.00285</link><description>&lt;p&gt;
Assembled-OpenML&#65306;&#20351;&#29992;OpenML&#21019;&#24314;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#20013;&#39640;&#25928;&#30340;&#38598;&#25104;&#31639;&#27861;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Assembled-OpenML: Creating Efficient Benchmarks for Ensembles in AutoML with OpenML. (arXiv:2307.00285v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00285
&lt;/p&gt;
&lt;p&gt;
Assembled-OpenML&#26159;&#19968;&#20010;&#20351;&#29992;OpenML&#26500;&#24314;&#38598;&#25104;&#31639;&#27861;&#20803;&#25968;&#25454;&#38598;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#27979;&#32467;&#26524;&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#20013;&#38598;&#25104;&#31639;&#27861;&#25216;&#26415;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#26694;&#26550;&#32463;&#24120;&#20351;&#29992;&#38598;&#25104;&#31639;&#27861;&#12290;&#24320;&#21457;&#20154;&#21592;&#38656;&#35201;&#27604;&#36739;&#19981;&#21516;&#30340;&#38598;&#25104;&#31639;&#27861;&#25216;&#26415;&#65292;&#20197;&#36873;&#25321;&#36866;&#21512;AutoML&#26694;&#26550;&#30340;&#25216;&#26415;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#38598;&#25104;&#31639;&#27861;&#25216;&#26415;&#30340;&#27604;&#36739;&#36890;&#24120;&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#65292;&#22240;&#20026;&#38656;&#35201;&#35757;&#32451;&#21644;&#35780;&#20272;&#35768;&#22810;&#22522;&#30784;&#27169;&#22411;&#19968;&#27425;&#25110;&#22810;&#27425;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Assembled-OpenML&#12290;Assembled-OpenML&#26159;&#19968;&#20010;&#22522;&#20110;Python&#30340;&#24037;&#20855;&#65292;&#20351;&#29992;OpenML&#26500;&#24314;&#38598;&#25104;&#31639;&#27861;&#30340;&#20803;&#25968;&#25454;&#38598;&#12290;&#20803;&#25968;&#25454;&#38598;&#31216;&#20026;Metatask&#65292;&#21253;&#21547;OpenML&#20219;&#21153;&#30340;&#25968;&#25454;&#12289;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#20219;&#21153;&#30340;&#27169;&#22411;&#35780;&#20272;&#39044;&#27979;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;&#23384;&#20648;&#22312;&#20803;&#25968;&#25454;&#38598;&#20013;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#38598;&#25104;&#31639;&#27861;&#25216;&#26415;&#30340;&#27604;&#36739;&#35745;&#31639;&#25104;&#26412;&#26356;&#20302;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#21644;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#20171;&#32461;Assembled-OpenML&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#24037;&#20855;&#30340;&#31532;&#19968;&#20010;&#29256;&#26412;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;Assembled-OpenML&#27604;&#36739;&#19968;&#32452;&#38598;&#25104;&#31639;&#27861;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Machine Learning (AutoML) frameworks regularly use ensembles. Developers need to compare different ensemble techniques to select appropriate techniques for an AutoML framework from the many potential techniques. So far, the comparison of ensemble techniques is often computationally expensive, because many base models must be trained and evaluated one or multiple times. Therefore, we present Assembled-OpenML. Assembled-OpenML is a Python tool, which builds meta-datasets for ensembles using OpenML. A meta-dataset, called Metatask, consists of the data of an OpenML task, the task's dataset, and prediction data from model evaluations for the task. We can make the comparison of ensemble techniques computationally cheaper by using the predictions stored in a metatask instead of training and evaluating base models. To introduce Assembled-OpenML, we describe the first version of our tool. Moreover, we present an example of using Assembled-OpenML to compare a set of ensemble technique
&lt;/p&gt;</description></item><item><title>SysNoise&#26159;&#19968;&#31181;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#35757;&#32451;-&#37096;&#32626;&#21608;&#26399;&#20013;&#32463;&#24120;&#21457;&#29983;&#30340;&#22122;&#38899;&#65292;&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;SysNoise&#23545;&#19981;&#21516;&#20219;&#21153;&#30340;&#27169;&#22411;&#31283;&#20581;&#24615;&#20250;&#24102;&#26469;&#19968;&#23450;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#24120;&#35265;&#30340;&#32531;&#35299;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.00280</link><description>&lt;p&gt;
SysNoise: &#25506;&#32034;&#21644;&#35780;&#20272;&#35757;&#32451;-&#37096;&#32626;&#31995;&#32479;&#30340;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
SysNoise: Exploring and Benchmarking Training-Deployment System Inconsistency. (arXiv:2307.00280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00280
&lt;/p&gt;
&lt;p&gt;
SysNoise&#26159;&#19968;&#31181;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#35757;&#32451;-&#37096;&#32626;&#21608;&#26399;&#20013;&#32463;&#24120;&#21457;&#29983;&#30340;&#22122;&#38899;&#65292;&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;SysNoise&#23545;&#19981;&#21516;&#20219;&#21153;&#30340;&#27169;&#22411;&#31283;&#20581;&#24615;&#20250;&#24102;&#26469;&#19968;&#23450;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#24120;&#35265;&#30340;&#32531;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#21644;&#33258;&#28982;&#22122;&#38899;&#30340;&#24433;&#21709;&#65292;&#28982;&#32780;&#23545;&#20110;&#30001;&#19981;&#21516;&#31995;&#32479;&#23454;&#29616;&#24341;&#36215;&#30340;&#22122;&#38899;&#23545;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#25991;&#39318;&#27425;&#24341;&#20837;SysNoise&#65292;&#19968;&#31181;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#35757;&#32451;-&#37096;&#32626;&#21608;&#26399;&#20013;&#32463;&#24120;&#21457;&#29983;&#20294;&#24448;&#24448;&#34987;&#24573;&#35270;&#30340;&#22122;&#38899;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SysNoise&#21457;&#29983;&#22312;&#28304;&#35757;&#32451;&#31995;&#32479;&#22312;&#37096;&#32626;&#26102;&#20999;&#25442;&#21040;&#19981;&#21516;&#30340;&#30446;&#26631;&#31995;&#32479;&#26102;&#65292;&#21508;&#31181;&#24494;&#23567;&#31995;&#32479;&#19981;&#21305;&#37197;&#32047;&#21152;&#36215;&#26469;&#20250;&#20135;&#29983;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;SysNoise&#36827;&#34892;&#20102;&#35782;&#21035;&#21644;&#20998;&#31867;&#65292;&#20998;&#20026;&#22522;&#20110;&#25512;&#29702;&#38454;&#27573;&#30340;&#19977;&#20010;&#31867;&#21035;&#65307;&#28982;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#35780;&#20272;&#26631;&#20934;&#65292;&#20197;&#23450;&#37327;&#35780;&#20272;SysNoise&#23545;20&#22810;&#31181;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#12289;&#23454;&#20363;&#20998;&#21106;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;SysNoise&#23545;&#19981;&#21516;&#20219;&#21153;&#30340;&#27169;&#22411;&#31283;&#20581;&#24615;&#20250;&#24102;&#26469;&#19968;&#23450;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#24120;&#35265;&#30340;&#32531;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extensive studies have shown that deep learning models are vulnerable to adversarial and natural noises, yet little is known about model robustness on noises caused by different system implementations. In this paper, we for the first time introduce SysNoise, a frequently occurred but often overlooked noise in the deep learning training-deployment cycle. In particular, SysNoise happens when the source training system switches to a disparate target system in deployments, where various tiny system mismatch adds up to a non-negligible difference. We first identify and classify SysNoise into three categories based on the inference stage; we then build a holistic benchmark to quantitatively measure the impact of SysNoise on 20+ models, comprehending image classification, object detection, instance segmentation and natural language processing tasks. Our extensive experiments revealed that SysNoise could bring certain impacts on model robustness across different tasks and common mitigations li
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24120;&#35782;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#26356;&#22909;&#30340;&#32593;&#32476;&#26435;&#37325;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#21487;&#36801;&#31227;&#24615;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#36755;&#20986;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26500;&#24314;&#22810;&#25945;&#24072;&#27169;&#22411;&#24182;&#25552;&#21462;&#30693;&#35782;&#65292;&#20943;&#23569;&#27169;&#22411;&#29305;&#23450;&#30340;&#29305;&#24449;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#36755;&#20986;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2307.00274</link><description>&lt;p&gt;
&#29983;&#25104;&#21487;&#36801;&#31227;&#23545;&#25239;&#26679;&#26412;&#30340;&#24120;&#35782;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Common Knowledge Learning for Generating Transferable Adversarial Examples. (arXiv:2307.00274v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24120;&#35782;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#26356;&#22909;&#30340;&#32593;&#32476;&#26435;&#37325;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#21487;&#36801;&#31227;&#24615;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#36755;&#20986;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26500;&#24314;&#22810;&#25945;&#24072;&#27169;&#22411;&#24182;&#25552;&#21462;&#30693;&#35782;&#65292;&#20943;&#23569;&#27169;&#22411;&#29305;&#23450;&#30340;&#29305;&#24449;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#36755;&#20986;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#19968;&#31181;&#37325;&#35201;&#30340;&#40657;&#30418;&#25915;&#20987;&#31867;&#22411;&#65292;&#21363;&#22522;&#20110;&#36801;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#22312;&#36825;&#31181;&#25915;&#20987;&#20013;&#65292;&#23545;&#25163;&#36890;&#36807;&#19968;&#20010;&#26367;&#20195;&#65288;&#21407;&#22987;&#65289;&#27169;&#22411;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#25915;&#20987;&#19968;&#20010;&#26410;&#30693;&#30340;&#30446;&#26631;&#27169;&#22411;&#65292;&#32780;&#19981;&#30693;&#36947;&#20854;&#20449;&#24687;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#28304;&#27169;&#22411;&#21644;&#30446;&#26631;&#27169;&#22411;&#26469;&#33258;&#19981;&#21516;&#31867;&#22411;&#30340;DNN&#26550;&#26500;&#65288;&#20363;&#22914;ResNet-18&#21644;Swin Transformer&#65289;&#26102;&#24448;&#24448;&#32473;&#20986;&#20102;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#23545;&#25239;&#21487;&#36801;&#31227;&#24615;&#12290;&#26412;&#25991;&#35266;&#23519;&#21040;&#19978;&#36848;&#29616;&#35937;&#26159;&#30001;&#36755;&#20986;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#24341;&#36215;&#30340;&#12290;&#20026;&#20102;&#22312;&#22266;&#23450;&#30340;&#32593;&#32476;&#26550;&#26500;&#19979;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#24182;&#26377;&#25928;&#21033;&#29992;&#29616;&#26377;&#30340;DNN&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24120;&#35782;&#23398;&#20064;&#65288;CKL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#26356;&#22909;&#30340;&#32593;&#32476;&#26435;&#37325;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#21487;&#36801;&#31227;&#24615;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#20943;&#23569;&#27169;&#22411;&#29305;&#23450;&#30340;&#29305;&#24449;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#36755;&#20986;&#20998;&#24067;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#25945;&#24072;&#26694;&#26550;&#65292;&#20174;&#19981;&#21516;&#25945;&#24072;&#27169;&#22411;&#20013;&#25552;&#21462;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on an important type of black-box attacks, i.e., transfer-based adversarial attacks, where the adversary generates adversarial examples by a substitute (source) model and utilize them to attack an unseen target model, without knowing its information. Existing methods tend to give unsatisfactory adversarial transferability when the source and target models are from different types of DNN architectures (e.g. ResNet-18 and Swin Transformer). In this paper, we observe that the above phenomenon is induced by the output inconsistency problem. To alleviate this problem while effectively utilizing the existing DNN models, we propose a common knowledge learning (CKL) framework to learn better network weights to generate adversarial examples with better transferability, under fixed network architectures. Specifically, to reduce the model-specific features and obtain better output distributions, we construct a multi-teacher framework, where the knowledge is distilled from diffe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#22122;&#38899;&#30340;&#26412;&#22320;&#20013;&#27602;&#25915;&#20987;&#26041;&#27861;&#65288;PeLPA&#65289;&#20197;&#32469;&#36807;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#24182;&#38024;&#23545;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;CMARL&#65289;&#20013;&#30340;&#31169;&#26377;&#30693;&#35782;&#20849;&#20139;&#36807;&#31243;&#20013;&#30340;&#20013;&#27602;&#23041;&#32961;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#65292;PeLPA&#25915;&#20987;&#33021;&#22815;&#26174;&#33879;&#22686;&#21152;&#24179;&#22343;&#27493;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.00268</link><description>&lt;p&gt;
&#26126;&#37324;&#26263;&#20013;&#65306;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#22122;&#38899;&#30340;&#25269;&#24481;&#24694;&#24847;&#25915;&#20987;&#30340;&#26412;&#22320;&#20013;&#27602;&#25915;&#20987;&#26041;&#27861;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hiding in Plain Sight: Differential Privacy Noise Exploitation for Evasion-resilient Localized Poisoning Attacks in Multiagent Reinforcement Learning. (arXiv:2307.00268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#22122;&#38899;&#30340;&#26412;&#22320;&#20013;&#27602;&#25915;&#20987;&#26041;&#27861;&#65288;PeLPA&#65289;&#20197;&#32469;&#36807;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#24182;&#38024;&#23545;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;CMARL&#65289;&#20013;&#30340;&#31169;&#26377;&#30693;&#35782;&#20849;&#20139;&#36807;&#31243;&#20013;&#30340;&#20013;&#27602;&#23041;&#32961;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#65292;PeLPA&#25915;&#20987;&#33021;&#22815;&#26174;&#33879;&#22686;&#21152;&#24179;&#22343;&#27493;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#34987;&#24341;&#20837;&#21040;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;CMARL&#65289;&#20013;&#65292;&#20197;&#20445;&#25252;&#26234;&#33021;&#20307;&#22312;&#30693;&#35782;&#20849;&#20139;&#36807;&#31243;&#20013;&#20813;&#21463;&#23545;&#25163;&#30340;&#25512;&#26029;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#30001;DP&#26426;&#21046;&#24341;&#20837;&#30340;&#22122;&#38899;&#21487;&#33021;&#20250;&#22312;CMARL&#20013;&#30340;&#31169;&#26377;&#30693;&#35782;&#20849;&#20139;&#36807;&#31243;&#20013;&#24847;&#22806;&#22320;&#20135;&#29983;&#19968;&#31181;&#26032;&#30340;&#20013;&#27602;&#23041;&#32961;&#65292;&#36825;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#12289;&#21033;&#29992;&#38544;&#31169;&#30340;&#12289;&#25269;&#24481;&#36867;&#36991;&#25915;&#20987;&#30340;&#26412;&#22320;&#20013;&#27602;&#25915;&#20987;&#26041;&#27861;&#65288;PeLPA&#65289;&#65292;&#21033;&#29992;&#20102;DP&#22122;&#38899;&#30340;&#29305;&#24615;&#65292;&#32469;&#36807;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#24182;&#38459;&#30861;CMARL&#27169;&#22411;&#30340;&#26368;&#20248;&#25910;&#25947;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;PeLPA&#25915;&#20987;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38750;&#23545;&#25239;&#21644;&#22810;&#23545;&#25239;&#29615;&#22659;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20013;&#31561;&#35268;&#27169;&#29615;&#22659;&#20013;&#65292;&#25915;&#20987;&#32773;&#27604;&#20363;&#20026;20%&#21644;40%&#30340;PeLPA&#25915;&#20987;&#21487;&#33021;&#23548;&#33268;&#24179;&#22343;&#27493;&#25968;&#30340;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lately, differential privacy (DP) has been introduced in cooperative multiagent reinforcement learning (CMARL) to safeguard the agents' privacy against adversarial inference during knowledge sharing. Nevertheless, we argue that the noise introduced by DP mechanisms may inadvertently give rise to a novel poisoning threat, specifically in the context of private knowledge sharing during CMARL, which remains unexplored in the literature. To address this shortcoming, we present an adaptive, privacy-exploiting, and evasion-resilient localized poisoning attack (PeLPA) that capitalizes on the inherent DP-noise to circumvent anomaly detection systems and hinder the optimal convergence of the CMARL model. We rigorously evaluate our proposed PeLPA attack in diverse environments, encompassing both non-adversarial and multiple-adversarial contexts. Our findings reveal that, in a medium-scale environment, the PeLPA attack with attacker ratios of 20% and 40% can lead to an increase in average steps t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#35299;&#20915;&#22855;&#28857;&#38382;&#39064;&#20013;&#30340;&#26368;&#20248;&#35299;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#22810;&#39033;&#24335;&#30456;&#21152;&#30340;&#24635;&#25968;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#36817;&#26399;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.00252</link><description>&lt;p&gt;
&#19968;&#31181;&#35299;&#20915;&#22855;&#28857;&#38382;&#39064;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An ML approach to resolution of singularities. (arXiv:2307.00252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00252
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#35299;&#20915;&#22855;&#28857;&#38382;&#39064;&#20013;&#30340;&#26368;&#20248;&#35299;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#22810;&#39033;&#24335;&#30456;&#21152;&#30340;&#24635;&#25968;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#36817;&#26399;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#24335;&#26041;&#31243;&#32452;&#30340;&#35299;&#38598;&#36890;&#24120;&#21253;&#21547;&#19981;&#20809;&#28369;&#12289;&#22855;&#24322;&#30340;&#28857;&#12290;&#35299;&#20915;&#22855;&#28857;&#26159;&#20960;&#20309;&#20013;&#30340;&#22522;&#26412;&#36807;&#31243;&#65292;&#25105;&#20204;&#23558;&#22855;&#28857;&#26367;&#25442;&#20026;&#20809;&#28369;&#28857;&#65292;&#21516;&#26102;&#20445;&#25345;&#35299;&#38598;&#30340;&#21097;&#20313;&#37096;&#20998;&#19981;&#21464;&#12290;&#35299;&#20915;&#22855;&#28857;&#24182;&#19981;&#26159;&#21807;&#19968;&#30340;&#65306;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#21453;&#22797;&#36827;&#34892;&#34987;&#31216;&#20026;&#8220;blowing-up&#8221;&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#35299;&#20915;&#30340;&#22797;&#26434;&#24615;&#39640;&#24230;&#20381;&#36182;&#20110;&#26576;&#20123;&#36873;&#25321;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#36716;&#21270;&#25104;&#19981;&#21516;&#29256;&#26412;&#30340;&#20004;&#20154;&#21338;&#24328;&#65292;&#21363;&#25152;&#35859;&#30340;Hironaka&#28216;&#25103;&#65292;&#32780;&#31532;&#19968;&#20301;&#29609;&#23478;&#30340;&#33719;&#32988;&#31574;&#30053;&#25552;&#20379;&#20102;&#35299;&#20915;&#22855;&#28857;&#38382;&#39064;&#30340;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;Hironaka&#28216;&#25103;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#23547;&#25214;&#22855;&#28857;&#30340;&#26368;&#20248;&#35299;&#12290;&#22312;&#26576;&#20123;&#39046;&#22495;&#20013;&#65292;&#35757;&#32451;&#27169;&#22411;&#22312;&#22810;&#39033;&#24335;&#30456;&#21152;&#30340;&#24635;&#25968;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36825;&#35777;&#26126;&#20102;&#26368;&#36817;&#21457;&#23637;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The solution set of a system of polynomial equations typically contains ill-behaved, singular points. Resolution is a fundamental process in geometry in which we replace singular points with smooth points, while keeping the rest of the solution set unchanged. Resolutions are not unique: the usual way to describe them involves repeatedly performing a fundamental operation known as "blowing-up", and the complexity of the resolution highly depends on certain choices. The process can be translated into various versions of a 2-player game, the so-called Hironaka game, and a winning strategy for the first player provides a solution to the resolution problem. In this paper we introduce a new approach to the Hironaka game that uses reinforcement learning agents to find optimal resolutions of singularities. In certain domains, the trained model outperforms state-of-the-art selection heuristics in total number of polynomial additions performed, which provides a proof-of-concept that recent devel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#23433;&#20840;&#31579;&#36873;&#25216;&#26415;&#26469;&#21152;&#36895;&#19981;&#24179;&#34913;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#20248;&#21270;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#28040;&#38500;&#31232;&#30095;&#35299;&#20013;&#30340;&#38646;&#20803;&#32032;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#31579;&#36873;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.00247</link><description>&lt;p&gt;
&#23433;&#20840;&#31579;&#36873;&#19981;&#24179;&#34913;&#26368;&#20248;&#20256;&#36755;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Safe Screening for Unbalanced Optimal Transport. (arXiv:2307.00247v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#23433;&#20840;&#31579;&#36873;&#25216;&#26415;&#26469;&#21152;&#36895;&#19981;&#24179;&#34913;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#20248;&#21270;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#28040;&#38500;&#31232;&#30095;&#35299;&#20013;&#30340;&#38646;&#20803;&#32032;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#31579;&#36873;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#23433;&#20840;&#31579;&#36873;&#25216;&#26415;&#26469;&#21152;&#36895;&#20248;&#21270;&#36807;&#31243;&#30340;&#19981;&#24179;&#34913;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20027;&#21160;&#35782;&#21035;&#21644;&#28040;&#38500;&#31232;&#30095;&#35299;&#20013;&#30340;&#38646;&#20803;&#32032;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#35299;&#30340;&#19978;&#30028;&#12289;&#32771;&#34385;&#23545;&#20598;&#38382;&#39064;&#30340;&#23616;&#37096;&#24378;&#20984;&#24615;&#65292;&#35777;&#26126;&#20102;&#23558;&#23433;&#20840;&#31579;&#36873;&#24212;&#29992;&#20110;&#20855;&#26377;$\ell_2$-&#24809;&#32602;&#21644;KL-&#24809;&#32602;&#30340;&#19981;&#24179;&#34913;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#21487;&#34892;&#24615;&#12290;&#32771;&#34385;&#21040;&#19981;&#24179;&#34913;&#26368;&#20248;&#20256;&#36755;&#30456;&#23545;&#20110;&#19968;&#33324;&#32034;&#24341;&#30697;&#38453;&#19978;&#30340;Lasso&#38382;&#39064;&#30340;&#29305;&#23450;&#32467;&#26500;&#29305;&#24449;&#65292;&#25105;&#20204;&#29305;&#21035;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#25237;&#24433;&#12289;&#26925;&#22278;&#23433;&#20840;&#21306;&#22495;&#26500;&#36896;&#21644;&#20004;&#20010;&#36229;&#24179;&#38754;&#26494;&#24347;&#26041;&#27861;&#12290;&#36825;&#20123;&#25913;&#36827;&#26174;&#33879;&#25552;&#39640;&#20102;&#19981;&#24179;&#34913;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#31579;&#36873;&#25928;&#29575;&#65292;&#32780;&#19981;&#25913;&#21464;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a framework that utilizes the Safe Screening technique to accelerate the optimization process of the Unbalanced Optimal Transport (UOT) problem by proactively identifying and eliminating zero elements in the sparse solutions. We demonstrate the feasibility of applying Safe Screening to the UOT problem with $\ell_2$-penalty and KL-penalty by conducting an analysis of the solution's bounds and considering the local strong convexity of the dual problem. Considering the specific structural characteristics of the UOT in comparison to general Lasso problems on the index matrix, we specifically propose a novel approximate projection, an elliptical safe region construction, and a two-hyperplane relaxation method. These enhancements significantly improve the screening efficiency for the UOT's without altering the algorithm's complexity.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;&#36895;&#29575;-&#22833;&#30495;&#20989;&#25968;&#21644;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#23558;&#23427;&#20204;&#22312;&#35299;&#20915;&#26631;&#37327;&#37327;&#21270;&#38382;&#39064;&#26041;&#38754;&#32479;&#19968;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2307.00246</link><description>&lt;p&gt;
&#20851;&#20110;&#36895;&#29575;-&#22833;&#30495;&#20989;&#25968;&#21644;&#26368;&#20248;&#20256;&#36755;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
On a Relation Between the Rate-Distortion Function and Optimal Transport. (arXiv:2307.00246v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00246
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;&#36895;&#29575;-&#22833;&#30495;&#20989;&#25968;&#21644;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#23558;&#23427;&#20204;&#22312;&#35299;&#20915;&#26631;&#37327;&#37327;&#21270;&#38382;&#39064;&#26041;&#38754;&#32479;&#19968;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35752;&#35770;&#20102;&#36895;&#29575;-&#22833;&#30495;&#20989;&#25968;&#21644;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#29702;&#35770;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23613;&#31649;&#23427;&#20204;&#20045;&#19968;&#30475;&#20284;&#20046;&#19981;&#30456;&#20851;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#26497;&#20540;&#29109;&#26368;&#20248;&#20256;&#36755;&#36317;&#31163;&#23450;&#20041;&#30340;&#20989;&#25968;&#31561;&#20215;&#20110;&#36895;&#29575;-&#22833;&#30495;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#39564;&#35777;&#20102;&#36825;&#20010;&#32467;&#26524;&#65292;&#20197;&#21450;&#23558;&#33945;&#26085;&#38382;&#39064;&#21644;&#22350;&#25176;&#27931;&#32500;&#22855;&#38382;&#39064;&#19982;&#26368;&#20248;&#26631;&#37327;&#37327;&#21270;&#30456;&#36830;&#30340;&#20808;&#21069;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#21508;&#33258;&#30340;&#26368;&#20248;&#20256;&#36755;&#27714;&#35299;&#22120;&#65292;&#20197;&#19968;&#31181;&#26367;&#20195;&#30340;&#26041;&#24335;&#32479;&#19968;&#20102;&#35299;&#20915;&#26631;&#37327;&#37327;&#21270;&#21644;&#36895;&#29575;-&#22833;&#30495;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We discuss a relationship between rate-distortion and optimal transport (OT) theory, even though they seem to be unrelated at first glance. In particular, we show that a function defined via an extremal entropic OT distance is equivalent to the rate-distortion function. We numerically verify this result as well as previous results that connect the Monge and Kantorovich problems to optimal scalar quantization. Thus, we unify solving scalar quantization and rate-distortion functions in an alternative fashion by using their respective optimal transport solvers.
&lt;/p&gt;</description></item><item><title>UTrans&#26159;&#19968;&#31181;&#32479;&#19968;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#33021;&#26816;&#27979;&#21487;&#36716;&#31227;&#21464;&#37327;&#21644;&#28304;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#20272;&#35745;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.00238</link><description>&lt;p&gt;
&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#30340;&#32479;&#19968;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Unified Transfer Learning Models for High-Dimensional Linear Regression. (arXiv:2307.00238v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00238
&lt;/p&gt;
&lt;p&gt;
UTrans&#26159;&#19968;&#31181;&#32479;&#19968;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#33021;&#26816;&#27979;&#21487;&#36716;&#31227;&#21464;&#37327;&#21644;&#28304;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#20272;&#35745;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#24403;&#30446;&#26631;&#25968;&#25454;&#31232;&#32570;&#32780;&#28304;&#25968;&#25454;&#20805;&#36275;&#65292;&#25110;&#32773;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#36716;&#31227;&#23398;&#20064;&#22312;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#32479;&#19968;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#31216;&#20026;UTrans&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26816;&#27979;&#21487;&#36716;&#31227;&#21464;&#37327;&#21644;&#28304;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20272;&#35745;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#30028;&#38480;&#20302;&#20110;&#20165;&#26377;&#30446;&#26631;&#25968;&#25454;&#30340;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#20551;&#35774;&#26816;&#39564;&#25552;&#20986;&#20102;&#19968;&#31181;&#28304;&#25968;&#25454;&#26816;&#27979;&#31639;&#27861;&#65292;&#29992;&#20110;&#25490;&#38500;&#19981;&#21487;&#36716;&#31227;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;UTrans&#19982;&#29616;&#26377;&#31639;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;UTrans&#22312;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#30340;&#21516;&#26102;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#20272;&#35745;&#21644;&#39044;&#27979;&#35823;&#24046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#32654;&#22269;&#20195;&#38469;&#27969;&#21160;&#25968;&#25454;&#65292;&#24182;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#19982;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning plays a key role in modern data analysis when: (1) the target data are scarce but the source data are sufficient; (2) the distributions of the source and target data are heterogeneous. This paper develops an interpretable unified transfer learning model, termed as UTrans, which can detect both transferable variables and source data. More specifically, we establish the estimation error bounds and prove that our bounds are lower than those with target data only. Besides, we propose a source detection algorithm based on hypothesis testing to exclude the nontransferable data. We evaluate and compare UTrans to the existing algorithms in multiple experiments. It is shown that UTrans attains much lower estimation and prediction errors than the existing methods, while preserving interpretability. We finally apply it to the US intergenerational mobility data and compare our proposed algorithms to the classical machine learning algorithms.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#27668;&#20307;&#20351;&#29992;&#20272;&#35745;&#28608;&#21169;&#26426;&#21046;&#65292;&#26088;&#22312;&#35299;&#20915;&#29123;&#27668;&#20844;&#21496;&#21644;&#20379;&#26262;&#31449;&#22312;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#31215;&#26497;&#21442;&#19982;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#21442;&#19982;&#32773;&#30340;&#36129;&#29486;&#26469;&#25903;&#25345;&#27700;&#24179;&#21644;&#22402;&#30452;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.00233</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#27425;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#27668;&#20307;&#20351;&#29992;&#20272;&#35745;&#28608;&#21169;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Federated Learning Incentivization for Gas Usage Estimation. (arXiv:2307.00233v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00233
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#27668;&#20307;&#20351;&#29992;&#20272;&#35745;&#28608;&#21169;&#26426;&#21046;&#65292;&#26088;&#22312;&#35299;&#20915;&#29123;&#27668;&#20844;&#21496;&#21644;&#20379;&#26262;&#31449;&#22312;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#31215;&#26497;&#21442;&#19982;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#21442;&#19982;&#32773;&#30340;&#36129;&#29486;&#26469;&#25903;&#25345;&#27700;&#24179;&#21644;&#22402;&#30452;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#20272;&#35745;&#27668;&#20307;&#20351;&#29992;&#37327;&#23545;&#20110;&#22825;&#28982;&#27668;&#37197;&#36865;&#32593;&#32476;&#30340;&#39640;&#25928;&#36816;&#34892;&#21644;&#33410;&#32422;&#36816;&#33829;&#25104;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#38598;&#20013;&#24335;&#25968;&#25454;&#22788;&#29702;&#65292;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#12290;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#20351;&#27599;&#20010;&#21442;&#19982;&#32773;&#65288;&#22914;&#29123;&#27668;&#20844;&#21496;&#21644;&#20379;&#26262;&#31449;&#65289;&#33021;&#22815;&#36827;&#34892;&#26412;&#22320;&#25968;&#25454;&#22788;&#29702;&#65292;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#26412;&#22320;&#35757;&#32451;&#21644;&#36890;&#20449;&#24320;&#38144;&#21487;&#33021;&#20250;&#20351;&#29123;&#27668;&#20844;&#21496;&#21644;&#20379;&#26262;&#31449;&#23545;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#36807;&#31243;&#19981;&#22826;&#31215;&#26497;&#21442;&#19982;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#27668;&#20307;&#20351;&#29992;&#20272;&#35745;&#28608;&#21169;&#26426;&#21046;&#65288;HI-GAS&#65289;&#65292;&#23427;&#22312;&#22825;&#28982;&#27668;&#21644;&#32511;&#33394;&#33021;&#28304;&#34892;&#19994;&#30340;&#39046;&#20808;&#20225;&#19994;&#20043;&#19968;&#8212;&#8212;ENN&#38598;&#22242;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;&#23427;&#26088;&#22312;&#25903;&#25345;&#29123;&#27668;&#20844;&#21496;&#20043;&#38388;&#30340;&#27700;&#24179;&#32852;&#37030;&#23398;&#20064;&#65292;&#20197;&#21450;&#22312;&#23618;&#27425;&#21270;&#32852;&#37030;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#27599;&#20010;&#29123;&#27668;&#20844;&#21496;&#21644;&#20379;&#26262;&#31449;&#20043;&#38388;&#30340;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65292;&#24182;&#26681;&#25454;&#20182;&#20204;&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#36129;&#29486;&#23545;&#21442;&#19982;&#32773;&#36827;&#34892;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately estimating gas usage is essential for the efficient functioning of gas distribution networks and saving operational costs. Traditional methods rely on centralized data processing, which poses privacy risks. Federated learning (FL) offers a solution to this problem by enabling local data processing on each participant, such as gas companies and heating stations. However, local training and communication overhead may discourage gas companies and heating stations from actively participating in the FL training process. To address this challenge, we propose a Hierarchical FL Incentive Mechanism for Gas Usage Estimation (HI-GAS), which has been testbedded in the ENN Group, one of the leading players in the natural gas and green energy industry. It is designed to support horizontal FL among gas companies, and vertical FL among each gas company and heating station within a hierarchical FL ecosystem, rewarding participants based on their contributions to FL. In addition, a hierarchic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#20013;&#24212;&#29992;&#21069;&#21521;&#21069;&#21521;&#31639;&#27861;&#65288;FFA&#65289;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35745;&#31639;&#23616;&#37096;&#20248;&#21183;&#20989;&#25968;&#26469;&#20943;&#36731;&#23545;&#35745;&#31639;&#36164;&#28304;&#21644;&#26550;&#26500;&#25193;&#23637;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2307.00231</link><description>&lt;p&gt;
&#21069;&#21521;&#21069;&#21521;&#31639;&#27861;&#29992;&#20110;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#65306;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Forward-Forward Algorithm for Hyperspectral Image Classification: A Preliminary Study. (arXiv:2307.00231v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#20013;&#24212;&#29992;&#21069;&#21521;&#21069;&#21521;&#31639;&#27861;&#65288;FFA&#65289;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35745;&#31639;&#23616;&#37096;&#20248;&#21183;&#20989;&#25968;&#26469;&#20943;&#36731;&#23545;&#35745;&#31639;&#36164;&#28304;&#21644;&#26550;&#26500;&#25193;&#23637;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#31070;&#32463;&#32593;&#32476;&#20013;&#20248;&#21270;&#26435;&#37325;&#21644;&#20559;&#24046;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#23588;&#20854;&#22312;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#12290;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#36965;&#24863;&#31561;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#26041;&#38754;&#24341;&#21457;&#20102;&#38761;&#21629;&#12290;&#21453;&#21521;&#20256;&#25773;&#30340;&#27969;&#34892;&#28304;&#20110;&#23427;&#22312;&#20998;&#31867;&#12289;&#26816;&#27979;&#21644;&#20998;&#21106;&#31561;&#20219;&#21153;&#20013;&#33021;&#22815;&#23454;&#29616;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21453;&#21521;&#20256;&#25773;&#24182;&#38750;&#27809;&#26377;&#38480;&#21046;&#65292;&#21253;&#25324;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#25935;&#24863;&#24615;&#12289;&#26799;&#24230;&#28040;&#22833;&#12289;&#36807;&#25311;&#21512;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#19968;&#31181;&#21069;&#21521;&#21069;&#21521;&#31639;&#27861;&#65288;FFA&#65289;&#36890;&#36807;&#35745;&#31639;&#23616;&#37096;&#20248;&#21183;&#20989;&#25968;&#26469;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#23545;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#21644;&#19981;&#26029;&#25193;&#23637;&#26550;&#26500;&#30340;&#20381;&#36182;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;FFA&#22312;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The back-propagation algorithm has long been the de-facto standard in optimizing weights and biases in neural networks, particularly in cutting-edge deep learning models. Its widespread adoption in fields like natural language processing, computer vision, and remote sensing has revolutionized automation in various tasks. The popularity of back-propagation stems from its ability to achieve outstanding performance in tasks such as classification, detection, and segmentation. Nevertheless, back-propagation is not without its limitations, encompassing sensitivity to initial conditions, vanishing gradients, overfitting, and computational complexity. The recent introduction of a forward-forward algorithm (FFA), which computes local goodness functions to optimize network parameters, alleviates the dependence on substantial computational resources and the constant need for architectural scaling. This study investigates the application of FFA for hyperspectral image classification. Experimental
&lt;/p&gt;</description></item><item><title>InferTurbo&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#31995;&#32479;&#65292;&#29992;&#20110;&#22788;&#29702;&#24037;&#19994;&#22330;&#26223;&#20013;&#22823;&#35268;&#27169;&#22270;&#19978;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#20219;&#21153;&#12290;&#23427;&#37319;&#29992;&#31867;&#20284;GAS&#27169;&#24335;&#30340;&#35745;&#31639;&#33539;&#24335;&#21644;&#25968;&#25454;&#27969;&#25551;&#36848;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#26041;&#24335;&#36827;&#34892;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2307.00228</link><description>&lt;p&gt;
InferTurbo: &#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#22270;&#19978;&#22270;&#31070;&#32463;&#32593;&#32476;&#20840;&#22270;&#25512;&#26029;&#30340;&#21487;&#25193;&#23637;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
InferTurbo: A Scalable System for Boosting Full-graph Inference of Graph Neural Network over Huge Graphs. (arXiv:2307.00228v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00228
&lt;/p&gt;
&lt;p&gt;
InferTurbo&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#31995;&#32479;&#65292;&#29992;&#20110;&#22788;&#29702;&#24037;&#19994;&#22330;&#26223;&#20013;&#22823;&#35268;&#27169;&#22270;&#19978;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#20219;&#21153;&#12290;&#23427;&#37319;&#29992;&#31867;&#20284;GAS&#27169;&#24335;&#30340;&#35745;&#31639;&#33539;&#24335;&#21644;&#25968;&#25454;&#27969;&#25551;&#36848;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#26041;&#24335;&#36827;&#34892;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#25512;&#26029;&#20219;&#21153;&#22312;&#24037;&#19994;&#22330;&#26223;&#20013;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#24040;&#22823;&#30340;&#22270;&#19978;&#65292;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#12289;&#19981;&#19968;&#33268;&#24615;&#21644;&#20887;&#20313;&#35745;&#31639;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InferTurbo&#30340;&#21487;&#25193;&#23637;&#31995;&#32479;&#65292;&#29992;&#20110;&#25552;&#21319;&#24037;&#19994;&#22330;&#26223;&#20013;&#30340;GNN&#25512;&#26029;&#20219;&#21153;&#12290;&#21463;&#8220;&#20687;&#39030;&#28857;&#19968;&#26679;&#24605;&#32771;&#8221;&#30340;&#21746;&#23398;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;GAS&#65288;Gather-Apply-Scatter&#65289;&#27169;&#24335;&#30340;&#35745;&#31639;&#33539;&#24335;&#21644;&#25968;&#25454;&#27969;&#25551;&#36848;&#26041;&#27861;&#12290;&#36890;&#36807;&#36845;&#20195;&#26041;&#24335;&#36827;&#34892;GNN&#30340;&#35745;&#31639;&#65292;&#39030;&#28857;&#36890;&#36807;&#20837;&#36793;&#25910;&#38598;&#28040;&#24687;&#65292;&#36890;&#36807;&#20256;&#36882;&#30456;&#20851;&#30340;GNN&#23618;&#21644;&#36825;&#20123;&#28040;&#24687;&#26469;&#26356;&#26032;&#29366;&#24577;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20986;&#36793;&#23558;&#26356;&#26032;&#21518;&#30340;&#20449;&#24687;&#21457;&#36865;&#32473;&#20854;&#20182;&#39030;&#28857;&#12290;&#25353;&#29031;&#36825;&#31181;&#27169;&#24335;&#65292;InferTurbo&#21487;&#20197;&#20351;&#29992;&#26367;&#20195;&#21518;&#31471;&#65288;&#22914;&#25209;&#22788;&#29702;&#65289;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
GNN inference is a non-trivial task, especially in industrial scenarios with giant graphs, given three main challenges, i.e., scalability tailored for full-graph inference on huge graphs, inconsistency caused by stochastic acceleration strategies (e.g., sampling), and the serious redundant computation issue. To address the above challenges, we propose a scalable system named InferTurbo to boost the GNN inference tasks in industrial scenarios. Inspired by the philosophy of ``think-like-a-vertex", a GAS-like (Gather-Apply-Scatter) schema is proposed to describe the computation paradigm and data flow of GNN inference. The computation of GNNs is expressed in an iteration manner, in which a vertex would gather messages via in-edges and update its state information by forwarding an associated layer of GNNs with those messages and then send the updated information to other vertexes via out-edges. Following the schema, the proposed InferTurbo can be built with alternative backends (e.g., batch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#27631;&#20132;&#38598;&#65292;&#24182;&#32467;&#21512;&#20102;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#30340;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;EEMBI-PC&#65292;&#23427;&#26159;EEMBI&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#23558;PC&#31639;&#27861;&#30340;&#26368;&#21518;&#19968;&#27493;&#38598;&#25104;&#21040;EEMBI&#20013;&#12290;</title><link>http://arxiv.org/abs/2307.00227</link><description>&lt;p&gt;
&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#27631;&#20132;&#38598;&#36827;&#34892;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Causal Structure Learning by Using Intersection of Markov Blankets. (arXiv:2307.00227v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#27631;&#20132;&#38598;&#65292;&#24182;&#32467;&#21512;&#20102;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#30340;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;EEMBI-PC&#65292;&#23427;&#26159;EEMBI&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#23558;PC&#31639;&#27861;&#30340;&#26368;&#21518;&#19968;&#27493;&#38598;&#25104;&#21040;EEMBI&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#20869;&#28304;&#21644;&#22806;&#28304;&#39532;&#23572;&#21487;&#22827;&#27631;&#20132;&#38598;&#65288;EEMBI&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#30340;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EEMBI&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#21363;EEMBI-PC&#65292;&#23427;&#23558;PC&#31639;&#27861;&#30340;&#26368;&#21518;&#19968;&#27493;&#38598;&#25104;&#21040;EEMBI&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel causal structure learning algorithm called Endogenous and Exogenous Markov Blankets Intersection (EEMBI), which combines the properties of Bayesian networks and Structural Causal Models (SCM). Furthermore, we propose an extended version of EEMBI, namely EEMBI-PC, which integrates the last step of the PC algorithm into EEMBI.
&lt;/p&gt;</description></item><item><title>S-Omninet&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#25968;&#25454;&#22686;&#24378;&#30340;&#36890;&#29992;&#22810;&#27169;&#24577;&#23398;&#20064;&#26550;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#20132;&#21449;&#32531;&#23384;&#27880;&#24847;&#21147;&#12289;&#25972;&#21512;&#35270;&#35273;&#36755;&#20837;&#30340;&#22359;&#23884;&#20837;&#21644;&#25903;&#25345;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;&#27169;&#24577;&#21644;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#26469;&#33258;&#21508;&#20010;&#32500;&#24230;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2307.00226</link><description>&lt;p&gt;
S-Omninet: &#32467;&#26500;&#21270;&#25968;&#25454;&#22686;&#24378;&#30340;&#36890;&#29992;&#22810;&#27169;&#24577;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
S-Omninet: Structured Data Enhanced Universal Multimodal Learning Architecture. (arXiv:2307.00226v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00226
&lt;/p&gt;
&lt;p&gt;
S-Omninet&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#25968;&#25454;&#22686;&#24378;&#30340;&#36890;&#29992;&#22810;&#27169;&#24577;&#23398;&#20064;&#26550;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#20132;&#21449;&#32531;&#23384;&#27880;&#24847;&#21147;&#12289;&#25972;&#21512;&#35270;&#35273;&#36755;&#20837;&#30340;&#22359;&#23884;&#20837;&#21644;&#25903;&#25345;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;&#27169;&#24577;&#21644;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#26469;&#33258;&#21508;&#20010;&#32500;&#24230;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#23398;&#20064;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#21333;&#27169;&#24577;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#22810;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#36827;&#23637;&#12290;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#22810;&#27169;&#24577;&#23398;&#20064;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#26426;&#20250;&#12290;&#35768;&#22810;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#22788;&#29702;&#29305;&#23450;&#31867;&#22411;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#22914;&#35270;&#35273;&#21644;&#35821;&#35328;&#25968;&#25454;&#12290;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#35774;&#35745;&#29992;&#20110;&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;&#27169;&#24577;&#21644;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20132;&#21449;&#32531;&#23384;&#27880;&#24847;&#21147;&#12289;&#25972;&#21512;&#35270;&#35273;&#36755;&#20837;&#30340;&#22359;&#23884;&#20837;&#21644;&#25903;&#25345;&#32467;&#26500;&#21270;&#25968;&#25454;&#26469;&#25193;&#23637;&#21644;&#25913;&#36827;Omninet&#65292;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;&#27169;&#24577;&#21644;&#20219;&#21153;&#30340;&#26550;&#26500;&#12290;&#25552;&#20986;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#22686;&#24378;&#30340;Omninet (S-Omninet) &#26159;&#19968;&#31181;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#20132;&#21449;&#32531;&#23384;&#27880;&#24847;&#21147;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#21508;&#20010;&#32500;&#24230;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal multitask learning has attracted an increasing interest in recent years. Singlemodal models have been advancing rapidly and have achieved astonishing results on various tasks across multiple domains. Multimodal learning offers opportunities for further improvements by integrating data from multiple modalities. Many methods are proposed to learn on a specific type of multimodal data, such as vision and language data. A few of them are designed to handle several modalities and tasks at a time. In this work, we extend and improve Omninet, an architecture that is capable of handling multiple modalities and tasks at a time, by introducing cross-cache attention, integrating patch embeddings for vision inputs, and supporting structured data. The proposed Structured-data-enhanced Omninet (S-Omninet) is a universal model that is capable of learning from structured data of various dimensions effectively with unstructured data through cross-cache attention, which enables interactions a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#21644;&#37325;&#26032;&#35774;&#35745;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#22270;&#25193;&#25955;&#20989;&#25968;&#31354;&#38388;&#20013;&#24341;&#20837;&#21464;&#20998;&#20998;&#26512;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#35299;&#20915;&#20102;GNN&#22312;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#21644;&#20840;&#23616;&#27169;&#24335;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#26469;&#35299;&#20915;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#20855;&#26377;&#25968;&#23398;&#20445;&#35777;&#30340;&#31163;&#25955;&#28145;&#24230;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.00222</link><description>&lt;p&gt;
&#22312;&#36830;&#32493;&#22270;&#25193;&#25955;&#20989;&#25968;&#31354;&#38388;&#20013;&#37325;&#26032;&#24605;&#32771;&#21644;&#37325;&#26032;&#35774;&#35745;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Re-Think and Re-Design Graph Neural Networks in Spaces of Continuous Graph Diffusion Functionals. (arXiv:2307.00222v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#21644;&#37325;&#26032;&#35774;&#35745;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#22270;&#25193;&#25955;&#20989;&#25968;&#31354;&#38388;&#20013;&#24341;&#20837;&#21464;&#20998;&#20998;&#26512;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#35299;&#20915;&#20102;GNN&#22312;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#21644;&#20840;&#23616;&#27169;&#24335;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#26469;&#35299;&#20915;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#20855;&#26377;&#25968;&#23398;&#20445;&#35777;&#30340;&#31163;&#25955;&#28145;&#24230;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#31038;&#20132;&#32593;&#32476;&#21644;&#29983;&#29289;&#31995;&#32479;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;GNN&#30340;&#23616;&#37096;&#24615;&#20551;&#35774;&#23558;&#20449;&#24687;&#20132;&#25442;&#38480;&#21046;&#22312;&#30456;&#37051;&#33410;&#28857;&#20043;&#38388;&#65292;&#21046;&#32422;&#20102;&#20854;&#25429;&#25417;&#22270;&#20013;&#38271;&#31243;&#20381;&#36182;&#21644;&#20840;&#23616;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#20998;&#26512;&#30340;&#26032;&#24402;&#32435;&#20559;&#32622;&#65292;&#21463;&#21040;&#20102;Brachistochrone&#38382;&#39064;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24314;&#31435;&#20102;&#31163;&#25955;GNN&#27169;&#22411;&#21644;&#36830;&#32493;&#25193;&#25955;&#20989;&#25968;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;&#36825;&#20351;&#24471;&#33021;&#22815;&#22312;&#36830;&#32493;&#22495;&#35774;&#35745;&#24212;&#29992;&#29305;&#23450;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#26500;&#24314;&#20855;&#26377;&#25968;&#23398;&#20445;&#35777;&#30340;&#31163;&#25955;&#28145;&#24230;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;GNN&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#36880;&#23618;&#22270;&#23884;&#20837;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#31561;&#20215;&#20110;&#22270;&#26799;&#24230;&#30340;l2-&#33539;&#25968;&#31215;&#20998;&#27867;&#20989;&#65292;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#12290;&#31867;&#20284;&#20110;&#22270;&#20687;&#21435;&#22122;&#20013;&#30340;&#36793;&#20445;&#25345;&#28388;&#27874;&#22120;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are widely used in domains like social networks and biological systems. However, the locality assumption of GNNs, which limits information exchange to neighboring nodes, hampers their ability to capture long-range dependencies and global patterns in graphs. To address this, we propose a new inductive bias based on variational analysis, drawing inspiration from the Brachistochrone problem. Our framework establishes a mapping between discrete GNN models and continuous diffusion functionals. This enables the design of application-specific objective functions in the continuous domain and the construction of discrete deep models with mathematical guarantees. To tackle over-smoothing in GNNs, we analyze the existing layer-by-layer graph embedding models and identify that they are equivalent to l2-norm integral functionals of graph gradients, which cause over-smoothing. Similar to edge-preserving filters in image denoising, we introduce total variation (TV) to ali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#20102;&#19968;&#31181;&#26500;&#36896;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;&#31995;&#32479;&#21160;&#21147;&#23398;&#26469;&#21051;&#30011;&#21487;&#20197;&#23454;&#29616;&#30340;&#20989;&#25968;&#31867;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#39640;&#22797;&#26434;&#24230;&#30340;&#25511;&#21046;&#12290;&#23454;&#29616;&#26041;&#27861;&#21253;&#25324;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#12289;&#30830;&#23450;&#24615;&#21160;&#21147;&#31995;&#32479;&#21644;&#36755;&#20986;&#26144;&#23556;&#30340;&#32423;&#32852;&#36830;&#25509;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#25552;&#39640;&#20989;&#25968;&#36924;&#36817;&#31639;&#27861;&#30340;&#23454;&#38469;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.00215</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#20989;&#25968;&#23454;&#29616;&#30340;&#26500;&#36896;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Constructive Approach to Function Realization by Neural Stochastic Differential Equations. (arXiv:2307.00215v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#20102;&#19968;&#31181;&#26500;&#36896;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;&#31995;&#32479;&#21160;&#21147;&#23398;&#26469;&#21051;&#30011;&#21487;&#20197;&#23454;&#29616;&#30340;&#20989;&#25968;&#31867;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#39640;&#22797;&#26434;&#24230;&#30340;&#25511;&#21046;&#12290;&#23454;&#29616;&#26041;&#27861;&#21253;&#25324;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#12289;&#30830;&#23450;&#24615;&#21160;&#21147;&#31995;&#32479;&#21644;&#36755;&#20986;&#26144;&#23556;&#30340;&#32423;&#32852;&#36830;&#25509;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#25552;&#39640;&#20989;&#25968;&#36924;&#36817;&#31639;&#27861;&#30340;&#23454;&#38469;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#21160;&#21147;&#31995;&#32479;&#36827;&#34892;&#20989;&#25968;&#36924;&#36817;&#30340;&#38382;&#39064;&#36890;&#24120;&#37319;&#29992;&#33258;&#19978;&#32780;&#19979;&#30340;&#26041;&#27861;&#65306;&#29992;&#32473;&#23450;&#32467;&#26500;&#30340;&#22797;&#26434;&#27169;&#22411;&#21487;&#20197;&#23558;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#36924;&#36817;&#21040;&#20219;&#24847;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#24212;&#29992;&#20013;&#19981;&#23454;&#38469;&#30340;&#39640;&#22797;&#26434;&#24230;&#25511;&#21046;&#12290;&#26412;&#25991;&#37319;&#29992;&#30456;&#21453;&#30340;&#26500;&#36896;&#24615;&#26041;&#27861;&#65306;&#25105;&#20204;&#23545;&#31995;&#32479;&#21160;&#21147;&#23398;&#26045;&#21152;&#21508;&#31181;&#32467;&#26500;&#38480;&#21046;&#65292;&#20174;&#32780;&#21051;&#30011;&#20102;&#21487;&#20197;&#36890;&#36807;&#36825;&#31181;&#31995;&#32479;&#23454;&#29616;&#30340;&#20989;&#25968;&#31867;&#12290;&#31995;&#32479;&#23454;&#29616;&#20026;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDE&#65289;&#12289;&#30830;&#23450;&#24615;&#21160;&#21147;&#31995;&#32479;&#21644;&#19968;&#20010;&#36755;&#20986;&#26144;&#23556;&#30340;&#32423;&#32852;&#36830;&#25509;&#12290;&#37319;&#29992;&#27010;&#29575;&#21644;&#20960;&#20309;&#65288;&#26446;&#35770;&#65289;&#26041;&#27861;&#26469;&#21051;&#30011;&#36825;&#20123;&#31995;&#32479;&#23454;&#29616;&#30340;&#20989;&#25968;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of function approximation by neural dynamical systems has typically been approached in a top-down manner: Any continuous function can be approximated to an arbitrary accuracy by a sufficiently complex model with a given architecture. This can lead to high-complexity controls which are impractical in applications. In this paper, we take the opposite, constructive approach: We impose various structural restrictions on system dynamics and consequently characterize the class of functions that can be realized by such a system. The systems are implemented as a cascade interconnection of a neural stochastic differential equation (Neural SDE), a deterministic dynamical system, and a readout map. Both probabilistic and geometric (Lie-theoretic) methods are used to characterize the classes of functions realized by such systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#32039;&#20945;&#21367;&#31215;&#36716;&#25442;&#22120;&#65288;CCT&#65289;&#22312;&#26377;&#38480;&#25968;&#25454;&#26465;&#20214;&#19979;&#36827;&#34892;&#31283;&#20581;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#23558;&#36716;&#25442;&#22120;&#19982;&#21367;&#31215;&#23618;&#32467;&#21512;&#65292;CCT&#34920;&#29616;&#20986;&#20102;&#22312;&#35268;&#27169;&#36866;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.00213</link><description>&lt;p&gt;
&#26356;&#23569;&#33719;&#21462;&#26356;&#22810;&#65306;&#32039;&#20945;&#21367;&#31215;&#36716;&#25442;&#22120;&#22312;&#26377;&#38480;&#25968;&#25454;&#26465;&#20214;&#19979;&#23454;&#29616;&#31283;&#20581;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
More for Less: Compact Convolutional Transformers Enable Robust Medical Image Classification with Limited Data. (arXiv:2307.00213v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#32039;&#20945;&#21367;&#31215;&#36716;&#25442;&#22120;&#65288;CCT&#65289;&#22312;&#26377;&#38480;&#25968;&#25454;&#26465;&#20214;&#19979;&#36827;&#34892;&#31283;&#20581;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#23558;&#36716;&#25442;&#22120;&#19982;&#21367;&#31215;&#23618;&#32467;&#21512;&#65292;CCT&#34920;&#29616;&#20986;&#20102;&#22312;&#35268;&#27169;&#36866;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#25442;&#22120;&#26159;&#19968;&#31181;&#38750;&#24120;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20174;&#25991;&#26412;&#29983;&#25104;&#21040;&#22270;&#20687;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#36716;&#25442;&#22120;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#33719;&#21462;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#24120;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#26114;&#36149;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#32039;&#20945;&#21367;&#31215;&#36716;&#25442;&#22120;&#65288;CCT&#65289;&#22312;&#26377;&#38480;&#25968;&#25454;&#26465;&#20214;&#19979;&#36827;&#34892;&#31283;&#20581;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#26377;&#25928;&#24615;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;Vision Transformers&#38754;&#20020;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#23427;&#20204;&#23545;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#35201;&#27714;&#12290;&#20316;&#20026;&#36716;&#25442;&#22120;&#21644;&#21367;&#31215;&#23618;&#30340;&#28151;&#21512;&#20307;&#65292;CCT&#22312;&#35268;&#27169;&#36866;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#20843;&#31181;&#19981;&#21516;&#32454;&#32990;&#31867;&#22411;&#30340;&#22806;&#21608;&#34880;&#32454;&#32990;&#22270;&#20687;&#65292;&#27599;&#31181;&#32454;&#32990;&#31867;&#22411;&#26377;&#32422;2000&#20010;&#20302;&#20998;&#36776;&#29575;&#65288;28x28x3&#20687;&#32032;&#65289;&#26679;&#26412;&#12290;&#23613;&#31649;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#27604;&#36890;&#24120;&#19982;Vision Transformers&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#35201;&#23567;&#65292;&#25105;&#20204;&#36798;&#21040;&#20102;&#21487;&#35266;&#30340;92.49%&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#21644;mi&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are very powerful tools for a variety of tasks across domains, from text generation to image captioning. However, transformers require substantial amounts of training data, which is often a challenge in biomedical settings, where high quality labeled data can be challenging or expensive to obtain. This study investigates the efficacy of Compact Convolutional Transformers (CCT) for robust medical image classification with limited data, addressing a key issue faced by conventional Vision Transformers - their requirement for large datasets. A hybrid of transformers and convolutional layers, CCTs demonstrate high accuracy on modestly sized datasets. We employed a benchmark dataset of peripheral blood cell images of eight distinct cell types, each represented by approximately 2,000 low-resolution (28x28x3 pixel) samples. Despite the dataset size being smaller than those typically used with Vision Transformers, we achieved a commendable classification accuracy of 92.49% and a mi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#30340;&#26500;&#36896;&#31639;&#27861;&#65292;&#36890;&#36807;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#21644;&#33410;&#28857;&#27744;&#31574;&#30053;&#35299;&#20915;&#20102;&#38590;&#20197;&#35299;&#37322;&#38544;&#34255;&#21442;&#25968;&#19982;&#27531;&#24046;&#35823;&#24046;&#20043;&#38388;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00185</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#24212;&#29992;&#30340;&#26500;&#36896;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Interpretable Constructive Algorithm for Incremental Random Weight Neural Networks and Its Application. (arXiv:2307.00185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#30340;&#26500;&#36896;&#31639;&#27861;&#65292;&#36890;&#36807;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#21644;&#33410;&#28857;&#27744;&#31574;&#30053;&#35299;&#20915;&#20102;&#38590;&#20197;&#35299;&#37322;&#38544;&#34255;&#21442;&#25968;&#19982;&#27531;&#24046;&#35823;&#24046;&#20043;&#38388;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;(IRWNNs)&#30001;&#20110;&#26131;&#20110;&#23454;&#29616;&#21644;&#24555;&#36895;&#23398;&#20064;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;IRWNNs&#30340;&#19968;&#20010;&#26174;&#33879;&#32570;&#28857;&#26159;&#38590;&#20197;&#35299;&#37322;&#38544;&#34255;&#21442;&#25968;&#65288;&#33410;&#28857;&#65289;&#19982;&#27531;&#24046;&#35823;&#24046;&#65288;&#27169;&#22411;&#24615;&#33021;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#30340;&#21487;&#35299;&#37322;&#30340;&#26500;&#36896;&#31639;&#27861;(ICA)&#12290;&#39318;&#20808;&#65292;&#22522;&#20110;&#38544;&#34255;&#21442;&#25968;&#19982;&#27531;&#24046;&#35823;&#24046;&#20043;&#38388;&#30340;&#20960;&#20309;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#26469;&#38543;&#26426;&#20998;&#37197;&#38544;&#34255;&#21442;&#25968;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#33410;&#28857;&#27744;&#31574;&#30053;&#33719;&#21462;&#26356;&#26377;&#21033;&#20110;&#25910;&#25947;&#30340;&#38544;&#34255;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#35777;&#26126;&#20102;ICA&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;ICA&#30340;&#36731;&#37327;&#32423;&#29256;&#26412;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24314;&#27169;&#20219;&#21153;&#12290;&#22312;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incremental random weight neural networks (IRWNNs) have gained attention in view of its easy implementation and fast learning. However, a significant drawback of IRWNNs is that the elationship between the hidden parameters (node)and the residual error (model performance) is difficult to be interpreted. To address the above issue, this article proposes an interpretable constructive algorithm (ICA) with geometric information constraint. First, based on the geometric relationship between the hidden parameters and the residual error, an interpretable geometric information constraint is proposed to randomly assign the hidden parameters. Meanwhile, a node pool strategy is employed to obtain hidden parameters that is more conducive to convergence from hidden parameters satisfying the proposed constraint. Furthermore, the universal approximation property of the ICA is proved. Finally, a lightweight version of ICA is presented for large-scale data modeling tasks. Experimental results on six ben
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#20449;&#24565;&#20197;&#21450;&#22914;&#20309;&#34913;&#37327;&#23427;&#20204;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#21644;&#23545;&#26368;&#26032;&#35770;&#35777;&#30340;&#20998;&#26512;&#65292;&#25351;&#20986;&#29616;&#22312;&#20173;&#28982;&#27809;&#26377;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35854;&#35328;&#25506;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2307.00175</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#27809;&#26377;&#35854;&#35328;&#25506;&#27979;&#22120;&#65306;&#25506;&#31350;&#32463;&#39564;&#21644;&#27010;&#24565;&#19978;&#30340;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks. (arXiv:2307.00175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#20449;&#24565;&#20197;&#21450;&#22914;&#20309;&#34913;&#37327;&#23427;&#20204;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#21644;&#23545;&#26368;&#26032;&#35770;&#35777;&#30340;&#20998;&#26512;&#65292;&#25351;&#20986;&#29616;&#22312;&#20173;&#28982;&#27809;&#26377;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35854;&#35328;&#25506;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#21542;&#20855;&#26377;&#20449;&#24565;&#20197;&#21450;&#22914;&#20309;&#34913;&#37327;&#23427;&#20204;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;Azaria&#21644;Mitchell&#65288;2023&#65289;&#20197;&#21450;Burns&#31561;&#20154;&#65288;2022&#65289;&#25552;&#20986;&#30340;&#20004;&#31181;&#29616;&#26377;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;&#22522;&#26412;&#26041;&#38754;&#26080;&#27861;&#25512;&#24191;&#12290;&#38543;&#21518;&#25105;&#20204;&#35748;&#20026;&#65292;&#21363;&#20351;LLM&#20855;&#26377;&#20449;&#24565;&#65292;&#36825;&#20123;&#26041;&#27861;&#20063;&#19981;&#22826;&#21487;&#33021;&#22312;&#27010;&#24565;&#19978;&#25104;&#21151;&#12290;&#22240;&#27492;&#65292;&#29616;&#22312;&#20173;&#28982;&#27809;&#26377;&#38024;&#23545;LLM&#30340;&#35854;&#35328;&#25506;&#27979;&#22120;&#12290;&#22312;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#21518;&#65292;&#25105;&#20204;&#36864;&#21518;&#19968;&#27493;&#65292;&#24605;&#32771;&#22312;&#39318;&#27425;&#20013;&#25105;&#20204;&#26159;&#21542;&#24212;&#35813;&#26399;&#24453;LLM&#20855;&#26377;&#31867;&#20284;&#20449;&#24565;&#30340;&#19996;&#35199;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20123;&#26088;&#22312;&#35777;&#26126;LLM&#19981;&#33021;&#26377;&#20449;&#24565;&#30340;&#26368;&#26032;&#35770;&#35777;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#35770;&#35777;&#26159;&#35823;&#23548;&#24615;&#30340;&#12290;&#25105;&#20204;&#23545;&#22260;&#32469;LLM&#20013;&#20449;&#24565;&#30340;&#38382;&#39064;&#30340;&#38382;&#39064;&#25552;&#20986;&#20102;&#26356;&#26377;&#25104;&#25928;&#30340;&#26694;&#26550;&#65292;&#24182;&#24378;&#35843;&#20102;&#35813;&#38382;&#39064;&#30340;&#32463;&#39564;&#24615;&#36136;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#24037;&#20316;&#30340;&#20855;&#20307;&#36335;&#24452;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the questions of whether or not large language models (LLMs) have beliefs, and, if they do, how we might measure them. First, we evaluate two existing approaches, one due to Azaria and Mitchell (2023) and the other to Burns et al. (2022). We provide empirical results that show that these methods fail to generalize in very basic ways. We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons. Thus, there is still no lie-detector for LLMs. After describing our empirical results we take a step back and consider whether or not we should expect LLMs to have something like beliefs in the first place. We consider some recent arguments aiming to show that LLMs cannot have beliefs. We show that these arguments are misguided. We provide a more productive framing of questions surrounding the status of beliefs in LLMs, and highlight the empirical nature of the problem. We conclude by suggesting some concrete paths for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#25512;&#29702;&#25163;&#20876;&#65292;&#29992;&#20110;&#23558;&#25512;&#29702;&#38382;&#39064;&#36716;&#21270;&#20026;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#23454;&#20363;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#25216;&#24039;&#30340;&#28436;&#31034;&#65292;&#24110;&#21161;&#35835;&#32773;&#29702;&#35299;&#22914;&#20309;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#12290;&#35770;&#25991;&#26368;&#21518;&#25552;&#20379;&#20102;&#20004;&#20010;&#31034;&#20363;&#20197;&#35828;&#26126;&#36825;&#20123;&#25216;&#24039;&#30340;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.00171</link><description>&lt;p&gt;
&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#25512;&#29702;&#25163;&#20876;
&lt;/p&gt;
&lt;p&gt;
The Integer Linear Programming Inference Cookbook. (arXiv:2307.00171v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#25512;&#29702;&#25163;&#20876;&#65292;&#29992;&#20110;&#23558;&#25512;&#29702;&#38382;&#39064;&#36716;&#21270;&#20026;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#23454;&#20363;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#25216;&#24039;&#30340;&#28436;&#31034;&#65292;&#24110;&#21161;&#35835;&#32773;&#29702;&#35299;&#22914;&#20309;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#12290;&#35770;&#25991;&#26368;&#21518;&#25552;&#20379;&#20102;&#20004;&#20010;&#31034;&#20363;&#20197;&#35828;&#26126;&#36825;&#20123;&#25216;&#24039;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#24050;&#34987;&#29992;&#20110;&#27169;&#25311;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#20013;&#30340;&#25512;&#29702;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#25351;&#23548;&#35835;&#32773;&#23558;&#26032;&#30340;&#25512;&#29702;&#38382;&#39064;&#26694;&#26550;&#21270;&#20026;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#23454;&#20363;&#65292;&#24182;&#20197;&#19968;&#31995;&#21015;&#30340;&#25216;&#24039;&#36827;&#34892;&#32452;&#32455;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#36890;&#36807;&#20004;&#20010;&#23454;&#20363;&#26469;&#35828;&#26126;&#36825;&#20123;&#25216;&#24039;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the years, integer linear programs have been employed to model inference in many natural language processing problems. This survey is meant to guide the reader through the process of framing a new inference problem as an instance of an integer linear program and is structured as a collection of recipes. At the end, we will see two worked examples to illustrate the use of these recipes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;VoxCeleb&#30340;&#24320;&#25918;&#24335;&#28436;&#35762;&#32773;&#35782;&#21035;&#22522;&#20934;&#65292;&#24182;&#25506;&#35752;&#20102;&#22788;&#29702;&#35266;&#23519;&#21517;&#21333;&#22823;&#23567;&#23545;&#26816;&#27979;&#24615;&#33021;&#24433;&#21709;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2307.00169</link><description>&lt;p&gt;
VoxWatch: VoxCeleb&#19978;&#30340;&#24320;&#25918;&#24335;&#28436;&#35762;&#32773;&#35782;&#21035;&#22522;&#20934;&#12290; (arXiv:2307.00169v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
VoxWatch: An open-set speaker recognition benchmark on VoxCeleb. (arXiv:2307.00169v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;VoxCeleb&#30340;&#24320;&#25918;&#24335;&#28436;&#35762;&#32773;&#35782;&#21035;&#22522;&#20934;&#65292;&#24182;&#25506;&#35752;&#20102;&#22788;&#29702;&#35266;&#23519;&#21517;&#21333;&#22823;&#23567;&#23545;&#26816;&#27979;&#24615;&#33021;&#24433;&#21709;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24320;&#25918;&#24335;&#28436;&#35762;&#32773;&#35782;&#21035;&#65288;OSI&#65289;&#22312;&#27450;&#35784;&#39044;&#38450;&#31561;&#24191;&#27867;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#19982;&#28436;&#35762;&#32773;&#39564;&#35777;&#65288;SV&#65289;&#30456;&#27604;&#65292;&#28436;&#35762;&#32773;&#35782;&#21035;&#31038;&#21306;&#23545;&#20854;&#20851;&#27880;&#36739;&#23569;&#12290; OSI&#28041;&#21450;&#30830;&#23450;&#27979;&#35797;&#35821;&#38899;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#19968;&#32452;&#39044;&#20808;&#27880;&#20876;&#30340;&#20010;&#20307;&#65288;&#20869;&#37096;&#38598;&#65289;&#30340;&#28436;&#35762;&#32773;&#65292;&#25110;&#32773;&#26159;&#21542;&#26469;&#33258;&#19968;&#20010;&#22806;&#37096;&#38598;&#28436;&#35762;&#32773;&#12290;&#38500;&#20102;&#19982;&#35821;&#38899;&#21464;&#24322;&#30456;&#20851;&#30340;&#20856;&#22411;&#25361;&#25112;&#22806;&#65292;OSI&#36824;&#23481;&#26131;&#20986;&#29616;&#8220;&#35823;&#25253;&#38382;&#39064;&#8221;&#65307;&#38543;&#30528;&#20869;&#37096;&#38598;&#28436;&#35762;&#32773;&#20154;&#21475;&#65288;&#20063;&#31216;&#20026;&#35266;&#23519;&#21517;&#21333;&#65289;&#30340;&#22686;&#21152;&#65292;&#22806;&#37096;&#38598;&#20998;&#25968;&#21464;&#22823;&#65292;&#23548;&#33268;&#35823;&#25253;&#29575;&#22686;&#21152;&#12290;&#36825;&#23545;&#37329;&#34701;&#26426;&#26500;&#21644;&#36793;&#22659;&#23433;&#20840;&#31561;&#24212;&#29992;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20854;&#20013;&#35266;&#23519;&#21517;&#21333;&#30340;&#22823;&#23567;&#36890;&#24120;&#20026;&#20960;&#21315;&#20010;&#28436;&#35762;&#32773;&#12290;&#22240;&#27492;&#65292;&#31995;&#32479;&#22320;&#37327;&#21270;&#35823;&#25253;&#38382;&#39064;&#24182;&#24320;&#21457;&#20943;&#36731;&#35266;&#23519;&#21517;&#21333;&#22823;&#23567;&#23545;&#26816;&#27979;&#24615;&#33021;&#24433;&#21709;&#30340;&#25216;&#26415;&#26159;&#37325;&#35201;&#30340;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Despite its broad practical applications such as in fraud prevention, open-set speaker identification (OSI) has received less attention in the speaker recognition community compared to speaker verification (SV). OSI deals with determining if a test speech sample belongs to a speaker from a set of pre-enrolled individuals (in-set) or if it is from an out-of-set speaker. In addition to the typical challenges associated with speech variability, OSI is prone to the "false-alarm problem"; as the size of the in-set speaker population (a.k.a watchlist) grows, the out-of-set scores become larger, leading to increased false alarm rates. This is in particular challenging for applications in financial institutions and border security where the watchlist size is typically of the order of several thousand speakers. Therefore, it is important to systematically quantify the false-alarm problem, and develop techniques that alleviate the impact of watchlist size on detection performance. Prior studies 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26410;&#30693;&#20195;&#29702;&#30340;&#39044;&#27979;&#38382;&#39064;&#30340;&#26032;&#24230;&#37327;&#25351;&#26631;U-&#26657;&#20934;&#65292;&#35813;&#25351;&#26631;&#33021;&#22815;&#20445;&#35777;&#25152;&#26377;&#20195;&#29702;&#20855;&#26377;&#27425;&#32447;&#24615;&#30340;&#24724;&#24680;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.00168</link><description>&lt;p&gt;
U-Calibration: &#38024;&#23545;&#26410;&#30693;&#20195;&#29702;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
U-Calibration: Forecasting for an Unknown Agent. (arXiv:2307.00168v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00168
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26410;&#30693;&#20195;&#29702;&#30340;&#39044;&#27979;&#38382;&#39064;&#30340;&#26032;&#24230;&#37327;&#25351;&#26631;U-&#26657;&#20934;&#65292;&#35813;&#25351;&#26631;&#33021;&#22815;&#20445;&#35777;&#25152;&#26377;&#20195;&#29702;&#20855;&#26377;&#27425;&#32447;&#24615;&#30340;&#24724;&#24680;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#35780;&#20272;&#38024;&#23545;&#30001;&#29702;&#24615;&#20195;&#29702;&#28040;&#36153;&#39044;&#27979;&#24182;&#26681;&#25454;&#39044;&#27979;&#37319;&#21462;&#34892;&#21160;&#30340;&#20108;&#20803;&#20107;&#20214;&#30340;&#39044;&#27979;&#38382;&#39064;&#65292;&#20294;&#26159;&#23545;&#20110;&#39044;&#27979;&#32773;&#26469;&#35828;&#29702;&#24615;&#20195;&#29702;&#30340;&#25928;&#29992;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20248;&#21270;&#21333;&#19968;&#35780;&#20998;&#35268;&#21017;&#65288;&#20363;&#22914;Brier&#24471;&#20998;&#65289;&#30340;&#39044;&#27979;&#26080;&#27861;&#20445;&#35777;&#23545;&#20110;&#25152;&#26377;&#21487;&#33021;&#30340;&#20195;&#29702;&#37117;&#20855;&#26377;&#20302;&#24724;&#24680;&#20540;&#12290;&#30456;&#21453;&#65292;&#33391;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#20445;&#35777;&#25152;&#26377;&#20195;&#29702;&#37117;&#20855;&#26377;&#27425;&#32447;&#24615;&#30340;&#24724;&#24680;&#20540;&#12290;&#28982;&#32780;&#65292;&#27492;&#22788;&#26657;&#20934;&#24182;&#19981;&#26159;&#24517;&#35201;&#26465;&#20214;&#65288;&#23545;&#20110;&#26657;&#20934;&#19981;&#33391;&#30340;&#39044;&#27979;&#65292;&#20173;&#21487;&#33021;&#20026;&#25152;&#26377;&#21487;&#33021;&#30340;&#20195;&#29702;&#25552;&#20379;&#33391;&#22909;&#30340;&#24724;&#24680;&#20540;&#20445;&#35777;&#65289;&#65292;&#32780;&#26657;&#20934;&#30340;&#39044;&#27979;&#31243;&#24207;&#19982;&#38024;&#23545;&#21333;&#19968;&#35780;&#20998;&#35268;&#21017;&#30340;&#39044;&#27979;&#31243;&#24207;&#30456;&#27604;&#20855;&#26377;&#26356;&#24046;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#35780;&#20272;&#39044;&#27979;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#31216;&#20026;U-&#26657;&#20934;&#65292;&#21363;&#22312;&#20219;&#20309;&#26377;&#30028;&#35780;&#20998;&#35268;&#21017;&#19979;&#35780;&#20272;&#26102;&#65292;&#39044;&#27979;&#24207;&#21015;&#30340;&#26368;&#22823;&#24724;&#24680;&#20540;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#27425;&#32447;&#24615;&#30340;U-&#26657;&#20934;&#35823;&#24046;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of evaluating forecasts of binary events whose predictions are consumed by rational agents who take an action in response to a prediction, but whose utility is unknown to the forecaster. We show that optimizing forecasts for a single scoring rule (e.g., the Brier score) cannot guarantee low regret for all possible agents. In contrast, forecasts that are well-calibrated guarantee that all agents incur sublinear regret. However, calibration is not a necessary criterion here (it is possible for miscalibrated forecasts to provide good regret guarantees for all possible agents), and calibrated forecasting procedures have provably worse convergence rates than forecasting procedures targeting a single scoring rule.  Motivated by this, we present a new metric for evaluating forecasts that we call U-calibration, equal to the maximal regret of the sequence of forecasts when evaluated under any bounded scoring rule. We show that sublinear U-calibration error is a necessary
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#23618;&#20013;&#32534;&#30721;&#20102;&#19981;&#21516;&#30340;&#35821;&#35328;&#20449;&#24687;&#65292;&#20063;&#23398;&#20064;&#20102;&#31867;&#20284;&#38899;&#32032;&#30340;&#23376;&#35789;&#21333;&#20803;&#12290;&#19982;&#21333;&#35789;&#30456;&#20851;&#30340;&#20449;&#24687;&#20027;&#35201;&#22312;&#20013;&#38388;&#30340;&#27169;&#22411;&#23618;&#20013;&#65292;&#21516;&#26102;&#19968;&#20123;&#20302;&#32423;&#20449;&#24687;&#22312;&#26356;&#39640;&#30340;&#23618;&#20013;&#20063;&#24471;&#20197;&#20445;&#30041;&#12290;</title><link>http://arxiv.org/abs/2307.00162</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#23545;&#21333;&#35789;&#30340;&#20102;&#35299;&#31243;&#24230;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What do self-supervised speech models know about words?. (arXiv:2307.00162v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00162
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#23618;&#20013;&#32534;&#30721;&#20102;&#19981;&#21516;&#30340;&#35821;&#35328;&#20449;&#24687;&#65292;&#20063;&#23398;&#20064;&#20102;&#31867;&#20284;&#38899;&#32032;&#30340;&#23376;&#35789;&#21333;&#20803;&#12290;&#19982;&#21333;&#35789;&#30456;&#20851;&#30340;&#20449;&#24687;&#20027;&#35201;&#22312;&#20013;&#38388;&#30340;&#27169;&#22411;&#23618;&#20013;&#65292;&#21516;&#26102;&#19968;&#20123;&#20302;&#32423;&#20449;&#24687;&#22312;&#26356;&#39640;&#30340;&#23618;&#20013;&#20063;&#24471;&#20197;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#35768;&#22810;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#65288;S3Ms&#65289;&#34987;&#24341;&#20837;&#65292;&#20026;&#21508;&#31181;&#35821;&#38899;&#20219;&#21153;&#25552;&#20379;&#20102;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#30340;&#25913;&#36827;&#12290;&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;S3Ms&#22312;&#19981;&#21516;&#30340;&#23618;&#20013;&#32534;&#30721;&#35821;&#35328;&#20449;&#24687;&#65292;&#32780;&#19988;&#19968;&#20123;S3Ms&#20284;&#20046;&#23398;&#20064;&#20102;&#31867;&#20284;&#20110;&#38899;&#32032;&#30340;&#23376;&#35789;&#21333;&#20803;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#25429;&#25417;&#26356;&#22823;&#30340;&#35821;&#35328;&#21333;&#20803;&#65288;&#22914;&#21333;&#35789;&#65289;&#30340;&#31243;&#24230;&#20197;&#21450;&#21333;&#35789;&#30456;&#20851;&#20449;&#24687;&#30340;&#32534;&#30721;&#20301;&#32622;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#26469;&#33258;&#19977;&#20010;S3Ms&#30340;&#19981;&#21516;&#23618;&#30340;&#21333;&#35789;&#29255;&#27573;&#34920;&#31034;&#36827;&#34892;&#20102;&#22810;&#31181;&#20998;&#26512;&#65306;wav2vec2&#12289;HuBERT&#21644;WavLM&#12290;&#25105;&#20204;&#21033;&#29992;&#35268;&#33539;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#20998;&#26512;&#24037;&#20855;&#65292;&#26469;&#34913;&#37327;&#36825;&#20123;&#34920;&#31034;&#19982;&#21333;&#35789;&#32423;&#35821;&#35328;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#26368;&#22823;&#30340;&#21333;&#35789;&#32423;&#35821;&#35328;&#20869;&#23481;&#24448;&#24448;&#20986;&#29616;&#22312;&#20013;&#38388;&#30340;&#27169;&#22411;&#23618;&#65292;&#32780;&#19968;&#20123;&#20302;&#32423;&#20449;&#24687;&#65288;&#22914;&#21457;&#38899;&#65289;&#20063;&#22312;&#26356;&#39640;&#30340;&#23618;&#20013;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many self-supervised speech models (S3Ms) have been introduced over the last few years, producing performance and data efficiency improvements for a variety of speech tasks. Evidence is emerging that different S3Ms encode linguistic information in different layers, and also that some S3Ms appear to learn phone-like sub-word units. However, the extent to which these models capture larger linguistic units, such as words, and where word-related information is encoded, remains unclear. In this study, we conduct several analyses of word segment representations extracted from different layers of three S3Ms: wav2vec2, HuBERT, and WavLM. We employ canonical correlation analysis (CCA), a lightweight analysis tool, to measure the similarity between these representations and word-level linguistic properties. We find that the maximal word-level linguistic content tends to be found in intermediate model layers, while some lower-level information like pronunciation is also retained in higher layers 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#12289;&#20844;&#24179;&#12289;&#28789;&#27963;&#21644;&#31169;&#23494;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#30495;&#23454;&#24212;&#29992;&#22330;&#26223;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2307.00161</link><description>&lt;p&gt;
FFPDG: &#24555;&#36895;&#12289;&#20844;&#24179;&#21644;&#31169;&#23494;&#30340;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
FFPDG: Fast, Fair and Private Data Generation. (arXiv:2307.00161v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#12289;&#20844;&#24179;&#12289;&#28789;&#27963;&#21644;&#31169;&#23494;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#30495;&#23454;&#24212;&#29992;&#22330;&#26223;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24314;&#27169;&#32463;&#24120;&#34987;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#20844;&#24179;&#24615;&#21644;&#38544;&#31169;&#26435;&#26159;&#21512;&#25104;&#25968;&#25454;&#38754;&#20020;&#30340;&#20004;&#20010;&#22823;&#38382;&#39064;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#22312;&#20445;&#25252;&#38544;&#31169;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#29983;&#25104;&#30340;&#25968;&#25454;&#21487;&#33021;&#26356;&#20855;&#20559;&#35265;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#39640;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#24555;&#36895;&#12289;&#20844;&#24179;&#12289;&#28789;&#27963;&#21644;&#31169;&#23494;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#36341;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#35813;&#26041;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#30495;&#23454;&#24212;&#29992;&#22330;&#26223;&#19979;&#65288;&#25512;&#26029;&#38454;&#27573;&#65289;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative modeling has been used frequently in synthetic data generation. Fairness and privacy are two big concerns for synthetic data. Although Recent GAN [\cite{goodfellow2014generative}] based methods show good results in preserving privacy, the generated data may be more biased. At the same time, these methods require high computation resources. In this work, we design a fast, fair, flexible and private data generation method. We show the effectiveness of our method theoretically and empirically. We show that models trained on data generated by the proposed method can perform well (in inference stage) on real application scenarios.
&lt;/p&gt;</description></item><item><title>&#24179;&#34913;&#26041;&#27861;&#23545;&#19981;&#24179;&#34913;&#20998;&#31867;&#38382;&#39064;&#20013;&#27169;&#22411;&#34892;&#20026;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#24179;&#34913;&#20998;&#26512;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.00157</link><description>&lt;p&gt;
&#24179;&#34913;&#26041;&#27861;&#23545;&#19981;&#24179;&#34913;&#20998;&#31867;&#38382;&#39064;&#20013;&#27169;&#22411;&#34892;&#20026;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Balancing Methods on Model Behavior in Imbalanced Classification Problems. (arXiv:2307.00157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00157
&lt;/p&gt;
&lt;p&gt;
&#24179;&#34913;&#26041;&#27861;&#23545;&#19981;&#24179;&#34913;&#20998;&#31867;&#38382;&#39064;&#20013;&#27169;&#22411;&#34892;&#20026;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#24179;&#34913;&#20998;&#26512;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#24179;&#34913;&#25968;&#25454;&#23545;&#20998;&#31867;&#38382;&#39064;&#26500;&#25104;&#20102;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#27169;&#22411;&#24615;&#33021;&#21463;&#21040;&#23545;&#23569;&#25968;&#31867;&#21035;&#23398;&#20064;&#19981;&#36275;&#30340;&#24433;&#21709;&#12290;&#24179;&#34913;&#26041;&#27861;&#36890;&#24120;&#34987;&#29992;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#21487;&#33021;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#25110;&#32773;&#20449;&#24687;&#20002;&#22833;&#31561;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24179;&#34913;&#26041;&#27861;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#8212;&#8212;&#23427;&#20204;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#25429;&#25417;&#36825;&#20123;&#21464;&#21270;&#65292;&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#26469;&#27604;&#36739;&#22312;&#24179;&#34913;&#21069;&#21518;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#38500;&#20102;&#21464;&#37327;&#37325;&#35201;&#24615;&#26041;&#27861;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#20351;&#29992;&#20102;&#37096;&#20998;&#20381;&#36182;&#36718;&#24275;&#21644;&#32047;&#31215;&#23616;&#37096;&#24433;&#21709;&#25216;&#26415;&#12290;&#36827;&#34892;&#20102;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#24320;&#28304;Python&#21253;edgaro&#26469;&#26041;&#20415;&#36827;&#34892;&#36825;&#31181;&#20998;&#26512;&#12290;&#25152;&#24471;&#21040;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#30001;&#20110;&#24179;&#34913;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#27169;&#22411;&#34892;&#20026;&#21457;&#29983;&#20102;&#26174;&#33879;&#21464;&#21270;&#65292;&#21487;&#33021;&#20250;&#20351;&#27169;&#22411;&#23545;&#24179;&#34913;&#20998;&#24067;&#20135;&#29983;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#35777;&#23454;&#20102;&#24179;&#34913;&#20998;&#26512;&#23545;&#27169;&#22411;&#34892;&#20026;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imbalanced data poses a significant challenge in classification as model performance is affected by insufficient learning from minority classes. Balancing methods are often used to address this problem. However, such techniques can lead to problems such as overfitting or loss of information. This study addresses a more challenging aspect of balancing methods - their impact on model behavior. To capture these changes, Explainable Artificial Intelligence tools are used to compare models trained on datasets before and after balancing. In addition to the variable importance method, this study uses the partial dependence profile and accumulated local effects techniques. Real and simulated datasets are tested, and an open-source Python package edgaro is developed to facilitate this analysis. The results obtained show significant changes in model behavior due to balancing methods, which can lead to biased models toward a balanced distribution. These findings confirm that balancing analysis sh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25340;&#25509;&#39044;&#35757;&#32451;&#27169;&#22411;&#26063;&#32676;&#65292;&#25552;&#20986;&#20102;SN-Netv2&#65292;&#23427;&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#35270;&#35273;&#39592;&#24178;&#32593;&#32476;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#23454;&#29616;&#22810;&#26679;&#24615;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2307.00154</link><description>&lt;p&gt;
Stitched ViTs&#26159;&#28789;&#27963;&#30340;&#35270;&#35273;&#39592;&#24178;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Stitched ViTs are Flexible Vision Backbones. (arXiv:2307.00154v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25340;&#25509;&#39044;&#35757;&#32451;&#27169;&#22411;&#26063;&#32676;&#65292;&#25552;&#20986;&#20102;SN-Netv2&#65292;&#23427;&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#35270;&#35273;&#39592;&#24178;&#32593;&#32476;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#23454;&#29616;&#22810;&#26679;&#24615;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#26222;&#36890;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#24050;&#25104;&#20026;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#30340;&#20027;&#21147;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#29616;&#25104;&#30340;ViTs&#30340;&#29616;&#26377;&#24037;&#20316;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#26041;&#38754;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#37319;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;ViTs&#38656;&#35201;&#21333;&#29420;&#35757;&#32451;&#65292;&#24182;&#21463;&#21040;&#22266;&#23450;&#30340;&#24615;&#33021;-&#25928;&#29575;&#26435;&#34913;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21463;&#21040;&#21487;&#25340;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#21551;&#21457;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#25340;&#25509;&#39044;&#35757;&#32451;&#27169;&#22411;&#26063;&#32676;&#26469;&#24555;&#36895;&#29983;&#25104;&#28085;&#30422;&#20016;&#23500;&#23376;&#32593;&#32476;&#30340;&#21333;&#19968;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#25903;&#25345;&#22312;&#36816;&#34892;&#26102;&#30340;&#22810;&#26679;&#24615;&#24615;&#33021;-&#25928;&#29575;&#26435;&#34913;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SN-Netv2&#65292;&#36825;&#26159;&#19968;&#20010;&#31995;&#32479;&#25913;&#36827;&#30340;&#27169;&#22411;&#25340;&#25509;&#26694;&#26550;&#65292;&#29992;&#20110;&#20419;&#36827;&#19979;&#28216;&#20219;&#21153;&#30340;&#36866;&#24212;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#21521;&#25340;&#25509;&#26041;&#26696;&#26469;&#25193;&#22823;&#25340;&#25509;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32771;&#34385;&#31354;&#38388;&#20013;&#24213;&#23618;FLOPs&#20998;&#24067;&#30340;&#36164;&#28304;&#21463;&#38480;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#25913;&#21892;&#37319;&#26679;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;SN-Netv2&#36827;&#34892;&#20102;&#32454;&#24494;&#35843;&#25972;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pretrained plain vision Transformers (ViTs) have been the workhorse for many downstream tasks. However, existing works utilizing off-the-shelf ViTs are inefficient in terms of training and deployment, because adopting ViTs with individual sizes requires separate training and is restricted by fixed performance-efficiency trade-offs. In this paper, we are inspired by stitchable neural networks, which is a new framework that cheaply produces a single model that covers rich subnetworks by stitching pretrained model families, supporting diverse performance-efficiency trade-offs at runtime. Building upon this foundation, we introduce SN-Netv2, a systematically improved model stitching framework to facilitate downstream task adaptation. Specifically, we first propose a Two-way stitching scheme to enlarge the stitching space. We then design a resource-constrained sampling strategy that takes into account the underlying FLOPs distributions in the space for improved sampling. Finally, we o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25511;&#30340;CAD&#27169;&#22411;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#31070;&#32463;&#32534;&#30721;&#21644;&#20195;&#30721;&#26641;&#26469;&#34920;&#31034;&#35774;&#35745;&#27010;&#24565;&#21644;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#22312;&#20256;&#32479;&#20219;&#21153;&#21644;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00149</link><description>&lt;p&gt;
&#21487;&#25511;&#30340;CAD&#27169;&#22411;&#29983;&#25104;&#30340;&#20998;&#23618;&#31070;&#32463;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Neural Coding for Controllable CAD Model Generation. (arXiv:2307.00149v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25511;&#30340;CAD&#27169;&#22411;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#31070;&#32463;&#32534;&#30721;&#21644;&#20195;&#30721;&#26641;&#26469;&#34920;&#31034;&#35774;&#35745;&#27010;&#24565;&#21644;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#22312;&#20256;&#32479;&#20219;&#21153;&#21644;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#65288;CAD&#65289;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;CAD&#27169;&#22411;&#30340;&#39640;&#32423;&#35774;&#35745;&#27010;&#24565;&#34920;&#31034;&#20026;&#19977;&#23618;&#31070;&#32463;&#32534;&#30721;&#30340;&#23618;&#27425;&#26641;&#65292;&#20174;&#20840;&#23616;&#37096;&#20214;&#25490;&#21015;&#21040;&#23616;&#37096;&#26354;&#32447;&#20960;&#20309;&#65292;&#21516;&#26102;&#36890;&#36807;&#20351;&#29992;&#20195;&#30721;&#26641;&#25351;&#23450;&#30446;&#26631;&#35774;&#35745;&#26469;&#25511;&#21046;CAD&#27169;&#22411;&#30340;&#29983;&#25104;&#25110;&#23436;&#25104;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#8220;&#25513;&#30721;&#36339;&#36291;&#36830;&#25509;&#8221;&#30340;&#30690;&#37327;&#37327;&#21270;VAE&#21464;&#20307;&#23558;&#35774;&#35745;&#21464;&#21270;&#25552;&#21462;&#20026;&#19977;&#20010;&#23618;&#27425;&#30340;&#31070;&#32463;&#20195;&#30721;&#31807;&#12290;&#20004;&#38454;&#27573;&#32423;&#32852;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#23398;&#20064;&#20174;&#19981;&#23436;&#25972;&#30340;CAD&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#26641;&#65292;&#28982;&#21518;&#26681;&#25454;&#39044;&#26399;&#30340;&#35774;&#35745;&#23436;&#25104;CAD&#27169;&#22411;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#20256;&#32479;&#20219;&#21153;&#65288;&#22914;&#38543;&#26426;&#29983;&#25104;&#65289;&#19978;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26032;&#39062;&#30340;&#20132;&#20114;&#33021;&#21147;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/samxuxiang/hnc-cad&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel generative model for Computer Aided Design (CAD) that 1) represents high-level design concepts of a CAD model as a three-level hierarchical tree of neural codes, from global part arrangement down to local curve geometry; and 2) controls the generation or completion of CAD models by specifying the target design using a code tree. Concretely, a novel variant of a vector quantized VAE with "masked skip connection" extracts design variations as neural codebooks at three levels. Two-stage cascaded auto-regressive transformers learn to generate code trees from incomplete CAD models and then complete CAD models following the intended design. Extensive experiments demonstrate superior performance on conventional tasks such as random generation while enabling novel interaction capabilities on conditional generation tasks. The code is available at https://github.com/samxuxiang/hnc-cad.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#21644;&#30740;&#31350;&#26799;&#24230;&#27969;&#20013;&#30340;&#23432;&#24658;&#23450;&#24459;&#65292;&#20197;&#21450;&#22312;&#27169;&#22411;&#30340;&#38597;&#21487;&#27604;&#30697;&#38453;&#29983;&#25104;&#30340;&#26446;&#20195;&#25968;&#19978;&#36827;&#34892;&#26377;&#38480;&#32500;&#20195;&#25968;&#36816;&#31639;&#65292;&#25581;&#31034;&#20102;&#36229;&#21442;&#25968;&#35843;&#33410;&#30340;&#27169;&#22411;&#20445;&#30041;&#20102;&#19968;&#20123;&#20248;&#21270;&#21021;&#22987;&#21270;&#30340;&#29305;&#24615;&#65292;&#36825;&#21487;&#33021;&#35299;&#37322;&#20102;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#27867;&#21270;&#29305;&#24615;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2307.00144</link><description>&lt;p&gt;
&#36981;&#23432;&#27861;&#24459;&#24182;&#36981;&#24490;&#27969;&#31243;&#65306;&#26799;&#24230;&#27969;&#30340;&#23432;&#24658;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Abide by the Law and Follow the Flow: Conservation Laws for Gradient Flows. (arXiv:2307.00144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#21644;&#30740;&#31350;&#26799;&#24230;&#27969;&#20013;&#30340;&#23432;&#24658;&#23450;&#24459;&#65292;&#20197;&#21450;&#22312;&#27169;&#22411;&#30340;&#38597;&#21487;&#27604;&#30697;&#38453;&#29983;&#25104;&#30340;&#26446;&#20195;&#25968;&#19978;&#36827;&#34892;&#26377;&#38480;&#32500;&#20195;&#25968;&#36816;&#31639;&#65292;&#25581;&#31034;&#20102;&#36229;&#21442;&#25968;&#35843;&#33410;&#30340;&#27169;&#22411;&#20445;&#30041;&#20102;&#19968;&#20123;&#20248;&#21270;&#21021;&#22987;&#21270;&#30340;&#29305;&#24615;&#65292;&#36825;&#21487;&#33021;&#35299;&#37322;&#20102;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#27867;&#21270;&#29305;&#24615;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#26799;&#24230;&#19979;&#38477;&#21160;&#21147;&#23398;&#30340;&#20960;&#20309;&#29305;&#24615;&#26159;&#35299;&#23494;&#38750;&#24120;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26368;&#36817;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#35266;&#23519;&#26159;&#65292;&#36229;&#21442;&#25968;&#35843;&#33410;&#30340;&#27169;&#22411;&#20445;&#30041;&#20102;&#19968;&#20123;&#20248;&#21270;&#21021;&#22987;&#21270;&#30340;&#29305;&#24615;&#12290;&#36825;&#31181;&#8220;&#38544;&#24335;&#20559;&#24046;&#8221;&#34987;&#35748;&#20026;&#26159;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#26377;&#21033;&#29305;&#24615;&#24182;&#33021;&#35299;&#37322;&#20854;&#33391;&#22909;&#27867;&#21270;&#29305;&#24615;&#30340;&#21407;&#22240;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26377;&#19977;&#20010;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20005;&#26684;&#20171;&#32461;&#20102;&#8220;&#23432;&#24658;&#23450;&#24459;&#8221;&#30340;&#23450;&#20041;&#21644;&#22522;&#26412;&#24615;&#36136;&#65292;&#36825;&#20123;&#23432;&#24658;&#23450;&#24459;&#26159;&#22312;&#32473;&#23450;&#27169;&#22411;&#65288;&#20363;&#22914;&#20855;&#26377;&#32473;&#23450;&#26550;&#26500;&#30340;ReLU&#32593;&#32476;&#65289;&#30340;&#26799;&#24230;&#27969;&#20013;&#29420;&#31435;&#20445;&#25345;&#30340;&#26368;&#22823;&#37327;&#12290;&#19981;&#35770;&#20351;&#29992;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#21644;&#20219;&#20309;&#25439;&#22833;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35299;&#37322;&#22914;&#20309;&#36890;&#36807;&#23545;&#27169;&#22411;&#30340;&#38597;&#21487;&#27604;&#30697;&#38453;&#29983;&#25104;&#30340;&#26446;&#20195;&#25968;&#36827;&#34892;&#26377;&#38480;&#32500;&#20195;&#25968;&#36816;&#31639;&#65292;&#25214;&#21040;&#36825;&#20123;&#25968;&#37327;&#30340;&#30830;&#20999;&#25968;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;SageMath&#20013;&#23454;&#29616;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the geometric properties of gradient descent dynamics is a key ingredient in deciphering the recent success of very large machine learning models. A striking observation is that trained over-parameterized models retain some properties of the optimization initialization. This "implicit bias" is believed to be responsible for some favorable properties of the trained models and could explain their good generalization properties. The purpose of this article is threefold. First, we rigorously expose the definition and basic properties of "conservation laws", which are maximal sets of independent quantities conserved during gradient flows of a given model (e.g. of a ReLU network with a given architecture) with any training data and any loss. Then we explain how to find the exact number of these quantities by performing finite-dimensional algebraic manipulations on the Lie algebra generated by the Jacobian of the model. Finally, we provide algorithms (implemented in SageMath) to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BuildingsBench&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;900K&#24231;&#24314;&#31569;&#29289;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#20013;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35813;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#32463;&#36807;&#21512;&#25104;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.00142</link><description>&lt;p&gt;
BuildingsBench&#65306;&#19968;&#20010;&#21253;&#21547;900K&#24231;&#24314;&#31569;&#29289;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BuildingsBench: A Large-Scale Dataset of 900K Buildings and Benchmark for Short-Term Load Forecasting. (arXiv:2307.00142v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BuildingsBench&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;900K&#24231;&#24314;&#31569;&#29289;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#20013;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35813;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#32463;&#36807;&#21512;&#25104;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;(STLF)&#20013;&#32570;&#20047;&#24320;&#25918;&#12289;&#22823;&#35268;&#27169;&#12289;&#39640;&#24314;&#31569;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;BuildingsBench&#65292;&#21253;&#25324;1)&#21253;&#21547;900K&#20010;&#27169;&#25311;&#24314;&#31569;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;Buildings-900K&#65292;&#20197;&#27169;&#25311;&#32654;&#22269;&#30340;&#24314;&#31569;&#24211;&#23384;&#65292;&#20197;&#21450;2)&#25317;&#26377;&#26469;&#33258;7&#20010;&#24320;&#25918;&#25968;&#25454;&#38598;&#30340;&#36229;&#36807;1900&#20010;&#30495;&#23454;&#20303;&#23429;&#21644;&#21830;&#19994;&#24314;&#31569;&#29289;&#30340;&#35780;&#20272;&#24179;&#21488;&#12290;BuildingsBench&#20026;&#20004;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#20219;&#21153;&#25552;&#20379;&#20102;&#22522;&#20934;&#65306;&#38646;-shot STLF&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#24314;&#31569;&#19978;&#36827;&#34892;&#35780;&#20272;&#32780;&#26080;&#38656;&#24494;&#35843;&#65307;&#20197;&#21450;&#36801;&#31227;&#23398;&#20064;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#30446;&#26631;&#24314;&#31569;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#26412;&#27425;&#22522;&#20934;&#20998;&#26512;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#32463;&#36807;&#21512;&#25104;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#24847;&#22806;&#22320;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Short-term forecasting of residential and commercial building energy consumption is widely used in power systems and continues to grow in importance. Data-driven short-term load forecasting (STLF), although promising, has suffered from a lack of open, large-scale datasets with high building diversity. This has hindered exploring the pretrain-then-finetune paradigm for STLF. To help address this, we present BuildingsBench, which consists of 1) Buildings-900K, a large-scale dataset of 900K simulated buildings representing the U.S. building stock, and 2) an evaluation platform with over 1,900 real residential and commercial buildings from 7 open datasets. BuildingsBench benchmarks two under-explored tasks: zero-shot STLF, where a pretrained model is evaluated on unseen buildings without fine-tuning, and transfer learning, where a pretrained model is fine-tuned on a target building. The main finding of our benchmark analysis is that synthetically pretrained models generalize surprisingly w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20984;&#20248;&#21270;&#23454;&#29616;&#39118;&#38505;&#25935;&#24863;&#30340;&#26080;&#28436;&#21592;&#31574;&#30053;&#65292;&#21033;&#29992;&#26465;&#20214;&#39118;&#38505;&#20540;&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;&#20984;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#35782;&#21035;&#20840;&#23616;&#26368;&#20248;&#21160;&#20316;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#26377;&#25928;&#30340;&#39118;&#38505;&#25511;&#21046;&#26041;&#38754;&#24456;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.00141</link><description>&lt;p&gt;
&#36890;&#36807;&#20984;&#20248;&#21270;&#23454;&#29616;&#39118;&#38505;&#25935;&#24863;&#30340;&#26080;&#28436;&#21592;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Risk-sensitive Actor-free Policy via Convex Optimization. (arXiv:2307.00141v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20984;&#20248;&#21270;&#23454;&#29616;&#39118;&#38505;&#25935;&#24863;&#30340;&#26080;&#28436;&#21592;&#31574;&#30053;&#65292;&#21033;&#29992;&#26465;&#20214;&#39118;&#38505;&#20540;&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;&#20984;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#35782;&#21035;&#20840;&#23616;&#26368;&#20248;&#21160;&#20316;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#26377;&#25928;&#30340;&#39118;&#38505;&#25511;&#21046;&#26041;&#38754;&#24456;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20248;&#21270;&#20195;&#29702;&#26102;&#27809;&#26377;&#32771;&#34385;&#23433;&#20840;&#24615;&#65292;&#21487;&#33021;&#23548;&#33268;&#24847;&#24819;&#19981;&#21040;&#30340;&#21518;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#39118;&#38505;&#20540;&#30340;&#39118;&#38505;&#25935;&#24863;&#30340;&#26368;&#20248;&#26080;&#28436;&#21592;&#31574;&#30053;&#12290;&#39118;&#38505;&#25935;&#24863;&#30340;&#30446;&#26631;&#20989;&#25968;&#20351;&#29992;&#20102;&#19968;&#20010;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#65292;&#20197;&#30830;&#20445;&#23545;&#21160;&#20316;&#30340;&#20984;&#24615;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#26799;&#24230;&#36319;&#38543;&#26041;&#27861;&#26469;&#35782;&#21035;&#20840;&#23616;&#26368;&#20248;&#21160;&#20316;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#26377;&#25928;&#30340;&#39118;&#38505;&#25511;&#21046;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional reinforcement learning methods optimize agents without considering safety, potentially resulting in unintended consequences. In this paper, we propose an optimal actor-free policy that optimizes a risk-sensitive criterion based on the conditional value at risk. The risk-sensitive objective function is modeled using an input-convex neural network ensuring convexity with respect to the actions and enabling the identification of globally optimal actions through simple gradient-following methods. Experimental results demonstrate the efficacy of our approach in maintaining effective risk control.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#23398;&#20064;&#36523;&#20221;&#25928;&#24212;&#30340;&#32972;&#26223;&#19979;&#65292;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#23646;&#24615;&#21644;&#22522;&#26412;&#38480;&#21046;&#26041;&#38754;&#30340;&#26032;&#24615;&#36136;&#65292;&#20197;&#21450;&#22312;&#20004;&#20010;&#23383;&#27597;&#30340;&#21333;&#35789;&#26696;&#20363;&#20013;&#30340;&#20855;&#20307;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.00134</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36523;&#20221;&#25928;&#24212;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Generalization Limits of Graph Neural Networks in Identity Effects Learning. (arXiv:2307.00134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#23398;&#20064;&#36523;&#20221;&#25928;&#24212;&#30340;&#32972;&#26223;&#19979;&#65292;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#23646;&#24615;&#21644;&#22522;&#26412;&#38480;&#21046;&#26041;&#38754;&#30340;&#26032;&#24615;&#36136;&#65292;&#20197;&#21450;&#22312;&#20004;&#20010;&#23383;&#27597;&#30340;&#21333;&#35789;&#26696;&#20363;&#20013;&#30340;&#20855;&#20307;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#22270;&#39046;&#22495;&#30340;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#20013;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#24037;&#20855;&#12290;&#23427;&#20204;&#36890;&#24120;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#24182;&#19988;&#30001;&#20110;&#20854;&#19982;Weisfeiler-Lehman(WL)&#22270;&#21516;&#26500;&#27979;&#35797;&#32039;&#23494;&#30456;&#36830;&#30340;&#30452;&#35266;&#34920;&#36848;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#27426;&#36814;&#65292;&#20174;&#34920;&#36798;&#33021;&#21147;&#19978;&#35762;&#65292;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#19982;WL&#27979;&#35797;&#31561;&#20215;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#23398;&#20064;&#25152;&#35859;&#30340;&#36523;&#20221;&#25928;&#24212;&#65288;&#21363;&#30830;&#23450;&#19968;&#20010;&#23545;&#35937;&#26159;&#21542;&#30001;&#20004;&#20010;&#30456;&#21516;&#30340;&#32452;&#20214;&#32452;&#25104;&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#24314;&#31435;&#20102;GNN&#22312;&#27867;&#21270;&#23646;&#24615;&#21644;&#22522;&#26412;&#38480;&#21046;&#26041;&#38754;&#30340;&#26032;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#20986;&#20110;&#29702;&#35299;GNN&#22312;&#25191;&#34892;&#31616;&#21333;&#35748;&#30693;&#20219;&#21153;&#26102;&#30340;&#33021;&#21147;&#30340;&#38656;&#27714;&#65292;&#21487;&#33021;&#22312;&#35745;&#31639;&#35821;&#35328;&#23398;&#21644;&#21270;&#23398;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65306;&#65288;i&#65289;&#20004;&#20010;&#23383;&#27597;&#30340;&#21333;&#35789;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;GNN&#22312;&#21033;&#29992;&#27491;&#20132;&#26102;&#26080;&#27861;&#23545;&#26410;&#35265;&#23383;&#27597;&#36827;&#34892;&#27867;&#21270;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as a powerful tool for data-driven learning on various graph domains. They are usually based on a message-passing mechanism and have gained increasing popularity for their intuitive formulation, which is closely linked to the Weisfeiler-Lehman (WL) test for graph isomorphism to which they have been proven equivalent in terms of expressive power. In this work, we establish new generalization properties and fundamental limits of GNNs in the context of learning so-called identity effects, i.e., the task of determining whether an object is composed of two identical components or not. Our study is motivated by the need to understand the capabilities of GNNs when performing simple cognitive tasks, with potential applications in computational linguistics and chemistry. We analyze two case studies: (i) two-letters words, for which we show that GNNs trained via stochastic gradient descent are unable to generalize to unseen letters when utilizing orthogo
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22312;&#20302;&#28201;&#31561;&#31163;&#23376;&#20307;&#24314;&#27169;&#21644;&#27169;&#25311;&#26041;&#38754;&#30340;&#24212;&#29992;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23427;&#33021;&#22815;&#25552;&#20379;&#26032;&#30340;&#26041;&#27861;&#21644;&#36884;&#24452;&#65292;&#20419;&#36827;&#31561;&#31163;&#23376;&#20307;&#31185;&#23398;&#21644;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2307.00131</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#25512;&#21160;&#20302;&#28201;&#31561;&#31163;&#23376;&#20307;&#24314;&#27169;&#21644;&#27169;&#25311;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine learning for advancing low-temperature plasma modeling and simulation. (arXiv:2307.00131v1 [physics.plasm-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00131
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20302;&#28201;&#31561;&#31163;&#23376;&#20307;&#24314;&#27169;&#21644;&#27169;&#25311;&#26041;&#38754;&#30340;&#24212;&#29992;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23427;&#33021;&#22815;&#25552;&#20379;&#26032;&#30340;&#26041;&#27861;&#21644;&#36884;&#24452;&#65292;&#20419;&#36827;&#31561;&#31163;&#23376;&#20307;&#31185;&#23398;&#21644;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#37117;&#20135;&#29983;&#20102;&#24040;&#22823;&#24433;&#21709;&#65292;&#36817;&#24180;&#26469;&#65292;&#22312;&#20302;&#28201;&#31561;&#31163;&#23376;&#20307;&#24314;&#27169;&#21644;&#27169;&#25311;&#39046;&#22495;&#20063;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#23427;&#30340;&#24212;&#29992;&#24212;&#35813;&#34987;&#35880;&#24910;&#35780;&#20272;&#65292;&#20294;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#31561;&#31163;&#23376;&#20307;&#24314;&#27169;&#21644;&#27169;&#25311;&#30340;&#35768;&#22810;&#26041;&#38754;&#37117;&#21463;&#30410;&#21290;&#27973;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20027;&#35201;&#36861;&#27714;&#20004;&#20010;&#30446;&#26631;&#65306;(a)&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#20302;&#28201;&#31561;&#31163;&#23376;&#20307;&#24314;&#27169;&#21644;&#27169;&#25311;&#30340;&#26041;&#27861;&#30340;&#29616;&#29366;&#36827;&#34892;&#32508;&#36848;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#32508;&#36848;&#21010;&#20998;&#20026;&#31561;&#31163;&#23376;&#20307;&#29289;&#29702;&#23398;&#12289;&#31561;&#31163;&#23376;&#20307;&#21270;&#23398;&#12289;&#31561;&#31163;&#23376;&#20307;&#34920;&#38754;&#30456;&#20114;&#20316;&#29992;&#21644;&#31561;&#31163;&#23376;&#20307;&#36807;&#31243;&#25511;&#21046;&#22235;&#20010;&#26041;&#38754;&#65292;&#25105;&#20204;&#26088;&#22312;&#24191;&#27867;&#35752;&#35770;&#25991;&#29486;&#20013;&#30340;&#30456;&#20851;&#23454;&#20363;&#12290;(b)&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#31561;&#31163;&#23376;&#20307;&#31185;&#23398;&#21644;&#25216;&#26415;&#28508;&#22312;&#36827;&#23637;&#30340;&#23637;&#26395;&#12290;&#25105;&#20204;&#29305;&#21035;&#38416;&#36848;&#20102;&#21487;&#33021;&#36890;&#36807;&#20174;&#20854;&#20182;&#31185;&#23398;&#23398;&#31185;&#20013;&#30340;&#36866;&#24212;&#25512;&#21160;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24050;&#30693;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#23558;&#20351;&#20302;&#28201;&#31561;&#31163;&#23376;&#20307;&#24314;&#27169;&#21644;&#27169;&#25311;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has had an enormous impact in many scientific disciplines. Also in the field of low-temperature plasma modeling and simulation it has attracted significant interest within the past years. Whereas its application should be carefully assessed in general, many aspects of plasma modeling and simulation have benefited substantially from recent developments within the field of machine learning and data-driven modeling. In this survey, we approach two main objectives: (a) We review the state-of-the-art focusing on approaches to low-temperature plasma modeling and simulation. By dividing our survey into plasma physics, plasma chemistry, plasma-surface interactions, and plasma process control, we aim to extensively discuss relevant examples from literature. (b) We provide a perspective of potential advances to plasma science and technology. We specifically elaborate on advances possibly enabled by adaptation from other scientific disciplines. We argue that not only the known un
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21152;&#36895;&#38750;&#31934;&#30830;&#36229;&#26799;&#24230;&#19979;&#38477;&#30340;&#26041;&#27861;&#29992;&#20110;&#21452;&#23618;&#20248;&#21270;&#65292;&#21487;&#20197;&#22312;&#36739;&#20302;&#30340;&#22797;&#26434;&#24230;&#19979;&#25214;&#21040;&#19968;&#38454;&#21644;&#20108;&#38454;&#31283;&#23450;&#28857;&#65292;&#25104;&#20026;&#21452;&#23618;&#20248;&#21270;&#21644;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#26368;&#26032;&#26368;&#20339;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2307.00126</link><description>&lt;p&gt;
&#21152;&#36895;&#38750;&#31934;&#30830;&#36229;&#26799;&#24230;&#19979;&#38477;&#29992;&#20110;&#21452;&#23618;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Accelerating Inexact HyperGradient Descent for Bilevel Optimization. (arXiv:2307.00126v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00126
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21152;&#36895;&#38750;&#31934;&#30830;&#36229;&#26799;&#24230;&#19979;&#38477;&#30340;&#26041;&#27861;&#29992;&#20110;&#21452;&#23618;&#20248;&#21270;&#65292;&#21487;&#20197;&#22312;&#36739;&#20302;&#30340;&#22797;&#26434;&#24230;&#19979;&#25214;&#21040;&#19968;&#38454;&#21644;&#20108;&#38454;&#31283;&#23450;&#28857;&#65292;&#25104;&#20026;&#21452;&#23618;&#20248;&#21270;&#21644;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#26368;&#26032;&#26368;&#20339;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#19968;&#33324;&#38750;&#20984;-&#20984;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8212;&#8212;"&#37325;&#26032;&#21551;&#21160;&#30340;&#21152;&#36895;&#36229;&#26799;&#24230;&#19979;&#38477;" (RAHGD) &#26041;&#27861;&#8212;&#8212;&#21487;&#20197;&#25214;&#21040;&#19968;&#20010; $\epsilon$-&#19968;&#38454;&#31283;&#23450;&#28857;&#65292;&#20854; oracle &#22797;&#26434;&#24230;&#20026; $\tilde{\mathcal{O}}(\kappa^{3.25}\epsilon^{-1.75})$&#65292;&#20854;&#20013; $\kappa$ &#26159;&#19979;&#23618;&#30446;&#26631;&#30340;&#26465;&#20214;&#25968;&#65292;$\epsilon$ &#26159;&#26399;&#26395;&#31934;&#24230;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102; RAHGD &#30340;&#25200;&#21160;&#21464;&#20307;&#65292;&#29992;&#20110;&#22312;&#30456;&#21516;&#30340; oracle &#22797;&#26434;&#24230;&#19979;&#25214;&#21040;&#19968;&#20010; $\big(\epsilon,\mathcal{O}(\kappa^{2.5}\sqrt{\epsilon}\,)\big)$-&#20108;&#38454;&#31283;&#23450;&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#21452;&#23618;&#20248;&#21270;&#20013;&#23454;&#29616;&#20102;&#24050;&#30693;&#26368;&#22909;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#19988;&#25913;&#36827;&#20102;&#29616;&#26377;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#20013;&#25214;&#21040;&#20108;&#38454;&#31283;&#23450;&#28857;&#30340;&#19978;&#30028;&#22797;&#26434;&#24230;&#65292;&#20026;&#26368;&#26032;&#30340;&#22522;&#20934;&#35774;&#32622;&#20102;&#19968;&#20010;&#26032;&#30340;&#26368;&#20339;&#29366;&#24577;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method for solving general nonconvex-strongly-convex bilevel optimization problems. Our method -- the \emph{Restarted Accelerated HyperGradient Descent} (\texttt{RAHGD}) method -- finds an $\epsilon$-first-order stationary point of the objective with $\tilde{\mathcal{O}}(\kappa^{3.25}\epsilon^{-1.75})$ oracle complexity, where $\kappa$ is the condition number of the lower-level objective and $\epsilon$ is the desired accuracy. We also propose a perturbed variant of \texttt{RAHGD} for finding an $\big(\epsilon,\mathcal{O}(\kappa^{2.5}\sqrt{\epsilon}\,)\big)$-second-order stationary point within the same order of oracle complexity. Our results achieve the best-known theoretical guarantees for finding stationary points in bilevel optimization and also improve upon the existing upper complexity bound for finding second-order stationary points in nonconvex-strongly-concave minimax optimization problems, setting a new state-of-the-art benchmark. Empirical studies are conducted t
&lt;/p&gt;</description></item><item><title>RObotic MAnipulation Network&#65288;ROMAN&#65289;&#36890;&#36807;&#28151;&#21512;&#23618;&#27425;&#23398;&#20064;&#26694;&#26550;&#35299;&#20915;&#22797;&#26434;&#30340;&#36830;&#32493;&#25805;&#20316;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#20219;&#21153;&#22810;&#26679;&#24615;&#21644;&#40065;&#26834;&#30340;&#22833;&#36133;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2307.00125</link><description>&lt;p&gt;
RObotic MAnipulation Network&#65288;ROMAN&#65289;--&#28151;&#21512;&#23618;&#27425;&#23398;&#20064;&#35299;&#20915;&#22797;&#26434;&#30340;&#36830;&#32493;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
RObotic MAnipulation Network (ROMAN) -- Hybrid Hierarchical Learning for Solving Complex Sequential Tasks. (arXiv:2307.00125v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00125
&lt;/p&gt;
&lt;p&gt;
RObotic MAnipulation Network&#65288;ROMAN&#65289;&#36890;&#36807;&#28151;&#21512;&#23618;&#27425;&#23398;&#20064;&#26694;&#26550;&#35299;&#20915;&#22797;&#26434;&#30340;&#36830;&#32493;&#25805;&#20316;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#20219;&#21153;&#22810;&#26679;&#24615;&#21644;&#40065;&#26834;&#30340;&#22833;&#36133;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#20307;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#35299;&#20915;&#38271;&#24207;&#21015;&#20219;&#21153;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20351;&#26426;&#22120;&#20154;&#31995;&#32479;&#33021;&#22815;&#25191;&#34892;&#22810;&#26679;&#21270;&#30340;&#36830;&#32493;&#20219;&#21153;&#65292;&#24182;&#20855;&#22791;&#24191;&#27867;&#30340;&#25805;&#20316;&#25216;&#33021;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#23618;&#27425;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;ROBOTIC Manipulation Network&#65288;ROMAN&#65289;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340;&#22810;&#20010;&#22797;&#26434;&#20219;&#21153;&#30340;&#38271;&#26102;&#38388;&#20219;&#21153;&#12290;ROMAN&#36890;&#36807;&#38598;&#25104;&#34892;&#20026;&#20811;&#38534;&#12289;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26469;&#23454;&#29616;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#20197;&#21450;&#40065;&#26834;&#30340;&#22833;&#36133;&#24674;&#22797;&#12290;&#23427;&#21253;&#25324;&#19968;&#20010;&#20013;&#22830;&#25805;&#20316;&#32593;&#32476;&#65292;&#21327;&#35843;&#19968;&#32452;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#27599;&#20010;&#32593;&#32476;&#19987;&#27880;&#20110;&#19981;&#21516;&#30340;&#21487;&#37325;&#32452;&#23376;&#20219;&#21153;&#65292;&#29983;&#25104;&#23427;&#20204;&#22312;&#22797;&#26434;&#30340;&#38271;&#26102;&#38388;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#27491;&#30830;&#36830;&#32493;&#21160;&#20316;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#21327;&#35843;&#21644;&#28608;&#27963;&#36825;&#20123;&#19987;&#38376;&#30340;&#25805;&#20316;&#19987;&#23478;&#65292;ROMAN&#29983;&#25104;&#20102;&#27491;&#30830;&#30340;&#39034;&#24207;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving long sequential tasks poses a significant challenge in embodied artificial intelligence. Enabling a robotic system to perform diverse sequential tasks with a broad range of manipulation skills is an active area of research. In this work, we present a Hybrid Hierarchical Learning framework, the Robotic Manipulation Network (ROMAN), to address the challenge of solving multiple complex tasks over long time horizons in robotic manipulation. ROMAN achieves task versatility and robust failure recovery by integrating behavioural cloning, imitation learning, and reinforcement learning. It consists of a central manipulation network that coordinates an ensemble of various neural networks, each specialising in distinct re-combinable sub-tasks to generate their correct in-sequence actions for solving complex long-horizon manipulation tasks. Experimental results show that by orchestrating and activating these specialised manipulation experts, ROMAN generates correct sequential activations f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#20197;&#20154;&#31867;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#30740;&#31350;&#25345;&#32493;&#23398;&#20064;&#65292;&#36890;&#36807;&#23545;40&#21517;&#21442;&#19982;&#32773;&#19982;&#25345;&#32493;&#23398;&#20064;&#26426;&#22120;&#20154;&#30340;&#38271;&#26399;&#20132;&#20114;&#36827;&#34892;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#21457;&#29616;&#20154;&#31867;&#30340;&#25945;&#23398;&#39118;&#26684;&#23384;&#22312;&#26174;&#33879;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.00123</link><description>&lt;p&gt;
&#20154;&#31867;&#29992;&#25143;&#22914;&#20309;&#36890;&#36807;&#37325;&#22797;&#20132;&#20114;&#25945;&#25480;&#25345;&#32493;&#23398;&#20064;&#26426;&#22120;&#20154;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Do Human Users Teach a Continual Learning Robot in Repeated Interactions?. (arXiv:2307.00123v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#20197;&#20154;&#31867;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#30740;&#31350;&#25345;&#32493;&#23398;&#20064;&#65292;&#36890;&#36807;&#23545;40&#21517;&#21442;&#19982;&#32773;&#19982;&#25345;&#32493;&#23398;&#20064;&#26426;&#22120;&#20154;&#30340;&#38271;&#26399;&#20132;&#20114;&#36827;&#34892;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#21457;&#29616;&#20154;&#31867;&#30340;&#25945;&#23398;&#39118;&#26684;&#23384;&#22312;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#36817;&#24180;&#26469;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30340;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#65292;&#26088;&#22312;&#35753;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#19982;&#20154;&#31867;&#30340;&#38271;&#26399;&#20132;&#20114;&#20013;&#19981;&#26029;&#23398;&#20064;&#20854;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25345;&#32493;&#23398;&#20064;&#30740;&#31350;&#37117;&#26159;&#20197;&#26426;&#22120;&#20154;&#20026;&#20013;&#24515;&#65292;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#26032;&#20449;&#24687;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#12290;&#26412;&#25991;&#37319;&#29992;&#20197;&#20154;&#31867;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#25345;&#32493;&#23398;&#20064;&#65292;&#20197;&#20102;&#35299;&#20154;&#31867;&#22914;&#20309;&#22312;&#38271;&#26399;&#26102;&#38388;&#20869;&#25945;&#25480;&#25345;&#32493;&#23398;&#20064;&#26426;&#22120;&#20154;&#65292;&#20197;&#21450;&#20182;&#20204;&#30340;&#25945;&#23398;&#39118;&#26684;&#26159;&#21542;&#23384;&#22312;&#21464;&#21270;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#19982;40&#21517;&#21442;&#19982;&#32773;&#30340;&#38754;&#23545;&#38754;&#30740;&#31350;&#65292;&#20182;&#20204;&#19982;&#19968;&#20010;&#25345;&#32493;&#23398;&#20064;&#26426;&#22120;&#20154;&#36827;&#34892;&#20102;200&#27425;&#20132;&#20114;&#12290;&#26412;&#27425;&#30740;&#31350;&#37319;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;&#65292;&#37096;&#32626;&#22312;&#19968;&#20010;Fetch&#31227;&#21160;&#25805;&#20316;&#26426;&#22120;&#20154;&#19978;&#12290;&#23545;&#30740;&#31350;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#23384;&#22312;&#26174;&#33879;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) has emerged as an important avenue of research in recent years, at the intersection of Machine Learning (ML) and Human-Robot Interaction (HRI), to allow robots to continually learn in their environments over long-term interactions with humans. Most research in continual learning, however, has been robot-centered to develop continual learning algorithms that can quickly learn new information on static datasets. In this paper, we take a human-centered approach to continual learning, to understand how humans teach continual learning robots over the long term and if there are variations in their teaching styles. We conducted an in-person study with 40 participants that interacted with a continual learning robot in 200 sessions. In this between-participant study, we used two different CL models deployed on a Fetch mobile manipulator robot. An extensive qualitative and quantitative analysis of the data collected in the study shows that there is significant variation a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#35821;&#35328;&#25509;&#21475;&#25511;&#21046;&#26426;&#22120;&#20154;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#22270;&#20687;&#21644;&#30446;&#26631;&#20449;&#24687;&#30340;&#31574;&#30053;&#20197;&#21450;&#23569;&#37327;&#30340;&#35821;&#35328;&#25968;&#25454;&#23454;&#29616;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#31283;&#20581;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00117</link><description>&lt;p&gt;
&#25351;&#23548;&#30446;&#26631;&#34920;&#24449;&#65306;&#19968;&#31181;&#21322;&#30417;&#30563;&#35821;&#35328;&#25509;&#21475;&#25511;&#21046;&#26426;&#22120;&#20154;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Goal Representations for Instruction Following: A Semi-Supervised Language Interface to Control. (arXiv:2307.00117v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#35821;&#35328;&#25509;&#21475;&#25511;&#21046;&#26426;&#22120;&#20154;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#22270;&#20687;&#21644;&#30446;&#26631;&#20449;&#24687;&#30340;&#31574;&#30053;&#20197;&#21450;&#23569;&#37327;&#30340;&#35821;&#35328;&#25968;&#25454;&#23454;&#29616;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#31283;&#20581;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#25353;&#29031;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#34892;&#21160;&#65292;&#20363;&#22914;&#8220;&#23558;&#27611;&#24062;&#25918;&#22312;&#24494;&#27874;&#28809;&#26049;&#36793;&#8221;&#12290;&#20294;&#26159;&#33719;&#21462;&#22823;&#37327;&#24102;&#26377;&#35821;&#35328;&#25351;&#20196;&#26631;&#31614;&#30340;&#26631;&#27880;&#25968;&#25454;&#38750;&#24120;&#22256;&#38590;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#33719;&#21462;&#23545;&#22270;&#20687;&#30446;&#26631;&#20316;&#20986;&#21709;&#24212;&#30340;&#31574;&#30053;&#35201;&#23481;&#26131;&#24471;&#22810;&#65292;&#22240;&#20026;&#20219;&#20309;&#33258;&#20027;&#23581;&#35797;&#25110;&#28436;&#31034;&#37117;&#21487;&#20197;&#22312;&#20107;&#21518;&#29992;&#26368;&#32456;&#29366;&#24577;&#20316;&#20026;&#30446;&#26631;&#36827;&#34892;&#26631;&#35760;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21482;&#21033;&#29992;&#23569;&#37327;&#30340;&#35821;&#35328;&#25968;&#25454;&#65292;&#21033;&#29992;&#32852;&#21512;&#22270;&#20687;&#21644;&#30446;&#26631;&#20449;&#24687;&#30340;&#31574;&#30053;&#26469;&#22788;&#29702;&#35821;&#35328;&#25509;&#21475;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#22312;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#25110;&#32852;&#21512;&#35757;&#32451;&#35821;&#35328;-&#30446;&#26631;-&#26465;&#20214;&#31574;&#30053;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#27809;&#26377;&#26377;&#25928;&#22320;&#25193;&#23637;&#21040;&#23454;&#38469;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#65292;&#32780;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20174;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#31181;&#23558;&#35821;&#35328;&#19982;&#30446;&#26631;&#22270;&#20687;&#23545;&#40784;&#30340;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#31283;&#20581;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our goal is for robots to follow natural language instructions like "put the towel next to the microwave." But getting large amounts of labeled data, i.e. data that contains demonstrations of tasks labeled with the language instruction, is prohibitive. In contrast, obtaining policies that respond to image goals is much easier, because any autonomous trial or demonstration can be labeled in hindsight with its final state as the goal. In this work, we contribute a method that taps into joint image- and goal- conditioned policies with language using only a small amount of language data. Prior work has made progress on this using vision-language models or by jointly training language-goal-conditioned policies, but so far neither method has scaled effectively to real-world robot tasks without significant human annotation. Our method achieves robust performance in the real world by learning an embedding from the labeled data that aligns language not to the goal image, but rather to the desir
&lt;/p&gt;</description></item><item><title>Ticket-BERT&#26159;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20026;&#20107;&#20214;&#31649;&#29702;&#31080;&#25454;&#36827;&#34892;&#26631;&#27880;&#30340;&#26041;&#27861;&#65292;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#31080;&#25454;&#25968;&#25454;&#21644;&#26102;&#38388;&#25935;&#24863;&#24615;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.00108</link><description>&lt;p&gt;
Ticket-BERT:&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20026;&#20107;&#20214;&#31649;&#29702;&#31080;&#25454;&#36827;&#34892;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Ticket-BERT: Labeling Incident Management Tickets with Language Models. (arXiv:2307.00108v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00108
&lt;/p&gt;
&lt;p&gt;
Ticket-BERT&#26159;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20026;&#20107;&#20214;&#31649;&#29702;&#31080;&#25454;&#36827;&#34892;&#26631;&#27880;&#30340;&#26041;&#27861;&#65292;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#31080;&#25454;&#25968;&#25454;&#21644;&#26102;&#38388;&#25935;&#24863;&#24615;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35299;&#20915;&#20248;&#20808;&#32423;&#20107;&#20214;&#31080;&#25454;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#26159;&#39640;&#25928;&#22320;&#20351;&#29992;&#31934;&#32454;&#20998;&#31867;&#26469;&#26631;&#27880;&#36825;&#20123;&#31080;&#25454;&#12290;&#28982;&#32780;&#65292;&#31080;&#25454;&#25968;&#25454;&#36890;&#24120;&#24456;&#22797;&#26434;&#65292;&#32473;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24102;&#26469;&#20102;&#20960;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#65288;1&#65289;&#31080;&#25454;&#26082;&#21487;&#20197;&#30001;&#39044;&#23450;&#20041;&#31639;&#27861;&#30340;&#26426;&#22120;&#29983;&#25104;&#65292;&#20063;&#21487;&#20197;&#30001;&#20855;&#26377;&#19981;&#21516;&#21327;&#35758;&#30340;&#20855;&#26377;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#30340;&#24037;&#31243;&#24072;&#26356;&#26032;&#21644;&#21019;&#24314;&#65292;&#65288;2&#65289;&#31080;&#25454;&#39057;&#32321;&#36827;&#34892;&#20462;&#35746;&#65292;&#36890;&#36807;&#20462;&#25913;&#20840;&#37096;&#25110;&#37096;&#20998;&#31080;&#25454;&#25551;&#36848;&#26469;&#26356;&#26032;&#31080;&#25454;&#29366;&#24577;&#65292;&#65288;3&#65289;&#31080;&#25454;&#26631;&#27880;&#26159;&#26102;&#38388;&#25935;&#24863;&#30340;&#65292;&#38656;&#35201;&#26681;&#25454;&#36719;&#20214;&#21644;&#30828;&#20214;&#25913;&#36827;&#30340;&#24555;&#36895;&#29983;&#21629;&#21608;&#26399;&#36827;&#34892;&#30693;&#35782;&#26356;&#26032;&#21644;&#26032;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Ticket-BERT&#65292;&#23427;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#31080;&#25454;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#20581;&#22766;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#20026;&#31080;&#25454;&#36827;&#34892;&#26631;&#27880;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Ticket-BERT&#22312;Azure&#35748;&#30693;&#26381;&#21153;&#19978;&#20248;&#20110;&#22522;&#32447;&#21644;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;Ticket-BERT&#23553;&#35013;&#21040;&#19968;&#20010;&#31215;&#26497;&#30340;le...
&lt;/p&gt;
&lt;p&gt;
An essential aspect of prioritizing incident tickets for resolution is efficiently labeling tickets with fine-grained categories. However, ticket data is often complex and poses several unique challenges for modern machine learning methods: (1) tickets are created and updated either by machines with pre-defined algorithms or by engineers with domain expertise that share different protocols, (2) tickets receive frequent revisions that update ticket status by modifying all or parts of ticket descriptions, and (3) ticket labeling is time-sensitive and requires knowledge updates and new labels per the rapid software and hardware improvement lifecycle. To handle these issues, we introduce Ticket- BERT which trains a simple yet robust language model for labeling tickets using our proposed ticket datasets. Experiments demonstrate the superiority of Ticket-BERT over baselines and state-of-the-art text classifiers on Azure Cognitive Services. We further encapsulate Ticket-BERT with an active le
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#27969;&#22330;&#26223;&#20013;&#30340;&#25968;&#25454;&#35268;&#33539;&#21270;&#38382;&#39064;&#65292;&#27604;&#36739;&#20102;&#20843;&#31181;&#36317;&#31163;&#20989;&#25968;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#21457;&#29616;&#22312;&#27809;&#26377;&#39044;&#20808;&#20102;&#35299;&#25968;&#25454;&#27969;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#27969;&#21644;Canberra&#36317;&#31163;&#30340;&#32452;&#21512;&#25928;&#26524;&#36739;&#22909;&#12290;</title><link>http://arxiv.org/abs/2307.00106</link><description>&lt;p&gt;
&#36317;&#31163;&#20989;&#25968;&#22312;&#27969;&#22330;&#26223;&#19979;&#30340;&#35268;&#33539;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Distance Functions and Normalization Under Stream Scenarios. (arXiv:2307.00106v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00106
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#27969;&#22330;&#26223;&#20013;&#30340;&#25968;&#25454;&#35268;&#33539;&#21270;&#38382;&#39064;&#65292;&#27604;&#36739;&#20102;&#20843;&#31181;&#36317;&#31163;&#20989;&#25968;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#21457;&#29616;&#22312;&#27809;&#26377;&#39044;&#20808;&#20102;&#35299;&#25968;&#25454;&#27969;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#27969;&#21644;Canberra&#36317;&#31163;&#30340;&#32452;&#21512;&#25928;&#26524;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#35268;&#33539;&#21270;&#26159;&#24314;&#27169;&#20998;&#31867;&#31995;&#32479;&#26102;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#22312;&#22788;&#29702;&#25968;&#25454;&#27969;&#26102;&#65292;&#30001;&#20110;&#21487;&#33021;&#26080;&#27861;&#39044;&#20808;&#20102;&#35299;&#29305;&#24449;&#30340;&#23646;&#24615;&#65288;&#22914;&#26368;&#23567;/&#26368;&#22823;&#20540;&#65289;&#65292;&#19988;&#36825;&#20123;&#23646;&#24615;&#21487;&#33021;&#20250;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#22240;&#27492;&#25968;&#25454;&#35268;&#33539;&#21270;&#21464;&#24471;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20843;&#31181;&#33879;&#21517;&#30340;&#36317;&#31163;&#20989;&#25968;&#22312;&#27809;&#26377;&#35268;&#33539;&#21270;&#30340;&#25968;&#25454;&#27969;&#20013;&#29983;&#25104;&#30340;&#20934;&#30830;&#24230;&#65292;&#20197;&#21450;&#32771;&#34385;&#21040;&#25509;&#25910;&#21040;&#30340;&#31532;&#19968;&#25209;&#25968;&#25454;&#30340;&#32479;&#35745;&#20449;&#24687;&#21644;&#32771;&#34385;&#21040;&#19978;&#19968;&#25209;&#25968;&#25454;&#30340;&#35268;&#33539;&#21270;&#12290;&#25105;&#20204;&#35748;&#20026;&#23558;&#20840;&#27969;&#31243;&#35270;&#20026;&#35268;&#33539;&#21270;&#30340;&#23454;&#39564;&#21327;&#35758;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#24182;&#20250;&#23548;&#33268;&#20559;&#20506;&#21644;&#31967;&#31957;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20107;&#20808;&#19981;&#20102;&#35299;&#25968;&#25454;&#27969;&#30340;&#20449;&#24687;&#26102;&#65292;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#27969;&#32780;&#19981;&#24212;&#29992;&#35268;&#33539;&#21270;&#65292;&#20197;&#21450;&#20351;&#29992;Canberra&#36317;&#31163;&#65292;&#21487;&#33021;&#26159;&#19968;&#20010;&#22909;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data normalization is an essential task when modeling a classification system. When dealing with data streams, data normalization becomes especially challenging since we may not know in advance the properties of the features, such as their minimum/maximum values, and these properties may change over time. We compare the accuracies generated by eight well-known distance functions in data streams without normalization, normalized considering the statistics of the first batch of data received, and considering the previous batch received. We argue that experimental protocols for streams that consider the full stream as normalized are unrealistic and can lead to biased and poor results. Our results indicate that using the original data stream without applying normalization, and the Canberra distance, can be a good combination when no information about the data stream is known beforehand.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#35270;&#39057;&#24207;&#21015;&#20013;&#28895;&#38654;&#27169;&#24335;&#30340;&#26102;&#38388;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#34987;&#36974;&#25377;&#30340;&#37326;&#28779;&#28779;&#28976;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#28779;&#28798;&#20301;&#32622;&#26469;&#24110;&#21161;&#26080;&#20154;&#26426;&#23545;&#25239;&#26862;&#26519;&#28779;&#28798;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#29420;&#29305;&#30340;&#26816;&#27979;&#34987;&#36974;&#25377;&#30340;&#28779;&#28798;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.00104</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#26080;&#20154;&#26426;&#25429;&#25417;&#30340;&#28895;&#38654;&#27169;&#24335;&#30340;&#26102;&#38388;&#20998;&#26512;&#26469;&#26816;&#27979;&#34987;&#36974;&#25377;&#30340;&#37326;&#28779;&#28779;&#28976;
&lt;/p&gt;
&lt;p&gt;
Obscured Wildfire Flame Detection By Temporal Analysis of Smoke Patterns Captured by Unmanned Aerial Systems. (arXiv:2307.00104v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#35270;&#39057;&#24207;&#21015;&#20013;&#28895;&#38654;&#27169;&#24335;&#30340;&#26102;&#38388;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#34987;&#36974;&#25377;&#30340;&#37326;&#28779;&#28779;&#28976;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#28779;&#28798;&#20301;&#32622;&#26469;&#24110;&#21161;&#26080;&#20154;&#26426;&#23545;&#25239;&#26862;&#26519;&#28779;&#28798;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#29420;&#29305;&#30340;&#26816;&#27979;&#34987;&#36974;&#25377;&#30340;&#28779;&#28798;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#35299;&#20915;&#20102;&#20351;&#29992;&#20165;&#37197;&#22791;RGB&#30456;&#26426;&#30340;&#26080;&#20154;&#26426;&#23454;&#26102;&#26816;&#27979;&#34987;&#26641;&#26408;&#12289;&#28895;&#38654;&#12289;&#20113;&#38654;&#21644;&#20854;&#20182;&#33258;&#28982;&#23631;&#38556;&#36974;&#25377;&#30340;&#37326;&#28779;&#65288;&#24403;&#28779;&#28976;&#34987;&#36974;&#25377;&#26102;&#65289;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35270;&#39057;&#24207;&#21015;&#20013;&#28895;&#38654;&#27169;&#24335;&#30340;&#35821;&#20041;&#20998;&#21106;&#36827;&#34892;&#26102;&#38388;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;CNN&#32534;&#30721;&#22120;&#21644;3D&#21367;&#31215;&#36827;&#34892;&#35299;&#30721;&#65292;&#24182;&#20351;&#29992;&#29305;&#24449;&#30340;&#39034;&#24207;&#21472;&#21152;&#26469;&#21033;&#29992;&#26102;&#38388;&#21464;&#21270;&#12290;&#39044;&#27979;&#30340;&#28779;&#28798;&#20301;&#32622;&#21487;&#20197;&#24110;&#21161;&#26080;&#20154;&#26426;&#26377;&#25928;&#22320;&#23545;&#25239;&#26862;&#26519;&#28779;&#28798;&#65292;&#24182;&#20934;&#30830;&#23450;&#20301;&#28779;&#28976;&#20301;&#32622;&#36827;&#34892;&#38459;&#29123;&#21270;&#23398;&#29289;&#36136;&#25237;&#25918;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20174;FLAME2&#25968;&#25454;&#38598;&#34893;&#29983;&#30340;&#31934;&#36873;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;RGB&#35270;&#39057;&#21644;IR&#35270;&#39057;&#20197;&#30830;&#23450;&#22320;&#38754;&#30495;&#23454;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26816;&#27979;&#34987;&#36974;&#25377;&#30340;&#28779;&#28798;&#30340;&#29420;&#29305;&#23646;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;Dice&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper addresses the challenge of detecting obscured wildfires (when the fire flames are covered by trees, smoke, clouds, and other natural barriers) in real-time using drones equipped only with RGB cameras. We propose a novel methodology that employs semantic segmentation based on the temporal analysis of smoke patterns in video sequences. Our approach utilizes an encoder-decoder architecture based on deep convolutional neural network architecture with a pre-trained CNN encoder and 3D convolutions for decoding while using sequential stacking of features to exploit temporal variations. The predicted fire locations can assist drones in effectively combating forest fires and pinpoint fire retardant chemical drop on exact flame locations. We applied our method to a curated dataset derived from the FLAME2 dataset that includes RGB video along with IR video to determine the ground truth. Our proposed method has a unique property of detecting obscured fire and achieves a Dice sc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20915;&#31574;&#24314;&#27169;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#36174;&#22238;&#25968;&#25454;&#31185;&#23398;&#23454;&#36341;&#24182;&#35299;&#20915;&#20854;&#22522;&#30784;&#22833;&#21435;&#30340;&#38382;&#39064;&#12290;&#20915;&#31574;&#24314;&#27169;&#23558;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19982;&#26174;&#24335;&#20215;&#20540;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20845;&#20010;&#21407;&#21017;&#26680;&#24515;&#26500;&#25104;&#65292;&#25552;&#39640;&#20102;&#20915;&#31574;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.00088</link><description>&lt;p&gt;
&#36890;&#36807;&#20915;&#31574;&#24314;&#27169;&#65292;&#36174;&#22238;&#25968;&#25454;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
Redeeming Data Science by Decision Modelling. (arXiv:2307.00088v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20915;&#31574;&#24314;&#27169;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#36174;&#22238;&#25968;&#25454;&#31185;&#23398;&#23454;&#36341;&#24182;&#35299;&#20915;&#20854;&#22522;&#30784;&#22833;&#21435;&#30340;&#38382;&#39064;&#12290;&#20915;&#31574;&#24314;&#27169;&#23558;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19982;&#26174;&#24335;&#20215;&#20540;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20845;&#20010;&#21407;&#21017;&#26680;&#24515;&#26500;&#25104;&#65292;&#25552;&#39640;&#20102;&#20915;&#31574;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#31185;&#23398;&#24212;&#29992;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#65292;&#35813;&#39046;&#22495;&#24050;&#32463;&#25670;&#33073;&#20102;&#20854;&#22522;&#30784;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#38656;&#35201;&#22312;&#23545;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#29087;&#24713;&#30340;&#39046;&#22495;&#36827;&#34892;&#26032;&#30340;&#24212;&#29992;&#30740;&#31350;&#65292;&#20197;&#20511;&#37492;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#24314;&#31435;&#25968;&#25454;&#31185;&#23398;&#23454;&#36341;&#65292;&#24182;&#23558;&#20854;&#31216;&#20026;&#8220;&#20915;&#31574;&#24314;&#27169;&#8221;&#12290;&#26412;&#25991;&#31616;&#35201;&#22238;&#39038;&#20102;&#24314;&#27169;&#36807;&#31243;&#65292;&#23558;&#20854;&#35270;&#20026;&#26500;&#24314;&#22240;&#26524;&#22270;&#27169;&#22411;&#65292;&#28982;&#21518;&#20174;&#27969;&#34892;&#30340;&#21830;&#19994;&#25991;&#29486;&#30340;&#35282;&#24230;&#35752;&#35770;&#35813;&#36807;&#31243;&#30340;&#20845;&#20010;&#21407;&#21017;&#65292;&#26500;&#25104;&#20102;&#8220;&#20915;&#31574;&#36136;&#37327;&#8221;&#26694;&#26550;&#12290;&#25105;&#20204;&#35748;&#20026;&#20219;&#20309;&#25104;&#21151;&#30340;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#24314;&#27169;&#24037;&#20316;&#37117;&#24517;&#39035;&#21253;&#25324;&#36825;&#20845;&#20010;&#21407;&#21017;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#20915;&#31574;&#24314;&#27169;&#22914;&#20309;&#23558;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19982;&#26174;&#24335;&#20215;&#20540;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#12290;&#20026;&#20102;&#32473;&#20986;&#20855;&#20307;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#27169;&#22411;&#30340;ROC&#26354;&#32447;&#19982;&#25928;&#29992;&#27169;&#22411;&#25972;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the explosion of applications of Data Science, the field is has come loose from its foundations. This article argues for a new program of applied research in areas familiar to researchers in Bayesian methods in AI that are needed to ground the practice of Data Science by borrowing from AI techniques for model formulation that we term ``Decision Modelling.'' This article briefly reviews the formulation process as building a causal graphical model, then discusses the process in terms of six principles that comprise \emph{Decision Quality}, a framework from the popular business literature. We claim that any successful applied ML modelling effort must include these six principles.  We explain how Decision Modelling combines a conventional machine learning model with an explicit value model. To give a specific example we show how this is done by integrating a model's ROC curve with a utility model.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36328;&#26696;&#20363;&#39044;&#27979;&#36807;&#31243;&#30417;&#27979;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#30340;&#24433;&#21709;&#65292;&#24182;&#21253;&#25324;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23545;&#20110;&#22788;&#29702;&#39640;&#32500;&#29305;&#24449;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.00080</link><description>&lt;p&gt;
&#36328;&#26696;&#20363;&#39044;&#27979;&#36807;&#31243;&#30417;&#27979;&#65306;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#20505;&#36873;&#32773;&#65311;
&lt;/p&gt;
&lt;p&gt;
Inter-case Predictive Process Monitoring: A candidate for Quantum Machine Learning?. (arXiv:2307.00080v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00080
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36328;&#26696;&#20363;&#39044;&#27979;&#36807;&#31243;&#30417;&#27979;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#30340;&#24433;&#21709;&#65292;&#24182;&#21253;&#25324;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23545;&#20110;&#22788;&#29702;&#39640;&#32500;&#29305;&#24449;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#35770;&#39046;&#22495;&#22914;&#20309;&#65292;&#39044;&#27979;&#27491;&#22312;&#36816;&#34892;&#30340;&#36807;&#31243;&#23454;&#20363;&#30340;&#26410;&#26469;&#34892;&#20026;&#37117;&#26159;&#20915;&#31574;&#32773;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#22810;&#20010;&#23454;&#20363;&#30456;&#20114;&#20316;&#29992;&#26102;&#12290;&#21463;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#25512;&#21160;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#39044;&#27979;&#36807;&#31243;&#30340;&#19979;&#19968;&#20010;&#27963;&#21160;&#12289;&#32467;&#26524;&#25110;&#21097;&#20313;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#20855;&#26377;&#39640;&#39044;&#27979;&#33021;&#21147;&#30340;&#27169;&#22411;&#26082;&#38656;&#35201;&#20174;&#20107;&#20214;&#26085;&#24535;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#30340;&#20869;&#22312;&#30693;&#35782;&#65292;&#21448;&#38656;&#35201;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#30340;&#27169;&#22411;&#12290;&#36825;&#39033;&#24037;&#20316;&#20197;&#36328;&#26696;&#20363;&#39044;&#27979;&#36807;&#31243;&#30417;&#27979;&#65288;PPM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#22522;&#30784;&#65292;&#20840;&#38754;&#35780;&#20272;&#20102;&#36328;&#26696;&#20363;&#29305;&#24449;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21253;&#25324;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#34987;&#39044;&#26399;&#22312;&#29305;&#24449;&#32500;&#24230;&#19981;&#26029;&#25193;&#23637;&#26102;&#27604;&#32463;&#20856;&#27169;&#22411;&#20855;&#26377;&#20248;&#21183;&#12290;&#23545;&#26469;&#33258;BPI&#25361;&#25112;&#36187;&#30340;&#30495;&#23454;&#35757;&#32451;&#25968;&#25454;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#36328;&#26696;&#20363;&#29305;&#24449;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#26377;&#30528;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regardless of the domain, forecasting the future behaviour of a running process instance is a question of interest for decision makers, especially when multiple instances interact. Fostered by the recent advances in machine learning research, several methods have been proposed to predict the next activity, outcome or remaining time of a process automatically. Still, building a model with high predictive power requires both - intrinsic knowledge of how to extract meaningful features from the event log data and a model that captures complex patterns in data. This work builds upon the recent progress in inter-case Predictive Process Monitoring (PPM) and comprehensively benchmarks the impact of inter-case features on prediction accuracy. Moreover, it includes quantum machine learning models, which are expected to provide an advantage over classical models with a scaling amount of feature dimensions. The evaluation on real-world training data from the BPI challenge shows that the inter-case
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#38598;&#24179;&#34913;&#26041;&#27861;&#22312;&#25552;&#39640;&#24120;&#35265;&#31867;&#21035;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20250;&#25439;&#23475;&#31232;&#26377;&#31867;&#21035;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20854;&#25928;&#26524;&#20381;&#36182;&#20110;&#35780;&#20272;&#38598;&#12290;</title><link>http://arxiv.org/abs/2307.00079</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#24179;&#34913;&#20250;&#25439;&#23475;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Dataset balancing can hurt model performance. (arXiv:2307.00079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00079
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#24179;&#34913;&#26041;&#27861;&#22312;&#25552;&#39640;&#24120;&#35265;&#31867;&#21035;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20250;&#25439;&#23475;&#31232;&#26377;&#31867;&#21035;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20854;&#25928;&#26524;&#20381;&#36182;&#20110;&#35780;&#20272;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#27599;&#20010;&#31867;&#21035;&#31034;&#20363;&#20998;&#24067;&#19981;&#22343;&#21248;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#21487;&#33021;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#24120;&#35265;&#31867;&#21035;&#19978;&#24615;&#33021;&#20248;&#31168;&#65292;&#20294;&#23545;&#20110;&#31232;&#26377;&#31867;&#21035;&#30340;&#24615;&#33021;&#19981;&#20339;&#12290;AudioSet&#25317;&#26377;527&#20010;&#22768;&#38899;&#20107;&#20214;&#31867;&#21035;&#65292;&#20854;&#20808;&#39564;&#20998;&#24067;&#33539;&#22260;&#38750;&#24120;&#24191;&#27867;&#12290;&#36890;&#24120;&#36890;&#36807;&#23545;&#27599;&#20010;&#31867;&#21035;&#25351;&#26631;&#30340;&#31616;&#21333;&#24179;&#22343;&#26469;&#35780;&#20272;AudioSet&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#36825;&#24847;&#21619;&#30528;&#31232;&#26377;&#31867;&#21035;&#30340;&#24615;&#33021;&#19982;&#24120;&#35265;&#31867;&#21035;&#30340;&#24615;&#33021;&#21516;&#31561;&#37325;&#35201;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#35770;&#25991;&#21033;&#29992;&#25968;&#25454;&#38598;&#24179;&#34913;&#25216;&#26415;&#26469;&#25552;&#39640;&#22312;AudioSet&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#24179;&#34913;&#22312;&#20844;&#24320;&#30340;AudioSet&#35780;&#20272;&#25968;&#25454;&#19978;&#25913;&#21892;&#20102;&#24615;&#33021;&#65292;&#20294;&#21516;&#26102;&#20063;&#25439;&#23475;&#20102;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#25910;&#38598;&#30340;&#26410;&#21457;&#24067;&#35780;&#20272;&#38598;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#35843;&#25972;&#24179;&#34913;&#31243;&#24230;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#22909;&#22788;&#26159;&#33030;&#24369;&#30340;&#65292;&#24182;&#19988;&#21462;&#20915;&#20110;&#35780;&#20272;&#38598;&#12290;&#25105;&#20204;&#20063;&#27809;&#26377;&#25214;&#21040;&#26174;&#31034;&#24179;&#34913;&#30456;&#23545;&#20110;&#24120;&#35265;&#31867;&#21035;&#20250;&#25913;&#21892;&#31232;&#26377;&#31867;&#21035;&#24615;&#33021;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning from training data with a skewed distribution of examples per class can lead to models that favor performance on common classes at the expense of performance on rare ones. AudioSet has a very wide range of priors over its 527 sound event classes. Classification performance on AudioSet is usually evaluated by a simple average over per-class metrics, meaning that performance on rare classes is equal in importance to the performance on common ones. Several recent papers have used dataset balancing techniques to improve performance on AudioSet. We find, however, that while balancing improves performance on the public AudioSet evaluation data it simultaneously hurts performance on an unpublished evaluation set collected under the same conditions. By varying the degree of balancing, we show that its benefits are fragile and depend on the evaluation set. We also do not find evidence indicating that balancing improves rare class performance relative to common classes. We there
&lt;/p&gt;</description></item><item><title>Transformer&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#20197;&#29992;&#20110;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#12289;&#20020;&#24202;&#35786;&#26029;&#12289;&#25253;&#21578;&#29983;&#25104;&#12289;&#25968;&#25454;&#37325;&#24314;&#21644;&#33647;&#29289;/&#34507;&#30333;&#36136;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.00067</link><description>&lt;p&gt;
&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;Transformer&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Transformers in Healthcare: A Survey. (arXiv:2307.00067v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00067
&lt;/p&gt;
&lt;p&gt;
Transformer&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#20197;&#29992;&#20110;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#12289;&#20020;&#24202;&#35786;&#26029;&#12289;&#25253;&#21578;&#29983;&#25104;&#12289;&#25968;&#25454;&#37325;&#24314;&#21644;&#33647;&#29289;/&#34507;&#30333;&#36136;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#31561;&#31038;&#20250;&#21508;&#20010;&#26041;&#38754;&#30340;&#26085;&#30410;&#28183;&#36879;&#65292;Transformer&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#37319;&#29992;&#27491;&#22312;&#24555;&#36895;&#25913;&#21464;&#35768;&#22810;&#24212;&#29992;&#12290;Transformer&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#26368;&#21021;&#26159;&#20026;&#20102;&#35299;&#20915;&#36890;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#32780;&#24320;&#21457;&#30340;&#65292;&#24182;&#38543;&#21518;&#22312;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#22312;&#20869;&#30340;&#35768;&#22810;&#39046;&#22495;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#22312;&#26412;&#35843;&#26597;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#35813;&#26550;&#26500;&#22914;&#20309;&#34987;&#24212;&#29992;&#20110;&#20998;&#26512;&#21508;&#31181;&#24418;&#24335;&#30340;&#25968;&#25454;&#65292;&#21253;&#25324;&#21307;&#23398;&#24433;&#20687;&#12289;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#12289;&#31038;&#20132;&#23186;&#20307;&#12289;&#29983;&#29702;&#20449;&#21495;&#21644;&#29983;&#29289;&#20998;&#23376;&#24207;&#21015;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#35786;&#26029;&#12289;&#25253;&#21578;&#29983;&#25104;&#12289;&#25968;&#25454;&#37325;&#24314;&#21644;&#33647;&#29289;/&#34507;&#30333;&#36136;&#21512;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#39318;&#36873;&#25253;&#21578;&#20107;&#39033;&#36827;&#34892;&#31995;&#32479;&#32508;&#36848;&#21644;meta&#20998;&#26512;&#65288;PRISMA&#65289;&#25351;&#21335;&#26469;&#30830;&#23450;&#30456;&#20851;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20351;&#29992;Transformer&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With Artificial Intelligence (AI) increasingly permeating various aspects of society, including healthcare, the adoption of the Transformers neural network architecture is rapidly changing many applications. Transformer is a type of deep learning architecture initially developed to solve general-purpose Natural Language Processing (NLP) tasks and has subsequently been adapted in many fields, including healthcare. In this survey paper, we provide an overview of how this architecture has been adopted to analyze various forms of data, including medical imaging, structured and unstructured Electronic Health Records (EHR), social media, physiological signals, and biomolecular sequences. Those models could help in clinical diagnosis, report generation, data reconstruction, and drug/protein synthesis. We identified relevant studies using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. We also discuss the benefits and limitations of using transformer
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#35299;&#33258;&#36866;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#35753;&#26550;&#26500;SeDAN&#65292;&#36890;&#36807;&#23545;&#40784;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#38598;&#30340;&#21487;&#36801;&#31227;&#30693;&#35782;&#26469;&#25913;&#21892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00066</link><description>&lt;p&gt;
&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#21487;&#36801;&#31227;&#24615;&#65306;&#36890;&#36807;&#20998;&#35299;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Improving the Transferability of Time Series Forecasting with Decomposition Adaptation. (arXiv:2307.00066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00066
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#35299;&#33258;&#36866;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#35753;&#26550;&#26500;SeDAN&#65292;&#36890;&#36807;&#23545;&#40784;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#38598;&#30340;&#21487;&#36801;&#31227;&#30693;&#35782;&#26469;&#25913;&#21892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31070;&#32463;&#39044;&#27979;&#27169;&#22411;&#36890;&#36807;&#26377;&#25928;&#30340;&#27169;&#24335;&#25366;&#25496;&#21644;&#29305;&#24449;&#34920;&#31034;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#26377;&#25928;&#23398;&#20064;&#30340;&#21069;&#25552;&#26159;&#25910;&#38598;&#36275;&#22815;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24456;&#38590;&#33719;&#24471;&#36275;&#22815;&#30340;&#25968;&#25454;&#65292;&#36825;&#38480;&#21046;&#20102;&#31070;&#32463;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#35753;&#26550;&#26500;Sequence Decomposition Adaptation Network&#65288;SeDAN&#65289;&#65292;&#36890;&#36807;&#23558;&#26469;&#33258;&#36328;&#39046;&#22495;&#25968;&#25454;&#38598;&#30340;&#21487;&#36801;&#31227;&#30693;&#35782;&#36827;&#34892;&#23545;&#40784;&#65292;&#20197;&#25552;&#39640;&#30446;&#26631;&#39046;&#22495;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#37325;&#26032;&#24605;&#32771;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#29305;&#24449;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38544;&#24335;&#23545;&#27604;&#20998;&#35299;&#26041;&#27861;&#65292;&#23558;&#21407;&#22987;&#29305;&#24449;&#20998;&#35299;&#20026;&#21253;&#25324;&#23395;&#33410;&#24615;&#21644;&#36235;&#21183;&#24615;&#29305;&#24449;&#22312;&#20869;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#20123;&#29305;&#24449;&#26356;&#23481;&#26131;&#36801;&#31227;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20026;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#20998;&#35299;&#29305;&#24449;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#23395;&#33410;&#24615;&#29305;&#24449;&#65292;&#25105;&#20204;&#36827;&#34892;&#32852;&#21512;&#20998;&#24067;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to effective pattern mining and feature representation, neural forecasting models based on deep learning have achieved great progress. The premise of effective learning is to collect sufficient data. However, in time series forecasting, it is difficult to obtain enough data, which limits the performance of neural forecasting models. To alleviate the data scarcity limitation, we design Sequence Decomposition Adaptation Network (SeDAN) which is a novel transfer architecture to improve forecasting performance on the target domain by aligning transferable knowledge from cross-domain datasets. Rethinking the transferability of features in time series data, we propose Implicit Contrastive Decomposition to decompose the original features into components including seasonal and trend features, which are easier to transfer. Then we design the corresponding adaptation methods for decomposed features in different domains. Specifically, for seasonal features, we perform joint distribution adapt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#23567;&#33041;&#20013;&#22522;&#20110;&#38169;&#35823;&#30340;&#31070;&#32463;&#20803;&#32452;&#32455;&#26041;&#24335;&#65292;&#21457;&#29616;&#22312;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#20837;&#20010;&#24615;&#21270;&#38169;&#35823;&#35270;&#22270;&#30340;&#31070;&#32463;&#20803;&#32676;&#20307;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12289;&#20943;&#23569;&#19981;&#24179;&#34913;&#25968;&#25454;&#21644;&#25463;&#24452;&#31574;&#30053;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.00039</link><description>&lt;p&gt;
&#20197;&#33041;&#21551;&#21457;&#35774;&#35745;&#35299;&#20915;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#36275;&#20043;&#22788;
&lt;/p&gt;
&lt;p&gt;
Towards Brain Inspired Design for Addressing the Shortcomings of ANNs. (arXiv:2307.00039v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#23567;&#33041;&#20013;&#22522;&#20110;&#38169;&#35823;&#30340;&#31070;&#32463;&#20803;&#32452;&#32455;&#26041;&#24335;&#65292;&#21457;&#29616;&#22312;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#20837;&#20010;&#24615;&#21270;&#38169;&#35823;&#35270;&#22270;&#30340;&#31070;&#32463;&#20803;&#32676;&#20307;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12289;&#20943;&#23569;&#19981;&#24179;&#34913;&#25968;&#25454;&#21644;&#25463;&#24452;&#31574;&#30053;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#22823;&#33041;&#21151;&#33021;&#26426;&#21046;&#30340;&#29702;&#35299;&#25552;&#39640;&#65292;&#20174;&#31070;&#32463;&#31185;&#23398;&#20013;&#33719;&#24471;&#30340;&#27934;&#23519;&#21147;&#23545;&#20110;AI&#31639;&#27861;&#30340;&#21457;&#23637;&#20540;&#24471;&#36827;&#19968;&#27493;&#32771;&#34385;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#19968;&#31181;&#22522;&#20110;&#26641;&#29366;&#32467;&#26500;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19982;&#26368;&#36817;&#30340;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;[27]&#36827;&#34892;&#23545;&#27604;&#65292;&#35748;&#20026;&#23567;&#33041;&#20013;&#22522;&#20110;&#38169;&#35823;&#30340;&#31070;&#32463;&#20803;&#32452;&#32455;&#26041;&#24335;&#21487;&#33021;&#26159;&#34892;&#20026;&#21644;&#23398;&#20064;&#30340;&#20960;&#20010;&#29702;&#24819;&#29305;&#24449;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#38543;&#21518;&#20998;&#26512;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#30340;&#23398;&#20064;&#34892;&#20026;&#21644;&#29305;&#28857;&#65292;&#20197;&#35780;&#20272;&#31867;&#20284;&#26426;&#21046;&#22312;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20010;&#24615;&#21270;&#38169;&#35823;&#35270;&#22270;&#30340;&#31070;&#32463;&#20803;&#32676;&#20307;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#20943;&#23569;&#21463;&#24847;&#22806;&#25463;&#24452;&#31574;&#30053;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#39033;&#24037;&#20316;&#31361;&#20986;&#20102;&#23558;&#23398;&#20064;&#26041;&#24335;&#20174;&#31070;&#32463;&#31185;&#23398;&#20013;&#36716;&#21270;&#20026;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
As our understanding of the mechanisms of brain function is enhanced, the value of insights gained from neuroscience to the development of AI algorithms deserves further consideration. Here, we draw parallels with an existing tree-based ANN architecture and a recent neuroscience study[27] arguing that the error-based organization of neurons in the cerebellum that share a preference for a personalized view of the entire error space, may account for several desirable features of behavior and learning. We then analyze the learning behavior and characteristics of the model under varying scenarios to gauge the potential benefits of a similar mechanism in ANN. Our empirical results suggest that having separate populations of neurons with personalized error views can enable efficient learning under class imbalance and limited data, and reduce the susceptibility to unintended shortcut strategies, leading to improved generalization. This work highlights the potential of translating the learning
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#38669;&#26684;&#27779;&#33576;&#39764;&#27861;&#23398;&#26657;&#30340;&#39764;&#33647;&#21457;&#23637;&#20013;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.00036</link><description>&lt;p&gt;
&#38669;&#26684;&#27779;&#33576;&#39764;&#27861;&#23398;&#26657;&#20013;&#30340;&#39764;&#33647;&#21457;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine learning for potion development at Hogwarts. (arXiv:2307.00036v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00036
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#38669;&#26684;&#27779;&#33576;&#39764;&#27861;&#23398;&#26657;&#30340;&#39764;&#33647;&#21457;&#23637;&#20013;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#30830;&#23450;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#20026;&#38669;&#26684;&#27779;&#33576;&#39764;&#27861;&#23398;&#26657;&#30340;&#30740;&#31350;&#21644;&#25945;&#23398;&#29983;&#25104;&#26377;&#29992;&#30340;&#39764;&#33647;&#37197;&#26041;&#12290;&#35774;&#35745;&#65306;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#29983;&#25104;&#30340;&#37197;&#26041;&#20998;&#31867;&#21040;&#26631;&#20934;&#33647;&#29289;&#20998;&#31867;&#31995;&#32479;&#20013;&#12290;&#25968;&#25454;&#26469;&#28304;&#65306;&#20174;&#21704;&#21033;&#27874;&#29305;&#32500;&#22522;&#20013;&#25552;&#21462;&#30340;&#38669;&#26684;&#27779;&#33576;&#35838;&#31243;&#30340;72&#20010;&#39764;&#33647;&#37197;&#26041;&#12290;&#32467;&#26524;&#65306;&#22823;&#22810;&#25968;&#29983;&#25104;&#30340;&#37197;&#26041;&#23646;&#20110;&#20852;&#22859;&#21058;&#21644;&#30382;&#32932;&#33647;&#21697;&#30340;&#31867;&#21035;&#12290;&#27599;&#20010;&#31867;&#21035;&#30340;&#39044;&#27979;&#37197;&#26041;&#25968;&#37327;&#19982;&#35757;&#32451;&#37197;&#26041;&#25968;&#37327;&#30456;&#23545;&#24212;&#12290;&#39044;&#27979;&#30340;&#27010;&#29575;&#36890;&#24120;&#36229;&#36807;90%&#65292;&#20294;&#26377;&#20123;&#37197;&#26041;&#34987;&#20998;&#31867;&#21040;2&#20010;&#25110;&#26356;&#22810;&#20855;&#26377;&#30456;&#20284;&#27010;&#29575;&#30340;&#31867;&#21035;&#20013;&#65292;&#36825;&#22686;&#21152;&#20102;&#23545;&#39044;&#27979;&#25928;&#26524;&#30340;&#39044;&#26399;&#30340;&#22797;&#26434;&#24615;&#12290;&#32467;&#35770;&#65306;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#33021;&#22815;&#20026;&#38669;&#26684;&#27779;&#33576;&#39764;&#27861;&#23398;&#26657;&#30340;&#25945;&#23398;&#21644;&#30740;&#31350;&#29983;&#25104;&#28508;&#22312;&#26377;&#29992;&#30340;&#39764;&#33647;&#37197;&#26041;&#12290;&#36825;&#19982;&#38750;&#39764;&#27861;&#19990;&#30028;&#20013;&#30340;&#31867;&#20284;&#21162;&#21147;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: To determine whether machine learning methods can generate useful potion recipes for research and teaching at Hogwarts School of Witchcraft and Wizardry. Design: Using deep neural networks to classify generated recipes into a standard drug classification system. Setting: Hogwarts School of Witchcraft and Wizardry. Data sources: 72 potion recipes from the Hogwarts curriculum, extracted from the Harry Potter Wiki. Results: Most generated recipes fall into the categories of psychoanaleptics and dermatologicals. The number of recipes predicted for each category reflected the number of training recipes. Predicted probabilities were often above 90% but some recipes were classified into 2 or more categories with similar probabilities which complicates anticipating the predicted effects. Conclusions: Machine learning powered methods are able to generate potentially useful potion recipes for teaching and research at Hogwarts. This corresponds to similar efforts in the non-magical wor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#31354;&#26102;&#21464;&#31995;&#25968;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#21442;&#25968;&#35782;&#21035;&#38382;&#39064;&#12290;&#36890;&#36807;&#32422;&#26463;&#33258;&#36866;&#24212;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21644;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#20934;&#30830;&#30830;&#23450;&#22797;&#26434;&#22810;&#29366;&#24577;&#31995;&#32479;&#30340;&#26410;&#30693;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.00035</link><description>&lt;p&gt;
&#21442;&#25968;&#35782;&#21035;&#21450;&#20854;&#22312;&#20855;&#26377;&#31354;&#26102;&#21464;&#31995;&#25968;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Parameter Identification for Partial Differential Equations with Spatiotemporal Varying Coefficients. (arXiv:2307.00035v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#31354;&#26102;&#21464;&#31995;&#25968;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#21442;&#25968;&#35782;&#21035;&#38382;&#39064;&#12290;&#36890;&#36807;&#32422;&#26463;&#33258;&#36866;&#24212;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21644;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#20934;&#30830;&#30830;&#23450;&#22797;&#26434;&#22810;&#29366;&#24577;&#31995;&#32479;&#30340;&#26410;&#30693;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#29702;&#35299;&#20855;&#26377;&#22810;&#20010;&#29366;&#24577;&#30340;&#22797;&#26434;&#31995;&#32479;&#65292;&#25581;&#31034;&#31995;&#32479;&#36755;&#20986;&#30340;&#36825;&#20123;&#29366;&#24577;&#30340;&#26631;&#35782;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#28982;&#32780;&#65292;&#25551;&#36848;&#36825;&#20123;&#31995;&#32479;&#30340;&#25968;&#23398;&#27169;&#22411;&#36890;&#24120;&#21576;&#29616;&#38750;&#32447;&#24615;&#65292;&#22240;&#27492;&#20174;&#35266;&#27979;&#21040;&#30340;&#31354;&#26102;&#25968;&#25454;&#20013;&#35299;&#20915;&#21442;&#25968;&#21453;&#38382;&#39064;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#20174;&#36825;&#20123;&#31995;&#32479;&#33719;&#24471;&#30340;&#35266;&#27979;&#25968;&#25454;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#30740;&#31350;&#30001;&#31354;&#26102;&#21464;&#21442;&#25968;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#25152;&#25903;&#37197;&#30340;&#22810;&#29366;&#24577;&#31995;&#32479;&#30340;&#21442;&#25968;&#35782;&#21035;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#19968;&#20010;&#32422;&#26463;&#30340;&#33258;&#36866;&#24212;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65292;&#20316;&#20026;&#25105;&#20204;&#30340;&#21442;&#25968;&#35782;&#21035;&#26041;&#27861;&#30340;&#19968;&#20010;&#23376;&#32593;&#32476;&#65292;&#20197;&#21450;&#19968;&#31181;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#26041;&#27861;&#26469;&#26816;&#27979;&#21487;&#33021;&#21442;&#25968;&#21464;&#21270;&#30340;&#21306;&#22495;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#26696;&#65292;&#25105;&#20204;&#21487;&#20197;&#20934;&#30830;&#30830;&#23450;&#22797;&#26434;&#22810;&#29366;&#24577;&#31995;&#32479;&#30340;&#26410;&#30693;&#21464;&#21270;&#21442;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21442;&#25968;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
To comprehend complex systems with multiple states, it is imperative to reveal the identity of these states by system outputs. Nevertheless, the mathematical models describing these systems often exhibit nonlinearity so that render the resolution of the parameter inverse problem from the observed spatiotemporal data a challenging endeavor. Starting from the observed data obtained from such systems, we propose a novel framework that facilitates the investigation of parameter identification for multi-state systems governed by spatiotemporal varying parametric partial differential equations. Our framework consists of two integral components: a constrained self-adaptive physics-informed neural network, encompassing a sub-network, as our methodology for parameter identification, and a finite mixture model approach to detect regions of probable parameter variations. Through our scheme, we can precisely ascertain the unknown varying parameters of the complex multi-state system, thereby accomp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24212;&#29992;&#25968;&#25454;&#24037;&#31243;&#26041;&#27861;&#35299;&#20915;&#24494;&#29983;&#29289;&#32452;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#39640;&#32500;&#24230;&#38382;&#39064;&#65292;&#38024;&#23545;&#22218;&#24615;&#32420;&#32500;&#21270;&#23156;&#20799;&#30340;&#25968;&#25454;&#38598;&#23454;&#26045;&#20102;&#22235;&#31181;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#39640;&#20102;&#21307;&#23398;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.00033</link><description>&lt;p&gt;
&#24212;&#29992;&#25968;&#25454;&#24037;&#31243;&#26041;&#27861;&#35299;&#20915;&#24494;&#29983;&#29289;&#32452;&#25968;&#25454;&#22312;&#21307;&#23398;&#20915;&#31574;&#20013;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Application of data engineering approaches to address challenges in microbiome data for optimal medical decision-making. (arXiv:2307.00033v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24212;&#29992;&#25968;&#25454;&#24037;&#31243;&#26041;&#27861;&#35299;&#20915;&#24494;&#29983;&#29289;&#32452;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#39640;&#32500;&#24230;&#38382;&#39064;&#65292;&#38024;&#23545;&#22218;&#24615;&#32420;&#32500;&#21270;&#23156;&#20799;&#30340;&#25968;&#25454;&#38598;&#23454;&#26045;&#20102;&#22235;&#31181;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#39640;&#20102;&#21307;&#23398;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23427;&#20204;&#19982;&#22810;&#20010;&#22120;&#23448;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20154;&#20307;&#32928;&#36947;&#24494;&#29983;&#29289;&#32676;&#33853;&#24050;&#34987;&#35777;&#26126;&#23545;&#26426;&#20307;&#30340;&#20247;&#22810;&#29983;&#29702;&#21151;&#33021;&#36215;&#30528;&#36129;&#29486;&#65292;&#24182;&#19988;&#36824;&#19982;&#19968;&#31995;&#21015;&#30149;&#29702;&#26465;&#20214;&#26377;&#20851;&#12290;&#36807;&#21435;&#20960;&#21313;&#24180;&#30340;&#22823;&#37327;&#30740;&#31350;&#24037;&#20316;&#24050;&#32463;&#25552;&#20379;&#20102;&#26377;&#20851;&#32928;&#36947;&#24494;&#29983;&#29289;&#32676;&#33853;&#30340;&#30456;&#23545;&#20998;&#31867;&#20998;&#24067;&#30340;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#23454;&#29616;&#20010;&#20307;&#21270;&#21307;&#23398;&#12290;&#28982;&#32780;&#65292;&#24494;&#29983;&#29289;&#32452;&#25968;&#25454;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#39640;&#32500;&#24230;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#24517;&#39035;&#21152;&#20197;&#35299;&#20915;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#25968;&#25454;&#24037;&#31243;&#31639;&#27861;&#26469;&#35299;&#20915;&#19982;&#24494;&#29983;&#29289;&#32452;&#25968;&#25454;&#30456;&#20851;&#30340;&#19978;&#36848;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#20197;&#22218;&#24615;&#32420;&#32500;&#21270;&#23156;&#20799;&#30340;&#20808;&#21069;&#21457;&#34920;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#22235;&#31181;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65288;&#36923;&#36753;&#22238;&#24402; (LR)&#12289;&#25903;&#25345;&#21521;&#37327;&#26426; (SVM)&#12289;&#38543;&#26426;&#26862;&#26519; (RF) &#21644;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319; (XGB) &#20915;&#31574;&#26641;&#65289;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25968;&#25454;&#24037;&#31243;&#26041;&#27861;&#33021;&#26377;&#25928;&#35299;&#20915;&#24494;&#29983;&#29289;&#32452;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#39640;&#32500;&#24230;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#21307;&#23398;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The human gut microbiota is known to contribute to numerous physiological functions of the body through their interplay with multiple organs and also implicated in a myriad of pathological conditions. Prolific research work in the past few decades have yielded valuable information regarding the relative taxonomic distribution of the gut microbiota that could enable personalized medicine. Unfortunately, the microbiome data suffers from class imbalance and high dimensionality issues that must be addressed. In this study, we have implemented data engineering algorithms to address the above-mentioned issues inherent to microbiome data. Four standard machine learning classifiers (logistic regression (LR), support vector machines (SVM), random forests (RF), and extreme gradient boosting (XGB) decision trees) were implemented on a previously published dataset of infants with cystic fibrosis exhibiting normal vs abnormal growth patterns. The issue of class imbalance and high dimensionality of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#21307;&#30103;&#36164;&#28304;&#65288;&#30123;&#33495;&#65289;&#19981;&#30830;&#23450;&#24615;&#23548;&#21521;&#22320;&#20998;&#37197;&#32473;&#24322;&#36136;&#20154;&#32676;&#20197;&#31649;&#29702;&#27969;&#34892;&#30149;&#20256;&#25773;&#12290;&#30740;&#31350;&#35299;&#20915;&#20102;&#21442;&#25968;&#20272;&#35745;&#21644;&#25972;&#21512;&#12289;&#38750;&#32447;&#24615;ODE&#32422;&#26463;&#21644;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.00032</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#23548;&#21521;&#26368;&#20248;&#36164;&#28304;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Informed Optimal Resource Allocation with Gaussian Process based Bayesian Inference. (arXiv:2307.00032v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#21307;&#30103;&#36164;&#28304;&#65288;&#30123;&#33495;&#65289;&#19981;&#30830;&#23450;&#24615;&#23548;&#21521;&#22320;&#20998;&#37197;&#32473;&#24322;&#36136;&#20154;&#32676;&#20197;&#31649;&#29702;&#27969;&#34892;&#30149;&#20256;&#25773;&#12290;&#30740;&#31350;&#35299;&#20915;&#20102;&#21442;&#25968;&#20272;&#35745;&#21644;&#25972;&#21512;&#12289;&#38750;&#32447;&#24615;ODE&#32422;&#26463;&#21644;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#20110;&#23558;&#21307;&#30103;&#36164;&#28304;&#65288;&#30123;&#33495;&#65289;&#19981;&#30830;&#23450;&#24615;&#23548;&#21521;&#22320;&#20998;&#37197;&#32473;&#24322;&#36136;&#20154;&#32676;&#20197;&#31649;&#29702;&#27969;&#34892;&#30149;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#20004;&#20010;&#30456;&#20851;&#38382;&#39064;&#65306;&#65288;1&#65289;&#23545;&#20110;&#19968;&#20010;&#27969;&#34892;&#30149;&#20256;&#25773;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#27169;&#22411;&#65292;&#25105;&#20204;&#22914;&#20309;&#22312;&#36164;&#28304;&#20998;&#37197;&#20915;&#31574;&#20013;&#20272;&#35745;&#21644;&#25972;&#21512;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#65311;&#65288;2&#65289;&#23545;&#20110;&#19968;&#20010;&#36890;&#29992;&#30340;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65288;&#36164;&#28304;&#20998;&#37197;&#65289;&#22914;&#20309;&#35745;&#31639;&#22788;&#29702;&#38750;&#32447;&#24615;ODE&#32422;&#26463;&#21644;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#65311;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#24403;&#21069;&#30340;&#25991;&#29486;&#26080;&#27861;&#23436;&#20840;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#22312;&#26032;&#30340;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#34920;&#36798;&#24335;&#20013;&#20934;&#30830;&#32780;&#21487;&#34892;&#22320;&#34920;&#31034;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#26469;&#20272;&#35745;ODE&#27169;&#22411;&#21442;&#25968;&#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#29983;&#25104;&#21487;&#34892;&#30340;&#24773;&#26223;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24182;&#34892;&#21270;&#30340;&#35299;&#31639;&#31639;&#27861;&#65292;&#32771;&#34385;&#21040;&#24773;&#26223;&#21644;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on the problem of uncertainty informed allocation of medical resources (vaccines) to heterogeneous populations for managing epidemic spread. We tackle two related questions: (1) For a compartmental ordinary differential equation (ODE) model of epidemic spread, how can we estimate and integrate parameter uncertainty into resource allocation decisions? (2) How can we computationally handle both nonlinear ODE constraints and parameter uncertainties for a generic stochastic optimization problem for resource allocation? To the best of our knowledge current literature does not fully resolve these questions. Here, we develop a data-driven approach to represent parameter uncertainty accurately and tractably in a novel stochastic optimization problem formulation. We first generate a tractable scenario set by estimating the distribution on ODE model parameters using Bayesian inference with Gaussian processes. Next, we develop a parallelized solution algorithm that accounts for scenario-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#29942;&#39048;&#23398;&#20064;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#34920;&#31034;&#29305;&#24449;&#30340;&#35270;&#35273;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#20998;&#31867;ImageNet&#22270;&#20687;&#65292;&#21487;&#20197;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.00028</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#29942;&#39048;&#23398;&#20064;&#20998;&#31867;&#30340;&#8220;&#30475;&#35265;&#25991;&#23383;&#8221;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Seeing in Words: Learning to Classify through Language Bottlenecks. (arXiv:2307.00028v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#29942;&#39048;&#23398;&#20064;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#34920;&#31034;&#29305;&#24449;&#30340;&#35270;&#35273;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#20998;&#31867;ImageNet&#22270;&#20687;&#65292;&#21487;&#20197;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#23427;&#20204;&#25552;&#21462;&#30340;&#29305;&#24449;&#24448;&#24448;&#26159;&#26080;&#27861;&#35299;&#37322;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#21487;&#20197;&#29992;&#31616;&#27905;&#30452;&#35266;&#30340;&#25551;&#36848;&#26469;&#35299;&#37322;&#20182;&#20204;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#23558;&#21487;&#35299;&#37322;&#24615;&#24341;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#23558;&#29305;&#24449;&#34920;&#31034;&#20026;&#25991;&#26412;&#30340;&#35270;&#35273;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;&#23545;ImageNet&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#26102;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks for computer vision extract uninterpretable features despite achieving high accuracy on benchmarks. In contrast, humans can explain their predictions using succinct and intuitive descriptions. To incorporate explainability into neural networks, we train a vision model whose feature representations are text. We show that such a model can effectively classify ImageNet images, and we discuss the challenges we encountered when training it.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#24815;&#24615;&#23548;&#33322;&#39046;&#22495;&#20013;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#19981;&#21516;&#36710;&#36742;&#25805;&#20316;&#39046;&#22495;&#30340;&#30740;&#31350;&#12289;&#28388;&#27874;&#21442;&#25968;&#23398;&#20064;&#30340;&#25913;&#36827;&#20197;&#21450;&#24815;&#24615;&#20256;&#24863;&#22120;&#30340;&#26657;&#20934;&#21644;&#21435;&#22122;&#26041;&#27861;&#12290;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;: &#24815;&#24615;&#23548;&#33322;&#19982;&#28145;&#24230;&#23398;&#20064;&#65306;&#24403;&#21069;&#36235;&#21183;&#19982;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#36848;</title><link>http://arxiv.org/abs/2307.00014</link><description>&lt;p&gt;
&#24815;&#24615;&#23548;&#33322;&#19982;&#28145;&#24230;&#23398;&#20064;&#65306;&#24403;&#21069;&#36235;&#21183;&#19982;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Inertial Navigation Meets Deep Learning: A Survey of Current Trends and Future Directions. (arXiv:2307.00014v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#24815;&#24615;&#23548;&#33322;&#39046;&#22495;&#20013;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#19981;&#21516;&#36710;&#36742;&#25805;&#20316;&#39046;&#22495;&#30340;&#30740;&#31350;&#12289;&#28388;&#27874;&#21442;&#25968;&#23398;&#20064;&#30340;&#25913;&#36827;&#20197;&#21450;&#24815;&#24615;&#20256;&#24863;&#22120;&#30340;&#26657;&#20934;&#21644;&#21435;&#22122;&#26041;&#27861;&#12290;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;: &#24815;&#24615;&#23548;&#33322;&#19982;&#28145;&#24230;&#23398;&#20064;&#65306;&#24403;&#21069;&#36235;&#21183;&#19982;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24815;&#24615;&#20256;&#24863;&#22312;&#35768;&#22810;&#24212;&#29992;&#21644;&#24179;&#21488;&#20013;&#34987;&#20351;&#29992;&#65292;&#20174;&#26234;&#33021;&#25163;&#26426;&#31561;&#26085;&#24120;&#35774;&#22791;&#21040;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#31561;&#22797;&#26434;&#35774;&#22791;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#24815;&#24615;&#20256;&#24863;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#21457;&#23637;&#12290;&#36825;&#26159;&#30001;&#20110;&#39640;&#25928;&#30340;&#35745;&#31639;&#30828;&#20214;&#30340;&#21457;&#23637;&#21644;&#20844;&#24320;&#21487;&#29992;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#21487;&#33719;&#24471;&#24615;&#12290;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#34987;&#29992;&#20110;&#24378;&#21270;&#22522;&#20110;&#27169;&#22411;&#30340;&#23548;&#33322;&#21644;&#20256;&#24863;&#22120;&#34701;&#21512;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#28145;&#20837;&#32508;&#36848;&#12290;&#25105;&#20204;&#20998;&#21035;&#32771;&#23519;&#20102;&#27599;&#20010;&#36710;&#36742;&#25805;&#20316;&#39046;&#22495;&#65292;&#21253;&#25324;&#38470;&#22320;&#12289;&#31354;&#20013;&#21644;&#28023;&#27915;&#12290;&#27599;&#20010;&#39046;&#22495;&#20998;&#20026;&#32431;&#24815;&#24615;&#36827;&#23637;&#21644;&#22522;&#20110;&#28388;&#27874;&#21442;&#25968;&#23398;&#20064;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#29992;&#20110;&#26657;&#20934;&#21644;&#21435;&#22122;&#24815;&#24615;&#20256;&#24863;&#22120;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#25972;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20123;&#36235;&#21183;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#24120;&#29992;&#30340;&#32479;&#35745;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inertial sensing is used in many applications and platforms, ranging from day-to-day devices such as smartphones to very complex ones such as autonomous vehicles. In recent years, the development of machine learning and deep learning techniques has increased significantly in the field of inertial sensing. This is due to the development of efficient computing hardware and the accessibility of publicly available sensor data. These data-driven approaches are used to empower model-based navigation and sensor fusion algorithms. This paper provides an in-depth review of those deep learning methods. We examine separately, each vehicle operation domain including land, air, and sea. Each domain is divided into pure inertial advances and improvements based on filter parameters learning. In addition, we review deep learning approaches for calibrating and denoising inertial sensors. Throughout the paper, we discuss these trends and future directions. We also provide statistics on the commonly used
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#26469;&#39044;&#27979;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;UniXcoder&#20248;&#20110;CodeBERT&#12290;</title><link>http://arxiv.org/abs/2307.00012</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#39044;&#27979;&#26131;&#20986;&#38169;&#27979;&#35797;&#20462;&#22797;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
Black-Box Prediction of Flaky Test Fix Categories Using Language Models. (arXiv:2307.00012v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#26469;&#39044;&#27979;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;UniXcoder&#20248;&#20110;CodeBERT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26131;&#20986;&#38169;&#27979;&#35797;&#20250;&#22312;&#30456;&#21516;&#36719;&#20214;&#29256;&#26412;&#30340;&#27979;&#35797;&#19979;&#38750;&#30830;&#23450;&#24615;&#22320;&#36890;&#36807;&#25110;&#22833;&#36133;&#65292;&#24341;&#36215;&#28151;&#20081;&#24182;&#28010;&#36153;&#24320;&#21457;&#32773;&#26102;&#38388;&#12290;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#34987;&#29992;&#20110;&#39044;&#27979;&#26131;&#20986;&#38169;&#24615;&#21450;&#20854;&#26681;&#26412;&#21407;&#22240;&#65292;&#20294;&#22312;&#25552;&#20379;&#20462;&#22797;&#25903;&#25345;&#26041;&#38754;&#20173;&#26377;&#36739;&#23569;&#24037;&#20316;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;13&#20010;&#20462;&#22797;&#31867;&#21035;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#27169;&#22411;&#26469;&#39044;&#27979;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#34429;&#28982;&#22312;&#24403;&#21069;&#38454;&#27573;&#20934;&#30830;&#39044;&#27979;&#20462;&#22797;&#26412;&#36523;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#20294;&#36825;&#20123;&#31867;&#21035;&#25552;&#20379;&#20102;&#20851;&#20110;&#38656;&#35201;&#26816;&#26597;&#30340;&#27979;&#35797;&#20195;&#30721;&#37096;&#20998;&#30340;&#31934;&#30830;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;CodeBERT&#21644;UniXcoder&#65292;&#20854;&#36755;&#20986;&#32463;&#36807;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#25110;&#22522;&#20110;&#23402;&#29983;&#32593;&#32476;&#30340;Few Shot Learning&#65288;FSL&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniXcoder&#22312;&#27491;&#30830;&#39044;&#27979;&#22823;&#22810;&#25968;&#20462;&#22797;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;CodeBERT&#12290;
&lt;/p&gt;
&lt;p&gt;
Flaky tests are problematic because they non-deterministically pass or fail for the same software version under test, causing confusion and wasting developer time. While machine learning models have been used to predict flakiness and its root causes, there is less work on providing support to fix the problem. To address this gap, we propose a framework that automatically generates labeled datasets for 13 fix categories and train models to predict the fix category of a flaky test by analyzing the test code only. Though it is unrealistic at this stage to accurately predict the fix itself, the categories provide precise guidance about what part of the test code to look at. Our approach is based on language models, namely CodeBERT and UniXcoder, whose output is fine-tuned with a Feed Forward Neural Network (FNN) or a Siamese Network-based Few Shot Learning (FSL). Our experimental results show that UniXcoder outperforms CodeBERT, in correctly predicting most of the categories of fixes a dev
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20998;&#37197;&#21644;&#20998;&#31867;&#36719;&#20214;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#35821;&#35328;&#29305;&#24449;&#21644;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#20998;&#37197;&#32473;&#26368;&#30456;&#20851;&#30340;&#22242;&#38431;&#25104;&#21592;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20197;&#25552;&#39640;&#24037;&#20316;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.00009</link><description>&lt;p&gt;
&#33258;&#21160;&#20998;&#37197;&#21644;&#20998;&#31867;&#36719;&#20214;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Automated Assignment and Classification of Software Issues. (arXiv:2307.00009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20998;&#37197;&#21644;&#20998;&#31867;&#36719;&#20214;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#35821;&#35328;&#29305;&#24449;&#21644;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#20998;&#37197;&#32473;&#26368;&#30456;&#20851;&#30340;&#22242;&#38431;&#25104;&#21592;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20197;&#25552;&#39640;&#24037;&#20316;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#38382;&#39064;&#21253;&#21547;&#20462;&#22797;&#12289;&#25913;&#36827;&#25110;&#21019;&#24314;&#26032;&#32447;&#31243;&#30340;&#24037;&#20316;&#21333;&#20803;&#65292;&#22312;&#24320;&#21457;&#36807;&#31243;&#20013;&#20419;&#36827;&#22242;&#38431;&#25104;&#21592;&#20043;&#38388;&#30340;&#27807;&#36890;&#12290;&#23558;&#38382;&#39064;&#20998;&#37197;&#32473;&#26368;&#30456;&#20851;&#30340;&#22242;&#38431;&#25104;&#21592;&#24182;&#30830;&#23450;&#38382;&#39064;&#30340;&#31867;&#21035;&#26159;&#19968;&#39033;&#32321;&#29712;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#38169;&#35823;&#30340;&#20998;&#31867;&#20250;&#23548;&#33268;&#39033;&#30446;&#24310;&#36831;&#21644;&#37325;&#26032;&#24037;&#20316;&#65292;&#32473;&#22242;&#38431;&#25104;&#21592;&#24102;&#26469;&#40635;&#28902;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#29992;&#20110;&#27973;&#23618;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#35821;&#35328;&#29305;&#24449;&#65292;&#24182;&#23558;&#27973;&#23618;&#26041;&#27861;&#21644;&#38598;&#25104;&#26041;&#27861;&#19982;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#20998;&#37197;&#32473;&#22235;&#31181;&#35282;&#33394;&#65288;&#35774;&#35745;&#24072;&#12289;&#24320;&#21457;&#20154;&#21592;&#12289;&#27979;&#35797;&#20154;&#21592;&#21644;&#39046;&#23548;&#32773;&#65289;&#65292;&#32780;&#19981;&#26159;&#29305;&#23450;&#30340;&#20010;&#20154;&#25110;&#22242;&#38431;&#65292;&#20197;&#20419;&#36827;&#25105;&#20204;&#35299;&#20915;&#26041;&#26696;&#30340;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#24320;&#21457;&#20154;&#21592;&#30340;&#32463;&#39564;&#27700;&#24179;&#65292;&#20197;&#21453;&#26144;&#25105;&#20204;&#35299;&#20915;&#26041;&#26696;&#30340;&#24037;&#19994;&#23454;&#36341;&#12290;&#25105;&#20204;&#37319;&#29992;&#20998;&#31867;&#26041;&#27861;&#23558;&#38382;&#39064;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#21253;&#25324;&#38169;&#35823;&#12289;&#26032;&#21151;&#33021;&#12289;&#25913;&#36827;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software issues contain units of work to fix, improve or create new threads during the development and facilitate communication among the team members. Assigning an issue to the most relevant team member and determining a category of an issue is a tedious and challenging task. Wrong classifications cause delays and rework in the project and trouble among the team members. This thesis proposes a set of carefully curated linguistic features for shallow machine learning methods and compares the performance of shallow and ensemble methods with deep language models. Unlike the state-of-the-art, we assign issues to four roles (designer, developer, tester, and leader) rather than to specific individuals or teams to contribute to the generality of our solution. We also consider the level of experience of the developers to reflect the industrial practices in our solution formulation. We employ a classification approach to categorize issues into distinct classes, namely bug, new feature, improve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24179;&#28369;&#21608;&#26399;&#39640;&#26031;Copula&#27169;&#22411;&#32852;&#21512;&#24314;&#27169;&#20809;&#20239;&#31995;&#32479;&#32676;&#21457;&#30005;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#26172;&#22812;&#21464;&#21270;&#12289;&#31995;&#32479;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#38543;&#26102;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#36890;&#36807;&#35813;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12289;&#32570;&#22833;&#25968;&#25454;&#22635;&#34917;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.00004</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#28369;&#21608;&#26399;&#39640;&#26031;Copula&#27169;&#22411;&#24314;&#27169;&#20809;&#20239;&#30005;&#31449;&#32676;
&lt;/p&gt;
&lt;p&gt;
PV Fleet Modeling via Smooth Periodic Gaussian Copula. (arXiv:2307.00004v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24179;&#28369;&#21608;&#26399;&#39640;&#26031;Copula&#27169;&#22411;&#32852;&#21512;&#24314;&#27169;&#20809;&#20239;&#31995;&#32479;&#32676;&#21457;&#30005;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#26172;&#22812;&#21464;&#21270;&#12289;&#31995;&#32479;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#38543;&#26102;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#36890;&#36807;&#35813;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12289;&#32570;&#22833;&#25968;&#25454;&#22635;&#34917;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#24314;&#27169;&#20809;&#20239;&#65288;PV&#65289;&#31995;&#32479;&#32676;&#30340;&#21457;&#30005;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30333;&#30418;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#25214;&#21040;&#19968;&#20010;&#20989;&#25968;&#65292;&#21487;&#20197;&#23558;&#30690;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26144;&#23556;&#20026;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#26631;&#20934;&#27491;&#24577;&#21464;&#37327;&#12290;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#25311;&#21512;&#24179;&#28369;&#21608;&#26399;Copula&#21464;&#25442;&#21040;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#21040;&#25968;&#25454;&#30340;&#35768;&#22810;&#26041;&#38754;&#65292;&#22914;&#21151;&#29575;&#36755;&#20986;&#20998;&#24067;&#30340;&#26172;&#22812;&#21464;&#21270;&#12289;&#19981;&#21516;PV&#31995;&#32479;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#23427;&#30001;&#21487;&#35299;&#37322;&#30340;&#27493;&#39588;&#32452;&#25104;&#65292;&#24182;&#19988;&#21487;&#25193;&#23637;&#21040;&#35768;&#22810;&#31995;&#32479;&#12290;&#36890;&#36807;&#31995;&#32479;&#21644;&#26102;&#38388;&#30340;&#20809;&#20239;&#30005;&#31449;&#32676;&#32852;&#21512;&#27010;&#29575;&#27169;&#22411;&#21487;&#20197;&#29992;&#26469;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12289;&#22635;&#34917;&#32570;&#22833;&#25968;&#25454;&#12289;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#21644;&#36827;&#34892;&#39044;&#27979;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#35813;&#26041;&#27861;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method for jointly modeling power generation from a fleet of photovoltaic (PV) systems. We propose a white-box method that finds a function that invertibly maps vector time-series data to independent and identically distributed standard normal variables. The proposed method, based on a novel approach for fitting a smooth, periodic copula transform to data, captures many aspects of the data such as diurnal variation in the distribution of power output, dependencies among different PV systems, and dependencies across time. It consists of interpretable steps and is scalable to many systems. The resulting joint probability model of PV fleet output across systems and time can be used to generate synthetic data, impute missing data, perform anomaly detection, and make forecasts. In this paper, we explain the method and demonstrate these applications.
&lt;/p&gt;</description></item><item><title>Sphere2Vec&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#20301;&#32622;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22312;&#29699;&#38754;&#19978;&#32534;&#30721;&#28857;&#22352;&#26631;&#26102;&#20445;&#25345;&#29699;&#38754;&#36317;&#31163;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;GPS&#22352;&#26631;&#25968;&#25454;&#38598;&#20013;&#30340;&#36317;&#31163;&#24230;&#37327;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17624</link><description>&lt;p&gt;
Sphere2Vec&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#30340;&#29699;&#38754;&#19978;&#36890;&#29992;&#20301;&#32622;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sphere2Vec: A General-Purpose Location Representation Learning over a Spherical Surface for Large-Scale Geospatial Predictions. (arXiv:2306.17624v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17624
&lt;/p&gt;
&lt;p&gt;
Sphere2Vec&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#20301;&#32622;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22312;&#29699;&#38754;&#19978;&#32534;&#30721;&#28857;&#22352;&#26631;&#26102;&#20445;&#25345;&#29699;&#38754;&#36317;&#31163;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;GPS&#22352;&#26631;&#25968;&#25454;&#38598;&#20013;&#30340;&#36317;&#31163;&#24230;&#37327;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20026;&#31354;&#38388;&#20013;&#30340;&#28857;&#29983;&#25104;&#36866;&#21512;&#23398;&#20064;&#30340;&#34920;&#31034;&#26159;&#19968;&#20010;&#22522;&#26412;&#19988;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#32534;&#30721;&#26041;&#26696;&#65288;&#22914;Space2Vec&#21644;NeRF&#65289;&#65292;&#21487;&#20197;&#30452;&#25509;&#23558;&#20108;&#32500;/&#19977;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#20219;&#24847;&#28857;&#32534;&#30721;&#20026;&#39640;&#32500;&#21521;&#37327;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#21644;&#29983;&#25104;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#25152;&#26377;&#30340;&#20108;&#32500;&#21644;&#19977;&#32500;&#20301;&#32622;&#32534;&#30721;&#22120;&#37117;&#26159;&#35774;&#35745;&#29992;&#26469;&#27169;&#25311;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#28857;&#36317;&#31163;&#12290;&#22240;&#27492;&#65292;&#22312;&#24212;&#29992;&#20110;&#38656;&#35201;&#22312;&#29699;&#38754;&#19978;&#36827;&#34892;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;GPS&#22352;&#26631;&#25968;&#25454;&#38598;&#26102;&#65292;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#27169;&#22411;&#37117;&#20250;&#20986;&#29616;&#38382;&#39064;&#65292;&#21407;&#22240;&#26159;&#22320;&#22270;&#25237;&#24433;&#22833;&#30495;&#38382;&#39064;&#65288;2D&#65289;&#21644;&#29699;&#38754;&#21040;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#36817;&#20284;&#35823;&#24046;&#65288;3D&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Sphere2Vec&#30340;&#22810;&#23610;&#24230;&#20301;&#32622;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#22312;&#29699;&#38754;&#19978;&#32534;&#30721;&#28857;&#22352;&#26631;&#26102;&#20445;&#25345;&#29699;&#38754;&#36317;&#31163;&#12290;&#25105;&#20204;&#22312;&#29699;&#38754;&#19978;&#30340;&#20301;&#32622;&#32534;&#30721;&#30340;&#36317;&#31163;&#20445;&#25345;&#32534;&#30721;&#30340;&#32479;&#19968;&#35270;&#35282;&#19978;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating learning-friendly representations for points in space is a fundamental and long-standing problem in ML. Recently, multi-scale encoding schemes (such as Space2Vec and NeRF) were proposed to directly encode any point in 2D/3D Euclidean space as a high-dimensional vector, and has been successfully applied to various geospatial prediction and generative tasks. However, all current 2D and 3D location encoders are designed to model point distances in Euclidean space. So when applied to large-scale real-world GPS coordinate datasets, which require distance metric learning on the spherical surface, both types of models can fail due to the map projection distortion problem (2D) and the spherical-to-Euclidean distance approximation error (3D). To solve these problems, we propose a multi-scale location encoder called Sphere2Vec which can preserve spherical distances when encoding point coordinates on a spherical surface. We developed a unified view of distance-reserving encoding on sph
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#29366;&#24577;&#31354;&#38388;&#32553;&#20943;&#21644;&#36755;&#20837;&#20998;&#21106;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;DNN&#20998;&#26512;&#30340;&#27169;&#22411;&#26816;&#39564;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#26816;&#39564;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17323</link><description>&lt;p&gt;
&#36890;&#36807;&#29366;&#24577;&#31354;&#38388;&#32553;&#20943;&#21644;&#36755;&#20837;&#20998;&#21106;&#26469;&#25193;&#23637;DNN&#20998;&#26512;&#30340;&#27169;&#22411;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Scaling Model Checking for DNN Analysis via State-Space Reduction and Input Segmentation (Extended Version). (arXiv:2306.17323v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17323
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#29366;&#24577;&#31354;&#38388;&#32553;&#20943;&#21644;&#36755;&#20837;&#20998;&#21106;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;DNN&#20998;&#26512;&#30340;&#27169;&#22411;&#26816;&#39564;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#26816;&#39564;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#24615;&#33021;&#65292;&#22522;&#20110;NN&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20351;&#29992;&#25345;&#32493;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#30340;&#21508;&#31181;&#26696;&#20363;&#30740;&#31350;&#21644;&#32463;&#39564;&#21457;&#29616;&#34920;&#26126;&#65292;&#24494;&#23567;&#30340;NN&#36755;&#20837;&#21464;&#21270;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#21644;&#19981;&#21487;&#21462;&#30340;NN&#34892;&#20026;&#12290;&#36825;&#24341;&#36215;&#20102;&#23545;&#20854;&#24418;&#24335;&#20998;&#26512;&#30340;&#24191;&#27867;&#20852;&#36259;&#65292;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#32473;&#23450;NN&#34892;&#20026;&#30340;&#20445;&#35777;&#12290;&#29616;&#26377;&#30340;&#26694;&#26550;&#20351;&#29992;&#21487;&#28385;&#36275;&#24615;&#27714;&#35299;&#21644;&#32447;&#24615;&#35268;&#21010;&#20026;&#35757;&#32451;&#30340;NN&#25552;&#20379;&#20102;&#31283;&#20581;&#24615;&#21644;/&#25110;&#23433;&#20840;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FANNet&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#26816;&#39564;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;NN&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#19982;&#27169;&#22411;&#26816;&#39564;&#30456;&#20851;&#30340;&#29366;&#24577;&#31354;&#38388;&#29190;&#28856;&#23548;&#33268;&#20102;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#20351;&#24471;FANNet&#21482;&#36866;&#29992;&#20110;&#23567;&#22411;NN&#12290;&#26412;&#24037;&#20316;&#24320;&#21457;&#20102;&#29366;&#24577;&#31354;&#38388;&#32553;&#20943;&#21644;&#36755;&#20837;&#20998;&#21106;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#21644;&#35745;&#26102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Owing to their remarkable learning capabilities and performance in real-world applications, the use of machine learning systems based on Neural Networks (NNs) has been continuously increasing. However, various case studies and empirical findings in the literature suggest that slight variations to NN inputs can lead to erroneous and undesirable NN behavior. This has led to considerable interest in their formal analysis, aiming to provide guarantees regarding a given NN's behavior. Existing frameworks provide robustness and/or safety guarantees for the trained NNs, using satisfiability solving and linear programming. We proposed FANNet, the first model checking-based framework for analyzing a broader range of NN properties. However, the state-space explosion associated with model checking entails a scalability problem, making the FANNet applicable only to small NNs. This work develops state-space reduction and input segmentation approaches, to improve the scalability and timing efficienc
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#25913;&#36827;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#35757;&#32451;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.16817</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#38598;&#25104;&#25913;&#36827;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Online Continual Learning Performance and Stability with Temporal Ensembles. (arXiv:2306.16817v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16817
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#25913;&#36827;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#35757;&#32451;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31070;&#32463;&#32593;&#32476;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#36845;&#20195;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#23427;&#20204;&#22312;&#38750;&#24179;&#31283;&#30340;&#25968;&#25454;&#27969;&#21644;&#22312;&#32447;&#26041;&#24335;&#19979;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20854;&#24615;&#33021;&#20250;&#19979;&#38477;&#65306;(1)&#22312;&#32447;&#35774;&#32622;&#38480;&#21046;&#20102;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;(2)&#30001;&#20110;&#25968;&#25454;&#30340;&#38750;&#24179;&#31283;&#24615;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#20960;&#31687;&#26368;&#36817;&#30340;&#25991;&#31456;&#34920;&#26126;&#36830;&#32493;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#37325;&#25918;&#26041;&#27861;&#22312;&#27169;&#22411;&#25345;&#32493;&#35780;&#20272;&#26102;&#23384;&#22312;&#31283;&#23450;&#24615;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#22411;&#38598;&#25104;&#20316;&#20026;&#25913;&#36827;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#31616;&#21333;&#22320;&#38598;&#25104;&#26469;&#33258;&#21508;&#31181;&#35757;&#32451;&#20219;&#21153;&#30340;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#24182;&#20174;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#33719;&#21462;&#28789;&#24863;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32508;&#21512;&#21033;&#29992;&#20102;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are very effective when trained on large datasets for a large number of iterations. However, when they are trained on non-stationary streams of data and in an online fashion, their performance is reduced (1) by the online setup, which limits the availability of data, (2) due to catastrophic forgetting because of the non-stationary nature of the data. Furthermore, several recent works (Caccia et al., 2022; Lange et al., 2023) arXiv:2205.1345(2) showed that replay methods used in continual learning suffer from the stability gap, encountered when evaluating the model continually (rather than only on task boundaries). In this article, we study the effect of model ensembling as a way to improve performance and stability in online continual learning. We notice that naively ensembling models coming from a variety of training tasks increases the performance in online continual learning considerably. Starting from this observation, and drawing inspirations from semi-supervised l
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;SPINN&#65289;&#65292;&#36890;&#36807;&#36880;&#20010;&#22788;&#29702;&#36724;&#26469;&#26174;&#33879;&#20943;&#23569;&#20102;&#22810;&#32500; PDE &#20013;&#30340;&#32593;&#32476;&#20256;&#25773;&#25968;&#37327;&#65292;&#24182;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#26222;&#36890; GPU &#19978;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.15969</link><description>&lt;p&gt;
&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Separable Physics-Informed Neural Networks. (arXiv:2306.15969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15969
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;SPINN&#65289;&#65292;&#36890;&#36807;&#36880;&#20010;&#22788;&#29702;&#36724;&#26469;&#26174;&#33879;&#20943;&#23569;&#20102;&#22810;&#32500; PDE &#20013;&#30340;&#32593;&#32476;&#20256;&#25773;&#25968;&#37327;&#65292;&#24182;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#26222;&#36890; GPU &#19978;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#26377;&#24076;&#26395;&#30340;&#22522;&#20110;&#25968;&#25454;&#30340;PDE&#27714;&#35299;&#22120;&#65292;&#22312;&#21508;&#31181;PDE&#19978;&#26174;&#31034;&#20986;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;PINNs&#26469;&#35299;&#20915;&#22810;&#32500;PDE&#21644;&#36924;&#36817;&#39640;&#24230;&#22797;&#26434;&#35299;&#20989;&#25968;&#23384;&#22312;&#26681;&#26412;&#38480;&#21046;&#12290;&#22312;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;PDE&#19978;&#25152;&#38656;&#30340;&#35757;&#32451;&#28857;&#25968;&#37327;(&#37197;&#28857;)&#22823;&#22823;&#22686;&#21152;&#65292;&#20294;&#30001;&#20110;&#26114;&#36149;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#24222;&#22823;&#30340;&#20869;&#23384;&#24320;&#38144;&#65292;&#20854;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;PINNs&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#20998;&#31163;&#30340;PINN (SPINN)&#65292;&#22312;&#22810;&#32500;PDE&#20013;&#25353;&#36724;&#36880;&#20010;&#22788;&#29702;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#32593;&#32476;&#20256;&#25773;&#30340;&#25968;&#37327;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;PINNs&#20013;&#30340;&#36880;&#28857;&#22788;&#29702;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#26469;&#38477;&#20302;&#35745;&#31639;PDE&#27531;&#24046;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20174;&#32780;&#22312;&#21333;&#20010;&#26222;&#36890;GPU&#19978;&#21487;&#20197;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;(&gt;10^7)&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) have recently emerged as promising data-driven PDE solvers showing encouraging results on various PDEs. However, there is a fundamental limitation of training PINNs to solve multi-dimensional PDEs and approximate highly complex solution functions. The number of training points (collocation points) required on these challenging PDEs grows substantially, but it is severely limited due to the expensive computational costs and heavy memory overhead. To overcome this issue, we propose a network architecture and training algorithm for PINNs. The proposed method, separable PINN (SPINN), operates on a per-axis basis to significantly reduce the number of network propagations in multi-dimensional PDEs unlike point-wise processing in conventional PINNs. We also propose using forward-mode automatic differentiation to reduce the computational cost of computing PDE residuals, enabling a large number of collocation points (&gt;10^7) on a single commodity GPU. The
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#24102;&#26377;&#26799;&#24230;&#24341;&#23548;&#37319;&#26679;&#31574;&#30053;&#65288;GraSS&#65289;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#27491;&#26679;&#26412;&#28151;&#28102;&#21644;&#29305;&#24449;&#36866;&#24212;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.15868</link><description>&lt;p&gt;
GraSS:&#24102;&#26377;&#26799;&#24230;&#24341;&#23548;&#37319;&#26679;&#31574;&#30053;&#30340;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
GraSS: Contrastive Learning with Gradient Guided Sampling Strategy for Remote Sensing Image Semantic Segmentation. (arXiv:2306.15868v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15868
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#24102;&#26377;&#26799;&#24230;&#24341;&#23548;&#37319;&#26679;&#31574;&#30053;&#65288;GraSS&#65289;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#27491;&#26679;&#26412;&#28151;&#28102;&#21644;&#29305;&#24449;&#36866;&#24212;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;SSCL&#65289;&#22312;&#36965;&#24863;&#22270;&#20687;&#65288;RSI&#65289;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#30340;&#37324;&#31243;&#30865;&#12290;&#20854;&#26680;&#24515;&#22312;&#20110;&#35774;&#35745;&#19968;&#31181;&#26080;&#30417;&#30563;&#23454;&#20363;&#21306;&#20998;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20174;&#22823;&#37327;&#26080;&#26631;&#31614;&#22270;&#20687;&#20013;&#25552;&#21462;&#26377;&#21033;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#22270;&#20687;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#23454;&#20363;&#21306;&#20998;&#30340;SSCL&#22312;&#24212;&#29992;&#20110;RSI&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#26102;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;1&#65289;&#27491;&#26679;&#26412;&#28151;&#28102;&#38382;&#39064;&#65307;2&#65289;&#29305;&#24449;&#36866;&#24212;&#20559;&#24046;&#12290;&#22312;&#38656;&#35201;&#20687;&#32032;&#32423;&#25110;&#30446;&#26631;&#32423;&#29305;&#24449;&#30340;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;&#23427;&#24341;&#20837;&#20102;&#29305;&#24449;&#36866;&#24212;&#20559;&#24046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#37492;&#21035;&#20449;&#24687;&#21487;&#20197;&#36890;&#36807;&#26080;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#30340;&#26799;&#24230;&#26144;&#23556;&#21040;RSI&#30340;&#29305;&#23450;&#21306;&#22495;&#65292;&#36825;&#20123;&#29305;&#23450;&#21306;&#22495;&#24448;&#24448;&#21253;&#21547;&#29305;&#27530;&#30340;&#22320;&#38754;&#23545;&#35937;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#26799;&#24230;&#24341;&#23548;&#37319;&#26679;&#31574;&#30053;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;GraSS&#65289;&#29992;&#20110;RSI&#35821;&#20041;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised contrastive learning (SSCL) has achieved significant milestones in remote sensing image (RSI) understanding. Its essence lies in designing an unsupervised instance discrimination pretext task to extract image features from a large number of unlabeled images that are beneficial for downstream tasks. However, existing instance discrimination based SSCL suffer from two limitations when applied to the RSI semantic segmentation task: 1) Positive sample confounding issue; 2) Feature adaptation bias. It introduces a feature adaptation bias when applied to semantic segmentation tasks that require pixel-level or object-level features. In this study, We observed that the discrimination information can be mapped to specific regions in RSI through the gradient of unsupervised contrastive loss, these specific regions tend to contain singular ground objects. Based on this, we propose contrastive learning with Gradient guided Sampling Strategy (GraSS) for RSI semantic segmentation. Gr
&lt;/p&gt;</description></item><item><title>SparseOptimizer&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#23427;&#37319;&#29992;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#26080;&#38656;&#23545;&#20195;&#30721;&#36827;&#34892;&#20462;&#25913;&#21363;&#21487;&#36866;&#24212;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#19982;&#23494;&#38598;&#22411;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.15656</link><description>&lt;p&gt;
SparseOptimizer: &#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#26469;&#38477;&#20302;&#35821;&#35328;&#27169;&#22411;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#36890;&#36807;&#32534;&#35793;&#22120;&#20849;&#21516;&#35774;&#35745;&#26469;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design. (arXiv:2306.15656v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15656
&lt;/p&gt;
&lt;p&gt;
SparseOptimizer&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#23427;&#37319;&#29992;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#26080;&#38656;&#23545;&#20195;&#30721;&#36827;&#34892;&#20462;&#25913;&#21363;&#21487;&#36866;&#24212;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#19982;&#23494;&#38598;&#22411;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SparseOptimizer&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65292;ALBERT&#21644;GPT&#65289;&#20013;&#33258;&#28982;&#22320;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;SparseOptimizer&#35774;&#35745;&#30340;&#20851;&#38190;&#26159;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#23427;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#30452;&#25509;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#36825;&#20010;&#25805;&#20316;&#31526;&#36890;&#36807;&#22362;&#23454;&#30340;&#29702;&#35770;&#26694;&#26550;&#25903;&#25345;&#65292;&#24182;&#21253;&#21547;&#20102;&#19968;&#20010;&#20998;&#26512;&#35299;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#20248;&#21270;&#22120;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#26524;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;SparseOptimizer&#30340;&#21363;&#25554;&#21363;&#29992;&#21151;&#33021;&#28040;&#38500;&#20102;&#23545;&#20195;&#30721;&#20462;&#25913;&#30340;&#38656;&#27714;&#65292;&#20351;&#20854;&#25104;&#20026;&#36866;&#29992;&#20110;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#36866;&#24212;&#24037;&#20855;&#12290;&#22312;GLUE&#12289;RACE&#12289;SQuAD1&#21644;SQuAD2&#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#36890;&#36807;SparseOptimizer&#31232;&#30095;&#21270;&#21518;&#30340;SparseBERT&#21644;SparseALBERT&#22312;&#24615;&#33021;&#19978;&#19982;&#23494;&#38598;&#22411;&#30340;BERT&#21644;ALBERT&#30456;&#24403;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces SparseOptimizer, a novel deep learning optimizer that exploits Moreau-Yosida regularization to naturally induce sparsity in large language models such as BERT, ALBERT and GPT. Key to the design of SparseOptimizer is an embedded shrinkage operator, which imparts sparsity directly within the optimization process. This operator, backed by a sound theoretical framework, includes an analytical solution, thereby reinforcing the optimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-play functionality eradicates the need for code modifications, making it a universally adaptable tool for a wide array of large language models. Empirical evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2 confirm that SparseBERT and SparseALBERT, when sparsified using SparseOptimizer, achieve performance comparable to their dense counterparts, BERT and ALBERT, while significantly reducing their parameter count. Further, this work proposes an innovati
&lt;/p&gt;</description></item><item><title>DataCI&#26159;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;&#65292;&#19987;&#20026;&#27969;&#25968;&#25454;&#20013;&#30340;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#32780;&#35774;&#35745;&#65292;&#25552;&#20379;&#20016;&#23500;&#30340;API&#21644;&#29256;&#26412;&#25511;&#21046;&#21151;&#33021;&#65292;&#20855;&#26377;&#26131;&#20110;&#20351;&#29992;&#21644;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#25913;&#21464;&#27969;&#25968;&#25454;&#32972;&#26223;&#19979;&#30340;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2306.15538</link><description>&lt;p&gt;
DataCI: &#19968;&#20010;&#29992;&#20110;&#27969;&#25968;&#25454;&#20013;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
DataCI: A Platform for Data-Centric AI on Streaming Data. (arXiv:2306.15538v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15538
&lt;/p&gt;
&lt;p&gt;
DataCI&#26159;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;&#65292;&#19987;&#20026;&#27969;&#25968;&#25454;&#20013;&#30340;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#32780;&#35774;&#35745;&#65292;&#25552;&#20379;&#20016;&#23500;&#30340;API&#21644;&#29256;&#26412;&#25511;&#21046;&#21151;&#33021;&#65292;&#20855;&#26377;&#26131;&#20110;&#20351;&#29992;&#21644;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#25913;&#21464;&#27969;&#25968;&#25454;&#32972;&#26223;&#19979;&#30340;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;DataCI&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#21160;&#24577;&#27969;&#25968;&#25454;&#22330;&#26223;&#20013;&#30340;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#32508;&#21512;&#24320;&#28304;&#24179;&#21488;&#12290;DataCI&#25552;&#20379;&#20102;&#22522;&#30784;&#35774;&#26045;&#65292;&#20855;&#26377;&#20016;&#23500;&#30340;API&#65292;&#29992;&#20110;&#26080;&#32541;&#27969;&#25968;&#25454;&#38598;&#31649;&#29702;&#12289;&#25968;&#25454;&#20013;&#24515;&#27969;&#31243;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#31934;&#24515;&#35774;&#35745;&#30340;&#29256;&#26412;&#25511;&#21046;&#21151;&#33021;&#65292;&#20197;&#36319;&#36394;&#27969;&#31243;&#30340;&#34893;&#29983;&#12290;&#21478;&#22806;&#65292;DataCI&#36824;&#25552;&#20379;&#20102;&#30452;&#35266;&#30340;&#22270;&#24418;&#30028;&#38754;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#20132;&#20114;&#20307;&#39564;&#12290;&#21021;&#27493;&#30740;&#31350;&#21644;&#28436;&#31034;&#35777;&#26126;&#20102;DataCI&#26131;&#20110;&#20351;&#29992;&#21644;&#26377;&#25928;&#24615;&#65292;&#20984;&#26174;&#20102;&#23427;&#22312;&#27969;&#25968;&#25454;&#32972;&#26223;&#19979;&#25913;&#21464;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#23454;&#36341;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce DataCI, a comprehensive open-source platform designed specifically for data-centric AI in dynamic streaming data settings. DataCI provides 1) an infrastructure with rich APIs for seamless streaming dataset management, data-centric pipeline development and evaluation on streaming scenarios, 2) an carefully designed versioning control function to track the pipeline lineage, and 3) an intuitive graphical interface for a better interactive user experience. Preliminary studies and demonstrations attest to the easy-to-use and effectiveness of DataCI, highlighting its potential to revolutionize the practice of data-centric AI in streaming data contexts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20165;&#29366;&#24577;&#24207;&#21015;&#23398;&#20064;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#23454;&#29616;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#20223;&#12290;&#23398;&#20064;&#30340;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#8220;&#25512;&#29702;&#24335;&#20915;&#31574;&#8221;&#65292;&#24182;&#22312;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15156</link><description>&lt;p&gt;
&#20174;&#20165;&#29366;&#24577;&#24207;&#21015;&#23398;&#20064;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Learning non-Markovian Decision-Making from State-only Sequences. (arXiv:2306.15156v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20165;&#29366;&#24577;&#24207;&#21015;&#23398;&#20064;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#23454;&#29616;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#20223;&#12290;&#23398;&#20064;&#30340;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#8220;&#25512;&#29702;&#24335;&#20915;&#31574;&#8221;&#65292;&#24182;&#22312;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#27169;&#20223;&#23398;&#20064;&#20551;&#35774;&#33021;&#22815;&#33719;&#24471;&#23637;&#31034;&#32773;&#30340;&#21160;&#20316;&#65292;&#20294;&#26159;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#36825;&#20123;&#21160;&#20316;&#36890;&#24120;&#26080;&#27861;&#35266;&#27979;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#30340;&#24207;&#21015;&#20915;&#31574;&#34892;&#20026;&#21487;&#33021;&#20559;&#31163;&#26631;&#20934;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;nMDP&#65289;&#20013;&#20165;&#29366;&#24577;&#24207;&#21015;&#30340;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#65292;&#20854;&#20013;&#31574;&#30053;&#26159;&#28508;&#22312;&#29366;&#24577;&#36716;&#31227;&#29983;&#25104;&#22120;&#30340;&#33021;&#37327;&#20808;&#39564;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#23454;&#29616;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#20223;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#20808;&#39564;&#36827;&#34892;&#30701;&#26399;MCMC&#37319;&#26679;&#21644;&#23545;&#21518;&#39564;&#36827;&#34892;&#37325;&#35201;&#24615;&#37319;&#26679;&#12290;&#23398;&#20064;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#8220;&#25512;&#29702;&#24335;&#20915;&#31574;&#8221;&#65292;&#21363;&#26080;&#27169;&#22411;&#31574;&#30053;&#25191;&#34892;&#31561;&#20215;&#20110;&#20808;&#39564;&#37319;&#26679;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#21017;&#26159;&#20174;&#31574;&#30053;&#21021;&#22987;&#21270;&#30340;&#21518;&#39564;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20855;&#26377;&#38750;&#39532;&#23572;&#31185;&#22827;&#29305;&#24449;&#30340;&#21407;&#22411;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional imitation learning assumes access to the actions of demonstrators, but these motor signals are often non-observable in naturalistic settings. Additionally, sequential decision-making behaviors in these settings can deviate from the assumptions of a standard Markov Decision Process (MDP). To address these challenges, we explore deep generative modeling of state-only sequences with non-Markov Decision Process (nMDP), where the policy is an energy-based prior in the latent space of the state transition generator. We develop maximum likelihood estimation to achieve model-based imitation, which involves short-run MCMC sampling from the prior and importance sampling for the posterior. The learned model enables \textit{decision-making as inference}: model-free policy execution is equivalent to prior sampling, model-based planning is posterior sampling initialized from the policy. We demonstrate the efficacy of the proposed method in a prototypical path planning task with non-Mark
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#31946;&#26465;&#20214;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#38544;&#24335;&#25193;&#25955;&#20808;&#39564;&#24182;&#20197;&#21487;&#25511;&#24378;&#24230;&#24212;&#29992;&#20110;&#22270;&#20687;&#20462;&#22797;&#12290;&#21516;&#26102;&#65292;&#32467;&#21512;&#25193;&#25955;&#24341;&#23548;&#30340;&#27880;&#24847;&#21147;&#22270;&#65292;&#21487;&#20197;&#23545;&#38754;&#37096;&#22270;&#20687;&#36827;&#34892;&#35299;&#37322;&#24615;&#21644;&#33258;&#20027;&#30340;&#20462;&#22797;&#12290;</title><link>http://arxiv.org/abs/2306.14891</link><description>&lt;p&gt;
&#27169;&#31946;&#26465;&#20214;&#25193;&#25955;&#21644;&#25193;&#25955;&#25237;&#24433;&#27880;&#24847;&#21147;&#22312;&#38754;&#37096;&#22270;&#20687;&#20462;&#22797;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fuzzy-Conditioned Diffusion and Diffusion Projection Attention Applied to Facial Image Correction. (arXiv:2306.14891v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14891
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#31946;&#26465;&#20214;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#38544;&#24335;&#25193;&#25955;&#20808;&#39564;&#24182;&#20197;&#21487;&#25511;&#24378;&#24230;&#24212;&#29992;&#20110;&#22270;&#20687;&#20462;&#22797;&#12290;&#21516;&#26102;&#65292;&#32467;&#21512;&#25193;&#25955;&#24341;&#23548;&#30340;&#27880;&#24847;&#21147;&#22270;&#65292;&#21487;&#20197;&#23545;&#38754;&#37096;&#22270;&#20687;&#36827;&#34892;&#35299;&#37322;&#24615;&#21644;&#33258;&#20027;&#30340;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#25193;&#25955;&#26368;&#36817;&#22312;&#22270;&#20687;&#21512;&#25104;&#21644;&#38544;&#24335;&#25104;&#20026;&#22270;&#20687;&#20808;&#39564;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#20808;&#39564;&#24050;&#32463;&#29992;&#20110;&#26465;&#20214;&#21270;&#26469;&#35299;&#20915;&#20462;&#34917;&#38382;&#39064;&#65292;&#20294;&#21482;&#25903;&#25345;&#22522;&#20110;&#20108;&#36827;&#21046;&#29992;&#25143;&#30340;&#26465;&#20214;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#31946;&#26465;&#20214;&#25193;&#25955;&#65292;&#21487;&#20197;&#21033;&#29992;&#21487;&#25511;&#24378;&#24230;&#30340;&#38544;&#24335;&#25193;&#25955;&#20808;&#39564;&#12290;&#25105;&#20204;&#30340;&#27169;&#31946;&#26465;&#20214;&#21487;&#20197;&#36880;&#20687;&#32032;&#24212;&#29992;&#65292;&#20351;&#24471;&#21487;&#20197;&#20197;&#19981;&#21516;&#31243;&#24230;&#20462;&#25913;&#19981;&#21516;&#22270;&#20687;&#32452;&#25104;&#37096;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#38754;&#37096;&#22270;&#20687;&#20462;&#22797;&#30340;&#26041;&#27861;&#65292;&#23558;&#25105;&#20204;&#30340;&#27169;&#31946;&#26465;&#20214;&#25193;&#25955;&#19982;&#25193;&#25955;&#24341;&#23548;&#30340;&#27880;&#24847;&#21147;&#22270;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#27880;&#24847;&#21147;&#22270;&#21487;&#20197;&#20272;&#35745;&#24322;&#24120;&#30340;&#31243;&#24230;&#65292;&#36890;&#36807;&#22312;&#25193;&#25955;&#31354;&#38388;&#19978;&#36827;&#34892;&#25237;&#24433;&#33719;&#24471;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#23548;&#33268;&#20102;&#35299;&#37322;&#24615;&#21644;&#33258;&#20027;&#30340;&#38754;&#37096;&#22270;&#20687;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image diffusion has recently shown remarkable performance in image synthesis and implicitly as an image prior. Such a prior has been used with conditioning to solve the inpainting problem, but only supporting binary user-based conditioning. We derive a fuzzy-conditioned diffusion, where implicit diffusion priors can be exploited with controllable strength. Our fuzzy conditioning can be applied pixel-wise, enabling the modification of different image components to varying degrees. Additionally, we propose an application to facial image correction, where we combine our fuzzy-conditioned diffusion with diffusion-derived attention maps. Our map estimates the degree of anomaly, and we obtain it by projecting on the diffusion space. We show how our approach also leads to interpretable and autonomous facial image correction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;PMaF&#26694;&#26550;&#65292;&#20351;&#29992;&#22768;&#26126;&#24615;&#30340;&#28145;&#24230;&#23618;&#26469;&#23398;&#20064;&#20027;&#35201;&#30697;&#38453;&#29305;&#24449;&#65292;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#35299;&#20915;&#38382;&#39064;&#24182;&#24212;&#29992;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.14759</link><description>&lt;p&gt;
PMaF:&#29992;&#20110;&#20027;&#35201;&#30697;&#38453;&#29305;&#24449;&#30340;&#28145;&#24230;&#22768;&#26126;&#24615;&#23618;
&lt;/p&gt;
&lt;p&gt;
PMaF: Deep Declarative Layers for Principal Matrix Features. (arXiv:2306.14759v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PMaF&#26694;&#26550;&#65292;&#20351;&#29992;&#22768;&#26126;&#24615;&#30340;&#28145;&#24230;&#23618;&#26469;&#23398;&#20064;&#20027;&#35201;&#30697;&#38453;&#29305;&#24449;&#65292;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#35299;&#20915;&#38382;&#39064;&#24182;&#24212;&#29992;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20004;&#31181;&#19981;&#21516;iable&#30340;&#28145;&#24230;&#22768;&#26126;&#24615;&#23618;&#65292;&#21363;&#29699;&#19978;&#30340;&#26368;&#23567;&#20108;&#20056;&#27861;(LESS)&#21644;&#38544;&#24335;&#29305;&#24449;&#20540;&#20998;&#35299;(IED)&#65292;&#29992;&#20110;&#23398;&#20064;&#20027;&#35201;&#30697;&#38453;&#29305;&#24449;(PMaF)&#12290;&#36825;&#21487;&#20197;&#29992;&#19968;&#20010;&#20302;&#32500;&#21521;&#37327;&#34920;&#31034;&#21253;&#21547;&#26469;&#33258;&#39640;&#32500;&#30697;&#38453;&#30340;&#20027;&#35201;&#20449;&#24687;&#30340;&#25968;&#25454;&#29305;&#24449;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#21069;&#21521;&#20256;&#36882;&#30340;&#36845;&#20195;&#20248;&#21270;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#28982;&#21518;&#22312;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#19979;&#21453;&#21521;&#20256;&#25773;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#38544;&#24335;&#26799;&#24230;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#19979;&#38477;&#27493;&#39588;&#21644;&#21453;&#21521;&#32447;&#25628;&#32034;&#26041;&#27861;&#20197;&#21450;&#20999;&#32447;&#31354;&#38388;&#20013;&#30340;&#19979;&#38477;&#34928;&#20943;&#65292;&#20197;&#25552;&#39640;LESS&#30340;&#21069;&#21521;&#36890;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22312;LESS&#21644;IED&#30340;&#21453;&#21521;&#20256;&#36882;&#20013;&#20351;&#29992;&#20102;&#20248;&#21270;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#22312;&#32463;&#39564;&#19978;&#65292;&#36890;&#36807;&#27604;&#36739;&#35299;&#20915;&#26041;&#26696;&#30340;&#26368;&#20248;&#24615;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23618;&#20248;&#20110;&#29616;&#25104;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore two differentiable deep declarative layers, namely least squares on sphere (LESS) and implicit eigen decomposition (IED), for learning the principal matrix features (PMaF). This can be used to represent data features with a low-dimension vector containing dominant information from a high-dimension matrix. We first solve the problems with iterative optimization in the forward pass and then backpropagate the solution for implicit gradients under a bi-level optimization framework. Particularly, adaptive descent steps with the backtracking line search method and descent decay in the tangent space are studied to improve the forward pass efficiency of LESS. Meanwhile, exploited data structures are used to greatly reduce the computational complexity in the backward pass of LESS and IED. Empirically, we demonstrate the superiority of our layers over the off-the-shelf baselines by comparing the solution optimality and computational requirements.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#29983;&#23398;&#20064;&#30340;&#24322;&#26041;&#24046;&#22238;&#24402;&#30340;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#32479;&#35745;&#23398;&#12289;&#35745;&#37327;&#32463;&#27982;&#23398;&#12289;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#31561;&#39046;&#22495;&#65292;&#20197;&#21450;&#22312;&#19981;&#21516;&#26469;&#28304;&#25968;&#25454;&#36136;&#37327;&#19981;&#19968;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.14288</link><description>&lt;p&gt;
&#22522;&#20110;&#20849;&#29983;&#23398;&#20064;&#30340;&#24322;&#26041;&#24046;&#22238;&#24402;&#30340;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Near Optimal Heteroscedastic Regression with Symbiotic Learning. (arXiv:2306.14288v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#29983;&#23398;&#20064;&#30340;&#24322;&#26041;&#24046;&#22238;&#24402;&#30340;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#32479;&#35745;&#23398;&#12289;&#35745;&#37327;&#32463;&#27982;&#23398;&#12289;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#31561;&#39046;&#22495;&#65292;&#20197;&#21450;&#22312;&#19981;&#21516;&#26469;&#28304;&#25968;&#25454;&#36136;&#37327;&#19981;&#19968;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#32463;&#20856;&#30340;&#24322;&#26041;&#24046;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#23637;&#24320;&#35752;&#35770;&#12290;&#20551;&#35774;&#25105;&#20204;&#26377;n&#20010;&#26679;&#26412; $(\mathbf{x}_i, y_i) \in \mathbb{R}^d \times \mathbb{R}$&#65292;&#20854;&#20013; $y_i = \langle \mathbf{w}^{*}, \mathbf{x}_i \rangle + \epsilon_i \cdot \langle \mathbf{f}^{*}, \mathbf{x}_i \rangle$&#65292; $\mathbf{x}_i \sim N(0,\mathbf{I})$&#65292;$\epsilon_i \sim N(0,1)$&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20272;&#35745; $\mathbf{w}^{*}$&#12290;&#22312;&#32479;&#35745;&#23398;&#12289;&#35745;&#37327;&#32463;&#27982;&#23398;&#12289;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#31561;&#39046;&#22495;&#65292;&#24322;&#26041;&#24046;&#27169;&#22411;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21516;&#26102;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#22914;&#26524;&#25968;&#25454;&#26469;&#28304;&#19981;&#21516;&#65292;&#32780;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#36136;&#37327;&#20063;&#19981;&#19968;&#65292;&#21017;&#24322;&#26041;&#24046;&#27169;&#22411;&#20063;&#26174;&#24471;&#29305;&#21035;&#30456;&#20851;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#20272;&#35745;&#20986;$\mathbf{w}^{*}$&#30340;&#24179;&#26041;&#33539;&#25968;&#65292;&#35823;&#24046;&#20026;$\tilde{O}\left(\|\mathbf{f}^{*}\|^2 \cdot \left(\frac{1}{n} + \left(\frac{d}{n}\right)^2\right)\right)$&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20010;&#21305;&#37197;&#30340;&#19979;&#38480;&#65288;&#19978;&#30028;&#23384;&#22312;&#23545;&#25968;&#22240;&#23376;&#65289;&#12290;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#26174;&#33879;&#25913;&#36827;&#20102;&#24322;&#26041;&#24046;&#22238;&#24402;&#38382;&#39064;&#30340;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the classical problem of heteroscedastic linear regression, where we are given $n$ samples $(\mathbf{x}_i, y_i) \in \mathbb{R}^d \times \mathbb{R}$ obtained from $y_i = \langle \mathbf{w}^{*}, \mathbf{x}_i \rangle + \epsilon_i \cdot \langle \mathbf{f}^{*}, \mathbf{x}_i \rangle$, where $\mathbf{x}_i \sim N(0,\mathbf{I})$, $\epsilon_i \sim N(0,1)$, and our task is to estimate $\mathbf{w}^{*}$. In addition to the classical applications of heteroscedastic models in fields such as statistics, econometrics, time series analysis etc., it is also particularly relevant in machine learning when data is collected from multiple sources of varying but apriori unknown quality, e.g., large model training. Our work shows that we can estimate $\mathbf{w}^{*}$ in squared norm up to an error of $\tilde{O}\left(\|\mathbf{f}^{*}\|^2 \cdot \left(\frac{1}{n} + \left(\frac{d}{n}\right)^2\right)\right)$ and prove a matching lower bound (up to logarithmic factors). Our result substantially improves 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#32852;&#37030;&#23398;&#20064;&#38450;&#24481;&#26041;&#27861;&#65292;&#20351;&#29992;&#20803;Stackelberg&#23398;&#20064;&#31639;&#27861;&#35299;&#20915;&#36125;&#21494;&#26031;Stackelberg&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#38450;&#24481;&#65292;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#21305;&#37197;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2306.13800</link><description>&lt;p&gt;
&#19968;&#31181;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#38454;Meta Stackelberg&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A First Order Meta Stackelberg Method for Robust Federated Learning. (arXiv:2306.13800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#32852;&#37030;&#23398;&#20064;&#38450;&#24481;&#26041;&#27861;&#65292;&#20351;&#29992;&#20803;Stackelberg&#23398;&#20064;&#31639;&#27861;&#35299;&#20915;&#36125;&#21494;&#26031;Stackelberg&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#38450;&#24481;&#65292;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#21305;&#37197;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#38754;&#20020;&#30528;&#21508;&#31181;&#23433;&#20840;&#39118;&#38505;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#22810;&#31181;&#38450;&#24481;&#31574;&#30053;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#26159;&#38750;&#33258;&#36866;&#24212;&#30340;&#65292;&#21482;&#38024;&#23545;&#26576;&#20123;&#31867;&#22411;&#30340;&#25915;&#20987;&#65292;&#20174;&#32780;&#26080;&#27861;&#25269;&#24481;&#19981;&#21487;&#39044;&#27979;&#25110;&#33258;&#36866;&#24212;&#30340;&#23041;&#32961;&#12290;&#26412;&#30740;&#31350;&#23558;&#23545;&#25239;&#24615;&#32852;&#37030;&#23398;&#20064;&#24314;&#27169;&#20026;&#36125;&#21494;&#26031;Stackelberg&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;(BSMG)&#20197;&#25429;&#25417;&#38450;&#24481;&#32773;&#23545;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20803;Stackelberg&#23398;&#20064;(meta-SL)&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#35777;&#26126;&#26377;&#25928;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;BSMG&#20013;&#30340;&#22343;&#34913;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;FL&#38450;&#24481;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;meta-SL&#22312;$O(\varepsilon^{-2})$&#26799;&#24230;&#36845;&#20195;&#20013;&#25910;&#25947;&#20110;&#19968;&#38454;$\varepsilon$-&#22343;&#34913;&#28857;&#65292;&#27599;&#27425;&#36845;&#20195;&#38656;&#35201;$O(\varepsilon^{-4})$&#20010;&#26679;&#26412;&#65292;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#21305;&#37197;&#12290;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20803;Stackelberg&#26694;&#26550;&#22312;&#24378;&#22823;&#30340;&#27169;&#22411;&#27745;&#26579;&#21644;&#21518;&#38376;&#25915;&#20987;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous research has shown that federated learning (FL) systems are exposed to an array of security risks. Despite the proposal of several defensive strategies, they tend to be non-adaptive and specific to certain types of attacks, rendering them ineffective against unpredictable or adaptive threats. This work models adversarial federated learning as a Bayesian Stackelberg Markov game (BSMG) to capture the defender's incomplete information of various attack types. We propose meta-Stackelberg learning (meta-SL), a provably efficient meta-learning algorithm, to solve the equilibrium strategy in BSMG, leading to an adaptable FL defense. We demonstrate that meta-SL converges to the first-order $\varepsilon$-equilibrium point in $O(\varepsilon^{-2})$ gradient iterations, with $O(\varepsilon^{-4})$ samples needed per iteration, matching the state of the art. Empirical evidence indicates that our meta-Stackelberg framework performs exceptionally well against potent model poisoning and backdo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#20010;&#25361;&#25112;&#65306;&#36807;&#24230;&#24179;&#28369;&#21644;&#24322;&#36136;&#24615;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.12640</link><description>&lt;p&gt;
&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Addressing the Limitations of Graph Neural Networks. (arXiv:2306.12640v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#20010;&#25361;&#25112;&#65306;&#36807;&#24230;&#24179;&#28369;&#21644;&#24322;&#36136;&#24615;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#20851;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#30340;&#20004;&#20010;&#38382;&#39064;&#65306;&#36807;&#24230;&#24179;&#28369;&#21644;&#24322;&#36136;&#24615;&#25361;&#25112;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#35201;&#25506;&#32034;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report gives a summary of two problems about graph convolutional networks (GCNs): over-smoothing and heterophily challenges, and outlines future directions to explore.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#31574;&#30053;&#25628;&#32034;&#31639;&#27861;&#65292;State-wise Constrained Policy Optimization (SCPO)&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#29366;&#24577;&#38480;&#21046;&#32422;&#26463;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#26399;&#26395;&#29366;&#24577;&#32422;&#26463;&#20445;&#35777;&#21644;&#26368;&#22351;&#23433;&#20840;&#36829;&#21453;&#30340;&#26377;&#30028;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12594</link><description>&lt;p&gt;
&#32422;&#26463;&#19979;&#30340;&#31574;&#30053;&#20248;&#21270;&#65306;State-wise Constrained Policy Optimization
&lt;/p&gt;
&lt;p&gt;
State-wise Constrained Policy Optimization. (arXiv:2306.12594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#31574;&#30053;&#25628;&#32034;&#31639;&#27861;&#65292;State-wise Constrained Policy Optimization (SCPO)&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#29366;&#24577;&#38480;&#21046;&#32422;&#26463;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#26399;&#26395;&#29366;&#24577;&#32422;&#26463;&#20445;&#35777;&#21644;&#26368;&#22351;&#23433;&#20840;&#36829;&#21453;&#30340;&#26377;&#30028;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#26159;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#24212;&#29992;&#20173;&#28982;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#20854;&#20013;&#23433;&#20840;&#24615;&#26159;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#35768;&#22810;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#24378;&#21046;&#25191;&#34892;&#29366;&#24577;&#38480;&#21046;&#26159;&#21313;&#20998;&#24517;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#26694;&#26550;&#19979;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#27809;&#26377;&#32771;&#34385;&#29366;&#24577;&#32422;&#26463;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;State-wise Constrained Policy Optimization&#65288;SCPO&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#22788;&#29702;&#29366;&#24577;&#38480;&#21046;&#30340;&#36890;&#29992;&#31574;&#30053;&#25628;&#32034;&#31639;&#27861;&#12290;SCPO&#33021;&#22815;&#22312;&#26399;&#26395;&#19978;&#20445;&#35777;&#29366;&#24577;&#32422;&#26463;&#30340;&#28385;&#36275;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26368;&#22823;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;SCPO&#19979;&#26368;&#22351;&#30340;&#23433;&#20840;&#36829;&#21453;&#26159;&#26377;&#30028;&#30340;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31574;&#30053;&#26102;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) algorithms have shown tremendous success in simulation environments, but their application to real-world problems faces significant challenges, with safety being a major concern. In particular, enforcing state-wise constraints is essential for many challenging tasks such as autonomous driving and robot manipulation. However, existing safe RL algorithms under the framework of Constrained Markov Decision Process (CMDP) do not consider state-wise constraints. To address this gap, we propose State-wise Constrained Policy Optimization (SCPO), the first general-purpose policy search algorithm for state-wise constrained reinforcement learning. SCPO provides guarantees for state-wise constraint satisfaction in expectation. In particular, we introduce the framework of Maximum Markov Decision Process, and prove that the worst-case safety violation is bounded under SCPO. We demonstrate the effectiveness of our approach on training neural network policies for extensive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#31995;&#32479;&#22238;&#39038;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20998;&#21035;&#35752;&#35770;&#20102;&#19981;&#21516;&#20219;&#21153;&#24182;&#25353;&#19981;&#21516;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#32452;&#32455;&#12290;&#35813;&#39046;&#22495;&#30340;&#21069;&#26223;&#30475;&#22909;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.11768</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Systematic Survey in Geometric Deep Learning for Structure-based Drug Design. (arXiv:2306.11768v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#31995;&#32479;&#22238;&#39038;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20998;&#21035;&#35752;&#35770;&#20102;&#19981;&#21516;&#20219;&#21153;&#24182;&#25353;&#19981;&#21516;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#32452;&#32455;&#12290;&#35813;&#39046;&#22495;&#30340;&#21069;&#26223;&#30475;&#22909;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#21033;&#29992;&#34507;&#30333;&#36136;&#30340;&#19977;&#32500;&#20960;&#20309;&#32467;&#26500;&#26469;&#35782;&#21035;&#28508;&#22312;&#30340;&#33647;&#29289;&#20505;&#36873;&#29289;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#29289;&#29702;&#21270;&#23398;&#24314;&#27169;&#21644;&#19987;&#23478;&#39046;&#22495;&#30693;&#35782;&#30340;&#20256;&#32479;&#26041;&#27861;&#36153;&#26102;&#36153;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#21487;&#20197;&#22788;&#29702;&#21644;&#25972;&#21512;&#19977;&#32500;&#20960;&#20309;&#25968;&#25454;&#65292;&#21152;&#19978;&#31867;&#20284;AlphaFold&#30340;&#24037;&#20855;&#25552;&#20379;&#20934;&#30830;&#30340;&#34507;&#30333;&#36136;&#19977;&#32500;&#32467;&#26500;&#39044;&#27979;&#65292;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#30340;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#20174;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#20027;&#27969;&#20219;&#21153;&#12289;&#24120;&#29992;&#30340;3D&#34507;&#30333;&#36136;&#34920;&#31034;&#21644;&#39044;&#27979;/&#29983;&#25104;&#27169;&#22411;&#20837;&#25163;&#65292;&#28982;&#21518;&#35814;&#32454;&#20171;&#32461;&#27599;&#20010;&#20219;&#21153;&#30340;&#22238;&#39038;&#65288;&#20363;&#22914;&#32467;&#21512;&#20301;&#28857;&#39044;&#27979;&#12289;&#32467;&#21512;&#26500;&#35937;&#29983;&#25104;&#12289;\emph{de novo} &#20998;&#23376;&#35774;&#35745;&#31561;&#65289;&#65292;&#24182;&#25353;&#19981;&#21516;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#32452;&#32455;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#35813;&#39046;&#22495;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure-based drug design (SBDD), which utilizes the three-dimensional geometry of proteins to identify potential drug candidates, is becoming increasingly vital in drug discovery. However, traditional methods based on physiochemical modeling and experts' domain knowledge are time-consuming and laborious. The recent advancements in geometric deep learning, which integrates and processes 3D geometric data, coupled with the availability of accurate protein 3D structure predictions from tools like AlphaFold, have significantly propelled progress in structure-based drug design. In this paper, we systematically review the recent progress of geometric deep learning for structure-based drug design. We start with a brief discussion of the mainstream tasks in structure-based drug design, commonly used 3D protein representations and representative predictive/generative models. Then we delve into detailed reviews for each task (binding site prediction, binding pose generation, \emph{de novo} mo
&lt;/p&gt;</description></item><item><title>G-NM&#26159;&#19968;&#32452;&#38598;&#21512;&#20102;&#20256;&#32479;&#21644;&#29616;&#20195;&#27169;&#22411;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#30340;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.11667</link><description>&lt;p&gt;
G-NM&#65306;&#19968;&#32452;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
G-NM: A Group of Numerical Time Series Prediction Models. (arXiv:2306.11667v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11667
&lt;/p&gt;
&lt;p&gt;
G-NM&#26159;&#19968;&#32452;&#38598;&#21512;&#20102;&#20256;&#32479;&#21644;&#29616;&#20195;&#27169;&#22411;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#30340;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#24320;&#21457;&#21644;&#23454;&#26045;&#19968;&#20010;&#32508;&#21512;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#38598;&#21512;&#65292;&#32479;&#31216;&#20026;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#32452;&#65288;G-NM&#65289;&#12290;&#35813;&#38598;&#21512;&#21253;&#25324;&#20256;&#32479;&#27169;&#22411;&#22914;&#33258;&#22238;&#24402;&#32508;&#21512;&#31227;&#21160;&#24179;&#22343;&#65288;ARIMA&#65289;&#12289;Holt-Winters&#26041;&#27861;&#21644;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;SVR&#65289;&#65292;&#20197;&#21450;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#12290;G-NM&#26126;&#30830;&#26500;&#24314;&#20197;&#22686;&#24378;&#25105;&#20204;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#22266;&#26377;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#19982;&#36825;&#20123;&#20107;&#20214;&#30456;&#20851;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;G-NM&#20415;&#20110;&#23545;&#27492;&#31867;&#29616;&#35937;&#22312;&#24310;&#38271;&#26102;&#38388;&#27573;&#20869;&#36827;&#34892;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25512;&#36827;&#25105;&#20204;&#23545;&#27492;&#31867;&#20107;&#20214;&#30340;&#29702;&#35299;&#65292;&#24182;&#22823;&#24133;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;G-NM&#21253;&#25324;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#21450;&#23395;&#33410;&#24615;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we focus on the development and implementation of a comprehensive ensemble of numerical time series forecasting models, collectively referred to as the Group of Numerical Time Series Prediction Model (G-NM). This inclusive set comprises traditional models such as Autoregressive Integrated Moving Average (ARIMA), Holt-Winters' method, and Support Vector Regression (SVR), in addition to modern neural network models including Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM). G-NM is explicitly constructed to augment our predictive capabilities related to patterns and trends inherent in complex natural phenomena. By utilizing time series data relevant to these events, G-NM facilitates the prediction of such phenomena over extended periods. The primary objective of this research is to both advance our understanding of such occurrences and to significantly enhance the accuracy of our forecasts. G-NM encapsulates both linear and non-linear dependencies, seasonal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#35821;&#20041;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#26681;&#25454;&#26053;&#23458;&#30340;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#65292;&#23454;&#39564;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.10946</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Tourist Attractions Recommendation based on Attention Knowledge Graph Convolution Network. (arXiv:2306.10946v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#35821;&#20041;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#26681;&#25454;&#26053;&#23458;&#30340;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#65292;&#23454;&#39564;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25512;&#33616;&#31639;&#27861;&#22312;&#30456;&#23545;&#25104;&#29087;&#38454;&#27573;&#65292;&#20294;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#25512;&#33616;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;&#20363;&#22914;&#22312;&#26053;&#28216;&#39046;&#22495;&#65292;&#36873;&#25321;&#36866;&#21512;&#30340;&#26053;&#28216;&#26223;&#28857;&#23646;&#24615;&#27969;&#31243;&#20316;&#20026;&#25512;&#33616;&#22522;&#30784;&#36739;&#20026;&#22797;&#26434;&#12290;&#26412;&#25991;&#25552;&#20986;&#25913;&#36827;&#30340;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#27169;&#22411;(Att-KGCN)&#65292;&#33258;&#21160;&#35821;&#20041;&#22320;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#21033;&#29992;&#27880;&#24847;&#21147;&#23618;&#23558;&#30456;&#23545;&#30456;&#20284;&#30340;&#20301;&#32622;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#36890;&#36807;&#25512;&#29702;&#26053;&#23458;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#12290;&#23454;&#39564;&#20013;&#65292;&#37319;&#29992;&#32034;&#31185;&#29305;&#25289;&#23707;-&#20063;&#38376;&#30340;&#26053;&#28216;&#25968;&#25454;&#65292;&#35777;&#26126;&#20102;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#26053;&#28216;&#39046;&#22495;&#30340;&#26223;&#28857;&#25512;&#33616;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation algorithm based on knowledge graphs is at a relatively mature stage. However, there are still some problems in the recommendation of specific areas. For example, in the tourism field, selecting suitable tourist attraction attributes process is complicated as the recommendation basis for tourist attractions. In this paper, we propose the improved Attention Knowledge Graph Convolution Network model, named (Att-KGCN), which automatically discovers the neighboring entities of the target scenic spot semantically. The attention layer aggregates relatively similar locations and represents them with an adjacent vector. Then, according to the tourist's preferred choices, the model predicts the probability of similar spots as a recommendation system. A knowledge graph dataset of tourist attractions used based on tourism data on Socotra Island-Yemen. Through experiments, it is verified that the Attention Knowledge Graph Convolution Network has a good effect on the recommendatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31454;&#20105;&#22810;&#26234;&#33021;&#20307;&#25628;&#32034;&#30340;&#28436;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36827;&#21270;&#35745;&#31639;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#22312;&#19981;&#21516;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#25628;&#32034;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.10640</link><description>&lt;p&gt;
&#22522;&#20110;&#31454;&#20105;&#22810;&#26234;&#33021;&#20307;&#25628;&#32034;&#30340;&#28436;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Evolving Strategies for Competitive Multi-Agent Search. (arXiv:2306.10640v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31454;&#20105;&#22810;&#26234;&#33021;&#20307;&#25628;&#32034;&#30340;&#28436;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36827;&#21270;&#35745;&#31639;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#22312;&#19981;&#21516;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#25628;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36827;&#21270;&#35745;&#31639;&#36866;&#29992;&#20110;&#24037;&#31243;&#39046;&#22495;&#30340;&#33258;&#21160;&#21457;&#29616;&#65292;&#20294;&#23427;&#20063;&#21487;&#29992;&#20110;&#25581;&#31034;&#20154;&#31867;&#21644;&#32452;&#32455;&#22914;&#20309;&#26356;&#26377;&#25928;&#22320;&#25191;&#34892;&#20219;&#21153;&#12290;&#36890;&#36807;&#20197;&#32452;&#32455;&#20013;&#30340;&#21019;&#26032;&#25628;&#32034;&#38382;&#39064;&#20026;&#20363;&#65292;&#26412;&#25991;&#39318;&#20808;&#23558;&#20154;&#31867;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#24418;&#24335;&#21270;&#20026;&#31454;&#20105;&#22810;&#26234;&#33021;&#20307;&#25628;&#32034;&#65288;CMAS&#65289;&#12290;CMAS&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;&#21333;&#26234;&#33021;&#20307;&#21644;&#22242;&#38431;&#25628;&#32034;&#38382;&#39064;&#65292;&#22240;&#20026;&#26234;&#33021;&#20307;&#36890;&#36807;&#20102;&#35299;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#25628;&#32034;&#24773;&#20917;&#20197;&#21450;&#36825;&#20123;&#25628;&#32034;&#23548;&#33268;&#30340;&#25628;&#32034;&#26223;&#35266;&#30340;&#21160;&#24577;&#21464;&#21270;&#26469;&#36827;&#34892;&#20132;&#20114;&#12290;&#20027;&#35201;&#20551;&#35774;&#26159;&#65292;&#36827;&#21270;&#35745;&#31639;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;CMAS&#30340;&#26377;&#25928;&#31574;&#30053;&#65307;&#35813;&#20551;&#35774;&#22312;&#19968;&#31995;&#21015;&#20851;&#20110;NK&#27169;&#22411;&#30340;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#65292;&#21363;&#37096;&#20998;&#30456;&#20851;&#19988;&#21487;&#35843;&#25972;&#23822;&#23702;&#30340;&#36866;&#24212;&#24230;&#26223;&#35266;&#12290;&#19981;&#21516;&#30340;&#19987;&#38376;&#31574;&#30053;&#38024;&#23545;&#27599;&#20010;&#19981;&#21516;&#30340;&#31454;&#20105;&#29615;&#22659;&#36827;&#34892;&#20102;&#28436;&#21270;&#65292;&#21516;&#26102;&#36824;&#28436;&#21270;&#20986;&#20102;&#34920;&#29616;&#33391;&#22909;&#30340;&#36890;&#29992;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
While evolutionary computation is well suited for automatic discovery in engineering, it can also be used to gain insight into how humans and organizations could perform more effectively. Using a real-world problem of innovation search in organizations as the motivating example, this article first formalizes human creative problem solving as competitive multi-agent search (CMAS). CMAS is different from existing single-agent and team search problems in that the agents interact through knowledge of other agents' searches and through the dynamic changes in the search landscape that result from these searches. The main hypothesis is that evolutionary computation can be used to discover effective strategies for CMAS; this hypothesis is verified in a series of experiments on the NK model, i.e.\ partially correlated and tunably rugged fitness landscapes. Different specialized strategies are evolved for each different competitive environment, and also general strategies that perform well acros
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;&#8221;&#30340;&#26032;&#38382;&#39064;&#65292;&#26088;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#29305;&#24449;&#25552;&#21462;&#21644;&#20219;&#21153;&#36866;&#24212;&#27169;&#22359;&#20197;&#21450;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#23569;&#26679;&#26412;&#23458;&#25143;&#31471;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.10234</link><description>&lt;p&gt;
&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Few-shot Learning. (arXiv:2306.10234v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;&#8221;&#30340;&#26032;&#38382;&#39064;&#65292;&#26088;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#29305;&#24449;&#25552;&#21462;&#21644;&#20219;&#21153;&#36866;&#24212;&#27169;&#22359;&#20197;&#21450;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#23569;&#26679;&#26412;&#23458;&#25143;&#31471;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#22810;&#20010;&#23458;&#25143;&#31471;&#21487;&#20197;&#22312;&#19981;&#20132;&#25442;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#20316;&#23398;&#20064;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#27599;&#20010;&#23458;&#25143;&#31471;&#37117;&#26377;&#36275;&#22815;&#30340;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#65292;&#32780;&#20107;&#23454;&#19978;&#26576;&#20123;&#23458;&#25143;&#31471;&#21487;&#33021;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#26679;&#26412;&#65292;&#21363;&#23569;&#26679;&#26412;&#25968;&#25454;&#12290;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#21487;&#33021;&#22312;&#36825;&#20123;&#23458;&#25143;&#31471;&#19978;&#36935;&#21040;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22788;&#29702;&#36825;&#19968;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#21644;&#20219;&#21153;&#36866;&#24212;&#27169;&#22359;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#23569;&#26679;&#26412;&#23458;&#25143;&#31471;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#21160;&#24577;&#22320;&#35843;&#25972;&#27599;&#20010;&#23458;&#25143;&#31471;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) enables multiple clients to collaboratively learn a machine learning model without exchanging their own local data. In this way, the server can exploit the computational power of all clients and train the model on a larger set of data samples among all clients. Although such a mechanism is proven to be effective in various fields, existing works generally assume that each client preserves sufficient data for training. In practice, however, certain clients may only contain a limited number of samples (i.e., few-shot samples). For example, the available photo data taken by a specific user with a new mobile device is relatively rare. In this scenario, existing FL efforts typically encounter a significant performance drop on these clients. Therefore, it is urgent to develop a few-shot model that can generalize to clients with limited data under the FL scenario. In this paper, we refer to this novel problem as federated few-shot learning. Nevertheless, the problem re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#31639;&#27861;&#65292;&#21487;&#20197;&#24212;&#23545;&#37325;&#23614;&#20998;&#24067;&#19988;&#20445;&#35777;&#26377;&#38480;&#30340;&#20551;&#38451;&#24615;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.09548</link><description>&lt;p&gt;
&#22312;&#32447;&#37325;&#23614;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Online Heavy-tailed Change-point detection. (arXiv:2306.09548v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#31639;&#27861;&#65292;&#21487;&#20197;&#24212;&#23545;&#37325;&#23614;&#20998;&#24067;&#19988;&#20445;&#35777;&#26377;&#38480;&#30340;&#20551;&#38451;&#24615;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979; (OCPD) &#30340;&#31639;&#27861;&#65292;&#20854;&#20013;&#26679;&#26412;&#21487;&#33021;&#26159;&#37325;&#23614;&#20998;&#24067;&#65292;&#19968;&#20010;&#25509;&#19968;&#20010;&#22320;&#21576;&#29616;&#65292;&#24182;&#19988;&#24517;&#39035;&#23613;&#26089;&#26816;&#27979;&#21040;&#24213;&#23618;&#22343;&#20540;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35009;&#21098;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477; (SGD) &#30340;&#31639;&#27861;&#65292;&#21363;&#20351;&#25105;&#20204;&#20165;&#20551;&#23450;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#31532;&#20108;&#38454;&#30697;&#26377;&#30028;&#65292;&#35813;&#31639;&#27861;&#20063;&#33021;&#27491;&#24120;&#24037;&#20316;&#12290;&#25105;&#20204;&#27966;&#29983;&#20102;&#22312;&#25152;&#26377;&#20855;&#26377;&#26377;&#30028;&#31532;&#20108;&#30697;&#30340;&#20998;&#24067;&#26063;&#20013;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26377;&#38480;&#26679;&#26412;&#20551;&#38451;&#24615;&#29575; (FPR) &#30340;&#20445;&#35777;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#20445;&#35777;&#26377;&#38480;&#26679;&#26412; FPR &#30340; OCPD &#31639;&#27861;&#65292;&#21363;&#20351;&#25968;&#25454;&#26159;&#39640;&#32500;&#30340;&#65292;&#24213;&#23618;&#20998;&#24067;&#26159;&#37325;&#23614;&#30340;&#12290;&#25105;&#20204;&#35770;&#25991;&#30340;&#25216;&#26415;&#36129;&#29486;&#26159;&#23637;&#31034;&#20102;&#35009;&#21098; SGD &#21487;&#20197;&#20272;&#35745;&#38543;&#26426;&#21521;&#37327;&#30340;&#22343;&#20540;&#24182;&#21516;&#26102;&#22312;&#25152;&#26377;&#32622;&#20449;&#24230;&#20540;&#19978;&#25552;&#20379;&#32622;&#20449;&#24230;&#30028;&#38480;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#31283;&#20581;&#30340;&#20272;&#35745;&#19982;&#24182;&#38598;&#36793;&#30028;&#35770;&#35777;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#19968;&#20010;&#26377;&#38480;&#30340;&#39034;&#24207;&#21464;&#28857;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study algorithms for online change-point detection (OCPD), where samples that are potentially heavy-tailed, are presented one at a time and a change in the underlying mean must be detected as early as possible. We present an algorithm based on clipped Stochastic Gradient Descent (SGD), that works even if we only assume that the second moment of the data generating process is bounded. We derive guarantees on worst-case, finite-sample false-positive rate (FPR) over the family of all distributions with bounded second moment. Thus, our method is the first OCPD algorithm that guarantees finite-sample FPR, even if the data is high dimensional and the underlying distributions are heavy-tailed. The technical contribution of our paper is to show that clipped-SGD can estimate the mean of a random vector and simultaneously provide confidence bounds at all confidence values. We combine this robust estimate with a union bound argument and construct a sequential change-point algorithm with finite
&lt;/p&gt;</description></item><item><title>&#26641;&#24418;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(TreeVAE)&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;&#23618;&#27425;&#32858;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#28789;&#27963;&#30340;&#26641;&#29366;&#28508;&#21464;&#37327;&#21518;&#39564;&#20998;&#24067;&#65292;&#23618;&#27425;&#21010;&#20998;&#25968;&#25454;&#26679;&#26412;&#24182;&#25581;&#31034;&#38544;&#34255;&#32467;&#26500;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#26641;&#30340;&#29983;&#25104;&#24335;&#26550;&#26500;&#36827;&#34892;&#36731;&#37327;&#32423;&#26465;&#20214;&#25512;&#29702;&#65292;&#21516;&#26102;&#36890;&#36807;&#19987;&#38376;&#30340;&#21494;&#23376;&#35299;&#30721;&#22120;&#25552;&#39640;&#29983;&#25104;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#65292;TreeVAE&#21457;&#29616;&#20102;&#28508;&#22312;&#31751;&#24182;&#25214;&#21040;&#20102;&#26377;&#24847;&#20041;&#30340;&#23618;&#27425;&#20851;&#31995;&#12290;&#19982;&#39034;&#24207;&#23545;&#24212;&#29289;&#30456;&#27604;&#65292;TreeVAE&#25552;&#20379;&#20102;&#26356;&#20855;&#31454;&#20105;&#21147;&#30340;&#23545;&#25968;&#20284;&#28982;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.08984</link><description>&lt;p&gt;
&#26641;&#24418;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Tree Variational Autoencoders. (arXiv:2306.08984v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08984
&lt;/p&gt;
&lt;p&gt;
&#26641;&#24418;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(TreeVAE)&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;&#23618;&#27425;&#32858;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#28789;&#27963;&#30340;&#26641;&#29366;&#28508;&#21464;&#37327;&#21518;&#39564;&#20998;&#24067;&#65292;&#23618;&#27425;&#21010;&#20998;&#25968;&#25454;&#26679;&#26412;&#24182;&#25581;&#31034;&#38544;&#34255;&#32467;&#26500;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#26641;&#30340;&#29983;&#25104;&#24335;&#26550;&#26500;&#36827;&#34892;&#36731;&#37327;&#32423;&#26465;&#20214;&#25512;&#29702;&#65292;&#21516;&#26102;&#36890;&#36807;&#19987;&#38376;&#30340;&#21494;&#23376;&#35299;&#30721;&#22120;&#25552;&#39640;&#29983;&#25104;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#65292;TreeVAE&#21457;&#29616;&#20102;&#28508;&#22312;&#31751;&#24182;&#25214;&#21040;&#20102;&#26377;&#24847;&#20041;&#30340;&#23618;&#27425;&#20851;&#31995;&#12290;&#19982;&#39034;&#24207;&#23545;&#24212;&#29289;&#30456;&#27604;&#65292;TreeVAE&#25552;&#20379;&#20102;&#26356;&#20855;&#31454;&#20105;&#21147;&#30340;&#23545;&#25968;&#20284;&#28982;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;&#23618;&#27425;&#32858;&#31867;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#22522;&#20110;&#26641;&#30340;&#28508;&#21464;&#37327;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#25552;&#20986;&#30340;&#26641;&#24418;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(TreeVAE)&#26681;&#25454;&#25968;&#25454;&#30340;&#22266;&#26377;&#29305;&#24449;&#23545;&#26679;&#26412;&#36827;&#34892;&#23618;&#27425;&#21010;&#20998;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#20013;&#30340;&#38544;&#34255;&#32467;&#26500;&#12290;&#23427;&#26681;&#25454;&#28508;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#35843;&#25972;&#20854;&#32467;&#26500;&#20197;&#21457;&#29616;&#26368;&#20248;&#30340;&#32534;&#30721;&#26641;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;&#26641;&#30340;&#29983;&#25104;&#24335;&#26550;&#26500;&#20801;&#35768;&#36731;&#37327;&#32423;&#30340;&#26465;&#20214;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#19987;&#38376;&#30340;&#21494;&#23376;&#35299;&#30721;&#22120;&#25552;&#39640;&#29983;&#25104;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;TreeVAE&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#65292;&#21253;&#25324;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#20687;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#28508;&#22312;&#31751;&#65292;&#24182;&#25214;&#21040;&#20102;&#19981;&#21516;&#32452;&#20043;&#38388;&#30340;&#26377;&#24847;&#20041;&#30340;&#23618;&#27425;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#39034;&#24207;&#23545;&#24212;&#29289;&#30456;&#27604;&#65292;TreeVAE&#25552;&#20379;&#20102;&#26356;&#20855;&#31454;&#20105;&#21147;&#30340;&#23545;&#25968;&#20284;&#28982;&#19979;&#30028;&#12290;&#26368;&#21518;&#65292;&#30001;&#20110;&#20854;&#29983;&#25104;&#24615;&#36136;&#65292;TreeVAE&#33021;&#22815;&#20174;&#24050;&#23398;&#20064;&#30340;&#20998;&#24067;&#20013;&#29983;&#25104;&#26032;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new generative hierarchical clustering model that learns a flexible tree-based posterior distribution over latent variables. The proposed Tree Variational Autoencoder (TreeVAE) hierarchically divides samples according to their intrinsic characteristics, shedding light on hidden structure in the data. It adapts its architecture to discover the optimal tree for encoding dependencies between latent variables. The proposed tree-based generative architecture permits lightweight conditional inference and improves generative performance by utilizing specialized leaf decoders. We show that TreeVAE uncovers underlying clusters in the data and finds meaningful hierarchical relations between the different groups on a variety of datasets, including real-world imaging data. We present empirically that TreeVAE provides a more competitive log-likelihood lower bound than the sequential counterparts. Finally, due to its generative nature, TreeVAE is able to generate new samples from the di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#20114;&#32852;&#39532;&#23572;&#31185;&#22827;&#38142;&#30340;&#19981;&#20559;&#24494;&#20998;&#65292;&#24320;&#21457;&#20986;&#19968;&#31181;&#26080;&#20559;&#12289;&#20302;&#26041;&#24046;&#21644;&#33258;&#21160;&#30340;&#26041;&#27861;&#23545;&#22797;&#26434;&#23494;&#24230;&#36827;&#34892;&#29983;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545; MH &#37319;&#26679;&#22120;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.07961</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#20559;&#24494;&#20998;&#23545;&#25239;&#22797;&#26434;&#23494;&#24230;&#29983;&#25104;&#65292;&#22522;&#20110;&#20114;&#32852;&#39532;&#23572;&#31185;&#22827;&#38142;&#19981;&#20559;&#24494;&#20998;&#20248;&#21270; MH &#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentiating Metropolis-Hastings to Optimize Intractable Densities. (arXiv:2306.07961v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#20114;&#32852;&#39532;&#23572;&#31185;&#22827;&#38142;&#30340;&#19981;&#20559;&#24494;&#20998;&#65292;&#24320;&#21457;&#20986;&#19968;&#31181;&#26080;&#20559;&#12289;&#20302;&#26041;&#24046;&#21644;&#33258;&#21160;&#30340;&#26041;&#27861;&#23545;&#22797;&#26434;&#23494;&#24230;&#36827;&#34892;&#29983;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545; MH &#37319;&#26679;&#22120;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27010;&#29575;&#27169;&#22411;&#25512;&#29702;&#20013;&#65292;&#30446;&#26631;&#23494;&#24230;&#20989;&#25968;&#36890;&#24120;&#21464;&#24471;&#38590;&#20197;&#35745;&#31639;&#65292;&#38656;&#35201;&#20351;&#29992; Monte Carlo &#35745;&#31639;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#20559;&#24494;&#20998; Metropolis-Hastings &#37319;&#26679;&#22120;&#30340;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#27010;&#29575;&#25512;&#29702;&#26469;&#36827;&#34892;&#24494;&#20998;&#12290;&#36890;&#36807;&#23558;&#38543;&#26426;&#24494;&#20998;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982; Markov &#38142;&#32806;&#21512;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23454;&#29616;&#26080;&#20559;&#65292;&#20302;&#26041;&#24046;&#21644;&#33258;&#21160;&#30340;&#31243;&#24207;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#24212;&#29992;&#20110;&#30001;&#20110;&#32321;&#29712;&#30340;&#30446;&#26631;&#23494;&#24230;&#23548;&#33268;&#26399;&#26395;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20013;&#25214;&#21040;&#19968;&#20010;&#27169;&#26865;&#20004;&#21487;&#30340;&#35266;&#23519;&#21644;&#22312; Ising &#27169;&#22411;&#20013;&#26368;&#22823;&#21270;&#27604;&#28909;&#26469;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
When performing inference on probabilistic models, target densities often become intractable, necessitating the use of Monte Carlo samplers. We develop a methodology for unbiased differentiation of the Metropolis-Hastings sampler, allowing us to differentiate through probabilistic inference. By fusing recent advances in stochastic differentiation with Markov chain coupling schemes, the procedure can be made unbiased, low-variance, and automatic. This allows us to apply gradient-based optimization to objectives expressed as expectations over intractable target densities. We demonstrate our approach by finding an ambiguous observation in a Gaussian mixture model and by maximizing the specific heat in an Ising model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#30340;&#24418;&#24335;&#21270;&#25551;&#36848;&#26469;&#32531;&#35299;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07874</link><description>&lt;p&gt;
&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Taxonomy-Structured Domain Adaptation. (arXiv:2306.07874v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#30340;&#24418;&#24335;&#21270;&#25551;&#36848;&#26469;&#32531;&#35299;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#26088;&#22312;&#32531;&#35299;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26041;&#27861;&#22823;&#22810;&#38480;&#20110;&#20998;&#31867;&#39046;&#22495;&#65292;&#36825;&#20005;&#37325;&#31616;&#21270;&#20102;&#30495;&#23454;&#19990;&#30028;&#20013;&#24494;&#22937;&#30340;&#39046;&#22495;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#36827;&#34892;&#25512;&#24191;&#65292;&#23558;&#39046;&#22495;&#24418;&#24335;&#21270;&#20026;&#20855;&#26377;&#23884;&#22871;&#30340;&#23618;&#27425;&#30456;&#20284;&#32467;&#26500;&#65292;&#20363;&#22914;&#21160;&#29289;&#29289;&#31181;&#21644;&#20135;&#21697;&#30446;&#24405;&#12290;&#25105;&#20204;&#24314;&#31435;&#22312;&#32463;&#20856;&#23545;&#25239;&#26694;&#26550;&#20043;&#19978;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#19982;&#23545;&#25239;&#24615;&#37492;&#21035;&#22120;&#31454;&#20105;&#20197;&#20445;&#30041;&#23618;&#27425;&#32467;&#26500;&#20449;&#24687;&#12290;&#24403;&#32473;&#23450;&#38750;&#20449;&#24687;&#39046;&#22495;&#20998;&#31867;&#65288;&#20363;&#22914;&#65292;&#25152;&#26377;&#21494;&#33410;&#28857;&#37117;&#38142;&#25509;&#21040;&#26681;&#33410;&#28857;&#30340;&#25153;&#24179;&#20998;&#31867;&#65289;&#26102;&#65292;&#24179;&#34913;&#28857;&#24674;&#22797;&#32463;&#20856;&#30340;&#23545;&#25239;&#39046;&#22495;&#36866;&#24212;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#20998;&#31867;&#20013;&#20135;&#29983;&#38750;&#24179;&#20961;&#30340;&#32467;&#26524;&#12290;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#21151;&#36827;&#34892;&#20102;&#33258;&#36866;&#24212;&#12290;&#20195;&#30721;&#21487;&#22312;https://gith&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation aims to mitigate distribution shifts among different domains. However, traditional formulations are mostly limited to categorical domains, greatly simplifying nuanced domain relationships in the real world. In this work, we tackle a generalization with taxonomy-structured domains, which formalizes domains with nested, hierarchical similarity structures such as animal species and product catalogs. We build on the classic adversarial framework and introduce a novel taxonomist, which competes with the adversarial discriminator to preserve the taxonomy information. The equilibrium recovers the classic adversarial domain adaptation's solution if given a non-informative domain taxonomy (e.g., a flat taxonomy where all leaf nodes connect to the root node) while yielding non-trivial results with other taxonomies. Empirically, our method achieves state-of-the-art performance on both synthetic and real-world datasets with successful adaptation. Code is available at https://gith
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;3D&#20998;&#23376;&#22270;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20998;&#23376;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20998;&#23376;&#20960;&#20309;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2306.07812</link><description>&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#33258;&#21160;&#21270;&#19977;&#32500;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Automated 3D Pre-Training for Molecular Property Prediction. (arXiv:2306.07812v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07812
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;3D&#20998;&#23376;&#22270;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20998;&#23376;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20998;&#23376;&#20960;&#20309;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#26159;&#33647;&#29289;&#30740;&#21457;&#21644;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#30001;&#20110;&#20998;&#23376;&#30340;&#20960;&#20309;&#32467;&#26500;&#23545;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#24517;&#35201;&#24615;&#24050;&#32463;&#34987;&#35777;&#26126;&#65292;&#22240;&#27492;&#23558;3D&#20449;&#24687;&#19982;&#21508;&#31181;&#22270;&#24418;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#33719;&#24471;&#20998;&#23376;&#30340;&#20960;&#20309;&#32467;&#26500;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;3D&#39044;&#35757;&#32451;&#26694;&#26550;&#65288;&#31216;&#20026;3D PGT&#65289;&#65292;&#23427;&#22312;3D&#20998;&#23376;&#22270;&#19978;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#27809;&#26377;3D&#32467;&#26500;&#30340;&#20998;&#23376;&#22270;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22522;&#20110;&#21270;&#23398;&#38190;&#38271;&#65292;&#21270;&#23398;&#38190;&#35282;&#21644;&#20108;&#38754;&#35282;&#36825;&#19977;&#20010;&#22522;&#26412;&#20960;&#20309;&#25551;&#36848;&#31526;&#23545;&#24212;&#20110;&#23436;&#25972;&#30340;&#20998;&#23376;3D&#26500;&#24418;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#36825;&#19977;&#20010;&#23646;&#24615;&#30340;&#22810;&#20219;&#21153;&#29983;&#25104;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#33258;&#21160;&#34701;&#21512;&#36825;&#19977;&#39033;&#29983;&#25104;&#20219;&#21153;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20351;&#29992;&#8220;&#24635;&#33021;&#37327;&#8221;&#26469;&#25628;&#32034;&#30340;&#26367;&#20195;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular property prediction is an important problem in drug discovery and materials science. As geometric structures have been demonstrated necessary for molecular property prediction, 3D information has been combined with various graph learning methods to boost prediction performance. However, obtaining the geometric structure of molecules is not feasible in many real-world applications due to the high computational cost. In this work, we propose a novel 3D pre-training framework (dubbed 3D PGT), which pre-trains a model on 3D molecular graphs, and then fine-tunes it on molecular graphs without 3D structures. Based on fact that bond length, bond angle, and dihedral angle are three basic geometric descriptors corresponding to a complete molecular 3D conformer, we first develop a multi-task generative pre-train framework based on these three attributes. Next, to automatically fuse these three generative tasks, we design a surrogate metric using the \textit{total energy} to search for 
&lt;/p&gt;</description></item><item><title>V-LoL&#26159;&#19968;&#20010;&#32467;&#21512;&#35270;&#35273;&#21644;&#36923;&#36753;&#25361;&#25112;&#30340;&#35786;&#26029;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;V-LoL-Trains&#65292;&#35813;&#25968;&#25454;&#38598;&#39318;&#27425;&#23558;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#21644;&#28789;&#27963;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#30740;&#31350;&#24191;&#27867;&#30340;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#25361;&#25112;&#25552;&#20379;&#20102;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2306.07743</link><description>&lt;p&gt;
V-LoL: &#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#30340;&#35786;&#26029;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
V-LoL: A Diagnostic Dataset for Visual Logical Learning. (arXiv:2306.07743v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07743
&lt;/p&gt;
&lt;p&gt;
V-LoL&#26159;&#19968;&#20010;&#32467;&#21512;&#35270;&#35273;&#21644;&#36923;&#36753;&#25361;&#25112;&#30340;&#35786;&#26029;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;V-LoL-Trains&#65292;&#35813;&#25968;&#25454;&#38598;&#39318;&#27425;&#23558;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#21644;&#28789;&#27963;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#30740;&#31350;&#24191;&#27867;&#30340;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#25361;&#25112;&#25552;&#20379;&#20102;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#26399;&#22312;&#35270;&#35273;AI&#39046;&#22495;&#26377;&#20102;&#35768;&#22810;&#25104;&#21151;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#19981;&#21516;&#30340;&#32570;&#28857;&#65307;&#21253;&#25324;&#32570;&#23569;&#31934;&#30830;&#30340;&#36923;&#36753;&#25512;&#29702;&#12289;&#25277;&#35937;&#30340;&#27010;&#25324;&#33021;&#21147;&#20197;&#21450;&#29702;&#35299;&#22797;&#26434;&#21644;&#22024;&#26434;&#30340;&#22330;&#26223;&#31561;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#24182;&#19981;&#33021;&#25429;&#25417;&#21040;&#36825;&#20123;&#26041;&#38754;&#20013;&#30340;&#22810;&#25968;&#12290;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#38598;&#20851;&#27880;&#35270;&#35273;&#22797;&#26434;&#25968;&#25454;&#20294;&#21482;&#26377;&#31616;&#21333;&#30340;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#65292;&#24402;&#32435;&#36923;&#36753;&#25968;&#25454;&#38598;&#21253;&#25324;&#22797;&#26434;&#30340;&#36923;&#36753;&#23398;&#20064;&#20219;&#21153;&#65292;&#20294;&#26159;&#32570;&#20047;&#35270;&#35273;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#25968;&#25454;&#38598;V-LoL&#65292;&#23427;&#26080;&#32541;&#22320;&#32467;&#21512;&#20102;&#35270;&#35273;&#21644;&#36923;&#36753;&#30340;&#25361;&#25112;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#39318;&#27425;&#25512;&#20986;&#20102;V-LoL&#30340;&#31532;&#19968;&#20010;&#23454;&#20363;&#65292;&#21517;&#20026;V-LoL-Trains&#65292;&#23427;&#26159;&#31526;&#21495;AI&#20013;&#19968;&#20010;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#30340;&#35270;&#35273;&#21576;&#29616;&#65292;&#21363;Michalski&#28779;&#36710;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20869;&#32467;&#21512;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#21644;&#28789;&#27963;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;V-LoL-Trains&#20026;&#30740;&#31350;&#24191;&#27867;&#30340;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#25361;&#25112;&#25552;&#20379;&#20102;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the successes of recent developments in visual AI, different shortcomings still exist; from missing exact logical reasoning, to abstract generalization abilities, to understanding complex and noisy scenes. Unfortunately, existing benchmarks, were not designed to capture more than a few of these aspects. Whereas deep learning datasets focus on visually complex data but simple visual reasoning tasks, inductive logic datasets involve complex logical learning tasks, however, lack the visual component. To address this, we propose the visual logical learning dataset, V-LoL, that seamlessly combines visual and logical challenges. Notably, we introduce the first instantiation of V-LoL, V-LoL-Trains, -- a visual rendition of a classic benchmark in symbolic AI, the Michalski train problem. By incorporating intricate visual scenes and flexible logical reasoning tasks within a versatile framework, V-LoL-Trains provides a platform for investigating a wide range of visual logical learning ch
&lt;/p&gt;</description></item><item><title>DRCFS&#26159;&#19968;&#20010;&#21452;&#37325;&#31283;&#20581;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#38750;&#32447;&#24615;&#21644;&#39640;&#32500;&#29615;&#22659;&#20013;&#35782;&#21035;&#22240;&#26524;&#29305;&#24449;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.07024</link><description>&lt;p&gt;
DRCFS&#65306;&#21452;&#37325;&#31283;&#20581;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
DRCFS: Doubly Robust Causal Feature Selection. (arXiv:2306.07024v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07024
&lt;/p&gt;
&lt;p&gt;
DRCFS&#26159;&#19968;&#20010;&#21452;&#37325;&#31283;&#20581;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#38750;&#32447;&#24615;&#21644;&#39640;&#32500;&#29615;&#22659;&#20013;&#35782;&#21035;&#22240;&#26524;&#29305;&#24449;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#20102;&#35299;&#23545;&#20110;&#29305;&#23450;&#30446;&#26631;&#21464;&#37327;&#38750;&#24120;&#30456;&#20851;&#30340;&#22797;&#26434;&#31995;&#32479;&#29305;&#24449;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#23616;&#38480;&#20110;&#32447;&#24615;&#35774;&#32622;&#65292;&#26377;&#26102;&#32570;&#20047;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#26080;&#27861;&#25193;&#23637;&#21040;&#38656;&#35201;&#22788;&#29702;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22270;&#20687;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DRCFS&#65292;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#38750;&#32447;&#24615;&#21644;&#39640;&#32500;&#35774;&#32622;&#20013;&#35782;&#21035;&#22240;&#26524;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#38416;&#26126;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#27169;&#25311;&#21644;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;DRCFS&#22312;&#36873;&#25321;&#20855;&#26377;&#40065;&#26834;&#24615;&#29305;&#24449;&#30340;&#25216;&#24039;&#38750;&#24120;&#38750;&#32447;&#24615;&#21644;&#39640;&#32500;&#38382;&#39064;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowing the features of a complex system that are highly relevant to a particular target variable is of fundamental interest in many areas of science. Existing approaches are often limited to linear settings, sometimes lack guarantees, and in most cases, do not scale to the problem at hand, in particular to images. We propose DRCFS, a doubly robust feature selection method for identifying the causal features even in nonlinear and high dimensional settings. We provide theoretical guarantees, illustrate necessary conditions for our assumptions, and perform extensive experiments across a wide range of simulated and semi-synthetic datasets. DRCFS significantly outperforms existing state-of-the-art methods, selecting robust features even in challenging highly non-linear and high-dimensional problems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#31561;&#24335;&#23884;&#20837;&#28145;&#24230;Lagrange&#23545;&#20598;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#24102;&#26631;&#31614;&#30340;&#36924;&#36817;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#12290;&#27492;&#26041;&#27861;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23884;&#20837;&#31561;&#24335;&#32422;&#26463;&#26469;&#30830;&#20445;&#21487;&#34892;&#35299;&#65292;&#24182;&#20351;&#29992;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;DeepLDE&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#20248;&#21270;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.06674</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#31561;&#24335;&#23884;&#20837;&#28145;&#24230;Lagrange&#23545;&#20598;&#31639;&#27861;&#20248;&#21270;&#36924;&#36817;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Equality Embedded Deep Lagrange Dual for Approximate Constrained Optimization. (arXiv:2306.06674v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06674
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#31561;&#24335;&#23884;&#20837;&#28145;&#24230;Lagrange&#23545;&#20598;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#24102;&#26631;&#31614;&#30340;&#36924;&#36817;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#12290;&#27492;&#26041;&#27861;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23884;&#20837;&#31561;&#24335;&#32422;&#26463;&#26469;&#30830;&#20445;&#21487;&#34892;&#35299;&#65292;&#24182;&#20351;&#29992;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;DeepLDE&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#20248;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#20256;&#32479;&#27714;&#35299;&#26041;&#27861;&#36890;&#24120;&#35745;&#31639;&#37327;&#36739;&#22823;&#65292;&#29305;&#21035;&#26159;&#22312;&#35268;&#27169;&#36739;&#22823;&#12289;&#26102;&#38388;&#25935;&#24863;&#30340;&#38382;&#39064;&#19978;&#26356;&#26159;&#22914;&#27492;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#24555;&#36895;&#26368;&#20248;&#35299;&#36924;&#36817;&#22120;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#36234;&#26469;&#36234;&#22823;&#20852;&#36259;&#65292;&#20294;&#26159;&#23558;&#32422;&#26463;&#26465;&#20214;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DeepLDE&#30340;&#28145;&#24230;Lagrange&#23545;&#20598;&#31639;&#27861;&#65292;&#35813;&#26694;&#26550;&#23398;&#20064;&#22312;&#19981;&#20351;&#29992;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23547;&#25214;&#26368;&#20248;&#35299;&#65292;&#36890;&#36807;&#23558;&#31561;&#24335;&#32422;&#26463;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#26469;&#30830;&#20445;&#21487;&#34892;&#35299;&#65292;&#24182;&#20351;&#29992;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#23545;&#19981;&#31561;&#24335;&#32422;&#26463;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DeepLDE&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#34920;&#26126;&#20165;&#38752;&#21407;&#22987;-&#23545;&#20598;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#30830;&#20445;&#31561;&#24335;&#32422;&#26463;&#65292;&#38656;&#35201;&#31561;&#24335;&#23884;&#20837;&#30340;&#24110;&#21161;&#12290;&#22312;&#20984;&#12289;&#38750;&#20984;&#21644;&#20132;&#27969;&#26368;&#20248;&#28526;&#27969;&#65288;AC-OPF&#65289;&#38382;&#39064;&#30340;&#27169;&#25311;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DeepLDE&#30340;&#26368;&#20248;&#24615;&#33021;&#32780;&#19988;&#22987;&#32456;&#20445;&#35777;&#21487;&#34892;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional solvers are often computationally expensive for constrained optimization, particularly in large-scale and time-critical problems. While this leads to a growing interest in using neural networks (NNs) as fast optimal solution approximators, incorporating the constraints with NNs is challenging. In this regard, we propose deep Lagrange dual with equality embedding (DeepLDE), a framework that learns to find an optimal solution without using labels. To ensure feasible solutions, we embed equality constraints into the NNs and train the NNs using the primal-dual method to impose inequality constraints. Furthermore, we prove the convergence of DeepLDE and show that the primal-dual learning method alone cannot ensure equality constraints without the help of equality embedding. Simulation results on convex, non-convex, and AC optimal power flow (AC-OPF) problems show that the proposed DeepLDE achieves the smallest optimality gap among all the NN-based approaches while always ensuri
&lt;/p&gt;</description></item><item><title>&#21435;&#22122;&#22768;&#26356;&#22909;&#65292;&#34920;&#29616;&#26356;&#22909;&#30340;&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#26426;&#21046;</title><link>http://arxiv.org/abs/2306.04940</link><description>&lt;p&gt;
&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Layer-level activation mechanism. (arXiv:2306.04940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04940
&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#22768;&#26356;&#22909;&#65292;&#34920;&#29616;&#26356;&#22909;&#30340;&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28608;&#27963;&#26426;&#21046;&#65292;&#26088;&#22312;&#24314;&#31435;&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#21151;&#33021;&#65288;LayerAct&#65289;&#12290;&#36825;&#20123;&#21151;&#33021;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36755;&#20837;&#20559;&#31227;&#25152;&#23548;&#33268;&#30340;&#28608;&#27963;&#36755;&#20986;&#30340;&#20998;&#23618;&#32423;&#27874;&#21160;&#26469;&#38477;&#20302;&#20256;&#32479;&#20803;&#32032;&#32423;&#28608;&#27963;&#21151;&#33021;&#30340;&#22122;&#38899;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;LayerAct&#21151;&#33021;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#38646;&#30340;&#24179;&#22343;&#28608;&#27963;&#36755;&#20986;&#65292;&#32780;&#19981;&#38480;&#21046;&#28608;&#27963;&#36755;&#20986;&#31354;&#38388;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#35777;&#26126;LayerAct&#21151;&#33021;&#22312;&#22122;&#22768;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#20803;&#32032;&#32423;&#28608;&#27963;&#21151;&#33021;&#65292;&#24182;&#19988;&#32463;&#39564;&#35777;&#26126;&#36825;&#20123;&#21151;&#33021;&#30340;&#24179;&#22343;&#28608;&#27963;&#32467;&#26524;&#31867;&#20284;&#20110;&#38646;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22788;&#29702;&#22024;&#26434;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#26102;&#65292;LayerAct&#21151;&#33021;&#27604;&#20803;&#32032;&#32423;&#28608;&#27963;&#21151;&#33021;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#28165;&#27905;&#25968;&#25454;&#38598;&#30340;&#34920;&#29616;&#20063;&#26159;&#20248;&#36234;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a novel activation mechanism aimed at establishing layer-level activation (LayerAct) functions. These functions are designed to be more noise-robust compared to traditional element-level activation functions by reducing the layer-level fluctuation of the activation outputs due to shift in inputs. Moreover, the LayerAct functions achieve a zero-like mean activation output without restricting the activation output space. We present an analysis and experiments demonstrating that LayerAct functions exhibit superior noise-robustness compared to element-level activation functions, and empirically show that these functions have a zero-like mean activation. Experimental results on three benchmark image classification tasks show that LayerAct functions excel in handling noisy image datasets, outperforming element-level activation functions, while the performance on clean datasets is also superior in most cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#22312;&#28151;&#21512;&#20854;&#20182;&#25991;&#26412;&#26469;&#28304;&#26102;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.04634</link><description>&lt;p&gt;
&#35770;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Reliability of Watermarks for Large Language Models. (arXiv:2306.04634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#22312;&#28151;&#21512;&#20854;&#20182;&#25991;&#26412;&#26469;&#28304;&#26102;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#24320;&#22987;&#24212;&#29992;&#20110;&#26085;&#24120;&#20351;&#29992;&#65292;&#24182;&#26377;&#33021;&#21147;&#22312;&#26410;&#26469;&#30340;&#21313;&#24180;&#20869;&#20135;&#29983;&#22823;&#37327;&#30340;&#25991;&#26412;&#12290;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#33021;&#20250;&#21462;&#20195;&#20114;&#32852;&#32593;&#19978;&#30340;&#20154;&#31867;&#20889;&#20316;&#25991;&#26412;&#65292;&#24182;&#26377;&#21487;&#33021;&#34987;&#29992;&#20110;&#24694;&#24847;&#30446;&#30340;&#65292;&#22914;&#38035;&#40060;&#25915;&#20987;&#21644;&#31038;&#20132;&#23186;&#20307;&#26426;&#22120;&#20154;&#12290;&#27700;&#21360;&#26159;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#26816;&#27979;&#21644;&#21487;&#35760;&#24405;&#65292;&#26469;&#38477;&#20302;&#36825;&#20123;&#20260;&#23475;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;&#22312;&#29616;&#23454;&#20013;&#28151;&#21512;&#20102;&#20854;&#20182;&#30340;&#25991;&#26412;&#26469;&#28304;&#65292;&#34987;&#20154;&#31867;&#20889;&#20316;&#32773;&#25110;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#25913;&#20889;&#65292;&#34987;&#29992;&#20110;&#31038;&#20132;&#21644;&#25216;&#26415;&#39046;&#22495;&#30340;&#21508;&#31181;&#24212;&#29992;&#26102;&#65292;&#27700;&#21360;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#30340;&#21487;&#38752;&#24615;&#22914;&#20309;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#26816;&#27979;&#26041;&#26696;&#65292;&#37327;&#21270;&#20102;&#23427;&#20204;&#26816;&#27979;&#27700;&#21360;&#30340;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#22312;&#27599;&#20010;&#24773;&#20917;&#19979;&#38656;&#35201;&#35266;&#23519;&#22810;&#23569;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#25165;&#33021;&#21487;&#38752;&#22320;&#26816;&#27979;&#27700;&#21360;&#12290;&#25105;&#20204;&#29305;&#21035;&#24378;&#35843;&#20102;&#24403;&#27700;&#21360;&#19982;&#20854;&#20182;&#25991;&#26412;&#26469;&#28304;&#28151;&#21512;&#26102;&#27700;&#21360;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#27700;&#21360;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are now deployed to everyday use and positioned to produce large quantities of text in the coming decade. Machine-generated text may displace human-written text on the internet and has the potential to be used for malicious purposes, such as spearphishing attacks and social media bots. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet, a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text might be mixed with other text sources, paraphrased by human writers or other language models, and used for applications in a broad number of domains, both social and technical. In this paper, we explore different detection schemes, quantify their power at detecting watermarks, and determine how much machine-generated text needs to be observed in each scenario to reliably detect the watermark. We especially highlight o
&lt;/p&gt;</description></item><item><title>ContriMix&#26159;&#19968;&#31181;&#26080;&#38656;&#26631;&#35782;&#21644;&#25163;&#24037;&#35843;&#20248;&#30340;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#65292;&#22312;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#36890;&#36807;&#20998;&#31163;&#21644;&#23398;&#20064;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#30340;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04527</link><description>&lt;p&gt;
ContriMix&#65306;&#26174;&#24494;&#38236;&#22270;&#20687;&#20998;&#26512;&#20013;&#22522;&#20110;&#26080;&#30417;&#30563;&#20869;&#23481;&#23646;&#24615;&#20998;&#31163;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ContriMix: Unsupervised disentanglement of content and attribute for domain generalization in microscopy image analysis. (arXiv:2306.04527v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04527
&lt;/p&gt;
&lt;p&gt;
ContriMix&#26159;&#19968;&#31181;&#26080;&#38656;&#26631;&#35782;&#21644;&#25163;&#24037;&#35843;&#20248;&#30340;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#65292;&#22312;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#36890;&#36807;&#20998;&#31163;&#21644;&#23398;&#20064;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#30340;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32452;&#32455;&#23398;&#21644;&#33639;&#20809;&#25104;&#20687;&#31561;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; ContriMix&#65292;&#23427;&#37319;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#24335;&#20998;&#31163;&#20986;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#30340;&#29983;&#29289;&#23398;&#20869;&#23481;&#21644;&#25216;&#26415;&#21464;&#24322;&#65292;&#24182;&#23398;&#20064;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#25163;&#24037; fine-tuning &#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#32452;&#32455;&#23398;&#21644;&#33639;&#20809;&#25104;&#20687;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102; ContriMix &#30340;&#26377;&#25928;&#24615;&#65292;&#21462;&#24471;&#20102;&#22522;&#20110;&#39046;&#22495;&#27867;&#21270;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization is critical for real-world applications of machine learning models to microscopy images, including histopathology and fluorescence imaging. Artifacts in histopathology arise through a complex combination of factors relating to tissue collection and laboratory processing, as well as factors intrinsic to patient samples. In fluorescence imaging, these artifacts stem from variations across experimental batches. The complexity and subtlety of these artifacts make the enumeration of data domains intractable. Therefore, augmentation-based methods of domain generalization that require domain identifiers and manual fine-tuning are inadequate in this setting. To overcome this challenge, we introduce ContriMix, a domain generalization technique that learns to generate synthetic images by disentangling and permuting the biological content ("content") and technical variations ("attributes") in microscopy images. ContriMix does not rely on domain identifiers or handcrafted aug
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DFM&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#26631;&#31614;&#20559;&#31227;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#19978;&#38480;&#21644;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#22522;&#20110;&#26680;&#30340;DFM&#29256;&#26412;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04376</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#24067;&#29305;&#24449;&#21305;&#37197;&#30340;&#26631;&#31614;&#20559;&#31227;&#37327;&#37327;&#21270;&#21450;&#20854;&#40065;&#26834;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Label Shift Quantification with Robustness Guarantees via Distribution Feature Matching. (arXiv:2306.04376v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DFM&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#26631;&#31614;&#20559;&#31227;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#19978;&#38480;&#21644;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#22522;&#20110;&#26680;&#30340;DFM&#29256;&#26412;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#23398;&#20064;&#22788;&#29702;&#22312;&#26631;&#31614;&#20559;&#31227;&#19979;&#20272;&#35745;&#30446;&#26631;&#26631;&#31614;&#20998;&#24067;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20998;&#24067;&#29305;&#24449;&#21305;&#37197;&#65288;DFM&#65289;&#65292;&#23558;&#20808;&#21069;&#25991;&#29486;&#20013;&#24341;&#20837;&#30340;&#21508;&#31181;&#20272;&#35745;&#22120;&#24674;&#22797;&#20026;&#29305;&#23450;&#23454;&#20363;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;DFM&#31243;&#24207;&#30340;&#19968;&#33324;&#24615;&#33021;&#30028;&#65292;&#25913;&#36827;&#20102;&#20808;&#21069;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#25512;&#23548;&#30340;&#30028;&#38480;&#30340;&#33509;&#24178;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#19968;&#20998;&#26512;&#25193;&#23637;&#21040;&#30740;&#31350;DFM&#31243;&#24207;&#22312;&#26410;&#31934;&#30830;&#20551;&#35774;&#26631;&#31614;&#20559;&#31227;&#37327;&#30340;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#30446;&#26631;&#21463;&#21040;&#26410;&#30693;&#20998;&#24067;&#27745;&#26579;&#30340;&#24773;&#20917;&#19979;&#12290;&#36825;&#20123;&#29702;&#35770;&#21457;&#29616;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#35814;&#32454;&#30340;&#25968;&#23383;&#30740;&#31350;&#30830;&#35748;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#21407;&#29702;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#65292;&#21487;&#25193;&#23637;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#22522;&#20110;&#26680;&#30340;DFM&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantification learning deals with the task of estimating the target label distribution under label shift. In this paper, we first present a unifying framework, distribution feature matching (DFM), that recovers as particular instances various estimators introduced in previous literature. We derive a general performance bound for DFM procedures, improving in several key aspects upon previous bounds derived in particular cases. We then extend this analysis to study robustness of DFM procedures in the misspecified setting under departure from the exact label shift hypothesis, in particular in the case of contamination of the target by an unknown distribution. These theoretical findings are confirmed by a detailed numerical study on simulated and real-world datasets. We also introduce an efficient, scalable and robust version of kernel-based DFM using the Random Fourier Feature principle.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#24555;&#36895;&#36873;&#25321;&#26368;&#20339;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#36229;&#21442;&#25968;&#65292;&#36890;&#36807;&#29983;&#25104;&#22823;&#35268;&#27169;&#20803;&#25968;&#25454;&#38598;&#24182;&#20803;&#23398;&#20064;&#22810;&#20445;&#30495;&#24230;&#24615;&#33021;&#39044;&#27979;&#22120;&#65292;&#24182;&#22312;&#23398;&#20064;&#26032;&#25968;&#25454;&#38598;&#26102;&#20351;&#29992;&#35813;&#39044;&#27979;&#22120;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#21487;&#20197;&#24555;&#36895;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.03828</link><description>&lt;p&gt;
Quick-Tune&#65306;&#24555;&#36895;&#23398;&#20064;&#24212;&#35813;&#20351;&#29992;&#21738;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#21450;&#22914;&#20309;&#24494;&#35843;&#23427;
&lt;/p&gt;
&lt;p&gt;
Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How. (arXiv:2306.03828v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#24555;&#36895;&#36873;&#25321;&#26368;&#20339;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#36229;&#21442;&#25968;&#65292;&#36890;&#36807;&#29983;&#25104;&#22823;&#35268;&#27169;&#20803;&#25968;&#25454;&#38598;&#24182;&#20803;&#23398;&#20064;&#22810;&#20445;&#30495;&#24230;&#24615;&#33021;&#39044;&#27979;&#22120;&#65292;&#24182;&#22312;&#23398;&#20064;&#26032;&#25968;&#25454;&#38598;&#26102;&#20351;&#29992;&#35813;&#39044;&#27979;&#22120;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#21487;&#20197;&#24555;&#36895;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#27169;&#22411;&#25968;&#37327;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#19981;&#26029;&#38754;&#20020;&#19968;&#20010;&#38382;&#39064;&#65306;&#24212;&#35813;&#20351;&#29992;&#21738;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#21450;&#35813;&#22914;&#20309;&#24494;&#35843;&#23427;&#20197;&#36866;&#24212;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#32852;&#21512;&#25628;&#32034;&#26368;&#20339;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35780;&#20272;&#36229;&#36807;20k&#20010;&#36229;&#21442;&#25968;&#37197;&#32622;&#22312;87&#20010;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;24&#20010;&#39044;&#35757;&#32451;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#26469;&#29983;&#25104;&#22823;&#35268;&#27169;&#20803;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20854;&#23398;&#20064;&#26354;&#32447;&#19978;&#20803;&#23398;&#20064;&#22810;&#20445;&#30495;&#24230;&#24615;&#33021;&#39044;&#27979;&#22120;&#65292;&#20197;&#29992;&#20110;&#24555;&#36895;&#36229;&#21442;&#25968;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#24555;&#36895;&#36873;&#25321;&#19968;&#20010;&#20934;&#30830;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#25214;&#21040;&#23427;&#30340;&#26368;&#20339;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the ever-increasing number of pretrained models, machine learning practitioners are continuously faced with which pretrained model to use, and how to finetune it for a new dataset. In this paper, we propose a methodology that jointly searches for the optimal pretrained model and the hyperparameters for finetuning it. Our method transfers knowledge about the performance of many pretrained models with multiple hyperparameter configurations on a series of datasets. To this aim, we evaluated over 20k hyperparameter configurations for finetuning 24 pretrained image classification models on 87 datasets to generate a large-scale meta-dataset. We meta-learn a multi-fidelity performance predictor on the learning curves of this meta-dataset and use it for fast hyperparameter optimization on new datasets. We empirically demonstrate that our resulting approach can quickly select an accurate pretrained model for a new dataset together with its optimal hyperparameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LLM-Blender&#65292;&#23427;&#26159;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19981;&#21516;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#31168;&#29305;&#24615;&#65292;&#23454;&#29616;&#22987;&#32456;&#22914;&#19968;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;PairRanker&#21644;GenFuser&#26159;&#35813;&#26694;&#26550;&#30340;&#20004;&#20010;&#27169;&#22359;&#65292;PairRanker&#20351;&#29992;&#25104;&#23545;&#27604;&#36739;&#26041;&#27861;&#26469;&#21306;&#20998;&#20505;&#36873;&#36755;&#20986;&#65292;&#24182;&#19988;GenFuser&#26088;&#22312;&#21512;&#24182;&#25490;&#21517;&#26368;&#39640;&#30340;&#20505;&#36873;&#32773;&#65292;&#20197;&#29983;&#25104;&#25913;&#36827;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2306.02561</link><description>&lt;p&gt;
LLM-Blender: &#21033;&#29992;&#25104;&#23545;&#25490;&#21517;&#21644;&#29983;&#25104;&#34701;&#21512;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion. (arXiv:2306.02561v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LLM-Blender&#65292;&#23427;&#26159;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19981;&#21516;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#31168;&#29305;&#24615;&#65292;&#23454;&#29616;&#22987;&#32456;&#22914;&#19968;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;PairRanker&#21644;GenFuser&#26159;&#35813;&#26694;&#26550;&#30340;&#20004;&#20010;&#27169;&#22359;&#65292;PairRanker&#20351;&#29992;&#25104;&#23545;&#27604;&#36739;&#26041;&#27861;&#26469;&#21306;&#20998;&#20505;&#36873;&#36755;&#20986;&#65292;&#24182;&#19988;GenFuser&#26088;&#22312;&#21512;&#24182;&#25490;&#21517;&#26368;&#39640;&#30340;&#20505;&#36873;&#32773;&#65292;&#20197;&#29983;&#25104;&#25913;&#36827;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LLM-Blender&#65292;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19981;&#21516;&#20248;&#21183;&#26469;&#36798;&#21040;&#22987;&#32456;&#22914;&#19968;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;PairRanker&#21644;GenFuser&#65292;&#20197;&#24212;&#23545;&#19981;&#21516;&#31034;&#20363;&#30340;&#26368;&#20248;LLMs&#21487;&#20197;&#26174;&#30528;&#21464;&#21270;&#30340;&#35266;&#23519;&#12290;PairRanker&#20351;&#29992;&#19987;&#38376;&#30340;&#25104;&#23545;&#27604;&#36739;&#26041;&#27861;&#26469;&#21306;&#20998;&#20505;&#36873;&#36755;&#20986;&#20043;&#38388;&#30340;&#24494;&#23567;&#24046;&#24322;&#12290;&#23427;&#32852;&#21512;&#32534;&#30721;&#36755;&#20837;&#25991;&#26412;&#21644;&#19968;&#23545;&#20505;&#36873;&#32773;&#65292;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#32534;&#30721;&#22120;&#26469;&#30830;&#23450;&#20248;&#36234;&#32773;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;PairRanker&#19982;ChatGPT&#30340;&#25490;&#21517;&#30456;&#20851;&#24615;&#26368;&#39640;&#12290;&#28982;&#21518;&#65292;GenFuser&#26088;&#22312;&#21512;&#24182;&#25490;&#21517;&#26368;&#39640;&#30340;&#20505;&#36873;&#32773;&#65292;&#36890;&#36807;&#21033;&#29992;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#20943;&#23569;&#23427;&#20204;&#30340;&#24369;&#28857;&#26469;&#29983;&#25104;&#25913;&#36827;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#20419;&#36827;&#22823;&#35268;&#27169;&#35780;&#20272;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;MixInstruct&#65292;&#23427;&#26159;&#22810;&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#28151;&#21512;&#65292;&#20855;&#26377;oracle p&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36882;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26680;&#22343;&#20540;&#23884;&#20837;&#22312;&#21407;&#23376;&#27169;&#25311;&#20013;&#23398;&#20064;&#20102;&#21183;&#33021;&#34920;&#38754;&#12290;&#35813;&#26041;&#27861;&#22312;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#23637;&#29616;&#20986;&#36739;&#22909;&#30340;&#21487;&#27010;&#25324;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01589</link><description>&lt;p&gt;
&#22522;&#20110;GNN&#21644;&#26680;&#22343;&#20540;&#23884;&#20837;&#30340;&#21407;&#23376;&#27169;&#25311;&#20256;&#36882;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer learning for atomistic simulations using GNNs and kernel mean embeddings. (arXiv:2306.01589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36882;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26680;&#22343;&#20540;&#23884;&#20837;&#22312;&#21407;&#23376;&#27169;&#25311;&#20013;&#23398;&#20064;&#20102;&#21183;&#33021;&#34920;&#38754;&#12290;&#35813;&#26041;&#27861;&#22312;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#23637;&#29616;&#20986;&#36739;&#22909;&#30340;&#21487;&#27010;&#25324;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#30340;&#21407;&#23376;&#30456;&#20114;&#20316;&#29992;&#21183;&#22312;&#21407;&#23376;&#27169;&#25311;&#20013;&#24471;&#21040;&#20102;&#25104;&#21151;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#31649;&#36947;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#65292;&#32780;&#29983;&#25104;&#21442;&#32771;&#35745;&#31639;&#26159;&#35745;&#31639;&#19978;&#35201;&#27714;&#24456;&#39640;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36882;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#25551;&#36848;&#21270;&#23398;&#29615;&#22659;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#26680;&#22343;&#20540;&#23884;&#20837;&#12290;&#25105;&#20204;&#20174;&#39044;&#20808;&#22312;OC20&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#36807;&#35757;&#32451;&#30340;GNN&#20013;&#25552;&#21462;&#29305;&#24449;&#26144;&#23556;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#20174;&#20652;&#21270;&#36807;&#31243;&#30340;&#31995;&#32479;&#29305;&#23450;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21183;&#33021;&#34920;&#38754;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#19968;&#27493;&#36890;&#36807;&#28789;&#27963;&#30340;&#26680;&#20989;&#25968;&#26469;&#22686;&#24378;&#65292;&#35813;&#26680;&#20989;&#25968;&#21253;&#25324;&#21270;&#23398;&#29289;&#31181;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#36880;&#28176;&#22797;&#26434;&#30340;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27010;&#25324;&#33021;&#21147;&#21644;&#21487;&#36716;&#31227;&#24615;&#33021;&#65292;&#25913;&#36827;&#20102;&#20381;&#36182;GNNs&#25110;&#23725;&#22238;&#24402;&#26041;&#27861;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interatomic potentials learned using machine learning methods have been successfully applied to atomistic simulations. However, deep learning pipelines are notoriously data-hungry, while generating reference calculations is computationally demanding. To overcome this difficulty, we propose a transfer learning algorithm that leverages the ability of graph neural networks (GNNs) in describing chemical environments, together with kernel mean embeddings. We extract a feature map from GNNs pre-trained on the OC20 dataset and use it to learn the potential energy surface from system-specific datasets of catalytic processes. Our method is further enhanced by a flexible kernel function that incorporates chemical species information, resulting in improved performance and interpretability. We test our approach on a series of realistic datasets of increasing complexity, showing excellent generalization and transferability performance, and improving on methods that rely on GNNs or ridge regression 
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#25552;&#20379;&#31163;&#32447;&#35757;&#32451;&#21644;&#22312;&#32447;&#27979;&#35797;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#31995;&#32479;&#65292;&#36890;&#36807;&#20849;&#20139;&#26426;&#22120;&#20154;&#30828;&#20214;&#21644;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26041;&#20415;&#21644;&#30452;&#25509;&#30340;&#27604;&#36739;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00942</link><description>&lt;p&gt;
&#31163;&#32447;&#35757;&#32451;&#65292;&#22312;&#32447;&#27979;&#35797;&#65306;&#19968;&#20010;&#30495;&#23454;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Train Offline, Test Online: A Real Robot Learning Benchmark. (arXiv:2306.00942v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00942
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#25552;&#20379;&#31163;&#32447;&#35757;&#32451;&#21644;&#22312;&#32447;&#27979;&#35797;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#31995;&#32479;&#65292;&#36890;&#36807;&#20849;&#20139;&#26426;&#22120;&#20154;&#30828;&#20214;&#21644;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26041;&#20415;&#21644;&#30452;&#25509;&#30340;&#27604;&#36739;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#30740;&#31350;&#21463;&#21040;&#19977;&#20010;&#25361;&#25112;&#30340;&#38480;&#21046;&#65306;&#26426;&#22120;&#20154;&#26114;&#36149;&#65288;&#24456;&#23569;&#26377;&#23454;&#39564;&#23460;&#21487;&#20197;&#21442;&#19982;&#65289;&#65292;&#27599;&#20010;&#20154;&#20351;&#29992;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#65288;&#30740;&#31350;&#32467;&#26524;&#22312;&#23454;&#39564;&#23460;&#20043;&#38388;&#19981;&#20855;&#26377;&#26222;&#36941;&#24615;&#65289;&#65292;&#25105;&#20204;&#32570;&#20047;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#12290;&#36890;&#36807;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#31995;&#32479;&#65306;&#31163;&#32447;&#35757;&#32451;&#12289;&#22312;&#32447;&#27979;&#35797;&#65288;TOTO&#65289;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;TOTO&#20026;&#36828;&#31243;&#29992;&#25143;&#25552;&#20379;&#20849;&#20139;&#26426;&#22120;&#20154;&#30828;&#20214;&#20197;&#35780;&#20272;&#24120;&#35265;&#20219;&#21153;&#19978;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#36825;&#20123;&#20219;&#21153;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#36827;&#34892;&#31163;&#32447;&#35757;&#32451;&#12290;&#23427;&#30340;&#25805;&#20316;&#20219;&#21153;&#22871;&#20214;&#38656;&#35201;&#22312;&#30475;&#19981;&#35265;&#30340;&#23545;&#35937;&#12289;&#20301;&#32622;&#21644;&#20809;&#29031;&#26465;&#20214;&#19979;&#36827;&#34892;&#25361;&#25112;&#24615;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;TOTO&#22312;&#20116;&#20010;&#26426;&#26500;&#36828;&#31243;&#36129;&#29486;&#30340;&#65292;&#27604;&#36739;&#20102;&#20116;&#20010;&#39044;&#35757;&#32451;&#35270;&#35273;&#34920;&#31034;&#21644;&#22235;&#20010;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#22522;&#32447;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;TOTO&#30495;&#27491;&#30340;&#28508;&#21147;&#22312;&#20110;&#26410;&#26469;&#65306;&#25105;&#20204;&#21457;&#24067;&#20102;&#36825;&#20010;&#22522;&#20934;&#31995;&#32479;&#65292;&#20219;&#20309;&#29992;&#25143;&#37117;&#21487;&#20197;&#25552;&#20132;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#36731;&#26494;&#22320;&#30452;&#25509;&#19982;&#22810;&#31181;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#26080;&#38656;&#33719;&#24471;&#30828;&#20214;&#25110;&#25910;&#38598;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Three challenges limit the progress of robot learning research: robots are expensive (few labs can participate), everyone uses different robots (findings do not generalize across labs), and we lack internet-scale robotics data. We take on these challenges via a new benchmark: Train Offline, Test Online (TOTO). TOTO provides remote users with access to shared robotic hardware for evaluating methods on common tasks and an open-source dataset of these tasks for offline training. Its manipulation task suite requires challenging generalization to unseen objects, positions, and lighting. We present initial results on TOTO comparing five pretrained visual representations and four offline policy learning baselines, remotely contributed by five institutions. The real promise of TOTO, however, lies in the future: we release the benchmark for additional submissions from any user, enabling easy, direct comparison to several methods without the need to obtain hardware or collect data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#21333;&#20010;&#24555;&#29031;&#20013;&#37325;&#24314;&#22270;&#25193;&#25955;&#21382;&#21490;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00488</link><description>&lt;p&gt;
&#20174;&#21333;&#20010;&#24555;&#29031;&#20013;&#37325;&#24314;&#22270;&#25193;&#25955;&#21382;&#21490;
&lt;/p&gt;
&lt;p&gt;
Reconstructing Graph Diffusion History from a Single Snapshot. (arXiv:2306.00488v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#21333;&#20010;&#24555;&#29031;&#20013;&#37325;&#24314;&#22270;&#25193;&#25955;&#21382;&#21490;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25193;&#25955;&#22312;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#23436;&#25972;&#30340;&#25193;&#25955;&#21382;&#21490;&#22312;&#30830;&#23450;&#21160;&#24577;&#27169;&#24335;&#12289;&#21453;&#24605;&#39044;&#38450;&#25514;&#26045;&#21644;&#39044;&#27979;&#24178;&#39044;&#25928;&#26524;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#37325;&#35201;&#65292;&#20294;&#23436;&#25972;&#30340;&#25193;&#25955;&#21382;&#21490;&#24456;&#23569;&#21487;&#29992;&#65292;&#24182;&#19988;&#30001;&#20110;&#30149;&#24577;&#12289;&#29190;&#28856;&#24615;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#32570;&#20047;&#32780;&#20855;&#26377;&#26497;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#29992;&#20110;&#25193;&#25955;&#21382;&#21490;&#37325;&#24314;&#12290;&#23427;&#20204;&#20165;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#20844;&#24335;&#65292;&#38656;&#35201;&#30693;&#36947;&#30495;&#23454;&#30340;&#25193;&#25955;&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#26356;&#38590;&#30340;&#38382;&#39064;&#65292;&#21363;&#20174;&#21333;&#20010;&#24555;&#29031;&#65288;DASH&#65289;&#20013;&#37325;&#24314;&#25193;&#25955;&#21382;&#21490;&#65292;&#25105;&#20204;&#35797;&#22270;&#20165;&#20174;&#26368;&#32456;&#24555;&#29031;&#37325;&#24314;&#21382;&#21490;&#65292;&#32780;&#19981;&#30693;&#36947;&#30495;&#23454;&#30340;&#25193;&#25955;&#21442;&#25968;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#20998;&#26512;&#24320;&#22987;&#65292;&#25581;&#31034;&#20102;MLE&#20844;&#24335;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion on graphs is ubiquitous with numerous high-impact applications. In these applications, complete diffusion histories play an essential role in terms of identifying dynamical patterns, reflecting on precaution actions, and forecasting intervention effects. Despite their importance, complete diffusion histories are rarely available and are highly challenging to reconstruct due to ill-posedness, explosive search space, and scarcity of training data. To date, few methods exist for diffusion history reconstruction. They are exclusively based on the maximum likelihood estimation (MLE) formulation and require to know true diffusion parameters. In this paper, we study an even harder problem, namely reconstructing Diffusion history from A single SnapsHot} (DASH), where we seek to reconstruct the history from only the final snapshot without knowing true diffusion parameters. We start with theoretical analyses that reveal a fundamental limitation of the MLE formulation. We prove: (a) est
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#39640;&#32500;&#27169;&#22411;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#34920;&#28857;&#36873;&#25321;&#26469;&#35299;&#37322;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36866;&#29992;&#20110;&#21508;&#31181;&#27491;&#21017;&#21270;&#27169;&#22411;&#65292;&#24182;&#22312;&#21327;&#21516;&#36807;&#28388;&#39046;&#22495;&#20013;&#26377;&#20855;&#20307;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.20002</link><description>&lt;p&gt;
&#35299;&#37322;&#27491;&#21017;&#21270;&#30340;&#39640;&#32500;&#27169;&#22411;&#30340;&#20195;&#34920;&#28857;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Representer Point Selection for Explaining Regularized High-dimensional Models. (arXiv:2305.20002v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.20002
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#39640;&#32500;&#27169;&#22411;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#34920;&#28857;&#36873;&#25321;&#26469;&#35299;&#37322;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36866;&#29992;&#20110;&#21508;&#31181;&#27491;&#21017;&#21270;&#27169;&#22411;&#65292;&#24182;&#22312;&#21327;&#21516;&#36807;&#28388;&#39046;&#22495;&#20013;&#26377;&#20855;&#20307;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26679;&#26412;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#31216;&#20026;&#39640;&#32500;&#20195;&#34920;&#28857;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#37322;&#27491;&#21017;&#21270;&#30340;&#39640;&#32500;&#27169;&#22411;&#23545;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#19968;&#33324;&#27491;&#21017;&#21270;&#39640;&#32500;&#27169;&#22411;&#30340;&#26032;&#22411;&#20195;&#34920;&#23450;&#29702;&#65292;&#35813;&#23450;&#29702;&#23558;&#27169;&#22411;&#30340;&#39044;&#27979;&#20998;&#35299;&#20026;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#36129;&#29486;&#65306;&#27491;&#65288;&#36127;&#65289;&#20540;&#23545;&#24212;&#20110;&#27491;&#65288;&#36127;&#65289;&#24433;&#21709;&#35757;&#32451;&#26679;&#26412;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;$\ell_1$&#27491;&#21017;&#21270;&#31232;&#30095;&#27169;&#22411;&#21644;&#26680;&#33539;&#25968;&#27491;&#21017;&#21270;&#20302;&#31209;&#27169;&#22411;&#30340;&#32463;&#20856;&#23454;&#20363;&#30340;&#32467;&#26524;&#12290;&#20316;&#20026;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#22312;&#21327;&#21516;&#36807;&#28388;&#30340;&#32972;&#26223;&#19979;&#20302;&#31209;&#27169;&#22411;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#25105;&#20204;&#20026;&#29305;&#23450;&#30340;&#27969;&#34892;&#27169;&#22411;&#31867;&#23454;&#20363;&#21270;&#20102;&#39640;&#32500;&#20195;&#34920;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#23454;&#38469;&#30340;&#20108;&#36827;&#21046;&#20998;&#31867;&#38382;&#39064;&#19978;&#30740;&#31350;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#23454;&#35777;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel class of sample-based explanations we term high-dimensional representers, that can be used to explain the predictions of a regularized high-dimensional model in terms of importance weights for each of the training samples. Our workhorse is a novel representer theorem for general regularized high-dimensional models, which decomposes the model prediction in terms of contributions from each of the training samples: with positive (negative) values corresponding to positive (negative) impact training samples to the model's prediction. We derive consequences for the canonical instances of $\ell_1$ regularized sparse models, and nuclear norm regularized low-rank models. As a case study, we further investigate the application of low-rank models in the context of collaborative filtering, where we instantiate high-dimensional representers for specific popular classes of models. Finally, we study the empirical performance of our proposed methods on three real-world binary cla
&lt;/p&gt;</description></item><item><title>W-procer&#26159;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18624</link><description>&lt;p&gt;
W-procer: &#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18624
&lt;/p&gt;
&lt;p&gt;
W-procer&#26159;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#19968;&#31181;&#21463;&#27426;&#36814;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20256;&#32479;&#37197;&#32622;&#21147;&#27714;&#20943;&#23569;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#22686;&#21152;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#23384;&#22312;&#22823;&#37327;&#34987;&#27880;&#37322;&#20026;&#8220;O&#8221;&#65288;&#21363;&#8220;OUTSIDE&#8221;&#65289;&#30340;&#23454;&#20307;&#65292;&#24182;&#19988;&#23427;&#20204;&#19981;&#24076;&#26395;&#34987;&#25512;&#31163;&#21040;&#24403;&#21069;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26631;&#35760;&#20026;&#8220;O&#8221;&#20197;&#22806;&#30340;&#20854;&#20182;&#23454;&#20307;&#65292;&#36825;&#31181;&#35774;&#23450;&#25928;&#26524;&#19981;&#20339;&#65292;&#21487;&#33021;&#20250;&#24471;&#20986;&#21547;&#26377;&#22122;&#22768;&#21407;&#22411;&#26631;&#31614;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#8220;O&#8221;&#26631;&#31614;&#23454;&#20307;&#19982;&#26377;&#26631;&#31614;&#23454;&#20307;&#30456;&#20851;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;W-PROCER&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#22260;&#32469;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#23637;&#24320;&#12290;&#36825;&#20123;&#32452;&#20214;&#22312;&#21327;&#21161;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;W-PROCER&#24212;&#29992;&#20110;&#19968;&#20010;&#20844;&#20849;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has become a popular solution for few-shot Name Entity Recognization (NER). The conventional configuration strives to reduce the distance between tokens with the same labels and increase the distance between tokens with different labels. The effect of this setup may, however, in the medical domain, there are a lot of entities annotated as OUTSIDE (O), and they are undesirably pushed apart to other entities that are not labeled as OUTSIDE (O) by the current contrastive learning method end up with a noisy prototype for the semantic representation of the label, though there are many OUTSIDE (O) labeled entities are relevant to the labeled entities. To address this challenge, we propose a novel method named Weighted Prototypical Contrastive Learning for Medical Few Shot Named Entity Recognization (W-PROCER). Our approach primarily revolves around constructing the prototype-based contractive loss and weighting network. These components play a crucial role in assisting t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;1&#23618;Transformer&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65292;&#20174;&#32780;&#36880;&#27493;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24635;&#32467;&#30456;&#20851;&#20449;&#24687;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#21516;&#26102;&#30740;&#31350;&#20102;&#26631;&#35760;&#39057;&#29575;&#12289;&#19978;&#19979;&#25991;&#21644;&#21021;&#22987;&#21270;&#33258;&#25105;&#20851;&#27880;&#23618;&#31561;&#23545;Transformer&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.16380</link><description>&lt;p&gt;
&#25195;&#25551;&#19982;&#25293;&#29031;&#65306;&#29702;&#35299;1&#23618;Transformer&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26631;&#35760;&#32452;&#25104;
&lt;/p&gt;
&lt;p&gt;
Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;1&#23618;Transformer&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65292;&#20174;&#32780;&#36880;&#27493;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24635;&#32467;&#30456;&#20851;&#20449;&#24687;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#21516;&#26102;&#30740;&#31350;&#20102;&#26631;&#35760;&#39057;&#29575;&#12289;&#19978;&#19979;&#25991;&#21644;&#21021;&#22987;&#21270;&#33258;&#25105;&#20851;&#27880;&#23618;&#31561;&#23545;Transformer&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#22312;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#20026;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20854;&#22914;&#20309;&#24037;&#20316;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#39044;&#27979;&#24615;&#25439;&#22833;&#65292;&#34920;&#31034;&#22914;&#20309;&#20174;&#26799;&#24230;&#35757;&#32451;&#21160;&#24577;&#20013;&#20986;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#38024;&#23545;&#20855;&#26377;&#19968;&#20010;&#33258;&#25105;&#20851;&#27880;&#23618;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;1&#23618;Transformer&#65292;&#25105;&#20204;&#20197;&#25968;&#23398;&#20005;&#35880;&#30340;&#26041;&#24335;&#20998;&#26512;&#20854;&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#12290;&#25105;&#20204;&#25171;&#24320;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#32452;&#21512;&#36755;&#20837;&#26631;&#35760;&#30340;&#21160;&#24577;&#36807;&#31243;&#30340;&#40657;&#30418;&#23376;&#65292;&#24182;&#25581;&#31034;&#20102;&#24213;&#23618;&#24402;&#32435;&#20559;&#24046;&#30340;&#26412;&#36136;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#12289;&#38271;&#36755;&#20837;&#24207;&#21015;&#21644;&#35299;&#30721;&#22120;&#23618;&#23398;&#20064;&#36895;&#24230;&#24555;&#20110;&#33258;&#25105;&#20851;&#27880;&#23618;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65306;&#20174;&#22343;&#21248;&#27880;&#24847;&#21147;&#24320;&#22987;&#65292;&#23427;&#36880;&#28176;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#65292;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#30452;&#21040;&#25152;&#26377;&#30456;&#20851;&#20449;&#24687;&#34987;&#25195;&#25551;&#24182;&#24635;&#32467;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#26174;&#31034;&#20102;&#26631;&#35760;&#39057;&#29575;&#21644;&#19978;&#19979;&#25991;&#22914;&#20309;&#24433;&#21709;&#27880;&#24847;&#26435;&#37325;&#65292;&#20197;&#21450;&#33258;&#25105;&#20851;&#27880;&#23618;&#21021;&#22987;&#21270;&#22914;&#20309;&#24433;&#21709;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends mor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Feature Heterogeneity Distance(FHD)&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#39046;&#22495;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#27169;&#24335;Contrastive Convergence for Domain Generalization (CCDG) &#26469;&#23547;&#25214;&#26368;&#20339;&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#25552;&#39640;&#27867;&#21270;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#21644;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2305.15889</link><description>&lt;p&gt;
&#38024;&#23545;&#39046;&#22495;&#27867;&#21270;&#30340;&#24322;&#36136;&#24615;&#37327;&#21270;&#21644;&#23545;&#27604;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Quantitatively Measuring and Contrastively Exploring Heterogeneity for Domain Generalization. (arXiv:2305.15889v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Feature Heterogeneity Distance(FHD)&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#39046;&#22495;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#27169;&#24335;Contrastive Convergence for Domain Generalization (CCDG) &#26469;&#23547;&#25214;&#26368;&#20339;&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#25552;&#39640;&#27867;&#21270;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#21644;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;(DG)&#26159;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#20960;&#20010;&#28304;&#22495;&#35757;&#32451;&#20986;&#23545;&#26410;&#35265;&#36807;&#30446;&#26631;&#22495;&#36827;&#34892;&#26377;&#25928;&#27867;&#21270;&#30340;&#27169;&#22411;&#12290;&#30001;&#20110;&#39046;&#22495;&#26631;&#31614;-&#21363;&#27599;&#20010;&#25968;&#25454;&#28857;&#26469;&#33258;&#21738;&#20010;&#22495;&#33258;&#28982;&#23384;&#22312;&#65292;&#22240;&#27492;&#22823;&#22810;&#25968;DG&#31639;&#27861;&#23558;&#23427;&#20204;&#20316;&#20026;&#19968;&#31181;&#30417;&#30563;&#20449;&#24687;&#26469;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39046;&#22495;&#20043;&#38388;&#32570;&#20047;&#24322;&#36136;&#24615;&#65292;&#21363;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21407;&#22987;&#30340;&#22495;&#26631;&#31614;&#21487;&#33021;&#19981;&#26159;&#26368;&#20339;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;Feature Heterogeneity Distance(FHD)&#26469;&#34913;&#37327;&#39046;&#22495;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#27169;&#24335;CCDG&#65292;&#29992;&#20110;&#23547;&#25214;&#26368;&#20339;&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#25552;&#39640;&#27867;&#21270;&#12290;&#22823;&#37327;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;FHD&#24230;&#37327;&#26631;&#20934;&#21644;CCDG&#27169;&#24335;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#21644;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization (DG) is a prevalent problem in real-world applications, which aims to train well-generalized models for unseen target domains by utilizing several source domains. Since domain labels, i.e., which domain each data point is sampled from, naturally exist, most DG algorithms treat them as a kind of supervision information to improve the generalization performance. However, the original domain labels may not be the optimal supervision signal due to the lack of domain heterogeneity, i.e., the diversity among domains. For example, a sample in one domain may be closer to another domain, its original label thus can be the noise to disturb the generalization learning. Although some methods try to solve it by re-dividing domains and applying the newly generated dividing pattern, the pattern they choose may not be the most heterogeneous due to the lack of the metric for heterogeneity. In this paper, we point out that domain heterogeneity mainly lies in variant features under 
&lt;/p&gt;</description></item><item><title>GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#26159;&#30446;&#21069;&#24191;&#27867;&#36941;&#24067;&#19988;&#21253;&#21547;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#30340;&#19968;&#31449;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20840;&#38754;&#28085;&#30422;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13681</link><description>&lt;p&gt;
GUARD: &#19968;&#20010;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
GUARD: A Safe Reinforcement Learning Benchmark. (arXiv:2305.13681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13681
&lt;/p&gt;
&lt;p&gt;
GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#26159;&#30446;&#21069;&#24191;&#27867;&#36941;&#24067;&#19988;&#21253;&#21547;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#30340;&#19968;&#31449;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20840;&#38754;&#28085;&#30422;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35797;&#38169;&#30340;&#24615;&#36136;&#65292;&#23558;RL&#31639;&#27861;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#29616;&#23454;&#24212;&#29992;&#65288;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#20154;&#26426;&#20132;&#20114;&#12289;&#26426;&#22120;&#20154;&#25805;&#20316;&#31561;&#65289;&#36890;&#24120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#38169;&#35823;&#26159;&#19981;&#21487;&#23481;&#24525;&#30340;&#12290;&#26368;&#36817;&#65292;&#23433;&#20840;RL&#65288;&#21363;&#32422;&#26463;RL&#65289;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#36805;&#36895;&#20986;&#29616;&#65292;&#20854;&#20013;&#20195;&#29702;&#22312;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#30340;&#21516;&#26102;&#65292;&#25506;&#32034;&#29615;&#22659;&#12290;&#30001;&#20110;&#31639;&#27861;&#21644;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#65292;&#27604;&#36739;&#29616;&#26377;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GUARD&#65292;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#30456;&#27604;&#65292;GUARD&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#20855;&#26377;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#12290;&#20854;&#27425;&#65292;GUARD&#20840;&#38754;&#28085;&#30422;&#20102;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#33258;&#21253;&#21547;&#30340;&#23454;&#29616;&#12290;&#31532;&#19977;&#65292;GUARD&#22312;&#20219;&#21153;&#21644;&#31639;&#27861;&#26041;&#38754;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29366;&#24577;&#19979;&#29616;&#26377;&#26041;&#27861;&#22312;GUARD&#19978;&#30340;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the trial-and-error nature, it is typically challenging to apply RL algorithms to safety-critical real-world applications, such as autonomous driving, human-robot interaction, robot manipulation, etc, where such errors are not tolerable. Recently, safe RL (i.e. constrained RL) has emerged rapidly in the literature, in which the agents explore the environment while satisfying constraints. Due to the diversity of algorithms and tasks, it remains difficult to compare existing safe RL algorithms. To fill that gap, we introduce GUARD, a Generalized Unified SAfe Reinforcement Learning Development Benchmark. GUARD has several advantages compared to existing benchmarks. First, GUARD is a generalized benchmark with a wide variety of RL agents, tasks, and safety constraint specifications. Second, GUARD comprehensively covers state-of-the-art safe RL algorithms with self-contained implementations. Third, GUARD is highly customizable in tasks and algorithms. We present a comparison of state
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#21644;&#22522;&#20110;&#23545;&#35937;&#32423;&#21035;&#30340;&#22270;&#20687;&#24674;&#22797;&#26041;&#27861;&#65292;&#21363;Restore Anything Pipeline (RAP)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#22270;&#20687;&#20998;&#21106;&#25216;&#26415;&#19982;&#21487;&#25511;&#30340;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20026;&#22810;&#20010;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.13093</link><description>&lt;p&gt;
Restore Anything Pipeline: Segment Anything Meets Image Restoration.
&lt;/p&gt;
&lt;p&gt;
Restore Anything Pipeline: Segment Anything Meets Image Restoration. (arXiv:2305.13093v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#21644;&#22522;&#20110;&#23545;&#35937;&#32423;&#21035;&#30340;&#22270;&#20687;&#24674;&#22797;&#26041;&#27861;&#65292;&#21363;Restore Anything Pipeline (RAP)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#22270;&#20687;&#20998;&#21106;&#25216;&#26415;&#19982;&#21487;&#25511;&#30340;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20026;&#22810;&#20010;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#22270;&#20687;&#24674;&#22797;&#26041;&#27861;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#23558;&#25972;&#20010;&#22270;&#20687;&#35270;&#20026;&#19968;&#20010;&#21333;&#19968;&#23454;&#20307;&#65292;&#26080;&#27861;&#32771;&#34385;&#21040;&#22270;&#20687;&#20013;&#34920;&#29616;&#20986;&#20010;&#20307;&#32441;&#29702;&#23646;&#24615;&#30340;&#19981;&#21516;&#23545;&#35937;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#29983;&#25104;&#21333;&#19968;&#32467;&#26524;&#65292;&#21487;&#33021;&#26080;&#27861;&#28385;&#36275;&#19981;&#21516;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#21644;&#22522;&#20110;&#23545;&#35937;&#32423;&#21035;&#30340;&#22270;&#20687;&#24674;&#22797;&#26041;&#27861;&#65292;&#21363;Restore Anything Pipeline (RAP)&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#21487;&#25511;&#27169;&#22411;&#26469;&#29983;&#25104;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#30340;&#19981;&#21516;&#32467;&#26524;&#12290;RAP&#23558;&#26368;&#36817;&#30340;Segment Anything Model (SAM)&#30340;&#22270;&#20687;&#20998;&#21106;&#25216;&#26415;&#19982;&#21487;&#25511;&#30340;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20026;&#22810;&#20010;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#27969;&#31243;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;RAP&#24212;&#29992;&#20110;&#22270;&#20687;&#21435;&#27169;&#31946;&#12289;&#22270;&#20687;&#21435;&#22122;&#21644;JPEG&#20266;&#24433;&#21435;&#38500;&#36825;&#19977;&#20010;&#24120;&#35265;&#30340;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#26469;&#23637;&#31034;&#20854;&#22810;&#21151;&#33021;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RAP&#33021;&#22815;&#22312;&#19981;&#21516;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent image restoration methods have produced significant advancements using deep learning. However, existing methods tend to treat the whole image as a single entity, failing to account for the distinct objects in the image that exhibit individual texture properties. Existing methods also typically generate a single result, which may not suit the preferences of different users. In this paper, we introduce the Restore Anything Pipeline (RAP), a novel interactive and per-object level image restoration approach that incorporates a controllable model to generate different results that users may choose from. RAP incorporates image segmentation through the recent Segment Anything Model (SAM) into a controllable image restoration model to create a user-friendly pipeline for several image restoration tasks. We demonstrate the versatility of RAP by applying it to three common image restoration tasks: image deblurring, image denoising, and JPEG artifact removal. Our experiments show that RAP p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#38754;&#21521;&#19977;&#32500;MOT&#20013;&#30340;&#28857;&#20113;&#20877;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#21305;&#37197;&#22836;&#29992;&#20110;&#28857;&#20113;ReID&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#20256;&#24863;&#22120;&#20998;&#36776;&#29575;&#30340;&#25552;&#39640;&#21644;&#35266;&#27979;&#28857;&#23494;&#24230;&#30340;&#22686;&#21152;&#65292;&#28857;&#20113;ReID&#30340;&#34920;&#29616;&#36880;&#28176;&#25509;&#36817;&#20110;&#22270;&#20687;ReID&#12290;</title><link>http://arxiv.org/abs/2305.10210</link><description>&lt;p&gt;
&#38754;&#21521;&#19977;&#32500;MOT&#20013;&#30340;&#28857;&#20113;&#30446;&#26631;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Towards Object Re-Identification from Point Clouds for 3D MOT. (arXiv:2305.10210v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10210
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#38754;&#21521;&#19977;&#32500;MOT&#20013;&#30340;&#28857;&#20113;&#20877;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#21305;&#37197;&#22836;&#29992;&#20110;&#28857;&#20113;ReID&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#20256;&#24863;&#22120;&#20998;&#36776;&#29575;&#30340;&#25552;&#39640;&#21644;&#35266;&#27979;&#28857;&#23494;&#24230;&#30340;&#22686;&#21152;&#65292;&#28857;&#20113;ReID&#30340;&#34920;&#29616;&#36880;&#28176;&#25509;&#36817;&#20110;&#22270;&#20687;ReID&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#20174;&#21098;&#35009;&#30340;&#28857;&#20113;&#35266;&#27979;&#20013;&#21305;&#37197;&#23545;&#35937;&#23545;&#65288;&#20363;&#22914;&#20351;&#29992;&#20854;&#39044;&#27979;&#30340;&#19977;&#32500;&#36793;&#30028;&#26694;&#65289;&#26469;&#35299;&#20915;&#19977;&#32500;&#22810;&#30446;&#26631;&#36319;&#36394;&#65288;MOT&#65289;&#19978;&#30340;&#23545;&#35937;&#20877;&#35782;&#21035;&#65288;ReID&#65289;&#38382;&#39064;&#12290; &#25105;&#20204;&#19981;&#20851;&#24515;&#19977;&#32500;MOT&#30340;SOTA&#24615;&#33021;&#65292;&#32780;&#26159;&#36861;&#27714;&#22238;&#31572;&#20197;&#19979;&#38382;&#39064;&#65306;&#22312;&#23454;&#38469;&#30340;&#36319;&#36394;&#26816;&#27979;&#29615;&#22659;&#20013;&#65292;&#19982;&#22270;&#29255;&#20013;&#30340;ReID&#30456;&#27604;&#65292;&#26469;&#33258;&#28857;&#20113;&#30340;&#23545;&#35937;ReID&#30340;&#34920;&#29616;&#22914;&#20309;&#65311; &#20026;&#20102;&#23454;&#29616;&#36825;&#26679;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#36830;&#25509;&#21040;&#20219;&#20309;&#38598;&#21512;&#25110;&#24207;&#21015;&#22788;&#29702;&#39592;&#24178;&#65288;&#20363;&#22914;PointNet&#25110;ViT&#65289;&#30340;&#36731;&#37327;&#32423;&#21305;&#37197;&#22836;&#65292;&#20026;&#20004;&#31181;&#27169;&#24577;&#21019;&#36896;&#21487;&#27604;&#36739;&#30340;&#23545;&#35937;ReID&#32593;&#32476;&#23478;&#26063;&#12290;&#22312;&#23402;&#29983;&#26679;&#24335;&#19979;&#36816;&#34892;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#28857;&#20113;ReID&#32593;&#32476;&#21487;&#20197;&#22312;&#23454;&#26102;&#65288;10 hz&#65289;&#20013;&#36827;&#34892;&#25968;&#21315;&#20010;&#25104;&#23545;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20854;&#34920;&#29616;&#38543;&#30528;&#26356;&#39640;&#30340;&#20256;&#24863;&#22120;&#20998;&#36776;&#29575;&#32780;&#25552;&#39640;&#65292;&#24182;&#22312;&#35266;&#27979;&#36275;&#22815;&#23494;&#38598;&#26102;&#25509;&#36817;&#22270;&#20687;ReID&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the problem of object re-identification (ReID) in a 3D multi-object tracking (MOT) context, by learning to match pairs of objects from cropped (e.g., using their predicted 3D bounding boxes) point cloud observations. We are not concerned with SOTA performance for 3D MOT, however. Instead, we seek to answer the following question: In a realistic tracking by-detection context, how does object ReID from point clouds perform relative to ReID from images? To enable such a study, we propose a lightweight matching head that can be concatenated to any set or sequence processing backbone (e.g., PointNet or ViT), creating a family of comparable object ReID networks for both modalities. Run in siamese style, our proposed point-cloud ReID networks can make thousands of pairwise comparisons in real-time (10 hz). Our findings demonstrate that their performance increases with higher sensor resolution and approaches that of image ReID when observations are sufficiently dense. Ad
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#36153;&#31859;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#26041;&#27861;&#30740;&#31350;&#20102;&#22343;&#21248;&#36153;&#31859;&#27668;&#20307;&#36229;&#27969;&#65292;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;FermiNet&#27169;&#22411;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#33719;&#24471;&#20102;&#26497;&#20854;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.06989</link><description>&lt;p&gt;
&#36229;&#27969;&#20307;&#30340;&#31070;&#32463;&#27874;&#20989;&#25968;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neural Wave Functions for Superfluids. (arXiv:2305.06989v1 [cond-mat.quant-gas])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#36153;&#31859;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#26041;&#27861;&#30740;&#31350;&#20102;&#22343;&#21248;&#36153;&#31859;&#27668;&#20307;&#36229;&#27969;&#65292;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;FermiNet&#27169;&#22411;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#33719;&#24471;&#20102;&#26497;&#20854;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#36229;&#27969;&#24615;&#20173;&#28982;&#26159;&#20957;&#32858;&#24577;&#29289;&#29702;&#30340;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#24320;&#21457;&#30340;&#36153;&#31859;&#31070;&#32463;&#32593;&#32476;&#65288;FermiNet&#65289;&#27874;&#20989;&#25968;Ansatz&#36827;&#34892;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#35745;&#31639;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#20855;&#26377;&#24378;&#28872;&#30701;&#31243;&#21452;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#31995;&#32479;-- &#22343;&#21248;&#36153;&#31859;&#27668;&#20307;&#65292;&#35813;&#31995;&#32479;&#24050;&#30693;&#23384;&#22312;&#36229;&#27969;&#22522;&#24577;&#65292;&#20294;&#38590;&#20197;&#23450;&#37327;&#25551;&#36848;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#30740;&#31350;&#22343;&#21248;&#36153;&#31859;&#27668;&#20307;&#26102;FermiNet Ansatz&#30340;&#20851;&#38190;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20462;&#25913;&#65292;&#20854;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#21407;&#22987;FermiNet&#65292;&#21487;&#20197;&#32473;&#20986;&#39640;&#24230;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25968;&#23398;&#35777;&#26126;&#20102;&#26032;&#30340;Ansatz&#26159;&#21407;&#22987;FermiNet&#20307;&#31995;&#32467;&#26500;&#30340;&#20005;&#26684;&#27010;&#25324;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#21442;&#25968;&#26356;&#23569;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;FermiNet&#20849;&#20139;&#20960;&#20010;&#20248;&#21183;:&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#28040;&#38500;&#20102;&#24213;&#23618;&#22522;&#32452;&#30340;&#38656;&#27714;;&#32593;&#32476;&#30340;&#28789;&#27963;&#24615;&#22312;&#21464;&#20998;&#37327;&#23376;Monte Carlo&#20013;&#20135;&#29983;&#20102;&#26497;&#20854;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding superfluidity remains a major goal of condensed matter physics. Here we tackle this challenge utilizing the recently developed Fermionic neural network (FermiNet) wave function Ansatz for variational Monte Carlo calculations. We study the unitary Fermi gas, a system with strong, short-range, two-body interactions known to possess a superfluid ground state but difficult to describe quantitively. We demonstrate key limitations of the FermiNet Ansatz in studying the unitary Fermi gas and propose a simple modification that outperforms the original FermiNet significantly, giving highly accurate results. We prove mathematically that the new Ansatz is a strict generalization of the original FermiNet architecture, despite the use of fewer parameters. Our approach shares several advantanges with the FermiNet: the use of a neural network removes the need for an underlying basis set; and the flexiblity of the network yields extremely accurate results within a variational quantum Mon
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06569</link><description>&lt;p&gt;
&#22914;&#20309;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#32034;&#24341;&#39033;&#30446;ID
&lt;/p&gt;
&lt;p&gt;
How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#23558;&#25512;&#33616;&#20219;&#21153;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25512;&#33616;&#12290;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#24314;&#35758;&#30340;&#39033;&#30446;&#32780;&#19981;&#26159;&#35745;&#31639;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#20013;&#27599;&#20010;&#20505;&#36873;&#39033;&#30446;&#30340;&#25490;&#21517;&#24471;&#20998;&#65292;&#31616;&#21270;&#20102;&#25512;&#33616;&#31649;&#36947;&#65292;&#36991;&#20813;&#20102;&#22810;&#27573;&#36807;&#28388;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#20915;&#23450;&#35201;&#25512;&#33616;&#21738;&#20123;&#39033;&#30446;&#26102;&#29983;&#25104;&#36807;&#38271;&#30340;&#25991;&#26412;&#65292;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#21019;&#24314;LLM&#20860;&#23481;&#30340;&#39033;&#30446;ID&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#65292;&#20197;P5&#20026;&#20195;&#34920;&#30340;&#20027;&#24178;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32034;&#24341;&#26041;&#27861;&#22797;&#21046;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#20960;&#31181;&#24494;&#19981;&#36275;&#36947;&#30340;&#39033;&#30446;&#32034;&#24341;&#26041;&#27861;&#65288;&#22914;&#29420;&#31435;&#32034;&#24341;&#12289;&#26631;&#39064;&#32034;&#24341;&#21644;&#38543;&#26426;&#32034;&#24341;&#65289;&#30340;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#32034;&#24341;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#32034;&#24341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#24037;&#19994;&#12289;&#20513;&#23548;&#32452;&#32455;&#21644;&#27668;&#20505;&#20513;&#23548;&#32452;&#32455;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#22914;&#20309;&#24433;&#21709;&#27668;&#20505;&#21464;&#21270;&#30340;&#21465;&#20107;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#30417;&#30563;&#27169;&#22411;&#32452;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;Facebook&#19978;&#27668;&#20505;&#24191;&#21578;&#30340;&#31435;&#22330;&#12290;</title><link>http://arxiv.org/abs/2305.06174</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#27668;&#20505;&#23459;&#20256;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
Analysis of Climate Campaigns on Social Media using Bayesian Model Averaging. (arXiv:2305.06174v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#24037;&#19994;&#12289;&#20513;&#23548;&#32452;&#32455;&#21644;&#27668;&#20505;&#20513;&#23548;&#32452;&#32455;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#22914;&#20309;&#24433;&#21709;&#27668;&#20505;&#21464;&#21270;&#30340;&#21465;&#20107;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#30417;&#30563;&#27169;&#22411;&#32452;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;Facebook&#19978;&#27668;&#20505;&#24191;&#21578;&#30340;&#31435;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#21464;&#21270;&#26159;&#25105;&#20204;&#26102;&#20195;&#30340;&#26680;&#24515;&#38382;&#39064;&#65292;&#25105;&#20204;&#27491;&#22788;&#20110;&#19968;&#20010;&#20851;&#38190;&#26102;&#21051;&#12290;&#21508;&#31181;&#21033;&#30410;&#38598;&#22242;&#12289;&#31038;&#20250;&#36816;&#21160;&#32452;&#32455;&#21644;&#20010;&#20154;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#24320;&#23637;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#38598;&#20307;&#34892;&#21160;&#12290;&#27492;&#22806;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#38382;&#39064;&#20513;&#23548;&#27963;&#21160;&#24448;&#24448;&#26159;&#38024;&#23545;&#24403;&#21069;&#31038;&#20250;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#33021;&#28304;&#34892;&#19994;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#20998;&#26512;&#24037;&#19994;&#12289;&#20513;&#23548;&#32452;&#32455;&#21644;&#27668;&#20505;&#20513;&#23548;&#32452;&#32455;&#22914;&#20309;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#27668;&#20505;&#21464;&#21270;&#30340;&#21465;&#20107;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#30417;&#30563;&#27169;&#22411;&#32452;&#21512;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#28040;&#24687;&#20027;&#39064;&#26469;&#35782;&#21035;Facebook&#19978;&#27668;&#20505;&#24191;&#21578;&#30340;&#31435;&#22330;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#31435;&#22330;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#19982;&#27668;&#20505;&#23459;&#20256;&#27963;&#21160;&#30456;&#20851;&#30340;&#20027;&#39064;&#65292;&#20379;&#26410;&#26469;&#30340;&#33286;&#24773;&#25366;&#25496;&#21644;&#33258;&#21160;&#26816;&#27979;&#27668;&#20505;&#21464;&#21270;&#31435;&#22330;&#30340;&#30740;&#31350;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate change is the defining issue of our time, and we are at a defining moment. Various interest groups, social movement organizations, and individuals engage in collective action on this issue on social media. In addition, issue advocacy campaigns on social media often arise in response to ongoing societal concerns, especially those faced by energy industries. Our goal in this paper is to analyze how those industries, their advocacy group, and climate advocacy group use social media to influence the narrative on climate change. In this work, we propose a minimally supervised model soup [56] approach combined with messaging themes to identify the stances of climate ads on Facebook. Finally, we release our stance dataset, model, and set of themes related to climate campaigns for future work on opinion mining and the automatic detection of climate change stances.
&lt;/p&gt;</description></item><item><title>SAM&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#65288;AT&#65289;&#37117;&#21487;&#20197;&#35270;&#20026;&#29305;&#23450;&#30340;&#29305;&#24449;&#25200;&#21160;&#65292;&#20854;&#25913;&#21892;&#20102;&#23545;&#25239;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;SAM&#21644;AT&#22312;&#25200;&#21160;&#24378;&#24230;&#26041;&#38754;&#26159;&#19981;&#21516;&#30340;&#65292;&#20174;&#32780;&#24102;&#26469;&#20102;&#19981;&#21516;&#30340;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#26435;&#34913;&#12290;SAM&#21333;&#29420;&#20351;&#29992;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#28165;&#26224;&#24230;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05392</link><description>&lt;p&gt;
&#20851;&#20110;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#19982;&#23545;&#25239;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
On the Relation between Sharpness-Aware Minimization and Adversarial Robustness. (arXiv:2305.05392v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05392
&lt;/p&gt;
&lt;p&gt;
SAM&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#65288;AT&#65289;&#37117;&#21487;&#20197;&#35270;&#20026;&#29305;&#23450;&#30340;&#29305;&#24449;&#25200;&#21160;&#65292;&#20854;&#25913;&#21892;&#20102;&#23545;&#25239;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;SAM&#21644;AT&#22312;&#25200;&#21160;&#24378;&#24230;&#26041;&#38754;&#26159;&#19981;&#21516;&#30340;&#65292;&#20174;&#32780;&#24102;&#26469;&#20102;&#19981;&#21516;&#30340;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#26435;&#34913;&#12290;SAM&#21333;&#29420;&#20351;&#29992;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#28165;&#26224;&#24230;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#32972;&#26223;&#19979;&#25552;&#20986;&#20102;&#23545;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#65288;SAM&#65289;&#30340;&#26032;&#29702;&#35299;&#12290;&#26412;&#25991;&#25351;&#20986;&#65292;SAM&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#65288;AT&#65289;&#37117;&#21487;&#20197;&#35270;&#20026;&#29305;&#23450;&#30340;&#29305;&#24449;&#25200;&#21160;&#65292;&#20854;&#25913;&#21892;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;SAM&#21644;AT&#22312;&#25200;&#21160;&#24378;&#24230;&#26041;&#38754;&#26159;&#19981;&#21516;&#30340;&#65292;&#20174;&#32780;&#24102;&#26469;&#20102;&#19981;&#21516;&#30340;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#26435;&#34913;&#12290;&#22312;&#19968;&#20010;&#31616;&#21270;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#20123;&#22768;&#26126;&#30340;&#29702;&#35770;&#35777;&#25454;&#21644;&#20005;&#26684;&#30340;&#25968;&#23398;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#20165;&#21033;&#29992;SAM&#21487;&#20197;&#23454;&#29616;&#27604;&#26631;&#20934;&#35757;&#32451;&#26356;&#22909;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#36825;&#26159;&#24847;&#22806;&#30340;&#22909;&#22788;&#12290;&#30001;&#20110;&#23545;&#25239;&#35757;&#32451;&#21487;&#33021;&#20250;&#23548;&#33268;&#28165;&#26224;&#24230;&#31934;&#24230;&#30340;&#38477;&#20302;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#20351;&#29992;SAM&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#28165;&#26224;&#24230;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/weizeming/SAM_AT&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel understanding of Sharpness-Aware Minimization (SAM) in the context of adversarial robustness. In this paper, we point out that both SAM and adversarial training (AT) can be viewed as specific feature perturbations, which improve adversarial robustness. However, we note that SAM and AT are distinct in terms of perturbation strength, leading to different accuracy and robustness trade-offs. We provide theoretical evidence for these claims in a simplified model with rigorous mathematical proofs. Furthermore, we conduct experiment to demonstrate that only utilizing SAM can achieve superior adversarial robustness compared to standard training, which is an unexpected benefit. As adversarial training can suffer from a decrease in clean accuracy, we show that using SAM alone can improve robustness without sacrificing clean accuracy. Code is available at https://github.com/weizeming/SAM_AT.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#30693;&#35782;&#30340;&#12289;&#36793;&#30028;&#26465;&#20214;&#24863;&#30693;&#30340;&#12289;&#23616;&#37096;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;E2C&#21644;E2CO&#27169;&#22411;&#25193;&#23637;&#21040;&#23545;&#27969;&#25193;&#25955;&#21453;&#24212;&#31995;&#32479;&#20013;&#65292;&#21487;&#20197;&#20197;&#26497;&#39640;&#30340;&#20934;&#30830;&#24615;&#39044;&#27979;&#31995;&#32479;&#26410;&#26469;&#29366;&#24577;&#65292;&#21516;&#26102;&#22823;&#24133;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.03774</link><description>&lt;p&gt;
&#29289;&#29702;&#30693;&#35782;&#30340;&#23616;&#37096;&#21270;&#23398;&#20064;&#29992;&#20110;&#23545;&#27969;&#25193;&#25955;&#21453;&#24212;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Localized Learning for Advection-Diffusion-Reaction Systems. (arXiv:2305.03774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03774
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#30693;&#35782;&#30340;&#12289;&#36793;&#30028;&#26465;&#20214;&#24863;&#30693;&#30340;&#12289;&#23616;&#37096;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;E2C&#21644;E2CO&#27169;&#22411;&#25193;&#23637;&#21040;&#23545;&#27969;&#25193;&#25955;&#21453;&#24212;&#31995;&#32479;&#20013;&#65292;&#21487;&#20197;&#20197;&#26497;&#39640;&#30340;&#20934;&#30830;&#24615;&#39044;&#27979;&#31995;&#32479;&#26410;&#26469;&#29366;&#24577;&#65292;&#21516;&#26102;&#22823;&#24133;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#23545;&#26032;&#33021;&#28304;&#35299;&#20915;&#26041;&#26696;&#30340;&#36861;&#27714;&#65292;&#22914;&#22320;&#28909;&#21644;&#30899;&#25429;&#38598;&#21644;&#23553;&#23384;&#35745;&#21010;&#65292;&#23545;&#24403;&#21069;&#20808;&#36827;&#30340;&#22320;&#19979;&#27969;&#20307;&#27169;&#25311;&#22120;&#20135;&#29983;&#20102;&#26032;&#30340;&#38656;&#27714;&#12290;&#35201;&#27714;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#21516;&#26102;&#27169;&#25311;&#22823;&#37327;&#20648;&#23618;&#29366;&#24577;&#65292;&#20026;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20195;&#29702;&#24314;&#27169;&#24320;&#36767;&#20102;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23616;&#37096;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#29289;&#29702;&#30693;&#35782;&#21644;&#36793;&#30028;&#26465;&#20214;&#24847;&#35782;&#65292;&#23558;&#23884;&#20837;&#24335;&#25511;&#21046;&#65288;E2C&#65289;&#21644;&#23884;&#20837;&#24335;&#25511;&#21046;&#21644;&#35266;&#23519;&#65288;E2CO&#65289;&#27169;&#22411;&#25193;&#23637;&#21040;&#23398;&#20064;&#23545;&#27969;&#25193;&#25955;&#21453;&#24212;&#31995;&#32479;&#20013;&#20840;&#23616;&#29366;&#24577;&#21464;&#37327;&#30340;&#23616;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20648;&#23618;&#27169;&#25311;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#33021;&#22815;&#20197;&#21482;&#20351;&#29992;&#23569;&#37327;&#21487;&#29992;&#30340;&#20449;&#24687;&#65292;&#39640;&#24230;&#20934;&#30830;&#22320;&#39044;&#27979;&#31995;&#32479;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#21516;&#26102;&#19982;&#21407;&#22987;&#30340;E2C&#21644;E2C
&lt;/p&gt;
&lt;p&gt;
The global push for new energy solutions, such as Geothermal, and Carbon Capture and Sequestration initiatives has thrust new demands upon the current state-of the-art subsurface fluid simulators. The requirement to be able to simulate a large order of reservoir states simultaneously in a short period of time has opened the door of opportunity for the application of machine learning techniques for surrogate modelling. We propose a novel physics-informed and boundary conditions-aware Localized Learning method which extends the Embed-to-Control (E2C) and Embed-to-Control and Observed (E2CO) models to learn local representations of global state variables in an Advection-Diffusion Reaction system. We show that our model trained on reservoir simulation data is able to predict future states of the system, given a set of controls, to a great deal of accuracy with only a fraction of the available information, while also reducing training times significantly compared to the original E2C and E2C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BrainNPT&#30340;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#33041;&#21151;&#33021;&#32593;&#32476;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#26469;&#23398;&#20064;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.01666</link><description>&lt;p&gt;
BrainNPT&#65306;&#29992;&#20110;&#33041;&#32593;&#32476;&#20998;&#31867;&#30340;Transformer&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
BrainNPT: Pre-training of Transformer networks for brain network classification. (arXiv:2305.01666v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BrainNPT&#30340;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#33041;&#21151;&#33021;&#32593;&#32476;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#26469;&#23398;&#20064;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#33041;&#25104;&#20687;&#20998;&#26512;&#26041;&#38754;&#30340;&#36827;&#23637;&#36805;&#36895;&#65292;&#20294;&#24448;&#24448;&#21463;&#21040;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#24050;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#26377;&#21069;&#26223;&#30340;&#29305;&#24449;&#23398;&#20064;&#25913;&#36827;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#12290;&#28982;&#32780;&#65292;&#22312;&#33041;&#32593;&#32476;&#20998;&#26512;&#20013;&#65292;&#36825;&#31181;&#25216;&#26415;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;Transformer&#32593;&#32476;&#20026;&#22522;&#30784;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#20026;&#37325;&#28857;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#33041;&#21151;&#33021;&#32593;&#32476;&#20998;&#31867;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21517;&#20026;BrainNPT&#65292;&#29992;&#20110;&#33041;&#21151;&#33021;&#32593;&#32476;&#20998;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&lt;cls&gt;&#26631;&#35760;&#20316;&#20026;&#20998;&#31867;&#23884;&#20837;&#21521;&#37327;&#65292;&#20197;&#20415;&#20110;Transformer&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#33719;&#33041;&#32593;&#32476;&#30340;&#34920;&#31034;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#39044;&#35757;&#32451;&#26550;&#26500;&#65292;&#29992;&#20110;BrainNPT&#27169;&#22411;&#65292;&#20197;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#26469;&#23398;&#20064;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning methods have advanced quickly in brain imaging analysis over the past few years, but they are usually restricted by the limited labeled data. Pre-trained model on unlabeled data has presented promising improvement in feature learning in many domains, including natural language processing and computer vision. However, this technique is under-explored in brain network analysis. In this paper, we focused on pre-training methods with Transformer networks to leverage existing unlabeled data for brain functional network classification. First, we proposed a Transformer-based neural network, named as BrainNPT, for brain functional network classification. The proposed method leveraged &lt;cls&gt; token as a classification embedding vector for the Transformer model to effectively capture the representation of brain network. Second, We proposed a pre-training architecture with two pre-training strategies for BrainNPT model to leverage unlabeled brain network data to learn the structure in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312; COVID-19 &#30123;&#24773;&#26399;&#38388;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21475;&#32617;&#20329;&#25140;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#65292;&#36873;&#25321;&#20102;&#36866;&#29992;&#20110;&#23454;&#26102;&#21644;&#31227;&#21160;&#35774;&#22791;&#24212;&#29992;&#30340;&#26368;&#20339;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.00068</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21475;&#32617;&#20329;&#25140;&#26816;&#27979;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Wearing face mask detection using deep learning through COVID-19 pandemic. (arXiv:2305.00068v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312; COVID-19 &#30123;&#24773;&#26399;&#38388;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21475;&#32617;&#20329;&#25140;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#65292;&#36873;&#25321;&#20102;&#36866;&#29992;&#20110;&#23454;&#26102;&#21644;&#31227;&#21160;&#35774;&#22791;&#24212;&#29992;&#30340;&#26368;&#20339;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;COVID-19&#30123;&#24773;&#26399;&#38388;&#65292;&#20329;&#25140;&#21475;&#32617;&#24050;&#34987;&#30693;&#26195;&#20026;&#39044;&#38450;&#30149;&#27602;&#20256;&#25773;&#30340;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20986;&#33394;&#24615;&#33021;&#21462;&#20195;&#20102;&#20154;&#31867;&#22312;&#35768;&#22810;&#30417;&#25511;&#20219;&#21153;&#20013;&#30340;&#35282;&#33394;&#12290;&#30417;&#25511;&#21475;&#32617;&#20329;&#25140;&#23601;&#26159;&#36825;&#26679;&#19968;&#39033;&#21487;&#20197;&#30001;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23436;&#25104;&#30340;&#20219;&#21153;&#12290; &#30001;&#20110;&#38548;&#31163;&#30340;&#21407;&#22240;&#65292;&#38754;&#37096;&#21475;&#32617;&#29031;&#29255;&#30340;&#25968;&#37327;&#26377;&#38480;&#65292;&#22240;&#27492;&#26159;&#36825;&#39033;&#20219;&#21153;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#20351;&#29992;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#26816;&#27979;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#21475;&#32617;&#26816;&#27979;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21253;&#25324; Single Shot Detector&#65288;SSD&#65289;&#12289;&#20004;&#20010;&#29256;&#26412;&#30340; You Only Look Once&#12290;&#26681;&#25454;&#19981;&#21516;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#22312;&#29616;&#23454;&#19990;&#30028;&#21644;&#31227;&#21160;&#35774;&#22791;&#24212;&#29992;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#29992;&#20110;&#23454;&#26102;&#21644;&#31227;&#21160;&#35774;&#22791;&#24212;&#29992;&#30340;&#39640;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
During the COVID-19 pandemic, wearing a face mask has been known to be an effective way to prevent the spread of COVID-19. In lots of monitoring tasks, humans have been replaced with computers thanks to the outstanding performance of the deep learning models. Monitoring the wearing of a face mask is another task that can be done by deep learning models with acceptable accuracy. The main challenge of this task is the limited amount of data because of the quarantine. In this paper, we did an investigation on the capability of three state-of-the-art object detection neural networks on face mask detection for real-time applications. As mentioned, here are three models used, Single Shot Detector (SSD), two versions of You Only Look Once (YOLO) i.e., YOLOv4-tiny, and YOLOv4-tiny-3l from which the best was selected. In the proposed method, according to the performance of different models, the best model that can be suitable for use in real-world and mobile device applications in comparison to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21407;&#23376;&#25991;&#20214;&#30340;&#32479;&#19968;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#23384;&#20648;&#26684;&#24335;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;LibCity&#30340;&#24320;&#28304;&#24211;&#65292;&#37325;&#26032;&#26500;&#24314;&#20102;65&#20010;&#31354;&#38388;&#26102;&#38388;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#25910;&#38598;&#20102;55&#20010;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#22522;&#20934;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2304.14343</link><description>&lt;p&gt;
&#23454;&#29616;&#39640;&#25928;&#21644;&#20840;&#38754;&#30340;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#24211;&#21644;&#24615;&#33021;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark. (arXiv:2304.14343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21407;&#23376;&#25991;&#20214;&#30340;&#32479;&#19968;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#23384;&#20648;&#26684;&#24335;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;LibCity&#30340;&#24320;&#28304;&#24211;&#65292;&#37325;&#26032;&#26500;&#24314;&#20102;65&#20010;&#31354;&#38388;&#26102;&#38388;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#25910;&#38598;&#20102;55&#20010;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#22522;&#20934;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#19981;&#26029;&#25512;&#36827;&#21644;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#30340;&#31215;&#32047;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#39046;&#22495;&#23384;&#22312;&#35768;&#22810;&#38480;&#21046;&#65292;&#21253;&#25324;&#24320;&#25918;&#25968;&#25454;&#20197;&#21508;&#31181;&#26684;&#24335;&#23384;&#22312;&#65292;&#20351;&#29992;&#22256;&#38590;&#65292;&#26497;&#23569;&#25968;&#35770;&#25991;&#20844;&#24320;&#20854;&#20195;&#30721;&#21644;&#25968;&#25454;&#65292;&#20197;&#21450;&#24320;&#28304;&#27169;&#22411;&#32463;&#24120;&#20351;&#29992;&#19981;&#21516;&#30340;&#26694;&#26550;&#21644;&#24179;&#21488;&#65292;&#20351;&#24471;&#27604;&#36739;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36843;&#20999;&#38656;&#35201;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#23454;&#26045;&#21644;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#30340;&#32508;&#21512;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21407;&#23376;&#25991;&#20214;&#30340;&#32479;&#19968;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#23384;&#20648;&#26684;&#24335;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LibCity&#30340;&#24320;&#28304;&#24211;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#23454;&#39564;&#24037;&#20855;&#21644;&#19968;&#20010;&#26041;&#20415;&#30340;&#24320;&#21457;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#24211;&#20013;&#65292;&#25105;&#20204;&#24050;&#32463;&#37325;&#26032;&#26500;&#24314;&#20102;65&#20010;&#31354;&#38388;&#26102;&#38388;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#25910;&#38598;&#20102;55&#20010;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#22522;&#20934;&#65292;&#21253;&#25324;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#24230;&#37327;&#65292;&#20197;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#32479;&#19968;&#24211;&#21644;&#22522;&#20934;&#30340;&#26377;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning technology advances and more urban spatial-temporal data accumulates, an increasing number of deep learning models are being proposed to solve urban spatial-temporal prediction problems. However, there are limitations in the existing field, including open-source data being in various formats and difficult to use, few papers making their code and data openly available, and open-source models often using different frameworks and platforms, making comparisons challenging. A standardized framework is urgently needed to implement and evaluate these methods. To address these issues, we provide a comprehensive review of urban spatial-temporal prediction and propose a unified storage format for spatial-temporal data called atomic files. We also propose LibCity, an open-source library that offers researchers a credible experimental tool and a convenient development framework. In this library, we have reproduced 65 spatial-temporal prediction models and collected 55 spatial-temp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#35686;&#31034;&#25925;&#20107;&#38416;&#37322;&#20102;&#20998;&#26512;&#25968;&#25454;&#26102;&#65292;&#27979;&#37327;&#20960;&#20309;&#21644;&#24213;&#23618;&#29616;&#35937;&#20960;&#20309;&#24046;&#24322;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36825;&#31181;&#24046;&#24322;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#22914;&#20309;&#23548;&#33268;&#23545;&#19968;&#20010;&#20462;&#27491;&#36807;&#30340;&#38382;&#39064;&#32473;&#20986;&#38169;&#35823;&#31572;&#26696;&#12290;&#36825;&#20123;&#38382;&#39064;&#36866;&#29992;&#20110;&#38477;&#32500;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2304.14248</link><description>&lt;p&gt;
&#20851;&#20110;&#27931;&#20811;&#26031;&#27934;&#31348;&#30340;&#27969;&#24418;&#23398;&#20064;&#65306;&#20851;&#20110;&#27969;&#24418;&#23398;&#20064;&#21644;&#29289;&#29702;&#29616;&#35937;&#30340;&#35780;&#35770;&#65288;arXiv:2304.14248v1 [stat.ML]&#65289;
&lt;/p&gt;
&lt;p&gt;
On Manifold Learning in Plato's Cave: Remarks on Manifold Learning and Physical Phenomena. (arXiv:2304.14248v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#35686;&#31034;&#25925;&#20107;&#38416;&#37322;&#20102;&#20998;&#26512;&#25968;&#25454;&#26102;&#65292;&#27979;&#37327;&#20960;&#20309;&#21644;&#24213;&#23618;&#29616;&#35937;&#20960;&#20309;&#24046;&#24322;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36825;&#31181;&#24046;&#24322;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#22914;&#20309;&#23548;&#33268;&#23545;&#19968;&#20010;&#20462;&#27491;&#36807;&#30340;&#38382;&#39064;&#32473;&#20986;&#38169;&#35823;&#31572;&#26696;&#12290;&#36825;&#20123;&#38382;&#39064;&#36866;&#29992;&#20110;&#38477;&#32500;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23581;&#35797;&#36890;&#36807;&#27979;&#37327;&#19981;&#38656;&#35201;&#23545;&#29289;&#29702;&#29616;&#35937;&#25110;&#27979;&#37327;&#35774;&#22791;&#36827;&#34892;&#26174;&#24335;&#24314;&#27169;&#30340;&#20302;&#32500;&#27969;&#24418;&#32467;&#26500;&#26469;&#25512;&#26029;&#28508;&#22312;&#29289;&#29702;&#29616;&#35937;&#30340;&#20302;&#32500;&#27969;&#24418;&#32467;&#26500;&#65292;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#27979;&#37327;&#20960;&#20309;&#21644;&#24213;&#23618;&#29616;&#35937;&#20960;&#20309;&#20043;&#38388;&#24046;&#24322;&#30340;&#35686;&#31034;&#25925;&#20107;&#12290;&#22312;&#26222;&#36890;&#24773;&#20917;&#19979;&#65292;&#36825;&#31687;&#35770;&#25991;&#25152;&#23637;&#31034;&#30340;&#24230;&#37327;&#24418;&#21464;&#22312;&#25968;&#23398;&#19978;&#26159;&#30452;&#25509;&#32780;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#24182;&#19988;&#23427;&#21482;&#26159;&#25968;&#20010;&#31867;&#20284;&#25928;&#24212;&#20013;&#30340;&#19968;&#20010;&#12290;&#34429;&#28982;&#36825;&#24182;&#19981;&#24635;&#26159;&#20986;&#29616;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#19988;&#26080;&#23475;&#25968;&#25454;&#22788;&#29702;&#36807;&#31243;&#30340;&#20363;&#23376;&#65292;&#20854;&#20013;&#36825;&#31181;&#24433;&#21709;&#23548;&#33268;&#23545;&#19968;&#20010;&#30475;&#20284;&#31616;&#21333;&#30340;&#38382;&#39064;&#32473;&#20986;&#20102;&#38169;&#35823;&#30340;&#31572;&#26696;&#12290;&#23613;&#31649;&#25105;&#20204;&#20851;&#27880;&#27969;&#24418;&#23398;&#20064;&#65292;&#20294;&#36825;&#20123;&#38382;&#39064;&#24191;&#27867;&#36866;&#29992;&#20110;&#38477;&#32500;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many techniques in machine learning attempt explicitly or implicitly to infer a low-dimensional manifold structure of an underlying physical phenomenon from measurements without an explicit model of the phenomenon or the measurement apparatus. This paper presents a cautionary tale regarding the discrepancy between the geometry of measurements and the geometry of the underlying phenomenon in a benign setting. The deformation in the metric illustrated in this paper is mathematically straightforward and unavoidable in the general case, and it is only one of several similar effects. While this is not always problematic, we provide an example of an arguably standard and harmless data processing procedure where this effect leads to an incorrect answer to a seemingly simple question. Although we focus on manifold learning, these issues apply broadly to dimensionality reduction and unsupervised learning.
&lt;/p&gt;</description></item><item><title>UNADON&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#20840;&#22522;&#22240;&#32452;&#30340;&#26579;&#33394;&#20307;&#31354;&#38388;&#20301;&#32622;&#12290;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#29305;&#24449;&#21644;&#34920;&#35266;&#36951;&#20256;&#20449;&#21495;&#65292;UNADON&#22312;&#35757;&#32451;&#21333;&#20010;&#32454;&#32990;&#31995;&#26102;&#33021;&#22815;&#39640;&#24230;&#20934;&#30830;&#22320;&#39044;&#27979;&#26579;&#33394;&#36136;&#31354;&#38388;&#23450;&#20301;&#21040;&#26680;&#20307;&#65292;&#24182;&#25581;&#31034;&#20102;&#28508;&#22312;&#24433;&#21709;&#26579;&#33394;&#36136;&#21306;&#38548;&#30340;&#24207;&#21015;&#21644;&#34920;&#35266;&#36951;&#20256;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2304.13230</link><description>&lt;p&gt;
UNADON&#65306;&#29992;&#20110;&#39044;&#27979;&#20840;&#22522;&#22240;&#32452;&#26579;&#33394;&#20307;&#31354;&#38388;&#20301;&#32622;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UNADON: Transformer-based model to predict genome-wide chromosome spatial position. (arXiv:2304.13230v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13230
&lt;/p&gt;
&lt;p&gt;
UNADON&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#20840;&#22522;&#22240;&#32452;&#30340;&#26579;&#33394;&#20307;&#31354;&#38388;&#20301;&#32622;&#12290;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#29305;&#24449;&#21644;&#34920;&#35266;&#36951;&#20256;&#20449;&#21495;&#65292;UNADON&#22312;&#35757;&#32451;&#21333;&#20010;&#32454;&#32990;&#31995;&#26102;&#33021;&#22815;&#39640;&#24230;&#20934;&#30830;&#22320;&#39044;&#27979;&#26579;&#33394;&#36136;&#31354;&#38388;&#23450;&#20301;&#21040;&#26680;&#20307;&#65292;&#24182;&#25581;&#31034;&#20102;&#28508;&#22312;&#24433;&#21709;&#26579;&#33394;&#36136;&#21306;&#38548;&#30340;&#24207;&#21015;&#21644;&#34920;&#35266;&#36951;&#20256;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26579;&#33394;&#20307;&#30456;&#23545;&#20110;&#21151;&#33021;&#24615;&#26680;&#20307;&#30340;&#31354;&#38388;&#23450;&#20301;&#19982;&#22522;&#22240;&#32452;&#21151;&#33021;&#65288;&#22914;&#36716;&#24405;&#65289;&#23494;&#20999;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#24433;&#21709;&#20840;&#22522;&#22240;&#32452;&#33539;&#22260;&#20869;&#26579;&#33394;&#36136;&#31354;&#38388;&#23450;&#20301;&#30340;&#24207;&#21015;&#27169;&#24335;&#21644;&#34920;&#35266;&#36951;&#20256;&#29305;&#24449;&#23578;&#26410;&#34987;&#24456;&#22909;&#22320;&#29702;&#35299;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;UNADON&#65292;&#23427;&#20351;&#29992;&#24207;&#21015;&#29305;&#24449;&#21644;&#34920;&#35266;&#36951;&#20256;&#20449;&#21495;&#65292;&#39044;&#27979;&#20102;&#36890;&#36807;TSA-seq&#27979;&#37327;&#30340;&#29305;&#23450;&#31867;&#22411;&#26680;&#20307;&#30340;&#20840;&#22522;&#22240;&#32452;&#32454;&#32990;&#23398;&#36317;&#31163;&#12290;&#22312;&#22235;&#31181;&#32454;&#32990;&#31995;&#65288;K562&#65292;H1&#65292;HFFc6&#65292;HCT116&#65289;&#20013;&#23545;UNADON&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#21333;&#20010;&#32454;&#32990;&#31995;&#35757;&#32451;&#26102;&#65292;&#39640;&#24230;&#20934;&#30830;&#22320;&#39044;&#27979;&#20102;&#26579;&#33394;&#36136;&#31354;&#38388;&#23450;&#20301;&#21040;&#26680;&#20307;&#12290;UNADON&#22312;&#26410;&#35265;&#36807;&#30340;&#32454;&#32990;&#31867;&#22411;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24433;&#21709;&#22823;&#23610;&#24230;&#26579;&#33394;&#36136;&#21306;&#38548;&#21040;&#26680;&#20307;&#30340;&#28508;&#22312;&#24207;&#21015;&#21644;&#34920;&#35266;&#36951;&#20256;&#22240;&#32032;&#12290;&#32508;&#19978;&#65292;UNADON&#20026;&#20102;&#35299;&#22522;&#22240;&#32452;&#20013;&#30340;&#24207;&#21015;&#29305;&#24449;&#21644;&#26579;&#33394;&#36136;&#31354;&#38388;&#23450;&#20301;&#30340;&#21407;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#20026;&#30740;&#31350;&#26680;&#30340;&#21151;&#33021;&#32452;&#32455;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spatial positioning of chromosomes relative to functional nuclear bodies is intertwined with genome functions such as transcription. However, the sequence patterns and epigenomic features that collectively influence chromatin spatial positioning in a genome-wide manner are not well understood. Here, we develop a new transformer-based deep learning model called UNADON, which predicts the genome-wide cytological distance to a specific type of nuclear body, as measured by TSA-seq, using both sequence features and epigenomic signals. Evaluations of UNADON in four cell lines (K562, H1, HFFc6, HCT116) show high accuracy in predicting chromatin spatial positioning to nuclear bodies when trained on a single cell line. UNADON also performed well in an unseen cell type. Importantly, we reveal potential sequence and epigenomic factors that affect large-scale chromatin compartmentalization to nuclear bodies. Together, UNADON provides new insights into the principles between sequence features a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#23398;&#20064;&#30340;&#21160;&#24577;&#28508;&#22312;&#26223;&#35266;&#26469;&#24314;&#27169;&#28508;&#22312;&#32467;&#26500;&#65292;&#20174;&#32780;&#23558;&#28508;&#22312;&#36941;&#21382;&#20316;&#20026;&#26679;&#26412;&#27839;&#30528;&#26223;&#35266;&#26799;&#24230;&#30340;&#27969;&#21160;&#36827;&#34892;&#65292;&#20197;&#23454;&#29616;&#35299;&#32544;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#22120;&#36827;&#34892;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2304.12944</link><description>&lt;p&gt;
&#28508;&#22312;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#36941;&#21382;&#20316;&#20026;&#28508;&#22312;&#27969;&#30340;&#28508;&#22312;&#36335;&#32447;
&lt;/p&gt;
&lt;p&gt;
Latent Traversals in Generative Models as Potential Flows. (arXiv:2304.12944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12944
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#23398;&#20064;&#30340;&#21160;&#24577;&#28508;&#22312;&#26223;&#35266;&#26469;&#24314;&#27169;&#28508;&#22312;&#32467;&#26500;&#65292;&#20174;&#32780;&#23558;&#28508;&#22312;&#36941;&#21382;&#20316;&#20026;&#26679;&#26412;&#27839;&#30528;&#26223;&#35266;&#26799;&#24230;&#30340;&#27969;&#21160;&#36827;&#34892;&#65292;&#20197;&#23454;&#29616;&#35299;&#32544;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#22120;&#36827;&#34892;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#36817;&#36827;&#23637;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#28508;&#22312;&#31354;&#38388;&#30340;&#22522;&#26412;&#32467;&#26500;&#20173;&#28982;&#24456;&#19981;&#22909;&#29702;&#35299;&#65292;&#22240;&#27492;&#25191;&#34892;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#28508;&#22312;&#36941;&#21382;&#30340;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#32447;&#24615;&#24314;&#27169;&#28508;&#22312;&#32467;&#26500;&#65292;&#24182;&#25214;&#21040;&#30456;&#24212;&#30340;&#32447;&#24615;&#26041;&#21521;&#65292;&#20174;&#32780;&#20135;&#29983;&#8220;&#35299;&#32544;&#8221;&#30340;&#20195;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#35758;&#25913;&#20026;&#20351;&#29992;&#23398;&#20064;&#30340;&#21160;&#24577;&#28508;&#22312;&#26223;&#35266;&#26469;&#24314;&#27169;&#28508;&#22312;&#32467;&#26500;&#65292;&#20174;&#32780;&#23558;&#28508;&#22312;&#36941;&#21382;&#20316;&#20026;&#26679;&#26412;&#27839;&#30528;&#26223;&#35266;&#26799;&#24230;&#30340;&#27969;&#21160;&#36827;&#34892;&#12290;&#36825;&#20123;&#28508;&#22312;&#26223;&#35266;&#21463;&#21040;&#29289;&#29702;&#23398;&#12289;&#26368;&#20248;&#36816;&#36755;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#21551;&#21457;&#65292;&#34987;&#20316;&#20026;&#29289;&#29702;&#19978;&#29616;&#23454;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#26469;&#23398;&#20064;&#65292;&#20174;&#32780;&#20801;&#35768;&#23427;&#20204;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#35299;&#32544;&#65292;&#21516;&#26102;&#23398;&#20064;&#20102;&#22810;&#20010;&#21183;&#33021;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#22120;&#36827;&#34892;&#32422;&#26463;&#65292;&#20351;&#20854;&#20855;&#26377;&#26126;&#26174;&#24046;&#24322;&#19988;&#22312;&#35821;&#20041;&#19978;&#33258;&#25105;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the significant recent progress in deep generative models, the underlying structure of their latent spaces is still poorly understood, thereby making the task of performing semantically meaningful latent traversals an open research challenge. Most prior work has aimed to solve this challenge by modeling latent structures linearly, and finding corresponding linear directions which result in `disentangled' generations. In this work, we instead propose to model latent structures with a learned dynamic potential landscape, thereby performing latent traversals as the flow of samples down the landscape's gradient. Inspired by physics, optimal transport, and neuroscience, these potential landscapes are learned as physically realistic partial differential equations, thereby allowing them to flexibly vary over both space and time. To achieve disentanglement, multiple potentials are learned simultaneously, and are constrained by a classifier to be distinct and semantically self-consisten
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#21363;&#20415;&#20351;&#29992;FedMD&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#20173;&#23384;&#22312;&#34987;&#31934;&#24515;&#35774;&#35745;&#30340;&#24694;&#24847;&#25915;&#20987;&#21033;&#29992;&#30340;&#39118;&#38505;&#65292;&#22914;Paired-Logits&#21453;&#28436;&#25915;&#20987;&#65292;&#20250;&#23548;&#33268;&#38544;&#31169;&#25968;&#25454;&#26333;&#20809;&#12290;</title><link>http://arxiv.org/abs/2304.11436</link><description>&lt;p&gt;
&#36890;&#36807;Paired-Logits&#21453;&#28436;&#25915;&#20987;&#24674;&#22797;&#22270;&#20687;&#30340;FedMD
&lt;/p&gt;
&lt;p&gt;
Breaching FedMD: Image Recovery via Paired-Logits Inversion Attack. (arXiv:2304.11436v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#21363;&#20415;&#20351;&#29992;FedMD&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#20173;&#23384;&#22312;&#34987;&#31934;&#24515;&#35774;&#35745;&#30340;&#24694;&#24847;&#25915;&#20987;&#21033;&#29992;&#30340;&#39118;&#38505;&#65292;&#22914;Paired-Logits&#21453;&#28436;&#25915;&#20987;&#65292;&#20250;&#23548;&#33268;&#38544;&#31169;&#25968;&#25454;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#19982;&#27169;&#22411;&#33976;&#39311;&#65288;FedMD&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#21327;&#20316;&#23398;&#20064;&#33539;&#24335;&#65292;&#20854;&#20013;&#20165;&#20256;&#36755;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#36755;&#20986;logits&#20316;&#20026;&#33976;&#39311;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#20256;&#36882;&#26131;&#21463;&#26799;&#24230;&#21453;&#28436;&#25915;&#20987;&#30340;&#31169;&#26377;&#27169;&#22411;&#21442;&#25968;&#65292;&#36825;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#24050;&#30693;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#21363;&#20351;&#20849;&#20139;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#36755;&#20986; logit&#27604;&#30452;&#25509;&#20849;&#20139;&#26799;&#24230;&#26356;&#23433;&#20840;&#65292;&#20173;&#23384;&#22312;&#22240;&#31934;&#24515;&#35774;&#35745;&#30340;&#24694;&#24847;&#25915;&#20987;&#23548;&#33268;&#30340;&#25968;&#25454;&#26333;&#20809;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24694;&#24847;&#26381;&#21153;&#22120;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;&#21453;&#28436;&#31070;&#32463;&#32593;&#32476;&#26469;&#21033;&#29992;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#32622;&#20449;&#24230;&#24046;&#65292;&#38024;&#23545;FedMD&#21450;&#20854;&#21464;&#31181;&#36827;&#34892;PLI&#65288;&#37197;&#23545;logits&#21453;&#28436;&#65289;&#25915;&#20987;&#12290;&#22312;&#22810;&#20010;&#20154;&#33080;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#31867;&#20284;&#20110;FedMD&#30340;&#26041;&#26696;&#20013;&#65292;&#20165;&#20351;&#29992;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#37197;&#23545;&#26381;&#21153;&#22120;-&#23458;&#25143;&#31471;logits&#65292;&#24694;&#24847;&#26381;&#21153;&#22120;&#33021;&#22815;&#37325;&#26500;&#31169;&#26377;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Model Distillation (FedMD) is a nascent collaborative learning paradigm, where only output logits of public datasets are transmitted as distilled knowledge, instead of passing on private model parameters that are susceptible to gradient inversion attacks, a known privacy risk in federated learning. In this paper, we found that even though sharing output logits of public datasets is safer than directly sharing gradients, there still exists a substantial risk of data exposure caused by carefully designed malicious attacks. Our study shows that a malicious server can inject a PLI (Paired-Logits Inversion) attack against FedMD and its variants by training an inversion neural network that exploits the confidence gap between the server and client models. Experiments on multiple facial recognition datasets validate that under FedMD-like schemes, by using paired server-client logits of public datasets only, the malicious server is able to reconstruct private images on a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#20998;&#26512;&#20102;&#26469;&#33258;15&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;220&#20010;&#25919;&#27835;&#39046;&#34966;&#30340;YouTube&#35270;&#39057;&#65292;&#24635;&#32467;&#20102;&#25919;&#27835;&#39046;&#34966;&#38754;&#37096;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.09914</link><description>&lt;p&gt;
&#26623;&#23376;&#25919;&#27835;&#30340;&#38754;&#23380;&#65306;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27604;&#36739;&#25919;&#27835;&#39046;&#34966;&#38754;&#37096;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
The Face of Populism: Examining Differences in Facial Emotional Expressions of Political Leaders Using Machine Learning. (arXiv:2304.09914v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#20998;&#26512;&#20102;&#26469;&#33258;15&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;220&#20010;&#25919;&#27835;&#39046;&#34966;&#30340;YouTube&#35270;&#39057;&#65292;&#24635;&#32467;&#20102;&#25919;&#27835;&#39046;&#34966;&#38754;&#37096;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23186;&#20307;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#25919;&#27835;&#20449;&#24687;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20256;&#25773;&#21644;&#28040;&#36153;&#26041;&#24335;&#65292;&#36825;&#31181;&#36716;&#21464;&#20419;&#20351;&#25919;&#27835;&#20154;&#29289;&#37319;&#21462;&#26032;&#30340;&#31574;&#30053;&#26469;&#25429;&#25417;&#21644;&#20445;&#25345;&#36873;&#27665;&#30340;&#27880;&#24847;&#21147;&#12290;&#36825;&#20123;&#31574;&#30053;&#24448;&#24448;&#20381;&#36182;&#20110;&#24773;&#24863;&#35828;&#26381;&#21644;&#21560;&#24341;&#12290;&#38543;&#30528;&#34394;&#25311;&#31354;&#38388;&#20013;&#35270;&#35273;&#20869;&#23481;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#24456;&#22810;&#25919;&#27835;&#27807;&#36890;&#20063;&#34987;&#26631;&#24535;&#30528;&#21796;&#36215;&#24773;&#24863;&#30340;&#35270;&#39057;&#20869;&#23481;&#21644;&#22270;&#20687;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;&#29616;&#26377;&#35757;&#32451;&#22909;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25552;&#20379;&#30340;Python&#24211;fer&#65292;&#24212;&#29992;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#65292;&#23545;&#25551;&#32472;&#26469;&#33258;15&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;&#25919;&#27835;&#39046;&#34966;&#30340;220&#20010;YouTube&#35270;&#39057;&#26679;&#26412;&#36827;&#34892;&#20998;&#26512;&#12290;&#35813;&#31639;&#27861;&#36820;&#22238;&#24773;&#32490;&#20998;&#25968;&#65292;&#27599;&#19968;&#24103;&#37117;&#20195;&#34920;6&#31181;&#24773;&#32490;&#29366;&#24577;&#65288;&#24868;&#24594;&#65292;&#21388;&#24694;&#65292;&#24656;&#24807;&#65292;&#24555;&#20048;&#65292;&#24754;&#20260;&#21644;&#24778;&#35766;&#65289;&#21644;&#19968;&#20010;&#20013;&#24615;&#34920;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online media has revolutionized the way political information is disseminated and consumed on a global scale, and this shift has compelled political figures to adopt new strategies of capturing and retaining voter attention. These strategies often rely on emotional persuasion and appeal, and as visual content becomes increasingly prevalent in virtual space, much of political communication too has come to be marked by evocative video content and imagery. The present paper offers a novel approach to analyzing material of this kind. We apply a deep-learning-based computer-vision algorithm to a sample of 220 YouTube videos depicting political leaders from 15 different countries, which is based on an existing trained convolutional neural network architecture provided by the Python library fer. The algorithm returns emotion scores representing the relative presence of 6 emotional states (anger, disgust, fear, happiness, sadness, and surprise) and a neutral expression for each frame of the pr
&lt;/p&gt;</description></item><item><title>MOJITO&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;Transformer&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#27880;&#24847;&#21147;&#28151;&#21512;&#24314;&#27169;&#29992;&#25143;&#20559;&#22909;&#21644;&#26102;&#38388;&#32972;&#26223;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#20934;&#30830;&#39044;&#27979;&#19979;&#19968;&#20010;&#25512;&#33616;&#29289;&#21697;&#12290;&#22312;&#22810;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#65292;MOJITO&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;Transformer&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.08158</link><description>&lt;p&gt;
&#26102;&#38388;&#24863;&#30693;&#39034;&#24207;&#25512;&#33616;&#20013;&#30340;&#27880;&#24847;&#21147;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
Attention Mixtures for Time-Aware Sequential Recommendation. (arXiv:2304.08158v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08158
&lt;/p&gt;
&lt;p&gt;
MOJITO&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;Transformer&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#27880;&#24847;&#21147;&#28151;&#21512;&#24314;&#27169;&#29992;&#25143;&#20559;&#22909;&#21644;&#26102;&#38388;&#32972;&#26223;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#20934;&#30830;&#39044;&#27979;&#19979;&#19968;&#20010;&#25512;&#33616;&#29289;&#21697;&#12290;&#22312;&#22810;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#65292;MOJITO&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;Transformer&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#39034;&#24207;&#25512;&#33616;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26550;&#26500;&#32463;&#24120;&#24573;&#35270;&#29992;&#25143;&#20559;&#22909;&#21644;&#26102;&#38388;&#32972;&#26223;&#20043;&#38388;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#26412;&#31687;&#30701;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MOJITO&#65292;&#19968;&#31181;&#25913;&#36827;&#30340;Transformer&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#35299;&#20915;&#20102;&#36825;&#20010;&#23616;&#38480;&#24615;&#12290;MOJITO&#21033;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26102;&#38388;&#32972;&#26223;&#21644;&#29289;&#21697;&#23884;&#20837;&#34920;&#31034;&#30340;&#39640;&#26031;&#28151;&#21512;&#36827;&#34892;&#39034;&#24207;&#24314;&#27169;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#19979;&#19968;&#20010;&#24212;&#35813;&#21521;&#29992;&#25143;&#25512;&#33616;&#21738;&#20123;&#29289;&#21697;&#65292;&#36825;&#21462;&#20915;&#20110;&#36807;&#21435;&#30340;&#34892;&#20026;&#21644;&#26102;&#38388;&#32972;&#26223;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#35777;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#30456;&#20851;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;Transformer&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers emerged as powerful methods for sequential recommendation. However, existing architectures often overlook the complex dependencies between user preferences and the temporal context. In this short paper, we introduce MOJITO, an improved Transformer sequential recommender system that addresses this limitation. MOJITO leverages Gaussian mixtures of attention-based temporal context and item embedding representations for sequential modeling. Such an approach permits to accurately predict which items should be recommended next to users depending on past actions and the temporal context. We demonstrate the relevance of our approach, by empirically outperforming existing Transformers for sequential recommendation on several real-world datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;Benchmark for Evaluating the Robustness of GNNs to Topological Changes (BREC)&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#34920;&#26126;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07702</link><description>&lt;p&gt;
&#29992;BREC&#25968;&#25454;&#38598;&#26356;&#22909;&#22320;&#35780;&#20272;GNN&#34920;&#36798;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Better Evaluation of GNN Expressiveness with BREC Dataset. (arXiv:2304.07702v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;Benchmark for Evaluating the Robustness of GNNs to Topological Changes (BREC)&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#34920;&#26126;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#29702;&#35770;&#34920;&#36798;&#21147;&#30340;&#30740;&#31350;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#35768;&#22810;&#22686;&#24378;&#34920;&#36798;&#21147;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#20005;&#26684;&#36981;&#24490;k&#32500;Weisfeiler-Lehman&#65288;k-WL&#65289;&#27979;&#35797;&#23618;&#27425;&#32467;&#26500;&#30340;&#23569;&#25968;&#26041;&#27861;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#27809;&#26377;&#32479;&#19968;&#30340;&#34920;&#36798;&#21147;&#24230;&#37327;&#12290;&#23427;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#36890;&#24120;&#38480;&#20110;&#21306;&#20998;&#26576;&#20123;&#38750;&#21516;&#26500;&#22270;&#26063;&#65292;&#23548;&#33268;&#22312;&#23450;&#37327;&#27604;&#36739;&#34920;&#36798;&#21147;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#19982;&#29702;&#35770;&#20998;&#26512;&#30456;&#21453;&#65292;&#34913;&#37327;&#34920;&#36798;&#33021;&#21147;&#30340;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#22312;&#21253;&#21547;1-WL&#19981;&#21487;&#21306;&#20998;&#22270;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#25968;&#25454;&#38598;&#38754;&#20020;&#30528;&#38590;&#24230;&#65288;&#20219;&#20309;&#36229;&#36234;1-WL&#30340;&#27169;&#22411;&#20934;&#30830;&#29575;&#20960;&#20046;&#36798;&#21040;100&#65285;&#65289;&#12289;&#31890;&#24230;&#65288;&#27169;&#22411;&#20542;&#21521;&#20110;&#35201;&#20040;&#23436;&#20840;&#27491;&#30830;&#65292;&#35201;&#20040;&#25509;&#36817;&#38543;&#26426;&#29468;&#27979;&#65289;&#21644;&#35268;&#27169;&#65288;&#27599;&#20010;&#25968;&#25454;&#38598;&#20013;&#20165;&#26377;&#23569;&#37327;&#26412;&#36136;&#19981;&#21516;&#30340;&#22270;&#65289;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#21463;&#38480;&#21046;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;GNN&#40065;&#26834;&#24615;&#35780;&#20272;&#22522;&#20934;&#65288;BREC&#65289;&#65292;&#35813;&#22522;&#20934;&#21253;&#21547;&#35768;&#22810;&#32467;&#26500;&#22810;&#26679;&#30340;&#22270;&#65292;&#24182;&#20801;&#35768;&#23545;&#27169;&#22411;&#34920;&#36798;&#21147;&#36827;&#34892;&#26356;&#31934;&#32454;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research on the theoretical expressiveness of Graph Neural Networks (GNNs) has developed rapidly, and many methods have been proposed to enhance the expressiveness. However, most methods do not have a uniform expressiveness measure except for a few that strictly follow the $k$-dimensional Weisfeiler-Lehman ($k$-WL) test hierarchy. Their theoretical analyses are often limited to distinguishing certain families of non-isomorphic graphs, leading to difficulties in quantitatively comparing their expressiveness. In contrast to theoretical analysis, another way to measure expressiveness is by evaluating model performance on certain datasets containing 1-WL-indistinguishable graphs. Previous datasets specifically designed for this purpose, however, face problems with difficulty (any model surpassing 1-WL has nearly 100% accuracy), granularity (models tend to be either 100% correct or near random guess), and scale (only a few essentially different graphs in each dataset). To address these limi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Squeeze and Excitation&#32593;&#32476;&#30340;&#21464;&#20307;&#26469;&#25913;&#36827;&#37325;&#35201;&#29305;&#24449;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#36825;&#20123;&#21464;&#20307;&#22312;&#27531;&#24046;&#32593;&#32476;&#19978;&#25928;&#26524;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.06502</link><description>&lt;p&gt;
Squeeze and Excitation&#32593;&#32476;&#30340;&#21464;&#20307;
&lt;/p&gt;
&lt;p&gt;
Variations of Squeeze and Excitation networks. (arXiv:2304.06502v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Squeeze and Excitation&#32593;&#32476;&#30340;&#21464;&#20307;&#26469;&#25913;&#36827;&#37325;&#35201;&#29305;&#24449;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#36825;&#20123;&#21464;&#20307;&#22312;&#27531;&#24046;&#32593;&#32476;&#19978;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31354;&#38388;&#29305;&#24449;&#65292;&#24182;&#22312;&#20869;&#26680;&#20013;&#32039;&#23494;&#30456;&#36830;&#12290;SE&#27169;&#22359;&#25171;&#30772;&#20102;&#31070;&#32463;&#32593;&#32476;&#20256;&#36882;&#25972;&#20307;&#32467;&#26524;&#33267;&#19979;&#19968;&#23618;&#30340;&#20256;&#32479;&#36335;&#32447;&#12290;&#30456;&#21453;&#65292;SE&#20165;&#20256;&#36882;&#21253;&#21547;&#20854;&#25380;&#21387;&#21644;&#28608;&#21169;&#27169;&#22359;&#30340;&#37325;&#35201;&#29305;&#24449;&#36827;&#34892;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SE&#27169;&#22359;&#30340;&#21464;&#20307;&#65292;&#25913;&#36827;&#20102;&#25380;&#21387;&#21644;&#28608;&#21169;&#30340;&#36807;&#31243;&#65292;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#25380;&#21387;&#25110;&#28608;&#21169;&#23618;&#20351;&#24471;&#23618;&#26435;&#37325;&#30340;&#36716;&#25442;&#21464;&#24471;&#26356;&#21152;&#24179;&#28369;&#12290;&#36825;&#20123;&#21464;&#21270;&#36824;&#20445;&#30041;&#20102;SE&#27169;&#22359;&#30340;&#29305;&#28857;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#27531;&#24046;&#32593;&#32476;&#19978;&#36827;&#34892;&#65292;&#24182;&#36827;&#34892;&#20102;&#34920;&#26684;&#21270;&#30340;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks learns spatial features and are heavily interlinked within kernels. The SE module have broken the traditional route of neural networks passing the entire result to next layer. Instead SE only passes important features to be learned with its squeeze and excitation (SE) module. We propose variations of the SE module which improvises the process of squeeze and excitation and enhances the performance. The proposed squeezing or exciting the layer makes it possible for having a smooth transition of layer weights. These proposed variations also retain the characteristics of SE module. The experimented results are carried out on residual networks and the results are tabulated.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;2022&#24180;&#22522;&#22240;&#19982;&#36827;&#21270;&#35745;&#31639;&#20250;&#35758;&#20030;&#21150;&#30340;&#31454;&#36187;&#65292;&#35780;&#20272;&#20102;&#31526;&#21495;&#22238;&#24402;&#30340;&#26032;&#26041;&#27861;&#22312;&#38754;&#23545;&#29616;&#23454;&#25968;&#25454;&#26102;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#30340;&#23454;&#38469;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01117</link><description>&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#21487;&#35299;&#37322;&#31526;&#21495;&#22238;&#24402;&#65306;2022&#24180;&#31454;&#36187;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Interpretable Symbolic Regression for Data Science: Analysis of the 2022 Competition. (arXiv:2304.01117v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;2022&#24180;&#22522;&#22240;&#19982;&#36827;&#21270;&#35745;&#31639;&#20250;&#35758;&#20030;&#21150;&#30340;&#31454;&#36187;&#65292;&#35780;&#20272;&#20102;&#31526;&#21495;&#22238;&#24402;&#30340;&#26032;&#26041;&#27861;&#22312;&#38754;&#23545;&#29616;&#23454;&#25968;&#25454;&#26102;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#30340;&#23454;&#38469;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#26159;&#23547;&#25214;&#33021;&#22815;&#20934;&#30830;&#25551;&#36848;&#30740;&#31350;&#29616;&#35937;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20027;&#35201;&#20248;&#21183;&#26159;&#36820;&#22238;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#32473;&#29992;&#25143;&#25552;&#20379;&#28145;&#21051;&#30340;&#35265;&#35299;&#12290;&#21382;&#21490;&#19978;&#65292;&#31526;&#21495;&#22238;&#24402;&#30340;&#22823;&#22810;&#25968;&#31639;&#27861;&#37117;&#22522;&#20110;&#36827;&#21270;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#22823;&#37327;&#26032;&#30340;&#25552;&#26696;&#65292;&#36825;&#20123;&#25552;&#26696;&#20351;&#29992;&#20102;&#21015;&#20030;&#31639;&#27861;&#12289;&#28151;&#21512;&#32447;&#24615;&#25972;&#25968;&#35268;&#21010;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#31561;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#26032;&#26041;&#27861;&#22312;&#38754;&#23545;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#19968;&#32452;&#24120;&#35265;&#25361;&#25112;&#26102;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#25105;&#20204;&#22312;2022&#24180;&#36951;&#20256;&#19982;&#36827;&#21270;&#35745;&#31639;&#20250;&#35758;&#19978;&#20030;&#21150;&#20102;&#19968;&#27425;&#31454;&#36187;&#65292;&#20854;&#20013;&#21253;&#21547;&#19981;&#21516;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#21442;&#36187;&#32773;&#23545;&#36825;&#20123;&#25968;&#25454;&#38598;&#26159;&#30450;&#27979;&#35797;&#30340;&#12290;&#23545;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#20998;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#39046;&#22495;&#19987;&#23478;&#26469;&#35780;&#20272;&#20505;&#36873;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23545;&#32467;&#26524;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression searches for analytic expressions that accurately describe studied phenomena. The main attraction of this approach is that it returns an interpretable model that can be insightful to users. Historically, the majority of algorithms for symbolic regression have been based on evolutionary algorithms. However, there has been a recent surge of new proposals that instead utilize approaches such as enumeration algorithms, mixed linear integer programming, neural networks, and Bayesian optimization. In order to assess how well these new approaches behave on a set of common challenges often faced in real-world data, we hosted a competition at the 2022 Genetic and Evolutionary Computation Conference consisting of different synthetic and real-world datasets which were blind to entrants. For the real-world track, we assessed interpretability in a realistic way by using a domain expert to judge the trustworthiness of candidate models.We present an in-depth analysis of the result
&lt;/p&gt;</description></item><item><title>TOFA&#20351;&#29992;&#26435;&#37325;&#20849;&#20139;&#26469;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#20197;&#20248;&#21270;&#36229;&#32593;&#20197;&#36866;&#24212;&#21508;&#31181;&#35774;&#22791;&#30340;&#21508;&#31181;&#37096;&#32626;&#24773;&#20917;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;TOFA&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35745;&#31639;&#35757;&#32451;&#25104;&#26412;&#19982;&#37096;&#32626;&#26041;&#26696;&#25968;&#37327;&#26080;&#20851;&#12290;TOFA&#20351;&#29992;&#32479;&#19968;&#30340;&#21322;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#26469;&#35299;&#20915;&#23567;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.15485</link><description>&lt;p&gt;
TOFA&#65306;&#19968;&#27425;&#36716;&#31227;&#20840;&#33021;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
TOFA: Transfer-Once-for-All. (arXiv:2303.15485v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15485
&lt;/p&gt;
&lt;p&gt;
TOFA&#20351;&#29992;&#26435;&#37325;&#20849;&#20139;&#26469;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#20197;&#20248;&#21270;&#36229;&#32593;&#20197;&#36866;&#24212;&#21508;&#31181;&#35774;&#22791;&#30340;&#21508;&#31181;&#37096;&#32626;&#24773;&#20917;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;TOFA&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35745;&#31639;&#35757;&#32451;&#25104;&#26412;&#19982;&#37096;&#32626;&#26041;&#26696;&#25968;&#37327;&#26080;&#20851;&#12290;TOFA&#20351;&#29992;&#32479;&#19968;&#30340;&#21322;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#26469;&#35299;&#20915;&#23567;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26435;&#37325;&#20849;&#20139;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26088;&#22312;&#20026;&#19981;&#21516;&#36164;&#28304;&#32422;&#26463;&#30340;&#35768;&#22810;&#35774;&#22791;&#20248;&#21270;&#21487;&#37197;&#32622;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;&#36229;&#32593;&#65289;&#20197;&#28385;&#36275;&#21508;&#31181;&#37096;&#32626;&#22330;&#26223;&#12290;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#20174;&#22312;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#36229;&#32593;&#20013;&#25552;&#21462;&#22810;&#20010;&#27169;&#22411;&#65292;&#28982;&#21518;&#23545;&#24863;&#20852;&#36259;&#30340;&#36890;&#24120;&#24456;&#23567;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#25552;&#21462;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#30340;&#35745;&#31639;&#25104;&#26412;&#38543;&#30528;&#19981;&#21516;&#27169;&#22411;&#37096;&#32626;&#26041;&#26696;&#30340;&#25968;&#37327;&#32780;&#32447;&#24615;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Transfer-Once-For-All&#65288;TOFA&#65289;&#65292;&#29992;&#20110;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#36229;&#32593;&#39118;&#26684;&#30340;&#35757;&#32451;&#65292;&#22312;&#20219;&#24847;&#25968;&#37327;&#30340;&#36793;&#32536;&#37096;&#32626;&#26041;&#26696;&#19978;&#20855;&#26377;&#24658;&#23450;&#30340;&#35745;&#31639;&#35757;&#32451;&#25104;&#26412;&#12290;&#32473;&#23450;&#20219;&#21153;&#65292;TOFA&#33719;&#24471;&#23450;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20248;&#21270;&#20219;&#24847;&#25968;&#37327;&#30340;&#36793;&#32536;&#37096;&#32626;&#26041;&#26696;&#30340;&#25299;&#25169;&#21644;&#26435;&#37325;&#12290;&#20026;&#20102;&#20811;&#26381;&#23567;&#25968;&#25454;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;TOFA&#21033;&#29992;&#32479;&#19968;&#30340;&#21322;&#30417;&#30563;&#35757;&#32451;&#25439;&#22833;&#21516;&#26102;&#35757;&#32451;&#36229;&#32593;&#20869;&#30340;&#25152;&#26377;&#23376;&#32593;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weight-sharing neural architecture search aims to optimize a configurable neural network model (supernet) for a variety of deployment scenarios across many devices with different resource constraints. Existing approaches use evolutionary search to extract a number of models from a supernet trained on a very large data set, and then fine-tune the extracted models on the typically small, real-world data set of interest. The computational cost of training thus grows linearly with the number of different model deployment scenarios. Hence, we propose Transfer-Once-For-All (TOFA) for supernet-style training on small data sets with constant computational training cost over any number of edge deployment scenarios. Given a task, TOFA obtains custom neural networks, both the topology and the weights, optimized for any number of edge deployment scenarios. To overcome the challenges arising from small data, TOFA utilizes a unified semi-supervised training loss to simultaneously train all subnets w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23545;Tsetlin Machine&#36827;&#34892;&#27491;&#24335;&#39564;&#35777;&#24615;&#36136;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12289;&#31561;&#20215;&#24615;&#21644;&#30456;&#20284;&#24615;&#26041;&#38754;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36824;&#23558;&#26032;&#30340;&#27169;&#22411;&#30456;&#20284;&#24615;&#27010;&#24565;&#24212;&#29992;&#20110;TsMs&#12290;</title><link>http://arxiv.org/abs/2303.14464</link><description>&lt;p&gt;
Tsetlin&#26426;&#22120;&#30340;&#24615;&#36136;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Verifying Properties of Tsetlin Machines. (arXiv:2303.14464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14464
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23545;Tsetlin Machine&#36827;&#34892;&#27491;&#24335;&#39564;&#35777;&#24615;&#36136;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12289;&#31561;&#20215;&#24615;&#21644;&#30456;&#20284;&#24615;&#26041;&#38754;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36824;&#23558;&#26032;&#30340;&#27169;&#22411;&#30456;&#20284;&#24615;&#27010;&#24565;&#24212;&#29992;&#20110;TsMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tsetlin&#26426;&#22120;&#65288;TsMs&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#19988;&#26131;&#20110;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;TsMs&#31934;&#30830;&#32534;&#30721;&#20026;&#21629;&#39064;&#36923;&#36753;&#24182;&#20351;&#29992;SAT&#27714;&#35299;&#22120;&#27491;&#24335;&#39564;&#35777;&#20102;TsMs&#30340;&#24615;&#36136;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#20284;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#27010;&#24565;&#24212;&#29992;&#20110;&#26816;&#26597;TsMs&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#25991;&#29486;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#31561;&#20215;&#24615;&#27010;&#24565;&#65292;&#24182;&#20026;TsMs&#37325;&#26032;&#35843;&#25972;&#20102;&#23427;&#20204;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#32534;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#20026;TsMs&#30340;&#24615;&#36136;&#8212;&#8212;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#31561;&#20215;&#24615;&#21644;&#30456;&#20284;&#24615;&#25552;&#20379;&#32467;&#26524;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20998;&#21035;&#24212;&#29992;MNIST&#21644;IMDB&#25968;&#25454;&#38598;&#36827;&#34892;&#22270;&#20687;&#21644;&#24773;&#24863;&#20998;&#31867;&#12290;&#25105;&#20204;&#23558;&#20351;&#29992;TsMs&#26816;&#26597;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#32467;&#26524;&#19982;&#25991;&#29486;&#20013;&#22312;MNIST&#19978;&#20351;&#29992;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#30340;&#32467;&#26524;&#36827;&#34892;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tsetlin Machines (TsMs) are a promising and interpretable machine learning method which can be applied for various classification tasks. We present an exact encoding of TsMs into propositional logic and formally verify properties of TsMs using a SAT solver. In particular, we introduce in this work a notion of similarity of machine learning models and apply our notion to check for similarity of TsMs. We also consider notions of robustness and equivalence from the literature and adapt them for TsMs. Then, we show the correctness of our encoding and provide results for the properties: adversarial robustness, equivalence, and similarity of TsMs. In our experiments, we employ the MNIST and IMDB datasets for (respectively) image and sentiment classification. We discuss the results for verifying robustness obtained with TsMs with those in the literature obtained with Binarized Neural Networks on MNIST.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36807;&#21442;&#25968;&#21270;&#20302;&#31209;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#22240;&#23376;&#21270;&#26041;&#27861;&#35757;&#32451;&#30340;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#21487;&#20197;&#25910;&#25947;&#65292;&#24182;&#19988;&#38544;&#24335;&#24179;&#34913;&#21644;&#27491;&#21017;&#21270;&#21487;&#20197;&#20419;&#36827;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.14244</link><description>&lt;p&gt;
&#38544;&#24335;&#24179;&#34913;&#21644;&#27491;&#21017;&#21270;&#65306;&#36807;&#21442;&#25968;&#21270;&#38750;&#23545;&#31216;&#30697;&#38453;&#24863;&#30693;&#20013;&#30340;&#27867;&#21270;&#21644;&#25910;&#25947;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Implicit Balancing and Regularization: Generalization and Convergence Guarantees for Overparameterized Asymmetric Matrix Sensing. (arXiv:2303.14244v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36807;&#21442;&#25968;&#21270;&#20302;&#31209;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#22240;&#23376;&#21270;&#26041;&#27861;&#35757;&#32451;&#30340;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#21487;&#20197;&#25910;&#25947;&#65292;&#24182;&#19988;&#38544;&#24335;&#24179;&#34913;&#21644;&#27491;&#21017;&#21270;&#21487;&#20197;&#20419;&#36827;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#35757;&#32451;&#36807;&#21442;&#25968;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#30340;&#25910;&#25947;&#21644;&#27867;&#21270;&#23646;&#24615;&#26377;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#35768;&#22810;&#26041;&#38754;&#65292;&#21253;&#25324;&#23567;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#35282;&#33394;&#20197;&#21450;&#27169;&#22411;&#30340;&#21508;&#31181;&#21442;&#25968;&#22312;&#26799;&#24230;&#26356;&#26032;&#20013;&#22914;&#20309;&#32806;&#21512;&#20197;&#20419;&#36827;&#33391;&#22909;&#30340;&#27867;&#21270;&#65292;&#20173;&#28982;&#26159;&#24456;&#31070;&#31192;&#30340;&#12290;&#26368;&#36817;&#19968;&#31995;&#21015;&#30340;&#35770;&#25991;&#24050;&#32463;&#24320;&#22987;&#30740;&#31350;&#38750;&#20984;&#23545;&#31216;&#21322;&#27491;&#23450;&#65288;PSD&#65289;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#30340;&#24418;&#24335;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#38656;&#35201;&#20174;&#20960;&#20010;&#32447;&#24615;&#27979;&#37327;&#20013;&#37325;&#24314;&#19968;&#20010;&#20302;&#31209;PSD&#30697;&#38453;&#12290;&#36825;&#31181;&#24213;&#23618;&#30340;&#23545;&#31216;&#24615;/PSD&#24615;&#23545;&#20110;&#29616;&#26377;&#30340;&#36825;&#20010;&#38382;&#39064;&#30340;&#25910;&#25947;&#21644;&#27867;&#21270;&#20445;&#35777;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#36807;&#21442;&#25968;&#21270;&#30340;&#20302;&#31209;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#65292;&#20854;&#20013;&#24076;&#26395;&#20174;&#23569;&#37327;&#30340;&#32447;&#24615;&#27979;&#37327;&#20013;&#37325;&#24314;&#19968;&#20010;&#38750;&#23545;&#31216;&#30697;&#24418;&#20302;&#31209;&#30697;&#38453;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#22240;&#23376;&#21270;&#26469;&#35757;&#32451;&#30340;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#21487;&#20197;&#25910;&#25947;&#65292;&#32780;&#38544;&#24335;&#24179;&#34913;&#21644;&#27491;&#21017;&#21270;&#21487;&#20197;&#20419;&#36827;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been significant progress in understanding the convergence and generalization properties of gradient-based methods for training overparameterized learning models. However, many aspects including the role of small random initialization and how the various parameters of the model are coupled during gradient-based updates to facilitate good generalization remain largely mysterious. A series of recent papers have begun to study this role for non-convex formulations of symmetric Positive Semi-Definite (PSD) matrix sensing problems which involve reconstructing a low-rank PSD matrix from a few linear measurements. The underlying symmetry/PSDness is crucial to existing convergence and generalization guarantees for this problem. In this paper, we study a general overparameterized low-rank matrix sensing problem where one wishes to reconstruct an asymmetric rectangular low-rank matrix from a few linear measurements. We prove that an overparameterized model trained via factori
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#29983;&#29289;&#20449;&#21495;&#39046;&#22495;&#27867;&#21270;&#35780;&#20272;&#22522;&#20934;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#19987;&#38376;&#35299;&#20915;&#29983;&#29289;&#20449;&#21495;&#20013;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;DGNet-Bio&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;DGNet-Bio&#22312;ECG&#21644;EEG&#20998;&#31867;&#39046;&#22495;&#27867;&#21270;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11338</link><description>&lt;p&gt;
&#38754;&#21521;&#24515;&#30005;&#22270;&#21644;&#33041;&#30005;&#22270;&#20998;&#31867;&#30340;&#39046;&#22495;&#27867;&#21270;&#65306;&#31639;&#27861;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Towards Domain Generalization for ECG and EEG Classification: Algorithms and Benchmarks. (arXiv:2303.11338v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#29983;&#29289;&#20449;&#21495;&#39046;&#22495;&#27867;&#21270;&#35780;&#20272;&#22522;&#20934;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#19987;&#38376;&#35299;&#20915;&#29983;&#29289;&#20449;&#21495;&#20013;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;DGNet-Bio&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;DGNet-Bio&#22312;ECG&#21644;EEG&#20998;&#31867;&#39046;&#22495;&#27867;&#21270;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#33021;&#22815;&#22312;&#21307;&#30103;&#20445;&#20581;&#30340;&#20851;&#38190;&#20219;&#21153;&#20013;&#29282;&#22266;&#22320;&#30830;&#31435;&#33258;&#24049;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#22312;&#20110;&#65292;&#24403;&#27169;&#22411;&#38754;&#23545;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#20043;&#22806;&#30340;&#26679;&#26412;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#36825;&#23601;&#26159;&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#20010;&#35780;&#20272;DG&#31639;&#27861;&#30340;&#22522;&#20934;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#26469;&#35299;&#20915;&#29983;&#29289;&#20449;&#21495;&#20998;&#31867;&#20013;&#30340;DG&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#29983;&#29289;&#20449;&#21495;&#30340;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#21644;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#65292;&#24182;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#24320;&#28304;&#29983;&#29289;&#20449;&#21495;&#39046;&#22495;&#27867;&#21270;&#35780;&#20272;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#26368;&#20808;&#36827;DG&#31639;&#27861;&#25913;&#36827;&#20026;1D&#29983;&#29289;&#20449;&#21495;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;DGNet-Bio&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#29983;&#29289;&#20449;&#21495;&#20013;&#30340;DG&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DGNet-Bio&#22312;&#26032;&#25552;&#20986;&#30340;ECG&#21644;EEG&#20998;&#31867;&#39046;&#22495;&#27867;&#21270;&#22522;&#20934;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their immense success in numerous fields, machine and deep learning systems have not have not yet been able to firmly establish themselves in mission-critical applications in healthcare. One of the main reasons lies in the fact that when models are presented with previously unseen, Out-of-Distribution samples, their performance deteriorates significantly. This is known as the Domain Generalization (DG) problem. Our objective in this work is to propose a benchmark for evaluating DG algorithms, in addition to introducing a novel architecture for tackling DG in biosignal classification. In this paper, we describe the Domain Generalization problem for biosignals, focusing on electrocardiograms (ECG) and electroencephalograms (EEG) and propose and implement an open-source biosignal DG evaluation benchmark. Furthermore, we adapt state-of-the-art DG algorithms from computer vision to the problem of 1D biosignal classification and evaluate their effectiveness. Finally, we also introduc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;ExoplANNET&#65292;&#26088;&#22312;&#35299;&#20915;&#24452;&#21521;&#36895;&#24230;&#27861;&#26816;&#27979;&#31995;&#22806;&#34892;&#26143;&#30340;&#25361;&#25112;&#65292;&#22312;&#23384;&#22312;&#19982;&#26143;&#20307;&#30456;&#20851;&#30340;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#34892;&#26143;&#20449;&#21495;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#65292;&#32463;&#36807;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#27979;&#35797;&#65292;&#21462;&#24471;&#20102;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.09335</link><description>&lt;p&gt;
ExoplANNET: &#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#21644;&#30830;&#35748;&#24452;&#21521;&#36895;&#24230;&#25968;&#25454;&#20013;&#34892;&#26143;&#20449;&#21495;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ExoplANNET: A deep learning algorithm to detect and identify planetary signals in radial velocity data. (arXiv:2303.09335v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;ExoplANNET&#65292;&#26088;&#22312;&#35299;&#20915;&#24452;&#21521;&#36895;&#24230;&#27861;&#26816;&#27979;&#31995;&#22806;&#34892;&#26143;&#30340;&#25361;&#25112;&#65292;&#22312;&#23384;&#22312;&#19982;&#26143;&#20307;&#30456;&#20851;&#30340;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#34892;&#26143;&#20449;&#21495;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#65292;&#32463;&#36807;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#27979;&#35797;&#65292;&#21462;&#24471;&#20102;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#24452;&#21521;&#36895;&#24230;&#27861;&#26816;&#27979;&#31995;&#22806;&#34892;&#26143;&#30340;&#26041;&#27861;&#22312;&#20110;&#25506;&#27979;&#30001;&#26410;&#30693;&#19979;&#37096;&#24658;&#26143;&#20276;&#26143;&#24341;&#36215;&#30340;&#24658;&#26143;&#36895;&#24230;&#21464;&#21270;&#12290;&#20202;&#22120;&#35823;&#24046;&#12289;&#19981;&#35268;&#21017;&#26102;&#38388;&#37319;&#26679;&#20197;&#21450;&#28304;&#33258;&#20110;&#24658;&#26143;&#20869;&#22312;&#21464;&#24322;&#30340;&#19981;&#21516;&#22122;&#22768;&#28304;&#21487;&#33021;&#20250;&#38459;&#30861;&#25968;&#25454;&#30340;&#35299;&#37322;&#65292;&#29978;&#33267;&#23548;&#33268;&#34394;&#20551;&#25506;&#27979;&#32467;&#26524;&#12290;&#36817;&#26399;&#65292;&#22312;&#31995;&#22806;&#34892;&#26143;&#39046;&#22495;&#20986;&#29616;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#30740;&#31350;&#65292;&#20854;&#20013;&#19968;&#20123;&#32467;&#26524;&#29978;&#33267;&#36229;&#36807;&#20102;&#20256;&#32479;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#22312;&#24452;&#21521;&#36895;&#24230;&#27861;&#20013;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#19982;&#26143;&#20307;&#30456;&#20851;&#30340;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#31995;&#22806;&#34892;&#26143;&#25506;&#27979;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#20197;&#20195;&#26367;&#24452;&#21521;&#36895;&#24230;&#27861;&#26816;&#27979;&#21040;&#30340;&#20449;&#21495;&#30340;&#37325;&#35201;&#24615;&#35745;&#31639;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#34892;&#26143;&#36824;&#26159;&#38750;&#34892;&#26143;&#26469;&#28304;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#24050;&#30693;&#26377;&#34892;&#26143;&#31995;&#32479;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#26410;&#30693;&#26377;&#34892;&#26143;&#31995;&#32479;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;ExoplANNET&#31639;&#27861;&#23637;&#29616;&#20986;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#24182;&#34920;&#26126;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#22312;&#31995;&#22806;&#34892;&#26143;&#30340;&#37492;&#23450;&#21644;&#34920;&#24449;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The detection of exoplanets with the radial velocity method consists in detecting variations of the stellar velocity caused by an unseen sub-stellar companion. Instrumental errors, irregular time sampling, and different noise sources originating in the intrinsic variability of the star can hinder the interpretation of the data, and even lead to spurious detections. In recent times, work began to emerge in the field of extrasolar planets that use Machine Learning algorithms, some with results that exceed those obtained with the traditional techniques in the field. We seek to explore the scope of the neural networks in the radial velocity method, in particular for exoplanet detection in the presence of correlated noise of stellar origin. In this work, a neural network is proposed to replace the computation of the significance of the signal detected with the radial velocity method and to classify it as of planetary origin or not. The algorithm is trained using synthetic data of systems wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25913;&#36827;&#20102;&#36719;Actor-Critic&#65288;SAC&#65289;&#31639;&#27861;&#30340;&#23454;&#29616;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#29366;&#24577;&#30456;&#20851;&#30340;&#26494;&#24347;&#21464;&#37327;&#26469;&#36866;&#24403;&#22788;&#29702;&#19981;&#31561;&#24335;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#31574;&#30053;&#29109;&#12290;&#36825;&#23545;&#20110;&#22686;&#24378;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#30340;&#40065;&#26834;&#24615;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.04356</link><description>&lt;p&gt;
&#20855;&#26377;&#30495;&#27491;&#28385;&#36275;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#36719;Actor-Critic&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Soft Actor-Critic Algorithm with Truly-satisfied Inequality Constraint. (arXiv:2303.04356v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25913;&#36827;&#20102;&#36719;Actor-Critic&#65288;SAC&#65289;&#31639;&#27861;&#30340;&#23454;&#29616;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#29366;&#24577;&#30456;&#20851;&#30340;&#26494;&#24347;&#21464;&#37327;&#26469;&#36866;&#24403;&#22788;&#29702;&#19981;&#31561;&#24335;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#31574;&#30053;&#29109;&#12290;&#36825;&#23545;&#20110;&#22686;&#24378;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#30340;&#40065;&#26834;&#24615;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36719;Actor-Critic&#65288;SAC&#65289;&#34987;&#35748;&#20026;&#26159;&#19979;&#19968;&#20195;&#26426;&#22120;&#20154;&#25511;&#21046;&#26041;&#26696;&#20043;&#19968;&#12290;&#20854;&#26368;&#22823;&#21270;&#31574;&#30053;&#29109;&#30340;&#33021;&#21147;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#23545;&#22122;&#22768;&#21644;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36825;&#23545;&#20110;&#23454;&#38469;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#24403;&#21069;&#30340;&#23454;&#29616;&#20013;&#65292;&#26368;&#22823;&#21270;&#31574;&#30053;&#29109;&#30340;&#20248;&#20808;&#32423;&#26159;&#33258;&#21160;&#35843;&#33410;&#30340;&#65292;&#20854;&#35268;&#21017;&#21487;&#20197;&#35299;&#37322;&#20026;&#31561;&#24335;&#32422;&#26463;&#65292;&#23558;&#31574;&#30053;&#29109;&#32465;&#23450;&#21040;&#25351;&#23450;&#30340;&#19979;&#30028;&#12290;&#22240;&#27492;&#65292;&#24403;&#21069;&#30340;SAC&#19981;&#20877;&#26368;&#22823;&#21270;&#31574;&#30053;&#29109;&#65292;&#19982;&#25105;&#20204;&#30340;&#26399;&#26395;&#30456;&#21453;&#12290;&#20026;&#20102;&#35299;&#20915;SAC&#20013;&#30340;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25913;&#36827;&#20102;&#20854;&#23454;&#29616;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#29366;&#24577;&#30456;&#20851;&#30340;&#26494;&#24347;&#21464;&#37327;&#65292;&#20197;&#36866;&#24403;&#22788;&#29702;&#19981;&#31561;&#24335;&#32422;&#26463;&#65292;&#36890;&#36807;&#23558;&#20854;&#37325;&#26032;&#21046;&#23450;&#20026;&#30456;&#24212;&#30340;&#31561;&#24335;&#32422;&#26463;&#26469;&#26368;&#22823;&#21270;&#31574;&#30053;&#29109;&#12290;&#24341;&#20837;&#30340;&#26494;&#24347;&#21464;&#37327;&#36890;&#36807;&#32771;&#34385;&#28385;&#36275;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#21452;&#37325;&#30446;&#26631;&#30340;&#20999;&#25442;&#22411;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Soft actor-critic (SAC) in reinforcement learning is expected to be one of the next-generation robot control schemes. Its ability to maximize policy entropy would make a robotic controller robust to noise and perturbation, which is useful for real-world robot applications. However, the priority of maximizing the policy entropy is automatically tuned in the current implementation, the rule of which can be interpreted as one for equality constraint, binding the policy entropy into its specified lower bound. The current SAC is therefore no longer maximize the policy entropy, contrary to our expectation. To resolve this issue in SAC, this paper improves its implementation with a learnable state-dependent slack variable for appropriately handling the inequality constraint to maximize the policy entropy by reformulating it as the corresponding equality constraint. The introduced slack variable is optimized by a switching-type loss function that takes into account the dual objectives of satis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#23558;&#29289;&#29702;&#30693;&#35782;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;PIDL&#22312;&#20132;&#36890;&#29366;&#24577;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#35774;&#35745;PIDL&#35745;&#31639;&#22270;&#30340;&#36807;&#31243;&#20013;&#65292;&#37325;&#28857;&#32771;&#34385;&#20102;&#22914;&#20309;&#23558;&#29289;&#29702;&#30693;&#35782;&#32534;&#30721;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24182;&#23637;&#31034;&#20102;&#19981;&#21516;&#26550;&#26500;&#35774;&#35745;&#22312;&#20132;&#36890;&#29366;&#24577;&#20272;&#35745;&#20013;&#30340;&#24046;&#24322;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.02063</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#20132;&#36890;&#29366;&#24577;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;: &#19968;&#39033;&#35843;&#26597;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Deep Learning For Traffic State Estimation: A Survey and the Outlook. (arXiv:2303.02063v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#23558;&#29289;&#29702;&#30693;&#35782;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;PIDL&#22312;&#20132;&#36890;&#29366;&#24577;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#35774;&#35745;PIDL&#35745;&#31639;&#22270;&#30340;&#36807;&#31243;&#20013;&#65292;&#37325;&#28857;&#32771;&#34385;&#20102;&#22914;&#20309;&#23558;&#29289;&#29702;&#30693;&#35782;&#32534;&#30721;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24182;&#23637;&#31034;&#20102;&#19981;&#21516;&#26550;&#26500;&#35774;&#35745;&#22312;&#20132;&#36890;&#29366;&#24577;&#20272;&#35745;&#20013;&#30340;&#24046;&#24322;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;Physics-Informed Deep Learning, PIDL&#65289;&#36890;&#36807;&#23558;&#29289;&#29702;&#27169;&#22411;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30456;&#32467;&#21512;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#35757;&#32451;&#25928;&#29575;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#35774;&#35745;&#19968;&#20010;&#32467;&#21512;&#29289;&#29702;&#30693;&#35782;&#21644;DNN&#30340;&#35745;&#31639;&#22270;&#65292;&#21363;&#22914;&#20309;&#23558;&#29289;&#29702;&#30693;&#35782;&#32534;&#30721;&#21040;DNN&#20013;&#65292;&#20197;&#21450;&#22914;&#20309;&#34920;&#31034;&#29289;&#29702;&#30693;&#35782;&#21644;&#25968;&#25454;&#32452;&#25104;&#37096;&#20998;&#12290;&#38024;&#23545;&#20132;&#36890;&#29366;&#24577;&#20272;&#35745;&#65288;Traffic State Estimation, TSE&#65289;&#36825;&#20010;&#20132;&#36890;&#24037;&#31243;&#20013;&#30340;&#26680;&#24515;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#22810;&#31181;PIDL&#35745;&#31639;&#22270;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35266;&#27979;&#25968;&#25454;&#12289;&#38382;&#39064;&#31867;&#22411;&#21644;&#30446;&#26631;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#20351;&#29992;&#30456;&#21516;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#21464;&#20307;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
For its robust predictive power (compared to pure physics-based models) and sample-efficient training (compared to pure deep learning models), physics-informed deep learning (PIDL), a paradigm hybridizing physics-based models and deep neural networks (DNN), has been booming in science and engineering fields. One key challenge of applying PIDL to various domains and problems lies in the design of a computational graph that integrates physics and DNNs. In other words, how physics are encoded into DNNs and how the physics and data components are represented. In this paper, we provide a variety of architecture designs of PIDL computational graphs and how these structures are customized to traffic state estimation (TSE), a central problem in transportation engineering. When observation data, problem type, and goal vary, we demonstrate potential architectures of PIDL computational graphs and compare these variants using the same real-world dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#20171;&#20837;&#31243;&#24207;&#30340;&#25552;&#39640;&#20171;&#20837;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20171;&#20837;&#27010;&#24565;&#20197;&#21450;&#28145;&#20837;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#26126;&#26234;&#30340;&#20171;&#20837;&#31574;&#30053;&#21487;&#20197;&#23558;&#20219;&#21153;&#35823;&#24046;&#38477;&#20302;&#21313;&#20493;&#20197;&#19978;&#65292;&#32780;&#19988;&#36825;&#20010;&#24046;&#24322;&#21487;&#30456;&#24403;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2302.14260</link><description>&lt;p&gt;
&#23545;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#20171;&#20837;&#31243;&#24207;&#30340;&#26356;&#28145;&#20837;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at the Intervention Procedure of Concept Bottleneck Models. (arXiv:2302.14260v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#20171;&#20837;&#31243;&#24207;&#30340;&#25552;&#39640;&#20171;&#20837;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20171;&#20837;&#27010;&#24565;&#20197;&#21450;&#28145;&#20837;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#26126;&#26234;&#30340;&#20171;&#20837;&#31574;&#30053;&#21487;&#20197;&#23558;&#20219;&#21153;&#35823;&#24046;&#38477;&#20302;&#21313;&#20493;&#20197;&#19978;&#65292;&#32780;&#19988;&#36825;&#20010;&#24046;&#24322;&#21487;&#30456;&#24403;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;(CBMs)&#26159;&#19968;&#31867;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22522;&#20110;&#20854;&#39640;&#32423;&#27010;&#24565;&#39044;&#27979;&#32473;&#23450;&#36755;&#20837;&#30340;&#30446;&#26631;&#21709;&#24212;&#12290;&#19982;&#26631;&#20934;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#19981;&#21516;&#65292;CBMs&#20351;&#39046;&#22495;&#19987;&#23478;&#33021;&#22815;&#22312;&#27979;&#35797;&#26102;&#23545;&#39044;&#27979;&#30340;&#27010;&#24565;&#36827;&#34892;&#24178;&#39044;&#24182;&#32416;&#27491;&#20219;&#20309;&#38169;&#35823;&#65292;&#20197;&#20415;&#22312;&#26368;&#32456;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20219;&#21153;&#39044;&#27979;&#12290;&#23613;&#31649;&#36825;&#31181;&#21487;&#24178;&#39044;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#25511;&#21046;&#36884;&#24452;&#65292;&#20294;&#20171;&#20837;&#31243;&#24207;&#30340;&#35768;&#22810;&#26041;&#38754;&#20173;&#28982;&#30456;&#24403;&#26410;&#30693;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#21508;&#31181;&#26041;&#24335;&#26469;&#36873;&#25321;&#20171;&#20837;&#27010;&#24565;&#20197;&#25552;&#39640;&#20171;&#20837;&#25928;&#26524;&#65292;&#24182;&#36827;&#34892;&#20102;&#21508;&#31181;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#20197;&#20102;&#35299;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#23427;&#20204;&#30340;&#28436;&#21464;&#26041;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#32463;&#36807;&#26126;&#26234;&#20171;&#20837;&#30340;&#31574;&#30053;&#21487;&#20197;&#23558;&#20219;&#21153;&#35823;&#24046;&#38477;&#20302;&#21313;&#20493;&#20197;&#19978;&#65292;&#32780;&#19982;&#24403;&#21069;&#22522;&#32447;&#30456;&#27604;&#65292;&#22312;&#30456;&#21516;&#25968;&#37327;&#30340;&#24178;&#39044;&#27425;&#25968;&#19979;&#65292;&#36825;&#20010;&#24046;&#24322;&#21487;&#20197;&#30456;&#24403;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept bottleneck models (CBMs) are a class of interpretable neural network models that predict the target response of a given input based on its high-level concepts. Unlike the standard end-to-end models, CBMs enable domain experts to intervene on the predicted concepts and rectify any mistakes at test time, so that more accurate task predictions can be made at the end. While such intervenability provides a powerful avenue of control, many aspects of the intervention procedure remain rather unexplored. In this work, we develop various ways of selecting intervening concepts to improve the intervention effectiveness and conduct an array of in-depth analyses as to how they evolve under different circumstances. Specifically, we find that an informed intervention strategy can reduce the task error more than ten times compared to the current baseline under the same amount of intervention counts in realistic settings, and yet, this can vary quite significantly when taking into account diffe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35270;&#20026;&#26377;&#22122;&#22768;&#30340;&#19981;&#21160;&#28857;&#36845;&#20195;&#65292;&#20174;&#32780;&#20174;&#36825;&#19968;&#26694;&#26550;&#20013;&#25512;&#23548;&#20986;&#38544;&#31169;&#21644;&#25928;&#29992;&#32467;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#27969;&#34892;&#30340;&#31169;&#26377;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#21644;&#20998;&#26512;&#26032;&#30340;&#31169;&#26377;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#36890;&#29992;&#26694;&#26550;&#25512;&#23548;&#20986;&#20102;&#29992;&#20110;&#38598;&#20013;&#24335;&#12289;&#32852;&#21512;&#21644;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#30340;&#26032;&#39062;&#31169;&#26377;ADMM&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#21644;&#23376;&#37319;&#26679;&#30340;&#38544;&#31169;&#25918;&#22823;&#24314;&#31435;&#20102;&#24378;&#38544;&#31169;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#30340;&#26377;&#22122;&#22768;&#22266;&#23450;&#28857;&#36845;&#20195;&#30340;&#32447;&#24615;&#25910;&#25947;&#32467;&#26524;&#25552;&#20379;&#25928;&#29992;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2302.12559</link><description>&lt;p&gt;
&#20174;&#26377;&#22122;&#22768;&#30340;&#19981;&#21160;&#28857;&#36845;&#20195;&#21040;&#31169;&#26377;&#30340;ADMM&#31639;&#27861;&#65292;&#29992;&#20110;&#38598;&#20013;&#24335;&#21644;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
From Noisy Fixed-Point Iterations to Private ADMM for Centralized and Federated Learning. (arXiv:2302.12559v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35270;&#20026;&#26377;&#22122;&#22768;&#30340;&#19981;&#21160;&#28857;&#36845;&#20195;&#65292;&#20174;&#32780;&#20174;&#36825;&#19968;&#26694;&#26550;&#20013;&#25512;&#23548;&#20986;&#38544;&#31169;&#21644;&#25928;&#29992;&#32467;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#27969;&#34892;&#30340;&#31169;&#26377;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#21644;&#20998;&#26512;&#26032;&#30340;&#31169;&#26377;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#36890;&#29992;&#26694;&#26550;&#25512;&#23548;&#20986;&#20102;&#29992;&#20110;&#38598;&#20013;&#24335;&#12289;&#32852;&#21512;&#21644;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#30340;&#26032;&#39062;&#31169;&#26377;ADMM&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#21644;&#23376;&#37319;&#26679;&#30340;&#38544;&#31169;&#25918;&#22823;&#24314;&#31435;&#20102;&#24378;&#38544;&#31169;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#30340;&#26377;&#22122;&#22768;&#22266;&#23450;&#28857;&#36845;&#20195;&#30340;&#32447;&#24615;&#25910;&#25947;&#32467;&#26524;&#25552;&#20379;&#25928;&#29992;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20316;&#20026;&#22122;&#22768;&#22266;&#23450;&#28857;&#36845;&#20195;&#30340;&#23454;&#20363;&#65292;&#20197;&#20415;&#20174;&#36825;&#20010;&#32463;&#36807;&#20805;&#20998;&#30740;&#31350;&#30340;&#26694;&#26550;&#20013;&#24471;&#20986;&#38544;&#31169;&#21644;&#25928;&#29992;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#26032;&#30340;&#35270;&#35282;&#24674;&#22797;&#20102;&#27969;&#34892;&#30340;&#31169;&#26377;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#22914;DP-SGD&#65292;&#24182;&#20026;&#35774;&#35745;&#21644;&#20998;&#26512;&#26032;&#30340;&#31169;&#26377;&#20248;&#21270;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#26041;&#24335;&#12290;&#19987;&#27880;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#65288;ADMM&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#36890;&#29992;&#26694;&#26550;&#25512;&#23548;&#20986;&#29992;&#20110;&#38598;&#20013;&#24335;&#12289;&#32852;&#21512;&#21644;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#30340;&#26032;&#39062;&#31169;&#26377;ADMM&#31639;&#27861;&#12290;&#23545;&#20110;&#36825;&#19977;&#20010;&#31639;&#27861;&#65292;&#25105;&#20204;&#21033;&#29992;&#36845;&#20195;&#21644;&#23376;&#37319;&#26679;&#30340;&#38544;&#31169;&#25918;&#22823;&#24314;&#31435;&#20102;&#24378;&#38544;&#31169;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#30340;&#26377;&#22122;&#22768;&#22266;&#23450;&#28857;&#36845;&#20195;&#30340;&#32447;&#24615;&#25910;&#25947;&#32467;&#26524;&#36827;&#34892;&#32479;&#19968;&#20998;&#26512;&#65292;&#25552;&#20379;&#25928;&#29992;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study differentially private (DP) machine learning algorithms as instances of noisy fixed-point iterations, in order to derive privacy and utility results from this well-studied framework. We show that this new perspective recovers popular private gradient-based methods like DP-SGD and provides a principled way to design and analyze new private optimization algorithms in a flexible manner. Focusing on the widely-used Alternating Directions Method of Multipliers (ADMM) method, we use our general framework to derive novel private ADMM algorithms for centralized, federated and fully decentralized learning. For these three algorithms, we establish strong privacy guarantees leveraging privacy amplification by iteration and by subsampling. Finally, we provide utility guarantees using a unified analysis that exploits a recent linear convergence result for noisy fixed-point iterations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20855;&#26377;&#21487;&#23398;&#20064;&#21644;&#26368;&#20248;&#22810;&#39033;&#24335;&#22522;&#20989;&#25968;&#30340;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#39033;&#24335;&#22522;&#20989;&#25968;&#21644;&#35745;&#31639;&#26368;&#20248;&#22522;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#22312;&#27169;&#22411;&#26377;&#25928;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.12432</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#23398;&#20064;&#21644;&#26368;&#20248;&#22810;&#39033;&#24335;&#22522;&#20989;&#25968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks with Learnable and Optimal Polynomial Bases. (arXiv:2302.12432v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20855;&#26377;&#21487;&#23398;&#20064;&#21644;&#26368;&#20248;&#22810;&#39033;&#24335;&#22522;&#20989;&#25968;&#30340;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#39033;&#24335;&#22522;&#20989;&#25968;&#21644;&#35745;&#31639;&#26368;&#20248;&#22522;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#22312;&#27169;&#22411;&#26377;&#25928;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#26159;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#24120;&#20351;&#29992;&#39044;&#23450;&#30340;&#22810;&#39033;&#24335;&#22522;&#20989;&#25968;&#65292;&#24182;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#31995;&#25968;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#22810;&#39033;&#24335;&#22522;&#20989;&#25968;&#30340;&#24615;&#36136;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#20204;&#33021;&#22815;&#32943;&#23450;&#22238;&#31572;&#19978;&#36848;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#21463;&#21040;Favard&#23450;&#29702;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FavardGNN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20174;&#25152;&#26377;&#21487;&#33021;&#30340;&#27491;&#20132;&#22522;&#20989;&#25968;&#31354;&#38388;&#20013;&#23398;&#20064;&#22810;&#39033;&#24335;&#22522;&#20989;&#25968;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Wang&#21644;Zhang&#65288;2022&#24180;&#65289;&#25552;&#20986;&#30340;&#25152;&#35859;&#26080;&#27861;&#35299;&#20915;&#30340;&#26368;&#20248;&#22810;&#39033;&#24335;&#22522;&#20989;&#25968;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#27169;&#22411;OptBasisGNN&#65292;&#21487;&#35745;&#31639;&#32473;&#23450;&#22270;&#32467;&#26500;&#21644;&#22270;&#20449;&#21495;&#30340;&#26368;&#20248;&#22522;&#20989;&#25968;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Polynomial filters, a kind of Graph Neural Networks, typically use a predetermined polynomial basis and learn the coefficients from the training data. It has been observed that the effectiveness of the model is highly dependent on the property of the polynomial basis. Consequently, two natural and fundamental questions arise: Can we learn a suitable polynomial basis from the training data? Can we determine the optimal polynomial basis for a given graph and node features?  In this paper, we propose two spectral GNN models that provide positive answers to the questions posed above. First, inspired by Favard's Theorem, we propose the FavardGNN model, which learns a polynomial basis from the space of all possible orthonormal bases. Second, we examine the supposedly unsolvable definition of optimal polynomial basis from Wang &amp; Zhang (2022) and propose a simple model, OptBasisGNN, which computes the optimal basis for a given graph structure and graph signal. Extensive experiments are conduct
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32463;&#27982;&#23398;&#20013;&#20004;&#31181;&#26426;&#21046;&#30340;&#21487;&#23398;&#20064;&#24615;&#65306;&#33756;&#21333;&#25277;&#22870;&#21644;&#20004;&#37096;&#20998;&#31080;&#20215;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#36825;&#20004;&#31181;&#26426;&#21046;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.11700</link><description>&lt;p&gt;
&#23398;&#20064;&#26368;&#22823;&#21270;&#33756;&#21333;&#25277;&#22870;&#21644;&#20004;&#37096;&#20998;&#31080;&#20215;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Learning Revenue Maximizing Menus of Lotteries and Two-Part Tariffs. (arXiv:2302.11700v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11700
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32463;&#27982;&#23398;&#20013;&#20004;&#31181;&#26426;&#21046;&#30340;&#21487;&#23398;&#20064;&#24615;&#65306;&#33756;&#21333;&#25277;&#22870;&#21644;&#20004;&#37096;&#20998;&#31080;&#20215;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#36825;&#20004;&#31181;&#26426;&#21046;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#22312;&#23398;&#20064;&#29702;&#35770;&#21644;&#35745;&#31639;&#32463;&#27982;&#23398;&#20132;&#21449;&#39046;&#22495;&#20013;&#36817;&#24180;&#26469;&#34028;&#21187;&#21457;&#23637;&#30340;&#19968;&#31995;&#21015;&#24037;&#20316;&#65292;&#25512;&#36827;&#20102;&#32463;&#27982;&#23398;&#20013;&#20004;&#31867;&#26426;&#21046;&#30340;&#21487;&#23398;&#20064;&#24615;&#30740;&#31350;&#65292;&#20998;&#21035;&#26159;&#33756;&#21333;&#25277;&#22870;&#21644;&#20004;&#37096;&#20998;&#31080;&#20215;&#12290;&#21069;&#32773;&#26159;&#19968;&#31867;&#26088;&#22312;&#38144;&#21806;&#22810;&#20010;&#29289;&#21697;&#30340;&#38543;&#26426;&#26426;&#21046;&#65292;&#24050;&#30693;&#33021;&#22815;&#23454;&#29616;&#36229;&#20986;&#30830;&#23450;&#24615;&#26426;&#21046;&#30340;&#25910;&#30410;&#65292;&#32780;&#21518;&#32773;&#21017;&#26159;&#38024;&#23545;&#38144;&#21806;&#21333;&#20010;&#29289;&#21697;&#22810;&#20010;&#21333;&#20301;&#65288;&#21103;&#26412;&#65289;&#30340;&#35774;&#35745;&#65292;&#36866;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22330;&#26223;&#65292;&#22914;&#27773;&#36710;&#25110;&#33258;&#34892;&#36710;&#20849;&#20139;&#26381;&#21153;&#31561;&#12290;&#25105;&#20204;&#20851;&#27880;&#22914;&#20309;&#20174;&#20080;&#23478;&#20272;&#20540;&#25968;&#25454;&#20013;&#23398;&#20064;&#20986;&#39640;&#25910;&#30410;&#30340;&#36825;&#31867;&#26426;&#21046;&#65292;&#28085;&#30422;&#22810;&#31181;&#20998;&#24067;&#35774;&#32622;&#65292;&#26082;&#26377;&#30452;&#25509;&#33719;&#24471;&#20080;&#23478;&#20272;&#20540;&#26679;&#26412;&#30340;&#24773;&#20917;&#65292;&#20063;&#26377;&#26356;&#20855;&#25361;&#25112;&#24615;&#12289;&#30740;&#31350;&#36739;&#23569;&#30340;&#22312;&#32447;&#35774;&#32622;&#65292;&#20854;&#20013;&#20080;&#23478;&#19968;&#20010;&#25509;&#19968;&#20010;&#21040;&#26469;&#65292;&#24182;&#19988;&#23545;&#20182;&#20204;&#30340;&#20272;&#20540;&#27809;&#26377;&#20998;&#24067;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#33756;&#21333;&#25277;&#22870;&#21644;&#20004;&#37096;&#20998;&#31080;&#20215;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We advance a recently flourishing line of work at the intersection of learning theory and computational economics by studying the learnability of two classes of mechanisms prominent in economics, namely menus of lotteries and two-part tariffs. The former is a family of randomized mechanisms designed for selling multiple items, known to achieve revenue beyond deterministic mechanisms, while the latter is designed for selling multiple units (copies) of a single item with applications in real-world scenarios such as car or bike-sharing services. We focus on learning high-revenue mechanisms of this form from buyer valuation data in both distributional settings, where we have access to buyers' valuation samples up-front, and the more challenging and less-studied online settings, where buyers arrive one-at-a-time and no distributional assumption is made about their values.  Our main contribution is proposing the first online learning algorithms for menus of lotteries and two-part tariffs wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#20998;&#21306;&#32858;&#21512;&#30340;&#35748;&#35777;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;$\ell_0$&#36867;&#36991;&#12289;&#21518;&#38376;&#21644;&#27745;&#26579;&#25915;&#20987;&#12290;&#19982;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#30456;&#27604;&#65292;FPA&#36895;&#24230;&#26356;&#24555;&#65292;&#25552;&#20379;&#26356;&#22823;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#19988;&#33021;&#22815;&#20813;&#36153;&#25552;&#20379;&#39069;&#22806;&#30340;&#40065;&#26834;&#24615;&#32500;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.11628</link><description>&lt;p&gt;
&#29305;&#24449;&#20998;&#21306;&#32858;&#21512;&#65306;&#19968;&#31181;&#24555;&#36895;&#30340;&#23545;$\ell_0$&#25915;&#20987;&#30340;&#35748;&#35777;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Feature Partition Aggregation: A Fast Certified Defense Against a Union of $\ell_0$ Attacks. (arXiv:2302.11628v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#20998;&#21306;&#32858;&#21512;&#30340;&#35748;&#35777;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;$\ell_0$&#36867;&#36991;&#12289;&#21518;&#38376;&#21644;&#27745;&#26579;&#25915;&#20987;&#12290;&#19982;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#30456;&#27604;&#65292;FPA&#36895;&#24230;&#26356;&#24555;&#65292;&#25552;&#20379;&#26356;&#22823;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#19988;&#33021;&#22815;&#20813;&#36153;&#25552;&#20379;&#39069;&#22806;&#30340;&#40065;&#26834;&#24615;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#30340;&#25110;$\ell_0$&#23545;&#25239;&#25915;&#20987;&#20250;&#20219;&#24847;&#25200;&#21160;&#26410;&#30693;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;$\ell_0$&#40065;&#26834;&#24615;&#20998;&#26512;&#29305;&#21035;&#36866;&#29992;&#20110;&#24322;&#26500;&#65288;&#34920;&#26684;&#65289;&#25968;&#25454;&#65292;&#20854;&#20013;&#29305;&#24449;&#20855;&#26377;&#19981;&#21516;&#30340;&#31867;&#22411;&#25110;&#23610;&#24230;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;$\ell_0$&#35748;&#35777;&#38450;&#24481;&#22522;&#20110;&#38543;&#26426;&#24179;&#28369;&#65292;&#24182;&#20165;&#36866;&#29992;&#20110;&#36867;&#36991;&#25915;&#20987;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#29305;&#24449;&#20998;&#21306;&#32858;&#21512;&#65288;FPA&#65289;--&#19968;&#31181;&#38024;&#23545;$\ell_0$&#36867;&#36991;&#12289;&#21518;&#38376;&#21644;&#27745;&#26579;&#25915;&#20987;&#30340;&#35748;&#35777;&#38450;&#24481;&#12290;FPA&#36890;&#36807;&#38598;&#25104;&#29983;&#25104;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#20854;&#23376;&#27169;&#22411;&#26159;&#22312;&#19981;&#30456;&#20132;&#30340;&#29305;&#24449;&#38598;&#19978;&#35757;&#32451;&#30340;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;$\ell_0$&#38450;&#24481;&#30456;&#27604;&#65292;FPA&#36895;&#24230;&#25552;&#39640;&#20102;&#22810;&#36798;3000&#20493;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#20013;&#20301;&#25968;&#40065;&#26834;&#24615;&#20445;&#35777;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;CIFAR10&#30340;&#20013;&#20301;&#25968;&#35777;&#20070;&#20026;13&#20687;&#32032;&#65292;MNIST&#30340;&#20013;&#20301;&#25968;&#35777;&#20070;&#20026;12&#20687;&#32032;&#65292;Weather&#30340;&#20013;&#20301;&#25968;&#35777;&#20070;&#20026;4&#20010;&#29305;&#24449;&#65292;Ames&#30340;&#20013;&#20301;&#25968;&#35777;&#20070;&#20026;3&#20010;&#29305;&#24449;&#65289;&#65292;&#36825;&#24847;&#21619;&#30528;FPA&#33021;&#22815;&#20813;&#36153;&#25552;&#20379;&#39069;&#22806;&#30340;&#40065;&#26834;&#24615;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse or $\ell_0$ adversarial attacks arbitrarily perturb an unknown subset of the features. $\ell_0$ robustness analysis is particularly well-suited for heterogeneous (tabular) data where features have different types or scales. State-of-the-art $\ell_0$ certified defenses are based on randomized smoothing and apply to evasion attacks only. This paper proposes feature partition aggregation (FPA) -- a certified defense against the union of $\ell_0$ evasion, backdoor, and poisoning attacks. FPA generates its stronger robustness guarantees via an ensemble whose submodels are trained on disjoint feature sets. Compared to state-of-the-art $\ell_0$ defenses, FPA is up to 3,000${\times}$ faster and provides larger median robustness guarantees (e.g., median certificates of 13 pixels over 10 for CIFAR10, 12 pixels over 10 for MNIST, 4 features over 1 for Weather, and 3 features over 1 for Ames), meaning FPA provides the additional dimensions of robustness essentially for free.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20284;&#28982;&#36864;&#28779;&#30340;&#24555;&#36895;&#26657;&#20934;&#22238;&#24402;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#33021;&#22815;&#25913;&#36827;&#28145;&#24230;&#22238;&#24402;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#24182;&#20135;&#29983;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2302.11012</link><description>&lt;p&gt;
&#21487;&#29992;&#20110;&#22238;&#24402;&#30340;&#24555;&#36895;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#30340;&#20284;&#28982;&#36864;&#28779;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Likelihood Annealing: Fast Calibrated Uncertainty for Regression. (arXiv:2302.11012v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11012
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20284;&#28982;&#36864;&#28779;&#30340;&#24555;&#36895;&#26657;&#20934;&#22238;&#24402;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#33021;&#22815;&#25913;&#36827;&#28145;&#24230;&#22238;&#24402;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#24182;&#20135;&#29983;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#34920;&#26126;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22312;&#21307;&#23398;&#24433;&#20687;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#33258;&#20027;&#31995;&#32479;&#31561;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#36755;&#20986;&#31354;&#38388;&#36830;&#32493;&#30340;&#22238;&#24402;&#20219;&#21153;&#20013;&#12290;&#20801;&#35768;&#22238;&#24402;&#38382;&#39064;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#65292;&#24182;&#20135;&#29983;&#19981;&#33391;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#19981;&#33021;&#26377;&#25928;&#29992;&#20110;&#37327;&#21270;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#20107;&#21518;&#26657;&#20934;&#25216;&#26415;&#24456;&#23569;&#36866;&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#19988;&#24120;&#24120;&#32473;&#24050;&#32463;&#36739;&#24930;&#30340;&#27169;&#22411;&#35757;&#32451;&#38454;&#27573;&#22686;&#21152;&#20102;&#39069;&#22806;&#24320;&#38144;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22238;&#24402;&#20219;&#21153;&#30340;&#24555;&#36895;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#31216;&#20026;&#20284;&#28982;&#36864;&#28779;&#65292;&#23427;&#33021;&#22815;&#25345;&#32493;&#25913;&#36827;&#28145;&#24230;&#22238;&#24402;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#27809;&#26377;&#20219;&#20309;&#20107;&#21518;&#26657;&#20934;&#38454;&#27573;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep learning have shown that uncertainty estimation is becoming increasingly important in applications such as medical imaging, natural language processing, and autonomous systems. However, accurately quantifying uncertainty remains a challenging problem, especially in regression tasks where the output space is continuous. Deep learning approaches that allow uncertainty estimation for regression problems often converge slowly and yield poorly calibrated uncertainty estimates that can not be effectively used for quantification. Recently proposed post hoc calibration techniques are seldom applicable to regression problems and often add overhead to an already slow model training phase. This work presents a fast calibrated uncertainty estimation method for regression tasks called Likelihood Annealing, that consistently improves the convergence of deep regression models and yields calibrated uncertainty without any post hoc calibration phase. Unlike previous methods for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20197;&#23545;&#29305;&#23450;&#35302;&#21457;&#22120;&#20135;&#29983;&#29305;&#23450;&#36755;&#20986;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26080;&#27861;&#20998;&#26512;&#26410;&#30693;&#29305;&#24449;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.10894</link><description>&lt;p&gt;
&#20351;&#29992;&#29305;&#24449;&#21512;&#25104;&#24037;&#20855;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32418;&#38431;&#28436;&#32451;
&lt;/p&gt;
&lt;p&gt;
Red Teaming Deep Neural Networks with Feature Synthesis Tools. (arXiv:2302.10894v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20197;&#23545;&#29305;&#23450;&#35302;&#21457;&#22120;&#20135;&#29983;&#29305;&#23450;&#36755;&#20986;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26080;&#27861;&#20998;&#26512;&#26410;&#30693;&#29305;&#24449;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#36890;&#24120;&#26088;&#22312;&#29702;&#35299;&#27169;&#22411;&#22312;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#65288;OOD&#65289;&#30340;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#23613;&#31649;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#21463;&#21040;&#20102;&#20851;&#27880;&#65292;&#20294;&#22312;&#36825;&#20123;&#24037;&#20855;&#20013;&#24456;&#23569;&#26377;&#33021;&#22815;&#21457;&#29616;&#27169;&#22411;&#20013;&#30340;&#26032;&#39062;&#12289;&#20197;&#21069;&#26410;&#30693;&#30340;&#38169;&#35823;&#30340;&#26696;&#20363;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#37096;&#20998;&#21407;&#22240;&#22312;&#20110;&#35768;&#22810;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#20849;&#21516;&#29305;&#28857;&#65306;&#23427;&#20204;&#20351;&#29992;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#20998;&#26512;&#21644;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#34429;&#28982;&#36825;&#24456;&#26377;&#29992;&#65292;&#20294;&#36825;&#20123;&#24037;&#20855;&#21482;&#33021;&#20998;&#26512;&#29992;&#25143;&#21487;&#20197;&#20107;&#20808;&#37319;&#26679;&#25110;&#35782;&#21035;&#30340;&#29305;&#24449;&#25152;&#24341;&#21457;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#20010;&#19981;&#26029;&#22686;&#21152;&#30340;&#30740;&#31350;&#39046;&#22495;&#28041;&#21450;&#20351;&#29992;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#21512;&#25104;&#26041;&#27861;&#26469;&#35299;&#37322;&#27169;&#22411;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;&#25105;&#20204;&#21487;&#20197;&#35757;&#32451;&#27169;&#22411;&#20197;&#23545;&#29305;&#23450;&#35302;&#21457;&#22120;&#65288;&#20363;&#22914;&#65292;&#25554;&#20837;&#22270;&#20687;&#30340;&#29305;&#23450;&#34917;&#19969;&#65289;&#20135;&#29983;&#29305;&#23450;&#36755;&#20986;&#65288;&#21363;&#26631;&#31614;&#65289;&#65292;&#28982;&#21518;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable AI tools are often motivated by the goal of understanding model behavior in out-of-distribution (OOD) contexts. Despite the attention this area of study receives, there are comparatively few cases where these tools have identified novel, previously unknown, bugs in models. We argue that this is due, in part, to a common feature of many interpretability methods: they analyze and explain the behavior of a model using a particular dataset. While this is useful, such tools can only analyze behaviors induced by features that the user can sample or identify in advance. To address this, a growing body of research involves interpreting models using feature synthesis methods which do not depend on a dataset.  In this paper, our primary contribution is a benchmark to evaluate interpretability tools. Our key insight is that we can train models that respond to specific triggers (e.g., a specific patch inserted into an image) with specific outputs (i.e. a label) and then evaluate inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26174;&#33879;&#24615;&#25351;&#23548;&#30340;&#31070;&#32463;&#29305;&#24449;&#21387;&#32553;&#19982;&#27973;&#23618;&#21464;&#20998;&#29942;&#39048;&#27880;&#20837;&#30340;&#26032;&#30340;&#36164;&#28304;&#24847;&#35782;&#21387;&#32553;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;SC&#26041;&#27861;&#20302;60&#65285;&#30340;&#27604;&#29305;&#29575;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#30340;&#32534;&#35299;&#30721;&#26631;&#20934;&#30340;&#19979;&#25918;&#24555;16&#20493;&#12290;</title><link>http://arxiv.org/abs/2302.10681</link><description>&lt;p&gt;
FrankenSplit:&#22522;&#20110;&#26174;&#33879;&#24615;&#25351;&#23548;&#30340;&#31070;&#32463;&#29305;&#24449;&#21387;&#32553;&#19982;&#27973;&#23618;&#21464;&#20998;&#29942;&#39048;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
FrankenSplit: Saliency Guided Neural Feature Compression with Shallow Variational Bottleneck Injection. (arXiv:2302.10681v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26174;&#33879;&#24615;&#25351;&#23548;&#30340;&#31070;&#32463;&#29305;&#24449;&#21387;&#32553;&#19982;&#27973;&#23618;&#21464;&#20998;&#29942;&#39048;&#27880;&#20837;&#30340;&#26032;&#30340;&#36164;&#28304;&#24847;&#35782;&#21387;&#32553;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;SC&#26041;&#27861;&#20302;60&#65285;&#30340;&#27604;&#29305;&#29575;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#30340;&#32534;&#35299;&#30721;&#26631;&#20934;&#30340;&#19979;&#25918;&#24555;16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;AI&#21152;&#36895;&#22120;&#30340;&#23835;&#36215;&#20351;&#24471;&#23545;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#21487;&#20197;&#22312;&#23458;&#25143;&#31471;&#19978;&#25191;&#34892;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#24378;&#22823;&#27169;&#22411;&#30340;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#23558;&#35831;&#27714;&#19979;&#25918;&#65292;&#32780;&#39640;&#32500;&#25968;&#25454;&#23558;&#20105;&#22842;&#26377;&#38480;&#30340;&#24102;&#23485;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36164;&#28304;&#24847;&#35782;&#21387;&#32553;&#27169;&#22411;&#30340;&#26694;&#26550;&#24182;&#22312;&#21453;&#26144;&#36793;&#32536;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#19981;&#23545;&#31216;&#36164;&#28304;&#20998;&#37197;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;SC&#26041;&#27861;&#20302;60&#65285;&#30340;&#27604;&#29305;&#29575;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#30340;&#32534;&#35299;&#30721;&#26631;&#20934;&#30340;&#19979;&#25918;&#24555;16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of mobile AI accelerators allows latency-sensitive applications to execute lightweight Deep Neural Networks (DNNs) on the client side. However, critical applications require powerful models that edge devices cannot host and must therefore offload requests, where the high-dimensional data will compete for limited bandwidth. This work proposes shifting away from focusing on executing shallow layers of partitioned DNNs. Instead, it advocates concentrating the local resources on variational compression optimized for machine interpretability. We introduce a novel framework for resource-conscious compression models and extensively evaluate our method in an environment reflecting the asymmetric resource distribution between edge devices and servers. Our method achieves 60\% lower bitrate than a state-of-the-art SC method without decreasing accuracy and is up to 16x faster than offloading with existing codec standards.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#40657;&#30418;&#27169;&#22411;&#20013;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#40657;&#30418;&#27169;&#22411;&#20998;&#25104;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#21644;&#27531;&#24046;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#36827;&#34892;&#22522;&#26412;&#25512;&#29702;&#12290;&#27492;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#19988;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.10289</link><description>&lt;p&gt;
&#23558;&#40657;&#21283;&#23376;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#65306;&#36335;&#32447;&#35268;&#21010;&#65292;&#35299;&#37322;&#65292;&#37325;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat. (arXiv:2302.10289v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#40657;&#30418;&#27169;&#22411;&#20013;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#40657;&#30418;&#27169;&#22411;&#20998;&#25104;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#21644;&#27531;&#24046;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#36827;&#34892;&#22522;&#26412;&#25512;&#29702;&#12290;&#27492;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#19988;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#35201;&#20040;&#20174;&#35299;&#37322;&#24615;&#27169;&#22411;&#24320;&#22987;&#65292;&#35201;&#20040;&#20174;&#40657;&#30418;&#24320;&#22987;&#24182;&#20107;&#21518;&#35299;&#37322;&#12290;&#40657;&#30418;&#27169;&#22411;&#28789;&#27963;&#20294;&#38590;&#20197;&#35299;&#37322;&#65292;&#32780;&#35299;&#37322;&#24615;&#27169;&#22411;&#26412;&#36136;&#19978;&#26159;&#21487;&#35299;&#37322;&#30340;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#24615;&#27169;&#22411;&#38656;&#35201;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#30693;&#35782;&#65292;&#24182;&#19988;&#24448;&#24448;&#27604;&#23427;&#20204;&#30340;&#40657;&#30418;&#21464;&#20307;&#19981;&#22815;&#28789;&#27963;&#21644;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#26088;&#22312;&#27169;&#31946;&#40657;&#30418;&#30340;&#20107;&#21518;&#35299;&#37322;&#21644;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#20043;&#38388;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#20174;&#40657;&#30418;&#24320;&#22987;&#65292;&#36845;&#20195;&#22320;Carve&#20986;&#19968;&#31181;&#28151;&#21512;&#35299;&#37322;&#27169;&#22411;&#65288;MoIE&#65289;&#21644;&#19968;&#20010;&#27531;&#20313;&#32593;&#32476;&#12290;&#27599;&#20010;&#21487;&#35299;&#37322;&#27169;&#22411;&#19987;&#38376;&#22788;&#29702;&#19968;&#20010;&#26679;&#26412;&#23376;&#38598;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;(FOL)&#23545;&#20854;&#36827;&#34892;&#35299;&#37322;&#65292;&#20174;&#40657;&#30418;&#20013;&#25552;&#20379;&#22522;&#26412;&#25512;&#29702;&#27010;&#24565;&#12290;&#25105;&#20204;&#36890;&#36807;&#28789;&#27963;&#30340;&#27531;&#24046;&#36335;&#30001;&#20854;&#20313;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#27531;&#36716;&#32593;&#32476;&#19978;&#37325;&#22797;&#35813;&#26041;&#27861;&#65292;&#30452;&#21040;&#25152;&#26377;&#21487;&#35299;&#37322;&#27169;&#22411;&#35299;&#37322;&#25152;&#38656;&#27604;&#20363;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#36335;&#32447;&#35268;&#21010;&#65292;&#35299;&#37322;&#21644;&#37325;&#22797;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#30446;&#21069;&#20960;&#31181;&#40657;&#21283;&#23376;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
ML model design either starts with an interpretable model or a Blackbox and explains it post hoc. Blackbox models are flexible but difficult to explain, while interpretable models are inherently explainable. Yet, interpretable models require extensive ML knowledge and tend to be less flexible and underperforming than their Blackbox variants. This paper aims to blur the distinction between a post hoc explanation of a Blackbox and constructing interpretable models. Beginning with a Blackbox, we iteratively carve out a mixture of interpretable experts (MoIE) and a residual network. Each interpretable model specializes in a subset of samples and explains them using First Order Logic (FOL), providing basic reasoning on concepts from the Blackbox. We route the remaining samples through a flexible residual. We repeat the method on the residual network until all the interpretable models explain the desired proportion of data. Our extensive experiments show that our route, interpret, and repeat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22240;&#26524;&#27491;&#21017;&#21270;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#23545;&#20110;&#26576;&#20123;&#20013;&#38388;&#35745;&#31639;&#26469;&#35828;&#23384;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#36755;&#20837;&#65292;&#21487;&#20197;&#24320;&#21457;&#25968;&#25454;&#22686;&#24378;&#31243;&#24207;&#65292;&#29983;&#25104;&#33021;&#22815;&#20351;&#30446;&#26631;&#31639;&#27861;&#26377;&#23436;&#20840;&#30456;&#21516;&#19979;&#19968;&#36712;&#36857;&#27493;&#39588;&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#25552;&#39640;&#22312;&#20998;&#24067;&#22806;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.10258</link><description>&lt;p&gt;
&#20855;&#26377;&#22240;&#26524;&#27491;&#21017;&#21270;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Neural Algorithmic Reasoning with Causal Regularisation. (arXiv:2302.10258v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10258
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22240;&#26524;&#27491;&#21017;&#21270;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#23545;&#20110;&#26576;&#20123;&#20013;&#38388;&#35745;&#31639;&#26469;&#35828;&#23384;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#36755;&#20837;&#65292;&#21487;&#20197;&#24320;&#21457;&#25968;&#25454;&#22686;&#24378;&#31243;&#24207;&#65292;&#29983;&#25104;&#33021;&#22815;&#20351;&#30446;&#26631;&#31639;&#27861;&#26377;&#23436;&#20840;&#30456;&#21516;&#19979;&#19968;&#36712;&#36857;&#27493;&#39588;&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#25552;&#39640;&#22312;&#20998;&#24067;&#22806;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#30340;&#30740;&#31350;&#25506;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#35777;&#26126;&#20102;&#23427;&#20204;&#21487;&#20197;&#23398;&#20064;&#22312;&#35757;&#32451;&#20998;&#24067;&#20013;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#19978;&#25191;&#34892;&#32463;&#20856;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31070;&#32463;&#25512;&#29702;&#22120;&#22312;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#30340;&#27979;&#35797;&#25968;&#25454;&#19978;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#22240;&#20026;&#36755;&#20837;&#30340;&#35268;&#27169;&#26356;&#22823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#37325;&#35201;&#35266;&#23519;&#65306;&#23545;&#20110;&#31639;&#27861;&#26469;&#35828;&#65292;&#26377;&#24456;&#22810;&#19981;&#21516;&#30340;&#36755;&#20837;&#20250;&#34920;&#29616;&#20986;&#23436;&#20840;&#30456;&#21516;&#30340;&#20013;&#38388;&#35745;&#31639;&#12290;&#36825;&#31181;&#27934;&#23519;&#21147;&#20351;&#25105;&#20204;&#33021;&#22815;&#24320;&#21457;&#25968;&#25454;&#22686;&#24378;&#30340;&#31243;&#24207;&#65292;&#26681;&#25454;&#31639;&#27861;&#30340;&#20013;&#38388;&#36712;&#36857;&#29983;&#25104;&#33021;&#22815;&#20351;&#30446;&#26631;&#31639;&#27861;&#26377;&#23436;&#20840;&#30456;&#21516;&#19979;&#19968;&#36712;&#36857;&#27493;&#39588;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#30001;&#25105;&#20204;&#30340;&#35266;&#23519;&#23548;&#20986;&#24182;&#22312;&#22240;&#26524;&#22270;&#20013;&#24418;&#24335;&#21270;&#30340;&#33258;&#30417;&#30563;&#30446;&#26631;&#26469;&#30830;&#20445;&#36825;&#20123;&#36755;&#20837;&#19979;&#30340;&#19979;&#19968;&#27493;&#39044;&#27979;&#30340;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;&#27492;&#20135;&#29983;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;Hint-ReLIC&#65292;&#25913;&#21892;&#20102;OOD&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work on neural algorithmic reasoning has investigated the reasoning capabilities of neural networks, effectively demonstrating they can learn to execute classical algorithms on unseen data coming from the train distribution. However, the performance of existing neural reasoners significantly degrades on out-of-distribution (OOD) test data, where inputs have larger sizes. In this work, we make an important observation: there are many different inputs for which an algorithm will perform certain intermediate computations identically. This insight allows us to develop data augmentation procedures that, given an algorithm's intermediate trajectory, produce inputs for which the target algorithm would have exactly the same next trajectory step. We ensure invariance in the next-step prediction across such inputs, by employing a self-supervised objective derived by our observation, formalised in a causal graph. We prove that the resulting method, which we call Hint-ReLIC, improves the OO
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#35770;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21033;&#29992;&#25299;&#25169;&#23398;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#22312;16&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20986;&#20248;&#20110;&#25110;&#21305;&#37197;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.09543</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#23398;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#22270;&#35770;&#30340;&#36807;&#28388;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Topological Feature Selection: A Graph-Based Filter Feature Selection Approach. (arXiv:2302.09543v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#35770;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21033;&#29992;&#25299;&#25169;&#23398;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#22312;16&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20986;&#20248;&#20110;&#25110;&#21305;&#37197;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;&#22270;&#35770;&#36807;&#28388;&#26041;&#27861;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#20102;&#25299;&#25169;&#32422;&#26463;&#32593;&#32476;&#34920;&#31034;&#30340;&#23041;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#24358;&#22270;&#65288;&#19977;&#35282;&#26368;&#22823;&#36807;&#28388;&#22270;&#65289;&#27169;&#25311;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#29305;&#24449;&#22312;&#32593;&#32476;&#20869;&#30340;&#30456;&#23545;&#20301;&#32622;&#26469;&#26368;&#22823;&#21270;&#29305;&#24449;&#30456;&#20851;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#26377;&#19977;&#20010;&#29305;&#28857;&#65306;&#65288;i&#65289;&#39640;&#24230;&#21487;&#35843;&#65292;&#26131;&#20110;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#30340;&#24615;&#36136;&#65307;&#65288;ii&#65289;&#23436;&#20840;&#21487;&#35299;&#37322;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#26174;&#33879;&#30340;&#31616;&#21333;&#24615;&#65307;&#65288;iii&#65289;&#35745;&#31639;&#25104;&#26412;&#27604;&#20854;&#26367;&#20195;&#26041;&#26696;&#26356;&#21152;&#20415;&#23452;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;16&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#24322;&#26500;&#35780;&#20272;&#26465;&#20214;&#19979;&#65292;&#23427;&#20248;&#20110;&#25110;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel unsupervised, graph-based filter feature selection technique which exploits the power of topologically constrained network representations. We model dependency structures among features using a family of chordal graphs (the Triangulated Maximally Filtered Graph), and we maximise the likelihood of features' relevance by studying their relative position inside the network. Such an approach presents three aspects that are particularly satisfactory compared to its alternatives: (i) it is highly tunable and easily adaptable to the nature of input data; (ii) it is fully explainable, maintaining, at the same time, a remarkable level of simplicity; (iii) it is computationally cheaper compared to its alternatives. We test our algorithm on 16 benchmark datasets from different applicative domains showing that it outperforms or matches the current state-of-the-art under heterogeneous evaluation conditions.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;XR&#29992;&#25143;&#22522;&#20110;&#36816;&#21160;&#30340;&#35782;&#21035;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20165;&#20351;&#29992;&#23569;&#37327;&#30340;&#27880;&#20876;&#25968;&#25454;&#26469;&#35782;&#21035;&#26032;&#29992;&#25143;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#27880;&#20876;&#26032;&#29992;&#25143;&#65292;&#32780;&#19988;&#22312;&#20165;&#26377;&#23569;&#37327;&#27880;&#20876;&#25968;&#25454;&#21487;&#29992;&#26102;&#20063;&#26356;&#21487;&#38752;&#12290;</title><link>http://arxiv.org/abs/2302.07517</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#29305;&#23450;&#36816;&#21160;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;XR&#29992;&#25143;&#22522;&#20110;&#36816;&#21160;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Extensible Motion-based Identification of XR Users using Non-Specific Motion Data. (arXiv:2302.07517v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07517
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;XR&#29992;&#25143;&#22522;&#20110;&#36816;&#21160;&#30340;&#35782;&#21035;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20165;&#20351;&#29992;&#23569;&#37327;&#30340;&#27880;&#20876;&#25968;&#25454;&#26469;&#35782;&#21035;&#26032;&#29992;&#25143;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#27880;&#20876;&#26032;&#29992;&#25143;&#65292;&#32780;&#19988;&#22312;&#20165;&#26377;&#23569;&#37327;&#27880;&#20876;&#25968;&#25454;&#21487;&#29992;&#26102;&#20063;&#26356;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#24335;&#21644;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#23558;&#36317;&#31163;&#21644;&#20998;&#31867;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#36890;&#36807;&#29992;&#25143;&#30340;&#36816;&#21160;&#26469;&#35782;&#21035;&#25193;&#23637;&#29616;&#23454;&#29992;&#25143;&#12290;&#25105;&#20204;&#22312;&#8220;&#21322;&#34928;&#26399;&#65306;Alyx&#8221;VR&#28216;&#25103;&#30340;&#29992;&#25143;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#30340;&#22522;&#32447;&#20998;&#31867;&#27169;&#22411;&#20316;&#20026;&#23545;&#27604;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#23884;&#20837;&#24335;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21482;&#20351;&#29992;&#20960;&#20998;&#38047;&#30340;&#27880;&#20876;&#25968;&#25454;&#65292;&#35782;&#21035;&#26032;&#29992;&#25143;&#30340;&#38750;&#29305;&#23450;&#36816;&#21160;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#27880;&#20876;&#26032;&#29992;&#25143;&#65292;&#32780;&#37325;&#26032;&#35757;&#32451;&#22522;&#32447;&#26041;&#27861;&#38656;&#35201;&#33457;&#36153;&#23558;&#36817;&#19968;&#22825;&#30340;&#26102;&#38388;&#65292;&#24403;&#21482;&#26377;&#24456;&#23569;&#30340;&#27880;&#20876;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#21487;&#38752;&#65292;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#20351;&#29992;&#19981;&#21516;VR&#35774;&#22791;&#35760;&#24405;&#30340;&#26032;&#29992;&#25143;&#25968;&#25454;&#38598;&#12290;&#32508;&#19978;&#25152;&#36848;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20026;&#26131;&#20110;&#25193;&#23637;&#30340;XR&#29992;&#25143;&#35782;&#21035;&#31995;&#32479;&#22880;&#23450;&#22522;&#30784;&#65292;&#21487;&#24212;&#29992;&#20110;&#24191;&#27867;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we combine the strengths of distance-based and classification-based approaches for the task of identifying extended reality users by their movements. For this we present an embedding-based approach that leverages deep metric learning. We train the model on a dataset of users playing the VR game ``Half-Life: Alyx'' and conduct multiple experiments and analyses using a state of the art classification-based model as baseline. The results show that the embedding-based method 1) is able to identify new users from non-specific movements using only a few minutes of enrollment data, 2) can enroll new users within seconds, while retraining the baseline approach takes almost a day, 3) is more reliable than the baseline approach when only little enrollment data is available, 4) can be used to identify new users from another dataset recorded with different VR devices.  Altogether, our solution is a foundation for easily extensible XR user identification systems, applicable to a wide
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;E-&#20540;&#26469;&#37327;&#21270;&#32479;&#35745;&#26174;&#33879;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#38543;&#26426;&#21270;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#31283;&#23450;&#22320;&#32858;&#21512;&#30456;&#21516;&#25968;&#25454;&#30340;&#22810;&#27425;&#20998;&#26512;&#30340;&#35777;&#25454;&#65292;&#21516;&#26102;&#25511;&#21046;&#34394;&#20551;&#21457;&#29616;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.07294</link><description>&lt;p&gt;
&#20351;&#29992;&#32479;&#19968;&#30340;E-&#20540;&#36890;&#36807;FDR&#25511;&#21046;&#21435;&#38543;&#26426;&#21270;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Derandomized Novelty Detection with FDR Control via Conformal E-values. (arXiv:2302.07294v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07294
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;E-&#20540;&#26469;&#37327;&#21270;&#32479;&#35745;&#26174;&#33879;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#38543;&#26426;&#21270;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#31283;&#23450;&#22320;&#32858;&#21512;&#30456;&#21516;&#25968;&#25454;&#30340;&#22810;&#27425;&#20998;&#26512;&#30340;&#35777;&#25454;&#65292;&#21516;&#26102;&#25511;&#21046;&#34394;&#20551;&#21457;&#29616;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#19968;&#25512;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26080;&#20998;&#24067;&#26041;&#27861;&#65292;&#29992;&#20110;&#20005;&#26684;&#26657;&#20934;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#36755;&#20986;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#26377;&#24456;&#22810;&#20248;&#28857;&#65292;&#20294;&#23427;&#20063;&#26377;&#38543;&#26426;&#24615;&#30340;&#38480;&#21046;&#65292;&#21363;&#22312;&#20998;&#26512;&#30456;&#21516;&#25968;&#25454;&#26102;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#21516;&#30340;&#32467;&#26524;&#65292;&#36825;&#21487;&#33021;&#20250;&#22952;&#30861;&#23545;&#20219;&#20309;&#21457;&#29616;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#21033;&#29992;&#36866;&#24403;&#30340;&#32479;&#19968;E-&#20540;&#32780;&#19981;&#26159;p-&#20540;&#26469;&#37327;&#21270;&#32479;&#35745;&#26174;&#33879;&#24615;&#65292;&#20351;&#32479;&#19968;&#25512;&#29702;&#26356;&#21152;&#31283;&#23450;&#12290;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#20801;&#35768;&#26377;&#25928;&#22320;&#27719;&#24635;&#23545;&#30456;&#21516;&#25968;&#25454;&#22810;&#27425;&#20998;&#26512;&#30340;&#35777;&#25454;&#65292;&#21516;&#26102;&#21487;&#38752;&#22320;&#25511;&#21046;&#34394;&#20551;&#21457;&#29616;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19982;&#26631;&#20934;&#32479;&#19968;&#25512;&#29702;&#30456;&#27604;&#65292;&#25152;&#25552;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#38543;&#26426;&#24615;&#32780;&#19981;&#20250;&#25439;&#22833;&#22826;&#22810;&#21151;&#29575;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#22522;&#20110;&#20174;&#30456;&#21516;&#25968;&#25454;&#20013;&#31934;&#24515;&#25552;&#21462;&#30340;&#38468;&#21152;&#36741;&#21161;&#20449;&#24687;&#26469;&#21152;&#26435;&#32479;&#19968;E-&#20540;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#30340;&#27169;&#25311;&#23454;&#39564;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Conformal inference provides a general distribution-free method to rigorously calibrate the output of any machine learning algorithm for novelty detection. While this approach has many strengths, it has the limitation of being randomized, in the sense that it may lead to different results when analyzing twice the same data, and this can hinder the interpretation of any findings. We propose to make conformal inferences more stable by leveraging suitable conformal e-values instead of p-values to quantify statistical significance. This solution allows the evidence gathered from multiple analyses of the same data to be aggregated effectively while provably controlling the false discovery rate. Further, we show that the proposed method can reduce randomness without much loss of power compared to standard conformal inference, partly thanks to an innovative way of weighting conformal e-values based on additional side information carefully extracted from the same data. Simulations with synthet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;Fine-tuned&#35821;&#35328;&#27169;&#22411;&#20013;&#20219;&#21153;&#29305;&#23450;&#25216;&#33021;&#23450;&#20301;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20248;&#21270;&#21487;&#20197;&#35782;&#21035;&#20986;&#36129;&#29486;&#27169;&#22411;&#24615;&#33021;&#30340;&#38750;&#24120;&#23567;&#30340;&#21442;&#25968;&#23376;&#38598;&#65292;&#20351;&#24471;&#23558;Fine-tuned&#30340;&#20540;&#23233;&#25509;&#21040;&#36825;&#20010;&#23376;&#38598;&#19978;&#21487;&#20197;&#33719;&#24471;&#20960;&#20046;&#21644;Fine-tuned&#27169;&#22411;&#19968;&#26679;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.06600</link><description>&lt;p&gt;
&#38024;&#23545;&#20219;&#21153;&#30340;&#25216;&#33021;&#23450;&#20301;&#22312;Fine-tuned&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Task-Specific Skill Localization in Fine-tuned Language Models. (arXiv:2302.06600v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;Fine-tuned&#35821;&#35328;&#27169;&#22411;&#20013;&#20219;&#21153;&#29305;&#23450;&#25216;&#33021;&#23450;&#20301;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20248;&#21270;&#21487;&#20197;&#35782;&#21035;&#20986;&#36129;&#29486;&#27169;&#22411;&#24615;&#33021;&#30340;&#38750;&#24120;&#23567;&#30340;&#21442;&#25968;&#23376;&#38598;&#65292;&#20351;&#24471;&#23558;Fine-tuned&#30340;&#20540;&#23233;&#25509;&#21040;&#36825;&#20010;&#23376;&#38598;&#19978;&#21487;&#20197;&#33719;&#24471;&#20960;&#20046;&#21644;Fine-tuned&#27169;&#22411;&#19968;&#26679;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;Fine-tuned&#26469;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#65292;&#21253;&#25324;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;Fine-tuning&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#24555;&#36895;&#25484;&#25569;&#20219;&#21153;&#29305;&#23450;&#30340;&#8220;&#25216;&#33021;&#8221;&#65292;&#20294;&#26159;&#20851;&#20110;&#36825;&#20123;&#26032;&#23398;&#21040;&#30340;&#25216;&#33021;&#22312;&#24222;&#22823;&#27169;&#22411;&#20013;&#30340;&#20301;&#32622;&#30340;&#30740;&#31350;&#36824;&#24456;&#26377;&#38480;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#25216;&#33021;&#23450;&#20301;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#32473;&#23450;&#19979;&#28216;&#20219;&#21153;&#21644;&#22312;&#35813;&#20219;&#21153;&#19978;&#36827;&#34892;Fine-tuned&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#31616;&#21333;&#30340;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#20986;&#36127;&#36131;&#27169;&#22411;&#24615;&#33021;&#30340;&#38750;&#24120;&#23567;&#30340;&#21442;&#25968;&#23376;&#38598;&#65288;&#21344;&#27169;&#22411;&#21442;&#25968;&#30340;&#32422;0.01%&#65289;&#65292;&#36825;&#20010;&#23376;&#38598;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36129;&#29486;&#21344;&#27604;&#36229;&#36807;95%&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#20165;&#23558;Fine-tuned&#30340;&#20540;&#23233;&#25509;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36825;&#20010;&#23567;&#23376;&#38598;&#19978;&#65292;&#23601;&#21487;&#20197;&#33719;&#24471;&#20960;&#20046;&#21644;Fine-tuned&#27169;&#22411;&#19968;&#26679;&#22909;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#19982;&#26368;&#36817;&#20851;&#20110;&#21442;&#25968;&#39640;&#25928;Fine-tuning&#30340;&#24037;&#20316;&#30456;&#20284;&#65292;&#20294;&#36825;&#37324;&#30340;&#20004;&#20010;&#26032;&#39062;&#20043;&#22788;&#26159;&#65306;&#65288;i&#65289;&#19981;&#38656;&#35201;&#22312;&#23376;&#38598;&#19978;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#37325;&#26032;&#35757;&#32451;&#65288;&#19981;&#20687;&#8220;lottery tickets&#8221;&#37027;&#26679;&#65289;&#12290;&#65288;ii&#65289;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;Fine-tuned&#27169;&#22411;&#65292;&#21487;&#20197;&#30475;&#21040;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models can be fine-tuned to solve diverse NLP tasks, including in few-shot settings. Thus fine-tuning allows the model to quickly pick up task-specific ``skills,'' but there has been limited study of where these newly-learnt skills reside inside the massive model. This paper introduces the term skill localization for this problem and proposes a solution. Given the downstream task and a model fine-tuned on that task, a simple optimization is used to identify a very small subset of parameters ($\sim0.01$% of model parameters) responsible for ($&gt;95$%) of the model's performance, in the sense that grafting the fine-tuned values for just this tiny subset onto the pre-trained model gives performance almost as well as the fine-tuned model. While reminiscent of recent works on parameter-efficient fine-tuning, the novel aspects here are that: (i) No further re-training is needed on the subset (unlike, say, with lottery tickets). (ii) Notable improvements are seen over vanil
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#23618;&#24494;&#35843;&#19982;&#23376;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#23545;&#31934;&#24515;&#36873;&#25321;&#30340;&#23618;&#36827;&#34892;&#24494;&#35843;&#65292;&#32780;&#23558;&#20854;&#20313;&#26435;&#37325;&#20445;&#25345;&#22312;&#39044;&#35757;&#32451;&#20540;&#19978;&#12290;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#19978;&#33021;&#22815;&#19982;&#20840;&#27169;&#22411;&#24494;&#35843;&#30456;&#23218;&#32654;&#65292;&#24182;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#26102;&#34920;&#29616;&#26356;&#22909;&#12290;&#36825;&#19968;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#24182;&#33021;&#22815;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23454;&#29616;&#20219;&#21153;&#38388;&#30340;&#36164;&#28304;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2302.06354</link><description>&lt;p&gt;
&#23569;&#21363;&#26159;&#22810;&#65306;&#36873;&#25321;&#24615;&#23618;&#24494;&#35843;&#19982;&#23376;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Less is More: Selective Layer Finetuning with SubTuning. (arXiv:2302.06354v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#23618;&#24494;&#35843;&#19982;&#23376;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#23545;&#31934;&#24515;&#36873;&#25321;&#30340;&#23618;&#36827;&#34892;&#24494;&#35843;&#65292;&#32780;&#23558;&#20854;&#20313;&#26435;&#37325;&#20445;&#25345;&#22312;&#39044;&#35757;&#32451;&#20540;&#19978;&#12290;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#19978;&#33021;&#22815;&#19982;&#20840;&#27169;&#22411;&#24494;&#35843;&#30456;&#23218;&#32654;&#65292;&#24182;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#26102;&#34920;&#29616;&#26356;&#22909;&#12290;&#36825;&#19968;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#24182;&#33021;&#22815;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23454;&#29616;&#20219;&#21153;&#38388;&#30340;&#36164;&#28304;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#24050;&#25104;&#20026;&#22312;&#26032;&#20219;&#21153;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#23548;&#33268;&#20102;&#24555;&#36895;&#25910;&#25947;&#21644;&#25913;&#21892;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26367;&#20195;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21363;&#19981;&#23545;&#32593;&#32476;&#30340;&#25152;&#26377;&#26435;&#37325;&#36827;&#34892;&#24494;&#35843;&#65292;&#32780;&#26159;&#21482;&#35757;&#32451;&#19968;&#32452;&#31934;&#24515;&#36873;&#25321;&#30340;&#23618;&#65292;&#20351;&#20854;&#20313;&#30340;&#26435;&#37325;&#20445;&#25345;&#22312;&#20854;&#21021;&#22987;&#65288;&#39044;&#35757;&#32451;&#65289;&#20540;&#19978;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;\emph{&#23376;&#24494;&#35843;}&#65288;SubTuning&#65289;&#32463;&#24120;&#33021;&#22815;&#36798;&#21040;&#19982;&#23545;&#27169;&#22411;&#36827;&#34892;&#20840;&#24494;&#35843;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#26102;&#29978;&#33267;&#36229;&#36807;&#20102;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;SubTuning&#20801;&#35768;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#37096;&#32626;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#20139;&#21463;&#25972;&#20010;&#27169;&#22411;&#24494;&#35843;&#30340;&#22909;&#22788;&#12290;&#36825;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#26102;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#19981;&#24178;&#25200;&#65292;&#32780;&#22312;&#22823;&#37096;&#20998;&#36164;&#28304;&#19978;&#20849;&#20139;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;SubTuning&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning a pretrained model has become a standard approach for training neural networks on novel tasks, resulting in fast convergence and improved performance. In this work, we study an alternative finetuning method, where instead of finetuning all the weights of the network, we only train a carefully chosen subset of layers, keeping the rest of the weights frozen at their initial (pretrained) values. We demonstrate that \emph{subset finetuning} (or SubTuning) often achieves accuracy comparable to full finetuning of the model, and even surpasses the performance of full finetuning when training data is scarce. Therefore, SubTuning allows deploying new tasks at minimal computational cost, while enjoying the benefits of finetuning the entire model. This yields a simple and effective method for multi-task learning, where different tasks do not interfere with one another, and yet share most of the resources at inference time. We demonstrate the efficiency of SubTuning across multiple task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#31070;&#32463;&#24418;&#24577;&#25968;&#25454;&#21644;&#22810;&#26679;&#21270;&#30340;&#21050;&#28608;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.06279</link><description>&lt;p&gt;
Sneaky Spikes: &#29992;&#31070;&#32463;&#24418;&#24577;&#25968;&#25454;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#25581;&#31034;&#38544;&#34109;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data. (arXiv:2302.06279v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#31070;&#32463;&#24418;&#24577;&#25968;&#25454;&#21644;&#22810;&#26679;&#21270;&#30340;&#21050;&#28608;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#22270;&#20687;&#21644;&#35821;&#38899;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#26368;&#22823;&#21270;DNN&#30340;&#25928;&#26524;&#38656;&#35201;&#36890;&#36807;&#35757;&#32451;&#23545;&#20247;&#22810;&#36229;&#21442;&#25968;&#21644;&#32593;&#32476;&#21442;&#25968;&#36827;&#34892;&#31934;&#32454;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#39640;&#24615;&#33021;&#30340;DNN&#28041;&#21450;&#35768;&#22810;&#21442;&#25968;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28040;&#32791;&#22823;&#37327;&#33021;&#28304;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#30740;&#31350;&#20154;&#21592;&#36716;&#21521;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#65292;&#20854;&#25552;&#20379;&#20102;&#22686;&#24378;&#30340;&#33021;&#28304;&#25928;&#29575;&#21644;&#29983;&#29289;&#23398;&#21487;&#34892;&#30340;&#25968;&#25454;&#22788;&#29702;&#33021;&#21147;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#24863;&#30693;&#25968;&#25454;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#31070;&#32463;&#24418;&#24577;&#25968;&#25454;&#26041;&#38754;&#12290;&#23613;&#31649;&#23384;&#22312;&#20248;&#21183;&#65292;SNN&#19982;DNN&#19968;&#26679;&#65292;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#23041;&#32961;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#23545;&#25239;&#24615;&#31034;&#20363;&#21644;&#21518;&#38376;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;SNN&#22312;&#29702;&#35299;&#21644;&#23545;&#25239;&#36825;&#20123;&#25915;&#20987;&#26041;&#38754;&#65292;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#24418;&#24577;&#25968;&#25454;&#21644;&#22810;&#26679;&#21270;&#30340;&#21050;&#28608;&#30340;SNN&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have demonstrated remarkable performance across various tasks, including image and speech recognition. However, maximizing the effectiveness of DNNs requires meticulous optimization of numerous hyperparameters and network parameters through training. Moreover, high-performance DNNs entail many parameters, which consume significant energy during training. In order to overcome these challenges, researchers have turned to spiking neural networks (SNNs), which offer enhanced energy efficiency and biologically plausible data processing capabilities, rendering them highly suitable for sensory data tasks, particularly in neuromorphic data. Despite their advantages, SNNs, like DNNs, are susceptible to various threats, including adversarial examples and backdoor attacks. Yet, the field of SNNs still needs to be explored in terms of understanding and countering these attacks.  This paper delves into backdoor attacks in SNNs using neuromorphic datasets and diverse trig
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#26680;&#24515;&#27979;&#35797;&#30340;&#39640;&#32500;U&#32479;&#35745;&#30340;&#25910;&#25947;&#23450;&#29702;&#65292;&#24182;&#21457;&#29616;U&#32479;&#35745;&#30340;&#26497;&#38480;&#20998;&#24067;&#20250;&#32463;&#21382;&#20174;&#38750;&#36864;&#21270;&#39640;&#26031;&#26497;&#38480;&#21040;&#36864;&#21270;&#26497;&#38480;&#30340;&#30456;&#21464;&#12290;&#36825;&#19968;&#29616;&#35937;&#23545;&#20110;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#38750;&#36864;&#21270;U&#32479;&#35745;&#20855;&#26377;&#36739;&#22823;&#26041;&#24046;&#21644;&#19981;&#23545;&#31216;&#20998;&#24067;&#30340;&#38750;&#39640;&#26031;&#26497;&#38480;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#30028;&#38480;&#36866;&#29992;&#20110;&#20219;&#20309;&#26377;&#38480;&#25968;&#37327;&#21644;&#32500;&#24230;&#30340;&#26679;&#26412;&#65292;&#19982;&#24213;&#23618;&#20989;&#25968;&#30340;&#29305;&#24449;&#20540;&#26080;&#20851;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#19982;&#32500;&#24230;&#26080;&#20851;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#29702;&#35770;&#24212;&#29992;&#21040;&#20004;&#20010;&#24120;&#29992;&#30340;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#20998;&#24067;&#27979;&#35797;&#26041;&#27861;&#65292;MMD&#21644;KSD&#65292;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#39640;&#32500;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#27979;&#35797;&#21151;&#29575;&#22914;&#20309;&#19982;&#32500;&#24230;&#21644;&#24102;&#23485;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.05686</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#26680;&#24515;&#27979;&#35797;&#30340;U&#32479;&#35745;&#30340;&#39640;&#32500;&#25910;&#25947;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
A High-dimensional Convergence Theorem for U-statistics with Applications to Kernel-based Testing. (arXiv:2302.05686v3 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#26680;&#24515;&#27979;&#35797;&#30340;&#39640;&#32500;U&#32479;&#35745;&#30340;&#25910;&#25947;&#23450;&#29702;&#65292;&#24182;&#21457;&#29616;U&#32479;&#35745;&#30340;&#26497;&#38480;&#20998;&#24067;&#20250;&#32463;&#21382;&#20174;&#38750;&#36864;&#21270;&#39640;&#26031;&#26497;&#38480;&#21040;&#36864;&#21270;&#26497;&#38480;&#30340;&#30456;&#21464;&#12290;&#36825;&#19968;&#29616;&#35937;&#23545;&#20110;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#38750;&#36864;&#21270;U&#32479;&#35745;&#20855;&#26377;&#36739;&#22823;&#26041;&#24046;&#21644;&#19981;&#23545;&#31216;&#20998;&#24067;&#30340;&#38750;&#39640;&#26031;&#26497;&#38480;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#30028;&#38480;&#36866;&#29992;&#20110;&#20219;&#20309;&#26377;&#38480;&#25968;&#37327;&#21644;&#32500;&#24230;&#30340;&#26679;&#26412;&#65292;&#19982;&#24213;&#23618;&#20989;&#25968;&#30340;&#29305;&#24449;&#20540;&#26080;&#20851;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#19982;&#32500;&#24230;&#26080;&#20851;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#29702;&#35770;&#24212;&#29992;&#21040;&#20004;&#20010;&#24120;&#29992;&#30340;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#20998;&#24067;&#27979;&#35797;&#26041;&#27861;&#65292;MMD&#21644;KSD&#65292;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#39640;&#32500;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#27979;&#35797;&#21151;&#29575;&#22914;&#20309;&#19982;&#32500;&#24230;&#21644;&#24102;&#23485;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;U&#32479;&#35745;&#30340;&#20108;&#27425;&#25910;&#25947;&#23450;&#29702;&#65292;&#20854;&#20013;&#25968;&#25454;&#32500;&#24230;$d$&#21487;&#20197;&#38543;&#26679;&#26412;&#22823;&#23567;$n$&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20010;U&#32479;&#35745;&#30340;&#26497;&#38480;&#20998;&#24067;&#20250;&#32463;&#21382;&#20174;&#38750;&#36864;&#21270;&#39640;&#26031;&#26497;&#38480;&#21040;&#36864;&#21270;&#26497;&#38480;&#30340;&#30456;&#21464;&#65292;&#19981;&#35770;&#20854;&#36864;&#21270;&#24615;&#22914;&#20309;&#65292;&#21482;&#21462;&#20915;&#20110;&#19968;&#20010;&#30697;&#27604;&#29575;&#12290;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#26159;&#65292;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#38750;&#36864;&#21270;&#30340;U&#32479;&#35745;&#21487;&#33021;&#20855;&#26377;&#19968;&#20010;&#20855;&#26377;&#36739;&#22823;&#26041;&#24046;&#21644;&#19981;&#23545;&#31216;&#20998;&#24067;&#30340;&#38750;&#39640;&#26031;&#26497;&#38480;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#23545;&#20219;&#20309;&#26377;&#38480;&#30340;$n$&#21644;$d$&#37117;&#26159;&#26377;&#25928;&#30340;&#65292;&#19982;&#24213;&#23618;&#20989;&#25968;&#30340;&#20010;&#21035;&#29305;&#24449;&#20540;&#26080;&#20851;&#65292;&#24182;&#19988;&#22312;&#19968;&#20010;&#36866;&#24230;&#30340;&#20551;&#35774;&#19979;&#19982;&#32500;&#24230;&#26080;&#20851;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#29702;&#35770;&#24212;&#29992;&#21040;&#20004;&#20010;&#27969;&#34892;&#30340;&#22522;&#20110;&#26680;&#24515;&#30340;&#20998;&#24067;&#27979;&#35797;&#65292;MMD&#21644;KSD&#19978;&#65292;&#36825;&#20123;&#27979;&#35797;&#22312;&#39640;&#32500;&#24615;&#33021;&#30340;&#30740;&#31350;&#19968;&#30452;&#26159;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#32463;&#39564;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#27491;&#30830;&#22320;&#39044;&#27979;&#20102;&#22312;&#22266;&#23450;&#38408;&#20540;&#19979;&#27979;&#35797;&#21151;&#29575;&#22914;&#20309;&#38543;&#30528;$d$&#21644;&#24102;&#23485;&#30340;&#32553;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove a convergence theorem for U-statistics of degree two, where the data dimension $d$ is allowed to scale with sample size $n$. We find that the limiting distribution of a U-statistic undergoes a phase transition from the non-degenerate Gaussian limit to the degenerate limit, regardless of its degeneracy and depending only on a moment ratio. A surprising consequence is that a non-degenerate U-statistic in high dimensions can have a non-Gaussian limit with a larger variance and asymmetric distribution. Our bounds are valid for any finite $n$ and $d$, independent of individual eigenvalues of the underlying function, and dimension-independent under a mild assumption. As an application, we apply our theory to two popular kernel-based distribution tests, MMD and KSD, whose high-dimensional performance has been challenging to study. In a simple empirical setting, our results correctly predict how the test power at a fixed threshold scales with $d$ and the bandwidth.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#22238;&#39038;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#35299;&#20915;&#22522;&#20110;&#29366;&#24577;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#23433;&#20840;&#20445;&#38556;&#21644;&#21487;&#25193;&#23637;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#22870;&#21169;&#34920;&#29616;&#12289;&#25910;&#25947;&#21518;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#30340;&#32852;&#31995;&#12289;&#24046;&#24322;&#21644;&#26435;&#34913;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.03122</link><description>&lt;p&gt;
&#22522;&#20110;&#29366;&#24577;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
State-wise Safe Reinforcement Learning: A Survey. (arXiv:2302.03122v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#22238;&#39038;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#35299;&#20915;&#22522;&#20110;&#29366;&#24577;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#23433;&#20840;&#20445;&#38556;&#21644;&#21487;&#25193;&#23637;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#22870;&#21169;&#34920;&#29616;&#12289;&#25910;&#25947;&#21518;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#30340;&#32852;&#31995;&#12289;&#24046;&#24322;&#21644;&#26435;&#34913;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#20173;&#28982;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#23433;&#20840;&#24615;&#65292;&#20063;&#23601;&#26159;&#32422;&#26463;&#28385;&#36275;&#12290;&#29366;&#24577;&#32422;&#26463;&#26159;&#23454;&#38469;&#24212;&#29992;&#20013;&#26368;&#24120;&#35265;&#19988;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#32422;&#26463;&#20043;&#19968;&#65292;&#36825;&#23545;&#20110;&#35768;&#22810;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#26426;&#22120;&#20154;&#25805;&#20316;&#31561;&#32780;&#35328;&#26159;&#24517;&#35201;&#21644;&#20851;&#38190;&#30340;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#35299;&#20915;&#22522;&#20110;&#29366;&#24577;&#30340;&#32422;&#26463;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;&#29366;&#24577;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26694;&#26550;&#19979;&#65292;&#20174;&#23433;&#20840;&#20445;&#38556;&#21644;&#21487;&#25193;&#23637;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#22870;&#21169;&#34920;&#29616;&#12289;&#25910;&#25947;&#21518;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#65292;&#35752;&#35770;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#32852;&#31995;&#12289;&#24046;&#24322;&#21644;&#26435;&#34913;&#12290;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the tremendous success of Reinforcement Learning (RL) algorithms in simulation environments, applying RL to real-world applications still faces many challenges. A major concern is safety, in another word, constraint satisfaction. State-wise constraints are one of the most common constraints in real-world applications and one of the most challenging constraints in Safe RL. Enforcing state-wise constraints is necessary and essential to many challenging tasks such as autonomous driving, robot manipulation. This paper provides a comprehensive review of existing approaches that address state-wise constraints in RL. Under the framework of State-wise Constrained Markov Decision Process (SCMDP), we will discuss the connections, differences, and trade-offs of existing approaches in terms of (i) safety guarantee and scalability, (ii) safety and reward performance, and (iii) safety after convergence and during training. We also summarize limitations of current methods and discuss potentia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#28508;&#31354;&#38388;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;LSBO&#65289;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#26041;&#27861;&#21253;&#25324;&#28508;&#22312;&#19968;&#33268;&#24615;&#24863;&#30693;&#33719;&#21462;&#20989;&#25968;&#65288;LCA-AF&#65289;&#21644;&#22686;&#21152;&#19968;&#33268;&#24615;&#28857;&#30340;&#28508;&#31354;&#38388;&#29983;&#25104;&#26041;&#27861;&#65288;LCA-VAE&#65289;&#65292;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#24418;&#25104;&#20102;LCA-LSBO&#12290;&#23454;&#39564;&#35777;&#26126;LCA-LSBO&#22312;&#22270;&#20687;&#29983;&#25104;&#21644;&#20840;&#26032;&#30340;&#21270;&#23398;&#35774;&#35745;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.02399</link><description>&lt;p&gt;
&#22686;&#24378;&#28508;&#31354;&#38388;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Exploration in Latent Space Bayesian Optimization. (arXiv:2302.02399v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#28508;&#31354;&#38388;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;LSBO&#65289;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#26041;&#27861;&#21253;&#25324;&#28508;&#22312;&#19968;&#33268;&#24615;&#24863;&#30693;&#33719;&#21462;&#20989;&#25968;&#65288;LCA-AF&#65289;&#21644;&#22686;&#21152;&#19968;&#33268;&#24615;&#28857;&#30340;&#28508;&#31354;&#38388;&#29983;&#25104;&#26041;&#27861;&#65288;LCA-VAE&#65289;&#65292;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#24418;&#25104;&#20102;LCA-LSBO&#12290;&#23454;&#39564;&#35777;&#26126;LCA-LSBO&#22312;&#22270;&#20687;&#29983;&#25104;&#21644;&#20840;&#26032;&#30340;&#21270;&#23398;&#35774;&#35745;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#31354;&#38388;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;LSBO&#65289;&#23558;&#29983;&#25104;&#27169;&#22411;&#65288;&#36890;&#24120;&#26159;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65289;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#24863;&#20852;&#36259;&#30340;&#20840;&#26032;&#23545;&#35937;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#30446;&#26631;&#19981;&#21305;&#37197;&#65292;&#23548;&#33268;&#20102;LSBO&#38754;&#20020;&#25361;&#25112;&#21644;&#25512;&#24191;&#33021;&#21147;&#30340;&#20943;&#24369;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;LSBO&#25928;&#29575;&#24182;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#30340;&#26032;&#24605;&#36335;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28508;&#22312;&#30340;&#19968;&#33268;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;LSBO&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#65292;&#36215;&#28304;&#20110;BO-VAE&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#19968;&#33268;&#24847;&#35782;&#33719;&#21462;&#20989;&#25968;&#65288;LCA-AF&#65289;&#65292;&#21033;&#29992;LSBO&#20013;&#30340;&#19968;&#33268;&#24615;&#21306;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LCA-VAE&#65292;&#19968;&#31181;&#26032;&#30340;VAE&#26041;&#27861;&#65292;&#23427;&#29983;&#25104;&#20855;&#26377;&#22686;&#21152;&#30340;&#19968;&#33268;&#24615;&#28857;&#30340;&#28508;&#31354;&#38388;&#65292;&#25552;&#39640;&#20102;BO&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;&#32467;&#21512;LCA-VAE&#21644;LCA-AF&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;LCA-LSBO&#12290;&#23454;&#39564;&#35780;&#20272;&#35777;&#23454;&#20102;LCA-LSBO&#22312;&#22270;&#20687;&#29983;&#25104;&#21644;&#20840;&#26032;&#30340;&#21270;&#23398;&#35774;&#35745;&#20219;&#21153;&#20013;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent Space Bayesian Optimization (LSBO) combines generative models, typically Variational Autoencoders (VAE), with Bayesian Optimization (BO) to generate de novo objects of interest. However, LSBO faces challenges due to the mismatch between the objectives of BO and VAE, resulting in poor extrapolation capabilities. In this paper, we propose novel contributions to enhance LSBO efficiency and overcome this challenge. We first introduce the concept of latent consistency/inconsistency as a crucial problem in LSBO, arising from the BO-VAE mismatch. To address this, we propose the Latent Consistent Aware-Acquisition Function (LCA-AF) that leverages consistent regions in LSBO. Additionally, we present LCA-VAE, a novel VAE method that generates a latent space with increased consistent points, improving BO's extrapolation capabilities. Combining LCA-VAE and LCA-AF, we develop LCA-LSBO. Experimental evaluations validate the improved performance of LCA-LSBO in image generation and de-novo chem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#23545;&#25112;&#25216;&#26415;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26041;&#27861;&#65292;&#26469;&#35782;&#21035;&#29615;&#22659;&#20013;&#30340;&#35266;&#23519;/&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#23558;&#22810;&#26679;&#24615;&#24341;&#20837;&#38750;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29615;&#22659;&#35774;&#35745;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02119</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#23545;&#25112;&#23454;&#29616;&#22810;&#26679;&#21270;&#35825;&#23548;&#30340;&#29615;&#22659;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Diversity Induced Environment Design via Self-Play. (arXiv:2302.02119v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#23545;&#25112;&#25216;&#26415;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26041;&#27861;&#65292;&#26469;&#35782;&#21035;&#29615;&#22659;&#20013;&#30340;&#35266;&#23519;/&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#23558;&#22810;&#26679;&#24615;&#24341;&#20837;&#38750;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29615;&#22659;&#35774;&#35745;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#29615;&#22659;&#20998;&#24067;&#35774;&#35745;&#30340;&#30740;&#31350;&#24050;&#32463;&#23637;&#31034;&#20986;&#35757;&#32451;&#26377;&#25928;&#30340;&#36890;&#29992;&#33021;&#21147;&#20195;&#29702;&#30340;&#21069;&#26223;&#12290;&#23427;&#30340;&#25104;&#21151;&#37096;&#20998;&#22312;&#20110;&#19968;&#31181;&#33258;&#36866;&#24212;&#35838;&#31243;&#23398;&#20064;&#30340;&#24418;&#24335;&#65292;&#35813;&#24418;&#24335;&#36890;&#36807;&#29983;&#25104;&#20195;&#29702;&#33021;&#21147;&#30340;&#21069;&#27839;&#29615;&#22659;&#23454;&#20363;&#65288;&#25110;&#32423;&#21035;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#29615;&#22659;&#35774;&#35745;&#26694;&#26550;&#32463;&#24120;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#35745;&#31354;&#38388;&#20013;&#21457;&#29616;&#26377;&#25928;&#32423;&#21035;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#39640;&#25104;&#26412;&#20132;&#20114;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#22312;&#38750;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#26694;&#26550;&#20013;&#24341;&#20837;&#22810;&#26679;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#23545;&#32473;&#23450;&#32423;&#21035;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#35266;&#23519;/&#38544;&#34255;&#29366;&#24577;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#30340;&#32467;&#26524;&#26469;&#34920;&#24449;&#20004;&#20010;&#32423;&#21035;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#27491;&#22914;&#25105;&#20204;&#25152;&#23637;&#31034;&#30340;&#65292;&#36825;&#23545;&#20110;&#26377;&#25928;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#25105;&#20204;&#21152;&#20837;&#20102;&#33258;&#25105;&#23545;&#25112;&#25216;&#26415;&#65292;&#20351;&#24471;&#29615;&#22659;&#29983;&#25104;&#22120;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work on designing an appropriate distribution of environments has shown promise for training effective generally capable agents. Its success is partly because of a form of adaptive curriculum learning that generates environment instances (or levels) at the frontier of the agent's capabilities. However, such an environment design framework often struggles to find effective levels in challenging design spaces and requires costly interactions with the environment. In this paper, we aim to introduce diversity in the Unsupervised Environment Design (UED) framework. Specifically, we propose a task-agnostic method to identify observed/hidden states that are representative of a given level. The outcome of this method is then utilized to characterize the diversity between two levels, which as we show can be crucial to effective performance. In addition, to improve sampling efficiency, we incorporate the self-play technique that allows the environment generator to automatically generate e
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#20248;&#21270;&#22120;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#38754;&#20020;&#26799;&#24230;&#33539;&#22260;&#21464;&#21270;&#22823;&#12289;&#26799;&#24230;&#20998;&#24067;&#38750;&#29420;&#31435;&#19988;&#19981;&#21516;&#12289;&#39640;&#26041;&#24046;&#20559;&#24046;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26799;&#24230;&#22788;&#29702;&#12289;&#31649;&#36947;&#35757;&#32451;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#22120;&#32467;&#26500;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.01470</link><description>&lt;p&gt;
&#23398;&#20064;&#20248;&#21270;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Optimize for Reinforcement Learning. (arXiv:2302.01470v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01470
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20248;&#21270;&#22120;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#38754;&#20020;&#26799;&#24230;&#33539;&#22260;&#21464;&#21270;&#22823;&#12289;&#26799;&#24230;&#20998;&#24067;&#38750;&#29420;&#31435;&#19988;&#19981;&#21516;&#12289;&#39640;&#26041;&#24046;&#20559;&#24046;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26799;&#24230;&#22788;&#29702;&#12289;&#31649;&#36947;&#35757;&#32451;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#22120;&#32467;&#26500;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#21033;&#29992;&#26356;&#22810;&#30340;&#25968;&#25454;&#12289;&#35745;&#31639;&#21644;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#23398;&#20064;&#20248;&#21270;&#22120;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#25163;&#21160;&#35774;&#35745;&#30340;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#19982;&#30417;&#30563;&#23398;&#20064;&#26412;&#36136;&#19978;&#19981;&#21516;&#65292;&#36825;&#20123;&#23398;&#20064;&#20248;&#21270;&#22120;&#22312;&#31616;&#21333;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#25928;&#26524;&#19981;&#20339;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#21457;&#29616;&#20102;&#19977;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26799;&#24230;&#22312;&#23545;&#25968;&#19978;&#21464;&#21270;&#33539;&#22260;&#24456;&#22823;&#65292;&#32780;&#22312;&#32477;&#23545;&#20540;&#19978;&#33539;&#22260;&#36739;&#23567;&#65292;&#36825;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#38590;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;&#21442;&#25968;&#26356;&#26032;&#12290;&#20854;&#27425;&#65292;&#20195;&#29702;&#26799;&#24230;&#20998;&#24067;&#38750;&#29420;&#31435;&#19988;&#19981;&#21516;&#65292;&#23548;&#33268;&#20803;&#35757;&#32451;&#25928;&#29575;&#20302;&#19979;&#12290;&#26368;&#21518;&#65292;&#30001;&#20110;&#20195;&#29702;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#39640;&#24230;&#38543;&#26426;&#20132;&#20114;&#65292;&#20195;&#29702;&#26799;&#24230;&#23384;&#22312;&#36739;&#39640;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#65292;&#22686;&#21152;&#20102;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#22120;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26799;&#24230;&#22788;&#29702;&#12289;&#31649;&#36947;&#35757;&#32451;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#22120;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, by leveraging more data, computation, and diverse tasks, learned optimizers have achieved remarkable success in supervised learning, outperforming classical hand-designed optimizers. Reinforcement learning (RL) is essentially different from supervised learning and in practice these learned optimizers do not work well even in simple RL tasks. We investigate this phenomenon and identity three issues. First, the gradients of an RL agent vary across a wide range in logarithms while their absolute values are in a small range, making neural networks hard to obtain accurate parameter updates. Second, the agent-gradient distribution is non-independent and identically distributed, leading to inefficient meta-training. Finally, due to highly stochastic agent-environment interactions, the agent-gradients have high bias and variance, which increase the difficulty of learning an optimizer for RL. We propose gradient processing, pipeline training, and a novel optimizer structure wit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UFLP&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#32622;&#29615;&#22659;&#21040;&#39640;&#19981;&#30830;&#23450;&#24615;&#29366;&#24577;&#26469;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20010;&#31616;&#21333;&#30340;&#36807;&#31243;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#37319;&#26679;&#25104;&#26412;&#65292;&#24182;&#22312;&#22256;&#38590;&#30340;&#25506;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2301.12579</link><description>&lt;p&gt;
&#36890;&#36807;&#26412;&#22320;&#35268;&#21010;&#23454;&#29616;&#26679;&#26412;&#26377;&#25928;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sample Efficient Deep Reinforcement Learning via Local Planning. (arXiv:2301.12579v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12579
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UFLP&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#32622;&#29615;&#22659;&#21040;&#39640;&#19981;&#30830;&#23450;&#24615;&#29366;&#24577;&#26469;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20010;&#31616;&#21333;&#30340;&#36807;&#31243;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#37319;&#26679;&#25104;&#26412;&#65292;&#24182;&#22312;&#22256;&#38590;&#30340;&#25506;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#22312;&#27169;&#25311;&#22120;&#19978;&#36827;&#34892;&#26679;&#26412;&#26377;&#25928;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#27169;&#25311;&#22120;&#30340;&#19968;&#20010;&#26377;&#29992;&#29305;&#24615;&#26159;&#21487;&#20197;&#23558;&#29615;&#22659;&#37325;&#32622;&#21040;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#19981;&#30830;&#23450;&#24615;&#20248;&#20808;&#26412;&#22320;&#35268;&#21010;&#8221;&#65288;UFLP&#65289;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#36825;&#20010;&#29305;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#27599;&#20010;&#25968;&#25454;&#25910;&#38598;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#30340;&#20803;&#31639;&#27861;&#20197;&#19968;&#23450;&#30340;&#27010;&#29575;&#23558;&#29615;&#22659;&#37325;&#32622;&#20026;&#20855;&#26377;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#35266;&#23519;&#29366;&#24577;&#65292;&#32780;&#19981;&#26159;&#26681;&#25454;&#21021;&#22987;&#29366;&#24577;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#12290;&#28982;&#21518;&#65292;&#20195;&#29702;-&#29615;&#22659;&#20132;&#20114;&#23601;&#20687;&#22312;&#26631;&#20934;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#20013;&#19968;&#26679;&#36827;&#34892;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#31616;&#21333;&#30340;&#36807;&#31243;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20960;&#20010;&#22522;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22256;&#38590;&#30340;&#25506;&#32034;&#20219;&#21153;&#19978;&#30340;&#37319;&#26679;&#25104;&#26412;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#31616;&#21333;&#65288;&#20998;&#24067;&#24335;&#65289;&#21452;&#37325;DQN&#22312;&#33261;&#21517;&#26157;&#33879;&#30340;&#38590;&#24230;&#24456;&#39640;&#30340;Atari&#28216;&#25103;&#8220;&#33945;&#29305;&#31062;&#29595;&#20043;&#22797;&#20167;&#8221;&#19978;&#23454;&#29616;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The focus of this work is sample-efficient deep reinforcement learning (RL) with a simulator. One useful property of simulators is that it is typically easy to reset the environment to a previously observed state. We propose an algorithmic framework, named uncertainty-first local planning (UFLP), that takes advantage of this property. Concretely, in each data collection iteration, with some probability, our meta-algorithm resets the environment to an observed state which has high uncertainty, instead of sampling according to the initial-state distribution. The agent-environment interaction then proceeds as in the standard online RL setting. We demonstrate that this simple procedure can dramatically improve the sample cost of several baseline RL algorithms on difficult exploration tasks. Notably, with our framework, we can achieve super-human performance on the notoriously hard Atari game, Montezuma's Revenge, with a simple (distributional) double DQN. Our work can be seen as an efficie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#21644;&#26816;&#27979;&#22270;&#20687;&#20013;&#21518;&#38376;&#27169;&#24335;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#31216;&#20026;&#35748;&#30693;&#31934;&#28860;&#65288;CD&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#36755;&#20837;&#25513;&#30721;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#21462;&#19968;&#20010;&#23567;&#27169;&#24335;&#65292;&#35813;&#27169;&#24335;&#21487;&#20197;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;&#20351;&#29992;CD&#21644;&#25552;&#21462;&#30340;&#27169;&#24335;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#19968;&#20010;&#26377;&#36259;&#29616;&#35937;&#65306;&#21518;&#38376;&#26679;&#26412;&#30340;&#27169;&#24335;&#37117;&#38750;&#24120;&#23567;&#12290;&#25152;&#20197;&#21487;&#20197;&#21033;&#29992;&#23398;&#21040;&#30340;&#25513;&#30721;&#20174;&#27745;&#26579;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#21644;&#21024;&#38500;&#21518;&#38376;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2301.10908</link><description>&lt;p&gt;
&#22312;&#22270;&#20687;&#20013;&#25552;&#21462;&#35748;&#30693;&#21518;&#38376;&#27169;&#24335;&#30340;&#26041;&#27861;: &#19968;&#31181;&#29992;&#20110;&#21518;&#38376;&#26679;&#26412;&#26816;&#27979;&#30340; SOTA &#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distilling Cognitive Backdoor Patterns within an Image: A SOTA Method for Backdoor Sample Detection. (arXiv:2301.10908v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#21644;&#26816;&#27979;&#22270;&#20687;&#20013;&#21518;&#38376;&#27169;&#24335;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#31216;&#20026;&#35748;&#30693;&#31934;&#28860;&#65288;CD&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#36755;&#20837;&#25513;&#30721;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#21462;&#19968;&#20010;&#23567;&#27169;&#24335;&#65292;&#35813;&#27169;&#24335;&#21487;&#20197;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;&#20351;&#29992;CD&#21644;&#25552;&#21462;&#30340;&#27169;&#24335;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#19968;&#20010;&#26377;&#36259;&#29616;&#35937;&#65306;&#21518;&#38376;&#26679;&#26412;&#30340;&#27169;&#24335;&#37117;&#38750;&#24120;&#23567;&#12290;&#25152;&#20197;&#21487;&#20197;&#21033;&#29992;&#23398;&#21040;&#30340;&#25513;&#30721;&#20174;&#27745;&#26579;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#21644;&#21024;&#38500;&#21518;&#38376;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25552;&#21462;&#21644;&#26816;&#27979;&#22270;&#20687;&#20013;&#30340;&#21518;&#38376;&#27169;&#24335;: &#35748;&#30693;&#31934;&#28860; (CD)&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#36755;&#20837;&#25513;&#30721;&#26469;&#25552;&#21462;&#36755;&#20837;&#22270;&#20687;&#20013;&#30340;&#23567;&#27169;&#24335;&#65292;&#35813;&#27169;&#24335;&#21487;&#20197;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;&#36890;&#36807;&#20351;&#29992;CD&#21644;&#25552;&#21462;&#30340;&#27169;&#24335;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#19968;&#20010;&#26377;&#36259;&#29616;&#35937;&#65306;&#23613;&#31649;&#19981;&#21516;&#25915;&#20987;&#20351;&#29992;&#19981;&#21516;&#24418;&#24335;&#21644;&#22823;&#23567;&#30340;&#35302;&#21457;&#27169;&#24335;&#65292;&#20294;&#21518;&#38376;&#26679;&#26412;&#30340;&#27169;&#24335;&#37117;&#24778;&#20154;&#22320;&#23567;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#21033;&#29992;&#23398;&#21040;&#30340;&#25513;&#30721;&#20174;&#34987;&#27745;&#26579;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#21644;&#21024;&#38500;&#21518;&#38376;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;CD&#21487;&#20197;&#31283;&#20581;&#22320;&#26816;&#27979;&#21508;&#31181;&#39640;&#32423;&#21518;&#38376;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a simple method to distill and detect backdoor patterns within an image: \emph{Cognitive Distillation} (CD). The idea is to extract the "minimal essence" from an input image responsible for the model's prediction. CD optimizes an input mask to extract a small pattern from the input image that can lead to the same model output (i.e., logits or deep features). The extracted pattern can help understand the cognitive mechanism of a model on clean vs. backdoor images and is thus called a \emph{Cognitive Pattern} (CP). Using CD and the distilled CPs, we uncover an interesting phenomenon of backdoor attacks: despite the various forms and sizes of trigger patterns used by different attacks, the CPs of backdoor samples are all surprisingly and suspiciously small. One thus can leverage the learned mask to detect and remove backdoor examples from poisoned training datasets. We conduct extensive experiments to show that CD can robustly detect a wide range of advanced backdoor a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#65292;&#21517;&#20026;&#21028;&#21035;&#39118;&#38505;&#65292;&#26088;&#22312;&#21453;&#26144;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#32773;&#36824;&#35752;&#35770;&#20102;&#20844;&#24179;&#24615;&#26159;&#21542;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2301.10813</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20445;&#35777;&#25552;&#39640;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Increasing Fairness via Combination with Learning Guarantees. (arXiv:2301.10813v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10813
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#65292;&#21517;&#20026;&#21028;&#21035;&#39118;&#38505;&#65292;&#26088;&#22312;&#21453;&#26144;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#32773;&#36824;&#35752;&#35770;&#20102;&#20844;&#24179;&#24615;&#26159;&#21542;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#23545;&#20110;&#38544;&#34255;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#27491;&#22312;&#22686;&#21152;&#12290;&#35768;&#22810;&#25216;&#26415;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#20197;&#22686;&#24378;&#20844;&#24179;&#24615;&#65292;&#21253;&#25324;&#24120;&#29992;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#24230;&#37327;&#21644;&#20960;&#31181;&#32467;&#21512;&#38598;&#25104;&#23398;&#20064;&#30340;&#20844;&#24179;&#24863;&#30693;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20844;&#24179;&#24230;&#37327;&#21482;&#33021;&#20851;&#27880;&#20854;&#20013;&#20043;&#19968;&#65292;&#21363;&#32676;&#20307;&#20844;&#24179;&#24615;&#25110;&#20010;&#20307;&#20844;&#24179;&#24615;&#65292;&#23427;&#20204;&#20043;&#38388;&#30340;&#30828;&#24615;&#20860;&#23481;&#24615;&#26263;&#31034;&#20102;&#21363;&#20351;&#20854;&#20013;&#20043;&#19968;&#24471;&#21040;&#28385;&#36275;&#65292;&#20173;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#25552;&#21319;&#20844;&#24179;&#24615;&#30340;&#26426;&#21046;&#36890;&#24120;&#21482;&#25552;&#20379;&#32463;&#39564;&#32467;&#26524;&#26469;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#35770;&#25991;&#35752;&#35770;&#20844;&#24179;&#24615;&#26159;&#21542;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;&#21028;&#21035;&#39118;&#38505;&#65292;&#20197;&#21453;&#26144;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#20004;&#20010;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;p...
&lt;/p&gt;
&lt;p&gt;
The concern about underlying discrimination hidden in ML models is increasing, as ML systems have been widely applied in more and more real-world scenarios and any discrimination hidden in them will directly affect human life. Many techniques have been developed to enhance fairness including commonly-used group fairness measures and several fairness-aware methods combining ensemble learning. However, existing fairness measures can only focus on one aspect -- either group or individual fairness, and the hard compatibility among them indicates a possibility of remaining biases even if one of them is satisfied. Moreover, existing mechanisms to boost fairness usually present empirical results to show validity, yet few of them discuss whether fairness can be boosted with certain theoretical guarantees. To address these issues, we propose a fairness quality measure named discriminative risk in this paper to reflect both individual and group fairness aspects. Furthermore, we investigate the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20197;&#23458;&#25143;&#20026;&#20013;&#24515;&#30340;&#33829;&#38144;&#27963;&#21160;&#20013;&#23547;&#25214;&#30456;&#20284;&#23458;&#25143;&#30340;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#33021;&#22788;&#29702;&#20159;&#32423;&#23458;&#25143;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23884;&#20837;&#27169;&#22411;&#21644;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#26041;&#27861;&#26469;&#23547;&#25214;&#24863;&#20852;&#36259;&#30340;&#30456;&#20284;&#23458;&#25143;&#12290;&#36890;&#36807;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#26377;&#24847;&#20041;&#30340;&#23458;&#25143;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#19994;&#21153;&#20852;&#36259;&#12290;</title><link>http://arxiv.org/abs/2301.03147</link><description>&lt;p&gt;
&#23547;&#25214;&#30005;&#23376;&#21830;&#21153;&#33829;&#38144;&#30340;&#30456;&#20284;&#23458;&#25143;
&lt;/p&gt;
&lt;p&gt;
Finding Lookalike Customers for E-Commerce Marketing. (arXiv:2301.03147v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20197;&#23458;&#25143;&#20026;&#20013;&#24515;&#30340;&#33829;&#38144;&#27963;&#21160;&#20013;&#23547;&#25214;&#30456;&#20284;&#23458;&#25143;&#30340;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#33021;&#22788;&#29702;&#20159;&#32423;&#23458;&#25143;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23884;&#20837;&#27169;&#22411;&#21644;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#26041;&#27861;&#26469;&#23547;&#25214;&#24863;&#20852;&#36259;&#30340;&#30456;&#20284;&#23458;&#25143;&#12290;&#36890;&#36807;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#26377;&#24847;&#20041;&#30340;&#23458;&#25143;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#19994;&#21153;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#23458;&#25143;&#20026;&#20013;&#24515;&#30340;&#33829;&#38144;&#27963;&#21160;&#20026;&#27779;&#23572;&#29595;&#30340;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#27969;&#37327;&#36129;&#29486;&#20102;&#24456;&#22823;&#30340;&#19968;&#37096;&#20998;&#12290;&#38543;&#30528;&#23458;&#25143;&#25968;&#25454;&#35268;&#27169;&#30340;&#22686;&#22823;&#65292;&#25193;&#22823;&#33829;&#38144;&#21463;&#20247;&#20197;&#35302;&#36798;&#26356;&#22810;&#23458;&#25143;&#23545;&#30005;&#23376;&#21830;&#21153;&#20844;&#21496;&#30340;&#19994;&#21153;&#22686;&#38271;&#21644;&#20026;&#23458;&#25143;&#24102;&#26469;&#26356;&#22810;&#20215;&#20540;&#21464;&#24471;&#26356;&#20026;&#20851;&#38190;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#19988;&#39640;&#25928;&#30340;&#31995;&#32479;&#26469;&#25193;&#22823;&#33829;&#38144;&#27963;&#21160;&#30340;&#30446;&#26631;&#21463;&#20247;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#22788;&#29702;&#20159;&#32423;&#23458;&#25143;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23884;&#20837;&#27169;&#22411;&#26469;&#34920;&#31034;&#23458;&#25143;&#65292;&#20351;&#29992;&#19968;&#31181;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#26041;&#27861;&#24555;&#36895;&#25214;&#21040;&#24863;&#20852;&#36259;&#30340;&#30456;&#20284;&#23458;&#25143;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#26377;&#24847;&#20041;&#30340;&#23458;&#25143;&#30456;&#20284;&#24230;&#24230;&#37327;&#26469;&#22788;&#29702;&#21508;&#31181;&#19994;&#21153;&#20852;&#36259;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#23637;&#31034;&#25105;&#20204;&#30340;&#31995;&#32479;&#21644;&#23458;&#25143;&#23884;&#20837;&#27169;&#22411;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Customer-centric marketing campaigns generate a large portion of e-commerce website traffic for Walmart. As the scale of customer data grows larger, expanding the marketing audience to reach more customers is becoming more critical for e-commerce companies to drive business growth and bring more value to customers. In this paper, we present a scalable and efficient system to expand targeted audience of marketing campaigns, which can handle hundreds of millions of customers. We use a deep learning based embedding model to represent customers and an approximate nearest neighbor search method to quickly find lookalike customers of interest. The model can deal with various business interests by constructing interpretable and meaningful customer similarity metrics. We conduct extensive experiments to demonstrate the great performance of our system and customer embedding model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;CC-FedAvg&#30340;&#35745;&#31639;&#23450;&#21046;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#65292;&#21487;&#35753;&#21442;&#19982;&#32773;&#26681;&#25454;&#20854;&#35745;&#31639;&#39044;&#31639;&#20915;&#23450;&#22312;&#27599;&#36718;&#20013;&#26159;&#21542;&#25191;&#34892;&#20256;&#32479;&#30340;&#26412;&#22320;&#35757;&#32451;&#25110;&#27169;&#22411;&#20272;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CC-FedAvg&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2212.13679</link><description>&lt;p&gt;
CC-FedAvg&#65306;&#35745;&#31639;&#23450;&#21046;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
CC-FedAvg: Computationally Customized Federated Averaging. (arXiv:2212.13679v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;CC-FedAvg&#30340;&#35745;&#31639;&#23450;&#21046;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#65292;&#21487;&#35753;&#21442;&#19982;&#32773;&#26681;&#25454;&#20854;&#35745;&#31639;&#39044;&#31639;&#20915;&#23450;&#22312;&#27599;&#36718;&#20013;&#26159;&#21542;&#25191;&#34892;&#20256;&#32479;&#30340;&#26412;&#22320;&#35757;&#32451;&#25110;&#27169;&#22411;&#20272;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CC-FedAvg&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#24335;&#65292;&#36890;&#36807;&#20998;&#24067;&#22312;&#20247;&#22810;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#23427;&#22312;&#26412;&#36136;&#19978;&#20551;&#35774;&#21442;&#19982;&#32773;&#30340;&#35745;&#31639;&#33021;&#21147;&#30456;&#21516;&#65292;&#20294;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#19981;&#21516;&#30340;&#33021;&#28304;&#39044;&#31639;&#25110;&#24182;&#34892;&#25191;&#34892;&#30340;&#20219;&#21153;&#19981;&#21516;&#65292;&#21442;&#19982;&#32773;&#35745;&#31639;&#36164;&#28304;&#23384;&#22312;&#30528;&#24046;&#24322;&#12290;&#32570;&#20047;&#35745;&#31639;&#39044;&#31639;&#30340;&#21442;&#19982;&#32773;&#24517;&#39035;&#36866;&#24403;&#35268;&#21010;&#20854;&#21463;&#38480;&#35745;&#31639;&#36164;&#28304;&#30340;&#20351;&#29992;&#65292;&#21542;&#21017;&#20182;&#20204;&#23558;&#26080;&#27861;&#23436;&#25104;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#65292;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#31639;&#26412;&#22320;&#27169;&#22411;&#32780;&#26080;&#38656;&#35745;&#31639;&#23494;&#38598;&#36845;&#20195;&#30340;&#31574;&#30053;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35745;&#31639;&#23450;&#21046;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;(CC-FedAvg)&#65292;&#20801;&#35768;&#21442;&#19982;&#32773;&#26681;&#25454;&#20854;&#24403;&#21069;&#30340;&#35745;&#31639;&#39044;&#31639;&#65292;&#22312;&#27599;&#20010;&#36718;&#27425;&#20013;&#20915;&#23450;&#26159;&#25191;&#34892;&#20256;&#32479;&#30340;&#26412;&#22320;&#35757;&#32451;&#36824;&#26159;&#27169;&#22411;&#20272;&#31639;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#22343;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#30456;&#27604;&#65292;CC-FedAvg&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is an emerging paradigm to train model with distributed data from numerous Internet of Things (IoT) devices. It inherently assumes a uniform capacity among participants. However, due to different conditions such as differing energy budgets or executing parallel unrelated tasks, participants have diverse computational resources in practice. Participants with insufficient computation budgets must plan for the use of restricted computational resources appropriately, otherwise they would be unable to complete the entire training procedure, resulting in model performance decline. To address the this issue, we propose a strategy for estimating local models without computationally intensive iterations. Based on it, we propose Computationally Customized Federated Averaging (CC-FedAvg), which allows participants to determine whether to perform traditional local training or model estimation in each round based on their current computational budgets. Both theoretical analy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#36739;&#22909;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.10511</link><description>&lt;p&gt;
&#20309;&#26102;&#19981;&#20449;&#20219;&#35821;&#35328;&#27169;&#22411;&#65306;&#25506;&#32034;&#21442;&#25968;&#21644;&#38750;&#21442;&#25968;&#35760;&#24518;&#30340;&#26377;&#25928;&#24615;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories. (arXiv:2212.10511v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#36739;&#22909;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#38656;&#35201;&#20016;&#23500;&#19990;&#30028;&#30693;&#35782;&#30340;&#20219;&#21153;&#65292;&#36825;&#26263;&#31034;&#20102;&#20165;&#20381;&#38752;&#20854;&#21442;&#25968;&#26469;&#32534;&#30721;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#22312;PopQA&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#30693;&#35782;&#25506;&#27979;&#23454;&#39564;&#65292;&#20197;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#20107;&#23454;&#30693;&#35782;&#26041;&#38754;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#38590;&#20197;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#38271;&#23614;&#20013;&#65292;&#25193;&#23637;&#35268;&#27169;&#26080;&#27861;&#26126;&#26174;&#25913;&#21892;&#35760;&#24518;&#23454;&#38469;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32988;&#36807;&#32423;&#21035;&#22823;&#24471;&#22810;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26410;&#32463;&#21327;&#21161;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#28041;&#21450;&#39640;&#27969;&#34892;&#23454;&#20307;&#30340;&#38382;&#39064;&#19978;&#20173;&#28982;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#22312;&#38656;&#35201;&#26102;&#26816;&#32034;&#38750;&#21442;&#25968;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge. This paper aims to understand LMs' strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments of 10 models and 4 augmentation methods on PopQA, our new open-domain QA dataset with 14k questions. We find that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the long tail. We then show that retrieval-augmented LMs largely outperform orders of magnitude larger LMs, while unassisted LMs remain competitive in questions about high-popularity entities. Based on those findings, we devise a simple, yet effective, method for powerful and efficient retrieval-augmented LMs, which retrieves non-parametric memories only whe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#32858;&#21512;&#29289;&#33258;&#27965;&#22330;&#29702;&#35770;&#27169;&#25311;&#25968;&#25454;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;&#30740;&#31350;&#23884;&#27573;&#20849;&#32858;&#29289;&#21442;&#25968;&#31354;&#38388;&#12290;&#21019;&#26032;&#21253;&#25324;&#20351;&#29992;CNN&#22788;&#29702;&#31163;&#25955;&#21270;&#30340;&#23616;&#37096;&#24179;&#22343;&#21333;&#20307;&#23494;&#24230;&#22330;&#12289;&#20351;&#29992;GAN&#39044;&#27979;&#23616;&#37096;&#24179;&#22343;&#21333;&#20307;&#23494;&#24230;&#22330;&#65292;&#20197;&#21450;&#23454;&#29616;&#31354;&#38388;&#24179;&#31227;&#21644;&#26059;&#36716;&#19981;&#21464;&#24615;&#12290;&#35813;&#26694;&#26550;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2212.10478</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#32858;&#21512;&#29289;&#33258;&#27965;&#22330;&#29702;&#35770;&#22312;&#20108;&#32500;&#31354;&#38388;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and Polymer Self-Consistent Field Theory in Two Spatial Dimensions. (arXiv:2212.10478v2 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#32858;&#21512;&#29289;&#33258;&#27965;&#22330;&#29702;&#35770;&#27169;&#25311;&#25968;&#25454;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;&#30740;&#31350;&#23884;&#27573;&#20849;&#32858;&#29289;&#21442;&#25968;&#31354;&#38388;&#12290;&#21019;&#26032;&#21253;&#25324;&#20351;&#29992;CNN&#22788;&#29702;&#31163;&#25955;&#21270;&#30340;&#23616;&#37096;&#24179;&#22343;&#21333;&#20307;&#23494;&#24230;&#22330;&#12289;&#20351;&#29992;GAN&#39044;&#27979;&#23616;&#37096;&#24179;&#22343;&#21333;&#20307;&#23494;&#24230;&#22330;&#65292;&#20197;&#21450;&#23454;&#29616;&#31354;&#38388;&#24179;&#31227;&#21644;&#26059;&#36716;&#19981;&#21464;&#24615;&#12290;&#35813;&#26694;&#26550;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20174;&#33258;&#27965;&#22330;&#29702;&#35770;&#27169;&#25311;&#20013;&#33719;&#21462;&#25968;&#25454;&#65292;&#21152;&#36895;&#30740;&#31350;&#23884;&#27573;&#20849;&#32858;&#29289;&#21442;&#25968;&#31354;&#38388;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;&#36825;&#26159;&#23545;[1]&#20013;&#24341;&#20837;&#30340;&#26694;&#26550;&#30340;&#23454;&#36136;&#24615;&#20108;&#32500;&#25193;&#23637;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#20960;&#20010;&#21019;&#26032;&#21644;&#25913;&#36827;: (1) &#20351;&#29992;Sobolev&#31354;&#38388;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26469;&#22788;&#29702;&#31163;&#25955;&#21270;&#30340;&#23616;&#37096;&#24179;&#22343;&#21333;&#20307;&#23494;&#24230;&#22330;&#30340;&#25351;&#25968;&#32500;&#24230;&#22686;&#21152;&#65292;&#24182;&#23545;&#39044;&#27979;&#30340;&#22330;&#35770;&#23494;&#38598;&#21704;&#23494;&#39039;&#37327;&#23454;&#26045;&#24378;&#21147;&#30340;&#31354;&#38388;&#24179;&#31227;&#21644;&#26059;&#36716;&#19981;&#21464;&#24615;&#12290; (2) &#24341;&#20837;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GAN)&#26469;&#39640;&#25928;&#20934;&#30830;&#22320;&#39044;&#27979;&#38797;&#28857;&#12289;&#23616;&#37096;&#24179;&#22343;&#21333;&#20307;&#23494;&#24230;&#22330;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#35757;&#32451;&#38598;&#36827;&#34892;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#12290;&#36825;&#31181;GAN&#26041;&#27861;&#33410;&#30465;&#20102;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290; (3) &#25152;&#25552;&#20986;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
A computational framework that leverages data from self-consistent field theory simulations with deep learning to accelerate the exploration of parameter space for block copolymers is presented. This is a substantial two-dimensional extension of the framework introduced in [1]. Several innovations and improvements are proposed. (1) A Sobolev space-trained, convolutional neural network (CNN) is employed to handle the exponential dimension increase of the discretized, local average monomer density fields and to strongly enforce both spatial translation and rotation invariance of the predicted, field-theoretic intensive Hamiltonian. (2) A generative adversarial network (GAN) is introduced to efficiently and accurately predict saddle point, local average monomer density fields without resorting to gradient descent methods that employ the training set. This GAN approach yields important savings of both memory and computational cost. (3) The proposed machine learning framework is successfull
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#26469;&#23637;&#31034;&#20854;&#33030;&#24369;&#24615;&#12290;&#35813;&#25915;&#20987;&#23548;&#33268;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#33030;&#24369;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.15875</link><description>&lt;p&gt;
&#38024;&#23545;&#25345;&#32493;&#23398;&#20064;&#33030;&#24369;&#24615;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Data Poisoning Attack Aiming the Vulnerability of Continual Learning. (arXiv:2211.15875v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15875
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#26469;&#23637;&#31034;&#20854;&#33030;&#24369;&#24615;&#12290;&#35813;&#25915;&#20987;&#23548;&#33268;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#33030;&#24369;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;&#38480;&#21046;&#23545;&#20808;&#21069;&#20219;&#21153;&#25968;&#25454;&#30340;&#35775;&#38382;&#65292;&#20197;&#27169;&#25311;&#19982;&#20869;&#23384;&#21644;&#38544;&#31169;&#26377;&#20851;&#30340;&#30495;&#23454;&#19990;&#30028;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#36825;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#26080;&#27861;&#36319;&#36394;&#27599;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23454;&#36136;&#19978;&#65292;&#24403;&#21069;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#23545;&#20808;&#21069;&#20219;&#21153;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#26469;&#23637;&#31034;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#33030;&#24369;&#24615;&#65292;&#35813;&#25915;&#20987;&#21487;&#20197;&#22312;&#26032;&#20219;&#21153;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#20351;&#29992;&#12290;&#25152;&#25552;&#25915;&#20987;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#23548;&#33268;&#25915;&#20987;&#32773;&#38024;&#23545;&#30340;&#29305;&#23450;&#20219;&#21153;&#24615;&#33021;&#19979;&#38477;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65288;&#24377;&#24615;&#26435;&#37325;&#25972;&#21512;&#65288;EWC&#65289;&#21644;&#31361;&#35302;&#26234;&#33021;&#65288;SI&#65289;&#65289;&#19978;&#36827;&#34892;&#20102;&#25915;&#20987;&#23454;&#39564;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;MNIST&#25968;&#25454;&#38598;&#30340;&#21464;&#20307;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#26412;&#25991;&#25552;&#20986;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generally, regularization-based continual learning models limit access to the previous task data to imitate the real-world constraints related to memory and privacy. However, this introduces a problem in these models by not being able to track the performance on each task. In essence, current continual learning methods are susceptible to attacks on previous tasks. We demonstrate the vulnerability of regularization-based continual learning methods by presenting a simple task-specific data poisoning attack that can be used in the learning process of a new task. Training data generated by the proposed attack causes performance degradation on a specific task targeted by the attacker. We experiment with the attack on the two representative regularization-based continual learning methods, Elastic Weight Consolidation (EWC) and Synaptic Intelligence (SI), trained with variants of MNIST dataset. The experiment results justify the vulnerability proposed in this paper and demonstrate the importa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#24179;&#28369;&#30340;&#35282;&#24230;&#24341;&#20837;&#20102;Shapley&#26354;&#32447;&#20316;&#20026;&#23616;&#37096;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#24230;&#37327;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#20272;&#35745;&#31574;&#30053;&#65292;&#24182;&#22312;&#29305;&#24449;&#30340;&#29420;&#31435;&#21644;&#20381;&#36182;&#24773;&#20917;&#19979;&#24471;&#21040;&#20102;&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#20026;&#20272;&#35745;&#30340;Shapley&#26354;&#32447;&#26500;&#24314;&#20102;&#32622;&#20449;&#21306;&#38388;&#24182;&#36827;&#34892;&#20102;&#25512;&#26029;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#28176;&#36817;&#32467;&#26524;&#12290;&#24212;&#29992;&#20013;&#20998;&#26512;&#20102;&#21738;&#20123;&#23646;&#24615;&#39537;&#21160;&#36710;&#36742;&#20215;&#26684;&#12290;</title><link>http://arxiv.org/abs/2211.13289</link><description>&lt;p&gt;
Shapley&#26354;&#32447;&#65306;&#19968;&#31181;&#24179;&#28369;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Shapley Curves: A Smoothing Perspective. (arXiv:2211.13289v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#24179;&#28369;&#30340;&#35282;&#24230;&#24341;&#20837;&#20102;Shapley&#26354;&#32447;&#20316;&#20026;&#23616;&#37096;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#24230;&#37327;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#20272;&#35745;&#31574;&#30053;&#65292;&#24182;&#22312;&#29305;&#24449;&#30340;&#29420;&#31435;&#21644;&#20381;&#36182;&#24773;&#20917;&#19979;&#24471;&#21040;&#20102;&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#20026;&#20272;&#35745;&#30340;Shapley&#26354;&#32447;&#26500;&#24314;&#20102;&#32622;&#20449;&#21306;&#38388;&#24182;&#36827;&#34892;&#20102;&#25512;&#26029;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#28176;&#36817;&#32467;&#26524;&#12290;&#24212;&#29992;&#20013;&#20998;&#26512;&#20102;&#21738;&#20123;&#23646;&#24615;&#39537;&#21160;&#36710;&#36742;&#20215;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28304;&#33258;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#65292;Shapley&#20540;&#24050;&#25104;&#20026;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#24230;&#37327;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23545;Shapley&#20540;&#30340;&#32479;&#35745;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#20197;&#38750;&#21442;&#25968;(&#25110;&#24179;&#28369;)&#30340;&#35282;&#24230;&#65292;&#24341;&#20837;Shapley&#26354;&#32447;&#20316;&#20026;&#23616;&#37096;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#20272;&#35745;&#31574;&#30053;&#65292;&#24182;&#22312;&#29305;&#24449;&#29420;&#31435;&#21644;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#37117;&#24471;&#20986;&#20102;&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#24182;&#23545;&#20272;&#35745;&#30340;Shapley&#26354;&#32447;&#36827;&#34892;&#25512;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37326;&#34542;&#24341;&#23548;&#31243;&#24207;&#29256;&#26412;&#65292;&#19987;&#38376;&#35843;&#25972;&#20197;&#33719;&#24471;Shapley&#26354;&#32447;&#30340;&#33391;&#22909;&#26377;&#38480;&#26679;&#26412;&#35206;&#30422;&#12290;&#28176;&#36817;&#32467;&#26524;&#22312;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#12290;&#22312;&#23454;&#35777;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21738;&#20123;&#23646;&#24615;&#39537;&#21160;&#20102;&#36710;&#36742;&#30340;&#20215;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
Originating from cooperative game theory, Shapley values have become one of the most widely used measures for variable importance in applied Machine Learning. However, the statistical understanding of Shapley values is still limited. In this paper, we take a nonparametric (or smoothing) perspective by introducing Shapley curves as a local measure of variable importance. We propose two estimation strategies and derive the consistency and asymptotic normality both under independence and dependence among the features. This allows us to construct confidence intervals and conduct inference on the estimated Shapley curves. We propose a novel version of the wild bootstrap procedure, specifically adjusted to give good finite sample coverage of the Shapley curves. The asymptotic results are validated in extensive experiments. In an empirical application, we analyze which attributes drive the prices of vehicles.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#21518;&#38376;&#28165;&#38500;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#23618;&#26435;&#37325;&#37325;&#26032;&#21021;&#22987;&#21270;&#21644;&#30693;&#35782;&#33976;&#39311;&#26469;&#26377;&#25928;&#28165;&#38500;&#21487;&#30097;&#32593;&#32476;&#30340;&#21518;&#38376;&#34892;&#20026;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#36739;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.12044</link><description>&lt;p&gt;
&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#21518;&#38376;&#28165;&#38500;
&lt;/p&gt;
&lt;p&gt;
Backdoor Cleansing with Unlabeled Data. (arXiv:2211.12044v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#21518;&#38376;&#28165;&#38500;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#23618;&#26435;&#37325;&#37325;&#26032;&#21021;&#22987;&#21270;&#21644;&#30693;&#35782;&#33976;&#39311;&#26469;&#26377;&#25928;&#28165;&#38500;&#21487;&#30097;&#32593;&#32476;&#30340;&#21518;&#38376;&#34892;&#20026;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#36739;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#38656;&#27714;&#22686;&#21152;&#65292;&#20844;&#21496;&#21644;&#32452;&#32455;&#24050;&#32463;&#24320;&#22987;&#22806;&#37096;&#21270;&#35757;&#32451;&#36807;&#31243;&#12290;&#20294;&#26159;&#65292;&#22806;&#37096;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#20250;&#38754;&#20020;&#21518;&#38376;&#25915;&#20987;&#12290;&#22240;&#27492;&#65292;&#20851;&#38190;&#22312;&#20110;&#38450;&#24481;&#36825;&#31181;&#25915;&#20987;&#65292;&#21363;&#21518;&#22788;&#29702;&#19968;&#20010;&#21487;&#30097;&#27169;&#22411;&#65292;&#20351;&#20854;&#30340;&#21518;&#38376;&#34892;&#20026;&#24471;&#21040;&#32531;&#35299;&#65292;&#21516;&#26102;&#20854;&#23545;&#20110;&#24178;&#20928;&#36755;&#20837;&#30340;&#27491;&#24120;&#39044;&#27979;&#33021;&#21147;&#20173;&#28982;&#20445;&#25345;&#19981;&#21463;&#24433;&#21709;&#12290;&#20026;&#20102;&#28040;&#38500;&#24322;&#24120;&#30340;&#21518;&#38376;&#34892;&#20026;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#39069;&#22806;&#30340;&#26631;&#35760;&#24178;&#20928;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35201;&#27714;&#21487;&#33021;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#23545;&#26368;&#32456;&#29992;&#25143;&#19981;&#21487;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#32469;&#36807;&#36825;&#31181;&#38556;&#30861;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#35757;&#32451;&#26631;&#31614;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#36880;&#23618;&#26435;&#37325;&#37325;&#26032;&#21021;&#22987;&#21270;&#21644;&#30693;&#35782;&#33976;&#39311;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#28165;&#38500;&#21487;&#30097;&#32593;&#32476;&#30340;&#21518;&#38376;&#34892;&#20026;&#65292;&#21516;&#26102;&#23545;&#20854;&#27491;&#24120;&#34892;&#20026;&#30340;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#26174;&#31034;&#23427;&#32988;&#36807;&#29616;&#26377;&#30340;&#26080;&#26631;&#31614;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the increasing computational demand of Deep Neural Networks (DNNs), companies and organizations have begun to outsource the training process. However, the externally trained DNNs can potentially be backdoor attacked. It is crucial to defend against such attacks, i.e., to postprocess a suspicious model so that its backdoor behavior is mitigated while its normal prediction power on clean inputs remain uncompromised. To remove the abnormal backdoor behavior, existing methods mostly rely on additional labeled clean samples. However, such requirement may be unrealistic as the training data are often unavailable to end users. In this paper, we investigate the possibility of circumventing such barrier. We propose a novel defense method that does not require training labels. Through a carefully designed layer-wise weight re-initialization and knowledge distillation, our method can effectively cleanse backdoor behaviors of a suspicious network with negligible compromise in its normal beh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#30693;&#35782;&#23548;&#20837;&#29616;&#26377;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#25216;&#33021;&#30340;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2211.11736</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22686;&#24378;&#23454;&#29616;&#26426;&#22120;&#20154;&#25216;&#33021;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models. (arXiv:2211.11736v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#30693;&#35782;&#23548;&#20837;&#29616;&#26377;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#25216;&#33021;&#30340;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22312;&#23398;&#20064;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#31574;&#30053;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20174;&#26426;&#22120;&#20154;-&#35821;&#35328;&#25968;&#25454;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#65292;&#35813;&#25968;&#25454;&#35201;&#20040;&#26159;&#20026;&#29305;&#23450;&#20219;&#21153;&#32780;&#25910;&#38598;&#30340;&#65292;&#35201;&#20040;&#26159;&#22312;&#20107;&#21518;&#30001;&#20154;&#24037;&#26114;&#36149;&#22320;&#37325;&#26032;&#26631;&#27880;&#30340;&#65292;&#24102;&#26377;&#20016;&#23500;&#30340;&#35821;&#35328;&#25551;&#36848;&#12290;&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22914;CLIP&#25110;ViLD&#24050;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20064;&#34920;&#31034;&#21644;&#22330;&#26223;&#25551;&#36848;&#12290;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#21542;&#20316;&#20026;&#26426;&#22120;&#20154;&#25968;&#25454;&#30340;&#33258;&#21160;&#26631;&#27880;&#24037;&#20855;&#65292;&#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#30693;&#35782;&#23548;&#20837;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#20351;&#20854;&#23545;&#20110;&#26410;&#22312;&#20854;&#22320;&#38754;&#30495;&#23454;&#27880;&#37322;&#20013;&#21453;&#26144;&#30340;&#20219;&#21153;&#20063;&#33021;&#21457;&#25381;&#20316;&#29992;&#65311;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#25351;&#20196;&#22686;&#24378;&#65288;DIAL&#65289;&#26469;&#29992;&#20110;&#22522;&#20110;&#35821;&#35328;&#30340;&#25511;&#21046;&#65306;&#25105;&#20204;&#21033;&#29992;CLIP&#30340;&#35821;&#20041;&#29702;&#35299;&#65292;&#21033;&#29992;&#21322;&#30417;&#30563;&#30340;&#35821;&#35328;&#26631;&#31614;&#23558;&#30693;&#35782;&#20256;&#36882;&#21040;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#28436;&#31034;&#25968;&#25454;&#38598;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, much progress has been made in learning robotic manipulation policies that follow natural language instructions. Such methods typically learn from corpora of robot-language data that was either collected with specific tasks in mind or expensively re-labelled by humans with rich language descriptions in hindsight. Recently, large-scale pretrained vision-language models (VLMs) like CLIP or ViLD have been applied to robotics for learning representations and scene descriptors. Can these pretrained models serve as automatic labelers for robot data, effectively importing Internet-scale knowledge into existing datasets to make them useful even for tasks that are not reflected in their ground truth annotations? To accomplish this, we introduce Data-driven Instruction Augmentation for Language-conditioned control (DIAL): we utilize semi-supervised language labels leveraging the semantic understanding of CLIP to propagate knowledge onto large datasets of unlabelled demonstration
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20915;&#31574;&#20219;&#21153;&#65292;&#20027;&#21160;&#33719;&#21462;&#22810;&#27169;&#24577;&#26102;&#38388;&#25968;&#25454;&#12290;&#36890;&#36807;&#26435;&#34913;&#33719;&#21462;&#25104;&#26412;&#21644;&#39044;&#27979;&#24615;&#33021;&#65292;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#26469;&#20027;&#21160;&#36873;&#25321;&#33719;&#21462;&#30340;&#36755;&#20837;&#27169;&#24577;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#20855;&#26377;&#23454;&#38469;&#30456;&#20851;&#25512;&#29702;&#25216;&#33021;&#30340;&#21512;&#25104;&#24773;&#26223;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#25104;&#21151;&#23398;&#20064;&#21040;&#25104;&#26412;&#21453;&#24212;&#24335;&#30340;&#33719;&#21462;&#34892;&#20026;&#65292;&#20294;&#26080;&#27861;&#23398;&#20064;&#21040;&#33258;&#36866;&#24212;&#30340;&#33719;&#21462;&#31574;&#30053;&#65292;&#31361;&#26174;&#20102;&#20219;&#21153;&#30340;&#22256;&#38590;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.05039</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#26102;&#38388;&#25968;&#25454;&#30340;&#20027;&#21160;&#33719;&#21462;&#65306;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20915;&#31574;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Active Acquisition for Multimodal Temporal Data: A Challenging Decision-Making Task. (arXiv:2211.05039v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05039
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20915;&#31574;&#20219;&#21153;&#65292;&#20027;&#21160;&#33719;&#21462;&#22810;&#27169;&#24577;&#26102;&#38388;&#25968;&#25454;&#12290;&#36890;&#36807;&#26435;&#34913;&#33719;&#21462;&#25104;&#26412;&#21644;&#39044;&#27979;&#24615;&#33021;&#65292;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#26469;&#20027;&#21160;&#36873;&#25321;&#33719;&#21462;&#30340;&#36755;&#20837;&#27169;&#24577;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#20855;&#26377;&#23454;&#38469;&#30456;&#20851;&#25512;&#29702;&#25216;&#33021;&#30340;&#21512;&#25104;&#24773;&#26223;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#25104;&#21151;&#23398;&#20064;&#21040;&#25104;&#26412;&#21453;&#24212;&#24335;&#30340;&#33719;&#21462;&#34892;&#20026;&#65292;&#20294;&#26080;&#27861;&#23398;&#20064;&#21040;&#33258;&#36866;&#24212;&#30340;&#33719;&#21462;&#31574;&#30053;&#65292;&#31361;&#26174;&#20102;&#20219;&#21153;&#30340;&#22256;&#38590;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20915;&#31574;&#20219;&#21153;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#22810;&#27169;&#24577;&#26102;&#38388;&#25968;&#25454;&#30340;&#20027;&#21160;&#33719;&#21462;&#65288;A2MT&#65289;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#36755;&#20837;&#29305;&#24449;&#22312;&#27979;&#35797;&#26102;&#19981;&#23481;&#26131;&#33719;&#24471;&#65292;&#24517;&#39035;&#20197;&#36739;&#22823;&#20195;&#20215;&#33719;&#21462;&#12290;&#36890;&#36807;A2MT&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#65292;&#20351;&#20854;&#33021;&#22815;&#20027;&#21160;&#36873;&#25321;&#35201;&#33719;&#21462;&#30340;&#36755;&#20837;&#27169;&#24577;&#65292;&#26435;&#34913;&#33719;&#21462;&#25104;&#26412;&#19982;&#39044;&#27979;&#24615;&#33021;&#12290;A2MT&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#20219;&#21153;&#65292;&#31216;&#20026;&#20027;&#21160;&#29305;&#24449;&#33719;&#21462;&#65292;&#20197;&#20415;&#36827;&#34892;&#20851;&#20110;&#39640;&#32500;&#36755;&#20837;&#30340;&#26102;&#38388;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Perceiver IO&#26550;&#26500;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;A2MT&#12290;&#25105;&#20204;&#30340;&#20195;&#29702;&#31243;&#24207;&#33021;&#22815;&#35299;&#20915;&#19968;&#20010;&#38656;&#35201;&#23454;&#38469;&#30456;&#20851;&#30340;&#36328;&#27169;&#24577;&#25512;&#29702;&#25216;&#33021;&#30340;&#26032;&#39062;&#21512;&#25104;&#24773;&#26223;&#12290;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;Kinetics-700&#21644;AudioSet&#19978;&#65292;&#25105;&#20204;&#30340;&#20195;&#29702;&#31243;&#24207;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#25104;&#26412;&#21453;&#24212;&#24335;&#30340;&#33719;&#21462;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#28040;&#34701;&#23454;&#39564;&#34920;&#26126;&#23427;&#20204;&#26080;&#27861;&#23398;&#20064;&#21040;&#33258;&#36866;&#24212;&#30340;&#33719;&#21462;&#31574;&#30053;&#65292;&#31361;&#26174;&#20102;&#35813;&#20219;&#21153;&#30340;&#22256;&#38590;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a challenging decision-making task that we call active acquisition for multimodal temporal data (A2MT). In many real-world scenarios, input features are not readily available at test time and must instead be acquired at significant cost. With A2MT, we aim to learn agents that actively select which modalities of an input to acquire, trading off acquisition cost and predictive performance. A2MT extends a previous task called active feature acquisition to temporal decision making about high-dimensional inputs. We propose a method based on the Perceiver IO architecture to address A2MT in practice. Our agents are able to solve a novel synthetic scenario requiring practically relevant cross-modal reasoning skills. On two large-scale, real-world datasets, Kinetics-700 and AudioSet, our agents successfully learn cost-reactive acquisition behavior. However, an ablation reveals they are unable to learn adaptive acquisition strategies, emphasizing the difficulty of the task even for 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23398;&#20064;&#21644;&#21033;&#29992;&#37327;&#23376;&#21147;&#23398;&#31995;&#32479;&#30340;&#21704;&#23494;&#39039;&#21644;&#20854;&#21464;&#20998;&#28909;&#29366;&#24577;&#20272;&#35745;&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#26041;&#27861;&#24212;&#29992;&#20110;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#25968;&#25454;&#30340;&#29983;&#25104;&#24314;&#27169;&#21644;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.03803</link><description>&lt;p&gt;
&#37327;&#23376;&#27010;&#29575;&#21704;&#23494;&#39039;&#23398;&#20064;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#21644;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Quantum-probabilistic Hamiltonian learning for generative modelling &amp; anomaly detection. (arXiv:2211.03803v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23398;&#20064;&#21644;&#21033;&#29992;&#37327;&#23376;&#21147;&#23398;&#31995;&#32479;&#30340;&#21704;&#23494;&#39039;&#21644;&#20854;&#21464;&#20998;&#28909;&#29366;&#24577;&#20272;&#35745;&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#26041;&#27861;&#24212;&#29992;&#20110;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#25968;&#25454;&#30340;&#29983;&#25104;&#24314;&#27169;&#21644;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#23396;&#31435;&#30340;&#37327;&#23376;&#21147;&#23398;&#31995;&#32479;&#30340;&#21704;&#23494;&#39039;&#20915;&#23450;&#20102;&#20854;&#21160;&#21147;&#23398;&#21644;&#29289;&#29702;&#34892;&#20026;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23398;&#20064;&#21644;&#21033;&#29992;&#31995;&#32479;&#30340;&#21704;&#23494;&#39039;&#21644;&#20854;&#21464;&#20998;&#28909;&#29366;&#24577;&#20272;&#35745;&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#37327;&#23376;&#21704;&#23494;&#39039;&#30340;&#27169;&#22411;&#26041;&#27861;&#23545;&#27169;&#25311;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#25968;&#25454;&#36827;&#34892;&#29983;&#25104;&#24314;&#27169;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#25968;&#25454;&#21487;&#20197;&#34920;&#31034;&#20026;&#28151;&#21512;&#24577;&#12290;&#22312;&#36827;&#19968;&#27493;&#30340;&#27493;&#39588;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#23398;&#21040;&#30340;&#21704;&#23494;&#39039;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#23637;&#31034;&#20102;&#19981;&#21516;&#26679;&#26412;&#31867;&#22411;&#22312;&#34987;&#35270;&#20026;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#26102;&#21487;&#20197;&#24418;&#25104;&#19981;&#21516;&#30340;&#21160;&#21147;&#23398;&#34892;&#20026;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#29305;&#24449;&#26469;&#37327;&#21270;&#26679;&#26412;&#31867;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35774;&#35745;&#29992;&#20110;&#22330;&#35770;&#35745;&#31639;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#21033;&#29992;&#65292;&#20174;&#32780;&#23558;&#29702;&#35770;&#26041;&#27861;&#24212;&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Hamiltonian of an isolated quantum mechanical system determines its dynamics and physical behaviour. This study investigates the possibility of learning and utilising a system's Hamiltonian and its variational thermal state estimation for data analysis techniques. For this purpose, we employ the method of Quantum Hamiltonian-Based Models for the generative modelling of simulated Large Hadron Collider data and demonstrate the representability of such data as a mixed state. In a further step, we use the learned Hamiltonian for anomaly detection, showing that different sample types can form distinct dynamical behaviours once treated as a quantum many-body system. We exploit these characteristics to quantify the difference between sample types. Our findings show that the methodologies designed for field theory computations can be utilised in machine learning applications to employ theoretical approaches in data analysis techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#25968;&#25454;&#36164;&#28304;&#19979;&#30340;&#22270;&#20998;&#31867;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#25955;&#23556;&#21464;&#25442;&#30340;&#38750;&#32447;&#24615;&#36817;&#20284;&#22270;&#36951;&#24536;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2211.03216</link><description>&lt;p&gt;
&#26377;&#38480;&#25968;&#25454;&#36164;&#28304;&#19979;&#30340;&#22270;&#20998;&#31867;&#22120;&#36951;&#24536;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unlearning Graph Classifiers with Limited Data Resources. (arXiv:2211.03216v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#25968;&#25454;&#36164;&#28304;&#19979;&#30340;&#22270;&#20998;&#31867;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#25955;&#23556;&#21464;&#25442;&#30340;&#38750;&#32447;&#24615;&#36817;&#20284;&#22270;&#36951;&#24536;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29992;&#25143;&#38544;&#31169;&#38656;&#27714;&#30340;&#22686;&#21152;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21463;&#25511;&#25968;&#25454;&#21024;&#38500;&#65288;&#26426;&#22120;&#36951;&#24536;&#65289;&#25104;&#20026;&#25968;&#25454;&#25935;&#24863;&#30340;&#32593;&#32476;&#24212;&#29992;&#65288;&#22914;&#31038;&#20132;&#32593;&#32476;&#21644;&#25512;&#33616;&#31995;&#32479;&#65289;&#20013;&#30340;&#37325;&#35201;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#22914;&#20309;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#26426;&#22120;&#36951;&#24536;&#65292;&#23588;&#20854;&#26159;&#22312;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#65292;&#27492;&#26102;&#36951;&#24536;&#21487;&#33021;&#20005;&#37325;&#25439;&#23475;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;&#36951;&#24536;&#22270;&#25955;&#23556;&#21464;&#25442;&#65288;GST&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#23545;&#29305;&#24449;&#25110;&#22270;&#25299;&#25169;&#25200;&#21160;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#19988;&#22312;&#22270;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#19982;GNNs&#30456;&#24403;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22522;&#20110;GSTs&#30340;&#31532;&#19968;&#20010;&#24050;&#30693;&#30340;&#38750;&#32447;&#24615;&#36817;&#20284;&#22270;&#36951;&#24536;&#26041;&#27861;&#12290;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#23545;&#25152;&#25552;&#20986;&#30340;&#36951;&#24536;&#26041;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the demand for user privacy grows, controlled data removal (machine unlearning) is becoming an important feature of machine learning models for data-sensitive Web applications such as social networks and recommender systems. Nevertheless, at this point it is still largely unknown how to perform efficient machine unlearning of graph neural networks (GNNs); this is especially the case when the number of training samples is small, in which case unlearning can seriously compromise the performance of the model. To address this issue, we initiate the study of unlearning the Graph Scattering Transform (GST), a mathematical framework that is efficient, provably stable under feature or graph topology perturbations, and offers graph classification performance comparable to that of GNNs. Our main contribution is the first known nonlinear approximate graph unlearning method based on GSTs. Our second contribution is a theoretical analysis of the computational complexity of the proposed unlearnin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#24402;&#22240;&#26041;&#27861;&#23545;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20998;&#31867;&#30340;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#19982;&#35786;&#26029;&#26631;&#20934;&#31867;&#20284;&#30340;&#23398;&#20064;&#29305;&#24449;&#12290;&#30456;&#20851;&#24615;&#20998;&#25968;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#38543;&#30528;&#20998;&#31867;&#27010;&#29575;&#30340;&#22686;&#21152;&#65292;&#24179;&#22343;&#20540;&#20063;&#22686;&#21152;&#65292;&#24182;&#19988;&#22312;&#25509;&#36817;&#38646;&#26102;&#20135;&#29983;&#35823;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2211.01738</link><description>&lt;p&gt;
&#23545;&#20110;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#25581;&#31034;&#20102;&#31867;&#20284;&#20110;&#35786;&#26029;&#26631;&#20934;&#30340;&#23398;&#20064;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Analysis of a Deep Learning Model for 12-Lead ECG Classification Reveals Learned Features Similar to Diagnostic Criteria. (arXiv:2211.01738v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#24402;&#22240;&#26041;&#27861;&#23545;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20998;&#31867;&#30340;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#19982;&#35786;&#26029;&#26631;&#20934;&#31867;&#20284;&#30340;&#23398;&#20064;&#29305;&#24449;&#12290;&#30456;&#20851;&#24615;&#20998;&#25968;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#38543;&#30528;&#20998;&#31867;&#27010;&#29575;&#30340;&#22686;&#21152;&#65292;&#24179;&#22343;&#20540;&#20063;&#22686;&#21152;&#65292;&#24182;&#19988;&#22312;&#25509;&#36817;&#38646;&#26102;&#20135;&#29983;&#35823;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#20173;&#26410;&#24471;&#21040;&#37319;&#29992;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#23427;&#20204;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#30740;&#31350;&#23558;&#24402;&#22240;&#26041;&#27861;&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20998;&#31867;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#65292;&#20197;&#25171;&#24320;&#36825;&#20010;&#8220;&#40657;&#30418;&#23376;&#8221;&#65292;&#29702;&#35299;&#27169;&#22411;&#39044;&#27979;&#21644;&#23398;&#20064;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#23545;&#26469;&#33258;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#19988;&#24402;&#22240;&#26041;&#27861;&#20026;&#27599;&#20010;&#20998;&#31867;&#20449;&#21495;&#26679;&#26412;&#20998;&#37197;&#8220;&#30456;&#20851;&#24615;&#20998;&#25968;&#8221;&#12290;&#36825;&#26679;&#21487;&#20197;&#20998;&#26512;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23398;&#21040;&#20102;&#20160;&#20040;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#23450;&#37327;&#26041;&#27861;&#65306;a&#65289;&#31867;&#21035;&#30340;&#24179;&#22343;&#30456;&#20851;&#24615;&#20998;&#25968;&#65292;b&#65289;&#23548;&#32852;&#30340;&#24179;&#22343;&#30456;&#20851;&#24615;&#20998;&#25968;&#65292;c&#65289;&#24179;&#22343;&#33410;&#25293;&#30340;&#24179;&#22343;&#30456;&#20851;&#24615;&#20998;&#25968;&#12290;&#23545;&#25151;&#39076;&#65288;AF&#65289;&#21644;&#24038;&#26463;&#25903;&#20256;&#23548;&#38459;&#28382;&#65288;LBBB&#65289;&#30340;&#30456;&#20851;&#24615;&#20998;&#25968;&#36827;&#34892;&#20998;&#26512;&#19982;&#20581;&#24247;&#23545;&#29031;&#32452;&#30456;&#27604;&#65292;&#32467;&#26524;&#34920;&#26126;a&#65289;&#38543;&#30528;&#20998;&#31867;&#27010;&#29575;&#30340;&#22686;&#21152;&#65292;&#24179;&#22343;&#20540;&#22686;&#21152;&#65292;&#24182;&#19988;&#25509;&#36817;&#38646;&#26102;&#23545;&#24212;&#20110;&#35823;&#20998;&#31867;&#65292;b&#65289;cor
&lt;/p&gt;
&lt;p&gt;
Despite their remarkable performance, deep neural networks remain unadopted in clinical practice, which is considered to be partially due to their lack in explainability. In this work, we apply attribution methods to a pre-trained deep neural network (DNN) for 12-lead electrocardiography classification to open this "black box" and understand the relationship between model prediction and learned features. We classify data from a public data set and the attribution methods assign a "relevance score" to each sample of the classified signals. This allows analyzing what the network learned during training, for which we propose quantitative methods: average relevance scores over a) classes, b) leads, and c) average beats. The analyses of relevance scores for atrial fibrillation (AF) and left bundle branch block (LBBB) compared to healthy controls show that their mean values a) increase with higher classification probability and correspond to false classifications when around zero, and b) cor
&lt;/p&gt;</description></item><item><title>CausalBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#21333;&#32454;&#32990;&#24178;&#25200;&#23454;&#39564;&#30340;&#30495;&#23454;&#19990;&#30028;&#24178;&#39044;&#25968;&#25454;&#19978;&#30340;&#32593;&#32476;&#25512;&#26029;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#12290;&#36890;&#36807;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#26041;&#27861;&#21487;&#25193;&#23637;&#24615;&#24046;&#65292;&#38480;&#21046;&#20102;&#24615;&#33021;&#12290;&#20351;&#29992;&#24178;&#39044;&#20449;&#24687;&#30340;&#26041;&#27861;&#24182;&#27809;&#26377;&#36229;&#36234;&#37027;&#20123;&#19981;&#20351;&#29992;&#24178;&#39044;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.17283</link><description>&lt;p&gt;
CausalBench&#65306;&#22522;&#20110;&#21333;&#32454;&#32990;&#24178;&#25200;&#25968;&#25454;&#30340;&#32593;&#32476;&#25512;&#26029;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CausalBench: A Large-scale Benchmark for Network Inference from Single-cell Perturbation Data. (arXiv:2210.17283v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17283
&lt;/p&gt;
&lt;p&gt;
CausalBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#21333;&#32454;&#32990;&#24178;&#25200;&#23454;&#39564;&#30340;&#30495;&#23454;&#19990;&#30028;&#24178;&#39044;&#25968;&#25454;&#19978;&#30340;&#32593;&#32476;&#25512;&#26029;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#12290;&#36890;&#36807;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#26041;&#27861;&#21487;&#25193;&#23637;&#24615;&#24046;&#65292;&#38480;&#21046;&#20102;&#24615;&#33021;&#12290;&#20351;&#29992;&#24178;&#39044;&#20449;&#24687;&#30340;&#26041;&#27861;&#24182;&#27809;&#26377;&#36229;&#36234;&#37027;&#20123;&#19981;&#20351;&#29992;&#24178;&#39044;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26159;&#22810;&#20010;&#31185;&#23398;&#23398;&#31185;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#24182;&#32463;&#24120;&#24212;&#29992;&#20110;&#21307;&#23398;&#31561;&#39640;&#24433;&#21709;&#24212;&#29992;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#35780;&#20272;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#30340;&#24615;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#22312;&#24178;&#39044;&#21644;&#23545;&#29031;&#26465;&#20214;&#19979;&#35266;&#23519;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20256;&#32479;&#35780;&#20272;&#19981;&#33021;&#21453;&#26144;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CausalBench&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#22823;&#35268;&#27169;&#21333;&#32454;&#32990;&#24178;&#25200;&#23454;&#39564;&#30340;&#30495;&#23454;&#19990;&#30028;&#24178;&#39044;&#25968;&#25454;&#19978;&#30340;&#32593;&#32476;&#25512;&#26029;&#26041;&#27861;&#12290;CausalBench&#21253;&#25324;&#29983;&#29289;&#23398;&#21160;&#26426;&#30340;&#24615;&#33021;&#24230;&#37327;&#65292;&#21253;&#25324;&#26032;&#30340;&#22522;&#20110;&#20998;&#24067;&#30340;&#24178;&#39044;&#24230;&#37327;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;CausalBench&#22871;&#20214;&#23545;&#26368;&#20808;&#36827;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#31361;&#20986;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#24046;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#24178;&#39044;&#20449;&#24687;&#30340;&#26041;&#27861;&#24182;&#27809;&#26377;&#36229;&#36234;&#37027;&#20123;&#19981;&#20351;&#29992;&#24178;&#39044;&#20449;&#24687;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Causal inference is a vital aspect of multiple scientific disciplines and is routinely applied to high-impact applications such as medicine. However, evaluating the performance of causal inference methods in real-world environments is challenging due to the need for observations under both interventional and control conditions. Traditional evaluations conducted on synthetic datasets do not reflect the performance in real-world systems. To address this, we introduce CausalBench, a benchmark suite for evaluating network inference methods on real-world interventional data from large-scale single-cell perturbation experiments. CausalBench incorporates biologically-motivated performance metrics, including new distribution-based interventional metrics. A systematic evaluation of state-of-the-art causal inference methods using our CausalBench suite highlights how poor scalability of current methods limits performance. Moreover, methods that use interventional information do not outperform tho
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#32852;&#37030;&#32858;&#31867;&#30340;&#26426;&#22120;&#26410;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#23433;&#20840;&#32852;&#37030;&#32858;&#31867;&#26694;&#26550;&#30340;&#39640;&#25928;&#26410;&#23398;&#20064;&#26426;&#21046;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#38544;&#31169;&#65292;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#23433;&#20840;&#21387;&#32553;&#22810;&#38598;&#21512;&#32858;&#21512;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24615;&#33021;&#20248;&#33391;&#65292;&#36866;&#29992;&#20110;&#35299;&#20915;&#31232;&#30095;&#23433;&#20840;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#21644;&#20854;&#20182;&#19968;&#33324;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.16424</link><description>&lt;p&gt;
&#32852;&#37030;&#32858;&#31867;&#30340;&#26426;&#22120;&#26410;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning of Federated Clusters. (arXiv:2210.16424v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#32852;&#37030;&#32858;&#31867;&#30340;&#26426;&#22120;&#26410;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#23433;&#20840;&#32852;&#37030;&#32858;&#31867;&#26694;&#26550;&#30340;&#39640;&#25928;&#26410;&#23398;&#20064;&#26426;&#21046;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#38544;&#31169;&#65292;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#23433;&#20840;&#21387;&#32553;&#22810;&#38598;&#21512;&#32858;&#21512;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24615;&#33021;&#20248;&#33391;&#65292;&#36866;&#29992;&#20110;&#35299;&#20915;&#31232;&#30095;&#23433;&#20840;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#21644;&#20854;&#20182;&#19968;&#33324;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#32858;&#31867; (FC) &#26159;&#19968;&#20010;&#20986;&#29616;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#21253;&#25324;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#21307;&#30103;&#31995;&#32479;&#12290;&#38543;&#30528;&#26368;&#36817;&#30830;&#20445;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27861;&#24459;&#30340;&#37319;&#32435;&#65292;&#26426;&#22120;&#26410;&#23398;&#20064;&#38382;&#39064;&#23545;&#20110;FC&#26041;&#27861;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#39318;&#27425;&#20171;&#32461;&#20102;FC&#30340;&#26426;&#22120;&#26410;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#23433;&#20840;FC&#26694;&#26550;&#30340;&#39640;&#25928;&#26410;&#23398;&#20064;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;FC&#26694;&#26550;&#21033;&#29992;&#20102;&#29305;&#27530;&#30340;&#21021;&#22987;&#21270;&#36807;&#31243;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#26410;&#23398;&#20064;&#38750;&#24120;&#36866;&#29992;&#12290;&#20026;&#20102;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#38544;&#31169;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#23433;&#20840;&#21387;&#32553;&#22810;&#38598;&#21512;&#32858;&#21512; (SCMA) &#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#32858;&#31867;&#20013;&#36935;&#21040;&#30340;&#31232;&#30095;&#23433;&#20840;&#32852;&#37030;&#23398;&#20064; (FL) &#38382;&#39064;&#20197;&#21450;&#26356;&#19968;&#33324;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#21516;&#26102;&#20419;&#36827;&#20302;&#36890;&#20449;&#22797;&#26434;&#24615;&#21644;&#31192;&#23494;&#20849;&#20139;&#21327;&#35758;&#65292;&#25105;&#20204;&#23558;Reed-Solomon&#32534;&#30721;&#19982;&#29305;&#27530;&#35780;&#20272;&#28857;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;SCMA&#27969;&#27700;&#32447;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated clustering (FC) is an unsupervised learning problem that arises in a number of practical applications, including personalized recommender and healthcare systems. With the adoption of recent laws ensuring the "right to be forgotten", the problem of machine unlearning for FC methods has become of significant importance. We introduce, for the first time, the problem of machine unlearning for FC, and propose an efficient unlearning mechanism for a customized secure FC framework. Our FC framework utilizes special initialization procedures that we show are well-suited for unlearning. To protect client data privacy, we develop the secure compressed multiset aggregation (SCMA) framework that addresses sparse secure federated learning (FL) problems encountered during clustering as well as more general problems. To simultaneously facilitate low communication complexity and secret sharing protocols, we integrate Reed-Solomon encoding with special evaluation points into our SCMA pipeline
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22240;&#26524;&#35299;&#37322;&#26426;&#21046;&#65292;&#33021;&#22815;&#37327;&#21270;&#29366;&#24577;&#23545;&#34892;&#21160;&#30340;&#22240;&#26524;&#37325;&#35201;&#24615;&#21644;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#27169;&#25311;&#30740;&#31350;&#35777;&#26126;&#20102;&#35813;&#26426;&#21046;&#22312;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35299;&#37322;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.13507</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#22240;&#26524;&#35299;&#37322;&#65306;&#37327;&#21270;&#29366;&#24577;&#21644;&#26102;&#38388;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Causal Explanation for Reinforcement Learning: Quantifying State and Temporal Importance. (arXiv:2210.13507v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22240;&#26524;&#35299;&#37322;&#26426;&#21046;&#65292;&#33021;&#22815;&#37327;&#21270;&#29366;&#24577;&#23545;&#34892;&#21160;&#30340;&#22240;&#26524;&#37325;&#35201;&#24615;&#21644;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#27169;&#25311;&#30740;&#31350;&#35777;&#26126;&#20102;&#35813;&#26426;&#21046;&#22312;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35299;&#37322;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36215;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#20154;&#20204;&#36890;&#36807;&#22240;&#26524;&#38236;&#22836;&#26469;&#30475;&#24453;&#19990;&#30028;&#65292;&#22240;&#27492;&#26356;&#20542;&#21521;&#20110;&#22240;&#26524;&#35299;&#37322;&#32780;&#38750;&#20851;&#32852;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22240;&#26524;&#35299;&#37322;&#26426;&#21046;&#65292;&#20197;&#37327;&#21270;&#29366;&#24577;&#23545;&#34892;&#21160;&#30340;&#22240;&#26524;&#37325;&#35201;&#24615;&#21644;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#27169;&#25311;&#30740;&#31350;&#65292;&#21253;&#25324;&#20892;&#30000;&#28748;&#28297;&#12289;21&#28857;&#12289;&#36991;&#30896;&#21644;&#26376;&#29699;&#30528;&#38470;&#22120;&#31561;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26426;&#21046;&#22312;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35299;&#37322;&#26041;&#38754;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#20851;&#32852;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainability plays an increasingly important role in machine learning. Furthermore, humans view the world through a causal lens and thus prefer causal explanations over associational ones. Therefore, in this paper, we develop a causal explanation mechanism that quantifies the causal importance of states on actions and such importance over time. We also demonstrate the advantages of our mechanism over state-of-the-art associational methods in terms of RL policy explanation through a series of simulation studies, including crop irrigation, Blackjack, collision avoidance, and lunar lander.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38543;&#26426;&#31232;&#30095;&#35745;&#31639;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31232;&#30095;&#22270;&#25805;&#20316;&#38590;&#20197;&#21152;&#36895;&#21644;&#19981;&#35268;&#21017;&#25968;&#25454;&#26684;&#24335;&#23548;&#33268;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.10737</link><description>&lt;p&gt;
RSC: &#36890;&#36807;&#38543;&#26426;&#31232;&#30095;&#35745;&#31639;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
RSC: Accelerating Graph Neural Networks Training via Randomized Sparse Computations. (arXiv:2210.10737v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10737
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#31232;&#30095;&#35745;&#31639;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31232;&#30095;&#22270;&#25805;&#20316;&#38590;&#20197;&#21152;&#36895;&#21644;&#19981;&#35268;&#21017;&#25968;&#25454;&#26684;&#24335;&#23548;&#33268;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#35757;&#32451;&#38750;&#24120;&#32791;&#26102;&#65292;&#22240;&#20026;&#30828;&#20214;&#38590;&#20197;&#21152;&#36895;&#31232;&#30095;&#22270;&#25805;&#20316;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#37319;&#26679;&#30340;&#36924;&#36817;&#26469;&#38477;&#20302;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#20294;&#29306;&#29298;&#20102;&#35745;&#31639;&#31934;&#24230;&#12290;&#22312;&#36825;&#20010;&#24605;&#36335;&#30340;&#22522;&#30784;&#19978;&#65292;&#20197;&#24448;&#30340;&#24037;&#20316;&#25104;&#21151;&#22320;&#21152;&#36895;&#20102;&#22522;&#20110;&#31264;&#23494;&#30697;&#38453;&#30340;&#25805;&#20316;&#65288;&#22914;&#21367;&#31215;&#21644;&#32447;&#24615;&#25805;&#20316;&#65289;&#65292;&#20934;&#30830;&#24230;&#19979;&#38477;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#28982;&#32780;&#65292;&#19982;&#31264;&#23494;&#30697;&#38453;&#19981;&#21516;&#65292;&#31232;&#30095;&#30697;&#38453;&#20197;&#19981;&#35268;&#21017;&#30340;&#25968;&#25454;&#26684;&#24335;&#23384;&#20648;&#65292;&#27599;&#34892;/&#21015;&#21487;&#33021;&#26377;&#19981;&#21516;&#25968;&#37327;&#30340;&#38750;&#38646;&#20803;&#32032;&#12290;&#22240;&#27492;&#65292;&#19982;&#31264;&#23494;&#30697;&#38453;&#30456;&#27604;&#65292;&#36924;&#36817;&#31232;&#30095;&#25805;&#20316;&#23384;&#22312;&#20004;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#65288;1&#65289;&#25105;&#20204;&#26080;&#27861;&#30452;&#25509;&#25511;&#21046;&#36924;&#36817;&#31232;&#30095;&#25805;&#20316;&#30340;&#25928;&#29575;&#65292;&#22240;&#20026;&#35745;&#31639;&#20165;&#22312;&#38750;&#38646;&#20803;&#32032;&#19978;&#25191;&#34892;&#65307;&#65288;2&#65289;&#22522;&#20110;&#23376;&#37319;&#26679;&#30340;&#31232;&#30095;&#30697;&#38453;&#22788;&#29702;&#25928;&#29575;&#26356;&#20302;&#65292;&#22240;&#20026;&#23384;&#22312;&#19981;&#35268;&#21017;&#25968;&#25454;&#26684;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#25511;&#21046;&#20934;&#30830;&#24230;&#21644;&#25928;&#29575;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training of graph neural networks (GNNs) is extremely time consuming because sparse graph-based operations are hard to be accelerated by hardware. Prior art explores trading off the computational precision to reduce the time complexity via sampling-based approximation. Based on the idea, previous works successfully accelerate the dense matrix based operations (e.g., convolution and linear) with negligible accuracy drop. However, unlike dense matrices, sparse matrices are stored in the irregular data format such that each row/column may have different number of non-zero entries. Thus, compared to the dense counterpart, approximating sparse operations has two unique challenges (1) we cannot directly control the efficiency of approximated sparse operation since the computation is only executed on non-zero entries; (2) sub-sampling sparse matrices is much more inefficient due to the irregular data format. To address the issues, our key idea is to control the accuracy-efficiency trade o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#27880;&#24847;&#21147;&#22522;&#20934;&#27979;&#35797;&#65288;CAB&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#22312;&#24314;&#27169;&#38271;&#24207;&#21015;&#26102;&#30340;&#39640;&#25928;&#27880;&#24847;&#21147;&#26041;&#27861;&#12290;CAB&#21253;&#25324;&#20102;&#32454;&#31890;&#24230;&#30340;&#27880;&#24847;&#21147;&#20998;&#31867;&#20307;&#31995;&#65292;&#28085;&#30422;&#20102;&#38750;&#22240;&#26524;&#33258;&#27880;&#24847;&#21147;&#12289;&#22240;&#26524;&#33258;&#27880;&#24847;&#21147;&#12289;&#38750;&#22240;&#26524;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#22240;&#26524;&#20132;&#21449;&#27880;&#24847;&#21147;&#22235;&#31181;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#24182;&#37319;&#38598;&#20102;&#19971;&#20010;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2210.07661</link><description>&lt;p&gt;
CAB: &#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#20840;&#38754;&#27880;&#24847;&#21147;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling. (arXiv:2210.07661v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#27880;&#24847;&#21147;&#22522;&#20934;&#27979;&#35797;&#65288;CAB&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#22312;&#24314;&#27169;&#38271;&#24207;&#21015;&#26102;&#30340;&#39640;&#25928;&#27880;&#24847;&#21147;&#26041;&#27861;&#12290;CAB&#21253;&#25324;&#20102;&#32454;&#31890;&#24230;&#30340;&#27880;&#24847;&#21147;&#20998;&#31867;&#20307;&#31995;&#65292;&#28085;&#30422;&#20102;&#38750;&#22240;&#26524;&#33258;&#27880;&#24847;&#21147;&#12289;&#22240;&#26524;&#33258;&#27880;&#24847;&#21147;&#12289;&#38750;&#22240;&#26524;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#22240;&#26524;&#20132;&#21449;&#27880;&#24847;&#21147;&#22235;&#31181;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#24182;&#37319;&#38598;&#20102;&#19971;&#20010;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#35821;&#35328;&#12289;&#22270;&#20687;&#21644;&#35821;&#38899;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#25552;&#20986;&#20102;&#21508;&#31181;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#26550;&#26500;&#65292;&#20197;&#25552;&#39640;Transformer&#22312;&#24314;&#27169;&#38271;&#24207;&#21015;&#26102;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#22823;&#24133;&#20445;&#30041;&#20854;&#34920;&#29616;&#21147;&#12290;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#29992;&#20110;&#27979;&#35797;&#36825;&#20123;&#39640;&#25928;&#26041;&#27861;&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#19978;&#30340;&#22522;&#20934;&#26159;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#65288;LRA&#65289;&#12290;&#28982;&#32780;&#65292;LRA&#21482;&#20851;&#27880;&#26631;&#20934;&#30340;&#21452;&#21521;&#65288;&#25110;&#38750;&#22240;&#26524;&#65289;&#33258;&#27880;&#24847;&#21147;&#65292;&#23436;&#20840;&#24573;&#30053;&#20102;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#21333;&#21521;&#65288;&#25110;&#22240;&#26524;&#65289;&#27880;&#24847;&#21147;&#65292;&#32780;&#36825;&#23545;&#20110;&#19979;&#28216;&#24212;&#29992;&#21516;&#26679;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#27880;&#24847;&#21147;&#22522;&#20934;&#27979;&#35797;&#65288;CAB&#65289;&#65292;&#37319;&#29992;&#20102;&#32454;&#31890;&#24230;&#30340;&#27880;&#24847;&#21147;&#20998;&#31867;&#20307;&#31995;&#65292;&#21253;&#25324;&#38750;&#22240;&#26524;&#33258;&#27880;&#24847;&#21147;&#12289;&#22240;&#26524;&#33258;&#27880;&#24847;&#21147;&#12289;&#38750;&#22240;&#26524;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#22240;&#26524;&#20132;&#21449;&#27880;&#24847;&#21147;&#22235;&#31181;&#21487;&#21306;&#20998;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#12290;CAB&#25910;&#38598;&#20102;&#26469;&#33258;&#19981;&#21516;&#30740;&#31350;&#39046;&#22495;&#30340;&#19971;&#20010;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#65292;&#20197;&#35780;&#20272;&#22312;&#22235;&#31181;&#27880;&#24847;&#21147;&#27169;&#24335;&#19979;&#30340;&#39640;&#25928;&#27880;&#24847;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer has achieved remarkable success in language, image, and speech processing. Recently, various efficient attention architectures have been proposed to improve transformer's efficiency while largely preserving its efficacy, especially in modeling long sequences. A widely-used benchmark to test these efficient methods' capability on long-range modeling is Long Range Arena (LRA). However, LRA only focuses on the standard bidirectional (or noncausal) self attention, and completely ignores cross attentions and unidirectional (or causal) attentions, which are equally important to downstream applications. In this paper, we propose Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self, noncausal cross, and causal cross attentions. CAB collects seven real-world tasks from different research areas to evaluate efficient attentions under the four attention patterns. Among these tas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#27719;&#32858;&#20449;&#24687;&#30340;&#21508;&#31181;&#27719;&#32858;&#36816;&#31639;&#31526;&#22312;&#23884;&#20837;&#32534;&#30721;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#32771;&#34385;&#30340;&#27719;&#32858;&#36816;&#31639;&#31526;&#37117;&#21487;&#20197;&#28385;&#36275;&#35748;&#35782;&#27719;&#32858;&#21407;&#21017;&#65292;&#20294;&#36825;&#20165;&#22312;&#23884;&#20837;&#20855;&#26377;&#36275;&#22815;&#39640;&#32500;&#24230;&#24182;&#28385;&#36275;&#29305;&#23450;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#25104;&#31435;&#12290;&#36825;&#20123;&#32422;&#26463;&#23545;&#23884;&#20837;&#22312;&#23454;&#36341;&#20013;&#30340;&#20351;&#29992;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2210.05723</link><description>&lt;p&gt;
&#23884;&#20837;&#20316;&#20026;&#35748;&#35782;&#29366;&#24577;&#65306;&#20851;&#20110;&#29992;&#20110;&#33719;&#24471;&#30693;&#35782;&#30340;&#27719;&#32858;&#36816;&#31639;&#31526;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Embeddings as Epistemic States: Limitations on the Use of Pooling Operators for Accumulating Knowledge. (arXiv:2210.05723v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#27719;&#32858;&#20449;&#24687;&#30340;&#21508;&#31181;&#27719;&#32858;&#36816;&#31639;&#31526;&#22312;&#23884;&#20837;&#32534;&#30721;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#32771;&#34385;&#30340;&#27719;&#32858;&#36816;&#31639;&#31526;&#37117;&#21487;&#20197;&#28385;&#36275;&#35748;&#35782;&#27719;&#32858;&#21407;&#21017;&#65292;&#20294;&#36825;&#20165;&#22312;&#23884;&#20837;&#20855;&#26377;&#36275;&#22815;&#39640;&#32500;&#24230;&#24182;&#28385;&#36275;&#29305;&#23450;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#25104;&#31435;&#12290;&#36825;&#20123;&#32422;&#26463;&#23545;&#23884;&#20837;&#22312;&#23454;&#36341;&#20013;&#30340;&#20351;&#29992;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20381;&#38752;&#27719;&#32858;&#36816;&#31639;&#31526;&#26469;&#32858;&#21512;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36890;&#24120;&#40664;&#35748;&#21521;&#37327;&#32534;&#30721;&#20026;&#35748;&#35782;&#29366;&#24577;&#65292;&#21363;&#21521;&#37327;&#25429;&#25417;&#21040;&#24050;&#33719;&#21462;&#26377;&#20851;&#26576;&#20123;&#24863;&#20852;&#36259;&#23646;&#24615;&#30340;&#35777;&#25454;&#65292;&#24182;&#19988;&#27719;&#32858;&#36825;&#20123;&#21521;&#37327;&#20250;&#24471;&#21040;&#19968;&#20010;&#32467;&#21512;&#36825;&#20123;&#35777;&#25454;&#30340;&#21521;&#37327;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#26631;&#20934;&#27719;&#32858;&#36816;&#31639;&#31526;&#65292;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#23427;&#20204;&#19982;&#36825;&#20010;&#34987;&#31216;&#20026;&#35748;&#35782;&#27719;&#32858;&#21407;&#21017;&#30340;&#24819;&#27861;&#20860;&#23481;&#12290;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#32771;&#34385;&#30340;&#27719;&#32858;&#36816;&#31639;&#31526;&#37117;&#21487;&#20197;&#28385;&#36275;&#35748;&#35782;&#27719;&#32858;&#21407;&#21017;&#65292;&#20294;&#36825;&#20165;&#22312;&#23884;&#20837;&#20855;&#26377;&#36275;&#22815;&#39640;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#25104;&#31435;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#27719;&#32858;&#36816;&#31639;&#31526;&#65292;&#23884;&#20837;&#38656;&#35201;&#28385;&#36275;&#29305;&#23450;&#32422;&#26463;&#65288;&#20363;&#22914;&#20855;&#26377;&#38750;&#36127;&#22352;&#26631;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#20123;&#32422;&#26463;&#23545;&#23884;&#20837;&#22312;&#23454;&#36341;&#20013;&#30340;&#20351;&#29992;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#35748;&#35782;&#27719;&#32858;&#21407;&#21017;&#19981;&#33021;&#34987;&#28385;&#36275;&#26102;&#65292;&#20351;&#29992;&#19968;&#20010;&#26080;&#32422;&#26463;&#30340;&#31232;&#30095;&#35299;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various neural network architectures rely on pooling operators to aggregate information coming from different sources. It is often implicitly assumed in such contexts that vectors encode epistemic states, i.e. that vectors capture the evidence that has been obtained about some properties of interest, and that pooling these vectors yields a vector that combines this evidence. We study, for a number of standard pooling operators, under what conditions they are compatible with this idea, which we call the epistemic pooling principle. While we find that all the considered pooling operators can satisfy the epistemic pooling principle, this only holds when embeddings are sufficiently high-dimensional and, for most pooling operators, when the embeddings satisfy particular constraints (e.g. having non-negative coordinates). We furthermore show that these constraints have important implications on how the embeddings can be used in practice. In particular, we find that when the epistemic pooling
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;Neural EKF&#65289;&#30340;&#21487;&#23398;&#20064;&#21345;&#23572;&#26364;&#28388;&#27874;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#28508;&#22312;&#28436;&#21270;&#21160;&#21147;&#23398;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#26469;&#23398;&#20064;&#36807;&#31243;&#21160;&#21147;&#23398;&#21644;&#20256;&#24863;&#35266;&#27979;&#30340;&#24314;&#27169;&#65292;&#25552;&#39640;&#32467;&#26500;&#21709;&#24212;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.04165</link><description>&lt;p&gt;
&#31070;&#32463;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#29992;&#20110;&#23398;&#20064;&#21644;&#39044;&#27979;&#32467;&#26500;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Neural Extended Kalman Filters for Learning and Predicting Dynamics of Structural Systems. (arXiv:2210.04165v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;Neural EKF&#65289;&#30340;&#21487;&#23398;&#20064;&#21345;&#23572;&#26364;&#28388;&#27874;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#28508;&#22312;&#28436;&#21270;&#21160;&#21147;&#23398;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#26469;&#23398;&#20064;&#36807;&#31243;&#21160;&#21147;&#23398;&#21644;&#20256;&#24863;&#35266;&#27979;&#30340;&#24314;&#27169;&#65292;&#25552;&#39640;&#32467;&#26500;&#21709;&#24212;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#32467;&#26500;&#21709;&#24212;&#39044;&#27979;&#26159;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#21644;&#25511;&#21046;&#24212;&#29992;&#30340;&#20027;&#35201;&#39537;&#21160;&#21147;&#12290;&#36825;&#24448;&#24448;&#38656;&#35201;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20805;&#20998;&#25429;&#25417;&#22797;&#26434;&#32467;&#26500;&#31995;&#32479;&#30340;&#22522;&#26412;&#21160;&#21147;&#23398;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;&#31216;&#20026;&#31070;&#32463;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65289;&#26469;&#23398;&#20064;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#28508;&#22312;&#28436;&#21270;&#21160;&#21147;&#23398;&#12290;&#31070;&#32463;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#26159;&#20256;&#32479;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#20854;&#20013;&#36807;&#31243;&#21160;&#21147;&#23398;&#21644;&#20256;&#24863;&#35266;&#27979;&#30340;&#24314;&#27169;&#21487;&#20197;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#26469;&#21442;&#25968;&#21270;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#26469;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#22312;&#21464;&#20998;&#25512;&#29702;&#26694;&#26550;&#19979;&#23454;&#29616;&#65292;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#36890;&#36807;&#24863;&#30693;&#27979;&#37327;&#36827;&#34892;&#25512;&#29702;&#12290;&#36890;&#24120;&#65292;&#20256;&#32479;&#30340;&#21464;&#20998;&#25512;&#29702;&#27169;&#22411;&#30340;&#21442;&#25968;&#26159;&#29420;&#31435;&#20110;&#28508;&#22312;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#12290;&#36825;&#31181;&#29305;&#28857;&#20351;&#24471;&#25512;&#29702;&#21644;&#37325;&#26500;&#30340;&#20934;&#30830;&#24615;&#30456;&#23545;&#36739;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate structural response prediction forms a main driver for structural health monitoring and control applications. This often requires the proposed model to adequately capture the underlying dynamics of complex structural systems. In this work, we utilize a learnable Extended Kalman Filter (EKF), named the Neural Extended Kalman Filter (Neural EKF) throughout this paper, for learning the latent evolution dynamics of complex physical systems. The Neural EKF is a generalized version of the conventional EKF, where the modeling of process dynamics and sensory observations can be parameterized by neural networks, therefore learned by end-to-end training. The method is implemented under the variational inference framework with the EKF conducting inference from sensing measurements. Typically, conventional variational inference models are parameterized by neural networks independent of the latent dynamics models. This characteristic makes the inference and reconstruction accuracy weakly b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#26679;&#24615;&#35780;&#20272;&#25351;&#26631;Vendi&#20998;&#25968;&#65292;&#23427;&#33021;&#22815;&#28789;&#27963;&#22320;&#34913;&#37327;&#19981;&#21516;&#24418;&#24335;&#30340;&#22810;&#26679;&#24615;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#21442;&#32771;&#25968;&#25454;&#38598;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2210.02410</link><description>&lt;p&gt;
The Vendi&#20998;&#25968;: &#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#26679;&#24615;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
The Vendi Score: A Diversity Evaluation Metric for Machine Learning. (arXiv:2210.02410v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#26679;&#24615;&#35780;&#20272;&#25351;&#26631;Vendi&#20998;&#25968;&#65292;&#23427;&#33021;&#22815;&#28789;&#27963;&#22320;&#34913;&#37327;&#19981;&#21516;&#24418;&#24335;&#30340;&#22810;&#26679;&#24615;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#21442;&#32771;&#25968;&#25454;&#38598;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26679;&#24615;&#26159;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20013;&#35768;&#22810;&#39046;&#22495;&#30340;&#37325;&#35201;&#26631;&#20934;&#65292;&#21253;&#25324;&#29983;&#25104;&#24314;&#27169;&#21644;&#25968;&#25454;&#38598;&#31574;&#21010;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#34913;&#37327;&#22810;&#26679;&#24615;&#30340;&#25351;&#26631;&#24448;&#24448;&#26159;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#65292;&#24182;&#19988;&#28789;&#27963;&#24615;&#26377;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;Vendi&#20998;&#25968;&#26469;&#35299;&#20915;&#22810;&#26679;&#24615;&#35780;&#20272;&#38382;&#39064;&#65292;&#35813;&#25351;&#26631;&#23558;&#29983;&#24577;&#23398;&#21644;&#37327;&#23376;&#32479;&#35745;&#21147;&#23398;&#30340;&#24605;&#24819;&#19982;ML&#30456;&#32467;&#21512;&#24182;&#36827;&#34892;&#25193;&#23637;&#12290;Vendi&#20998;&#25968;&#23450;&#20041;&#20026;&#30456;&#20284;&#24615;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#30340;&#39321;&#20892;&#29109;&#30340;&#25351;&#25968;&#20989;&#25968;&#12290;&#36825;&#20010;&#30697;&#38453;&#26159;&#30001;&#29992;&#25143;&#23450;&#20041;&#30340;&#30456;&#20284;&#24615;&#20989;&#25968;&#24212;&#29992;&#20110;&#35201;&#35780;&#20272;&#22810;&#26679;&#24615;&#30340;&#26679;&#26412;&#32780;&#35825;&#23548;&#20986;&#30340;&#12290;&#36890;&#36807;&#20351;&#29992;&#30456;&#20284;&#24615;&#20989;&#25968;&#20316;&#20026;&#36755;&#20837;&#65292;Vendi&#20998;&#25968;&#20351;&#29992;&#25143;&#33021;&#22815;&#25351;&#23450;&#20219;&#20309;&#25152;&#38656;&#30340;&#22810;&#26679;&#24615;&#24418;&#24335;&#12290;&#19982;ML&#20013;&#30340;&#35768;&#22810;&#29616;&#26377;&#25351;&#26631;&#19981;&#21516;&#65292;Vendi&#20998;&#25968;&#19981;&#38656;&#35201;&#21442;&#32771;&#25968;&#25454;&#38598;&#25110;&#26679;&#26412;&#25110;&#26631;&#31614;&#30340;&#20998;&#24067;&#65292;&#22240;&#27492;&#23427;&#36890;&#29992;&#19988;&#36866;&#29992;&#20110;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#12289;&#35299;&#30721;&#31639;&#27861;&#21644;&#26469;&#33258;&#20219;&#20309;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diversity is an important criterion for many areas of machine learning (ML), including generative modeling and dataset curation. However, existing metrics for measuring diversity are often domain-specific and limited in flexibility. In this paper, we address the diversity evaluation problem by proposing the Vendi Score, which connects and extends ideas from ecology and quantum statistical mechanics to ML. The Vendi Score is defined as the exponential of the Shannon entropy of the eigenvalues of a similarity matrix. This matrix is induced by a user-defined similarity function applied to the sample to be evaluated for diversity. In taking a similarity function as input, the Vendi Score enables its user to specify any desired form of diversity. Importantly, unlike many existing metrics in ML, the Vendi Score does not require a reference dataset or distribution over samples or labels, it is therefore general and applicable to any generative model, decoding algorithm, and dataset from any d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#34892;&#20026;&#30340;&#20998;&#31867;&#12290;&#36890;&#36807;&#20351;&#29992;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#65288;STL&#65289;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#22270;&#30340;&#25628;&#32034;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#26102;&#38388;&#20989;&#25968;&#21644;&#31232;&#30095;softmax&#20989;&#25968;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#19968;&#20010;&#32039;&#20945;&#30340;STL&#20844;&#24335;&#65292;&#23454;&#29616;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20998;&#31867;&#12290;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#22312;&#39550;&#39542;&#22330;&#26223;&#21644;&#28023;&#20891;&#30417;&#35270;&#26696;&#20363;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#12289;&#32039;&#20945;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.01910</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#36827;&#34892;&#21487;&#35299;&#37322;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Learning Signal Temporal Logic through Neural Network for Interpretable Classification. (arXiv:2210.01910v2 [cs.FL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#34892;&#20026;&#30340;&#20998;&#31867;&#12290;&#36890;&#36807;&#20351;&#29992;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#65288;STL&#65289;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#22270;&#30340;&#25628;&#32034;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#26102;&#38388;&#20989;&#25968;&#21644;&#31232;&#30095;softmax&#20989;&#25968;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#19968;&#20010;&#32039;&#20945;&#30340;STL&#20844;&#24335;&#65292;&#23454;&#29616;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20998;&#31867;&#12290;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#22312;&#39550;&#39542;&#22330;&#26223;&#21644;&#28023;&#20891;&#30417;&#35270;&#26696;&#20363;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#12289;&#32039;&#20945;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20135;&#29983;&#30340;&#27169;&#22411;&#24456;&#38590;&#36827;&#34892;&#39564;&#35777;&#21644;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#34892;&#20026;&#30340;&#20998;&#31867;&#12290;&#25105;&#20204;&#29305;&#21035;&#20351;&#29992;&#19968;&#31181;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#24418;&#24335;&#35821;&#35328;&#65292;&#21363;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#65288;Signal Temporal Logic&#65292;STL&#65289;&#65292;&#26469;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#22270;&#30340;&#25628;&#32034;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#20989;&#25968;&#21644;&#31232;&#30095;softmax&#20989;&#25968;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;-STL&#26694;&#26550;&#30340;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#29616;&#25104;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#24037;&#20855;&#39640;&#25928;&#22320;&#23398;&#20064;&#19968;&#20010;&#32039;&#20945;&#30340;STL&#20844;&#24335;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20998;&#31867;&#12290;&#36890;&#36807;&#39550;&#39542;&#22330;&#26223;&#21644;&#28023;&#20891;&#30417;&#35270;&#26696;&#20363;&#30740;&#31350;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#12289;&#32039;&#20945;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning techniques using neural networks have achieved promising success for time-series data classification. However, the models that they produce are challenging to verify and interpret. In this paper, we propose an explainable neural-symbolic framework for the classification of time-series behaviors. In particular, we use an expressive formal language, namely Signal Temporal Logic (STL), to constrain the search of the computation graph for a neural network. We design a novel time function and sparse softmax function to improve the soundness and precision of the neural-STL framework. As a result, we can efficiently learn a compact STL formula for the classification of time-series data through off-the-shelf gradient-based tools. We demonstrate the computational efficiency, compactness, and interpretability of the proposed method through driving scenarios and naval surveillance case studies, compared with state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#22402;&#30452;&#21322;&#32852;&#21512;&#23398;&#20064;&#20026;&#22312;&#32447;&#24191;&#21578;&#39046;&#22495;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#32852;&#21512;&#24863;&#30693;&#30340;&#23616;&#37096;&#27169;&#22411;&#20197;&#24212;&#23545;&#20256;&#32479;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2209.15635</link><description>&lt;p&gt;
&#22402;&#30452;&#21322;&#32852;&#21512;&#23398;&#20064;&#29992;&#20110;&#39640;&#25928;&#22312;&#32447;&#24191;&#21578;
&lt;/p&gt;
&lt;p&gt;
Vertical Semi-Federated Learning for Efficient Online Advertising. (arXiv:2209.15635v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15635
&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#21322;&#32852;&#21512;&#23398;&#20064;&#20026;&#22312;&#32447;&#24191;&#21578;&#39046;&#22495;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#32852;&#21512;&#24863;&#30693;&#30340;&#23616;&#37096;&#27169;&#22411;&#20197;&#24212;&#23545;&#20256;&#32479;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#26550;&#26500;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;1&#65289;&#36866;&#29992;&#33539;&#22260;&#21463;&#38480;&#20110;&#37325;&#21472;&#26679;&#26412;&#65307;2&#65289;&#23454;&#26102;&#32852;&#21512;&#26381;&#21153;&#30340;&#31995;&#32479;&#25361;&#25112;&#36739;&#39640;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#22312;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#35774;&#32622;&#8212;&#8212;&#21322;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;(Semi-VFL)&#65292;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#21322;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#26088;&#22312;&#23454;&#29616;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#30340;&#23454;&#38469;&#24037;&#19994;&#24212;&#29992;&#26041;&#24335;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#32852;&#21512;&#24863;&#30693;&#30340;&#23616;&#37096;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#21333;&#26041;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23616;&#37096;&#26381;&#21153;&#30340;&#20415;&#21033;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31934;&#24515;&#35774;&#35745;&#30340;&#32852;&#21512;&#29305;&#26435;&#23398;&#20064;&#26694;&#26550;(JPL)&#65292;&#26469;&#35299;&#20915;&#34987;&#21160;&#26041;&#29305;&#24449;&#32570;&#22833;&#21644;&#36866;&#24212;&#25972;&#20010;&#26679;&#26412;&#31354;&#38388;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25512;&#29702;&#39640;&#25928;&#30340;&#36866;&#29992;&#20110;&#25972;&#20010;&#26679;&#26412;&#31354;&#38388;&#30340;&#21333;&#26041;&#23398;&#29983;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#32852;&#21512;&#29305;&#24449;&#25193;&#23637;&#30340;&#20248;&#21183;&#12290;&#26032;&#30340;&#34920;&#31034;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
The traditional vertical federated learning schema suffers from two main issues: 1) restricted applicable scope to overlapped samples and 2) high system challenge of real-time federated serving, which limits its application to advertising systems. To this end, we advocate a new learning setting Semi-VFL (Vertical Semi-Federated Learning) to tackle these challenge. Semi-VFL is proposed to achieve a practical industry application fashion for VFL, by learning a federation-aware local model which performs better than single-party models and meanwhile maintain the convenience of local-serving. For this purpose, we propose the carefully designed Joint Privileged Learning framework (JPL) to i) alleviate the absence of the passive party's feature and ii) adapt to the whole sample space. Specifically, we build an inference-efficient single-party student model applicable to the whole sample space and meanwhile maintain the advantage of the federated feature extension. New representation distilla
&lt;/p&gt;</description></item><item><title>FAIR-FATE&#26159;&#19968;&#31181;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20844;&#24179;&#24863;&#30693;&#30340;&#32858;&#21512;&#26041;&#27861;&#23454;&#29616;&#32452;&#20844;&#24179;&#24615;&#24182;&#20445;&#25345;&#39640;&#25928;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.13678</link><description>&lt;p&gt;
FAIR-FATE: &#20855;&#26377;&#21160;&#37327;&#30340;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FAIR-FATE: Fair Federated Learning with Momentum. (arXiv:2209.13678v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13678
&lt;/p&gt;
&lt;p&gt;
FAIR-FATE&#26159;&#19968;&#31181;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20844;&#24179;&#24863;&#30693;&#30340;&#32858;&#21512;&#26041;&#27861;&#23454;&#29616;&#32452;&#20844;&#24179;&#24615;&#24182;&#20445;&#25345;&#39640;&#25928;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20844;&#24179;&#24863;&#30693;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#37325;&#28857;&#19968;&#30452;&#26159;&#38598;&#20013;&#24335;&#26426;&#22120;&#23398;&#20064;&#65292;&#23545;&#20998;&#25955;&#24335;&#26041;&#27861;&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#25955;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#24418;&#24335;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#65292;&#26381;&#21153;&#22120;&#27719;&#24635;&#23427;&#20204;&#20197;&#33719;&#24471;&#20849;&#20139;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#26159;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#20010;&#24120;&#35265;&#29305;&#24449;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#25110;&#21152;&#21095;&#30001;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#31181;&#26063;&#25110;&#24615;&#21035;&#65289;&#23450;&#20041;&#30340;&#29305;&#26435;&#32452;&#30340;&#27495;&#35270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FAIR-FATE&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#30340;&#32858;&#21512;&#26041;&#27861;&#35745;&#31639;&#20840;&#23616;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#32452;&#20844;&#24179;&#24615;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#29992;&#24615;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20840;&#23616;&#27169;&#22411;&#26356;&#26032;&#37319;&#29992;&#19968;&#20010;&#21160;&#37327;&#39033;&#26469;&#20272;&#35745;&#20844;&#24179;&#27169;&#22411;&#26356;&#26032;&#65292;&#24110;&#21161;&#20811;&#26381;&#38750;&#20844;&#24179;&#26799;&#24230;&#30340;&#38663;&#33633;&#12290;
&lt;/p&gt;
&lt;p&gt;
While fairness-aware machine learning algorithms have been receiving increasing attention, the focus has been on centralized machine learning, leaving decentralized methods underexplored. Federated Learning is a decentralized form of machine learning where clients train local models with a server aggregating them to obtain a shared global model. Data heterogeneity amongst clients is a common characteristic of Federated Learning, which may induce or exacerbate discrimination of unprivileged groups defined by sensitive attributes such as race or gender. In this work we propose FAIR-FATE: a novel FAIR FederATEd Learning algorithm that aims to achieve group fairness while maintaining high utility via a fairness-aware aggregation method that computes the global model by taking into account the fairness of the clients. To achieve that, the global model update is computed by estimating a fair model update using a Momentum term that helps to overcome the oscillations of non-fair gradients. To 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#35774;&#35745;&#65292;&#20854;&#20013;&#31639;&#27861;&#19982;&#20154;&#31867;&#29992;&#25143;&#20197;&#21452;&#21521;&#20114;&#21160;&#30340;&#26041;&#24335;&#20132;&#20114;&#65292;&#20165;&#22312;&#23545;&#29992;&#25143;&#30340;&#20915;&#31574;&#26377;&#30410;&#26102;&#25552;&#20379;&#24314;&#35758;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#20154;&#31867;&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#24182;&#22312;&#20419;&#36827;&#20154;&#31867;&#23398;&#20064;&#12289;&#20445;&#30041;&#20154;&#31867;&#20915;&#31574;&#30340;&#34917;&#20805;&#20248;&#21183;&#26041;&#38754;&#20855;&#26377;&#39069;&#22806;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2209.13578</link><description>&lt;p&gt;
&#23398;&#20064;&#20160;&#20040;&#26102;&#20505;&#20026;&#20154;&#31867;&#20915;&#31574;&#32773;&#25552;&#20379;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Learning When to Advise Human Decision Makers. (arXiv:2209.13578v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#35774;&#35745;&#65292;&#20854;&#20013;&#31639;&#27861;&#19982;&#20154;&#31867;&#29992;&#25143;&#20197;&#21452;&#21521;&#20114;&#21160;&#30340;&#26041;&#24335;&#20132;&#20114;&#65292;&#20165;&#22312;&#23545;&#29992;&#25143;&#30340;&#20915;&#31574;&#26377;&#30410;&#26102;&#25552;&#20379;&#24314;&#35758;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#20154;&#31867;&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#24182;&#22312;&#20419;&#36827;&#20154;&#31867;&#23398;&#20064;&#12289;&#20445;&#30041;&#20154;&#31867;&#20915;&#31574;&#30340;&#34917;&#20805;&#20248;&#21183;&#26041;&#38754;&#20855;&#26377;&#39069;&#22806;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#25552;&#20379;&#24314;&#35758;&#65292;&#20197;&#20419;&#36827;&#22312;&#21307;&#30103;&#12289;&#21009;&#20107;&#21496;&#27861;&#21644;&#37329;&#34701;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#20154;&#31867;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#26032;&#39062;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#35774;&#35745;&#65292;&#20854;&#20013;&#31639;&#27861;&#19982;&#20154;&#31867;&#29992;&#25143;&#20197;&#21452;&#21521;&#20114;&#21160;&#30340;&#26041;&#24335;&#20132;&#20114;&#65292;&#26088;&#22312;&#20165;&#22312;&#23545;&#29992;&#25143;&#30340;&#20915;&#31574;&#26377;&#30410;&#26102;&#25552;&#20379;&#24314;&#35758;&#12290;&#19968;&#39033;&#22823;&#35268;&#27169;&#23454;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22266;&#23450;&#30340;&#38750;&#20132;&#20114;&#24335;&#24314;&#35758;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24314;&#35758;&#26041;&#27861;&#33021;&#22815;&#22312;&#38656;&#35201;&#30340;&#26102;&#20505;&#25552;&#20379;&#24314;&#35758;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20154;&#31867;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20419;&#36827;&#20154;&#31867;&#23398;&#20064;&#12289;&#20445;&#30041;&#20154;&#31867;&#20915;&#31574;&#30340;&#34917;&#20805;&#20248;&#21183;&#26041;&#38754;&#36824;&#20855;&#26377;&#39069;&#22806;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) systems are increasingly used for providing advice to facilitate human decision making in a wide range of domains, such as healthcare, criminal justice, and finance. Motivated by limitations of the current practice where algorithmic advice is provided to human users as a constant element in the decision-making pipeline, in this paper we raise the question of when should algorithms provide advice? We propose a novel design of AI systems in which the algorithm interacts with the human user in a two-sided manner and aims to provide advice only when it is likely to be beneficial for the user in making their decision. The results of a large-scale experiment show that our advising approach manages to provide advice at times of need and to significantly improve human decision making compared to fixed, non-interactive, advising approaches. This approach has additional advantages in facilitating human learning, preserving complementary strengths of human decision ma
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26469;&#20943;&#23569;JPEG&#21387;&#32553;&#20013;&#30001;&#31163;&#25955;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;DFT&#65289;&#24341;&#36215;&#30340;&#26500;&#36896;&#24615;&#25439;&#22833;&#12290;&#36890;&#36807;&#22312;&#29305;&#24449;&#25552;&#21462;&#20013;&#20351;&#29992;&#21367;&#31215;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#23569;&#20887;&#20313;&#30340;&#29305;&#24449;&#22270;&#21644;&#26356;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#20687;&#21387;&#32553;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.03475</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20943;&#23569;JPEG&#21387;&#32553;&#20013;&#30001;&#31163;&#25955;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;DFT&#65289;&#24341;&#36215;&#30340;&#26500;&#36896;&#24615;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Network (CNN) to reduce construction loss in JPEG compression caused by Discrete Fourier Transform (DFT). (arXiv:2209.03475v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03475
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26469;&#20943;&#23569;JPEG&#21387;&#32553;&#20013;&#30001;&#31163;&#25955;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;DFT&#65289;&#24341;&#36215;&#30340;&#26500;&#36896;&#24615;&#25439;&#22833;&#12290;&#36890;&#36807;&#22312;&#29305;&#24449;&#25552;&#21462;&#20013;&#20351;&#29992;&#21367;&#31215;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#23569;&#20887;&#20313;&#30340;&#29305;&#24449;&#22270;&#21644;&#26356;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#20687;&#21387;&#32553;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#25968;&#23383;&#22270;&#20687;&#22788;&#29702;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#21457;&#23637;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#25968;&#25454;&#21387;&#32553;&#31574;&#30053;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#34920;&#31034;&#22270;&#20687;&#25152;&#38656;&#30340;&#20449;&#24687;&#37327;&#12290;&#20854;&#20013;&#65292;JPEG&#21387;&#32553;&#26159;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#22312;&#22810;&#23186;&#20307;&#21644;&#25968;&#23383;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;DFT&#30340;&#21608;&#26399;&#24615;&#29305;&#24615;&#20351;&#24471;&#24456;&#38590;&#28385;&#36275;&#22270;&#20687;&#30456;&#23545;&#36793;&#30028;&#30340;&#21608;&#26399;&#26465;&#20214;&#65292;&#20174;&#32780;&#20135;&#29983;&#20005;&#37325;&#30340;&#20266;&#24433;&#65292;&#38477;&#20302;&#20102;&#22270;&#20687;&#30340;&#24863;&#30693;&#35270;&#35273;&#36136;&#37327;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#28145;&#24230;&#23398;&#20064;&#26368;&#36817;&#22312;&#35821;&#38899;&#35782;&#21035;&#12289;&#22270;&#20687;&#38477;&#22122;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#24212;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#32467;&#26524;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27604;&#22823;&#22810;&#25968;&#20854;&#20182;&#31867;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26356;&#21463;&#20851;&#27880;&#12290;&#22312;&#29305;&#24449;&#25552;&#21462;&#20013;&#20351;&#29992;&#21367;&#31215;&#21487;&#20197;&#24471;&#21040;&#26356;&#23569;&#20887;&#20313;&#30340;&#29305;&#24449;&#22270;&#21644;&#26356;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#23545;&#22270;&#20687;&#21387;&#32553;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent decades, digital image processing has gained enormous popularity. Consequently, a number of data compression strategies have been put forth, with the goal of minimizing the amount of information required to represent images. Among them, JPEG compression is one of the most popular methods that has been widely applied in multimedia and digital applications. The periodic nature of DFT makes it impossible to meet the periodic condition of an image's opposing edges without producing severe artifacts, which lowers the image's perceptual visual quality. On the other hand, deep learning has recently achieved outstanding results for applications like speech recognition, image reduction, and natural language processing. Convolutional Neural Networks (CNN) have received more attention than most other types of deep neural networks. The use of convolution in feature extraction results in a less redundant feature map and a smaller dataset, both of which are crucial for image compression. I
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#28385;&#36275;&#25237;&#25918;&#25104;&#26412;&#22238;&#25253;&#29575;&#38480;&#21046;&#30340;&#24191;&#21578;&#21830;&#30340;&#22312;&#32447;&#31454;&#20215;&#31639;&#27861;&#65292;&#36890;&#36807;&#31616;&#20415;&#30340;&#22312;&#32447;&#31639;&#27861;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#36951;&#25022;&#20540;&#65292;&#24182;&#19988;&#24635;&#32467;&#20102;&#19982;&#20808;&#21069;&#24037;&#20316;&#30340;&#38598;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.13713</link><description>&lt;p&gt;
&#38754;&#21521;&#25910;&#30410;&#38480;&#21046;&#24191;&#21578;&#21830;&#30340;&#22312;&#32447;&#31454;&#20215;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Bidding Algorithms for Return-on-Spend Constrained Advertisers. (arXiv:2208.13713v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#28385;&#36275;&#25237;&#25918;&#25104;&#26412;&#22238;&#25253;&#29575;&#38480;&#21046;&#30340;&#24191;&#21578;&#21830;&#30340;&#22312;&#32447;&#31454;&#20215;&#31639;&#27861;&#65292;&#36890;&#36807;&#31616;&#20415;&#30340;&#22312;&#32447;&#31639;&#27861;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#36951;&#25022;&#20540;&#65292;&#24182;&#19988;&#24635;&#32467;&#20102;&#19982;&#20808;&#21069;&#24037;&#20316;&#30340;&#38598;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24191;&#21578;&#19994;&#36817;&#24180;&#26469;&#25104;&#38271;&#20026;&#19968;&#20010;&#31454;&#20105;&#28608;&#28872;&#19988;&#22797;&#26434;&#30340;&#25968;&#21313;&#20159;&#32654;&#20803;&#34892;&#19994;&#65292;&#24191;&#21578;&#21830;&#22312;&#22823;&#35268;&#27169;&#21644;&#39640;&#39057;&#29575;&#19979;&#36827;&#34892;&#24191;&#21578;&#20301;&#31454;&#20215;&#12290;&#36825;&#23548;&#33268;&#38656;&#27714;&#22686;&#21152;&#20102;&#23545;&#20110;&#39640;&#25928;&#30340;"&#33258;&#21160;&#31454;&#20215;"&#31639;&#27861;&#65292;&#20197;&#30830;&#23450;&#26368;&#22823;&#21270;&#24191;&#21578;&#21830;&#30446;&#26631;&#30340;&#25237;&#26631;&#20215;&#26684;&#65292;&#21516;&#26102;&#28385;&#36275;&#29305;&#23450;&#30340;&#38480;&#21046;&#26465;&#20214;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21333;&#20010;&#26368;&#22823;&#21270;&#20215;&#20540;&#24191;&#21578;&#21830;&#38754;&#20020;&#30340;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#38480;&#21046;&#26465;&#20214;&#20043;&#19968;&#65306;&#25237;&#25918;&#25104;&#26412;&#22238;&#25253;&#29575;&#65288;RoS&#65289;&#12290;&#25105;&#20204;&#20197;&#30456;&#23545;&#20110;&#30693;&#36947;&#25152;&#26377;&#26597;&#35810;&#30340;&#26368;&#20248;&#31639;&#27861;&#30340;&#36951;&#25022;&#20540;&#20026;&#34913;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#24403;&#26597;&#35810;&#24207;&#21015;&#26159;&#26469;&#33258;&#26576;&#20010;&#20998;&#24067;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#26679;&#26412;&#26102;&#65292;&#35813;&#31639;&#27861;&#22312;&#26399;&#26395;&#20540;&#19978;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#36951;&#25022;&#20540;&#65292;&#21516;&#26102;&#22987;&#32456;&#36981;&#23432;&#25351;&#23450;&#30340;RoS&#32422;&#26463;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;Balseiro&#12289;Lu&#21644;Mirrokni [BLM20]&#30340;&#20808;&#21069;&#24037;&#20316;&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#23562;&#37325;&#32422;&#26463;&#30340;&#21516;&#26102;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#36951;&#25022;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online advertising has recently grown into a highly competitive and complex multi-billion-dollar industry, with advertisers bidding for ad slots at large scales and high frequencies. This has resulted in a growing need for efficient "auto-bidding" algorithms that determine the bids for incoming queries to maximize advertisers' targets subject to their specified constraints. This work explores efficient online algorithms for a single value-maximizing advertiser under an increasingly popular constraint: Return-on-Spend (RoS). We quantify efficiency in terms of regret relative to the optimal algorithm, which knows all queries a priori.  We contribute a simple online algorithm that achieves near-optimal regret in expectation while always respecting the specified RoS constraint when the input sequence of queries are i.i.d. samples from some distribution. We also integrate our results with the previous work of Balseiro, Lu, and Mirrokni [BLM20] to achieve near-optimal regret while respecting
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36229;&#22270;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#31038;&#21306;&#21457;&#29616;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#26512;&#30456;&#20284;&#24615;&#30697;&#38453;&#21644;&#35774;&#35745;&#35889;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#23494;&#24230;&#26465;&#20214;&#19979;&#30340;&#31934;&#30830;&#24674;&#22797;&#21644;&#39640;&#25928;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2208.12227</link><description>&lt;p&gt;
&#36229;&#22270;SBM&#20013;&#30340;&#31038;&#21306;&#21457;&#29616;&#65306;&#32473;&#23450;&#30456;&#20284;&#24615;&#30697;&#38453;&#30340;&#26368;&#20248;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Community Detection in the Hypergraph SBM: Optimal Recovery Given the Similarity Matrix. (arXiv:2208.12227v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36229;&#22270;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#31038;&#21306;&#21457;&#29616;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#26512;&#30456;&#20284;&#24615;&#30697;&#38453;&#21644;&#35774;&#35745;&#35889;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#23494;&#24230;&#26465;&#20214;&#19979;&#30340;&#31934;&#30830;&#24674;&#22797;&#21644;&#39640;&#25928;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#21457;&#29616;&#26159;&#32593;&#32476;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#30784;&#38382;&#39064;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#36229;&#22270;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;HSBM&#65289;&#20013;&#30340;&#31038;&#21306;&#21457;&#29616;&#38382;&#39064;&#65292;&#37325;&#28857;&#26159;&#30830;&#20999;&#30340;&#31038;&#21306;&#24674;&#22797;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#30456;&#20284;&#24615;&#30697;&#38453;W&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;W_{ij}&#34920;&#31034;&#21516;&#26102;&#21253;&#21547;i&#21644;j&#30340;&#36229;&#36793;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#20010;&#20449;&#24687;&#27169;&#22411;&#19979;&#65292;Kim&#65292;Bandeira&#21644;Goemans&#30830;&#23450;&#20102;&#22312;&#23545;&#25968;&#24230;&#25968;&#21306;&#38388;&#20869;&#30340;&#31934;&#30830;&#24674;&#22797;&#30340;&#20449;&#24687;&#35770;&#38408;&#20540;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#23450;&#35268;&#21010;&#26494;&#24347;&#65292;&#35748;&#20026;&#36825;&#26159;&#26368;&#20248;&#30340;&#29468;&#24819;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#36825;&#20010;&#29468;&#24819;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#35889;&#31639;&#27861;&#65292;&#20960;&#20046;&#20855;&#26377;&#32447;&#24615;&#36816;&#34892;&#26102;&#38388;&#65292;&#24182;&#19988;&#35777;&#26126;&#23427;&#36798;&#21040;&#20102;&#20449;&#24687;&#35770;&#38408;&#20540;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#35889;&#31639;&#27861;&#36824;&#33021;&#22312;&#26356;&#23494;&#38598;&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#65292;&#24182;&#19988;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#26356;&#39640;&#25928;&#65292;&#22240;&#27492;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community detection is a fundamental problem in network science. In this paper, we consider community detection in hypergraphs drawn from the $hypergraph$ $stochastic$ $block$ $model$ (HSBM), with a focus on exact community recovery. We study the performance of polynomial-time algorithms which operate on the $similarity$ $matrix$ $W$, where $W_{ij}$ reports the number of hyperedges containing both $i$ and $j$. Under this information model, Kim, Bandeira, and Goemans determined the information-theoretic threshold for exact recovery in the logarithmic degree regime, and proposed a semidefinite programming relaxation which they conjectured to be optimal. In this paper, we confirm this conjecture. We also design a simple and highly efficient spectral algorithm with nearly linear runtime and show that it achieves the information-theoretic threshold. Moreover, the spectral algorithm also succeeds in denser regimes and is considerably more efficient than previous approaches, establishing it a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22635;&#34917;&#36873;&#25321;&#23545;&#19981;&#21516;&#32676;&#20307;&#30340;&#37325;&#24314;&#35823;&#24046;&#21644;&#19979;&#28216;&#39044;&#27979;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2208.06648</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#23384;&#22312;&#19979;&#30340;&#22635;&#34917;&#31574;&#30053;&#65306;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Imputation Strategies Under Clinical Presence: Impact on Algorithmic Fairness. (arXiv:2208.06648v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22635;&#34917;&#36873;&#25321;&#23545;&#19981;&#21516;&#32676;&#20307;&#30340;&#37325;&#24314;&#35823;&#24046;&#21644;&#19979;&#28216;&#39044;&#27979;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21487;&#33021;&#20250;&#24378;&#21270;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#65292;&#32780;&#25105;&#20204;&#22312;&#36825;&#20010;&#24037;&#20316;&#20013;&#25552;&#20986;&#65292;&#25968;&#25454;&#20013;&#32570;&#22833;&#30340;&#20869;&#23481;&#20063;&#20250;&#20135;&#29983;&#20559;&#35265;&#12290;&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#20559;&#35265;&#24050;&#32463;&#22312;&#21307;&#30103;&#21382;&#21490;&#19978;&#30041;&#19979;&#20102;&#28145;&#28145;&#30340;&#28889;&#21360;&#65292;&#23548;&#33268;&#36793;&#32536;&#21270;&#32676;&#20307;&#21463;&#21040;&#19981;&#24179;&#31561;&#30340;&#25252;&#29702;&#12290;&#32570;&#22833;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#36890;&#24120;&#21453;&#26144;&#20102;&#36825;&#20123;&#32676;&#20307;&#30340;&#24046;&#24322;&#65292;&#20294;&#26159;&#29305;&#23450;&#32676;&#20307;&#32570;&#22833;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#24433;&#21709;&#36824;&#19981;&#22826;&#28165;&#26970;&#12290;&#23613;&#31649;&#20854;&#28508;&#22312;&#24433;&#21709;&#24040;&#22823;&#65292;&#20294;&#22635;&#34917;&#24448;&#24448;&#34987;&#24573;&#35270;&#20026;&#19968;&#20010;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#32780;&#20851;&#27880;&#28857;&#25918;&#22312;&#20102;&#37325;&#24314;&#35823;&#24046;&#30340;&#20943;&#23569;&#21644;&#25972;&#20307;&#24615;&#33021;&#19978;&#65292;&#24573;&#30053;&#20102;&#22635;&#34917;&#22914;&#20309;&#23545;&#19981;&#21516;&#32676;&#20307;&#20135;&#29983;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#22635;&#34917;&#36873;&#25321;&#23545;&#19981;&#21516;&#32676;&#20307;&#30340;&#37325;&#24314;&#35823;&#24046;&#21644;&#19979;&#28216;&#39044;&#27979;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning risks reinforcing biases present in data, and, as we argue in this work, in what is absent from data. In healthcare, biases have marked medical history, leading to unequal care affecting marginalised groups. Patterns in missing data often reflect these group discrepancies, but the algorithmic fairness implications of group-specific missingness are not well understood. Despite its potential impact, imputation is often an overlooked preprocessing step, with attention placed on the reduction of reconstruction error and overall performance, ignoring how imputation can affect groups differently. Our work studies how imputation choices affect reconstruction errors across groups and algorithmic fairness properties of downstream predictions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#22312;R&#20013;&#20351;&#29992;theft&#21253;&#36827;&#34892;&#22522;&#20110;&#29305;&#24449;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#21253;&#25324;&#32570;&#20047;&#32479;&#19968;&#30340;&#35775;&#38382;&#28857;&#20197;&#21450;&#29992;&#25143;&#38656;&#35201;&#25484;&#25569;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#26469;&#33719;&#24471;&#25152;&#26377;&#29305;&#24449;&#38598;&#12290;</title><link>http://arxiv.org/abs/2208.06146</link><description>&lt;p&gt;
&#22312;R&#20013;&#20351;&#29992;theft&#21253;&#36827;&#34892;&#22522;&#20110;&#29305;&#24449;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Feature-Based Time-Series Analysis in R using the theft Package. (arXiv:2208.06146v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#22312;R&#20013;&#20351;&#29992;theft&#21253;&#36827;&#34892;&#22522;&#20110;&#29305;&#24449;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#21253;&#25324;&#32570;&#20047;&#32479;&#19968;&#30340;&#35775;&#38382;&#28857;&#20197;&#21450;&#29992;&#25143;&#38656;&#35201;&#25484;&#25569;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#26469;&#33719;&#24471;&#25152;&#26377;&#29305;&#24449;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#20013;&#34987;&#27979;&#37327;&#21644;&#20998;&#26512;&#12290;&#19968;&#31181;&#37327;&#21270;&#26102;&#38388;&#24207;&#21015;&#32467;&#26500;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#35745;&#31639;&#19968;&#32452;&#25688;&#35201;&#32479;&#35745;&#37327;&#25110;"&#29305;&#24449;"&#65292;&#28982;&#21518;&#29992;&#29305;&#24449;&#21521;&#37327;&#30340;&#23646;&#24615;&#26469;&#34920;&#31034;&#26102;&#38388;&#24207;&#21015;&#12290;&#32467;&#26524;&#24471;&#21040;&#30340;&#29305;&#24449;&#31354;&#38388;&#26159;&#21487;&#35299;&#37322;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#65292;&#20351;&#24471;&#20256;&#32479;&#30340;&#32479;&#35745;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#32858;&#31867;&#12289;&#22238;&#24402;&#21644;&#20998;&#31867;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#12290;&#23384;&#22312;&#35768;&#22810;&#24320;&#28304;&#36719;&#20214;&#21253;&#22312;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#20013;&#35745;&#31639;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#38598;&#65292;&#21253;&#25324;catch22&#65288;22&#20010;&#29305;&#24449;&#65306;Matlab&#12289;R&#12289;Python&#12289;Julia&#65289;&#12289;feasts&#65288;42&#20010;&#29305;&#24449;&#65306;R&#65289;&#12289;tsfeatures&#65288;63&#20010;&#29305;&#24449;&#65306;R&#65289;&#12289;Kats&#65288;40&#20010;&#29305;&#24449;&#65306;Python&#65289;&#12289;tsfresh&#65288;779&#20010;&#29305;&#24449;&#65306;Python&#65289;&#21644;TSFEL&#65288;390&#20010;&#29305;&#24449;&#65306;Python&#65289;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#20960;&#20010;&#38382;&#39064;&#65306;&#65288;i&#65289;&#30446;&#21069;&#23578;&#26080;&#36825;&#20123;&#36719;&#20214;&#21253;&#30340;&#21333;&#19968;&#35775;&#38382;&#28857;&#65307;&#65288;ii&#65289;&#35201;&#35775;&#38382;&#25152;&#26377;&#29305;&#24449;&#38598;&#65292;&#29992;&#25143;&#24517;&#39035;&#31934;&#36890;&#22810;&#31181;&#35821;&#35328;&#65307;&#65288;iii&#65289;th
&lt;/p&gt;
&lt;p&gt;
Time series are measured and analyzed across the sciences. One method of quantifying the structure of time series is by calculating a set of summary statistics or `features', and then representing a time series in terms of its properties as a feature vector. The resulting feature space is interpretable and informative, and enables conventional statistical learning approaches, including clustering, regression, and classification, to be applied to time-series datasets. Many open-source software packages for computing sets of time-series features exist across multiple programming languages, including catch22 (22 features: Matlab, R, Python, Julia), feasts (42 features: R), tsfeatures (63 features: R), Kats (40 features: Python), tsfresh (779 features: Python), and TSFEL (390 features: Python). However, there are several issues: (i) a singular access point to these packages is not currently available; (ii) to access all feature sets, users must be fluent in multiple languages; and (iii) th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#22411;&#30340;&#38750;&#20405;&#20837;&#24335;&#26041;&#27861;&#65292;&#20351;&#29992;&#21387;&#21147;&#20256;&#24863;&#22120;&#23545;&#23156;&#20799;&#30340;&#19968;&#33324;&#36816;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#26088;&#22312;&#23454;&#29616;&#26089;&#26399;&#31070;&#32463;&#32908;&#32905;&#38556;&#30861;&#65288;&#22914;&#33041;&#30251;&#65289;&#30340;&#23458;&#35266;&#26816;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21387;&#21147;&#25968;&#25454;&#21487;&#20197;&#21306;&#20998;&#23156;&#20799;&#30340;&#20856;&#22411;&#36816;&#21160;&#27169;&#24335;&#65292;&#21363;&#8220;&#22352;&#31435;&#19981;&#23433;&#26399;&#8221;&#21644;&#8220;&#22352;&#31435;&#19981;&#23433;&#21069;&#26399;&#8221;&#12290;</title><link>http://arxiv.org/abs/2208.00884</link><description>&lt;p&gt;
&#22522;&#20110;&#21387;&#21147;&#20998;&#24067;&#20998;&#26512;&#30340;&#23156;&#20799;&#36816;&#21160;&#20998;&#31867;&#8212;&#8212;&#30740;&#31350;&#21644;&#20020;&#24202;&#24212;&#29992;&#30340;&#38468;&#21152;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Infant movement classification through pressure distribution analysis -- added value for research and clinical implementation. (arXiv:2208.00884v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#22411;&#30340;&#38750;&#20405;&#20837;&#24335;&#26041;&#27861;&#65292;&#20351;&#29992;&#21387;&#21147;&#20256;&#24863;&#22120;&#23545;&#23156;&#20799;&#30340;&#19968;&#33324;&#36816;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#26088;&#22312;&#23454;&#29616;&#26089;&#26399;&#31070;&#32463;&#32908;&#32905;&#38556;&#30861;&#65288;&#22914;&#33041;&#30251;&#65289;&#30340;&#23458;&#35266;&#26816;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21387;&#21147;&#25968;&#25454;&#21487;&#20197;&#21306;&#20998;&#23156;&#20799;&#30340;&#20856;&#22411;&#36816;&#21160;&#27169;&#24335;&#65292;&#21363;&#8220;&#22352;&#31435;&#19981;&#23433;&#26399;&#8221;&#21644;&#8220;&#22352;&#31435;&#19981;&#23433;&#21069;&#26399;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#21387;&#21147;&#20256;&#24863;&#35774;&#22791;&#26469;&#23545;&#23156;&#20799;&#30340;&#19968;&#33324;&#36816;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#23454;&#29616;&#26089;&#26399;&#30340;&#31070;&#32463;&#32908;&#32905;&#38556;&#30861;&#65288;&#22914;&#33041;&#30251;&#65289;&#30340;&#23458;&#35266;&#26816;&#27979;&#12290;&#26412;&#25991;&#27979;&#35797;&#20102;&#20351;&#29992;&#21387;&#21147;&#25968;&#25454;&#26469;&#21306;&#20998;&#8220;&#22352;&#31435;&#19981;&#23433;&#26399;&#8221;&#65288;&#21363;&#22352;&#31435;&#19981;&#23433;&#36816;&#21160;&#65289;&#19982;&#8220;&#22352;&#31435;&#19981;&#23433;&#21069;&#26399;&#8221;&#65288;&#21363;&#25197;&#21160;&#36816;&#21160;&#65289;&#30340;&#20856;&#22411;&#36816;&#21160;&#27169;&#24335;&#30340;&#21487;&#34892;&#24615;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#27599;&#20010;&#23156;&#20799;&#22312;&#20986;&#29983;&#21518; 4-16 &#21608;&#30340;&#38388;&#38548;&#26399;&#20869;&#36830;&#32493;&#19971;&#20010;&#23454;&#39564;&#23460;&#20250;&#35805;&#30340;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#21253;&#25324;&#26469;&#33258;&#19968;&#20010; 32x32 &#32593;&#26684;&#21387;&#21147;&#20256;&#24863;&#22443;&#21450;&#20854; 1024 &#20010;&#20256;&#24863;&#22120;&#30340;&#21387;&#21147;&#25968;&#25454;&#12290;&#20026;&#20102;&#39564;&#35777;&#27010;&#24565;&#65292;&#20174;&#20004;&#20010;&#30446;&#26631;&#24180;&#40836;&#27573;&#20013;&#65292;&#27599;&#20010;&#25345;&#32493; 5 &#31186;&#30340; 1776 &#20010;&#21387;&#21147;&#25968;&#25454;&#29255;&#27573;&#34987;&#29992;&#20110;&#36816;&#21160;&#20998;&#31867;&#12290;&#27599;&#20010;&#29255;&#27573;&#37117;&#26159;&#26681;&#25454;&#30456;&#24212;&#30340;&#21516;&#27493;&#35270;&#39057;&#25968;&#25454;&#30001;&#20154;&#24037;&#35780;&#20272;&#21592;&#36827;&#34892;&#39044;&#27880;&#37322;&#30340;&#65292;&#26631;&#35760;&#20026;&#22352;&#31435;&#19981;&#23433;&#23384;&#22312;&#25110;&#19981;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aiming at objective early detection of neuromotor disorders such as cerebral palsy, we proposed an innovative non-intrusive approach using a pressure sensing device to classify infant general movements (GMs). Here, we tested the feasibility of using pressure data to differentiate typical GM patterns of the ''fidgety period'' (i.e., fidgety movements) vs. the ''pre-fidgety period'' (i.e., writhing movements). Participants (N = 45) were sampled from a typically-developing infant cohort. Multi-modal sensor data, including pressure data from a 32x32-grid pressure sensing mat with 1024 sensors, were prospectively recorded for each infant in seven succeeding laboratory sessions in biweekly intervals from 4-16 weeks of post-term age. For proof-of-concept, 1776 pressure data snippets, each 5s long, from the two targeted age periods were taken for movement classification. Each snippet was pre-annotated based on corresponding synchronised video data by human assessors as either fidgety present (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#32500;&#30340;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#65288;DRSOM&#65289;&#65292;&#36890;&#36807;&#22312;&#23569;&#25968;&#26041;&#21521;&#19978;&#21033;&#29992;&#26354;&#29575;&#20449;&#24687;&#26469;&#20445;&#25345;&#20108;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#23558;&#35745;&#31639;&#24320;&#38144;&#25511;&#21046;&#22312;&#19982;&#19968;&#38454;&#26041;&#27861;&#30456;&#24403;&#30340;&#31243;&#24230;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#28385;&#36275;&#36817;&#20284;&#40657;&#22622;&#30697;&#38453;&#20551;&#35774;&#30340;&#26465;&#20214;&#19979;&#20855;&#26377;&#23616;&#37096;&#20108;&#27425;&#25910;&#25947;&#21644;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#20026; $O(\epsilon^{-3/2})$&#12290;&#36890;&#36807;&#21508;&#31181;&#35745;&#31639;&#23454;&#39564;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.00208</link><description>&lt;p&gt;
DRSOM: &#19968;&#31181;&#38477;&#32500;&#30340;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DRSOM: A Dimension Reduced Second-Order Method. (arXiv:2208.00208v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#32500;&#30340;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#65288;DRSOM&#65289;&#65292;&#36890;&#36807;&#22312;&#23569;&#25968;&#26041;&#21521;&#19978;&#21033;&#29992;&#26354;&#29575;&#20449;&#24687;&#26469;&#20445;&#25345;&#20108;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#23558;&#35745;&#31639;&#24320;&#38144;&#25511;&#21046;&#22312;&#19982;&#19968;&#38454;&#26041;&#27861;&#30456;&#24403;&#30340;&#31243;&#24230;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#28385;&#36275;&#36817;&#20284;&#40657;&#22622;&#30697;&#38453;&#20551;&#35774;&#30340;&#26465;&#20214;&#19979;&#20855;&#26377;&#23616;&#37096;&#20108;&#27425;&#25910;&#25947;&#21644;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#20026; $O(\epsilon^{-3/2})$&#12290;&#36890;&#36807;&#21508;&#31181;&#35745;&#31639;&#23454;&#39564;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20984;&#21644;&#38750;&#20984;&#65288;&#26080;&#32422;&#26463;&#65289;&#20248;&#21270;&#30340;&#38477;&#32500;&#20108;&#38454;&#26041;&#27861;&#65288;DRSOM&#65289;&#12290;&#22312;&#31867;&#20284;&#20449;&#36182;&#22495;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21482;&#20351;&#29992;&#23569;&#25968;&#26041;&#21521;&#30340;&#26354;&#29575;&#20449;&#24687;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#20108;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#35745;&#31639;&#24320;&#38144;&#20173;&#28982;&#19982;&#19968;&#38454;&#26041;&#27861;&#65288;&#22914;&#26799;&#24230;&#19979;&#38477;&#27861;&#65289;&#30456;&#24403;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#23616;&#37096;&#20108;&#27425;&#25910;&#25947;&#24615;&#21644;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#20026; $O(\epsilon^{-3/2})$&#65292;&#20197;&#28385;&#36275;&#19968;&#38454;&#21644;&#20108;&#38454;&#26465;&#20214;&#65292;&#22914;&#26524;&#23376;&#31354;&#38388;&#28385;&#36275;&#24120;&#29992;&#30340;&#36817;&#20284;&#40657;&#22622;&#30697;&#38453;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#65292;&#22914;&#26524;&#22312;&#31639;&#27861;&#30340;&#26368;&#21518;&#38454;&#27573;&#23450;&#26399;&#20351;&#29992;&#31867;&#20284;Krylov&#26041;&#27861;&#30340;&#32416;&#27491;&#27493;&#39588;&#65292;&#37027;&#20040;&#21487;&#20197;&#28040;&#38500;&#35813;&#20551;&#35774;&#12290;&#36890;&#36807;&#21508;&#31181;&#35745;&#31639;&#23454;&#39564;&#23637;&#31034;&#20102;DRSOM&#30340;&#36866;&#29992;&#24615;&#21644;&#24615;&#33021;&#65292;&#21253;&#25324;$L_2 - L_p$&#26368;&#23567;&#21270;&#12289;CUTEst&#38382;&#39064;&#21644;&#20256;&#24863;&#22120;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a Dimension-Reduced Second-Order Method (DRSOM) for convex and nonconvex (unconstrained) optimization. Under a trust-region-like framework, our method preserves the convergence of the second-order method while using only curvature information in a few directions. Consequently, the computational overhead of our method remains comparable to the first-order such as the gradient descent method. Theoretically, we show that the method has a local quadratic convergence and a global convergence rate of $O(\epsilon^{-3/2})$ to satisfy the first-order and second-order conditions if the subspace satisfies a commonly adopted approximated Hessian assumption. We further show that this assumption can be removed if we perform a corrector step using a Krylov-like method periodically at the end stage of the algorithm. The applicability and performance of DRSOM are exhibited by various computational experiments, including $L_2 - L_p$ minimization, CUTEst problems, and sensor net
&lt;/p&gt;</description></item><item><title>MABe22&#26159;&#19968;&#20010;&#22810;&#29289;&#31181;&#22810;&#20219;&#21153;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#23398;&#20064;&#34892;&#20026;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#23427;&#37319;&#38598;&#20102;&#26469;&#33258;&#21508;&#31181;&#29983;&#29289;&#23398;&#23454;&#39564;&#30340;&#25968;&#25454;&#65292;&#27979;&#35797;&#20102;&#22810;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#34892;&#21160;&#25968;&#25454;&#38598;&#19978;&#30340;&#26041;&#27861;&#19981;&#33021;&#23436;&#20840;&#36866;&#29992;&#20110;&#21160;&#29289;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2207.10553</link><description>&lt;p&gt;
MABe22&#65306;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#34892;&#20026;&#34920;&#31034;&#30340;&#22810;&#29289;&#31181;&#22810;&#20219;&#21153;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MABe22: A Multi-Species Multi-Task Benchmark for Learned Representations of Behavior. (arXiv:2207.10553v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10553
&lt;/p&gt;
&lt;p&gt;
MABe22&#26159;&#19968;&#20010;&#22810;&#29289;&#31181;&#22810;&#20219;&#21153;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#23398;&#20064;&#34892;&#20026;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#23427;&#37319;&#38598;&#20102;&#26469;&#33258;&#21508;&#31181;&#29983;&#29289;&#23398;&#23454;&#39564;&#30340;&#25968;&#25454;&#65292;&#27979;&#35797;&#20102;&#22810;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#34892;&#21160;&#25968;&#25454;&#38598;&#19978;&#30340;&#26041;&#27861;&#19981;&#33021;&#23436;&#20840;&#36866;&#29992;&#20110;&#21160;&#29289;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;MABe22&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#26234;&#33021;&#20307;&#35270;&#39057;&#21644;&#36712;&#36857;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#23398;&#20064;&#34892;&#20026;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#35813;&#25968;&#25454;&#38598;&#37319;&#38598;&#33258;&#21508;&#31181;&#29983;&#29289;&#23398;&#23454;&#39564;&#65292;&#21253;&#25324;&#19977;&#20010;&#30456;&#20114;&#20316;&#29992;&#30340;&#23567;&#40736;&#19977;&#20803;&#32452;&#65288;470&#19975;&#24103;&#30340;&#35270;&#39057;+&#23039;&#24577;&#36319;&#36394;&#25968;&#25454;&#65292;1000&#19975;&#24103;&#30340;&#20165;&#23039;&#24577;&#25968;&#25454;&#65289;&#65292;&#20849;&#29983;&#30002;&#34411;-&#34434;&#34433;&#30456;&#20114;&#20316;&#29992;&#65288;1000&#19975;&#24103;&#30340;&#35270;&#39057;&#25968;&#25454;&#65289;&#21644;&#19968;&#32676;&#20114;&#21160;&#30340;&#33485;&#34631;&#65288;440&#19975;&#24103;&#30340;&#23039;&#24577;&#36319;&#36394;&#25968;&#25454;&#65289;&#12290;&#38500;&#20102;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#30495;&#23454;&#29983;&#27963;&#20013;&#30340;&#19979;&#28216;&#20998;&#26512;&#20219;&#21153;&#65292;&#20197;&#35780;&#20272;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#65292;&#36890;&#36807;&#35780;&#20272;&#23427;&#20204;&#22312;&#20445;&#30041;&#20851;&#20110;&#23454;&#39564;&#26465;&#20214;&#65288;&#20363;&#22914;&#21697;&#31995;&#12289;&#26102;&#38388;&#12289;&#20809;&#36951;&#20256;&#21050;&#28608;&#65289;&#21644;&#21160;&#29289;&#34892;&#20026;&#20449;&#24687;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#22810;&#31181;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#35270;&#39057;&#21644;&#36712;&#36857;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#22522;&#20934;&#30340;&#29992;&#36884;&#65292;&#24182;&#25581;&#31034;&#20102;&#20351;&#29992;&#20154;&#31867;&#34892;&#21160;&#25968;&#25454;&#38598;&#24320;&#21457;&#30340;&#26041;&#27861;&#19981;&#33021;&#23436;&#20840;&#36866;&#29992;&#20110;&#21160;&#29289;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MABe22, a large-scale, multi-agent video and trajectory benchmark to assess the quality of learned behavior representations. This dataset is collected from a variety of biology experiments, and includes triplets of interacting mice (4.7 million frames video+pose tracking data, 10 million frames pose only), symbiotic beetle-ant interactions (10 million frames video data), and groups of interacting flies (4.4 million frames of pose tracking data). Accompanying these data, we introduce a panel of real-life downstream analysis tasks to assess the quality of learned representations by evaluating how well they preserve information about the experimental conditions (e.g. strain, time of day, optogenetic stimulation) and animal behavior. We test multiple state-of-the-art self-supervised video and trajectory representation learning methods to demonstrate the use of our benchmark, revealing that methods developed using human action datasets do not fully translate to animal datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#24182;&#37325;&#26032;&#21019;&#24314;&#20102;&#31526;&#21495;&#22238;&#24402;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#31526;&#21495;&#22238;&#24402;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#28508;&#21147;&#21644;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#24402;&#19968;&#21270;&#32534;&#36753;&#36317;&#31163;&#36827;&#34892;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.10540</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#31185;&#23398;&#21457;&#29616;&#20013;&#31526;&#21495;&#22238;&#24402;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery. (arXiv:2206.10540v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#24182;&#37325;&#26032;&#21019;&#24314;&#20102;&#31526;&#21495;&#22238;&#24402;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#31526;&#21495;&#22238;&#24402;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#28508;&#21147;&#21644;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#24402;&#19968;&#21270;&#32534;&#36753;&#36317;&#31163;&#36827;&#34892;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26631;&#20934;&#65292;&#29305;&#21035;&#20851;&#27880;&#20854;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#28508;&#21147;&#12290;&#38024;&#23545;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#22522;&#20110;&#36153;&#26364;&#29289;&#29702;&#35762;&#20041;&#30340;&#19968;&#32452;&#20844;&#24335;&#65292;&#25105;&#20204;&#37325;&#26032;&#21019;&#24314;&#20102;120&#20010;&#25968;&#25454;&#38598;&#65292;&#35752;&#35770;&#31526;&#21495;&#22238;&#24402;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#24615;&#33021;&#65288;SRSD&#65289;&#12290;&#23545;&#20110;&#36825;120&#20010;SRSD&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20180;&#32454;&#23457;&#26597;&#20102;&#20844;&#24335;&#21450;&#20854;&#21464;&#37327;&#30340;&#23646;&#24615;&#65292;&#35774;&#35745;&#20102;&#21512;&#29702;&#30340;&#23454;&#20540;&#33539;&#22260;&#26469;&#37319;&#26679;&#20540;&#65292;&#20197;&#20415;&#25105;&#20204;&#30340;&#26032;SRSD&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;SRSD&#30340;&#28508;&#21147;&#65292;&#22914;SR&#26041;&#27861;&#26159;&#21542;&#33021;&#20174;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#20013;&#65288;&#37325;&#26032;&#65289;&#21457;&#29616;&#29289;&#29702;&#23450;&#24459;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#21478;&#22806;120&#20010;&#21253;&#21547;&#34394;&#25311;&#21464;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#26816;&#39564;SR&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#20165;&#36873;&#25321;&#24517;&#35201;&#21464;&#37327;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#39044;&#27979;&#26041;&#31243;&#19982;&#30495;&#23454;&#26041;&#31243;&#26641;&#20043;&#38388;&#30340;&#24402;&#19968;&#21270;&#32534;&#36753;&#36317;&#31163;&#65288;NED&#65289;&#26469;&#35299;&#20915;&#29616;&#26377;SR&#24230;&#37327;&#23384;&#22312;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#20108;&#20803;&#24230;&#37327;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits datasets and evaluation criteria for Symbolic Regression (SR), specifically focused on its potential for scientific discovery. Focused on a set of formulas used in the existing datasets based on Feynman Lectures on Physics, we recreate 120 datasets to discuss the performance of symbolic regression for scientific discovery (SRSD). For each of the 120 SRSD datasets, we carefully review the properties of the formula and its variables to design reasonably realistic sampling ranges of values so that our new SRSD datasets can be used for evaluating the potential of SRSD such as whether or not an SR method can (re)discover physical laws from such datasets. We also create another 120 datasets that contain dummy variables to examine whether SR methods can choose necessary variables only. Besides, we propose to use normalized edit distances (NED) between a predicted equation and the true equation trees for addressing a critical issue that existing SR metrics are either binary
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#38543;&#26426;&#36924;&#36817;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#23398;&#20064;&#21338;&#24328;&#30340;&#38271;&#26399;&#34892;&#20026;&#12290;&#35813;&#26694;&#26550;&#25972;&#21512;&#20102;&#22810;&#31181;&#24120;&#35265;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#24471;&#21040;&#20102;&#22810;&#20010;&#26032;&#30340;&#25910;&#25947;&#32467;&#26524;&#65292;&#21253;&#25324;&#23545;&#32435;&#20160;&#22343;&#34913;&#30340;&#35782;&#21035;&#21644;&#21560;&#24341;&#34892;&#21160;&#37197;&#32622;&#38598;&#21512;&#30340;&#27010;&#29575;&#39640;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2206.03922</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#38543;&#26426;&#36924;&#36817;&#26694;&#26550;&#29992;&#20110;&#23398;&#20064;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
A unified stochastic approximation framework for learning in games. (arXiv:2206.03922v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03922
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#38543;&#26426;&#36924;&#36817;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#23398;&#20064;&#21338;&#24328;&#30340;&#38271;&#26399;&#34892;&#20026;&#12290;&#35813;&#26694;&#26550;&#25972;&#21512;&#20102;&#22810;&#31181;&#24120;&#35265;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#24471;&#21040;&#20102;&#22810;&#20010;&#26032;&#30340;&#25910;&#25947;&#32467;&#26524;&#65292;&#21253;&#25324;&#23545;&#32435;&#20160;&#22343;&#34913;&#30340;&#35782;&#21035;&#21644;&#21560;&#24341;&#34892;&#21160;&#37197;&#32622;&#38598;&#21512;&#30340;&#27010;&#29575;&#39640;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#38543;&#26426;&#36924;&#36817;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#23398;&#20064;&#21338;&#24328;&#30340;&#38271;&#26399;&#34892;&#20026;&#65288;&#21253;&#25324;&#36830;&#32493;&#21644;&#26377;&#38480;&#21338;&#24328;&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;&#20998;&#26512;&#27169;&#26495;&#28085;&#30422;&#20102;&#21253;&#25324;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#12289;&#29992;&#20110;&#26377;&#38480;&#21338;&#24328;&#23398;&#20064;&#30340;&#25351;&#25968;/&#20056;&#27861;&#26435;&#37325;&#31639;&#27861;&#12289;&#19978;&#38480;&#21644;&#36951;&#28431;&#21464;&#20307;&#31561;&#22312;&#20869;&#30340;&#22810;&#31181;&#24120;&#35265;&#23398;&#20064;&#31639;&#27861;&#12290;&#38500;&#20102;&#25552;&#20379;&#23545;&#36825;&#20123;&#31639;&#27861;&#30340;&#25972;&#21512;&#35270;&#35282;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#33021;&#22815;&#24471;&#21040;&#22810;&#20010;&#26032;&#30340;&#25910;&#25947;&#32467;&#26524;&#65292;&#21253;&#25324;&#28176;&#36817;&#21644;&#26377;&#38480;&#26102;&#38388;&#20869;&#30340;&#32467;&#26524;&#65292;&#22312;&#36830;&#32493;&#21644;&#26377;&#38480;&#21338;&#24328;&#20013;&#22343;&#36866;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#21487;&#29992;&#20110;&#35782;&#21035;&#32435;&#20160;&#22343;&#34913;&#31867;&#21035;&#21644;&#39640;&#27010;&#29575;&#21560;&#24341;&#34892;&#21160;&#37197;&#32622;&#38598;&#21512;&#30340;&#26631;&#20934;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#36830;&#36143;&#24615;&#30340;&#27010;&#24565;&#65292;&#21363;&#28216;&#25103;&#29702;&#35770;&#23646;&#24615;&#65292;&#21253;&#25324;&#20005;&#26684;&#21644;&#23574;&#38160;&#22343;&#34913;&#65292;&#24182;&#19988;&#23548;&#33268;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#25910;&#25947;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#36866;&#29992;&#20110;&#22522;&#20110;oracle&#30340;&#26041;&#27861;&#21644;&#36951;&#28431;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a flexible stochastic approximation framework for analyzing the long-run behavior of learning in games (both continuous and finite). The proposed analysis template incorporates a wide array of popular learning algorithms, including gradient-based methods, the exponential/multiplicative weights algorithm for learning in finite games, optimistic and bandit variants of the above, etc. In addition to providing an integrated view of these algorithms, our framework further allows us to obtain several new convergence results, both asymptotic and in finite time, in both continuous and finite games. Specifically, we provide a range of criteria for identifying classes of Nash equilibria and sets of action profiles that are attracting with high probability, and we also introduce the notion of coherence, a game-theoretic property that includes strict and sharp equilibria, and which leads to convergence in finite time. Importantly, our analysis applies to both oracle-based and bandit, pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20998;&#24067;&#36716;&#31227;&#35270;&#35282;&#20986;&#21457;&#65292;&#30740;&#31350;&#20102;&#20174;&#20559;&#21521;&#21453;&#39304;&#20013;&#23398;&#20064;&#26080;&#20559;&#31639;&#27861;&#36827;&#34892;&#25512;&#33616;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24314;&#31435;&#26080;&#20559;&#25512;&#33616;&#19982;&#20998;&#24067;&#36716;&#31227;&#30340;&#20851;&#31995;&#65292;&#23545;&#29616;&#26377;&#26080;&#20559;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#35299;&#37322;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#27867;&#21270;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2206.03851</link><description>&lt;p&gt;
&#22312;&#26080;&#20559;&#25512;&#33616;&#20013;&#37325;&#26032;&#32771;&#34385;&#23398;&#20064;&#30446;&#26631;&#65306;&#20998;&#24067;&#36716;&#31227;&#35270;&#35282;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reconsidering Learning Objectives in Unbiased Recommendation: A Distribution Shift Perspective. (arXiv:2206.03851v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20998;&#24067;&#36716;&#31227;&#35270;&#35282;&#20986;&#21457;&#65292;&#30740;&#31350;&#20102;&#20174;&#20559;&#21521;&#21453;&#39304;&#20013;&#23398;&#20064;&#26080;&#20559;&#31639;&#27861;&#36827;&#34892;&#25512;&#33616;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24314;&#31435;&#26080;&#20559;&#25512;&#33616;&#19982;&#20998;&#24067;&#36716;&#31227;&#30340;&#20851;&#31995;&#65292;&#23545;&#29616;&#26377;&#26080;&#20559;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#35299;&#37322;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#27867;&#21270;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#20559;&#21521;&#21453;&#39304;&#20013;&#23398;&#20064;&#26080;&#20559;&#31639;&#27861;&#36827;&#34892;&#25512;&#33616;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#24067;&#36716;&#31227;&#35270;&#35282;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26368;&#36817;&#22312;&#26080;&#20559;&#25512;&#33616;&#39046;&#22495;&#30340;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#21508;&#31181;&#25216;&#26415;&#22914;&#37325;&#26032;&#21152;&#26435;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#65292;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#23454;&#35777;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22823;&#37096;&#20998;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#65292;&#23548;&#33268;&#20102;&#29702;&#35770;&#21644;&#26368;&#26032;&#31639;&#27861;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#29616;&#26377;&#26080;&#20559;&#23398;&#20064;&#30446;&#26631;&#20026;&#20309;&#36866;&#29992;&#20110;&#26080;&#20559;&#25512;&#33616;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#26080;&#20559;&#25512;&#33616;&#19982;&#20998;&#24067;&#36716;&#31227;&#20043;&#38388;&#30340;&#23494;&#20999;&#20851;&#31995;&#65292;&#26174;&#31034;&#20102;&#29616;&#26377;&#30340;&#26080;&#20559;&#23398;&#20064;&#30446;&#26631;&#38544;&#21547;&#22320;&#23558;&#26377;&#20559;&#30340;&#35757;&#32451;&#20998;&#24067;&#19982;&#26080;&#20559;&#30340;&#27979;&#35797;&#20998;&#24067;&#23545;&#40784;&#12290;&#22522;&#20110;&#36825;&#20010;&#20851;&#31995;&#65292;&#25105;&#20204;&#38024;&#23545;&#29616;&#26377;&#30340;&#26080;&#20559;&#23398;&#20064;&#26041;&#27861;&#21457;&#23637;&#20102;&#20004;&#20010;&#27867;&#21270;&#30028;&#38480;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#23398;&#20064;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies the problem of learning unbiased algorithms from biased feedback for recommendation. We address this problem from a novel distribution shift perspective. Recent works in unbiased recommendation have advanced the state-of-the-art with various techniques such as re-weighting, multi-task learning, and meta-learning. Despite their empirical successes, most of them lack theoretical guarantees, forming non-negligible gaps between theories and recent algorithms. In this paper, we propose a theoretical understanding of why existing unbiased learning objectives work for unbiased recommendation. We establish a close connection between unbiased recommendation and distribution shift, which shows that existing unbiased learning objectives implicitly align biased training and unbiased test distributions. Built upon this connection, we develop two generalization bounds for existing unbiased learning methods and analyze their learning behavior. Besides, as a result of the distributio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#38543;&#26426;&#36817;&#20284;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#21033;&#29992;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#32467;&#26500;&#21560;&#25910;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#36924;&#36817;&#35823;&#24046;&#24182;&#33719;&#24471;&#20102;&#25913;&#36827;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;SGLD&#21644;RBM&#65292;&#25105;&#20204;&#20998;&#21035;&#35777;&#26126;&#20102;&#19981;&#21516;&#30340;&#20551;&#35774;&#26465;&#20214;&#19979;&#36739;&#20248;&#30340;&#25910;&#25947;&#29575;&#21644;&#21442;&#25968;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2206.03792</link><description>&lt;p&gt;
&#21033;&#29992;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#32467;&#26500;&#30340;&#38543;&#26426;&#26799;&#24230;&#37319;&#26679;&#26041;&#27861;&#65306;&#25913;&#36827;&#30340;&#20998;&#26512;&#21644;&#26356;&#24555;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Utilising the CLT Structure in Stochastic Gradient based Sampling : Improved Analysis and Faster Algorithms. (arXiv:2206.03792v3 [math.PR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#38543;&#26426;&#36817;&#20284;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#21033;&#29992;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#32467;&#26500;&#21560;&#25910;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#36924;&#36817;&#35823;&#24046;&#24182;&#33719;&#24471;&#20102;&#25913;&#36827;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;SGLD&#21644;RBM&#65292;&#25105;&#20204;&#20998;&#21035;&#35777;&#26126;&#20102;&#19981;&#21516;&#30340;&#20551;&#35774;&#26465;&#20214;&#19979;&#36739;&#20248;&#30340;&#25910;&#25947;&#29575;&#21644;&#21442;&#25968;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#38543;&#26426;&#36817;&#20284;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#22914;&#38543;&#26426;&#26799;&#24230; langevin &#21160;&#21147;&#23398;&#65288;SGLD&#65289;&#21644;&#38543;&#26426;&#25209;&#22788;&#29702;&#26041;&#27861;&#65288;RBM&#65289;&#29992;&#20110;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#21160;&#21147;&#23398;&#65288;IPD&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30001;&#20110;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65288;CLT&#65289;&#65292;&#38543;&#26426;&#36924;&#36817;&#24341;&#20837;&#30340;&#22122;&#22768;&#20960;&#20046;&#26159;&#39640;&#26031;&#20998;&#24067;&#65292;&#32780;&#39537;&#21160;&#24067;&#26391;&#36816;&#21160;&#21017;&#26159;&#30830;&#20999;&#30340;&#39640;&#26031;&#20998;&#24067;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#26469;&#21560;&#25910;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#36924;&#36817;&#35823;&#24046;&#65292;&#24182;&#33719;&#24471;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#25913;&#36827;&#25910;&#25947;&#20445;&#35777;&#12290;&#23545;&#20110; SGLD&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19981;&#38656;&#35201;&#32479;&#19968;&#28201;&#26262;&#21551;&#21160;&#30340;&#24773;&#20917;&#19979;KL&#25955;&#24230;&#30340;&#31532;&#19968;&#20010;&#31283;&#23450;&#25910;&#25947;&#29575;&#65292;&#20551;&#35774;&#30446;&#26631;&#23494;&#24230;&#28385;&#36275;&#19968;&#20010;&#23545;&#25968; Sobolev &#19981;&#31561;&#24335;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24847;&#21619;&#30528;&#22312;&#26174;&#33879;&#36739;&#36731;&#30340;&#20551;&#35774;&#26465;&#20214;&#19979;&#65292;&#30456;&#23545;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#19968;&#38454; oracle &#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102; SGLD &#30340;&#31532;&#19968;&#20010;&#20445;&#35777;&#65292;&#23545;&#20110;&#26356;&#24369;&#30340;&#26465;&#20214;&#65292;&#22914; H\''{o}lder &#24179;&#28369;&#24615;&#21644; Poincare&#19981;&#31561;&#24335;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#29616;&#26377;&#25216;&#26415;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23545;&#20110; RBM&#65292;&#25105;&#20204;&#22312; IPD &#30340;&#24369;&#28151;&#21512;&#26465;&#20214;&#19979;&#33719;&#24471;&#20102;&#31532;&#19968;&#27425;&#25910;&#25947;&#20998;&#26512;&#21644;&#26368;&#20339;&#21442;&#25968;&#33539;&#22260;&#65292;&#36825;&#22312;&#32479;&#35745;&#29289;&#29702;&#21644;&#23398;&#20064;&#29702;&#35770;&#20013;&#20855;&#26377;&#20960;&#20010;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider stochastic approximations of sampling algorithms, such as Stochastic Gradient Langevin Dynamics (SGLD) and the Random Batch Method (RBM) for Interacting Particle Dynamcs (IPD). We observe that the noise introduced by the stochastic approximation is nearly Gaussian due to the Central Limit Theorem (CLT) while the driving Brownian motion is exactly Gaussian. We harness this structure to absorb the stochastic approximation error inside the diffusion process, and obtain improved convergence guarantees for these algorithms. For SGLD, we prove the first stable convergence rate in KL divergence without requiring uniform warm start, assuming the target density satisfies a Log-Sobolev Inequality. Our result implies superior first-order oracle complexity compared to prior works, under significantly milder assumptions. We also prove the first guarantees for SGLD under even weaker conditions such as H\"{o}lder smoothness and Poincare Inequality, thus bridging the gap between the state-
&lt;/p&gt;</description></item><item><title>NIPQ&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#20195;&#29702;&#30340;&#38598;&#25104;&#20266;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#25130;&#26029;&#29702;&#24565;&#65292;&#33021;&#22815;&#32479;&#19968;&#25903;&#25345;&#28608;&#27963;&#21644;&#26435;&#37325;&#30340;&#20266;&#37327;&#21270;&#12290;NIPQ&#36991;&#20813;&#20102;&#30452;&#36890;&#20272;&#35745;&#22120;(STE)&#22312;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#20248;&#20110;&#29616;&#26377;&#37327;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.00820</link><description>&lt;p&gt;
NIPQ: &#22522;&#20110;&#22122;&#22768;&#20195;&#29702;&#30340;&#38598;&#25104;&#20266;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
NIPQ: Noise proxy-based Integrated Pseudo-Quantization. (arXiv:2206.00820v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00820
&lt;/p&gt;
&lt;p&gt;
NIPQ&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#20195;&#29702;&#30340;&#38598;&#25104;&#20266;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#25130;&#26029;&#29702;&#24565;&#65292;&#33021;&#22815;&#32479;&#19968;&#25903;&#25345;&#28608;&#27963;&#21644;&#26435;&#37325;&#30340;&#20266;&#37327;&#21270;&#12290;NIPQ&#36991;&#20813;&#20102;&#30452;&#36890;&#20272;&#35745;&#22120;(STE)&#22312;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#20248;&#20110;&#29616;&#26377;&#37327;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#36890;&#20272;&#35745;&#22120;&#65288;STE&#65289;&#22312;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#30456;&#20851;&#30740;&#31350;&#20013;&#22791;&#21463;&#38738;&#30544;&#65292;&#23427;&#36890;&#36807;&#36817;&#20284;&#20351;&#26799;&#24230;&#27969;&#36807;&#38750;&#21487;&#24494;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;QAT&#20013;&#65292;STE&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#25910;&#25947;&#65292;&#38477;&#20302;&#20102;&#20302;&#31934;&#24230;&#30340;&#36136;&#37327;&#12290;&#26368;&#36817;&#65292;&#20266;&#37327;&#21270;&#35757;&#32451;&#34987;&#25552;&#20986;&#20316;&#20026;&#20351;&#29992;&#20266;&#37327;&#21270;&#22122;&#22768;&#32780;&#19981;&#26159;STE&#26356;&#26032;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22122;&#22768;&#20195;&#29702;&#30340;&#38598;&#25104;&#20266;&#37327;&#21270;&#65288;NIPQ&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25130;&#26029;&#29702;&#24565;&#34701;&#20837;&#20266;&#37327;&#21270;&#26694;&#26550;&#65292;&#20026;&#28608;&#27963;&#21644;&#26435;&#37325;&#20004;&#26041;&#38754;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#20266;&#37327;&#21270;&#25903;&#25345;&#12290;NIPQ&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#25152;&#26377;&#30340;&#37327;&#21270;&#21442;&#25968;&#65288;&#22914;&#20301;&#23485;&#21644;&#25130;&#26029;&#36793;&#30028;&#65289;&#20197;&#21450;&#32593;&#32476;&#21442;&#25968;&#65292;&#36991;&#20813;&#20102;STE&#19981;&#31283;&#23450;&#24615;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;NIPQ&#22312;&#21508;&#31181;&#35270;&#22270;&#19979;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#37327;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Straight-through estimator (STE), which enables the gradient flow over the non-differentiable function via approximation, has been favored in studies related to quantization-aware training (QAT). However, STE incurs unstable convergence during QAT, resulting in notable quality degradation in low precision. Recently, pseudoquantization training has been proposed as an alternative approach to updating the learnable parameters using the pseudo-quantization noise instead of STE. In this study, we propose a novel noise proxy-based integrated pseudoquantization (NIPQ) that enables unified support of pseudoquantization for both activation and weight by integrating the idea of truncation on the pseudo-quantization framework. NIPQ updates all of the quantization parameters (e.g., bit-width and truncation boundary) as well as the network parameters via gradient descent without STE instability. According to our extensive experiments, NIPQ outperforms existing quantization algorithms in various vi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20844;&#27491;&#24433;&#21709;&#20989;&#25968;&#65288;FIF&#65289;&#65292;&#36890;&#36807;&#20840;&#23616;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#26041;&#27861;&#37327;&#21270;&#20102;&#19981;&#21516;&#29305;&#24449;&#23545;&#20998;&#31867;&#22120;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20844;&#24179;&#24615;&#38382;&#39064;&#20013;&#30340;&#26680;&#24515;&#20851;&#27880;&#28857;&#12290;</title><link>http://arxiv.org/abs/2206.00667</link><description>&lt;p&gt;
&#29305;&#24449;&#26377;&#22810;&#20559;&#35265;&#65311;&#65306;&#36890;&#36807;&#20840;&#23616;&#25935;&#24863;&#24615;&#20998;&#26512;&#35745;&#31639;&#20844;&#27491;&#24433;&#21709;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
How Biased are Your Features?: Computing Fairness Influence Functions with Global Sensitivity Analysis. (arXiv:2206.00667v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20844;&#27491;&#24433;&#21709;&#20989;&#25968;&#65288;FIF&#65289;&#65292;&#36890;&#36807;&#20840;&#23616;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#26041;&#27861;&#37327;&#21270;&#20102;&#19981;&#21516;&#29305;&#24449;&#23545;&#20998;&#31867;&#22120;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20844;&#24179;&#24615;&#38382;&#39064;&#20013;&#30340;&#26680;&#24515;&#20851;&#27880;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#22240;&#20854;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#26410;&#21463;&#30417;&#31649;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#21487;&#33021;&#23545;&#25968;&#25454;&#20013;&#30340;&#26576;&#20123;&#20154;&#21475;&#32676;&#20307;&#34920;&#29616;&#20986;&#20559;&#35265;&#65292;&#22240;&#27492;&#37327;&#21270;&#21644;&#20943;&#36731;&#20998;&#31867;&#22120;&#20559;&#35265;&#26159;&#20844;&#24179;&#24615;&#38382;&#39064;&#30340;&#26680;&#24515;&#20851;&#27880;&#28857;&#12290;&#26412;&#25991;&#26088;&#22312;&#37327;&#21270;&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#29305;&#24449;&#23545;&#20998;&#31867;&#22120;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20844;&#27491;&#24433;&#21709;&#20989;&#25968;&#65288;FIF&#65289;&#12290;&#35813;&#20989;&#25968;&#23558;&#20559;&#35265;&#20998;&#35299;&#20026;&#20854;&#22312;&#20010;&#20307;&#29305;&#24449;&#21644;&#22810;&#20010;&#29305;&#24449;&#30340;&#20132;&#38598;&#20013;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#29616;&#26377;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#24230;&#37327;&#34920;&#31034;&#20026;&#20998;&#31867;&#22120;&#39044;&#27979;&#30340;&#26465;&#20214;&#26041;&#24046;&#30340;&#24046;&#24322;&#65292;&#24182;&#26681;&#25454;&#20840;&#23616;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#20998;&#35299;&#36827;&#34892;&#26041;&#24046;&#20272;&#35745;&#12290;&#20026;&#20102;&#20272;&#35745;FIFs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FairXplainer&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#24212;&#29992;&#20998;&#31867;&#22120;&#39044;&#27979;&#30340;&#26041;&#24046;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness in machine learning has attained significant focus due to the widespread application in high-stake decision-making tasks. Unregulated machine learning classifiers can exhibit bias towards certain demographic groups in data, thus the quantification and mitigation of classifier bias is a central concern in fairness in machine learning. In this paper, we aim to quantify the influence of different features in a dataset on the bias of a classifier. To do this, we introduce the Fairness Influence Function (FIF). This function breaks down bias into its components among individual features and the intersection of multiple features. The key idea is to represent existing group fairness metrics as the difference of the scaled conditional variances in the classifier's prediction and apply a decomposition of variance according to global sensitivity analysis. To estimate FIFs, we instantiate an algorithm FairXplainer that applies variance decomposition of classifier's prediction following l
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#28145;&#24230;&#30452;&#25509;&#21028;&#21035;&#35299;&#30721;&#22120;&#65288;D4&#65289;&#12290;D4&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#26377;&#25928;&#22320;&#20272;&#35745;&#20102;&#39640;&#32500;&#35266;&#27979;&#20449;&#21495;&#19979;&#30340;&#28508;&#22312;&#29366;&#24577;&#36807;&#31243;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.10947</link><description>&lt;p&gt;
&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#30340;&#28145;&#24230;&#30452;&#25509;&#21028;&#21035;&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Deep Direct Discriminative Decoders for High-dimensional Time-series Data Analysis. (arXiv:2205.10947v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10947
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#28145;&#24230;&#30452;&#25509;&#21028;&#21035;&#35299;&#30721;&#22120;&#65288;D4&#65289;&#12290;D4&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#26377;&#25928;&#22320;&#20272;&#35745;&#20102;&#39640;&#32500;&#35266;&#27979;&#20449;&#21495;&#19979;&#30340;&#28508;&#22312;&#29366;&#24577;&#36807;&#31243;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#20013;&#12290;SSMs&#20381;&#36182;&#20110;&#23545;&#29366;&#24577;&#21644;&#35266;&#27979;&#36807;&#31243;&#30340;&#26126;&#30830;&#23450;&#20041;&#12290;&#24403;&#35266;&#27979;&#25968;&#25454;&#30340;&#32500;&#24230;&#22686;&#21152;&#25110;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#20559;&#31163;&#27491;&#24577;&#20998;&#24067;&#26102;&#65292;&#25551;&#36848;&#36825;&#20123;&#36807;&#31243;&#24182;&#19981;&#24635;&#26159;&#23481;&#26131;&#30340;&#65292;&#36825;&#25104;&#20026;&#24314;&#27169;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#35266;&#27979;&#36807;&#31243;&#30340;&#26032;&#30340;SSM&#34920;&#36798;&#24418;&#24335;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#35299;&#20915;&#26041;&#26696;&#31216;&#20026;&#28145;&#24230;&#30452;&#25509;&#21028;&#21035;&#35299;&#30721;&#22120;&#65288;D4&#65289;&#12290;D4&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#24341;&#20837;&#21040;SSM&#34920;&#36798;&#24418;&#24335;&#20013;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#39640;&#32500;&#35266;&#27979;&#20449;&#21495;&#39640;&#25928;&#22320;&#20272;&#35745;&#28508;&#22312;&#30340;&#29366;&#24577;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#65288;&#22914;Lorenz&#21560;&#24341;&#23376;&#12289;Langevin&#21160;&#21147;&#23398;&#12289;&#38543;&#26426;&#34892;&#36208;&#21160;&#21147;&#23398;&#21644;&#22823;&#40736;&#28023;&#39532;&#38829;&#29366;&#31070;&#32463;&#25968;&#25454;&#65289;&#19978;&#28436;&#31034;&#20102;D4&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#27604;&#20256;&#32479;&#30340;SSMs&#21644;RNNs&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;D4&#21487;&#20197;&#24212;&#29992;&#20110;
&lt;/p&gt;
&lt;p&gt;
The state-space models (SSMs) are widely utilized in the analysis of time-series data. SSMs rely on an explicit definition of the state and observation processes. Characterizing these processes is not always easy and becomes a modeling challenge when the dimension of observed data grows or the observed data distribution deviates from the normal distribution. Here, we propose a new formulation of SSM for high-dimensional observation processes. We call this solution the deep direct discriminative decoder (D4). The D4 brings deep neural networks' expressiveness and scalability to the SSM formulation letting us build a novel solution that efficiently estimates the underlying state processes through high-dimensional observation signal. We demonstrate the D4 solutions in simulated and real data such as Lorenz attractors, Langevin dynamics, random walk dynamics, and rat hippocampus spiking neural data and show that the D4 performs better than traditional SSMs and RNNs. The D4 can be applied t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25945;&#31243;&#20171;&#32461;&#20102;Transformer&#26550;&#26500;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#26680;&#24515;&#32452;&#20214;&#21644;&#22810;&#39033;&#22686;&#24378;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#35757;&#32451;Transformer&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#25216;&#24039;&#12290;</title><link>http://arxiv.org/abs/2205.01138</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;Transformer&#65306;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Transformers in Time-series Analysis: A Tutorial. (arXiv:2205.01138v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.01138
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25945;&#31243;&#20171;&#32461;&#20102;Transformer&#26550;&#26500;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#26680;&#24515;&#32452;&#20214;&#21644;&#22810;&#39033;&#22686;&#24378;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#35757;&#32451;Transformer&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#25216;&#24039;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;Transformer&#34987;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#26412;&#25945;&#31243;&#27010;&#36848;&#20102;Transformer&#26550;&#26500;&#21450;&#20854;&#24212;&#29992;&#65292;&#24182;&#20174;&#26368;&#36817;&#30340;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#20379;&#20102;&#19968;&#20123;&#31034;&#20363;&#12290;&#25105;&#20204;&#35814;&#32454;&#35299;&#37322;&#20102;Transformer&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#21253;&#25324;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12289;&#20301;&#32622;&#32534;&#30721;&#12289;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#12290;&#36824;&#24378;&#35843;&#20102;&#23545;&#21021;&#22987;Transformer&#26550;&#26500;&#30340;&#22810;&#39033;&#22686;&#24378;&#26041;&#27861;&#26469;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#12290;&#26412;&#25945;&#31243;&#36824;&#25552;&#20379;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#26377;&#25928;&#35757;&#32451;Transformer&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer architecture has widespread applications, particularly in Natural Language Processing and computer vision. Recently Transformers have been employed in various aspects of time-series analysis. This tutorial provides an overview of the Transformer architecture, its applications, and a collection of examples from recent research papers in time-series analysis. We delve into an explanation of the core components of the Transformer, including the self-attention mechanism, positional encoding, multi-head, and encoder/decoder. Several enhancements to the initial, Transformer architecture are highlighted to tackle time-series tasks. The tutorial also provides best practices and techniques to overcome the challenge of effectively training Transformers for time-series analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#26368;&#21518;&#19968;&#23618;&#37325;&#26032;&#35757;&#32451;&#36275;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#25110;&#32988;&#36807;&#65292;&#20294;&#20854;&#22797;&#26434;&#24230;&#21644;&#35745;&#31639;&#24320;&#38144;&#36739;&#20302;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#22312;ImageNet&#35757;&#32451;&#30340;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#26368;&#21518;&#19968;&#23618;&#37325;&#26032;&#35757;&#32451;&#65292;&#20165;&#20960;&#20998;&#38047;&#30340;&#35757;&#32451;&#26102;&#38388;&#23601;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#23545;&#32972;&#26223;&#21644;&#32441;&#29702;&#20449;&#24687;&#30340;&#20381;&#36182;&#65292;&#25552;&#39640;&#23545;&#21327;&#21464;&#37327;&#36716;&#21464;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.02937</link><description>&lt;p&gt;
&#26368;&#21518;&#19968;&#23618;&#37325;&#26032;&#35757;&#32451;&#36275;&#20197;&#25552;&#39640;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations. (arXiv:2204.02937v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.02937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#26368;&#21518;&#19968;&#23618;&#37325;&#26032;&#35757;&#32451;&#36275;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#25110;&#32988;&#36807;&#65292;&#20294;&#20854;&#22797;&#26434;&#24230;&#21644;&#35745;&#31639;&#24320;&#38144;&#36739;&#20302;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#22312;ImageNet&#35757;&#32451;&#30340;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#26368;&#21518;&#19968;&#23618;&#37325;&#26032;&#35757;&#32451;&#65292;&#20165;&#20960;&#20998;&#38047;&#30340;&#35757;&#32451;&#26102;&#38388;&#23601;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#23545;&#32972;&#26223;&#21644;&#32441;&#29702;&#20449;&#24687;&#30340;&#20381;&#36182;&#65292;&#25552;&#39640;&#23545;&#21327;&#21464;&#37327;&#36716;&#21464;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#21487;&#20197;&#20027;&#35201;&#20381;&#38752;&#31616;&#21333;&#30340;&#34394;&#20551;&#29305;&#24449;&#65288;&#22914;&#32972;&#26223;&#65289;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#20173;&#28982;&#32463;&#24120;&#23398;&#20064;&#19982;&#25968;&#25454;&#25152;&#38656;&#23646;&#24615;&#30456;&#20851;&#30340;&#26680;&#24515;&#29305;&#24449;&#65292;&#19982;&#26368;&#36817;&#30340;&#30740;&#31350;&#32467;&#26524;&#30456;&#21453;&#12290;&#21463;&#21040;&#36825;&#19968;&#21551;&#31034;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;&#26368;&#21518;&#19968;&#23618;&#37325;&#26032;&#35757;&#32451;&#21487;&#20197;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#29978;&#33267;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#20294;&#20854;&#22797;&#26434;&#24230;&#21644;&#35745;&#31639;&#24320;&#38144;&#26174;&#33879;&#36739;&#20302;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23545;&#20110;&#22312;ImageNet&#35757;&#32451;&#30340;&#22823;&#22411;&#27169;&#22411;&#19978;&#36827;&#34892;&#26368;&#21518;&#19968;&#23618;&#37325;&#26032;&#35757;&#32451;&#65292;&#20165;&#32463;&#36807;&#20960;&#20998;&#38047;&#30340;&#21333;GPU&#35757;&#32451;&#65292;&#20063;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#23545;&#32972;&#26223;&#21644;&#32441;&#29702;&#20449;&#24687;&#30340;&#20381;&#36182;&#65292;&#25552;&#39640;&#23545;&#21327;&#21464;&#37327;&#36716;&#21464;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network classifiers can largely rely on simple spurious features, such as backgrounds, to make predictions. However, even in these cases, we show that they still often learn core features associated with the desired attributes of the data, contrary to recent findings. Inspired by this insight, we demonstrate that simple last layer retraining can match or outperform state-of-the-art approaches on spurious correlation benchmarks, but with profoundly lower complexity and computational expenses. Moreover, we show that last layer retraining on large ImageNet-trained models can also significantly reduce reliance on background and texture information, improving robustness to covariate shift, after only minutes of training on a single GPU.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#32570;&#22833;&#35266;&#27979;&#30340;&#20301;&#32622;&#26410;&#30693;&#26102;&#23398;&#20064;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19981;&#38656;&#35201;&#20808;&#39564;&#20449;&#24687;&#30340;&#37325;&#24314;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2203.06527</link><description>&lt;p&gt;
&#24403;&#32570;&#22833;&#35266;&#27979;&#30340;&#20301;&#32622;&#26410;&#30693;&#26102;&#23398;&#20064;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Hidden Markov Models When the Locations of Missing Observations are Unknown. (arXiv:2203.06527v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.06527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#32570;&#22833;&#35266;&#27979;&#30340;&#20301;&#32622;&#26410;&#30693;&#26102;&#23398;&#20064;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19981;&#38656;&#35201;&#20808;&#39564;&#20449;&#24687;&#30340;&#37325;&#24314;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HMM&#65289;&#26159;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#30340;&#26368;&#24120;&#29992;&#30340;&#32479;&#35745;&#27169;&#22411;&#20043;&#19968;&#12290;HMM&#20855;&#26377;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#36825;&#20063;&#26159;&#23427;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#20851;&#38190;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;HMM&#23398;&#20064;&#31639;&#27861;&#22522;&#20110;&#32570;&#22833;&#35266;&#27979;&#22312;&#35266;&#27979;&#24207;&#21015;&#20013;&#30340;&#20301;&#32622;&#24050;&#30693;&#30340;&#20551;&#35774;&#12290;&#22312;&#33258;&#28982;&#31185;&#23398;&#20013;&#65292;&#36825;&#31181;&#20551;&#35774;&#24120;&#24120;&#19981;&#25104;&#31435;&#65292;&#22240;&#27492;&#36890;&#24120;&#20351;&#29992;&#29305;&#27530;&#21464;&#20307;&#30340;HMM&#65292;&#31216;&#20026;Silent-state HMMs&#65288;SHMMs&#65289;&#12290;&#23613;&#31649;&#36825;&#20123;&#31639;&#27861;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#28508;&#22312;&#38142;&#30340;&#29305;&#23450;&#32467;&#26500;&#20551;&#35774;&#65292;&#27604;&#22914;&#38750;&#24490;&#29615;&#24615;&#65292;&#36825;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;&#32780;&#19988;&#65292;&#21363;&#20351;&#22312;&#38750;&#24490;&#29615;&#24773;&#20917;&#19979;&#65292;&#24050;&#32463;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#37325;&#24314;&#25928;&#26524;&#24046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#20855;&#26377;&#26410;&#30693;&#32570;&#22833;&#35266;&#27979;&#20301;&#32622;&#25968;&#25454;&#20013;&#23398;&#20064;HMM&#30340;&#19968;&#33324;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19981;&#38656;&#35201;&#20219;&#20309;&#20808;&#39564;&#20449;&#24687;&#30340;&#37325;&#24314;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Hidden Markov Model (HMM) is one of the most widely used statistical models for sequential data analysis. One of the key reasons for this versatility is the ability of HMM to deal with missing data. However, standard HMM learning algorithms rely crucially on the assumption that the positions of the missing observations \emph{within the observation sequence} are known. In the natural sciences, where this assumption is often violated, special variants of HMM, commonly known as Silent-state HMMs (SHMMs), are used. Despite their widespread use, these algorithms strongly rely on specific structural assumptions of the underlying chain, such as acyclicity, thus limiting the applicability of these methods. Moreover, even in the acyclic case, it has been shown that these methods can lead to poor reconstruction. In this paper we consider the general problem of learning an HMM from data with unknown missing observation locations. We provide reconstruction algorithms that do not require any as
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#32570;&#20047;&#24179;&#28369;&#24615;&#30340;&#21183;&#33021;&#30340;&#37319;&#26679;&#38382;&#39064;&#30340;&#36817;&#31471;&#31639;&#27861;&#65292;&#22312;&#20984;&#21644;&#38750;&#20984;&#24773;&#20917;&#19979;&#22343;&#21487;&#36866;&#29992;&#12290;&#35813;&#31639;&#27861;&#30340;&#20851;&#38190;&#21019;&#26032;&#28857;&#22312;&#20110;&#22522;&#20110;&#25298;&#32477;&#37319;&#26679;&#30340;&#20132;&#26367;&#37319;&#26679;&#26694;&#26550;&#30340;&#23454;&#38469;&#23454;&#29616;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2202.13975</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#37319;&#26679;&#30340;&#36817;&#31471;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Proximal Algorithm for Sampling. (arXiv:2202.13975v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.13975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#32570;&#20047;&#24179;&#28369;&#24615;&#30340;&#21183;&#33021;&#30340;&#37319;&#26679;&#38382;&#39064;&#30340;&#36817;&#31471;&#31639;&#27861;&#65292;&#22312;&#20984;&#21644;&#38750;&#20984;&#24773;&#20917;&#19979;&#22343;&#21487;&#36866;&#29992;&#12290;&#35813;&#31639;&#27861;&#30340;&#20851;&#38190;&#21019;&#26032;&#28857;&#22312;&#20110;&#22522;&#20110;&#25298;&#32477;&#37319;&#26679;&#30340;&#20132;&#26367;&#37319;&#26679;&#26694;&#26550;&#30340;&#23454;&#38469;&#23454;&#29616;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;&#32570;&#20047;&#24179;&#28369;&#24615;&#30340;&#21183;&#33021;&#30456;&#20851;&#30340;&#37319;&#26679;&#38382;&#39064;&#12290;&#36825;&#20123;&#21183;&#33021;&#21487;&#20197;&#26159;&#20984;&#30340;&#25110;&#38750;&#20984;&#30340;&#12290;&#19981;&#21516;&#20110;&#26631;&#20934;&#30340;&#24179;&#28369;&#35774;&#32622;&#65292;&#36825;&#20123;&#21183;&#33021;&#21482;&#34987;&#35748;&#20026;&#26159;&#24369;&#24179;&#28369;&#25110;&#38750;&#24179;&#28369;&#30340;&#65292;&#25110;&#32773;&#26159;&#22810;&#20010;&#36825;&#26679;&#30340;&#20989;&#25968;&#30340;&#27714;&#21644;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#37319;&#26679;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#31867;&#20284;&#20110;&#29992;&#20110;&#36825;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#37319;&#26679;&#20219;&#21153;&#30340;&#20248;&#21270;&#38382;&#39064;&#30340;&#36817;&#31471;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;&#19968;&#31181;&#31216;&#20026;&#20132;&#26367;&#37319;&#26679;&#26694;&#26550;&#65288;ASF&#65289;&#30340;Gibbs&#37319;&#26679;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20851;&#38190;&#36129;&#29486;&#26159;&#22522;&#20110;&#25298;&#32477;&#37319;&#26679;&#30340;ASF&#30340;&#23454;&#38469;&#23454;&#29616;&#65292;&#36866;&#29992;&#20110;&#38750;&#20984;&#21644;&#19981;&#19968;&#23450;&#24179;&#28369;&#30340;&#20984;&#21183;&#33021;&#12290;&#22312;&#26412;&#24037;&#20316;&#32771;&#34385;&#30340;&#20960;&#20046;&#25152;&#26377;&#37319;&#26679;&#26696;&#20363;&#20013;&#65292;&#25105;&#20204;&#30340;&#36817;&#31471;&#37319;&#26679;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#37117;&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study sampling problems associated with potentials that lack smoothness. The potentials can be either convex or non-convex. Departing from the standard smooth setting, the potentials are only assumed to be weakly smooth or non-smooth, or the summation of multiple such functions. We develop a sampling algorithm that resembles proximal algorithms in optimization for this challenging sampling task. Our algorithm is based on a special case of Gibbs sampling known as the alternating sampling framework (ASF). The key contribution of this work is a practical realization of the ASF based on rejection sampling for both non-convex and convex potentials that are not necessarily smooth. In almost all the cases of sampling considered in this work, our proximal sampling algorithm achieves better complexity than all existing methods.
&lt;/p&gt;</description></item><item><title>Motif&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MGNN&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#25429;&#25417;&#39640;&#38454;&#22270;&#32467;&#26500;&#65292;&#24182;&#35299;&#20915;&#20256;&#32479;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21306;&#20998;&#20302;&#38454;&#21644;&#39640;&#38454;&#22270;&#32467;&#26500;&#19978;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2112.14900</link><description>&lt;p&gt;
Motif&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Motif Graph Neural Network. (arXiv:2112.14900v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.14900
&lt;/p&gt;
&lt;p&gt;
Motif&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MGNN&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#25429;&#25417;&#39640;&#38454;&#22270;&#32467;&#26500;&#65292;&#24182;&#35299;&#20915;&#20256;&#32479;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21306;&#20998;&#20302;&#38454;&#21644;&#39640;&#38454;&#22270;&#32467;&#26500;&#19978;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21487;&#20197;&#27169;&#25311;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;&#35768;&#22810;&#37325;&#35201;&#30340;&#24212;&#29992;&#20013;&#33258;&#28982;&#21457;&#29983;&#12290;&#36825;&#20123;&#24212;&#29992;&#36890;&#24120;&#21487;&#20197;&#24402;&#32435;&#20026;&#26631;&#20934;&#30340;&#22270;&#23398;&#20064;&#20219;&#21153;&#65292;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#26159;&#23398;&#20064;&#20302;&#32500;&#22270;&#34920;&#31034;&#12290;&#30446;&#21069;&#65292;&#22312;&#22270;&#23884;&#20837;&#26041;&#27861;&#20013;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#37051;&#22495;&#32858;&#21512;&#19979;&#30340;GNNs&#22312;&#21306;&#20998;&#39640;&#38454;&#22270;&#32467;&#26500;&#21644;&#20302;&#38454;&#22270;&#32467;&#26500;&#26041;&#38754;&#65292;&#20855;&#26377;&#26377;&#38480;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;&#20026;&#20102;&#25429;&#25417;&#39640;&#38454;&#32467;&#26500;&#65292;&#30740;&#31350;&#20154;&#21592;&#36716;&#32780;&#20351;&#29992;&#22270;&#26696;&#65292;&#21457;&#23637;&#20102;&#22522;&#20110;&#22270;&#26696;&#30340;GNNs&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#22270;&#26696;&#30340;GNNs&#22312;&#39640;&#38454;&#32467;&#26500;&#19978;&#20173;&#28982;&#23384;&#22312;&#36739;&#20302;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#19978;&#36848;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Motif&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MGNN&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#39640;&#38454;&#32467;&#26500;&#65292;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#22270;&#26696;&#20887;&#20313;&#24615;&#26368;&#23567;&#21270;&#31639;&#23376;&#21644;&#21333;&#23556;&#22270;&#26696;&#20114;&#34917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs can model complicated interactions between entities, which naturally emerge in many important applications. These applications can often be cast into standard graph learning tasks, in which a crucial step is to learn low-dimensional graph representations. Graph neural networks (GNNs) are currently the most popular model in graph embedding approaches. However, standard GNNs in the neighborhood aggregation paradigm suffer from limited discriminative power in distinguishing \emph{high-order} graph structures as opposed to \emph{low-order} structures. To capture high-order structures, researchers have resorted to motifs and developed motif-based GNNs. However, existing motif-based GNNs still often suffer from less discriminative power on high-order structures. To overcome the above limitations, we propose Motif Graph Neural Network (MGNN), a novel framework to better capture high-order structures, hinging on our proposed motif redundancy minimization operator and injective motif com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40657;&#30418;NLP&#20998;&#31867;&#22120;&#25915;&#20987;&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#35789;&#36873;&#25321;&#21644;&#36138;&#23146;&#25628;&#32034;&#31639;&#27861;&#36827;&#34892;&#35789;&#26367;&#25442;&#65292;&#35299;&#20915;&#20102;&#24433;&#21709;NLP&#39046;&#22495;&#20256;&#32479;&#22270;&#20687;&#25915;&#20987;&#26041;&#27861;&#19981;&#36866;&#29992;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2112.11660</link><description>&lt;p&gt;
&#19968;&#20010;&#40657;&#30418;NLP&#20998;&#31867;&#22120;&#25915;&#20987;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Black-box NLP Classifier Attacker. (arXiv:2112.11660v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.11660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40657;&#30418;NLP&#20998;&#31867;&#22120;&#25915;&#20987;&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#35789;&#36873;&#25321;&#21644;&#36138;&#23146;&#25628;&#32034;&#31639;&#27861;&#36827;&#34892;&#35789;&#26367;&#25442;&#65292;&#35299;&#20915;&#20102;&#24433;&#21709;NLP&#39046;&#22495;&#20256;&#32479;&#22270;&#20687;&#25915;&#20987;&#26041;&#27861;&#19981;&#36866;&#29992;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#24050;&#32463;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#21508;&#31181;&#30740;&#31350;&#24050;&#32463;&#26174;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#33030;&#24369;&#24615;&#12290;&#20197;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20026;&#20363;&#65292;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#34987;&#19968;&#20010;&#19982;&#21407;&#22987;&#25991;&#26412;&#39640;&#24230;&#30456;&#20284;&#30340;&#12289;&#32463;&#36807;&#20180;&#32454;&#20462;&#25913;&#30340;&#25991;&#26412;&#25152;&#36855;&#24785;&#12290;&#26681;&#25454;&#20043;&#21069;&#30340;&#30740;&#31350;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#22270;&#20687;&#39046;&#22495;&#65307;&#19982;&#22270;&#20687;&#23545;&#25239;&#25915;&#20987;&#19981;&#21516;&#65292;&#25991;&#26412;&#20197;&#31163;&#25955;&#24207;&#21015;&#34920;&#31034;&#65292;&#20256;&#32479;&#30340;&#22270;&#20687;&#25915;&#20987;&#26041;&#27861;&#22312;NLP&#39046;&#22495;&#19981;&#36866;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#35789;&#32423;NLP&#24773;&#24863;&#20998;&#31867;&#22120;&#25915;&#20987;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;&#35789;&#36873;&#25321;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#36138;&#23146;&#25628;&#32034;&#31639;&#27861;&#36827;&#34892;&#35789;&#26367;&#25442;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;...
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have a wide range of applications in solving various real-world tasks and have achieved satisfactory results, in domains such as computer vision, image classification, and natural language processing. Meanwhile, the security and robustness of neural networks have become imperative, as diverse researches have shown the vulnerable aspects of neural networks. Case in point, in Natural language processing tasks, the neural network may be fooled by an attentively modified text, which has a high similarity to the original one. As per previous research, most of the studies are focused on the image domain; Different from image adversarial attacks, the text is represented in a discrete sequence, traditional image attack methods are not applicable in the NLP field. In this paper, we propose a word-level NLP sentiment classifier attack model, which includes a self-attention mechanism-based word selection method and a greedy search algorithm for word substitution. We experimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20108;&#38454;&#38236;&#20687;&#19979;&#38477;&#65288;MD2&#65289;&#21160;&#21147;&#23398;&#65292;&#22312;&#21338;&#24328;&#20013;&#25910;&#25947;&#21040;&#38750;&#20005;&#26684;&#21464;&#20998;&#31283;&#23450;&#29366;&#24577;&#65288;VSS&#65289;&#65292;&#19981;&#38656;&#20351;&#29992;&#24120;&#35265;&#25216;&#24039;&#65292;&#21516;&#26102;&#20855;&#26377;&#26080;&#36951;&#25022;&#21644;&#25351;&#25968;&#25910;&#25947;&#36895;&#29575;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#23548;&#20986;&#35768;&#22810;&#26032;&#30340;&#36830;&#32493;&#26102;&#38388;&#21407;&#22987;&#31354;&#38388;&#21160;&#21147;&#23398;&#65292;&#24182;&#19988;&#36890;&#36807;&#38543;&#26426;&#36924;&#36817;&#25216;&#26415;&#23545;&#35266;&#27979;&#26377;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#31163;&#25955;&#26102;&#38388;MD2&#25552;&#20379;&#20102;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2111.09982</link><description>&lt;p&gt;
&#20108;&#38454;&#38236;&#20687;&#19979;&#38477;&#65306;&#36229;&#36234;&#24179;&#22343;&#21644;&#25240;&#25187;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Second-Order Mirror Descent: Convergence in Games Beyond Averaging and Discounting. (arXiv:2111.09982v4 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.09982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20108;&#38454;&#38236;&#20687;&#19979;&#38477;&#65288;MD2&#65289;&#21160;&#21147;&#23398;&#65292;&#22312;&#21338;&#24328;&#20013;&#25910;&#25947;&#21040;&#38750;&#20005;&#26684;&#21464;&#20998;&#31283;&#23450;&#29366;&#24577;&#65288;VSS&#65289;&#65292;&#19981;&#38656;&#20351;&#29992;&#24120;&#35265;&#25216;&#24039;&#65292;&#21516;&#26102;&#20855;&#26377;&#26080;&#36951;&#25022;&#21644;&#25351;&#25968;&#25910;&#25947;&#36895;&#29575;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#23548;&#20986;&#35768;&#22810;&#26032;&#30340;&#36830;&#32493;&#26102;&#38388;&#21407;&#22987;&#31354;&#38388;&#21160;&#21147;&#23398;&#65292;&#24182;&#19988;&#36890;&#36807;&#38543;&#26426;&#36924;&#36817;&#25216;&#26415;&#23545;&#35266;&#27979;&#26377;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#31163;&#25955;&#26102;&#38388;MD2&#25552;&#20379;&#20102;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36830;&#32493;&#26102;&#38388;&#21338;&#24328;&#29702;&#35770;&#38236;&#20687;&#19979;&#38477;&#65288;MD&#65289;&#21160;&#24577;&#30340;&#20108;&#38454;&#25193;&#23637;&#65292;&#31216;&#20026;MD2&#65292;&#20854;&#35777;&#26126;&#20102;&#22312;&#19981;&#20351;&#29992;&#24120;&#35265;&#30340;&#36741;&#21161;&#25216;&#26415;&#22914;&#26102;&#38388;&#24179;&#22343;&#25110;&#25240;&#25187;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#20165;&#20165;&#65288;&#20294;&#19981;&#19968;&#23450;&#26159;&#20005;&#26684;&#30340;&#65289;&#21464;&#20998;&#31283;&#23450;&#29366;&#24577;&#65288;VSS&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MD2&#22312;&#31245;&#20316;&#20462;&#25913;&#21518;&#21487;&#20197;&#20139;&#21463;&#26080;&#36951;&#25022;&#21644;&#25351;&#25968;&#25910;&#25947;&#36895;&#29575;&#65292;&#26397;&#30528;&#24378;VSS&#30340;&#26041;&#21521;&#12290;MD2&#36824;&#21487;&#20197;&#29992;&#20110;&#23548;&#20986;&#35768;&#22810;&#26032;&#39062;&#30340;&#36830;&#32493;&#26102;&#38388;&#21407;&#22987;&#31354;&#38388;&#21160;&#21147;&#23398;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#36924;&#36817;&#25216;&#26415;&#25552;&#20379;&#20102;&#31163;&#25955;&#26102;&#38388;MD2&#22312;&#35266;&#27979;&#26377;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#25910;&#25947;&#21040;&#20869;&#37096;&#20165;&#20165;VSS&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#20026;&#20102;&#35828;&#26126;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36873;&#23450;&#30340;&#27169;&#25311;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a second-order extension of the continuous-time game-theoretic mirror descent (MD) dynamics, referred to as MD2, which provably converges to mere (but not necessarily strict) variationally stable states (VSS) without using common auxiliary techniques such as time-averaging or discounting. We show that MD2 enjoys no-regret as well as an exponential rate of convergence towards strong VSS upon a slight modification. MD2 can also be used to derive many novel continuous-time primal-space dynamics. We then use stochastic approximation techniques to provide a convergence guarantee of discrete-time MD2 with noisy observations towards interior mere VSS. Selected simulations are provided to illustrate our results.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;TND-NAS&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#24494;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#21644;&#22810;&#30446;&#26631;NAS&#30340;&#20860;&#23481;&#24615;&#65292;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#23454;&#29616;&#39640;&#25928;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2111.03892</link><description>&lt;p&gt;
&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#39640;&#25928;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Efficient Multi-objective Neural Architecture Search Framework via Policy Gradient Algorithm. (arXiv:2111.03892v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.03892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;TND-NAS&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#24494;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#21644;&#22810;&#30446;&#26631;NAS&#30340;&#20860;&#23481;&#24615;&#65292;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#23454;&#29616;&#39640;&#25928;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#26550;&#26500;&#25628;&#32034;&#24050;&#25104;&#20026;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#39046;&#22495;&#30340;&#20027;&#27969;&#30740;&#31350;&#35838;&#39064;&#65292;&#30456;&#23545;&#20110;&#26089;&#26399;&#30340;EA-based&#21644;RL-based&#26041;&#27861;&#65292;&#20854;&#39640;&#25928;&#29575;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24050;&#19981;&#20877;&#33021;&#22815;&#33258;&#28982;&#22320;&#24212;&#23545;&#19981;&#21487;&#24494;&#21442;&#25968;&#65292;&#22914;&#33021;&#28304;&#21644;&#36164;&#28304;&#21463;&#38480;&#25928;&#29575;&#31561;&#12290;&#38024;&#23545;&#22810;&#30446;&#26631;NAS&#39046;&#22495;&#30340;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#23545;&#27599;&#20010;&#20505;&#36873;&#26550;&#26500;&#36827;&#34892;&#21807;&#19968;&#30340;&#20248;&#21270;&#65292;&#22240;&#27492;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TND-NAS&#65292;&#23427;&#20855;&#26377;&#19981;&#21487;&#24494;&#21442;&#25968;&#30340;&#20860;&#23481;&#24615;&#21644;&#19981;&#21516;iable NAS&#26694;&#26550;&#20013;&#30340;&#39640;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;TND-NAS&#22312;&#25628;&#32034;&#25928;&#29575;&#21644;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable architecture search has gradually become the mainstream research topic in the field of Neural Architecture Search (NAS) for its high efficiency compared with the early NAS (EA-based, RL-based) methods. Recent differentiable NAS also aims at further improving the search performance and reducing the GPU-memory consumption. However, these methods are no longer naturally capable of tackling the non-differentiable objectives, e.g., energy, resource-constrained efficiency, and other metrics, let alone the multi-objective search demands. Researches in the multi-objective NAS field target this but requires vast computational resources cause of the sole optimization of each candidate architecture. In light of this discrepancy, we propose the TND-NAS, which is with the merits of the high efficiency in differentiable NAS framework and the compatibility among non-differentiable metrics in Multi-objective NAS. Under the differentiable NAS framework, with the continuous relaxation of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20559;&#24046;&#25968;&#25454;&#25439;&#22833;&#30340;&#36890;&#29992;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#24212;&#29992;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#29702;&#35770;&#25552;&#20379;&#20102;&#25903;&#25345;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#28789;&#27963;&#30340;&#31639;&#27861;&#21644;&#22788;&#29702;&#32467;&#26500;&#24615;&#38646;&#20803;&#32032;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#21644;&#26696;&#20363;&#30740;&#31350;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#24212;&#29992;&#24191;&#27867;&#24615;&#12290;</title><link>http://arxiv.org/abs/2110.05674</link><description>&lt;p&gt;
&#20559;&#24046;&#30697;&#38453;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Deviance Matrix Factorization. (arXiv:2110.05674v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.05674
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20559;&#24046;&#25968;&#25454;&#25439;&#22833;&#30340;&#36890;&#29992;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#24212;&#29992;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#29702;&#35770;&#25552;&#20379;&#20102;&#25903;&#25345;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#28789;&#27963;&#30340;&#31639;&#27861;&#21644;&#22788;&#29702;&#32467;&#26500;&#24615;&#38646;&#20803;&#32032;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#21644;&#26696;&#20363;&#30740;&#31350;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#24212;&#29992;&#24191;&#27867;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#29992;&#20110;&#20559;&#24046;&#25968;&#25454;&#25439;&#22833;&#30340;&#36890;&#29992;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#23558;&#26222;&#36941;&#23384;&#22312;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#25193;&#23637;&#21040;&#24179;&#26041;&#35823;&#24046;&#25439;&#22833;&#20043;&#22806;&#12290;&#34429;&#28982;&#20043;&#21069;&#24050;&#32463;&#26377;&#31867;&#20284;&#30340;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLM&#65289;&#30340;&#32463;&#20856;&#32479;&#35745;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#26465;&#30446;&#26435;&#37325;&#26469;&#22788;&#29702;&#32467;&#26500;&#24615;&#38646;&#20803;&#32032;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#35843;&#25972;GLM&#29702;&#35770;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#25903;&#25345;&#36825;&#20123;&#20998;&#35299;&#65306;&#65288;i&#65289;&#22312;GLM&#35774;&#32622;&#19979;&#26174;&#31034;&#24378;&#19968;&#33268;&#24615;&#65292;&#65288;ii&#65289;&#36890;&#36807;&#24191;&#20041;Hosmer-Lemeshow&#26816;&#39564;&#26816;&#39564;&#25152;&#36873;&#25321;&#25351;&#25968;&#26063;&#20998;&#24067;&#30340;&#36866;&#24212;&#24615;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#36890;&#36807;&#26368;&#22823;&#29305;&#24449;&#20540;&#38388;&#38548;&#27861;&#30830;&#23450;&#20998;&#35299;&#30340;&#31209;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25903;&#25345;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#27169;&#25311;&#30740;&#31350;&#65292;&#35780;&#20272;&#23545;&#20998;&#35299;&#20551;&#35774;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#20687;&#20154;&#33080;&#35782;&#21035;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#32593;&#32476;&#20998;&#26512;&#21644;&#29983;&#29289;&#21307;&#23398;&#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate a general matrix factorization for deviance-based data losses, extending the ubiquitous singular value decomposition beyond squared error loss. While similar approaches have been explored before, our method leverages classical statistical methodology from generalized linear models (GLMs) and provides an efficient algorithm that is flexible enough to allow for structural zeros via entry weights. Moreover, by adapting results from GLM theory, we provide support for these decompositions by (i) showing strong consistency under the GLM setup, (ii) checking the adequacy of a chosen exponential family via a generalized Hosmer-Lemeshow test, and (iii) determining the rank of the decomposition via a maximum eigenvalue gap method. To further support our findings, we conduct simulation studies to assess robustness to decomposition assumptions and extensive case studies using benchmark datasets from image face recognition, natural language processing, network analysis, and biomedica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#65288;NETS&#65289;&#39044;&#27979;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;NETS-ImpGAN&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#35757;&#32451;&#19981;&#23436;&#25972;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#20102;&#22270;&#26102;&#24207;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2110.02271</link><description>&lt;p&gt;
&#20855;&#26377;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Networked Time Series Prediction with Incomplete Data. (arXiv:2110.02271v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.02271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#65288;NETS&#65289;&#39044;&#27979;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;NETS-ImpGAN&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#35757;&#32451;&#19981;&#23436;&#25972;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#20102;&#22270;&#26102;&#24207;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#65288;NETS&#65289;&#26159;&#32473;&#23450;&#22270;&#19978;&#30340;&#19968;&#32452;&#26102;&#38388;&#24207;&#21015;&#65292;&#27599;&#20010;&#33410;&#28857;&#23545;&#24212;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#12290;&#23427;&#22312;&#26234;&#33021;&#20132;&#36890;&#12289;&#29615;&#22659;&#30417;&#27979;&#21644;&#31227;&#21160;&#32593;&#32476;&#31649;&#29702;&#31561;&#39046;&#22495;&#37117;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#26159;&#22522;&#20110;&#21382;&#21490;&#20540;&#21644;&#24213;&#23618;&#22270;&#26469;&#39044;&#27979;NETS&#30340;&#26410;&#26469;&#20540;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#38656;&#35201;&#23436;&#25972;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#20256;&#24863;&#22120;&#25925;&#38556;&#12289;&#19981;&#23436;&#20840;&#24863;&#30693;&#35206;&#30422;&#31561;&#21407;&#22240;&#65292;&#25968;&#25454;&#32570;&#22833;&#26159;&#24120;&#35265;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;NETS&#39044;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;NETS-ImpGAN&#65292;&#19968;&#31181;&#21487;&#20197;&#22312;&#21382;&#21490;&#21644;&#26410;&#26469;&#30340;&#32570;&#22833;&#20540;&#19978;&#35757;&#32451;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#22270;&#26102;&#24207;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
A networked time series (NETS) is a family of time series on a given graph, one for each node. It has found a wide range of applications from intelligent transportation, environment monitoring to mobile network management. An important task in such applications is to predict the future values of a NETS based on its historical values and the underlying graph. Most existing methods require complete data for training. However, in real-world scenarios, it is not uncommon to have missing data due to sensor malfunction, incomplete sensing coverage, etc. In this paper, we study the problem of NETS prediction with incomplete data. We propose NETS-ImpGAN, a novel deep learning framework that can be trained on incomplete data with missing values in both history and future. Furthermore, we propose novel Graph Temporal Attention Networks by incorporating the attention mechanism to capture both inter-time series correlations and temporal correlations. We conduct extensive experiments on three real-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65288;$AggSLC$&#65289;&#65292;&#20174;&#20247;&#21253;&#25968;&#25454;&#20013;&#25512;&#26029;&#39034;&#24207;&#26631;&#31614;&#30340;&#30495;&#23454;&#20540;&#65292;&#20197;&#35299;&#20915;&#39034;&#24207;&#26631;&#31614;&#27719;&#24635;&#20219;&#21153;&#20013;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2109.04470</link><description>&lt;p&gt;
&#20174;&#20247;&#21253;&#25968;&#25454;&#20013;&#21457;&#29616;&#39034;&#24207;&#26631;&#31614;&#20013;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Truth Discovery in Sequence Labels from Crowds. (arXiv:2109.04470v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.04470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65288;$AggSLC$&#65289;&#65292;&#20174;&#20247;&#21253;&#25968;&#25454;&#20013;&#25512;&#26029;&#39034;&#24207;&#26631;&#31614;&#30340;&#30495;&#23454;&#20540;&#65292;&#20197;&#35299;&#20915;&#39034;&#24207;&#26631;&#31614;&#27719;&#24635;&#20219;&#21153;&#20013;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#27880;&#36136;&#37327;&#21644;&#25968;&#37327;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#39034;&#24207;&#26631;&#31614;&#23398;&#20064;&#20219;&#21153;&#30340;&#24615;&#33021;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#38599;&#20323;&#39046;&#22495;&#19987;&#23478;&#23545;&#35821;&#26009;&#24211;&#36827;&#34892;&#26631;&#27880;&#22312;&#26102;&#38388;&#21644;&#37329;&#38065;&#19978;&#38750;&#24120;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#24320;&#22987;&#20351;&#29992;&#20247;&#21253;&#24179;&#21488;&#65288;&#22914;Amazon Mechanical Turk&#65289;&#26469;&#36741;&#21161;&#26631;&#27880;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#25910;&#38598;&#30340;&#26631;&#27880;&#23481;&#26131;&#21463;&#21040;&#20247;&#21253;&#24037;&#20154;&#19987;&#19994;&#30693;&#35782;&#19981;&#36275;&#32780;&#20135;&#29983;&#30340;&#20154;&#20026;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#26631;&#27880;&#27719;&#24635;&#26041;&#27861;&#20551;&#35774;&#26631;&#27880;&#26159;&#29420;&#31435;&#30340;&#65292;&#22240;&#27492;&#22312;&#22788;&#29702;&#20855;&#26377;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#30340;&#39034;&#24207;&#26631;&#31614;&#27719;&#24635;&#20219;&#21153;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#24037;&#20316;&#32773;&#25552;&#20379;&#30340;&#26631;&#27880;&#26469;&#25512;&#26029;&#39034;&#24207;&#26631;&#31614;&#30340;&#30495;&#23454;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#39034;&#24207;&#26631;&#31614;&#27719;&#24635;&#26041;&#27861;&#65288;$AggSLC$&#65289;&#21516;&#26102;&#32771;&#34385;&#20102;&#39034;&#24207;&#26631;&#31614;&#20219;&#21153;&#30340;&#29305;&#24449;&#12289;&#24037;&#20316;&#32773;&#21487;&#20449;&#24230;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotation quality and quantity positively affect the learning performance of sequence labeling, a vital task in Natural Language Processing. Hiring domain experts to annotate a corpus is very costly in terms of money and time. Crowdsourcing platforms, such as Amazon Mechanical Turk (AMT), have been deployed to assist in this purpose. However, the annotations collected this way are prone to human errors due to the lack of expertise of the crowd workers. Existing literature in annotation aggregation assumes that annotations are independent and thus faces challenges when handling the sequential label aggregation tasks with complex dependencies. To conquer the challenges, we propose an optimization-based method that infers the ground truth labels using annotations provided by workers for sequential labeling tasks. The proposed Aggregation method for Sequential Labels from Crowds ($AggSLC$) jointly considers the characteristics of sequential labeling tasks, workers' reliabilities, and adva
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#20010;&#26174;&#24335;&#36229;&#21442;&#25968;&#39044;&#27979;&#20989;&#25968;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#26597;&#35810;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2107.02378</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#30340;&#26174;&#24335;&#36229;&#21442;&#25968;&#39044;&#27979;&#27169;&#22411;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning an Explicit Hyperparameter Prediction Function Conditioned on Tasks. (arXiv:2107.02378v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.02378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#20010;&#26174;&#24335;&#36229;&#21442;&#25968;&#39044;&#27979;&#20989;&#25968;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#26597;&#35810;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#26426;&#22120;&#23398;&#20064;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#20869;&#22312;&#30340;&#39044;&#27979;&#35268;&#21017;&#20197;&#39044;&#27979;&#26032;&#30340;&#26597;&#35810;&#25968;&#25454;&#26631;&#31614;&#19981;&#21516;&#65292;&#20803;&#23398;&#20064;&#26088;&#22312;&#20174;&#35266;&#23519;&#21040;&#30340;&#20219;&#21153;&#20013;&#23398;&#20064;&#26426;&#22120;&#23398;&#20064;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#20415;&#36890;&#36807;&#21033;&#29992;&#20803;&#23398;&#20064;&#30340;&#23398;&#20064;&#26041;&#27861;&#23545;&#26032;&#30340;&#26597;&#35810;&#20219;&#21153;&#36827;&#34892;&#27867;&#21270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#23398;&#20064;&#26041;&#27861;&#35299;&#37322;&#20026;&#23398;&#20064;&#19968;&#20010;&#26174;&#24335;&#36229;&#21442;&#25968;&#39044;&#27979;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#30001;&#25152;&#26377;&#35757;&#32451;&#20219;&#21153;&#20849;&#20139;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#20010;&#20989;&#25968;&#34987;&#34920;&#31034;&#20026;&#19968;&#20010;&#21442;&#25968;&#21270;&#20989;&#25968;&#65292;&#31216;&#20026;&#20803;&#23398;&#20064;&#22120;&#65292;&#23558;&#35757;&#32451;/&#27979;&#35797;&#20219;&#21153;&#26144;&#23556;&#21040;&#20854;&#21512;&#36866;&#30340;&#36229;&#21442;&#25968;&#35774;&#32622;&#65292;&#25552;&#21462;&#33258;&#31216;&#20026;&#20803;&#23398;&#20064;&#26426;&#30340;&#39044;&#20808;&#25351;&#23450;&#30340;&#20989;&#25968;&#38598;&#12290;&#36825;&#31181;&#35774;&#32622;&#20445;&#35777;&#20102;&#20803;&#23398;&#20064;&#30340;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#28789;&#27963;&#22320;&#36866;&#24212;&#19981;&#21516;&#30340;&#26597;&#35810;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#20687;&#35768;&#22810;&#29616;&#26377;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#19968;&#26679;&#65292;&#21482;&#33719;&#24471;&#22266;&#23450;&#30340;&#36229;&#21442;&#25968;&#65292;&#36866;&#24212;&#24615;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta learning has attracted much attention recently in machine learning community. Contrary to conventional machine learning aiming to learn inherent prediction rules to predict labels for new query data, meta learning aims to learn the learning methodology for machine learning from observed tasks, so as to generalize to new query tasks by leveraging the meta-learned learning methodology. In this study, we interpret such learning methodology as learning an explicit hyper-parameter prediction function shared by all training tasks. Specifically, this function is represented as a parameterized function called meta-learner, mapping from a training/test task to its suitable hyper-parameter setting, extracted from a pre-specified function set called meta learning machine. Such setting guarantees that the meta-learned learning methodology is able to flexibly fit diverse query tasks, instead of only obtaining fixed hyper-parameters by many current meta learning methods, with less adaptability 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#20855;&#26377;&#21453;&#39304;&#20449;&#24687;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#24773;&#22659;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#35266;&#23519;&#26368;&#20339;&#21160;&#20316;&#24182;&#26368;&#23567;&#21270;&#21518;&#24724;&#26469;&#20248;&#21270;&#20915;&#31574;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2106.14015</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#36870;&#20248;&#21270;&#65306;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contextual Inverse Optimization: Offline and Online Learning. (arXiv:2106.14015v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.14015
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#20855;&#26377;&#21453;&#39304;&#20449;&#24687;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#24773;&#22659;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#35266;&#23519;&#26368;&#20339;&#21160;&#20316;&#24182;&#26368;&#23567;&#21270;&#21518;&#24724;&#26469;&#20248;&#21270;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#21453;&#39304;&#20449;&#24687;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#24773;&#22659;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#35266;&#23519;&#21040;&#30340;&#19981;&#26159;&#25439;&#22833;&#65292;&#32780;&#26159;&#19968;&#20010;&#20855;&#26377;&#23436;&#20840;&#20102;&#35299;&#30446;&#26631;&#20989;&#25968;&#30340;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#23558;&#20250;&#37319;&#21462;&#30340;&#26368;&#20339;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#21518;&#24724;&#65292;&#21518;&#24724;&#23450;&#20041;&#20026;&#25105;&#20204;&#30340;&#25439;&#22833;&#19982;&#20840;&#30693;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#20135;&#29983;&#30340;&#25439;&#22833;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22312;&#31163;&#32447;&#24773;&#22659;&#20013;&#65292;&#20915;&#31574;&#32773;&#21487;&#20197;&#33719;&#24471;&#36807;&#21435;&#26102;&#26399;&#30340;&#20449;&#24687;&#24182;&#38656;&#35201;&#20570;&#20986;&#19968;&#20010;&#20915;&#31574;&#65292;&#32780;&#22312;&#22312;&#32447;&#24773;&#22659;&#20013;&#65292;&#20915;&#31574;&#32773;&#26681;&#25454;&#27599;&#20010;&#26102;&#26399;&#30340;&#26032;&#19968;&#32452;&#21487;&#34892;&#21160;&#20316;&#21644;&#24773;&#22659;&#20989;&#25968;&#26469;&#21160;&#24577;&#20248;&#21270;&#20915;&#31574;&#12290;&#23545;&#20110;&#31163;&#32447;&#24773;&#22659;&#65292;&#25105;&#20204;&#23558;&#26368;&#20248;&#26497;&#23567;&#26497;&#22823;&#31574;&#30053;&#29305;&#24449;&#21270;&#65292;&#30830;&#23450;&#20102;&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#20135;&#29983;&#30340;&#20449;&#24687;&#30340;&#22522;&#30784;&#20960;&#20309;&#24418;&#29366;&#30340;&#20989;&#25968;&#34920;&#29616;&#12290;&#22312;&#22312;&#32447;&#24773;&#22659;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#20960;&#20309;&#29305;&#24449;&#26469;&#20248;&#21270;&#32047;&#31215;&#21518;&#24724;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#31639;&#27861;&#26469;&#25214;&#21040;&#32047;&#31215;&#21518;&#24724;&#30340;&#26368;&#23567;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problems of offline and online contextual optimization with feedback information, where instead of observing the loss, we observe, after-the-fact, the optimal action an oracle with full knowledge of the objective function would have taken. We aim to minimize regret, which is defined as the difference between our losses and the ones incurred by an all-knowing oracle. In the offline setting, the decision-maker has information available from past periods and needs to make one decision, while in the online setting, the decision-maker optimizes decisions dynamically over time based a new set of feasible actions and contextual functions in each period. For the offline setting, we characterize the optimal minimax policy, establishing the performance that can be achieved as a function of the underlying geometry of the information induced by the data. In the online setting, we leverage this geometric characterization to optimize the cumulative regret. We develop an algorithm that y
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#20195;&#29702;&#22240;&#26524;&#23398;&#20064;&#65288;PCL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#27835;&#30103;&#23545;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#26500;&#24314;&#27835;&#30103;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#35813;&#27169;&#22411;&#22312;&#32473;&#23450;&#20195;&#29702;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27835;&#30103;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;PCL&#21487;&#20197;&#20445;&#35777;&#24674;&#22797;&#30495;&#23454;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#29305;&#24449;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#65288;DFPV&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#39640;&#32500;&#21644;&#38750;&#32447;&#24615;&#22797;&#26434;&#20851;&#31995;&#30340;&#24773;&#20917;&#65292;&#24182;&#34920;&#26126;DFPV&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;PCL&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2106.03907</link><description>&lt;p&gt;
&#28145;&#24230;&#20195;&#29702;&#22240;&#26524;&#23398;&#20064;&#21450;&#20854;&#22312;&#28151;&#28102;&#36172;&#21338;&#31574;&#30053;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Proxy Causal Learning and its Application to Confounded Bandit Policy Evaluation. (arXiv:2106.03907v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.03907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#20195;&#29702;&#22240;&#26524;&#23398;&#20064;&#65288;PCL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#27835;&#30103;&#23545;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#26500;&#24314;&#27835;&#30103;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#35813;&#27169;&#22411;&#22312;&#32473;&#23450;&#20195;&#29702;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27835;&#30103;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;PCL&#21487;&#20197;&#20445;&#35777;&#24674;&#22797;&#30495;&#23454;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#29305;&#24449;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#65288;DFPV&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#39640;&#32500;&#21644;&#38750;&#32447;&#24615;&#22797;&#26434;&#20851;&#31995;&#30340;&#24773;&#20917;&#65292;&#24182;&#34920;&#26126;DFPV&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;PCL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#29702;&#22240;&#26524;&#23398;&#20064;&#65288;PCL&#65289;&#26159;&#19968;&#31181;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#26102;&#65292;&#21033;&#29992;&#20195;&#29702;&#65288;&#32467;&#26500;&#21270;&#20391;&#38754;&#20449;&#24687;&#65289;&#20272;&#35745;&#27835;&#30103;&#23545;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#30340;&#26041;&#27861;&#12290;&#36825;&#26159;&#36890;&#36807;&#20004;&#38454;&#27573;&#22238;&#24402;&#23454;&#29616;&#30340;&#65306;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#24314;&#27169;&#27835;&#30103;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#20851;&#31995;&#65307;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#27169;&#22411;&#26469;&#23398;&#20064;&#22312;&#32473;&#23450;&#20195;&#29702;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#19979;&#65292;&#27835;&#30103;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;PCL&#22312;&#21487;&#35782;&#21035;&#26465;&#20214;&#19979;&#20445;&#35777;&#24674;&#22797;&#30495;&#23454;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PCL&#26041;&#27861;&#65292;&#28145;&#24230;&#29305;&#24449;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#65288;DFPV&#65289;&#65292;&#20197;&#35299;&#20915;&#20195;&#29702;&#12289;&#27835;&#30103;&#21644;&#32467;&#26524;&#20026;&#39640;&#32500;&#19988;&#20855;&#26377;&#38750;&#32447;&#24615;&#22797;&#26434;&#20851;&#31995;&#30340;&#24773;&#20917;&#65292;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29305;&#24449;&#34920;&#31034;&#12290;&#25105;&#20204;&#34920;&#26126;DFPV&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#30340;PCL&#26041;&#27861;&#65292;&#21253;&#25324;&#28041;&#21450;&#39640;&#32500;&#22270;&#20687;&#25968;&#25454;&#30340;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;PCL&#30340;&#24212;&#29992;...
&lt;/p&gt;
&lt;p&gt;
Proxy causal learning (PCL) is a method for estimating the causal effect of treatments on outcomes in the presence of unobserved confounding, using proxies (structured side information) for the confounder. This is achieved via two-stage regression: in the first stage, we model relations among the treatment and proxies; in the second stage, we use this model to learn the effect of treatment on the outcome, given the context provided by the proxies. PCL guarantees recovery of the true causal effect, subject to identifiability conditions. We propose a novel method for PCL, the deep feature proxy variable method (DFPV), to address the case where the proxies, treatments, and outcomes are high-dimensional and have nonlinear complex relationships, as represented by deep neural network features. We show that DFPV outperforms recent state-of-the-art PCL methods on challenging synthetic benchmarks, including settings involving high dimensional image data. Furthermore, we show that PCL can be app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33322;&#28857;&#30340;&#36827;&#21270;&#31574;&#30053;&#24418;&#29366;&#21270;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#20013;&#40657;&#30418;&#26041;&#27861;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#20195;&#29702;&#30340;&#21160;&#21147;&#23398;&#65292;&#32467;&#21512;&#20248;&#21270;&#36807;&#31243;&#21152;&#36895;&#35757;&#32451;&#65292;&#24182;&#22312;Carla&#39550;&#39542;&#21644;UR5&#26426;&#26800;&#33218;&#27169;&#25311;&#22120;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20854;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2105.14639</link><description>&lt;p&gt;
&#20351;&#29992;&#33322;&#28857;&#30340;&#36827;&#21270;&#31574;&#30053;&#24418;&#29366;&#21270;&#31574;&#30053;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Shaped Policy Search for Evolutionary Strategies using Waypoints. (arXiv:2105.14639v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.14639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33322;&#28857;&#30340;&#36827;&#21270;&#31574;&#30053;&#24418;&#29366;&#21270;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#20013;&#40657;&#30418;&#26041;&#27861;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#20195;&#29702;&#30340;&#21160;&#21147;&#23398;&#65292;&#32467;&#21512;&#20248;&#21270;&#36807;&#31243;&#21152;&#36895;&#35757;&#32451;&#65292;&#24182;&#22312;Carla&#39550;&#39542;&#21644;UR5&#26426;&#26800;&#33218;&#27169;&#25311;&#22120;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20854;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#25913;&#36827;&#40657;&#30418;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#36827;&#21270;&#31574;&#30053;&#22312;&#24212;&#29992;&#20110;&#20855;&#26377;&#20013;&#38388;&#33322;&#28857;/&#23376;&#30446;&#26631;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26102;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#30001;&#20110;&#36827;&#21270;&#31574;&#30053;&#20855;&#26377;&#39640;&#24230;&#21487;&#24182;&#34892;&#21270;&#30340;&#29305;&#28857;&#65292;&#25105;&#20204;&#23558;&#19981;&#20165;&#25552;&#21462;&#26631;&#37327;&#32047;&#31215;&#22870;&#21169;&#65292;&#32780;&#26159;&#21033;&#29992;&#22312;&#25512;&#20986;/&#35780;&#20272;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#36712;&#36857;&#20013;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#26469;&#23398;&#20064;&#20195;&#29702;&#30340;&#21160;&#21147;&#23398;&#12290;&#28982;&#21518;&#65292;&#23558;&#23398;&#21040;&#30340;&#21160;&#21147;&#23398;&#29992;&#20110;&#20248;&#21270;&#36807;&#31243;&#20197;&#21152;&#36895;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;Carla&#39550;&#39542;&#21644;UR5&#26426;&#26800;&#33218;&#27169;&#25311;&#22120;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we try to improve exploration in Blackbox methods, particularly Evolution strategies (ES), when applied to Reinforcement Learning (RL) problems where intermediate waypoints/subgoals are available. Since Evolutionary strategies are highly parallelizable, instead of extracting just a scalar cumulative reward, we use the state-action pairs from the trajectories obtained during rollouts/evaluations, to learn the dynamics of the agent. The learnt dynamics are then used in the optimization procedure to speed-up training. Lastly, we show how our proposed approach is universally applicable by presenting results from experiments conducted on Carla driving and UR5 robotic arm simulators.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#26426;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24179;&#28369;&#25554;&#20540;&#30340;&#26041;&#24335;&#23558;&#27169;&#20223;&#23398;&#20064;&#21644;&#32431;&#32852;&#26426;&#24378;&#21270;&#23398;&#20064;&#32479;&#19968;&#36215;&#26469;&#12290;&#26694;&#26550;&#22260;&#32469;&#30528;&#19968;&#31181;&#34913;&#37327;&#34892;&#20026;&#31574;&#30053;&#19982;&#19987;&#23478;&#31574;&#30053;&#20559;&#31163;&#31243;&#24230;&#30340;&#24369;&#29256;&#26412;&#38598;&#20013;&#31995;&#25968;&#23637;&#24320;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#30740;&#31350;&#32773;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#31639;&#27861;&#35774;&#35745;&#30340;&#38382;&#39064;&#65306;&#33021;&#21542;&#24320;&#21457;&#20986;&#23454;&#29616;&#26368;&#23567;&#26497;&#22823;&#26368;&#20248;&#24615;&#30340;&#31639;&#27861;&#65311;</title><link>http://arxiv.org/abs/2103.12021</link><description>&lt;p&gt;
&#32852;&#26426;&#24378;&#21270;&#23398;&#20064;&#19982;&#27169;&#20223;&#23398;&#20064;&#30340;&#26725;&#26753;&#65306;&#19968;&#20010;&#24754;&#35266;&#30340;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism. (arXiv:2103.12021v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.12021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#26426;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24179;&#28369;&#25554;&#20540;&#30340;&#26041;&#24335;&#23558;&#27169;&#20223;&#23398;&#20064;&#21644;&#32431;&#32852;&#26426;&#24378;&#21270;&#23398;&#20064;&#32479;&#19968;&#36215;&#26469;&#12290;&#26694;&#26550;&#22260;&#32469;&#30528;&#19968;&#31181;&#34913;&#37327;&#34892;&#20026;&#31574;&#30053;&#19982;&#19987;&#23478;&#31574;&#30053;&#20559;&#31163;&#31243;&#24230;&#30340;&#24369;&#29256;&#26412;&#38598;&#20013;&#31995;&#25968;&#23637;&#24320;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#30740;&#31350;&#32773;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#31639;&#27861;&#35774;&#35745;&#30340;&#38382;&#39064;&#65306;&#33021;&#21542;&#24320;&#21457;&#20986;&#23454;&#29616;&#26368;&#23567;&#26497;&#22823;&#26368;&#20248;&#24615;&#30340;&#31639;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#26426;&#65288;&#25110;&#25209;&#27425;&#65289;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26088;&#22312;&#20174;&#22266;&#23450;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#20027;&#21160;&#25910;&#38598;&#25968;&#25454;&#12290;&#26681;&#25454;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#32452;&#25104;&#65292;&#20027;&#35201;&#20351;&#29992;&#20004;&#31181;&#26041;&#27861;&#65306;&#36866;&#29992;&#20110;&#19987;&#23478;&#25968;&#25454;&#38598;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#36890;&#24120;&#38656;&#35201;&#22343;&#21248;&#35206;&#30422;&#25968;&#25454;&#38598;&#30340;&#32431;&#32852;&#26426;&#24378;&#21270;&#23398;&#20064;&#12290;&#20174;&#23454;&#36341;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25968;&#25454;&#38598;&#36890;&#24120;&#20559;&#31163;&#36825;&#20004;&#20010;&#26497;&#31471;&#65292;&#24182;&#19988;&#36890;&#24120;&#20107;&#20808;&#19981;&#30693;&#36947;&#30830;&#20999;&#30340;&#25968;&#25454;&#32452;&#25104;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32852;&#26426;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#22312;&#25968;&#25454;&#32452;&#25104;&#30340;&#20004;&#20010;&#26497;&#31471;&#20043;&#38388;&#24179;&#28369;&#25554;&#20540;&#65292;&#20174;&#32780;&#32479;&#19968;&#20102;&#27169;&#20223;&#23398;&#20064;&#21644;&#32431;&#32852;&#26426;&#24378;&#21270;&#23398;&#20064;&#12290;&#26032;&#30340;&#26694;&#26550;&#22260;&#32469;&#19968;&#20010;&#24369;&#29256;&#26412;&#30340;&#38598;&#20013;&#31995;&#25968;&#23637;&#24320;&#65292;&#35813;&#31995;&#25968;&#34913;&#37327;&#20102;&#34892;&#20026;&#31574;&#30053;&#19982;&#19987;&#23478;&#31574;&#30053;&#20043;&#38388;&#30340;&#20559;&#31163;&#31243;&#24230;&#12290;&#22312;&#36825;&#20010;&#26032;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#31639;&#27861;&#35774;&#35745;&#30340;&#38382;&#39064;&#65306;&#33021;&#21542;&#24320;&#21457;&#20986;&#19968;&#31181;&#23454;&#29616;&#26368;&#23567;&#26497;&#22823;&#26368;&#20248;&#24615;&#30340;&#31639;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
Offline (or batch) reinforcement learning (RL) algorithms seek to learn an optimal policy from a fixed dataset without active data collection. Based on the composition of the offline dataset, two main categories of methods are used: imitation learning which is suitable for expert datasets and vanilla offline RL which often requires uniform coverage datasets. From a practical standpoint, datasets often deviate from these two extremes and the exact data composition is usually unknown a priori. To bridge this gap, we present a new offline RL framework that smoothly interpolates between the two extremes of data composition, hence unifying imitation learning and vanilla offline RL. The new framework is centered around a weak version of the concentrability coefficient that measures the deviation from the behavior policy to the expert policy alone.  Under this new framework, we further investigate the question on algorithm design: can one develop an algorithm that achieves a minimax optimal r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;&#30340;&#36845;&#20195;&#21644;&#31614;&#21517;&#30340;&#20195;&#25968;&#24615;&#36136;&#65292;&#24182;&#20171;&#32461;&#20102;&#19977;&#20010;&#31867;&#20284;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#38750;&#32447;&#24615;&#21464;&#25442;&#12290;</title><link>http://arxiv.org/abs/2012.04597</link><description>&lt;p&gt;
&#24191;&#20041;&#30340;&#36845;&#20195;&#21644;&#31614;&#21517;
&lt;/p&gt;
&lt;p&gt;
Generalized iterated-sums signatures. (arXiv:2012.04597v3 [math.RA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.04597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;&#30340;&#36845;&#20195;&#21644;&#31614;&#21517;&#30340;&#20195;&#25968;&#24615;&#36136;&#65292;&#24182;&#20171;&#32461;&#20102;&#19977;&#20010;&#31867;&#20284;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#38750;&#32447;&#24615;&#21464;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#36845;&#20195;&#21644;&#31614;&#21517;&#30340;&#24191;&#20041;&#29256;&#26412;&#30340;&#20195;&#25968;&#24615;&#36136;&#65292;&#21463;&#21040;F. Kir&#225;ly&#21644;H. Oberhauser&#20043;&#21069;&#30340;&#24037;&#20316;&#21551;&#21457;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#32771;&#34385;&#21518;&#32773;&#19978;&#30340;&#21464;&#24418;&#20934;&#28151;&#27927;&#20056;&#31215;&#65292;&#22914;&#20309;&#24674;&#22797;&#19982;&#24352;&#37327;&#20195;&#25968;&#19978;&#30340;&#30456;&#20851;&#32447;&#24615;&#26144;&#23556;&#30340;&#29305;&#24449;&#24615;&#36136;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19977;&#20010;&#38750;&#32447;&#24615;&#21464;&#25442;&#22312;&#36845;&#20195;&#21644;&#31614;&#21517;&#19978;&#65292;&#31867;&#20284;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#19968;&#20123;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the algebraic properties of a generalized version of the iterated-sums signature, inspired by previous work of F.~Kir\'aly and H.~Oberhauser. In particular, we show how to recover the character property of the associated linear map over the tensor algebra by considering a deformed quasi-shuffle product of words on the latter. We introduce three non-linear transformations on iterated-sums signatures, close in spirit to Machine Learning applications, and show some of their properties.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#26694;&#26550;&#12290;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#30446;&#26631;&#12289;&#26041;&#27861;&#23398;&#12289;&#20860;&#23481;&#30340;&#24378;&#21270;&#23398;&#20064;&#32972;&#26223;&#20197;&#21450;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#36801;&#31227;&#23398;&#20064;&#19982;&#20854;&#20182;&#30456;&#20851;&#20027;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2009.07888</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning in Deep Reinforcement Learning: A Survey. (arXiv:2009.07888v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.07888
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#26694;&#26550;&#12290;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#30446;&#26631;&#12289;&#26041;&#27861;&#23398;&#12289;&#20860;&#23481;&#30340;&#24378;&#21270;&#23398;&#20064;&#32972;&#26223;&#20197;&#21450;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#36801;&#31227;&#23398;&#20064;&#19982;&#20854;&#20182;&#30456;&#20851;&#20027;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#35299;&#20915;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#24378;&#21270;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#38500;&#20102;&#22312;&#26426;&#22120;&#20154;&#21644;&#28216;&#25103;&#31561;&#35832;&#22810;&#39046;&#22495;&#20013;&#20855;&#26377;&#33391;&#22909;&#21069;&#26223;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#36801;&#31227;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#21508;&#31181;&#25361;&#25112;&#30340;&#26041;&#27861;&#24050;&#32463;&#20986;&#29616;&#65292;&#36890;&#36807;&#20174;&#22806;&#37096;&#19987;&#19994;&#30693;&#35782;&#20013;&#36716;&#31227;&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#26368;&#20808;&#36827;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#26694;&#26550;&#65292;&#22312;&#27492;&#26694;&#26550;&#19979;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#30446;&#26631;&#12289;&#26041;&#27861;&#23398;&#12289;&#20860;&#23481;&#30340;&#24378;&#21270;&#23398;&#20064;&#32972;&#26223;&#20197;&#21450;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#36801;&#31227;&#23398;&#20064;&#19982;&#20854;&#20182;&#30456;&#20851;&#20027;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the efficiency and effectiveness of the learning process. In this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep reinforcement learning. Specifically, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under which we analyze their goals, methodologies, compatible reinforcement learning backbones, and practical applications. We also draw connections between transfer learning and other relevant topics 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22242;&#38431;&#20013;&#23398;&#20064;&#20999;&#25442;&#20195;&#29702;&#25511;&#21046;&#30340;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20195;&#29702;&#30340;&#31574;&#30053;&#21644;&#29615;&#22659;&#30340;&#36716;&#31227;&#27010;&#29575;&#65292;&#22312;&#19981;&#21516;&#33258;&#21160;&#21270;&#27700;&#24179;&#19979;&#20351;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#24037;&#20316;&#12290;&#35813;&#31639;&#27861;&#30340;&#24635;&#36951;&#25022;&#19982;&#26368;&#20339;&#20999;&#25442;&#31574;&#30053;&#30456;&#27604;&#26159;&#27425;&#32447;&#24615;&#30340;&#65292;&#24403;&#22810;&#20010;&#20195;&#29702;&#22242;&#38431;&#22312;&#30456;&#20284;&#29615;&#22659;&#20013;&#36816;&#34892;&#26102;&#65292;&#35813;&#31639;&#27861;&#20174;&#32500;&#25252;&#29615;&#22659;&#30340;&#20849;&#20139;&#32622;&#20449;&#30028;&#20013;&#33719;&#30410;&#21290;&#27973;&#12290;</title><link>http://arxiv.org/abs/2002.04258</link><description>&lt;p&gt;
&#36890;&#36807;2&#23618;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#23398;&#20064;&#22312;&#22242;&#38431;&#20013;&#20999;&#25442;&#20195;&#29702;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning to Switch Among Agents in a Team via 2-Layer Markov Decision Processes. (arXiv:2002.04258v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.04258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22242;&#38431;&#20013;&#23398;&#20064;&#20999;&#25442;&#20195;&#29702;&#25511;&#21046;&#30340;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20195;&#29702;&#30340;&#31574;&#30053;&#21644;&#29615;&#22659;&#30340;&#36716;&#31227;&#27010;&#29575;&#65292;&#22312;&#19981;&#21516;&#33258;&#21160;&#21270;&#27700;&#24179;&#19979;&#20351;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#24037;&#20316;&#12290;&#35813;&#31639;&#27861;&#30340;&#24635;&#36951;&#25022;&#19982;&#26368;&#20339;&#20999;&#25442;&#31574;&#30053;&#30456;&#27604;&#26159;&#27425;&#32447;&#24615;&#30340;&#65292;&#24403;&#22810;&#20010;&#20195;&#29702;&#22242;&#38431;&#22312;&#30456;&#20284;&#29615;&#22659;&#20013;&#36816;&#34892;&#26102;&#65292;&#35813;&#31639;&#27861;&#20174;&#32500;&#25252;&#29615;&#22659;&#30340;&#20849;&#20139;&#32622;&#20449;&#30028;&#20013;&#33719;&#30410;&#21290;&#27973;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#36890;&#24120;&#20197;&#23436;&#20840;&#33258;&#20027;&#30340;&#26041;&#24335;&#24037;&#20316;&#30340;&#20551;&#35774;&#19979;&#24320;&#21457;&#21644;&#35780;&#20272; - &#23427;&#20204;&#23558;&#37319;&#21462;&#25152;&#26377;&#34892;&#21160;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22312;&#20195;&#29702;&#20043;&#38388;&#20999;&#25442;&#25511;&#21046;&#65292;&#20351;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#33258;&#21160;&#21270;&#27700;&#24179;&#19979;&#24037;&#20316;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#27491;&#24335;&#23450;&#20041;&#20102;&#36890;&#36807;2&#23618;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#22312;&#22242;&#38431;&#20013;&#23398;&#20064;&#20999;&#25442;&#25511;&#21046;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20195;&#29702;&#30340;&#31574;&#30053;&#21644;&#29615;&#22659;&#30340;&#36716;&#31227;&#27010;&#29575;&#30340;&#19978;&#32622;&#20449;&#30028;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#25214;&#21040;&#19968;&#31995;&#21015;&#20999;&#25442;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#30456;&#23545;&#20110;&#26368;&#20339;&#20999;&#25442;&#31574;&#30053;&#30340;&#24635;&#36951;&#25022;&#22312;&#23398;&#20064;&#27493;&#39588;&#30340;&#25968;&#37327;&#19978;&#26159;&#27425;&#32447;&#24615;&#30340;&#65292;&#24182;&#19988;&#27599;&#24403;&#22810;&#20010;&#20195;&#29702;&#22242;&#38431;&#22312;&#30456;&#20284;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20174;&#32500;&#25252;&#29615;&#22659;&#30340;&#20849;&#20139;&#32622;&#20449;&#30028;&#20013;&#33719;&#24471;&#24456;&#22823;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning agents have been mostly developed and evaluated under the assumption that they will operate in a fully autonomous manner -- they will take all actions. In this work, our goal is to develop algorithms that, by learning to switch control between agents, allow existing reinforcement learning agents to operate under different automation levels. To this end, we first formally define the problem of learning to switch control among agents in a team via a 2-layer Markov decision process. Then, we develop an online learning algorithm that uses upper confidence bounds on the agents' policies and the environment's transition probabilities to find a sequence of switching policies. The total regret of our algorithm with respect to the optimal switching policy is sublinear in the number of learning steps and, whenever multiple teams of agents operate in a similar environment, our algorithm greatly benefits from maintaining shared confidence bounds for the environments' transit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#35201;&#26435;&#37325;&#31034;&#33539;&#30340;&#20803;&#36866;&#24212;&#24615;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#20808;&#21069;&#30693;&#35782;&#36827;&#34892;&#20998;&#37197;&#37325;&#35201;&#26435;&#37325;&#65292;&#23454;&#29616;&#20102;&#22312;&#20219;&#20309;&#30456;&#20851;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20351;&#26426;&#22120;&#20154;&#22312;&#22810;&#26679;&#21270;&#29615;&#22659;&#20219;&#21153;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#23569;&#37327;&#31034;&#33539;&#36866;&#24212;&#26410;&#30693;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/1911.10322</link><description>&lt;p&gt;
&#20351;&#29992;&#37325;&#35201;&#26435;&#37325;&#31034;&#33539;&#30340;&#20803;&#36866;&#24212;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta Adaptation using Importance Weighted Demonstrations. (arXiv:1911.10322v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1911.10322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#35201;&#26435;&#37325;&#31034;&#33539;&#30340;&#20803;&#36866;&#24212;&#24615;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#20808;&#21069;&#30693;&#35782;&#36827;&#34892;&#20998;&#37197;&#37325;&#35201;&#26435;&#37325;&#65292;&#23454;&#29616;&#20102;&#22312;&#20219;&#20309;&#30456;&#20851;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20351;&#26426;&#22120;&#20154;&#22312;&#22810;&#26679;&#21270;&#29615;&#22659;&#20219;&#21153;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#23569;&#37327;&#31034;&#33539;&#36866;&#24212;&#26410;&#30693;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#27169;&#20223;&#23398;&#20064;&#21464;&#24471;&#26497;&#20026;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#20219;&#21153;&#30340;&#36712;&#36857;&#20998;&#24067;&#19981;&#26029;&#21464;&#21270;&#65292;&#20165;&#20165;&#22522;&#20110;&#36830;&#32493;&#32858;&#21512;&#30340;&#25968;&#25454;&#26469;&#36827;&#34892;&#27169;&#22411;&#25311;&#21512;&#26159;&#24466;&#21171;&#30340;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20998;&#24067;&#21457;&#29983;&#22914;&#27492;&#22823;&#30340;&#21464;&#21270;&#65292;&#20197;&#33267;&#20110;&#26234;&#33021;&#20307;&#24456;&#38590;&#25512;&#26029;&#20986;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#19968;&#32452;&#29305;&#23450;&#20219;&#21153;&#30340;&#20808;&#21069;&#30693;&#35782;&#36827;&#34892;&#20998;&#37197;&#37325;&#35201;&#26435;&#37325;&#65292;&#20174;&#32780;&#22312;&#20219;&#20309;&#30456;&#20851;&#20219;&#21153;&#19978;&#36827;&#34892;&#27867;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20123;&#23454;&#39564;&#65292;&#22312;&#36825;&#20123;&#23454;&#39564;&#20013;&#65292;&#26426;&#22120;&#20154;&#20174;&#22810;&#26679;&#21270;&#30340;&#29615;&#22659;&#20219;&#21153;&#20013;&#35757;&#32451;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#23569;&#37327;&#31034;&#33539;&#36827;&#34892;&#23398;&#20064;&#65292;&#20174;&#32780;&#36866;&#24212;&#26410;&#30693;&#29615;&#22659;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#21407;&#22411;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#22312;&#35270;&#35273;&#23548;&#33322;&#20219;&#21153;&#19978;&#27979;&#35797;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#33719;&#24471;&#20102;&#33021;&#22815;&#39564;&#35777;&#36825;&#20123;&#20551;&#35774;&#30340;&#23454;&#39564;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning has gained immense popularity because of its high sample-efficiency. However, in real-world scenarios, where the trajectory distribution of most of the tasks dynamically shifts, model fitting on continuously aggregated data alone would be futile. In some cases, the distribution shifts, so much, that it is difficult for an agent to infer the new task. We propose a novel algorithm to generalize on any related task by leveraging prior knowledge on a set of specific tasks, which involves assigning importance weights to each past demonstration. We show experiments where the robot is trained from a diversity of environmental tasks and is also able to adapt to an unseen environment, using few-shot learning. We also developed a prototype robot system to test our approach on the task of visual navigation, and experimental results obtained were able to confirm these suppositions.
&lt;/p&gt;</description></item></channel></rss>