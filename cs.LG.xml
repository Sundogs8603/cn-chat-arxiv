<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Lipschitz&#36830;&#32493;&#30340;Transformer&#27169;&#22411;LipsFormer&#65292;&#20854;&#20013;&#24341;&#20837;&#20102;&#22810;&#39033;Lipschitz&#36830;&#32493;&#30340;&#32452;&#20214;&#26469;&#20445;&#35777;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#21487;&#20197;&#35753;&#28145;&#24230;Transformer&#26550;&#26500;&#31283;&#23450;&#36827;&#34892;&#35757;&#32451;&#65292;&#26080;&#38656;&#36807;&#22810;&#35843;&#25972;&#23398;&#20064;&#29575;&#31561;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.09856</link><description>&lt;p&gt;
LipsFormer&#65306;&#21521;Vision Transformer&#24341;&#20837;Lipschitz&#36830;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
LipsFormer: Introducing Lipschitz Continuity to Vision Transformers. (arXiv:2304.09856v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Lipschitz&#36830;&#32493;&#30340;Transformer&#27169;&#22411;LipsFormer&#65292;&#20854;&#20013;&#24341;&#20837;&#20102;&#22810;&#39033;Lipschitz&#36830;&#32493;&#30340;&#32452;&#20214;&#26469;&#20445;&#35777;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#21487;&#20197;&#35753;&#28145;&#24230;Transformer&#26550;&#26500;&#31283;&#23450;&#36827;&#34892;&#35757;&#32451;&#65292;&#26080;&#38656;&#36807;&#22810;&#35843;&#25972;&#23398;&#20064;&#29575;&#31561;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LipsFormer&#30340;Lipschitz&#36830;&#32493;Transformer&#65292;&#26088;&#22312;&#20174;&#29702;&#35770;&#19978;&#21644;&#23454;&#36341;&#19978;&#36861;&#27714;Transformer-based&#27169;&#22411;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;&#30456;&#27604;&#20110;&#20197;&#21069;&#36890;&#36807;&#23398;&#20064;&#29575;warmup&#12289;&#23618;&#24402;&#19968;&#21270;&#12289;attention&#20844;&#24335;&#12289;&#26435;&#37325;&#21021;&#22987;&#21270;&#31561;&#26041;&#27861;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#30340;&#23454;&#38469;&#25216;&#24039;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Lipschitz&#36830;&#32493;&#24615;&#26159;&#30830;&#20445;&#35757;&#32451;&#31283;&#23450;&#24615;&#30340;&#26356;&#37325;&#35201;&#30340;&#23646;&#24615;&#12290;&#22312;LipsFormer&#20013;&#65292;&#25105;&#20204;&#29992;Lipschitz&#36830;&#32493;&#30340;&#20013;&#24515;&#24402;&#19968;&#21270;&#20195;&#26367;&#19981;&#31283;&#23450;&#30340;Transformer&#32452;&#20214;&#27169;&#22359;&#65306;&#20013;&#24515;&#24402;&#19968;&#21270;&#20195;&#26367;&#23618;&#24402;&#19968;&#21270;&#65292;&#35889;&#21021;&#22987;&#21270;&#20195;&#26367;Xavier&#21021;&#22987;&#21270;&#65292;&#32553;&#25918;&#20313;&#24358;&#30456;&#20284;&#24230;&#27880;&#24847;&#20195;&#26367;&#28857;&#31215;&#27880;&#24847;&#65292;&#24182;&#20351;&#29992;&#21152;&#26435;&#27531;&#24046;&#24555;&#25463;&#26041;&#24335;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#24341;&#20837;&#30340;&#27169;&#22359;&#26159;Lipschitz&#36830;&#32493;&#30340;&#65292;&#24182;&#25512;&#23548;&#20986;LipsFormer&#30340;Lipschitz&#24120;&#25968;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LipsFormer&#20801;&#35768;&#28145;&#24230;Transformer&#26550;&#26500;&#30340;&#31283;&#23450;&#35757;&#32451;&#65292;&#26080;&#38656;&#20180;&#32454;&#35843;&#25972;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a Lipschitz continuous Transformer, called LipsFormer, to pursue training stability both theoretically and empirically for Transformer-based models. In contrast to previous practical tricks that address training instability by learning rate warmup, layer normalization, attention formulation, and weight initialization, we show that Lipschitz continuity is a more essential property to ensure training stability. In LipsFormer, we replace unstable Transformer component modules with Lipschitz continuous counterparts: CenterNorm instead of LayerNorm, spectral initialization instead of Xavier initialization, scaled cosine similarity attention instead of dot-product attention, and weighted residual shortcut. We prove that these introduced modules are Lipschitz continuous and derive an upper bound on the Lipschitz constant of LipsFormer. Our experiments show that LipsFormer allows stable training of deep Transformer architectures without the need of careful learning rate tuning such 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#24120;&#35265;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#22522;&#20934;&#20013;155&#20010;MDP&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#24403;&#26368;&#39640;Q&#20540;&#30340;&#21160;&#20316;&#22312;&#38543;&#26426;&#31574;&#30053;&#19979;Q&#20540;&#26368;&#39640;&#26102;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24448;&#24448;&#20250;&#25104;&#21151;&#65307;&#21453;&#20043;&#65292;&#21017;&#22833;&#36133;&#30340;&#21487;&#33021;&#24615;&#36739;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.09853</link><description>&lt;p&gt;
&#29992;&#26377;&#25928;&#30340;&#35270;&#37326;&#36830;&#25509;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#21644;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Bridging RL Theory and Practice with the Effective Horizon. (arXiv:2304.09853v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#24120;&#35265;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#22522;&#20934;&#20013;155&#20010;MDP&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#24403;&#26368;&#39640;Q&#20540;&#30340;&#21160;&#20316;&#22312;&#38543;&#26426;&#31574;&#30053;&#19979;Q&#20540;&#26368;&#39640;&#26102;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24448;&#24448;&#20250;&#25104;&#21151;&#65307;&#21453;&#20043;&#65292;&#21017;&#22833;&#36133;&#30340;&#21487;&#33021;&#24615;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#26576;&#20123;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20854;&#20182;&#29615;&#22659;&#20013;&#21364;&#22833;&#36133;&#24471;&#38750;&#24120;&#20005;&#37325;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#24212;&#35813;&#33021;&#22815;&#35299;&#37322;&#36825;&#31181;&#29616;&#35937;&#65292;&#25552;&#20379;&#39044;&#27979;&#23454;&#38469;&#24615;&#33021;&#30340;&#30028;&#38480;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24403;&#21069;&#30340;&#29702;&#35770;&#36824;&#27809;&#26377;&#36825;&#31181;&#33021;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#21253;&#21547;155&#20010;MDP&#30340;&#26032;&#25968;&#25454;&#38598;BRIDGE&#65292;&#23558;&#26631;&#20934;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19982;&#20043;&#21069;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20808;&#21069;&#30028;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20010;&#24847;&#24819;&#19981;&#21040;&#30340;&#24615;&#36136;&#65306;&#24403;&#26368;&#39640;Q&#20540;&#30340;&#21160;&#20316;&#22312;&#38543;&#26426;&#31574;&#30053;&#19979;&#30340;Q&#20540;&#20063;&#26159;&#26368;&#39640;&#30340;&#26102;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24448;&#24448;&#20250;&#25104;&#21151;&#65307;&#21453;&#20043;&#65292;&#22833;&#36133;&#30340;&#21487;&#33021;&#24615;&#36739;&#39640;&#12290;&#22522;&#20110;&#36825;&#19968;&#24615;&#36136;&#65292;&#25105;&#20204;&#23558;&#20854;&#27010;&#25324;&#20026;&#19968;&#20010;&#26032;&#30340;MDP&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#31216;&#20026;&#26377;&#25928;&#30340;&#35270;&#37326;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (RL) works impressively in some environments and fails catastrophically in others. Ideally, RL theory should be able to provide an understanding of why this is, i.e. bounds predictive of practical performance. Unfortunately, current theory does not quite have this ability. We compare standard deep RL algorithms to prior sample complexity prior bounds by introducing a new dataset, BRIDGE. It consists of 155 MDPs from common deep RL benchmarks, along with their corresponding tabular representations, which enables us to exactly compute instance-dependent bounds. We find that prior bounds do not correlate well with when deep RL succeeds vs. fails, but discover a surprising property that does. When actions with the highest Q-values under the random policy also have the highest Q-values under the optimal policy, deep RL tends to succeed; when they don't, deep RL tends to fail. We generalize this property into a new complexity measure of an MDP that we call the eff
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20174;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21442;&#25968;&#20998;&#24067;&#20986;&#21457;&#65292;&#25506;&#31350;&#32593;&#32476;&#36755;&#20986;&#20989;&#25968;&#20960;&#20309;&#24418;&#29366;&#30340;&#31616;&#21333;&#24615;&#19982;&#20449;&#24687;&#29109;&#22797;&#26434;&#24230;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.09837</link><description>&lt;p&gt;
&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#30340;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#28857;
&lt;/p&gt;
&lt;p&gt;
Points of non-linearity of functions generated by random neural networks. (arXiv:2304.09837v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09837
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20174;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21442;&#25968;&#20998;&#24067;&#20986;&#21457;&#65292;&#25506;&#31350;&#32593;&#32476;&#36755;&#20986;&#20989;&#25968;&#20960;&#20309;&#24418;&#29366;&#30340;&#31616;&#21333;&#24615;&#19982;&#20449;&#24687;&#29109;&#22797;&#26434;&#24230;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#30001;&#20855;&#26377;1&#20010;&#38544;&#34255;&#28608;&#27963;&#23618;&#65292;&#20219;&#24847;&#23485;&#24230;&#21644;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#20135;&#29983;&#30340;&#20174;&#23454;&#25968;&#21040;&#23454;&#25968;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#20551;&#35774;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#22522;&#20110;&#19981;&#21516;&#27010;&#29575;&#20998;&#24067;&#30340;&#22343;&#21248;&#38543;&#26426;&#36873;&#25321;&#65292;&#24182;&#35745;&#31639;&#38750;&#32447;&#24615;&#28857;&#30340;&#26399;&#26395;&#20998;&#24067;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#38416;&#26126;&#20026;&#20160;&#20040;&#32593;&#32476;&#21487;&#33021;&#20559;&#21521;&#20110;&#36755;&#20986;&#20855;&#26377;&#26356;&#31616;&#21333;&#20960;&#20309;&#24418;&#29366;&#30340;&#20989;&#25968;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;&#26576;&#20123;&#20449;&#24687;&#29109;&#22797;&#26434;&#24230;&#36739;&#20302;&#30340;&#20989;&#25968;&#20173;&#28982;&#38590;&#20197;&#29992;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider functions from the real numbers to the real numbers, output by a neural network with 1 hidden activation layer, arbitrary width, and ReLU activation function. We assume that the parameters of the neural network are chosen uniformly at random with respect to various probability distributions, and compute the expected distribution of the points of non-linearity. We use these results to explain why the network may be biased towards outputting functions with simpler geometry, and why certain functions with low information-theoretic complexity are nonetheless hard for a neural network to approximate.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#21644;&#21151;&#29575;&#20998;&#26512;&#30830;&#23450;&#20102;&#22810;&#20803;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#35780;&#20998;&#35268;&#21017;&#30340;&#21487;&#38752;&#24615;&#21306;&#22495;&#65292;&#24182;&#22312;&#30005;&#21147;&#29983;&#20135;&#38382;&#39064;&#19978;&#35780;&#20272;&#20102;&#32467;&#26524;&#23545;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#30340;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09836</link><description>&lt;p&gt;
&#22810;&#20803;&#27010;&#29575;&#39044;&#27979;&#35780;&#20272;&#20013;&#30340;&#21487;&#38752;&#24615;&#21306;&#22495;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Regions of Reliability in the Evaluation of Multivariate Probabilistic Forecasts. (arXiv:2304.09836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#21644;&#21151;&#29575;&#20998;&#26512;&#30830;&#23450;&#20102;&#22810;&#20803;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#35780;&#20998;&#35268;&#21017;&#30340;&#21487;&#38752;&#24615;&#21306;&#22495;&#65292;&#24182;&#22312;&#30005;&#21147;&#29983;&#20135;&#38382;&#39064;&#19978;&#35780;&#20272;&#20102;&#32467;&#26524;&#23545;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20803;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#35780;&#20272;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#36866;&#24403;&#30340;&#35780;&#20998;&#35268;&#21017;&#36827;&#34892;&#35780;&#20272;&#65292;&#21363;&#23545;&#20110;&#22522;&#20934;&#20998;&#24067;&#26399;&#26395;&#26368;&#23567;&#30340;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#28176;&#36827;&#24773;&#20917;&#19979;&#65292;&#36825;&#19968;&#23646;&#24615;&#19981;&#33021;&#20445;&#35777;&#20855;&#26377;&#33391;&#22909;&#30340;&#21306;&#20998;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#31687;&#31995;&#32479;&#30340;&#26377;&#38480;&#26679;&#26412;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#30740;&#31350;&#65292;&#36890;&#36807;&#21151;&#29575;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#20998;&#25968;&#35268;&#21017;&#30340;&#8220;&#21487;&#38752;&#24615;&#21306;&#22495;&#8221;&#65292;&#21363;&#23427;&#21487;&#20197;&#21487;&#38752;&#22320;&#35782;&#21035;&#39044;&#27979;&#35823;&#24046;&#30340;&#19968;&#32452;&#23454;&#38469;&#26465;&#20214;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20840;&#38754;&#30340;&#20154;&#36896;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#35813;&#27979;&#35797;&#19987;&#38376;&#35774;&#35745;&#20197;&#27979;&#35797;&#22522;&#20934;&#20998;&#24067;&#19982;&#39044;&#27979;&#20998;&#24067;&#20043;&#38388;&#30340;&#20960;&#20010;&#20851;&#38190;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#22312;&#30005;&#21147;&#29983;&#20135;&#38382;&#39064;&#19978;&#24212;&#29992;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#30340;&#26222;&#36866;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#22312;&#22810;&#20803;&#27010;&#29575;&#39044;&#27979;&#30340;&#35780;&#20272;&#20013;&#30340;&#37325;&#22823;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate probabilistic time series forecasts are commonly evaluated via proper scoring rules, i.e., functions that are minimal in expectation for the ground-truth distribution. However, this property is not sufficient to guarantee good discrimination in the non-asymptotic regime. In this paper, we provide the first systematic finite-sample study of proper scoring rules for time-series forecasting evaluation. Through a power analysis, we identify the "region of reliability" of a scoring rule, i.e., the set of practical conditions where it can be relied on to identify forecasting errors. We carry out our analysis on a comprehensive synthetic benchmark, specifically designed to test several key discrepancies between ground-truth and forecast distributions, and we gauge the generalizability of our findings to real-world tasks with an application to an electricity production problem. Our results reveal critical shortcomings in the evaluation of multivariate probabilistic forecasts as co
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#30740;&#31350;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#39118;&#21147;&#28065;&#36718;&#26426;&#21151;&#29575;&#26354;&#32447;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#21487;&#20197;&#26356;&#22909;&#22320;&#25351;&#31034;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#39640;&#24230;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#23398;&#20064;&#21040;&#29289;&#29702;&#19978;&#19981;&#21512;&#29702;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.09835</link><description>&lt;p&gt;
&#36879;&#26126;&#19988;&#31283;&#20581;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#39118;&#21147;&#28065;&#36718;&#26426;&#21151;&#29575;&#26354;&#32447;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards transparent and robust data-driven wind turbine power curve models. (arXiv:2304.09835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09835
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#30740;&#31350;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#39118;&#21147;&#28065;&#36718;&#26426;&#21151;&#29575;&#26354;&#32447;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#21487;&#20197;&#26356;&#22909;&#22320;&#25351;&#31034;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#39640;&#24230;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#23398;&#20064;&#21040;&#29289;&#29702;&#19978;&#19981;&#21512;&#29702;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#21147;&#28065;&#36718;&#26426;&#21151;&#29575;&#26354;&#32447;&#27169;&#22411;&#23558;&#29615;&#22659;&#26465;&#20214;&#36716;&#21270;&#20026;&#28065;&#36718;&#26426;&#30340;&#21151;&#29575;&#36755;&#20986;&#12290;&#23427;&#20204;&#23545;&#20110;&#33021;&#37327;&#20135;&#20986;&#39044;&#27979;&#21644;&#28065;&#36718;&#26426;&#24615;&#33021;&#30417;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#20248;&#20110;&#22522;&#20110;&#21442;&#25968;&#21644;&#29289;&#29702;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24120;&#24120;&#34987;&#25209;&#35780;&#20026;&#26159;&#19981;&#36879;&#26126;&#30340;&#8220;&#40657;&#21283;&#23376;&#8221;&#65292;&#36825;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20110;&#23427;&#20204;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#30340;&#31283;&#20581;&#24615;&#30340;&#25285;&#24551;&#65292;&#20363;&#22914;&#39118;&#21147;&#28065;&#36718;&#26426;&#25152;&#38754;&#20020;&#30340;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26694;&#26550;&#65292;&#26681;&#25454;&#36816;&#34892;&#20013;&#30340;SCADA&#25968;&#25454;&#26469;&#30740;&#31350;&#21644;&#39564;&#35777;&#25968;&#25454;&#39537;&#21160;&#30340;&#21151;&#29575;&#26354;&#32447;&#27169;&#22411;&#25152;&#23398;&#20064;&#30340;&#31574;&#30053;&#12290;&#23427;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32771;&#34385;&#19982;Shapley&#20540;&#21644;&#26368;&#26032;&#30340;XAI&#22238;&#24402;&#30740;&#31350;&#32467;&#26524;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#21487;&#20197;&#26356;&#22909;&#22320;&#25351;&#31034;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#26159;&#39564;&#35777;&#25110;&#27979;&#35797;&#38598;&#30340;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#39640;&#24230;&#22797;&#26434;&#65292;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24456;&#23481;&#26131;&#23398;&#20064;&#21040;&#29289;&#29702;&#19978;&#19981;&#21512;&#29702;&#30340;&#31574;&#30053;&#65292;&#36825;&#19968;&#28857;&#24212;&#24341;&#36215;&#27880;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wind turbine power curve models translate ambient conditions into turbine power output. They are essential for energy yield prediction and turbine performance monitoring. In recent years, data-driven machine learning methods have outperformed parametric, physics-informed approaches. However, they are often criticised for being opaque "black boxes" which raises concerns regarding their robustness in non-stationary environments, such as faced by wind turbines. We, therefore, introduce an explainable artificial intelligence (XAI) framework to investigate and validate strategies learned by data-driven power curve models from operational SCADA data. It combines domain-specific considerations with Shapley Values and the latest findings from XAI for regression. Our results suggest, that learned strategies can be better indicators for model robustness than validation or test set errors. Moreover, we observe that highly complex, state-of-the-art ML models are prone to learn physically implausib
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25351;&#20986;&#32570;&#20047;AI&#20844;&#24179;&#24615;&#20250;&#21152;&#28145;&#20559;&#35265;&#25104;&#20026;&#31038;&#20250;&#21387;&#21147;&#22240;&#32032;&#65292;&#21487;&#33021;&#23545;&#31038;&#20250;&#20135;&#29983;&#38271;&#26399;&#24433;&#21709;&#65292;&#22240;&#27492;&#38656;&#35201;&#23547;&#27714;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.09826</link><description>&lt;p&gt;
AI&#30340;&#20844;&#24179;&#24615;&#21450;&#20854;&#23545;&#31038;&#20250;&#30340;&#38271;&#26399;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Fairness in AI and Its Long-Term Implications on Society. (arXiv:2304.09826v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25351;&#20986;&#32570;&#20047;AI&#20844;&#24179;&#24615;&#20250;&#21152;&#28145;&#20559;&#35265;&#25104;&#20026;&#31038;&#20250;&#21387;&#21147;&#22240;&#32032;&#65292;&#21487;&#33021;&#23545;&#31038;&#20250;&#20135;&#29983;&#38271;&#26399;&#24433;&#21709;&#65292;&#22240;&#27492;&#38656;&#35201;&#23547;&#27714;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#30340;&#25104;&#21151;&#37096;&#32626;&#24050;&#32463;&#20026;&#20010;&#20154;&#21644;&#31038;&#20250;&#24102;&#26469;&#20102;&#35768;&#22810;&#31215;&#26497;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39044;&#27979;&#30340;&#20559;&#35265;&#65292;AI&#31995;&#32479;&#20063;&#34987;&#35777;&#26126;&#23545;&#37096;&#20998;&#20154;&#21475;&#36896;&#25104;&#20102;&#20260;&#23475;&#12290;&#25105;&#20204;&#30528;&#30524;&#20110;AI&#30340;&#20844;&#24179;&#24615;&#65292;&#20998;&#26512;&#20102;&#32570;&#20047;AI&#20844;&#24179;&#24615;&#26102;&#22914;&#20309;&#23548;&#33268;&#20559;&#35265;&#38543;&#30528;&#26102;&#38388;&#30340;&#21152;&#28145;&#32780;&#25104;&#20026;&#31038;&#20250;&#21387;&#21147;&#22240;&#32032;&#12290;&#22914;&#26524;&#38382;&#39064;&#25345;&#32493;&#23384;&#22312;&#65292;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#19981;&#33391;&#30340;&#38271;&#26399;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#19982;&#20854;&#20182;&#39118;&#38505;&#30340;&#20132;&#20114;&#26469;&#21152;&#24378;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#25552;&#39640;AI&#20844;&#24179;&#24615;&#30340;&#24403;&#21069;&#31574;&#30053;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#23454;&#38469;&#37096;&#32626;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#25506;&#35752;&#20102;&#30830;&#20445;&#25105;&#20204;&#22312;&#19981;&#25439;&#23475;&#31038;&#20250;&#37325;&#35201;&#37096;&#20998;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;AI&#30340;&#22909;&#22788;&#30340;&#28508;&#22312;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful deployment of artificial intelligence (AI) in various settings has led to numerous positive outcomes for individuals and society. However, AI systems have also been shown to harm parts of the population due to biased predictions. We take a closer look at AI fairness and analyse how lack of AI fairness can lead to deepening of biases over time and act as a social stressor. If the issues persist, it could have undesirable long-term implications on society, reinforced by interactions with other risks. We examine current strategies for improving AI fairness, assess their limitations in terms of real-world deployment, and explore potential paths forward to ensure we reap AI's benefits without harming significant parts of the society.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#31243;&#24207;&#29983;&#25104;&#29615;&#22659;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#20351;&#29992;&#27169;&#20223;&#23398;&#20064;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#21516;&#26102;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.09825</link><description>&lt;p&gt;
&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#21152;&#36895;&#31243;&#24207;&#29983;&#25104;&#29615;&#22659;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Using Offline Data to Speed-up Reinforcement Learning in Procedurally Generated Environments. (arXiv:2304.09825v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#31243;&#24207;&#29983;&#25104;&#29615;&#22659;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#20351;&#29992;&#27169;&#20223;&#23398;&#20064;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#21516;&#26102;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#20195;&#29702;&#33021;&#22815;&#23558;&#20854;&#23398;&#20064;&#31574;&#30053;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#29615;&#22659;&#20013;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#22823;&#37327;&#20132;&#20114;&#12290;&#21463;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#26368;&#36817;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#20197;&#35843;&#26597;&#20195;&#29702;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#36712;&#36857;&#30340;&#31163;&#32447;&#25968;&#25454;&#26469;&#25552;&#39640;&#31243;&#24207;&#29983;&#25104;&#29615;&#22659;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65306;&#65288;1&#65289;&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#20043;&#21069;&#39044;&#35757;&#32451;&#31574;&#30053;&#21644;&#65288;2&#65289;&#21516;&#26102;&#35757;&#32451;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#26469;&#33258;&#31163;&#32447;&#25968;&#25454;&#30340;&#27169;&#20223;&#23398;&#20064;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21487;&#29992;&#30340;&#31163;&#32447;&#36712;&#36857;&#30340;&#36136;&#37327;&#65288;&#36712;&#36857;&#30340;&#26368;&#20339;&#24615;&#65289;&#21644;&#22810;&#26679;&#24615;&#65288;&#36712;&#36857;&#25968;&#37327;&#21644;&#35206;&#30422;&#32423;&#21035;&#65289;&#23545;&#20004;&#31181;&#26041;&#27861;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#12290;&#22312;MiniGrid&#29615;&#22659;&#20013;&#30340;&#22235;&#20010;&#30693;&#21517;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#27169;&#20223;&#23398;&#20064;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#21516;&#26102;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the key challenges of Reinforcement Learning (RL) is the ability of agents to generalise their learned policy to unseen settings. Moreover, training RL agents requires large numbers of interactions with the environment. Motivated by the recent success of Offline RL and Imitation Learning (IL), we conduct a study to investigate whether agents can leverage offline data in the form of trajectories to improve the sample-efficiency in procedurally generated environments. We consider two settings of using IL from offline data for RL: (1) pre-training a policy before online RL training and (2) concurrently training a policy with online RL and IL from offline data. We analyse the impact of the quality (optimality of trajectories) and diversity (number of trajectories and covered level) of available offline trajectories on the effectiveness of both approaches. Across four well-known sparse reward tasks in the MiniGrid environment, we find that using IL for pre-training and concurrently d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#25552;&#20379;&#20803;&#35748;&#30693;&#24178;&#39044;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#24178;&#39044;&#33021;&#22815;&#32553;&#23567;&#23398;&#29983;&#20043;&#38388;&#30340;&#20803;&#35748;&#30693;&#25216;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2304.09821</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#25552;&#20379;&#20803;&#35748;&#30693;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Leveraging Deep Reinforcement Learning for Metacognitive Interventions across Intelligent Tutoring Systems. (arXiv:2304.09821v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#25552;&#20379;&#20803;&#35748;&#30693;&#24178;&#39044;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#24178;&#39044;&#33021;&#22815;&#32553;&#23567;&#23398;&#29983;&#20043;&#38388;&#30340;&#20803;&#35748;&#30693;&#25216;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#31181;&#19981;&#21516;&#26041;&#27861;&#26469;&#25552;&#20379;&#20803;&#35748;&#30693;&#24178;&#39044;&#65292;&#20197;&#21450;&#23427;&#20204;&#23545;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;ITSs&#65289;&#20013;&#23398;&#29983;&#26410;&#26469;&#23398;&#20064;&#20934;&#22791;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#36830;&#32493;&#30340;&#23398;&#26399;&#20013;&#36827;&#34892;&#20102;&#20004;&#20010;&#35838;&#22530;&#23454;&#39564;&#65306;&#23454;&#39564;1&#20351;&#29992;&#32463;&#20856;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#23558;&#23398;&#29983;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#20803;&#35748;&#30693;&#32452;&#65292;&#24182;&#26681;&#25454;&#20182;&#20204;&#30340;&#20998;&#31867;&#32452;&#25552;&#20379;&#38745;&#24577;&#30340;&#24178;&#39044;&#12290;&#22312;&#23454;&#39564;2&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26469;&#25552;&#20379;&#33258;&#36866;&#24212;&#24178;&#39044;&#65292;&#32771;&#34385;&#21040;&#23398;&#29983;&#20803;&#35748;&#30693;&#27700;&#24179;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#22312;&#20004;&#20010;&#23454;&#39564;&#20013;&#65292;&#23398;&#29983;&#25509;&#21463;&#20102;&#36825;&#20123;&#24178;&#39044;&#65292;&#23398;&#20064;&#22914;&#20309;&#21644;&#20309;&#26102;&#22312;&#36923;&#36753;&#36741;&#23548;&#31243;&#24207;&#19978;&#20351;&#29992;&#21521;&#21518;&#38142;&#25509;&#65288;BC&#65289;&#31574;&#30053;&#65292;&#35813;&#31243;&#24207;&#25903;&#25345;&#40664;&#35748;&#30340;&#21521;&#21069;&#38142;&#25509;&#31574;&#30053;&#12290;&#20845;&#21608;&#21518;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#21482;&#25903;&#25345;BC&#32780;&#27809;&#26377;&#24178;&#39044;&#30340;&#27010;&#29575;&#36741;&#23548;&#31243;&#24207;&#19978;&#23545;&#23398;&#29983;&#36827;&#34892;&#20102;&#22521;&#35757;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#36866;&#24212;&#30340;&#22522;&#20110;DRL&#30340;&#24178;&#39044;&#32553;&#23567;&#20102;&#23398;&#29983;&#20043;&#38388;&#30340;&#20803;&#35748;&#30693;&#25216;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work compares two approaches to provide metacognitive interventions and their impact on preparing students for future learning across Intelligent Tutoring Systems (ITSs). In two consecutive semesters, we conducted two classroom experiments: Exp. 1 used a classic artificial intelligence approach to classify students into different metacognitive groups and provide static interventions based on their classified groups. In Exp. 2, we leveraged Deep Reinforcement Learning (DRL) to provide adaptive interventions that consider the dynamic changes in the student's metacognitive levels. In both experiments, students received these interventions that taught how and when to use a backward-chaining (BC) strategy on a logic tutor that supports a default forward-chaining strategy. Six weeks later, we trained students on a probability tutor that only supports BC without interventions. Our results show that adaptive DRL-based interventions closed the metacognitive skills gap between students. In 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#31232;&#30095;&#24674;&#22797;&#20013;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22797;&#26434;&#24230;&#37327;&#26377;&#21161;&#20110;&#25552;&#39640;&#20854;&#27867;&#21270;&#21644;&#20272;&#35745;&#35823;&#24046;&#30028;&#38480;</title><link>http://arxiv.org/abs/2304.09802</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#21644;&#20272;&#35745;&#35823;&#24046;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Generalization and Estimation Error Bounds for Model-based Neural Networks. (arXiv:2304.09802v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09802
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#31232;&#30095;&#24674;&#22797;&#20013;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22797;&#26434;&#24230;&#37327;&#26377;&#21161;&#20110;&#25552;&#39640;&#20854;&#27867;&#21270;&#21644;&#20272;&#35745;&#35823;&#24046;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#20219;&#21153;&#65288;&#22914;&#31232;&#30095;&#32534;&#30721;&#21644;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#65289;&#20013;&#25552;&#20379;&#20102;&#26080;&#19982;&#20262;&#27604;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#19982;&#20256;&#24863;&#27169;&#22411;&#30340;&#24378;&#20851;&#32852;&#65292;&#36825;&#20123;&#32593;&#32476;&#26159;&#21487;&#35299;&#37322;&#30340;&#24182;&#32487;&#25215;&#20102;&#38382;&#39064;&#30340;&#20808;&#21069;&#32467;&#26500;&#12290;&#23454;&#36341;&#20013;&#65292;&#19982;ReLU&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#29616;&#35937;&#22312;&#29702;&#35770;&#19978;&#36824;&#27809;&#26377;&#24471;&#21040;&#35299;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22797;&#26434;&#24230;&#37327;&#65288;&#21253;&#25324;&#20840;&#23616;&#21644;&#23616;&#37096;Rademacher&#22797;&#26434;&#24230;&#65289;&#30340;&#26041;&#27861;&#65292;&#20026;&#22522;&#20110;&#27169;&#22411;&#30340;&#32593;&#32476;&#25552;&#20379;&#27867;&#21270;&#21644;&#20272;&#35745;&#35823;&#24046;&#30340;&#19978;&#38480;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#32593;&#32476;&#22312;&#31232;&#30095;&#24674;&#22797;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#20248;&#20110;&#24120;&#35268;&#30340;ReLU&#32593;&#32476;&#65292;&#24182;&#23548;&#20986;&#20801;&#35768;&#26500;&#24314;&#20855;&#26377;&#39640;&#20445;&#35777;&#24615;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#32593;&#32476;&#30340;&#23454;&#38469;&#35774;&#35745;&#35268;&#21017;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#35265;&#35299;&#20026;&#28145;&#24230;&#23398;&#20064;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#19968;&#20123;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based neural networks provide unparalleled performance for various tasks, such as sparse coding and compressed sensing problems. Due to the strong connection with the sensing model, these networks are interpretable and inherit prior structure of the problem. In practice, model-based neural networks exhibit higher generalization capability compared to ReLU neural networks. However, this phenomenon was not addressed theoretically. Here, we leverage complexity measures including the global and local Rademacher complexities, in order to provide upper bounds on the generalization and estimation errors of model-based networks. We show that the generalization abilities of model-based networks for sparse recovery outperform those of regular ReLU networks, and derive practical design rules that allow to construct model-based networks with guaranteed high generalization. We demonstrate through a series of experiments that our theoretical insights shed light on a few behaviours experienced 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20132;&#32593;&#32476;&#29702;&#35770;&#30340;&#21160;&#24577;&#38598;&#25104;&#22411;&#22238;&#24402;&#31639;&#27861;(SFNR)&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#31383;&#21475;(ADWIN)&#31639;&#27861;&#26816;&#27979;&#27010;&#24565;&#28418;&#31227;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#27010;&#24565;&#28418;&#31227;&#24773;&#20917;&#19979;SFNR&#30340;&#20934;&#30830;&#24615;&#26356;&#22909;&#19988;&#34920;&#29616;&#36229;&#36807;&#20854;&#20182;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.09788</link><description>&lt;p&gt;
&#22522;&#20110;&#31038;&#20132;&#32593;&#32476;&#29702;&#35770;&#30340;&#22238;&#24402;&#20219;&#21153;&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Advances on Concept Drift Detection in Regression Tasks using Social Networks Theory. (arXiv:2304.09788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20132;&#32593;&#32476;&#29702;&#35770;&#30340;&#21160;&#24577;&#38598;&#25104;&#22411;&#22238;&#24402;&#31639;&#27861;(SFNR)&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#31383;&#21475;(ADWIN)&#31639;&#27861;&#26816;&#27979;&#27010;&#24565;&#28418;&#31227;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#27010;&#24565;&#28418;&#31227;&#24773;&#20917;&#19979;SFNR&#30340;&#20934;&#30830;&#24615;&#26356;&#22909;&#19988;&#34920;&#29616;&#36229;&#36807;&#20854;&#20182;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27969;&#25366;&#25496;&#26159;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#20027;&#35201;&#30740;&#31350;&#20043;&#19968;&#65292;&#22240;&#20854;&#22312;&#35768;&#22810;&#30693;&#35782;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#27010;&#24565;&#28418;&#31227;&#26159;&#25366;&#25496;&#25968;&#25454;&#27969;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#65292;&#38656;&#35201;&#23398;&#20064;&#32773;&#25918;&#24323;&#24403;&#21069;&#27010;&#24565;&#24182;&#36866;&#24212;&#26032;&#27010;&#24565;&#12290;&#38598;&#25104;&#22411;&#28418;&#31227;&#26816;&#27979;&#31639;&#27861;&#24050;&#25104;&#21151;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#65292;&#20294;&#36890;&#24120;&#20250;&#32500;&#25252;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#23398;&#20064;&#22120;&#38598;&#21512;&#65292;&#20174;&#32780;&#20882;&#30528;&#38656;&#35201;&#19981;&#24517;&#35201;&#22320;&#33457;&#36153;&#22788;&#29702;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#22312;&#31038;&#20132;&#32593;&#32476;&#29702;&#35770;&#22522;&#30784;&#19978;&#23545;&#27604;&#20363;&#33258;&#30001;&#32593;&#32476;&#22238;&#24402;&#22120;(SFNR)&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#22238;&#24402;&#30340;&#21160;&#24577;&#38598;&#25104;&#22411;&#26041;&#27861;&#12290;&#20026;&#20102;&#26816;&#27979;&#27010;&#24565;&#28418;&#31227;&#65292;SFNR&#20351;&#29992;&#33258;&#36866;&#24212;&#31383;&#21475;(ADWIN)&#31639;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#20013;&#65292;SFNR&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#27010;&#24565;&#28418;&#31227;&#24773;&#20917;&#19979;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mining data streams is one of the main studies in machine learning area due to its application in many knowledge areas. One of the major challenges on mining data streams is concept drift, which requires the learner to discard the current concept and adapt to a new one. Ensemble-based drift detection algorithms have been used successfully to the classification task but usually maintain a fixed size ensemble of learners running the risk of needlessly spending processing time and memory. In this paper we present improvements to the Scale-free Network Regressor (SFNR), a dynamic ensemble-based method for regression that employs social networks theory. In order to detect concept drifts SFNR uses the Adaptive Window (ADWIN) algorithm. Results show improvements in accuracy, especially in concept drift situations and better performance compared to other state-of-the-art algorithms in both real and synthetic data.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#24179;&#31561;&#21270;&#21463;&#20445;&#25252;&#23376;&#32676;&#20307;&#20043;&#38388;&#30340;&#39044;&#27979;&#20998;&#24067;&#26469;&#23454;&#29616;&#32452;&#20844;&#24179;&#21644;&#35270;&#30456;&#20284;&#20010;&#20307;&#21516;&#31561;&#23545;&#24453;&#23454;&#29616;&#20010;&#20154;&#20844;&#27491;&#26159;&#19981;&#20860;&#23481;&#30340;&#12290; &#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#36830;&#32493;&#27010;&#29575;&#20989;&#25968;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#26469;&#23454;&#29616;&#32452;&#21644;&#20010;&#20154;&#20844;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.09779</link><description>&lt;p&gt;
&#24179;&#31561;&#25912;&#20851;&#19981;&#31561;&#20110;&#24179;&#31561;&#20010;&#20154;&#20960;&#29575;: &#29992;&#20110;&#32452;&#21644;&#20010;&#20154;&#20844;&#24179;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Equalised Odds is not Equal Individual Odds: Post-processing for Group and Individual Fairness. (arXiv:2304.09779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09779
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#24179;&#31561;&#21270;&#21463;&#20445;&#25252;&#23376;&#32676;&#20307;&#20043;&#38388;&#30340;&#39044;&#27979;&#20998;&#24067;&#26469;&#23454;&#29616;&#32452;&#20844;&#24179;&#21644;&#35270;&#30456;&#20284;&#20010;&#20307;&#21516;&#31561;&#23545;&#24453;&#23454;&#29616;&#20010;&#20154;&#20844;&#27491;&#26159;&#19981;&#20860;&#23481;&#30340;&#12290; &#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#36830;&#32493;&#27010;&#29575;&#20989;&#25968;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#26469;&#23454;&#29616;&#32452;&#21644;&#20010;&#20154;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#20844;&#24179;&#36890;&#36807;&#24179;&#34913;&#21463;&#20445;&#25252;&#23376;&#32676;&#20307;&#20043;&#38388;&#30340;&#39044;&#27979;&#20998;&#24067;&#26469;&#23454;&#29616;&#65307;&#20010;&#20154;&#20844;&#24179;&#35201;&#27714;&#23558;&#30456;&#20284;&#30340;&#20010;&#20307;&#35270;&#20026;&#21516;&#31561;&#23545;&#24453;&#12290;&#28982;&#32780;&#65292;&#24403;&#35780;&#20998;&#27169;&#22411;&#36890;&#36807;&#19981;&#36830;&#32493;&#30340;&#27010;&#29575;&#20989;&#25968;&#36827;&#34892;&#26657;&#20934;&#26102;&#65292;&#36825;&#20004;&#20010;&#30446;&#26631;&#26159;&#19981;&#20860;&#23481;&#30340;&#65292;&#20854;&#20013;&#20010;&#20307;&#21487;&#33021;&#20250;&#38543;&#26426;&#20998;&#37197;&#30001;&#22266;&#23450;&#27010;&#29575;&#30830;&#23450;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#33021;&#20250;&#20351;&#26469;&#33258;&#21516;&#19968;&#21463;&#20445;&#25252;&#32452;&#30340;&#20004;&#20010;&#30456;&#20284;&#20010;&#20307;&#30340;&#20998;&#31867;&#20960;&#29575;&#24046;&#21035;&#26126;&#26174;&#19981;&#21516;&#65292;&#36825;&#26159;&#20010;&#20154;&#20844;&#24179;&#30340;&#26126;&#26174;&#36829;&#21453;&#12290;&#20026;&#27599;&#20010;&#21463;&#20445;&#25252;&#23376;&#32676;&#20307;&#20998;&#37197;&#21807;&#19968;&#30340;&#20960;&#29575;&#20063;&#21487;&#33021;&#20250;&#38459;&#27490;&#19968;&#20010;&#23376;&#32676;&#20307;&#30340;&#25104;&#21592;&#25509;&#21040;&#21478;&#19968;&#20010;&#23376;&#32676;&#20307;&#26377;&#27491;&#38754;&#32467;&#26524;&#30340;&#24179;&#31561;&#26426;&#20250;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#21478;&#19968;&#31181;&#31216;&#20026;&#20010;&#20154;&#20960;&#29575;&#30340;&#19981;&#20844;&#24179;&#31867;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#21463;&#32676;&#20307;&#38408;&#20540;&#32422;&#26463;&#30340;&#36830;&#32493;&#27010;&#29575;&#20989;&#25968;&#26469;&#35299;&#20915;&#25152;&#26377;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20445;&#30041;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group fairness is achieved by equalising prediction distributions between protected sub-populations; individual fairness requires treating similar individuals alike. These two objectives, however, are incompatible when a scoring model is calibrated through discontinuous probability functions, where individuals can be randomly assigned an outcome determined by a fixed probability. This procedure may provide two similar individuals from the same protected group with classification odds that are disparately different -- a clear violation of individual fairness. Assigning unique odds to each protected sub-population may also prevent members of one sub-population from ever receiving equal chances of a positive outcome to another, which we argue is another type of unfairness called individual odds. We reconcile all this by constructing continuous probability functions between group thresholds that are constrained by their Lipschitz constant. Our solution preserves the model's predictive powe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#21644;&#25308;&#21344;&#24237;&#23481;&#38169;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#65292;&#25298;&#32477;&#22810;&#31181;&#25915;&#20987;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#38544;&#31169;&#20445;&#25252;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.09762</link><description>&lt;p&gt;
&#23454;&#29992;&#30340;&#24046;&#20998;&#38544;&#31169;&#21644;&#25308;&#21344;&#24237;&#23481;&#38169;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Practical Differentially Private and Byzantine-resilient Federated Learning. (arXiv:2304.09762v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#21644;&#25308;&#21344;&#24237;&#23481;&#38169;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#65292;&#25298;&#32477;&#22810;&#31181;&#25915;&#20987;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#38544;&#31169;&#20445;&#25252;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#21644;&#25308;&#21344;&#24237;&#23481;&#38169;&#26159;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#19981;&#21487;&#25110;&#32570;&#30340;&#35201;&#27714;&#12290;&#23613;&#31649;&#38544;&#31169;&#21644;&#25308;&#21344;&#24237;&#23433;&#20840;&#37117;&#26377;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#21516;&#26102;&#32771;&#34385;&#36825;&#20004;&#20010;&#35201;&#27714;&#30340;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#24456;&#23569;&#12290;&#36825;&#26159;&#30001;&#20110;&#21327;&#35843;&#38544;&#31169;&#20445;&#25252;&#21644;&#25308;&#21344;&#24237;&#23481;&#38169;&#31639;&#27861;&#30340;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#20445;&#25252;&#38544;&#31169;&#65292;&#28982;&#21518;&#24212;&#29992;&#25105;&#20204;&#30340;&#25308;&#21344;&#24237;&#23481;&#38169;&#31639;&#27861;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#23613;&#31649;&#29616;&#26377;&#30340;&#24037;&#20316;&#36981;&#24490;&#36825;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20294;&#23545; DP &#21644;&#25308;&#21344;&#24237;&#23481;&#38169;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#36825;&#24050;&#34987;&#24573;&#30053;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#20943;&#23569; DP &#24341;&#20837;&#30340;&#38543;&#26426;&#22122;&#22768;&#23545; Byzantine &#32858;&#21512;&#30340;&#24433;&#21709;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#21162;&#21147;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#38543;&#26426;&#22122;&#22768;&#26500;&#24314;&#19968;&#20010;&#32858;&#21512;&#65292;&#26377;&#25928;&#22320;&#25298;&#32477;&#20102;&#35768;&#22810;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#22312;&#20855;&#26377;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#24403;&#30340;&#38544;&#31169;&#20445;&#25252;&#27700;&#24179;&#26102;&#65292;&#25552;&#21319;&#20102;&#25972;&#20307; FL &#31995;&#32479;&#30340; Byzantine &#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy and Byzantine resilience are two indispensable requirements for a federated learning (FL) system. Although there have been extensive studies on privacy and Byzantine security in their own track, solutions that consider both remain sparse. This is due to difficulties in reconciling privacy-preserving and Byzantine-resilient algorithms.  In this work, we propose a solution to such a two-fold issue. We use our version of differentially private stochastic gradient descent (DP-SGD) algorithm to preserve privacy and then apply our Byzantine-resilient algorithms. We note that while existing works follow this general approach, an in-depth analysis on the interplay between DP and Byzantine resilience has been ignored, leading to unsatisfactory performance. Specifically, for the random noise introduced by DP, previous works strive to reduce its impact on the Byzantine aggregation. In contrast, we leverage the random noise to construct an aggregation that effectively rejects many existing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21382;&#21490;&#20215;&#26684;&#20449;&#24687;&#12289;&#27668;&#20505;&#26465;&#20214;&#12289;&#22303;&#22756;&#31867;&#22411;&#12289;&#22320;&#29702;&#20301;&#32622;&#21644;&#20854;&#20182;&#20851;&#38190;&#20915;&#31574;&#22240;&#32032;&#65292;&#31934;&#30830;&#39044;&#27979;&#20892;&#20135;&#21697;&#20215;&#26684;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#36866;&#29992;&#20110;&#26377;&#22122;&#22768;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#24182;&#19988;&#34920;&#29616;&#33267;&#23569;&#27604;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#32467;&#26524;&#22909;20%&#12290;</title><link>http://arxiv.org/abs/2304.09761</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21019;&#26032;&#26041;&#27861;&#29992;&#20110;&#20934;&#30830;&#30340;&#20892;&#20135;&#21697;&#20215;&#26684;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
An innovative Deep Learning Based Approach for Accurate Agricultural Crop Price Prediction. (arXiv:2304.09761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21382;&#21490;&#20215;&#26684;&#20449;&#24687;&#12289;&#27668;&#20505;&#26465;&#20214;&#12289;&#22303;&#22756;&#31867;&#22411;&#12289;&#22320;&#29702;&#20301;&#32622;&#21644;&#20854;&#20182;&#20851;&#38190;&#20915;&#31574;&#22240;&#32032;&#65292;&#31934;&#30830;&#39044;&#27979;&#20892;&#20135;&#21697;&#20215;&#26684;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#36866;&#29992;&#20110;&#26377;&#22122;&#22768;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#24182;&#19988;&#34920;&#29616;&#33267;&#23569;&#27604;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#32467;&#26524;&#22909;20%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20892;&#20135;&#21697;&#20215;&#26684;&#30340;&#20934;&#30830;&#39044;&#27979;&#23545;&#20110;&#20892;&#19994;&#21508;&#21033;&#30410;&#30456;&#20851;&#32773;&#65288;&#20892;&#27665;&#12289;&#28040;&#36153;&#32773;&#12289;&#38646;&#21806;&#21830;&#12289;&#25209;&#21457;&#21830;&#21644;&#25919;&#24220;&#65289;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20892;&#27665;&#30340;&#32463;&#27982;&#31119;&#31049;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#21382;&#21490;&#20215;&#26684;&#20449;&#24687;&#12289;&#27668;&#20505;&#26465;&#20214;&#12289;&#22303;&#22756;&#31867;&#22411;&#12289;&#22320;&#29702;&#20301;&#32622;&#21644;&#20854;&#20182;&#20851;&#38190;&#20915;&#31574;&#22240;&#32032;&#65292;&#31934;&#30830;&#39044;&#27979;&#20892;&#20135;&#21697;&#20215;&#26684;&#12290;&#36825;&#26159;&#19968;&#20010;&#25216;&#26415;&#25361;&#25112;&#65292;&#20197;&#21069;&#20063;&#26366;&#34987;&#23581;&#35797;&#36807;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#20215;&#26684;&#39044;&#27979;&#20013;&#30340;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#19982;&#26631;&#20934;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#20215;&#26684;&#30340;&#22320;&#29702;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#26377;&#22122;&#22768;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#24182;&#19988;&#34920;&#29616;&#33267;&#23569;&#27604;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#32467;&#26524;&#22909;20%&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate prediction of agricultural crop prices is a crucial input for decision-making by various stakeholders in agriculture: farmers, consumers, retailers, wholesalers, and the Government. These decisions have significant implications including, most importantly, the economic well-being of the farmers. In this paper, our objective is to accurately predict crop prices using historical price information, climate conditions, soil type, location, and other key determinants of crop prices. This is a technically challenging problem, which has been attempted before. In this paper, we propose an innovative deep learning based approach to achieve increased accuracy in price prediction. The proposed approach uses graph neural networks (GNNs) in conjunction with a standard convolutional neural network (CNN) model to exploit geospatial dependencies in prices. Our approach works well with noisy legacy data and produces a performance that is at least 20% better than the results available in the li
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#22788;&#29702;&#24494;&#26426;&#30005;&#31995;&#32479;&#20013;&#38750;&#32447;&#24615;&#25391;&#33633;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;&#22686;&#24133;&#27491;&#24358;&#21333;&#20803;&#65288;ASU&#65289;&#30340;&#26032;&#22411;&#33033;&#20914;&#20989;&#25968;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.09759</link><description>&lt;p&gt;
&#22686;&#24133;&#27491;&#24358;&#21333;&#20803;&#65306;&#19968;&#31181;&#29992;&#20110;&#24674;&#22797;&#38750;&#32447;&#24615;&#25391;&#33633;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38663;&#33633;&#28608;&#27963;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Amplifying Sine Unit: An Oscillatory Activation Function for Deep Neural Networks to Recover Nonlinear Oscillations Efficiently. (arXiv:2304.09759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09759
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#22788;&#29702;&#24494;&#26426;&#30005;&#31995;&#32479;&#20013;&#38750;&#32447;&#24615;&#25391;&#33633;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;&#22686;&#24133;&#27491;&#24358;&#21333;&#20803;&#65288;ASU&#65289;&#30340;&#26032;&#22411;&#33033;&#20914;&#20989;&#25968;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24037;&#19994;&#21644;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#38382;&#39064;&#34920;&#29616;&#20986;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#21608;&#26399;&#34892;&#20026;&#65292;&#20256;&#32479;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#25214;&#21040;&#23427;&#20204;&#30340;&#35299;&#26512;&#25110;&#38381;&#21512;&#24418;&#24335;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#38382;&#39064;&#38656;&#27714;&#19968;&#20123;&#21069;&#27839;&#30340;&#35745;&#31639;&#24037;&#20855;&#65292;&#20855;&#26377;&#22686;&#24378;&#30340;&#21151;&#33021;&#24615;&#21644;&#38477;&#20302;&#25104;&#26412;&#12290;&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#22788;&#29702;&#22823;&#25968;&#25454;&#21644;&#23398;&#20064;&#22797;&#26434;&#20989;&#25968;&#30340;&#26222;&#36866;&#24615;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#24341;&#36215;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21709;&#24212;&#24615;&#23618;&#32467;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#24494;&#26426;&#30005;&#31995;&#32479;&#20013;&#30340;&#38750;&#32447;&#24615;&#25391;&#33633;&#12290;&#25105;&#20204;&#22312;&#35774;&#35745;&#30340;&#32593;&#32476;&#20013;&#32467;&#21512;&#20102;&#19968;&#20123;&#25391;&#33633;&#21644;&#38750;&#25391;&#33633;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;Growing Cosine Unit&#65288;GCU&#65289;&#12289;Sine&#12289;Mish&#21644;Tanh&#65292;&#20197;&#23545;&#23427;&#20204;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#21644;&#25391;&#21160;&#38382;&#39064;&#30340;&#24615;&#33021;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#12290;&#23558;&#25391;&#33633;&#28608;&#27963;&#20989;&#25968;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#65292;&#21487;&#20197;&#20197;&#39640;&#31934;&#24230;&#21644;&#25928;&#29575;&#39044;&#27979;&#21608;&#26399;&#24615;&#27169;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#28608;&#27963;&#20989;&#25968;&#31216;&#20026;&#22686;&#24133;&#27491;&#24358;&#21333;&#20803;&#65288;ASU&#65289;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#38382;&#39064;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#65292;&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many industrial and real life problems exhibit highly nonlinear periodic behaviors and the conventional methods may fall short of finding their analytical or closed form solutions. Such problems demand some cutting edge computational tools with increased functionality and reduced cost. Recently, deep neural networks have gained massive research interest due to their ability to handle large data and universality to learn complex functions. In this work, we put forward a methodology based on deep neural networks with responsive layers structure to deal nonlinear oscillations in microelectromechanical systems. We incorporated some oscillatory and non oscillatory activation functions such as growing cosine unit known as GCU, Sine, Mish and Tanh in our designed network to have a comprehensive analysis on their performance for highly nonlinear and vibrational problems. Integrating oscillatory activation functions with deep neural networks definitely outperform in predicting the periodic patt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KCFCA&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;K-means&#31639;&#27861;&#26469;&#23545;&#26631;&#35760;&#30340;&#35757;&#32451;&#38598;&#21644;&#26080;&#26631;&#31614;&#30340;&#27979;&#35797;&#38598;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#19968;&#33268;&#24615;&#26469;&#23545;&#40784;&#32858;&#31867;&#20013;&#24515;&#12290;&#21478;&#22806;&#65292;&#25991;&#31456;&#36824;&#35774;&#35745;&#20102;&#21160;&#24577;&#22238;&#24402;&#27169;&#22411;&#26469;&#30740;&#31350;&#20998;&#24067;&#20559;&#31227;&#19982;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#25490;&#38500;&#24322;&#24120;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#20026;&#26080;&#26631;&#31614;&#27169;&#22411;&#35780;&#20272;&#25552;&#20379;&#20102;&#26356;&#21152;&#20840;&#38754;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.09758</link><description>&lt;p&gt;
&#22522;&#20110;K-means&#32858;&#31867;&#30340;&#29305;&#24449;&#19968;&#33268;&#24615;&#23545;&#40784;&#29992;&#20110;&#26080;&#26631;&#31614;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
K-means Clustering Based Feature Consistency Alignment for Label-free Model Evaluation. (arXiv:2304.09758v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KCFCA&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;K-means&#31639;&#27861;&#26469;&#23545;&#26631;&#35760;&#30340;&#35757;&#32451;&#38598;&#21644;&#26080;&#26631;&#31614;&#30340;&#27979;&#35797;&#38598;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#19968;&#33268;&#24615;&#26469;&#23545;&#40784;&#32858;&#31867;&#20013;&#24515;&#12290;&#21478;&#22806;&#65292;&#25991;&#31456;&#36824;&#35774;&#35745;&#20102;&#21160;&#24577;&#22238;&#24402;&#27169;&#22411;&#26469;&#30740;&#31350;&#20998;&#24067;&#20559;&#31227;&#19982;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#25490;&#38500;&#24322;&#24120;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#20026;&#26080;&#26631;&#31614;&#27169;&#22411;&#35780;&#20272;&#25552;&#20379;&#20102;&#26356;&#21152;&#20840;&#38754;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#26631;&#31614;&#27169;&#22411;&#35780;&#20272;&#26088;&#22312;&#22312;&#19981;&#20381;&#36182;&#20110;&#30495;&#23454;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#27169;&#22411;&#22312;&#21508;&#31181;&#27979;&#35797;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#25105;&#20204;&#38024;&#23545;CVPR 2023 Visual Dataset Understanding&#30740;&#35752;&#20250;&#30340;&#31532;&#19968;&#20010;DataCV&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-means&#32858;&#31867;&#29305;&#24449;&#19968;&#33268;&#24615;&#23545;&#40784;&#65288;KCFCA&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;KCFCA&#21033;&#29992;K-means&#31639;&#27861;&#26469;&#32858;&#31867;&#26631;&#35760;&#30340;&#35757;&#32451;&#38598;&#21644;&#26080;&#26631;&#31614;&#30340;&#27979;&#35797;&#38598;&#65292;&#28982;&#21518;&#36890;&#36807;&#29305;&#24449;&#19968;&#33268;&#24615;&#26469;&#23545;&#40784;&#32858;&#31867;&#20013;&#24515;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21160;&#24577;&#22238;&#24402;&#27169;&#22411;&#26469;&#25429;&#25417;&#20998;&#24067;&#20559;&#31227;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#21457;&#29616;&#24322;&#24120;&#27169;&#22411;&#22240;&#32032;&#65292;&#25490;&#38500;&#24322;&#24120;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#22810;&#20010;&#33258;&#21160;&#35780;&#20272;&#27169;&#22411;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The label-free model evaluation aims to predict the model performance on various test sets without relying on ground truths. The main challenge of this task is the absence of labels in the test data, unlike in classical supervised model evaluation. This paper presents our solutions for the 1st DataCV Challenge of the Visual Dataset Understanding workshop at CVPR 2023. Firstly, we propose a novel method called K-means Clustering Based Feature Consistency Alignment (KCFCA), which is tailored to handle the distribution shifts of various datasets. KCFCA utilizes the K-means algorithm to cluster labeled training sets and unlabeled test sets, and then aligns the cluster centers with feature consistency. Secondly, we develop a dynamic regression model to capture the relationship between the shifts in distribution and model accuracy. Thirdly, we design an algorithm to discover the outlier model factors, eliminate the outlier models, and combine the strengths of multiple autoeval models. On the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20351;&#29992;&#28508;&#22312;&#35821;&#20041;&#32500;&#24230;&#20316;&#20026;&#25351;&#32441;&#30340;&#36861;&#28335;&#26041;&#27861;&#65292;&#21487;&#20197;&#20998;&#26512;&#35774;&#35745;&#21464;&#37327;&#23545;&#20110;&#20934;&#30830;&#24615;-&#36136;&#37327;&#26435;&#34913;&#30340;&#24433;&#21709;&#65292;&#22312;&#20445;&#35777;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#35745;&#31639;&#37327;&#65292;&#26356;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.09752</link><description>&lt;p&gt;
&#20351;&#29992;&#28508;&#22312;&#25351;&#32441;&#36861;&#28335;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Attributing Image Generative Models using Latent Fingerprints. (arXiv:2304.09752v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20351;&#29992;&#28508;&#22312;&#35821;&#20041;&#32500;&#24230;&#20316;&#20026;&#25351;&#32441;&#30340;&#36861;&#28335;&#26041;&#27861;&#65292;&#21487;&#20197;&#20998;&#26512;&#35774;&#35745;&#21464;&#37327;&#23545;&#20110;&#20934;&#30830;&#24615;-&#36136;&#37327;&#26435;&#34913;&#30340;&#24433;&#21709;&#65292;&#22312;&#20445;&#35777;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#35745;&#31639;&#37327;&#65292;&#26356;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#20351;&#24471;&#20135;&#29983;&#30340;&#20869;&#23481;&#38590;&#20197;&#21306;&#20998;&#26159;&#21542;&#28304;&#20110;&#33258;&#28982;&#29615;&#22659;&#12290;&#36825;&#31181;&#27169;&#22411;&#30340;&#24320;&#28304;&#24320;&#21457;&#24341;&#36215;&#20102;&#23545;&#20110;&#20854;&#34987;&#24694;&#24847;&#21033;&#29992;&#30340;&#25285;&#24551;&#12290;&#20854;&#20013;&#19968;&#20010;&#28508;&#22312;&#30340;&#39118;&#38505;&#32531;&#35299;&#31574;&#30053;&#26159;&#36890;&#36807;&#25351;&#32441;&#36861;&#28335;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25351;&#32441;&#36861;&#28335;&#26041;&#27861;&#22312;&#36861;&#28335;&#20934;&#30830;&#24615;&#19982;&#29983;&#25104;&#36136;&#37327;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#26435;&#34913;&#65292;&#24182;&#19988;&#32570;&#20047;&#25913;&#21892;&#36825;&#31181;&#26435;&#34913;&#30340;&#35774;&#35745;&#21407;&#21017;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28508;&#22312;&#35821;&#20041;&#32500;&#24230;&#20316;&#20026;&#25351;&#32441;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#21487;&#20197;&#20998;&#26512;&#35774;&#35745;&#21464;&#37327;&#23545;&#20110;&#20934;&#30830;&#24615;-&#36136;&#37327;&#26435;&#34913;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#25351;&#32441;&#32500;&#24230;&#30340;&#36873;&#25321;&#12289;&#24378;&#24230;&#21644;&#23481;&#37327;&#12290;&#30456;&#27604;&#20043;&#21069;&#30340; SOTA&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38656;&#35201;&#26368;&#23569;&#30340;&#35745;&#31639;&#65292;&#24182;&#19988;&#26356;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992; StyleGAN2 &#21644;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models have enabled the creation of contents that are indistinguishable from those taken from the nature. Open-source development of such models raised concerns about the risks in their misuse for malicious purposes. One potential risk mitigation strategy is to attribute generative models via fingerprinting. Current fingerprinting methods exhibit significant tradeoff between robust attribution accuracy and generation quality, and also lack designing principles to improve this tradeoff. This paper investigates the use of latent semantic dimensions as fingerprints, from where we can analyze the effects of design variables, including the choice of fingerprinting dimensions, strength, and capacity, on the accuracy-quality tradeoff. Compared with previous SOTA, our method requires minimum computation and is more applicable to large-scale models. We use StyleGAN2 and the latent diffusion model to demonstrate the efficacy of our method.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#26694;&#26550;&#29992;&#20110;&#27880;&#24847;&#21147;&#32570;&#38519;&#22810;&#21160;&#38556;&#30861;&#30340;&#35786;&#26029;&#65292;&#20854;&#26174;&#31034;&#20986;&#25104;&#26412;&#25928;&#30410;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#21021;&#22987;ADHD&#35786;&#26029;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.09751</link><description>&lt;p&gt;
&#22522;&#20110;&#39592;&#26550;&#30340;&#34892;&#20026;&#20998;&#26512;&#29992;&#20110;&#27880;&#24847;&#21147;&#32570;&#38519;&#22810;&#21160;&#38556;&#30861;&#30340;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Skeleton-based action analysis for ADHD diagnosis. (arXiv:2304.09751v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09751
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#26694;&#26550;&#29992;&#20110;&#27880;&#24847;&#21147;&#32570;&#38519;&#22810;&#21160;&#38556;&#30861;&#30340;&#35786;&#26029;&#65292;&#20854;&#26174;&#31034;&#20986;&#25104;&#26412;&#25928;&#30410;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#21021;&#22987;ADHD&#35786;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#32570;&#38519;&#22810;&#21160;&#38556;&#30861; (ADHD)&#26159;&#19990;&#30028;&#33539;&#22260;&#20869;&#24120;&#35265;&#30340;&#31070;&#32463;&#34892;&#20026;&#38556;&#30861;&#12290;&#23613;&#31649;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;ADHD&#35786;&#26029;&#19978;&#30340;&#30740;&#31350;&#38750;&#24120;&#24191;&#27867;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#20381;&#36182;&#20110;&#39640;&#25104;&#26412;&#35774;&#22791;&#65292;&#20363;&#22914;MRI&#26426;&#22120;&#21644;EEG&#36148;&#29255;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;ADHD&#34892;&#20026;&#29305;&#24449;&#30340;&#20302;&#25104;&#26412;&#35786;&#26029;&#26041;&#27861;&#22791;&#21463;&#26399;&#24453;&#12290;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#30001;&#20110;&#20854;&#32858;&#28966;&#20110;&#21160;&#20316;&#21644;&#40065;&#26834;&#24615;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;ADHD&#35786;&#26029;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#21547;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#26694;&#26550;&#65292;&#21033;&#29992;&#30495;&#23454;&#30340;&#22810;&#27169;&#24577;ADHD&#25968;&#25454;&#38598;&#21644;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#31639;&#27861;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#25104;&#26412;&#25928;&#30410;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#20351;&#20854;&#26356;&#26131;&#20110;&#36827;&#34892;&#24191;&#27867;&#30340;&#21021;&#22987;ADHD&#35786;&#26029;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;AUC&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#24191;&#27867;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#35786;&#26029;&#21644;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention Deficit Hyperactivity Disorder (ADHD) is a common neurobehavioral disorder worldwide. While extensive research has focused on machine learning methods for ADHD diagnosis, most research relies on high-cost equipment, e.g., MRI machine and EEG patch. Therefore, low-cost diagnostic methods based on the action characteristics of ADHD are desired. Skeleton-based action recognition has gained attention due to the action-focused nature and robustness. In this work, we propose a novel ADHD diagnosis system with a skeleton-based action recognition framework, utilizing a real multi-modal ADHD dataset and state-of-the-art detection algorithms. Compared to conventional methods, the proposed method shows cost-efficiency and significant performance improvement, making it more accessible for a broad range of initial ADHD diagnoses. Through the experiment results, the proposed method outperforms the conventional methods in accuracy and AUC. Meanwhile, our method is widely applicable for mass
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#29992;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;(TNN)&#23545;&#30334;&#24917;&#22823;&#25481;&#26399;&#36827;&#34892;&#23450;&#20215;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;TNN&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#20943;&#23569;&#21442;&#25968;&#25935;&#24863;&#24230;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.09750</link><description>&lt;p&gt;
&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#22312;&#30334;&#24917;&#22823;&#25481;&#26399;&#23450;&#20215;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Tensor Neural Networks to Pricing Bermudan Swaptions. (arXiv:2304.09750v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#29992;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;(TNN)&#23545;&#30334;&#24917;&#22823;&#25481;&#26399;&#36827;&#34892;&#23450;&#20215;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;TNN&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#20943;&#23569;&#21442;&#25968;&#25935;&#24863;&#24230;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Cheyette&#27169;&#22411;&#26159;&#19968;&#31181;&#20934;&#39640;&#26031;&#27874;&#21160;&#29575;&#21033;&#29575;&#27169;&#22411;&#65292;&#24191;&#27867;&#29992;&#20110;&#23450;&#20215;&#21033;&#29575;&#34893;&#29983;&#21697;&#65292;&#20363;&#22914;&#27431;&#24335;&#25481;&#26399;&#21644;&#30334;&#24917;&#22823;&#25481;&#26399;&#65292;&#32780;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#24050;&#25104;&#20026;&#34892;&#19994;&#26631;&#20934;&#12290;&#22312;&#20302;&#32500;&#24230;&#19979;&#65292;&#36825;&#20123;&#26041;&#27861;&#20026;&#27431;&#24335;&#25481;&#26399;&#25552;&#20379;&#20102;&#20934;&#30830;&#32780;&#31283;&#20581;&#30340;&#20215;&#26684;&#65292;&#20294;&#21363;&#20351;&#22312;&#36825;&#31181;&#35745;&#31639;&#31616;&#21333;&#30340;&#24773;&#20917;&#19979;&#65292;&#24403;&#20351;&#29992;&#29366;&#24577;&#21464;&#37327;&#20316;&#20026;&#22238;&#24402;&#22120;&#26102;&#65292;&#23427;&#20204;&#20063;&#20250;&#20302;&#20272;&#30334;&#24917;&#22823;&#25481;&#26399;&#30340;&#20215;&#20540;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#25152;&#29992;&#22238;&#24402;&#22120;&#20013;&#39044;&#20808;&#30830;&#23450;&#30340;&#22522;&#20989;&#25968;&#25968;&#37327;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#20063;&#38754;&#20020;&#30528;&#32500;&#24230;&#28798;&#38590;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#21033;&#29992;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;(TNN)&#26469;&#36827;&#34892;&#30334;&#24917;&#22823;&#25481;&#26399;&#30340;&#23450;&#20215;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;TNN&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#23545;&#20110;&#22238;&#24402;&#22120;&#20013;&#25152;&#29992;&#22522;&#20989;&#25968;&#30340;&#25968;&#37327;&#31561;&#21442;&#25968;&#65292;&#20943;&#23569;&#20102;&#25935;&#24863;&#24230;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;TNN&#33021;&#22815;&#22312;&#39640;&#32500;&#24230;&#24773;&#20917;&#19979;&#20934;&#30830;&#22320;&#23450;&#20215;&#27431;&#24335;&#25481;&#26399;&#21644;&#30334;&#24917;&#22823;&#25481;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Cheyette model is a quasi-Gaussian volatility interest rate model widely used to price interest rate derivatives such as European and Bermudan Swaptions for which Monte Carlo simulation has become the industry standard. In low dimensions, these approaches provide accurate and robust prices for European Swaptions but, even in this computationally simple setting, they are known to underestimate the value of Bermudan Swaptions when using the state variables as regressors. This is mainly due to the use of a finite number of predetermined basis functions in the regression. Moreover, in high-dimensional settings, these approaches succumb to the Curse of Dimensionality. To address these issues, Deep-learning techniques have been used to solve the backward Stochastic Differential Equation associated with the value process for European and Bermudan Swaptions; however, these methods are constrained by training time and memory. To overcome these limitations, we propose leveraging Tensor Neura
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21463;&#21040;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#36825;&#20010;&#26041;&#27861;&#37319;&#29992;&#33258;&#21160;&#24494;&#20998;&#30340;ODE&#34920;&#36798;&#30001;&#21487;&#23398;&#20064;&#30340;&#27721;&#23494;&#23572;&#39039;&#23433;&#25490;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#26469;&#36817;&#20284;&#29615;&#22659;&#65292;&#22312;&#38376;&#25511;&#21046;&#21644;&#27721;&#23494;&#23572;&#39039;&#21442;&#25968;&#30340;&#23398;&#20064;&#20013;&#36890;&#36807;&#31995;&#32479;&#20132;&#20114;&#35299;&#20915;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#27604;&#26631;&#20934;&#22522;&#20110;&#27169;&#22411;&#33258;&#30001;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#20248;&#21183;&#65292;&#36866;&#29992;&#20110;&#22122;&#22768;&#26102;&#21464;&#38376;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.09718</link><description>&lt;p&gt;
&#22522;&#20110;&#26679;&#26412;&#25928;&#29575;&#30340;&#27169;&#22411;&#39537;&#21160;&#37327;&#23376;&#25511;&#21046;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sample-efficient Model-based Reinforcement Learning for Quantum Control. (arXiv:2304.09718v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21463;&#21040;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#36825;&#20010;&#26041;&#27861;&#37319;&#29992;&#33258;&#21160;&#24494;&#20998;&#30340;ODE&#34920;&#36798;&#30001;&#21487;&#23398;&#20064;&#30340;&#27721;&#23494;&#23572;&#39039;&#23433;&#25490;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#26469;&#36817;&#20284;&#29615;&#22659;&#65292;&#22312;&#38376;&#25511;&#21046;&#21644;&#27721;&#23494;&#23572;&#39039;&#21442;&#25968;&#30340;&#23398;&#20064;&#20013;&#36890;&#36807;&#31995;&#32479;&#20132;&#20114;&#35299;&#20915;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#27604;&#26631;&#20934;&#22522;&#20110;&#27169;&#22411;&#33258;&#30001;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#20248;&#21183;&#65292;&#36866;&#29992;&#20110;&#22122;&#22768;&#26102;&#21464;&#38376;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22122;&#22768;&#26102;&#21464;&#38376;&#20248;&#21270;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20248;&#20110;&#22522;&#20110;&#27169;&#22411;&#33258;&#30001;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#26679;&#26412;&#22797;&#26434;&#24230;&#26159;&#25511;&#21046;&#22120;&#19982;&#29289;&#29702;&#31995;&#32479;&#20132;&#20114;&#30340;&#27425;&#25968;&#12290;&#20511;&#21161;&#19968;&#20010;&#24402;&#32435;&#20559;&#32622;&#65292;&#21463;&#26368;&#36817;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#24494;&#30340;ODE&#65292;&#20854;&#30001;&#21487;&#23398;&#20064;&#30340;&#27721;&#23494;&#23572;&#39039;&#23433;&#25490;&#21442;&#25968;&#21270;&#65292;&#20197;&#34920;&#31034;&#27169;&#22411;&#36817;&#20284;&#29615;&#22659;&#65292;&#20854;&#26102;&#21464;&#37096;&#20998;&#65288;&#21253;&#25324;&#25511;&#21046;&#65289;&#23436;&#20840;&#24050;&#30693;&#12290;&#25511;&#21046;&#22120;&#21644;&#36830;&#32493;&#26102;&#22495;&#29420;&#31435;&#21442;&#25968;&#30340;&#27721;&#23494;&#23572;&#39039;&#23398;&#20064;&#26159;&#36890;&#36807;&#19982;&#31995;&#32479;&#30340;&#20132;&#20114;&#26469;&#35299;&#20915;&#30340;&#12290;&#22312;&#30495;&#23454;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#22312;&#20934;&#22791;&#19968;&#20123;&#26631;&#20934;&#21333;&#37327;&#23376;&#38376;&#30340;&#38381;&#21512;&#21644;&#24320;&#25918;&#31995;&#32479;&#21160;&#24577;&#26102;&#65292;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#19982;&#26631;&#20934;&#27169;&#22411;&#33258;&#30001;&#24378;&#21270;&#23398;&#20064;&#30456;&#27604;&#65292;&#20855;&#26377;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#20248;&#21183;&#65292;&#36825;&#21253;&#25324;&#21333;&#27425;&#27979;&#37327;&#12289;&#20219;&#24847;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#25130;&#26029;&#21644;&#19981;&#30830;&#23450;&#24615;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a model-based reinforcement learning (RL) approach for noisy time-dependent gate optimization with improved sample complexity over model-free RL. Sample complexity is the number of controller interactions with the physical system. Leveraging an inductive bias, inspired by recent advances in neural ordinary differential equations (ODEs), we use an auto-differentiable ODE parametrised by a learnable Hamiltonian ansatz to represent the model approximating the environment whose time-dependent part, including the control, is fully known. Control alongside Hamiltonian learning of continuous time-independent parameters is addressed through interactions with the system. We demonstrate an order of magnitude advantage in the sample complexity of our method over standard model-free RL in preparing some standard unitary gates with closed and open system dynamics, in realistic numerical experiments incorporating single shot measurements, arbitrary Hilbert space truncations and uncertaint
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#22810;&#20041;&#31070;&#32463;&#20803;&#35299;&#24320;&#20026;&#23553;&#35013;&#19981;&#21516;&#29305;&#24449;&#30340;&#27010;&#24565;&#21521;&#37327;&#65292;&#36825;&#20123;&#21521;&#37327;&#32534;&#30721;&#20102;&#36830;&#36143;&#30340;&#12289;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.09707</link><description>&lt;p&gt;
&#29992;&#27010;&#24565;&#21521;&#37327;&#35299;&#24320;&#31070;&#32463;&#20803;&#34920;&#31034;&#30340;&#32416;&#32544;
&lt;/p&gt;
&lt;p&gt;
Disentangling Neuron Representations with Concept Vectors. (arXiv:2304.09707v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#22810;&#20041;&#31070;&#32463;&#20803;&#35299;&#24320;&#20026;&#23553;&#35013;&#19981;&#21516;&#29305;&#24449;&#30340;&#27010;&#24565;&#21521;&#37327;&#65292;&#36825;&#20123;&#21521;&#37327;&#32534;&#30721;&#20102;&#36830;&#36143;&#30340;&#12289;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#26088;&#22312;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#21333;&#20803;&#26469;&#29702;&#35299;&#27169;&#22411;&#23384;&#20648;&#34920;&#31034;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#22810;&#20041;&#31070;&#32463;&#20803;&#30340;&#20986;&#29616;&#20351;&#24471;&#35299;&#37322;&#21333;&#20010;&#31070;&#32463;&#20803;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#23548;&#33268;&#20102;&#22312;&#28608;&#27963;&#31354;&#38388;&#20013;&#23547;&#25214;&#26377;&#24847;&#20041;&#30340;&#21521;&#37327;&#65292;&#31216;&#20026;&#27010;&#24565;&#21521;&#37327;&#65292;&#32780;&#19981;&#26159;&#21333;&#20010;&#31070;&#32463;&#20803;&#12290;&#26412;&#25991;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#22810;&#20041;&#31070;&#32463;&#20803;&#35299;&#24320;&#20026;&#23553;&#35013;&#19981;&#21516;&#29305;&#24449;&#30340;&#27010;&#24565;&#21521;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#25152;&#38656;&#30340;&#27010;&#24565;&#20998;&#31163;&#32423;&#21035;&#25628;&#32034;&#32454;&#31890;&#24230;&#27010;&#24565;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#22810;&#20041;&#31070;&#32463;&#20803;&#21487;&#20197;&#35299;&#24320;&#20026;&#30001;&#31070;&#32463;&#20803;&#30340;&#32447;&#24615;&#32452;&#21512;&#32452;&#25104;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25214;&#21040;&#30340;&#27010;&#24565;&#21521;&#37327;&#32534;&#30721;&#20102;&#36830;&#32493;&#30340;&#12289;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mechanistic interpretability aims to understand how models store representations by breaking down neural networks into interpretable units. However, the occurrence of polysemantic neurons, or neurons that respond to multiple unrelated features, makes interpreting individual neurons challenging. This has led to the search for meaningful vectors, known as concept vectors, in activation space instead of individual neurons. The main contribution of this paper is a method to disentangle polysemantic neurons into concept vectors encapsulating distinct features. Our method can search for fine-grained concepts according to the user's desired level of concept separation. The analysis shows that polysemantic neurons can be disentangled into directions consisting of linear combinations of neurons. Our evaluations show that the concept vectors found encode coherent, human-understandable features.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36817;&#38408;&#20540;&#22788;&#29702;&#22120;&#22312;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#33021;&#37327;&#33410;&#30465;&#65292;&#24182;&#25552;&#20986;&#22823;-&#23567;&#33258;&#36866;&#24212;&#31070;&#32463;&#32593;&#32476;&#31574;&#30053;&#20197;&#25552;&#39640;&#30005;&#27744;&#23551;&#21629;&#21644;&#20445;&#25345;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09695</link><description>&lt;p&gt;
&#20302;&#21151;&#32791;&#36817;&#38408;&#20540;&#22788;&#29702;&#22120;&#19978;&#30340;&#22823;-&#23567;&#33258;&#36866;&#24212;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Big-Little Adaptive Neural Networks on Low-Power Near-Subthreshold Processors. (arXiv:2304.09695v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36817;&#38408;&#20540;&#22788;&#29702;&#22120;&#22312;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#33021;&#37327;&#33410;&#30465;&#65292;&#24182;&#25552;&#20986;&#22823;-&#23567;&#33258;&#36866;&#24212;&#31070;&#32463;&#32593;&#32476;&#31574;&#30053;&#20197;&#25552;&#39640;&#30005;&#27744;&#23551;&#21629;&#21644;&#20445;&#25345;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36817;&#38408;&#20540;&#22788;&#29702;&#22120;&#22312;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#21487;&#20197;&#33719;&#24471;&#30340;&#33021;&#37327;&#33410;&#30465;&#65292;&#24182;&#25552;&#20986;&#20102;&#31574;&#30053;&#26469;&#25552;&#39640;&#23427;&#20204;&#21516;&#26102;&#20445;&#25345;&#24212;&#29992;&#30340;&#20934;&#30830;&#24615;&#12290;&#36873;&#25321;&#30340;&#22788;&#29702;&#22120;&#37319;&#29992;&#33258;&#36866;&#24212;&#30005;&#21387;&#32553;&#25918;&#25216;&#26415;&#65292;&#22788;&#29702;&#22120;&#26680;&#30340;&#39057;&#29575;&#21644;&#30005;&#21387;&#32423;&#21035;&#22312;&#36816;&#34892;&#26102;&#30830;&#23450;&#12290;&#22312;&#36825;&#20123;&#31995;&#32479;&#20013;&#65292;&#23884;&#20837;&#24335;RAM&#21644;Flash&#23384;&#20648;&#22120;&#30340;&#22823;&#23567;&#36890;&#24120;&#38480;&#21046;&#22312;&#19981;&#21040;1&#20806;&#23383;&#33410;&#20197;&#33410;&#30465;&#30005;&#21147;&#12290;&#36825;&#31181;&#26377;&#38480;&#20869;&#23384;&#23545;&#21487;&#20197;&#26144;&#23556;&#21040;&#36825;&#20123;&#35774;&#22791;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#31934;&#24230;&#19982;&#30005;&#27744;&#32493;&#33322;&#26102;&#38388;&#20043;&#38388;&#25152;&#38656;&#30340;&#26435;&#34913;&#20135;&#29983;&#20102;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#26367;&#20195;&#30340;&#8220;&#22823;-&#23567;&#8221;&#31070;&#32463;&#32593;&#32476;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#30005;&#27744;&#23551;&#21629;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#24212;&#29992;&#20110;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#24212;&#29992;&#31243;&#24207;&#20316;&#20026;&#31034;&#33539;&#65292;&#32467;&#26524;&#26174;&#31034;&#30456;&#27604;&#20110;&#21407;&#22987;&#32593;&#32476;&#65292;&#26368;&#20339;&#37197;&#32622;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the energy savings that near-subthreshold processors can obtain in edge AI applications and proposes strategies to improve them while maintaining the accuracy of the application. The selected processors deploy adaptive voltage scaling techniques in which the frequency and voltage levels of the processor core are determined at the run-time. In these systems, embedded RAM and flash memory size is typically limited to less than 1 megabyte to save power. This limited memory imposes restrictions on the complexity of the neural networks model that can be mapped to these devices and the required trade-offs between accuracy and battery life. To address these issues, we propose and evaluate alternative 'big-little' neural network strategies to improve battery life while maintaining prediction accuracy. The strategies are applied to a human activity recognition application selected as a demonstrator that shows that compared to the original network, the best configurations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#21644;&#25237;&#24433;&#36861;&#36394;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20415;&#23452;&#12289;&#39640;&#25928;&#22320;&#29983;&#25104;&#26102;&#24577;&#23494;&#24230;&#24314;&#27169;&#65292;&#20854;&#26368;&#20248;&#26144;&#23556;&#19982;&#24658;&#31561;&#26144;&#23556;&#25509;&#36817;&#65292;&#35757;&#32451;&#36807;&#31243;&#39640;&#24230;&#24182;&#34892;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.09663</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#21644;&#25237;&#24433;&#36861;&#36394;&#30340;&#26102;&#21464;&#23494;&#24230;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling of Time-Dependent Densities via Optimal Transport and Projection Pursuit. (arXiv:2304.09663v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#21644;&#25237;&#24433;&#36861;&#36394;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20415;&#23452;&#12289;&#39640;&#25928;&#22320;&#29983;&#25104;&#26102;&#24577;&#23494;&#24230;&#24314;&#27169;&#65292;&#20854;&#26368;&#20248;&#26144;&#23556;&#19982;&#24658;&#31561;&#26144;&#23556;&#25509;&#36817;&#65292;&#35757;&#32451;&#36807;&#31243;&#39640;&#24230;&#24182;&#34892;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#23545;&#20110;&#26102;&#24577;&#23494;&#24230;&#29983;&#25104;&#24314;&#27169;&#25152;&#24102;&#26469;&#30340;&#35745;&#31639;&#22256;&#38590;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20415;&#23452;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#38656;&#35201;&#26368;&#23569;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#24182;&#19988;&#21487;&#20197;&#24456;&#22909;&#22320;&#25193;&#23637;&#21040;&#39640;&#32500;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#25237;&#24433;&#30340;&#26368;&#20248;&#36755;&#36816;&#27714;&#35299;&#22120; [Meng&#31561;&#65292;2019] &#26469;&#36830;&#25509;&#36830;&#32493;&#30340;&#26679;&#26412;&#65292;&#28982;&#21518;&#20351;&#29992;&#20256;&#36755;&#26679;&#26465; [Chewi&#31561;&#65292;2020] &#26469;&#25554;&#20540;&#28436;&#21270;&#30340;&#23494;&#24230;&#12290;&#24403;&#37319;&#26679;&#39057;&#29575;&#36275;&#22815;&#39640;&#26102;&#65292;&#26368;&#20248;&#26144;&#23556;&#25509;&#36817;&#20110;&#24658;&#31561;&#26144;&#23556;&#65292;&#22240;&#27492;&#35745;&#31639;&#25928;&#29575;&#39640;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#36807;&#31243;&#21487;&#20197;&#39640;&#24230;&#24182;&#34892;&#21270;&#65292;&#22240;&#20026;&#25152;&#26377;&#26368;&#20248;&#26144;&#23556;&#26159;&#29420;&#31435;&#30340;&#65292;&#22240;&#27492;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#35813;&#26041;&#27861;&#20165;&#22522;&#20110;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#32780;&#19981;&#26159;&#26368;&#23567;&#21270;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36731;&#26494;&#20998;&#26512;&#21644;&#25511;&#21046;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20960;&#20010;&#25968;&#20540;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the computational difficulties incurred by popular deep learning algorithms for the generative modeling of temporal densities, we propose a cheap alternative which requires minimal hyperparameter tuning and scales favorably to high dimensional problems. In particular, we use a projection-based optimal transport solver [Meng et al., 2019] to join successive samples and subsequently use transport splines [Chewi et al., 2020] to interpolate the evolving density. When the sampling frequency is sufficiently high, the optimal maps are close to the identity and are thus computationally efficient to compute. Moreover, the training process is highly parallelizable as all optimal maps are independent and can thus be learned simultaneously. Finally, the approach is based solely on numerical linear algebra rather than minimizing a nonconvex objective function, allowing us to easily analyze and control the algorithm. We present several numerical experiments on both synthetic and real-w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102; QDQN-DPER &#26694;&#26550;&#65292;&#23427;&#23558;&#20998;&#24067;&#24335;&#20248;&#20808;&#32463;&#39564;&#22238;&#25918;&#21644;&#24322;&#27493;&#35757;&#32451;&#32435;&#20837;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.09648</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#24067;&#24335;&#20248;&#20808;&#32463;&#39564;&#22238;&#25918;&#30340;&#37327;&#23376;&#28145;&#24230; Q &#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Quantum deep Q learning with distributed prioritized experience replay. (arXiv:2304.09648v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102; QDQN-DPER &#26694;&#26550;&#65292;&#23427;&#23558;&#20998;&#24067;&#24335;&#20248;&#20808;&#32463;&#39564;&#22238;&#25918;&#21644;&#24322;&#27493;&#35757;&#32451;&#32435;&#20837;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102; QDQN-DPER &#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064; (QRL) &#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#12290;&#35813;&#26694;&#26550;&#23558;&#20248;&#20808;&#32463;&#39564;&#22238;&#25918;&#21644;&#24322;&#27493;&#35757;&#32451;&#32435;&#20837;&#35757;&#32451;&#31639;&#27861;&#65292;&#20197;&#20943;&#23569;&#39640;&#37319;&#26679;&#22797;&#26434;&#24615;&#12290;&#25968;&#20540;&#27169;&#25311;&#34920;&#26126;&#65292;QDQN-DPER &#22312;&#20855;&#26377;&#30456;&#21516;&#27169;&#22411;&#26550;&#26500;&#30340;&#20998;&#24067;&#24335;&#37327;&#23376; Q &#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#20445;&#25345;&#35757;&#32451;&#25928;&#29575;&#30340;&#21516;&#26102;&#36866;&#29992;&#20110;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the QDQN-DPER framework to enhance the efficiency of quantum reinforcement learning (QRL) in solving sequential decision tasks. The framework incorporates prioritized experience replay and asynchronous training into the training algorithm to reduce the high sampling complexities. Numerical simulations demonstrate that QDQN-DPER outperforms the baseline distributed quantum Q learning with the same model architecture. The proposed framework holds potential for more complex tasks while maintaining training efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#948;&#35843;&#33410;&#26041;&#27861;&#8212;&#8212;AdapterGNN&#65292;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#21033;&#29992;&#39640;&#24230;&#34920;&#36798;&#30340;&#36866;&#37197;&#22120;&#33021;&#22815;&#22312;&#20165;&#26377;&#23569;&#37327;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20248;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.09595</link><description>&lt;p&gt;
AdapterGNN&#65306;&#39640;&#25928;&#30340;&#948;&#35843;&#33410;&#25552;&#39640;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
AdapterGNN: Efficient Delta Tuning Improves Generalization Ability in Graph Neural Networks. (arXiv:2304.09595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#948;&#35843;&#33410;&#26041;&#27861;&#8212;&#8212;AdapterGNN&#65292;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#21033;&#29992;&#39640;&#24230;&#34920;&#36798;&#30340;&#36866;&#37197;&#22120;&#33021;&#22815;&#22312;&#20165;&#26377;&#23569;&#37327;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#38500;&#20102;&#39044;&#35757;&#32451;&#25216;&#26415;&#22806;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#39046;&#22495;&#30340;&#26368;&#26032;&#24037;&#20316;&#30340;&#21551;&#31034;&#65292;&#26356;&#36817;&#26399;&#30340;&#30740;&#31350;&#36716;&#21521;&#24212;&#29992;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20363;&#22914;&#21442;&#25968;&#26377;&#25928;&#30340;&#35843;&#33410;&#65288;&#948;&#35843;&#33410;&#65289;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;GNNs&#21644;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#24046;&#24322;&#65292;&#23558;&#36825;&#20123;&#26041;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;GNNs&#35777;&#26126;&#25928;&#26524;&#36739;&#24369;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;GNNs&#30340;&#948;&#35843;&#33410;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#20026;GNNs&#35774;&#35745;&#30340;&#26032;&#22411;&#948;&#35843;&#33410;&#26041;&#27861;&#8212;&#8212;AdapterGNN&#12290;AdapterGNN&#20445;&#30041;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#24182;&#21033;&#29992;&#39640;&#24230;&#34920;&#36798;&#30340;GNN&#36866;&#37197;&#22120;&#65292;&#22312;&#20165;&#26377;&#23569;&#37327;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;AdapterGNN&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#39640;&#25928;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pre-trained models has recently yielded remarkable performance gains in graph neural networks (GNNs). In addition to pre-training techniques, inspired by the latest work in the natural language fields, more recent work has shifted towards applying effective fine-tuning approaches, such as parameter-efficient tuning (delta tuning). However, given the substantial differences between GNNs and transformer-based models, applying such approaches directly to GNNs proved to be less effective. In this paper, we present a comprehensive comparison of delta tuning techniques for GNNs and propose a novel delta tuning method specifically designed for GNNs, called AdapterGNN. AdapterGNN preserves the knowledge of the large pre-trained model and leverages highly expressive adapters for GNNs, which can adapt to downstream tasks effectively with only a few parameters, while also improving the model's generalization ability on the downstream tasks. Extensive experiments show that AdapterGNN a
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#21452;&#26102;&#38388;&#23610;&#24230;&#21046;&#24230;&#19979;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#26080;&#38656;&#31070;&#32463;&#20803;&#25968;&#37327;&#36235;&#20110;&#26080;&#38480;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2304.09576</link><description>&lt;p&gt;
&#21033;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#21046;&#24230;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Leveraging the two timescale regime to demonstrate convergence of neural networks. (arXiv:2304.09576v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09576
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#21452;&#26102;&#38388;&#23610;&#24230;&#21046;&#24230;&#19979;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#26080;&#38656;&#31070;&#32463;&#20803;&#25968;&#37327;&#36235;&#20110;&#26080;&#38480;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#22312;&#20869;&#23618;&#27493;&#38271;&#36828;&#23567;&#20110;&#22806;&#23618;&#27493;&#38271;&#30340;&#21452;&#26102;&#38388;&#23610;&#24230;&#21046;&#24230;&#19979;&#12290;&#22312;&#36825;&#20010;&#21046;&#24230;&#19979;&#65292;&#22312;&#31616;&#21333;&#30340;&#21333;&#21464;&#37327;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#25910;&#25947;&#20110;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#38656;&#35201;&#31070;&#32463;&#20803;&#25968;&#37327;&#36235;&#20110;&#26080;&#38480;&#65292;&#36825;&#20351;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#21516;&#20110;&#26368;&#36817;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#22914;&#31070;&#32463;&#20999;&#21521;&#26680;&#25110;&#24179;&#22343;&#22330;&#21046;&#24230;&#12290;&#25105;&#20204;&#25552;&#20379;&#23454;&#39564;&#35828;&#26126;&#65292;&#26174;&#31034;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#25353;&#29031;&#25105;&#20204;&#23545;&#26799;&#24230;&#27969;&#30340;&#25551;&#36848;&#36827;&#34892;&#34892;&#20026;&#65292;&#24182;&#22240;&#27492;&#22312;&#21452;&#26102;&#38388;&#23610;&#24230;&#21046;&#24230;&#19979;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#20294;&#22312;&#27492;&#21046;&#24230;&#20043;&#22806;&#21487;&#33021;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the training dynamics of shallow neural networks, in a two-timescale regime in which the stepsizes for the inner layer are much smaller than those for the outer layer. In this regime, we prove convergence of the gradient flow to a global optimum of the non-convex optimization problem in a simple univariate setting. The number of neurons need not be asymptotically large for our result to hold, distinguishing our result from popular recent approaches such as the neural tangent kernel or mean-field regimes. Experimental illustration is provided, showing that the stochastic gradient descent behaves according to our description of the gradient flow and thus converges to a global optimum in the two-timescale regime, but can fail outside of this regime.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;(NNs)&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#31216;&#20026;&#23433;&#20840;&#22686;&#24378;&#65292;&#21487;&#20197;&#20351;&#35299;&#20915;&#26041;&#26696;&#22312;&#32447;&#21487;&#34892;&#24182;&#20855;&#26377;&#25910;&#25947;&#21644;&#32422;&#26463;&#26465;&#20214;&#30340;&#30830;&#23450;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.09575</link><description>&lt;p&gt;
&#22522;&#20110;&#23433;&#20840;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#36817;&#20284;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Approximate non-linear model predictive control with safety-augmented neural networks. (arXiv:2304.09575v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;(NNs)&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#31216;&#20026;&#23433;&#20840;&#22686;&#24378;&#65292;&#21487;&#20197;&#20351;&#35299;&#20915;&#26041;&#26696;&#22312;&#32447;&#21487;&#34892;&#24182;&#20855;&#26377;&#25910;&#25947;&#21644;&#32422;&#26463;&#26465;&#20214;&#30340;&#30830;&#23450;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#21487;&#20197;&#23454;&#29616;&#23545;&#20110;&#19968;&#33324;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#21644;&#32422;&#26463;&#26465;&#20214;&#30340;&#28385;&#36275;&#65292;&#20294;&#38656;&#35201;&#36827;&#34892;&#35745;&#31639;&#24320;&#38144;&#24456;&#22823;&#30340;&#22312;&#32447;&#20248;&#21270;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;(NNs)&#23545;&#36825;&#31181;MPC&#25511;&#21046;&#22120;&#30340;&#36817;&#20284;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#30340;&#22312;&#32447;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23433;&#20840;&#22686;&#24378;&#65292;&#23613;&#31649;&#23384;&#22312;&#36817;&#20284;&#19981;&#20934;&#30830;&#24615;&#65292;&#20294;&#21487;&#20197;&#33719;&#24471;&#25910;&#25947;&#21644;&#32422;&#26463;&#26465;&#20214;&#30340;&#30830;&#23450;&#20445;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;NN&#36817;&#20284;MPC&#30340;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#22312;&#32447;&#39564;&#35777;&#23427;&#26159;&#21542;&#26159;MPC&#38382;&#39064;&#30340;&#21487;&#34892;&#35299;&#12290;&#24403;&#35813;&#35299;&#20915;&#26041;&#26696;&#19981;&#21487;&#34892;&#25110;&#25104;&#26412;&#26356;&#39640;&#26102;&#65292;&#25105;&#20204;&#22522;&#20110;&#26631;&#20934;MPC&#25216;&#26415;&#23558;NN&#35299;&#20915;&#26041;&#26696;&#26367;&#25442;&#20026;&#23433;&#20840;&#20505;&#36873;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#38656;&#35201;&#23545;NN&#36827;&#34892;&#19968;&#27425;&#35780;&#20272;&#21644;&#23545;&#36755;&#20837;&#24207;&#21015;&#36827;&#34892;&#22312;&#32447;&#21069;&#21521;&#31215;&#20998;&#65292;&#36825;&#22312;&#36164;&#28304;&#21463;&#38480;&#31995;&#32479;&#19978;&#30340;&#35745;&#31639;&#36895;&#24230;&#24456;&#24555;&#12290;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#26694;&#26550;&#22312;&#19977;&#20010;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#38750;&#32447;&#24615;MPC&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#23637;&#31034;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model predictive control (MPC) achieves stability and constraint satisfaction for general nonlinear systems, but requires computationally expensive online optimization. This paper studies approximations of such MPC controllers via neural networks (NNs) to achieve fast online evaluation. We propose safety augmentation that yields deterministic guarantees for convergence and constraint satisfaction despite approximation inaccuracies. We approximate the entire input sequence of the MPC with NNs, which allows us to verify online if it is a feasible solution to the MPC problem. We replace the NN solution by a safe candidate based on standard MPC techniques whenever it is infeasible or has worse cost. Our method requires a single evaluation of the NN and forward integration of the input sequence online, which is fast to compute on resource-constrained systems. The proposed control framework is illustrated on three non-linear MPC benchmarks of different complexity, demonstrating computational
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#29289;&#32852;&#32593;&#12289;&#22823;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#31354;&#27668;&#27745;&#26579;&#30417;&#27979;&#21644;&#39044;&#27979;&#31995;&#32479;&#30340;&#29616;&#29366;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#36825;&#20123;&#25216;&#26415;&#26469;&#25552;&#39640;&#31354;&#27668;&#36136;&#37327;&#25968;&#25454;&#30340;&#25910;&#38598;&#12289;&#20998;&#26512;&#21644;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09574</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#32852;&#32593;&#12289;&#22823;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#31354;&#27668;&#27745;&#26579;&#30417;&#27979;&#21644;&#39044;&#27979;&#31995;&#32479;&#30340;&#29616;&#29366;
&lt;/p&gt;
&lt;p&gt;
The State-of-the-Art in Air Pollution Monitoring and Forecasting Systems using IoT, Big Data, and Machine Learning. (arXiv:2304.09574v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#29289;&#32852;&#32593;&#12289;&#22823;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#31354;&#27668;&#27745;&#26579;&#30417;&#27979;&#21644;&#39044;&#27979;&#31995;&#32479;&#30340;&#29616;&#29366;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#36825;&#20123;&#25216;&#26415;&#26469;&#25552;&#39640;&#31354;&#27668;&#36136;&#37327;&#25968;&#25454;&#30340;&#25910;&#38598;&#12289;&#20998;&#26512;&#21644;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#36136;&#37327;&#19982;&#20154;&#31867;&#12289;&#26893;&#34987;&#21644;&#37326;&#29983;&#21160;&#29289;&#30340;&#29983;&#27963;&#36136;&#37327;&#23494;&#20999;&#30456;&#20851;&#12290; &#22240;&#27492;&#65292;&#38656;&#35201;&#23545;&#20854;&#36827;&#34892;&#25345;&#32493;&#30417;&#27979;&#21644;&#20445;&#25252;&#12290; &#20132;&#36890;&#65292;&#24037;&#19994;&#65292;&#24314;&#31569;&#24037;&#22320;&#65292;&#21457;&#30005;&#26426;&#65292;&#28895;&#33457;&#21644;&#28954;&#28903;&#24223;&#21697;&#31561;&#26469;&#28304;&#22312;&#38477;&#20302;&#31354;&#27668;&#36136;&#37327;&#26041;&#38754;&#21344;&#26377;&#37325;&#35201;&#27604;&#20363;&#12290; &#36825;&#20123;&#26469;&#28304;&#38656;&#35201;&#20197;&#23433;&#20840;&#21644;&#21487;&#25511;&#30340;&#26041;&#24335;&#20351;&#29992;&#12290; &#20351;&#29992;&#20256;&#32479;&#30340;&#23454;&#39564;&#23460;&#20998;&#26512;&#25110;&#27599;&#38548;&#20960;&#33521;&#37324;&#23601;&#23433;&#35013;&#22823;&#22411;&#26114;&#36149;&#27169;&#22411;&#24050;&#19981;&#20877;&#39640;&#25928;&#12290; &#38656;&#35201;&#26234;&#33021;&#35774;&#22791;&#26469;&#25910;&#38598;&#21644;&#20998;&#26512;&#31354;&#27668;&#25968;&#25454;&#12290; &#31354;&#27668;&#36136;&#37327;&#21462;&#20915;&#20110;&#21508;&#31181;&#22240;&#32032;&#65292;&#21253;&#25324;&#20301;&#32622;&#65292;&#20132;&#36890;&#21644;&#26102;&#38388;&#12290; &#26368;&#36817;&#30340;&#30740;&#31350;&#27491;&#22312;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#22823;&#25968;&#25454;&#25216;&#26415;&#21644;&#29289;&#32852;&#32593;&#26469;&#25552;&#20986;&#31283;&#23450;&#32780;&#26377;&#25928;&#30340;&#27169;&#22411;&#26469;&#23454;&#29616;&#19978;&#36848;&#30446;&#30340;&#12290; &#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#21644;&#32534;&#35793;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#25968;&#25454;&#28304;&#65292;&#30417;&#27979;&#21644;&#39044;&#27979;&#27169;&#22411;&#12290; &#26412;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#27010;&#36848;&#20351;&#29992;&#29289;&#32852;&#32593;&#65292;&#22823;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#31354;&#27668;&#27745;&#26579;&#30417;&#27979;&#21644;&#39044;&#27979;&#30340;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of air is closely linked with the life quality of humans, plantations, and wildlife. It needs to be monitored and preserved continuously. Transportations, industries, construction sites, generators, fireworks, and waste burning have a major percentage in degrading the air quality. These sources are required to be used in a safe and controlled manner. Using traditional laboratory analysis or installing bulk and expensive models every few miles is no longer efficient. Smart devices are needed for collecting and analyzing air data. The quality of air depends on various factors, including location, traffic, and time. Recent researches are using machine learning algorithms, big data technologies, and the Internet of Things to propose a stable and efficient model for the stated purpose. This review paper focuses on studying and compiling recent research in this field and emphasizes the Data sources, Monitoring, and Forecasting models. The main objective of this paper is to provid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#22122;&#20313;&#24358;&#30456;&#20284;&#24230;&#65288;dCS&#65289;&#25439;&#22833;&#20989;&#25968;&#65292; &#21487;&#20197;&#29992;&#20110;&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#40065;&#26834;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;</title><link>http://arxiv.org/abs/2304.09552</link><description>&lt;p&gt;
&#21435;&#22122;&#20313;&#24358;&#30456;&#20284;&#24230;&#65306;&#19968;&#31181;&#29702;&#35770;&#39537;&#21160;&#30340;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Denoising Cosine Similarity: A Theory-Driven Approach for Efficient Representation Learning. (arXiv:2304.09552v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#22122;&#20313;&#24358;&#30456;&#20284;&#24230;&#65288;dCS&#65289;&#25439;&#22833;&#20989;&#25968;&#65292; &#21487;&#20197;&#29992;&#20110;&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#40065;&#26834;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#20013;&#21457;&#25381;&#30528;&#36234;&#26469;&#36234;&#22823;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#23398;&#20064;&#20986;&#21487;&#20197;&#26377;&#25928;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24456;&#23569;&#20851;&#27880;&#21040;&#34920;&#31034;&#23398;&#20064;&#38454;&#27573;&#20013;&#20351;&#29992;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#36890;&#24120;&#20250;&#21463;&#21040;&#22122;&#22768;&#27745;&#26579;&#65292;&#36825;&#21487;&#33021;&#20250;&#38477;&#20302;&#23398;&#20064;&#20986;&#30340;&#34920;&#31034;&#24418;&#24335;&#30340;&#36136;&#37327;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#40065;&#26834;&#34920;&#31034;&#24418;&#24335;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#21463;&#21040;&#26368;&#36817;&#21435;&#22122;&#30456;&#20851;&#24037;&#20316;&#21644;&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#30446;&#26631;&#20989;&#25968;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21435;&#22122;&#20313;&#24358;&#30456;&#20284;&#24230;&#65288;dCS&#65289;&#25439;&#22833;&#20989;&#25968;&#12290;dCS&#25439;&#22833;&#26159;&#19968;&#31181;&#20462;&#25913;&#36807;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#25439;&#22833;&#20989;&#25968;&#65292;&#20855;&#26377;&#21435;&#22122;&#23646;&#24615;&#65292;&#36825;&#19968;&#28857;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#30340;&#25903;&#25345;&#12290;&#20026;&#20102;&#20351;dCS&#25439;&#22833;&#21487;&#23454;&#29616;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#20855;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;dCS&#25439;&#22833;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning has been increasing its impact on the research and practice of machine learning, since it enables to learn representations that can apply to various downstream tasks efficiently. However, recent works pay little attention to the fact that real-world datasets used during the stage of representation learning are commonly contaminated by noise, which can degrade the quality of learned representations. This paper tackles the problem to learn robust representations against noise in a raw dataset. To this end, inspired by recent works on denoising and the success of the cosine-similarity-based objective functions in representation learning, we propose the denoising Cosine-Similarity (dCS) loss. The dCS loss is a modified cosine-similarity loss and incorporates a denoising property, which is supported by both our theoretical and empirical findings. To make the dCS loss implementable, we also construct the estimators of the dCS loss with statistical guarantees. Finally,
&lt;/p&gt;</description></item><item><title>SemEval 2023&#20030;&#21150;&#20102;LegalEval&#20849;&#20139;&#20219;&#21153;&#65292;&#21363;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#65292;&#21253;&#25324; &#33258;&#21160;&#32467;&#26500;&#21270;&#21644;&#35821;&#20041;&#36830;&#36143;&#21270;&#30340;&#27861;&#24459;&#25991;&#20214;&#65288;Task-A&#65289;&#65292;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;Task-B&#65289;&#20197;&#21450;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#21644;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#65288;Task-C&#65289;&#12290;26&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#24182;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2304.09548</link><description>&lt;p&gt;
SemEval 2023 &#20219;&#21153;6: LegalEval -- &#29702;&#35299;&#27861;&#24459;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
SemEval 2023 Task 6: LegalEval -- Understanding Legal Texts. (arXiv:2304.09548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09548
&lt;/p&gt;
&lt;p&gt;
SemEval 2023&#20030;&#21150;&#20102;LegalEval&#20849;&#20139;&#20219;&#21153;&#65292;&#21363;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#65292;&#21253;&#25324; &#33258;&#21160;&#32467;&#26500;&#21270;&#21644;&#35821;&#20041;&#36830;&#36143;&#21270;&#30340;&#27861;&#24459;&#25991;&#20214;&#65288;Task-A&#65289;&#65292;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;Task-B&#65289;&#20197;&#21450;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#21644;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#65288;Task-C&#65289;&#12290;26&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#24182;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#21475;&#20247;&#22810;&#30340;&#22269;&#23478;&#65292;&#24453;&#22788;&#29702;&#30340;&#27861;&#24459;&#26696;&#20214;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#26377;&#24517;&#35201;&#24320;&#21457;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25216;&#26415;&#65292;&#23545;&#27861;&#24459;&#25991;&#20214;&#36827;&#34892;&#22788;&#29702;&#21644;&#33258;&#21160;&#29702;&#35299;&#12290;&#20026;&#20102;&#20419;&#36827;&#22312;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#22312; SemEval 2023 &#19978;&#32452;&#32455;&#20102;&#20849;&#20139;&#20219;&#21153; LegalEval - &#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#12290;LegalEval &#20219;&#21153;&#26377;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;Task-A&#65288;&#20462;&#36766;&#35282;&#33394;&#26631;&#35760;&#65289;&#26159;&#33258;&#21160;&#23558;&#27861;&#24459;&#25991;&#20214;&#32467;&#26500;&#21270;&#20026;&#35821;&#20041;&#36830;&#36143;&#30340;&#21333;&#20803;&#65292;Task-B&#65288;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65289;&#22788;&#29702;&#22312;&#27861;&#24459;&#25991;&#20214;&#20013;&#35782;&#21035;&#30456;&#20851;&#23454;&#20307;&#65292;&#32780; Task-C&#65288;&#27861;&#38498;&#21028;&#20915;&#39044;&#27979;&#19982;&#35299;&#37322;&#65289;&#25506;&#32034;&#20102;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#20197;&#21450;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#30340;&#21487;&#33021;&#24615;&#12290;&#20849;&#26377;26&#20010;&#22242;&#38431;&#65288;&#20998;&#24067;&#22312;&#20840;&#29699;&#30340;&#32422;100&#21517;&#21442;&#19982;&#32773;&#65289;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#12290;&#22312;&#27599;&#20010;&#23376;&#20219;&#21153;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#37117;&#20248;&#20110;&#22522;&#20934;&#32447;&#65307;&#20294;&#26159;&#65292;&#20173;&#28982;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; LegalEval &#20219;&#21153;&#30340;&#32452;&#32455;&#21644;&#32454;&#33410;&#65292;&#24182;&#27010;&#36848;&#20102;&#21442;&#19982;&#31995;&#32479;&#21450;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In populous countries, pending legal cases have been growing exponentially. There is a need for developing NLP-based techniques for processing and automatically understanding legal documents. To promote research in the area of Legal NLP we organized the shared task LegalEval - Understanding Legal Texts at SemEval 2023. LegalEval task has three sub-tasks: Task-A (Rhetorical Roles Labeling) is about automatically structuring legal documents into semantically coherent units, Task-B (Legal Named Entity Recognition) deals with identifying relevant entities in a legal document and Task-C (Court Judgement Prediction with Explanation) explores the possibility of automatically predicting the outcome of a legal case along with providing an explanation for the prediction. In total 26 teams (approx. 100 participants spread across the world) submitted systems paper. In each of the sub-tasks, the proposed systems outperformed the baselines; however, there is a lot of scope for improvement. This pape
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#36890;&#20449;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#25216;&#26415;&#65292;&#22312;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#19979;&#20272;&#35745;&#29366;&#24577;&#34892;&#20026;&#31354;&#38388;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#25506;&#32034;&#34892;&#20026;&#65292;&#21516;&#26102;&#20063;&#19981;&#38656;&#35201;&#35745;&#25968;&#26426;&#21046;&#21644;&#22797;&#26434;&#30340;&#36716;&#25442;&#25216;&#26415;&#65292;&#35813;&#26041;&#26696;&#20801;&#35768;&#26234;&#33021;&#20307;&#22312;&#23436;&#20840;&#20998;&#25955;&#30340;&#26041;&#24335;&#19979;&#36827;&#34892;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2304.09547</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;Q&#23398;&#20064;&#30340;&#22270;&#25506;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Exploration for Effective Multi-agent Q-Learning. (arXiv:2304.09547v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#36890;&#20449;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#25216;&#26415;&#65292;&#22312;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#19979;&#20272;&#35745;&#29366;&#24577;&#34892;&#20026;&#31354;&#38388;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#25506;&#32034;&#34892;&#20026;&#65292;&#21516;&#26102;&#20063;&#19981;&#38656;&#35201;&#35745;&#25968;&#26426;&#21046;&#21644;&#22797;&#26434;&#30340;&#36716;&#25442;&#25216;&#26415;&#65292;&#35813;&#26041;&#26696;&#20801;&#35768;&#26234;&#33021;&#20307;&#22312;&#23436;&#20840;&#20998;&#25955;&#30340;&#26041;&#24335;&#19979;&#36827;&#34892;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#22522;&#20110;&#22270;&#36890;&#20449;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#25506;&#32034;&#25216;&#26415;&#12290;&#25105;&#20204;&#20551;&#35774;&#26234;&#33021;&#20307;&#25910;&#21040;&#30340;&#20010;&#20307;&#22870;&#21169;&#21644;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#21160;&#20316;&#26080;&#20851;&#65292;&#20294;&#31574;&#30053;&#30456;&#20114;&#32806;&#21512;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#30456;&#37051;&#30340;&#26234;&#33021;&#20307;&#21512;&#20316;&#20272;&#35745;&#29366;&#24577;&#34892;&#20026;&#31354;&#38388;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#25191;&#34892;&#26356;&#26377;&#25928;&#30340;&#25506;&#32034;&#34892;&#20026;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#19981;&#38656;&#35201;&#35745;&#25968;&#26426;&#21046;&#65292;&#24182;&#21487;&#20197;&#24212;&#29992;&#20110;&#36830;&#32493;&#29366;&#24577;&#29615;&#22659;&#65292;&#32780;&#26080;&#38656;&#22797;&#26434;&#30340;&#36716;&#25442;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#20801;&#35768;&#26234;&#33021;&#20307;&#20197;&#23436;&#20840;&#20998;&#25955;&#30340;&#26041;&#24335;&#36827;&#34892;&#36890;&#20449;&#65292;&#26368;&#23567;&#21270;&#20449;&#24687;&#20132;&#25442;&#12290;&#23545;&#20110;&#36830;&#32493;&#29366;&#24577;&#22330;&#26223;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#21482;&#38656;&#35201;&#20132;&#25442;&#19968;&#20010;&#21442;&#25968;&#21521;&#37327;&#12290;&#31163;&#25955;&#29366;&#24577;&#22330;&#26223;&#30340;&#29702;&#35770;&#32467;&#26524;&#20197;&#21450;&#36830;&#32493;&#29366;&#24577;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an exploration technique for multi-agent reinforcement learning (MARL) with graph-based communication among agents. We assume the individual rewards received by the agents are independent of the actions by the other agents, while their policies are coupled. In the proposed framework, neighbouring agents collaborate to estimate the uncertainty about the state-action space in order to execute more efficient explorative behaviour. Different from existing works, the proposed algorithm does not require counting mechanisms and can be applied to continuous-state environments without requiring complex conversion techniques. Moreover, the proposed scheme allows agents to communicate in a fully decentralized manner with minimal information exchange. And for continuous-state scenarios, each agent needs to exchange only a single parameter vector. The performance of the algorithm is verified with theoretical results for discrete-state scenarios and with experiments for continuou
&lt;/p&gt;</description></item><item><title>SelfAct&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#26080;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#27599;&#20010;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#27963;&#21160;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2304.09530</link><description>&lt;p&gt;
SelfAct: &#22522;&#20110;&#33258;&#30417;&#30563;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#20010;&#24615;&#21270;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
SelfAct: Personalized Activity Recognition based on Self-Supervised and Active Learning. (arXiv:2304.09530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09530
&lt;/p&gt;
&lt;p&gt;
SelfAct&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#26080;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#27599;&#20010;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#27963;&#21160;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#24050;&#32463;&#25104;&#20026;&#20102;&#31359;&#25140;&#35774;&#22791;&#21644;&#31227;&#21160;&#35774;&#22791;&#19978;&#24212;&#29992;&#24191;&#27867;&#30340;&#26041;&#27861;&#65292;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#30446;&#21069;&#39046;&#20808;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#37319;&#38598;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#26159;&#32791;&#26102;&#12289;&#26114;&#36149;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#30001;&#20110;&#27963;&#21160;&#25191;&#34892;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#21487;&#21464;&#24615;&#65292;&#27963;&#21160;&#27169;&#22411;&#24212;&#35813;&#20026;&#27599;&#20010;&#29992;&#25143;&#20010;&#24615;&#21270;&#35774;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SelfAct&#65306;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#33258;&#30417;&#30563;&#21644;&#20027;&#21160;&#23398;&#20064;&#12290;SelfAct&#21033;&#29992;&#20174;&#35768;&#22810;&#29992;&#25143;&#25910;&#38598;&#30340;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#26377;&#24847;&#20041;&#12289;&#39640;&#25928;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#30001;&#27492;&#24471;&#20986;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#34987;&#26032;&#29992;&#25143;&#24403;&#22320;&#20351;&#29992;&#65292;&#20182;&#20204;&#23558;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#26469;&#24494;&#35843;&#36825;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SelfAct&#22312;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#25928;&#29575;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised Deep Learning (DL) models are currently the leading approach for sensor-based Human Activity Recognition (HAR) on wearable and mobile devices. However, training them requires large amounts of labeled data whose collection is often time-consuming, expensive, and error-prone. At the same time, due to the intra- and inter-variability of activity execution, activity models should be personalized for each user. In this work, we propose SelfAct: a novel framework for HAR combining self-supervised and active learning to mitigate these problems. SelfAct leverages a large pool of unlabeled data collected from many users to pre-train through self-supervision a DL model, with the goal of learning a meaningful and efficient latent representation of sensor data. The resulting pre-trained model can be locally used by new users, which will fine-tune it thanks to a novel unsupervised active learning strategy. Our experiments on two publicly available HAR datasets demonstrate that SelfAct ac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#23398;&#20064;&#38754;&#20020;&#30340;&#23433;&#20840;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#36890;&#36947;&#65292;&#20351;&#29992;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;R3eLU&#36991;&#20813;&#23646;&#24615;&#25512;&#26029;&#12289;&#25968;&#25454;&#37325;&#32452;&#21644;&#29305;&#24449;&#21163;&#25345;&#31561;&#25915;&#20987;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#24615;&#33021;&#30456;&#24403;&#20110;&#19981;&#20445;&#25252;&#38544;&#31169;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.09515</link><description>&lt;p&gt;
&#38450;&#27490;&#23646;&#24615;&#25512;&#26029;&#12289;&#25968;&#25454;&#37325;&#26500;&#21644;&#29305;&#24449;&#31354;&#38388;&#21163;&#25345;&#25915;&#20987;&#30340;&#23433;&#20840;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Secure Split Learning against Property Inference, Data Reconstruction, and Feature Space Hijacking Attacks. (arXiv:2304.09515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#23398;&#20064;&#38754;&#20020;&#30340;&#23433;&#20840;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#36890;&#36947;&#65292;&#20351;&#29992;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;R3eLU&#36991;&#20813;&#23646;&#24615;&#25512;&#26029;&#12289;&#25968;&#25454;&#37325;&#32452;&#21644;&#29305;&#24449;&#21163;&#25345;&#31561;&#25915;&#20987;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#24615;&#33021;&#30456;&#24403;&#20110;&#19981;&#20445;&#25252;&#38544;&#31169;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#35299;&#20915;&#26469;&#33258;&#19981;&#21516;&#32972;&#26223;&#30340;&#23458;&#25143;&#21644;&#20027;&#26426;&#20849;&#21516;&#23398;&#20064;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20026;&#24694;&#24847;&#25915;&#20987;&#32773;&#21019;&#36896;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#20351;&#20854;&#22312;&#23454;&#38469;&#20351;&#29992;&#20013;&#20855;&#26377;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#38024;&#23545;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#21487;&#33021;&#36973;&#21463;&#30340;&#23646;&#24615;&#25512;&#26029;&#12289;&#25968;&#25454;&#37325;&#32452;&#21644;&#29305;&#24449;&#21163;&#25345;&#31561;&#23041;&#32961;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#30830;&#20445;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#23398;&#20064;&#20445;&#38556;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#38544;&#31169;&#20445;&#25252;&#36890;&#36947;&#65292;&#29992;&#20110;&#23458;&#25143;&#21644;&#20027;&#26426;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#38450;&#27490;&#28508;&#22312;&#30340;&#23041;&#32961;&#65292;&#24182;&#20445;&#35777;&#20102;&#31995;&#32479;&#30340;&#23433;&#20840;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split learning of deep neural networks (SplitNN) has provided a promising solution to learning jointly for the mutual interest of a guest and a host, which may come from different backgrounds, holding features partitioned vertically. However, SplitNN creates a new attack surface for the adversarial participant, holding back its practical use in the real world. By investigating the adversarial effects of highly threatening attacks, including property inference, data reconstruction, and feature hijacking attacks, we identify the underlying vulnerability of SplitNN and propose a countermeasure. To prevent potential threats and ensure the learning guarantees of SplitNN, we design a privacy-preserving tunnel for information exchange between the guest and the host. The intuition is to perturb the propagation of knowledge in each direction with a controllable unified solution. To this end, we propose a new activation function named R3eLU, transferring private smashed data and partial loss int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#32593;&#32476;&#27969;&#37327;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;NetGPT&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20248;&#21270;&#32593;&#32476;&#20219;&#21153;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09513</link><description>&lt;p&gt;
NetGPT&#65306;&#32593;&#32476;&#27969;&#37327;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NetGPT: Generative Pretrained Transformer for Network Traffic. (arXiv:2304.09513v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#32593;&#32476;&#27969;&#37327;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;NetGPT&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20248;&#21270;&#32593;&#32476;&#20219;&#21153;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#21407;&#22987;&#25968;&#25454;&#23398;&#20064;&#32593;&#32476;&#27969;&#37327;&#30340;&#22522;&#26412;&#29305;&#24449;&#65292;&#24182;&#20026;&#36755;&#20837;&#27969;&#37327;&#29983;&#25104;&#21487;&#21306;&#20998;&#30340;&#32467;&#26524;&#65292;&#32780;&#19981;&#32771;&#34385;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#20248;&#21270;&#19979;&#28216;&#20219;&#21153;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#65292;&#20363;&#22914;&#27969;&#37327;&#20998;&#31867;&#12289;&#25915;&#20987;&#26816;&#27979;&#12289;&#36164;&#28304;&#35843;&#24230;&#12289;&#21327;&#35758;&#20998;&#26512;&#21644;&#27969;&#37327;&#29983;&#25104;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;NetGPT&#65292;&#26088;&#22312;&#20026;&#32593;&#32476;&#27969;&#37327;&#26500;&#24314;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#35299;&#20915;&#22810;&#26679;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained models for network traffic can utilize large-scale raw data to learn the essential characteristics of network traffic, and generate distinguishable results for input traffic without considering specific downstream tasks. Effective pretrained models can significantly optimize the training efficiency and effectiveness of downstream tasks, such as traffic classification, attack detection, resource scheduling, protocol analysis, and traffic generation. Despite the great success of pretraining in natural language processing, there is no work in the network field. Considering the diverse demands and characteristics of network traffic and network tasks, it is non-trivial to build a pretrained model for network traffic and we face various challenges, especially the heterogeneous headers and payloads in the multi-pattern network traffic and the different dependencies for contexts of diverse downstream network tasks.  To tackle these challenges, in this paper, we make the first attemp
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20462;&#27491;Medoid-Shift&#65288;RMS&#65289;&#30340;&#26032;&#32858;&#31867;&#31639;&#27861;&#65292;&#29992;&#20110;&#31038;&#21306;&#26816;&#27979;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.09512</link><description>&lt;p&gt;
&#22522;&#20110;KNN&#30340;&#20462;&#27491;Medoid-Shift&#30340;&#31038;&#21306;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Community Detection Using Revised Medoid-Shift Based on KNN. (arXiv:2304.09512v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09512
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20462;&#27491;Medoid-Shift&#65288;RMS&#65289;&#30340;&#26032;&#32858;&#31867;&#31639;&#27861;&#65292;&#29992;&#20110;&#31038;&#21306;&#26816;&#27979;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#32593;&#32476;&#30340;&#20852;&#36215;&#65292;&#31038;&#21306;&#26816;&#27979;&#25104;&#20026;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#22343;&#20540;&#28418;&#31227;&#26159;&#19968;&#31181;&#20986;&#33394;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#20294;&#30001;&#20110;&#22343;&#20540;&#28418;&#31227;&#31639;&#27861;&#21482;&#33021;&#22788;&#29702;&#20855;&#26377;&#22352;&#26631;&#30340;&#25968;&#25454;&#65292;&#32780;&#31038;&#21306;&#26816;&#27979;&#38382;&#39064;&#20013;&#30340;&#25968;&#25454;&#22823;&#22810;&#20197;&#22270;&#30340;&#24418;&#24335;&#34920;&#31034;&#65292;&#21487;&#20197;&#35270;&#20026;&#20855;&#26377;&#36317;&#31163;&#30697;&#38453;&#65288;&#25110;&#30456;&#20284;&#24230;&#30697;&#38453;&#65289;&#30340;&#25968;&#25454;&#65292;&#22240;&#27492;&#22343;&#20540;&#28418;&#31227;&#31639;&#27861;&#19981;&#33021;&#30452;&#25509;&#29992;&#20110;&#31038;&#21306;&#26816;&#27979;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#31639;&#27861; Medoid-Shift&#65292;&#35813;&#31639;&#27861;&#20445;&#30041;&#20102;&#22343;&#20540;&#28418;&#31227;&#30340;&#20248;&#28857;&#65292;&#24182;&#21487;&#20197;&#24212;&#29992;&#20110;&#22522;&#20110;&#36317;&#31163;&#30697;&#38453;&#30340;&#38382;&#39064;&#65292;&#22914;&#31038;&#21306;&#26816;&#27979;&#12290;Medoid-Shift&#31639;&#27861;&#30340;&#19968;&#20010;&#32570;&#28857;&#26159;&#21487;&#33021;&#22312;&#30001;&#36317;&#31163;&#21442;&#25968;&#23450;&#20041;&#30340;&#37051;&#22495;&#21306;&#22495;&#20869;&#27809;&#26377;&#25968;&#25454;&#28857;&#12290; &#20026;&#20102;&#26356;&#22909;&#22320;&#22788;&#29702;&#31038;&#21306;&#26816;&#27979;&#38382;&#39064;&#65292;&#22240;&#27492;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20462;&#27491;Medoid-Shift&#65288;RMS&#65289;&#30340;&#26032;&#31639;&#27861;&#12290; &#22312;&#23547;&#25214;&#19979;&#19968;&#20010;&#20013;&#24515;&#28857;&#30340;&#36807;&#31243;&#20013;&#65292;RMS&#31639;&#27861;&#22522;&#20110;&#37051;&#22495;&#30340;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community detection becomes an important problem with the booming of social networks. As an excellent clustering algorithm, Mean-Shift can not be applied directly to community detection, since Mean-Shift can only handle data with coordinates, while the data in the community detection problem is mostly represented by a graph that can be treated as data with a distance matrix (or similarity matrix). Fortunately, a new clustering algorithm called Medoid-Shift is proposed. The Medoid-Shift algorithm preserves the benefits of Mean-Shift and can be applied to problems based on distance matrix, such as community detection. One drawback of the Medoid-Shift algorithm is that there may be no data points within the neighborhood region defined by a distance parameter. To deal with the community detection problem better, a new algorithm called Revised Medoid-Shift (RMS) in this work is thus proposed. During the process of finding the next medoid, the RMS algorithm is based on a neighborhood defined
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#26080;&#24207;&#30446;&#26631;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#36830;&#32493;&#24615;&#38382;&#39064;&#65292;&#24182;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2304.09499</link><description>&lt;p&gt;
&#26080;&#24207;&#30446;&#26631;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#36131;&#20219;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Responsibility Problem in Neural Networks with Unordered Targets. (arXiv:2304.09499v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#26080;&#24207;&#30446;&#26631;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#36830;&#32493;&#24615;&#38382;&#39064;&#65292;&#24182;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#23558;&#26080;&#24207;&#23545;&#35937;&#26144;&#23556;&#21040;&#22266;&#23450;&#25490;&#21015;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#26102;&#20986;&#29616;&#30340;&#19981;&#36830;&#32493;&#24615;&#65292;&#31216;&#20043;&#20026;&#36131;&#20219;&#38382;&#39064;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#36890;&#36807;&#35782;&#21035;&#21333;&#20010;&#19981;&#36830;&#32493;&#24615;&#35777;&#26126;&#20102;&#35813;&#38382;&#39064;&#30340;&#23384;&#22312;&#12290;&#26412;&#25991;&#35777;&#26126;&#65292;&#22312;&#36825;&#31181;&#27169;&#22411;&#19979;&#19981;&#36830;&#32493;&#24615;&#26159;&#19981;&#21487;&#25968;&#26080;&#38480;&#30340;&#65292;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#36866;&#29992;&#20110;&#26080;&#24207;&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
We discuss the discontinuities that arise when mapping unordered objects to neural network outputs of fixed permutation, referred to as the responsibility problem. Prior work has proved the existence of the issue by identifying a single discontinuity. Here, we show that discontinuities under such models are uncountably infinite, motivating further research into neural networks for unordered data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#21033;&#29992;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;\ddpg&#65289;&#26041;&#27861;&#23398;&#20064;&#36890;&#20449;&#36164;&#28304;&#35843;&#24230;&#31639;&#27861;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#38480;&#21046;&#20110;&#20248;&#20808;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2304.09488</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#36827;&#34892;&#39640;&#20248;&#20808;&#32423;&#29992;&#25143;&#30340;&#23398;&#20064;&#36164;&#28304;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Learning Resource Scheduling with High Priority Users using Deep Deterministic Policy Gradients. (arXiv:2304.09488v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#21033;&#29992;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;\ddpg&#65289;&#26041;&#27861;&#23398;&#20064;&#36890;&#20449;&#36164;&#28304;&#35843;&#24230;&#31639;&#27861;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#38480;&#21046;&#20110;&#20248;&#20808;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#36890;&#20449;&#21151;&#33021;&#30340;&#36827;&#27493;&#20026;&#38498;&#21069;&#21644;&#38498;&#20869;&#25252;&#29702;&#36807;&#31243;&#30340;&#26356;&#32039;&#23494;&#25972;&#21512;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;&#20363;&#22914;&#65292;&#21307;&#23398;&#19987;&#23478;&#21487;&#20197;&#25351;&#23548;&#29616;&#22330;&#21307;&#25252;&#20154;&#21592;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#20379;&#23454;&#26102;&#29983;&#21629;&#20307;&#24449;&#25110;&#35270;&#35273;&#12290;&#23558;&#36825;&#31181;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#30340;&#24212;&#29992;&#19982;&#31227;&#21160;&#36890;&#20449;&#30340;&#39640;&#24230;&#22797;&#26434;&#30340;&#24037;&#20316;&#32467;&#21512;&#36215;&#26469;&#65292;&#38656;&#35201;&#21487;&#38752;&#19988;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#26131;&#20110;&#19982;&#29616;&#26377;&#31995;&#32479;&#38598;&#25104;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;\ddpg&#65289;&#26041;&#27861;&#23398;&#20064;&#36890;&#20449;&#36164;&#28304;&#35843;&#24230;&#31639;&#27861;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#38480;&#21046;&#20110;&#20248;&#20808;&#29992;&#25143;&#12290;&#19982;&#26222;&#36890;&#30340;&#28145;&#24230;Q&#32593;&#32476;&#19981;&#21516;&#65292;\ddpg &#33021;&#22815;&#20135;&#29983;&#36830;&#32493;&#20540;&#30340;&#36755;&#20986;&#12290;&#32463;&#36807;&#36731;&#24494;&#30340;&#21518;&#22788;&#29702;&#65292;&#24471;&#21040;&#30340;&#35843;&#24230;&#22120;&#33021;&#22815;&#22312;&#28789;&#27963;&#30340;&#24635;&#21644;&#25928;&#29992;&#30446;&#26631;&#19978;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in mobile communication capabilities open the door for closer integration of pre-hospital and in-hospital care processes. For example, medical specialists can be enabled to guide on-site paramedics and can, in turn, be supplied with live vitals or visuals. Consolidating such performance-critical applications with the highly complex workings of mobile communications requires solutions both reliable and efficient, yet easy to integrate with existing systems. This paper explores the application of Deep Deterministic Policy Gradient~(\ddpg) methods for learning a communications resource scheduling algorithm with special regards to priority users. Unlike the popular Deep-Q-Network methods, the \ddpg is able to produce continuous-valued output. With light post-processing, the resulting scheduler is able to achieve high performance on a flexible sum-utility goal.
&lt;/p&gt;</description></item><item><title>&#35821;&#38899;&#21161;&#25163;&#24212;&#29992;&#38754;&#20020;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#21253;&#25324;&#25915;&#20987;&#27169;&#22411;&#21644;&#31169;&#20154;&#20449;&#24687;&#27844;&#38706;&#65292;&#25152;&#20197;&#38656;&#35201;&#36827;&#34892;&#20840;&#38754;&#30340;&#35843;&#26597;&#20197;&#20415;&#27010;&#35272;&#24403;&#21069;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2304.09486</link><description>&lt;p&gt;
&#35821;&#38899;&#21161;&#25163;&#24212;&#29992;&#20013;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Security and Privacy Problems in Voice Assistant Applications: A Survey. (arXiv:2304.09486v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09486
&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21161;&#25163;&#24212;&#29992;&#38754;&#20020;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#21253;&#25324;&#25915;&#20987;&#27169;&#22411;&#21644;&#31169;&#20154;&#20449;&#24687;&#27844;&#38706;&#65292;&#25152;&#20197;&#38656;&#35201;&#36827;&#34892;&#20840;&#38754;&#30340;&#35843;&#26597;&#20197;&#20415;&#27010;&#35272;&#24403;&#21069;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#65292;&#35821;&#38899;&#21161;&#25163;&#24212;&#29992;&#24050;&#32463;&#26080;&#22788;&#19981;&#22312;&#12290;&#25552;&#20379;&#29616;&#23454;&#29983;&#27963;&#20013;&#20004;&#20010;&#26368;&#37325;&#35201;&#21151;&#33021;&#65288;&#22914;&#35895;&#27468;&#23478;&#24237;&#12289;&#20122;&#39532;&#36874;Alexa&#12289;Siri&#31561;&#65289;&#30340;&#20004;&#20010;&#20027;&#35201;&#27169;&#22411;&#26159;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#21644;&#35828;&#35805;&#20154;&#35782;&#21035;&#65288;SI&#65289;&#27169;&#22411;&#12290;&#26681;&#25454;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#38543;&#30528;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23433;&#20840;&#21644;&#38544;&#31169;&#23041;&#32961;&#20063;&#20986;&#29616;&#20102;&#12290;&#30740;&#31350;&#30340;&#23433;&#20840;&#38382;&#39064;&#21253;&#25324;&#38024;&#23545;&#35821;&#38899;&#21161;&#25163;&#24212;&#29992;&#31243;&#24207;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#20854;&#20182;&#30828;&#20214;&#32452;&#20214;&#30340;&#25915;&#20987;&#25216;&#26415;&#12290;&#38544;&#31169;&#38382;&#39064;&#21253;&#25324;&#25216;&#26415;&#19978;&#30340;&#20449;&#24687;&#31363;&#21462;&#21644;&#25919;&#31574;&#19978;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#35821;&#38899;&#21161;&#25163;&#24212;&#29992;&#31243;&#24207;&#27599;&#24180;&#21344;&#25454;&#30528;&#19981;&#26029;&#22686;&#38271;&#30340;&#24066;&#22330;&#20221;&#39069;&#65292;&#20294;&#23427;&#20204;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#19968;&#30452;&#22312;&#36896;&#25104;&#24040;&#22823;&#30340;&#32463;&#27982;&#25439;&#22833;&#24182;&#21361;&#21450;&#29992;&#25143;&#30340;&#20010;&#20154;&#25935;&#24863;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#36827;&#34892;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#20197;&#27010;&#36848;&#24403;&#21069;&#30740;&#31350;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Voice assistant applications have become omniscient nowadays. Two models that provide the two most important functions for real-life applications (i.e., Google Home, Amazon Alexa, Siri, etc.) are Automatic Speech Recognition (ASR) models and Speaker Identification (SI) models. According to recent studies, security and privacy threats have also emerged with the rapid development of the Internet of Things (IoT). The security issues researched include attack techniques toward machine learning models and other hardware components widely used in voice assistant applications. The privacy issues include technical-wise information stealing and policy-wise privacy breaches. The voice assistant application takes a steadily growing market share every year, but their privacy and security issues never stopped causing huge economic losses and endangering users' personal sensitive information. Thus, it is important to have a comprehensive survey to outline the categorization of the current research r
&lt;/p&gt;</description></item><item><title>DiFaReli&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#35299;&#30721;&#35299;&#32806;&#30340;&#20809;&#32534;&#30721;&#20197;&#21450;&#20174;&#29616;&#25104;&#30340;&#20272;&#31639;&#22120;&#25512;&#26029;&#20986;&#30340;&#19982;3D&#24418;&#29366;&#21644;&#38754;&#37096;&#36523;&#20221;&#30456;&#20851;&#30340;&#20854;&#20182;&#32534;&#30721;&#65292;&#33021;&#22815;&#22788;&#29702;&#21333;&#35270;&#35282;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#30340;&#20154;&#33080;&#37325;&#29031;&#65292;&#26080;&#38656;&#20809;&#32447;&#33310;&#21488;&#25968;&#25454;&#12289;&#22810;&#35270;&#22270;&#22270;&#20687;&#25110;&#20809;&#29031;&#22522;&#30784;&#20107;&#23454;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.09479</link><description>&lt;p&gt;
DiFaReli: &#25193;&#25955;&#20154;&#33080;&#37325;&#29031;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
DiFaReli : Diffusion Face Relighting. (arXiv:2304.09479v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09479
&lt;/p&gt;
&lt;p&gt;
DiFaReli&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#35299;&#30721;&#35299;&#32806;&#30340;&#20809;&#32534;&#30721;&#20197;&#21450;&#20174;&#29616;&#25104;&#30340;&#20272;&#31639;&#22120;&#25512;&#26029;&#20986;&#30340;&#19982;3D&#24418;&#29366;&#21644;&#38754;&#37096;&#36523;&#20221;&#30456;&#20851;&#30340;&#20854;&#20182;&#32534;&#30721;&#65292;&#33021;&#22815;&#22788;&#29702;&#21333;&#35270;&#35282;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#30340;&#20154;&#33080;&#37325;&#29031;&#65292;&#26080;&#38656;&#20809;&#32447;&#33310;&#21488;&#25968;&#25454;&#12289;&#22810;&#35270;&#22270;&#22270;&#20687;&#25110;&#20809;&#29031;&#22522;&#30784;&#20107;&#23454;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#37326;&#22806;&#29615;&#22659;&#19979;&#30340;&#21333;&#35270;&#35282;&#20154;&#33080;&#37325;&#29031;&#12290;&#22788;&#29702;&#20840;&#23616;&#29031;&#26126;&#25110;&#25237;&#24433;&#38452;&#24433;&#31561;&#38750;&#28459;&#21453;&#23556;&#25928;&#24212;&#19968;&#30452;&#26159;&#20154;&#33080;&#37325;&#29031;&#39046;&#22495;&#30340;&#38590;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#20551;&#23450;&#20848;&#20271;&#29305;&#21453;&#23556;&#34920;&#38754;&#65292;&#31616;&#21270;&#20809;&#29031;&#27169;&#22411;&#65292;&#25110;&#32773;&#38656;&#35201;&#20272;&#35745;&#19977;&#32500;&#24418;&#29366;&#12289;&#21453;&#23556;&#29575;&#25110;&#38452;&#24433;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20272;&#35745;&#26159;&#23481;&#26131;&#20986;&#38169;&#30340;&#65292;&#38656;&#35201;&#35768;&#22810;&#20855;&#26377;&#20809;&#29031;&#22522;&#30784;&#20107;&#23454;&#30340;&#35757;&#32451;&#26679;&#26412;&#25165;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32469;&#36807;&#20102;&#20934;&#30830;&#20272;&#35745;&#22266;&#26377;&#32452;&#20214;&#30340;&#38656;&#35201;&#65292;&#21487;&#20197;&#20165;&#36890;&#36807;2D&#22270;&#20687;&#35757;&#32451;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#20809;&#32447;&#33310;&#21488;&#25968;&#25454;&#12289;&#22810;&#35270;&#22270;&#22270;&#20687;&#25110;&#20809;&#29031;&#22522;&#30784;&#20107;&#23454;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#65288;DDIM&#65289;&#35299;&#30721;&#35299;&#32806;&#30340;&#20809;&#32534;&#30721;&#20197;&#21450;&#20174;&#29616;&#25104;&#30340;&#20272;&#31639;&#22120;&#25512;&#26029;&#20986;&#30340;&#19982;3D&#24418;&#29366;&#21644;&#38754;&#37096;&#36523;&#20221;&#30456;&#20851;&#30340;&#20854;&#20182;&#32534;&#30721;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#33410;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#24402;&#19968;&#21270;&#26041;&#26696;&#65292;&#31616;&#21270;&#20809;&#19982;&#20960;&#20309;&#20043;&#38388;&#22797;&#26434;&#20114;&#21160;&#30340;&#24314;&#27169;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to single-view face relighting in the wild. Handling non-diffuse effects, such as global illumination or cast shadows, has long been a challenge in face relighting. Prior work often assumes Lambertian surfaces, simplified lighting models or involves estimating 3D shape, albedo, or a shadow map. This estimation, however, is error-prone and requires many training examples with lighting ground truth to generalize well. Our work bypasses the need for accurate estimation of intrinsic components and can be trained solely on 2D images without any light stage data, multi-view images, or lighting ground truth. Our key idea is to leverage a conditional diffusion implicit model (DDIM) for decoding a disentangled light encoding along with other encodings related to 3D shape and facial identity inferred from off-the-shelf estimators. We also propose a novel conditioning technique that eases the modeling of the complex interaction between light and geometry by using a ren
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MAMAF-Net&#30340;&#32593;&#32476;&#29992;&#20110;&#26816;&#27979;&#22810;&#27169;&#24577;&#26816;&#26597;&#35270;&#39057;&#20013;&#30340;&#20013;&#39118;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#25968;&#37319;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#30340;&#35270;&#39057;&#20998;&#26512;&#20013;&#30340;&#20013;&#39118;&#26816;&#27979;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2304.09466</link><description>&lt;p&gt;
MAMAF-Net&#65306;&#29992;&#20110;&#20013;&#39118;&#35786;&#26029;&#30340;&#36816;&#21160;&#24863;&#30693;&#21644;&#22810;&#20851;&#27880;&#34701;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MAMAF-Net: Motion-Aware and Multi-Attention Fusion Network for Stroke Diagnosis. (arXiv:2304.09466v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MAMAF-Net&#30340;&#32593;&#32476;&#29992;&#20110;&#26816;&#27979;&#22810;&#27169;&#24577;&#26816;&#26597;&#35270;&#39057;&#20013;&#30340;&#20013;&#39118;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#25968;&#37319;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#30340;&#35270;&#39057;&#20998;&#26512;&#20013;&#30340;&#20013;&#39118;&#26816;&#27979;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#39118;&#26159;&#20840;&#29699;&#27515;&#20129;&#29575;&#21644;&#27531;&#30142;&#29575;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#27599;&#22235;&#20154;&#20013;&#23601;&#26377;&#19968;&#20154;&#26377;&#20013;&#39118;&#30340;&#21361;&#38505;&#12290;&#39044;&#20808;&#21307;&#38498;&#20013;&#39118;&#35780;&#20272;&#22312;&#20934;&#30830;&#35782;&#21035;&#20013;&#39118;&#24739;&#32773;&#20197;&#21152;&#36895;&#36827;&#19968;&#27493;&#26816;&#26597;&#21644;&#27835;&#30103;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAMAF-Net&#30340;&#36816;&#21160;&#24863;&#30693;&#21644;&#22810;&#20851;&#27880;&#34701;&#21512;&#32593;&#32476;&#65292;&#21487;&#20197;&#26816;&#27979;&#22810;&#27169;&#24335;&#26816;&#26597;&#35270;&#39057;&#20013;&#30340;&#20013;&#39118;&#24773;&#20917;&#12290;&#19982;&#20854;&#20182;&#35270;&#39057;&#20998;&#26512;&#20013;&#30340;&#20013;&#39118;&#26816;&#27979;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#25968;&#37319;&#26679;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20174;&#27599;&#20010;&#34987;&#35797;&#32773;&#30340;&#22810;&#20010;&#35270;&#39057;&#35760;&#24405;&#20013;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#20013;&#39118;&#12289;&#30701;&#26242;&#24615;&#32570;&#34880;&#24615;&#21457;&#20316;&#65288;TIA&#65289;&#21644;&#20581;&#24247;&#23545;&#29031;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stroke is a major cause of mortality and disability worldwide from which one in four people are in danger of incurring in their lifetime. The pre-hospital stroke assessment plays a vital role in identifying stroke patients accurately to accelerate further examination and treatment in hospitals. Accordingly, the National Institutes of Health Stroke Scale (NIHSS), Cincinnati Pre-hospital Stroke Scale (CPSS) and Face Arm Speed Time (F.A.S.T.) are globally known tests for stroke assessment. However, the validity of these tests is skeptical in the absence of neurologists. Therefore, in this study, we propose a motion-aware and multi-attention fusion network (MAMAF-Net) that can detect stroke from multimodal examination videos. Contrary to other studies on stroke detection from video analysis, our study for the first time proposes an end-to-end solution from multiple video recordings of each subject with a dataset encapsulating stroke, transient ischemic attack (TIA), and healthy controls. T
&lt;/p&gt;</description></item><item><title>EC^2 &#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#32039;&#24613;&#36890;&#20449;&#26041;&#26696;&#65292;&#29992;&#20110;&#35270;&#39057;&#35821;&#35328;&#39044;&#35757;&#32451;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#36523;&#20307;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312; EmbodiedAI &#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.09448</link><description>&lt;p&gt;
EC^2: &#22522;&#20110;&#36523;&#20307;&#25511;&#21046;&#30340;&#26032;&#22411;&#32039;&#24613;&#20132;&#27969;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
EC^2: Emergent Communication for Embodied Control. (arXiv:2304.09448v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09448
&lt;/p&gt;
&lt;p&gt;
EC^2 &#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#32039;&#24613;&#36890;&#20449;&#26041;&#26696;&#65292;&#29992;&#20110;&#35270;&#39057;&#35821;&#35328;&#39044;&#35757;&#32451;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#36523;&#20307;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312; EmbodiedAI &#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36523;&#20307;&#25511;&#21046;&#38656;&#35201;&#20195;&#29702;&#36890;&#36807;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#24555;&#36895;&#23398;&#20064;&#22914;&#20309;&#22312;&#26032;&#29615;&#22659;&#20013;&#34892;&#21160;&#65292;&#20854;&#20013;&#35270;&#39057;&#28436;&#31034;&#21253;&#21547;&#25152;&#38656;&#30340;&#35270;&#35273;&#21644;&#36816;&#21160;&#32454;&#33410;&#20197;&#36827;&#34892;&#20302;&#32423;&#21035;&#30693;&#35273;&#21644;&#25511;&#21046;&#65292;&#35821;&#35328;&#25351;&#20196;&#25903;&#25345;&#36890;&#36807;&#25277;&#35937;&#31526;&#21495;&#32467;&#26500;&#36827;&#34892;&#27867;&#21270;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#26041;&#27861;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#24378;&#21046;&#20004;&#31181;&#27169;&#24335;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#20294;&#25105;&#20204;&#20551;&#35774;&#26356;&#22909;&#22320;&#24314;&#27169;&#23427;&#20204;&#20043;&#38388;&#30340;&#20114;&#34917;&#24046;&#24322;&#21487;&#20197;&#24102;&#26469;&#26356;&#20840;&#38754;&#30340;&#34920;&#31034;&#20197;&#36827;&#34892;&#19979;&#28216;&#36866;&#24212;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Emergent Communication for Embodied Control (EC^2)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#35270;&#39057;&#35821;&#35328;&#34920;&#31034;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#36523;&#20307;&#25511;&#21046;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#32039;&#24613;&#36890;&#20449;&#23398;&#20064;&#35270;&#39057;&#30340;&#8220;&#35821;&#35328;&#8221;&#65292;&#23427;&#26725;&#25509;&#20102;&#35270;&#39057;&#32454;&#33410;&#30340;&#35821;&#20041;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35270;&#39057;&#36712;&#36857;&#65292;&#32039;&#24613;&#35821;&#35328;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#36523;&#20307;&#34920;&#31034;&#65292;&#28982;&#21518;&#23545;&#23427;&#20204;&#36827;&#34892;&#24494;&#35843;&#20197;&#36827;&#34892;&#36523;&#20307;&#25511;&#21046;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312; EmbodiedAI &#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272; EC^2&#65292;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied control requires agents to leverage multi-modal pre-training to quickly learn how to act in new environments, where video demonstrations contain visual and motion details needed for low-level perception and control, and language instructions support generalization with abstract, symbolic structures. While recent approaches apply contrastive learning to force alignment between the two modalities, we hypothesize better modeling their complementary differences can lead to more holistic representations for downstream adaption. To this end, we propose Emergent Communication for Embodied Control (EC^2), a novel scheme to pre-train video-language representations for few-shot embodied control. The key idea is to learn an unsupervised "language" of videos via emergent communication, which bridges the semantics of video details and structures of natural language. We learn embodied representations of video trajectories, emergent language, and natural language using a language model, whic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38789;&#21518;&#39564;&#30340;&#31070;&#32463;&#36807;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#38544;&#24335;&#23450;&#20041;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#22312; benchmark &#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.09431</link><description>&lt;p&gt;
&#38789;&#21518;&#39564;&#31070;&#32463;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Martingale Posterior Neural Processes. (arXiv:2304.09431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38789;&#21518;&#39564;&#30340;&#31070;&#32463;&#36807;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#38544;&#24335;&#23450;&#20041;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#22312; benchmark &#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36807;&#31243;(NP)&#21487;&#29992;&#20110;&#20272;&#35745;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#38544;&#24335;&#23450;&#20041;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#32780;&#19981;&#26159;&#39044;&#20808;&#35268;&#23450;&#24050;&#30693;&#20808;&#39564;&#30340;&#36807;&#31243;&#65292;&#20363;&#22914;&#39640;&#26031;&#36807;&#31243;&#12290;&#29702;&#24819;&#30340;NP&#23558;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20999;&#32780;&#27809;&#26377;&#20219;&#20309;&#24402;&#32435;&#20559;&#24046;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#24120;&#24120;&#20026;&#20102;&#26041;&#20415;&#20272;&#35745;&#32780;&#38480;&#21046;&#20102;&#38543;&#26426;&#36807;&#31243;&#30340;&#31867;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38789;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#20026;&#26410;&#26469;&#25968;&#25454;&#25351;&#23450;&#20102;&#19968;&#31181;&#39044;&#27979;&#20998;&#24067;&#65292;&#20174;&#32780;&#20943;&#23567;&#20102;&#29983;&#25104;&#20989;&#25968;&#26102;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;MPNP&#26694;&#26550;&#65292;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;NP&#26041;&#27861;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Neural Process (NP) estimates a stochastic process implicitly defined with neural networks given a stream of data, rather than pre-specifying priors already known, such as Gaussian processes. An ideal NP would learn everything from data without any inductive biases, but in practice, we often restrict the class of stochastic processes for the ease of estimation. One such restriction is the use of a finite-dimensional latent variable accounting for the uncertainty in the functions drawn from NPs. Some recent works show that this can be improved with more "data-driven" source of uncertainty such as bootstrapping. In this work, we take a different approach based on the martingale posterior, a recently developed alternative to Bayesian inference. For the martingale posterior, instead of specifying prior-likelihood pairs, a predictive distribution for future data is specified. Under specific conditions on the predictive distribution, it can be shown that the uncertainty in the generated fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#32806;&#35757;&#32451;&#21644;&#21033;&#29992;&#38543;&#26426;&#34920;&#31034;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#23614;&#20998;&#31867;&#38382;&#39064;&#65292;&#20351;&#29992;SWA&#20248;&#21270;&#25216;&#26415;&#24471;&#21040;&#26356;&#22909;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#34920;&#31034;&#36827;&#34892;&#20998;&#31867;&#22120;&#37325;&#26032;&#35757;&#32451;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.09426</link><description>&lt;p&gt;
&#38271;&#23614;&#20998;&#31867;&#30340;&#35299;&#32806;&#35757;&#32451;&#21644;&#38543;&#26426;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decoupled Training for Long-Tailed Classification With Stochastic Representations. (arXiv:2304.09426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#32806;&#35757;&#32451;&#21644;&#21033;&#29992;&#38543;&#26426;&#34920;&#31034;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#23614;&#20998;&#31867;&#38382;&#39064;&#65292;&#20351;&#29992;SWA&#20248;&#21270;&#25216;&#26415;&#24471;&#21040;&#26356;&#22909;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#34920;&#31034;&#36827;&#34892;&#20998;&#31867;&#22120;&#37325;&#26032;&#35757;&#32451;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38271;&#23614;&#25968;&#25454;&#20998;&#31867;&#20013;&#65292;&#35299;&#32806;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23398;&#20064;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#26500;&#24314;&#35299;&#32806;&#24335;&#23398;&#20064;&#26041;&#26696;&#26377;&#20004;&#20010;&#20027;&#35201;&#22240;&#32032;&#65306;1&#65289;&#22914;&#20309;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#20415;&#25552;&#20379;&#21487;&#27867;&#21270;&#30340;&#34920;&#31034;&#65307;2&#65289;&#22914;&#20309;&#37325;&#26032;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#22788;&#29702;&#38271;&#23614;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#26469;&#26500;&#24314;&#36866;&#24403;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290; &#26412;&#25991;&#39318;&#20808;&#24212;&#29992;&#38543;&#26426;&#26435;&#37325;&#24179;&#22343;&#65288;SWA&#65289;&#20248;&#21270;&#25216;&#26415;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#29992;&#20110;&#38271;&#23614;&#20998;&#31867;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;SWA-Gaussian&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#22120;&#37325;&#26032;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#22810;&#26679;&#38543;&#26426;&#34920;&#31034;&#26469;&#26500;&#24314;&#26356;&#24378;&#22823;&#30340;&#20998;&#31867;&#22120;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#38271;&#23614;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoupling representation learning and classifier learning has been shown to be effective in classification with long-tailed data. There are two main ingredients in constructing a decoupled learning scheme; 1) how to train the feature extractor for representation learning so that it provides generalizable representations and 2) how to re-train the classifier that constructs proper decision boundaries by handling class imbalances in long-tailed data. In this work, we first apply Stochastic Weight Averaging (SWA), an optimization technique for improving the generalization of deep neural networks, to obtain better generalizing feature extractors for long-tailed classification. We then propose a novel classifier re-training algorithm based on stochastic representation obtained from the SWA-Gaussian, a Gaussian perturbed SWA, and a self-distillation strategy that can harness the diverse stochastic representations based on uncertainty estimates to build more robust classifiers. Extensive exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#23545;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#65292;&#26368;&#20248;&#22320;&#26368;&#23567;&#21270;&#25439;&#22833;&#20250;&#23548;&#33268;&#22810;&#26657;&#20934;&#65292;&#20197;&#25552;&#20379;&#20844;&#24179;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09424</link><description>&lt;p&gt;
&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#26657;&#20934;&#21487;&#26368;&#23567;&#21270;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Loss minimization yields multicalibration for large neural networks. (arXiv:2304.09424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#23545;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#65292;&#26368;&#20248;&#22320;&#26368;&#23567;&#21270;&#25439;&#22833;&#20250;&#23548;&#33268;&#22810;&#26657;&#20934;&#65292;&#20197;&#25552;&#20379;&#20844;&#24179;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26657;&#20934;&#26159;&#19968;&#31181;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#26088;&#22312;&#25552;&#20379;&#36328;&#22823;&#37327;&#22242;&#20307;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#21363;&#20351;&#23545;&#20110;&#31616;&#21333;&#30340;&#39044;&#27979;&#22120;&#65292;&#22914;&#32447;&#24615;&#20989;&#25968;&#65292;&#22810;&#26657;&#20934;&#20063;&#34987;&#35748;&#20026;&#26159;&#19982;&#26368;&#23567;&#21270;&#25439;&#22833;&#19981;&#21516;&#30340;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#65288;&#20960;&#20046;&#25152;&#26377;&#30340;&#65289;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#65292;&#26368;&#20248;&#22320;&#26368;&#23567;&#21270;&#24179;&#26041;&#35823;&#24046;&#20250;&#23548;&#33268;&#22810;&#26657;&#20934;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#24449;&#26041;&#38754;&#65292;&#32780;&#19981;&#26159;&#20851;&#20110;&#31639;&#27861;&#25110;&#26679;&#26412;&#22797;&#26434;&#24615;&#32771;&#34385;&#12290;&#20197;&#21069;&#30340;&#36825;&#26679;&#30340;&#32467;&#26524;&#20165;&#36866;&#29992;&#20110;&#20960;&#20046;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#39044;&#27979;&#22120;&#65292;&#22240;&#27492;&#26159;&#34920;&#24449;&#26080;&#20851;&#30340;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#36866;&#29992;&#20110;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#23450;&#31639;&#27861;&#65292;&#22914; SGD&#65292;&#24182;&#19988;&#19981;&#24212;&#35299;&#37322;&#20026;&#8220;&#20844;&#24179;&#24615;&#20174;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#33719;&#24471;&#20813;&#36153;&#30340;&#22909;&#22788;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multicalibration is a notion of fairness that aims to provide accurate predictions across a large set of groups. Multicalibration is known to be a different goal than loss minimization, even for simple predictors such as linear functions. In this note, we show that for (almost all) large neural network sizes, optimally minimizing squared error leads to multicalibration. Our results are about representational aspects of neural networks, and not about algorithmic or sample complexity considerations. Previous such results were known only for predictors that were nearly Bayes-optimal and were therefore representation independent. We emphasize that our results do not apply to specific algorithms for optimizing neural networks, such as SGD, and they should not be interpreted as "fairness comes for free from optimizing neural networks".
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;TieFake&#65292;&#36890;&#36807;&#26816;&#27979;&#26032;&#38395;&#26631;&#39064;&#21644;&#27491;&#25991;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#21644;&#24773;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#36890;&#36807;BERT&#21644;ResNeSt&#23398;&#20064;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#34920;&#31034;&#65292;&#23454;&#29616;&#23545;&#22810;&#27169;&#24577;&#25991;&#26412;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.09421</link><description>&lt;p&gt;
TieFake: &#26631;&#39064;-&#27491;&#25991;&#30456;&#20284;&#24230;&#21644;&#24773;&#24863;&#24863;&#30693;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
TieFake: Title-Text Similarity and Emotion-Aware Fake News Detection. (arXiv:2304.09421v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09421
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;TieFake&#65292;&#36890;&#36807;&#26816;&#27979;&#26032;&#38395;&#26631;&#39064;&#21644;&#27491;&#25991;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#21644;&#24773;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#36890;&#36807;BERT&#21644;ResNeSt&#23398;&#20064;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#34920;&#31034;&#65292;&#23454;&#29616;&#23545;&#22810;&#27169;&#24577;&#25991;&#26412;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#26816;&#27979;&#26088;&#22312;&#26816;&#27979;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#24191;&#27867;&#20256;&#25773;&#30340;&#34394;&#20551;&#26032;&#38395;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#20844;&#20247;&#21644;&#25919;&#24220;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#35768;&#22810;&#26041;&#27861;&#24050;&#32463;&#24320;&#21457;&#20986;&#26469;&#65292;&#20197;&#20174;&#26032;&#38395;&#22270;&#20687;&#12289;&#25991;&#26412;&#25110;&#35270;&#39057;&#20013;&#33719;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#38754;&#20020;&#20197;&#19979;&#38480;&#21046;&#65306;&#65288;1&#65289;&#24573;&#30053;&#26032;&#38395;&#20013;&#22266;&#26377;&#30340;&#24773;&#24863;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#26377;&#30410;&#65292;&#22240;&#20026;&#23427;&#21253;&#21547;&#20316;&#32773;&#30340;&#20027;&#35266;&#24847;&#22270;&#65307;&#65288;2&#65289;&#23545;&#20110;&#26032;&#38395;&#25991;&#31456;&#20013;&#26631;&#39064;&#19982;&#27491;&#25991;&#20043;&#38388;&#30340;&#20851;&#31995;&#65288;&#30456;&#20284;&#24230;&#65289;&#32473;&#20104;&#24456;&#23569;&#20851;&#27880;&#65292;&#36825;&#32463;&#24120;&#20351;&#29992;&#26080;&#20851;&#26631;&#39064;&#26469;&#21560;&#24341;&#35835;&#32773;&#30340;&#27880;&#24847;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#39064;-&#27491;&#25991;&#30456;&#20284;&#24230;&#21644;&#24773;&#24863;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#65288;TieFake&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32479;&#19968;&#26694;&#26550;&#20013;&#20849;&#21516;&#24314;&#27169;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#20316;&#32773;&#24773;&#24863;&#26469;&#23454;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#21035;&#20351;&#29992;BERT&#21644;ResNeSt&#26469;&#23398;&#20064;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#21457;&#34892;&#26041;&#24773;&#24863;&#25552;&#21462;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fake news detection aims to detect fake news widely spreading on social media platforms, which can negatively influence the public and the government. Many approaches have been developed to exploit relevant information from news images, text, or videos. However, these methods may suffer from the following limitations: (1) ignore the inherent emotional information of the news, which could be beneficial since it contains the subjective intentions of the authors; (2) pay little attention to the relation (similarity) between the title and textual information in news articles, which often use irrelevant title to attract reader' attention. To this end, we propose a novel Title-Text similarity and emotion-aware Fake news detection (TieFake) method by jointly modeling the multi-modal context information and the author sentiment in a unified framework. Specifically, we respectively employ BERT and ResNeSt to learn the representations for text and images, and utilize publisher emotion extractor 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#24320;&#21457;&#20986;&#33021;&#22815;&#20687;&#20154;&#31867;&#35270;&#35273;&#19968;&#26679;&#40065;&#26834;&#22320;&#27010;&#25324;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#39640;&#23545;&#25239;&#24615;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#28789;&#38271;&#31867;&#21160;&#29289;&#35270;&#35273;&#30382;&#23618;(V1)&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20854;&#22312;&#23567;&#25200;&#21160;&#19979;&#20855;&#26377;&#38750;&#24179;&#20961;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#23567;&#27874;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.09403</link><description>&lt;p&gt;
&#23567;&#27874;&#32988;&#36807;&#20102;&#29492;&#23376;&#8212;&#8212;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Wavelets Beat Monkeys at Adversarial Robustness. (arXiv:2304.09403v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09403
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#24320;&#21457;&#20986;&#33021;&#22815;&#20687;&#20154;&#31867;&#35270;&#35273;&#19968;&#26679;&#40065;&#26834;&#22320;&#27010;&#25324;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#39640;&#23545;&#25239;&#24615;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#28789;&#38271;&#31867;&#21160;&#29289;&#35270;&#35273;&#30382;&#23618;(V1)&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20854;&#22312;&#23567;&#25200;&#21160;&#19979;&#20855;&#26377;&#38750;&#24179;&#20961;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#23567;&#27874;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#33268;&#21147;&#20110;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23545;&#23545;&#25239;&#24615;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#8212;&#8212;&#21363;&#23545;&#25968;&#25454;&#20013;&#24494;&#19981;&#21487;&#35265;&#30340;&#24694;&#24847;&#25200;&#21160;&#30340;&#25269;&#24481;&#33021;&#21147;&#12290;&#30446;&#21069;&#65292;&#33719;&#21462;&#24378;&#22823;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#26080;&#21487;&#20105;&#35758;&#30340;&#26368;&#20808;&#36827;&#38450;&#24481;&#31574;&#30053;&#26159;&#23545;&#25239;&#24615;&#35757;&#32451;(AT)&#65292;&#20294;&#23427;&#27604;&#26631;&#20934;&#35757;&#32451;&#32791;&#36153;&#26356;&#22810;&#36164;&#28304;&#65292;&#21516;&#26102;&#22312;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#26377;&#25152;&#25240;&#34935;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#40723;&#33310;&#20154;&#24515;&#30340;&#24037;&#20316;[Dapello&#31561;&#20154;]&#26088;&#22312;&#23558;&#31070;&#32463;&#29983;&#29289;&#23398;&#24037;&#20855;&#24212;&#29992;&#20110;&#38382;&#39064;&#65306;&#25105;&#20204;&#22914;&#20309;&#24320;&#21457;&#33021;&#22815;&#20687;&#20154;&#31867;&#35270;&#35273;&#19968;&#26679;&#40065;&#26834;&#22320;&#27010;&#25324;&#30340;&#31070;&#32463;&#32593;&#32476;&#65311;[Dapello&#31561;&#20154;]&#35774;&#35745;&#20102;&#19968;&#20010;&#31070;&#32463;&#38544;&#34255;&#31532;&#19968;&#23618;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#35813;&#23618;&#27169;&#20223;&#20102;&#28789;&#38271;&#31867;&#21160;&#29289;&#30340;&#35270;&#35273;&#30382;&#23618;(V1)&#65292;&#20854;&#21518;&#26159;&#20174;&#24403;&#21069;CNN&#35270;&#35273;&#27169;&#22411;&#35843;&#25972;&#32780;&#26469;&#30340;&#21518;&#31471;&#32467;&#26500;&#12290;&#24403;&#22312;&#23567;&#25200;&#21160;&#26465;&#20214;&#19979;&#22312;&#26631;&#20934;&#35270;&#35273;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#26102;&#65292;&#20284;&#20046;&#23454;&#29616;&#20102;&#38750;&#24179;&#20961;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#36825;&#39033;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#30740;&#31350;&#65292;&#24182;&#38382;&#19968;&#20010;&#38382;&#39064;&#65306;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#22522;&#20110;&#21407;&#21017;&#30340;&#26080;&#21442;&#25968;&#34920;&#31034;&#26041;&#27861;?
&lt;/p&gt;
&lt;p&gt;
Research on improving the robustness of neural networks to adversarial noise - imperceptible malicious perturbations of the data - has received significant attention. The currently uncontested state-of-the-art defense to obtain robust deep neural networks is Adversarial Training (AT), but it consumes significantly more resources compared to standard training and trades off accuracy for robustness. An inspiring recent work [Dapello et al.] aims to bring neurobiological tools to the question: How can we develop Neural Nets that robustly generalize like human vision? [Dapello et al.] design a network structure with a neural hidden first layer that mimics the primate primary visual cortex (V1), followed by a back-end structure adapted from current CNN vision models. It seems to achieve non-trivial adversarial robustness on standard vision benchmarks when tested on small perturbations. Here we revisit this biologically inspired work, and ask whether a principled parameter-free representatio
&lt;/p&gt;</description></item><item><title>MixPro&#26159;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#21407;&#22987;&#36755;&#20837;&#21644;&#27169;&#26495;&#36827;&#34892;&#28151;&#21512;&#26469;&#25552;&#39640;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#24615;&#33021;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;5.08%&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.09402</link><description>&lt;p&gt;
MixPro&#65306;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;&#30340;&#31616;&#21333;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
MixPro: Simple yet Effective Data Augmentation for Prompt-based Learning. (arXiv:2304.09402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09402
&lt;/p&gt;
&lt;p&gt;
MixPro&#26159;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#21407;&#22987;&#36755;&#20837;&#21644;&#27169;&#26495;&#36827;&#34892;&#28151;&#21512;&#26469;&#25552;&#39640;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#24615;&#33021;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;5.08%&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#36890;&#36807;&#23558;&#36755;&#20837;&#19982;&#27169;&#26495;&#32452;&#21512;&#36215;&#26469;&#65292;&#23558;&#19979;&#28216;&#20219;&#21153;&#37325;&#26500;&#20026;&#22635;&#31354;&#38382;&#39064;&#12290;&#36825;&#31181;&#25216;&#26415;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#29305;&#21035;&#26377;&#29992;&#65292;&#28982;&#32780;&#65292;&#20351;&#29992;&#26377;&#38480;&#30340;&#27169;&#26495;&#21644;&#25991;&#26412;&#20173;&#28982;&#23384;&#22312;&#26174;&#30528;&#30340;&#24615;&#33021;&#25913;&#36827;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#20351;&#29992;&#27169;&#22411;&#38598;&#25104;&#30340;&#26041;&#27861;&#21487;&#20197;&#38480;&#21046;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MixPro&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26631;&#35760;&#32423;&#12289;&#21477;&#23376;&#32423;&#21644;&#26102;&#20195;&#32423;&#30340;&#28151;&#21512;&#31574;&#30053;&#26469;&#22686;&#24378;&#21407;&#22987;&#36755;&#20837;&#25991;&#26412;&#21644;&#27169;&#26495;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#23569;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;MixPro&#20248;&#20110;&#20854;&#20182;&#22686;&#24378;&#22522;&#32447;&#65292;&#30456;&#27604;&#22686;&#24378;&#21069;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;5.08%&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based learning reformulates downstream tasks as cloze problems by combining the original input with a template. This technique is particularly useful in few-shot learning, where a model is trained on a limited amount of data. However, the limited templates and text used in few-shot prompt-based learning still leave significant room for performance improvement. Additionally, existing methods using model ensembles can constrain the model efficiency. To address these issues, we propose an augmentation method called MixPro, which augments both the vanilla input text and the templates through token-level, sentence-level, and epoch-level Mixup strategies. We conduct experiments on five few-shot datasets, and the results show that MixPro outperforms other augmentation baselines, improving model performance by an average of 5.08% compared to before augmentation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22522;&#20110;&#20449;&#24687;&#20960;&#20309;&#26694;&#26550;&#32479;&#19968;&#20102;&#21327;&#21464;&#37327;&#28418;&#31227;&#36866;&#24212;&#23478;&#26063;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20960;&#20309;&#24191;&#20041;&#21327;&#21464;&#37327;&#28418;&#31227;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.09387</link><description>&lt;p&gt;
&#20449;&#24687;&#20960;&#20309;&#24191;&#20041;&#21327;&#21464;&#37327;&#28418;&#31227;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Information Geometrically Generalized Covariate Shift Adaptation. (arXiv:2304.09387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09387
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22522;&#20110;&#20449;&#24687;&#20960;&#20309;&#26694;&#26550;&#32479;&#19968;&#20102;&#21327;&#21464;&#37327;&#28418;&#31227;&#36866;&#24212;&#23478;&#26063;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20960;&#20309;&#24191;&#20041;&#21327;&#21464;&#37327;&#28418;&#31227;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#36981;&#24490;&#30456;&#21516;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#36825;&#20010;&#20551;&#35774;&#32463;&#24120;&#34987;&#36829;&#21453;&#12290;&#29305;&#21035;&#22320;&#65292;&#25968;&#25454;&#30340;&#36793;&#32536;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#30340;&#29616;&#35937;&#31216;&#20026;&#21327;&#21464;&#37327;&#28418;&#31227;&#65292;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#30740;&#31350;&#20027;&#39064;&#20043;&#19968;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20247;&#25152;&#21608;&#30693;&#30340;&#21327;&#21464;&#37327;&#28418;&#31227;&#36866;&#24212;&#26041;&#27861;&#23478;&#26063;&#22312;&#20449;&#24687;&#20960;&#20309;&#26694;&#26550;&#19979;&#26159;&#32479;&#19968;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23454;&#29616;&#20960;&#20309;&#24191;&#20041;&#21327;&#21464;&#37327;&#28418;&#31227;&#36866;&#24212;&#26041;&#27861;&#30340;&#21442;&#25968;&#25628;&#32034;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#24191;&#20041;&#26041;&#27861;&#21487;&#20197;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many machine learning methods assume that the training and test data follow the same distribution. However, in the real world, this assumption is very often violated. In particular, the phenomenon that the marginal distribution of the data changes is called covariate shift, one of the most important research topics in machine learning. We show that the well-known family of covariate shift adaptation methods is unified in the framework of information geometry. Furthermore, we show that parameter search for geometrically generalized covariate shift adaptation method can be achieved efficiently. Numerical experiments show that our generalization can achieve better performance than the existing methods it encompasses.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#21382;&#21490;&#35266;&#27979;&#25968;&#25454;&#20013;&#36716;&#31227;&#29289;&#29702;&#30693;&#35782;&#21040;&#25968;&#20540;&#27169;&#22411;&#30340;&#28023;&#34920;&#28201;&#24230;&#39044;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09376</link><description>&lt;p&gt;
&#22686;&#24378;&#29289;&#29702;&#30693;&#35782;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#28023;&#34920;&#28201;&#24230;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Physical Knowledge Enhanced Deep Neural Network for Sea Surface Temperature Prediction. (arXiv:2304.09376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09376
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#21382;&#21490;&#35266;&#27979;&#25968;&#25454;&#20013;&#36716;&#31227;&#29289;&#29702;&#30693;&#35782;&#21040;&#25968;&#20540;&#27169;&#22411;&#30340;&#28023;&#34920;&#28201;&#24230;&#39044;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#25968;&#20540;&#27169;&#22411;&#22312;&#28023;&#27915;&#23398;&#30740;&#31350;&#20013;&#34987;&#29992;&#26469;&#27169;&#25311;&#28023;&#27915;&#21160;&#21147;&#23398;&#65292;&#20195;&#34920;&#20102;&#29289;&#29702;&#26041;&#31243;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#19982;&#28023;&#27915;&#21160;&#21147;&#23398;&#26377;&#20851;&#30340;&#22240;&#32032;&#20284;&#20046;&#26159;&#19981;&#30830;&#23450;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23558;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#29289;&#29702;&#30693;&#35782;&#36716;&#31227;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25968;&#20540;&#27169;&#22411;&#22312;&#39044;&#27979;&#28023;&#34920;&#28201;&#24230;&#65288;SST&#65289;&#26102;&#30340;&#20934;&#30830;&#24615;&#12290;&#26368;&#36817;&#65292;&#22320;&#29699;&#35266;&#27979;&#25216;&#26415;&#30340;&#36827;&#27493;&#20135;&#29983;&#20102;&#24040;&#22823;&#30340;&#25968;&#25454;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#25506;&#32034;&#21033;&#29992;&#36234;&#26469;&#36234;&#22810;&#30340;&#21382;&#21490;&#35266;&#27979;&#25968;&#25454;&#26469;&#25913;&#36827;&#21644;&#34917;&#20805;&#25968;&#20540;&#27169;&#22411;&#30340;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#21382;&#21490;&#35266;&#27979;&#25968;&#25454;&#20013;&#36716;&#31227;&#29289;&#29702;&#30693;&#35782;&#21040;&#25968;&#20540;&#27169;&#22411;&#30340;SST&#39044;&#27979;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#32452;&#21512;&#26469;&#25429;&#33719;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#23558;&#25968;&#20540;&#27169;&#22411;&#25968;&#25454;&#39304;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, numerical models have been deployed in oceanography studies to simulate ocean dynamics by representing physical equations. However, many factors pertaining to ocean dynamics seem to be ill-defined. We argue that transferring physical knowledge from observed data could further improve the accuracy of numerical models when predicting Sea Surface Temperature (SST). Recently, the advances in earth observation technologies have yielded a monumental growth of data. Consequently, it is imperative to explore ways in which to improve and supplement numerical models utilizing the ever-increasing amounts of historical observational data. To this end, we introduce a method for SST prediction that transfers physical knowledge from historical observations to numerical models. Specifically, we use a combination of an encoder and a generative adversarial network (GAN) to capture physical knowledge from the observed data. The numerical model data is then fed into the pre-trained model to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#8220;&#27927;&#29260;&#21644;&#20999;&#21106;&#8221;&#31639;&#27861;&#23545;&#38271;&#25991;&#26412;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#25552;&#21462;BERT&#23884;&#20837;&#65292;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23545;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#25552;&#39640;20.94%&#12290;</title><link>http://arxiv.org/abs/2304.09374</link><description>&lt;p&gt;
&#27927;&#29260;&#21644;&#20999;&#21106;&#65306;&#38271;&#25991;&#26412;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Shuffle &amp; Divide: Contrastive Learning for Long Text. (arXiv:2304.09374v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#8220;&#27927;&#29260;&#21644;&#20999;&#21106;&#8221;&#31639;&#27861;&#23545;&#38271;&#25991;&#26412;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#25552;&#21462;BERT&#23884;&#20837;&#65292;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23545;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#25552;&#39640;20.94%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#38271;&#25991;&#26412;&#25991;&#26723;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#8220;&#27927;&#29260;&#21644;&#20999;&#21106;&#8221;&#65288;SaD&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#25991;&#26412;&#22686;&#24191;&#31639;&#27861;&#65292;&#21487;&#20026;&#22522;&#20110;BERT&#30340;&#25991;&#26723;&#23884;&#20837;&#25152;&#38656;&#30340;&#23545;&#27604;&#26356;&#26032;&#35774;&#32622;&#19968;&#20010;&#21069;&#32622;&#20219;&#21153;&#12290;SaD&#23558;&#25991;&#26723;&#25286;&#20998;&#20026;&#20004;&#20010;&#23376;&#25991;&#26723;&#65292;&#20854;&#20013;&#21253;&#21547;&#38543;&#26426;&#27927;&#29260;&#30340;&#21333;&#35789;&#12290;&#36825;&#20123;&#23376;&#25991;&#26723;&#34987;&#35270;&#20026;&#27491;&#26679;&#26412;&#65292;&#23558;&#25152;&#26377;&#20854;&#20182;&#25991;&#26723;&#35270;&#20026;&#36127;&#26679;&#26412;&#12290;&#22312;SaD&#20043;&#21518;&#65292;&#25105;&#20204;&#37325;&#22797;&#23545;&#27604;&#26356;&#26032;&#21644;&#32858;&#31867;&#38454;&#27573;&#65292;&#30452;&#33267;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20943;&#36731;&#20154;&#21147;&#36164;&#28304;&#30340;&#24037;&#20316;&#37327;&#65292;&#20174;&#32780;&#33410;&#30465;&#26114;&#36149;&#30340;AI&#36164;&#28304;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;20 Newsgroups&#12289;Reuters-21578&#12289;BBC&#21644;BBCSport&#25968;&#25454;&#38598;&#36827;&#34892;&#26080;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#32463;&#39564;&#35780;&#20272;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;20 Newsgroups&#19978;&#23558;&#24403;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;SS-SB-MT&#25552;&#39640;&#20102;20.94&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a self-supervised learning method for long text documents based on contrastive learning. A key to our method is Shuffle and Divide (SaD), a simple text augmentation algorithm that sets up a pretext task required for contrastive updates to BERT-based document embedding. SaD splits a document into two sub-documents containing randomly shuffled words in the entire documents. The sub-documents are considered positive examples, leaving all other documents in the corpus as negatives. After SaD, we repeat the contrastive update and clustering phases until convergence. It is naturally a time-consuming, cumbersome task to label text documents, and our method can help alleviate human efforts, which are most expensive resources in AI. We have empirically evaluated our method by performing unsupervised text classification on the 20 Newsgroups, Reuters-21578, BBC, and BBCSport datasets. In particular, our method pushes the current state-of-the-art, SS-SB-MT, on 20 Newsgroups by 20.94% in
&lt;/p&gt;</description></item><item><title>ContraCluster&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#32858;&#31867;&#21644;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;ContraCluster&#20351;&#29992;&#23545;&#27604;&#21407;&#22411;&#37319;&#26679;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#21322;&#30417;&#30563;&#24494;&#35843;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;CIFAR-10&#31561;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09369</link><description>&lt;p&gt;
ContraCluster&#65306;&#36890;&#36807;&#23545;&#27604;&#33258;&#30417;&#30563;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#23398;&#20064;&#26080;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
ContraCluster: Learning to Classify without Labels by Contrastive Self-Supervision and Prototype-Based Semi-Supervision. (arXiv:2304.09369v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09369
&lt;/p&gt;
&lt;p&gt;
ContraCluster&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#32858;&#31867;&#21644;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;ContraCluster&#20351;&#29992;&#23545;&#27604;&#21407;&#22411;&#37319;&#26679;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#21322;&#30417;&#30563;&#24494;&#35843;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;CIFAR-10&#31561;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;&#21551;&#21457;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#26041;&#24335;&#24212;&#23545;&#26080;&#30417;&#30563;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ContraCluster&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#32858;&#31867;&#19982;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;ContraCluster&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;(1) &#23545;&#27604;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;(CPT)&#65292;(2) &#23545;&#27604;&#21407;&#22411;&#37319;&#26679;(CPS)&#65292;(3) &#22522;&#20110;&#21407;&#22411;&#30340;&#21322;&#30417;&#30563;&#24494;&#35843;(PB-SFT)&#12290;CPS&#21487;&#20197;&#22312;&#30001;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#36873;&#25321;&#39640;&#20934;&#30830;&#24615;&#30340;&#31867;&#21035;&#21407;&#22411;&#22270;&#20687;&#12290;&#25105;&#20204;&#20351;&#29992;&#37319;&#26679;&#30340;&#21407;&#22411;&#20316;&#20026;&#24102;&#22122;&#22768;&#30340;&#26631;&#35760;&#25968;&#25454;&#26469;&#25191;&#34892;&#21322;&#30417;&#30563;&#24494;&#35843;(PB-SFT)&#65292;&#21033;&#29992;&#23567;&#21407;&#22411;&#21644;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#19968;&#27493;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;ContraCluster&#22312;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#21253;&#25324;CIFAR-10&#65292;STL-10&#21644;ImageNet-10&#20013;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#20363;&#22914;&#65292;ContraCluster&#22312;&#20165;&#26377;10%&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;87.9&#65285;&#30340;CIFAR-10&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#20102;&#20808;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#27700;&#24179;3.8&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in representation learning inspire us to take on the challenging problem of unsupervised image classification tasks in a principled way. We propose ContraCluster, an unsupervised image classification method that combines clustering with the power of contrastive self-supervised learning. ContraCluster consists of three stages: (1) contrastive self-supervised pre-training (CPT), (2) contrastive prototype sampling (CPS), and (3) prototype-based semi-supervised fine-tuning (PB-SFT). CPS can select highly accurate, categorically prototypical images in an embedding space learned by contrastive learning. We use sampled prototypes as noisy labeled data to perform semi-supervised fine-tuning (PB-SFT), leveraging small prototypes and large unlabeled data to further enhance the accuracy. We demonstrate empirically that ContraCluster achieves new state-of-the-art results for standard benchmark datasets including CIFAR-10, STL-10, and ImageNet-10. For example, ContraCluster achi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;Graph Deviation Network (GDN)&#26469;&#25429;&#25417;&#27827;&#27969;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#22797;&#26434;&#26102;&#31354;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#22791;&#29992;&#24322;&#24120;&#38408;&#20540;&#26631;&#20934;GDN+&#65292;&#20197;&#23454;&#29616;&#23545;&#27700;&#36136;&#30340;&#20934;&#30830;&#25345;&#32493;&#30417;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.09367</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27827;&#27969;&#31995;&#32479;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network-Based Anomaly Detection for River Network Systems. (arXiv:2304.09367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;Graph Deviation Network (GDN)&#26469;&#25429;&#25417;&#27827;&#27969;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#22797;&#26434;&#26102;&#31354;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#22791;&#29992;&#24322;&#24120;&#38408;&#20540;&#26631;&#20934;GDN+&#65292;&#20197;&#23454;&#29616;&#23545;&#27700;&#36136;&#30340;&#20934;&#30830;&#25345;&#32493;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#26159;&#27827;&#27969;&#32593;&#32476;&#30340;&#29983;&#21629;&#32447;&#65292;&#20854;&#36136;&#37327;&#23545;&#32500;&#25252;&#27700;&#29983;&#24577;&#31995;&#32479;&#21644;&#20154;&#31867;&#31038;&#20250;&#37117;&#26377;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#22330;&#20256;&#24863;&#22120;&#25216;&#26415;&#36234;&#26469;&#36234;&#20381;&#36182;&#23454;&#26102;&#30417;&#27979;&#27700;&#36136;&#12290;&#24322;&#24120;&#26816;&#27979;&#26159;&#35782;&#21035;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#38169;&#35823;&#27169;&#24335;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#22797;&#26434;&#24615;&#21644;&#21464;&#24322;&#24615;&#65292;&#21363;&#20351;&#22312;&#27491;&#24120;&#24773;&#20917;&#19979;&#20063;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27827;&#27969;&#32593;&#32476;&#20256;&#24863;&#22120;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#23545;&#20110;&#31934;&#30830;&#25345;&#32493;&#30417;&#27979;&#27700;&#36136;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#8212;&#8212;&#26368;&#36817;&#25552;&#20986;&#30340;Graph Deviation Network (GDN)&#65292;&#23427;&#21033;&#29992;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#39044;&#27979;&#26469;&#25429;&#25417;&#20256;&#24863;&#22120;&#20043;&#38388;&#22797;&#26434;&#30340;&#26102;&#31354;&#20851;&#31995;&#12290;&#25105;&#20204;&#26681;&#25454;&#25152;&#23398;&#22270;&#24418;&#25552;&#20986;&#20102;&#27169;&#22411;GDN+&#30340;&#22791;&#29992;&#24322;&#24120;&#38408;&#20540;&#26631;&#20934;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#20934;&#20223;&#30495;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Water is the lifeblood of river networks, and its quality plays a crucial role in sustaining both aquatic ecosystems and human societies. Real-time monitoring of water quality is increasingly reliant on in-situ sensor technology. Anomaly detection is crucial for identifying erroneous patterns in sensor data, but can be a challenging task due to the complexity and variability of the data, even under normal conditions. This paper presents a solution to the challenging task of anomaly detection for river network sensor data, which is essential for the accurate and continuous monitoring of water quality. We use a graph neural network model, the recently proposed Graph Deviation Network (GDN), which employs graph attention-based forecasting to capture the complex spatio-temporal relationships between sensors. We propose an alternate anomaly threshold criteria for the model, GDN+, based on the learned graph. To evaluate the model's efficacy, we introduce new benchmarking simulation experimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26410;&#30693;&#21160;&#24577;&#19979;&#36861;&#27714;&#38271;&#26399;&#20844;&#24179;&#24615;&#65292;&#23454;&#29616;&#31639;&#27861;&#30340;&#21160;&#24577;&#36866;&#24212;&#21644;&#26435;&#34913;&#65292;&#21487;&#20026;&#20998;&#31867;&#22120;-&#20154;&#32676;&#31995;&#32479;&#25512;&#21521;&#26356;&#29702;&#24819;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.09362</link><description>&lt;p&gt;
&#26410;&#30693;&#21160;&#24577;&#19979;&#30340;&#38271;&#26399;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Long-Term Fairness with Unknown Dynamics. (arXiv:2304.09362v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26410;&#30693;&#21160;&#24577;&#19979;&#36861;&#27714;&#38271;&#26399;&#20844;&#24179;&#24615;&#65292;&#23454;&#29616;&#31639;&#27861;&#30340;&#21160;&#24577;&#36866;&#24212;&#21644;&#26435;&#34913;&#65292;&#21487;&#20026;&#20998;&#31867;&#22120;-&#20154;&#32676;&#31995;&#32479;&#25512;&#21521;&#26356;&#29702;&#24819;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#21487;&#33021;&#20250;&#24378;&#21270;&#31038;&#20250;&#19981;&#24179;&#31561;&#65292;&#20294;&#23427;&#20063;&#21487;&#20197;&#29992;&#20110;&#21160;&#24577;&#22320;&#23547;&#27714;&#20844;&#24179;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#20013;&#35268;&#33539;&#20102;&#38271;&#26399;&#20844;&#27491;&#24615;&#12290;&#35813;&#20844;&#24335;&#21487;&#20197;&#23481;&#32435;&#21160;&#24577;&#25511;&#21046;&#30446;&#26631;&#65292;&#20363;&#22914;&#25512;&#21160;&#20154;&#32676;&#29366;&#24577;&#20013;&#22266;&#26377;&#30340;&#24179;&#31561;&#65292;&#36825;&#20123;&#30446;&#26631;&#19981;&#33021;&#34987;&#32435;&#20837;&#21040;&#20844;&#24179;&#24615;&#30340;&#38745;&#24577;&#20844;&#24335;&#20013;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#31639;&#27861;&#36890;&#36807;&#29306;&#29298;&#30701;&#26399;&#28608;&#21169;&#65292;&#23558;&#20998;&#31867;&#22120;-&#20154;&#32676;&#31995;&#32479;&#25512;&#21521;&#26356;&#29702;&#24819;&#30340;&#24179;&#34913;&#65292;&#20197;&#36866;&#24212;&#26410;&#30693;&#30340;&#21160;&#24577;&#31995;&#32479;&#12290;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36866;&#24212;&#20102;&#26368;&#36817;&#30340;&#22312;&#32447;&#23398;&#20064;&#24037;&#20316;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#31181;&#31639;&#27861;&#22312;&#32047;&#31215;&#25439;&#22833;&#21644;&#20844;&#24179;&#24615;&#36829;&#35268;&#30340;&#32047;&#31215;&#24615;&#65288;&#20316;&#20026;&#20154;&#32676;&#20043;&#38388;&#30340;&#32479;&#35745;&#35268;&#24459;&#65289;&#19978;&#23454;&#29616;&#20102;&#21516;&#26102;&#30340;&#27010;&#29575;&#30028;&#38480;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#22522;&#20934;&#30340;&#32454;&#24494;&#20998;&#31867;&#22120;&#30340;&#37325;&#22797;&#35757;&#32451;&#20197;&#21450;&#19968;&#20010;&#35299;&#39064;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#22312;&#20004;&#20010;&#21327;&#35843;&#30446;&#26631;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
While machine learning can myopically reinforce social inequalities, it may also be used to dynamically seek equitable outcomes. In this paper, we formalize long-term fairness in the context of online reinforcement learning. This formulation can accommodate dynamical control objectives, such as driving equity inherent in the state of a population, that cannot be incorporated into static formulations of fairness. We demonstrate that this framing allows an algorithm to adapt to unknown dynamics by sacrificing short-term incentives to drive a classifier-population system towards more desirable equilibria. For the proposed setting, we develop an algorithm that adapts recent work in online learning. We prove that this algorithm achieves simultaneous probabilistic bounds on cumulative loss and cumulative violations of fairness (as statistical regularities between demographic groups). We compare our proposed algorithm to the repeated retraining of myopic classifiers, as a baseline, and to a d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#23545;&#26032;&#35270;&#22270;&#25512;&#24191;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#28145;&#24230;&#27169;&#22411;&#20855;&#26377;&#24456;&#22909;&#30340;&#25512;&#24191;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#26041;&#24335;&#19982;&#25152;&#26377;&#29616;&#26377;&#27169;&#22411;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2304.09358</link><description>&lt;p&gt;
&#25506;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#19977;&#32500;&#27867;&#21270;&#30340;&#26412;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Investigating the Nature of 3D Generalization in Deep Neural Networks. (arXiv:2304.09358v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#23545;&#26032;&#35270;&#22270;&#25512;&#24191;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#28145;&#24230;&#27169;&#22411;&#20855;&#26377;&#24456;&#22909;&#30340;&#25512;&#24191;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#26041;&#24335;&#19982;&#25152;&#26377;&#29616;&#26377;&#27169;&#22411;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#23545;&#35937;&#35782;&#21035;&#31995;&#32479;&#38656;&#35201;&#20174;&#19968;&#32452;&#20108;&#32500;&#35757;&#32451;&#35270;&#22270;&#25512;&#24191;&#21040;&#26032;&#35270;&#22270;&#12290;&#22914;&#20309;&#20351;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#21487;&#20197;&#25512;&#24191;&#21040;&#26032;&#35270;&#22270;&#30340;&#38382;&#39064;&#24050;&#32463;&#22312;&#24515;&#29702;&#23398;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#31070;&#32463;&#31185;&#23398;&#20013;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#24314;&#27169;&#12290;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#29992;&#20110;&#23545;&#35937;&#35782;&#21035;&#23545;&#26032;&#35270;&#22270;&#20855;&#26377;&#24456;&#22909;&#30340;&#25512;&#24191;&#33021;&#21147;&#65292;&#20294;&#26426;&#21046;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#24120;&#35265;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#23545;&#26032;&#35270;&#22270;&#25512;&#24191;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#20854;&#21046;&#23450;&#20026;&#19968;&#20010;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#65292;&#20854;&#20013;&#26631;&#31614;&#23545;&#24212;&#20110;&#21807;&#19968;&#30340;&#19977;&#32500;&#29289;&#20307;&#65292;&#31034;&#20363;&#23545;&#24212;&#20110;&#29289;&#20307;&#22312;&#19981;&#21516;&#19977;&#32500;&#26041;&#21521;&#19978;&#30340;&#20108;&#32500;&#35270;&#22270;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#24120;&#35265;&#30340;&#25512;&#24191;&#21040;&#26032;&#35270;&#22270;&#30340;&#27169;&#22411;&#65306;(i)&#23436;&#20840;&#30340;&#19977;&#32500;&#27867;&#21270;&#65292;(ii)&#32431;&#20108;&#32500;&#21305;&#37197;&#65292;(iii)&#22522;&#20110;&#35270;&#22270;&#30340;&#32447;&#24615;&#32452;&#21512;&#21305;&#37197;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#28145;&#24230;&#27169;&#22411;&#20855;&#26377;&#24456;&#22909;&#30340;&#25512;&#24191;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#26041;&#24335;&#19982;&#25152;&#26377;&#36825;&#20123;&#29616;&#26377;&#27169;&#22411;&#19981;&#21516;&#12290;&#22806;&#25512;&#21040;vi
&lt;/p&gt;
&lt;p&gt;
Visual object recognition systems need to generalize from a set of 2D training views to novel views. The question of how the human visual system can generalize to novel views has been studied and modeled in psychology, computer vision, and neuroscience. Modern deep learning architectures for object recognition generalize well to novel views, but the mechanisms are not well understood. In this paper, we characterize the ability of common deep learning architectures to generalize to novel views. We formulate this as a supervised classification task where labels correspond to unique 3D objects and examples correspond to 2D views of the objects at different 3D orientations. We consider three common models of generalization to novel views: (i) full 3D generalization, (ii) pure 2D matching, and (iii) matching based on a linear combination of views. We find that deep models generalize well to novel views, but they do so in a way that differs from all these existing models. Extrapolation to vi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#22238;&#39038;&#20102;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#8220;&#33258;&#30417;&#30563;&#20449;&#24687;&#29702;&#35770;&#23398;&#20064;&#38382;&#39064;&#8221;&#32479;&#19968;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#35752;&#35770;&#20102;&#21387;&#32553;&#24615;&#21644;&#21387;&#32553;&#31639;&#27861;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#20984;&#26174;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.09355</link><description>&lt;p&gt;
&#21387;&#32553;&#19982;&#21542;&#8212;&#8212;&#33258;&#30417;&#30563;&#23398;&#20064;&#19982;&#20449;&#24687;&#35770;:&#19968;&#31687;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
To Compress or Not to Compress -- Self-Supervised Learning and Information Theory: A Review. (arXiv:2304.09355v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#22238;&#39038;&#20102;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#8220;&#33258;&#30417;&#30563;&#20449;&#24687;&#29702;&#35770;&#23398;&#20064;&#38382;&#39064;&#8221;&#32479;&#19968;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#35752;&#35770;&#20102;&#21387;&#32553;&#24615;&#21644;&#21387;&#32553;&#31639;&#27861;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#20984;&#26174;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#33539;&#20363;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#26126;&#30830;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#12290;&#20449;&#24687;&#35770;&#22312;&#29702;&#35299;&#21644;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#34987;&#24212;&#29992;&#20110;&#22312;&#30417;&#30563;&#35774;&#32622;&#20013;&#20248;&#21270;&#21387;&#32553;&#21644;&#30456;&#20851;&#20449;&#24687;&#20445;&#23384;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#28982;&#32780;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26368;&#20339;&#20449;&#24687;&#30446;&#26631;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#22238;&#39038;&#20102;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#8220;&#33258;&#30417;&#30563;&#20449;&#24687;&#29702;&#35770;&#23398;&#20064;&#38382;&#39064;&#8221;&#32479;&#19968;&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30740;&#31350;&#34701;&#21512;&#25104;&#19968;&#20010;&#19968;&#33268;&#30340;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#26368;&#36817;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#30740;&#31350;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#21387;&#32553;&#24615;&#21644;&#21387;&#32553;&#31639;&#27861;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#20984;&#26174;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have demonstrated remarkable performance in supervised learning tasks but require large amounts of labeled data. Self-supervised learning offers an alternative paradigm, enabling the model to learn from data without explicit labels. Information theory has been instrumental in understanding and optimizing deep neural networks. Specifically, the information bottleneck principle has been applied to optimize the trade-off between compression and relevant information preservation in supervised settings. However, the optimal information objective in self-supervised learning remains unclear. In this paper, we review various approaches to self-supervised learning from an information-theoretic standpoint and present a unified framework that formalizes the \textit{self-supervised information-theoretic learning problem}. We integrate existing research into a coherent framework, examine recent self-supervised methods, and identify research opportunities and challenges. Moreove
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#37197;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21457;&#23556;&#21151;&#29575;&#65292;&#20197;&#20248;&#21270;&#36890;&#20449;&#32422;&#26463;&#19979;FL&#36807;&#31243;&#20013;&#26381;&#21153;&#22120;&#31471;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#65292;&#24182;&#25552;&#39640;&#20840;&#23616;FL&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.09329</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#20013;&#20855;&#26377;&#21487;&#35777;&#26126;&#20445;&#38556;&#30340;&#21457;&#23556;
&lt;/p&gt;
&lt;p&gt;
Learning to Transmit with Provable Guarantees in Wireless Federated Learning. (arXiv:2304.09329v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09329
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#37197;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21457;&#23556;&#21151;&#29575;&#65292;&#20197;&#20248;&#21270;&#36890;&#20449;&#32422;&#26463;&#19979;FL&#36807;&#31243;&#20013;&#26381;&#21153;&#22120;&#31471;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#65292;&#24182;&#25552;&#39640;&#20840;&#23616;FL&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#37197;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21457;&#23556;&#21151;&#29575;&#65292;&#36866;&#29992;&#20110;&#24178;&#25200;&#21463;&#38480;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#25361;&#25112;&#24615;&#24773;&#26223;&#65292;&#22914;FL&#35757;&#32451;&#36807;&#31243;&#20013;&#26080;&#32447;&#20449;&#36947;&#27491;&#22312;&#21464;&#21270;&#20197;&#21450;&#26412;&#22320;&#35774;&#22791;&#19978;&#30340;&#35757;&#32451;&#25968;&#25454;&#19981;&#26159;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#12290;&#30452;&#35266;&#26469;&#35828;&#65292;&#21151;&#29575;&#31574;&#30053;&#26088;&#22312;&#20248;&#21270;&#36890;&#20449;&#32422;&#26463;&#19979;FL&#36807;&#31243;&#20013;&#26381;&#21153;&#22120;&#31471;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#39640;&#20840;&#23616;FL&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#35813;&#31574;&#30053;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24182;&#36890;&#36807;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#27714;&#35299;&#30456;&#20851;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#34920;&#26126;&#20102;&#21046;&#23450;&#38382;&#39064;&#20855;&#26377;&#38646;&#23545;&#20598;&#38388;&#38553;&#65292;&#24182;&#19988;&#19968;&#26086;&#21442;&#25968;&#21270;&#21151;&#29575;&#31574;&#30053;&#65292;&#21017;&#26368;&#20248;&#24615;&#21462;&#20915;&#20110;&#27492;&#21442;&#25968;&#21270;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#25968;&#20540;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#23454;&#38469;&#30340;&#26080;&#32447;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20256;&#32479;&#30340;&#21151;&#29575;&#20998;&#37197;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel data-driven approach to allocate transmit power for federated learning (FL) over interference-limited wireless networks. The proposed method is useful in challenging scenarios where the wireless channel is changing during the FL training process and when the training data are not independent and identically distributed (non-i.i.d.) on the local devices. Intuitively, the power policy is designed to optimize the information received at the server end during the FL process under communication constraints. Ultimately, our goal is to improve the accuracy and efficiency of the global FL model being trained. The proposed power allocation policy is parameterized using a graph convolutional network and the associated constrained optimization problem is solved through a primal-dual (PD) algorithm. Theoretically, we show that the formulated problem has zero duality gap and, once the power policy is parameterized, optimality depends on how expressive this parameterization is. Nu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#20132;&#26367;&#35757;&#32451;(FAT)&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#22312;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#22320;&#38754;&#30495;&#20540;&#26631;&#31614;&#30340;&#25968;&#25454;&#24211;&#20043;&#38388;&#20132;&#26367;&#35757;&#32451;&#65292;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#25968;&#25454;&#26469;&#36741;&#21161;&#27169;&#22411;&#23398;&#20064;&#65292;&#36866;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2304.09327</link><description>&lt;p&gt;
&#32852;&#37030;&#20132;&#26367;&#35757;&#32451;&#65306;&#22312;&#32852;&#37030;&#20998;&#21106;&#20013;&#21033;&#29992;&#26410;&#27880;&#37322;&#25968;&#25454;&#24211;(arXiv:2304.09327v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
Federated Alternate Training (FAT): Leveraging Unannotated Data Silos in Federated Segmentation for Medical Imaging. (arXiv:2304.09327v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#20132;&#26367;&#35757;&#32451;(FAT)&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#22312;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#22320;&#38754;&#30495;&#20540;&#26631;&#31614;&#30340;&#25968;&#25454;&#24211;&#20043;&#38388;&#20132;&#26367;&#35757;&#32451;&#65292;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#25968;&#25454;&#26469;&#36741;&#21161;&#27169;&#22411;&#23398;&#20064;&#65292;&#36866;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Fl)&#26088;&#22312;&#20998;&#24067;&#24335;&#22320;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;(ML)&#27169;&#22411;&#65292;&#20197;&#21152;&#24378;&#25968;&#25454;&#38544;&#31169;&#65292;&#21516;&#26102;&#20943;&#23569;&#25968;&#25454;&#36801;&#31227;&#25104;&#26412;&#12290;&#23427;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#38544;&#31169;&#25935;&#24863;&#30340;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#22522;&#20110;FL&#30340;&#21307;&#23398;&#22270;&#20687;&#30740;&#31350;&#37117;&#20551;&#23450;&#25968;&#25454;&#24211;&#20855;&#26377;&#22320;&#38754;&#30495;&#20540;&#26631;&#31614;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#21307;&#23398;&#39046;&#22495;&#30340;&#26631;&#31614;&#33719;&#21462;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#32463;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#31934;&#21147;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#24182;&#21033;&#29992;&#26410;&#27880;&#37322;&#25968;&#25454;&#24211;&#26469;&#25913;&#36827;&#24314;&#27169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26367;&#20195;&#35757;&#32451;&#30340;&#26694;&#26550;&#8212;&#8212;&#32852;&#37030;&#20132;&#26367;&#35757;&#32451;(FAT)&#65292;&#23427;&#22312;&#24102;&#27880;&#37322;&#25968;&#25454;&#24211;&#21644;&#26410;&#24102;&#27880;&#37322;&#25968;&#25454;&#24211;&#20043;&#38388;&#20132;&#26367;&#35757;&#32451;&#12290;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#24211;&#21033;&#29992;&#27880;&#37322;&#26469;&#23398;&#20064;&#19968;&#20010;&#21512;&#29702;&#30340;&#20840;&#23616;&#20998;&#21106;&#27169;&#22411;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26410;&#27880;&#37322;&#30340;&#25968;&#25454;&#24211;&#20351;&#29992;&#20840;&#23616;&#20998;&#21106;&#27169;&#22411;&#20316;&#20026;&#30446;&#26631;&#27169;&#22411;&#65292;&#20026;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#29983;&#25104;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;FAT&#22312;&#25968;&#25454;&#20445;&#25252;&#21644;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) aims to train a machine learning (ML) model in a distributed fashion to strengthen data privacy with limited data migration costs. It is a distributed learning framework naturally suitable for privacy-sensitive medical imaging datasets. However, most current FL-based medical imaging works assume silos have ground truth labels for training. In practice, label acquisition in the medical field is challenging as it often requires extensive labor and time costs. To address this challenge and leverage the unannotated data silos to improve modeling, we propose an alternate training-based framework, Federated Alternate Training (FAT), that alters training between annotated data silos and unannotated data silos. Annotated data silos exploit annotations to learn a reasonable global segmentation model. Meanwhile, unannotated data silos use the global segmentation model as a target model to generate pseudo labels for self-supervised learning. We evaluate the performance of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; M3S &#30340;&#22810;&#27169;&#24577;&#22810;&#23610;&#24230;&#27169;&#22411;&#65292;&#36890;&#36807; Gramian Angular Summation Field &#26041;&#27861;&#23558; Raman &#20809;&#35889;&#25968;&#25454;&#36716;&#25442;&#20026;&#21508;&#31181;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#20016;&#23500;&#20102;&#20854;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#30149;&#21490;&#20449;&#24687;&#20316;&#20026;&#36741;&#21161;&#36755;&#20837;&#65292;&#26368;&#32456;&#22312;&#22235;&#31181; CVD &#20122;&#22411;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;95.71%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.09322</link><description>&lt;p&gt;
&#22522;&#20110; Raman &#25104;&#20687;&#21644;&#30149;&#21490;&#30340;&#22810;&#27169;&#24577;&#22810;&#23610;&#24230;&#24515;&#34880;&#31649;&#30142;&#30149;&#20122;&#22411;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-Modality Multi-Scale Cardiovascular Disease Subtypes Classification Using Raman Image and Medical History. (arXiv:2304.09322v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; M3S &#30340;&#22810;&#27169;&#24577;&#22810;&#23610;&#24230;&#27169;&#22411;&#65292;&#36890;&#36807; Gramian Angular Summation Field &#26041;&#27861;&#23558; Raman &#20809;&#35889;&#25968;&#25454;&#36716;&#25442;&#20026;&#21508;&#31181;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#20016;&#23500;&#20102;&#20854;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#30149;&#21490;&#20449;&#24687;&#20316;&#20026;&#36741;&#21161;&#36755;&#20837;&#65292;&#26368;&#32456;&#22312;&#22235;&#31181; CVD &#20122;&#22411;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;95.71%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110; Raman &#20809;&#35889;&#23398;&#30340;&#35786;&#26029;&#21487;&#38752;&#24615;&#21644;&#25104;&#20998;&#29305;&#24322;&#24615;&#27979;&#35797;&#33021;&#21147;&#65292;&#20854;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#30142;&#30149;&#35786;&#26029;&#65292;&#20363;&#22914;&#24515;&#34880;&#31649;&#30142;&#30149;(CVD)&#12290;&#26368;&#36817;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064; RS &#30340;&#32454;&#24494;&#29305;&#24449;&#36827;&#34892;&#20108;&#20803;&#20998;&#31867;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#31867; CVD &#23376;&#31867;&#22411;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; M3S &#30340;&#22810;&#27169;&#24577;&#22810;&#23610;&#24230;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#20004;&#20010;&#26680;&#24515;&#27169;&#22359;&#65292;&#36890;&#36807; Gramian Angular Summation Field &#26041;&#27861;&#23558; RS &#25968;&#25454;&#36716;&#25442;&#20026;&#21508;&#31181;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#26469;&#20016;&#23500; RS &#24207;&#21015;&#30340;&#34920;&#31034;&#65292;&#24182;&#23558;&#30149;&#21490;&#20449;&#24687;&#24341;&#20837;&#25105;&#20204;&#30340;&#27169;&#22411;&#20316;&#20026;&#36741;&#21161;&#36755;&#20837;&#65292;&#24182;&#22686;&#24378;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#19968;&#20010;&#25317;&#26377;&#22235;&#20010; CVD &#20122;&#22411;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#21462;&#24471;&#20102;95.71% &#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Raman spectroscopy (RS) has been widely used for disease diagnosis, e.g., cardiovascular disease (CVD), owing to its efficiency and component-specific testing capabilities. A series of popular deep learning methods have recently been introduced to learn nuance features from RS for binary classifications and achieved outstanding performance than conventional machine learning methods. However, these existing deep learning methods still confront some challenges in classifying subtypes of CVD. For example, the nuance between subtypes is quite hard to capture and represent by intelligent models due to the chillingly similar shape of RS sequences. Moreover, medical history information is an essential resource for distinguishing subtypes, but they are underutilized. In light of this, we propose a multi-modality multi-scale model called M3S, which is a novel deep learning method with two core modules to address these issues. First, we convert RS data to various resolution images by the Gramian
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#28145;&#24230;&#20113;&#20809;&#29031;&#27169;&#22411;&#65292;&#33021;&#22815;&#27169;&#25311;&#25972;&#20010;&#22825;&#31354;&#30340;&#20113;&#36816;&#21160;&#65292;&#25552;&#39640;&#20113;&#22825;&#31354;&#29031;&#26126;&#26041;&#27861;&#30340;&#21160;&#24577;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09317</link><description>&lt;p&gt;
&#28145;&#20837;&#21160;&#24577;&#20113;&#20809;&#29031;
&lt;/p&gt;
&lt;p&gt;
Deep Dynamic Cloud Lighting. (arXiv:2304.09317v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#28145;&#24230;&#20113;&#20809;&#29031;&#27169;&#22411;&#65292;&#33021;&#22815;&#27169;&#25311;&#25972;&#20010;&#22825;&#31354;&#30340;&#20113;&#36816;&#21160;&#65292;&#25552;&#39640;&#20113;&#22825;&#31354;&#29031;&#26126;&#26041;&#27861;&#30340;&#21160;&#24577;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#31354;&#29031;&#26126;&#26159;&#28210;&#26579;&#20013;&#30340;&#26680;&#24515;&#20809;&#28304;&#65292;&#24050;&#32463;&#26377;&#24456;&#22810;&#24037;&#20316;&#24320;&#23637;&#20102;&#27169;&#25311;&#26228;&#31354;&#29031;&#26126;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#20113;&#26421;&#20250;&#26174;&#33879;&#25913;&#21464;&#22825;&#31354;&#30340;&#22806;&#35266;&#65292;&#36827;&#32780;&#25913;&#21464;&#22330;&#26223;&#30340;&#29031;&#26126;&#12290;&#23613;&#31649;&#26368;&#36817;&#22312;&#24320;&#21457;&#21253;&#25324;&#20113;&#30340;&#22825;&#31354;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#37117;&#24573;&#30053;&#20102;&#20113;&#36816;&#21160;&#65292;&#32780;&#20113;&#36816;&#21160;&#26159;&#20113;&#22825;&#31354;&#22806;&#35266;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#20219;&#20309;&#35270;&#39057;&#25110;&#20132;&#20114;&#29615;&#22659;&#20013;&#65292;&#21487;&#20197;&#39044;&#26399;&#20113;&#20250;&#31227;&#21160;&#65292;&#22312;&#30701;&#26102;&#38388;&#20869;&#26377;&#26102;&#20250;&#26377;&#30456;&#24403;&#22823;&#30340;&#31227;&#21160;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#25972;&#20010;&#22825;&#31354;&#30340;&#21160;&#24577;&#20113;&#21512;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#22810;&#26102;&#38388;&#23610;&#24230;&#22825;&#31354;&#22806;&#35266;&#27169;&#22411;&#65292;&#23398;&#20064;&#39044;&#27979;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#22825;&#31354;&#29031;&#26126;&#65292;&#20174;&#32780;&#33021;&#22815;&#22686;&#21152;&#20808;&#21069;&#38745;&#24577;&#12289;&#20113;&#22825;&#31354;&#29031;&#26126;&#26041;&#27861;&#30340;&#21160;&#24577;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sky illumination is a core source of lighting in rendering, and a substantial amount of work has been developed to simulate lighting from clear skies. However, in reality, clouds substantially alter the appearance of the sky and subsequently change the scene's illumination. While there have been recent advances in developing sky models which include clouds, these all neglect cloud movement which is a crucial component of cloudy sky appearance. In any sort of video or interactive environment, it can be expected that clouds will move, sometimes quite substantially in a short period of time. Our work proposes a solution to this which enables whole-sky dynamic cloud synthesis for the first time. We achieve this by proposing a multi-timescale sky appearance model which learns to predict the sky illumination over various timescales, and can be used to add dynamism to previous static, cloudy sky lighting approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#40065;&#26834;&#30340;&#33258;&#36866;&#24212; $\tau$-Lasso &#20272;&#35745;&#22120;&#65292;&#21516;&#26102;&#37319;&#29992;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#20197;&#38477;&#20302;&#30495;&#23454;&#22238;&#24402;&#31995;&#25968;&#30340;&#20559;&#24046;&#12290;&#23427;&#20855;&#26377;&#21464;&#37327;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#30495;&#23454;&#25903;&#25345;&#19979;&#22238;&#24402;&#21521;&#37327;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26368;&#20248;&#24615;&#36136;&#65292;&#20551;&#23450;&#24050;&#30693;&#30495;&#23454;&#22238;&#24402;&#21521;&#37327;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2304.09310</link><description>&lt;p&gt;
&#33258;&#36866;&#24212; $\tau$-Lasso&#65306;&#20854;&#20581;&#22766;&#24615;&#21644;&#26368;&#20248;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Adaptive $\tau$-Lasso: Its Robustness and Oracle Properties. (arXiv:2304.09310v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#40065;&#26834;&#30340;&#33258;&#36866;&#24212; $\tau$-Lasso &#20272;&#35745;&#22120;&#65292;&#21516;&#26102;&#37319;&#29992;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#20197;&#38477;&#20302;&#30495;&#23454;&#22238;&#24402;&#31995;&#25968;&#30340;&#20559;&#24046;&#12290;&#23427;&#20855;&#26377;&#21464;&#37327;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#30495;&#23454;&#25903;&#25345;&#19979;&#22238;&#24402;&#21521;&#37327;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26368;&#20248;&#24615;&#36136;&#65292;&#20551;&#23450;&#24050;&#30693;&#30495;&#23454;&#22238;&#24402;&#21521;&#37327;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#40065;&#26834; $\tau$-&#22238;&#24402;&#20272;&#35745;&#22120;&#65292;&#20197;&#24212;&#23545;&#21709;&#24212;&#21464;&#37327;&#21644;&#21327;&#21464;&#37327;&#30340;&#20005;&#37325;&#27745;&#26579;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#20272;&#35745;&#22120;&#20026;&#33258;&#36866;&#24212; $\tau$-Lasso&#65292;&#23427;&#23545;&#24322;&#24120;&#20540;&#21644;&#39640;&#26464;&#26438;&#28857;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#37319;&#29992;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#26469;&#20943;&#23569;&#30495;&#23454;&#22238;&#24402;&#31995;&#25968;&#30340;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#20026;&#27599;&#20010;&#22238;&#24402;&#31995;&#25968;&#20998;&#37197;&#19968;&#20010;&#26435;&#37325;&#12290;&#23545;&#20110;&#22266;&#23450;&#25968;&#37327;&#30340;&#39044;&#27979;&#21464;&#37327; $p$&#65292;&#25105;&#20204;&#26174;&#31034;&#20986;&#33258;&#36866;&#24212; $\tau$-Lasso &#20855;&#26377;&#21464;&#37327;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#30495;&#23454;&#25903;&#25345;&#19979;&#22238;&#24402;&#21521;&#37327;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26368;&#20248;&#24615;&#36136;&#65292;&#20551;&#23450;&#24050;&#30693;&#30495;&#23454;&#22238;&#24402;&#21521;&#37327;&#30340;&#25903;&#25345;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#26029;&#28857;&#21644;&#24433;&#21709;&#20989;&#25968;&#26469;&#34920;&#24449;&#20854;&#20581;&#22766;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27169;&#25311;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#20272;&#35745;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new regularized version of the robust $\tau$-regression estimator for analyzing high-dimensional data sets subject to gross contamination in the response variables and covariates. We call the resulting estimator adaptive $\tau$-Lasso that is robust to outliers and high-leverage points and simultaneously employs adaptive $\ell_1$-norm penalty term to reduce the bias associated with large true regression coefficients. More specifically, this adaptive $\ell_1$-norm penalty term assigns a weight to each regression coefficient. For a fixed number of predictors $p$, we show that the adaptive $\tau$-Lasso has the oracle property with respect to variable-selection consistency and asymptotic normality for the regression vector corresponding to the true support, assuming knowledge of the true regression vector support. We then characterize its robustness via the finite-sample breakdown point and the influence function. We carry-out extensive simulations to compare the per
&lt;/p&gt;</description></item><item><title>&#29992;&#26426;&#22120;&#23398;&#20064;&#21457;&#29616;&#24102;&#29366;&#29289;&#65292;&#21453;&#39539;&#22235;&#32500;&#24179;&#20961; Poincar&#233; &#29468;&#24819;&#12290;</title><link>http://arxiv.org/abs/2304.09304</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#25628;&#32034;&#24102;&#29366;&#29289;
&lt;/p&gt;
&lt;p&gt;
Searching for ribbons with machine learning. (arXiv:2304.09304v1 [math.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09304
&lt;/p&gt;
&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#21457;&#29616;&#24102;&#29366;&#29289;&#65292;&#21453;&#39539;&#22235;&#32500;&#24179;&#20961; Poincar&#233; &#29468;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#25299;&#25169;&#23398;&#20013;&#30340;&#19968;&#20010;&#38382;&#39064;&#65306;&#22914;&#20309;&#30830;&#23450;&#19968;&#20010;&#32467;&#19981;&#33021;&#38480;&#23450;&#19968;&#20010;&#24102;&#29366;&#29289;&#12290;&#35813;&#38382;&#39064;&#22312;&#21453;&#39539;&#22235;&#32500;&#24179;&#20961; Poincar&#233; &#29468;&#24819;&#30340;&#26041;&#27861;&#20013;&#26159;&#30456;&#20851;&#30340;&#65307;&#21033;&#29992;&#25105;&#20204;&#30340;&#31243;&#24207;&#65292;&#25105;&#20204;&#25490;&#38500;&#20102;&#35768;&#22810;&#29468;&#24819;&#30340;&#21453;&#20363;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#20123;&#31243;&#24207;&#25104;&#21151;&#26816;&#27979;&#20102;&#33539;&#22260;&#22312;70&#33410;&#28857;&#20869;&#30340;&#35768;&#22810;&#24102;&#29366;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We apply Bayesian optimization and reinforcement learning to a problem in topology: the question of when a knot bounds a ribbon disk. This question is relevant in an approach to disproving the four-dimensional smooth Poincar\'e conjecture; using our programs, we rule out many potential counterexamples to the conjecture. We also show that the programs are successful in detecting many ribbon knots in the range of up to 70 crossings.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#38745;&#24577;&#21644;&#21160;&#24577;&#21487;&#23398;&#20064;&#20010;&#24615;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26102;&#31354;&#28023;&#34920;&#28201;&#24230;&#39044;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#21033;&#29992;&#20004;&#20010;&#22270;&#23398;&#20064;&#23618;&#20998;&#21035;&#27169;&#22411;&#21270;&#20102;SST&#25968;&#25454;&#30340;&#22266;&#23450;&#32593;&#32476;&#21644;&#21160;&#24577;&#32593;&#32476;&#65292;&#24182;&#35774;&#35745;&#20102;&#20010;&#24615;&#21270;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#23618;&#20197;&#31934;&#30830;&#39044;&#27979;&#26102;&#31354;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.09290</link><description>&lt;p&gt;
&#22522;&#20110;&#38745;&#24577;&#21644;&#21160;&#24577;&#21487;&#23398;&#20064;&#20010;&#24615;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26102;&#31354;&#28023;&#34920;&#28201;&#24230;&#39044;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Spatio-temporal Sea Surface Temperature Forecasting via Static and Dynamic Learnable Personalized Graph Convolution Network. (arXiv:2304.09290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09290
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#38745;&#24577;&#21644;&#21160;&#24577;&#21487;&#23398;&#20064;&#20010;&#24615;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26102;&#31354;&#28023;&#34920;&#28201;&#24230;&#39044;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#21033;&#29992;&#20004;&#20010;&#22270;&#23398;&#20064;&#23618;&#20998;&#21035;&#27169;&#22411;&#21270;&#20102;SST&#25968;&#25454;&#30340;&#22266;&#23450;&#32593;&#32476;&#21644;&#21160;&#24577;&#32593;&#32476;&#65292;&#24182;&#35774;&#35745;&#20102;&#20010;&#24615;&#21270;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#23618;&#20197;&#31934;&#30830;&#39044;&#27979;&#26102;&#31354;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#34920;&#28201;&#24230;&#23545;&#20110;&#22320;&#29699;&#22823;&#27668;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#20854;&#21160;&#21147;&#23398;&#23545;&#20110;&#22609;&#36896;&#26412;&#22320;&#21644;&#20840;&#29699;&#27668;&#20505;&#26377;&#24456;&#22823;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#28145;&#21051;&#22320;&#24433;&#21709;&#30528;&#25105;&#20204;&#30340;&#29983;&#24577;&#31995;&#32479;&#12290;&#20934;&#30830;&#39044;&#27979;&#28023;&#34920;&#28201;&#24230;&#33021;&#22815;&#24102;&#26469;&#37325;&#22823;&#30340;&#32463;&#27982;&#21644;&#31038;&#20250;&#24433;&#21709;&#65292;&#20363;&#22914;&#25552;&#21069;&#25968;&#26376;&#26356;&#22909;&#22320;&#20934;&#22791;&#26497;&#31471;&#22825;&#27668;&#65292;&#22914;&#20005;&#37325;&#24178;&#26097;&#25110;&#28909;&#24102;&#27668;&#26059;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28023;&#27915;&#31995;&#32479;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#39033;&#20219;&#21153;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#38745;&#24577;&#21644;&#21160;&#24577;&#21487;&#23398;&#20064;&#20010;&#24615;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476; (SD-LPGC)&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SD-LPGC&#26041;&#27861;&#22312;&#39044;&#27979;&#31934;&#24230;&#26041;&#38754;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sea surface temperature (SST) is uniquely important to the Earth's atmosphere since its dynamics are a major force in shaping local and global climate and profoundly affect our ecosystems. Accurate forecasting of SST brings significant economic and social implications, for example, better preparation for extreme weather such as severe droughts or tropical cyclones months ahead. However, such a task faces unique challenges due to the intrinsic complexity and uncertainty of ocean systems. Recently, deep learning techniques, such as graphical neural networks (GNN), have been applied to address this task. Even though these methods have some success, they frequently have serious drawbacks when it comes to investigating dynamic spatiotemporal dependencies between signals. To solve this problem, this paper proposes a novel static and dynamic learnable personalized graph convolution network (SD-LPGC). Specifically, two graph learning layers are first constructed to respectively model the stabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pelphix&#30340;X&#20809;&#24341;&#23548;&#19979;&#30340;&#32463;&#30382;&#30406;&#39592;&#25240;&#20462;&#22797;&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;&#26041;&#27861;&#65292;&#20351;&#29992;&#39532;&#23572;&#31185;&#22827;&#36807;&#31243;&#27169;&#25311;&#36807;&#31243;&#24182;&#25552;&#20379;&#23436;&#20840;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#22235;&#20010;&#31890;&#24230;&#32423;&#21035;&#19978;&#22238;&#24402;&#25163;&#26415;&#38454;&#27573;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.09285</link><description>&lt;p&gt;
Pelphix&#65306;&#32463;&#30382;&#30406;&#39592;&#22266;&#23450;&#26415;&#20013;&#22522;&#20110;X&#20809;&#22270;&#20687;&#30340;&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Pelphix: Surgical Phase Recognition from X-ray Images in Percutaneous Pelvic Fixation. (arXiv:2304.09285v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pelphix&#30340;X&#20809;&#24341;&#23548;&#19979;&#30340;&#32463;&#30382;&#30406;&#39592;&#25240;&#20462;&#22797;&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;&#26041;&#27861;&#65292;&#20351;&#29992;&#39532;&#23572;&#31185;&#22827;&#36807;&#31243;&#27169;&#25311;&#36807;&#31243;&#24182;&#25552;&#20379;&#23436;&#20840;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#22235;&#20010;&#31890;&#24230;&#32423;&#21035;&#19978;&#22238;&#24402;&#25163;&#26415;&#38454;&#27573;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;(SPR)&#26159;&#29616;&#20195;&#25163;&#26415;&#23460;&#25968;&#23383;&#21270;&#36716;&#22411;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20803;&#32032;&#12290;&#34429;&#28982;&#22522;&#20110;&#35270;&#39057;&#28304;&#30340;SPR&#24050;&#32463;&#24456;&#25104;&#29087;&#65292;&#20294;&#25554;&#31649;X&#20809;&#24207;&#21015;&#30340;&#25972;&#21512;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Pelphix&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;X&#20809;&#24341;&#23548;&#19979;&#30340;&#32463;&#30382;&#30406;&#39592;&#25240;&#20462;&#22797;&#30340;SPR&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#35813;&#36807;&#31243;&#24314;&#27169;&#20026;&#22235;&#20010;&#31890;&#24230;&#27700;&#24179;&#8212;&#8212;&#36208;&#24266;&#12289;&#27963;&#21160;&#12289;&#35270;&#22270;&#21644;&#24103;&#20540;&#8212;&#8212;&#23558;&#30406;&#39592;&#25240;&#20462;&#22797;&#24037;&#20316;&#27969;&#31243;&#27169;&#25311;&#20026;&#39532;&#23572;&#31185;&#22827;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#20379;&#23436;&#20840;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290; &#20351;&#29992;&#20174;&#39592;&#36208;&#24266;&#12289;&#24037;&#20855;&#21644;&#35299;&#21078;&#23398;&#26816;&#27979;&#20013;&#28155;&#21152;&#30340;&#30417;&#30563;&#23398;&#20064;&#65292;&#25105;&#20204;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#39304;&#36865;&#21040;Transformer&#27169;&#22411;&#20013;&#65292;&#20197;&#22312;&#22235;&#20010;&#31890;&#24230;&#32423;&#21035;&#19978;&#22238;&#24402;&#25163;&#26415;&#38454;&#27573;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22522;&#20110;X&#20809;&#30340;SPR&#30340;&#21487;&#34892;&#24615;&#65292;&#22312;&#27169;&#25311;&#24207;&#21015;&#20013;&#23454;&#29616;&#20102;93.8&#65285;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#22312;&#23608;&#20307;&#20013;&#23454;&#29616;&#20102;67.57&#65285;&#30340;&#25152;&#26377;&#31890;&#24230;&#32423;&#21035;&#65292;&#24182;&#38024;&#23545;&#30446;&#26631;&#36208;&#24266;&#30340;&#20934;&#30830;&#29575;&#39640;&#36798;88&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surgical phase recognition (SPR) is a crucial element in the digital transformation of the modern operating theater. While SPR based on video sources is well-established, incorporation of interventional X-ray sequences has not yet been explored. This paper presents Pelphix, a first approach to SPR for X-ray-guided percutaneous pelvic fracture fixation, which models the procedure at four levels of granularity -- corridor, activity, view, and frame value -- simulating the pelvic fracture fixation workflow as a Markov process to provide fully annotated training data. Using added supervision from detection of bony corridors, tools, and anatomy, we learn image representations that are fed into a transformer model to regress surgical phases at the four granularity levels. Our approach demonstrates the feasibility of X-ray-based SPR, achieving an average accuracy of 93.8% on simulated sequences and 67.57% in cadaver across all granularity levels, with up to 88% accuracy for the target corrido
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24207;&#21015;&#23398;&#20064;&#26469;&#39640;&#25928;&#20248;&#21270;&#22810;&#20010;&#30456;&#20114;&#20914;&#31361;&#30446;&#26631;&#30340;&#22797;&#26434;&#31995;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2304.09278</link><description>&lt;p&gt;
&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#24207;&#21015;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;&#21644;&#20248;&#21270;&#22810;&#30446;&#26631;&#21046;&#36896;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
A Data Driven Sequential Learning Framework to Accelerate and Optimize Multi-Objective Manufacturing Decisions. (arXiv:2304.09278v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24207;&#21015;&#23398;&#20064;&#26469;&#39640;&#25928;&#20248;&#21270;&#22810;&#20010;&#30456;&#20114;&#20914;&#31361;&#30446;&#26631;&#30340;&#22797;&#26434;&#31995;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21046;&#36896;&#20855;&#26377;&#29305;&#23450;&#24615;&#36136;&#25110;&#24615;&#36136;&#32452;&#21512;&#30340;&#20808;&#36827;&#26448;&#26009;&#21644;&#20135;&#21697;&#36890;&#24120;&#26159;&#24517;&#35201;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25214;&#21040;&#33021;&#22815;&#29983;&#25104;&#36825;&#20123;&#24615;&#36136;&#29702;&#24819;&#32452;&#21512;&#30340;&#26368;&#20339;&#37197;&#26041;&#25110;&#22788;&#29702;&#26465;&#20214;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#26102;&#20505;&#65292;&#38656;&#35201;&#36827;&#34892;&#36275;&#22815;&#25968;&#37327;&#30340;&#23454;&#39564;&#25165;&#33021;&#29983;&#25104;Pareto&#21069;&#27839;&#12290;&#28982;&#32780;&#65292;&#21046;&#36896;&#23454;&#39564;&#36890;&#24120;&#24456;&#26114;&#36149;&#65292;&#29978;&#33267;&#36827;&#34892;&#19968;&#27425;&#23454;&#39564;&#20063;&#21487;&#33021;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#30830;&#23450;&#26368;&#20339;&#25968;&#25454;&#25910;&#38598;&#20301;&#32622;&#20197;&#33719;&#24471;&#23545;&#36807;&#31243;&#30340;&#26368;&#20840;&#38754;&#29702;&#35299;&#38750;&#24120;&#20851;&#38190;&#12290;&#24207;&#21015;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#36827;&#34892;&#20013;&#30340;&#23454;&#39564;&#20013;&#20027;&#21160;&#23398;&#20064;&#65292;&#36845;&#20195;&#26356;&#26032;&#22522;&#30784;&#20248;&#21270;&#20363;&#31243;&#65292;&#24182;&#38543;&#26102;&#35843;&#25972;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#24207;&#21015;&#23398;&#20064;&#26469;&#39640;&#25928;&#20248;&#21270;&#20855;&#26377;&#22810;&#20010;&#30456;&#20114;&#20914;&#31361;&#30446;&#26631;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manufacturing advanced materials and products with a specific property or combination of properties is often warranted. To achieve that it is crucial to find out the optimum recipe or processing conditions that can generate the ideal combination of these properties. Most of the time, a sufficient number of experiments are needed to generate a Pareto front. However, manufacturing experiments are usually costly and even conducting a single experiment can be a time-consuming process. So, it's critical to determine the optimal location for data collection to gain the most comprehensive understanding of the process. Sequential learning is a promising approach to actively learn from the ongoing experiments, iteratively update the underlying optimization routine, and adapt the data collection process on the go. This paper presents a novel data-driven Bayesian optimization framework that utilizes sequential learning to efficiently optimize complex systems with multiple conflicting objectives. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#955;&#28436;&#31639;&#27861;&#65292;&#20351;&#29992;&#955;&#35821;&#35328;&#32534;&#31243;&#65292;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#22312;&#25191;&#34892;&#25972;&#20010;&#31243;&#24207;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#25299;&#23637;&#31070;&#32463;&#32593;&#32476;&#22312;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.09276</link><description>&lt;p&gt;
&#19968;&#31181;&#31070;&#32463;&#955;&#28436;&#31639;&#27861;&#65306;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#36935;&#35265;&#35745;&#31639;&#21644;&#20989;&#25968;&#24335;&#32534;&#31243;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Neural Lambda Calculus: Neurosymbolic AI meets the foundations of computing and functional programming. (arXiv:2304.09276v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#955;&#28436;&#31639;&#27861;&#65292;&#20351;&#29992;&#955;&#35821;&#35328;&#32534;&#31243;&#65292;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#22312;&#25191;&#34892;&#25972;&#20010;&#31243;&#24207;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#25299;&#23637;&#31070;&#32463;&#32593;&#32476;&#22312;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#25104;&#20026;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20027;&#23548;&#33539;&#24335;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#35748;&#20026;&#22312;&#31526;&#21495;&#23398;&#20064;&#20013;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26159;&#36234;&#26469;&#36234;&#30456;&#20851;&#30340;&#12290;&#20026;&#20102;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#22312;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25506;&#32034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25968;&#23398;&#26500;&#36896;&#65288;&#22914;&#21152;&#27861;&#21644;&#20056;&#27861;&#65289;&#12289;&#36923;&#36753;&#25512;&#29702;&#65288;&#22914;&#23450;&#29702;&#35777;&#26126;&#22120;&#65289;&#29978;&#33267;&#25191;&#34892;&#35745;&#31639;&#26426;&#31243;&#24207;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21518;&#32773;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#26159;&#22826;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#32467;&#26524;&#24182;&#19981;&#24635;&#26159;&#25104;&#21151;&#30340;&#65292;&#24182;&#19988;&#24448;&#24448;&#38656;&#35201;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#24341;&#20837;&#26377;&#20559;&#35265;&#30340;&#20803;&#32032;&#65292;&#20197;&#38480;&#21046;&#21487;&#33021;&#35201;&#25191;&#34892;&#30340;&#31243;&#24207;&#30340;&#33539;&#22260;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#22914;&#20309;&#25191;&#34892;&#25972;&#20010;&#31243;&#24207;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#19981;&#20351;&#29992;&#21629;&#20196;&#24335;&#32534;&#31243;&#35821;&#35328;&#65292;&#32780;&#26159;&#37319;&#29992;&#955;&#35821;&#35328;&#36827;&#34892;&#32534;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decades, deep neural networks based-models became the dominant paradigm in machine learning. Further, the use of artificial neural networks in symbolic learning has been seen as increasingly relevant recently. To study the capabilities of neural networks in the symbolic AI domain, researchers have explored the ability of deep neural networks to learn mathematical constructions, such as addition and multiplication, logic inference, such as theorem provers, and even the execution of computer programs. The latter is known to be too complex a task for neural networks. Therefore, the results were not always successful, and often required the introduction of biased elements in the learning process, in addition to restricting the scope of possible programs to be executed. In this work, we will analyze the ability of neural networks to learn how to execute programs as a whole. To do so, we propose a different approach. Instead of using an imperative programming language, with com
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#20165;&#20381;&#36182;&#31895;&#31961;&#30340;&#31181;&#26063;&#31867;&#21035;&#21487;&#33021;&#25513;&#30422;&#20102;&#20020;&#24202;&#39118;&#38505;&#35780;&#20998;&#34920;&#29616;&#20013;&#30340;&#37325;&#35201;&#24046;&#24322;&#65292;&#38656;&#35201;&#26356;&#31934;&#32454;&#30340;&#31181;&#26063;&#25968;&#25454;&#37319;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.09270</link><description>&lt;p&gt;
&#31895;&#31961;&#30340;&#31181;&#26063;&#25968;&#25454;&#25513;&#30422;&#20102;&#20020;&#24202;&#39118;&#38505;&#35780;&#20998;&#34920;&#29616;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Coarse race data conceals disparities in clinical risk score performance. (arXiv:2304.09270v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09270
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#20165;&#20381;&#36182;&#31895;&#31961;&#30340;&#31181;&#26063;&#31867;&#21035;&#21487;&#33021;&#25513;&#30422;&#20102;&#20020;&#24202;&#39118;&#38505;&#35780;&#20998;&#34920;&#29616;&#20013;&#30340;&#37325;&#35201;&#24046;&#24322;&#65292;&#38656;&#35201;&#26356;&#31934;&#32454;&#30340;&#31181;&#26063;&#25968;&#25454;&#37319;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#30340;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#36890;&#24120;&#21482;&#35760;&#24405;&#30149;&#20154;&#30340;&#31895;&#30053;&#31181;&#26063;&#32452;&#65306;&#20363;&#22914;&#65292;&#21360;&#24230;&#21644;&#20013;&#22269;&#30149;&#20154;&#36890;&#24120;&#37117;&#34987;&#32534;&#30721;&#20026;&#8220;&#20122;&#27954;&#20154;&#8221;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#36825;&#31181;&#31895;&#30053;&#32534;&#30721;&#26159;&#21542;&#25513;&#30422;&#20102;&#31934;&#32454;&#31181;&#26063;&#32452;&#20043;&#38388;&#30340;&#20020;&#24202;&#39118;&#38505;&#35780;&#20998;&#34920;&#29616;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#26412;&#25991;&#21033;&#29992;418K&#32039;&#24613;&#31185;&#23460;&#23601;&#35786;&#30340;&#25968;&#25454;&#65292;&#35780;&#20272;&#20102;&#19977;&#31181;&#32467;&#23616;&#12289;&#20116;&#31181;&#39118;&#38505;&#35780;&#20998;&#21644;&#22235;&#31181;&#34920;&#29616;&#25351;&#26631;&#30340;&#31934;&#32454;&#31181;&#26063;&#32452;&#20043;&#38388;&#30340;&#20020;&#24202;&#39118;&#38505;&#35780;&#20998;&#34920;&#29616;&#24046;&#24322;&#12290;&#22312;&#21508;&#31181;&#32467;&#23616;&#21644;&#25351;&#26631;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#31895;&#30053;&#31181;&#26063;&#31867;&#21035;&#20869;&#23384;&#22312;&#37325;&#35201;&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;&#20107;&#23454;&#19978;&#65292;&#22312;&#31895;&#30053;&#31867;&#21035;&#20869;&#65292;&#24615;&#33021;&#25351;&#26631;&#30340;&#21464;&#24322;&#24120;&#24120;&#36229;&#36807;&#31895;&#30053;&#31867;&#21035;&#20043;&#38388;&#30340;&#21464;&#24322;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#24046;&#24322;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#32467;&#23616;&#29575;&#12289;&#29305;&#24449;&#20998;&#24067;&#20197;&#21450;&#29305;&#24449;&#19982;&#32467;&#26524;&#20043;&#38388;&#30340;&#20851;&#31995;&#22312;&#19981;&#21516;&#31934;&#32454;&#31181;&#26063;&#31867;&#21035;&#20043;&#38388;&#37117;&#26377;&#26174;&#30528;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20381;&#36182;&#31895;&#31961;&#30340;&#31181;&#26063;&#31867;&#21035;&#21487;&#33021;&#25513;&#30422;&#20102;&#20020;&#24202;&#39118;&#38505;&#35780;&#20998;&#34920;&#29616;&#20013;&#30340;&#37325;&#35201;&#24046;&#24322;&#65292;&#24182;&#24378;&#35843;&#20102;&#21307;&#30103;&#20445;&#20581;&#29615;&#22659;&#20013;&#38656;&#35201;&#25910;&#38598;&#26356;&#31934;&#32454;&#30340;&#31181;&#26063;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Healthcare data in the United States often records only a patient's coarse race group: for example, both Indian and Chinese patients are typically coded as ``Asian.'' It is unknown, however, whether this coarse coding conceals meaningful disparities in the performance of clinical risk scores across granular race groups. Here we show that it does. Using data from 418K emergency department visits, we assess clinical risk score performance disparities across granular race groups for three outcomes, five risk scores, and four performance metrics. Across outcomes and metrics, we show that there are significant granular disparities in performance within coarse race categories. In fact, variation in performance metrics within coarse groups often exceeds the variation between coarse groups. We explore why these disparities arise, finding that outcome rates, feature distributions, and the relationships between features and outcomes all vary significantly across granular race categories. Our res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20869;&#23384;&#27169;&#25311;&#35745;&#31639;&#65288;IMAC&#65289;&#21333;&#20803;&#19982;&#24352;&#37327;&#22788;&#29702;&#21333;&#20803;&#65288;TPUs&#65289;&#38598;&#25104;&#30340;&#26032;&#22411;&#12289;&#24322;&#26500;&#12289;&#28151;&#21512;&#20449;&#21495;&#21644;&#28151;&#21512;&#31934;&#24230;&#26550;&#26500;&#65292;&#20197;&#25552;&#39640;&#31227;&#21160;CNN&#24615;&#33021;&#12290;&#32467;&#21512;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#25216;&#26415;&#65292;&#20805;&#20998;&#21033;&#29992;TPUs&#22312;&#21367;&#31215;&#23618;&#20013;&#30340;&#20248;&#28857;&#21644;IMAC&#30005;&#36335;&#22312;&#23494;&#38598;&#23618;&#20013;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;...&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2304.09258</link><description>&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#22788;&#29702;&#21333;&#20803;&#30340;&#20869;&#23384;&#27169;&#25311;&#27169;&#25311;&#35745;&#31639;&#24322;&#26500;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Integration of In-Memory Analog Computing Architectures with Tensor Processing Units. (arXiv:2304.09258v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20869;&#23384;&#27169;&#25311;&#35745;&#31639;&#65288;IMAC&#65289;&#21333;&#20803;&#19982;&#24352;&#37327;&#22788;&#29702;&#21333;&#20803;&#65288;TPUs&#65289;&#38598;&#25104;&#30340;&#26032;&#22411;&#12289;&#24322;&#26500;&#12289;&#28151;&#21512;&#20449;&#21495;&#21644;&#28151;&#21512;&#31934;&#24230;&#26550;&#26500;&#65292;&#20197;&#25552;&#39640;&#31227;&#21160;CNN&#24615;&#33021;&#12290;&#32467;&#21512;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#25216;&#26415;&#65292;&#20805;&#20998;&#21033;&#29992;TPUs&#22312;&#21367;&#31215;&#23618;&#20013;&#30340;&#20248;&#28857;&#21644;IMAC&#30005;&#36335;&#22312;&#23494;&#38598;&#23618;&#20013;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;...&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#22788;&#29702;&#21333;&#20803;&#65288;TPUs&#65289;&#26159;&#19987;&#38376;&#21152;&#36895;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#22312;&#25191;&#34892;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#20013;&#30340;&#21367;&#31215;&#23618;&#26102;&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#22312;&#20840;&#36830;&#25509;&#65288;FC&#65289;&#23618;&#20013;&#65292;&#23427;&#20204;&#24456;&#38590;&#20445;&#25345;&#30456;&#21516;&#30340;&#25928;&#29575;&#65292;&#23548;&#33268;&#30828;&#20214;&#21033;&#29992;&#29575;&#19981;&#20339;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20869;&#23384;&#27169;&#25311;&#35745;&#31639;&#65288;IMAC&#65289;&#26550;&#26500;&#22312;&#25191;&#34892;FC&#23618;&#26102;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#24322;&#26500;&#12289;&#28151;&#21512;&#20449;&#21495;&#21644;&#28151;&#21512;&#31934;&#24230;&#26550;&#26500;&#65292;&#23558;IMAC&#21333;&#20803;&#19982;&#36793;&#32536;TPU&#38598;&#25104;&#65292;&#20197;&#25552;&#39640;&#31227;&#21160;CNN&#24615;&#33021;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;TPUs&#22312;&#21367;&#31215;&#23618;&#20013;&#30340;&#20248;&#28857;&#21644;IMAC&#30005;&#36335;&#22312;&#23494;&#38598;&#23618;&#20013;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#32467;&#21512;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#25216;&#26415;&#26469;&#20943;&#36731;&#22312;TPU-IMAC&#26550;&#26500;&#19978;&#37096;&#32626;&#27169;&#22411;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#31934;&#24230;&#38477;&#20302;&#38382;&#39064;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;TPU-IMAC&#37197;&#32622;&#23454;&#29616;&#20102;&#39640;&#36798;...
&lt;/p&gt;
&lt;p&gt;
Tensor processing units (TPUs), specialized hardware accelerators for machine learning tasks, have shown significant performance improvements when executing convolutional layers in convolutional neural networks (CNNs). However, they struggle to maintain the same efficiency in fully connected (FC) layers, leading to suboptimal hardware utilization. In-memory analog computing (IMAC) architectures, on the other hand, have demonstrated notable speedup in executing FC layers. This paper introduces a novel, heterogeneous, mixed-signal, and mixed-precision architecture that integrates an IMAC unit with an edge TPU to enhance mobile CNN performance. To leverage the strengths of TPUs for convolutional layers and IMAC circuits for dense layers, we propose a unified learning algorithm that incorporates mixed-precision training techniques to mitigate potential accuracy drops when deploying models on the TPU-IMAC architecture. The simulations demonstrate that the TPU-IMAC configuration achieves up 
&lt;/p&gt;</description></item><item><title>IMAC-Sim &#26159;&#19968;&#31181;&#38024;&#23545;&#20869;&#23384;&#27169;&#25311;&#27169;&#25311;&#35745;&#31639;&#26550;&#26500;&#30340;&#30005;&#36335;&#32423;&#27169;&#25311;&#22120;&#65292;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#25351;&#23450;&#30340;&#36229;&#21442;&#25968;&#24555;&#36895;&#21019;&#24314;&#30005;&#36335;&#65292;&#24182;&#33258;&#21160;&#35780;&#20272;&#20854;&#20934;&#30830;&#24615;&#12289;&#21151;&#32791;&#21644;&#24310;&#36831;&#65292;&#21516;&#26102;&#36824;&#32771;&#34385;&#20102;&#20114;&#36830;&#23492;&#29983;&#30005;&#38459;&#21644;&#30005;&#23481;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.09252</link><description>&lt;p&gt;
IMAC-Sim&#65306;&#19968;&#31181;&#38754;&#21521;&#20869;&#23384;&#27169;&#25311;&#27169;&#25311;&#35745;&#31639;&#26550;&#26500;&#30340;&#30005;&#36335;&#32423;&#27169;&#25311;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
IMAC-Sim: A Circuit-level Simulator For In-Memory Analog Computing Architectures. (arXiv:2304.09252v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09252
&lt;/p&gt;
&lt;p&gt;
IMAC-Sim &#26159;&#19968;&#31181;&#38024;&#23545;&#20869;&#23384;&#27169;&#25311;&#27169;&#25311;&#35745;&#31639;&#26550;&#26500;&#30340;&#30005;&#36335;&#32423;&#27169;&#25311;&#22120;&#65292;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#25351;&#23450;&#30340;&#36229;&#21442;&#25968;&#24555;&#36895;&#21019;&#24314;&#30005;&#36335;&#65292;&#24182;&#33258;&#21160;&#35780;&#20272;&#20854;&#20934;&#30830;&#24615;&#12289;&#21151;&#32791;&#21644;&#24310;&#36831;&#65292;&#21516;&#26102;&#36824;&#32771;&#34385;&#20102;&#20114;&#36830;&#23492;&#29983;&#30005;&#38459;&#21644;&#30005;&#23481;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#25226;&#22522;&#20110;&#24518;&#38459;&#22120;&#30340;&#20869;&#23384;&#27169;&#25311;&#27169;&#25311;&#35745;&#31639; (IMAC) &#26550;&#26500;&#20316;&#20026;&#26367;&#20195;&#39640;&#33021;&#32791;&#35745;&#31639;&#26426;&#31995;&#32479;&#30340;&#36873;&#25321;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#22240;&#27492;&#65292;&#19968;&#31181;&#21487;&#20197;&#25506;&#32034;IMAC&#26550;&#26500;&#20013;&#30340;&#35774;&#22791;&#21644;&#30005;&#36335;&#35774;&#35745;&#31354;&#38388;&#30340;&#24037;&#20855;&#21487;&#20197;&#26174;&#33879;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;IMAC-Sim&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;IMAC&#26550;&#26500;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#30340;&#30005;&#36335;&#32423;&#27169;&#25311;&#22120;&#12290;IMAC-Sim&#26159;&#22522;&#20110;Python&#30340;&#27169;&#25311;&#26694;&#26550;&#65292;&#23427;&#26681;&#25454;&#29992;&#25143;&#36873;&#25321;&#30340;&#21508;&#31181;&#35774;&#22791;&#21644;&#30005;&#36335;&#32423;&#36229;&#21442;&#25968;&#21019;&#24314;IMAC&#30005;&#36335;&#30340;SPICE&#32593;&#34920;&#65292;&#24182;&#33258;&#21160;&#35780;&#20272;&#25152;&#24320;&#21457;&#30005;&#36335;&#30340;&#20934;&#30830;&#24615;&#12289;&#21151;&#32791;&#21644;&#24310;&#36831;&#65292;&#20351;&#29992;&#29992;&#25143;&#25351;&#23450;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;IMAC-Sim&#27169;&#25311;&#20102;IMAC&#26550;&#26500;&#20013;&#30340;&#20114;&#36830;&#23492;&#29983;&#30005;&#38459;&#21644;&#30005;&#23481;&#65292;&#24182;&#37197;&#22791;&#20102;&#27700;&#24179;&#21644;&#22402;&#30452;&#20998;&#21306;&#25216;&#26415;&#20197;&#20811;&#26381;&#36825;&#20123;&#21487;&#38752;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increased attention to memristive-based in-memory analog computing (IMAC) architectures as an alternative for energy-hungry computer systems for machine learning applications, a tool that enables exploring their device- and circuit-level design space can significantly boost the research and development in this area. Thus, in this paper, we develop IMAC-Sim, a circuit-level simulator for the design space exploration of IMAC architectures. IMAC-Sim is a Python-based simulation framework, which creates the SPICE netlist of the IMAC circuit based on various device- and circuit-level hyperparameters selected by the user, and automatically evaluates the accuracy, power consumption, and latency of the developed circuit using a user-specified dataset. Moreover, IMAC-Sim simulates the interconnect parasitic resistance and capacitance in the IMAC architectures and is also equipped with horizontal and vertical partitioning techniques to surmount these reliability challenges. IMAC-Sim is 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20998;&#26512;&#36816;&#21160;&#30151;&#29366;&#21644;&#27493;&#24577;&#30456;&#20851;&#21442;&#25968;&#65292;&#20197;&#31579;&#36873;&#20986;&#26377;&#25928;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#36827;&#32780;&#23454;&#29616;&#24085;&#37329;&#26862;&#30149;&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#32463;&#27982;&#21644;&#31283;&#20581;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#27169;&#22411;&#20934;&#30830;&#29575;&#39640;&#36798;91.9&#65285;&#12290;</title><link>http://arxiv.org/abs/2304.09245</link><description>&lt;p&gt;
&#21033;&#29992;&#36816;&#21160;&#30151;&#29366;&#21644;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#24085;&#37329;&#26862;&#30149;&#30340;&#26089;&#26399;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Early Detection of Parkinson's Disease using Motor Symptoms and Machine Learning. (arXiv:2304.09245v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20998;&#26512;&#36816;&#21160;&#30151;&#29366;&#21644;&#27493;&#24577;&#30456;&#20851;&#21442;&#25968;&#65292;&#20197;&#31579;&#36873;&#20986;&#26377;&#25928;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#36827;&#32780;&#23454;&#29616;&#24085;&#37329;&#26862;&#30149;&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#32463;&#27982;&#21644;&#31283;&#20581;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#27169;&#22411;&#20934;&#30830;&#29575;&#39640;&#36798;91.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#37329;&#26862;&#30149;(PD)&#24050;&#32463;&#21457;&#29616;&#24433;&#21709;&#21040;&#27599;1000&#20154;&#20013;&#30340;1&#20010;&#20154;&#65292;&#26356;&#20542;&#21521;&#20110;60&#23681;&#20197;&#19978;&#30340;&#20154;&#32676;&#12290;&#21033;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#23547;&#25214;&#20934;&#30830;&#30340;&#35786;&#26029;&#29983;&#29289;&#26631;&#24535;&#29289;&#24050;&#25104;&#20026;&#26102;&#20195;&#30340;&#38656;&#27714;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20687;&#24085;&#37329;&#26862;&#36825;&#26679;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#12290;&#26412;&#25991;&#26088;&#22312;&#20851;&#27880;&#26089;&#26399;&#21457;&#29983;&#30340;&#24120;&#35265;&#30151;&#29366;&#65292;&#22914;&#36816;&#21160;&#21644;&#27493;&#24577;&#30456;&#20851;&#21442;&#25968;&#65292;&#20197;&#20415;&#23545;&#32463;&#27982;&#21644;&#31283;&#20581;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#21487;&#34892;&#24615;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#12290;&#32463;&#36807;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24443;&#24213;&#20998;&#26512;&#65292;&#21033;&#29992;&#24085;&#37329;&#26862;&#30142;&#30149;&#36827;&#23637;&#26631;&#24535;&#20513;&#35758; (PPMI) &#30340;&#19968;&#20010;&#23376;&#38598;PPMI&#27493;&#24577;&#25968;&#25454;&#38598;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#12290;&#30830;&#35748;&#30340;&#37325;&#35201;&#29305;&#24449;&#24050;&#34987;&#29992;&#20110;&#27979;&#35797;&#23454;&#26102;&#25968;&#25454;&#20197;&#26089;&#26399;&#26816;&#27979;&#24085;&#37329;&#26862;&#32508;&#21512;&#30151;&#65292;&#27169;&#22411;&#20934;&#30830;&#29575;&#36798;91.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parkinson's disease (PD) has been found to affect 1 out of every 1000 people, being more inclined towards the population above 60 years. Leveraging wearable-systems to find accurate biomarkers for diagnosis has become the need of the hour, especially for a neurodegenerative condition like Parkinson's. This work aims at focusing on early-occurring, common symptoms, such as motor and gait related parameters to arrive at a quantitative analysis on the feasibility of an economical and a robust wearable device. A subset of the Parkinson's Progression Markers Initiative (PPMI), PPMI Gait dataset has been utilised for feature-selection after a thorough analysis with various Machine Learning algorithms. Identified influential features has then been used to test real-time data for early detection of Parkinson Syndrome, with a model accuracy of 91.9%
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#32447;&#24615;&#20989;&#25968;&#26500;&#24314;&#20132;&#21449;&#30456;&#20851;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;Price&#23450;&#29702;&#21644;&#20998;&#27573;&#32447;&#24615;&#20998;&#35299;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#26469;&#20998;&#26512;&#36825;&#31181;&#20132;&#21449;&#30456;&#20851;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.09242</link><description>&lt;p&gt;
&#20351;&#29992;Price&#23450;&#29702;&#21644;&#20998;&#27573;&#32447;&#24615;&#20998;&#35299;&#20998;&#26512;&#22312;&#32447;&#20132;&#21449;&#30456;&#20851;&#22120;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Framework for Analyzing Online Cross-correlators using Price's Theorem and Piecewise-Linear Decomposition. (arXiv:2304.09242v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#32447;&#24615;&#20989;&#25968;&#26500;&#24314;&#20132;&#21449;&#30456;&#20851;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;Price&#23450;&#29702;&#21644;&#20998;&#27573;&#32447;&#24615;&#20998;&#35299;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#26469;&#20998;&#26512;&#36825;&#31181;&#20132;&#21449;&#30456;&#20851;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#20272;&#35745;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#20132;&#21449;&#30456;&#20851;&#25110;&#30456;&#20284;&#24230;&#26159;&#20449;&#21495;&#26816;&#27979;&#12289;&#39640;&#32500;&#35745;&#31639;&#12289;&#32852;&#24819;&#35760;&#24518;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#26500;&#24314;&#20855;&#26377;&#26356;&#39640;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#30340;&#20132;&#21449;&#30456;&#20851;&#22120;&#30340;&#22823;&#37327;&#31616;&#21333;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;Price&#23450;&#29702;&#21644;&#20998;&#27573;&#32447;&#24615;&#20998;&#35299;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#20197;&#20998;&#26512;&#20351;&#29992;&#28151;&#21512;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#26500;&#24314;&#30340;&#20132;&#21449;&#30456;&#20851;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise estimation of cross-correlation or similarity between two random variables lies at the heart of signal detection, hyperdimensional computing, associative memories, and neural networks. Although a vast literature exists on different methods for estimating cross-correlations, the question what is the best and simplest method to estimate cross-correlations using finite samples ? is still not clear. In this paper, we first argue that the standard empirical approach might not be the optimal method even though the estimator exhibits uniform convergence to the true cross-correlation. Instead, we show that there exists a large class of simple non-linear functions that can be used to construct cross-correlators with a higher signal-to-noise ratio (SNR). To demonstrate this, we first present a general mathematical framework using Price's Theorem that allows us to analyze cross-correlators constructed using a mixture of piece-wise linear functions. Using this framework and high-dimensiona
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;&#24182;&#34892;&#37327;&#23376;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#19968;&#20010;&#20855;&#26377;&#37327;&#23376;&#21367;&#31215;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20854;&#20013;&#19968;&#20010;&#28151;&#21512;&#37327;&#23376;&#26041;&#27861;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#36229;&#36807;99%&#30340;&#24778;&#20154;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.09224</link><description>&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning for image classification. (arXiv:2304.09224v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;&#24182;&#34892;&#37327;&#23376;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#19968;&#20010;&#20855;&#26377;&#37327;&#23376;&#21367;&#31215;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20854;&#20013;&#19968;&#20010;&#28151;&#21512;&#37327;&#23376;&#26041;&#27861;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#36229;&#36807;99%&#30340;&#24778;&#20154;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#35782;&#21035;&#21644;&#20998;&#31867;&#26159;&#21508;&#34892;&#21508;&#19994;&#20013;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#26159;&#29616;&#20195;&#19990;&#30028;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#39046;&#22495;&#12290;&#36817;&#24180;&#26469;&#65292;&#23588;&#20854;&#26159;&#31070;&#32463;&#32593;&#32476;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26041;&#27861;&#21033;&#29992;&#37327;&#23376;&#25928;&#24212;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20256;&#32479;&#32463;&#20856;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20004;&#31181;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#27169;&#22411;&#65306;&#19968;&#20010;&#20855;&#26377;&#24182;&#34892;&#37327;&#23376;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#19968;&#20010;&#20855;&#26377;&#37327;&#23376;&#21367;&#31215;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26469;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#20854;&#20013;&#19968;&#20010;&#28151;&#21512;&#37327;&#23376;&#26041;&#27861;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#36229;&#36807;99%&#30340;&#24778;&#20154;&#20934;&#30830;&#29575;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#37327;&#23376;&#30005;&#36335;&#20013;&#65292;&#25152;&#26377;&#21487;&#21464;&#21442;&#25968;&#37117;&#26159;&#21487;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#23558;&#37327;&#23376;&#37096;&#20998;&#20998;&#25104;&#22810;&#20010;&#24182;&#34892;&#21487;&#21464;&#37327;&#23376;&#30005;&#36335;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image recognition and classification are fundamental tasks with diverse practical applications across various industries, making them critical in the modern world. Recently, machine learning models, particularly neural networks, have emerged as powerful tools for solving these problems. However, the utilization of quantum effects through hybrid quantum-classical approaches can further enhance the capabilities of traditional classical models. Here, we propose two hybrid quantum-classical models: a neural network with parallel quantum layers and a neural network with a quanvolutional layer, which address image classification problems. One of our hybrid quantum approaches demonstrates remarkable accuracy of more than 99% on the MNIST dataset. Notably, in the proposed quantum circuits all variational parameters are trainable, and we divide the quantum part into multiple parallel variational quantum circuits for efficient neural network learning. In summary, our study contributes to the ong
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30740;&#31350;&#20102;&#35299;&#26512;&#24230;&#20989;&#25968;&#20026;&#38750;&#20984;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#24403;&#26426;&#22120;&#23398;&#20064;&#22122;&#22768;&#30340;&#23610;&#24230;&#19982;&#30446;&#26631;&#20989;&#25968;&#30456;&#31561;&#26102;&#65292;&#22312;&#23616;&#37096;&#21306;&#22495;&#20869;&#21021;&#22987;&#21270;&#21518;&#65292;&#20197;&#27491;&#30340;&#27010;&#29575;&#33021;&#22815;&#25910;&#25947;&#21040;&#35813;&#21306;&#22495;&#20869;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2304.09221</link><description>&lt;p&gt;
&#22522;&#20110;&#23616;&#37096;Lajasiewicz&#26465;&#20214;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convergence of stochastic gradient descent under a local Lajasiewicz condition for deep neural networks. (arXiv:2304.09221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30740;&#31350;&#20102;&#35299;&#26512;&#24230;&#20989;&#25968;&#20026;&#38750;&#20984;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#24403;&#26426;&#22120;&#23398;&#20064;&#22122;&#22768;&#30340;&#23610;&#24230;&#19982;&#30446;&#26631;&#20989;&#25968;&#30456;&#31561;&#26102;&#65292;&#22312;&#23616;&#37096;&#21306;&#22495;&#20869;&#21021;&#22987;&#21270;&#21518;&#65292;&#20197;&#27491;&#30340;&#27010;&#29575;&#33021;&#22815;&#25910;&#25947;&#21040;&#35813;&#21306;&#22495;&#20869;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#35299;&#26512;&#24230;&#20989;&#25968;&#20026;&#38750;&#20984;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#22312;&#26377;&#38480;&#23485;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#21152;&#20837;&#26368;&#23567;&#30340;&#39069;&#22806;&#20551;&#35774;&#24182;&#20445;&#35777;&#26426;&#22120;&#23398;&#20064;&#22122;&#22768;&#30340;&#23610;&#24230;&#19982;&#30446;&#26631;&#20989;&#25968;&#30456;&#31561;&#65292;&#35777;&#26126;&#20102;&#22312;&#23616;&#37096;&#21306;&#22495;&#20869;&#21021;&#22987;&#21270;&#26102;&#65292;&#20197;&#27491;&#30340;&#27010;&#29575;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36845;&#20195;&#25910;&#25947;&#21040;&#35813;&#21306;&#22495;&#20869;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;&#26412;&#25991;&#30340;&#20851;&#38190;&#26159;&#30830;&#20445;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#25972;&#20010;&#36712;&#36857;&#20197;&#27491;&#30340;&#27010;&#29575;&#20445;&#30041;&#22312;&#23616;&#37096;&#21306;&#22495;&#20869;&#12290;&#25991;&#31456;&#25552;&#20379;&#20102;&#36127;&#38754;&#20998;&#26512;&#65292;&#34920;&#26126;&#20351;&#29992;Robbins-Monro&#31867;&#22411;&#30340;&#27493;&#38271;&#20043;&#38388;&#20855;&#26377;&#26377;&#30028;&#22122;&#22768;&#30340;&#20551;&#35774;&#19981;&#36275;&#20197;&#20445;&#25345;&#35813;&#20851;&#38190;&#37096;&#20998;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We extend the global convergence result of Chatterjee \cite{chatterjee2022convergence} by considering the stochastic gradient descent (SGD) for non-convex objective functions. With minimal additional assumptions that can be realized by finitely wide neural networks, we prove that if we initialize inside a local region where the \L{}ajasiewicz condition holds, with a positive probability, the stochastic gradient iterates converge to a global minimum inside this region. A key component of our proof is to ensure that the whole trajectories of SGD stay inside the local region with a positive probability. For that, we assume the SGD noise scales with the objective function, which is called machine learning noise and achievable in many real examples. Furthermore, we provide a negative argument to show why using the boundedness of noise with Robbins-Monro type step sizes is not enough to keep the key component valid.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#30340;&#20132;&#36890;&#25968;&#25454;&#22635;&#34917;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#32570;&#22833;&#25110;&#19981;&#23436;&#25972;&#25968;&#25454;&#38382;&#39064;&#65292;&#20197;&#36827;&#19968;&#27493;&#24212;&#29992;&#35813;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.09182</link><description>&lt;p&gt;
&#32771;&#34385;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#30340;&#20132;&#36890;&#25968;&#25454;&#22635;&#34917;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Framework for Traffic Data Imputation Considering Spatiotemporal Dependencies. (arXiv:2304.09182v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09182
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#30340;&#20132;&#36890;&#25968;&#25454;&#22635;&#34917;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#32570;&#22833;&#25110;&#19981;&#23436;&#25972;&#25968;&#25454;&#38382;&#39064;&#65292;&#20197;&#36827;&#19968;&#27493;&#24212;&#29992;&#35813;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#26102;&#31354;&#65288;ST&#65289;&#25968;&#25454;&#21487;&#20197;&#34920;&#31034;&#20026;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#36825;&#26159;&#25353;&#26102;&#38388;&#39034;&#24207;&#21015;&#20986;&#30340;&#25968;&#25454;&#28857;&#24207;&#21015;&#12290;&#23613;&#31649;&#23384;&#22312;&#22823;&#37327;&#26377;&#29992;&#20449;&#24687;&#65292;&#20294;ST&#25968;&#25454;&#36890;&#24120;&#23384;&#22312;&#32570;&#22833;&#25110;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#36825;&#20063;&#38480;&#21046;&#20102;&#23427;&#30340;&#24212;&#29992;&#12290;&#25968;&#25454;&#22635;&#34917;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32463;&#24120;&#29992;&#20110;&#39044;&#22788;&#29702;&#25968;&#25454;&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#20132;&#36890;&#32593;&#32476;&#20013;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#21464;&#21270;&#30340;&#22797;&#26434;&#24615;&#65292;&#26102;&#31354;&#25968;&#25454;&#22635;&#34917;&#38750;&#24120;&#22256;&#38590;&#65292;&#26159;&#36827;&#19968;&#27493;&#24212;&#29992;&#30340;&#20851;&#38190;&#21069;&#25552;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22823;&#22810;&#21482;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#25110;&#38745;&#24577;&#31354;&#38388;&#20381;&#36182;&#24615;&#12290;&#20182;&#20204;&#26080;&#27861;&#30452;&#25509;&#24314;&#27169;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#30456;&#23545;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal (ST) data collected by sensors can be represented as multi-variate time series, which is a sequence of data points listed in an order of time. Despite the vast amount of useful information, the ST data usually suffer from the issue of missing or incomplete data, which also limits its applications. Imputation is one viable solution and is often used to prepossess the data for further applications. However, in practice, n practice, spatiotemporal data imputation is quite difficult due to the complexity of spatiotemporal dependencies with dynamic changes in the traffic network and is a crucial prepossessing task for further applications. Existing approaches mostly only capture the temporal dependencies in time series or static spatial dependencies. They fail to directly model the spatiotemporal dependencies, and the representation ability of the models is relatively limited.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20998;&#26512;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26159;&#30740;&#31350;&#28909;&#28857;&#65292;&#21487;&#29992;&#20110;&#26089;&#26399;&#30149;&#21464;&#26816;&#27979;&#21644;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2304.09178</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31687;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Alzheimers Disease Diagnosis using Machine Learning: A Review. (arXiv:2304.09178v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20998;&#26512;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26159;&#30740;&#31350;&#28909;&#28857;&#65292;&#21487;&#29992;&#20110;&#26089;&#26399;&#30149;&#21464;&#26816;&#27979;&#21644;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26159;&#19968;&#31181;&#20005;&#37325;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#20250;&#36880;&#28176;&#23548;&#33268;&#35760;&#24518;&#21147;&#20007;&#22833;&#12290;&#36825;&#31181;&#33268;&#21629;&#24615;&#33041;&#30149;&#20027;&#35201;&#24433;&#21709;&#32769;&#24180;&#20154;&#65292;&#23548;&#33268;&#35748;&#30693;&#21644;&#29983;&#29289;&#23398;&#21151;&#33021;&#34928;&#36864;&#24182;&#36880;&#28176;&#24341;&#36215;&#33041;&#33806;&#32553;&#12290;&#20026;&#20102;&#20934;&#30830;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65292;&#38656;&#35201;&#37319;&#29992;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#34892;&#19994;&#20013;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#21644;&#24212;&#29992;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#21028;&#26029;&#19968;&#20010;&#20154;&#26159;&#21542;&#26377;&#26089;&#26399;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;2008&#24180;&#33267;2023&#24180;&#38388;&#36890;&#36807;&#35895;&#27468;&#23398;&#26415;&#21457;&#29616;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimers Disease AD is an acute neuro disease that degenerates the brain cells and thus leads to memory loss progressively. It is a fatal brain disease that mostly affects the elderly. It steers the decline of cognitive and biological functions of the brain and shrinks the brain successively, which in turn is known as Atrophy. For an accurate diagnosis of Alzheimers disease, cutting edge methods like machine learning are essential. Recently, machine learning has gained a lot of attention and popularity in the medical industry. As the illness progresses, those with Alzheimers have a far more difficult time doing even the most basic tasks, and in the worst case, their brain completely stops functioning. A persons likelihood of having early-stage Alzheimers disease may be determined using the ML method. In this analysis, papers on Alzheimers disease diagnosis based on deep learning techniques and reinforcement learning between 2008 and 2023 found in google scholar were studied. Sixty re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#21644;&#21487;&#24494;&#20998;&#30340;AUC&#20248;&#21270;&#26041;&#27861;&#65288;PDAOM&#65289;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#20108;&#20803;&#20998;&#31867;&#22120;&#24182;&#21521;&#20854;&#25552;&#20379;&#22312;&#29420;&#31435;&#29992;&#25143;&#32452;&#20013;&#32039;&#23494;&#30456;&#20851;&#30340;&#27491;&#36127;&#26679;&#26412;&#23545;&#65292;&#20197;&#20419;&#36827;&#20998;&#31867;&#22120;&#20851;&#27880;&#19981;&#26131;&#21306;&#20998;&#30340;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;AUC&#21644;GAUC&#25351;&#26631;&#65292;&#36824;&#20943;&#23569;&#20102;&#35757;&#32451;&#30446;&#26631;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.09176</link><description>&lt;p&gt;
&#21033;&#29992;&#19981;&#21487;&#24494;&#20998;&#30340;&#32676;&#32452; AUC &#20248;&#21270;&#25552;&#21319;&#20010;&#24615;&#21270;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Enhancing Personalized Ranking With Differentiable Group AUC Optimization. (arXiv:2304.09176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#21644;&#21487;&#24494;&#20998;&#30340;AUC&#20248;&#21270;&#26041;&#27861;&#65288;PDAOM&#65289;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#20108;&#20803;&#20998;&#31867;&#22120;&#24182;&#21521;&#20854;&#25552;&#20379;&#22312;&#29420;&#31435;&#29992;&#25143;&#32452;&#20013;&#32039;&#23494;&#30456;&#20851;&#30340;&#27491;&#36127;&#26679;&#26412;&#23545;&#65292;&#20197;&#20419;&#36827;&#20998;&#31867;&#22120;&#20851;&#27880;&#19981;&#26131;&#21306;&#20998;&#30340;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;AUC&#21644;GAUC&#25351;&#26631;&#65292;&#36824;&#20943;&#23569;&#20102;&#35757;&#32451;&#30446;&#26631;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AUC&#26159;&#35780;&#20272;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#24120;&#35265;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20998;&#31867;&#22120;&#26159;&#20351;&#29992;&#20132;&#21449;&#29109;&#35757;&#32451;&#30340;&#65292;&#23427;&#24182;&#19981;&#30452;&#25509;&#20248;&#21270;AUC&#25351;&#26631;&#65292;&#36825;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#38454;&#27573;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;PDAOM&#25439;&#22833;&#65292;&#19968;&#31181;&#20855;&#26377;&#26368;&#22823;&#36829;&#35268;&#35268;&#23450;&#30340;&#20010;&#24615;&#21270;&#21644;&#21487;&#24494;&#20998;AUC&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#30452;&#25509;&#24212;&#29992;&#20110;&#35757;&#32451;&#20108;&#20803;&#20998;&#31867;&#22120;&#24182;&#29992;&#26799;&#24230;&#20248;&#21270;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#25104;&#23545;&#25351;&#25968;&#25439;&#22833;&#20989;&#25968;&#65292;&#23558;&#29992;&#25143;ID&#20998;&#32452;&#30340;&#23376;&#25209;&#27425;&#20013;&#30340;&#38590;&#20998;&#36776;&#27491;&#36127;&#26679;&#26412;&#23545;&#25286;&#20998;&#20986;&#26469;&#65292;&#26088;&#22312;&#25351;&#23548;&#20998;&#31867;&#22120;&#20174;&#29420;&#31435;&#29992;&#25143;&#30340;&#35282;&#24230;&#20851;&#27880;&#30456;&#21453;&#26679;&#26412;&#20043;&#38388;&#30340;&#38590;&#20197;&#21306;&#20998;&#30340;&#20851;&#31995;&#12290;&#19982;&#25104;&#23545;&#25351;&#25968;&#25439;&#22833;&#20989;&#25968;&#30340;&#21407;&#22987;&#24418;&#24335;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;PDAOM&#25439;&#22833;&#20989;&#25968;&#19981;&#20165;&#22312;&#31163;&#32447;&#35780;&#20272;&#20013;&#25552;&#39640;&#20102;AUC&#21644;GAUC&#25351;&#26631;&#65292;&#32780;&#19988;&#20943;&#23569;&#20102;&#35757;&#32451;&#30446;&#26631;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
AUC is a common metric for evaluating the performance of a classifier. However, most classifiers are trained with cross entropy, and it does not optimize the AUC metric directly, which leaves a gap between the training and evaluation stage. In this paper, we propose the PDAOM loss, a Personalized and Differentiable AUC Optimization method with Maximum violation, which can be directly applied when training a binary classifier and optimized with gradient-based methods. Specifically, we construct the pairwise exponential loss with difficult pair of positive and negative samples within sub-batches grouped by user ID, aiming to guide the classifier to pay attention to the relation between hard-distinguished pairs of opposite samples from the perspective of independent users. Compared to the origin form of pairwise exponential loss, the proposed PDAOM loss not only improves the AUC and GAUC metrics in the offline evaluation, but also reduces the computation complexity of the training objecti
&lt;/p&gt;</description></item><item><title>Memento&#26159;&#19968;&#20010;Python&#21253;&#65292;&#26088;&#22312;&#21327;&#21161;&#30740;&#31350;&#20154;&#21592;&#39640;&#25928;&#22320;&#31649;&#29702;&#21644;&#25191;&#34892;&#35745;&#31639;&#23494;&#38598;&#22411;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;&#12290;&#23427;&#25552;&#20379;&#20102;&#31616;&#21333;&#26126;&#20102;&#30340;&#37197;&#32622;&#30697;&#38453;&#21644;&#24182;&#21457;&#36816;&#34892;&#23454;&#39564;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.09175</link><description>&lt;p&gt;
Memento: &#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;&#30340;&#36731;&#26494;&#12289;&#39640;&#25928;&#21644;&#21487;&#38752;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Memento: Facilitating Effortless, Efficient, and Reliable ML Experiments. (arXiv:2304.09175v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09175
&lt;/p&gt;
&lt;p&gt;
Memento&#26159;&#19968;&#20010;Python&#21253;&#65292;&#26088;&#22312;&#21327;&#21161;&#30740;&#31350;&#20154;&#21592;&#39640;&#25928;&#22320;&#31649;&#29702;&#21644;&#25191;&#34892;&#35745;&#31639;&#23494;&#38598;&#22411;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;&#12290;&#23427;&#25552;&#20379;&#20102;&#31616;&#21333;&#26126;&#20102;&#30340;&#37197;&#32622;&#30697;&#38453;&#21644;&#24182;&#21457;&#36816;&#34892;&#23454;&#39564;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#32479;&#19968;&#26694;&#26550;&#65292;&#36816;&#34892;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;&#38598;&#38750;&#24120;&#20855;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#12290;&#36825;&#36843;&#20351;&#30740;&#31350;&#20154;&#21592;&#33258;&#24049;&#33457;&#36153;&#26102;&#38388;&#23454;&#29616;&#24517;&#35201;&#30340;&#21151;&#33021;&#65292;&#22914;&#24182;&#34892;&#21270;&#12289;&#32531;&#23384;&#21644;&#26816;&#26597;&#28857;&#65292;&#32780;&#19981;&#26159;&#38598;&#20013;&#31934;&#21147;&#20110;&#20182;&#20204;&#30340;&#39033;&#30446;&#12290;&#20026;&#20102;&#31616;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102; Memento&#65292;&#19968;&#20010;&#26088;&#22312;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#25968;&#25454;&#31185;&#23398;&#23478;&#39640;&#25928;&#31649;&#29702;&#21644;&#25191;&#34892;&#35745;&#31639;&#23494;&#38598;&#22411;&#23454;&#39564;&#30340; Python &#21253;&#12290;Memento &#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#31616;&#21333;&#26126;&#20102;&#30340;&#37197;&#32622;&#30697;&#38453;&#21644;&#33021;&#22815;&#21516;&#26102;&#36816;&#34892;&#22810;&#20010;&#32447;&#31243;&#30340;&#23454;&#39564;&#26469;&#20248;&#21270;&#20219;&#20309;&#23454;&#39564;&#27969;&#31243;&#12290;Memento &#30340;&#28436;&#31034;&#21487;&#22312;&#20197;&#19979;&#32593;&#31449;&#26597;&#30475;&#65306;https://wickerlab.org/publication/memento&#12290;
&lt;/p&gt;
&lt;p&gt;
Running complex sets of machine learning experiments is challenging and time-consuming due to the lack of a unified framework. This leaves researchers forced to spend time implementing necessary features such as parallelization, caching, and checkpointing themselves instead of focussing on their project. To simplify the process, in this paper, we introduce Memento, a Python package that is designed to aid researchers and data scientists in the efficient management and execution of computationally intensive experiments. Memento has the capacity to streamline any experimental pipeline by providing a straightforward configuration matrix and the ability to concurrently run experiments across multiple threads. A demonstration of Memento is available at: https://wickerlab.org/publication/memento.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#27969;&#20197;&#36827;&#34892;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;CF-VAE&#65292;&#21033;&#29992;&#22240;&#26524;&#27969;&#22686;&#24378;&#20102;VAE&#32534;&#30721;&#22120;&#30340;&#20998;&#31163;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#22240;&#26524;&#20998;&#31163;&#24182;&#36827;&#34892;&#24178;&#39044;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09010</link><description>&lt;p&gt;
CF-VAE&#65306;&#22522;&#20110;VAE&#21644;&#22240;&#26524;&#27969;&#30340;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CF-VAE: Causal Disentangled Representation Learning with VAE and Causal Flows. (arXiv:2304.09010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#27969;&#20197;&#36827;&#34892;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;CF-VAE&#65292;&#21033;&#29992;&#22240;&#26524;&#27969;&#22686;&#24378;&#20102;VAE&#32534;&#30721;&#22120;&#30340;&#20998;&#31163;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#22240;&#26524;&#20998;&#31163;&#24182;&#36827;&#34892;&#24178;&#39044;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#26088;&#22312;&#23398;&#20064;&#25968;&#25454;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#20854;&#20013;&#27599;&#20010;&#32500;&#24230;&#23545;&#24212;&#19968;&#20010;&#28508;&#22312;&#30340;&#29983;&#25104;&#22240;&#32032;&#12290;&#30001;&#20110;&#29983;&#25104;&#22240;&#32032;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#22240;&#26524;&#20851;&#31995;&#65292;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#20197;&#23558;&#22240;&#26524;&#32467;&#26500;&#20449;&#24687;&#24341;&#20837;&#27169;&#22411;&#20013;&#30340;&#27969;&#65292;&#31216;&#20026;&#22240;&#26524;&#27969;&#12290;&#22522;&#20110;&#24191;&#27867;&#29992;&#20110;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;CF-VAE&#65292;&#21033;&#29992;&#22240;&#26524;&#27969;&#22686;&#24378;&#20102;VAE&#32534;&#30721;&#22120;&#30340;&#20998;&#31163;&#33021;&#21147;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#24341;&#20837;&#22522;&#20934;&#22240;&#32032;&#30340;&#30417;&#30563;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#20998;&#31163;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CF-VAE&#21487;&#20197;&#23454;&#29616;&#22240;&#26524;&#20998;&#31163;&#24182;&#36827;&#34892;&#24178;&#39044;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning disentangled representations is important in representation learning, aiming to learn a low dimensional representation of data where each dimension corresponds to one underlying generative factor. Due to the possibility of causal relationships between generative factors, causal disentangled representation learning has received widespread attention. In this paper, we first propose a new flows that can incorporate causal structure information into the model, called causal flows. Based on the variational autoencoders(VAE) commonly used in disentangled representation learning, we design a new model, CF-VAE, which enhances the disentanglement ability of the VAE encoder by utilizing the causal flows. By further introducing the supervision of ground-truth factors, we demonstrate the disentanglement identifiability of our model. Experimental results on both synthetic and real datasets show that CF-VAE can achieve causal disentanglement and perform intervention experiments. Moreover, C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#36890;&#20449;&#21463;&#38480;&#12289;&#25910;&#25947;&#36895;&#24230;&#24930;&#21644;&#36164;&#28304;&#26377;&#38480;&#31561;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24180;&#40836;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#26696;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#27599;&#36718;&#32852;&#37030;&#23398;&#20064;&#30340;&#24635;&#26102;&#38388;&#28040;&#32791;&#65292;&#20174;&#32780;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.08996</link><description>&lt;p&gt;
&#22522;&#20110;&#24180;&#40836;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;NOMA&#32593;&#32476;&#19979;&#36890;&#20449;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Joint Age-based Client Selection and Resource Allocation for Communication-Efficient Federated Learning over NOMA Networks. (arXiv:2304.08996v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#36890;&#20449;&#21463;&#38480;&#12289;&#25910;&#25947;&#36895;&#24230;&#24930;&#21644;&#36164;&#28304;&#26377;&#38480;&#31561;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24180;&#40836;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#26696;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#27599;&#36718;&#32852;&#37030;&#23398;&#20064;&#30340;&#24635;&#26102;&#38388;&#28040;&#32791;&#65292;&#20174;&#32780;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#21487;&#20197;&#20351;&#24471;&#23458;&#25143;&#31471;&#20351;&#29992;&#26412;&#22320;&#25968;&#25454;&#21327;&#21516;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24403;FL&#37096;&#32626;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#26102;&#65292;&#30001;&#20110;&#36890;&#20449;&#38142;&#36335;&#24046;&#21644;&#25910;&#25947;&#36895;&#24230;&#24930;&#65292;FL&#30340;&#24615;&#33021;&#24120;&#24120;&#21463;&#21040;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#26080;&#32447;&#36164;&#28304;&#21463;&#38480;&#65292;&#20934;&#30830;&#36873;&#21462;&#23458;&#25143;&#31471;&#21644;&#25511;&#21046;&#36164;&#28304;&#20998;&#37197;&#23545;&#20110;&#25552;&#39640;FL&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#23458;&#25143;&#31471;&#36873;&#25321;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#32852;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#22312;&#38750;&#27491;&#20132;&#22810;&#22336;&#65288;NOMA&#65289;&#26080;&#32447;&#32593;&#32476;&#19978;&#27599;&#36718;FL&#30340;&#24635;&#26102;&#38388;&#28040;&#32791;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#25910;&#21040;&#30340;&#26412;&#22320;FL&#27169;&#22411;&#30340;&#26032;&#26087;&#31243;&#24230;&#26469;&#35774;&#35745;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#24180;&#40836;&#26356;&#26032;&#65288;AoU&#65289;&#25351;&#26631;&#33719;&#24471;&#36164;&#28304;&#20998;&#37197;&#30340;&#38381;&#21512;&#24335;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a promising paradigm that enables distributed clients to collaboratively train a shared global model while keeping the training data locally. However, the performance of FL is often limited by poor communication links and slow convergence when FL is deployed over wireless networks. Besides, due to the limited radio resources, it is crucial to select clients and control resource allocation accurately for improved FL performance. Motivated by these challenges, a joint optimization problem of client selection and resource allocation is formulated in this paper, aiming to minimize the total time consumption of each round in FL over non-orthogonal multiple access (NOMA) enabled wireless network. Specifically, based on a metric termed the age of update (AoU), we first propose a novel client selection scheme by accounting for the staleness of the received local FL models. After that, the closed-form solutions of resource allocation are obtained by monotonicity analy
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#38544;&#24335;&#34920;&#31034;&#26500;&#24314;&#20102;&#22330;&#26223;&#30456;&#20851;&#20808;&#39564;&#65292;&#20174;&#32780;&#22312;&#19981;&#35268;&#21017;&#29615;&#22659;&#20013;&#21033;&#29992;&#22522;&#20110;&#27169;&#25311;&#30340;&#39640;&#25928;&#36125;&#21494;&#26031;&#25512;&#29702;&#31639;&#27861;&#25104;&#21151;&#35782;&#21035;&#25235;&#21462;&#23039;&#24577;&#12290;</title><link>http://arxiv.org/abs/2304.08805</link><description>&lt;p&gt;
&#22522;&#20110;&#40654;&#26364;&#20960;&#20309;&#19982;&#38544;&#24335;&#34920;&#31034;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#26426;&#22120;&#20154;&#25235;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Implicit representation priors meet Riemannian geometry for Bayesian robotic grasping. (arXiv:2304.08805v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08805
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#38544;&#24335;&#34920;&#31034;&#26500;&#24314;&#20102;&#22330;&#26223;&#30456;&#20851;&#20808;&#39564;&#65292;&#20174;&#32780;&#22312;&#19981;&#35268;&#21017;&#29615;&#22659;&#20013;&#21033;&#29992;&#22522;&#20110;&#27169;&#25311;&#30340;&#39640;&#25928;&#36125;&#21494;&#26031;&#25512;&#29702;&#31639;&#27861;&#25104;&#21151;&#35782;&#21035;&#25235;&#21462;&#23039;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#22122;&#22768;&#29615;&#22659;&#19979;&#36827;&#34892;&#26426;&#22120;&#20154;&#25235;&#21462;&#38754;&#20020;&#30528;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#32570;&#20047;&#22330;&#26223;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#30001;&#20110;&#20004;&#20010;&#21407;&#22240;&#65292;&#29992;&#36125;&#21494;&#26031;&#25512;&#29702;&#26469;&#35782;&#21035;&#33391;&#22909;&#30340;&#25235;&#21462;&#23039;&#24577;&#21464;&#24471;&#22256;&#38590;&#65306;i&#65289;&#20174;&#26080;&#20449;&#24687;&#30340;&#20808;&#39564;&#29983;&#25104;&#25968;&#25454;&#25928;&#29575;&#20302;&#19979;&#65292;ii&#65289;&#21518;&#39564;&#36890;&#24120;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#23450;&#20041;&#19968;&#20010;&#22797;&#26434;&#20998;&#24067;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#38544;&#24335;&#34920;&#31034;&#26500;&#24314;&#22330;&#26223;&#30456;&#20851;&#24615;&#20808;&#39564;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#20351;&#24471;&#22312;&#19981;&#35268;&#21017;&#29615;&#22659;&#20013;&#24212;&#29992;&#22522;&#20110;&#27169;&#25311;&#30340;&#39640;&#25928;&#36125;&#21494;&#26031;&#25512;&#29702;&#31639;&#27861;&#26469;&#30830;&#23450;&#25104;&#21151;&#30340;&#25235;&#21462;&#23039;&#24577;&#25104;&#20026;&#21487;&#33021;&#12290;&#27169;&#25311;&#21644;&#29289;&#29702;&#22522;&#20934;&#27979;&#35797;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#39640;&#25104;&#21151;&#29575;&#21644;&#33391;&#22909;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic grasping in highly noisy environments presents complex challenges, especially with limited prior knowledge about the scene. In particular, identifying good grasping poses with Bayesian inference becomes difficult due to two reasons: i) generating data from uninformative priors proves to be inefficient, and ii) the posterior often entails a complex distribution defined on a Riemannian manifold. In this study, we explore the use of implicit representations to construct scene-dependent priors, thereby enabling the application of efficient simulation-based Bayesian inference algorithms for determining successful grasp poses in unstructured environments. Results from both simulation and physical benchmarks showcase the high success rate and promising potential of this approach.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; EEGSN &#30340;&#22270;&#24418;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26550;&#26500;&#65292;&#38754;&#21521;&#22810;&#36890;&#36947; EEG &#20998;&#31867;&#20219;&#21153;&#65292;&#22312;&#23398;&#20064;&#20998;&#24067;&#24335; EEG &#20256;&#24863;&#22120;&#20013;&#30340;&#21160;&#24577;&#20851;&#31995;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#23558;&#25512;&#26029;&#35745;&#31639;&#22797;&#26434;&#24230;&#38477;&#20302;&#20102;20&#20493;&#65292;&#20026;&#20302;&#24310;&#36831;&#21644;&#21151;&#32791;&#25928;&#29575;&#30340;&#33041;&#35745;&#31639;&#26426;&#25509;&#21475;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2304.07655</link><description>&lt;p&gt;
EEG SN&#65306;&#38754;&#21521; EEG &#30340;&#22270;&#24418;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#20302;&#24310;&#36831;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
EEGSN: Towards Efficient Low-latency Decoding of EEG with Graph Spiking Neural Networks. (arXiv:2304.07655v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; EEGSN &#30340;&#22270;&#24418;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26550;&#26500;&#65292;&#38754;&#21521;&#22810;&#36890;&#36947; EEG &#20998;&#31867;&#20219;&#21153;&#65292;&#22312;&#23398;&#20064;&#20998;&#24067;&#24335; EEG &#20256;&#24863;&#22120;&#20013;&#30340;&#21160;&#24577;&#20851;&#31995;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#23558;&#25512;&#26029;&#35745;&#31639;&#22797;&#26434;&#24230;&#38477;&#20302;&#20102;20&#20493;&#65292;&#20026;&#20302;&#24310;&#36831;&#21644;&#21151;&#32791;&#25928;&#29575;&#30340;&#33041;&#35745;&#31639;&#26426;&#25509;&#21475;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#30340;&#35757;&#32451;&#20381;&#36182;&#20110;&#24402;&#32435;&#20559;&#24046;&#65292;&#36825;&#24182;&#19981;&#19968;&#23450;&#36866;&#29992;&#20110;&#22810;&#20010;&#38656;&#35201;&#20302;&#24310;&#36831;&#21644;&#21151;&#32791;&#25928;&#29575;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290; &#22522;&#20110;&#30456;&#20851;&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#25512;&#26029;&#22823;&#33041;&#34892;&#20026;&#23601;&#26159;&#19968;&#20010;&#36825;&#26679;&#30340;&#20363;&#23376;&#65292;&#23398;&#20064;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#20250;&#20005;&#37325;&#24433;&#21709;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#25928;&#29575;&#12290;&#30446;&#21069;&#65292;SNN&#20165;&#20165;&#20381;&#38752;&#19968;&#33324;&#24402;&#32435;&#20559;&#24046;&#26469;&#27169;&#25311;&#19981;&#21516;&#25968;&#25454;&#27969;&#20043;&#38388;&#30340;&#21160;&#24577;&#20851;&#31995;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#36890;&#36947; EEG &#20998;&#31867;&#30340;&#22270;&#24418;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;EEGSN&#65289;&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#20998;&#24067;&#22312; EEG &#20256;&#24863;&#22120;&#20013;&#30340;&#21160;&#24577;&#20851;&#31995;&#20449;&#24687;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25512;&#26029;&#35745;&#31639;&#22797;&#26434;&#24230;&#38477;&#20302;&#20102;20&#20493;&#65292;&#21516;&#26102;&#22312;&#36816;&#21160;&#25191;&#34892;&#20998;&#31867;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#21487;&#35299;&#37322;&#21644;&#39640;&#25928;&#35757;&#32451; EEG &#25968;&#25454;&#30340;&#22270;&#24418;SNN&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#21644;&#21151;&#32791;&#25928;&#29575;&#30340;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
A vast majority of spiking neural networks (SNNs) are trained based on inductive biases that are not necessarily a good fit for several critical tasks that require low-latency and power efficiency. Inferring brain behavior based on the associated electroenchephalography (EEG) signals is an example of how networks training and inference efficiency can be heavily impacted by learning spatio-temporal dependencies. Up to now, SNNs rely solely on general inductive biases to model the dynamic relations between different data streams. Here, we propose a graph spiking neural network architecture for multi-channel EEG classification (EEGSN) that learns the dynamic relational information present in the distributed EEG sensors. Our method reduced the inference computational complexity by $\times 20$ compared to the state-of-the-art SNNs, while achieved comparable accuracy on motor execution classification tasks. Overall, our work provides a framework for interpretable and efficient training of gr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#22238;&#24402;&#30340;&#23545;&#25239;&#35757;&#32451;&#21644;&#24102;&#22122;&#22768;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#21457;&#29616;&#22914;&#26524;&#27809;&#26377;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#20294;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#29616;&#35937;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06326</link><description>&lt;p&gt;
&#29702;&#35299;&#26680;&#22238;&#24402;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#36807;&#25311;&#21512;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Understanding Overfitting in Adversarial Training in Kernel Regression. (arXiv:2304.06326v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#22238;&#24402;&#30340;&#23545;&#25239;&#35757;&#32451;&#21644;&#24102;&#22122;&#22768;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#21457;&#29616;&#22914;&#26524;&#27809;&#26377;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#20294;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#29616;&#35937;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#21644;&#24102;&#22122;&#22768;&#30340;&#25968;&#25454;&#22686;&#24378;&#26159;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20877;&#29983;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#20013;&#27491;&#21017;&#21270;&#22238;&#24402;&#30340;&#23545;&#25239;&#35757;&#32451;&#21644;&#24102;&#22122;&#22768;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;&#24403;&#25915;&#20987;&#21644;&#22122;&#22768;&#22823;&#23567;&#20197;&#21450;&#27491;&#21017;&#21270;&#21442;&#25968;&#36235;&#21521;&#20110;&#38646;&#26102;&#65292;&#24314;&#31435;&#20102;&#36825;&#20123;&#25216;&#26415;&#30340;&#26497;&#38480;&#20844;&#24335;&#12290;&#26681;&#25454;&#35813;&#26497;&#38480;&#20844;&#24335;&#65292;&#20998;&#26512;&#20102;&#29305;&#23450;&#24773;&#20917;&#24182;&#35777;&#26126;&#20102;&#65292;&#22914;&#26524;&#27809;&#26377;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#33021;&#20855;&#26377;&#22823;&#20110;&#26631;&#20934;&#26680;&#22238;&#24402;&#30340;&#24191;&#20041;&#35823;&#24046;&#21644;Lipschitz&#24120;&#25968;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#26631;&#20934;&#26680;&#22238;&#24402;&#65292;&#36798;&#21040;&#26356;&#23567;&#30340;&#24191;&#20041;&#35823;&#24046;&#21644;Lipschitz&#24120;&#25968;&#12290;&#36825;&#20123;&#21457;&#29616;&#25903;&#25345;&#23545;&#25239;&#35757;&#32451;&#21487;&#33021;&#23548;&#33268;&#36807;&#25311;&#21512;&#30340;&#32463;&#39564;&#35266;&#23519;&#65292;&#20197;&#21450;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#33021;&#22815;&#32531;&#35299;&#36825;&#31181;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training and data augmentation with noise are widely adopted techniques to enhance the performance of neural networks. This paper investigates adversarial training and data augmentation with noise in the context of regularized regression in a reproducing kernel Hilbert space (RKHS). We establish the limiting formula for these techniques as the attack and noise size, as well as the regularization parameter, tend to zero. Based on this limiting formula, we analyze specific scenarios and demonstrate that, without appropriate regularization, these two methods may have larger generalization error and Lipschitz constant than standard kernel regression. However, by selecting the appropriate regularization parameter, these two methods can outperform standard kernel regression and achieve smaller generalization error and Lipschitz constant. These findings support the empirical observations that adversarial training can lead to overfitting, and appropriate regularization methods, suc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.05055</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Deep Graph Representation Learning. (arXiv:2304.05055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23558;&#39640;&#32500;&#31232;&#30095;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#26377;&#25928;&#22320;&#32534;&#30721;&#25104;&#20302;&#32500;&#23494;&#38598;&#21521;&#37327;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#39046;&#22495;&#37117;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#20256;&#32479;&#22270;&#23884;&#20837;&#26041;&#27861;&#36981;&#24490;&#36825;&#26679;&#19968;&#31181;&#22522;&#26412;&#24605;&#24819;&#65292;&#21363;&#22270;&#20013;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#30340;&#23884;&#20837;&#30690;&#37327;&#20173;&#28982;&#33021;&#22815;&#20445;&#25345;&#30456;&#23545;&#25509;&#36817;&#30340;&#36317;&#31163;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;&#65288;i&#65289;&#20256;&#32479;&#26041;&#27861;&#30340;&#27169;&#22411;&#23481;&#37327;&#21463;&#38480;&#65292;&#38480;&#21046;&#20102;&#23398;&#20064;&#24615;&#33021;; &#65288;ii&#65289;&#29616;&#26377;&#25216;&#26415;&#36890;&#24120;&#20381;&#36182;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#26080;&#27861;&#19982;&#26368;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#30456;&#32467;&#21512;&#65307;&#65288;iii&#65289;&#34920;&#31034;&#23398;&#20064;&#21644;&#19979;&#28216;&#20219;&#21153;&#30456;&#20114;&#20381;&#23384;&#65292;&#24212;&#20849;&#21516;&#21152;&#24378;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#26174;&#30528;&#25104;&#21151;&#65292;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph representation learning aims to effectively encode high-dimensional sparse graph-structured data into low-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields, including machine learning and data mining. Classic graph embedding methods follow the basic idea that the embedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby preserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i) traditional methods have limited model capacity which limits the learning performance; (ii) existing techniques typically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii) representation learning and downstream tasks are dependent on each other which should be jointly enhanced. With the remarkable success of deep learning, deep graph representation learning has shown great potential and advantages 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#28608;&#27963;&#20989;&#25968;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;MCMC&#37319;&#26679;&#30340;&#27169;&#22411;&#65292;&#33021;&#36890;&#36807;&#26377;&#25928;&#30340;&#37319;&#26679;&#21644;&#20840;&#23616;&#20248;&#21270;&#26469;&#35299;&#20915;&#36807;&#25311;&#21512;&#24182;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.04455</link><description>&lt;p&gt;
&#21487;&#35757;&#32451;&#28608;&#27963;&#20989;&#25968;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization for sparse neural networks with trainable activation functions. (arXiv:2304.04455v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#28608;&#27963;&#20989;&#25968;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;MCMC&#37319;&#26679;&#30340;&#27169;&#22411;&#65292;&#33021;&#36890;&#36807;&#26377;&#25928;&#30340;&#37319;&#26679;&#21644;&#20840;&#23616;&#20248;&#21270;&#26469;&#35299;&#20915;&#36807;&#25311;&#21512;&#24182;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#29486;&#20013;&#65292;&#20154;&#20204;&#23545;&#24320;&#21457;&#33021;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#28608;&#27963;&#20989;&#25968;&#38750;&#24120;&#24863;&#20852;&#36259;&#12290;&#26368;&#36817;&#65292;&#31185;&#23398;&#30028;&#25552;&#20986;&#20102;&#21487;&#20197;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#22240;&#20026;&#23427;&#20204;&#20284;&#20046;&#21487;&#20197;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#38656;&#35201;&#20272;&#35745;&#20854;&#21442;&#25968;&#12290;&#24320;&#21457;&#20102;&#19968;&#20010;&#23436;&#20840;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#33258;&#21160;&#20174;&#23398;&#20064;&#25968;&#25454;&#20013;&#20272;&#35745;&#20986;&#27169;&#22411;&#26435;&#37325;&#21644;&#28608;&#27963;&#20989;&#25968;&#21442;&#25968;&#12290;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;MCMC&#30340;&#20248;&#21270;&#26041;&#26696;&#26469;&#26500;&#24314;&#25512;&#29702;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#26377;&#25928;&#30340;&#37319;&#26679;&#26041;&#26696;&#26469;&#20445;&#35777;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#22823;&#20540;&#65292;&#20174;&#32780;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#24182;&#25913;&#21892;&#25910;&#25947;&#26102;&#38388;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;CNN&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#12290;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the literature on deep neural networks, there is considerable interest in developing activation functions that can enhance neural network performance. In recent years, there has been renewed scientific interest in proposing activation functions that can be trained throughout the learning process, as they appear to improve network performance, especially by reducing overfitting. In this paper, we propose a trainable activation function whose parameters need to be estimated. A fully Bayesian model is developed to automatically estimate from the learning data both the model weights and activation function parameters. An MCMC-based optimization scheme is developed to build the inference. The proposed method aims to solve the aforementioned problems and improve convergence time by using an efficient sampling scheme that guarantees convergence to the global maximum. The proposed scheme is tested on three datasets with three different CNNs. Promising results demonstrate the usefulness of o
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#65292;&#23481;&#26131;&#23558;&#20854;&#20869;&#23481;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#21516;&#26102;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#20250;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;</title><link>http://arxiv.org/abs/2304.02819</link><description>&lt;p&gt;
GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#30340;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT detectors are biased against non-native English writers. (arXiv:2304.02819v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02819
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#65292;&#23481;&#26131;&#23558;&#20854;&#20869;&#23481;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#21516;&#26102;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#20250;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#25512;&#24191;&#24102;&#26469;&#20102;&#25968;&#23383;&#36890;&#20449;&#26041;&#38754;&#30340;&#23454;&#36136;&#24615;&#36827;&#23637;&#65292;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;AI&#29983;&#25104;&#20869;&#23481;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26816;&#27979;&#26041;&#27861;&#26469;&#21306;&#20998;AI&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#20294;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#33521;&#35821;&#27597;&#35821;&#21644;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#30340;&#20889;&#20316;&#26679;&#26412;&#35780;&#20272;&#20102;&#20960;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;GPT&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#25345;&#32493;&#23558;&#38750;&#33521;&#35821;&#27597;&#35821;&#30340;&#20889;&#20316;&#26679;&#26412;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#32780;&#21407;&#29983;&#20889;&#20316;&#26679;&#26412;&#21017;&#33021;&#22815;&#34987;&#20934;&#30830;&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#19981;&#20165;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#26080;&#24847;&#20013;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#21628;&#21505;&#36827;&#34892;&#26356;&#24191;&#27867;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid adoption of generative language models has brought about substantial advancements in digital communication, while simultaneously raising concerns regarding the potential misuse of AI-generated content. Although numerous detection methods have been proposed to differentiate between AI and human-generated content, the fairness and robustness of these detectors remain underexplored. In this study, we evaluate the performance of several widely-used GPT detectors using writing samples from native and non-native English writers. Our findings reveal that these detectors consistently misclassify non-native English writing samples as AI-generated, whereas native writing samples are accurately identified. Furthermore, we demonstrate that simple prompting strategies can not only mitigate this bias but also effectively bypass GPT detectors, suggesting that GPT detectors may unintentionally penalize writers with constrained linguistic expressions. Our results call for a broader conversati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#26041;&#27861;&#65292;&#21363;REPORT&#65292;&#33021;&#22815;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#12290;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.00215</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#24402;&#32435;&#20851;&#31995;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers. (arXiv:2304.00215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#26041;&#27861;&#65292;&#21363;REPORT&#65292;&#33021;&#22815;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#12290;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#20851;&#31995;&#39044;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#29616;&#26377;&#30340;&#23884;&#20837;&#24335;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#36716;&#23548;&#35774;&#32622;&#65292;&#32570;&#20047;&#24402;&#32435;&#33021;&#21147;&#65292;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#23454;&#20307;&#19978;&#36827;&#34892;&#25512;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;&#30340;&#20998;&#23618;Transformer&#26694;&#26550;&#65292;&#21363;REPORT&#65292;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;REPORT&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#65292;&#29978;&#33267;&#22312;&#20004;&#20010;&#23436;&#20840;&#24402;&#32435;&#30340;&#25968;&#25454;&#38598;&#30340;&#20843;&#20010;&#29256;&#26412;&#23376;&#38598;&#19978;&#20063;&#26159;&#22914;&#27492;&#12290;&#27492;&#22806;&#65292;REPORT&#33021;&#22815;&#23558;&#25512;&#29702;&#25512;&#24191;&#21040;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#27809;&#26377;&#20844;&#20849;&#23454;&#20307;&#30340;&#26032;&#23454;&#20307;&#19978;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation prediction on knowledge graphs (KGs) is a key research topic. Dominant embedding-based methods mainly focus on the transductive setting and lack the inductive ability to generalize to new entities for inference. Existing methods for inductive reasoning mostly mine the connections between entities, i.e., relational paths, without considering the nature of head and tail entities contained in the relational context. This paper proposes a novel method that captures both connections between entities and the intrinsic nature of entities, by simultaneously aggregating RElational Paths and cOntext with a unified hieRarchical Transformer framework, namely REPORT. REPORT relies solely on relation semantics and can naturally generalize to the fully-inductive setting, where KGs for training and inference have no common entities. In the experiments, REPORT performs consistently better than all baselines on almost all the eight version subsets of two fully-inductive datasets. Moreover. REPO
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#23545;&#20004;&#31181;GAN&#26550;&#26500;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#20197;GSWGAN&#34920;&#29616;&#26368;&#20339;&#65292;&#21487;&#20197;&#31169;&#23494;&#22320;&#29983;&#25104;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#20844;&#20849;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.15916</link><description>&lt;p&gt;
&#20174;&#31169;&#26377;&#21040;&#20844;&#26377;&#65306;&#22312;&#31169;&#26377;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#24773;&#22659;&#19979;&#23545;GAN&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
From Private to Public: Benchmarking GANs in the Context of Private Time Series Classification. (arXiv:2303.15916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#23545;&#20004;&#31181;GAN&#26550;&#26500;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#20197;GSWGAN&#34920;&#29616;&#26368;&#20339;&#65292;&#21487;&#20197;&#31169;&#23494;&#22320;&#29983;&#25104;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#20844;&#20849;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#20010;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#37117;&#24456;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#21040;&#31169;&#20154;&#25968;&#25454;&#26102;&#65292;&#20960;&#20010;&#38480;&#21046;&#20351;&#24471;&#38590;&#20197;&#22312;&#36825;&#20123;&#24212;&#29992;&#39046;&#22495;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23581;&#35797;&#31169;&#23494;&#22320;&#29983;&#25104;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#22312;&#20998;&#31867;&#22120;&#20043;&#19978;&#30452;&#25509;&#24212;&#29992;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#12290;&#35299;&#20915;&#26041;&#26696;&#26159;&#20197;&#19968;&#31181;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#24335;&#20174;&#31169;&#26377;&#25968;&#25454;&#21019;&#24314;&#20844;&#20849;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#38024;&#23545;&#31169;&#26377;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#24773;&#22659;&#65292;&#35780;&#20272;&#20102;&#20004;&#31181;&#38750;&#24120;&#31361;&#20986;&#30340;&#22522;&#20110;GAN&#30340;&#26550;&#26500;&#12290;&#19982;&#20808;&#21069;&#20027;&#35201;&#23616;&#38480;&#20110;&#22270;&#20687;&#39046;&#22495;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#33539;&#22260;&#26159;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#23588;&#20854;&#26159;GSWGAN&#22312;&#22810;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;DPWGAN&#12290;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;GSWGAN&#22312;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#30340;&#24773;&#22659;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has proven to be successful in various domains and for different tasks. However, when it comes to private data several restrictions are making it difficult to use deep learning approaches in these application fields. Recent approaches try to generate data privately instead of applying a privacy-preserving mechanism directly, on top of the classifier. The solution is to create public data from private data in a manner that preserves the privacy of the data. In this work, two very prominent GAN-based architectures were evaluated in the context of private time series classification. In contrast to previous work, mostly limited to the image domain, the scope of this benchmark was the time series domain. The experiments show that especially GSWGAN performs well across a variety of public datasets outperforming the competitor DPWGAN. An analysis of the generated datasets further validates the superiority of GSWGAN in the context of time series generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26410;&#30693;&#21957;&#25506;&#22120;(UnSniffer)&#65292;&#29992;&#20110;&#21516;&#26102;&#23547;&#25214;&#26410;&#30693;&#21644;&#24050;&#30693;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#29289;&#20307;&#32622;&#20449;&#24230;(GOC)&#20998;&#25968;&#21644;&#36127;&#33021;&#37327;&#25233;&#21046;&#25439;&#22833;&#26469;&#25552;&#39640;&#26410;&#30693;&#23545;&#35937;&#22312;&#32972;&#26223;&#20013;&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#38590;&#20197;&#33719;&#24471;&#27599;&#20010;&#26410;&#30693;&#30446;&#26631;&#26368;&#20339;&#26694;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13769</link><description>&lt;p&gt;
&#26410;&#30693;&#21957;&#25506;&#22120;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#65306;&#19981;&#35201;&#23545;&#26410;&#30693;&#23545;&#35937;&#35270;&#32780;&#19981;&#35265;
&lt;/p&gt;
&lt;p&gt;
Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects. (arXiv:2303.13769v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26410;&#30693;&#21957;&#25506;&#22120;(UnSniffer)&#65292;&#29992;&#20110;&#21516;&#26102;&#23547;&#25214;&#26410;&#30693;&#21644;&#24050;&#30693;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#29289;&#20307;&#32622;&#20449;&#24230;(GOC)&#20998;&#25968;&#21644;&#36127;&#33021;&#37327;&#25233;&#21046;&#25439;&#22833;&#26469;&#25552;&#39640;&#26410;&#30693;&#23545;&#35937;&#22312;&#32972;&#26223;&#20013;&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#38590;&#20197;&#33719;&#24471;&#27599;&#20010;&#26410;&#30693;&#30446;&#26631;&#26368;&#20339;&#26694;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#25552;&#20986;&#30340;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#21644;&#24320;&#25918;&#38598;&#26816;&#27979;&#22312;&#23547;&#25214;&#20174;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#24182;&#23558;&#20854;&#19982;&#24050;&#30693;&#31867;&#21035;&#21306;&#20998;&#24320;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#23545;&#20174;&#24050;&#30693;&#31867;&#21035;&#21521;&#26410;&#30693;&#31867;&#21035;&#30340;&#30693;&#35782;&#20256;&#36882;&#30340;&#30740;&#31350;&#38656;&#35201;&#26356;&#28145;&#20837;&#65292;&#20174;&#32780;&#23548;&#33268;&#25506;&#27979;&#38544;&#34255;&#22312;&#32972;&#26223;&#20013;&#30340;&#26410;&#30693;&#29289;&#20307;&#30340;&#33021;&#21147;&#19981;&#36275;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#30693;&#21957;&#25506;&#22120;(UnSniffer)&#26469;&#23547;&#25214;&#26410;&#30693;&#21644;&#24050;&#30693;&#30340;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#24191;&#20041;&#29289;&#20307;&#32622;&#20449;&#24230;(GOC)&#20998;&#25968;&#65292;&#20165;&#20351;&#29992;&#24050;&#30693;&#31867;&#21035;&#26679;&#26412;&#36827;&#34892;&#30417;&#30563;&#21644;&#36991;&#20813;&#22312;&#32972;&#26223;&#20013;&#19981;&#36866;&#24403;&#22320;&#21387;&#21046;&#26410;&#30693;&#29289;&#20307;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20174;&#24050;&#30693;&#29289;&#20307;&#23398;&#20064;&#21040;&#30340;&#36825;&#31181;&#32622;&#20449;&#24230;&#20998;&#25968;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#30693;&#29289;&#20307;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36127;&#33021;&#37327;&#25233;&#21046;&#25439;&#22833;&#26469;&#36827;&#19968;&#27493;&#38480;&#21046;&#32972;&#26223;&#20013;&#38750;&#29289;&#20307;&#26679;&#26412;&#12290;&#25509;&#19979;&#26469;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#30001;&#20110;&#32570;&#20047;&#23427;&#20204;&#22312;&#35757;&#32451;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#38590;&#20197;&#33719;&#24471;&#27599;&#20010;&#26410;&#30693;&#30446;&#26631;&#30340;&#26368;&#20339;&#26694;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;
&lt;/p&gt;
&lt;p&gt;
The recently proposed open-world object and open-set detection achieve a breakthrough in finding never-seen-before objects and distinguishing them from class-known ones. However, their studies on knowledge transfer from known classes to unknown ones need to be deeper, leading to the scanty capability for detecting unknowns hidden in the background. In this paper, we propose the unknown sniffer (UnSniffer) to find both unknown and known objects. Firstly, the generalized object confidence (GOC) score is introduced, which only uses class-known samples for supervision and avoids improper suppression of unknowns in the background. Significantly, such confidence score learned from class-known objects can be generalized to unknown ones. Additionally, we propose a negative energy suppression loss to further limit the non-object samples in the background. Next, the best box of each unknown is hard to obtain during inference due to lacking their semantic information in training. To solve this is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#25512;&#29702;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#38382;&#39064;&#65292;&#21457;&#29616;CLIP&#23384;&#22312;&#35823;&#26657;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#29256;&#30340;&#28201;&#24230;&#32553;&#25918;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#27599;&#20010;&#29305;&#23450;&#30340;CLIP&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.12748</link><description>&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#38646;&#26679;&#26412;&#25512;&#29702;&#20013;&#30340;&#26657;&#20934;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enabling Calibration In The Zero-Shot Inference of Large Vision-Language Models. (arXiv:2303.12748v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#25512;&#29702;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#38382;&#39064;&#65292;&#21457;&#29616;CLIP&#23384;&#22312;&#35823;&#26657;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#29256;&#30340;&#28201;&#24230;&#32553;&#25918;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#27599;&#20010;&#29305;&#23450;&#30340;CLIP&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26657;&#20934;&#23545;&#20110;&#20445;&#35777;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#20351;&#29992;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#27492;&#22312;&#30417;&#30563;&#20998;&#31867;&#27169;&#22411;&#20013;&#23545;&#20854;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#38477;&#20302;&#35823;&#26657;&#20934;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#36827;&#34892;&#38646;&#26679;&#26412;&#25512;&#29702;&#26102;&#30340;&#26657;&#20934;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#20363;&#22914;CLIP&#12290;&#26412;&#30740;&#31350;&#34913;&#37327;&#20102;&#36328;&#30456;&#20851;&#21464;&#37327;&#65288;&#22914;&#25552;&#31034;&#65292;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#65289;&#30340;&#26657;&#20934;&#24773;&#20917;&#65292;&#24182;&#21457;&#29616;CLIP&#30340;&#38646;&#26679;&#26412;&#25512;&#29702;&#23384;&#22312;&#35823;&#26657;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#29256;&#30340;&#28201;&#24230;&#32553;&#25918;&#26041;&#27861;&#65292;&#19982;CLIP&#20316;&#20026;&#38646;&#26679;&#26412;&#25512;&#29702;&#27169;&#22411;&#30340;&#24120;&#35265;&#29992;&#20363;&#30456;&#19968;&#33268;&#65292;&#24182;&#23637;&#31034;&#20986;&#21333;&#20010;&#23398;&#20064;&#30340;&#28201;&#24230;&#20540;&#21487;&#20197;&#24191;&#27867;&#36866;&#29992;&#20110;&#27599;&#20010;&#29305;&#23450;&#30340;CLIP&#27169;&#22411;&#65288;&#30001;&#36873;&#23450;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#23450;&#20041;&#65289;&#65292;&#36328;&#19981;&#21516;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#21644;&#25552;&#31034;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calibration of deep learning models is crucial to their trustworthiness and safe usage, and as such, has been extensively studied in supervised classification models, with methods crafted to decrease miscalibration. However, there has yet to be a comprehensive study of the calibration of vision-language models that are used for zero-shot inference, like CLIP. We measure calibration across relevant variables like prompt, dataset, and architecture, and find that zero-shot inference with CLIP is miscalibrated. Furthermore, we propose a modified version of temperature scaling that is aligned with the common use cases of CLIP as a zero-shot inference model, and show that a single learned temperature generalizes for each specific CLIP model (defined by a chosen pre-training dataset and architecture) across inference dataset and prompt choice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#30340;&#25968;&#25454;&#65292;&#25506;&#31350;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#65307;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22312;&#36890;&#29992;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.07925</link><description>&lt;p&gt;
&#36890;&#36807; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#26696;&#20363;&#65292;&#29702;&#35299;&#26102;&#38388;&#34920;&#26684;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament. (arXiv:2303.07925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#30340;&#25968;&#25454;&#65292;&#25506;&#31350;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#65307;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22312;&#36890;&#29992;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#20351;&#29992;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;&#21033;&#29992;&#20174; Numerai &#25968;&#25454;&#31454;&#36187;&#21019;&#24314;&#30340;&#29305;&#24449;&#30446;&#26631;&#20132;&#21449;&#30456;&#20851;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#39044;&#27979;&#20250;&#25910;&#25947;&#21040;&#21487;&#30001;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#21051;&#30011;&#30340;&#30456;&#21516;&#24179;&#34913;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#19981;&#21516;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#21464;&#25442;&#65292;&#38543;&#21518;&#37319;&#29992;&#23725;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#12290;&#19982;&#19968;&#20123;&#24120;&#29992;&#30340;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914; LSTM &#21644; transformer&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#21152;&#40065;&#26834;&#65288;&#22312;&#19981;&#21516;&#30340;&#38543;&#26426;&#31181;&#23376;&#19979;&#20855;&#26377;&#36739;&#20302;&#30340;&#27169;&#22411;&#26041;&#24046;&#65292;&#19988;&#23545;&#26550;&#26500;&#30340;&#36873;&#25321;&#19981;&#22826;&#25935;&#24863;&#65289;&#65292;&#24182;&#19988;&#26356;&#26377;&#25928;&#29575;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#21478;&#19968;&#20010;&#20248;&#21183;&#22312;&#20110;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#65292;&#22240;&#20026;&#27809;&#26377;&#24517;&#35201;&#20351;&#29992;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the use of different feature engineering and dimensionality reduction methods in multi-variate time-series modelling. Using a feature-target cross correlation time series dataset created from Numerai tournament, we demonstrate under over-parameterised regime, both the performance and predictions from different feature engineering methods converge to the same equilibrium, which can be characterised by the reproducing kernel Hilbert space. We suggest a new Ensemble method, which combines different random non-linear transforms followed by ridge regression for modelling high dimensional time-series. Compared to some commonly used deep learning models for sequence modelling, such as LSTM and transformers, our method is more robust (lower model variance over different random seeds and less sensitive to the choice of architecture) and more efficient. An additional advantage of our method is model simplicity as there is no need to use sophisticated deep learning frame
&lt;/p&gt;</description></item><item><title>TSMixer&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#65292;&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#33021;&#22815;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.06053</link><description>&lt;p&gt;
TSMixer&#65306;&#19968;&#31181;&#20840;MLP&#26550;&#26500;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TSMixer: An all-MLP Architecture for Time Series Forecasting. (arXiv:2303.06053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06053
&lt;/p&gt;
&lt;p&gt;
TSMixer&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#65292;&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#33021;&#22815;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#22810;&#21464;&#37327;&#19988;&#20855;&#26377;&#22797;&#26434;&#30340;&#21160;&#24577;&#12290;&#20026;&#20102;&#25429;&#33719;&#36825;&#31181;&#22797;&#26434;&#24615;&#65292;&#20687;&#24490;&#29615;&#25110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#39034;&#24207;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36825;&#26679;&#30340;&#39640;&#23481;&#37327;&#32467;&#26500;&#21464;&#24471;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#21333;&#21464;&#37327;&#32447;&#24615;&#27169;&#22411;&#21487;&#20197;&#22312;&#20960;&#20010;&#24120;&#29992;&#30340;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#36825;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25193;&#23637;&#23427;&#20204;&#65292;&#26412;&#25991;&#30740;&#31350;&#32447;&#24615;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#24207;&#28151;&#21512;&#22120;&#65288;TSMixer&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#12290; TSMixer&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#20449;&#24687;&#12290;&#22312;&#27969;&#34892;&#30340;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#31616;&#21333;&#26131;&#34892;&#30340;TSMixer&#19982;&#21033;&#29992;&#29305;&#23450;&#22522;&#20934;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#19987;&#19994;&#20808;&#36827;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#22823;&#35268;&#27169;&#30340;M5&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21363;&#19968;&#20010;&#23454;&#38469;&#30340;&#38646;&#21806;&#25968;&#25454;&#38598;&#19978;&#65292;TSMixer&#34920;&#29616;&#20986;&#38750;&#24120;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world time-series datasets are often multivariate with complex dynamics. To capture this complexity, high capacity architectures like recurrent- or attention-based sequential deep learning models have become popular. However, recent work demonstrates that simple univariate linear models can outperform such deep learning models on several commonly used academic benchmarks. Extending them, in this paper, we investigate the capabilities of linear models for time-series forecasting and present Time-Series Mixer (TSMixer), a novel architecture designed by stacking multi-layer perceptrons (MLPs). TSMixer is based on mixing operations along both the time and feature dimensions to extract information efficiently. On popular academic benchmarks, the simple-to-implement TSMixer is comparable to specialized state-of-the-art models that leverage the inductive biases of specific benchmarks. On the challenging and large scale M5 benchmark, a real-world retail dataset, TSMixer demonstrates super
&lt;/p&gt;</description></item><item><title>&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#26159;&#19968;&#31181;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#19982;&#38598;&#25104;&#23398;&#20064;&#65288;EL&#65289;&#30456;&#32467;&#21512;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#27169;&#22411;&#25110;&#22521;&#35757;&#31639;&#27861;&#20840;&#38754;&#25506;&#32034;&#38382;&#39064;&#31354;&#38388;&#65292;&#24182;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.02618</link><description>&lt;p&gt;
&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Ensemble Reinforcement Learning: A Survey. (arXiv:2303.02618v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02618
&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#26159;&#19968;&#31181;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#19982;&#38598;&#25104;&#23398;&#20064;&#65288;EL&#65289;&#30456;&#32467;&#21512;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#27169;&#22411;&#25110;&#22521;&#35757;&#31639;&#27861;&#20840;&#38754;&#25506;&#32034;&#38382;&#39064;&#31354;&#38388;&#65292;&#24182;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#21508;&#31181;&#31185;&#23398;&#21644;&#24212;&#29992;&#38382;&#39064;&#30340;&#39640;&#25928;&#25216;&#26415;&#12290;&#23613;&#31649;&#20854;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26576;&#20123;&#22797;&#26434;&#20219;&#21153;&#20173;&#38590;&#20197;&#20165;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#21644;&#31639;&#27861;&#35299;&#20915;&#12290;&#20316;&#20026;&#21709;&#24212;&#65292;&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;RL&#21644;&#38598;&#25104;&#23398;&#20064;&#65288;EL&#65289;&#30340;&#20248;&#28857;&#65292;&#24050;&#32463;&#24191;&#27867;&#21463;&#21040;&#27426;&#36814;&#12290;ERL&#21033;&#29992;&#22810;&#20010;&#27169;&#22411;&#25110;&#22521;&#35757;&#31639;&#27861;&#20840;&#38754;&#25506;&#32034;&#38382;&#39064;&#31354;&#38388;&#65292;&#24182;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;ERL&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#20197;&#20415;&#20026;&#35835;&#32773;&#25552;&#20379;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#27010;&#36848;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;ERL&#30340;&#32972;&#26223;&#21644;&#21160;&#26426;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35814;&#32454;&#20998;&#26512;&#20102;&#25104;&#21151;&#24212;&#29992;&#20110;ERL&#20013;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#27169;&#22411;&#24179;&#22343;&#12289;&#27169;&#22411;&#36873;&#25321;&#21644;&#27169;&#22411;&#32452;&#21512;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#24182;&#20998;&#26512;&#20102;&#25152;&#20351;&#29992;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has emerged as a highly effective technique for addressing various scientific and applied problems. Despite its success, certain complex tasks remain challenging to be addressed solely with a single model and algorithm. In response, ensemble reinforcement learning (ERL), a promising approach that combines the benefits of both RL and ensemble learning (EL), has gained widespread popularity. ERL leverages multiple models or training algorithms to comprehensively explore the problem space and possesses strong generalization capabilities. In this study, we present a comprehensive survey on ERL to provide readers with an overview of recent advances and challenges in the field. First, we introduce the background and motivation for ERL. Second, we analyze in detail the strategies that have been successfully applied in ERL, including model averaging, model selection, and model combination. Subsequently, we summarize the datasets and analyze algorithms used in releva
&lt;/p&gt;</description></item><item><title>RAFEN&#26159;&#19968;&#20010;&#33410;&#28857;&#23884;&#20837;&#30340;&#27491;&#21017;&#21270;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23398;&#20064;&#23545;&#40784;&#33410;&#28857;&#23884;&#20837;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#21442;&#25968;&#35774;&#32622;&#12290;RAFEN&#22312;&#29616;&#26377;&#26041;&#27861;&#19978;&#23454;&#29616;&#20102;&#19982;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21160;&#24577;&#22270;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#20445;&#25345;&#33410;&#28857;&#23884;&#20837;&#30340;&#21487;&#27604;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.01926</link><description>&lt;p&gt;
RAFEN -- &#33410;&#28857;&#23884;&#20837;&#30340;&#27491;&#21017;&#21270;&#23545;&#40784;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
RAFEN -- Regularized Alignment Framework for Embeddings of Nodes. (arXiv:2303.01926v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01926
&lt;/p&gt;
&lt;p&gt;
RAFEN&#26159;&#19968;&#20010;&#33410;&#28857;&#23884;&#20837;&#30340;&#27491;&#21017;&#21270;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23398;&#20064;&#23545;&#40784;&#33410;&#28857;&#23884;&#20837;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#21442;&#25968;&#35774;&#32622;&#12290;RAFEN&#22312;&#29616;&#26377;&#26041;&#27861;&#19978;&#23454;&#29616;&#20102;&#19982;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21160;&#24577;&#22270;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#20445;&#25345;&#33410;&#28857;&#23884;&#20837;&#30340;&#21487;&#27604;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33410;&#28857;&#23884;&#20837;&#23398;&#20064;&#26159;&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#30740;&#31350;&#39046;&#22495;&#12290;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#33410;&#28857;&#23884;&#20837;&#27169;&#22411;&#24212;&#35813;&#22312;&#26368;&#32456;&#23884;&#20837;&#20013;&#21453;&#26144;&#20986;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#32467;&#26500;&#12290;&#22312;&#21160;&#24577;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#29305;&#24449;&#21644;&#32467;&#26500;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#22312;&#22270;&#30340;&#28436;&#21270;&#36807;&#31243;&#20013;&#65292;&#26576;&#20123;&#33410;&#28857;&#30340;&#23884;&#20837;&#24212;&#35813;&#20445;&#25345;&#21487;&#27604;&#24615;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#24212;&#29992;&#23545;&#40784;&#36807;&#31243;&#26469;&#23454;&#29616;&#12290;&#22312;&#29616;&#26377;&#30340;&#24037;&#20316;&#20013;&#65292;&#36825;&#19968;&#27493;&#32463;&#24120;&#26159;&#22312;&#33410;&#28857;&#23884;&#20837;&#24050;&#32463;&#35745;&#31639;&#20986;&#26469;&#21518;&#36827;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;--RAFEN&#65292;&#23427;&#20801;&#35768;&#20351;&#29992;&#19978;&#36848;&#23545;&#40784;&#39033;&#26469;&#20016;&#23500;&#20219;&#20309;&#29616;&#26377;&#30340;&#33410;&#28857;&#23884;&#20837;&#26041;&#27861;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23398;&#20064;&#23545;&#40784;&#33410;&#28857;&#23884;&#20837;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#21464;&#20307;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#20845;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#12290;RAFEN &#23454;&#29616;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#21442;&#25968;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning representations of nodes has been a crucial area of the graph machine learning research area. A well-defined node embedding model should reflect both node features and the graph structure in the final embedding. In the case of dynamic graphs, this problem becomes even more complex as both features and structure may change over time. The embeddings of particular nodes should remain comparable during the evolution of the graph, what can be achieved by applying an alignment procedure. This step was often applied in existing works after the node embedding was already computed. In this paper, we introduce a framework -- RAFEN -- that allows to enrich any existing node embedding method using the aforementioned alignment term and learning aligned node embedding during training time. We propose several variants of our framework and demonstrate its performance on six real-world datasets. RAFEN achieves on-par or better performance than existing approaches without requiring additional p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;In-Sample Softmax&#65292;&#36890;&#36807;&#20351;&#29992;&#21482;&#30001;&#25968;&#25454;&#38598;&#20013;&#30340;&#25805;&#20316;&#32452;&#25104;&#30340;In-Sample softmax&#26469;&#35299;&#20915;&#25805;&#20316;&#35206;&#30422;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#19988;In-Sample Actor-Critic&#19982;&#35813;&#26041;&#27861;&#30456;&#27604;&#22312;&#31283;&#23450;&#24615;&#25110;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.14372</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;In-Sample Softmax
&lt;/p&gt;
&lt;p&gt;
The In-Sample Softmax for Offline Reinforcement Learning. (arXiv:2302.14372v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;In-Sample Softmax&#65292;&#36890;&#36807;&#20351;&#29992;&#21482;&#30001;&#25968;&#25454;&#38598;&#20013;&#30340;&#25805;&#20316;&#32452;&#25104;&#30340;In-Sample softmax&#26469;&#35299;&#20915;&#25805;&#20316;&#35206;&#30422;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#19988;In-Sample Actor-Critic&#19982;&#35813;&#26041;&#27861;&#30456;&#27604;&#22312;&#31283;&#23450;&#24615;&#25110;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;RL&#20195;&#29702;&#21487;&#20197;&#21033;&#29992;&#20197;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#30340;&#25209;&#27425;&#26469;&#25552;&#21462;&#21512;&#29702;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#31163;&#32447;RL&#35774;&#32622;&#20013;&#65292;&#19968;&#20010;&#19981;&#26029;&#20986;&#29616;&#30340;&#38382;&#39064;&#26159;&#65292;&#35768;&#22810;&#26041;&#27861;&#19979;&#30340;bootstrapping&#26356;&#26032;&#21463;&#21040;&#34892;&#21160;&#35206;&#30422;&#19981;&#36275;&#30340;&#24433;&#21709;&#65306;&#26631;&#20934;max&#36816;&#31639;&#31526;&#21487;&#33021;&#20250;&#36873;&#25321;&#22312;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#20986;&#29616;&#36807;&#30340;&#26368;&#22823;&#21160;&#20316;&#12290;&#20174;&#36825;&#20123;&#19981;&#20934;&#30830;&#30340;&#20540;&#36827;&#34892;bootstrapping&#26356;&#26032;&#20250;&#23548;&#33268;&#39640;&#20272;&#29978;&#33267;&#21457;&#25955;&#12290;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#26041;&#27861;&#23581;&#35797;&#36817;&#20284;&#19968;&#20010;&#20165;&#20351;&#29992;&#25968;&#25454;&#38598;&#20013;&#28085;&#30422;&#33391;&#22909;&#30340;&#25805;&#20316;&#30340;in-sample max&#12290;&#26412;&#25991;&#24378;&#35843;&#19968;&#20010;&#31616;&#21333;&#30340;&#20107;&#23454;&#65306;&#20351;&#29992;&#20165;&#30001;&#25968;&#25454;&#38598;&#20013;&#30340;&#21160;&#20316;&#36817;&#20284;In-Sample softmax&#26356;&#21152;&#30452;&#35266;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;In-Sample softmax&#22522;&#30784;&#19978;&#30340;&#31574;&#30053;&#36845;&#20195;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#19988;&#23545;&#20110;&#28201;&#24230;&#30340;&#19979;&#38477;&#65292;&#23427;&#20250;&#25509;&#36817;In-Sample max&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;In-Sample softmax&#25512;&#23548;&#20986;&#19968;&#20010;In-Sample Actor-Critic&#65288;AC&#65289;&#65292;&#24182;&#19988;&#35777;&#26126;&#20854;&#22312;&#31283;&#23450;&#24615;&#25110;&#24615;&#33021;&#19978;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) agents can leverage batches of previously collected data to extract a reasonable control policy. An emerging issue in this offline RL setting, however, is that the bootstrapping update underlying many of our methods suffers from insufficient action-coverage: standard max operator may select a maximal action that has not been seen in the dataset. Bootstrapping from these inaccurate values can lead to overestimation and even divergence. There are a growing number of methods that attempt to approximate an \emph{in-sample} max, that only uses actions well-covered by the dataset. We highlight a simple fact: it is more straightforward to approximate an in-sample \emph{softmax} using only actions in the dataset. We show that policy iteration based on the in-sample softmax converges, and that for decreasing temperatures it approaches the in-sample max. We derive an In-Sample Actor-Critic (AC), using this in-sample softmax, and show that it is consistently better or 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#28151;&#21512;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20013;&#24515;&#21270;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;&#37329;&#34701;&#29359;&#32618;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2302.03654</link><description>&lt;p&gt;
&#38754;&#21521;&#37329;&#34701;&#29359;&#32618;&#26816;&#27979;&#30340;&#38544;&#31169;&#20445;&#25252;&#28151;&#21512;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Privacy-Preserving Hybrid Federated Learning Framework for Financial Crime Detection. (arXiv:2302.03654v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#28151;&#21512;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20013;&#24515;&#21270;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;&#37329;&#34701;&#29359;&#32618;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#21313;&#24180;&#26469;&#65292;&#20844;&#20849;&#21644;&#31169;&#20154;&#37096;&#38376;&#30340;&#37329;&#34701;&#29359;&#32618;&#25968;&#37327;&#28608;&#22686;&#65292;2022&#24180;&#37329;&#34701;&#26426;&#26500;&#30340;&#35784;&#39575;&#24179;&#22343;&#25104;&#26412;&#20026;1.02&#20159;&#32654;&#20803;&#12290;&#24320;&#21457;&#19968;&#31181;&#25171;&#20987;&#37329;&#34701;&#29359;&#32618;&#30340;&#26426;&#21046;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#22810;&#20010;&#26426;&#26500;&#28145;&#24230;&#21512;&#20316;&#65292;&#28982;&#32780;&#36825;&#26679;&#30340;&#21512;&#20316;&#30001;&#20110;&#20998;&#24067;&#24335;&#37329;&#34701;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#35201;&#27714;&#32780;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;&#26412;&#25991;&#25910;&#38598;&#24182;&#39044;&#22788;&#29702;&#20102;&#22810;&#23478;&#38134;&#34892;&#30340;&#22823;&#35268;&#27169;&#37329;&#34701;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20013;&#24515;&#21270;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#20248;&#28857;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#26816;&#27979;&#37329;&#34701;&#29359;&#32618;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21033;&#29992;&#38544;&#31169;&#20445;&#25252;&#32858;&#21512;&#22120;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#20013;&#24515;&#27169;&#22411;&#20316;&#20026;&#20998;&#24067;&#24335;&#23398;&#20064;&#36807;&#31243;&#28909;&#36523;&#26469;&#22686;&#24378;&#32852;&#21512;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#31934;&#30830;&#24230;&#20248;&#20110;&#20013;&#24515;&#21270;&#21644;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent decade witnessed a surge of increase in financial crimes across the public and private sectors, with an average cost of scams of $102m to financial institutions in 2022. Developing a mechanism for battling financial crimes is an impending task that requires in-depth collaboration from multiple institutions, and yet such collaboration imposed significant technical challenges due to the privacy and security requirements of distributed financial data. For example, consider the modern payment network systems, which can generate millions of transactions per day across a large number of global institutions. Training a detection model of fraudulent transactions requires not only secured transactions but also the private account activities of those involved in each transaction from corresponding bank systems. The distributed nature of both samples and features prevents most existing learning systems from being directly adopted to handle the data mining task. In this paper, we collec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#31070;&#32463;&#31639;&#31526;&#21450;&#20854;&#34893;&#29983;&#32467;&#26500;&#30340;&#27867;&#21270;&#29305;&#24615;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#26041;&#27861;&#65292;&#21253;&#25324;&#24341;&#20837;&#26680;&#31215;&#20998;&#31639;&#31526;&#26469;&#20195;&#26367;&#33258;&#20851;&#27880;&#26426;&#21046;&#21644;&#36880;&#28176;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#30340;&#35757;&#32451;&#35838;&#31243;&#65292;&#32467;&#26524;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11509</link><description>&lt;p&gt;
&#32454;&#35843;&#31070;&#32463;&#31639;&#31526;&#32467;&#26500;&#20197;&#25552;&#39640;&#35757;&#32451;&#21644;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Neural-Operator architectures for training and generalization. (arXiv:2301.11509v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#31070;&#32463;&#31639;&#31526;&#21450;&#20854;&#34893;&#29983;&#32467;&#26500;&#30340;&#27867;&#21270;&#29305;&#24615;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#26041;&#27861;&#65292;&#21253;&#25324;&#24341;&#20837;&#26680;&#31215;&#20998;&#31639;&#31526;&#26469;&#20195;&#26367;&#33258;&#20851;&#27880;&#26426;&#21046;&#21644;&#36880;&#28176;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#30340;&#35757;&#32451;&#35838;&#31243;&#65292;&#32467;&#26524;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#31070;&#32463;&#31639;&#31526;&#65288;NOs&#65289;&#21450;&#20854;&#34893;&#29983;&#32467;&#26500;&#30340;&#27867;&#21270;&#29305;&#24615;&#12290;&#36890;&#36807;&#23545;&#27979;&#35797;&#25439;&#22833;&#30340;&#32463;&#39564;&#35780;&#20272;&#12289;&#22522;&#20110;&#22797;&#26434;&#24615;&#30340;&#27867;&#21270;&#30028;&#38480;&#30340;&#20998;&#26512;&#20197;&#21450;&#23545;&#25439;&#22833;&#26223;&#35266;&#21487;&#35270;&#21270;&#30340;&#23450;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26088;&#22312;&#25552;&#39640;NOs&#27867;&#21270;&#33021;&#21147;&#30340;&#20462;&#25913;&#12290;&#21463;Transformer&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;${\textit{s}}{\text{NO}}+\varepsilon$&#65292;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#26680;&#31215;&#20998;&#31639;&#31526;&#26469;&#20195;&#26367;&#33258;&#20851;&#27880;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20276;&#38543;&#30528;&#25439;&#22833;&#26223;&#35266;&#21487;&#35270;&#21270;&#30340;&#23450;&#24615;&#21464;&#21270;&#65292;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#20102;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#29468;&#27979;&#65292;Transformer&#30340;&#24067;&#23616;&#20351;&#20248;&#21270;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#26356;&#22909;&#30340;&#26497;&#23567;&#20540;&#65292;&#24182;&#19988;&#38543;&#26426;&#28145;&#24230;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#30001;&#20110;&#35757;&#32451;&#21160;&#24577;&#30340;&#20005;&#26684;&#20998;&#26512;&#26159;&#28145;&#24230;&#23398;&#20064;&#26368;&#31361;&#20986;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#65292;&#22240;&#27492;&#25105;&#20204;&#36824;&#25512;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35757;&#32451;&#35838;&#31243;&#65292;&#37325;&#28857;&#26159;&#36880;&#28176;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a comprehensive analysis of the generalization properties of Neural Operators (NOs) and their derived architectures. Through empirical evaluation of the test loss, analysis of the complexity-based generalization bounds, and qualitative assessments of the visualization of the loss landscape, we investigate modifications aimed at enhancing the generalization capabilities of NOs. Inspired by the success of Transformers, we propose ${\textit{s}}{\text{NO}}+\varepsilon$, which introduces a kernel integral operator in lieu of self-Attention. Our results reveal significantly improved performance across datasets and initializations, accompanied by qualitative changes in the visualization of the loss landscape. We conjecture that the layout of Transformers enables the optimization algorithm to find better minima, and stochastic depth, improve the generalization performance. As a rigorous analysis of training dynamics is one of the most prominent unsolved problems in deep lear
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#28982;&#28798;&#23475;&#20998;&#21106;&#27169;&#22411;&#22312;&#36879;&#26126;&#20113;&#65292;&#28895;&#38654;&#26609;&#21644;&#27946;&#27700;&#20998;&#21106;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#22312;&#36866;&#24403;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.09318</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#28982;&#28798;&#23475;&#20998;&#21106;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Toward Foundation Models for Earth Monitoring: Generalizable Deep Learning Models for Natural Hazard Segmentation. (arXiv:2301.09318v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09318
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#28982;&#28798;&#23475;&#20998;&#21106;&#27169;&#22411;&#22312;&#36879;&#26126;&#20113;&#65292;&#28895;&#38654;&#26609;&#21644;&#27946;&#27700;&#20998;&#21106;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#22312;&#36866;&#24403;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#21464;&#21270;&#23548;&#33268;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#30340;&#27010;&#29575;&#22686;&#21152;&#65292;&#36825;&#23545;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#31038;&#20250;&#21644;&#20225;&#19994;&#26500;&#25104;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#36817;&#23454;&#26102;&#22320;&#22270;&#21046;&#20316;&#24050;&#25104;&#20026;&#25903;&#25345;&#33258;&#28982;&#28798;&#23475;&#25937;&#25588;&#12289;&#39118;&#38505;&#31649;&#29702;&#21644;&#25919;&#24220;&#25919;&#31574;&#20915;&#31574;&#30340;&#26032;&#20248;&#20808;&#20107;&#39033;&#12290;&#26368;&#36817;&#65292;&#23454;&#29616;&#36817;&#23454;&#26102;&#21046;&#22270;&#30340;&#26041;&#27861;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;DL&#30340;&#26041;&#27861;&#20165;&#38024;&#23545;&#21333;&#20010;&#22320;&#29702;&#21306;&#22495;&#20013;&#29305;&#23450;&#39057;&#27573;&#30340;&#21355;&#26143;&#25968;&#25454;&#30340;&#19968;&#20010;&#29305;&#23450;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#29992;&#20110;&#21046;&#22270;&#29305;&#23450;&#33258;&#28982;&#28798;&#23475;&#30340;DL&#27169;&#22411;&#38590;&#20197;&#23558;&#20854;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#20854;&#20182;&#31867;&#22411;&#30340;&#33258;&#28982;&#28798;&#23475;&#19978;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36866;&#24403;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26174;&#30528;&#25552;&#39640;DL&#33258;&#28982;&#28798;&#23475;&#26144;&#23556;&#22120;&#30340;&#36890;&#29992;&#24615;&#12290;&#22312;&#27809;&#26377;&#20219;&#20309;&#30446;&#26631;&#39046;&#22495;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#25913;&#36827;&#30340;&#36890;&#29992;&#24615;&#36328;&#36234;&#20102;&#22235;&#20010;&#36879;&#26126;&#20113;&#65292;&#28895;&#38654;&#26609;&#21644;&#27946;&#27700;&#20998;&#27573;&#20219;&#21153;&#30340;&#22810;&#20010;&#22320;&#29702;&#21306;&#22495;&#12290;&#36890;&#36807;&#21512;&#24182;&#39044;&#35757;&#32451;&#27493;&#39588;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#19977;&#20010;&#33258;&#28982;&#28798;&#23475;&#20998;&#21106;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#19982;&#20854;&#20182;DL&#20307;&#31995;&#32467;&#26500;&#30456;&#27604;&#65292;&#20445;&#25345;&#26356;&#24555;&#30340;&#35745;&#31639;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20195;&#34920;&#30528;&#36808;&#21521;&#33021;&#22815;&#22312;&#22810;&#20010;&#39046;&#22495;&#36890;&#29992;&#30340;&#36817;&#23454;&#26102;&#33258;&#28982;&#28798;&#23475;&#21046;&#22270;&#22522;&#26412;&#27169;&#22411;&#30340;&#26377;&#24076;&#26395;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate change results in an increased probability of extreme weather events that put societies and businesses at risk on a global scale. Therefore, near real-time mapping of natural hazards is an emerging priority for the support of natural disaster relief, risk management, and informing governmental policy decisions. Recent methods to achieve near real-time mapping increasingly leverage deep learning (DL). However, DL-based approaches are designed for one specific task in a single geographic region based on specific frequency bands of satellite data. Therefore, DL models used to map specific natural hazards struggle with their generalization to other types of natural hazards in unseen regions. In this work, we propose a methodology to significantly improve the generalizability of DL natural hazards mappers based on pre-training on a suitable pre-task. Without access to any data from the target domain, we demonstrate this improved generalizability across four U-Net architectures for t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22352;&#26631;&#30340;MLPs&#30340;&#35889;&#20559;&#32622;&#23545;&#39640;&#39057;&#32452;&#20214;&#25910;&#25947;&#30340;&#38459;&#30861;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#39640;&#39057;&#27491;&#24358;&#27874;&#32534;&#30721;&#36755;&#20837;&#26469;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2301.05816</link><description>&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#21160;&#24577;&#29702;&#35299;&#22522;&#20110;&#22352;&#26631;&#30340;MLPs&#30340;&#35889;&#20559;&#32622;
&lt;/p&gt;
&lt;p&gt;
Understanding the Spectral Bias of Coordinate Based MLPs Via Training Dynamics. (arXiv:2301.05816v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05816
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22352;&#26631;&#30340;MLPs&#30340;&#35889;&#20559;&#32622;&#23545;&#39640;&#39057;&#32452;&#20214;&#25910;&#25947;&#30340;&#38459;&#30861;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#39640;&#39057;&#27491;&#24358;&#27874;&#32534;&#30721;&#36755;&#20837;&#26469;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35889;&#20559;&#32622;&#26159;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#37325;&#35201;&#35266;&#23519;&#32467;&#26524;&#65292;&#23427;&#34920;&#31034;&#32593;&#32476;&#22312;&#25910;&#25947;&#21040;&#26356;&#39640;&#39057;&#29575;&#32452;&#20214;&#21069;&#65292;&#20250;&#23398;&#20064;&#30446;&#26631;&#20989;&#25968;&#30340;&#20302;&#39057;&#34920;&#31034;&#12290;&#36825;&#19968;&#23646;&#24615;&#19982;&#36229;&#21442;&#25968;&#32593;&#32476;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#26377;&#20851;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#22330;&#26223;&#28210;&#26579;&#26102;&#65292;&#37319;&#29992;&#20855;&#26377;ReLU&#28608;&#27963;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;(MLPs)&#21033;&#29992;&#23494;&#38598;&#30340;&#20302;&#32500;&#22352;&#26631;&#36755;&#20837;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#35889;&#20559;&#24046;&#65292;&#23436;&#20840;&#38459;&#30861;&#20102;&#25910;&#25947;&#21040;&#39640;&#39057;&#32452;&#20214;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#21487;&#20197;&#20351;&#29992;&#39640;&#39057;&#27491;&#24358;&#27874;&#32534;&#30721;&#36755;&#20837;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;(NTK)&#21644;&#20613;&#37324;&#21494;&#20998;&#26512;&#26469;&#35299;&#37322;&#22352;&#26631;&#31995;&#20013;&#30340;&#35889;&#20559;&#24046;&#21450;&#20854;&#20005;&#37325;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#65292;&#22240;&#20026;NTK&#19981;&#33021;&#25429;&#25417;&#21040;&#30495;&#27491;&#30340;&#32593;&#32476;&#21160;&#24577;&#65292;&#32780;&#20613;&#37324;&#21494;&#20998;&#26512;&#21482;&#33021;&#25552;&#20379;&#23545;&#39057;&#29575;&#32452;&#20214;&#30340;&#20840;&#23616;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral bias is an important observation of neural network training, stating that the network will learn a low frequency representation of the target function before converging to higher frequency components. This property is interesting due to its link to good generalization in over-parameterized networks. However, in applications to scene rendering, where multi-layer perceptrons (MLPs) with ReLU activations utilize dense, low dimensional coordinate based inputs, a severe spectral bias occurs that obstructs convergence to high freqeuncy components entirely. In order to overcome this limitation, one can encode the inputs using high frequency sinusoids. Previous works attempted to explain both spectral bias and its severity in the coordinate based regime using Neural Tangent Kernel (NTK) and Fourier analysis. However, such methods come with various limitations, since NTK does not capture real network dynamics, and Fourier analysis only offers a global perspective on the frequency compo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#65292;&#26088;&#22312;&#24179;&#31283;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;&#20302;&#30417;&#30563;&#22522;&#20110;&#22270;&#30340;&#20998;&#31867;&#20043;&#38388;&#30340;&#36807;&#28193;&#65292;&#24182;&#19988;&#26080;&#38656;&#20351;&#29992;Dirichlet&#27491;&#21017;&#21270;&#25110;&#32773;&#22522;&#20110;&#26680;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.04956</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Graph Laplacian for Semi-Supervised Learning. (arXiv:2301.04956v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#65292;&#26088;&#22312;&#24179;&#31283;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;&#20302;&#30417;&#30563;&#22522;&#20110;&#22270;&#30340;&#20998;&#31867;&#20043;&#38388;&#30340;&#36807;&#28193;&#65292;&#24182;&#19988;&#26080;&#38656;&#20351;&#29992;Dirichlet&#27491;&#21017;&#21270;&#25110;&#32773;&#22522;&#20110;&#26680;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#22312;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#20294;&#26410;&#26631;&#35760;&#25968;&#25454;&#20016;&#23500;&#30340;&#24120;&#35265;&#22330;&#26223;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#22270;&#65288;&#25110;&#38750;&#26412;&#22320;&#65289;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#26159;&#35299;&#20915;&#21508;&#31181;&#23398;&#20064;&#20219;&#21153;&#30340;&#22522;&#26412;&#24179;&#28369;&#31639;&#23376;&#12290;&#23545;&#20110;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#22270;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#30340;&#35889;&#23884;&#20837;&#12290;&#23545;&#20110;&#21322;&#30417;&#30563;&#38382;&#39064;&#65292;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#22522;&#20110;Dirichlet&#33021;&#37327;&#26469;&#35299;&#20915;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#30417;&#30563;&#20943;&#23569;&#65292;Dirichlet&#20248;&#21270;&#21464;&#24471;&#27425;&#20248;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;&#20302;&#30417;&#30563;&#22522;&#20110;&#22270;&#30340;&#20998;&#31867;&#20043;&#38388;&#33719;&#24471;&#24179;&#28369;&#30340;&#36807;&#28193;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#22411;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#12290;&#23427;&#22522;&#20110;&#23494;&#24230;&#21644;&#23545;&#27604;&#24230;&#24230;&#37327;&#65292;&#24182;&#20801;&#35768;&#30452;&#25509;&#22312;&#36816;&#31639;&#31526;&#20013;&#23545;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#32534;&#30721;&#12290; &#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#25104;&#21151;&#22320;&#36890;&#36807;&#26368;&#23567;&#21270;&#25152;&#25552;&#20986;&#30340;&#24179;&#28369;SSL&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#65292;&#32780;&#26080;&#38656;&#21333;&#29420;&#20351;&#29992;Dirichlet&#27491;&#21017;&#21270;&#25110;&#22522;&#20110;&#26680;&#30340;&#26041;&#27861;&#65292;&#26469;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning is highly useful in common scenarios where labeled data is scarce but unlabeled data is abundant. The graph (or nonlocal) Laplacian is a fundamental smoothing operator for solving various learning tasks. For unsupervised clustering, a spectral embedding is often used, based on graph-Laplacian eigenvectors. For semi-supervised problems, the common approach is to solve a constrained optimization problem, regularized by a Dirichlet energy, based on the graph-Laplacian. However, as supervision decreases, Dirichlet optimization becomes suboptimal. We therefore would like to obtain a smooth transition between unsupervised clustering and low-supervised graph-based classification. In this paper, we propose a new type of graph-Laplacian which is adapted for Semi-Supervised Learning (SSL) problems. It is based on both density and contrastive measures and allows the encoding of the labeled data directly in the operator. Thus, we can perform successfully semi-supervised le
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;BASiS&#65292;&#19968;&#31181;&#29992;&#20110;&#30452;&#25509;&#23398;&#20064;&#29305;&#24449;&#21521;&#37327;&#31354;&#38388;&#30340;&#23545;&#40784;&#26426;&#21046;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#25209;&#37327;&#21644;&#22270;&#24230;&#37327;&#21464;&#21270;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20339;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.16960</link><description>&lt;p&gt;
BASiS&#65306;&#25209;&#37327;&#23545;&#40784;&#35889;&#23884;&#20837;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
BASiS: Batch Aligned Spectral Embedding Space. (arXiv:2211.16960v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16960
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;BASiS&#65292;&#19968;&#31181;&#29992;&#20110;&#30452;&#25509;&#23398;&#20064;&#29305;&#24449;&#21521;&#37327;&#31354;&#38388;&#30340;&#23545;&#40784;&#26426;&#21046;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#25209;&#37327;&#21644;&#22270;&#24230;&#37327;&#21464;&#21270;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20339;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#26159;&#19968;&#31181;&#39640;&#24230;&#36890;&#29992;&#21644;&#22810;&#26679;&#21270;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#36866;&#29992;&#20110;&#20960;&#20046;&#20219;&#20309;&#25968;&#25454;&#22788;&#29702;&#38382;&#39064;&#12290;&#35889;&#22270;&#29702;&#35770;&#24050;&#34987;&#35777;&#26126;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#31639;&#27861;&#65292;&#24182;&#24471;&#21040;&#20102;&#22362;&#23454;&#30340;&#32447;&#24615;&#20195;&#25968;&#29702;&#35770;&#25903;&#25345;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#20855;&#26377;&#35889;&#22270;&#29305;&#24615;&#30340;&#28145;&#24230;&#32593;&#32476;&#24314;&#27169;&#22359;&#21487;&#33021;&#20250;&#38750;&#24120;&#26377;&#29992;&#12290;&#20363;&#22914;&#65292;&#36825;&#26679;&#30340;&#32593;&#32476;&#20801;&#35768;&#20026;&#26576;&#20123;&#20219;&#21153;&#35774;&#35745;&#26368;&#20339;&#22270;&#24418;&#25110;&#33719;&#24471;&#25968;&#25454;&#30340;&#35268;&#33539;&#27491;&#20132;&#20302;&#32500;&#23884;&#20837;&#12290;&#26368;&#36817;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#23581;&#35797;&#26159;&#22522;&#20110;&#26368;&#23567;&#21270;&#29790;&#21033;&#21830;&#25439;&#22833;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#23398;&#20064;&#29305;&#24449;&#21521;&#37327;&#31354;&#38388;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#24212;&#29992;&#20110;&#25209;&#37327;&#23398;&#20064;&#30340;&#30452;&#25509;&#26041;&#27861;&#30340;&#19968;&#20010;&#20005;&#37325;&#38382;&#39064;&#26159;&#23558;&#29305;&#24449;&#19981;&#19968;&#33268;&#22320;&#26144;&#23556;&#21040;&#19981;&#21516;&#25209;&#27425;&#20013;&#30340;&#29305;&#24449;&#21521;&#37327;&#22352;&#26631;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20351;&#29992;&#25209;&#37327;&#23398;&#20064;&#36825;&#39033;&#20219;&#21153;&#30340;&#33258;&#30001;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31283;&#23450;&#30340;&#23545;&#40784;&#26426;&#21046;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#25209;&#37327;&#21464;&#21270;&#21644;&#22270;&#24230;&#37327;&#21464;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#24050;&#26377;&#30340;&#26368;&#20339;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph is a highly generic and diverse representation, suitable for almost any data processing problem. Spectral graph theory has been shown to provide powerful algorithms, backed by solid linear algebra theory. It thus can be extremely instrumental to design deep network building blocks with spectral graph characteristics. For instance, such a network allows the design of optimal graphs for certain tasks or obtaining a canonical orthogonal low-dimensional embedding of the data. Recent attempts to solve this problem were based on minimizing Rayleigh-quotient type losses. We propose a different approach of directly learning the eigensapce. A severe problem of the direct approach, applied in batch-learning, is the inconsistent mapping of features to eigenspace coordinates in different batches. We analyze the degrees of freedom of learning this task using batches and propose a stable alignment mechanism that can work both with batch changes and with graph-metric changes. We show that our l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#32500;&#20998;&#20301;&#25968;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#36866;&#24212;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20013;&#30340;&#24322;&#36136;&#24615;&#21644;&#37325;&#23614;&#20998;&#24067;&#12290;&#26681;&#25454;&#31934;&#24515;&#36873;&#25321;&#30340;&#21487;&#36716;&#31227;&#28304;&#22495;&#24314;&#31435;&#20102;&#36716;&#31227;&#23398;&#20064;&#20272;&#35745;&#37327;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#32622;&#20449;&#21306;&#38388;&#21644;&#20551;&#35774;&#26816;&#39564;&#31243;&#24207;&#65292;&#20197;&#23454;&#29616;&#19968;&#27493;&#23436;&#25104;&#12290;</title><link>http://arxiv.org/abs/2211.14578</link><description>&lt;p&gt;
&#39640;&#32500;&#20998;&#20301;&#25968;&#22238;&#24402;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#32479;&#35745;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Statistical inference for transfer learning with high-dimensional quantile regression. (arXiv:2211.14578v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#32500;&#20998;&#20301;&#25968;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#36866;&#24212;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20013;&#30340;&#24322;&#36136;&#24615;&#21644;&#37325;&#23614;&#20998;&#24067;&#12290;&#26681;&#25454;&#31934;&#24515;&#36873;&#25321;&#30340;&#21487;&#36716;&#31227;&#28304;&#22495;&#24314;&#31435;&#20102;&#36716;&#31227;&#23398;&#20064;&#20272;&#35745;&#37327;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#32622;&#20449;&#21306;&#38388;&#21644;&#20551;&#35774;&#26816;&#39564;&#31243;&#24207;&#65292;&#20197;&#23454;&#29616;&#19968;&#27493;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#21033;&#29992;&#28304;&#22495;&#20013;&#30340;&#20449;&#24687;&#26469;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#39640;&#32500;&#25968;&#25454;&#26222;&#36941;&#23384;&#22312;&#24322;&#36136;&#24615;&#21644;/&#25110;&#37325;&#23614;&#20998;&#24067;&#65292;&#20294;&#30446;&#21069;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#36825;&#20123;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#32467;&#26524;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#39640;&#32500;&#20998;&#20301;&#25968;&#22238;&#24402;&#27169;&#22411;&#26694;&#26550;&#19979;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#31227;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#36866;&#24212;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20013;&#30340;&#24322;&#36136;&#24615;&#21644;&#37325;&#23614;&#20998;&#24067;&#12290;&#25105;&#20204;&#26681;&#25454;&#31934;&#24515;&#36873;&#25321;&#30340;&#21487;&#36716;&#31227;&#28304;&#22495;&#24314;&#31435;&#20102;&#36716;&#31227;&#23398;&#20064;&#20272;&#35745;&#37327;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#26174;&#31034;&#22312;&#20851;&#38190;&#36873;&#25321;&#26631;&#20934;&#21644;&#36739;&#22823;&#30340;&#28304;&#20219;&#21153;&#26679;&#26412;&#37327;&#19979;&#21487;&#20197;&#23454;&#29616;&#26356;&#20302;&#30340;&#35823;&#24046;&#30028;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#32622;&#20449;&#21306;&#38388;&#21644;&#20551;&#35774;&#26816;&#39564;&#31243;&#24207;&#65292;&#29992;&#20110;&#39640;&#32500;&#20998;&#20301;&#25968;&#22238;&#24402;&#31995;&#25968;&#30340;&#21508;&#20010;&#20998;&#37327;&#65292;&#36890;&#36807;&#20513;&#23548;&#21452;&#37325;&#36716;&#31227;&#23398;&#20064;&#20272;&#35745;&#37327;&#65292;&#23454;&#29616;&#19968;&#27493;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning has become an essential technique to exploit information from the source domain to boost performance of the target task. Despite the prevalence in high-dimensional data, heterogeneity and/or heavy tails are insufficiently accounted for by current transfer learning approaches and thus may undermine the resulting performance. We propose a transfer learning procedure in the framework of high-dimensional quantile regression models to accommodate the heterogeneity and heavy tails in the source and target domains. We establish error bounds of the transfer learning estimator based on delicately selected transferable source domains, showing that lower error bounds can be achieved for critical selection criterion and larger sample size of source tasks. We further propose valid confidence interval and hypothesis test procedures for individual component of high-dimensional quantile regression coefficients by advocating a double transfer learning estimator, which is the one-step 
&lt;/p&gt;</description></item><item><title>Psiformer&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#20174;&#22836;&#35745;&#31639;&#37327;&#23376;&#21270;&#23398;&#20013;&#22522;&#24577;&#33021;&#37327;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22823;&#20998;&#23376;&#19978;&#12290;</title><link>http://arxiv.org/abs/2211.13672</link><description>&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#26694;&#26550;&#22312;&#20174;&#22836;&#35745;&#31639;&#37327;&#23376;&#21270;&#23398;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Self-Attention Ansatz for Ab-initio Quantum Chemistry. (arXiv:2211.13672v2 [physics.chem-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13672
&lt;/p&gt;
&lt;p&gt;
Psiformer&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#20174;&#22836;&#35745;&#31639;&#37327;&#23376;&#21270;&#23398;&#20013;&#22522;&#24577;&#33021;&#37327;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22823;&#20998;&#23376;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;-&#27874;&#20989;&#25968;&#21464;&#21387;&#22120;&#65288;Psiformer&#65289;&#65292;&#23427;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#20316;&#20026;&#36817;&#20284;&#26041;&#27861;&#65288;&#25110;Ansatz&#65289;&#26469;&#35299;&#20915;&#35768;&#22810;&#30005;&#23376;&#34203;&#23450;&#35860;&#26041;&#31243;&#65292;&#36825;&#26159;&#37327;&#23376;&#21270;&#23398;&#21644;&#26448;&#26009;&#31185;&#23398;&#30340;&#22522;&#26412;&#26041;&#31243;&#12290;&#36825;&#20010;&#26041;&#31243;&#21487;&#20197;&#20174;&#31532;&#19968;&#21407;&#29702;&#20013;&#35299;&#20915;&#65292;&#19981;&#38656;&#35201;&#22806;&#37096;&#35757;&#32451;&#25968;&#25454;&#12290;&#36817;&#24180;&#26469;&#65292;&#20687;FermiNet&#21644;PauliNet&#36825;&#26679;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#29992;&#20110;&#26174;&#33879;&#25552;&#39640;&#36825;&#20123;&#31532;&#19968;&#21407;&#29702;&#35745;&#31639;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#25511;&#21046;&#30005;&#23376;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Psiformer&#21487;&#20197;&#20316;&#20026;&#36825;&#20123;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#26367;&#25442;&#21697;&#65292;&#32463;&#24120;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#30340;&#20934;&#30830;&#24615;&#12290;&#23588;&#20854;&#26159;&#22312;&#22823;&#20998;&#23376;&#19978;&#65292;&#22522;&#24577;&#33021;&#37327;&#30340;&#25552;&#39640;&#21487;&#20197;&#36798;&#21040;&#20960;&#21313;kcal/mol&#65292;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26377;&#20102;&#36136;&#30340;&#39134;&#36291;&#12290;&#36825;&#34920;&#26126;&#65292;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#22797;&#26434;&#30340;&#37327;&#23376;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel neural network architecture using self-attention, the Wavefunction Transformer (Psiformer), which can be used as an approximation (or Ansatz) for solving the many-electron Schr\"odinger equation, the fundamental equation for quantum chemistry and material science. This equation can be solved from first principles, requiring no external training data. In recent years, deep neural networks like the FermiNet and PauliNet have been used to significantly improve the accuracy of these first-principle calculations, but they lack an attention-like mechanism for gating interactions between electrons. Here we show that the Psiformer can be used as a drop-in replacement for these other neural networks, often dramatically improving the accuracy of the calculations. On larger molecules especially, the ground state energy can be improved by dozens of kcal/mol, a qualitative leap over previous methods. This demonstrates that self-attention networks can learn complex quantum mechani
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Waveformer&#27169;&#22411;&#65292;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#26102;&#27969;&#30446;&#26631;&#22768;&#38899;&#25552;&#21462;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#20351;&#29992;&#28151;&#21512;&#20307;&#31995;&#32467;&#26500;&#65292;&#33021;&#22815;&#22788;&#29702;&#22823;&#25509;&#21463;&#22495;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#27169;&#22411;&#22823;&#23567;&#21644;&#36816;&#34892;&#26102;&#38388;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2211.02250</link><description>&lt;p&gt;
&#23454;&#26102;&#30446;&#26631;&#22768;&#38899;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Real-Time Target Sound Extraction. (arXiv:2211.02250v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02250
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Waveformer&#27169;&#22411;&#65292;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#26102;&#27969;&#30446;&#26631;&#22768;&#38899;&#25552;&#21462;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#20351;&#29992;&#28151;&#21512;&#20307;&#31995;&#32467;&#26500;&#65292;&#33021;&#22815;&#22788;&#29702;&#22823;&#25509;&#21463;&#22495;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#27169;&#22411;&#22823;&#23567;&#21644;&#36816;&#34892;&#26102;&#38388;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#23454;&#26102;&#27969;&#30446;&#26631;&#22768;&#38899;&#25552;&#21462;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#20026;&#20102;&#23454;&#29616;&#27492;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Waveformer&#65292;&#23427;&#26159;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#20307;&#31995;&#32467;&#26500;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#26159;&#22534;&#21472;&#30340;&#33192;&#32960;&#22240;&#26524;&#21367;&#31215;&#23618;&#65292;&#35299;&#30721;&#22120;&#21017;&#26159;&#21464;&#21387;&#22120;&#23618;&#12290;&#35813;&#28151;&#21512;&#20307;&#31995;&#32467;&#26500;&#20351;&#29992;&#33192;&#32960;&#22240;&#26524;&#21367;&#31215;&#20197;&#35745;&#31639;&#39640;&#25928;&#24615;&#22320;&#22788;&#29702;&#22823;&#25509;&#21463;&#22495;&#65292;&#24182;&#21033;&#29992;&#21464;&#21387;&#22120;&#20307;&#31995;&#32467;&#26500;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#19982;&#35813;&#20219;&#21153;&#30340;&#20808;&#21069;&#27169;&#22411;&#30456;&#27604;&#65292;SI-SNRi&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;2.2-3.3 dB&#65292;&#21516;&#26102;&#27169;&#22411;&#22823;&#23567;&#20943;&#23567;&#20102;1.2-4&#20493;&#65292;&#24182;&#19988;&#36816;&#34892;&#26102;&#38388;&#38477;&#20302;&#20102;1.5-2&#20493;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20195;&#30721;&#12289;&#25968;&#25454;&#38598;&#21644;&#38899;&#39057;&#26679;&#26412;&#65306;https://waveformer.cs.washington.edu/&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the first neural network model to achieve real-time and streaming target sound extraction. To accomplish this, we propose Waveformer, an encoder-decoder architecture with a stack of dilated causal convolution layers as the encoder, and a transformer decoder layer as the decoder. This hybrid architecture uses dilated causal convolutions for processing large receptive fields in a computationally efficient manner while also leveraging the generalization performance of transformer-based architectures. Our evaluations show as much as 2.2-3.3 dB improvement in SI-SNRi compared to the prior models for this task while having a 1.2-4x smaller model size and a 1.5-2x lower runtime. We provide code, dataset, and audio samples: https://waveformer.cs.washington.edu/.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29992;&#28508;&#22312;&#35821;&#20041;&#20998;&#26512;&#12289;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#21644;&#32858;&#31867;&#35789;&#21521;&#37327;&#19977;&#31181;&#19981;&#21516;&#30340;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#33258;&#21160;&#25552;&#21462;&#35752;&#35770;&#29256;&#25968;&#25454;&#38598;&#20013;&#30340;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#33258;&#21160;&#21270;&#26041;&#27861;&#20063;&#26377;&#21161;&#20110;&#25552;&#21462;&#35752;&#35770;&#20195;&#30721;&#65292;&#24182;&#29992;&#20110;&#35748;&#35782;&#32593;&#32476;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2210.17495</link><description>&lt;p&gt;
&#35752;&#35770;&#29256;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#30340;&#33258;&#21160;&#21270;&#20195;&#30721;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Automated Code Extraction from Discussion Board Text Dataset. (arXiv:2210.17495v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29992;&#28508;&#22312;&#35821;&#20041;&#20998;&#26512;&#12289;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#21644;&#32858;&#31867;&#35789;&#21521;&#37327;&#19977;&#31181;&#19981;&#21516;&#30340;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#33258;&#21160;&#25552;&#21462;&#35752;&#35770;&#29256;&#25968;&#25454;&#38598;&#20013;&#30340;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#33258;&#21160;&#21270;&#26041;&#27861;&#20063;&#26377;&#21161;&#20110;&#25552;&#21462;&#35752;&#35770;&#20195;&#30721;&#65292;&#24182;&#29992;&#20110;&#35748;&#35782;&#32593;&#32476;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#24182;&#25506;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#65292;&#21363;&#28508;&#22312;&#35821;&#20041;&#20998;&#26512;&#12289;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#21644;&#32858;&#31867;&#35789;&#21521;&#37327;&#65292;&#29992;&#20110;&#33258;&#21160;&#25552;&#21462;&#30456;&#23545;&#36739;&#23567;&#30340;&#35752;&#35770;&#29256;&#25968;&#25454;&#38598;&#20013;&#30340;&#20195;&#30721;&#12290;&#25105;&#20204;&#23558;&#27599;&#31181;&#31639;&#27861;&#30340;&#36755;&#20986;&#19982;&#20808;&#21069;&#30001;&#20004;&#20010;&#20154;&#25163;&#21160;&#32534;&#30721;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#30456;&#23545;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#33258;&#21160;&#21270;&#26041;&#27861;&#20063;&#21487;&#20197;&#25104;&#20026;&#35838;&#31243;&#25945;&#24072;&#30340;&#26377;&#30410;&#24037;&#20855;&#65292;&#36890;&#36807;&#25552;&#21462;&#19968;&#20123;&#35752;&#35770;&#20195;&#30721;&#65292;&#21487;&#29992;&#20110;&#35748;&#35782;&#32593;&#32476;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces and investigates the capabilities of three different text mining approaches, namely Latent Semantic Analysis, Latent Dirichlet Analysis, and Clustering Word Vectors, for automating code extraction from a relatively small discussion board dataset. We compare the outputs of each algorithm with a previous dataset that was manually coded by two human raters. The results show that even with a relatively small dataset, automated approaches can be an asset to course instructors by extracting some of the discussion codes, which can be used in Epistemic Network Analysis.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;Lojasiewicz&#20989;&#25968;&#19978;&#65292;&#38543;&#26426;&#38646;&#38454;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20855;&#26377;&#25910;&#25947;&#36895;&#29575;&#65292;&#19988;&#27604; $\{ \|\mathbf{x}_t-\mathbf{x}_\infty\| \}_{t \in \mathbb{N}}$&#26356;&#24555;&#65292;&#26080;&#35770;$f$&#26159;&#24179;&#28369;&#36824;&#26159;&#38750;&#24179;&#28369;&#30340;&#12290;</title><link>http://arxiv.org/abs/2210.16997</link><description>&lt;p&gt;
&#38543;&#26426;&#38646;&#38454;&#26799;&#24230;&#19979;&#38477;&#22312;L-Lojasiewicz&#20989;&#25968;&#19978;&#30340;&#25910;&#25947;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Convergence Rates of Stochastic Zeroth-order Gradient Descent for \L ojasiewicz Functions. (arXiv:2210.16997v5 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16997
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;Lojasiewicz&#20989;&#25968;&#19978;&#65292;&#38543;&#26426;&#38646;&#38454;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20855;&#26377;&#25910;&#25947;&#36895;&#29575;&#65292;&#19988;&#27604; $\{ \|\mathbf{x}_t-\mathbf{x}_\infty\| \}_{t \in \mathbb{N}}$&#26356;&#24555;&#65292;&#26080;&#35770;$f$&#26159;&#24179;&#28369;&#36824;&#26159;&#38750;&#24179;&#28369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#38543;&#26426;&#38646;&#38454;&#26799;&#24230;&#19979;&#38477;&#65288;SZGD&#65289;&#31639;&#27861;&#22312;Lojasiewicz&#20989;&#25968;&#19978;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;SZGD&#31639;&#27861;&#36845;&#20195;&#22914;&#19979;&#65306;$ \mathbf{x}_{t+1} = \mathbf{x}_t - \eta_t \widehat{\nabla} f (\mathbf{x}_t) $&#65292;&#20854;&#20013;$f$&#26159;&#28385;&#36275;Lojasiewicz&#19981;&#31561;&#24335;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#20855;&#26377;Lojasiewicz&#25351;&#25968;$\theta$&#65292;$\eta_t$&#26159;&#27493;&#38271;&#65288;&#23398;&#20064;&#29575;&#65289;&#65292;$ \widehat{\nabla} f (\mathbf{x}_t)$&#26159;&#20351;&#29992;&#38646;&#38454;&#20449;&#24687;&#20272;&#35745;&#30340;&#36817;&#20284;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;$ \{ f (\mathbf{x}_t) - f (\mathbf{x}_\infty) \}_{t \in \mathbb{N} }$&#30340;&#25910;&#25947;&#36895;&#24230;&#21487;&#20197;&#27604; $ \{ \| \mathbf{x}_t \mathbf{x}_\infty \| \}_{t \in \mathbb{N} } $&#26356;&#24555;&#65292;&#26080;&#35770;&#30446;&#26631;$f$&#26159;&#24179;&#28369;&#36824;&#26159;&#38750;&#24179;&#28369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove convergence rates of Stochastic Zeroth-order Gradient Descent (SZGD) algorithms for Lojasiewicz functions. The SZGD algorithm iterates as \begin{align*}  \mathbf{x}_{t+1} = \mathbf{x}_t - \eta_t \widehat{\nabla} f (\mathbf{x}_t), \qquad t = 0,1,2,3,\cdots , \end{align*} where $f$ is the objective function that satisfies the \L ojasiewicz inequality with \L ojasiewicz exponent $\theta$, $\eta_t$ is the step size (learning rate), and $ \widehat{\nabla} f (\mathbf{x}_t) $ is the approximate gradient estimated using zeroth-order information only.  Our results show that $ \{ f (\mathbf{x}_t) - f (\mathbf{x}_\infty) \}_{t \in \mathbb{N} } $ can converge faster than $ \{ \| \mathbf{x}_t \mathbf{x}_\infty \| \}_{t \in \mathbb{N} }$, regardless of whether the objective $f$ is smooth or nonsmooth.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#24335;&#27880;&#24847;&#21147;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#21462;&#23454;&#38469;&#34013;&#29273;&#35774;&#22791;&#30340;&#25351;&#32441;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#26377;&#25152;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.02897</link><description>&lt;p&gt;
&#23884;&#20837;&#24335;&#27880;&#24847;&#21147;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#23454;&#38469;&#34013;&#29273;&#23556;&#39057;&#25351;&#32441;&#30340;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Embedding-Assisted Attentional Deep Learning for Real-World RF Fingerprinting of Bluetooth. (arXiv:2210.02897v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#24335;&#27880;&#24847;&#21147;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#21462;&#23454;&#38469;&#34013;&#29273;&#35774;&#22791;&#30340;&#25351;&#32441;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#26377;&#25152;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#21462;&#23454;&#38469;&#34013;&#29273;&#35774;&#22791;&#30340;&#25351;&#32441;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#24335;&#27880;&#24847;&#21147;&#26694;&#26550;&#65288;Mbed-ATN&#65289;&#65292;&#36866;&#29992;&#20110;&#33719;&#21462;&#23454;&#38469;&#34013;&#29273;&#35774;&#22791;&#30340;&#25351;&#32441;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20854;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#28436;&#31034;&#20102;&#26679;&#26412;&#38271;&#24230;&#21644;&#25239;&#28151;&#21472;&#25277;&#21462;&#30340;&#25928;&#26524;&#12290;&#23884;&#20837;&#24335;&#27169;&#22359;&#20316;&#20026;&#38477;&#32500;&#21333;&#20803;&#65292;&#23558;&#39640;&#32500;&#30340;&#19977;&#32500;&#36755;&#20837;&#24352;&#37327;&#26144;&#23556;&#20026;&#19968;&#32500;&#29305;&#24449;&#21521;&#37327;&#65292;&#20379;ATN&#27169;&#22359;&#36827;&#19968;&#27493;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#35813;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#36827;&#34892;&#20102;&#23494;&#20999;&#35780;&#20272;&#65292;&#24182;&#22312;&#35757;&#32451;&#21518;&#20351;&#29992;&#21478;&#19968;&#20010;&#22312;&#19981;&#21516;&#26102;&#27573;&#21644;&#23454;&#39564;&#35774;&#32622;&#19979;&#25910;&#38598;&#30340;&#30495;&#23454;&#34013;&#29273;&#25968;&#25454;&#38598;&#27979;&#35797;&#20102;&#20854;&#25351;&#32441;&#33021;&#21147;&#65292;&#36825;&#26159;&#35813;&#39046;&#22495;&#20808;&#21069;&#30740;&#31350;&#25152;&#27809;&#26377;&#20570;&#36807;&#30340;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26679;&#26412;&#38271;&#24230;&#20026;100 kS&#26102;&#65292;&#19982;&#22522;&#20934;&#8212;&#8212;GRU&#21644;Oracle&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20869;&#23384;&#20351;&#29992;&#37327;&#20998;&#21035;&#38477;&#20302;&#20102;9.17&#20493;&#21644;65.2&#20493;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;Mbed-ATN&#22312;&#23460;&#20869;&#21644;&#23460;&#22806;&#29615;&#22659;&#20013;&#22343;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#20998;&#21035;&#20026;99.4&#65285;&#21644;97.4&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
A scalable and computationally efficient framework is designed to fingerprint real-world Bluetooth devices. We propose an embedding-assisted attentional framework (Mbed-ATN) suitable for fingerprinting actual Bluetooth devices. Its generalization capability is analyzed in different settings and the effect of sample length and anti-aliasing decimation is demonstrated. The embedding module serves as a dimensionality reduction unit that maps the high dimensional 3D input tensor to a 1D feature vector for further processing by the ATN module. Furthermore, unlike the prior research in this field, we closely evaluate the complexity of the model and test its fingerprinting capability with real-world Bluetooth dataset collected under a different time frame and experimental setting while being trained on another. Our study reveals a 9.17x and 65.2x lesser memory usage at a sample length of 100 kS when compared to the benchmark - GRU and Oracle models respectively. Further, the proposed Mbed-ATN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22788;&#29702;&#27010;&#29575;&#23433;&#20840;&#32422;&#26463;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#26159;&#39318;&#20010;&#32473;&#20986;&#27010;&#29575;&#32422;&#26463;&#26799;&#24230;&#34920;&#36798;&#24335;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2210.00596</link><description>&lt;p&gt;
&#27010;&#29575;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Policy Gradients for Probabilistic Constrained Reinforcement Learning. (arXiv:2210.00596v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22788;&#29702;&#27010;&#29575;&#23433;&#20840;&#32422;&#26463;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#26159;&#39318;&#20010;&#32473;&#20986;&#27010;&#29575;&#32422;&#26463;&#26799;&#24230;&#34920;&#36798;&#24335;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#24773;&#22659;&#19979;&#23398;&#20064;&#23433;&#20840;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#27010;&#29575;&#23433;&#20840;&#30340;&#27010;&#24565;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#33021;&#22815;&#22312;&#39640;&#27010;&#29575;&#19979;&#23558;&#31995;&#32479;&#29366;&#24577;&#20445;&#25345;&#22312;&#23433;&#20840;&#38598;&#21512;&#20013;&#30340;&#31574;&#30053;&#12290;&#36825;&#19968;&#27010;&#24565;&#19981;&#21516;&#20110;&#25991;&#29486;&#20013;&#24120;&#32771;&#34385;&#30340;&#32047;&#31215;&#32422;&#26463;&#12290;&#22788;&#29702;&#27010;&#29575;&#23433;&#20840;&#30340;&#25361;&#25112;&#22312;&#20110;&#32570;&#20047;&#20854;&#26799;&#24230;&#30340;&#34920;&#36798;&#24335;&#12290;&#23454;&#38469;&#19978;&#65292;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#20381;&#36182;&#20110;&#30446;&#26631;&#20989;&#25968;&#21644;&#32422;&#26463;&#30340;&#26799;&#24230;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#39318;&#20010;&#26126;&#30830;&#32473;&#20986;&#27010;&#29575;&#32422;&#26463;&#26799;&#24230;&#34920;&#36798;&#24335;&#30340;&#24037;&#20316;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#32422;&#26463;&#30340;&#26799;&#24230;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#22522;&#20110;&#31574;&#30053;&#30340;&#31639;&#27861;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#19968;&#39033;&#36830;&#32493;&#23548;&#33322;&#38382;&#39064;&#20013;&#22788;&#29702;&#27010;&#29575;&#32422;&#26463;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the problem of learning safe policies in the context of reinforcement learning (RL). In particular, we consider the notion of probabilistic safety. This is, we aim to design policies that maintain the state of the system in a safe set with high probability. This notion differs from cumulative constraints often considered in the literature. The challenge of working with probabilistic safety is the lack of expressions for their gradients. Indeed, policy optimization algorithms rely on gradients of the objective function and the constraints. To the best of our knowledge, this work is the first one providing such explicit gradient expressions for probabilistic constraints. It is worth noting that the gradient of this family of constraints can be applied to various policy-based algorithms. We demonstrate empirically that it is possible to handle probabilistic constraints in a continuous navigation problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20027;&#35201;&#23637;&#31034;&#20102;AdaGrad&#22312;&#24179;&#28369;&#20984;&#20989;&#25968;&#21644;&#26356;&#19968;&#33324;&#30340;quasar&#20984;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#25216;&#26415;&#65292;&#26126;&#30830;&#38480;&#23450;&#20102;vanilla AdaGrad&#22312;&#26080;&#32422;&#26463;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;AdaGrad&#21464;&#31181;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2209.14827</link><description>&lt;p&gt;
&#35770;AdaGrad&#22312;$\R^{d}$&#19978;&#30340;&#25910;&#25947;&#24615;&#65306;&#36229;&#36234;&#20984;&#24615;&#12289;&#38750;&#28176;&#36817;&#36895;&#29575;&#21644;&#21152;&#36895;&#65288;arXiv&#65306;2209.14827v2 [cs.LG] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of AdaGrad on $\R^{d}$: Beyond Convexity, Non-Asymptotic Rate and Acceleration. (arXiv:2209.14827v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20027;&#35201;&#23637;&#31034;&#20102;AdaGrad&#22312;&#24179;&#28369;&#20984;&#20989;&#25968;&#21644;&#26356;&#19968;&#33324;&#30340;quasar&#20984;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#25216;&#26415;&#65292;&#26126;&#30830;&#38480;&#23450;&#20102;vanilla AdaGrad&#22312;&#26080;&#32422;&#26463;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;AdaGrad&#21464;&#31181;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20851;&#20110;&#24179;&#28369;&#20984;&#20248;&#21270;&#30340;AdaGrad&#21644;&#20854;&#20182;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#20998;&#26512;&#36890;&#24120;&#26159;&#38024;&#23545;&#20855;&#26377;&#26377;&#30028;&#23450;&#20041;&#22495;&#30452;&#24452;&#30340;&#20989;&#25968;&#12290;&#22312;&#26080;&#32422;&#26463;&#38382;&#39064;&#20013;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#20445;&#35777;&#20102;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#65292;&#20294;&#27809;&#26377;&#26126;&#30830;&#30340;&#24658;&#23450;&#22240;&#23376;&#65292;&#36825;&#36866;&#29992;&#20110;&#25972;&#20010;&#20989;&#25968;&#31867;&#12290;&#27492;&#22806;&#65292;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#65292;&#21482;&#20998;&#26512;&#20102;&#19968;&#20010;&#20462;&#25913;&#29256;&#26412;&#30340;AdaGrad&#65292;&#19982;&#36890;&#24120;&#23454;&#36341;&#20013;&#20351;&#29992;&#30340;&#29256;&#26412;&#19981;&#21516;&#65292;&#22312;&#36825;&#20010;&#22238;&#24402;&#20013;&#19981;&#20351;&#29992;&#26368;&#26032;&#30340;&#26799;&#24230;&#26469;&#26356;&#26032;&#27493;&#24133;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#26088;&#22312;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#65292;&#24182;&#22312;&#24179;&#28369;&#20984;&#20989;&#25968;&#30340;&#26631;&#20934;&#24773;&#20917;&#19979;&#20197;&#21450;&#26356;&#19968;&#33324;&#30340;quasar&#20984;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#28145;&#20837;&#29702;&#35299;AdaGrad&#21450;&#20854;&#21464;&#31181;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26032;&#25216;&#26415;&#65292;&#26126;&#30830;&#22320;&#38480;&#23450;&#20102;vanilla AdaGrad&#22312;&#26080;&#32422;&#26463;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#26080;&#35770;&#26159;&#30830;&#23450;&#24615;&#30340;&#36824;&#26159;&#38543;&#26426;&#30340;&#24773;&#20917;&#19979;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;AdaGrad&#21464;&#31181;&#65292;&#25105;&#20204;&#21487;&#20197;&#23637;&#31034;l&#30340;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Existing analysis of AdaGrad and other adaptive methods for smooth convex optimization is typically for functions with bounded domain diameter. In unconstrained problems, previous works guarantee an asymptotic convergence rate without an explicit constant factor that holds true for the entire function class. Furthermore, in the stochastic setting, only a modified version of AdaGrad, different from the one commonly used in practice, in which the latest gradient is not used to update the stepsize, has been analyzed. Our paper aims at bridging these gaps and developing a deeper understanding of AdaGrad and its variants in the standard setting of smooth convex functions as well as the more general setting of quasar convex functions. First, we demonstrate new techniques to explicitly bound the convergence rate of the vanilla AdaGrad for unconstrained problems in both deterministic and stochastic settings. Second, we propose a variant of AdaGrad for which we can show the convergence of the l
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#24046;&#20998;&#38544;&#31169;&#30340;&#20998;&#21306;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#26159;&#31532;&#19968;&#31181;&#22312;&#32852;&#37030;&#36125;&#21494;&#26031;&#23398;&#20064;&#29615;&#22659;&#19979;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.11595</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#30340;&#20998;&#21306;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially private partitioned variational inference. (arXiv:2209.11595v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#24046;&#20998;&#38544;&#31169;&#30340;&#20998;&#21306;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#26159;&#31532;&#19968;&#31181;&#22312;&#32852;&#37030;&#36125;&#21494;&#26031;&#23398;&#20064;&#29615;&#22659;&#19979;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20998;&#24067;&#22312;&#22810;&#20010;&#35774;&#22791;&#19978;&#30340;&#25935;&#24863;&#25968;&#25454;&#20013;&#23398;&#20064;&#38544;&#31169;&#20445;&#25252;&#27169;&#22411;&#26159;&#19968;&#20010;&#26085;&#30410;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#36890;&#24120;&#22312;&#32852;&#37030;&#23398;&#20064;&#32972;&#26223;&#19979;&#36827;&#34892;&#35268;&#21010;&#65292;&#30446;&#26631;&#26159;&#22312;&#20445;&#25345;&#25968;&#25454;&#20998;&#24067;&#30340;&#21516;&#26102;&#23398;&#20064;&#21333;&#20010;&#20840;&#23616;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36125;&#21494;&#26031;&#23398;&#20064;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#33258;&#28982;&#22320;&#25903;&#25345;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#38598;&#20013;&#30340;&#38750;&#38544;&#31169;&#25968;&#25454;&#65292;&#36125;&#21494;&#26031;&#23398;&#20064;&#20063;&#36890;&#24120;&#26159;&#19981;&#21487;&#25805;&#20316;&#30340;&#65292;&#22240;&#27492;&#21464;&#20998;&#25512;&#26029;&#31561;&#36817;&#20284;&#25216;&#26415;&#26159;&#24517;&#38656;&#30340;&#12290;&#36817;&#26399;&#65292;&#36890;&#36807;&#20998;&#21306;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#21464;&#20998;&#25512;&#26029;&#24050;&#32463;&#25193;&#23637;&#21040;&#38750;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#30340;&#24773;&#20917;&#12290;&#23545;&#20110;&#38544;&#31169;&#20445;&#25252;&#65292;&#30446;&#21069;&#30340;&#40644;&#37329;&#26631;&#20934;&#34987;&#31216;&#20026;&#24046;&#20998;&#38544;&#31169;&#12290;&#24046;&#20998;&#38544;&#31169;&#22312;&#25968;&#23398;&#19978;&#23450;&#20041;&#20102;&#19968;&#20010;&#24378;&#30340;&#38544;&#31169;&#20445;&#25252;&#27010;&#24565;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24046;&#20998;&#38544;&#31169;&#30340;&#20998;&#21306;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#26159;&#31532;&#19968;&#31181;&#36890;&#36807;&#20998;&#21306;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#22312;&#32852;&#37030;&#36125;&#21494;&#26031;&#23398;&#20064;&#29615;&#22659;&#19979;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20154;&#36896;&#21644;&#30495;&#23454;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning a privacy-preserving model from sensitive data which are distributed across multiple devices is an increasingly important problem. The problem is often formulated in the federated learning context, with the aim of learning a single global model while keeping the data distributed. Moreover, Bayesian learning is a popular approach for modelling, since it naturally supports reliable uncertainty estimates. However, Bayesian learning is generally intractable even with centralised non-private data and so approximation techniques such as variational inference are a necessity. Variational inference has recently been extended to the non-private federated learning setting via the partitioned variational inference algorithm. For privacy protection, the current gold standard is called differential privacy. Differential privacy guarantees privacy in a strong, mathematically clearly defined sense.  In this paper, we present differentially private partitioned variational inference, the first
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#27169;&#24335;&#26045;&#21152;&#31867;&#24863;&#30693;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#24635;&#28608;&#27963;&#20998;&#31867;&#22120;&#65288;TAC&#65289;&#21487;&#20197;&#35753;&#27169;&#22411;&#26356;&#21152;&#23433;&#20840;&#12289;&#21487;&#38752;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2208.14488</link><description>&lt;p&gt;
&#38480;&#21046;&#32593;&#32476;&#34920;&#31034;&#65292;&#35753;&#27169;&#22411;&#30693;&#36947;&#33258;&#24049;&#30340;&#19981;&#36275;
&lt;/p&gt;
&lt;p&gt;
Constraining Representations Yields Models That Know What They Don't Know. (arXiv:2208.14488v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14488
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#27169;&#24335;&#26045;&#21152;&#31867;&#24863;&#30693;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#24635;&#28608;&#27963;&#20998;&#31867;&#22120;&#65288;TAC&#65289;&#21487;&#20197;&#35753;&#27169;&#22411;&#26356;&#21152;&#23433;&#20840;&#12289;&#21487;&#38752;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#24050;&#30693;&#22833;&#36133;&#27169;&#24335;&#26159;&#23427;&#20204;&#21487;&#33021;&#33258;&#20449;&#22320;&#36820;&#22238;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#36825;&#31181;&#19981;&#23433;&#20840;&#30340;&#34892;&#20026;&#22312;&#20351;&#29992;&#26696;&#20363;&#30053;&#26377;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#29305;&#21035;&#39057;&#32321;&#65292;&#25110;&#32773;&#22312;&#38754;&#23545;&#25932;&#25163;&#26102;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20197;&#19968;&#31181;&#24191;&#27867;&#19988;&#19968;&#33324;&#30340;&#26041;&#24335;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65306;&#23545;&#27169;&#22411;&#20869;&#37096;&#30340;&#28608;&#27963;&#27169;&#24335;&#26045;&#21152;&#31867;&#24863;&#30693;&#32422;&#26463;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#31867;&#20998;&#37197;&#19968;&#20010;&#29420;&#29305;&#30340;&#12289;&#22266;&#23450;&#30340;&#12289;&#38543;&#26426;&#29983;&#25104;&#30340;&#20108;&#36827;&#21046;&#21521;&#37327;&#65288;&#21518;&#25991;&#31216;&#20026;&#31867;&#32534;&#30721;&#65289;&#65292;&#24182;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#20854;&#36890;&#36807;&#20132;&#21449;&#28145;&#24230;&#30340;&#28608;&#27963;&#27169;&#24335;&#26681;&#25454;&#36755;&#20837;&#26679;&#26412;&#30340;&#31867;&#21035;&#39044;&#27979;&#30456;&#24212;&#30340;&#31867;&#32534;&#30721;&#12290;&#32467;&#26524;&#39044;&#27979;&#22120;&#34987;&#31216;&#20026;&#24635;&#28608;&#27963;&#20998;&#31867;&#22120;&#65288;TAC&#65289;&#65292;TAC&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#20063;&#21487;&#20197;&#22312;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#39030;&#37096;&#29992;&#26497;&#23567;&#30340;&#20195;&#20215;&#20316;&#20026;&#34180;&#23618;&#38468;&#21152;&#20351;&#29992;&#12290;TAC&#30340;&#28608;&#27963;&#27169;&#24335;&#19982;&#26368;&#25509;&#36817;&#30340;&#26377;&#25928;&#32534;&#30721;&#20043;&#38388;&#30340;&#36317;&#31163;&#20316;&#20026;&#39069;&#22806;&#30340;&#32622;&#20449;&#24230;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
A well-known failure mode of neural networks is that they may confidently return erroneous predictions. Such unsafe behaviour is particularly frequent when the use case slightly differs from the training context, and/or in the presence of an adversary. This work presents a novel direction to address these issues in a broad, general manner: imposing class-aware constraints on a model's internal activation patterns. Specifically, we assign to each class a unique, fixed, randomly-generated binary vector - hereafter called class code and train the model so that its cross-depths activation patterns predict the appropriate class code according to the input sample's class. The resulting predictors are dubbed Total Activation Classifiers (TAC), and TACs may either be trained from scratch, or used with negligible cost as a thin add-on on top of a frozen, pre-trained neural network. The distance between a TAC's activation pattern and the closest valid code acts as an additional confidence scor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#19982;&#31639;&#27861;&#21644;&#20998;&#24067;&#26080;&#20851;&#30340;&#38750;&#28176;&#36827;&#24615;&#26435;&#34913;&#26041;&#27861;&#65292;&#26469;&#34913;&#37327;&#32447;&#24615;&#39044;&#27979;&#22120;&#30340;&#27169;&#22411;&#22823;&#23567;&#12289;&#27979;&#35797;&#25439;&#22833;&#21644;&#35757;&#32451;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#27979;&#35797;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#33394;&#30340;&#27169;&#22411;&#35201;&#20040;&#26159;&#32463;&#20856;&#30340;&#65292;&#35201;&#20040;&#26159;&#29616;&#20195;&#30340;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#24403;&#30333;&#21270;&#29305;&#24449;&#30340;&#26497;&#38480;&#35889;&#20998;&#24067;&#20026;Marchenko-Pastur&#26102;&#30340;&#26356;&#20026;&#31934;&#30830;&#30340;&#28176;&#36827;&#20998;&#26512;&#65292;&#20351;&#24471;&#20998;&#26512;&#26356;&#21152;&#31934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2207.11621</link><description>&lt;p&gt;
&#32447;&#24615;&#39044;&#27979;&#22120;&#30340;&#27169;&#22411;&#22823;&#23567;&#12289;&#27979;&#35797;&#25439;&#22833;&#21644;&#35757;&#32451;&#25439;&#22833;&#20043;&#38388;&#30340;&#36890;&#29992;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
A Universal Trade-off Between the Model Size, Test Loss, and Training Loss of Linear Predictors. (arXiv:2207.11621v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#19982;&#31639;&#27861;&#21644;&#20998;&#24067;&#26080;&#20851;&#30340;&#38750;&#28176;&#36827;&#24615;&#26435;&#34913;&#26041;&#27861;&#65292;&#26469;&#34913;&#37327;&#32447;&#24615;&#39044;&#27979;&#22120;&#30340;&#27169;&#22411;&#22823;&#23567;&#12289;&#27979;&#35797;&#25439;&#22833;&#21644;&#35757;&#32451;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#27979;&#35797;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#33394;&#30340;&#27169;&#22411;&#35201;&#20040;&#26159;&#32463;&#20856;&#30340;&#65292;&#35201;&#20040;&#26159;&#29616;&#20195;&#30340;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#24403;&#30333;&#21270;&#29305;&#24449;&#30340;&#26497;&#38480;&#35889;&#20998;&#24067;&#20026;Marchenko-Pastur&#26102;&#30340;&#26356;&#20026;&#31934;&#30830;&#30340;&#28176;&#36827;&#20998;&#26512;&#65292;&#20351;&#24471;&#20998;&#26512;&#26356;&#21152;&#31934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#19982;&#31639;&#27861;&#21644;&#20998;&#24067;&#26080;&#20851;&#30340;&#38750;&#28176;&#36827;&#24615;&#26435;&#34913;&#26041;&#27861;&#65292;&#26469;&#34913;&#37327;&#32447;&#24615;&#39044;&#27979;&#22120;&#30340;&#27169;&#22411;&#22823;&#23567;&#12289;&#36229;&#39069;&#27979;&#35797;&#25439;&#22833;&#21644;&#35757;&#32451;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#33394;&#30340;&#27169;&#22411;&#35201;&#20040;&#26159;&#8220;&#32463;&#20856;&#8221;&#30340;&#8212;&#8212;&#20854;&#35757;&#32451;&#25439;&#22833;&#25509;&#36817;&#22122;&#22768;&#27700;&#24179;&#65292;&#35201;&#20040;&#26159;&#8220;&#29616;&#20195;&#30340;&#8221;&#8212;&#8212;&#20854;&#21442;&#25968;&#25968;&#37327;&#36828;&#36828;&#36229;&#36807;&#20165;&#20165;&#33021;&#31934;&#30830;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#25152;&#38656;&#30340;&#26368;&#23567;&#21442;&#25968;&#25968;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#24403;&#30333;&#21270;&#29305;&#24449;&#30340;&#26497;&#38480;&#35889;&#20998;&#24067;&#20026;Marchenko-Pastur&#26102;&#30340;&#26356;&#20026;&#31934;&#30830;&#30340;&#28176;&#36827;&#20998;&#26512;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#25554;&#20540;&#39030;&#28857;&#38468;&#36817;&#65292;&#21363;&#21442;&#25968;&#25968;&#37327;&#21018;&#22909;&#36275;&#20197;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;Marchenko-Pastur&#20998;&#26512;&#26356;&#21152;&#31934;&#30830;&#65292;&#32780;&#38543;&#30528;&#36807;&#24230;&#21442;&#25968;&#21270;&#31243;&#24230;&#30340;&#22686;&#21152;&#65292;&#23427;&#24688;&#22909;&#19982;&#26080;&#20998;&#24067;&#38480;&#21046;&#30340;&#29702;&#35770;&#19978;&#30028;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we establish an algorithm and distribution independent non-asymptotic trade-off between the model size, excess test loss, and training loss of linear predictors. Specifically, we show that models that perform well on the test data (have low excess loss) are either "classical" -- have training loss close to the noise level, or are "modern" -- have a much larger number of parameters compared to the minimum needed to fit the training data exactly.  We also provide a more precise asymptotic analysis when the limiting spectral distribution of the whitened features is Marchenko-Pastur. Remarkably, while the Marchenko-Pastur analysis is far more precise near the interpolation peak, where the number of parameters is just enough to fit the training data, it coincides exactly with the distribution independent bound as the level of overparametrization increases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#19982;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#20195;&#30721;&#30340;&#35821;&#20041;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20195;&#30721;&#32763;&#35793;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.03578</link><description>&lt;p&gt;
&#21033;&#29992;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#20195;&#30721;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Code Translation with Compiler Representations. (arXiv:2207.03578v4 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#19982;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#20195;&#30721;&#30340;&#35821;&#20041;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20195;&#30721;&#32763;&#35793;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20302;&#32423;&#21035;&#30340;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#65288;IR&#65289;&#26469;&#25913;&#36827;&#20195;&#30721;&#32763;&#35793;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#21644;IR&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#20195;&#30721;&#30340;&#35821;&#20041;&#65292;&#36991;&#20813;&#20197;&#24448;&#26041;&#27861;&#23384;&#22312;&#30340;&#24120;&#35265;&#38169;&#35823;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32763;&#35793;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we leverage low-level compiler intermediate representations (IR) to improve code translation. Traditional transpilers rely on syntactic information and handcrafted rules, which limits their applicability and produces unnatural-looking code. Applying neural machine translation (NMT) approaches to code has successfully broadened the set of programs on which one can get a natural-looking translation. However, they treat the code as sequences of text tokens, and still do not differentiate well enough between similar pieces of code which have different semantics in different languages. The consequence is low quality translation, reducing the practicality of NMT, and stressing the need for an approach significantly increasing its accuracy. Here we propose to augment code translation with IRs, specifically LLVM IR, with results on the C++, Java, Rust, and Go languages. Our method improves upon the state of the art for unsupervised code translation, increasing the number of corr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;RL&#31639;&#27861;PARTED&#65292;&#23427;&#36890;&#36807;&#22522;&#20110;&#26368;&#23567;&#20108;&#20056;&#30340;&#22870;&#21169;&#37325;&#26032;&#20998;&#37197;&#23558;&#36712;&#36857;&#22238;&#25253;&#20998;&#35299;&#20026;&#27599;&#27493;&#20195;&#29702;&#22870;&#21169;&#65292;&#28982;&#21518;&#22522;&#20110;&#23398;&#20064;&#30340;&#20195;&#29702;&#22870;&#21169;&#25191;&#34892;&#24754;&#35266;&#20540;&#36845;&#20195;&#65292;&#29992;&#20110;&#35299;&#20915;&#36712;&#36857;&#22870;&#21169;&#38590;&#20197;&#24456;&#22909;&#22320;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.06426</link><description>&lt;p&gt;
&#24102;&#36712;&#36857;&#22870;&#21169;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#35777;&#25928;&#29575;&#24615;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Offline Reinforcement Learning with Trajectory-Wise Reward. (arXiv:2206.06426v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;RL&#31639;&#27861;PARTED&#65292;&#23427;&#36890;&#36807;&#22522;&#20110;&#26368;&#23567;&#20108;&#20056;&#30340;&#22870;&#21169;&#37325;&#26032;&#20998;&#37197;&#23558;&#36712;&#36857;&#22238;&#25253;&#20998;&#35299;&#20026;&#27599;&#27493;&#20195;&#29702;&#22870;&#21169;&#65292;&#28982;&#21518;&#22522;&#20110;&#23398;&#20064;&#30340;&#20195;&#29702;&#22870;&#21169;&#25191;&#34892;&#24754;&#35266;&#20540;&#36845;&#20195;&#65292;&#29992;&#20110;&#35299;&#20915;&#36712;&#36857;&#22870;&#21169;&#38590;&#20197;&#24456;&#22909;&#22320;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26174;&#33879;&#25104;&#21151;&#20005;&#37325;&#20381;&#36182;&#20110;&#35266;&#27979;&#27599;&#20010;&#35775;&#38382;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#22870;&#21169;&#12290;&#28982;&#32780;&#22312;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#20195;&#29702;&#21482;&#33021;&#35266;&#23519;&#34920;&#31034;&#25972;&#20010;&#36712;&#36857;&#36136;&#37327;&#30340;&#24471;&#20998;&#65292;&#31216;&#20026;"&#36712;&#36857;&#22870;&#21169;"&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26631;&#20934;&#30340;RL&#26041;&#27861;&#24456;&#38590;&#24456;&#22909;&#22320;&#21033;&#29992;&#36712;&#36857;&#22870;&#21169;&#65292;&#24182;&#19988;&#21487;&#33021;&#20250;&#22312;&#31574;&#30053;&#35780;&#20272;&#20013;&#20135;&#29983;&#22823;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#35823;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;RL&#31639;&#27861;PARTED&#65292;&#23427;&#36890;&#36807;&#22522;&#20110;&#26368;&#23567;&#20108;&#20056;&#30340;&#22870;&#21169;&#37325;&#26032;&#20998;&#37197;&#23558;&#36712;&#36857;&#22238;&#25253;&#20998;&#35299;&#20026;&#27599;&#27493;&#20195;&#29702;&#22870;&#21169;&#65292;&#28982;&#21518;&#22522;&#20110;&#23398;&#20064;&#30340;&#20195;&#29702;&#22870;&#21169;&#25191;&#34892;&#24754;&#35266;&#20540;&#36845;&#20195;&#12290;&#20026;&#20102;&#30830;&#20445;PARTED&#26500;&#24314;&#30340;&#20540;&#20989;&#25968;&#22987;&#32456;&#23545;&#26368;&#20248;&#20540;&#20989;&#25968;&#24754;&#35266;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#24809;&#32602;&#39033;&#26469;&#25269;&#28040;&#20195;&#29702;&#22870;&#21169;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable success of reinforcement learning (RL) heavily relies on observing the reward of every visited state-action pair. In many real world applications, however, an agent can observe only a score that represents the quality of the whole trajectory, which is referred to as the {\em trajectory-wise reward}. In such a situation, it is difficult for standard RL methods to well utilize trajectory-wise reward, and large bias and variance errors can be incurred in policy evaluation. In this work, we propose a novel offline RL algorithm, called Pessimistic vAlue iteRaTion with rEward Decomposition (PARTED), which decomposes the trajectory return into per-step proxy rewards via least-squares-based reward redistribution, and then performs pessimistic value iteration based on the learned proxy reward. To ensure the value functions constructed by PARTED are always pessimistic with respect to the optimal ones, we design a new penalty term to offset the uncertainty of the proxy reward. For 
&lt;/p&gt;</description></item><item><title>&#25688;&#35201;&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HiLo&#27880;&#24847;&#21147;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20351;&#29992;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#22312;&#27880;&#24847;&#21147;&#36716;&#25442;&#22120;&#20013;&#20998;&#35299;&#39640;/&#20302;&#39057;&#27169;&#24335;&#65292;&#21487;&#20197;&#26356;&#21152;&#39640;&#25928;&#22320;&#36816;&#34892;&#35270;&#35273;Transformer&#65292;&#24182;&#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#33539;&#22260;&#20869;&#32988;&#36807;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2205.13213</link><description>&lt;p&gt;
&#39640;&#20302;&#27880;&#24847;&#21147;&#30340;&#24555;&#36895;&#35270;&#35273;Transformer
&lt;/p&gt;
&lt;p&gt;
Fast Vision Transformers with HiLo Attention. (arXiv:2205.13213v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13213
&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HiLo&#27880;&#24847;&#21147;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20351;&#29992;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#22312;&#27880;&#24847;&#21147;&#36716;&#25442;&#22120;&#20013;&#20998;&#35299;&#39640;/&#20302;&#39057;&#27169;&#24335;&#65292;&#21487;&#20197;&#26356;&#21152;&#39640;&#25928;&#22320;&#36816;&#34892;&#35270;&#35273;Transformer&#65292;&#24182;&#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#33539;&#22260;&#20869;&#32988;&#36807;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer (ViT) &#24050;&#32463;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24341;&#36215;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#23427;&#20204;&#30340;&#39640;&#25928;&#35774;&#35745;&#22823;&#22810;&#30001;&#35745;&#31639;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#38388;&#25509;&#25351;&#26631;&#65288;&#21363; FLOP&#65289;&#25351;&#23548;&#65292;&#32780;&#19982;&#30452;&#25509;&#25351;&#26631;&#65288;&#22914;&#21534;&#21520;&#37327;&#65289;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#30446;&#26631;&#24179;&#21488;&#19978;&#30340;&#30452;&#25509;&#36895;&#24230;&#35780;&#20272;&#20316;&#20026;&#39640;&#25928;ViTs&#30340;&#35774;&#35745;&#21407;&#21017;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LITv2&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;ViT&#65292;&#23427;&#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#33539;&#22260;&#20869;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20248;&#24322;&#65292;&#36895;&#24230;&#26356;&#24555;&#12290;LITv2&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#39640;&#20302;&#27880;&#24847;&#21147;&#8221;&#12290;&#39640;&#20302;&#27880;&#24847;&#21147;&#30340;&#28789;&#24863;&#26469;&#33258;&#20110;&#22270;&#20687;&#20013;&#30340;&#39640;&#39057;&#25429;&#25417;&#23616;&#37096;&#32454;&#33410;&#65292;&#20302;&#39057;&#19987;&#27880;&#20110;&#20840;&#23616;&#32467;&#26500;&#65292;&#32780;&#22810;&#22836;&#33258;&#27880;&#24847;&#23618;&#24573;&#30053;&#20102;&#19981;&#21516;&#39057;&#29575;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#22312;&#27880;&#24847;&#21147;&#36716;&#25442;&#22120;&#20013;&#20998;&#35299;&#39640;/&#20302;&#39057;&#27169;&#24335;&#65292;&#20854;&#20013;&#33258;&#27880;&#24847;&#23618;&#20998;&#20026;&#20004;&#20010;&#20998;&#25903;&#65292;&#27599;&#20010;&#20998;&#25903;&#19987;&#38376;&#25429;&#25417;&#23616;&#37096;&#25110;&#20840;&#23616;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have triggered the most recent and significant breakthroughs in computer vision. Their efficient designs are mostly guided by the indirect metric of computational complexity, i.e., FLOPs, which however has a clear gap with the direct metric such as throughput. Thus, we propose to use the direct speed evaluation on the target platform as the design principle for efficient ViTs. Particularly, we introduce LITv2, a simple and effective ViT which performs favourably against the existing state-of-the-art methods across a spectrum of different model sizes with faster speed. At the core of LITv2 is a novel self-attention mechanism, which we dub HiLo. HiLo is inspired by the insight that high frequencies in an image capture local fine details and low frequencies focus on global structures, whereas a multi-head self-attention layer neglects the characteristic of different frequencies. Therefore, we propose to disentangle the high/low frequency patterns in an attention
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23485;&#27867;&#25512;&#33616;&#31995;&#32479;(BroadCF)&#65292;&#20351;&#29992;&#23485;&#27867;&#23398;&#20064;&#31995;&#32479;(BLS)&#20316;&#20026;&#26144;&#23556;&#20989;&#25968;&#26469;&#23398;&#20064;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#21516;&#26102;&#36890;&#36807;&#29992;&#25143;-&#39033;&#35780;&#32423;&#21327;&#21516;&#21521;&#37327;&#39044;&#22788;&#29702;&#31243;&#24207;&#23558;&#21407;&#22987;&#25968;&#25454;&#36716;&#25442;&#20026;&#26356;&#36866;&#21512;BLS&#23398;&#20064;&#30340;&#26684;&#24335;&#12290;BroadCF&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#29992;&#25143;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#37117;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;CF&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.11602</link><description>&lt;p&gt;
&#23485;&#27867;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#31181;&#39640;&#25928;&#30340;&#38750;&#32447;&#24615;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Broad Recommender System: An Efficient Nonlinear Collaborative Filtering Approach. (arXiv:2204.11602v4 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23485;&#27867;&#25512;&#33616;&#31995;&#32479;(BroadCF)&#65292;&#20351;&#29992;&#23485;&#27867;&#23398;&#20064;&#31995;&#32479;(BLS)&#20316;&#20026;&#26144;&#23556;&#20989;&#25968;&#26469;&#23398;&#20064;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#21516;&#26102;&#36890;&#36807;&#29992;&#25143;-&#39033;&#35780;&#32423;&#21327;&#21516;&#21521;&#37327;&#39044;&#22788;&#29702;&#31243;&#24207;&#23558;&#21407;&#22987;&#25968;&#25454;&#36716;&#25442;&#20026;&#26356;&#36866;&#21512;BLS&#23398;&#20064;&#30340;&#26684;&#24335;&#12290;BroadCF&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#29992;&#25143;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#37117;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;CF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#34987;&#24191;&#27867;&#24341;&#20837;&#21040;&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#20013;&#65292;&#20197;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#25512;&#33616;&#32467;&#26524;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#25429;&#33719;&#39033;&#30446;&#21644;&#29992;&#25143;&#20043;&#38388;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;DNN&#30340;&#27169;&#22411;&#36890;&#24120;&#36973;&#21463;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#38382;&#39064;&#65292;&#21363;&#28040;&#32791;&#38750;&#24120;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#24182;&#23384;&#20648;&#22823;&#37327;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23485;&#27867;&#25512;&#33616;&#31995;&#32479;&#65292;&#31216;&#20026;&#23485;&#27867;&#21327;&#21516;&#36807;&#28388;&#65288;BroadCF&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#38750;&#32447;&#24615;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#12290;&#23485;&#27867;&#23398;&#20064;&#31995;&#32479;&#65288;BLS&#65289;&#34987;&#29992;&#20316;&#26144;&#23556;&#20989;&#25968;&#65292;&#20197;&#23398;&#20064;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#21487;&#20197;&#36991;&#20813;&#19978;&#36848;&#38382;&#39064;&#65292;&#21516;&#26102;&#23454;&#29616;&#38750;&#24120;&#20196;&#20154;&#28385;&#24847;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#23558;&#21407;&#22987;&#35780;&#20998;&#25968;&#25454;&#30452;&#25509;&#39304;&#36865;&#21040;BLS&#20013;&#24182;&#19981;&#21487;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25143;-&#39033;&#35780;&#32423;&#21327;&#21516;&#21521;&#37327;&#39044;&#22788;&#29702;&#31243;&#24207;&#65292;&#23558;&#21407;&#22987;&#25968;&#25454;&#36716;&#25442;&#20026;&#26356;&#36866;&#21512;BLS&#23398;&#20064;&#30340;&#26684;&#24335;&#12290;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;BroadCF&#22312;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;CF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Deep Neural Networks (DNNs) have been widely introduced into Collaborative Filtering (CF) to produce more accurate recommendation results due to their capability of capturing the complex nonlinear relationships between items and users.However, the DNNs-based models usually suffer from high computational complexity, i.e., consuming very long training time and storing huge amount of trainable parameters. To address these problems, we propose a new broad recommender system called Broad Collaborative Filtering (BroadCF), which is an efficient nonlinear collaborative filtering approach. Instead of DNNs, Broad Learning System (BLS) is used as a mapping function to learn the complex nonlinear relationships between users and items, which can avoid the above issues while achieving very satisfactory recommendation performance. However, it is not feasible to directly feed the original rating data into BLS. To this end, we propose a user-item rating collaborative vector preprocessing pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32500;&#24230;&#25193;&#23637;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#36127;&#36733;&#30417;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;5&#20010;&#19981;&#21516;&#30340;&#20302;&#39057;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#35745;&#31639;&#12290;&#30740;&#31350;&#34920;&#26126;&#22312;&#36328;&#25968;&#25454;&#38598;&#20869;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#26041;&#38754;&#20063;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2204.02802</link><description>&lt;p&gt;
&#24212;&#29992;&#32500;&#24230;&#25193;&#23637;&#21644;&#36801;&#31227;&#23398;&#20064;&#20110;EMS&#36127;&#33655;&#30417;&#27979;&#20013;
&lt;/p&gt;
&lt;p&gt;
Dimensionality Expansion of Load Monitoring Time Series and Transfer Learning for EMS. (arXiv:2204.02802v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.02802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32500;&#24230;&#25193;&#23637;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#36127;&#36733;&#30417;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;5&#20010;&#19981;&#21516;&#30340;&#20302;&#39057;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#35745;&#31639;&#12290;&#30740;&#31350;&#34920;&#26126;&#22312;&#36328;&#25968;&#25454;&#38598;&#20869;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#26041;&#38754;&#20063;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#65288;EMS&#65289;&#36890;&#36807;&#65288;&#38750;&#65289;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979;&#65288;(N)ILM&#65289;&#30417;&#25511;&#21644;&#31649;&#29702;&#30005;&#22120;&#65292;&#24110;&#21161;&#23621;&#27665;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#20197;&#21450;&#33410;&#30465;&#29992;&#30005;&#36153;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#32500;&#24230;&#25193;&#23637;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#36127;&#36733;&#30417;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;5&#20010;&#19981;&#21516;&#30340;&#20302;&#39057;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#24182;&#20351;&#29992;&#31867;&#20284;&#35270;&#39057;&#30340;&#36716;&#25442;&#21644;&#36164;&#28304;&#24863;&#30693;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#25552;&#20986;&#20102;&#29305;&#24449;&#32500;&#24230;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#26041;&#27861;&#22312;29&#31181;&#30005;&#22120;&#30340;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#24179;&#22343;&#21152;&#26435;F1&#24471;&#20998;0.88&#65292;&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#30340;&#25104;&#20687;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#22312;&#36328;&#25968;&#25454;&#38598;&#20869;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy management systems (EMS) rely on (non)-intrusive load monitoring (N)ILM to monitor and manage appliances and help residents be more energy efficient and thus more frugal. The robustness as well as the transfer potential of the most promising machine learning solutions for (N)ILM is not yet fully understood as they are trained and evaluated on relatively limited data. In this paper, we propose a new approach for load monitoring in building EMS based on dimensionality expansion of time series and transfer learning. We perform an extensive evaluation on 5 different low-frequency datasets. The proposed feature dimensionality expansion using video-like transformation and resource-aware deep learning architecture achieves an average weighted F1 score of 0.88 across the datasets with 29 appliances and is computationally more efficient compared to the state-of-the-art imaging methods. Investigating the proposed method for cross-dataset intra-domain transfer learning, we find that 1) our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#19982;&#26368;&#26032;&#31639;&#27861;&#31361;&#30772;&#65292;&#31163;&#32447;RL&#31639;&#27861;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#25945;&#32946;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#26426;&#22120;&#20154;&#31561;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2203.01387</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#32508;&#36848;&#65306;&#20998;&#31867;&#12289;&#22238;&#39038;&#21644;&#26410;&#35299;&#20915;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems. (arXiv:2203.01387v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#19982;&#26368;&#26032;&#31639;&#27861;&#31361;&#30772;&#65292;&#31163;&#32447;RL&#31639;&#27861;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#25945;&#32946;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#26426;&#22120;&#20154;&#31561;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#35299;&#20915;&#20197;&#24448;&#26080;&#27861;&#22788;&#29702;&#30340;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22914;&#20174;&#20687;&#32032;&#35266;&#23519;&#20013;&#29609;&#22797;&#26434;&#28216;&#25103;&#12289;&#19982;&#20154;&#31867;&#36827;&#34892;&#23545;&#35805;&#20197;&#21450;&#25511;&#21046;&#26426;&#22120;&#20154;&#26234;&#33021;&#20307;&#12290;&#28982;&#32780;&#65292;&#20173;&#26377;&#35768;&#22810;&#39046;&#22495;&#30001;&#20110;&#19982;&#29615;&#22659;&#20114;&#21160;&#30340;&#39640;&#25104;&#26412;&#21644;&#21361;&#38505;&#32780;&#26080;&#27861;&#29992;RL&#35299;&#20915;&#12290;&#31163;&#32447;RL&#26159;&#19968;&#31181;&#33539;&#24335;&#65292;&#23427;&#20165;&#20174;&#20197;&#21069;&#25910;&#38598;&#30340;&#20132;&#20114;&#30340;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#65292;&#22240;&#27492;&#21487;&#20197;&#20174;&#22823;&#22411;&#21644;&#22810;&#26679;&#21270;&#30340;&#22521;&#35757;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#31574;&#30053;&#12290;&#26377;&#25928;&#30340;&#31163;&#32447;RL&#31639;&#27861;&#27604;&#22312;&#32447;RL&#31639;&#27861;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#25945;&#32946;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#26426;&#22120;&#20154;&#31561;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#31163;&#32447;RL&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#35813;&#39046;&#22495;&#26368;&#26032;&#30340;&#31639;&#27861;&#31361;&#30772;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread adoption of deep learning, reinforcement learning (RL) has experienced a dramatic increase in popularity, scaling to previously intractable problems, such as playing complex games from pixel observations, sustaining conversations with humans, and controlling robotic agents. However, there is still a wide range of domains inaccessible to RL due to the high cost and danger of interacting with the environment. Offline RL is a paradigm that learns exclusively from static datasets of previously collected interactions, making it feasible to extract policies from large and diverse training datasets. Effective offline RL algorithms have a much wider range of applications than online RL, being particularly appealing for real-world applications, such as education, healthcare, and robotics. In this work, we contribute with a unifying taxonomy to classify offline RL methods. Furthermore, we provide a comprehensive review of the latest algorithmic breakthroughs in the field usin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#32463;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#24471;&#21040;&#30340;&#32437;&#21521;&#29992;&#25143;&#25968;&#25454;&#65292;&#36890;&#36807; Z-&#20272;&#35745;&#22120;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#65292;&#21253;&#25324;&#24341;&#20837;&#26657;&#27491;&#22841;&#24515;&#26041;&#24046;&#20272;&#35745;&#22120;&#31561;&#12290;&#36890;&#36807;&#27719;&#38598;&#29992;&#25143;&#25968;&#25454;&#65292;&#33258;&#36866;&#24212;&#37319;&#26679;&#31639;&#27861;&#33021;&#22815;&#28508;&#22312;&#22320;&#26356;&#24555;&#22320;&#23398;&#20064;&#65292;&#20294;&#21516;&#26102;&#20063;&#24341;&#20837;&#20102;&#20381;&#36182;&#20851;&#31995;&#23548;&#33268;&#26041;&#24046;&#20272;&#35745;&#20302;&#20272;&#65292;&#22240;&#27492;&#38656;&#35201;&#37319;&#29992;&#26032;&#30340;&#26041;&#27861;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2202.07098</link><description>&lt;p&gt;
&#32463;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#32437;&#21521;&#25968;&#25454;&#30340;&#32479;&#35745;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Statistical Inference After Adaptive Sampling for Longitudinal Data. (arXiv:2202.07098v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#32463;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#24471;&#21040;&#30340;&#32437;&#21521;&#29992;&#25143;&#25968;&#25454;&#65292;&#36890;&#36807; Z-&#20272;&#35745;&#22120;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#65292;&#21253;&#25324;&#24341;&#20837;&#26657;&#27491;&#22841;&#24515;&#26041;&#24046;&#20272;&#35745;&#22120;&#31561;&#12290;&#36890;&#36807;&#27719;&#38598;&#29992;&#25143;&#25968;&#25454;&#65292;&#33258;&#36866;&#24212;&#37319;&#26679;&#31639;&#27861;&#33021;&#22815;&#28508;&#22312;&#22320;&#26356;&#24555;&#22320;&#23398;&#20064;&#65292;&#20294;&#21516;&#26102;&#20063;&#24341;&#20837;&#20102;&#20381;&#36182;&#20851;&#31995;&#23548;&#33268;&#26041;&#24046;&#20272;&#35745;&#20302;&#20272;&#65292;&#22240;&#27492;&#38656;&#35201;&#37319;&#29992;&#26032;&#30340;&#26041;&#27861;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#24178;&#39044;&#23454;&#39564;&#20013;&#65292;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#20854;&#20182;&#33258;&#36866;&#24212;&#37319;&#26679;&#31639;&#27861;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#20248;&#21270;&#29992;&#25143;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#30001;&#22823;&#37327;&#33258;&#36866;&#24212;&#37319;&#26679;&#31639;&#27861;&#35774;&#35745;&#29992;&#20110;&#21033;&#29992;&#22810;&#20010;&#29992;&#25143;&#30340;&#19981;&#26029;&#32047;&#21152;&#25968;&#25454;&#22312;&#32447;&#20248;&#21270;&#27835;&#30103;&#20915;&#31574;&#30340;&#32437;&#21521;&#29992;&#25143;&#25968;&#25454;&#12290;&#36890;&#36807;&#21512;&#24182;&#25110;"&#27719;&#38598;"&#29992;&#25143;&#25968;&#25454;&#65292;&#33258;&#36866;&#24212;&#37319;&#26679;&#31639;&#27861;&#33021;&#22815;&#28508;&#22312;&#22320;&#26356;&#24555;&#22320;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#27719;&#38598;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#26679;&#26412;&#29992;&#25143;&#25968;&#25454;&#36712;&#36857;&#20043;&#38388;&#24341;&#20837;&#20102;&#20381;&#36182;&#20851;&#31995;&#65307;&#25105;&#20204;&#34920;&#26126;&#36825;&#20250;&#23548;&#33268;&#23545;&#20110;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#26631;&#20934;&#26041;&#24046;&#20272;&#35745;&#20302;&#20272;&#35813;&#25968;&#25454;&#31867;&#22411;&#19978;&#24120;&#35265;&#20272;&#35745;&#37327;&#30340;&#30495;&#23454;&#26041;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;Z-&#20272;&#35745;&#22120;&#24320;&#21457;&#20986;&#26032;&#30340;&#26041;&#27861;&#26469;&#23545;&#36825;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#25968;&#25454;&#36827;&#34892;&#21508;&#31181;&#32479;&#35745;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;"&#33258;&#36866;&#24212;"&#22841;&#24515;&#26041;&#24046;&#20272;&#35745;&#22120;&#8212;&#8212;&#19968;&#31181;&#26657;&#27491;&#22841;&#24515;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#24471;&#21040;&#19968;&#33268;&#30340;&#26041;&#24046;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online reinforcement learning and other adaptive sampling algorithms are increasingly used in digital intervention experiments to optimize treatment delivery for users over time. In this work, we focus on longitudinal user data collected by a large class of adaptive sampling algorithms that are designed to optimize treatment decisions online using accruing data from multiple users. Combining or "pooling" data across users allows adaptive sampling algorithms to potentially learn faster. However, by pooling, these algorithms induce dependence between the sampled user data trajectories; we show that this can cause standard variance estimators for i.i.d. data to underestimate the true variance of common estimators on this data type. We develop novel methods to perform a variety of statistical analyses on such adaptively sampled data via Z-estimation. Specifically, we introduce the \textit{adaptive} sandwich variance estimator, a corrected sandwich estimator that leads to consistent varianc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37319;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20196;&#24211;&#23384;&#31649;&#29702;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#20915;&#31574;&#22914;&#20309;&#37325;&#26032;&#34917;&#20805;&#24211;&#23384;&#65292;&#29305;&#21035;&#26159;&#22312;&#32771;&#34385;&#22810;&#20010;&#20379;&#24212;&#21830;&#35746;&#21333;&#21644;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#24555;&#36895;&#35843;&#25972;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2201.06126</link><description>&lt;p&gt;
&#37319;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#21452;&#37325;&#20379;&#24212;&#38142;&#24211;&#23384;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Control of Dual-Sourcing Inventory Systems using Recurrent Neural Networks. (arXiv:2201.06126v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.06126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37319;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20196;&#24211;&#23384;&#31649;&#29702;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#20915;&#31574;&#22914;&#20309;&#37325;&#26032;&#34917;&#20805;&#24211;&#23384;&#65292;&#29305;&#21035;&#26159;&#22312;&#32771;&#34385;&#22810;&#20010;&#20379;&#24212;&#21830;&#35746;&#21333;&#21644;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#24555;&#36895;&#35843;&#25972;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24211;&#23384;&#31649;&#29702;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;&#35782;&#21035;&#20986;&#26368;&#20248;&#30340;&#20174;&#22810;&#20010;&#20379;&#24212;&#21830;&#37325;&#26032;&#34917;&#20805;&#24211;&#23384;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31867;&#20248;&#21270;&#38382;&#39064;&#65292;&#24211;&#23384;&#31649;&#29702;&#32773;&#38656;&#35201;&#20915;&#23450;&#20174;&#27599;&#20010;&#20379;&#24212;&#21830;&#35746;&#36141;&#21738;&#20123;&#25968;&#37327;&#30340;&#20135;&#21697;&#65292;&#32771;&#34385;&#21040;&#20928;&#24211;&#23384;&#21644;&#24453;&#22788;&#29702;&#35746;&#21333;&#65292;&#20197;&#20415;&#21516;&#26102;&#26368;&#23567;&#21270;&#39044;&#26399;&#30340;&#28382;&#21518;&#12289;&#25345;&#26377;&#21644;&#35843;&#24230;&#25104;&#26412;&#12290;&#24211;&#23384;&#31649;&#29702;&#38382;&#39064;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#36229;&#36807;60&#24180;&#65292;&#28982;&#32780;&#21363;&#20351;&#26159;&#22522;&#26412;&#30340;&#21452;&#37325;&#37319;&#36141;&#38382;&#39064;&#65292;&#22312;&#20854;&#20013;&#19968;&#20010;&#20379;&#24212;&#21830;&#30340;&#35746;&#21333;&#27604;&#21478;&#19968;&#20010;&#20379;&#24212;&#21830;&#30340;&#35746;&#21333;&#35201;&#24555;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20854;&#19968;&#33324;&#24418;&#24335;&#20013;&#20381;&#28982;&#38590;&#20197;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;&#36824;&#26377;&#19968;&#20010;&#26032;&#20852;&#30340;&#38656;&#27714;&#65292;&#21363;&#24320;&#21457;&#20027;&#21160;&#30340;&#21487;&#25193;&#23637;&#20248;&#21270;&#31639;&#27861;&#65292;&#33021;&#22815;&#21450;&#26102;&#22320;&#26681;&#25454;&#21160;&#24577;&#30340;&#38656;&#27714;&#21464;&#21270;&#35843;&#25972;&#20854;&#25512;&#33616;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#21452;&#37325;&#37319;&#36141;&#38382;&#39064;&#35270;&#20026;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#24211;&#23384;&#21160;&#24577;&#21450;&#20854;&#34917;&#20805;&#65288;&#21363;&#25511;&#21046;&#65289;&#31574;&#30053;&#30340;&#20449;&#24687;&#34701;&#20837;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in inventory management is to identify policies that optimally replenish inventory from multiple suppliers. To solve such optimization problems, inventory managers need to decide what quantities to order from each supplier, given the net inventory and outstanding orders, so that the expected backlogging, holding, and sourcing costs are jointly minimized. Inventory management problems have been studied extensively for over 60 years, and yet even basic dual-sourcing problems, in which orders from an expensive supplier arrive faster than orders from a regular supplier, remain intractable in their general form. In addition, there is an emerging need to develop proactive, scalable optimization algorithms that can adjust their recommendations to dynamic demand shifts in a timely fashion. In this work, we approach dual sourcing from a neural network--based optimization lens and incorporate information on inventory dynamics and its replenishment (i.e., control) policies into th
&lt;/p&gt;</description></item><item><title>&#22312;QMIX&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;LSF-SAC&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#28508;&#22312;&#20449;&#24687;&#20849;&#20139;&#26426;&#21046;&#65292;&#21487;&#26174;&#33879;&#25193;&#23637;&#20215;&#20540;&#20989;&#25968;&#20998;&#35299;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#23436;&#20840;&#20998;&#25955;&#25191;&#34892;&#20013;&#20445;&#25345;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.01247</link><description>&lt;p&gt;
&#24102;&#26377;&#28508;&#22312;&#29366;&#24577;&#20449;&#24687;&#20849;&#20139;&#30340;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Value Functions Factorization with Latent State Information Sharing in Decentralized Multi-Agent Policy Gradients. (arXiv:2201.01247v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.01247
&lt;/p&gt;
&lt;p&gt;
&#22312;QMIX&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;LSF-SAC&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#28508;&#22312;&#20449;&#24687;&#20849;&#20139;&#26426;&#21046;&#65292;&#21487;&#26174;&#33879;&#25193;&#23637;&#20215;&#20540;&#20989;&#25968;&#20998;&#35299;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#23436;&#20840;&#20998;&#25955;&#25191;&#34892;&#20013;&#20445;&#25345;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38598;&#20013;&#35757;&#32451;&#21644;&#20998;&#25955;&#25191;&#34892;&#30340;&#20215;&#20540;&#20989;&#25968;&#20998;&#35299;&#26041;&#27861;&#26377;&#26395;&#35299;&#20915;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;QMIX&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#24050;&#32463;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#24182;&#22312;StarCraft II&#24494;&#35266;&#31649;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;QMIX&#20013;&#30340;&#21333;&#20010;&#26234;&#33021;&#20307;&#20272;&#35745;&#30340;&#21333;&#35843;&#28151;&#21512;&#34987;&#35748;&#20026;&#38480;&#21046;&#20102;&#23427;&#33021;&#34920;&#31034;&#30340;&#32852;&#21512;&#21160;&#20316;Q&#20540;&#30340;&#33539;&#22260;&#65292;&#21516;&#26102;&#20840;&#23616;&#29366;&#24577;&#20449;&#24687;&#19981;&#36275;&#20197;&#36827;&#34892;&#21333;&#20010;&#26234;&#33021;&#20307;&#20540;&#20989;&#25968;&#20272;&#35745;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#32467;&#26524;&#27425;&#20248;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LSF-SAC&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#22522;&#20110;&#21464;&#20998;&#25512;&#29702;&#30340;&#20449;&#24687;&#20849;&#20139;&#26426;&#21046;&#20316;&#20026;&#39069;&#22806;&#30340;&#29366;&#24577;&#20449;&#24687;&#65292;&#20197;&#36741;&#21161;&#20010;&#20307;&#26234;&#33021;&#20307;&#22312;&#20215;&#20540;&#20989;&#25968;&#20998;&#35299;&#20013;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#28508;&#22312;&#30340;&#20010;&#20307;&#29366;&#24577;&#20449;&#24687;&#20849;&#20139;&#21487;&#20197;&#26174;&#33879;&#25193;&#23637;&#20540;&#20989;&#25968;&#20998;&#35299;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;LSF-SAC&#20013;&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#36719;&#38480;&#21046;&#23454;&#29616;&#23436;&#20840;&#20998;&#25955;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Value function factorization via centralized training and decentralized execution is promising for solving cooperative multi-agent reinforcement tasks. One of the approaches in this area, QMIX, has become state-of-the-art and achieved the best performance on the StarCraft II micromanagement benchmark. However, the monotonic-mixing of per agent estimates in QMIX is known to restrict the joint action Q-values it can represent, as well as the insufficient global state information for single agent value function estimation, often resulting in suboptimality. To this end, we present LSF-SAC, a novel framework that features a variational inference-based information-sharing mechanism as extra state information to assist individual agents in the value function factorization. We demonstrate that such latent individual state information sharing can significantly expand the power of value function factorization, while fully decentralized execution can still be maintained in LSF-SAC through a soft-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#22266;&#23450;&#32764;&#26080;&#20154;&#26426;&#30340;&#23039;&#24577;&#25511;&#21046;&#65292;&#21482;&#38656;&#19977;&#20998;&#38047;&#30340;&#39134;&#34892;&#25968;&#25454;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#30452;&#25509;&#25805;&#20316;&#21407;&#22987;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#25216;&#26415;&#20855;&#22791;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.04153</link><description>&lt;p&gt;
&#22266;&#23450;&#32764;&#26080;&#20154;&#26426;&#23039;&#24577;&#25511;&#21046;&#30340;&#25968;&#25454;&#39640;&#25928;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65306;&#29616;&#22330;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Deep Reinforcement Learning for Attitude Control of Fixed-Wing UAVs: Field Experiments. (arXiv:2111.04153v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.04153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#22266;&#23450;&#32764;&#26080;&#20154;&#26426;&#30340;&#23039;&#24577;&#25511;&#21046;&#65292;&#21482;&#38656;&#19977;&#20998;&#38047;&#30340;&#39134;&#34892;&#25968;&#25454;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#30452;&#25509;&#25805;&#20316;&#21407;&#22987;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#25216;&#26415;&#20855;&#22791;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23384;&#22312;&#19981;&#30830;&#23450;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#12289;&#25191;&#34892;&#26426;&#26500;&#32422;&#26463;&#20197;&#21450;&#32437;&#21521;&#21644;&#27178;&#21521;&#36816;&#21160;&#30340;&#32806;&#21512;&#65292;&#22266;&#23450;&#32764;&#26080;&#20154;&#26426;&#30340;&#23039;&#24577;&#25511;&#21046;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#39550;&#39542;&#20173;&#22522;&#20110;&#32447;&#24615;&#25511;&#21046;&#65292;&#20854;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#21487;&#20197;&#25104;&#21151;&#23398;&#20064;&#30452;&#25509;&#25805;&#20316;&#21407;&#22987;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#22266;&#23450;&#32764;&#26080;&#20154;&#26426;&#30340;&#23039;&#24577;&#25511;&#21046;&#65292;&#21482;&#38656;&#35201;&#19977;&#20998;&#38047;&#30340;&#39134;&#34892;&#25968;&#25454;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#39134;&#34892;&#27979;&#35797;&#20013;&#23558;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#24212;&#29992;&#20110;&#26080;&#20154;&#26426;&#19978;&#65292;&#20854;&#34920;&#29616;&#19982;&#29616;&#26377;&#30340;ArduPlane&#27604;&#20363;&#31215;&#20998;&#24494;&#20998;&#65288;PID&#65289;&#23039;&#24577;&#25511;&#21046;&#22120;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attitude control of fixed-wing unmanned aerial vehicles (UAVs) is a difficult control problem in part due to uncertain nonlinear dynamics, actuator constraints, and coupled longitudinal and lateral motions. Current state-of-the-art autopilots are based on linear control and are thus limited in their effectiveness and performance. Deep reinforcement learning (DRL) is a machine learning method to automatically discover optimal control laws through interaction with the controlled system, which can handle complex nonlinear dynamics. We show in this paper that DRL can successfully learn to perform attitude control of a fixed-wing UAV operating directly on the original nonlinear dynamics, requiring as little as three minutes of flight data. We initially train our model in a simulation environment and then deploy the learned controller on the UAV in flight tests, demonstrating comparable performance to the state-of-the-art ArduPlane proportional-integral-derivative (PID) attitude controller w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31232;&#30095;&#21152;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;(SLR)&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#27169;&#22411;&#21644;&#27714;&#35299;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2109.12701</link><description>&lt;p&gt;
&#31232;&#30095;&#21152;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;: &#19968;&#31181;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sparse Plus Low Rank Matrix Decomposition: A Discrete Optimization Approach. (arXiv:2109.12701v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.12701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31232;&#30095;&#21152;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;(SLR)&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#27169;&#22411;&#21644;&#27714;&#35299;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31232;&#30095;&#21152;&#20302;&#31209;&#20998;&#35299;&#38382;&#39064;(SLR)&#65292;&#21363;&#23558;&#25439;&#22351;&#30340;&#25968;&#25454;&#30697;&#38453;&#20998;&#35299;&#20026;&#21253;&#21547;&#22522;&#26412;&#30495;&#20540;&#30340;&#20302;&#31209;&#30697;&#38453;&#21644;&#21253;&#21547;&#25200;&#21160;&#30340;&#31232;&#30095;&#30697;&#38453;&#12290; SLR&#26159;&#36816;&#31609;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#22522;&#30784;&#38382;&#39064;&#65292;&#22312;&#25968;&#25454;&#21387;&#32553;&#12289;&#28508;&#22312;&#35821;&#20041;&#32034;&#24341;&#12289;&#21327;&#21516;&#36807;&#28388;&#21644;&#21307;&#23398;&#25104;&#20687;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#27169;&#22411;&#65292;&#24182;&#35774;&#35745;&#20102;&#20132;&#26367;&#26368;&#23567;&#21270;&#21551;&#21457;&#24335;&#31639;&#27861;&#20197;&#21450;&#26032;&#30340;&#21322;&#23450;&#26494;&#24347;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#23450;&#20041;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#21644;&#20984;&#26494;&#24347;&#26469;&#35299;&#20915;&#23567;&#35268;&#27169;&#30340;SLR&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915; $n=10000$ &#30340;&#38382;&#39064;&#35268;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the Sparse Plus Low-Rank decomposition problem (SLR), which is the problem of decomposing a corrupted data matrix into a sparse matrix of perturbations plus a low-rank matrix containing the ground truth. SLR is a fundamental problem in Operations Research and Machine Learning which arises in various applications, including data compression, latent semantic indexing, collaborative filtering, and medical imaging. We introduce a novel formulation for SLR that directly models its underlying discreteness. For this formulation, we develop an alternating minimization heuristic that computes high-quality solutions and a novel semidefinite relaxation that provides meaningful bounds for the solutions returned by our heuristic. We also develop a custom branch-and-bound algorithm that leverages our heuristic and convex relaxations to solve small instances of SLR to certifiable (near) optimality. Given an input $n$-by-$n$ matrix, our heuristic scales to solve instances where $n=10000$ in m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#37319;&#26679;&#25104;&#26412;&#30340;&#36830;&#32493;&#26102;&#38388;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#22312;&#36830;&#32493;&#26102;&#38388;&#37324;&#65292;&#23398;&#20064;&#32773;&#35201;&#22312;&#33719;&#24471;&#26356;&#39640;&#22870;&#21169;&#21644;&#25215;&#25285;&#37319;&#26679;&#25104;&#26412;&#20043;&#38388;&#36827;&#34892;&#26377;&#25928;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36798;&#21040;&#19979;&#30028;&#30340;&#31639;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#20256;&#32479;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#19981;&#21516;&#30340;&#29305;&#27530;&#29616;&#35937;&#65292;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2107.05289</link><description>&lt;p&gt;
&#24102;&#37319;&#26679;&#25104;&#26412;&#30340;&#36830;&#32493;&#26102;&#38388;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Continuous Time Bandits With Sampling Costs. (arXiv:2107.05289v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.05289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#37319;&#26679;&#25104;&#26412;&#30340;&#36830;&#32493;&#26102;&#38388;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#22312;&#36830;&#32493;&#26102;&#38388;&#37324;&#65292;&#23398;&#20064;&#32773;&#35201;&#22312;&#33719;&#24471;&#26356;&#39640;&#22870;&#21169;&#21644;&#25215;&#25285;&#37319;&#26679;&#25104;&#26412;&#20043;&#38388;&#36827;&#34892;&#26377;&#25928;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36798;&#21040;&#19979;&#30028;&#30340;&#31639;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#20256;&#32479;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#19981;&#21516;&#30340;&#29305;&#27530;&#29616;&#35937;&#65292;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;(Continuous Time Multi-arm Bandit Problem&#65292;CTMAB)&#12290;&#22312;&#32473;&#23450;&#26102;&#38388;&#27573;&#20869;&#65292;&#23398;&#20064;&#32773;&#21487;&#20197;&#23545;&#33218;&#36827;&#34892;&#20219;&#24847;&#27425;&#37319;&#26679;&#65292;&#27599;&#27425;&#37319;&#26679;&#37117;&#33021;&#33719;&#24471;&#38543;&#26426;&#22870;&#21169;&#65292;&#20294;&#37319;&#26679;&#39057;&#29575;&#30340;&#25552;&#39640;&#20250;&#24102;&#26469;&#39069;&#22806;&#30340;&#24809;&#32602;/&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#23384;&#22312;&#33719;&#24471;&#26356;&#39640;&#22870;&#21169;&#19982;&#25215;&#25285;&#37319;&#26679;&#25104;&#26412;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#26088;&#22312;&#35774;&#35745;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#36951;&#25022;&#65288;regret&#65292;&#23450;&#20041;&#20026;&#23398;&#20064;&#31639;&#27861;&#19982;&#29702;&#35770;&#26368;&#20248;&#31574;&#30053;&#25910;&#30410;&#20043;&#38388;&#30340;&#24046;&#20540;&#65289;&#36798;&#21040;&#26368;&#23567;&#12290;CTMAB&#19982;&#36890;&#24120;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;(Multi-armed Bandit Problem&#65292;MAB)&#26377;&#26681;&#26412;&#30340;&#21306;&#21035;&#65292;&#20363;&#22914;&#65292;&#22312;CTMAB&#20013;&#65292;&#21333;&#33218;&#24773;&#20917;&#37117;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#65292;&#22240;&#20026;&#26368;&#20248;&#37319;&#26679;&#39057;&#29575;&#21462;&#20915;&#20110;&#33218;&#30340;&#22343;&#20540;&#65292;&#32780;&#35813;&#22343;&#20540;&#38656;&#35201;&#34987;&#20272;&#35745;&#12290;&#26412;&#25991;&#39318;&#20808;&#24314;&#31435;&#20102;&#25152;&#26377;&#31639;&#27861;&#21487;&#36798;&#21040;&#30340;&#36951;&#25022;&#19979;&#30028;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25968;&#22240;&#23376;&#19978;&#36798;&#21040;&#19979;&#30028;&#30340;&#31639;&#27861;&#12290;&#23545;&#20110;&#21333;&#33218;&#24773;&#20917;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19979;&#38480;&#21644;&#19978;&#38480;&#22823;&#33268;&#31526;&#21512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#22312;&#32463;&#20856;MAB&#38382;&#39064;&#20013;&#19981;&#23384;&#22312;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#29616;&#35937;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a continuous-time multi-arm bandit problem (CTMAB), where the learner can sample arms any number of times in a given interval and obtain a random reward from each sample, however, increasing the frequency of sampling incurs an additive penalty/cost. Thus, there is a tradeoff between obtaining large reward and incurring sampling cost as a function of the sampling frequency. The goal is to design a learning algorithm that minimizes regret, that is defined as the difference of the payoff of the oracle policy and that of the learning algorithm. CTMAB is fundamentally different than the usual multi-arm bandit problem (MAB), e.g., even the single-arm case is non-trivial in CTMAB, since the optimal sampling frequency depends on the mean of the arm, which needs to be estimated. We first establish lower bounds on the regret achievable with any algorithm and then propose algorithms that achieve the lower bound up to logarithmic factors. For the single-arm case, we show that the lower
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#38750;&#21033;&#26222;&#24076;&#33576;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#25915;&#20987;&#27169;&#22411;&#24110;&#21161;&#29702;&#35299;&#20869;&#22312;&#23646;&#24615;&#65292;&#35777;&#26126;&#20102;&#27492;&#31867;&#25915;&#20987;&#32773;&#21487;&#20197;&#25112;&#32988;&#25152;&#26377;&#24517;&#39035;&#23545;&#20854;&#36755;&#20837;&#36827;&#34892;&#20998;&#31867;&#30340;&#31639;&#27861;&#65292;&#20294;&#20063;&#25552;&#20986;&#20102;&#20811;&#26381;&#27492;&#31867;&#25915;&#20987;&#32773;&#30340;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#24182;&#20026;&#26368;&#36817;&#37051;&#31639;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2010.06154</link><description>&lt;p&gt;
&#38750;&#21033;&#26222;&#24076;&#33576;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Robustness of Non-Lipschitz Networks. (arXiv:2010.06154v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.06154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#38750;&#21033;&#26222;&#24076;&#33576;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#25915;&#20987;&#27169;&#22411;&#24110;&#21161;&#29702;&#35299;&#20869;&#22312;&#23646;&#24615;&#65292;&#35777;&#26126;&#20102;&#27492;&#31867;&#25915;&#20987;&#32773;&#21487;&#20197;&#25112;&#32988;&#25152;&#26377;&#24517;&#39035;&#23545;&#20854;&#36755;&#20837;&#36827;&#34892;&#20998;&#31867;&#30340;&#31639;&#27861;&#65292;&#20294;&#20063;&#25552;&#20986;&#20102;&#20811;&#26381;&#27492;&#31867;&#25915;&#20987;&#32773;&#30340;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#24182;&#20026;&#26368;&#36817;&#37051;&#31639;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#28145;&#24230;&#32593;&#32476;&#20173;&#28982;&#26497;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#20854;&#20013;&#19968;&#20010;&#26681;&#26412;&#24615;&#30340;&#25361;&#25112;&#26159;&#65306;&#21363;&#20351;&#36755;&#20837;&#30053;&#24494;&#25200;&#21160;&#65292;&#20063;&#21487;&#33021;&#20250;&#20135;&#29983;&#32593;&#32476;&#26368;&#32456;&#23618;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#22823;&#24133;&#31227;&#21160;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#25915;&#20987;&#27169;&#22411;&#26469;&#25277;&#35937;&#36825;&#20010;&#25361;&#25112;&#65292;&#20197;&#24110;&#21161;&#29702;&#35299;&#23427;&#30340;&#20869;&#22312;&#23646;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#23545;&#25163;&#21487;&#20197;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20219;&#24847;&#36317;&#31163;&#19978;&#31227;&#21160;&#25968;&#25454;&#65292;&#20294;&#21482;&#33021;&#22312;&#38543;&#26426;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#20869;&#36827;&#34892;&#25805;&#20316;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#25915;&#20987;&#32773;&#21487;&#20197;&#38750;&#24120;&#24378;&#22823;&#65306;&#23427;&#20204;&#21487;&#20197;&#25112;&#32988;&#20219;&#20309;&#24517;&#39035;&#23545;&#20854;&#25910;&#21040;&#30340;&#25152;&#26377;&#36755;&#20837;&#36827;&#34892;&#20998;&#31867;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20801;&#35768;&#31639;&#27861;&#25918;&#24323;&#22788;&#29702;&#19981;&#23547;&#24120;&#30340;&#36755;&#20837;&#65292;&#25105;&#20204;&#34920;&#26126;&#24403;&#31867;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30456;&#23545;&#20998;&#31163;&#24471;&#24456;&#22909;&#26102;&#65292;&#36825;&#31181;&#25915;&#20987;&#32773;&#26159;&#21487;&#20197;&#34987;&#20811;&#26381;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#20197;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#35774;&#32622;&#31639;&#27861;&#21442;&#25968;&#20197;&#20248;&#21270;&#31934;&#24230;-&#25918;&#24323;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#26368;&#36817;&#37051;&#31639;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant advances, deep networks remain highly susceptible to adversarial attack. One fundamental challenge is that small input perturbations can often produce large movements in the network's final-layer feature space. In this paper, we define an attack model that abstracts this challenge, to help understand its intrinsic properties. In our model, the adversary may move data an arbitrary distance in feature space but only in random low-dimensional subspaces. We prove such adversaries can be quite powerful: defeating any algorithm that must classify any input it is given. However, by allowing the algorithm to abstain on unusual inputs, we show such adversaries can be overcome when classes are reasonably well-separated in feature space. We further provide strong theoretical guarantees for setting algorithm parameters to optimize over accuracy-abstention trade-offs using data-driven methods. Our results provide new robustness guarantees for nearest-neighbor style algorithms, a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36873;&#25321;&#21644;&#25104;&#26412;&#25910;&#30410;&#30340;&#31995;&#32479;&#25551;&#36848;&#26041;&#24335;&#65292;&#24182;&#20174;&#32534;&#31243;&#35821;&#35328;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#27492;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#23450;&#20041;&#20102;&#20004;&#31181;&#25903;&#25345;&#20915;&#31574;&#25277;&#35937;&#30340;&#23567;&#35821;&#35328;&#65292;&#24182;&#32473;&#20986;&#20102;&#23427;&#20204;&#30340;&#25805;&#20316;&#35821;&#20041;&#21644;&#24213;&#23618;&#35821;&#20041;&#65292;&#24182;&#23558;&#24213;&#23618;&#35821;&#20041;&#22686;&#24378;&#20026;&#36873;&#25321;&#21644;&#27010;&#29575;&#21333;&#23376;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#20110;&#20004;&#20010;&#31616;&#21333;&#20363;&#23376;&#23637;&#31034;&#20102;&#27492;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2007.08926</link><description>&lt;p&gt;
&#26234;&#33021;&#36873;&#25321;&#21644;&#36873;&#25321;&#21333;&#23376;
&lt;/p&gt;
&lt;p&gt;
Smart Choices and the Selection Monad. (arXiv:2007.08926v8 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.08926
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36873;&#25321;&#21644;&#25104;&#26412;&#25910;&#30410;&#30340;&#31995;&#32479;&#25551;&#36848;&#26041;&#24335;&#65292;&#24182;&#20174;&#32534;&#31243;&#35821;&#35328;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#27492;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#23450;&#20041;&#20102;&#20004;&#31181;&#25903;&#25345;&#20915;&#31574;&#25277;&#35937;&#30340;&#23567;&#35821;&#35328;&#65292;&#24182;&#32473;&#20986;&#20102;&#23427;&#20204;&#30340;&#25805;&#20316;&#35821;&#20041;&#21644;&#24213;&#23618;&#35821;&#20041;&#65292;&#24182;&#23558;&#24213;&#23618;&#35821;&#20041;&#22686;&#24378;&#20026;&#36873;&#25321;&#21644;&#27010;&#29575;&#21333;&#23376;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#20110;&#20004;&#20010;&#31616;&#21333;&#20363;&#23376;&#23637;&#31034;&#20102;&#27492;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#36873;&#25321;&#21644;&#30456;&#24212;&#30340;&#25104;&#26412;&#21644;&#25910;&#30410;&#26469;&#25551;&#36848;&#31995;&#32479;&#65292;&#26377;&#26395;&#20351;&#31639;&#27861;&#35774;&#35745;&#20154;&#21592;&#21644;&#31243;&#24207;&#21592;&#20174;&#25351;&#23450;&#22914;&#20309;&#36827;&#34892;&#36873;&#25321;&#20013;&#35299;&#25918;&#20986;&#26469;&#65307;&#22312;&#23454;&#38469;&#23454;&#29616;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#25216;&#26415;&#21644;&#36234;&#26469;&#36234;&#22810;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#20123;&#36873;&#25321;&#12290;&#25105;&#20204;&#20174;&#32534;&#31243;&#35821;&#35328;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#20004;&#31181;&#25903;&#25345;&#20915;&#31574;&#25277;&#35937;&#30340;&#23567;&#35821;&#35328;&#65306;&#19968;&#20010;&#20855;&#26377;&#36873;&#25321;&#21644;&#25910;&#30410;&#65292;&#21478;&#19968;&#20010;&#21017;&#39069;&#22806;&#21152;&#20837;&#20102;&#27010;&#29575;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#25805;&#20316;&#35821;&#20041;&#21644;&#24213;&#23618;&#35821;&#20041;&#12290;&#38024;&#23545;&#31532;&#20108;&#31181;&#35821;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#24213;&#23618;&#35821;&#20041;&#65292;&#23427;&#20204;&#22312;&#21487;&#33021;&#30340;&#31243;&#24207;&#20540;&#21644;&#39044;&#26399;&#22238;&#25253;&#20043;&#38388;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#30456;&#20851;&#24615;&#12290;&#25805;&#20316;&#35821;&#20041;&#23558;&#26631;&#20934;&#26500;&#36896;&#30340;&#36890;&#24120;&#35821;&#20041;&#19982;&#21487;&#33021;&#30340;&#25191;&#34892;&#31574;&#30053;&#31354;&#38388;&#30340;&#20248;&#21270;&#30456;&#32467;&#21512;&#12290;&#22522;&#20110;&#36873;&#25321;&#21333;&#23376;&#30340;&#32452;&#21512;&#24213;&#23618;&#35821;&#20041;&#65292;&#22686;&#21152;&#31532;&#20108;&#31181;&#35821;&#35328;&#20013;&#30340;&#27010;&#29575;&#21333;&#23376;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36873;&#25321;&#21333;&#23376;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340; continuation &#21333;&#23376;&#65292;&#32780;&#35821;&#35328;&#30340;&#25805;&#20316;&#35821;&#20041;&#21644;&#24213;&#23618;&#35821;&#20041;&#21017;&#36890;&#36807;&#21333;&#23376;&#32763;&#35793;&#30456;&#20851;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#20004;&#20010;&#31616;&#21333;&#30340;&#20363;&#23376;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Describing systems in terms of choices and their resulting costs and rewards offers the promise of freeing algorithm designers and programmers from specifying how those choices should be made; in implementations, the choices can be realized by optimization techniques and, increasingly, by machine-learning methods. We study this approach from a programming-language perspective. We define two small languages that support decision-making abstractions: one with choices and rewards, and the other additionally with probabilities. We give both operational and denotational semantics.  In the case of the second language we consider three denotational semantics, with varying degrees of correlation between possible program values and expected rewards. The operational semantics combine the usual semantics of standard constructs with optimization over spaces of possible execution strategies. The denotational semantics, which are compositional, rely on the selection monad, to handle choice, augmente
&lt;/p&gt;</description></item></channel></rss>