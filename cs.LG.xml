<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29983;&#25104;&#25554;&#20540;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#22810;&#26679;&#21270;&#36229;&#39046;&#22495;&#26679;&#26412;&#26469;&#25913;&#36827;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#28304;&#22495;&#19978;&#35757;&#32451;StyleGAN&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#20854;&#20182;&#22495;&#19978;&#24494;&#35843;&#65292;&#24471;&#21040;&#20102;&#22810;&#20010;&#30456;&#20851;&#30340;&#29983;&#25104;&#22120;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.12219</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#25554;&#20540;&#25913;&#36827;&#20998;&#31867;&#22120;&#30340;&#36229;&#39046;&#22495;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Out-of-Distribution Robustness of Classifiers via Generative Interpolation. (arXiv:2307.12219v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29983;&#25104;&#25554;&#20540;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#22810;&#26679;&#21270;&#36229;&#39046;&#22495;&#26679;&#26412;&#26469;&#25913;&#36827;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#28304;&#22495;&#19978;&#35757;&#32451;StyleGAN&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#20854;&#20182;&#22495;&#19978;&#24494;&#35843;&#65292;&#24471;&#21040;&#20102;&#22810;&#20010;&#30456;&#20851;&#30340;&#29983;&#25104;&#22120;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;i.i.d.&#65289;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#36229;&#39046;&#22495;&#65288;OoD&#65289;&#25968;&#25454;&#26102;&#65292;&#20854;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#19981;&#21516;&#30340;&#20998;&#24067;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#28304;&#65292;&#25506;&#32034;&#20102;&#25913;&#36827;&#31070;&#32463;&#20998;&#31867;&#22120;&#30340;&#36229;&#39046;&#22495;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#29983;&#25104;&#25554;&#20540;&#65292;&#29992;&#20110;&#21512;&#25104;&#22810;&#20010;&#22495;&#30340;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#30340;&#22810;&#26679;&#21270;OoD&#26679;&#26412;&#12290;&#30452;&#25509;&#22312;&#28304;&#22495;&#19978;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#24448;&#24448;&#23481;&#26131;&#20986;&#29616;&#27169;&#24335;&#23849;&#28291;&#65292;&#24182;&#19988;&#26377;&#26102;&#20250;&#25918;&#22823;&#25968;&#25454;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#19968;&#20010;&#28304;&#22495;&#19978;&#35757;&#32451;StyleGAN&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#20854;&#20182;&#22495;&#19978;&#24494;&#35843;&#65292;&#20174;&#32780;&#24471;&#21040;&#35768;&#22810;&#30456;&#20851;&#30340;&#29983;&#25104;&#22120;&#65292;&#23427;&#20204;&#30340;&#27169;&#22411;&#21442;&#25968;&#20855;&#26377;&#30456;&#21516;&#30340;&#21021;&#22987;&#21270;&#65292;&#22240;&#27492;&#26159;&#23545;&#40784;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks achieve superior performance for learning from independent and identically distributed (i.i.d.) data. However, their performance deteriorates significantly when handling out-of-distribution (OoD) data, where the training and test are drawn from different distributions. In this paper, we explore utilizing the generative models as a data augmentation source for improving out-of-distribution robustness of neural classifiers. Specifically, we develop a simple yet effective method called Generative Interpolation to fuse generative models trained from multiple domains for synthesizing diverse OoD samples. Training a generative model directly on the source domains tends to suffer from mode collapse and sometimes amplifies the data bias. Instead, we first train a StyleGAN model on one source domain and then fine-tune it on the other domains, resulting in many correlated generators where their model parameters have the same initialization thus are aligned. We then linearly 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23545;&#35821;&#38899;&#28608;&#27963;&#35774;&#22791;&#36827;&#34892;&#26080;&#22768;&#25915;&#20987;&#30340;&#39118;&#38505;&#65292;&#24182;&#21457;&#29616;&#23384;&#22312;&#37325;&#22823;&#23433;&#20840;&#28431;&#27934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#32447;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#27169;&#25311;&#20102;&#22810;&#31181;&#25915;&#20987;&#22330;&#26223;&#65292;&#25581;&#31034;&#20102;&#36890;&#36807;&#26080;&#38656;&#28155;&#21152;&#30828;&#20214;&#25110;&#22686;&#21152;&#35774;&#22791;&#25216;&#33021;&#30340;&#29289;&#29702;&#35775;&#38382;&#26469;&#21457;&#29616;&#21644;&#25317;&#26377;&#29305;&#26435;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;&#20351;&#29992;&#28145;&#24230;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#22312;&#23569;&#25968;&#27493;&#39588;&#20013;&#24555;&#36895;&#25317;&#26377;&#20102;&#25152;&#26377;&#33410;&#28857;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#23545;&#38750;&#20256;&#32479;&#32593;&#32476;&#21644;&#26032;&#30340;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.12204</link><description>&lt;p&gt;
&#23545;&#26080;&#22768;&#35821;&#38899;&#28608;&#27963;&#35774;&#22791;&#36827;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#23545;&#31574;
&lt;/p&gt;
&lt;p&gt;
Adversarial Agents For Attacking Inaudible Voice Activated Devices. (arXiv:2307.12204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23545;&#35821;&#38899;&#28608;&#27963;&#35774;&#22791;&#36827;&#34892;&#26080;&#22768;&#25915;&#20987;&#30340;&#39118;&#38505;&#65292;&#24182;&#21457;&#29616;&#23384;&#22312;&#37325;&#22823;&#23433;&#20840;&#28431;&#27934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#32447;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#27169;&#25311;&#20102;&#22810;&#31181;&#25915;&#20987;&#22330;&#26223;&#65292;&#25581;&#31034;&#20102;&#36890;&#36807;&#26080;&#38656;&#28155;&#21152;&#30828;&#20214;&#25110;&#22686;&#21152;&#35774;&#22791;&#25216;&#33021;&#30340;&#29289;&#29702;&#35775;&#38382;&#26469;&#21457;&#29616;&#21644;&#25317;&#26377;&#29305;&#26435;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;&#20351;&#29992;&#28145;&#24230;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#22312;&#23569;&#25968;&#27493;&#39588;&#20013;&#24555;&#36895;&#25317;&#26377;&#20102;&#25152;&#26377;&#33410;&#28857;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#23545;&#38750;&#20256;&#32479;&#32593;&#32476;&#21644;&#26032;&#30340;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#26080;&#22768;&#25915;&#20987;&#23545;&#35821;&#38899;&#28608;&#27963;&#35774;&#22791;&#30340;&#20998;&#26512;&#30830;&#35748;&#20102;7.6&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#24378;&#35843;&#20102;&#30001;NIST&#22269;&#23478;&#28431;&#27934;&#25968;&#25454;&#24211;&#65288;NVD&#65289;&#29420;&#31435;&#35780;&#20998;&#30340;&#37325;&#22823;&#23433;&#20840;&#28431;&#27934;&#12290;&#25105;&#20204;&#30340;&#22522;&#32447;&#32593;&#32476;&#27169;&#22411;&#23637;&#31034;&#20102;&#19968;&#31181;&#25915;&#20987;&#32773;&#20351;&#29992;&#26080;&#22768;&#35821;&#38899;&#21629;&#20196;&#26410;&#32463;&#25480;&#26435;&#35775;&#38382;&#21463;&#20445;&#25252;&#31508;&#35760;&#26412;&#19978;&#26426;&#23494;&#20449;&#24687;&#30340;&#22330;&#26223;&#12290;&#25105;&#20204;&#22312;&#35813;&#22522;&#32447;&#32593;&#32476;&#27169;&#22411;&#19978;&#27169;&#25311;&#20102;&#35768;&#22810;&#25915;&#20987;&#22330;&#26223;&#65292;&#25581;&#31034;&#20102;&#36890;&#36807;&#29289;&#29702;&#35775;&#38382;&#32780;&#26080;&#38656;&#28155;&#21152;&#26032;&#30828;&#20214;&#25110;&#22686;&#24378;&#35774;&#22791;&#25216;&#33021;&#30340;&#20114;&#32852;&#35774;&#22791;&#30340;&#22823;&#35268;&#27169;&#21033;&#29992;&#28508;&#21147;&#26469;&#21457;&#29616;&#21644;&#25317;&#26377;&#29305;&#26435;&#20449;&#24687;&#12290;&#20351;&#29992;&#24494;&#36719;&#30340;CyberBattleSim&#26694;&#26550;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20845;&#31181;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21457;&#29616;&#28145;&#24230;Q&#23398;&#20064;&#19982;&#24320;&#21457;&#35777;&#26126;&#26159;&#26368;&#20248;&#30340;&#65292;&#21487;&#20197;&#22312;&#26356;&#23569;&#30340;&#27493;&#39588;&#20013;&#36805;&#36895;&#25317;&#26377;&#25152;&#26377;&#33410;&#28857;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#23545;&#38750;&#20256;&#32479;&#32593;&#32476;&#21644;&#26032;&#30340;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our analysis of inaudible attacks on voice-activated devices confirms the alarming risk factor of 7.6 out of 10, underlining significant security vulnerabilities scored independently by NIST National Vulnerability Database (NVD). Our baseline network model showcases a scenario in which an attacker uses inaudible voice commands to gain unauthorized access to confidential information on a secured laptop. We simulated many attack scenarios on this baseline network model, revealing the potential for mass exploitation of interconnected devices to discover and own privileged information through physical access without adding new hardware or amplifying device skills. Using Microsoft's CyberBattleSim framework, we evaluated six reinforcement learning algorithms and found that Deep-Q learning with exploitation proved optimal, leading to rapid ownership of all nodes in fewer steps. Our findings underscore the critical need for understanding non-conventional networks and new cybersecurity measure
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NCART&#30340;&#21487;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21033;&#29992;&#22810;&#20010;&#21487;&#24494;&#24615;&#20915;&#31574;&#26641;&#26367;&#20195;&#20840;&#36830;&#25509;&#23618;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#30340;&#21516;&#26102;&#20805;&#20998;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;&#25110;&#39640;&#32500;&#25968;&#25454;&#38598;&#19978;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2307.12198</link><description>&lt;p&gt;
NCART: &#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#31070;&#32463;&#20998;&#31867;&#19982;&#22238;&#24402;&#26641;
&lt;/p&gt;
&lt;p&gt;
NCART: Neural Classification and Regression Tree for Tabular Data. (arXiv:2307.12198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12198
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NCART&#30340;&#21487;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21033;&#29992;&#22810;&#20010;&#21487;&#24494;&#24615;&#20915;&#31574;&#26641;&#26367;&#20195;&#20840;&#36830;&#25509;&#23618;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#30340;&#21516;&#26102;&#20805;&#20998;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;&#25110;&#39640;&#32500;&#25968;&#25454;&#38598;&#19978;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#20013;&#21464;&#24471;&#27969;&#34892;&#65292;&#22240;&#20026;&#23427;&#20204;&#35299;&#20915;&#20102;&#20915;&#31574;&#26641;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22312;&#32447;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#26377;&#20215;&#20540;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20250;&#36935;&#21040;&#19968;&#20010;&#25240;&#34935;&#12290;&#19968;&#26041;&#38754;&#65292;&#24403;&#22788;&#29702;&#22823;&#35268;&#27169;&#25110;&#39640;&#32500;&#25968;&#25454;&#38598;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#35745;&#31639;&#37327;&#24456;&#22823;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#20204;&#21487;&#33021;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#19981;&#36866;&#29992;&#20110;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;&#31070;&#32463;&#20998;&#31867;&#19982;&#22238;&#24402;&#26641;&#65288;NCART&#65289;&#65292;&#20197;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;NCART&#26159;&#27531;&#24046;&#32593;&#32476;&#30340;&#21464;&#20307;&#65292;&#23427;&#23558;&#20840;&#36830;&#25509;&#23618;&#26367;&#25442;&#20026;&#22810;&#20010;&#21487;&#24494;&#24615;&#30340;&#26080;&#35270;&#20915;&#31574;&#26641;&#12290;&#36890;&#36807;&#23558;&#20915;&#31574;&#26641;&#38598;&#25104;&#21040;&#26550;&#26500;&#20013;&#65292;NCART&#20445;&#25345;&#20854;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#21448;&#33021;&#20174;&#31070;&#32463;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#33021;&#21147;&#20013;&#21463;&#30410;&#12290;NCART&#26550;&#26500;&#30340;&#31616;&#27905;&#24615;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have become popular in the analysis of tabular data, as they address the limitations of decision trees and enable valuable applications like semi-supervised learning, online learning, and transfer learning. However, these deep-learning approaches often encounter a trade-off. On one hand, they can be computationally expensive when dealing with large-scale or high-dimensional datasets. On the other hand, they may lack interpretability and may not be suitable for small-scale datasets. In this study, we propose a novel interpretable neural network called Neural Classification and Regression Tree (NCART) to overcome these challenges. NCART is a modified version of Residual Networks that replaces fully-connected layers with multiple differentiable oblivious decision trees. By integrating decision trees into the architecture, NCART maintains its interpretability while benefiting from the end-to-end capabilities of neural networks. The simplicity of the NCART architecture 
&lt;/p&gt;</description></item><item><title>DeepLearning.scala 2&#35299;&#20915;&#20102;&#22312;&#38745;&#24577;&#31867;&#22411;&#35821;&#35328;&#20013;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#26102;&#30340;&#33258;&#21160;&#27714;&#23548;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#32452;&#21333;&#23376;&#21644;&#21333;&#23376;&#21464;&#25442;&#22120;&#12290;</title><link>http://arxiv.org/abs/2307.12187</link><description>&lt;p&gt;
&#21333;&#23376;&#21270;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Monadic Deep Learning. (arXiv:2307.12187v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12187
&lt;/p&gt;
&lt;p&gt;
DeepLearning.scala 2&#35299;&#20915;&#20102;&#22312;&#38745;&#24577;&#31867;&#22411;&#35821;&#35328;&#20013;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#26102;&#30340;&#33258;&#21160;&#27714;&#23548;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#32452;&#21333;&#23376;&#21644;&#21333;&#23376;&#21464;&#25442;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Java&#21644;Scala&#31038;&#21306;&#24314;&#31435;&#20102;&#19968;&#20010;&#38750;&#24120;&#25104;&#21151;&#30340;&#22823;&#25968;&#25454;&#29983;&#24577;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#22312;&#20854;&#19978;&#36816;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#29992;&#21160;&#24577;&#31867;&#22411;&#32534;&#31243;&#35821;&#35328;&#24314;&#27169;&#30340;&#12290;&#36825;&#20123;&#21160;&#24577;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#23558;&#31070;&#32463;&#32593;&#32476;&#35270;&#20026;&#21253;&#21547;&#35768;&#22810;&#21487;&#35757;&#32451;&#21464;&#37327;&#30340;&#21487;&#24494;&#20998;&#34920;&#36798;&#24335;&#65292;&#24182;&#22312;&#35757;&#32451;&#26102;&#23545;&#36825;&#20123;&#34920;&#36798;&#24335;&#36827;&#34892;&#33258;&#21160;&#27714;&#23548;&#12290;&#30452;&#21040;2019&#24180;&#65292;&#38745;&#24577;&#31867;&#22411;&#35821;&#35328;&#30340;&#23398;&#20064;&#26694;&#26550;&#27809;&#26377;&#25552;&#20379;&#20256;&#32479;&#26694;&#26550;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#38500;&#38750;&#21019;&#24314;&#22823;&#37327;&#26679;&#26495;&#20195;&#30721;&#36827;&#34892;&#30828;&#32534;&#30721;&#30340;&#21453;&#21521;&#20256;&#25773;&#65292;&#21542;&#21017;&#29992;&#25143;&#26080;&#27861;&#20351;&#29992;&#33258;&#23450;&#20041;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;DeepLearning.scala 2&#20013;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#65306;1.&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#21253;&#21547;&#22810;&#20010;&#21487;&#35757;&#32451;&#21464;&#37327;&#30340;&#38745;&#24577;&#31867;&#22411;&#20989;&#25968;&#36827;&#34892;&#21453;&#21521;&#27169;&#24335;&#30340;&#33258;&#21160;&#27714;&#23548;&#65292;&#24182;&#19988;&#21487;&#20197;&#33258;&#30001;&#22320;&#19982;&#20803;&#35821;&#35328;&#20114;&#25805;&#20316;&#12290;2.&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#32452;&#21333;&#23376;&#21644;&#21333;&#23376;&#21464;&#25442;&#22120;&#65292;
&lt;/p&gt;
&lt;p&gt;
The Java and Scala community has built a very successful big data ecosystem. However, most of neural networks running on it are modeled in dynamically typed programming languages. These dynamically typed deep learning frameworks treat neural networks as differentiable expressions that contain many trainable variable, and perform automatic differentiation on those expressions when training them.  Until 2019, none of the learning frameworks in statically typed languages provided the expressive power of traditional frameworks. Their users are not able to use custom algorithms unless creating plenty of boilerplate code for hard-coded back-propagation.  We solved this problem in DeepLearning.scala 2. Our contributions are:  1. We discovered a novel approach to perform automatic differentiation in reverse mode for statically typed functions that contain multiple trainable variable, and can interoperate freely with the metalanguage.  2. We designed a set of monads and monad transformers, whic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#23545;&#36779;&#23376;&#21644;&#24179;&#38754;&#36779;&#23376;&#36827;&#34892;&#20998;&#31867;&#65292;&#24471;&#20986;&#20102;&#26032;&#30340;&#20415;&#21033;&#19981;&#21464;&#37327;&#65292;&#21253;&#25324;&#24179;&#38754;&#36779;&#23376;&#30340;&#23436;&#20840;&#19981;&#21464;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.12185</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21457;&#29616;&#36779;&#23376;&#21644;&#24179;&#38754;&#36779;&#23376;&#30340;&#19981;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Machine learning discovers invariants of braids and flat braids. (arXiv:2307.12185v1 [math.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#23545;&#36779;&#23376;&#21644;&#24179;&#38754;&#36779;&#23376;&#36827;&#34892;&#20998;&#31867;&#65292;&#24471;&#20986;&#20102;&#26032;&#30340;&#20415;&#21033;&#19981;&#21464;&#37327;&#65292;&#21253;&#25324;&#24179;&#38754;&#36779;&#23376;&#30340;&#23436;&#20840;&#19981;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#23558;&#36779;&#23376;&#65288;&#25110;&#24179;&#38754;&#36779;&#23376;&#65289;&#30340;&#31034;&#20363;&#20998;&#31867;&#20026;&#24179;&#20961;&#25110;&#38750;&#24179;&#20961;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#37319;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#65288;&#22810;&#23618;&#24863;&#30693;&#22120;&#65289;&#30340;&#30417;&#30563;&#23398;&#20064;&#24418;&#24335;&#12290;&#24403;&#23427;&#20204;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#26102;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#23427;&#20204;&#30340;&#32467;&#26500;&#35299;&#37322;&#20026;&#25968;&#23398;&#29468;&#24819;&#65292;&#28982;&#21518;&#35777;&#26126;&#36825;&#20123;&#29468;&#24819;&#25104;&#20026;&#23450;&#29702;&#12290;&#32467;&#26524;&#65292;&#22312;&#36779;&#23376;&#20013;&#25214;&#21040;&#20102;&#26032;&#30340;&#20415;&#21033;&#19981;&#21464;&#37327;&#65292;&#21253;&#25324;&#24179;&#38754;&#36779;&#23376;&#30340;&#23436;&#20840;&#19981;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use machine learning to classify examples of braids (or flat braids) as trivial or non-trivial. Our ML takes form of supervised learning using neural networks (multilayer perceptrons). When they achieve good results in classification, we are able to interpret their structure as mathematical conjectures and then prove these conjectures as theorems. As a result, we find new convenient invariants of braids, including a complete invariant of flat braids.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#39537;&#21160;&#21644;&#22810;&#19987;&#23478;&#38598;&#25104;&#30340;&#22810;&#27169;&#24577;MR&#33041;&#32959;&#30244;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#33041;&#32959;&#30244;&#21407;&#22411;&#30340;&#25351;&#23548;&#65292;&#31361;&#20986;&#26174;&#31034;&#20102;&#27599;&#20010;&#32959;&#30244;&#20122;&#21306;&#30340;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#30456;&#20114;&#36716;&#31227;&#26426;&#21046;&#35299;&#20915;&#20102;&#21333;&#19968;&#27169;&#24577;&#29305;&#24449;&#20449;&#24687;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.12180</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#22411;&#39537;&#21160;&#21644;&#22810;&#19987;&#23478;&#38598;&#25104;&#30340;&#22810;&#27169;&#24577;MR&#33041;&#32959;&#30244;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Prototype-Driven and Multi-Expert Integrated Multi-Modal MR Brain Tumor Image Segmentation. (arXiv:2307.12180v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#39537;&#21160;&#21644;&#22810;&#19987;&#23478;&#38598;&#25104;&#30340;&#22810;&#27169;&#24577;MR&#33041;&#32959;&#30244;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#33041;&#32959;&#30244;&#21407;&#22411;&#30340;&#25351;&#23548;&#65292;&#31361;&#20986;&#26174;&#31034;&#20102;&#27599;&#20010;&#32959;&#30244;&#20122;&#21306;&#30340;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#30456;&#20114;&#36716;&#31227;&#26426;&#21046;&#35299;&#20915;&#20102;&#21333;&#19968;&#27169;&#24577;&#29305;&#24449;&#20449;&#24687;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22810;&#27169;&#24577;&#30913;&#20849;&#25391;&#65288;MR&#65289;&#33041;&#32959;&#30244;&#22270;&#20687;&#20998;&#21106;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#30452;&#25509;&#20174;&#36755;&#20837;&#22270;&#20687;&#20013;&#25552;&#21462;&#21028;&#21035;&#29305;&#24449;&#65292;&#29992;&#20110;&#30830;&#23450;&#21644;&#23450;&#20301;&#32959;&#30244;&#20122;&#21306;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32959;&#30244;&#20122;&#21306;&#30340;&#30456;&#20114;&#21253;&#21547;&#23548;&#33268;&#30340;&#20449;&#24687;&#37325;&#21472;&#24448;&#24448;&#34987;&#24573;&#35270;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#19981;&#20250;&#38024;&#23545;&#21333;&#19968;&#32959;&#30244;&#20122;&#21306;&#29305;&#24449;&#36827;&#34892;&#37327;&#36523;&#23450;&#21046;&#30340;&#21162;&#21147;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#32959;&#30244;&#21407;&#22411;&#39537;&#21160;&#21644;&#22810;&#19987;&#23478;&#38598;&#25104;&#30340;&#22810;&#27169;&#24577;MR&#33041;&#32959;&#30244;&#20998;&#21106;&#26041;&#27861;&#12290;&#23427;&#21487;&#20197;&#22312;&#32959;&#30244;&#21407;&#22411;&#30340;&#25351;&#23548;&#19979;&#31361;&#20986;&#26174;&#31034;&#27599;&#20010;&#32959;&#30244;&#20122;&#21306;&#30340;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#33719;&#24471;&#20855;&#26377;&#23436;&#25972;&#20449;&#24687;&#30340;&#21407;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20114;&#20256;&#36755;&#26426;&#21046;&#65292;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#29305;&#24449;&#30456;&#20114;&#36716;&#31227;&#65292;&#20197;&#35299;&#20915;&#21333;&#19968;&#27169;&#24577;&#29305;&#24449;&#20449;&#24687;&#19981;&#36275;&#24341;&#36215;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#39537;&#21160;&#30340;&#29305;&#24449;&#34920;&#31034;&#21644;&#34701;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
For multi-modal magnetic resonance (MR) brain tumor image segmentation, current methods usually directly extract the discriminative features from input images for tumor sub-region category determination and localization. However, the impact of information aliasing caused by the mutual inclusion of tumor sub-regions is often ignored. Moreover, existing methods usually do not take tailored efforts to highlight the single tumor sub-region features. To this end, a multi-modal MR brain tumor segmentation method with tumor prototype-driven and multi-expert integration is proposed. It could highlight the features of each tumor sub-region under the guidance of tumor prototypes. Specifically, to obtain the prototypes with complete information, we propose a mutual transmission mechanism to transfer different modal features to each other to address the issues raised by insufficient information on single-modal features. Furthermore, we devise a prototype-driven feature representation and fusion me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LtC&#30340;&#21327;&#20316;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#26469;&#26377;&#25928;&#20943;&#23567;&#27969;&#23186;&#20307;&#35270;&#39057;&#30340;&#27969;&#37327;&#65292;&#25552;&#39640;&#27969;&#23186;&#20307;&#35270;&#39057;&#20998;&#26512;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.12171</link><description>&lt;p&gt;
&#23398;&#20064;&#21387;&#32553;&#65288;LtC&#65289;&#65306;&#39640;&#25928;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#27969;&#23186;&#20307;&#35270;&#39057;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Learn to Compress (LtC): Efficient Learning-based Streaming Video Analytics. (arXiv:2307.12171v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LtC&#30340;&#21327;&#20316;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#26469;&#26377;&#25928;&#20943;&#23567;&#27969;&#23186;&#20307;&#35270;&#39057;&#30340;&#27969;&#37327;&#65292;&#25552;&#39640;&#27969;&#23186;&#20307;&#35270;&#39057;&#20998;&#26512;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20998;&#26512;&#36890;&#24120;&#20316;&#20026;&#36793;&#32536;&#35774;&#32622;&#20013;&#30340;&#20113;&#26381;&#21153;&#36827;&#34892;&#65292;&#20027;&#35201;&#26159;&#20026;&#20102;&#21368;&#36733;&#35745;&#31639;&#36127;&#36733;&#65292;&#24182;&#19988;&#22312;&#35270;&#39057;&#20256;&#24863;&#22120;&#19981;&#30452;&#25509;&#20351;&#29992;&#32467;&#26524;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#12290;&#20174;&#36793;&#32536;&#35774;&#22791;&#21457;&#36865;&#39640;&#36136;&#37327;&#30340;&#35270;&#39057;&#25968;&#25454;&#22312;&#24102;&#23485;&#21644;&#21151;&#32791;&#26041;&#38754;&#37117;&#20250;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#26500;&#24314;&#19968;&#20010;&#39640;&#25928;&#21033;&#29992;&#36825;&#20123;&#36164;&#28304;&#30340;&#27969;&#23186;&#20307;&#35270;&#39057;&#20998;&#26512;&#27969;&#27700;&#32447;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#20943;&#23567;&#35270;&#39057;&#27969;&#30340;&#22823;&#23567;&#12290;&#20256;&#32479;&#30340;&#35270;&#39057;&#21387;&#32553;&#31639;&#27861;&#23545;&#35270;&#39057;&#30340;&#35821;&#20041;&#19981;&#25935;&#24863;&#65292;&#21487;&#33021;&#26082;&#20302;&#25928;&#21448;&#23545;&#20998;&#26512;&#24615;&#33021;&#26377;&#23475;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LtC&#65292;&#36825;&#26159;&#35270;&#39057;&#28304;&#21644;&#20998;&#26512;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#22312;&#20998;&#26512;&#27969;&#27700;&#32447;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;&#26469;&#20943;&#23567;&#35270;&#39057;&#27969;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LtC&#23558;&#26381;&#21153;&#22120;&#19978;&#30340;&#20840;&#21151;&#33021;&#20998;&#26512;&#31639;&#27861;&#20316;&#20026;&#25945;&#24072;&#65292;&#35757;&#32451;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#23398;&#29983;&#31070;&#32463;&#32593;&#32476;&#65292;&#28982;&#21518;&#37096;&#32626;&#22312;&#35270;&#39057;&#28304;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video analytics are often performed as cloud services in edge settings, mainly to offload computation, and also in situations where the results are not directly consumed at the video sensors. Sending high-quality video data from the edge devices can be expensive both in terms of bandwidth and power use. In order to build a streaming video analytics pipeline that makes efficient use of these resources, it is therefore imperative to reduce the size of the video stream. Traditional video compression algorithms are unaware of the semantics of the video, and can be both inefficient and harmful for the analytics performance. In this paper, we introduce LtC, a collaborative framework between the video source and the analytics server, that efficiently learns to reduce the video streams within an analytics pipeline. Specifically, LtC uses the full-fledged analytics algorithm at the server as a teacher to train a lightweight student neural network, which is then deployed at the video source. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20010;&#26550;&#26500;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#20449;&#38656;&#27714;&#65292;&#23558;&#38598;&#32676;&#20998;&#21106;&#25104;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#36712;&#36947;&#36830;&#25509;&#20165;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#65292;&#20174;&#32780;&#38477;&#20302;&#32593;&#32476;&#25104;&#26412;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.12169</link><description>&lt;p&gt;
&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Optimized Network Architectures for Large Language Model Training with Billions of Parameters. (arXiv:2307.12169v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20010;&#26550;&#26500;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#20449;&#38656;&#27714;&#65292;&#23558;&#38598;&#32676;&#20998;&#21106;&#25104;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#36712;&#36947;&#36830;&#25509;&#20165;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#65292;&#20174;&#32780;&#38477;&#20302;&#32593;&#32476;&#25104;&#26412;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25361;&#25112;&#20102;&#20026;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26500;&#24314;&#20219;&#24847;&#21040;&#20219;&#24847;&#32593;&#32476;&#30340;&#20256;&#32479;&#33539;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#21576;&#29616;&#20986;&#19968;&#31181;&#29420;&#29305;&#30340;&#36890;&#20449;&#27169;&#24335;&#65292;&#22312;&#20854;&#20013;&#65292;&#21482;&#26377;&#23567;&#32452;&#30340;GPU&#38656;&#35201;&#39640;&#24102;&#23485;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;&#36890;&#20449;&#65292;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#22312;&#36825;&#20123;GPU&#23567;&#32452;&#20043;&#38388;&#65292;&#36890;&#20449;&#38750;&#24120;&#24494;&#19981;&#36275;&#36947;&#12289;&#31232;&#30095;&#19988;&#22343;&#21248;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#32039;&#23494;&#21305;&#37197;LLMs&#30340;&#36890;&#20449;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#23558;&#38598;&#32676;&#20998;&#21106;&#20026;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#20219;&#24847;&#21040;&#20219;&#24847;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;HB&#22495;&#12290;&#22312;HB&#22495;&#20043;&#38388;&#65292;&#32593;&#32476;&#21482;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#32593;&#32476;&#36830;&#25509;&#31216;&#20026;&#8220;&#20165;&#36712;&#36947;&#36830;&#25509;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;Clos&#32593;&#32476;&#21487;&#20197;&#23558;&#32593;&#32476;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;LLM&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper challenges the well-established paradigm for building any-to-any networks for training Large Language Models (LLMs). We show that LLMs exhibit a unique communication pattern where only small groups of GPUs require high-bandwidth any-to-any communication within them, to achieve near-optimal training performance. Across these groups of GPUs, the communication is insignificant, sparse, and homogeneous. We propose a new network architecture that closely resembles the communication requirement of LLMs. Our architecture partitions the cluster into sets of GPUs interconnected with non-blocking any-to-any high-bandwidth interconnects that we call HB domains. Across the HB domains, the network only connects GPUs with communication demands. We call this network a "rail-only" connection, and show that our proposed architecture reduces the network cost by up to 75% compared to the state-of-the-art any-to-any Clos networks without compromising the performance of LLM training.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#32908;&#33806;&#32553;&#20391;&#32034;&#30828;&#21270;&#65288;ALS&#65289;&#30340;&#38754;&#37096;&#20851;&#38190;&#28857;&#22270;&#26041;&#27861;&#12290;&#36890;&#36807;&#20998;&#26512;&#24739;&#32773;&#30340;&#38754;&#37096;&#34920;&#24773;&#65292;&#21487;&#20197;&#26356;&#31616;&#21333;&#21644;&#26356;&#20415;&#23452;&#22320;&#35786;&#26029;&#21644;&#26816;&#27979;ALS&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20262;&#22810;&#31070;&#32463;&#33080;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#24102;&#26469;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2307.12159</link><description>&lt;p&gt;
&#38754;&#37096;&#20851;&#38190;&#28857;&#22270;&#29992;&#20110;&#32908;&#33806;&#32553;&#20391;&#32034;&#30828;&#21270;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Facial Point Graphs for Amyotrophic Lateral Sclerosis Identification. (arXiv:2307.12159v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12159
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#32908;&#33806;&#32553;&#20391;&#32034;&#30828;&#21270;&#65288;ALS&#65289;&#30340;&#38754;&#37096;&#20851;&#38190;&#28857;&#22270;&#26041;&#27861;&#12290;&#36890;&#36807;&#20998;&#26512;&#24739;&#32773;&#30340;&#38754;&#37096;&#34920;&#24773;&#65292;&#21487;&#20197;&#26356;&#31616;&#21333;&#21644;&#26356;&#20415;&#23452;&#22320;&#35786;&#26029;&#21644;&#26816;&#27979;ALS&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20262;&#22810;&#31070;&#32463;&#33080;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#24102;&#26469;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32908;&#33806;&#32553;&#20391;&#32034;&#30828;&#21270;&#65288;ALS&#65289;&#30340;&#26089;&#26399;&#38454;&#27573;&#35782;&#21035;&#32908;&#33806;&#32553;&#20391;&#32034;&#30828;&#21270;&#65288;ALS&#65289;&#23545;&#20110;&#30830;&#23450;&#27835;&#30103;&#30340;&#24320;&#22987;&#12289;&#20016;&#23500;&#21069;&#26223;&#21644;&#25552;&#39640;&#21463;&#24433;&#21709;&#20010;&#20307;&#30340;&#25972;&#20307;&#24184;&#31119;&#24863;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26089;&#26399;&#35786;&#26029;&#21644;&#26816;&#27979;&#30142;&#30149;&#30340;&#36857;&#35937;&#24182;&#19981;&#30452;&#25509;&#12290;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#20998;&#26512;&#24739;&#32773;&#30340;&#38754;&#37096;&#34920;&#24773;&#21487;&#20197;&#25552;&#20379;&#19968;&#31181;&#26356;&#31616;&#21333;&#21644;&#26356;&#20415;&#23452;&#30340;&#26041;&#24335;&#12290;&#24403;ALS&#24739;&#32773;&#36827;&#34892;&#29305;&#23450;&#30340;&#21160;&#20316;&#65292;&#20363;&#22914;&#24352;&#22068;&#65292;&#29305;&#23450;&#38754;&#37096;&#32908;&#32905;&#30340;&#36816;&#21160;&#19982;&#20581;&#24247;&#20010;&#20307;&#20013;&#35266;&#23519;&#21040;&#30340;&#19981;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#37096;&#20851;&#38190;&#28857;&#22270;&#20197;&#20174;&#38754;&#37096;&#22270;&#20687;&#30340;&#20960;&#20309;&#23398;&#20013;&#23398;&#20064;&#20449;&#24687;&#20197;&#33258;&#21160;&#35782;&#21035;ALS&#12290;&#22810;&#20262;&#22810;&#31070;&#32463;&#33080;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20419;&#36827;&#20102;&#35813;&#39046;&#22495;&#30340;&#26377;&#24076;&#26395;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying Amyotrophic Lateral Sclerosis (ALS) in its early stages is essential for establishing the beginning of treatment, enriching the outlook, and enhancing the overall well-being of those affected individuals. However, early diagnosis and detecting the disease's signs is not straightforward. A simpler and cheaper way arises by analyzing the patient's facial expressions through computational methods. When a patient with ALS engages in specific actions, e.g., opening their mouth, the movement of specific facial muscles differs from that observed in a healthy individual. This paper proposes Facial Point Graphs to learn information from the geometry of facial images to identify ALS automatically. The experimental outcomes in the Toronto Neuroface dataset show the proposed approach outperformed state-of-the-art results, fostering promising developments in the area.
&lt;/p&gt;</description></item><item><title>DIP-RL&#26159;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#28436;&#31034;&#30340;&#31639;&#27861;&#65292;&#22312;&#38750;&#32467;&#26500;&#21270;&#21644;&#24320;&#25918;&#30340;&#29615;&#22659;&#20013;&#36890;&#36807;&#22810;&#31181;&#26041;&#24335;&#25512;&#23548;&#20559;&#22909;&#24182;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#20854;&#22312;Minecraft&#20013;&#30340;&#30733;&#26641;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.12158</link><description>&lt;p&gt;
DIP-RL&#65306;&#22312;Minecraft&#20013;&#30340;&#28436;&#31034;&#25512;&#23548;&#20559;&#22909;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DIP-RL: Demonstration-Inferred Preference Learning in Minecraft. (arXiv:2307.12158v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12158
&lt;/p&gt;
&lt;p&gt;
DIP-RL&#26159;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#28436;&#31034;&#30340;&#31639;&#27861;&#65292;&#22312;&#38750;&#32467;&#26500;&#21270;&#21644;&#24320;&#25918;&#30340;&#29615;&#22659;&#20013;&#36890;&#36807;&#22810;&#31181;&#26041;&#24335;&#25512;&#23548;&#20559;&#22909;&#24182;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#20854;&#22312;Minecraft&#20013;&#30340;&#30733;&#26641;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#31639;&#27861;&#20195;&#29702;&#36890;&#36807;&#25509;&#25910;&#22870;&#21169;&#20449;&#21495;&#30340;&#21453;&#39304;&#26469;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#38750;&#32467;&#26500;&#21270;&#30340;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#65292;&#36825;&#26679;&#30340;&#22870;&#21169;&#20449;&#21495;&#26159;&#26410;&#30693;&#30340;&#65292;&#24182;&#19988;&#20154;&#31867;&#26080;&#27861;&#21487;&#38752;&#22320;&#26500;&#24314;&#19968;&#20010;&#27491;&#30830;&#25429;&#25417;&#25152;&#38656;&#34892;&#20026;&#30340;&#22870;&#21169;&#20449;&#21495;&#12290;&#20026;&#20102;&#22312;&#36825;&#26679;&#30340;&#38750;&#32467;&#26500;&#21270;&#21644;&#24320;&#25918;&#30340;&#29615;&#22659;&#20013;&#23436;&#25104;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Demonstration-Inferred Preference Reinforcement Learning (DIP-RL)&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#28436;&#31034;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#28436;&#31034;&#25968;&#25454;&#31181;&#23376;&#24378;&#21270;&#23398;&#20064; (RL)&#35757;&#32451;&#25209;&#27425;&#65292;&#24182;&#25512;&#23548;&#20986;&#20559;&#22909;&#20197;&#23398;&#20064;&#24341;&#23548;RL&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;Minecraft&#20013;&#30340;&#30733;&#26641;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;DIP-RL&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25351;&#23548;RL&#20195;&#29702;&#23398;&#20064;&#19968;&#20010;&#21453;&#26144;&#20154;&#31867;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;DIP-RL&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning for sequential decision-making, an algorithmic agent learns to interact with an environment while receiving feedback in the form of a reward signal. However, in many unstructured real-world settings, such a reward signal is unknown and humans cannot reliably craft a reward signal that correctly captures desired behavior. To solve tasks in such unstructured and open-ended environments, we present Demonstration-Inferred Preference Reinforcement Learning (DIP-RL), an algorithm that leverages human demonstrations in three distinct ways, including training an autoencoder, seeding reinforcement learning (RL) training batches with demonstration data, and inferring preferences over behaviors to learn a reward function to guide RL. We evaluate DIP-RL in a tree-chopping task in Minecraft. Results suggest that the method can guide an RL agent to learn a reward function that reflects human preferences and that DIP-RL performs competitively relative to baselines. DIP-RL is inspi
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#23545;&#22810;&#23618;&#32423;&#20379;&#24212;&#38142;&#20013;&#24863;&#20852;&#36259;&#25351;&#26631;&#30340;&#20272;&#35745;&#36129;&#29486;&#36827;&#34892;&#35745;&#31639;&#65292;&#26080;&#38656;&#25968;&#25454;&#20849;&#20139;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#36136;&#37327;&#21464;&#21270;&#30340;&#21407;&#22240;&#26041;&#38754;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.12157</link><description>&lt;p&gt;
&#22312;&#22810;&#23618;&#32423;&#29615;&#22659;&#20013;&#35782;&#21035;&#20379;&#24212;&#38142;&#32467;&#26524;&#30340;&#36129;&#29486;&#32773;&#65306;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Identifying contributors to supply chain outcomes in a multi-echelon setting: a decentralised approach. (arXiv:2307.12157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12157
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#23545;&#22810;&#23618;&#32423;&#20379;&#24212;&#38142;&#20013;&#24863;&#20852;&#36259;&#25351;&#26631;&#30340;&#20272;&#35745;&#36129;&#29486;&#36827;&#34892;&#35745;&#31639;&#65292;&#26080;&#38656;&#25968;&#25454;&#20849;&#20139;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#36136;&#37327;&#21464;&#21270;&#30340;&#21407;&#22240;&#26041;&#38754;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#32463;&#24120;&#38590;&#20197;&#30830;&#23450;&#20135;&#21697;&#36136;&#37327;&#21644;&#20132;&#36135;&#26102;&#38388;&#31561;&#25351;&#26631;&#21464;&#21270;&#30340;&#21407;&#22240;&#12290;&#24403;&#21407;&#22240;&#20301;&#20110;&#20844;&#21496;&#36793;&#30028;&#20043;&#22806;&#65292;&#22788;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#22810;&#23618;&#32423;&#20379;&#24212;&#38142;&#20013;&#26102;&#65292;&#36825;&#39033;&#20219;&#21153;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#20379;&#24212;&#38142;&#31649;&#29702;&#20027;&#24352;&#36890;&#36807;&#25968;&#25454;&#20849;&#20139;&#33719;&#24471;&#26356;&#22909;&#30340;&#27934;&#23519;&#21147;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#24182;&#19981;&#24120;&#35265;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26469;&#23454;&#29616;&#23545;&#22810;&#38454;&#27573;&#29983;&#20135;&#36807;&#31243;&#20013;&#24863;&#20852;&#36259;&#25351;&#26631;&#30340;&#20272;&#35745;&#36129;&#29486;&#30340;&#21435;&#20013;&#24515;&#21270;&#35745;&#31639;&#12290;&#36825;&#31181;&#26041;&#27861;&#20943;&#36731;&#20102;&#35828;&#26381;&#20379;&#24212;&#38142;&#21442;&#19982;&#32773;&#20849;&#20139;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#22240;&#20026;&#25152;&#26377;&#35745;&#31639;&#37117;&#26159;&#20197;&#21435;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#36827;&#34892;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#23545;&#30495;&#23454;&#22810;&#38454;&#27573;&#21046;&#36896;&#36807;&#31243;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#36827;&#34892;&#32463;&#39564;&#35777;&#23454;&#30340;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20351;&#29992;&#38598;&#20013;&#21270;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#27979;&#36136;&#37327;&#21464;&#21270;&#30340;&#21407;&#22240;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Organisations often struggle to identify the causes of change in metrics such as product quality and delivery duration. This task becomes increasingly challenging when the cause lies outside of company borders in multi-echelon supply chains that are only partially observable. Although traditional supply chain management has advocated for data sharing to gain better insights, this does not take place in practice due to data privacy concerns. We propose the use of explainable artificial intelligence for decentralised computing of estimated contributions to a metric of interest in a multi-stage production process. This approach mitigates the need to convince supply chain actors to share data, as all computations occur in a decentralised manner. Our method is empirically validated using data collected from a real multi-stage manufacturing process. The results demonstrate the effectiveness of our approach in detecting the source of quality variations compared to a centralised approach using
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#23454;&#29616;&#23454;&#26102;&#31070;&#32463;&#35270;&#39057;&#24674;&#22797;&#21644;&#22686;&#24378;&#30340;&#26032;&#26041;&#27861;&#65292;&#21253;&#25324;&#35270;&#39057;&#24103;&#24674;&#22797;&#26041;&#26696;&#12289;&#36229;&#20998;&#36776;&#29575;&#31639;&#27861;&#21644;&#22686;&#24378;&#24863;&#30693;&#30340;&#35270;&#39057;&#27604;&#29305;&#29575;&#33258;&#36866;&#24212;&#31639;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25903;&#25345;&#27599;&#31186;30&#24103;&#65292;&#24182;&#22312;&#19981;&#21516;&#32593;&#32476;&#29615;&#22659;&#19979;&#23454;&#29616;&#23454;&#26102;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2307.12152</link><description>&lt;p&gt;
&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#23454;&#26102;&#31070;&#32463;&#35270;&#39057;&#24674;&#22797;&#21644;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Real-Time Neural Video Recovery and Enhancement on Mobile Devices. (arXiv:2307.12152v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#23454;&#29616;&#23454;&#26102;&#31070;&#32463;&#35270;&#39057;&#24674;&#22797;&#21644;&#22686;&#24378;&#30340;&#26032;&#26041;&#27861;&#65292;&#21253;&#25324;&#35270;&#39057;&#24103;&#24674;&#22797;&#26041;&#26696;&#12289;&#36229;&#20998;&#36776;&#29575;&#31639;&#27861;&#21644;&#22686;&#24378;&#24863;&#30693;&#30340;&#35270;&#39057;&#27604;&#29305;&#29575;&#33258;&#36866;&#24212;&#31639;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25903;&#25345;&#27599;&#31186;30&#24103;&#65292;&#24182;&#22312;&#19981;&#21516;&#32593;&#32476;&#29615;&#22659;&#19979;&#23454;&#29616;&#23454;&#26102;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31227;&#21160;&#35774;&#22791;&#22312;&#35270;&#39057;&#27969;&#23186;&#20307;&#26041;&#38754;&#30340;&#21463;&#27426;&#36814;&#31243;&#24230;&#19981;&#26029;&#22686;&#21152;&#65292;&#20248;&#21270;&#36825;&#20123;&#35774;&#22791;&#30340;&#27969;&#23186;&#20307;&#20307;&#39564;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35270;&#39057;&#22686;&#24378;&#25216;&#26415;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#22823;&#22810;&#25968;&#25216;&#26415;&#19981;&#33021;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#25903;&#25345;&#23454;&#26102;&#22686;&#24378;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#25216;&#26415;&#20165;&#20851;&#27880;&#36229;&#20998;&#36776;&#29575;&#65292;&#26080;&#27861;&#22788;&#29702;&#35270;&#39057;&#24103;&#30340;&#37096;&#20998;&#25110;&#23436;&#20840;&#20002;&#22833;&#25110;&#25439;&#22351;&#65292;&#32780;&#36825;&#22312;&#20114;&#32852;&#32593;&#21644;&#26080;&#32447;&#32593;&#32476;&#19978;&#38750;&#24120;&#24120;&#35265;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#65288;i&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#39057;&#24103;&#24674;&#22797;&#26041;&#26696;&#65292;&#65288;ii&#65289;&#19968;&#31181;&#26032;&#30340;&#36229;&#20998;&#36776;&#29575;&#31639;&#27861;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#19968;&#31181;&#25509;&#25910;&#22686;&#24378;&#24863;&#30693;&#30340;&#35270;&#39057;&#27604;&#29305;&#29575;&#33258;&#36866;&#24212;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;iPhone 12&#19978;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#27599;&#31186;30&#24103;&#65288;FPS&#65289;&#12290;&#25105;&#20204;&#22312;WiFi&#12289;3G&#12289;4G&#21644;5G&#32593;&#32476;&#31561;&#19981;&#21516;&#32593;&#32476;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#23454;&#26102;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
As mobile devices become increasingly popular for video streaming, it's crucial to optimize the streaming experience for these devices. Although deep learning-based video enhancement techniques are gaining attention, most of them cannot support real-time enhancement on mobile devices. Additionally, many of these techniques are focused solely on super-resolution and cannot handle partial or complete loss or corruption of video frames, which is common on the Internet and wireless networks.  To overcome these challenges, we present a novel approach in this paper. Our approach consists of (i) a novel video frame recovery scheme, (ii) a new super-resolution algorithm, and (iii) a receiver enhancement-aware video bit rate adaptation algorithm. We have implemented our approach on an iPhone 12, and it can support 30 frames per second (FPS). We have evaluated our approach in various networks such as WiFi, 3G, 4G, and 5G networks. Our evaluation shows that our approach enables real-time enhancem
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24322;&#36136;&#27169;&#22411;&#21644;&#19981;&#21487;&#29992;&#33410;&#28857;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#26435;&#37325;&#25237;&#24433;&#21040;&#20849;&#21516;&#30340;&#28508;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#37325;&#26500;&#25439;&#22833;&#21644;&#26368;&#22823;&#21270;&#30456;&#20851;&#24615;&#26469;&#23454;&#29616;&#27169;&#22411;&#30340;&#22635;&#34917;&#21644;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.12149</link><description>&lt;p&gt;
CorrFL: &#24322;&#36136;IoT&#29615;&#22659;&#20013;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#21487;&#29992;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
CorrFL: Correlation-Based Neural Network Architecture for Unavailability Concerns in a Heterogeneous IoT Environment. (arXiv:2307.12149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24322;&#36136;&#27169;&#22411;&#21644;&#19981;&#21487;&#29992;&#33410;&#28857;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#26435;&#37325;&#25237;&#24433;&#21040;&#20849;&#21516;&#30340;&#28508;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#37325;&#26500;&#25439;&#22833;&#21644;&#26368;&#22823;&#21270;&#30456;&#20851;&#24615;&#26469;&#23454;&#29616;&#27169;&#22411;&#30340;&#22635;&#34917;&#21644;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Federated Learning, FL)&#33539;&#24335;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#38754;&#20020;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#26412;&#22320;&#27169;&#22411;&#26550;&#26500;&#30340;&#24322;&#36136;&#24615;&#20197;&#21450;&#30001;&#20110;&#36830;&#25509;&#38382;&#39064;&#23548;&#33268;&#20998;&#24067;&#24335;&#29289;&#32852;&#32593;(IoT)&#33410;&#28857;&#19981;&#21487;&#29992;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#22240;&#32032;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#8220;&#22914;&#20309;&#22635;&#34917;&#19981;&#21487;&#29992;&#27169;&#22411;&#30340;&#35757;&#32451;&#24046;&#36317;&#65311;&#8221;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#8220;&#26012;&#32447;&#32852;&#21512;&#23398;&#20064;&#8221;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21463;&#34920;&#24449;&#23398;&#20064;&#39046;&#22495;&#24433;&#21709;&#30340;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;FL (CorrFL)&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;CorrFL&#23558;&#21508;&#31181;&#27169;&#22411;&#26435;&#37325;&#25237;&#24433;&#21040;&#19968;&#20010;&#20849;&#21516;&#30340;&#28508;&#31354;&#38388;&#20013;&#65292;&#20197;&#35299;&#20915;&#27169;&#22411;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#20854;&#25439;&#22833;&#20989;&#25968;&#22312;&#27169;&#22411;&#32570;&#22833;&#26102;&#26368;&#23567;&#21270;&#37325;&#26500;&#25439;&#22833;&#65292;&#24182;&#26368;&#22823;&#21270;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Federated Learning (FL) paradigm faces several challenges that limit its application in real-world environments. These challenges include the local models' architecture heterogeneity and the unavailability of distributed Internet of Things (IoT) nodes due to connectivity problems. These factors posit the question of "how can the available models fill the training gap of the unavailable models?". This question is referred to as the "Oblique Federated Learning" problem. This problem is encountered in the studied environment that includes distributed IoT nodes responsible for predicting CO2 concentrations. This paper proposes the Correlation-based FL (CorrFL) approach influenced by the representational learning field to address this problem. CorrFL projects the various model weights to a common latent space to address the model heterogeneity. Its loss function minimizes the reconstruction loss when models are absent and maximizes the correlation between the generated models. The latte
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#24555;&#29031;&#39640;&#20809;&#35889;&#25104;&#20687;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27827;&#27969;&#29615;&#22659;&#20013;&#39640;&#25928;&#33258;&#21160;&#21270;&#30417;&#27979;&#23439;&#35266;&#22609;&#26009;&#22403;&#22334;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#26816;&#27979;&#31934;&#24230;&#65292;&#20026;&#35299;&#20915;&#20840;&#29699;&#33539;&#22260;&#30340;&#28418;&#28014;&#22403;&#22334;&#38382;&#39064;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.12145</link><description>&lt;p&gt;
&#19968;&#20010;&#28165;&#27905;&#27827;&#27969;&#30340;&#24895;&#26223;&#65306;&#21033;&#29992;&#24555;&#29031;&#39640;&#20809;&#35889;&#25104;&#20687;&#25216;&#26415;&#26816;&#27979;&#23439;&#35266;&#22609;&#26009;&#22403;&#22334;
&lt;/p&gt;
&lt;p&gt;
A Vision for Cleaner Rivers: Harnessing Snapshot Hyperspectral Imaging to Detect Macro-Plastic Litter. (arXiv:2307.12145v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12145
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#24555;&#29031;&#39640;&#20809;&#35889;&#25104;&#20687;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27827;&#27969;&#29615;&#22659;&#20013;&#39640;&#25928;&#33258;&#21160;&#21270;&#30417;&#27979;&#23439;&#35266;&#22609;&#26009;&#22403;&#22334;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#26816;&#27979;&#31934;&#24230;&#65292;&#20026;&#35299;&#20915;&#20840;&#29699;&#33539;&#22260;&#30340;&#28418;&#28014;&#22403;&#22334;&#38382;&#39064;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#20837;&#27827;&#27969;&#30340;&#22609;&#26009;&#24223;&#29289;&#23545;&#24403;&#22320;&#29983;&#24577;&#31995;&#32479;&#36896;&#25104;&#20102;&#20260;&#23475;&#65292;&#23548;&#33268;&#20102;&#36127;&#38754;&#30340;&#29983;&#24577;&#21644;&#32463;&#27982;&#24433;&#21709;&#12290;&#22823;&#37327;&#30340;&#22609;&#26009;&#24223;&#29289;&#20174;&#20869;&#38470;&#36816;&#36755;&#21040;&#28023;&#27915;&#65292;&#24418;&#25104;&#20102;&#20840;&#29699;&#33539;&#22260;&#30340;&#28418;&#28014;&#22403;&#22334;&#22330;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#39640;&#25928;&#33258;&#21160;&#21270;&#22320;&#30417;&#27979;&#31649;&#29702;&#19981;&#21892;&#30340;&#22609;&#26009;&#24223;&#29289;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#31867;&#20284;&#27827;&#27969;&#30340;&#29615;&#22659;&#20013;&#21033;&#29992;&#35745;&#31639;&#25104;&#20687;&#26041;&#27861;&#26816;&#27979;&#23439;&#35266;&#22609;&#26009;&#22403;&#22334;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#24555;&#29031;&#21487;&#35265;-&#30701;&#27874;&#32418;&#22806;&#39640;&#20809;&#35889;&#25104;&#20687;&#23454;&#29616;&#20102;&#37096;&#20998;&#28024;&#27809;&#22609;&#26009;&#30340;&#36817;&#23454;&#26102;&#36319;&#36394;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#30340;&#25104;&#20687;&#31574;&#30053;&#21487;&#20197;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#23454;&#29616;&#39640;&#26816;&#27979;&#31934;&#24230;&#65292;&#23588;&#20854;&#26159;&#22312;&#21033;&#29992;&#39640;&#20809;&#35889;&#25968;&#25454;&#21644;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#26102;&#12290;&#25152;&#26377;&#30340;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#27169;&#22411;&#37117;&#21487;&#20197;&#22312;&#32447;&#19978;&#33719;&#21462;&#65306;https://github.com/RIVeR-Lab/hyperspectral_macro_plastic_detection
&lt;/p&gt;
&lt;p&gt;
Plastic waste entering the riverine harms local ecosystems leading to negative ecological and economic impacts. Large parcels of plastic waste are transported from inland to oceans leading to a global scale problem of floating debris fields. In this context, efficient and automatized monitoring of mismanaged plastic waste is paramount. To address this problem, we analyze the feasibility of macro-plastic litter detection using computational imaging approaches in river-like scenarios. We enable near-real-time tracking of partially submerged plastics by using snapshot Visible-Shortwave Infrared hyperspectral imaging. Our experiments indicate that imaging strategies associated with machine learning classification approaches can lead to high detection accuracy even in challenging scenarios, especially when leveraging hyperspectral data and nonlinear classifiers. All code, data, and models are available online: https://github.com/RIVeR-Lab/hyperspectral_macro_plastic_detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20013;&#33258;&#36866;&#24212;&#26172;&#22812;&#33410;&#24459;&#30340;&#20986;&#29616;&#65292;&#36890;&#36807;&#22312;&#21608;&#26399;&#24615;&#21464;&#21270;&#29615;&#22659;&#20013;&#36827;&#34892;&#35269;&#39135;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#26234;&#33021;&#20307;&#33021;&#22815;&#20869;&#21270;&#29615;&#22659;&#20449;&#21495;&#24182;&#36866;&#24212;&#30456;&#20301;&#21464;&#21270;&#65292;&#36825;&#19968;&#33258;&#36866;&#24212;&#36807;&#31243;&#26159;&#36890;&#36807;&#20154;&#24037;&#31070;&#32463;&#20803;&#30340;&#21160;&#21147;&#23398;&#26469;&#23454;&#29616;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.12143</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#33258;&#36866;&#24212;&#26172;&#22812;&#33410;&#24459;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Emergence of Adaptive Circadian Rhythms in Deep Reinforcement Learning. (arXiv:2307.12143v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20013;&#33258;&#36866;&#24212;&#26172;&#22812;&#33410;&#24459;&#30340;&#20986;&#29616;&#65292;&#36890;&#36807;&#22312;&#21608;&#26399;&#24615;&#21464;&#21270;&#29615;&#22659;&#20013;&#36827;&#34892;&#35269;&#39135;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#26234;&#33021;&#20307;&#33021;&#22815;&#20869;&#21270;&#29615;&#22659;&#20449;&#21495;&#24182;&#36866;&#24212;&#30456;&#20301;&#21464;&#21270;&#65292;&#36825;&#19968;&#33258;&#36866;&#24212;&#36807;&#31243;&#26159;&#36890;&#36807;&#20154;&#24037;&#31070;&#32463;&#20803;&#30340;&#21160;&#21147;&#23398;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#29615;&#22659;&#30340;&#35268;&#24459;&#23545;&#29983;&#29289;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#20351;&#20854;&#33021;&#22815;&#39044;&#27979;&#20107;&#20214;&#21644;&#21046;&#23450;&#35745;&#21010;&#12290;&#20854;&#20013;&#19968;&#20010;&#26174;&#33879;&#30340;&#20363;&#23376;&#26159;&#29983;&#29289;&#23545;&#22320;&#29699;&#33258;&#36716;24&#23567;&#26102;&#21608;&#26399;&#30340;&#20869;&#21270;&#65292;&#21363;&#26172;&#22812;&#33410;&#24459;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20013;&#31867;&#20284;&#26172;&#22812;&#33410;&#24459;&#30340;&#20986;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#21608;&#26399;&#24615;&#21464;&#21270;&#21487;&#38752;&#30340;&#29615;&#22659;&#20013;&#37096;&#32626;&#20102;&#26234;&#33021;&#20307;&#65292;&#24182;&#35299;&#20915;&#20102;&#19968;&#20010;&#35269;&#39135;&#20219;&#21153;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#34920;&#24449;&#20102;&#26234;&#33021;&#20307;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#34892;&#20026;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20010;&#20869;&#28304;&#24615;&#19988;&#21487;&#35843;&#21305;&#30340;&#33410;&#24459;&#30340;&#20986;&#29616;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#20869;&#37096;&#33410;&#24459;&#36866;&#24212;&#20102;&#29615;&#22659;&#20449;&#21495;&#30456;&#20301;&#30340;&#21464;&#21270;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#23700;&#21644;&#30456;&#20301;&#21709;&#24212;&#26354;&#32447;&#20998;&#26512;&#23637;&#31034;&#20102;&#20154;&#24037;&#31070;&#32463;&#20803;&#22914;&#20309;&#21457;&#23637;&#20986;&#25903;&#25345;&#29615;&#22659;&#33410;&#24459;&#20869;&#21270;&#30340;&#21160;&#21147;&#23398;&#12290;&#20174;&#21160;&#21147;&#23398;&#31995;&#32479;&#35270;&#35282;&#30475;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27492;&#33258;&#36866;&#24212;&#36807;&#31243;&#30340;&#23454;&#29616;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapting to regularities of the environment is critical for biological organisms to anticipate events and plan. A prominent example is the circadian rhythm corresponding to the internalization by organisms of the $24$-hour period of the Earth's rotation. In this work, we study the emergence of circadian-like rhythms in deep reinforcement learning agents. In particular, we deployed agents in an environment with a reliable periodic variation while solving a foraging task. We systematically characterize the agent's behavior during learning and demonstrate the emergence of a rhythm that is endogenous and entrainable. Interestingly, the internal rhythm adapts to shifts in the phase of the environmental signal without any re-training. Furthermore, we show via bifurcation and phase response curve analyses how artificial neurons develop dynamics to support the internalization of the environmental rhythm. From a dynamical systems view, we demonstrate that the adaptation proceeds by the emergenc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#19977;&#32500;&#35013;&#36733;&#26377;&#38480;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#26377;&#25928;&#35745;&#31639;&#20849;&#36733;&#21644;&#36335;&#24452;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#37322;&#25918;&#20102;&#30899;&#20943;&#25490;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.12136</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#37322;&#25918;&#19977;&#32500;&#35013;&#36733;&#26377;&#38480;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#30899;&#20943;&#25490;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlocking Carbon Reduction Potential with Reinforcement Learning for the Three-Dimensional Loading Capacitated Vehicle Routing Problem. (arXiv:2307.12136v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12136
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#19977;&#32500;&#35013;&#36733;&#26377;&#38480;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#26377;&#25928;&#35745;&#31639;&#20849;&#36733;&#21644;&#36335;&#24452;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#37322;&#25918;&#20102;&#30899;&#20943;&#25490;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#22411;&#36135;&#36710;&#26159;&#20379;&#24212;&#38142;&#20132;&#20184;&#31995;&#32479;&#30340;&#37325;&#35201;&#25903;&#26609;&#65292;&#20294;&#22312;&#33521;&#22269;&#20165;&#20855;&#26377;60&#65285;&#30340;&#35013;&#36733;&#25928;&#29575;&#65292;&#23545;&#30899;&#25490;&#25918;&#26377;&#26174;&#33879;&#36129;&#29486;&#12290;&#21327;&#21516;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#34987;&#25552;&#20986;&#20316;&#20026;&#25552;&#39640;&#25928;&#29575;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#26377;&#25928;&#35745;&#31639;&#20849;&#36733;&#21644;&#36335;&#24452;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;&#24403;&#21069;&#30340;&#36816;&#31609;&#23398;&#26041;&#27861;&#22312;&#38382;&#39064;&#35268;&#27169;&#22686;&#22823;&#26102;&#23384;&#22312;&#38750;&#32447;&#24615;&#25193;&#23637;&#65292;&#22240;&#27492;&#21482;&#33021;&#38480;&#20110;&#22312;&#26377;&#38480;&#30340;&#22320;&#29702;&#33539;&#22260;&#20869;&#35745;&#31639;&#32467;&#26524;&#20197;&#28385;&#36275;&#26085;&#24120;&#36816;&#33829;&#30340;&#26102;&#38388;&#35201;&#27714;&#12290;&#36825;&#20165;&#20801;&#35768;&#22312;&#36335;&#24452;&#35268;&#21010;&#19978;&#23547;&#25214;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#26080;&#27861;&#23454;&#29616;&#20840;&#23616;&#20248;&#21270;&#28508;&#21147;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#26469;&#36817;&#32447;&#24615;&#26102;&#38388;&#35299;&#20915;&#19977;&#32500;&#35013;&#36733;&#26377;&#38480;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#12290;&#34429;&#28982;&#27492;&#38382;&#39064;&#22312;&#36816;&#31609;&#23398;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#27809;&#26377;&#20851;&#20110;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#20986;&#29256;&#29289;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heavy goods vehicles are vital backbones of the supply chain delivery system but also contribute significantly to carbon emissions with only 60% loading efficiency in the United Kingdom. Collaborative vehicle routing has been proposed as a solution to increase efficiency, but challenges remain to make this a possibility. One key challenge is the efficient computation of viable solutions for co-loading and routing. Current operations research methods suffer from non-linear scaling with increasing problem size and are therefore bound to limited geographic areas to compute results in time for day-to-day operations. This only allows for local optima in routing and leaves global optimisation potential untouched. We develop a reinforcement learning model to solve the three-dimensional loading capacitated vehicle routing problem in approximately linear time. While this problem has been studied extensively in operations research, no publications on solving it with reinforcement learning exist.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20998;&#24067;&#23398;&#20064;&#23545;VC&#31867;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#19978;&#19979;&#30028;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65292;&#24182;&#35752;&#35770;&#20102;&#19968;&#20123;&#28041;&#21450;&#32479;&#35745;&#23398;&#20064;&#20013;&#21338;&#24328;&#21160;&#24577;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2307.12135</link><description>&lt;p&gt;
&#22810;&#20998;&#24067;&#23398;&#20064;&#23545;VC&#31867;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
The Sample Complexity of Multi-Distribution Learning for VC Classes. (arXiv:2307.12135v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12135
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20998;&#24067;&#23398;&#20064;&#23545;VC&#31867;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#19978;&#19979;&#30028;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65292;&#24182;&#35752;&#35770;&#20102;&#19968;&#20123;&#28041;&#21450;&#32479;&#35745;&#23398;&#20064;&#20013;&#21338;&#24328;&#21160;&#24577;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20998;&#24067;&#23398;&#20064;&#26159;&#23558;PAC&#23398;&#20064;&#25512;&#24191;&#21040;&#20855;&#26377;&#22810;&#20010;&#25968;&#25454;&#20998;&#24067;&#30340;&#24773;&#22659;&#12290;&#25105;&#20204;&#23545;&#20110;PAC&#21487;&#23398;&#20064;&#31867;&#30340;&#24050;&#30693;&#19978;&#19979;&#30028;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#23613;&#31649;&#25105;&#20204;&#20102;&#35299;&#22312;$k$&#20010;&#20998;&#24067;&#19978;&#23398;&#20064;&#20855;&#26377;VC&#32500;&#24230;d&#30340;&#31867;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$O(\epsilon^{-2} \ln(k)(d + k) + \min\{\epsilon^{-1} dk, \epsilon^{-4} \ln(k) d\})$&#65292;&#20294;&#26368;&#22909;&#30340;&#19979;&#30028;&#26159;$\Omega(\epsilon^{-2}(d + k \ln(k)))$&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20197;&#21450;&#22312;&#32479;&#35745;&#23398;&#20064;&#20013;&#20351;&#29992;&#21338;&#24328;&#21160;&#24577;&#30340;&#19968;&#20123;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-distribution learning is a natural generalization of PAC learning to settings with multiple data distributions. There remains a significant gap between the known upper and lower bounds for PAC-learnable classes. In particular, though we understand the sample complexity of learning a VC dimension d class on $k$ distributions to be $O(\epsilon^{-2} \ln(k)(d + k) + \min\{\epsilon^{-1} dk, \epsilon^{-4} \ln(k) d\})$, the best lower bound is $\Omega(\epsilon^{-2}(d + k \ln(k)))$. We discuss recent progress on this problem and some hurdles that are fundamental to the use of game dynamics in statistical learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#26234;&#33021;&#22478;&#24066;&#20013;&#20132;&#36890;&#20107;&#25925;&#21450;&#20107;&#25925;&#26816;&#27979;&#31995;&#32479;&#30340;&#32508;&#21512;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#20132;&#36890;&#30417;&#25511;&#25668;&#20687;&#22836;&#21644;&#34892;&#20026;&#35782;&#21035;&#31995;&#32479;&#65292;&#21487;&#20197;&#23454;&#26102;&#26816;&#27979;&#21644;&#21709;&#24212;&#20132;&#36890;&#20107;&#25925;&#65292;&#24182;&#23558;&#20854;&#19982;&#32039;&#24613;&#26381;&#21153;&#25972;&#21512;&#65292;&#20197;&#20943;&#23569;&#20154;&#20026;&#38169;&#35823;&#21644;&#25913;&#21892;&#20132;&#36890;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2307.12128</link><description>&lt;p&gt;
AI&#22312;&#36947;&#36335;&#19978;&#30340;&#24212;&#29992;&#65306;&#26234;&#33021;&#22478;&#24066;&#20013;&#20132;&#36890;&#20107;&#25925;&#21450;&#20107;&#25925;&#26816;&#27979;&#31995;&#32479;&#30340;&#32508;&#21512;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
AI on the Road: A Comprehensive Analysis of Traffic Accidents and Accident Detection System in Smart Cities. (arXiv:2307.12128v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#26234;&#33021;&#22478;&#24066;&#20013;&#20132;&#36890;&#20107;&#25925;&#21450;&#20107;&#25925;&#26816;&#27979;&#31995;&#32479;&#30340;&#32508;&#21512;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#20132;&#36890;&#30417;&#25511;&#25668;&#20687;&#22836;&#21644;&#34892;&#20026;&#35782;&#21035;&#31995;&#32479;&#65292;&#21487;&#20197;&#23454;&#26102;&#26816;&#27979;&#21644;&#21709;&#24212;&#20132;&#36890;&#20107;&#25925;&#65292;&#24182;&#23558;&#20854;&#19982;&#32039;&#24613;&#26381;&#21153;&#25972;&#21512;&#65292;&#20197;&#20943;&#23569;&#20154;&#20026;&#38169;&#35823;&#21644;&#25913;&#21892;&#20132;&#36890;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#25925;&#26816;&#27979;&#21644;&#20132;&#36890;&#20998;&#26512;&#26159;&#26234;&#33021;&#22478;&#24066;&#21644;&#33258;&#21160;&#21270;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21487;&#20197;&#20943;&#23569;&#20107;&#25925;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#65292;&#25913;&#21892;&#20132;&#36890;&#31649;&#29702;&#12290;&#26412;&#25991;&#21033;&#29992;&#32654;&#22269;&#22269;&#23478;&#20844;&#36335;&#20132;&#36890;&#23433;&#20840;&#31649;&#29702;&#23616;&#65288;NHTSA&#65289;&#30340;Crash Report Sampling System&#65288;CRSS&#65289;&#30340;&#25968;&#25454;&#65292;&#23545;&#32654;&#22269;&#19981;&#21516;&#22320;&#21306;&#30340;&#20132;&#36890;&#20107;&#25925;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#20026;&#20102;&#24212;&#23545;&#20107;&#25925;&#26816;&#27979;&#21644;&#20132;&#36890;&#20998;&#26512;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#20132;&#36890;&#30417;&#25511;&#25668;&#20687;&#22836;&#21644;&#34892;&#20026;&#35782;&#21035;&#31995;&#32479;&#26469;&#21363;&#26102;&#26816;&#27979;&#21644;&#21709;&#24212;&#20132;&#36890;&#20107;&#25925;&#12290;&#23558;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#19982;&#32039;&#24613;&#26381;&#21153;&#25972;&#21512;&#65292;&#23558;&#20132;&#36890;&#25668;&#20687;&#22836;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#21147;&#37327;&#29992;&#20110;&#21709;&#24212;&#20132;&#36890;&#20107;&#25925;&#21644;&#20943;&#23569;&#20154;&#20026;&#38169;&#35823;&#12290;&#26234;&#33021;&#22478;&#24066;&#20013;&#30340;&#20107;&#25925;&#26816;&#27979;&#31995;&#32479;&#31561;&#20808;&#36827;&#26234;&#33021;&#25216;&#26415;&#23558;&#25913;&#21892;&#20132;&#36890;&#23433;&#20840;&#65292;&#25552;&#39640;&#20132;&#36890;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accident detection and traffic analysis is a critical component of smart city and autonomous transportation systems that can reduce accident frequency, severity and improve overall traffic management. This paper presents a comprehensive analysis of traffic accidents in different regions across the United States using data from the National Highway Traffic Safety Administration (NHTSA) Crash Report Sampling System (CRSS). To address the challenges of accident detection and traffic analysis, this paper proposes a framework that uses traffic surveillance cameras and action recognition systems to detect and respond to traffic accidents spontaneously. Integrating the proposed framework with emergency services will harness the power of traffic cameras and machine learning algorithms to create an efficient solution for responding to traffic accidents and reducing human errors. Advanced intelligence technologies, such as the proposed accident detection systems in smart cities, will improve tra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;StyleGAN2-Ada&#21644;&#25193;&#25955;&#25216;&#26415;&#65292;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#26550;&#26500;&#21644;&#20351;&#29992;&#31934;&#36873;&#25968;&#25454;&#38598;&#65292;&#25104;&#21151;&#20135;&#29983;&#20102;&#36924;&#30495;&#19988;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#24052;&#33922;&#20811;&#22270;&#26696;&#65292;&#20026;&#24052;&#33922;&#20811;&#35774;&#35745;&#24072;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#36741;&#21161;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2307.12122</link><description>&lt;p&gt;
&#29992;&#25193;&#25955; - &#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21512;&#25104;&#24052;&#33922;&#20811;&#22270;&#26696;
&lt;/p&gt;
&lt;p&gt;
Synthesis of Batik Motifs using a Diffusion -- Generative Adversarial Network. (arXiv:2307.12122v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;StyleGAN2-Ada&#21644;&#25193;&#25955;&#25216;&#26415;&#65292;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#26550;&#26500;&#21644;&#20351;&#29992;&#31934;&#36873;&#25968;&#25454;&#38598;&#65292;&#25104;&#21151;&#20135;&#29983;&#20102;&#36924;&#30495;&#19988;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#24052;&#33922;&#20811;&#22270;&#26696;&#65292;&#20026;&#24052;&#33922;&#20811;&#35774;&#35745;&#24072;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#36741;&#21161;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24052;&#33922;&#20811;&#26159;&#21360;&#24230;&#23612;&#35199;&#20122;&#31038;&#20250;&#30340;&#19968;&#31181;&#29420;&#29305;&#30340;&#33402;&#26415;&#19982;&#24037;&#33402;&#21697;&#30340;&#32467;&#21512;&#65292;&#30740;&#31350;&#24052;&#33922;&#20811;&#22270;&#26696;&#20027;&#35201;&#38598;&#20013;&#22312;&#20998;&#31867;&#26041;&#38754;&#65292;&#20294;&#26159;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21487;&#20197;&#25193;&#23637;&#21040;&#24052;&#33922;&#20811;&#22270;&#26696;&#30340;&#21512;&#25104;&#19978;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20294;&#36890;&#24120;&#38754;&#20020;&#32467;&#26524;&#30340;&#31283;&#23450;&#24615;&#21644;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#38598;&#20013;&#22312;&#20351;&#29992;StyleGAN2-Ada&#21644;&#25193;&#25955;&#25216;&#26415;&#26469;&#20135;&#29983;&#36924;&#30495;&#19988;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#24052;&#33922;&#20811;&#22270;&#26696;&#12290;StyleGAN2-Ada&#26159;GAN&#27169;&#22411;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#23558;&#22270;&#20687;&#20013;&#30340;&#39118;&#26684;&#21644;&#20869;&#23481;&#20998;&#31163;&#65292;&#32780;&#25193;&#25955;&#25216;&#26415;&#21017;&#21521;&#25968;&#25454;&#20013;&#24341;&#20837;&#38543;&#26426;&#22122;&#22768;&#12290;&#22312;&#24052;&#33922;&#20811;&#30340;&#35821;&#22659;&#20013;&#65292;&#20351;&#29992;StyleGAN2-Ada&#21644;&#25193;&#25955;&#25216;&#26415;&#26469;&#20135;&#29983;&#36924;&#30495;&#30340;&#21512;&#25104;&#24052;&#33922;&#20811;&#22270;&#26696;&#12290;&#26412;&#30740;&#31350;&#36824;&#23545;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#20102;&#35843;&#25972;&#65292;&#24182;&#20351;&#29992;&#20102;&#31934;&#36873;&#30340;&#24052;&#33922;&#20811;&#25968;&#25454;&#38598;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#24110;&#21161;&#24052;&#33922;&#20811;&#35774;&#35745;&#24072;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batik, a unique blend of art and craftsmanship, is a distinct artistic and technological creation for Indonesian society. Research on batik motifs is primarily focused on classification. However, further studies may extend to the synthesis of batik patterns. Generative Adversarial Networks (GANs) have been an important deep learning model for generating synthetic data, but often face challenges in the stability and consistency of results. This research focuses on the use of StyleGAN2-Ada and Diffusion techniques to produce realistic and high-quality synthetic batik patterns. StyleGAN2-Ada is a variation of the GAN model that separates the style and content aspects in an image, whereas diffusion techniques introduce random noise into the data. In the context of batik, StyleGAN2-Ada and Diffusion are used to produce realistic synthetic batik patterns. This study also made adjustments to the model architecture and used a well-curated batik dataset. The main goal is to assist batik designe
&lt;/p&gt;</description></item><item><title>&#31227;&#21160;AIGC&#25216;&#26415;&#21487;&#20197;&#25512;&#21160;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#22312;&#20010;&#24615;&#21270;&#21307;&#30103;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#29983;&#25104;&#32597;&#35265;&#30142;&#30149;&#25968;&#25454;&#12289;&#24314;&#27169;&#39640;&#20445;&#30495;&#25968;&#23383;&#23402;&#29983;&#20197;&#21450;&#25552;&#20379;24/7&#23450;&#21046;&#21307;&#30103;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.12115</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#21307;&#30103;&#38761;&#21629;&#65306;&#21033;&#29992;&#31227;&#21160;AIGC&#23454;&#29616;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;
&lt;/p&gt;
&lt;p&gt;
A Revolution of Personalized Healthcare: Enabling Human Digital Twin with Mobile AIGC. (arXiv:2307.12115v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12115
&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;AIGC&#25216;&#26415;&#21487;&#20197;&#25512;&#21160;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#22312;&#20010;&#24615;&#21270;&#21307;&#30103;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#29983;&#25104;&#32597;&#35265;&#30142;&#30149;&#25968;&#25454;&#12289;&#24314;&#27169;&#39640;&#20445;&#30495;&#25968;&#23383;&#23402;&#29983;&#20197;&#21450;&#25552;&#20379;24/7&#23450;&#21046;&#21307;&#30103;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#25216;&#26415;&#25351;&#30340;&#26159;&#22312;&#31227;&#21160;&#36793;&#32536;&#32593;&#32476;&#19978;&#37319;&#29992;AI&#31639;&#27861;&#33258;&#21160;&#21270;&#20449;&#24687;&#21019;&#24314;&#36807;&#31243;&#65292;&#21516;&#26102;&#28385;&#36275;&#32456;&#31471;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;&#31227;&#21160;AIGC&#26368;&#36817;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#20026;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#65288;HDT&#65289;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#31227;&#21160;AIGC&#39537;&#21160;&#30340;HDT&#26377;&#26395;&#36890;&#36807;&#29983;&#25104;&#32597;&#35265;&#30142;&#30149;&#25968;&#25454;&#12289;&#24314;&#27169;&#39640;&#20445;&#30495;&#25968;&#23383;&#23402;&#29983;&#12289;&#26500;&#24314;&#22810;&#21151;&#33021;&#35797;&#39564;&#24179;&#21488;&#21644;&#25552;&#20379;&#20840;&#22825;&#20505;&#23450;&#21046;&#21307;&#30103;&#26381;&#21153;&#26469;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20010;&#24615;&#21270;&#21307;&#30103;&#12290;&#20026;&#20102;&#25512;&#21160;&#36825;&#19968;&#26032;&#22411;&#33539;&#24335;&#30340;&#21457;&#23637;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31227;&#21160;AIGC&#39537;&#21160;&#30340;HDT&#31995;&#32479;&#26550;&#26500;&#65292;&#24182;&#24378;&#35843;&#20102;&#30456;&#24212;&#30340;&#35774;&#35745;&#35201;&#27714;&#21644;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20030;&#20363;&#35828;&#26126;&#20102;&#20004;&#31181;&#29992;&#20363;&#65292;&#21363;&#22312;&#23450;&#21046;&#25163;&#26415;&#35268;&#21010;&#21644;&#20010;&#24615;&#21270;&#33647;&#29289;&#27835;&#30103;&#20013;&#20351;&#29992;&#31227;&#21160;AIGC&#39537;&#21160;&#30340;HDT&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#39564;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile Artificial Intelligence-Generated Content (AIGC) technology refers to the adoption of AI algorithms deployed at mobile edge networks to automate the information creation process while fulfilling the requirements of end users. Mobile AIGC has recently attracted phenomenal attentions and can be a key enabling technology for an emerging application, called human digital twin (HDT). HDT empowered by the mobile AIGC is expected to revolutionize the personalized healthcare by generating rare disease data, modeling high-fidelity digital twin, building versatile testbeds, and providing 24/7 customized medical services. To promote the development of this new breed of paradigm, in this article, we propose a system architecture of mobile AIGC-driven HDT and highlight the corresponding design requirements and challenges. Moreover, we illustrate two use cases, i.e., mobile AIGC-driven HDT in customized surgery planning and personalized medication. In addition, we conduct an experimental stud
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#31245;&#36874;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#27809;&#26377;&#19968;&#20010;&#27169;&#22411;&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.12114</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#30340;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks. (arXiv:2307.12114v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12114
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#31245;&#36874;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#27809;&#26377;&#19968;&#20010;&#27169;&#22411;&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#8212;&#8212;ChatGPT&#12289;Flan-T5 UL2&#12289;Tk-Instruct&#21644;Alpaca&#8212;&#8212;&#22312;13&#20010;&#23454;&#38469;&#19990;&#30028;&#30340;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#20363;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#12289;&#38382;&#31572;&#65288;QA&#65289;&#12289;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#31561;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#35780;&#20272;&#30340;LLM&#24320;&#22987;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#23545;&#20110;QA&#20219;&#21153;&#34920;&#29616;&#24471;&#29305;&#21035;&#22909;&#65292;&#21363;&#20351;&#23427;&#20204;&#20043;&#21069;&#27809;&#26377;&#35265;&#36807;&#36825;&#20123;&#20219;&#21153;&#30340;&#31034;&#20363;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#34920;&#29616;&#20302;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#65288;&#22914;PubMedBERT&#65289;&#21487;&#20197;&#36798;&#21040;&#30340;&#27700;&#24179;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#27809;&#26377;&#19968;&#20010;LLM&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#37117;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#20110;&#29305;&#23450;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and particularly well for the QA task, even though they have never seen examples from these tasks before. However, we observed that the classification and RE tasks perform below what can be achieved with a specifically trained model for the medical field, such as PubMedBERT. Finally, we noted that no LLM outperforms all the others on all the studied tasks, with some models being better suited for certain tasks than others.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#32467;&#21512;&#26059;&#36716;&#21644;&#22810;&#20010;&#21487;&#25511;&#21943;&#21475;&#65292;&#36890;&#36807;&#20248;&#21270;&#21943;&#21475;&#25968;&#37327;&#21644;&#20301;&#32622;&#65292;&#20256;&#24863;&#22120;&#20301;&#32622;&#20197;&#21450;&#27599;&#20010;&#21160;&#20316;&#21487;&#20801;&#35768;&#30340;&#26368;&#22823;&#27969;&#37327;&#21644;&#27599;&#20010;episode&#20013;&#20801;&#35768;&#30340;&#24635;&#21943;&#21475;&#25968;&#30340;&#24418;&#24335;&#65292;&#23454;&#29616;&#23545;&#26059;&#36716;&#22278;&#26609;&#20307;&#27969;&#21160;&#30340;&#20027;&#21160;&#25511;&#21046;&#65292;&#25233;&#21046;&#28065;&#27969;&#33073;&#33853;&#21644;&#31283;&#23450;&#21345;&#38376;&#28065;&#27969;&#12290;</title><link>http://arxiv.org/abs/2307.12083</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#21943;&#21475;&#23545;&#26059;&#36716;&#22278;&#26609;&#20307;&#27969;&#21160;&#36827;&#34892;&#20027;&#21160;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Active Control of Flow over Rotating Cylinder by Multiple Jets using Deep Reinforcement Learning. (arXiv:2307.12083v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#32467;&#21512;&#26059;&#36716;&#21644;&#22810;&#20010;&#21487;&#25511;&#21943;&#21475;&#65292;&#36890;&#36807;&#20248;&#21270;&#21943;&#21475;&#25968;&#37327;&#21644;&#20301;&#32622;&#65292;&#20256;&#24863;&#22120;&#20301;&#32622;&#20197;&#21450;&#27599;&#20010;&#21160;&#20316;&#21487;&#20801;&#35768;&#30340;&#26368;&#22823;&#27969;&#37327;&#21644;&#27599;&#20010;episode&#20013;&#20801;&#35768;&#30340;&#24635;&#21943;&#21475;&#25968;&#30340;&#24418;&#24335;&#65292;&#23454;&#29616;&#23545;&#26059;&#36716;&#22278;&#26609;&#20307;&#27969;&#21160;&#30340;&#20027;&#21160;&#25511;&#21046;&#65292;&#25233;&#21046;&#28065;&#27969;&#33073;&#33853;&#21644;&#31283;&#23450;&#21345;&#38376;&#28065;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#30495;&#27491;&#23041;&#21147;&#20307;&#29616;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30001;&#20110;&#20854;&#21160;&#24577;&#24615;&#36136;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;&#35745;&#31639;&#21644;&#29289;&#29702;&#26041;&#38754;&#26356;&#20026;&#22797;&#26434;&#12290;&#26059;&#36716;&#21644;&#21943;&#23556;&#24050;&#34987;&#35777;&#23454;&#26159;&#20943;&#23569;&#38045;&#20307;&#25152;&#21463;&#38459;&#21147;&#30340;&#19968;&#31181;&#26377;&#25928;&#30340;&#20027;&#21160;&#27969;&#21160;&#25511;&#21046;&#26041;&#24335;&#12290;&#26412;&#30740;&#31350;&#23558;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;(DRL)&#26469;&#28155;&#21152;&#26059;&#36716;&#65292;&#24182;&#21033;&#29992;&#22810;&#20010;&#21487;&#25511;&#21943;&#21475;&#20197;&#23454;&#29616;&#26368;&#22823;&#21487;&#33021;&#30340;&#38459;&#21147;&#25233;&#21046;&#12290;&#23558;&#20171;&#32461;DRL&#20195;&#30721;&#30340;&#29305;&#28857;&#65292;&#21253;&#25324;&#25511;&#21046;&#21442;&#25968;&#12289;&#20854;&#38480;&#21046;&#20197;&#21450;&#29992;&#20110;&#26059;&#36716;&#30340;DRL&#32593;&#32476;&#30340;&#20248;&#21270;&#12290;&#26412;&#30740;&#31350;&#23558;&#37325;&#28857;&#20248;&#21270;&#21943;&#21475;&#30340;&#25968;&#37327;&#21644;&#20301;&#32622;&#12289;&#20256;&#24863;&#22120;&#20301;&#32622;&#20197;&#21450;&#27599;&#20010;&#21160;&#20316;&#21487;&#20801;&#35768;&#30340;&#26368;&#22823;&#27969;&#37327;&#21644;&#27599;&#20010;episode&#20013;&#20801;&#35768;&#30340;&#24635;&#21943;&#21475;&#25968;&#30340;&#24418;&#24335;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#26059;&#36716;&#19982;DRL&#24037;&#20855;&#30456;&#32467;&#21512;&#26159;&#26377;&#24076;&#26395;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25233;&#21046;&#28065;&#27969;&#33073;&#33853;&#65292;&#31283;&#23450;&#21345;&#38376;&#28065;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
The real power of artificial intelligence appears in reinforcement learning, which is computationally and physically more sophisticated due to its dynamic nature. Rotation and injection have been a proven way of active flow control to reduce the drag force exerted on blunt bodies. Rotation will be added to the cylinder alongside the deep reinforcement learning (DRL) algorithm, which uses multiple controlled jets to reach maximum possible drag suppression. Characteristics of the DRL code, including controlling parameters, their limitations, and optimization of the DRL network for use with rotation will be presented. This work will focus on optimizing the number and positions of the jets, sensors location, and maximum allowed flow rate to jets in the form of maximum allowed flow rate of each actuation and the total number of them per episode. It is found that combining the rotation with the DRL tools is promising, since it suppresses the vortex shedding, stabilizes the Karman vortex stre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35889;&#24402;&#19968;&#21270;&#20999;&#22270;&#20998;&#21106;&#30340;&#20844;&#24179;&#32422;&#26463;&#21464;&#31181;&#38382;&#39064;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#35889;&#31639;&#27861;FNM&#65292;&#23454;&#29616;&#20102;&#22312;&#26368;&#23567;&#21270;&#24402;&#19968;&#21270;&#20999;&#20540;&#30340;&#21516;&#26102;&#20445;&#35777;&#27599;&#20010;&#32858;&#31867;&#20013;&#21508;&#20010;&#20154;&#21475;&#32676;&#20307;&#30340;&#36817;&#20284;&#27604;&#20363;&#12290;</title><link>http://arxiv.org/abs/2307.12065</link><description>&lt;p&gt;
&#20855;&#26377;&#20844;&#24179;&#32422;&#26463;&#30340;&#35889;&#24402;&#19968;&#21270;&#20999;&#22270;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Spectral Normalized-Cut Graph Partitioning with Fairness Constraints. (arXiv:2307.12065v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35889;&#24402;&#19968;&#21270;&#20999;&#22270;&#20998;&#21106;&#30340;&#20844;&#24179;&#32422;&#26463;&#21464;&#31181;&#38382;&#39064;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#35889;&#31639;&#27861;FNM&#65292;&#23454;&#29616;&#20102;&#22312;&#26368;&#23567;&#21270;&#24402;&#19968;&#21270;&#20999;&#20540;&#30340;&#21516;&#26102;&#20445;&#35777;&#27599;&#20010;&#32858;&#31867;&#20013;&#21508;&#20010;&#20154;&#21475;&#32676;&#20307;&#30340;&#36817;&#20284;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#19968;&#21270;&#20999;&#22270;&#20998;&#21106;&#30340;&#30446;&#26631;&#26159;&#23558;&#22270;&#20013;&#30340;&#33410;&#28857;&#38598;&#21512;&#21010;&#20998;&#20026;$k$&#20010;&#19981;&#30456;&#20132;&#30340;&#32858;&#31867;&#65292;&#20197;&#26368;&#23567;&#21270;&#20219;&#24847;&#32858;&#31867;&#19982;&#20854;&#20182;&#32858;&#31867;&#20043;&#38388;&#30340;&#24635;&#36793;&#25968;&#21344;&#27604;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#21010;&#20998;&#38382;&#39064;&#21464;&#31181;&#65292;&#22312;&#27492;&#38382;&#39064;&#20013;&#65292;&#33410;&#28857;&#30001;&#19968;&#20010;&#20998;&#31867;&#25935;&#24863;&#23646;&#24615;&#65288;&#20363;&#22914;&#24615;&#21035;&#25110;&#31181;&#26063;&#65289;&#34920;&#24449;&#65292;&#25351;&#31034;&#20854;&#23646;&#20110;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#26368;&#23567;&#21270;&#24402;&#19968;&#21270;&#20999;&#20540;&#30340;&#21516;&#26102;&#65292;&#30830;&#20445;&#27599;&#20010;&#32676;&#20307;&#22312;&#27599;&#20010;&#32858;&#31867;&#20013;&#36817;&#20284;&#25353;&#27604;&#20363;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FNM&#30340;&#20004;&#38454;&#27573;&#35889;&#31639;&#27861;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#26681;&#25454;&#20844;&#24179;&#26631;&#20934;&#21521;&#30446;&#26631;&#20989;&#25968;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#22686;&#24191;Lagrangian&#39033;&#65292;&#20197;&#33719;&#24471;&#26356;&#20844;&#24179;&#30340;&#35889;&#33410;&#28857;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33293;&#20837;&#26041;&#26696;&#65292;&#20174;&#20844;&#24179;&#23884;&#20837;&#20013;&#29983;&#25104;$k$&#20010;&#32858;&#31867;&#65292;&#26377;&#25928;&#22320;&#26435;&#34913;&#20102;&#20844;&#24179;&#24615;&#21644;&#21010;&#20998;&#36136;&#37327;&#12290;&#36890;&#36807;&#23545;&#20061;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Normalized-cut graph partitioning aims to divide the set of nodes in a graph into $k$ disjoint clusters to minimize the fraction of the total edges between any cluster and all other clusters. In this paper, we consider a fair variant of the partitioning problem wherein nodes are characterized by a categorical sensitive attribute (e.g., gender or race) indicating membership to different demographic groups. Our goal is to ensure that each group is approximately proportionally represented in each cluster while minimizing the normalized cut value. To resolve this problem, we propose a two-phase spectral algorithm called FNM. In the first phase, we add an augmented Lagrangian term based on our fairness criteria to the objective function for obtaining a fairer spectral node embedding. Then, in the second phase, we design a rounding scheme to produce $k$ clusters from the fair embedding that effectively trades off fairness and partition quality. Through comprehensive experiments on nine bench
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21160;&#24577;&#26500;&#24314;&#28508;&#22312;&#22320;&#26631;&#22270;&#26469;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#35299;&#20915;&#20102;&#30446;&#26631;&#26465;&#20214;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#23376;&#30446;&#26631;&#36873;&#25321;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.12063</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#26500;&#24314;&#28508;&#22312;&#22320;&#26631;&#22270;&#22312;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20013;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs. (arXiv:2307.12063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21160;&#24577;&#26500;&#24314;&#28508;&#22312;&#22320;&#26631;&#22270;&#26469;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#35299;&#20915;&#20102;&#30446;&#26631;&#26465;&#20214;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#23376;&#30446;&#26631;&#36873;&#25321;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26465;&#20214;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;(GCHRL)&#26159;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;-&#21033;&#29992;&#22256;&#22659;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#12290;&#23427;&#23558;&#28304;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#30446;&#26631;&#26465;&#20214;&#23376;&#20219;&#21153;&#65292;&#24182;&#22312;&#23376;&#30446;&#26631;&#31354;&#38388;&#20013;&#36827;&#34892;&#25506;&#32034;&#21644;&#21033;&#29992;&#12290;GCHRL&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#23376;&#30446;&#26631;&#34920;&#31034;&#20989;&#25968;&#21644;&#23376;&#30446;&#26631;&#36873;&#25321;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#24120;&#24120;&#24573;&#35270;&#20102;GCHRL&#20013;&#23398;&#20064;&#28508;&#22312;&#23376;&#30446;&#26631;&#34920;&#31034;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#32570;&#20047;&#19968;&#31181;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#39640;&#25928;&#23376;&#30446;&#26631;&#36873;&#25321;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#21160;&#24577;&#26500;&#24314;Latent Landmark&#22270;&#30340;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#65288;HILL&#65289;&#26469;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;HILL&#20351;&#29992;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#30446;&#26631;&#23398;&#20064;&#28385;&#36275;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#28508;&#22312;&#23376;&#30446;&#26631;&#34920;&#31034;&#12290;&#22522;&#20110;&#36825;&#20123;&#34920;&#31034;&#65292;HILL&#21160;&#24577;&#26500;&#24314;&#28508;&#22312;&#22320;&#26631;&#22270;&#65292;&#24182;&#22312;&#33410;&#28857;&#21644;&#36793;&#19978;&#20351;&#29992;&#26032;&#39062;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal-Conditioned Hierarchical Reinforcement Learning (GCHRL) is a promising paradigm to address the exploration-exploitation dilemma in reinforcement learning. It decomposes the source task into subgoal conditional subtasks and conducts exploration and exploitation in the subgoal space. The effectiveness of GCHRL heavily relies on subgoal representation functions and subgoal selection strategy. However, existing works often overlook the temporal coherence in GCHRL when learning latent subgoal representations and lack an efficient subgoal selection strategy that balances exploration and exploitation. This paper proposes HIerarchical reinforcement learning via dynamically building Latent Landmark graphs (HILL) to overcome these limitations. HILL learns latent subgoal representations that satisfy temporal coherence using a contrastive representation learning objective. Based on these representations, HILL dynamically builds latent landmark graphs and employs a novelty measure on nodes and
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26102;&#38388;&#32806;&#21512;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#20004;&#20154;&#38646;&#21644;&#28216;&#25103;&#26469;&#22788;&#29702;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#36817;&#20284;&#22343;&#34913;&#26469;&#30830;&#20445;&#20195;&#29702;&#23545;&#26102;&#38388;&#32806;&#21512;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#40065;&#26834;&#24615;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.12062</link><description>&lt;p&gt;
&#28216;&#25103;&#29702;&#35770;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#22788;&#29702;&#26102;&#38388;&#32806;&#21512;&#30340;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations. (arXiv:2307.12062v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12062
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26102;&#38388;&#32806;&#21512;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#20004;&#20154;&#38646;&#21644;&#28216;&#25103;&#26469;&#22788;&#29702;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#36817;&#20284;&#22343;&#34913;&#26469;&#30830;&#20445;&#20195;&#29702;&#23545;&#26102;&#38388;&#32806;&#21512;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#40065;&#26834;&#24615;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#35757;&#32451;&#33021;&#22815;&#22312;&#29615;&#22659;&#24178;&#25200;&#25110;&#23545;&#25239;&#25915;&#20987;&#19979;&#34920;&#29616;&#33391;&#22909;&#30340;&#31574;&#30053;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#21487;&#33021;&#24178;&#25200;&#30340;&#31354;&#38388;&#22312;&#21508;&#20010;&#26102;&#38388;&#27493;&#39588;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#26102;&#38388;&#27493;&#39588;&#19978;&#21487;&#33021;&#24178;&#25200;&#30340;&#31354;&#38388;&#21462;&#20915;&#20110;&#36807;&#21435;&#30340;&#24178;&#25200;&#12290;&#25105;&#20204;&#27491;&#24335;&#24341;&#20837;&#26102;&#38388;&#32806;&#21512;&#24178;&#25200;&#65292;&#23545;&#29616;&#26377;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GRAD&#65292;&#19968;&#31181;&#26032;&#30340;&#28216;&#25103;&#29702;&#35770;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#32806;&#21512;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#20004;&#20154;&#38646;&#21644;&#28216;&#25103;&#12290;&#36890;&#36807;&#22312;&#36825;&#20010;&#28216;&#25103;&#20013;&#25214;&#21040;&#19968;&#20010;&#36817;&#20284;&#22343;&#34913;&#65292;GRAD&#30830;&#20445;&#20102;&#20195;&#29702;&#30340;&#23545;&#26102;&#38388;&#32806;&#21512;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#12290;&#23545;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#22312;&#26631;&#20934;&#21644;&#26102;&#38388;&#32806;&#21512;&#24178;&#25200;&#19979;&#20855;&#26377;&#26174;&#33879;&#30340;&#40065;&#26834;&#24615;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust reinforcement learning (RL) seeks to train policies that can perform well under environment perturbations or adversarial attacks. Existing approaches typically assume that the space of possible perturbations remains the same across timesteps. However, in many settings, the space of possible perturbations at a given timestep depends on past perturbations. We formally introduce temporally-coupled perturbations, presenting a novel challenge for existing robust RL methods. To tackle this challenge, we propose GRAD, a novel game-theoretic approach that treats the temporally-coupled robust RL problem as a partially-observable two-player zero-sum game. By finding an approximate equilibrium in this game, GRAD ensures the agent's robustness against temporally-coupled perturbations. Empirical experiments on a variety of continuous control tasks demonstrate that our proposed approach exhibits significant robustness advantages compared to baselines against both standard and temporally-coupl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;GPU&#19978;&#39640;&#25928;&#23436;&#25104;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30690;&#37327;&#26469;&#28155;&#21152;&#26032;&#20851;&#31995;&#12290;&#35813;&#26694;&#26550;&#23558;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#38382;&#39064;&#36716;&#21270;&#20026;&#30456;&#20284;&#24615;&#36830;&#25509;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#22788;&#29702;&#30456;&#20284;&#24615;&#36830;&#25509;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.12059</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#24555;&#36895;&#23436;&#25104;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Fast Knowledge Graph Completion using Graphics Processing Units. (arXiv:2307.12059v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;GPU&#19978;&#39640;&#25928;&#23436;&#25104;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30690;&#37327;&#26469;&#28155;&#21152;&#26032;&#20851;&#31995;&#12290;&#35813;&#26694;&#26550;&#23558;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#38382;&#39064;&#36716;&#21270;&#20026;&#30456;&#20284;&#24615;&#36830;&#25509;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#22788;&#29702;&#30456;&#20284;&#24615;&#36830;&#25509;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#21487;&#20197;&#22312;&#19982;&#25968;&#25454;&#35821;&#20041;&#30456;&#20851;&#30340;&#22810;&#20010;&#39046;&#22495;&#20013;&#20351;&#29992;&#65292;&#20363;&#22914;&#38382;&#31572;&#31995;&#32479;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#26500;&#24314;&#30340;&#30693;&#35782;&#22270;&#35889;&#38656;&#35201;&#36890;&#36807;&#34917;&#20805;&#20851;&#31995;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#30693;&#35782;&#12290;&#36825;&#34987;&#31216;&#20026;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12290;&#20026;&#20102;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#21521;&#29616;&#26377;&#30693;&#35782;&#22270;&#35889;&#28155;&#21152;&#26032;&#20851;&#31995;&#65292;&#25105;&#20204;&#24517;&#39035;&#35780;&#20272;N&#215;N&#215;R&#20010;&#30690;&#37327;&#25805;&#20316;&#65292;&#20854;&#20013;N&#26159;&#23454;&#20307;&#25968;&#37327;&#65292;R&#26159;&#20851;&#31995;&#31867;&#22411;&#25968;&#37327;&#12290;&#36825;&#38750;&#24120;&#26114;&#36149;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;GPU&#19978;&#39640;&#25928;&#23436;&#25104;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#20197;&#33719;&#21462;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30690;&#37327;&#30340;&#26032;&#20851;&#31995;&#12290;&#22312;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#8220;&#21487;&#20197;&#36716;&#25442;&#20026;&#24230;&#37327;&#31354;&#38388;&#8221;&#65292;&#28982;&#21518;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#38382;&#39064;&#36716;&#21270;&#20026;&#8220;&#21487;&#20197;&#36716;&#25442;&#20026;&#24230;&#37327;&#31354;&#38388;&#8221;&#30340;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#36830;&#25509;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#20043;&#21518;&#65292;&#20026;&#20102;&#39640;&#25928;&#22788;&#29702;&#30456;&#20284;&#24615;&#36830;&#25509;&#38382;&#39064;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs can be used in many areas related to data semantics such as question-answering systems, knowledge based systems. However, the currently constructed knowledge graphs need to be complemented for better knowledge in terms of relations. It is called knowledge graph completion. To add new relations to the existing knowledge graph by using knowledge graph embedding models, we have to evaluate $N\times N \times R$ vector operations, where $N$ is the number of entities and $R$ is the number of relation types. It is very costly.  In this paper, we provide an efficient knowledge graph completion framework on GPUs to get new relations using knowledge graph embedding vectors. In the proposed framework, we first define "transformable to a metric space" and then provide a method to transform the knowledge graph completion problem into the similarity join problem for a model which is "transformable to a metric space". After that, to efficiently process the similarity join problem, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20174;&#22806;&#37096;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#24615;&#22320;&#38598;&#25104;&#30693;&#35782;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22806;&#37096;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#20363;&#23376;&#26159;ChatPDF&#12290;</title><link>http://arxiv.org/abs/2307.12057</link><description>&lt;p&gt;
&#22806;&#37096;&#25512;&#29702;&#65306;&#26397;&#30528;&#22810;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20114;&#25442;&#36741;&#21161;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#26041;&#21521;&#21069;&#36827;
&lt;/p&gt;
&lt;p&gt;
External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback. (arXiv:2307.12057v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20174;&#22806;&#37096;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#24615;&#22320;&#38598;&#25104;&#30693;&#35782;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22806;&#37096;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#20363;&#23376;&#26159;ChatPDF&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#34987;&#35748;&#20026;&#26159;&#20351;&#28023;&#39532;&#20307;&#21644;&#33041;&#31070;&#32463;&#20803;&#20869;&#20445;&#25345;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#24687;&#12289;&#38543;&#21518;&#29992;&#20110;&#35299;&#20915;&#36890;&#36807;&#23398;&#20064;&#19968;&#29983;&#20013;&#36935;&#21040;&#30340;&#29616;&#23454;&#25361;&#25112;&#30340;&#20851;&#38190;&#20154;&#31867;&#33021;&#21147;&#12290;&#36890;&#36807;&#24212;&#29992;&#24050;&#33719;&#24471;&#30340;&#30693;&#35782;&#35299;&#20915;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#26159;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#19968;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20687;GPT-3.5&#21644;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#20132;&#20114;&#21644;&#25512;&#29702;&#26041;&#38754;&#26174;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#38480;&#21046;&#65292;&#23427;&#20204;&#26080;&#27861;&#22788;&#29702;&#24191;&#27867;&#12289;&#19981;&#26029;&#28436;&#21464;&#30340;&#30693;&#35782;&#24211;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20174;&#22806;&#37096;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#24615;&#22320;&#38598;&#25104;&#30693;&#35782;&#26469;&#22686;&#24378;LLMs&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22806;&#37096;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#20363;&#23376;&#26159;ChatPDF&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory is identified as a crucial human faculty that allows for the retention of visual and linguistic information within the hippocampus and neurons in the brain, which can subsequently be retrieved to address real-world challenges that arise through a lifetime of learning. The resolution of complex AI tasks through the application of acquired knowledge represents a stride toward the realization of artificial general intelligence. However, despite the prevalence of Large Language Models (LLMs) like GPT-3.5 and GPT-4 , which have displayed remarkable capabilities in language comprehension, generation, interaction, and reasoning, they are inhibited by constraints on context length that preclude the processing of extensive, continually evolving knowledge bases. This paper proposes that LLMs could be augmented through the selective integration of knowledge from external repositories, and in doing so, introduces a novel methodology for External Reasoning, exemplified by ChatPDF. Central to
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#21019;&#26032;&#30340;&#22686;&#24378;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#21450;&#36890;&#36807;&#22312;&#38669;&#22827;&#31354;&#38388;&#20013;&#20351;&#29992;&#26032;&#30340;SR&#25439;&#22833;&#20989;&#25968;&#25913;&#21892;&#39134;&#34892;&#36712;&#36857;&#32447;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#39134;&#34892;&#36712;&#36857;&#24182;&#19988;&#23545;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#65292;&#20026;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33322;&#31354;&#30740;&#31350;&#20013;&#30340;&#39134;&#34892;&#36712;&#36857;&#26816;&#27979;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.12032</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#38669;&#22827;&#31354;&#38388;&#20013;&#37319;&#29992;&#26032;&#30340;SR&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#22686;&#24378;&#30340;&#36801;&#31227;&#23398;&#20064;&#30340;&#39134;&#34892;&#36712;&#36857;&#21010;&#20998;
&lt;/p&gt;
&lt;p&gt;
Flight Contrail Segmentation via Augmented Transfer Learning with Novel SR Loss Function in Hough Space. (arXiv:2307.12032v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#21019;&#26032;&#30340;&#22686;&#24378;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#21450;&#36890;&#36807;&#22312;&#38669;&#22827;&#31354;&#38388;&#20013;&#20351;&#29992;&#26032;&#30340;SR&#25439;&#22833;&#20989;&#25968;&#25913;&#21892;&#39134;&#34892;&#36712;&#36857;&#32447;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#39134;&#34892;&#36712;&#36857;&#24182;&#19988;&#23545;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#65292;&#20026;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33322;&#31354;&#30740;&#31350;&#20013;&#30340;&#39134;&#34892;&#36712;&#36857;&#26816;&#27979;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#20013;&#20132;&#36890;&#23545;&#29615;&#22659;&#20135;&#29983;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#39134;&#34892;&#36712;&#36857;&#36896;&#25104;&#30340;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#36129;&#29486;&#65292;&#30001;&#20110;&#20854;&#28508;&#22312;&#30340;&#20840;&#29699;&#21464;&#26262;&#24433;&#21709;&#12290;&#20174;&#21355;&#26143;&#22270;&#20687;&#20013;&#26816;&#27979;&#39134;&#34892;&#36712;&#36857;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38590;&#39064;&#12290;&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#22312;&#19981;&#21516;&#30340;&#22270;&#20687;&#26465;&#20214;&#19979;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#32780;&#20351;&#29992;&#20856;&#22411;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21463;&#21040;&#25163;&#24037;&#26631;&#27880;&#30340;&#39134;&#34892;&#36712;&#36857;&#25968;&#25454;&#38598;&#21644;&#19987;&#38376;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#22411;&#65292;&#22522;&#20110;&#22686;&#24378;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#39134;&#34892;&#36712;&#36857;&#65292;&#24182;&#19988;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;SR Loss&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#31354;&#38388;&#36716;&#25442;&#20026;&#38669;&#22827;&#31354;&#38388;&#26469;&#25913;&#21892;&#39134;&#34892;&#36712;&#36857;&#32447;&#30340;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33322;&#31354;&#30740;&#31350;&#20013;&#30340;&#39134;&#34892;&#36712;&#36857;&#26816;&#27979;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#22823;&#22411;&#25163;&#24037;&#26631;&#27880;&#25968;&#25454;&#38598;&#32570;&#20047;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#22686;&#24378;&#20102;&#39134;&#34892;&#36712;&#36857;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Air transport poses significant environmental challenges, particularly the contribution of flight contrails to climate change due to their potential global warming impact. Detecting contrails from satellite images has been a long-standing challenge. Traditional computer vision techniques have limitations under varying image conditions, and machine learning approaches using typical convolutional neural networks are hindered by the scarcity of hand-labeled contrail datasets and contrail-tailored learning processes. In this paper, we introduce an innovative model based on augmented transfer learning that accurately detects contrails with minimal data. We also propose a novel loss function, SR Loss, which improves contrail line detection by transforming the image space into Hough space. Our research opens new avenues for machine learning-based contrail detection in aviation research, offering solutions to the lack of large hand-labeled datasets, and significantly enhancing contrail detecti
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28508;&#22312;&#25928;&#29992;Q&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#24739;&#32773;&#20559;&#22909;&#32435;&#20837;&#22797;&#21512;&#32467;&#26524;&#30340;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#20013;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#23545;&#26102;&#38388;&#28857;&#21644;&#32467;&#26524;&#25968;&#37327;&#30340;&#38480;&#21046;&#65292;&#33021;&#22815;&#23454;&#29616;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.12022</link><description>&lt;p&gt;
&#23558;&#24739;&#32773;&#20559;&#22909;&#32435;&#20837;Q&#23398;&#20064;&#30340;&#28789;&#27963;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Flexible Framework for Incorporating Patient Preferences Into Q-Learning. (arXiv:2307.12022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12022
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28508;&#22312;&#25928;&#29992;Q&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#24739;&#32773;&#20559;&#22909;&#32435;&#20837;&#22797;&#21512;&#32467;&#26524;&#30340;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#20013;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#23545;&#26102;&#38388;&#28857;&#21644;&#32467;&#26524;&#25968;&#37327;&#30340;&#38480;&#21046;&#65292;&#33021;&#22815;&#23454;&#29616;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#21307;&#30103;&#38382;&#39064;&#20013;&#65292;&#36890;&#24120;&#23384;&#22312;&#22810;&#20010;&#31454;&#20105;&#24615;&#30340;&#20851;&#27880;&#28857;&#65292;&#22914;&#27835;&#30103;&#30103;&#25928;&#21644;&#21103;&#20316;&#29992;&#20005;&#37325;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#20272;&#35745;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696; (DTRs) &#30340;&#32479;&#35745;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#21482;&#26377;&#19968;&#20010;&#20851;&#27880;&#28857;&#65292;&#32780;&#22788;&#29702;&#22797;&#21512;&#32467;&#26524;&#30340;&#26041;&#27861;&#24456;&#23569;&#65292;&#23384;&#22312;&#37325;&#35201;&#38480;&#21046;&#65292;&#21253;&#25324;&#23545;&#21333;&#20010;&#26102;&#38388;&#28857;&#21644;&#20004;&#20010;&#32467;&#26524;&#30340;&#38480;&#21046;&#12289;&#26080;&#27861;&#32435;&#20837;&#24739;&#32773;&#30340;&#33258;&#36848;&#20559;&#22909;&#20197;&#21450;&#26377;&#38480;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#28508;&#22312;&#25928;&#29992;Q&#23398;&#20064;(LUQ-Learning)&#12290;LUQ-Learning&#37319;&#29992;&#28508;&#22312;&#27169;&#22411;&#26041;&#27861;&#65292;&#33258;&#28982;&#22320;&#23558;Q&#23398;&#20064;&#25193;&#23637;&#21040;&#22797;&#21512;&#32467;&#26524;&#35774;&#32622;&#65292;&#24182;&#20026;&#27599;&#20010;&#24739;&#32773;&#36873;&#25321;&#29702;&#24819;&#30340;&#32467;&#26524;&#26435;&#34913;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#20219;&#24847;&#25968;&#37327;&#30340;&#26102;&#38388;&#28857;&#21644;&#32467;&#26524;&#65292;&#32435;&#20837;&#38472;&#36848;&#30340;&#20559;&#22909;&#65292;&#24182;&#23454;&#29616;&#24378;&#22823;&#30340;&#28176;&#36817;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world healthcare problems, there are often multiple competing outcomes of interest, such as treatment efficacy and side effect severity. However, statistical methods for estimating dynamic treatment regimes (DTRs) usually assume a single outcome of interest, and the few methods that deal with composite outcomes suffer from important limitations. This includes restrictions to a single time point and two outcomes, the inability to incorporate self-reported patient preferences and limited theoretical guarantees. To this end, we propose a new method to address these limitations, which we dub Latent Utility Q-Learning (LUQ-Learning). LUQ-Learning uses a latent model approach to naturally extend Q-learning to the composite outcome setting and adopt the ideal trade-off between outcomes to each patient. Unlike previous approaches, our framework allows for an arbitrary number of time points and outcomes, incorporates stated preferences and achieves strong asymptotic performance with rea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#22810;&#27493;&#34880;&#31958;&#39044;&#27979;&#22120;&#19982;&#32447;&#24615;&#26102;&#21464;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#30456;&#32467;&#21512;&#30340;&#38381;&#29615;&#33008;&#23707;&#32032;&#36755;&#36865;&#31639;&#27861;&#35774;&#35745;&#65292;&#29992;&#20110;&#27835;&#30103;1&#22411;&#31958;&#23615;&#30149;&#12290;&#20316;&#32773;&#36890;&#36807;&#30452;&#25509;&#25311;&#21512;&#25972;&#20010;&#34880;&#31958;&#39044;&#27979;&#26354;&#32447;&#65292;&#32467;&#21512;&#38750;&#32447;&#24615;&#21644;&#32447;&#24615;&#27169;&#22411;&#65292;&#19982;&#20256;&#32479;&#32447;&#24615;MPC&#36827;&#34892;&#23545;&#27604;&#65292;&#35780;&#20272;&#20102;&#31639;&#27861;&#30340;&#20248;&#21155;&#12290;</title><link>http://arxiv.org/abs/2307.12015</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#22810;&#27493;&#39044;&#27979;&#34880;&#31958;&#39044;&#27979;&#22120;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#20154;&#24037;&#33008;&#33146;
&lt;/p&gt;
&lt;p&gt;
Model Predictive Control (MPC) of an Artificial Pancreas with Data-Driven Learning of Multi-Step-Ahead Blood Glucose Predictors. (arXiv:2307.12015v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#22810;&#27493;&#34880;&#31958;&#39044;&#27979;&#22120;&#19982;&#32447;&#24615;&#26102;&#21464;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#30456;&#32467;&#21512;&#30340;&#38381;&#29615;&#33008;&#23707;&#32032;&#36755;&#36865;&#31639;&#27861;&#35774;&#35745;&#65292;&#29992;&#20110;&#27835;&#30103;1&#22411;&#31958;&#23615;&#30149;&#12290;&#20316;&#32773;&#36890;&#36807;&#30452;&#25509;&#25311;&#21512;&#25972;&#20010;&#34880;&#31958;&#39044;&#27979;&#26354;&#32447;&#65292;&#32467;&#21512;&#38750;&#32447;&#24615;&#21644;&#32447;&#24615;&#27169;&#22411;&#65292;&#19982;&#20256;&#32479;&#32447;&#24615;MPC&#36827;&#34892;&#23545;&#27604;&#65292;&#35780;&#20272;&#20102;&#31639;&#27861;&#30340;&#20248;&#21155;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23553;&#38381;&#24335;&#33008;&#23707;&#32032;&#36755;&#36865;&#31639;&#27861;&#30340;&#35774;&#35745;&#21644;\textit{in-silico}&#35780;&#20272;&#65292;&#29992;&#20110;&#27835;&#30103;1&#22411;&#31958;&#23615;&#30149;&#65288;T1D&#65289;&#65292;&#35813;&#31639;&#27861;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#22810;&#27493;&#21069;&#30651;&#34880;&#31958;(BG)&#39044;&#27979;&#22120;&#65292;&#19982;&#19968;&#20010;&#32447;&#24615;&#26102;&#21464;&#65288;LTV&#65289;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#26694;&#26550;&#38598;&#25104;&#12290;&#25105;&#20204;&#24314;&#35758;&#30452;&#25509;&#25311;&#21512;&#25972;&#20010;BG&#39044;&#27979;&#26354;&#32447;&#65292;&#20316;&#20026;MPC&#20013;&#39044;&#23450;&#20041;&#39044;&#27979;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#36825;&#20010;&#38750;&#32447;&#24615;&#20989;&#25968;&#30001;&#36807;&#21435;&#30340;&#36755;&#20837;-&#36755;&#20986;&#25968;&#25454;&#21644;&#26410;&#26469;&#30340;&#33008;&#23707;&#32032;&#25511;&#21046;&#36755;&#20837;&#30340;&#32447;&#24615;&#20989;&#25968;&#32452;&#25104;&#12290;&#23545;&#20110;&#38750;&#32447;&#24615;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#65292;&#32780;&#23545;&#20110;&#32447;&#24615;&#37096;&#20998;&#65292;&#36873;&#25321;&#20102;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;&#19982;&#20174;&#25968;&#25454;&#20013;&#35782;&#21035;&#20986;&#30340;&#20256;&#32479;&#32447;&#24615;MPC&#65288;&#22522;&#20110;&#33258;&#22238;&#24402;&#22806;&#29983;&#65288;ARX&#65289;&#36755;&#20837;&#27169;&#22411;&#65289;&#30456;&#27604;&#30340;&#20248;&#21155;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#27169;&#25311;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;&#25552;&#35758;&#30340;LSTM-MPC&#25511;&#21046;&#22120;&#65306;&#19968;&#20010;&#27491;&#24120;&#24773;&#20917;wit
&lt;/p&gt;
&lt;p&gt;
We present the design and \textit{in-silico} evaluation of a closed-loop insulin delivery algorithm to treat type 1 diabetes (T1D) consisting in a data-driven multi-step-ahead blood glucose (BG) predictor integrated into a Linear Time-Varying (LTV) Model Predictive Control (MPC) framework. Instead of identifying an open-loop model of the glucoregulatory system from available data, we propose to directly fit the entire BG prediction over a predefined prediction horizon to be used in the MPC, as a nonlinear function of past input-ouput data and an affine function of future insulin control inputs. For the nonlinear part, a Long Short-Term Memory (LSTM) network is proposed, while for the affine component a linear regression model is chosen. To assess benefits and drawbacks when compared to a traditional linear MPC based on an auto-regressive with exogenous (ARX) input model identified from data, we evaluated the proposed LSTM-MPC controller in three simulation scenarios: a nominal case wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21457;&#38469;&#32447;&#32454;&#33410;&#30340;&#21333;&#22270;&#20687;&#36229;&#20998;&#36776;&#32593;&#32476;&#65288;NLCUnet&#65289;&#12290;&#35813;&#32593;&#32476;&#20351;&#29992;&#38750;&#23616;&#37096;&#27880;&#24847;&#26426;&#21046;&#24674;&#22797;&#23616;&#37096;&#32454;&#33410;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#27169;&#31946;&#26680;&#20272;&#35745;&#30340;&#26032;&#32593;&#32476;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20013;&#24515;&#35009;&#21098;&#20869;&#37096;&#36827;&#34892;&#38543;&#26426;&#35009;&#21098;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#35821;&#20041;&#20449;&#24687;&#30340;&#21033;&#29992;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;NLCUnet&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2307.12014</link><description>&lt;p&gt;
NLCUnet: &#20855;&#26377;&#21457;&#38469;&#32447;&#32454;&#33410;&#30340;&#21333;&#22270;&#20687;&#36229;&#20998;&#36776;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
NLCUnet: Single-Image Super-Resolution Network with Hairline Details. (arXiv:2307.12014v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21457;&#38469;&#32447;&#32454;&#33410;&#30340;&#21333;&#22270;&#20687;&#36229;&#20998;&#36776;&#32593;&#32476;&#65288;NLCUnet&#65289;&#12290;&#35813;&#32593;&#32476;&#20351;&#29992;&#38750;&#23616;&#37096;&#27880;&#24847;&#26426;&#21046;&#24674;&#22797;&#23616;&#37096;&#32454;&#33410;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#27169;&#31946;&#26680;&#20272;&#35745;&#30340;&#26032;&#32593;&#32476;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20013;&#24515;&#35009;&#21098;&#20869;&#37096;&#36827;&#34892;&#38543;&#26426;&#35009;&#21098;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#35821;&#20041;&#20449;&#24687;&#30340;&#21033;&#29992;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;NLCUnet&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21333;&#22270;&#20687;&#36229;&#20998;&#36776;&#20219;&#21153;&#26469;&#35828;&#65292;&#36861;&#27714;&#36229;&#20998;&#36776;&#22270;&#20687;&#30340;&#31934;&#32454;&#32454;&#33410;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21457;&#38469;&#32447;&#32454;&#33410;&#30340;&#21333;&#22270;&#20687;&#36229;&#20998;&#36776;&#32593;&#32476;&#65288;&#31216;&#20026;NLCUnet&#65289;&#65292;&#21253;&#25324;&#19977;&#20010;&#26680;&#24515;&#35774;&#35745;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38750;&#23616;&#37096;&#27880;&#24847;&#26426;&#21046;&#65292;&#36890;&#36807;&#20174;&#25972;&#20010;&#22270;&#20687;&#21306;&#22495;&#23398;&#20064;&#26469;&#24674;&#22797;&#23616;&#37096;&#32454;&#33410;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#29616;&#26377;&#24037;&#20316;&#35757;&#32451;&#30340;&#27169;&#31946;&#26680;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#23558;&#28145;&#24230;&#21367;&#31215;&#19982;&#36890;&#36947;&#27880;&#24847;&#21147;&#38598;&#25104;&#65292;&#32780;&#19981;&#38656;&#35201;&#20272;&#35745;&#27169;&#31946;&#26680;&#65292;&#32467;&#26524;&#23548;&#33268;&#24615;&#33021;&#25552;&#21319;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#20351;&#35009;&#21098;&#21306;&#22495;&#21253;&#21547;&#23613;&#21487;&#33021;&#22810;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20013;&#24515;512x512&#35009;&#21098;&#20869;&#37096;&#36827;&#34892;&#38543;&#26426;64x64&#35009;&#21098;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#22312;&#25972;&#20010;2K&#22823;&#23567;&#30340;&#22270;&#20687;&#20869;&#36827;&#34892;&#38543;&#26426;&#35009;&#21098;&#12290;&#22312;&#22522;&#20934;DF2K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;NLCUnet&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pursuing the precise details of super-resolution images is challenging for single-image super-resolution tasks. This paper presents a single-image super-resolution network with hairline details (termed NLCUnet), including three core designs. Specifically, a non-local attention mechanism is first introduced to restore local pieces by learning from the whole image region. Then, we find that the blur kernel trained by the existing work is unnecessary. Based on this finding, we create a new network architecture by integrating depth-wise convolution with channel attention without the blur kernel estimation, resulting in a performance improvement instead. Finally, to make the cropped region contain as much semantic information as possible, we propose a random 64$\times$64 crop inside the central 512$\times$512 crop instead of a direct random crop inside the whole image of 2K size. Numerous experiments conducted on the benchmark DF2K dataset demonstrate that our NLCUnet performs better than t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#65292;&#21517;&#20026;MIMIC-Diff-VQA&#65292;&#20026;&#33258;&#21160;&#21270;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#22238;&#31572;&#20851;&#20110;&#30142;&#30149;&#21644;&#22270;&#20687;&#24046;&#24322;&#30340;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20102;&#19987;&#23478;&#30693;&#35782;&#24863;&#30693;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.11986</link><description>&lt;p&gt;
Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering.&#65288;&#19987;&#23478;&#30693;&#35782;&#24863;&#30693;&#30340;&#22270;&#20687;&#21464;&#21270;&#22270;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#20851;&#27880;&#24046;&#24322;&#30340;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#65289;
&lt;/p&gt;
&lt;p&gt;
Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering. (arXiv:2307.11986v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#65292;&#21517;&#20026;MIMIC-Diff-VQA&#65292;&#20026;&#33258;&#21160;&#21270;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#22238;&#31572;&#20851;&#20110;&#30142;&#30149;&#21644;&#22270;&#20687;&#24046;&#24322;&#30340;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20102;&#19987;&#23478;&#30693;&#35782;&#24863;&#30693;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20026;&#33258;&#21160;&#21270;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20570;&#20986;&#36129;&#29486;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33016;&#37096;X&#20809;&#22270;&#20687;&#24046;&#24322;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#22238;&#31572;&#20960;&#20010;&#20851;&#20110;&#30142;&#30149;&#20197;&#21450;&#26356;&#37325;&#35201;&#30340;&#26159;&#23427;&#20204;&#20043;&#38388;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;&#36825;&#19982;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#35786;&#26029;&#23454;&#36341;&#30456;&#19968;&#33268;&#65292;&#25918;&#23556;&#31185;&#21307;&#29983;&#22312;&#24471;&#20986;&#25253;&#21578;&#20043;&#21069;&#20250;&#23545;&#24403;&#21069;&#22270;&#20687;&#19982;&#21442;&#32771;&#22270;&#20687;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;MIMIC-Diff-VQA&#65292;&#21253;&#25324;&#26469;&#33258;164,324&#23545;&#20027;&#22270;&#20687;&#21644;&#21442;&#32771;&#22270;&#20687;&#30340;700,703&#20010;&#38382;&#39064;-&#31572;&#26696;&#37197;&#23545;&#12290;&#19982;&#29616;&#26377;&#30340;&#21307;&#23398;VQA&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#38382;&#39064;&#38024;&#23545;&#20102;&#20020;&#24202;&#19987;&#19994;&#20154;&#21592;&#20351;&#29992;&#30340;&#35780;&#20272;-&#35786;&#26029;-&#24178;&#39044;-&#35780;&#20272;&#27835;&#30103;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#30693;&#35782;&#24863;&#30693;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
To contribute to automating the medical vision-language model, we propose a novel Chest-Xray Difference Visual Question Answering (VQA) task. Given a pair of main and reference images, this task attempts to answer several questions on both diseases and, more importantly, the differences between them. This is consistent with the radiologist's diagnosis practice that compares the current image with the reference before concluding the report. We collect a new dataset, namely MIMIC-Diff-VQA, including 700,703 QA pairs from 164,324 pairs of main and reference images. Compared to existing medical VQA datasets, our questions are tailored to the Assessment-Diagnosis-Intervention-Evaluation treatment procedure used by clinical professionals. Meanwhile, we also propose a novel expert knowledge-aware graph representation learning model to address this task. The proposed baseline model leverages expert knowledge such as anatomical structure prior, semantic, and spatial knowledge to construct a mul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#23646;&#24615;&#32593;&#32476;&#23884;&#20837;&#65292;&#36890;&#36807;&#28145;&#20837;&#21442;&#19982;&#33410;&#28857;&#23646;&#24615;&#26469;&#22686;&#24378;&#33410;&#28857;&#36830;&#25509;&#24182;&#25913;&#21892;&#23545;&#38750;&#27963;&#21160;&#33410;&#28857;&#30340;&#24863;&#21463;&#22495;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.11981</link><description>&lt;p&gt;
&#21327;&#20316;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#23646;&#24615;&#32593;&#32476;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Collaborative Graph Neural Networks for Attributed Network Embedding. (arXiv:2307.11981v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#23646;&#24615;&#32593;&#32476;&#23884;&#20837;&#65292;&#36890;&#36807;&#28145;&#20837;&#21442;&#19982;&#33410;&#28857;&#23646;&#24615;&#26469;&#22686;&#24378;&#33410;&#28857;&#36830;&#25509;&#24182;&#25913;&#21892;&#23545;&#38750;&#27963;&#21160;&#33410;&#28857;&#30340;&#24863;&#21463;&#22495;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#23646;&#24615;&#32593;&#32476;&#23884;&#20837;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21033;&#29992;&#32593;&#32476;&#32467;&#26500;&#65292;&#32780;&#23545;&#33410;&#28857;&#23646;&#24615;&#30340;&#21033;&#29992;&#30456;&#23545;&#36739;&#23569;&#65292;&#23427;&#20204;&#21482;&#22312;&#21021;&#22987;&#23618;&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#12290;&#36825;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#38480;&#21046;&#20102;&#33410;&#28857;&#23646;&#24615;&#22312;&#22686;&#24378;&#33410;&#28857;&#36830;&#25509;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#23548;&#33268;&#20102;&#23545;&#20855;&#26377;&#23569;&#37327;&#29978;&#33267;&#27809;&#26377;&#37051;&#23621;&#30340;&#38750;&#27963;&#21160;&#33410;&#28857;&#30340;&#26377;&#38480;&#24863;&#21463;&#22495;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;GNN&#30340;&#35757;&#32451;&#30446;&#26631;&#65288;&#21363;&#37325;&#26500;&#32593;&#32476;&#32467;&#26500;&#65289;&#20063;&#19981;&#21253;&#25324;&#33410;&#28857;&#23646;&#24615;&#65292;&#23613;&#31649;&#30740;&#31350;&#34920;&#26126;&#37325;&#26500;&#33410;&#28857;&#23646;&#24615;&#26159;&#26377;&#30410;&#30340;&#12290;&#22240;&#27492;&#65292;&#28145;&#20837;&#21442;&#19982;&#22270;&#21367;&#31215;&#25805;&#20316;&#21644;&#35757;&#32451;&#30446;&#26631;&#31561;GNN&#20851;&#38190;&#32452;&#20214;&#20013;&#30340;&#33410;&#28857;&#23646;&#24615;&#26159;&#24456;&#26377;&#21069;&#26223;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#38656;&#35201;&#36866;&#24403;&#30340;&#38598;&#25104;&#26041;&#24335;&#26469;&#20445;&#25345;GNN&#30340;&#20248;&#28857;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21327;&#20316;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;...
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have shown prominent performance on attributed network embedding. However, existing efforts mainly focus on exploiting network structures, while the exploitation of node attributes is rather limited as they only serve as node features at the initial layer. This simple strategy impedes the potential of node attributes in augmenting node connections, leading to limited receptive field for inactive nodes with few or even no neighbors. Furthermore, the training objectives (i.e., reconstructing network structures) of most GNNs also do not include node attributes, although studies have shown that reconstructing node attributes is beneficial. Thus, it is encouraging to deeply involve node attributes in the key components of GNNs, including graph convolution operations and training objectives. However, this is a nontrivial task since an appropriate way of integration is required to maintain the merits of GNNs. To bridge the gap, in this paper, we propose COllaborat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#21464;&#25442;&#22120;&#30340;&#36845;&#20195;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;MRI&#20013;&#21512;&#25104;&#20855;&#26377;&#20219;&#24847;&#23545;&#27604;&#22686;&#24378;&#27700;&#24179;&#30340;&#22270;&#20687;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#23376;&#37319;&#26679;&#30340;&#27880;&#24847;&#26426;&#21046;&#21644;&#26059;&#36716;&#24179;&#31227;&#27169;&#22359;&#26469;&#25429;&#25417;&#21508;&#31181;&#23545;&#27604;&#24230;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#23450;&#37327;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#27169;&#22411;&#22312;&#21058;&#37327;&#38477;&#20302;&#21644;&#32959;&#30244;&#20998;&#21106;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.11980</link><description>&lt;p&gt;
&#20351;&#29992;&#36845;&#20195;&#20840;&#23616;&#21464;&#25442;&#22120;&#27169;&#22411;&#22312;MRI&#20013;&#27169;&#25311;&#20219;&#24847;&#32423;&#21035;&#23545;&#27604;&#21058;&#21058;&#37327;&#30340;&#20223;&#30495;
&lt;/p&gt;
&lt;p&gt;
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model. (arXiv:2307.11980v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#21464;&#25442;&#22120;&#30340;&#36845;&#20195;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;MRI&#20013;&#21512;&#25104;&#20855;&#26377;&#20219;&#24847;&#23545;&#27604;&#22686;&#24378;&#27700;&#24179;&#30340;&#22270;&#20687;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#23376;&#37319;&#26679;&#30340;&#27880;&#24847;&#26426;&#21046;&#21644;&#26059;&#36716;&#24179;&#31227;&#27169;&#22359;&#26469;&#25429;&#25417;&#21508;&#31181;&#23545;&#27604;&#24230;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#23450;&#37327;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#27169;&#22411;&#22312;&#21058;&#37327;&#38477;&#20302;&#21644;&#32959;&#30244;&#20998;&#21106;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;MRI&#22270;&#20687;&#20013;&#23545;&#27604;&#21058;&#21058;&#37327;&#30340;&#38477;&#20302;&#21644;&#28040;&#38500;&#24471;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#32771;&#34385;&#21040;&#38022;&#22522;&#23545;&#27604;&#21058;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#21463;&#21040;&#39640;&#36136;&#37327;&#20302;&#21058;&#37327;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#27604;&#21058;&#21644;&#30149;&#29702;&#38656;&#35201;&#19981;&#21516;&#21058;&#37327;&#27700;&#24179;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#25165;&#33021;&#21487;&#38752;&#24037;&#20316;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#21464;&#25442;&#22120;(Gformer)&#30340;&#26032;&#22411;&#36845;&#20195;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#20855;&#26377;&#19982;&#19981;&#21516;&#21058;&#37327;&#27700;&#24179;&#30456;&#23545;&#24212;&#30340;&#20219;&#24847;&#23545;&#27604;&#22686;&#24378;&#22270;&#20687;&#12290;&#25152;&#25552;&#20986;&#30340;Gformer&#32467;&#21512;&#20102;&#22522;&#20110;&#23376;&#37319;&#26679;&#30340;&#27880;&#24847;&#26426;&#21046;&#21644;&#26059;&#36716;&#24179;&#31227;&#27169;&#22359;&#65292;&#25429;&#25417;&#20102;&#21508;&#31181;&#19982;&#23545;&#27604;&#24230;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#23450;&#37327;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#23545;&#21058;&#37327;&#38477;&#20302;&#21644;&#32959;&#30244;&#20998;&#21106;&#31561;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#20102;&#23450;&#37327;&#35780;&#20272;&#65292;&#20197;&#23637;&#31034;&#20854;&#20020;&#24202;-
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) based contrast dose reduction and elimination in MRI imaging is gaining traction, given the detrimental effects of Gadolinium-based Contrast Agents (GBCAs). These DL algorithms are however limited by the availability of high quality low dose datasets. Additionally, different types of GBCAs and pathologies require different dose levels for the DL algorithms to work reliably. In this work, we formulate a novel transformer (Gformer) based iterative modelling approach for the synthesis of images with arbitrary contrast enhancement that corresponds to different dose levels. The proposed Gformer incorporates a sub-sampling based attention mechanism and a rotational shift module that captures the various contrast related features. Quantitative evaluation indicates that the proposed model performs better than other state-of-the-art methods. We further perform quantitative evaluation on downstream tasks such as dose reduction and tumor segmentation to demonstrate the clinical
&lt;/p&gt;</description></item><item><title>&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#23569;&#26679;&#26412;&#25552;&#31034;&#35843;&#21442;&#30340;&#26041;&#24335;&#36866;&#24212;&#26032;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#19988;&#23545;&#20110;&#22122;&#22768;&#26631;&#31614;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#20851;&#38190;&#21407;&#22240;&#21253;&#25324;&#22266;&#23450;&#30340;&#31867;&#21517;&#26631;&#35760;&#23545;&#27169;&#22411;&#20248;&#21270;&#30340;&#27491;&#21017;&#21270;&#20316;&#29992;&#20197;&#21450;&#20174;&#22810;&#26679;&#19988;&#36890;&#29992;&#30340;&#32593;&#32476;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#30340;&#24378;&#22823;&#39044;&#35757;&#32451;&#22270;&#20687;-&#25991;&#26412;&#23884;&#20837;&#25552;&#20379;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2307.11978</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#35843;&#21442;&#23545;&#20110;&#22122;&#22768;&#26631;&#31614;&#20855;&#26377;&#40065;&#26834;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?. (arXiv:2307.11978v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11978
&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#23569;&#26679;&#26412;&#25552;&#31034;&#35843;&#21442;&#30340;&#26041;&#24335;&#36866;&#24212;&#26032;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#19988;&#23545;&#20110;&#22122;&#22768;&#26631;&#31614;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#20851;&#38190;&#21407;&#22240;&#21253;&#25324;&#22266;&#23450;&#30340;&#31867;&#21517;&#26631;&#35760;&#23545;&#27169;&#22411;&#20248;&#21270;&#30340;&#27491;&#21017;&#21270;&#20316;&#29992;&#20197;&#21450;&#20174;&#22810;&#26679;&#19988;&#36890;&#29992;&#30340;&#32593;&#32476;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#30340;&#24378;&#22823;&#39044;&#35757;&#32451;&#22270;&#20687;-&#25991;&#26412;&#23884;&#20837;&#25552;&#20379;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#36890;&#36807;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#23398;&#20064;&#20102;&#36890;&#29992;&#30340;&#25991;&#26412;-&#22270;&#20687;&#23884;&#20837;&#12290;&#36890;&#36807;&#23569;&#26679;&#26412;&#25552;&#31034;&#35843;&#21442;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#20351;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#26032;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#25552;&#31034;&#35843;&#21442;&#36807;&#31243;&#23545;&#20110;&#22122;&#22768;&#26631;&#31614;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#30740;&#31350;&#25552;&#31034;&#35843;&#21442;&#33539;&#24335;&#40065;&#26834;&#24615;&#30340;&#20851;&#38190;&#21407;&#22240;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#20851;&#38190;&#22240;&#32032;&#21253;&#25324;&#65306;1&#65289;&#22266;&#23450;&#30340;&#31867;&#21517;&#26631;&#35760;&#23545;&#27169;&#22411;&#30340;&#20248;&#21270;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#27491;&#21017;&#21270;&#20316;&#29992;&#65292;&#20943;&#23569;&#20102;&#22122;&#22768;&#26679;&#26412;&#24341;&#36215;&#30340;&#26799;&#24230;&#65307;2&#65289;&#20174;&#22810;&#26679;&#19988;&#36890;&#29992;&#30340;&#32593;&#32476;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#30340;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#22270;&#20687;-&#25991;&#26412;&#23884;&#20837;&#20026;&#22270;&#20687;&#20998;&#31867;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#21033;&#29992;CLIP&#20013;&#30340;&#22122;&#22768;&#38646;&#26679;&#26412;&#39044;&#27979;&#26469;&#35843;&#25972;&#20854;&#33258;&#36523;&#30340;&#25552;&#31034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/CE&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models such as CLIP learn a generic text-image embedding from large-scale training data. A vision-language model can be adapted to a new classification task through few-shot prompt tuning. We find that such a prompt tuning process is highly robust to label noises. This intrigues us to study the key reasons contributing to the robustness of the prompt tuning paradigm. We conducted extensive experiments to explore this property and find the key factors are: 1) the fixed classname tokens provide a strong regularization to the optimization of the model, reducing gradients induced by the noisy samples; 2) the powerful pre-trained image-text embedding that is learned from diverse and generic web data provides strong prior knowledge for image classification. Further, we demonstrate that noisy zero-shot predictions from CLIP can be used to tune its own prompt, significantly enhancing prediction accuracy in the unsupervised setting. The code is available at https://github.com/CE
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;IRM&#30340;&#29702;&#35770;&#39564;&#35777;&#65292;&#20005;&#26684;&#35777;&#26126;&#20102;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#26368;&#23567;&#21270;&#21306;&#22806;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2307.11972</link><description>&lt;p&gt;
&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#21306;&#22806;&#20248;&#21270;&#24615;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution Optimality of Invariant Risk Minimization. (arXiv:2307.11972v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;IRM&#30340;&#29702;&#35770;&#39564;&#35777;&#65292;&#20005;&#26684;&#35777;&#26126;&#20102;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#26368;&#23567;&#21270;&#21306;&#22806;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#32487;&#25215;&#35757;&#32451;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#22240;&#27492;&#21487;&#33021;&#26080;&#27861;&#27867;&#21270;&#21040;&#20855;&#26377;&#19982;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#30340;&#39046;&#22495;&#19981;&#21516;&#30340;&#26410;&#30693;&#22495;&#12290;M. Arjovsky&#31561;&#20154;&#65288;2019&#24180;&#65289;&#24341;&#20837;&#20102;&#21306;&#22806;&#65288;o.o.d.&#65289;&#39118;&#38505;&#30340;&#27010;&#24565;&#65292;&#21363;&#25152;&#26377;&#22495;&#20013;&#30340;&#26368;&#22823;&#39118;&#38505;&#65292;&#24182;&#23558;&#30001;&#34394;&#20551;&#30456;&#20851;&#24615;&#24341;&#36215;&#30340;&#38382;&#39064;&#35268;&#23450;&#20026;&#26368;&#23567;&#21270;&#21306;&#22806;&#39118;&#38505;&#30340;&#38382;&#39064;&#12290;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;IRM&#65289;&#34987;&#35748;&#20026;&#26159;&#26368;&#23567;&#21270;&#21306;&#22806;&#39118;&#38505;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65306;IRM&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#26469;&#20272;&#35745;&#26368;&#23567;&#21270;&#30340;&#21306;&#22806;&#39118;&#38505;&#12290;&#23613;&#31649;IRM&#20197;&#23454;&#35777;&#25104;&#21151;&#21560;&#24341;&#20102;&#30456;&#24403;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#23427;&#32570;&#20047;&#19968;&#20123;&#29702;&#35770;&#20445;&#35777;&#12290;&#29305;&#21035;&#26159;&#65292;&#36824;&#27809;&#26377;&#30830;&#31435;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#32473;&#20986;&#26368;&#23567;&#21270;&#21306;&#22806;&#39118;&#38505;&#30340;&#22362;&#23454;&#29702;&#35770;&#20445;&#35777;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;IRM&#30340;&#29702;&#35770;&#39564;&#35777;&#65292;&#20005;&#26684;&#35777;&#26126;&#20102;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#36890;&#36807;&#22312;&#22823;&#20223;&#30495;&#36319;&#36394;&#25968;&#25454;&#24211;&#20013;&#36827;&#34892;&#23454;&#26102;&#20223;&#30495;&#65292;&#20854;&#21253;&#25324;&#23545;&#21608;&#22260;&#29615;&#22659;&#30340;&#30452;&#25509;&#24863;&#30693;&#65292;&#23545;&#28508;&#22312;&#36335;&#32447;&#35268;&#21010;&#30340;&#31574;&#30053;&#35748;&#35782;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#22810;&#36710;&#36742;&#20132;&#20114;&#65292;&#20197;&#23454;&#29616;&#35813;&#38382;&#39064;&#30340;&#20840;&#23616;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks often inherit spurious correlations embedded in training data and hence may fail to generalize to unseen domains, which have different distributions from the domain to provide training data. M. Arjovsky et al. (2019) introduced the concept out-of-distribution (o.o.d.) risk, which is the maximum risk among all domains, and formulated the issue caused by spurious correlations as a minimization problem of the o.o.d. risk. Invariant Risk Minimization (IRM) is considered to be a promising approach to minimize the o.o.d. risk: IRM estimates a minimum of the o.o.d. risk by solving a bi-level optimization problem. While IRM has attracted considerable attention with empirical success, it comes with few theoretical guarantees. Especially, a solid theoretical guarantee that the bi-level optimization problem gives the minimum of the o.o.d. risk has not yet been established. Aiming at providing a theoretical justification for IRM, this paper rigorously proves that a solution to
&lt;/p&gt;</description></item><item><title>DHC&#26159;&#19968;&#31181;&#29992;&#20110;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#21452;&#21435;&#20559;&#24322;&#26500;&#32852;&#21512;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20998;&#24067;&#24863;&#30693;&#21644;&#38590;&#24230;&#24863;&#30693;&#30340;&#21435;&#20559;&#26435;&#37325;&#31574;&#30053;&#65292;&#20197;&#21160;&#24577;&#24341;&#23548;&#27169;&#22411;&#35299;&#20915;&#25968;&#25454;&#21644;&#23398;&#20064;&#20559;&#35265;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11960</link><description>&lt;p&gt;
DHC: &#21452;&#21435;&#20559;&#24322;&#26500;&#32852;&#21512;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
DHC: Dual-debiased Heterogeneous Co-training Framework for Class-imbalanced Semi-supervised Medical Image Segmentation. (arXiv:2307.11960v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11960
&lt;/p&gt;
&lt;p&gt;
DHC&#26159;&#19968;&#31181;&#29992;&#20110;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#21452;&#21435;&#20559;&#24322;&#26500;&#32852;&#21512;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20998;&#24067;&#24863;&#30693;&#21644;&#38590;&#24230;&#24863;&#30693;&#30340;&#21435;&#20559;&#26435;&#37325;&#31574;&#30053;&#65292;&#20197;&#21160;&#24577;&#24341;&#23548;&#27169;&#22411;&#35299;&#20915;&#25968;&#25454;&#21644;&#23398;&#20064;&#20559;&#35265;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#30340;&#20307;&#31215;&#26631;&#35760;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#19988;&#32791;&#26102;&#65292;&#22240;&#27492;&#23545;&#20110;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#65292;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26159;&#38750;&#24120;&#29702;&#24819;&#30340;&#12290;&#19981;&#24179;&#34913;&#30340;&#31867;&#21035;&#20998;&#24067;&#26159;&#19968;&#20010;&#20005;&#37325;&#38382;&#39064;&#65292;&#21046;&#32422;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#36827;&#23637;&#65292;&#20294;&#21364;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#35299;&#20915;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#21435;&#20559;&#24322;&#26500;&#32852;&#21512;&#35757;&#32451;&#65288;DHC&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25439;&#22833;&#21152;&#26435;&#31574;&#30053;&#65292;&#21363;&#20998;&#24067;&#24863;&#30693;&#21435;&#20559;&#26435;&#37325;&#65288;DistDW&#65289;&#21644;&#38590;&#24230;&#24863;&#30693;&#21435;&#20559;&#26435;&#37325;&#65288;DiffDW&#65289;&#65292;&#21033;&#29992;&#20266;&#26631;&#31614;&#21160;&#24577;&#24341;&#23548;&#27169;&#22411;&#35299;&#20915;&#25968;&#25454;&#21644;&#23398;&#20064;&#20559;&#35265;&#12290;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#36825;&#20004;&#20010;&#22810;&#26679;&#19988;&#20934;&#30830;&#30340;&#23376;&#27169;&#22411;&#65292;&#26694;&#26550;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#22522;&#20934;&#65292;&#20805;&#20998;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The volume-wise labeling of 3D medical images is expertise-demanded and time-consuming; hence semi-supervised learning (SSL) is highly desirable for training with limited labeled data. Imbalanced class distribution is a severe problem that bottlenecks the real-world application of these methods but was not addressed much. Aiming to solve this issue, we present a novel Dual-debiased Heterogeneous Co-training (DHC) framework for semi-supervised 3D medical image segmentation. Specifically, we propose two loss weighting strategies, namely Distribution-aware Debiased Weighting (DistDW) and Difficulty-aware Debiased Weighting (DiffDW), which leverage the pseudo labels dynamically to guide the model to solve data and learning biases. The framework improves significantly by co-training these two diverse and accurate sub-models. We also introduce more representative benchmarks for class-imbalanced semi-supervised medical image segmentation, which can fully demonstrate the efficacy of the class-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#37325;&#35201;&#24615;&#26435;&#37325;&#24863;&#30693;&#65288;IWA&#65289;&#26356;&#26032;&#23545;&#20110;&#20984;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#20248;&#20110;&#26222;&#36890;&#26799;&#24230;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2307.11955</link><description>&lt;p&gt;
&#38544;&#24335;&#35299;&#37322;&#37325;&#35201;&#24615;&#26435;&#37325;&#24863;&#30693;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Implicit Interpretation of Importance Weight Aware Updates. (arXiv:2307.11955v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#37325;&#35201;&#24615;&#26435;&#37325;&#24863;&#30693;&#65288;IWA&#65289;&#26356;&#26032;&#23545;&#20110;&#20984;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#20248;&#20110;&#26222;&#36890;&#26799;&#24230;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#20854;&#36895;&#24230;&#21644;&#31616;&#21333;&#24615;&#65292;&#23376;&#26799;&#24230;&#19979;&#38477;&#26159;&#20984;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#26368;&#24120;&#29992;&#30340;&#20248;&#21270;&#31639;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#35843;&#25972;&#20854;&#23398;&#20064;&#29575;&#21487;&#33021;&#26159;&#23454;&#29616;&#19968;&#33268;&#33391;&#22909;&#24615;&#33021;&#30340;&#26368;&#20005;&#37325;&#29942;&#39048;&#12290;&#20943;&#23569;&#23545;&#23398;&#20064;&#29575;&#30340;&#20381;&#36182;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#38544;&#24335;/&#36817;&#31471;&#26356;&#26032;&#12290;&#20854;&#20013;&#19968;&#31181;&#21464;&#20307;&#26159;&#37325;&#35201;&#24615;&#26435;&#37325;&#24863;&#30693;&#65288;IWA&#65289;&#26356;&#26032;&#65292;&#20854;&#30001;&#27599;&#20010;&#25439;&#22833;&#20989;&#25968;&#19978;&#26080;&#38480;&#22810;&#20010;&#26080;&#31351;&#23567;&#26356;&#26032;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;IWA&#26356;&#26032;&#30340;&#32463;&#39564;&#25104;&#21151;&#24182;&#19981;&#33021;&#23436;&#20840;&#36890;&#36807;&#20854;&#29702;&#35770;&#26469;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;IWA&#26356;&#26032;&#22312;&#22312;&#32447;&#23398;&#20064;&#35774;&#32622;&#20013;&#20855;&#26377;&#20005;&#26684;&#26356;&#22909;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#20248;&#20110;&#26222;&#36890;&#26799;&#24230;&#26356;&#26032;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;&#26032;&#26694;&#26550;&#65306;&#24191;&#20041;&#38544;&#24335;Follow-the-Regularized-Leader&#65288;FTRL&#65289;&#65288;Chen&#21644;Orabona, 2023&#65289;&#65292;&#20351;&#29992;&#23545;&#20598;&#34920;&#36848;&#26469;&#20998;&#26512;&#24191;&#20041;&#38544;&#24335;&#26356;&#26032;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26263;&#31034;&#20102;IWA&#26356;&#26032;&#21487;&#20197;&#34987;&#35270;&#20026;
&lt;/p&gt;
&lt;p&gt;
Due to its speed and simplicity, subgradient descent is one of the most used optimization algorithms in convex machine learning algorithms. However, tuning its learning rate is probably its most severe bottleneck to achieve consistent good performance. A common way to reduce the dependency on the learning rate is to use implicit/proximal updates. One such variant is the Importance Weight Aware (IWA) updates, which consist of infinitely many infinitesimal updates on each loss function. However, IWA updates' empirical success is not completely explained by their theory. In this paper, we show for the first time that IWA updates have a strictly better regret upper bound than plain gradient updates in the online learning setting. Our analysis is based on the new framework, generalized implicit Follow-the-Regularized-Leader (FTRL) (Chen and Orabona, 2023), to analyze generalized implicit updates using a dual formulation. In particular, our results imply that IWA updates can be considered as
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#35299;&#20915;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#22823;&#37327;&#25968;&#25454;&#38656;&#27714;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19987;&#23478;&#30693;&#35782;&#25429;&#25417;&#24182;&#24418;&#24335;&#21270;&#20026;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#20351;&#29992;&#22522;&#20110;&#26679;&#26412;&#30340;&#22312;&#32447;&#35299;&#20915;&#26041;&#27861;&#26469;&#25512;&#21160;&#22522;&#20110;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#22312;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.11954</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26426;&#22120;&#20154;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On-Robot Bayesian Reinforcement Learning for POMDPs. (arXiv:2307.11954v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#35299;&#20915;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#22823;&#37327;&#25968;&#25454;&#38656;&#27714;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19987;&#23478;&#30693;&#35782;&#25429;&#25417;&#24182;&#24418;&#24335;&#21270;&#20026;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#20351;&#29992;&#22522;&#20110;&#26679;&#26412;&#30340;&#22312;&#32447;&#35299;&#20915;&#26041;&#27861;&#26469;&#25512;&#21160;&#22522;&#20110;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#22312;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33719;&#21462;&#25968;&#25454;&#30340;&#25104;&#26412;&#36739;&#39640;&#65292;&#26426;&#22120;&#20154;&#23398;&#20064;&#24448;&#24448;&#22256;&#38590;&#37325;&#37325;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#31639;&#27861;&#21644;&#20805;&#20998;&#21033;&#29992;&#19987;&#23478;&#23545;&#26426;&#22120;&#20154;&#21160;&#24577;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#35299;&#20915;&#22823;&#37327;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#22522;&#20110;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#65288;BRL&#65289;&#30001;&#20110;&#20854;&#26679;&#26412;&#25928;&#29575;&#21644;&#23545;&#20808;&#39564;&#30693;&#35782;&#30340;&#21033;&#29992;&#33021;&#21147;&#65292;&#22312;&#36825;&#19968;&#38382;&#39064;&#19978;&#29420;&#20855;&#20248;&#21183;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#34920;&#36798;&#19987;&#23478;&#30693;&#35782;&#21644;&#35299;&#20915;&#21518;&#32493;&#25512;&#29702;&#38382;&#39064;&#30340;&#22256;&#38590;&#65292;BRL&#30340;&#24212;&#29992;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#29289;&#29702;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#25512;&#21160;&#20102;&#26426;&#22120;&#20154;&#20013;&#30340;BRL&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20197;&#20998;&#35299;&#34920;&#31034;&#30340;&#24418;&#24335;&#25429;&#25417;&#36825;&#20123;&#30693;&#35782;&#65292;&#28982;&#21518;&#23637;&#31034;&#20102;&#21518;&#39564;&#27010;&#29575;&#30340;&#20998;&#35299;&#24418;&#24335;&#65292;&#24182;&#26368;&#32456;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#23558;&#27169;&#22411;&#24418;&#24335;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#31890;&#23376;&#28388;&#27874;&#30340;&#22522;&#20110;&#26679;&#26412;&#30340;&#22312;&#32447;&#35299;&#20915;&#26041;&#27861;&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#20915;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot learning is often difficult due to the expense of gathering data. The need for large amounts of data can, and should, be tackled with effective algorithms and leveraging expert information on robot dynamics. Bayesian reinforcement learning (BRL), thanks to its sample efficiency and ability to exploit prior knowledge, is uniquely positioned as such a solution method. Unfortunately, the application of BRL has been limited due to the difficulties of representing expert knowledge as well as solving the subsequent inference problem. This paper advances BRL for robotics by proposing a specialized framework for physical systems. In particular, we capture this knowledge in a factored representation, then demonstrate the posterior factorizes in a similar shape, and ultimately formalize the model in a Bayesian framework. We then introduce a sample-based online solution method, based on Monte-Carlo tree search and particle filtering, specialized to solve the resulting model. This approach c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#20351;&#29992;&#19968;&#20010;&#26080;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2307.11949</link><description>&lt;p&gt;
HIQL: &#20197;&#28508;&#22312;&#29366;&#24577;&#20316;&#20026;&#21160;&#20316;&#30340;&#31163;&#32447;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HIQL: Offline Goal-Conditioned RL with Latent States as Actions. (arXiv:2307.11949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#20351;&#29992;&#19968;&#20010;&#26080;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26368;&#36817;&#24050;&#25104;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#30707;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#28508;&#22312;&#22320;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#65288;&#26080;&#22870;&#21169;&#65289;&#25968;&#25454;&#65292;&#25552;&#20379;&#31867;&#20284;&#20110;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#26377;&#25928;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#30452;&#25509;&#20174;&#22810;&#26679;&#21270;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20934;&#30830;&#20272;&#35745;&#36828;&#26399;&#30446;&#26631;&#30340;&#20215;&#20540;&#20989;&#25968;&#24456;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#32467;&#26500;&#65292;&#21363;&#36798;&#21040;&#36828;&#26399;&#30446;&#26631;&#38656;&#35201;&#39318;&#20808;&#36890;&#36807;&#36739;&#36817;&#23376;&#30446;&#26631;&#12290;&#36825;&#31181;&#32467;&#26500;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#35780;&#20272;&#37051;&#36817;&#30446;&#26631;&#30340;&#21160;&#20316;&#36136;&#37327;&#36890;&#24120;&#27604;&#26356;&#36828;&#30446;&#26631;&#23481;&#26131;&#12290;&#22522;&#20110;&#36825;&#19968;&#24605;&#24819;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#12290;&#21033;&#29992;&#19968;&#20010;&#27809;&#26377;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20801;&#35768;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#65306;&#19968;&#20010;&#39640;&#23618;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Unsupervised pre-training has recently become the bedrock for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL can potentially provide an analogous self-supervised approach for making use of large quantities of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, because it is hard to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals entails first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies that allow us to exploit this structure: a high-level policy 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#23398;&#20064;&#29575;&#19979;&#32593;&#32476;&#35757;&#32451;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#24182;&#21457;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#30340;&#19981;&#31283;&#23450;&#24615;&#29616;&#35937;&#65292;&#21253;&#25324;&#26223;&#35266;&#21464;&#24179;&#21644;&#26223;&#35266;&#21464;&#31227;&#12290;</title><link>http://arxiv.org/abs/2307.11948</link><description>&lt;p&gt;
&#22823;&#23398;&#20064;&#29575;&#35757;&#32451;&#30340;&#19981;&#31283;&#23450;&#24615;: &#19968;&#20010;&#25439;&#22833;&#26223;&#35266;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
The instabilities of large learning rate training: a loss landscape view. (arXiv:2307.11948v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#23398;&#20064;&#29575;&#19979;&#32593;&#32476;&#35757;&#32451;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#24182;&#21457;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#30340;&#19981;&#31283;&#23450;&#24615;&#29616;&#35937;&#65292;&#21253;&#25324;&#26223;&#35266;&#21464;&#24179;&#21644;&#26223;&#35266;&#21464;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#26080;&#30097;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#35768;&#22810;&#30740;&#31350;&#25506;&#35752;&#20102;&#25439;&#22833;&#26223;&#35266;&#30340;&#26354;&#29575;&#22914;&#20309;&#24433;&#21709;&#35299;&#30340;&#36136;&#37327;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#32771;&#34385;Hessian&#30697;&#38453;&#26469;&#30740;&#31350;&#25439;&#22833;&#26223;&#35266;&#65292;&#22312;&#20351;&#29992;&#22823;&#23398;&#20064;&#29575;&#36827;&#34892;&#32593;&#32476;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#65288;&#38750;&#65289;&#33879;&#21517;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#23545;&#26799;&#24230;&#19979;&#38477;&#30340;&#19981;&#31283;&#23450;&#24615;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#25439;&#22833;&#26223;&#35266;&#30340;&#21464;&#24179;&#21644;&#21464;&#31227;&#30340;&#24341;&#20154;&#27880;&#30446;&#29616;&#35937;&#65292;&#36825;&#20004;&#32773;&#37117;&#19982;&#35757;&#32451;&#30340;&#19981;&#31283;&#23450;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern neural networks are undeniably successful. Numerous works study how the curvature of loss landscapes can affect the quality of solutions. In this work we study the loss landscape by considering the Hessian matrix during network training with large learning rates - an attractive regime that is (in)famously unstable. We characterise the instabilities of gradient descent, and we observe the striking phenomena of \textit{landscape flattening} and \textit{landscape shift}, both of which are intimately connected to the instabilities of training.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21327;&#21516;&#23398;&#20064;&#32447;&#24615;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#12289;&#21322;&#30417;&#30563;&#30340;&#31639;&#27861;Collab&#65292;&#35813;&#31639;&#27861;&#22312;&#26080;&#27861;&#35775;&#38382;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#36890;&#20449;&#25928;&#29575;&#21644;&#23454;&#29992;&#24615;&#65292;&#21516;&#26102;&#22312;&#28176;&#36817;&#23616;&#37096;&#26497;&#23567;&#26497;&#20540;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11947</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#21270;&#32570;&#22833;&#25968;&#25454;&#21327;&#21516;&#23398;&#20064;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Collaboratively Learning Linear Models with Structured Missing Data. (arXiv:2307.11947v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21327;&#21516;&#23398;&#20064;&#32447;&#24615;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#12289;&#21322;&#30417;&#30563;&#30340;&#31639;&#27861;Collab&#65292;&#35813;&#31639;&#27861;&#22312;&#26080;&#27861;&#35775;&#38382;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#36890;&#20449;&#25928;&#29575;&#21644;&#23454;&#29992;&#24615;&#65292;&#21516;&#26102;&#22312;&#28176;&#36817;&#23616;&#37096;&#26497;&#23567;&#26497;&#20540;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;$m$&#20010;&#20195;&#29702;&#21327;&#21516;&#23398;&#20064;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#27599;&#20010;&#20195;&#29702;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#29305;&#24449;&#23376;&#38598;&#65292;&#20363;&#22914;&#20174;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#22914;&#20309;&#21327;&#35843;&#20195;&#29702;&#20197;&#20135;&#29983;&#26368;&#20339;&#30340;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#12289;&#21322;&#30417;&#30563;&#30340;&#31639;&#27861;Collab&#65292;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;&#26412;&#22320;&#35757;&#32451;&#12289;&#32858;&#21512;&#21644;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#36890;&#20449;&#26631;&#35760;&#25968;&#25454;&#65292;&#20351;&#20854;&#22312;&#26080;&#27861;&#35775;&#38382;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#36890;&#20449;&#25928;&#29575;&#21644;&#23454;&#29992;&#24615;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#20010;&#38556;&#30861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20960;&#20046;&#26159;&#28176;&#36817;&#23616;&#37096;&#26497;&#23567;&#26497;&#20540;$\unicode{x2013}$&#21363;&#20351;&#22312;&#20801;&#35768;&#36890;&#20449;&#26631;&#35760;&#25968;&#25454;&#30340;&#20272;&#35745;&#22120;&#20013;&#65292;&#22914;&#25554;&#34917;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of collaboratively learning least squares estimates for $m$ agents. Each agent observes a different subset of the features$\unicode{x2013}$e.g., containing data collected from sensors of varying resolution. Our goal is to determine how to coordinate the agents in order to produce the best estimator for each agent. We propose a distributed, semi-supervised algorithm Collab, consisting of three steps: local training, aggregation, and distribution. Our procedure does not require communicating the labeled data, making it communication efficient and useful in settings where the labeled data is inaccessible. Despite this handicap, our procedure is nearly asymptotically local minimax optimal$\unicode{x2013}$even among estimators allowed to communicate the labeled data such as imputation methods. We test our method on real and synthetic data.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#25209;&#27425;&#21098;&#35009;&#21644;&#33258;&#36866;&#24212;&#36880;&#23618;&#21098;&#35009;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#25209;&#27425;&#21098;&#35009;&#65292;&#21487;&#20197;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#20351;&#29992;&#25209;&#27425;&#27491;&#21017;&#21270;&#23618;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36880;&#23618;&#21098;&#35009;&#65292;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#23618;&#30340;&#25935;&#24863;&#24615;&#35843;&#25972;&#21098;&#35009;&#24120;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.11939</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#25209;&#27425;&#21098;&#35009;&#21644;&#33258;&#36866;&#24212;&#36880;&#23618;&#21098;&#35009;
&lt;/p&gt;
&lt;p&gt;
Batch Clipping and Adaptive Layerwise Clipping for Differential Private Stochastic Gradient Descent. (arXiv:2307.11939v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11939
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#25209;&#27425;&#21098;&#35009;&#21644;&#33258;&#36866;&#24212;&#36880;&#23618;&#21098;&#35009;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#25209;&#27425;&#21098;&#35009;&#65292;&#21487;&#20197;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#20351;&#29992;&#25209;&#27425;&#27491;&#21017;&#21270;&#23618;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36880;&#23618;&#21098;&#35009;&#65292;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#23618;&#30340;&#25935;&#24863;&#24615;&#35843;&#25972;&#21098;&#35009;&#24120;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DPSGD&#65289;&#20013;&#65292;&#27599;&#19968;&#36718;&#20256;&#36755;&#30340;&#21098;&#35009;&#26799;&#24230;&#20043;&#21644;&#34987;&#39640;&#26031;&#22122;&#22768;&#28151;&#28102;&#65292;&#29992;&#20110;&#26356;&#26032;&#20840;&#23616;&#27169;&#22411;&#65292;&#24120;&#34920;&#31034;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#30001;&#20110;&#21098;&#35009;&#26799;&#24230;&#26159;&#20998;&#21035;&#35745;&#31639;&#30340;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20010;&#20307;&#21098;&#35009;&#65288;IC&#65289;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22914;resnet-18 &#26080;&#27861;&#20351;&#29992;&#25209;&#27425;&#27491;&#21017;&#21270;&#23618;&#65288;BNL&#65289;&#65292;&#36825;&#26159;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#20026;&#20102;&#21033;&#29992;BNL&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25209;&#27425;&#21098;&#35009;&#65288;BC&#65289;&#65292;&#22312;&#21407;&#22987;&#30340;DPSGD&#20013;&#65292;&#25105;&#20204;&#23558;&#21333;&#20010;&#26799;&#24230;&#36827;&#34892;&#24179;&#22343;&#21644;&#21098;&#35009;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#23618;&#30340;&#27169;&#22411;&#26465;&#30446;&#23545;&#28155;&#21152;&#30340;&#39640;&#26031;&#22122;&#22768;&#20855;&#26377;&#19981;&#21516;&#30340;&#25935;&#24863;&#24615;&#12290;&#22240;&#27492;&#65292;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#36880;&#23618;&#21098;&#35009;&#26041;&#27861;&#65288;ALC&#65289;&#65292;&#20854;&#20013;&#27599;&#19968;&#23618;&#37117;&#26377;&#33258;&#24049;&#36866;&#24212;&#35843;&#25972;&#30340;&#21098;&#35009;&#24120;&#25968;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#27809;&#26377;&#20005;&#23494;&#30340;&#24046;&#20998;&#38544;&#31169;&#35777;&#26126;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Each round in Differential Private Stochastic Gradient Descent (DPSGD) transmits a sum of clipped gradients obfuscated with Gaussian noise to a central server which uses this to update a global model which often represents a deep neural network. Since the clipped gradients are computed separately, which we call Individual Clipping (IC), deep neural networks like resnet-18 cannot use Batch Normalization Layers (BNL) which is a crucial component in deep neural networks for achieving a high accuracy. To utilize BNL, we introduce Batch Clipping (BC) where, instead of clipping single gradients as in the orginal DPSGD, we average and clip batches of gradients. Moreover, the model entries of different layers have different sensitivities to the added Gaussian noise. Therefore, Adaptive Layerwise Clipping methods (ALC), where each layer has its own adaptively finetuned clipping constant, have been introduced and studied, but so far without rigorous DP proofs. In this paper, we propose {\em a ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#23725;&#20989;&#25968;&#30340;&#35282;&#24230;&#20171;&#32461;&#20102;Mercer&#22823;&#35268;&#27169;&#26680;&#26426;&#22120;&#65292;&#36890;&#36807;&#30740;&#31350;&#21738;&#20123;&#26680;&#20989;&#25968;&#21487;&#20197;&#34987;&#20313;&#24358;&#20989;&#25968;&#20056;&#31215;&#36924;&#36817;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#26680;&#26426;&#22120;&#20013;&#30340;&#38590;&#39064;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#28145;&#24230;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22270;&#20687;&#22788;&#29702;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.11925</link><description>&lt;p&gt;
&#22522;&#20110;&#23725;&#20989;&#25968;&#35282;&#24230;&#30340;Mercer&#22823;&#35268;&#27169;&#26680;&#26426;&#22120;
&lt;/p&gt;
&lt;p&gt;
Mercer Large-Scale Kernel Machines from Ridge Function Perspective. (arXiv:2307.11925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#23725;&#20989;&#25968;&#30340;&#35282;&#24230;&#20171;&#32461;&#20102;Mercer&#22823;&#35268;&#27169;&#26680;&#26426;&#22120;&#65292;&#36890;&#36807;&#30740;&#31350;&#21738;&#20123;&#26680;&#20989;&#25968;&#21487;&#20197;&#34987;&#20313;&#24358;&#20989;&#25968;&#20056;&#31215;&#36924;&#36817;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#26680;&#26426;&#22120;&#20013;&#30340;&#38590;&#39064;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#28145;&#24230;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22270;&#20687;&#22788;&#29702;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20174;&#23725;&#20989;&#25968;&#30340;&#35282;&#24230;&#20171;&#32461;Mercer&#22823;&#35268;&#27169;&#26680;&#26426;&#22120;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;Lin&#21644;Pinkus&#22312;&#23725;&#20989;&#25968;&#30340;&#22522;&#26412;&#24615;&#19978;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;Rachimi&#21644;Recht&#22312;&#36817;&#20284;&#29702;&#35770;&#20013;&#30340;&#26368;&#36817;&#19968;&#31687;&#35770;&#25991;&#30340;&#20027;&#35201;&#23450;&#29702;&#65292;&#21363;&#22823;&#35268;&#27169;&#26680;&#26426;&#22120;&#30340;&#38543;&#26426;&#29305;&#24449;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21738;&#20123;&#26680;&#20989;&#25968;&#21487;&#20197;&#34987;$x$&#21644;$y$&#30340;&#20313;&#24358;&#20989;&#25968;&#20056;&#31215;&#30340;&#21644;&#36924;&#36817;&#65292;&#24182;&#25552;&#20986;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#38556;&#30861;&#12290;&#26412;&#25991;&#30340;&#32467;&#26524;&#21487;&#33021;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#19982;&#22270;&#20687;&#22788;&#29702;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
To present Mercer large-scale kernel machines from a ridge function perspective, we recall the results by Lin and Pinkus from Fundamentality of ridge functions. We consider the main theorem of the recent paper by Rachimi and Recht, 2008, Random features for large-scale kernel machines in terms of the Approximation Theory. We study which kernels can be approximated by a sum of cosine function products with arguments depending on $x$ and $y$ and present the obstacles of such an approach. The results of this article may have various applications in Deep Learning, especially in problems related to Image Processing.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BLINDER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#26465;&#20214;&#19979;&#29366;&#24577;&#25551;&#36848;&#30340;&#20540;&#20989;&#25968;&#65292;&#33258;&#21160;&#36873;&#25321;&#31616;&#26126;&#30340;&#29366;&#24577;&#25551;&#36848;&#65292;&#20197;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#28436;&#21592;&#22312;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.11922</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#24863;&#30693;&#65306;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20026;&#35821;&#35328;&#27169;&#22411;&#28436;&#21592;&#20248;&#21270;&#29366;&#24577;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors. (arXiv:2307.11922v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BLINDER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#26465;&#20214;&#19979;&#29366;&#24577;&#25551;&#36848;&#30340;&#20540;&#20989;&#25968;&#65292;&#33258;&#21160;&#36873;&#25321;&#31616;&#26126;&#30340;&#29366;&#24577;&#25551;&#36848;&#65292;&#20197;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#28436;&#21592;&#22312;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#34987;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#21644;&#28216;&#25103;&#31561;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#30340;&#28436;&#21592;&#20013;&#65292;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#24456;&#23569;&#25506;&#32034;&#36890;&#36807;&#35821;&#35328;&#21521;LLM&#28436;&#21592;&#25552;&#20379;&#20160;&#20040;&#29615;&#22659;&#29366;&#24577;&#20449;&#24687;&#12290;&#35814;&#23613;&#25551;&#36848;&#39640;&#32500;&#29366;&#24577;&#21487;&#33021;&#20250;&#24433;&#21709;&#24615;&#33021;&#24182;&#22686;&#21152;LLM&#28436;&#21592;&#30340;&#25512;&#29702;&#25104;&#26412;&#12290;&#20197;&#21069;&#30340;LLM&#28436;&#21592;&#36890;&#36807;&#20381;&#36182;&#25163;&#24037;&#35774;&#35745;&#30340;&#20219;&#21153;&#29305;&#23450;&#21327;&#35758;&#26469;&#30830;&#23450;&#35813;&#29366;&#24577;&#30340;&#21738;&#20123;&#29305;&#24449;&#38656;&#35201;&#36827;&#34892;&#20256;&#36882;&#65292;&#21738;&#20123;&#19981;&#38656;&#35201;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BLINDER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#26465;&#20214;&#19979;&#29366;&#24577;&#25551;&#36848;&#30340;&#20540;&#20989;&#25968;&#65292;&#33258;&#21160;&#36873;&#25321;&#31616;&#26126;&#30340;&#29366;&#24577;&#25551;&#36848;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#39057;&#28216;&#25103;NetHack&#21644;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;BLINDER&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#20943;&#23569;&#20102;&#36755;&#20837;&#22823;&#23567;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are being applied as actors for sequential decision making tasks in domains such as robotics and games, utilizing their general world knowledge and planning abilities. However, previous work does little to explore what environment state information is provided to LLM actors via language. Exhaustively describing high-dimensional states can impair performance and raise inference costs for LLM actors. Previous LLM actors avoid the issue by relying on hand-engineered, task-specific protocols to determine which features to communicate about a state and which to leave out. In this work, we propose Brief Language INputs for DEcision-making Responses (BLINDER), a method for automatically selecting concise state descriptions by learning a value function for task-conditioned state descriptions. We evaluate BLINDER on the challenging video game NetHack and a robotic manipulation task. Our method improves task success rate, reduces input size and compute costs, and gen
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22810;&#27169;&#24335;&#35843;&#26597;&#21644;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#36139;&#22256;&#29575;&#12290;&#36890;&#36807;&#20351;&#29992;&#20174;&#21355;&#26143;&#22270;&#20687;&#20013;&#25552;&#21462;&#30340;&#21487;&#35270;&#29305;&#24449;&#21644;&#35843;&#26597;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#36139;&#22256;&#29575;&#26102;&#34920;&#29616;&#20986;&#26356;&#20302;&#30340;&#24179;&#22343;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2307.11921</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#24335;&#35843;&#26597;&#21644;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#39044;&#27979;&#36139;&#22256;&#29575;
&lt;/p&gt;
&lt;p&gt;
Poverty rate prediction using multi-modal survey and earth observation data. (arXiv:2307.11921v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11921
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22810;&#27169;&#24335;&#35843;&#26597;&#21644;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#36139;&#22256;&#29575;&#12290;&#36890;&#36807;&#20351;&#29992;&#20174;&#21355;&#26143;&#22270;&#20687;&#20013;&#25552;&#21462;&#30340;&#21487;&#35270;&#29305;&#24449;&#21644;&#35843;&#26597;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#36139;&#22256;&#29575;&#26102;&#34920;&#29616;&#20986;&#26356;&#20302;&#30340;&#24179;&#22343;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23478;&#24237;&#20154;&#21475;&#32479;&#35745;&#21644;&#29983;&#27963;&#27700;&#24179;&#35843;&#26597;&#38382;&#39064;&#19982;&#20174;&#21355;&#26143;&#22270;&#20687;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#39044;&#27979;&#19968;&#20010;&#22320;&#21306;&#30340;&#36139;&#22256;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20174;&#21333;&#19968;&#27493;&#39588;&#25552;&#21462;&#29305;&#24449;&#30340;&#26041;&#27861;&#33719;&#21462;&#30340;&#21487;&#35270;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#20844;&#24320;&#21487;&#29992;&#30340;10&#31859;/&#20687;&#32032;Sentinel-2&#22320;&#34920;&#21453;&#23556;&#21355;&#26143;&#22270;&#20687;&#12290;&#36825;&#20123;&#21487;&#35270;&#29305;&#24449;&#19982;&#21313;&#20010;&#35843;&#26597;&#38382;&#39064;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#36890;&#36807;&#20195;&#29702;&#25163;&#27573;&#27979;&#35797;&#65288;PMT&#65289;&#20272;&#35745;&#19968;&#20010;&#23478;&#24237;&#26159;&#21542;&#20302;&#20110;&#36139;&#22256;&#32447;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21253;&#21547;&#21487;&#35270;&#29305;&#24449;&#21487;&#20197;&#23558;&#36139;&#22256;&#29575;&#20272;&#35745;&#30340;&#24179;&#22343;&#35823;&#24046;&#20174;4.09&#65285;&#38477;&#20302;&#21040;3.88&#65285;&#12290;&#38500;&#20102;&#22312;&#20195;&#29702;&#25163;&#27573;&#27979;&#35797;&#20013;&#21253;&#21547;&#21355;&#26143;&#22270;&#20687;&#29305;&#24449;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#19982;&#20174;&#21355;&#26143;&#22270;&#20687;&#20013;&#25552;&#21462;&#30340;&#21487;&#35270;&#29305;&#24449;&#20114;&#34917;&#30340;&#35843;&#26597;&#38382;&#39064;&#23376;&#38598;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#30001;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#22270;&#20687;&#25351;&#23548;&#30340;&#35843;&#26597;&#21464;&#37327;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents an approach for combining household demographic and living standards survey questions with features derived from satellite imagery to predict the poverty rate of a region. Our approach utilizes visual features obtained from a single-step featurization method applied to freely available 10m/px Sentinel-2 surface reflectance satellite imagery. These visual features are combined with ten survey questions in a proxy means test (PMT) to estimate whether a household is below the poverty line. We show that the inclusion of visual features reduces the mean error in poverty rate estimates from 4.09% to 3.88% over a nationally representative out-of-sample test set. In addition to including satellite imagery features in proxy means tests, we propose an approach for selecting a subset of survey questions that are complementary to the visual features extracted from satellite imagery. Specifically, we design a survey variable selection approach guided by the full survey and image 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26597;&#35810;&#26377;&#25928;&#30340;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#28431;&#27934;&#12290;&#36825;&#31181;&#25915;&#20987;&#26041;&#27861;&#26159;&#22522;&#20110;&#24494;&#29983;&#29289;&#36951;&#20256;&#31639;&#27861;&#65292;&#26080;&#38656;&#20808;&#39564;&#30693;&#35782;&#65292;&#32467;&#21512;&#20102;&#36716;&#31227;&#21644;&#35780;&#20998;&#26041;&#27861;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#39640;&#19988;&#38590;&#20197;&#34987;&#26816;&#27979;&#21040;&#12290;</title><link>http://arxiv.org/abs/2307.11906</link><description>&lt;p&gt;
&#36890;&#36807;&#26597;&#35810;&#26377;&#25928;&#30340;&#40657;&#30418;&#25915;&#20987;&#25581;&#31034;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Unveiling Vulnerabilities in Interpretable Deep Learning Systems with Query-Efficient Black-box Attacks. (arXiv:2307.11906v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11906
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26597;&#35810;&#26377;&#25928;&#30340;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#28431;&#27934;&#12290;&#36825;&#31181;&#25915;&#20987;&#26041;&#27861;&#26159;&#22522;&#20110;&#24494;&#29983;&#29289;&#36951;&#20256;&#31639;&#27861;&#65292;&#26080;&#38656;&#20808;&#39564;&#30693;&#35782;&#65292;&#32467;&#21512;&#20102;&#36716;&#31227;&#21644;&#35780;&#20998;&#26041;&#27861;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#39640;&#19988;&#38590;&#20197;&#34987;&#26816;&#27979;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#36805;&#36895;&#24212;&#29992;&#20110;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#65292;&#38761;&#26032;&#20102;&#35768;&#22810;&#34892;&#19994;&#65292;&#20294;&#24050;&#30693;&#20854;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#36825;&#31181;&#25915;&#20987;&#23545;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31995;&#32479;&#26500;&#25104;&#20102;&#20005;&#37325;&#23041;&#32961;&#65292;&#25439;&#23475;&#20102;&#20854;&#30340;&#23436;&#25972;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#20449;&#20219;&#24615;&#12290;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65288;IDLSes&#65289;&#26088;&#22312;&#20351;&#31995;&#32479;&#26356;&#21152;&#36879;&#26126;&#21644;&#21487;&#35299;&#37322;&#65292;&#20294;&#21516;&#26102;&#20063;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#29983;&#29289;&#36951;&#20256;&#31639;&#27861;&#30340;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#25915;&#20987;IDLSes&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#30446;&#26631;&#27169;&#22411;&#21450;&#20854;&#35299;&#37322;&#27169;&#22411;&#26377;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#12290;&#25152;&#25552;&#20986;&#30340;&#25915;&#20987;&#26159;&#19968;&#31181;&#26597;&#35810;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#36716;&#31227;&#21644;&#22522;&#20110;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#25104;&#20026;&#25581;&#31034;IDLS&#28431;&#27934;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#23454;&#39564;&#23637;&#31034;&#20102;&#20351;&#29992;&#20855;&#26377;&#19982;&#33391;&#24615;&#26679;&#26412;&#38750;&#24120;&#30456;&#20284;&#30340;&#23646;&#24615;&#22270;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#36825;&#20351;&#24471;&#21363;&#20351;&#26159;&#20154;&#24037;&#20998;&#26512;&#20154;&#21592;&#20063;&#24456;&#38590;&#26816;&#27979;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has been rapidly employed in many applications revolutionizing many industries, but it is known to be vulnerable to adversarial attacks. Such attacks pose a serious threat to deep learning-based systems compromising their integrity, reliability, and trust. Interpretable Deep Learning Systems (IDLSes) are designed to make the system more transparent and explainable, but they are also shown to be susceptible to attacks. In this work, we propose a novel microbial genetic algorithm-based black-box attack against IDLSes that requires no prior knowledge of the target model and its interpretation model. The proposed attack is a query-efficient approach that combines transfer-based and score-based methods, making it a powerful tool to unveil IDLS vulnerabilities. Our experiments of the attack show high attack success rates using adversarial examples with attribution maps that are highly similar to those of benign samples which makes it difficult to detect even by human analysts. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#38024;&#23545;YOLOv5&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#32593;&#32476;&#35009;&#21098;&#21644;&#37327;&#21270;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#21644;&#25512;&#29702;&#26102;&#38388;&#26041;&#38754;&#21462;&#24471;&#20102;&#31215;&#26497;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11904</link><description>&lt;p&gt;
YOLOv5&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Model Compression Methods for YOLOv5: A Review. (arXiv:2307.11904v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#38024;&#23545;YOLOv5&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#32593;&#32476;&#35009;&#21098;&#21644;&#37327;&#21270;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#21644;&#25512;&#29702;&#26102;&#38388;&#26041;&#38754;&#21462;&#24471;&#20102;&#31215;&#26497;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20154;&#20204;&#23545;&#25552;&#21319;YOLO&#29289;&#20307;&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#12290;&#33258;&#24341;&#20837;&#20197;&#26469;&#65292;&#24050;&#32463;&#25512;&#20986;&#20102;&#20843;&#20010;&#20027;&#35201;&#29256;&#26412;&#30340;YOLO&#65292;&#26088;&#22312;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#23613;&#31649;YOLO&#30340;&#26126;&#26174;&#20248;&#28857;&#20351;&#20854;&#22312;&#35768;&#22810;&#39046;&#22495;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23558;&#20854;&#37096;&#32626;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#26041;&#27861;&#65292;&#20998;&#20026;&#32593;&#32476;&#35009;&#21098;&#12289;&#37327;&#21270;&#21644;&#30693;&#35782;&#33976;&#39311;&#19977;&#31867;&#12290;&#21033;&#29992;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#21462;&#24471;&#30340;&#25104;&#26524;&#65292;&#22914;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#20351;&#20854;&#22312;&#30828;&#20214;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#21464;&#24471;&#21487;&#34892;&#29978;&#33267;&#24517;&#35201;&#12290;&#22312;&#26412;&#32508;&#36848;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#32593;&#32476;&#35009;&#21098;&#21644;&#37327;&#21270;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#30456;&#23545;&#36739;&#22909;&#30340;&#27169;&#22359;&#24615;&#12290;&#25105;&#20204;&#23558;&#23427;&#20204;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20998;&#26512;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;YOLOv5&#30340;&#23454;&#38469;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, extensive research has been devoted to enhancing YOLO object detectors. Since its introduction, eight major versions of YOLO have been introduced with the purpose of improving its accuracy and efficiency. While the evident merits of YOLO have yielded to its extensive use in many areas, deploying it on resource-limited devices poses challenges. To address this issue, various neural network compression methods have been developed, which fall under three main categories, namely network pruning, quantization, and knowledge distillation. The fruitful outcomes of utilizing model compression methods, such as lowering memory usage and inference time, make them favorable, if not necessary, for deploying large neural networks on hardware-constrained edge devices. In this review paper, our focus is on pruning and quantization due to their comparative modularity. We categorize them and analyze the practical results of applying those methods to YOLOv5. By doing so, we ident
&lt;/p&gt;</description></item><item><title>&#39033;&#30446;&#20315;&#32599;&#37324;&#36798;&#26159;&#19968;&#20010;&#31616;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#31995;&#32479;&#26550;&#26500;&#21644;SDK&#65292;&#33021;&#22815;&#22312;&#24322;&#26500;&#35774;&#22791;&#29983;&#24577;&#31995;&#32479;&#20013;&#37096;&#32626;&#22823;&#35268;&#27169;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#36890;&#36807;&#20445;&#25252;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#65292;&#23558;&#27169;&#22411;&#35757;&#32451;&#20998;&#25955;&#22312;&#35774;&#22791;&#21644;&#20113;&#23384;&#20648;&#20043;&#38388;&#65292;&#21516;&#26102;&#23454;&#29616;&#27169;&#22411;&#26356;&#26032;&#30340;&#38598;&#20013;&#32858;&#21512;&#65292;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#35268;&#27169;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11899</link><description>&lt;p&gt;
&#39033;&#30446;&#20315;&#32599;&#37324;&#36798;: &#31616;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Project Florida: Federated Learning Made Easy. (arXiv:2307.11899v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11899
&lt;/p&gt;
&lt;p&gt;
&#39033;&#30446;&#20315;&#32599;&#37324;&#36798;&#26159;&#19968;&#20010;&#31616;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#31995;&#32479;&#26550;&#26500;&#21644;SDK&#65292;&#33021;&#22815;&#22312;&#24322;&#26500;&#35774;&#22791;&#29983;&#24577;&#31995;&#32479;&#20013;&#37096;&#32626;&#22823;&#35268;&#27169;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#36890;&#36807;&#20445;&#25252;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#65292;&#23558;&#27169;&#22411;&#35757;&#32451;&#20998;&#25955;&#22312;&#35774;&#22791;&#21644;&#20113;&#23384;&#20648;&#20043;&#38388;&#65292;&#21516;&#26102;&#23454;&#29616;&#27169;&#22411;&#26356;&#26032;&#30340;&#38598;&#20013;&#32858;&#21512;&#65292;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#35268;&#27169;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#39033;&#30446;&#20315;&#32599;&#37324;&#36798;&#65292;&#36825;&#26159;&#19968;&#20010;&#31995;&#32479;&#26550;&#26500;&#21644;&#36719;&#20214;&#24320;&#21457;&#24037;&#20855;&#21253;&#65288;SDK&#65289;&#65292;&#33021;&#22815;&#22312;&#24322;&#26500;&#35774;&#22791;&#29983;&#24577;&#31995;&#32479;&#20013;&#37096;&#32626;&#22823;&#35268;&#27169;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#35299;&#20915;&#26041;&#26696;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#20027;&#26435;&#21407;&#21017;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#23558;&#25968;&#25454;&#23384;&#20648;&#22312;&#20854;&#21407;&#22987;&#20301;&#32622;&#65288;&#26080;&#35770;&#26159;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#36824;&#26159;&#22312;&#20998;&#31163;&#30340;&#20113;&#23384;&#20648;&#20998;&#21306;&#20013;&#65289;&#26469;&#23454;&#29616;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#12290;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#23558;&#27169;&#22411;&#24555;&#29031;&#20998;&#21457;&#32473;&#22312;&#23433;&#20840;&#36793;&#30028;&#20869;&#36816;&#34892;&#30340;&#23458;&#25143;&#31471;&#65292;&#24182;&#36816;&#34892;&#23458;&#25143;&#31471;&#20195;&#30721;&#26356;&#26032;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#20013;&#22830;&#21327;&#35843;&#32773;&#20013;&#32858;&#21512;&#26356;&#26032;&#30340;&#24555;&#29031;&#65292;&#20174;&#32780;&#22312;&#35774;&#22791;&#21644;&#20998;&#21306;&#20043;&#38388;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#32780;&#35757;&#32451;&#25968;&#25454;&#20173;&#20445;&#30041;&#22312;&#20854;&#23433;&#20840;&#36793;&#30028;&#20869;&#12290;&#37096;&#32626;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#23454;&#29616;&#22797;&#26434;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#26426;&#21046;&#20197;&#21450;&#21487;&#25193;&#23637;&#30340;&#21327;&#35843;&#22522;&#30784;&#35774;&#26045;&#12290;&#35268;&#27169;&#21644;&#24615;&#33021;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#22240;&#20026;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#30340;&#25928;&#30410;&#21463;&#21040;&#36825;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Project Florida, a system architecture and software development kit (SDK) enabling deployment of large-scale Federated Learning (FL) solutions across a heterogeneous device ecosystem. Federated learning is an approach to machine learning based on a strong data sovereignty principle, i.e., that privacy and security of data is best enabled by storing it at its origin, whether on end-user devices or in segregated cloud storage silos. Federated learning enables model training across devices and silos while the training data remains within its security boundary, by distributing a model snapshot to a client running inside the boundary, running client code to update the model, and then aggregating updated snapshots across many clients in a central orchestrator. Deploying a FL solution requires implementation of complex privacy and security mechanisms as well as scalable orchestration infrastructure. Scale and performance is a paramount concern, as the model training process benefit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Hindsight-DICE&#31639;&#27861;&#65292;&#21033;&#29992;&#37325;&#35201;&#25277;&#26679;&#27604;&#29575;&#20272;&#35745;&#25216;&#26415;&#25913;&#21892;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.11897</link><description>&lt;p&gt;
Hindsight-DICE&#65306;&#31283;&#23450;&#20449;&#29992;&#20998;&#37197;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hindsight-DICE: Stable Credit Assignment for Deep Reinforcement Learning. (arXiv:2307.11897v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Hindsight-DICE&#31639;&#27861;&#65292;&#21033;&#29992;&#37325;&#35201;&#25277;&#26679;&#27604;&#29575;&#20272;&#35745;&#25216;&#26415;&#25913;&#21892;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#29615;&#22659;&#24448;&#24448;&#25552;&#20379;&#24456;&#23569;&#30340;&#35780;&#20272;&#21453;&#39304;&#26469;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#65292;&#34892;&#20026;&#30340;&#38271;&#26102;&#38388;&#36712;&#36857;&#20165;&#20197;&#19968;&#20010;&#32456;&#27490;&#20449;&#21495;&#26631;&#35760;&#65292;&#23548;&#33268;&#35266;&#23519;&#21040;&#38750;&#24179;&#20961;&#22870;&#21169;&#21644;&#35302;&#21457;&#27492;&#31867;&#21453;&#39304;&#30340;&#20010;&#20307;&#27493;&#39588;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#26102;&#38388;&#24310;&#36831;&#12290;&#35299;&#20915;&#36825;&#31181;&#20449;&#29992;&#20998;&#37197;&#25361;&#25112;&#26159;&#24378;&#21270;&#23398;&#20064;&#30340;&#37325;&#35201;&#29305;&#24449;&#20043;&#19968;&#65292;&#26412;&#30740;&#31350;&#21033;&#29992;&#29616;&#26377;&#30340;&#37325;&#35201;&#25277;&#26679;&#27604;&#29575;&#20272;&#35745;&#25216;&#26415;&#26469;&#26174;&#33879;&#25913;&#21892;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20013;&#30340;&#20449;&#29992;&#20998;&#37197;&#22788;&#29702;&#12290;&#34429;&#28982;&#20351;&#29992;&#25152;&#35859;&#30340;&#20107;&#21518;&#31574;&#30053;&#20026;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#36820;&#22238;&#36820;&#22238;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#26426;&#21046;&#65292;&#20294;&#26159;&#31616;&#21333;&#22320;&#24212;&#29992;&#37325;&#35201;&#25277;&#26679;&#20250;&#23548;&#33268;&#19981;&#31283;&#23450;&#25110;&#36807;&#24230;&#28382;&#21518;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Oftentimes, environments for sequential decision-making problems can be quite sparse in the provision of evaluative feedback to guide reinforcement-learning agents. In the extreme case, long trajectories of behavior are merely punctuated with a single terminal feedback signal, engendering a significant temporal delay between the observation of non-trivial reward and the individual steps of behavior culpable for eliciting such feedback. Coping with such a credit assignment challenge is one of the hallmark characteristics of reinforcement learning and, in this work, we capitalize on existing importance-sampling ratio estimation techniques for off-policy evaluation to drastically improve the handling of credit assignment with policy-gradient methods. While the use of so-called hindsight policies offers a principled mechanism for reweighting on-policy data by saliency to the observed trajectory return, naively applying importance sampling results in unstable or excessively lagged learning.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#23545;&#24694;&#24847;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#20351;&#29992;&#38543;&#26426;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#31934;&#24230;&#19978;&#21482;&#25439;&#22833;$\Theta(\alpha)$&#21644;$O(\sqrt{\alpha})$&#65292;&#23545;&#24212;&#19981;&#21516;&#30340;&#20844;&#27491;&#32422;&#26463;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.11892</link><description>&lt;p&gt;
&#20851;&#20110;&#21463;&#24694;&#24847;&#22122;&#22768;&#24433;&#21709;&#30340;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Vulnerability of Fairness Constrained Learning to Malicious Noise. (arXiv:2307.11892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11892
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#23545;&#24694;&#24847;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#20351;&#29992;&#38543;&#26426;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#31934;&#24230;&#19978;&#21482;&#25439;&#22833;$\Theta(\alpha)$&#21644;$O(\sqrt{\alpha})$&#65292;&#23545;&#24212;&#19981;&#21516;&#30340;&#20844;&#27491;&#32422;&#26463;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#23545;&#35757;&#32451;&#25968;&#25454;&#20013;&#24494;&#23567;&#24694;&#24847;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#12290;Konstantinov&#21644;Lampert (2021)&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#36127;&#38754;&#32467;&#26524;&#65292;&#34920;&#26126;&#22312;&#19981;&#24179;&#34913;&#30340;&#32676;&#32452;&#22823;&#23567;&#19979;&#23384;&#22312;&#19968;&#20123;&#25968;&#25454;&#20998;&#24067;&#65292;&#20219;&#20309;&#36866;&#24403;&#30340;&#23398;&#20064;&#22120;&#37117;&#20250;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#33030;&#24369;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26356;&#20048;&#35266;&#30340;&#35266;&#28857;&#65292;&#22914;&#26524;&#20801;&#35768;&#38543;&#26426;&#20998;&#31867;&#22120;&#65292;&#21017;&#24773;&#20917;&#26356;&#21152;&#32454;&#33268;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#24615;&#65292;&#25105;&#20204;&#26174;&#31034;&#21482;&#20250;&#20135;&#29983;$\Theta(\alpha)$&#30340;&#31934;&#24230;&#25439;&#22833;&#65292;&#20854;&#20013;$\alpha$&#26159;&#24694;&#24847;&#22122;&#22768;&#29575;&#65292;&#29978;&#33267;&#21487;&#20197;&#19982;&#27809;&#26377;&#20844;&#27491;&#32422;&#26463;&#30340;&#24773;&#20917;&#23436;&#20840;&#21305;&#37197;&#12290;&#23545;&#20110;&#26426;&#20250;&#22343;&#31561;&#24615;&#65292;&#25105;&#20204;&#26174;&#31034;&#21482;&#20250;&#20135;&#29983;$O(\sqrt{\alpha})$&#30340;&#25439;&#22833;&#65292;&#24182;&#32473;&#20986;&#19968;&#20010;&#21305;&#37197;&#30340;$\Omega(\sqrt{\alpha})$&#30340;&#19979;&#30028;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;Konstantinov&#21644;Lampert (2021)&#31034;&#33539;&#20102;&#23545;&#20110;&#36866;&#24403;&#30340;&#23398;&#20064;&#22120;&#65292;&#36825;&#20004;&#20010;&#27010;&#24565;&#30340;&#31934;&#24230;&#25439;&#22833;&#37117;&#26159;$\Omega(1)$&#12290;&#20851;&#38190;&#30340;&#25216;&#26415;&#21019;&#26032;&#26159;
&lt;/p&gt;
&lt;p&gt;
We consider the vulnerability of fairness-constrained learning to small amounts of malicious noise in the training data. Konstantinov and Lampert (2021) initiated the study of this question and presented negative results showing there exist data distributions where for several fairness constraints, any proper learner will exhibit high vulnerability when group sizes are imbalanced. Here, we present a more optimistic view, showing that if we allow randomized classifiers, then the landscape is much more nuanced. For example, for Demographic Parity we show we can incur only a $\Theta(\alpha)$ loss in accuracy, where $\alpha$ is the malicious noise rate, matching the best possible even without fairness constraints. For Equal Opportunity, we show we can incur an $O(\sqrt{\alpha})$ loss, and give a matching $\Omega(\sqrt{\alpha})$lower bound. In contrast, Konstantinov and Lampert (2021) showed for proper learners the loss in accuracy for both notions is $\Omega(1)$. The key technical novelty 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#32447;&#24615;&#23618;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#36924;&#36817;&#20219;&#20309;&#35268;&#21017;&#30340;&#38750;&#32447;&#24615;&#24207;&#21015;&#21040;&#24207;&#21015;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2307.11888</link><description>&lt;p&gt;
&#20851;&#20110;&#32447;&#24615;&#36882;&#25512;&#21644;&#38750;&#32447;&#24615;&#25237;&#24433;&#30340;&#26222;&#36941;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Universality of Linear Recurrences Followed by Nonlinear Projections. (arXiv:2307.11888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#32447;&#24615;&#23618;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#36924;&#36817;&#20219;&#20309;&#35268;&#21017;&#30340;&#38750;&#32447;&#24615;&#24207;&#21015;&#21040;&#24207;&#21015;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#27880;&#37322;&#65288;&#20316;&#20026;&#19968;&#31687;&#20840;&#25991;&#35770;&#25991;&#30340;&#24037;&#20316;&#36827;&#23637;&#65289;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#26063;&#22522;&#20110;&#24490;&#29615;&#32447;&#24615;&#23618;&#65288;&#21253;&#25324;S4&#12289;S5&#21644;LRU&#65289;&#21644;&#20301;&#32622;&#36880;&#20803;&#32032;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLPs&#65289;&#30340;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#24456;&#22909;&#22320;&#36924;&#36817;&#20219;&#24847;&#35268;&#21017;&#30340;&#38750;&#32447;&#24615;&#24207;&#21015;&#21040;&#24207;&#21015;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#32972;&#21518;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#23558;&#24490;&#29615;&#23618;&#35270;&#20026;&#21487;&#20197;&#24544;&#23454;&#22320;&#23384;&#20648;&#36755;&#20837;&#24207;&#21015;&#20449;&#24687;&#21040;&#20869;&#37096;&#29366;&#24577;&#30340;&#21387;&#32553;&#31639;&#27861;&#65292;&#28982;&#21518;&#30001;&#39640;&#24230;&#34920;&#36798;&#33021;&#21147;&#30340;MLP&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note (work in progress towards a full-length paper) we show that a family of sequence models based on recurrent linear layers~(including S4, S5, and the LRU) interleaved with position-wise multi-layer perceptrons~(MLPs) can approximate arbitrarily well any sufficiently regular non-linear sequence-to-sequence map. The main idea behind our result is to see recurrent layers as compression algorithms that can faithfully store information about the input sequence into an inner state, before it is processed by the highly expressive MLP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#37327;&#23376;&#22810;&#20998;&#31867;&#22120;MORE&#65292;&#36890;&#36807;&#27979;&#37327;&#21644;&#30456;&#20851;&#24615;&#26469;&#23454;&#29616;&#22810;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#21333;&#20010;&#35835;&#20986;&#37327;&#23376;&#20301;&#30340;&#37327;&#23376;&#20449;&#24687;&#65292;&#32780;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#21518;&#22788;&#29702;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.11875</link><description>&lt;p&gt;
MORE: &#22522;&#20110;&#27979;&#37327;&#21644;&#30456;&#20851;&#24615;&#30340;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#29992;&#20110;&#22810;&#20998;&#31867;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MORE: Measurement and Correlation Based Variational Quantum Circuit for Multi-classification. (arXiv:2307.11875v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#37327;&#23376;&#22810;&#20998;&#31867;&#22120;MORE&#65292;&#36890;&#36807;&#27979;&#37327;&#21644;&#30456;&#20851;&#24615;&#26469;&#23454;&#29616;&#22810;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#21333;&#20010;&#35835;&#20986;&#37327;&#23376;&#20301;&#30340;&#37327;&#23376;&#20449;&#24687;&#65292;&#32780;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#21518;&#22788;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#37327;&#23376;&#35745;&#31639;&#22312;&#35745;&#31639;&#23494;&#38598;&#22411;&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#30456;&#24403;&#22823;&#30340;&#28508;&#21147;&#12290;&#20363;&#22914;&#65292;&#22522;&#20110;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#30340;&#20998;&#31867;&#20219;&#21153;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#24182;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37327;&#23376;&#35745;&#31639;&#36164;&#28304;&#21463;&#38480;&#25110;&#38656;&#35201;&#22823;&#37327;&#32463;&#20856;&#21518;&#22788;&#29702;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#37327;&#23376;&#20998;&#31867;&#22120;&#20165;&#36866;&#29992;&#20110;&#20108;&#20998;&#31867;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#37327;&#23376;&#22810;&#20998;&#31867;&#22120;MORE&#65292;&#35813;&#20998;&#31867;&#22120;&#20197;&#27979;&#37327;&#21644;&#30456;&#20851;&#24615;&#20026;&#22522;&#30784;&#26500;&#24314;&#20102;&#21464;&#20998;&#37327;&#23376;&#22810;&#20998;&#31867;&#22120;&#12290;MORE&#20351;&#29992;&#19982;&#20108;&#20998;&#31867;&#22120;&#30456;&#21516;&#30340;&#21464;&#20998;&#24335;&#35774;&#35745;&#24182;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#21333;&#20010;&#35835;&#20986;&#37327;&#23376;&#20301;&#30340;&#37327;&#23376;&#20449;&#24687;&#26469;&#25191;&#34892;&#22810;&#20998;&#31867;&#12290;&#20026;&#20102;&#20174;&#35835;&#20986;&#37327;&#23376;&#20301;&#25552;&#21462;&#23436;&#25972;&#20449;&#24687;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19977;&#20010;&#21487;&#35266;&#27979;&#37327;&#20316;&#20026;&#20108;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#22522;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#37327;&#23376;&#24577;&#37325;&#26500;&#25216;&#26415;&#23545;&#37327;&#23376;&#29366;&#24577;&#36827;&#34892;&#20102;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum computing has shown considerable promise for compute-intensive tasks in recent years. For instance, classification tasks based on quantum neural networks (QNN) have garnered significant interest from researchers and have been evaluated in various scenarios. However, the majority of quantum classifiers are currently limited to binary classification tasks due to either constrained quantum computing resources or the need for intensive classical post-processing. In this paper, we propose an efficient quantum multi-classifier called MORE, which stands for measurement and correlation based variational quantum multi-classifier. MORE adopts the same variational ansatz as binary classifiers while performing multi-classification by fully utilizing the quantum information of a single readout qubit. To extract the complete information from the readout qubit, we select three observables that form the basis of a two-dimensional Hilbert space. We then use the quantum state tomography techniqu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#39046;&#33521;&#24179;&#21488;&#19978;&#26816;&#27979;&#34394;&#20551;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20010;&#20154;&#36164;&#26009;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#39046;&#33521;&#20010;&#20154;&#36164;&#26009;&#20013;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#8220;&#37096;&#20998;&#21644;&#23376;&#37096;&#20998;&#26631;&#31614;&#23884;&#20837;&#8221;&#65288;SSTE&#65289;&#26041;&#27861;&#20197;&#22686;&#24378;&#25968;&#25454;&#30340;&#29305;&#24449;&#12290;&#30740;&#31350;&#36824;&#36890;&#36807;&#25910;&#38598;3600&#20010;&#39046;&#33521;&#20010;&#20154;&#36164;&#26009;&#24314;&#31435;&#20102;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2307.11864</link><description>&lt;p&gt;
&#34394;&#20551;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#39046;&#33521;&#20010;&#20154;&#36164;&#26009;&#30340;&#28508;&#22312;&#23041;&#32961;&#65306;&#26816;&#27979;&#21644;&#39044;&#38450;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
The Looming Threat of Fake and LLM-generated LinkedIn Profiles: Challenges and Opportunities for Detection and Prevention. (arXiv:2307.11864v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#39046;&#33521;&#24179;&#21488;&#19978;&#26816;&#27979;&#34394;&#20551;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20010;&#20154;&#36164;&#26009;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#39046;&#33521;&#20010;&#20154;&#36164;&#26009;&#20013;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#8220;&#37096;&#20998;&#21644;&#23376;&#37096;&#20998;&#26631;&#31614;&#23884;&#20837;&#8221;&#65288;SSTE&#65289;&#26041;&#27861;&#20197;&#22686;&#24378;&#25968;&#25454;&#30340;&#29305;&#24449;&#12290;&#30740;&#31350;&#36824;&#36890;&#36807;&#25910;&#38598;3600&#20010;&#39046;&#33521;&#20010;&#20154;&#36164;&#26009;&#24314;&#31435;&#20102;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39046;&#33521;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#27880;&#20876;&#26102;&#21644;&#24314;&#31435;&#36830;&#25509;&#20043;&#21069;&#31435;&#21363;&#26816;&#27979;&#34394;&#20551;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20010;&#20154;&#36164;&#26009;&#12290;&#26089;&#26399;&#35782;&#21035;&#34394;&#20551;&#36164;&#26009;&#23545;&#20110;&#32500;&#25252;&#24179;&#21488;&#30340;&#23436;&#25972;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#21487;&#20197;&#38450;&#27490;&#20882;&#21517;&#39030;&#26367;&#32773;&#33719;&#21462;&#21512;&#27861;&#29992;&#25143;&#30340;&#31169;&#23494;&#21644;&#25935;&#24863;&#20449;&#24687;&#65292;&#24182;&#38450;&#27490;&#20182;&#20204;&#33719;&#24471;&#22686;&#21152;&#26410;&#26469;&#38035;&#40060;&#21644;&#27450;&#35784;&#27963;&#21160;&#21487;&#20449;&#24230;&#30340;&#26426;&#20250;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#39046;&#33521;&#20010;&#20154;&#36164;&#26009;&#20013;&#25552;&#20379;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#20102;&#8220;&#37096;&#20998;&#21644;&#23376;&#37096;&#20998;&#26631;&#31614;&#23884;&#20837;&#8221;&#65288;SSTE&#65289;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#21306;&#20998;&#21512;&#27861;&#36164;&#26009;&#21644;&#20882;&#21517;&#39030;&#26367;&#32773;&#25163;&#21160;&#25110;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#36164;&#26009;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#37492;&#20110;&#30446;&#21069;&#20844;&#24320;&#21487;&#29992;&#30340;&#39046;&#33521;&#25968;&#25454;&#38598;&#36739;&#23569;&#65292;&#25105;&#20204;&#20026;&#30740;&#31350;&#25910;&#38598;&#20102;3600&#20010;&#39046;&#33521;&#20010;&#20154;&#36164;&#26009;&#65292;&#24182;&#23558;&#20844;&#24320;&#21457;&#24067;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20379;&#30740;&#31350;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel method for detecting fake and Large Language Model (LLM)-generated profiles in the LinkedIn Online Social Network immediately upon registration and before establishing connections. Early fake profile identification is crucial to maintaining the platform's integrity since it prevents imposters from acquiring the private and sensitive information of legitimate users and from gaining an opportunity to increase their credibility for future phishing and scamming activities. This work uses textual information provided in LinkedIn profiles and introduces the Section and Subsection Tag Embedding (SSTE) method to enhance the discriminative characteristics of these data for distinguishing between legitimate profiles and those created by imposters manually or by using an LLM. Additionally, the dearth of a large publicly available LinkedIn dataset motivated us to collect 3600 LinkedIn profiles for our research. We will release our dataset publicly for research pur
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#28909;&#21147;&#23398;&#35266;&#28857;&#65292;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#30340;Ising&#27169;&#22411;&#26469;&#35745;&#31639;&#30001;&#35757;&#32451;&#25968;&#25454;&#24341;&#21457;&#30340;&#31232;&#30095;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#20248;&#21270;&#20256;&#24863;&#22120;&#30340;&#31354;&#38388;&#37197;&#32622;&#21644;&#37325;&#26500;&#22797;&#26434;&#31995;&#32479;&#30340;&#23436;&#25972;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2307.11838</link><description>&lt;p&gt;
&#31232;&#30095;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#24341;&#21457;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Data-Induced Interactions of Sparse Sensors. (arXiv:2307.11838v1 [cond-mat.stat-mech])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#28909;&#21147;&#23398;&#35266;&#28857;&#65292;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#30340;Ising&#27169;&#22411;&#26469;&#35745;&#31639;&#30001;&#35757;&#32451;&#25968;&#25454;&#24341;&#21457;&#30340;&#31232;&#30095;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#20248;&#21270;&#20256;&#24863;&#22120;&#30340;&#31354;&#38388;&#37197;&#32622;&#21644;&#37325;&#26500;&#22797;&#26434;&#31995;&#32479;&#30340;&#23436;&#25972;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#65292;&#22823;&#32500;&#24230;&#30340;&#32463;&#39564;&#25968;&#25454;&#32463;&#24120;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#34920;&#31034;&#20026;&#20165;&#30001;&#20960;&#20010;&#29305;&#24449;&#27169;&#24335;&#30340;&#32452;&#21512;&#12290;&#30001;&#20110;&#36825;&#31181;&#32467;&#26500;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#20165;&#26377;&#23569;&#25968;&#23616;&#37096;&#21270;&#30340;&#20256;&#24863;&#22120;&#27979;&#37327;&#26469;&#37325;&#26032;&#26500;&#24314;&#22797;&#26434;&#31995;&#32479;&#30340;&#23436;&#25972;&#29366;&#24577;&#12290;&#36825;&#31181;&#37325;&#26500;&#30340;&#36136;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#20256;&#24863;&#22120;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#21462;&#20915;&#20110;&#20256;&#24863;&#22120;&#30340;&#31354;&#38388;&#37197;&#32622;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#22522;&#20110;&#32570;&#22833;&#25554;&#20540;&#21644;QR&#20998;&#35299;&#30340;&#31639;&#27861;&#26469;&#20248;&#21270;&#20256;&#24863;&#22120;&#20301;&#32622;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#37319;&#29992;&#28909;&#21147;&#23398;&#35266;&#28857;&#35745;&#31639;&#30001;&#35757;&#32451;&#25968;&#25454;&#24341;&#21457;&#30340;&#20256;&#24863;&#22120;&#30456;&#20114;&#20316;&#29992;&#30340;&#23436;&#25972;&#22320;&#24418;&#12290;&#35813;&#22320;&#24418;&#37319;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#30340;Ising&#27169;&#22411;&#30340;&#24418;&#24335;&#65292;&#32771;&#34385;&#21040;&#27599;&#20010;&#20256;&#24863;&#22120;&#20301;&#32622;&#25429;&#33719;&#30340;&#25968;&#25454;&#26041;&#24046;&#20197;&#21450;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#20018;&#25200;&#12290;&#32472;&#21046;&#20986;&#36825;&#20123;&#25968;&#25454;&#24341;&#21457;&#30340;&#20256;&#24863;&#22120;&#30456;&#20114;&#20316;&#29992;&#30340;&#22270;&#26223;&#20801;&#35768;
&lt;/p&gt;
&lt;p&gt;
Large-dimensional empirical data in science and engineering frequently has low-rank structure and can be represented as a combination of just a few eigenmodes. Because of this structure, we can use just a few spatially localized sensor measurements to reconstruct the full state of a complex system. The quality of this reconstruction, especially in the presence of sensor noise, depends significantly on the spatial configuration of the sensors. Multiple algorithms based on gappy interpolation and QR factorization have been proposed to optimize sensor placement. Here, instead of an algorithm that outputs a singular "optimal" sensor configuration, we take a thermodynamic view to compute the full landscape of sensor interactions induced by the training data. The landscape takes the form of the Ising model in statistical physics, and accounts for both the data variance captured at each sensor location and the crosstalk between sensors. Mapping out these data-induced sensor interactions allow
&lt;/p&gt;</description></item><item><title>PINNsFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#24615;&#20934;&#30830;&#36924;&#36817;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11833</link><description>&lt;p&gt;
PINNsFormer: &#22522;&#20110;Transformer&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks. (arXiv:2307.11833v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11833
&lt;/p&gt;
&lt;p&gt;
PINNsFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#24615;&#20934;&#30830;&#36924;&#36817;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#36817;&#20284;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#25968;&#20540;&#35299;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;PINNs&#21644;&#22823;&#22810;&#25968;&#30456;&#20851;&#30740;&#31350;&#37319;&#29992;&#20840;&#36830;&#25509;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#20316;&#20026;&#26680;&#24515;&#32467;&#26500;&#65292;&#24573;&#30053;&#20102;PDEs&#20013;&#30340;&#26102;&#38388;&#20851;&#31995;&#65292;&#26080;&#27861;&#20934;&#30830;&#36924;&#36817;&#30495;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#21363;PINNsFormer&#65292;&#36890;&#36807;Transformer-based&#27169;&#22411;&#20013;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#20934;&#30830;&#36924;&#36817;PDEs&#30340;&#35299;&#12290;PINNsFormer&#19981;&#20165;&#36866;&#24212;&#36755;&#20837;&#21521;&#37327;&#20197;&#20266;&#24207;&#21015;&#30340;&#24418;&#24335;&#36827;&#34892;&#36817;&#20284;&#39044;&#27979;&#65292;&#36824;&#23558;&#36880;&#28857;&#30340;PINNs&#25439;&#22833;&#25913;&#20026;&#20102;&#39034;&#24207;&#30340;PINNs&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;PINNsFormer&#36824;&#37197;&#22791;&#20102;&#19968;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#21363;&#23567;&#27874;&#20989;&#25968;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#23545;&#20613;&#37324;&#21494;&#20998;&#35299;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;PINNsFormer&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks (PINNs) have emerged as a promising deep learning framework for approximating numerical solutions for partial differential equations (PDEs). While conventional PINNs and most related studies adopt fully-connected multilayer perceptrons (MLP) as the backbone structure, they have neglected the temporal relations in PDEs and failed to approximate the true solution. In this paper, we propose a novel Transformer-based framework, namely PINNsFormer, that accurately approximates PDEs' solutions by capturing the temporal dependencies with multi-head attention mechanisms in Transformer-based models. Instead of approximating point predictions, PINNsFormer adapts input vectors to pseudo sequences and point-wise PINNs loss to a sequential PINNs loss. In addition, PINNsFormer is equipped with a novel activation function, namely Wavelet, which anticipates the Fourier decomposition through deep neural networks. We empirically demonstrate PINNsFormer's ability to captu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#26412;&#22320;&#26680;&#24402;&#19968;&#21270;&#20026;&#26426;&#21046;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#29305;&#24449;&#23398;&#20064;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2307.11807</link><description>&lt;p&gt;
&#20197;&#26412;&#22320;&#26680;&#24402;&#19968;&#21270;&#20026;&#26426;&#21046;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Local Kernel Renormalization as a mechanism for feature learning in overparametrized Convolutional Neural Networks. (arXiv:2307.11807v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#26412;&#22320;&#26680;&#24402;&#19968;&#21270;&#20026;&#26426;&#21046;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#29305;&#24449;&#23398;&#20064;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#23398;&#20064;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#23398;&#20064;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#26159;&#20854;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#20248;&#31168;&#33021;&#21147;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#22312;&#20840;&#36830;&#25509;&#25110;&#21367;&#31215;&#26550;&#26500;&#20013;&#65292;&#29305;&#24449;&#23398;&#20064;&#20284;&#20046;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;&#32463;&#39564;&#35777;&#25454;&#26174;&#31034;&#65292;&#26080;&#38480;&#23485;&#24230;&#26497;&#38480;&#19979;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#26368;&#32456;&#20248;&#20110;&#26377;&#38480;&#23485;&#24230;&#30340;&#23545;&#24212;&#32593;&#32476;&#12290;&#30001;&#20110;&#25551;&#36848;&#26080;&#38480;&#23485;&#24230;&#32593;&#32476;&#30340;&#26680;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#20250;&#25913;&#21464;&#65292;&#25152;&#20197;&#22312;&#28145;&#24230;&#20840;&#36830;&#25509;&#26550;&#26500;&#20013;&#20986;&#29616;&#30340;&#20219;&#20309;&#24418;&#24335;&#30340;&#29305;&#24449;&#23398;&#20064;&#23545;&#20110;&#25913;&#21892;&#27867;&#21270;&#27809;&#26377;&#22826;&#22823;&#24110;&#21161;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20855;&#26377;&#21367;&#31215;&#23618;&#30340;&#26368;&#20808;&#36827;&#26550;&#26500;&#22312;&#26377;&#38480;&#23485;&#24230;&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#36825;&#34920;&#26126;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20986;&#29616;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#29305;&#24449;&#23398;&#20064;&#24418;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20197;&#35299;&#37322;&#36825;&#20123;&#24046;&#24322;&#65292;&#20197;&#21450;&#22312;&#19968;&#20010;&#38544;&#34255;&#23618;&#32593;&#32476;&#20013;&#30340;&#29702;&#30001;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature learning, or the ability of deep neural networks to automatically learn relevant features from raw data, underlies their exceptional capability to solve complex tasks. However, feature learning seems to be realized in different ways in fully-connected (FC) or convolutional architectures (CNNs). Empirical evidence shows that FC neural networks in the infinite-width limit eventually outperform their finite-width counterparts. Since the kernel that describes infinite-width networks does not evolve during training, whatever form of feature learning occurs in deep FC architectures is not very helpful in improving generalization. On the other hand, state-of-the-art architectures with convolutional layers achieve optimal performances in the finite-width regime, suggesting that an effective form of feature learning emerges in this case. In this work, we present a simple theoretical framework that provides a rationale for these differences, in one hidden layer networks. First, we show t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#12290;&#35813;&#26041;&#27861;&#23558;&#20154;&#20307;&#27963;&#21160;&#25237;&#24433;&#21040;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#24110;&#21161;&#32858;&#31867;&#31639;&#27861;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11796</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#26080;&#30417;&#30563;&#23884;&#20837;&#23398;&#20064;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Embedding Learning for Human Activity Recognition Using Wearable Sensor Data. (arXiv:2307.11796v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#12290;&#35813;&#26041;&#27861;&#23558;&#20154;&#20307;&#27963;&#21160;&#25237;&#24433;&#21040;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#24110;&#21161;&#32858;&#31867;&#31639;&#27861;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#26426;&#21644;&#20854;&#20182;&#21487;&#31359;&#25140;&#35774;&#22791;&#20013;&#30340;&#23884;&#20837;&#24335;&#20256;&#24863;&#22120;&#20351;&#24471;&#20154;&#20307;&#27963;&#21160;&#25968;&#25454;&#26356;&#26131;&#33719;&#21462;&#12290;&#28982;&#32780;&#65292;&#20174;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#35782;&#21035;&#19981;&#21516;&#30340;&#20154;&#20307;&#27963;&#21160;&#20173;&#28982;&#26159;&#26222;&#36866;&#35745;&#31639;&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#33719;&#21462;&#30340;&#25968;&#25454;&#22823;&#37096;&#20998;&#27809;&#26377;&#26631;&#31614;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#20307;&#27963;&#21160;&#29305;&#24615;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#23558;&#20154;&#20307;&#27963;&#21160;&#25237;&#24433;&#21040;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#30456;&#20284;&#30340;&#27963;&#21160;&#20250;&#34987;&#32039;&#23494;&#22320;&#32858;&#38598;&#22312;&#19968;&#36215;&#12290;&#21033;&#29992;&#36825;&#20010;&#23884;&#20837;&#31354;&#38388;&#65292;&#21518;&#32493;&#30340;&#32858;&#31867;&#31639;&#27861;&#21487;&#20197;&#20174;&#20013;&#21463;&#30410;&#65292;&#24418;&#25104;&#20195;&#34920;&#19968;&#20010;&#20154;&#36827;&#34892;&#30340;&#19981;&#21516;&#27963;&#21160;&#30340;&#34892;&#20026;&#31751;&#12290;&#22312;&#19977;&#20010;&#24102;&#26377;&#26631;&#31614;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#32858;&#31867;&#31639;&#27861;&#22312;&#35782;&#21035;&#21644;&#20998;&#31867;&#28508;&#22312;&#20154;&#20307;&#27963;&#21160;&#26041;&#38754;&#23454;&#29616;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The embedded sensors in widely used smartphones and other wearable devices make the data of human activities more accessible. However, recognizing different human activities from the wearable sensor data remains a challenging research problem in ubiquitous computing. One of the reasons is that the majority of the acquired data has no labels. In this paper, we present an unsupervised approach, which is based on the nature of human activity, to project the human activities into an embedding space in which similar activities will be located closely together. Using this, subsequent clustering algorithms can benefit from the embeddings, forming behavior clusters that represent the distinct activities performed by a person. Results of experiments on three labeled benchmark datasets demonstrate the effectiveness of the framework and show that our approach can help the clustering algorithm achieve improved performance in identifying and categorizing the underlying human activities compared to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#20351;&#20854;&#20855;&#22791;&#20102;&#35821;&#38899;&#35782;&#21035;&#33021;&#21147;&#12290;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#26679;&#30340;&#25193;&#23637;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#23454;&#29616;&#20102;&#35821;&#38899;&#35782;&#21035;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#21487;&#20197;&#20923;&#32467;&#27169;&#22411;&#20197;&#20445;&#25345;&#20854;&#21407;&#26377;&#21151;&#33021;&#65292;&#24182;&#19988;&#25552;&#21319;&#38899;&#39057;&#32534;&#30721;&#22120;&#30340;&#35268;&#27169;&#26377;&#21161;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11795</link><description>&lt;p&gt;
&#29992;&#35821;&#38899;&#35782;&#21035;&#33021;&#21147;&#20419;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models with Speech Recognition Abilities. (arXiv:2307.11795v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#20351;&#20854;&#20855;&#22791;&#20102;&#35821;&#38899;&#35782;&#21035;&#33021;&#21147;&#12290;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#26679;&#30340;&#25193;&#23637;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#23454;&#29616;&#20102;&#35821;&#38899;&#35782;&#21035;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#21487;&#20197;&#20923;&#32467;&#27169;&#22411;&#20197;&#20445;&#25345;&#20854;&#21407;&#26377;&#21151;&#33021;&#65292;&#24182;&#19988;&#25552;&#21319;&#38899;&#39057;&#32534;&#30721;&#22120;&#30340;&#35268;&#27169;&#26377;&#21161;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#35777;&#26126;&#20854;&#39640;&#24230;&#28789;&#27963;&#65292;&#33021;&#22815;&#35299;&#20915;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#65292;&#22914;&#27010;&#25324;&#24615;&#25688;&#35201;&#21644;&#24320;&#25918;&#24615;&#38382;&#31572;&#12290;&#26412;&#25991;&#36890;&#36807;&#30452;&#25509;&#38468;&#21152;&#19968;&#20010;&#23567;&#22411;&#38899;&#39057;&#32534;&#30721;&#22120;&#26469;&#25193;&#23637;LLM&#30340;&#21151;&#33021;&#65292;&#20351;&#20854;&#33021;&#22815;&#25191;&#34892;&#35821;&#38899;&#35782;&#21035;&#12290;&#36890;&#36807;&#23558;&#19968;&#31995;&#21015;&#22768;&#38899;&#23884;&#20837;&#30452;&#25509;&#39044;&#32622;&#21040;&#25991;&#26412;&#20196;&#29260;&#23884;&#20837;&#20043;&#21069;&#65292;LLM&#21487;&#20197;&#36716;&#25442;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20854;&#25991;&#26412;&#23545;&#24212;&#29289;&#20197;&#23436;&#20840;&#30456;&#21516;&#30340;&#26041;&#24335;&#20351;&#29992;&#12290;&#22312;&#22810;&#35821;&#35328;LibriSpeech&#65288;MLS&#65289;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;&#19968;&#20010;conformer&#32534;&#30721;&#22120;&#34701;&#20837;&#21040;&#24320;&#28304;&#30340;LLaMA-7B&#20013;&#65292;&#20351;&#20854;&#22312;&#21333;&#19968;&#35821;&#35328;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#36229;&#36807;18%&#65292;&#24182;&#33021;&#22815;&#25191;&#34892;&#22810;&#35821;&#35328;&#35821;&#38899;&#35782;&#21035;&#65292;&#23613;&#31649;LLaMA&#30340;&#35757;&#32451;&#20027;&#35201;&#20381;&#36182;&#20110;&#33521;&#25991;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#35843;&#26597;LLM&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#21542;&#21487;&#20197;&#23436;&#20840;&#20923;&#32467;&#20197;&#20445;&#25345;&#20854;&#21407;&#26377;&#21151;&#33021;&#65292;&#20197;&#21450;&#25552;&#21319;&#38899;&#39057;&#32534;&#30721;&#22120;&#30340;&#35268;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have proven themselves highly flexible, able to solve a wide range of generative tasks, such as abstractive summarization and open-ended question answering. In this paper we extend the capabilities of LLMs by directly attaching a small audio encoder allowing it to perform speech recognition. By directly prepending a sequence of audial embeddings to the text token embeddings, the LLM can be converted to an automatic speech recognition (ASR) system, and be used in the exact same manner as its textual counterpart. Experiments on Multilingual LibriSpeech (MLS) show that incorporating a conformer encoder into the open sourced LLaMA-7B allows it to outperform monolingual baselines by 18% and perform multilingual speech recognition despite LLaMA being trained overwhelmingly on English text. Furthermore, we perform ablation studies to investigate whether the LLM can be completely frozen during training to maintain its original capabilities, scaling up the audio encoder, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;Transformer&#21644;CGAN&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21453;&#21521;&#35774;&#35745;&#30707;&#22696;&#28911;&#36229;&#34920;&#38754;&#65292;&#36890;&#36807;&#23545;&#22826;&#36203;&#20857;&#22810;&#35856;&#25391;&#21560;&#25910;&#35889;&#36827;&#34892;S2V&#21644;S2I&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#31934;&#30830;&#24230;&#21644;&#20840;&#38754;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11794</link><description>&lt;p&gt;
&#36890;&#36807;&#25913;&#36827;Transformer&#21644;CGAN&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20135;&#29983;&#30340;&#22826;&#36203;&#20857;&#22810;&#35856;&#25391;&#36229;&#34920;&#38754;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence-Generated Terahertz Multi-Resonant Metasurfaces via Improved Transformer and CGAN Neural Networks. (arXiv:2307.11794v1 [physics.optics])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;Transformer&#21644;CGAN&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21453;&#21521;&#35774;&#35745;&#30707;&#22696;&#28911;&#36229;&#34920;&#38754;&#65292;&#36890;&#36807;&#23545;&#22826;&#36203;&#20857;&#22810;&#35856;&#25391;&#21560;&#25910;&#35889;&#36827;&#34892;S2V&#21644;S2I&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#31934;&#30830;&#24230;&#21644;&#20840;&#38754;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#21033;&#29992;&#20256;&#32479;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36827;&#34892;&#22826;&#36203;&#20857;&#22810;&#35856;&#25391;&#30707;&#22696;&#28911;&#36229;&#34920;&#38754;&#30340;&#21453;&#21521;&#35774;&#35745;&#20855;&#26377;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;Transformer&#21644;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#65288;CGAN&#65289;&#26469;&#22522;&#20110;&#22826;&#36203;&#20857;&#22810;&#35856;&#25391;&#21560;&#25910;&#35889;&#36827;&#34892;&#30707;&#22696;&#28911;&#36229;&#34920;&#38754;&#30340;&#21453;&#21521;&#35774;&#35745;&#12290;&#25913;&#36827;&#30340;Transformer&#22312;Spectrum to Vector&#65288;S2V&#65289;&#35774;&#35745;&#20013;&#30456;&#27604;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#31934;&#30830;&#24230;&#21644;&#27867;&#21270;&#24615;&#33021;&#65307;&#32780;&#36890;&#36807;CGAN&#23454;&#29616;&#30340;Spectrum to Image&#65288;S2I&#65289;&#35774;&#35745;&#21487;&#20197;&#25552;&#20379;&#27604;MLP&#33719;&#24471;&#30340;S2V&#35774;&#35745;&#26356;&#20840;&#38754;&#30340;&#20449;&#24687;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;&#27492;&#22806;&#65292;&#25913;&#36827;&#30340;CGAN&#36824;&#21487;&#20197;&#30452;&#25509;&#20174;&#25152;&#38656;&#30340;&#22810;&#35856;&#25391;&#21560;&#25910;&#35889;&#20013;&#23454;&#29616;&#30707;&#22696;&#28911;&#36229;&#34920;&#38754;&#22270;&#20687;&#30340;&#21453;&#21521;&#35774;&#35745;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#39033;&#24037;&#20316;&#21487;&#20197;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#36229;&#34920;&#38754;&#35774;&#35745;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well known that the inverse design of terahertz (THz) multi-resonant graphene metasurfaces by using traditional deep neural networks (DNNs) has limited generalization ability. In this paper, we propose improved Transformer and conditional generative adversarial neural networks (CGAN) for the inverse design of graphene metasurfaces based upon THz multi-resonant absorption spectra. The improved Transformer can obtain higher accuracy and generalization performance in the StoV (Spectrum to Vector) design compared to traditional multilayer perceptron (MLP) neural networks, while the StoI (Spectrum to Image) design achieved through CGAN can provide more comprehensive information and higher accuracy than the StoV design obtained by MLP. Moreover, the improved CGAN can achieve the inverse design of graphene metasurface images directly from the desired multi-resonant absorption spectra. It is turned out that this work can finish facilitating the design process of artificial intelligence-g
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27973;&#23618;&#36882;&#24402;&#35299;&#30721;&#32593;&#32476;&#21644;&#31227;&#21160;&#20256;&#24863;&#22120;&#36712;&#36857;&#23454;&#29616;&#20840;&#29366;&#24577;&#37325;&#24314;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.11793</link><description>&lt;p&gt;
&#21033;&#29992;&#27973;&#23618;&#36882;&#24402;&#35299;&#30721;&#32593;&#32476;&#23558;&#20219;&#24847;&#31227;&#21160;&#20256;&#24863;&#22120;&#36712;&#36857;&#29992;&#20110;&#20840;&#29366;&#24577;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Leveraging arbitrary mobile sensor trajectories with shallow recurrent decoder networks for full-state reconstruction. (arXiv:2307.11793v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27973;&#23618;&#36882;&#24402;&#35299;&#30721;&#32593;&#32476;&#21644;&#31227;&#21160;&#20256;&#24863;&#22120;&#36712;&#36857;&#23454;&#29616;&#20840;&#29366;&#24577;&#37325;&#24314;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#26159;&#30417;&#27979;&#12289;&#39044;&#27979;&#21644;&#25511;&#21046;&#22797;&#26434;&#26102;&#31354;&#31995;&#32479;&#26368;&#22522;&#26412;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#26377;&#38480;&#25968;&#37327;&#30340;&#20256;&#24863;&#22120;&#26159;&#31227;&#21160;&#30340;&#65292;&#24182;&#38543;&#30528;&#21160;&#24577;&#30340;&#25913;&#21464;&#20301;&#32622;&#65292;&#20363;&#22914;&#21487;&#31359;&#25140;&#25216;&#26415;&#12289;&#28023;&#27915;&#30417;&#27979;&#28014;&#26631;&#21644;&#22825;&#27668;&#27668;&#29699;&#12290;&#22312;&#36825;&#20123;&#21160;&#24577;&#31995;&#32479;&#20013;&#65288;&#27809;&#26377;&#32479;&#35745;&#29420;&#31435;&#30340;&#21306;&#22495;&#65289;&#65292;&#27979;&#37327;&#26102;&#38388;&#21382;&#21490;&#32534;&#30721;&#20102;&#22823;&#37327;&#21487;&#29992;&#20110;&#20851;&#38190;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;&#22823;&#22810;&#25968;&#26080;&#27169;&#22411;&#24863;&#30693;&#33539;&#20363;&#30340;&#30446;&#26631;&#26159;&#23558;&#24403;&#21069;&#31232;&#30095;&#20256;&#24863;&#22120;&#27979;&#37327;&#26144;&#23556;&#21040;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#65292;&#23436;&#20840;&#24573;&#30053;&#26102;&#38388;&#21382;&#21490;&#12290;&#21033;&#29992;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#21521;&#37327;&#27169;&#22411;&#65288;&#22914;LSTM&#32593;&#32476;&#65289;&#21644;&#35299;&#30721;&#32593;&#32476;&#65292;&#21160;&#24577;&#36712;&#36857;&#20449;&#24687;&#21487;&#20197;&#26144;&#23556;&#21040;&#23436;&#25972;&#30340;&#29366;&#24577;&#31354;&#38388;&#20272;&#35745;&#20013;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#21033;&#29992;&#27973;&#23618;&#36882;&#24402;&#35299;&#30721;&#32593;&#32476;&#21644;&#31227;&#21160;&#20256;&#24863;&#22120;&#36712;&#36857;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#20840;&#29366;&#24577;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sensing is one of the most fundamental tasks for the monitoring, forecasting and control of complex, spatio-temporal systems. In many applications, a limited number of sensors are mobile and move with the dynamics, with examples including wearable technology, ocean monitoring buoys, and weather balloons. In these dynamic systems (without regions of statistical-independence), the measurement time history encodes a significant amount of information that can be extracted for critical tasks. Most model-free sensing paradigms aim to map current sparse sensor measurements to the high-dimensional state space, ignoring the time-history all together. Using modern deep learning architectures, we show that a sequence-to-vector model, such as an LSTM (long, short-term memory) network, with a decoder network, dynamic trajectory information can be mapped to full state-space estimates. Indeed, we demonstrate that by leveraging mobile sensor trajectories with shallow recurrent decoder networks, we can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24341;&#20837;&#20102;&#19977;&#37327;&#23376;&#20301;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#22411;&#20132;&#20114;&#23618;&#30340;&#37327;&#23376;&#21367;&#31215;&#32593;&#32476;&#65292;&#22686;&#21152;&#20102;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#32416;&#32544;&#33021;&#21147;&#65292;&#29992;&#20110;&#23545;&#22270;&#20687;&#21644;&#19968;&#32500;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2307.11792</link><description>&lt;p&gt;
&#20855;&#26377;&#20132;&#20114;&#23618;&#30340;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32463;&#20856;&#25968;&#25454;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Quantum Convolutional Neural Networks with Interaction Layers for Classification of Classical Data. (arXiv:2307.11792v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24341;&#20837;&#20102;&#19977;&#37327;&#23376;&#20301;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#22411;&#20132;&#20114;&#23618;&#30340;&#37327;&#23376;&#21367;&#31215;&#32593;&#32476;&#65292;&#22686;&#21152;&#20102;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#32416;&#32544;&#33021;&#21147;&#65292;&#29992;&#20110;&#23545;&#22270;&#20687;&#21644;&#19968;&#32500;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#37327;&#23376;&#35745;&#31639;&#26426;&#20855;&#26377;&#24322;&#24120;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#65292;&#20960;&#20046;&#27809;&#26377;&#38169;&#35823;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#25215;&#35834;&#20043;&#19979;&#65292;&#23545;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20013;&#22810;&#37327;&#23376;&#20301;&#30456;&#20114;&#20316;&#29992;&#30340;&#24433;&#21709;&#36827;&#34892;&#24191;&#27867;&#30740;&#31350;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24341;&#20837;&#20102;&#19977;&#37327;&#23376;&#20301;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#22411;&#20132;&#20114;&#23618;&#30340;&#37327;&#23376;&#21367;&#31215;&#32593;&#32476;&#65292;&#22686;&#21152;&#20102;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#32416;&#32544;&#33021;&#21147;&#65292;&#29992;&#20110;&#23545;&#22270;&#20687;&#21644;&#19968;&#32500;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;MNIST&#12289;Fashion MNIST&#21644;Iris&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#29992;&#20110;&#36827;&#34892;&#20108;&#20803;&#21644;&#22810;&#31867;&#21035;&#20998;&#31867;&#65292;&#24182;&#21457;&#29616;&#36229;&#36234;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum Machine Learning (QML) has come into the limelight due to the exceptional computational abilities of quantum computers. With the promises of near error-free quantum computers in the not-so-distant future, it is important that the effect of multi-qubit interactions on quantum neural networks is studied extensively. This paper introduces a Quantum Convolutional Network with novel Interaction layers exploiting three-qubit interactions increasing the network's expressibility and entangling capability, for classifying both image and one-dimensional data. The proposed approach is tested on three publicly available datasets namely MNIST, Fashion MNIST, and Iris datasets, to perform binary and multiclass classifications and is found to supersede the performance of the existing state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35748;&#30693;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2307.11787</link><description>&lt;p&gt;
LLM&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#26377;&#25152;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
LLM Cognitive Judgements Differ From Human. (arXiv:2307.11787v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35748;&#30693;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#12289;&#20225;&#19994;&#21644;&#28040;&#36153;&#32773;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#34429;&#28982;&#36825;&#31867;&#27169;&#22411;&#30340;&#35821;&#35328;&#33021;&#21147;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#23427;&#20204;&#20316;&#20026;&#35748;&#30693;&#20027;&#20307;&#30340;&#35843;&#26597;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#23545;GPT-3&#21644;ChatGPT&#22312;&#19968;&#20010;&#26469;&#33258;&#35748;&#30693;&#31185;&#23398;&#25991;&#29486;&#30340;&#26377;&#38480;&#25968;&#25454;&#24402;&#32435;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have lately been on the spotlight of researchers, businesses, and consumers alike. While the linguistic capabilities of such models have been studied extensively, there is growing interest in investigating them as cognitive subjects. In the present work I examine GPT-3 and ChatGPT capabilities on an limited-data inductive reasoning task from the cognitive science literature. The results suggest that these models' cognitive judgements are not human-like.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#23545;&#25239;&#23545;&#35805;&#22609;&#36896;&#26469;&#22686;&#24378;&#26234;&#33021;&#23545;&#35805;&#20195;&#29702;&#30340;&#20004;&#20010;&#27169;&#22411;&#65306;GANPG&#21644;REGS&#12290;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25913;&#36827;&#24403;&#21069;&#30340;&#33258;&#21160;&#25320;&#21495;&#31995;&#32479;&#65292;&#25552;&#39640;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11785</link><description>&lt;p&gt;
&#26234;&#33021;&#20195;&#29702;&#30340;&#23545;&#25239;&#23545;&#35805;&#22609;&#36896;
&lt;/p&gt;
&lt;p&gt;
Adversarial Conversational Shaping for Intelligent Agents. (arXiv:2307.11785v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#23545;&#25239;&#23545;&#35805;&#22609;&#36896;&#26469;&#22686;&#24378;&#26234;&#33021;&#23545;&#35805;&#20195;&#29702;&#30340;&#20004;&#20010;&#27169;&#22411;&#65306;GANPG&#21644;REGS&#12290;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25913;&#36827;&#24403;&#21069;&#30340;&#33258;&#21160;&#25320;&#21495;&#31995;&#32479;&#65292;&#25552;&#39640;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20986;&#29616;&#20351;&#24471;&#30740;&#31350;&#30028;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#33258;&#21160;&#25320;&#21495;&#31995;&#32479;&#20173;&#28982;&#19981;&#31283;&#23450;&#19988;&#19981;&#20934;&#30830;&#65306;&#25991;&#26412;&#29983;&#25104;&#22120;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#33021;&#20250;&#36831;&#38045;&#24182;&#35823;&#35299;&#20154;&#31867;&#23545;&#35805;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23427;&#20204;&#36890;&#36807;&#23545;&#25239;&#23545;&#35805;&#22609;&#36896;&#26469;&#22686;&#24378;&#26234;&#33021;&#23545;&#35805;&#20195;&#29702;&#65306;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANPG&#65289;&#21644;&#22522;&#20110;Li&#31561;&#20154;&#25552;&#20986;&#30340;REGS&#27169;&#22411;&#30340;&#27599;&#19968;&#20195;&#29983;&#25104;&#27493;&#39588;&#37117;&#26377;&#22870;&#21169;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;REGS&#65289;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20026;&#37096;&#20998;&#21644;&#23436;&#25972;&#30340;&#29983;&#25104;&#25991;&#26412;&#24207;&#21015;&#20998;&#37197;&#22870;&#21169;&#12290;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#35752;&#35770;&#20102;&#20351;&#29992;&#19981;&#21516;&#35757;&#32451;&#32454;&#33410;&#30340;&#24615;&#33021;&#65306;seq2seq [36]&#21644;transformers [37]&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent emergence of deep learning methods has enabled the research community to achieve state-of-the art results in several domains including natural language processing. However, the current robocall system remains unstable and inaccurate: text generator and chat-bots can be tedious and misunderstand human-like dialogue. In this work, we study the performance of two models able to enhance an intelligent conversational agent through adversarial conversational shaping: a generative adversarial network with policy gradient (GANPG) and a generative adversarial network with reward for every generation step (REGS) based on the REGS model presented in Li et al. [18] . This model is able to assign rewards to both partially and fully generated text sequences. We discuss performance with different training details : seq2seq [ 36] and transformers [37 ] in a reinforcement learning framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#23398;&#20064;&#26426;&#21046;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#39564;&#35777;&#26041;&#27861;&#26469;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#32479;&#35745;&#20445;&#38556;&#12290;</title><link>http://arxiv.org/abs/2307.11784</link><description>&lt;p&gt;
&#31350;&#31455;&#20160;&#20040;&#26159;&#21487;&#23454;&#29616;&#30340;&#21487;&#35777;&#26126;&#20445;&#38556;&#30340;&#23398;&#20064;&#26426;&#21046;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
What, Indeed, is an Achievable Provable Guarantee for Learning-Enabled Safety Critical Systems. (arXiv:2307.11784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#23398;&#20064;&#26426;&#21046;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#39564;&#35777;&#26041;&#27861;&#26469;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#32479;&#35745;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#33258;&#20449;&#22320;&#21033;&#29992;&#23398;&#20064;&#26426;&#21046;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#36825;&#20123;&#25361;&#25112;&#20013;&#65292;&#23454;&#29616;&#23433;&#20840;&#20445;&#35777;&#30340;&#19968;&#31181;&#20005;&#35880;&#20294;&#23454;&#29992;&#30340;&#26041;&#27861;&#26159;&#26368;&#20026;&#31361;&#20986;&#30340;&#12290;&#26412;&#25991;&#39318;&#20808;&#35752;&#35770;&#20102;&#35774;&#35745;&#21644;&#39564;&#35777;&#36825;&#31181;&#31995;&#32479;&#25152;&#28041;&#21450;&#30340;&#24037;&#31243;&#21644;&#30740;&#31350;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#23545;&#29616;&#26377;&#24037;&#20316;&#26080;&#27861;&#23454;&#29616;&#21487;&#35777;&#26126;&#20445;&#35777;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#39564;&#35777;&#26041;&#27861;&#65292;&#20197;&#26368;&#32456;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#32479;&#35745;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has made remarkable advancements, but confidently utilising learning-enabled components in safety-critical domains still poses challenges. Among the challenges, it is known that a rigorous, yet practical, way of achieving safety guarantees is one of the most prominent. In this paper, we first discuss the engineering and research challenges associated with the design and verification of such systems. Then, based on the observation that existing works cannot actually achieve provable guarantees, we promote a two-step verification method for the ultimate achievement of provable statistical guarantees.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;Adam&#31639;&#27861;&#22312;&#38750;&#20984;&#30446;&#26631;&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#36890;&#36807;&#25506;&#32034;&#36229;&#21442;&#25968;&#35774;&#32622;&#21644;&#35299;&#20915;&#38750;&#36951;&#20256;&#24615;&#25910;&#25947;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20960;&#20046;&#30830;&#23450;&#24615;&#30340;&#36951;&#20256;&#25910;&#25947;&#36895;&#29575;&#65292;&#35777;&#26126;&#20102;&#38750;&#36951;&#20256;&#24615;&#25910;&#25947;&#20248;&#20110;&#36951;&#20256;&#24615;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2307.11782</link><description>&lt;p&gt;
Adam&#31639;&#27861;&#22312;&#38750;&#20984;&#30446;&#26631;&#20013;&#30340;&#25910;&#25947;&#24615;&#65306;&#25918;&#26494;&#30340;&#36229;&#21442;&#25968;&#21644;&#38750;&#36951;&#20256;&#24615;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Convergence of Adam for Non-convex Objectives: Relaxed Hyperparameters and Non-ergodic Case. (arXiv:2307.11782v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11782
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;Adam&#31639;&#27861;&#22312;&#38750;&#20984;&#30446;&#26631;&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#36890;&#36807;&#25506;&#32034;&#36229;&#21442;&#25968;&#35774;&#32622;&#21644;&#35299;&#20915;&#38750;&#36951;&#20256;&#24615;&#25910;&#25947;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20960;&#20046;&#30830;&#23450;&#24615;&#30340;&#36951;&#20256;&#25910;&#25947;&#36895;&#29575;&#65292;&#35777;&#26126;&#20102;&#38750;&#36951;&#20256;&#24615;&#25910;&#25947;&#20248;&#20110;&#36951;&#20256;&#24615;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Adam&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20854;&#25910;&#25947;&#24615;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#23436;&#20840;&#29702;&#35299;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#20984;&#35774;&#32622;&#19979;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#25506;&#32034;Adam&#30340;&#25910;&#25947;&#24615;&#36229;&#21442;&#25968;&#35774;&#32622;&#65292;&#24182;&#35299;&#20915;&#19982;&#23454;&#38469;&#24212;&#29992;&#30456;&#20851;&#30340;&#38750;&#36951;&#20256;&#24615;&#25910;&#25947;&#30340;&#25361;&#25112;&#12290;&#20027;&#35201;&#36129;&#29486;&#24635;&#32467;&#22914;&#19979;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36951;&#20256;&#24615;&#21644;&#38750;&#36951;&#20256;&#24615;&#25910;&#25947;&#30340;&#31934;&#30830;&#23450;&#20041;&#65292;&#28085;&#30422;&#20102;&#20960;&#20046;&#25152;&#26377;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#25910;&#25947;&#30340;&#24418;&#24335;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24378;&#35843;&#38750;&#36951;&#20256;&#24615;&#25910;&#25947;&#20248;&#20110;&#36951;&#20256;&#24615;&#25910;&#25947;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;Adam&#36951;&#20256;&#24615;&#25910;&#25947;&#20445;&#35777;&#30340;&#19968;&#20010;&#36739;&#24369;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20801;&#35768;&#26356;&#25918;&#26494;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;Adam&#30340;&#20960;&#20046;&#30830;&#23450;&#24615;&#36951;&#20256;&#25910;&#25947;&#36895;&#24230;&#65292;&#36825;&#20010;&#36895;&#24230;&#21487;&#20197;&#20219;&#24847;&#25509;&#36817;$o(1/\sqrt{K})$&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;las
&lt;/p&gt;
&lt;p&gt;
Adam is a commonly used stochastic optimization algorithm in machine learning. However, its convergence is still not fully understood, especially in the non-convex setting. This paper focuses on exploring hyperparameter settings for the convergence of vanilla Adam and tackling the challenges of non-ergodic convergence related to practical application. The primary contributions are summarized as follows: firstly, we introduce precise definitions of ergodic and non-ergodic convergence, which cover nearly all forms of convergence for stochastic optimization algorithms. Meanwhile, we emphasize the superiority of non-ergodic convergence over ergodic convergence. Secondly, we establish a weaker sufficient condition for the ergodic convergence guarantee of Adam, allowing a more relaxed choice of hyperparameters. On this basis, we achieve the almost sure ergodic convergence rate of Adam, which is arbitrarily close to $o(1/\sqrt{K})$. More importantly, we prove, for the first time, that the las
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#25552;&#21462;-&#25688;&#35201;&#36724;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#34913;&#37327;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#20869;&#23481;&#30340;"&#20511;&#29992;"&#31243;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#24320;&#21457;&#30456;&#24212;&#24230;&#37327;&#26631;&#20934;&#12289;&#25968;&#25454;&#38598;&#21644;&#27880;&#37322;&#25351;&#21335;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.11779</link><description>&lt;p&gt;
Generate&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25552;&#21462;-&#25688;&#35201;&#36724;:&#27979;&#37327;&#20869;&#23481;"&#20511;&#29992;"
&lt;/p&gt;
&lt;p&gt;
The Extractive-Abstractive Axis: Measuring Content "Borrowing" in Generative Language Models. (arXiv:2307.11779v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11779
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#25552;&#21462;-&#25688;&#35201;&#36724;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#34913;&#37327;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#20869;&#23481;&#30340;"&#20511;&#29992;"&#31243;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#24320;&#21457;&#30456;&#24212;&#24230;&#37327;&#26631;&#20934;&#12289;&#25968;&#25454;&#38598;&#21644;&#27880;&#37322;&#25351;&#21335;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#35774;&#35745;&#20135;&#29983;&#39640;&#24230;&#25688;&#35201;&#30340;&#36755;&#20986;&#65292;&#19982;&#25628;&#32034;&#24341;&#25806;&#20013;&#30340;&#25552;&#21462;&#24335;&#21709;&#24212;&#24418;&#25104;&#23545;&#27604;&#12290;&#37492;&#20110;LLMs&#30340;&#36825;&#19968;&#29305;&#28857;&#21450;&#20854;&#23545;&#20869;&#23481;&#35768;&#21487;&#21644;&#24402;&#23646;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25152;&#35859;&#30340;&#25552;&#21462;-&#25688;&#35201;&#36724;&#65292;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#24378;&#35843;&#24320;&#21457;&#30456;&#24212;&#30340;&#24230;&#37327;&#26631;&#20934;&#12289;&#25968;&#25454;&#38598;&#21644;&#27880;&#37322;&#25351;&#21335;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#23558;&#35752;&#35770;&#38480;&#21046;&#22312;&#25991;&#26412;&#24418;&#24335;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative language models produce highly abstractive outputs by design, in contrast to extractive responses in search engines. Given this characteristic of LLMs and the resulting implications for content Licensing &amp; Attribution, we propose the the so-called Extractive-Abstractive axis for benchmarking generative models and highlight the need for developing corresponding metrics, datasets and annotation guidelines. We limit our discussion to the text modality.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32479;&#35745;&#22686;&#24378;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#25163;&#29699;&#27604;&#36187;&#65292;&#20934;&#30830;&#29575;&#36229;&#36807;80%&#65292;&#21487;&#20197;&#20026;&#25163;&#29699;&#38431;&#25945;&#32451;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#32479;&#35745;&#21644;&#39044;&#27979;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.11777</link><description>&lt;p&gt;
&#36890;&#36807;&#20272;&#35745;&#22242;&#38431;&#23454;&#21147;&#30340;&#32479;&#35745;&#22686;&#24378;&#23398;&#20064;&#26469;&#39044;&#27979;&#25163;&#29699;&#27604;&#36187;
&lt;/p&gt;
&lt;p&gt;
Prediction of Handball Matches with Statistically Enhanced Learning via Estimated Team Strengths. (arXiv:2307.11777v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11777
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32479;&#35745;&#22686;&#24378;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#25163;&#29699;&#27604;&#36187;&#65292;&#20934;&#30830;&#29575;&#36229;&#36807;80%&#65292;&#21487;&#20197;&#20026;&#25163;&#29699;&#38431;&#25945;&#32451;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#32479;&#35745;&#21644;&#39044;&#27979;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#22686;&#24378;&#23398;&#20064;&#27169;&#22411;&#65288;&#21363; SEL &#27169;&#22411;&#65289;&#26469;&#39044;&#27979;&#25163;&#29699;&#27604;&#36187;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807; SEL &#29305;&#24449;&#30340;&#22686;&#24378;&#34920;&#29616;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#36229;&#36807;80%&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#20197;&#22312;&#36807;&#21435;&#30340;&#22899;&#23376;&#20465;&#20048;&#37096;&#27604;&#36187;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#21518;&#25105;&#20204;&#27604;&#36739;&#19981;&#21516;&#30340;&#27169;&#22411;&#24182;&#35780;&#20272;&#23427;&#20204;&#26469;&#35780;&#20272;&#20854;&#24615;&#33021;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#25105;&#20204;&#30340;&#24037;&#20855;&#20174;&#32431;&#31929;&#30340;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#36716;&#21464;&#20026;&#39640;&#24230;&#26377;&#35265;&#35299;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;&#36825;&#23545;&#20110;&#25163;&#29699;&#38431;&#30340;&#25945;&#32451;&#26469;&#35828;&#21487;&#20197;&#25104;&#20026;&#23453;&#36149;&#30340;&#36164;&#20135;&#65292;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#32479;&#35745;&#21644;&#39044;&#27979;&#27934;&#23519;&#21147;&#65292;&#20197;&#20934;&#22791;&#26410;&#26469;&#30340;&#27604;&#36187;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a Statistically Enhanced Learning (aka. SEL) model to predict handball games. Our Machine Learning model augmented with SEL features outperforms state-of-the-art models with an accuracy beyond 80%. In this work, we show how we construct the data set to train Machine Learning models on past female club matches. We then compare different models and evaluate them to assess their performance capabilities. Finally, explainability methods allow us to change the scope of our tool from a purely predictive solution to a highly insightful analytical tool. This can become a valuable asset for handball teams' coaches providing valuable statistical and predictive insights to prepare future competitions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23884;&#20837;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#65292;&#23884;&#20837;&#23618;&#27425;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#21644;&#38754;&#21521;&#26102;&#38388;&#30340;&#21160;&#24577;&#23884;&#20837;&#19977;&#31181;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#22024;&#26434;&#30340;&#22823;&#25968;&#25454;&#29615;&#22659;&#20013;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#20027;&#39064;&#25552;&#21462;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.11775</link><description>&lt;p&gt;
&#25429;&#25417;&#31038;&#20132;&#23186;&#20307;&#20013;&#23458;&#25143;&#35265;&#35299;&#30340;&#20027;&#39064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Topical Approach to Capturing Customer Insight In Social Media. (arXiv:2307.11775v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23884;&#20837;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#65292;&#23884;&#20837;&#23618;&#27425;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#21644;&#38754;&#21521;&#26102;&#38388;&#30340;&#21160;&#24577;&#23884;&#20837;&#19977;&#31181;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#22024;&#26434;&#30340;&#22823;&#25968;&#25454;&#29615;&#22659;&#20013;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#20027;&#39064;&#25552;&#21462;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#26102;&#20195;&#20026;&#20225;&#19994;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#36825;&#31181;&#32321;&#33635;&#30340;&#20449;&#24687;&#36130;&#23500;&#36229;&#20986;&#20102;&#20256;&#32479;&#33829;&#38144;&#30740;&#31350;&#30340;&#28192;&#36947;&#21644;&#26694;&#26550;&#65292;&#21253;&#25324;&#33829;&#38144;&#32452;&#21512;&#24314;&#27169;(MMM)&#12290;&#29305;&#21035;&#26159;&#65292;&#25991;&#26412;&#25968;&#25454;&#25552;&#20986;&#20102;&#35768;&#22810;&#25968;&#25454;&#20998;&#26512;&#20174;&#19994;&#20154;&#21592;&#24517;&#39035;&#24212;&#23545;&#30340;&#25361;&#25112;&#12290;&#31038;&#20132;&#23186;&#20307;&#26500;&#25104;&#20102;&#22823;&#35268;&#27169;&#12289;&#24322;&#26500;&#21644;&#22024;&#26434;&#30340;&#25991;&#26723;&#26469;&#28304;&#12290;&#24037;&#19994;&#25968;&#25454;&#37319;&#38598;&#36807;&#31243;&#21253;&#25324;&#19968;&#23450;&#37327;&#30340;ETL&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#20013;&#22122;&#22768;&#30340;&#21464;&#24322;&#24615;&#21644;&#19981;&#21516;&#26469;&#28304;&#24341;&#20837;&#30340;&#24322;&#26500;&#24615;&#32473;&#20104;&#20102;&#20020;&#26102;&#24037;&#20855;&#30340;&#38656;&#27714;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#12289;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#25552;&#21462;&#23458;&#25143;&#35265;&#35299;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#22024;&#26434;&#30340;&#22823;&#25968;&#25454;&#29615;&#22659;&#20013;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#20027;&#39064;&#25552;&#21462;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#19978;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#27861;&#65306;&#23884;&#20837;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#12289;&#23884;&#20837;&#23618;&#27425;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#21644;&#38754;&#21521;&#26102;&#38388;&#30340;&#21160;&#24577;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
The age of social media has opened new opportunities for businesses. This flourishing wealth of information is outside traditional channels and frameworks of classical marketing research, including that of Marketing Mix Modeling (MMM). Textual data, in particular, poses many challenges that data analysis practitioners must tackle. Social media constitute massive, heterogeneous, and noisy document sources. Industrial data acquisition processes include some amount of ETL. However, the variability of noise in the data and the heterogeneity induced by different sources create the need for ad-hoc tools. Put otherwise, customer insight extraction in fully unsupervised, noisy contexts is an arduous task. This research addresses the challenge of fully unsupervised topic extraction in noisy, Big Data contexts. We present three approaches we built on the Variational Autoencoder framework: the Embedded Dirichlet Process, the Embedded Hierarchical Dirichlet Process, and the time-aware Dynamic Embe
&lt;/p&gt;</description></item><item><title>AutoAlign&#26159;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25429;&#25417;&#35859;&#35789;&#30456;&#20284;&#24615;&#65292;&#24182;&#20351;&#29992;TransE&#35745;&#31639;&#23454;&#20307;&#23884;&#20837;&#26469;&#23454;&#29616;&#23454;&#20307;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2307.11772</link><description>&lt;p&gt;
AutoAlign&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#33258;&#21160;&#26377;&#25928;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models. (arXiv:2307.11772v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11772
&lt;/p&gt;
&lt;p&gt;
AutoAlign&#26159;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25429;&#25417;&#35859;&#35789;&#30456;&#20284;&#24615;&#65292;&#24182;&#20351;&#29992;TransE&#35745;&#31639;&#23454;&#20307;&#23884;&#20837;&#26469;&#23454;&#29616;&#23454;&#20307;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#38388;&#30340;&#23454;&#20307;&#23545;&#40784;&#20219;&#21153;&#26088;&#22312;&#35782;&#21035;&#20986;&#20004;&#20010;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#20013;&#34920;&#31034;&#30456;&#21516;&#23454;&#20307;&#30340;&#27599;&#23545;&#23454;&#20307;&#12290;&#35768;&#22810;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#37117;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#65292;&#36825;&#26159;&#38750;&#24120;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21517;&#20026;AutoAlign&#30340;&#23436;&#20840;&#33258;&#21160;&#23545;&#40784;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#35859;&#35789;&#23884;&#20837;&#65292;AutoAlign&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#35859;&#35789;&#36817;&#37051;&#22270;&#65292;&#33258;&#21160;&#25429;&#25417;&#20004;&#20010;&#30693;&#35782;&#22270;&#35889;&#20013;&#35859;&#35789;&#30340;&#30456;&#20284;&#24615;&#12290;&#23545;&#20110;&#23454;&#20307;&#23884;&#20837;&#65292;AutoAlign&#39318;&#20808;&#20351;&#29992;TransE&#29420;&#31435;&#35745;&#31639;&#27599;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#20307;&#23884;&#20837;&#65292;&#28982;&#21518;&#36890;&#36807;&#35745;&#31639;&#22522;&#20110;&#23454;&#20307;&#23646;&#24615;&#30340;&#23454;&#20307;&#30456;&#20284;&#24615;&#65292;&#23558;&#20004;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#20307;&#23884;&#20837;&#31227;&#21160;&#21040;&#30456;&#21516;&#30340;&#21521;&#37327;&#31354;&#38388;&#20013;&#12290;&#22240;&#27492;&#65292;AutoAlign&#23454;&#29616;&#20102;&#35859;&#35789;&#23545;&#40784;&#21644;&#23454;&#20307;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of entity alignment between knowledge graphs (KGs) aims to identify every pair of entities from two different KGs that represent the same entity. Many machine learning-based methods have been proposed for this task. However, to our best knowledge, existing methods all require manually crafted seed alignments, which are expensive to obtain. In this paper, we propose the first fully automatic alignment method named AutoAlign, which does not require any manually crafted seed alignments. Specifically, for predicate embeddings, AutoAlign constructs a predicate-proximity-graph with the help of large language models to automatically capture the similarity between predicates across two KGs. For entity embeddings, AutoAlign first computes the entity embeddings of each KG independently using TransE, and then shifts the two KGs' entity embeddings into the same vector space by computing the similarity between entities based on their attributes. Thus, both predicate alignment and entity al
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#35268;&#27169;&#35745;&#31639;&#35780;&#20272;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;2D&#25991;&#26412;&#31354;&#38388;&#21270;&#30340;&#20027;&#39064;&#27169;&#22411;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#35821;&#26009;&#24211;&#20998;&#26512;&#25552;&#20379;&#20102;&#20855;&#26377;&#39640;&#36136;&#37327;&#24067;&#23616;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.11770</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35780;&#20272;&#29992;&#20110;2D&#25991;&#26412;&#31354;&#38388;&#21270;&#30340;&#20027;&#39064;&#27169;&#22411;&#21644;&#38477;&#32500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Evaluation of Topic Models and Dimensionality Reduction Methods for 2D Text Spatialization. (arXiv:2307.11770v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11770
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#35268;&#27169;&#35745;&#31639;&#35780;&#20272;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;2D&#25991;&#26412;&#31354;&#38388;&#21270;&#30340;&#20027;&#39064;&#27169;&#22411;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#35821;&#26009;&#24211;&#20998;&#26512;&#25552;&#20379;&#20102;&#20855;&#26377;&#39640;&#36136;&#37327;&#24067;&#23616;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#27169;&#22411;&#26159;&#19968;&#31867;&#26080;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30340;&#35821;&#20041;&#32467;&#26500;&#12290;&#19982;&#21518;&#32493;&#30340;&#38477;&#32500;&#31639;&#27861;&#19968;&#36215;&#65292;&#20027;&#39064;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#20026;&#25991;&#26412;&#35821;&#26009;&#24211;&#23548;&#20986;&#31354;&#38388;&#21270;&#30340;&#20108;&#32500;&#25955;&#28857;&#22270;&#65292;&#21453;&#26144;&#25991;&#26723;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24182;&#25903;&#25345;&#35821;&#26009;&#24211;&#20998;&#26512;&#12290;&#23613;&#31649;&#20027;&#39064;&#27169;&#22411;&#12289;&#38477;&#32500;&#31639;&#27861;&#21450;&#20854;&#24213;&#23618;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#23545;&#20135;&#29983;&#30340;&#24067;&#23616;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#21738;&#31181;&#29305;&#23450;&#32452;&#21512;&#33021;&#22815;&#24471;&#21040;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#24067;&#23616;&#65292;&#20934;&#30830;&#24230;&#21644;&#24863;&#30693;&#24230;&#25351;&#26631;&#26041;&#38754;&#12290;&#20026;&#20102;&#30740;&#31350;&#20027;&#39064;&#27169;&#22411;&#21644;&#38477;&#32500;&#26041;&#27861;&#22312;&#20316;&#20026;&#20108;&#32500;&#25955;&#28857;&#22270;&#30340;&#35821;&#26009;&#24211;&#31354;&#38388;&#21270;&#65288;&#25110;&#20316;&#20026;&#26223;&#35266;&#31867;&#22411;&#21487;&#35270;&#21270;&#30340;&#22522;&#30784;&#65289;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#20934;&#30340;&#22823;&#35268;&#27169;&#35745;&#31639;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#21253;&#25324;&#65288;1&#65289;&#19968;&#32452;&#35821;&#26009;&#24211;&#65292;&#65288;2&#65289;&#19968;&#32452;&#24067;&#23616;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic models are a class of unsupervised learning algorithms for detecting the semantic structure within a text corpus. Together with a subsequent dimensionality reduction algorithm, topic models can be used for deriving spatializations for text corpora as two-dimensional scatter plots, reflecting semantic similarity between the documents and supporting corpus analysis. Although the choice of the topic model, the dimensionality reduction, and their underlying hyperparameters significantly impact the resulting layout, it is unknown which particular combinations result in high-quality layouts with respect to accuracy and perception metrics. To investigate the effectiveness of topic models and dimensionality reduction methods for the spatialization of corpora as two-dimensional scatter plots (or basis for landscape-type visualizations), we present a large-scale, benchmark-based computational evaluation. Our evaluation consists of (1) a set of corpora, (2) a set of layout algorithms that a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.11768</link><description>&lt;p&gt;
&#38382;&#39064;&#20998;&#35299;&#25552;&#39640;&#20102;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;
&lt;/p&gt;
&lt;p&gt;
Question Decomposition Improves the Faithfulness of Model-Generated Reasoning. (arXiv:2307.11768v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11768
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25191;&#34892;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#39564;&#35777;&#20854;&#34892;&#20026;&#30340;&#27491;&#30830;&#24615;&#21644;&#23433;&#20840;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#20854;&#20013;&#19968;&#31181;&#35299;&#20915;&#26041;&#27861;&#26159;&#35201;&#27714;LLM&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#20197;&#36880;&#27493;&#25512;&#29702;&#30340;&#26041;&#24335;&#22806;&#21270;&#20854;&#25512;&#29702;&#36807;&#31243;&#65288;&#24605;&#32500;&#38142;&#65307;CoT&#65289;&#12290;&#25512;&#29702;&#36807;&#31243;&#21487;&#20197;&#35753;&#25105;&#20204;&#26816;&#26597;&#27169;&#22411;&#25191;&#34892;&#20219;&#21153;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20381;&#36182;&#20110;&#25152;&#38472;&#36848;&#30340;&#25512;&#29702;&#33021;&#22815;&#24544;&#23454;&#22320;&#21453;&#26144;&#27169;&#22411;&#30340;&#23454;&#38469;&#25512;&#29702;&#65292;&#32780;&#36825;&#24182;&#38750;&#24635;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#25552;&#39640;CoT&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#26469;&#29983;&#25104;&#25512;&#29702;&#12290;&#22522;&#20110;&#20998;&#35299;&#30340;&#26041;&#27861;&#22312;&#38382;&#31572;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#26377;&#26102;&#25509;&#36817;CoT&#65292;&#24182;&#22312;&#20960;&#20010;&#26368;&#36817;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#25152;&#38472;&#36848;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#12290;&#36890;&#36807;&#24378;&#21046;&#27169;&#22411;&#22312;&#21333;&#29420;&#30340;&#19978;&#19979;&#25991;&#20013;&#22238;&#31572;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25105;&#20204;&#22823;&#22823;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of CoT while improving the faithfulness of the model's stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithf
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35843;&#21462;&#29992;&#25143;&#30340;&#24515;&#26234;&#27169;&#22411;&#26469;&#27979;&#37327;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#24863;&#30693;&#20449;&#20219;&#24230;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#27169;&#31946;&#35748;&#30693;&#22270;&#26469;&#35780;&#20272;&#35299;&#37322;&#23545;&#24863;&#30693;&#20449;&#20219;&#24230;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20026;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#25552;&#20379;&#21442;&#32771;&#21644;&#25913;&#36827;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.11765</link><description>&lt;p&gt;
&#20351;&#29992;&#35843;&#21462;&#24515;&#26234;&#27169;&#22411;&#30340;&#26041;&#24335;&#26469;&#27979;&#37327;&#23545;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#20915;&#31574;&#30340;&#24863;&#30693;&#20449;&#20219;&#24230;
&lt;/p&gt;
&lt;p&gt;
Measuring Perceived Trust in XAI-Assisted Decision-Making by Eliciting a Mental Model. (arXiv:2307.11765v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11765
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35843;&#21462;&#29992;&#25143;&#30340;&#24515;&#26234;&#27169;&#22411;&#26469;&#27979;&#37327;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#24863;&#30693;&#20449;&#20219;&#24230;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#27169;&#31946;&#35748;&#30693;&#22270;&#26469;&#35780;&#20272;&#35299;&#37322;&#23545;&#24863;&#30693;&#20449;&#20219;&#24230;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20026;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#25552;&#20379;&#21442;&#32771;&#21644;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#23454;&#35777;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#27979;&#37327;&#29992;&#25143;&#23545;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#24863;&#30693;&#20449;&#20219;&#24230;&#12290;&#21033;&#29992;&#27169;&#31946;&#35748;&#30693;&#22270;&#26469;&#33719;&#21462;&#29992;&#25143;&#30340;&#24515;&#26234;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23558;&#30097;&#20284;COVID-19&#24739;&#32773;&#20998;&#31867;&#20026;&#38451;&#24615;&#25110;&#38452;&#24615;&#12290;&#28982;&#21518;&#65292;&#21307;&#30103;&#19987;&#23478;&#26681;&#25454;&#20182;&#20204;&#30340;&#19987;&#19994;&#30693;&#35782;&#20197;&#21450;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#25552;&#20379;&#30340;&#39044;&#27979;&#21644;&#35299;&#37322;&#36827;&#34892;&#35786;&#26029;&#20915;&#31574;&#20219;&#21153;&#12290;&#20026;&#20102;&#35780;&#20272;&#35299;&#37322;&#23545;&#24863;&#30693;&#20449;&#20219;&#24230;&#30340;&#24433;&#21709;&#65292;&#21307;&#30103;&#19987;&#23478;&#36890;&#36807;&#35843;&#26597;&#23545;&#35299;&#37322;&#28385;&#24847;&#24230;&#36827;&#34892;&#35780;&#20998;&#12290;&#28982;&#21518;&#65292;&#23558;&#20854;&#20316;&#20026;&#27169;&#31946;&#35748;&#30693;&#22270;&#30340;&#27010;&#24565;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#23545;&#24444;&#27492;&#20197;&#21450;&#26368;&#32456;&#23545;&#24863;&#30693;&#20449;&#20219;&#24230;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#32771;&#34385;&#21307;&#30103;&#19987;&#23478;&#30340;&#20027;&#35266;&#24615;&#65292;&#20351;&#29992;&#27169;&#31946;&#35821;&#35328;&#21464;&#37327;&#26469;&#30830;&#23450;&#24433;&#21709;&#30340;&#24378;&#24230;&#12290;&#22312;&#27169;&#31946;&#35748;&#30693;&#22270;&#36798;&#21040;&#31283;&#23450;&#29366;&#24577;&#21518;&#65292;&#23558;&#33719;&#24471;&#19968;&#20010;&#37327;&#21270;&#20540;&#26469;&#34913;&#37327;&#24863;&#30693;&#20449;&#20219;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This empirical study proposes a novel methodology to measure users' perceived trust in an Explainable Artificial Intelligence (XAI) model. To do so, users' mental models are elicited using Fuzzy Cognitive Maps (FCMs). First, we exploit an interpretable Machine Learning (ML) model to classify suspected COVID-19 patients into positive or negative cases. Then, Medical Experts' (MEs) conduct a diagnostic decision-making task based on their knowledge and then prediction and interpretations provided by the XAI model. In order to evaluate the impact of interpretations on perceived trust, explanation satisfaction attributes are rated by MEs through a survey. Then, they are considered as FCM's concepts to determine their influences on each other and, ultimately, on the perceived trust. Moreover, to consider MEs' mental subjectivity, fuzzy linguistic variables are used to determine the strength of influences. After reaching the steady state of FCMs, a quantified value is obtained to measure the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#35760;&#24518;&#22686;&#24378;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20219;&#21153;&#20043;&#38388;&#24314;&#31435;&#21452;&#21521;&#20869;&#23384;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#26356;&#20934;&#30830;&#22320;&#25191;&#34892;&#25991;&#26723;&#32423;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#38382;&#39064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;BioCreative V CDR&#35821;&#26009;&#24211;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11762</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#35760;&#24518;&#22686;&#24378;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Similarity-based Memory Enhanced Joint Entity and Relation Extraction. (arXiv:2307.11762v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#35760;&#24518;&#22686;&#24378;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20219;&#21153;&#20043;&#38388;&#24314;&#31435;&#21452;&#21521;&#20869;&#23384;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#26356;&#20934;&#30830;&#22320;&#25191;&#34892;&#25991;&#26723;&#32423;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#38382;&#39064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;BioCreative V CDR&#35821;&#26009;&#24211;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20449;&#24687;&#25552;&#21462;&#38382;&#39064;&#65292;&#38656;&#35201;&#19968;&#20010;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#22312;&#20854;&#20013;&#19968;&#20010;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#25191;&#34892;&#22235;&#20010;&#23376;&#20219;&#21153;&#65306;&#25552;&#21450;&#26816;&#27979;&#65292;&#20849;&#25351;&#35299;&#26512;&#65292;&#23454;&#20307;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#39034;&#24207;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20854;&#20013;&#20219;&#24847;&#20998;&#35299;&#23548;&#33268;&#24403;&#21069;&#20219;&#21153;&#20165;&#20381;&#36182;&#20110;&#21069;&#19968;&#20010;&#20219;&#21153;&#65292;&#24573;&#30053;&#20102;&#23427;&#20204;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#30340;&#26356;&#22797;&#26434;&#20851;&#31995;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;&#21521;&#20869;&#23384;&#20381;&#36182;&#20851;&#31995;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#24182;&#26356;&#20934;&#30830;&#22320;&#25191;&#34892;&#32852;&#21512;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;BioCreative V CDR&#35821;&#26009;&#24211;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level joint entity and relation extraction is a challenging information extraction problem that requires a unified approach where a single neural network performs four sub-tasks: mention detection, coreference resolution, entity classification, and relation extraction. Existing methods often utilize a sequential multi-task learning approach, in which the arbitral decomposition causes the current task to depend only on the previous one, missing the possible existence of the more complex relationships between them. In this paper, we present a multi-task learning framework with bidirectional memory-like dependency between tasks to address those drawbacks and perform the joint problem more accurately. Our empirical studies show that the proposed approach outperforms the existing methods and achieves state-of-the-art results on the BioCreative V CDR corpus.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#23574;&#38160;&#24615;&#26368;&#23567;&#21270;&#31639;&#27861;&#19981;&#20165;&#20165;&#26159;&#20026;&#20102;&#26368;&#23567;&#21270;&#23574;&#38160;&#24615;&#32780;&#36798;&#21040;&#26356;&#22909;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23574;&#38160;&#24615;&#19982;&#27867;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#21462;&#20915;&#20110;&#25968;&#25454;&#20998;&#24067;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2307.11007</link><description>&lt;p&gt;
&#23574;&#38160;&#24615;&#26368;&#23567;&#21270;&#31639;&#27861;&#19981;&#20165;&#20165;&#26159;&#20026;&#20102;&#26356;&#22909;&#22320;&#27867;&#21270;&#32780;&#26368;&#23567;&#21270;&#23574;&#38160;&#24615;
&lt;/p&gt;
&lt;p&gt;
Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization. (arXiv:2307.11007v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#23574;&#38160;&#24615;&#26368;&#23567;&#21270;&#31639;&#27861;&#19981;&#20165;&#20165;&#26159;&#20026;&#20102;&#26368;&#23567;&#21270;&#23574;&#38160;&#24615;&#32780;&#36798;&#21040;&#26356;&#22909;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23574;&#38160;&#24615;&#19982;&#27867;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#21462;&#20915;&#20110;&#25968;&#25454;&#20998;&#24067;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#36807;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#27867;&#21270;&#30340;&#22522;&#26412;&#21407;&#22240;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#29616;&#26377;&#30340;&#29702;&#35770;&#34920;&#26126;&#65292;&#24120;&#35265;&#30340;&#38543;&#26426;&#20248;&#21270;&#22120;&#26356;&#20542;&#21521;&#20110;&#35757;&#32451;&#25439;&#22833;&#26356;&#24179;&#22374;&#30340;&#26368;&#23567;&#21270;&#22120;&#65292;&#22240;&#27492;&#33258;&#28982;&#32780;&#28982;&#30340;&#35299;&#37322;&#26159;&#24179;&#22374;&#24615;&#24847;&#21619;&#30528;&#27867;&#21270;&#12290;&#26412;&#25991;&#23545;&#36825;&#19968;&#35299;&#37322;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#35843;&#26597;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;&#20004;&#23618;ReLU&#32593;&#32476;&#23384;&#22312;&#20197;&#19979;&#19977;&#31181;&#24773;&#20917;&#65306;(1) &#24179;&#22374;&#24615;&#30830;&#23454;&#26263;&#31034;&#27867;&#21270;&#65307;(2) &#23384;&#22312;&#26368;&#24179;&#22374;&#30340;&#38750;&#27867;&#21270;&#27169;&#22411;&#65292;&#23574;&#38160;&#24615;&#26368;&#23567;&#21270;&#31639;&#27861;&#26080;&#27861;&#27867;&#21270;&#65307;(3) &#26356;&#21152;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23384;&#22312;&#38750;&#27867;&#21270;&#26368;&#24179;&#22374;&#30340;&#27169;&#22411;&#65292;&#20294;&#23574;&#38160;&#24615;&#26368;&#23567;&#21270;&#31639;&#27861;&#20173;&#28982;&#33021;&#22815;&#27867;&#21270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23574;&#38160;&#24615;&#19982;&#27867;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#25968;&#25454;&#20998;&#24067;&#21644;&#27169;&#22411;&#26550;&#26500;&#65292;&#23574;&#38160;&#24615;&#26368;&#23567;&#21270;&#31639;&#27861;&#19981;&#20165;&#20165;&#26159;&#20026;&#20102;&#26368;&#23567;&#21270;&#23574;&#38160;&#24615;&#32780;&#36798;&#21040;&#26356;&#22909;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite extensive studies, the underlying reason as to why overparameterized neural networks can generalize remains elusive. Existing theory shows that common stochastic optimizers prefer flatter minimizers of the training loss, and thus a natural potential explanation is that flatness implies generalization. This work critically examines this explanation. Through theoretical and empirical investigation, we identify the following three scenarios for two-layer ReLU networks: (1) flatness provably implies generalization; (2) there exist non-generalizing flattest models and sharpness minimization algorithms fail to generalize, and (3) perhaps most surprisingly, there exist non-generalizing flattest models, but sharpness minimization algorithms still generalize. Our results suggest that the relationship between sharpness and generalization subtly depends on the data distributions and the model architectures and sharpness minimization algorithms do not only minimize sharpness to achieve bet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#23457;&#26597;&#38382;&#39064;&#65292;&#25351;&#20986;&#29616;&#26377;&#30340;&#35821;&#20041;&#23457;&#26597;&#26041;&#27861;&#23384;&#22312;&#29702;&#35770;&#19978;&#30340;&#38480;&#21046;&#65292;&#30001;&#20110;LLM&#30340;&#31243;&#24207;&#21270;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#35821;&#20041;&#23457;&#26597;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#19981;&#21487;&#21028;&#23450;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#26377;&#30693;&#35782;&#30340;&#25915;&#20987;&#32773;&#21487;&#20197;&#37325;&#26500;&#19981;&#21487;&#23481;&#35768;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2307.10719</link><description>&lt;p&gt;
LLM&#23457;&#26597;&#65306;&#26426;&#22120;&#23398;&#20064;&#25361;&#25112;&#36824;&#26159;&#35745;&#31639;&#26426;&#23433;&#20840;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?. (arXiv:2307.10719v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#23457;&#26597;&#38382;&#39064;&#65292;&#25351;&#20986;&#29616;&#26377;&#30340;&#35821;&#20041;&#23457;&#26597;&#26041;&#27861;&#23384;&#22312;&#29702;&#35770;&#19978;&#30340;&#38480;&#21046;&#65292;&#30001;&#20110;LLM&#30340;&#31243;&#24207;&#21270;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#35821;&#20041;&#23457;&#26597;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#19981;&#21487;&#21028;&#23450;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#26377;&#30693;&#35782;&#30340;&#25915;&#20987;&#32773;&#21487;&#20197;&#37325;&#26500;&#19981;&#21487;&#23481;&#35768;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#29702;&#35299;&#22797;&#26434;&#25351;&#20196;&#26041;&#38754;&#23637;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#25552;&#20379;&#30340;&#25351;&#20196;&#30340;&#30450;&#30446;&#36981;&#24490;&#24341;&#21457;&#20102;&#23545;&#24694;&#24847;&#20351;&#29992;&#39118;&#38505;&#30340;&#25285;&#24551;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#22914;LLM&#30340;&#27169;&#22411;&#24494;&#35843;&#25110;&#20351;&#29992;LLM&#36827;&#34892;&#36755;&#20986;&#23457;&#26597;&#65292;&#24050;&#35777;&#26126;&#26159;&#26377;&#32570;&#38519;&#30340;&#65292;&#22240;&#20026;LLM&#20173;&#28982;&#21487;&#20197;&#29983;&#25104;&#26377;&#38382;&#39064;&#30340;&#22238;&#31572;&#12290;&#24120;&#29992;&#30340;&#23457;&#26597;&#26041;&#27861;&#23558;&#36825;&#20010;&#38382;&#39064;&#35270;&#20026;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#20381;&#36182;&#20110;&#21478;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;LLM&#36755;&#20986;&#20013;&#30340;&#19981;&#33391;&#20869;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#36825;&#31181;&#35821;&#20041;&#23457;&#26597;&#26041;&#27861;&#30340;&#29702;&#35770;&#38480;&#21046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35821;&#20041;&#23457;&#26597;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#19981;&#21487;&#21028;&#23450;&#30340;&#38382;&#39064;&#65292;&#31361;&#20986;&#20102;&#30001;&#20110;LLM&#30340;&#31243;&#24207;&#21270;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#32780;&#24341;&#36215;&#30340;&#23457;&#26597;&#20013;&#30340;&#22266;&#26377;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#25361;&#25112;&#19981;&#20165;&#38480;&#20110;&#35821;&#20041;&#23457;&#26597;&#65292;&#22240;&#20026;&#26377;&#30693;&#35782;&#30340;&#25915;&#20987;&#32773;&#21487;&#20197;&#37325;&#26500;&#19981;&#21487;&#23481;&#35768;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, as LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;TwinLiteNet&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#21487;&#39537;&#21160;&#21306;&#22495;&#21644;&#36710;&#36947;&#20998;&#21106;&#12290;&#35813;&#27169;&#22411;&#25104;&#26412;&#20302;&#24265;&#19988;&#39640;&#25928;&#20934;&#30830;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#35745;&#31639;&#36164;&#28304;&#33410;&#32422;&#12290;</title><link>http://arxiv.org/abs/2307.10705</link><description>&lt;p&gt;
TwinLiteNet&#65306;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20013;&#21487;&#39537;&#21160;&#21306;&#22495;&#21644;&#36710;&#36947;&#20998;&#21106;&#30340;&#39640;&#25928;&#36731;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars. (arXiv:2307.10705v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;TwinLiteNet&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#21487;&#39537;&#21160;&#21306;&#22495;&#21644;&#36710;&#36947;&#20998;&#21106;&#12290;&#35813;&#27169;&#22411;&#25104;&#26412;&#20302;&#24265;&#19988;&#39640;&#25928;&#20934;&#30830;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#35745;&#31639;&#36164;&#28304;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#19968;&#20010;&#24120;&#35265;&#30340;&#20219;&#21153;&#65292;&#29992;&#20110;&#29702;&#35299;&#21608;&#22260;&#29615;&#22659;&#12290;&#23545;&#20110;&#36947;&#36335;&#19978;&#30340;&#23433;&#20840;&#21644;&#39640;&#25928;&#23548;&#33322;&#26469;&#35828;&#65292;&#21487;&#39537;&#21160;&#21306;&#22495;&#20998;&#21106;&#21644;&#36710;&#36947;&#26816;&#27979;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#35745;&#31639;&#24320;&#38144;&#22823;&#65292;&#38656;&#35201;&#39640;&#31471;&#30828;&#20214;&#65292;&#36825;&#23545;&#20110;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#26469;&#35828;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21487;&#39537;&#21160;&#21306;&#22495;&#21644;&#36710;&#36947;&#32447;&#20998;&#21106;&#27169;&#22411;&#12290;TwinLiteNet&#35774;&#35745;&#25104;&#25104;&#26412;&#20302;&#24265;&#65292;&#20294;&#33021;&#22815;&#23454;&#29616;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;BDD100K&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;TwinLiteNet&#65292;&#24182;&#19982;&#29616;&#20195;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;TwinLiteNet&#19982;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#30456;&#20284;&#65292;&#20294;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#26174;&#33879;&#20943;&#23569;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TwinLiteNet&#22312;&#21487;&#39537;&#21160;&#21306;&#22495;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;91.3%&#30340;mIoU&#35780;&#20998;&#65292;&#22312;&#36710;&#36947;&#26816;&#27979;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;31.08%&#30340;IoU&#35780;&#20998;&#65292;&#20165;&#20351;&#29992;&#20102;40&#19975;&#20010;&#21442;&#25968;&#65292;&#22312;GPU RTX&#19978;&#23454;&#29616;&#20102;415 FPS&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation is a common task in autonomous driving to understand the surrounding environment. Driveable Area Segmentation and Lane Detection are particularly important for safe and efficient navigation on the road. However, original semantic segmentation models are computationally expensive and require high-end hardware, which is not feasible for embedded systems in autonomous vehicles. This paper proposes a lightweight model for the driveable area and lane line segmentation. TwinLiteNet is designed cheaply but achieves accurate and efficient segmentation results. We evaluate TwinLiteNet on the BDD100K dataset and compare it with modern models. Experimental results show that our TwinLiteNet performs similarly to existing approaches, requiring significantly fewer computational resources. Specifically, TwinLiteNet achieves a mIoU score of 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task with only 0.4 million parameters and achieves 415 FPS on GPU RTX 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#65292;&#24182;&#36890;&#36807;&#22312;&#39184;&#39302;&#35780;&#35770;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10617</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#26816;&#27979;&#34394;&#20551;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Detecting deceptive reviews using text classification. (arXiv:2307.10617v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10617
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#65292;&#24182;&#36890;&#36807;&#22312;&#39184;&#39302;&#35780;&#35770;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#32447;&#35780;&#35770;&#22312;&#25512;&#24191;&#20219;&#20309;&#20135;&#21697;&#25110;&#26381;&#21153;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20225;&#19994;&#21487;&#33021;&#20250;&#23884;&#20837;&#34394;&#20551;&#35780;&#35770;&#20197;&#21560;&#24341;&#23458;&#25143;&#36141;&#20080;&#20182;&#20204;&#30340;&#20135;&#21697;&#12290;&#20182;&#20204;&#29978;&#33267;&#21487;&#33021;&#31361;&#20986;&#24378;&#35843;&#33258;&#24049;&#20135;&#21697;&#30340;&#20248;&#28857;&#25110;&#25209;&#35780;&#31454;&#20105;&#23545;&#25163;&#30340;&#20135;&#21697;&#12290;&#24066;&#22330;&#33829;&#38144;&#20154;&#21592;&#12289;&#24191;&#21578;&#21830;&#21644;&#20854;&#20182;&#22312;&#32447;&#21830;&#19994;&#29992;&#25143;&#26377;&#21160;&#26426;&#20026;&#20182;&#20204;&#24819;&#35201;&#25512;&#24191;&#30340;&#20135;&#21697;&#32534;&#20889;&#34394;&#20551;&#30340;&#27491;&#38754;&#35780;&#35770;&#65292;&#25110;&#32773;&#20026;&#20182;&#20204;&#30495;&#27491;&#19981;&#21916;&#27426;&#30340;&#20135;&#21697;&#25552;&#20379;&#34394;&#20551;&#30340;&#36127;&#38754;&#35780;&#35770;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#26159;&#19968;&#20010;&#32039;&#36843;&#19988;&#25345;&#32493;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26041;&#27861;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#12290;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#19968;&#20010;&#39184;&#39302;&#35780;&#35770;&#30340;&#34394;&#20551;&#24847;&#35265;&#22403;&#22334;&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22810;&#27425;&#23454;&#39564;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;n-gram&#27169;&#22411;&#21644;&#26368;&#22823;&#29305;&#24449;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, online reviews play a vital role for promoting any kind of product or services. Businesses may embed fake reviews in order to attract customers to purchase their products. They may even highlight the benefits of their own product or criticize the competition's product. Marketers, advertisers, and other online business users have incentive to create fake positive reviews for products which they want to promote or give fake negative reviews for products which they really don't like. So now-a-days writing a deceptive review is inevitable thing for promoting their own business or degrading competitor's reputation. Thus, identifying deceptive reviews is an intense and on-going research area. This research paper proposes machine learning model approach to identify deceptive reviews. The paper investigates the performance of the several experiments done on a Deceptive Opinion Spam Corpus dataset of restaurants reviews. We developed a n-gram model and max features to identify 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#35268;&#21270;&#21160;&#24577;&#22270;&#23398;&#20064;&#26469;&#39044;&#27979;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#24037;&#20316;&#36127;&#33655;&#27700;&#24179;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#36864;&#20241;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#27169;&#25311;&#65292;&#21033;&#29992;&#31354;&#20013;&#20132;&#36890;&#25968;&#25454;&#21644;&#24037;&#20316;&#36127;&#33655;&#26631;&#31614;&#36827;&#34892;&#39044;&#27979;&#21644;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.10559</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#35268;&#21270;&#21160;&#24577;&#22270;&#23398;&#20064;&#39044;&#27979;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#24037;&#20316;&#36127;&#33655;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Air Traffic Controller Workload Level Prediction using Conformalized Dynamical Graph Learning. (arXiv:2307.10559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#35268;&#21270;&#21160;&#24577;&#22270;&#23398;&#20064;&#26469;&#39044;&#27979;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#24037;&#20316;&#36127;&#33655;&#27700;&#24179;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#36864;&#20241;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#27169;&#25311;&#65292;&#21033;&#29992;&#31354;&#20013;&#20132;&#36890;&#25968;&#25454;&#21644;&#24037;&#20316;&#36127;&#33655;&#26631;&#31614;&#36827;&#34892;&#39044;&#27979;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#26159;&#19968;&#20010;&#23433;&#20840;&#20851;&#38190;&#30340;&#26381;&#21153;&#31995;&#32479;&#65292;&#35201;&#27714;&#22320;&#38754;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#65288;ATCo&#65289;&#26102;&#21051;&#20851;&#27880;&#20197;&#32500;&#25345;&#26085;&#24120;&#33322;&#31354;&#36816;&#33829;&#12290;ATCo&#30340;&#24037;&#20316;&#36127;&#33655;&#21487;&#33021;&#23545;&#36816;&#33829;&#23433;&#20840;&#21644;&#31354;&#22495;&#20351;&#29992;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#36991;&#20813;&#36807;&#36733;&#24182;&#30830;&#20445;ATCo&#30340;&#21487;&#25509;&#21463;&#24037;&#20316;&#36127;&#33655;&#27700;&#24179;&#65292;&#20934;&#30830;&#39044;&#27979;ATCo&#30340;&#24037;&#20316;&#36127;&#33655;&#23545;&#20110;&#37319;&#21462;&#32531;&#35299;&#25514;&#26045;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;ATCo&#24037;&#20316;&#36127;&#33655;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#20027;&#35201;&#20174;&#31354;&#20013;&#20132;&#36890;&#30340;&#35282;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31616;&#35201;&#20171;&#32461;&#20102;&#19982;&#36864;&#20241;ATCo&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#27169;&#25311;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#33719;&#24471;&#20102;&#31354;&#20013;&#20132;&#36890;&#25968;&#25454;&#21644;&#24037;&#20316;&#36127;&#33655;&#26631;&#31614;&#12290;&#27169;&#25311;&#22312;&#19977;&#31181;&#33778;&#23612;&#20811;&#26031;&#25509;&#36817;&#22330;&#26223;&#19979;&#36827;&#34892;&#65292;&#35201;&#27714;&#20154;&#31867;ATCo&#33258;&#25105;&#35780;&#20272;&#20854;&#24037;&#20316;&#36127;&#33655;&#35780;&#32423;&#65288;&#21363;&#65292;&#20302;-1&#21040;&#39640;-7&#65289;&#12290;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#25968;&#25454;&#20998;&#26512;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#21512;&#35268;&#21270;&#39044;&#27979;&#65292;&#26469;&#23545;ATCo&#30340;&#24037;&#20316;&#36127;&#33655;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Air traffic control (ATC) is a safety-critical service system that demands constant attention from ground air traffic controllers (ATCos) to maintain daily aviation operations. The workload of the ATCos can have negative effects on operational safety and airspace usage. To avoid overloading and ensure an acceptable workload level for the ATCos, it is important to predict the ATCos' workload accurately for mitigation actions. In this paper, we first perform a review of research on ATCo workload, mostly from the air traffic perspective. Then, we briefly introduce the setup of the human-in-the-loop (HITL) simulations with retired ATCos, where the air traffic data and workload labels are obtained. The simulations are conducted under three Phoenix approach scenarios while the human ATCos are requested to self-evaluate their workload ratings (i.e., low-1 to high-7). Preliminary data analysis is conducted. Next, we propose a graph-based deep-learning framework with conformal prediction to ide
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2307.10490</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#22768;&#38899;&#30340;&#28389;&#29992;&#29992;&#20110;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. (arXiv:2307.10490v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25552;&#31034;&#21644;&#25351;&#20196;&#27880;&#20837;&#12290;&#25915;&#20987;&#32773;&#29983;&#25104;&#19982;&#25552;&#31034;&#30456;&#23545;&#24212;&#30340;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#12290;&#24403;&#29992;&#25143;&#21521;&#65288;&#26410;&#20462;&#25913;&#30340;&#33391;&#24615;&#65289;&#27169;&#22411;&#35810;&#38382;&#34987;&#25200;&#21160;&#30340;&#22270;&#20687;&#25110;&#38899;&#39057;&#26102;&#65292;&#25200;&#21160;&#20250;&#24341;&#23548;&#27169;&#22411;&#36755;&#20986;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#25991;&#26412;&#21644;/&#25110;&#20351;&#21518;&#32493;&#23545;&#35805;&#36981;&#24490;&#25915;&#20987;&#32773;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#29992;&#20960;&#20010;&#27010;&#24565;&#39564;&#35777;&#31034;&#20363;&#38024;&#23545;LLaVa&#21644;PandaGPT&#26469;&#35828;&#26126;&#36825;&#31181;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#31185;&#23398;&#30340;&#20215;&#20540;&#21462;&#21521;&#65292;&#25506;&#35752;&#20102;&#20854;&#29305;&#24449;&#21644;&#20316;&#29992;&#12290;&#25968;&#25454;&#31185;&#23398;&#19981;&#26159;&#19968;&#38376;&#31185;&#23398;&#65292;&#32780;&#26159;&#19968;&#31181;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#37325;&#22823;&#30340;&#24433;&#21709;&#65292;&#20294;&#20063;&#23384;&#22312;&#30528;&#26410;&#30693;&#30340;&#39118;&#38505;&#12290;&#36825;&#19968;&#39046;&#22495;&#20173;&#28982;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.10460</link><description>&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#30340;&#20215;&#20540;&#21462;&#21521;&#65306;&#25968;&#25454;&#31185;&#23398;&#30340;&#26412;&#36136;&#12289;&#20215;&#20540;&#21644;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
A data science axiology: the nature, value, and risks of data science. (arXiv:2307.10460v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10460
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#31185;&#23398;&#30340;&#20215;&#20540;&#21462;&#21521;&#65292;&#25506;&#35752;&#20102;&#20854;&#29305;&#24449;&#21644;&#20316;&#29992;&#12290;&#25968;&#25454;&#31185;&#23398;&#19981;&#26159;&#19968;&#38376;&#31185;&#23398;&#65292;&#32780;&#26159;&#19968;&#31181;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#37325;&#22823;&#30340;&#24433;&#21709;&#65292;&#20294;&#20063;&#23384;&#22312;&#30528;&#26410;&#30693;&#30340;&#39118;&#38505;&#12290;&#36825;&#19968;&#39046;&#22495;&#20173;&#28982;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#19981;&#26159;&#19968;&#38376;&#31185;&#23398;&#65292;&#32780;&#26159;&#19968;&#31181;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#26080;&#27861;&#39044;&#27979;&#30340;&#33539;&#22260;&#12289;&#35268;&#27169;&#12289;&#22797;&#26434;&#24615;&#21644;&#30693;&#35782;&#21457;&#29616;&#33021;&#21147;&#65292;&#36825;&#26159;&#20854;&#20182;&#26041;&#24335;&#26080;&#27861;&#23454;&#29616;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#36229;&#20986;&#20154;&#31867;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#23427;&#24050;&#32463;&#22312;AI&#20891;&#22791;&#31454;&#36187;&#20013;&#24191;&#27867;&#24212;&#29992;&#20110;&#25968;&#20197;&#19975;&#35745;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#24050;&#32463;&#23454;&#36136;&#24615;&#22320;&#25913;&#21464;&#20102;&#25105;&#20204;&#30340;&#19990;&#30028;&#65292;&#20294;&#30001;&#20110;&#20854;&#19981;&#21487;&#24605;&#35758;&#30340;&#22797;&#26434;&#24615;&#65292;&#21487;&#33021;&#24102;&#26469;&#26410;&#30693;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#31185;&#23398;&#30340;&#20215;&#20540;&#21462;&#21521;&#65292;&#25506;&#35752;&#21644;&#35780;&#20272;&#20102;&#20854;&#26174;&#33879;&#32780;&#20915;&#23450;&#24615;&#30340;&#29305;&#24449;&#65292;&#20197;&#20415;&#20102;&#35299;&#21644;&#23450;&#20041;&#25968;&#25454;&#31185;&#23398;&#65292;&#35748;&#35782;&#21040;&#20854;&#28508;&#22312;&#30340;&#30410;&#22788;&#12289;&#39118;&#38505;&#21644;&#24320;&#25918;&#24615;&#30740;&#31350;&#25361;&#25112;&#12290;&#22522;&#20110;AI&#30340;&#25968;&#25454;&#31185;&#23398;&#26412;&#36136;&#19978;&#28041;&#21450;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#21487;&#33021;&#27604;&#25105;&#20204;&#23545;&#31185;&#23398;&#30830;&#23450;&#24615;&#30340;&#20559;&#22909;&#26356;&#21152;&#29616;&#23454;&#12290;&#25968;&#25454;&#31185;&#23398;&#23558;&#20135;&#29983;&#36828;&#36828;&#36229;&#20986;&#30693;&#35782;&#21457;&#29616;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data science is not a science. It is a research paradigm with an unfathomed scope, scale, complexity, and power for knowledge discovery that is not otherwise possible and can be beyond human reasoning. It is changing our world practically and profoundly already widely deployed in tens of thousands of applications in every discipline in an AI Arms Race that, due to its inscrutability, can lead to unfathomed risks. This paper presents an axiology of data science, its purpose, nature, importance, risks, and value for problem solving, by exploring and evaluating its remarkable, definitive features. As data science is in its infancy, this initial, speculative axiology is intended to aid in understanding and defining data science to recognize its potential benefits, risks, and open research challenges. AI based data science is inherently about uncertainty that may be more realistic than our preference for the certainty of science. Data science will have impacts far beyond knowledge discovery
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#20351;&#29992;&#22270;&#22686;&#24378;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#20174;&#24322;&#26500;&#22270;&#20013;&#23548;&#20986;&#30340;&#25512;&#29702;&#30693;&#35782;&#25972;&#21512;&#21040;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#27880;&#24847;&#21147;&#12289;&#22270;&#27880;&#24847;&#21147;&#21644;&#20851;&#31995;&#31867;&#22411;&#32771;&#34385;&#65292;&#20248;&#21270;&#20102;&#23454;&#20307;&#21644;&#21333;&#35789;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#12290;&#35813;&#27169;&#24335;&#19982;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#19982;LUKE&#30340;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#38598;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.10443</link><description>&lt;p&gt;
&#20351;&#29992;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#23558;&#24322;&#26500;&#22270;&#19982;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#30456;&#32467;&#21512;&#30340;&#38405;&#35835;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model. (arXiv:2307.10443v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#20351;&#29992;&#22270;&#22686;&#24378;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#20174;&#24322;&#26500;&#22270;&#20013;&#23548;&#20986;&#30340;&#25512;&#29702;&#30693;&#35782;&#25972;&#21512;&#21040;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#27880;&#24847;&#21147;&#12289;&#22270;&#27880;&#24847;&#21147;&#21644;&#20851;&#31995;&#31867;&#22411;&#32771;&#34385;&#65292;&#20248;&#21270;&#20102;&#23454;&#20307;&#21644;&#21333;&#35789;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#12290;&#35813;&#27169;&#24335;&#19982;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#19982;LUKE&#30340;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#36755;&#20837;&#24207;&#21015;&#20013;&#32570;&#23569;&#26174;&#24335;&#30693;&#35782;&#65292;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#26469;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#23427;&#21033;&#29992;&#22686;&#24378;&#22270;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#30001;&#24322;&#26500;&#22270;&#23548;&#20986;&#30340;&#25512;&#29702;&#30693;&#35782;&#25972;&#21512;&#21040;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#12290;&#25552;&#20986;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#35201;&#32032;&#65306;&#21333;&#35789;&#26631;&#35760;&#30340;&#20840;&#23616;-&#23616;&#37096;&#27880;&#24847;&#21147;&#65292;&#23545;&#23454;&#20307;&#26631;&#35760;&#30340;&#22270;&#27880;&#24847;&#21147;&#65292;&#23454;&#20307;&#26631;&#35760;&#23545;&#30456;&#20851;&#32852;&#30340;&#26631;&#35760;&#26174;&#31034;&#24378;&#28872;&#30340;&#27880;&#24847;&#21147;&#32780;&#23545;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#26174;&#31034;&#36739;&#24369;&#30340;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#32771;&#34385;&#27599;&#20010;&#23454;&#20307;&#26631;&#35760;&#19982;&#21333;&#35789;&#26631;&#35760;&#20043;&#38388;&#30340;&#20851;&#31995;&#31867;&#22411;&#12290;&#36825;&#26679;&#65292;&#22914;&#26524;&#23384;&#22312;&#20851;&#31995;&#65292;&#21017;&#21487;&#20197;&#20248;&#21270;&#20004;&#32773;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#12290;&#35813;&#27169;&#24335;&#19982;&#29305;&#27530;&#30340;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;LUKE&#30340;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the significant progress made by transformer models in machine reading comprehension tasks, they still face limitations in handling complex reasoning tasks due to the absence of explicit knowledge in the input sequence. This paper proposes a novel attention pattern to overcome this limitation, which integrates reasoning knowledge derived from a heterogeneous graph into the transformer architecture using a graph-enhanced self-attention mechanism. The proposed attention pattern comprises three key elements: global-local attention for word tokens, graph attention for entity tokens that exhibit strong attention towards tokens connected in the graph as opposed to those unconnected, and the consideration of the type of relationship between each entity token and word token. This results in optimized attention between the two if a relationship exists. The pattern is coupled with special relative position labels, allowing it to integrate with LUKE's entity-aware self-attention mechanism
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;GPT&#36827;&#34892;&#39640;&#32423;&#24773;&#24863;&#20998;&#26512;&#65292;&#24182;&#32771;&#23519;&#20854;&#19982;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#26041;&#27861;&#30456;&#36739;&#20110;&#20854;&#20182;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#24182;&#26377;&#25928;&#35299;&#20915;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#26816;&#27979;&#35773;&#21050;&#12290;</title><link>http://arxiv.org/abs/2307.10234</link><description>&lt;p&gt;
SentimentGPT&#65306;&#21033;&#29992;GPT&#36827;&#34892;&#39640;&#32423;&#24773;&#24863;&#20998;&#26512;&#21450;&#20854;&#19982;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning. (arXiv:2307.10234v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;GPT&#36827;&#34892;&#39640;&#32423;&#24773;&#24863;&#20998;&#26512;&#65292;&#24182;&#32771;&#23519;&#20854;&#19982;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#26041;&#27861;&#30456;&#36739;&#20110;&#20854;&#20182;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#24182;&#26377;&#25928;&#35299;&#20915;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#26816;&#27979;&#35773;&#21050;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#24773;&#24863;&#20998;&#26512;&#20013;&#21508;&#31181;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32771;&#23519;&#65292;&#29305;&#21035;&#26159;&#22312;SemEval 2017&#25968;&#25454;&#38598;&#30340;&#20219;&#21153;4&#20013;&#12290;&#37319;&#29992;&#20102;&#19977;&#31181;&#20027;&#35201;&#31574;&#30053;&#65306;1&#65289;&#20351;&#29992;GPT-3.5 Turbo&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#65292;2&#65289;&#23545;GPT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;3&#65289;&#37319;&#29992;&#21019;&#26032;&#30340;&#23884;&#20837;&#20998;&#31867;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#36825;&#20123;&#31574;&#30053;&#21644;&#20010;&#21035;GPT&#27169;&#22411;&#20043;&#38388;&#30340;&#35814;&#32454;&#27604;&#36739;&#35265;&#35299;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#29420;&#29305;&#30340;&#20248;&#21183;&#21644;&#28508;&#22312;&#30340;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#23558;&#36825;&#20123;&#22522;&#20110;GPT&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#21516;&#26102;&#20195;&#12289;&#39640;&#24615;&#33021;&#30340;&#27169;&#22411;&#22312;&#30456;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GPT&#26041;&#27861;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;F1&#20998;&#25968;&#22686;&#21152;&#20102;22%&#20197;&#19978;&#12290;&#27492;&#22806;&#65292;&#26412;&#35770;&#25991;&#36824;&#25506;&#35752;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#24120;&#35265;&#25361;&#25112;&#65292;&#22914;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#26816;&#27979;&#35773;&#21050;&#12290;&#30740;&#31350;&#24378;&#35843;&#20102;GPT&#26041;&#27861;&#30340;&#37325;&#35201;&#20215;&#20540;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a thorough examination of various Generative Pretrained Transformer (GPT) methodologies in sentiment analysis, specifically in the context of Task 4 on the SemEval 2017 dataset. Three primary strategies are employed: 1) prompt engineering using the advanced GPT-3.5 Turbo, 2) fine-tuning GPT models, and 3) an inventive approach to embedding classification. The research yields detailed comparative insights among these strategies and individual GPT models, revealing their unique strengths and potential limitations. Additionally, the study compares these GPT-based methodologies with other contemporary, high-performing models previously used with the same dataset. The results illustrate the significant superiority of the GPT approaches in terms of predictive performance, more than 22% in F1-score compared to the state-of-the-art. Further, the paper addresses common challenges in sentiment analysis tasks, such as understanding context and detecting sarcasm. It underscores
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#30005;&#36335;&#20998;&#26512;&#22312;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#23545;70B&#27611;&#20011;&#40736;&#27169;&#22411;&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#36923;&#36753;&#23618;&#24402;&#22240;&#21644;&#28608;&#27963;&#20462;&#34917;&#25216;&#26415;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#22836;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2307.09458</link><description>&lt;p&gt;
&#30005;&#36335;&#20998;&#26512;&#30340;&#21487;&#35299;&#37322;&#24615;&#26159;&#21542;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65311;&#26469;&#33258;&#27611;&#20011;&#40736;&#20013;&#22810;&#39033;&#36873;&#25321;&#33021;&#21147;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla. (arXiv:2307.09458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#30005;&#36335;&#20998;&#26512;&#22312;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#23545;70B&#27611;&#20011;&#40736;&#27169;&#22411;&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#36923;&#36753;&#23618;&#24402;&#22240;&#21644;&#28608;&#27963;&#20462;&#34917;&#25216;&#26415;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#22836;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#36335;&#20998;&#26512;&#26159;&#19968;&#31181;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#26426;&#21046;&#30340;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20998;&#26512;&#37117;&#26159;&#22312;&#36828;&#31163;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#23567;&#22411;&#27169;&#22411;&#20013;&#36827;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;70B&#27611;&#20011;&#40736;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#19968;&#39033;&#26696;&#20363;&#30740;&#31350;&#65292;&#26088;&#22312;&#27979;&#35797;&#30005;&#36335;&#20998;&#26512;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#35843;&#26597;&#20102;&#27611;&#20011;&#40736;&#22312;&#30693;&#36947;&#27491;&#30830;&#31572;&#26696;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#33021;&#22815;&#35782;&#21035;&#20986;&#27491;&#30830;&#31572;&#26696;&#26631;&#31614;&#12290;&#25105;&#20204;&#21457;&#29616;&#24050;&#26377;&#30340;&#36923;&#36753;&#23618;&#24402;&#22240;&#12289;&#27880;&#24847;&#21147;&#27169;&#24335;&#21487;&#35270;&#21270;&#21644;&#28608;&#27963;&#20462;&#34917;&#25216;&#26415;&#22312;&#27611;&#20011;&#40736;&#27169;&#22411;&#20013;&#20855;&#26377;&#33258;&#28982;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#21644;&#20998;&#31867;&#19968;&#23567;&#32452;&#8220;&#36755;&#20986;&#33410;&#28857;&#8221;&#65288;&#27880;&#24847;&#21147;&#22836;&#21644;&#22810;&#23618;&#24863;&#30693;&#26426;&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#8220;&#27491;&#30830;&#23383;&#27597;&#8221;&#31867;&#21035;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26088;&#22312;&#20102;&#35299;&#20854;&#29305;&#24449;&#30340;&#35821;&#20041;&#65292;&#32467;&#26524;&#26377;&#25152;&#19981;&#21516;&#12290;&#23545;&#20110;&#27491;&#24120;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#25105;&#20204;&#26174;&#33879;&#21387;&#32553;&#20102;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
\emph{Circuit analysis} is a promising technique for understanding the internal mechanisms of language models. However, existing analyses are done in small models far from the state of the art. To address this, we present a case study of circuit analysis in the 70B Chinchilla model, aiming to test the scalability of circuit analysis. In particular, we study multiple-choice question answering, and investigate Chinchilla's capability to identify the correct answer \emph{label} given knowledge of the correct answer \emph{text}. We find that the existing techniques of logit attribution, attention pattern visualization, and activation patching naturally scale to Chinchilla, allowing us to identify and categorize a small set of `output nodes' (attention heads and MLPs).  We further study the `correct letter' category of attention heads aiming to understand the semantics of their features, with mixed results. For normal multiple-choice question answers, we significantly compress the query, ke
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#26368;&#23567;&#38169;&#35823;&#29109;&#20934;&#21017;&#22312;&#22788;&#29702;&#38750;&#39640;&#26031;&#22122;&#22768;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#23454;&#38469;&#36716;&#31227;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#29992;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22522;&#26412;&#36716;&#31227;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#36890;&#36807;&#29992;&#26368;&#23567;&#38169;&#35823;&#29109;&#20195;&#26367;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#65292;&#21487;&#20197;&#21462;&#24471;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.08572</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#26368;&#23567;&#38169;&#35823;&#29109;&#20934;&#21017;&#30340;&#40065;&#26834;&#24615;&#65306;&#36716;&#31227;&#23398;&#20064;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Robustness of the Minimum Error Entropy Criterion: A Transfer Learning Case Study. (arXiv:2307.08572v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#26368;&#23567;&#38169;&#35823;&#29109;&#20934;&#21017;&#22312;&#22788;&#29702;&#38750;&#39640;&#26031;&#22122;&#22768;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#23454;&#38469;&#36716;&#31227;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#29992;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22522;&#26412;&#36716;&#31227;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#36890;&#36807;&#29992;&#26368;&#23567;&#38169;&#35823;&#29109;&#20195;&#26367;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#65292;&#21487;&#20197;&#21462;&#24471;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#23545;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#26159;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20197;&#20415;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#20551;&#35774;&#25968;&#25454;&#19981;&#21253;&#21547;&#22122;&#22768;&#65292;&#35201;&#20040;&#37319;&#29992;&#22797;&#26434;&#30340;&#35757;&#32451;&#33539;&#24335;&#25110;&#27169;&#22411;&#35774;&#35745;&#26469;&#22788;&#29702;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#32479;&#35745;&#20449;&#21495;&#22788;&#29702;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26368;&#23567;&#38169;&#35823;&#29109;&#65288;MEE&#65289;&#20934;&#21017;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#30740;&#31350;&#20854;&#22312;&#23454;&#38469;&#36716;&#31227;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#29992;&#24615;&#65292;&#20854;&#20013;&#20998;&#24067;&#36716;&#31227;&#26159;&#24120;&#35265;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;MEE&#23545;&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#36890;&#36807;&#31616;&#21333;&#22320;&#23558;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#25439;&#22833;&#26367;&#25442;&#20026;MEE&#65292;&#22312;&#22522;&#26412;&#30340;&#36716;&#31227;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914;&#24494;&#35843;&#21644;&#32447;&#24615;&#25506;&#27979;&#65289;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#19982;&#29616;&#26377;&#26041;&#27861;&#31454;&#20105;&#21147;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coping with distributional shifts is an important part of transfer learning methods in order to perform well in real-life tasks. However, most of the existing approaches in this area either focus on an ideal scenario in which the data does not contain noises or employ a complicated training paradigm or model design to deal with distributional shifts. In this paper, we revisit the robustness of the minimum error entropy (MEE) criterion, a widely used objective in statistical signal processing to deal with non-Gaussian noises, and investigate its feasibility and usefulness in real-life transfer learning regression tasks, where distributional shifts are common. Specifically, we put forward a new theoretical result showing the robustness of MEE against covariate shift. We also show that by simply replacing the mean squared error (MSE) loss with the MEE on basic transfer learning algorithms such as fine-tuning and linear probing, we can achieve competitive performance with respect to state-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08303</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#36716;&#21270;&#20026;&#23494;&#38598;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22312;&#21521;&#37327;&#31354;&#38388;&#20013;&#27979;&#37327;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;DR&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;DR&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#20174;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#22914;MS MARCO&#65289;&#20013;&#23398;&#20064;&#65292;&#20294;&#35777;&#25454;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;DR&#27169;&#22411;&#21644;&#39046;&#22495;&#37117;&#33021;&#21516;&#31561;&#21463;&#30410;&#20110;&#36801;&#31227;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#36716;&#21521;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;DR&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#37319;&#29992;&#30340;&#30828;&#25552;&#31034;&#25110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25552;&#31034;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#30340;&#24369;&#26597;&#35810;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22686;&#24378;DR&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;&#65288;SPTAR&#65289;&#65306;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#22312;&#26377;&#38480;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#25552;&#31034;&#24341;&#23548;LLMs&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#26631;&#35760;&#24369;&#26597;&#35810;&#65292;&#20174;&#32780;&#24471;&#21040;&#36275;&#22815;&#30340;&#24369;&#25991;&#26723;-&#26597;&#35810;&#23545;&#26469;&#35757;&#32451;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;AFT&#25490;&#21517;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#28789;&#27963;&#22320;&#36827;&#34892;&#26102;&#38388;&#20107;&#20214;&#24314;&#27169;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#24182;&#20943;&#36731;&#20005;&#26684;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2307.08044</link><description>&lt;p&gt;
&#26580;&#24615;&#26102;&#38388;&#20107;&#20214;&#24314;&#27169;&#65306;&#36890;&#36807;&#25490;&#21517;&#22238;&#24402;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Towards Flexible Time-to-event Modeling: Optimizing Neural Networks via Rank Regression. (arXiv:2307.08044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;AFT&#25490;&#21517;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#28789;&#27963;&#22320;&#36827;&#34892;&#26102;&#38388;&#20107;&#20214;&#24314;&#27169;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#24182;&#20943;&#36731;&#20005;&#26684;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#20107;&#20214;&#20998;&#26512;&#65292;&#20063;&#34987;&#31216;&#20026;&#29983;&#23384;&#20998;&#26512;&#65292;&#26088;&#22312;&#26681;&#25454;&#19968;&#32452;&#29305;&#24449;&#39044;&#27979;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#38388;&#12290;&#36825;&#20010;&#39046;&#22495;&#38754;&#20020;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22788;&#29702;&#34987;&#25130;&#23614;&#30340;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#20351;&#23398;&#20064;&#31639;&#27861;&#26356;&#21152;&#22797;&#26434;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;Cox&#27604;&#20363;&#39118;&#38505;&#27169;&#22411;&#21644;&#21152;&#36895;&#22833;&#25928;&#26102;&#38388;&#65288;AFT&#65289;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#38656;&#35201;&#19968;&#20123;&#20551;&#35774;&#65292;&#22914;&#27604;&#20363;&#39118;&#38505;&#21644;&#32447;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;AFT&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#39044;&#20808;&#25351;&#23450;&#30340;&#21442;&#25968;&#20998;&#24067;&#20551;&#35774;&#12290;&#20026;&#20102;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#20943;&#36731;&#20005;&#26684;&#30340;&#20551;&#35774;&#65292;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#35768;&#22810;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21361;&#38505;&#27169;&#22411;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#25991;&#29486;&#20013;&#23545;&#20110;AFT&#30340;&#34920;&#31034;&#23398;&#20064;&#23578;&#26410;&#24191;&#27867;&#25506;&#32034;&#65292;&#23613;&#31649;&#30456;&#23545;&#20110;&#20197;&#21361;&#38505;&#20026;&#37325;&#28857;&#30340;&#26041;&#27861;&#32780;&#35328;&#65292;&#23427;&#26356;&#21152;&#31616;&#21333;&#21644;&#21487;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28145;&#24230;AFT&#25490;&#21517;&#22238;&#24402;&#27169;&#22411;&#26469;&#36827;&#34892;&#26102;&#38388;&#20107;&#20214;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-to-event analysis, also known as survival analysis, aims to predict the time of occurrence of an event, given a set of features. One of the major challenges in this area is dealing with censored data, which can make learning algorithms more complex. Traditional methods such as Cox's proportional hazards model and the accelerated failure time (AFT) model have been popular in this field, but they often require assumptions such as proportional hazards and linearity. In particular, the AFT models often require pre-specified parametric distributional assumptions. To improve predictive performance and alleviate strict assumptions, there have been many deep learning approaches for hazard-based models in recent years. However, representation learning for AFT has not been widely explored in the neural network literature, despite its simplicity and interpretability in comparison to hazard-focused methods. In this work, we introduce the Deep AFT Rank-regression model for Time-to-event predic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20113;&#28216;&#25103;&#20013;&#24674;&#22797;&#20002;&#22833;&#25110;&#25439;&#22351;&#30340;&#35270;&#39057;&#24103;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21033;&#29992;&#28216;&#25103;&#29366;&#24577;&#21644;&#37096;&#20998;&#35299;&#30721;&#24103;&#26469;&#25552;&#39640;&#24674;&#22797;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07847</link><description>&lt;p&gt;
&#20113;&#28216;&#25103;&#20013;&#30340;&#31070;&#32463;&#35270;&#39057;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Neural Video Recovery for Cloud Gaming. (arXiv:2307.07847v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20113;&#28216;&#25103;&#20013;&#24674;&#22797;&#20002;&#22833;&#25110;&#25439;&#22351;&#30340;&#35270;&#39057;&#24103;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21033;&#29992;&#28216;&#25103;&#29366;&#24577;&#21644;&#37096;&#20998;&#35299;&#30721;&#24103;&#26469;&#25552;&#39640;&#24674;&#22797;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#28216;&#25103;&#26159;&#19968;&#20010;&#20215;&#20540;&#25968;&#21313;&#20159;&#32654;&#20803;&#30340;&#34892;&#19994;&#12290;&#22312;&#20113;&#28216;&#25103;&#20013;&#65292;&#23458;&#25143;&#31471;&#23558;&#33258;&#24049;&#30340;&#31227;&#21160;&#21457;&#36865;&#21040;&#20114;&#32852;&#32593;&#19978;&#30340;&#28216;&#25103;&#26381;&#21153;&#22120;&#65292;&#26381;&#21153;&#22120;&#23558;&#28210;&#26579;&#24182;&#20256;&#36755;&#32467;&#26524;&#35270;&#39057;&#22238;&#26469;&#12290;&#20026;&#20102;&#25552;&#20379;&#33391;&#22909;&#30340;&#28216;&#25103;&#20307;&#39564;&#65292;&#38656;&#35201;&#20302;&#20110;80&#27627;&#31186;&#30340;&#24310;&#36831;&#12290;&#36825;&#24847;&#21619;&#30528;&#35270;&#39057;&#30340;&#28210;&#26579;&#12289;&#32534;&#30721;&#12289;&#20256;&#36755;&#12289;&#35299;&#30721;&#21644;&#26174;&#31034;&#24517;&#39035;&#22312;&#36825;&#20010;&#26102;&#38388;&#33539;&#22260;&#20869;&#23436;&#25104;&#65292;&#30001;&#20110;&#26381;&#21153;&#22120;&#36807;&#36733;&#12289;&#32593;&#32476;&#25317;&#22622;&#21644;&#20002;&#21253;&#31561;&#22240;&#32032;&#65292;&#36825;&#19968;&#28857;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20113;&#28216;&#25103;&#20013;&#24674;&#22797;&#20002;&#22833;&#25110;&#25439;&#22351;&#35270;&#39057;&#24103;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#35270;&#39057;&#24103;&#24674;&#22797;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#28216;&#25103;&#29366;&#24577;&#26174;&#33879;&#25552;&#21319;&#24674;&#22797;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#37096;&#20998;&#35299;&#30721;&#30340;&#24103;&#26469;&#24674;&#22797;&#20002;&#22833;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#31995;&#32479;&#65292;&#21253;&#25324;(i)&#39640;&#25928;&#25552;&#21462;&#28216;&#25103;&#29366;&#24577;&#65292;(ii)&#20462;&#25913; H.264 &#35270;&#39057;&#35299;&#30721;&#22120;&#29983;&#25104;&#19968;&#20010;&#25351;&#31034;&#38656;&#35201;&#24674;&#22797;&#35270;&#39057;&#24103;&#21738;&#20123;&#37096;&#20998;&#30340;&#25513;&#30721;&#65292;&#21644; (iii)&#35774;&#35745;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35270;&#39057;&#24103;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud gaming is a multi-billion dollar industry. A client in cloud gaming sends its movement to the game server on the Internet, which renders and transmits the resulting video back. In order to provide a good gaming experience, a latency below 80 ms is required. This means that video rendering, encoding, transmission, decoding, and display have to finish within that time frame, which is especially challenging to achieve due to server overload, network congestion, and losses. In this paper, we propose a new method for recovering lost or corrupted video frames in cloud gaming. Unlike traditional video frame recovery, our approach uses game states to significantly enhance recovery accuracy and utilizes partially decoded frames to recover lost portions. We develop a holistic system that consists of (i) efficiently extracting game states, (ii) modifying H.264 video decoder to generate a mask to indicate which portions of video frames need recovery, and (iii) designing a novel neural networ
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36328;&#25209;&#27425;&#24230;&#37327;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#23398;&#20064;&#21407;&#22411;&#30340;&#20840;&#23616;&#24179;&#22343;&#27719;&#32858;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#36890;&#29992;&#23454;&#20307;&#20197;&#34920;&#31034;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2307.07620</link><description>&lt;p&gt;
&#36890;&#36807;&#36328;&#25209;&#27425;&#24230;&#37327;&#23398;&#20064;&#23454;&#29616;&#36890;&#29992;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Generalizable Embeddings with Cross-batch Metric Learning. (arXiv:2307.07620v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07620
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36328;&#25209;&#27425;&#24230;&#37327;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#23398;&#20064;&#21407;&#22411;&#30340;&#20840;&#23616;&#24179;&#22343;&#27719;&#32858;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#36890;&#29992;&#23454;&#20307;&#20197;&#34920;&#31034;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#23616;&#24179;&#22343;&#27719;&#32858;&#65288;GAP&#65289;&#26159;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#65288;DML&#65289;&#20013;&#24120;&#29992;&#30340;&#32452;&#20214;&#65292;&#29992;&#20110;&#32858;&#21512;&#29305;&#24449;&#12290;&#20854;&#26377;&#25928;&#24615;&#36890;&#24120;&#24402;&#22240;&#20110;&#23558;&#27599;&#20010;&#29305;&#24449;&#21521;&#37327;&#35270;&#20026;&#29420;&#31435;&#30340;&#35821;&#20041;&#23454;&#20307;&#65292;&#24182;&#23558;GAP&#35270;&#20026;&#23427;&#20204;&#30340;&#32452;&#21512;&#12290;&#23613;&#31649;&#32463;&#36807;&#35777;&#23454;&#65292;&#20294;&#36825;&#31181;&#35299;&#37322;&#22312;&#23398;&#20064;&#21487;&#29992;&#20110;&#34920;&#31034;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#30340;&#36890;&#29992;&#23454;&#20307;&#30340;&#31639;&#27861;&#24847;&#20041;&#19978;&#20173;&#19981;&#28165;&#26970;&#65292;&#36825;&#26159;DML&#30340;&#20851;&#38190;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;GAP&#23450;&#20041;&#20026;&#21487;&#23398;&#20064;&#21407;&#22411;&#30340;&#20984;&#32452;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21407;&#22411;&#23398;&#20064;&#21487;&#20197;&#34987;&#34920;&#36798;&#20026;&#23558;&#32447;&#24615;&#39044;&#27979;&#22120;&#25311;&#21512;&#21040;&#19968;&#25209;&#26679;&#26412;&#30340;&#36882;&#24402;&#36807;&#31243;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#28857;&#65292;&#25105;&#20204;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#32771;&#34385;&#20004;&#20010;&#19981;&#30456;&#20132;&#31867;&#21035;&#30340;&#25209;&#27425;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#36866;&#24212;&#20110;&#21478;&#19968;&#20010;&#25209;&#27425;&#30340;&#21407;&#22411;&#26469;&#35268;&#33539;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;4&#20010;&#28909;&#38376;DML&#22522;&#20934;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global average pooling (GAP) is a popular component in deep metric learning (DML) for aggregating features. Its effectiveness is often attributed to treating each feature vector as a distinct semantic entity and GAP as a combination of them. Albeit substantiated, such an explanation's algorithmic implications to learn generalizable entities to represent unseen classes, a crucial DML goal, remain unclear. To address this, we formulate GAP as a convex combination of learnable prototypes. We then show that the prototype learning can be expressed as a recursive process fitting a linear predictor to a batch of samples. Building on that perspective, we consider two batches of disjoint classes at each iteration and regularize the learning by expressing the samples of a batch with the prototypes that are fitted to the other batch. We validate our approach on 4 popular DML benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20197;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#23398;&#20064;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#28151;&#21512;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#31639;&#27861;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#27809;&#26377;&#32452;&#20214;&#20998;&#31163;&#26465;&#20214;&#30340;&#24773;&#20917;&#65292;&#24182;&#21487;&#20197;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;&#32858;&#31867;&#31454;&#20105;&#12290;&#27492;&#22806;&#65292;&#31639;&#27861;&#21487;&#20197;&#22312;&#37096;&#20998;&#35266;&#27979;&#35774;&#32622;&#19979;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2307.06538</link><description>&lt;p&gt;
&#24352;&#37327;&#20998;&#35299;&#19982;&#25511;&#21046;&#29702;&#35770;&#30340;&#32467;&#21512;&#65306;&#23398;&#20064;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tensor Decompositions Meet Control Theory: Learning General Mixtures of Linear Dynamical Systems. (arXiv:2307.06538v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20197;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#23398;&#20064;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#28151;&#21512;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#31639;&#27861;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#27809;&#26377;&#32452;&#20214;&#20998;&#31163;&#26465;&#20214;&#30340;&#24773;&#20917;&#65292;&#24182;&#21487;&#20197;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;&#32858;&#31867;&#31454;&#20105;&#12290;&#27492;&#22806;&#65292;&#31639;&#27861;&#21487;&#20197;&#22312;&#37096;&#20998;&#35266;&#27979;&#35774;&#32622;&#19979;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Chen&#21644;Poor&#24320;&#22987;&#30740;&#31350;&#23398;&#20064;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;&#34429;&#28982;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#24050;&#32463;&#22312;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#20351;&#29992;&#28151;&#21512;&#27169;&#22411;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#25311;&#21512;&#25110;&#32773;&#23545;&#25968;&#25454;&#20013;&#34920;&#31034;&#30340;&#22522;&#30784;&#23376;&#32676;&#20307;&#26377;&#26356;&#20016;&#23500;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#20998;&#35299;&#30340;&#23398;&#20064;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#28151;&#21512;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#32452;&#20214;&#26080;&#24378;&#20998;&#31163;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#19982;&#36712;&#36857;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#32858;&#31867;&#31454;&#20105;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36866;&#29992;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#37096;&#20998;&#35266;&#27979;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#36215;&#28857;&#26159;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#35266;&#23519;&#65292;&#21363;&#32463;&#20856;&#30340;&#20309;-&#21345;&#23572;&#26364;&#31639;&#27861;&#26159;&#23398;&#20064;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#29616;&#20195;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#30340;&#36817;&#20146;&#12290;&#36825;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25193;&#23637;&#21040;&#26356;&#22797;&#26434;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#25805;&#20316;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently Chen and Poor initiated the study of learning mixtures of linear dynamical systems. While linear dynamical systems already have wide-ranging applications in modeling time-series data, using mixture models can lead to a better fit or even a richer understanding of underlying subpopulations represented in the data. In this work we give a new approach to learning mixtures of linear dynamical systems that is based on tensor decompositions. As a result, our algorithm succeeds without strong separation conditions on the components, and can be used to compete with the Bayes optimal clustering of the trajectories. Moreover our algorithm works in the challenging partially-observed setting. Our starting point is the simple but powerful observation that the classic Ho-Kalman algorithm is a close relative of modern tensor decomposition methods for learning latent variable models. This gives us a playbook for how to extend it to work with more complicated generative models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26412;&#22320;&#26465;&#20214;&#31070;&#32463;&#22330;&#65288;LCNF&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#36830;&#32493;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#26469;&#35299;&#20915;&#20256;&#32479;&#20687;&#32032;&#34920;&#31034;&#22312;&#25429;&#25417;&#23545;&#35937;&#36830;&#32493;&#12289;&#22810;&#23610;&#24230;&#32454;&#33410;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;LCNF&#22312;&#22810;&#23610;&#24230;&#20449;&#24687;&#37325;&#24314;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#20613;&#31435;&#21494;&#30456;&#20301;&#26174;&#24494;&#38236;&#20013;&#30340;&#36870;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#30456;&#20301;&#24674;&#22797;&#12290;&#19982;&#20256;&#32479;&#31070;&#32463;&#22330;&#26694;&#26550;&#19981;&#21516;&#30340;&#26159;&#65292;LCNF&#32467;&#21512;&#20102;&#26412;&#22320;&#26465;&#20214;&#34920;&#31034;&#65292;&#20419;&#36827;&#20102;&#27169;&#22411;&#27867;&#21270;&#12289;&#23398;&#20064;&#22810;&#23610;&#24230;&#20449;&#24687;&#20197;&#21450;&#39640;&#25928;&#22788;&#29702;&#22823;&#35268;&#27169;&#25104;&#20687;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.06207</link><description>&lt;p&gt;
&#29992;&#20110;&#35745;&#31639;&#25104;&#20687;&#20013;&#22810;&#26679;&#21270;&#21644;&#36890;&#29992;&#22823;&#35268;&#27169;&#37325;&#24314;&#30340;&#26412;&#22320;&#26465;&#20214;&#31070;&#32463;&#22330;
&lt;/p&gt;
&lt;p&gt;
Local Conditional Neural Fields for Versatile and Generalizable Large-Scale Reconstructions in Computational Imaging. (arXiv:2307.06207v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26412;&#22320;&#26465;&#20214;&#31070;&#32463;&#22330;&#65288;LCNF&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#36830;&#32493;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#26469;&#35299;&#20915;&#20256;&#32479;&#20687;&#32032;&#34920;&#31034;&#22312;&#25429;&#25417;&#23545;&#35937;&#36830;&#32493;&#12289;&#22810;&#23610;&#24230;&#32454;&#33410;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;LCNF&#22312;&#22810;&#23610;&#24230;&#20449;&#24687;&#37325;&#24314;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#20613;&#31435;&#21494;&#30456;&#20301;&#26174;&#24494;&#38236;&#20013;&#30340;&#36870;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#30456;&#20301;&#24674;&#22797;&#12290;&#19982;&#20256;&#32479;&#31070;&#32463;&#22330;&#26694;&#26550;&#19981;&#21516;&#30340;&#26159;&#65292;LCNF&#32467;&#21512;&#20102;&#26412;&#22320;&#26465;&#20214;&#34920;&#31034;&#65292;&#20419;&#36827;&#20102;&#27169;&#22411;&#27867;&#21270;&#12289;&#23398;&#20064;&#22810;&#23610;&#24230;&#20449;&#24687;&#20197;&#21450;&#39640;&#25928;&#22788;&#29702;&#22823;&#35268;&#27169;&#25104;&#20687;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#25913;&#21464;&#20102;&#35745;&#31639;&#25104;&#20687;&#65292;&#20294;&#20256;&#32479;&#30340;&#22522;&#20110;&#20687;&#32032;&#30340;&#34920;&#31034;&#38480;&#21046;&#20102;&#23427;&#20204;&#25429;&#25417;&#23545;&#35937;&#30340;&#36830;&#32493;&#12289;&#22810;&#23610;&#24230;&#32454;&#33410;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26412;&#22320;&#26465;&#20214;&#31070;&#32463;&#22330;&#65288;LCNF&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#36830;&#32493;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;LCNF&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#23545;&#35937;&#34920;&#31034;&#65292;&#24182;&#20419;&#36827;&#20102;&#22810;&#23610;&#24230;&#20449;&#24687;&#30340;&#37325;&#24314;&#12290;&#25105;&#20204;&#22312;&#20613;&#31435;&#21494;&#30456;&#20301;&#26174;&#24494;&#38236;&#65288;FPM&#65289;&#20013;&#23637;&#31034;&#20102;LCNF&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#22810;&#36335;&#22797;&#29992;&#27979;&#37327;&#35299;&#20915;&#20102;&#39640;&#24230;&#30149;&#24577;&#30340;&#36870;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#24378;&#22823;&#12289;&#21487;&#25193;&#23637;&#21644;&#36890;&#29992;&#30340;&#22823;&#35268;&#27169;&#30456;&#20301;&#24674;&#22797;&#12290;&#19982;&#20256;&#32479;&#30340;&#31070;&#32463;&#22330;&#26694;&#26550;&#19981;&#21516;&#65292;LCNF&#38598;&#25104;&#20102;&#19968;&#20010;&#20419;&#36827;&#27169;&#22411;&#27867;&#21270;&#12289;&#23398;&#20064;&#22810;&#23610;&#24230;&#20449;&#24687;&#21644;&#39640;&#25928;&#22788;&#29702;&#22823;&#35268;&#27169;&#25104;&#20687;&#25968;&#25454;&#30340;&#26412;&#22320;&#26465;&#20214;&#34920;&#31034;&#12290;&#36890;&#36807;&#23558;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#19982;&#23398;&#20064;&#30340;&#28508;&#22312;&#21521;&#37327;&#30456;&#20851;&#32852;&#65292;LCNF&#23454;&#29616;&#20102;&#22810;&#21151;&#33021;&#36830;&#32493;&#25104;&#20687;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has transformed computational imaging, but traditional pixel-based representations limit their ability to capture continuous, multiscale details of objects. Here we introduce a novel Local Conditional Neural Fields (LCNF) framework, leveraging a continuous implicit neural representation to address this limitation. LCNF enables flexible object representation and facilitates the reconstruction of multiscale information. We demonstrate the capabilities of LCNF in solving the highly ill-posed inverse problem in Fourier ptychographic microscopy (FPM) with multiplexed measurements, achieving robust, scalable, and generalizable large-scale phase retrieval. Unlike traditional neural fields frameworks, LCNF incorporates a local conditional representation that promotes model generalization, learning multiscale information, and efficient processing of large-scale imaging data. By combining an encoder and a decoder conditioned on a learned latent vector, LCNF achieves versatile conti
&lt;/p&gt;</description></item><item><title>NetGPT&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#36793;&#32536;&#21644;&#20113;&#31471;&#37096;&#32626;&#36866;&#24403;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26412;&#22320;AI&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#29983;&#25104;&#26381;&#21153;&#65292;&#24182;&#36890;&#36807;&#21327;&#20316;&#20113;&#36793;&#26041;&#27861;&#35770;&#26469;&#20248;&#21270;&#36164;&#28304;&#21327;&#35843;&#21644;&#20114;&#21160;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06148</link><description>&lt;p&gt;
NetGPT: &#36229;&#36234;&#25552;&#20379;&#20010;&#24615;&#21270;&#29983;&#25104;&#26381;&#21153;&#30340;&#26412;&#22320;AI&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized Generative Services. (arXiv:2307.06148v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06148
&lt;/p&gt;
&lt;p&gt;
NetGPT&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#36793;&#32536;&#21644;&#20113;&#31471;&#37096;&#32626;&#36866;&#24403;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26412;&#22320;AI&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#29983;&#25104;&#26381;&#21153;&#65292;&#24182;&#36890;&#36807;&#21327;&#20316;&#20113;&#36793;&#26041;&#27861;&#35770;&#26469;&#20248;&#21270;&#36164;&#28304;&#21327;&#35843;&#21644;&#20114;&#21160;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#29983;&#25104;&#20449;&#24687;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;LLMs&#30340;&#20010;&#24615;&#21270;&#21487;&#33021;&#36827;&#19968;&#27493;&#20419;&#36827;&#23427;&#20204;&#22312;&#24212;&#29992;&#20013;&#30340;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#12290;&#38024;&#23545;&#20010;&#24615;&#21270;&#29983;&#25104;&#26381;&#21153;&#65292;&#21327;&#20316;&#20113;&#36793;&#26041;&#27861;&#35770;&#21548;&#36215;&#26469;&#24456;&#26377;&#21069;&#26223;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#26377;&#25928;&#21327;&#35843;&#24322;&#26500;&#20998;&#24067;&#24335;&#36890;&#20449;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20960;&#31181;&#20505;&#36873;&#30340;&#20113;&#36793;&#21327;&#20316;&#25216;&#26415;&#30340;&#21033;&#24330;&#65292;&#25552;&#20986;&#20102;NetGPT&#65292;&#26681;&#25454;&#20854;&#35745;&#31639;&#33021;&#21147;&#22312;&#36793;&#32536;&#21644;&#20113;&#31471;&#37096;&#32626;&#36866;&#24403;&#30340;LLMs&#12290;&#27492;&#22806;&#65292;&#36793;&#32536;LLMs&#21487;&#20197;&#39640;&#25928;&#21033;&#29992;&#22522;&#20110;&#20301;&#32622;&#30340;&#20449;&#24687;&#36827;&#34892;&#20010;&#24615;&#21270;&#25552;&#31034;&#23436;&#25104;&#65292;&#20174;&#32780;&#26377;&#30410;&#20110;&#19982;&#20113;&#31471;LLMs&#30340;&#20114;&#21160;&#12290;&#22312;&#36793;&#32536;&#21644;&#20113;&#31471;&#37096;&#32626;&#20195;&#34920;&#24615;&#30340;&#24320;&#28304;LLMs&#65288;&#20363;&#22914;GPT-2-base&#21644;LLaMA&#27169;&#22411;&#65289;&#20043;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NetGPT&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have triggered tremendous success to empower daily life by generative information, and the personalization of LLMs could further contribute to their applications due to better alignment with human intents. Towards personalized generative services, a collaborative cloud-edge methodology sounds promising, as it facilitates the effective orchestration of heterogeneous distributed communication and computing resources. In this article, after discussing the pros and cons of several candidate cloud-edge collaboration techniques, we put forward NetGPT to capably deploy appropriate LLMs at the edge and the cloud in accordance with their computing capacity. In addition, edge LLMs could efficiently leverage location-based information for personalized prompt completion, thus benefiting the interaction with cloud LLMs. After deploying representative open-source LLMs (e.g., GPT-2-base and LLaMA model) at the edge and the cloud, we present the feasibility of NetGPT on th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;2L&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#24341;&#23548;&#21512;&#25104;&#31243;&#24207;&#21270;&#31574;&#30053;&#30340;&#21442;&#32771;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#23454;&#39564;&#20013;&#30340;&#34920;&#29616;&#21644;&#22312;MicroRTS&#38182;&#26631;&#36187;&#20013;&#30340;&#32988;&#21033;&#65292;&#35777;&#26126;&#20102;2L&#31639;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.04893</link><description>&lt;p&gt;
&#36873;&#25321;&#22909;&#23545;&#25163;&#65306;&#22914;&#20309;&#25351;&#23548;&#31243;&#24207;&#21270;&#31574;&#30053;&#30340;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Choosing Well Your Opponents: How to Guide the Synthesis of Programmatic Strategies. (arXiv:2307.04893v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04893
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;2L&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#24341;&#23548;&#21512;&#25104;&#31243;&#24207;&#21270;&#31574;&#30053;&#30340;&#21442;&#32771;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#23454;&#39564;&#20013;&#30340;&#34920;&#29616;&#21644;&#22312;MicroRTS&#38182;&#26631;&#36187;&#20013;&#30340;&#32988;&#21033;&#65292;&#35777;&#26126;&#20102;2L&#31639;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Local Learner (2L)&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25552;&#20379;&#19968;&#32452;&#21442;&#32771;&#31574;&#30053;&#65292;&#20197;&#25351;&#23548;&#22312;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#25628;&#32034;&#31243;&#24207;&#21270;&#31574;&#30053;&#12290;&#20043;&#21069;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;&#36845;&#20195;&#26368;&#20339;&#21709;&#24212;&#31639;&#27861;(IBR)&#65292;&#34394;&#26500;&#28216;&#25103;&#31639;&#27861;(FP)&#21644;&#21452;&#27491;&#20132;&#31639;&#27861;(DO)&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#36739;&#39640;&#25110;&#20250;&#28431;&#25481;&#25351;&#23548;&#25628;&#32034;&#31639;&#27861;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;2L&#20027;&#21160;&#36873;&#25321;&#19968;&#32452;&#21442;&#32771;&#31574;&#30053;&#20197;&#25552;&#39640;&#25628;&#32034;&#20449;&#21495;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19977;&#20010;&#28216;&#25103;&#20013;&#24341;&#23548;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#26469;&#23454;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#21183;&#65292;&#20854;&#20013;&#21253;&#25324;MicroRTS&#65292;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#26102;&#25112;&#30053;&#28216;&#25103;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;2L&#23398;&#20064;&#21040;&#30340;&#21442;&#32771;&#31574;&#30053;&#25552;&#20379;&#20102;&#27604;IBR&#65292;FP&#21644;DO&#26356;&#24378;&#30340;&#25628;&#32034;&#20449;&#21495;&#12290;&#25105;&#20204;&#36824;&#27169;&#25311;&#20102;&#19968;&#22330;MicroRTS&#38182;&#26631;&#36187;&#65292;&#20854;&#20013;&#20351;&#29992;2L&#21512;&#25104;&#22120;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#20004;&#20010;&#26368;&#26032;MicroRTS&#27604;&#36187;&#30340;&#32988;&#32773;&#65292;&#36825;&#20123;&#32988;&#32773;&#22343;&#20026;&#20154;&#31867;&#32534;&#31243;&#21592;&#32534;&#20889;&#30340;&#31243;&#24207;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Local Learner (2L), an algorithm for providing a set of reference strategies to guide the search for programmatic strategies in two-player zero-sum games. Previous learning algorithms, such as Iterated Best Response (IBR), Fictitious Play (FP), and Double-Oracle (DO), can be computationally expensive or miss important information for guiding search algorithms. 2L actively selects a set of reference strategies to improve the search signal. We empirically demonstrate the advantages of our approach while guiding a local search algorithm for synthesizing strategies in three games, including MicroRTS, a challenging real-time strategy game. Results show that 2L learns reference strategies that provide a stronger search signal than IBR, FP, and DO. We also simulate a tournament of MicroRTS, where a synthesizer using 2L outperformed the winners of the two latest MicroRTS competitions, which were programmatic strategies written by human programmers.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#65288;CPDG&#65289;&#29992;&#20110;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DGNNs&#65289;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#32467;&#26500;-&#26102;&#24207;&#23376;&#22270;&#37319;&#26679;&#21644;&#32467;&#26500;-&#26102;&#24207;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;DGNNs&#39044;&#35757;&#32451;&#20013;&#30340;&#27867;&#21270;&#21644;&#38271;&#30701;&#26399;&#24314;&#27169;&#33021;&#21147;&#31561;&#25361;&#25112;&#65292;&#23454;&#39564;&#35777;&#26126;CPDG&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02813</link><description>&lt;p&gt;
CPDG: &#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CPDG: A Contrastive Pre-Training Method for Dynamic Graph Neural Networks. (arXiv:2307.02813v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02813
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#65288;CPDG&#65289;&#29992;&#20110;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DGNNs&#65289;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#32467;&#26500;-&#26102;&#24207;&#23376;&#22270;&#37319;&#26679;&#21644;&#32467;&#26500;-&#26102;&#24207;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;DGNNs&#39044;&#35757;&#32451;&#20013;&#30340;&#27867;&#21270;&#21644;&#38271;&#30701;&#26399;&#24314;&#27169;&#33021;&#21147;&#31561;&#25361;&#25112;&#65292;&#23454;&#39564;&#35777;&#26126;CPDG&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#21160;&#24577;&#22270;&#20013;&#34164;&#21547;&#20016;&#23500;&#20449;&#24687;&#24182;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#21160;&#24577;&#22270;&#25968;&#25454;&#25366;&#25496;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#23613;&#31649;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DGNNs&#65289;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#20854;&#20016;&#23500;&#30340;&#20449;&#24687;&#21644;&#22810;&#26679;&#30340;&#19979;&#28216;&#20219;&#21153;&#32473;&#22312;&#24037;&#19994;&#24773;&#26223;&#20013;&#23454;&#38469;&#24212;&#29992;&#24102;&#26469;&#20102;&#26174;&#33879;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#65288;CPDG&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;CPDG&#36890;&#36807;&#19968;&#31181;&#28789;&#27963;&#30340;&#32467;&#26500;-&#26102;&#24207;&#23376;&#22270;&#37319;&#26679;&#22120;&#21644;&#32467;&#26500;-&#26102;&#24207;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;DGNNs&#39044;&#35757;&#32451;&#20013;&#30340;&#27867;&#21270;&#21644;&#38271;&#30701;&#26399;&#24314;&#27169;&#33021;&#21147;&#31561;&#25361;&#25112;&#12290;&#22312;&#22823;&#35268;&#27169;&#30340;&#30740;&#31350;&#21644;&#24037;&#19994;&#21160;&#24577;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;CPDG&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic graph data mining has gained popularity in recent years due to the rich information contained in dynamic graphs and their widespread use in the real world. Despite the advances in dynamic graph neural networks (DGNNs), the rich information and diverse downstream tasks have posed significant difficulties for the practical application of DGNNs in industrial scenarios. To this end, in this paper, we propose to address them by pre-training and present the Contrastive Pre-Training Method for Dynamic Graph Neural Networks (CPDG). CPDG tackles the challenges of pre-training for DGNNs, including generalization and long-short term modeling capability, through a flexible structural-temporal subgraph sampler along with structural-temporal contrastive pre-training schemes. Extensive experiments conducted on both large-scale research and industrial dynamic graph datasets show that CPDG outperforms existing methods in dynamic graph pre-training for various downstream tasks under three transf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35266;&#27979;&#20195;&#20215;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#27979;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DMSOA&#65292;&#24182;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;DMSOA&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#20915;&#31574;&#27493;&#39588;&#21644;&#27979;&#37327;&#27425;&#25968;&#23398;&#21040;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.02620</link><description>&lt;p&gt;
&#22312;&#35266;&#27979;&#20195;&#20215;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21160;&#24577;&#35266;&#27979;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Dynamic Observation Policies in Observation Cost-Sensitive Reinforcement Learning. (arXiv:2307.02620v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35266;&#27979;&#20195;&#20215;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#27979;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DMSOA&#65292;&#24182;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;DMSOA&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#20915;&#31574;&#27493;&#39588;&#21644;&#27979;&#37327;&#27425;&#25968;&#23398;&#21040;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#30340;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#21253;&#25324;&#28216;&#25103;&#12289;&#26426;&#22120;&#20154;&#12289;&#20379;&#26262;&#19982;&#21046;&#20919;&#31995;&#32479;&#21644;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21160;&#20316;-&#24863;&#30693;&#24490;&#29615;&#36890;&#24120;&#20551;&#35774;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#37117;&#21487;&#20197;&#33719;&#24471;&#23545;&#29615;&#22659;&#29366;&#24577;&#30340;&#27979;&#37327;&#65292;&#19988;&#19981;&#20135;&#29983;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#22312;&#28145;&#28023;&#21644;&#34892;&#26143;&#26426;&#22120;&#20154;&#25506;&#32034;&#12289;&#26448;&#26009;&#35774;&#35745;&#21644;&#21307;&#23398;&#31561;&#24212;&#29992;&#20013;&#65292;&#27979;&#37327;&#25110;&#32773;&#36817;&#20284;&#29615;&#22659;&#29366;&#24577;&#21487;&#33021;&#20250;&#20135;&#29983;&#39640;&#26114;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#36817;&#26469;&#19981;&#26029;&#22686;&#38271;&#30340;&#25991;&#29486;&#65292;&#37319;&#21462;&#20102;RL&#20195;&#29702;&#21487;&#33021;&#19981;&#38656;&#35201;&#25110;&#32773;&#19981;&#24819;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#36827;&#34892;&#26114;&#36149;&#27979;&#37327;&#30340;&#35266;&#28857;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Deep Dynamic Multi-Step Observationless Agent (DMSOA)&#65292;&#24182;&#23558;&#20854;&#19982;&#25991;&#29486;&#36827;&#34892;&#23545;&#27604;&#65292;&#24182;&#22312;OpenAI gym&#21644;Atari Pong&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;DMSOA&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#20915;&#31574;&#27493;&#39588;&#21644;&#27979;&#37327;&#27425;&#25968;&#23398;&#21040;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has been shown to learn sophisticated control policies for complex tasks including games, robotics, heating and cooling systems and text generation. The action-perception cycle in RL, however, generally assumes that a measurement of the state of the environment is available at each time step without a cost. In applications such as deep-sea and planetary robot exploration, materials design and medicine, however, there can be a high cost associated with measuring, or even approximating, the state of the environment. In this paper, we survey the recently growing literature that adopts the perspective that an RL agent might not need, or even want, a costly measurement at each time step. Within this context, we propose the Deep Dynamic Multi-Step Observationless Agent (DMSOA), contrast it with the literature and empirically evaluate it on OpenAI gym and Atari Pong environments. Our results, show that DMSOA learns a better policy with fewer decision steps and meas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#27969;&#24418;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#30340;&#32447;&#24615;&#22238;&#24402;&#65292;&#25581;&#31034;&#20102;&#22806;&#22312;&#20960;&#20309;&#23545;&#22238;&#24402;&#35299;&#21807;&#19968;&#24615;&#20197;&#21450;&#23545;&#36229;&#20986;&#20998;&#24067;&#25512;&#26029;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.02478</link><description>&lt;p&gt;
&#22312;&#27969;&#24418;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#30340;&#32447;&#24615;&#22238;&#24402;&#65306;&#22806;&#22312;&#20960;&#20309;&#23545;&#35299;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Linear Regression on Manifold Structured Data: the Impact of Extrinsic Geometry on Solutions. (arXiv:2307.02478v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#27969;&#24418;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#30340;&#32447;&#24615;&#22238;&#24402;&#65292;&#25581;&#31034;&#20102;&#22806;&#22312;&#20960;&#20309;&#23545;&#22238;&#24402;&#35299;&#21807;&#19968;&#24615;&#20197;&#21450;&#23545;&#36229;&#20986;&#20998;&#24067;&#25512;&#26029;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#27969;&#24418;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#30340;&#32447;&#24615;&#22238;&#24402;&#12290;&#25105;&#20204;&#20551;&#35774;&#25968;&#25454;&#27969;&#24418;&#26159;&#20809;&#28369;&#30340;&#65292;&#24182;&#23884;&#20837;&#22312;&#19968;&#20010;&#27431;&#27663;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25581;&#31034;&#25968;&#25454;&#27969;&#24418;&#30340;&#22806;&#22312;&#20960;&#20309;&#23545;&#22238;&#24402;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#27969;&#24418;&#30340;&#26354;&#29575;&#65288;&#25110;&#24403;&#26354;&#29575;&#23616;&#37096;&#20026;&#38646;&#26102;&#30340;&#39640;&#38454;&#38750;&#32447;&#24615;&#21442;&#25968;&#21270;&#65289;&#23545;&#22238;&#24402;&#35299;&#21807;&#19968;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#23884;&#20837;&#30340;&#23376;&#27969;&#24418;&#22312;&#26576;&#20123;&#32500;&#24230;&#19978;&#26159;&#24179;&#22374;&#30340;&#26102;&#65292;&#30456;&#24212;&#30340;&#32447;&#24615;&#22238;&#24402;&#35299;&#19981;&#26159;&#21807;&#19968;&#30340;&#12290;&#21542;&#21017;&#65292;&#27969;&#24418;&#30340;&#26354;&#29575;&#65288;&#25110;&#23884;&#20837;&#20013;&#30340;&#39640;&#38454;&#38750;&#32447;&#24615;&#24230;&#37327;&#65289;&#21487;&#33021;&#22312;&#19982;&#27969;&#24418;&#30340;&#27861;&#26041;&#21521;&#30456;&#20851;&#30340;&#35299;&#20013;&#26377;&#26174;&#33879;&#36129;&#29486;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#25968;&#25454;&#27969;&#24418;&#20960;&#20309;&#22312;&#30830;&#20445;&#22238;&#24402;&#27169;&#22411;&#23545;&#20110;&#36229;&#20986;&#20998;&#24067;&#30340;&#25512;&#26029;&#30340;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study linear regression applied to data structured on a manifold. We assume that the data manifold is smooth and is embedded in a Euclidean space, and our objective is to reveal the impact of the data manifold's extrinsic geometry on the regression. Specifically, we analyze the impact of the manifold's curvatures (or higher order nonlinearity in the parameterization when the curvatures are locally zero) on the uniqueness of the regression solution. Our findings suggest that the corresponding linear regression does not have a unique solution when the embedded submanifold is flat in some dimensions. Otherwise, the manifold's curvature (or higher order nonlinearity in the embedding) may contribute significantly, particularly in the solution associated with the normal directions of the manifold. Our findings thus reveal the role of data manifold geometry in ensuring the stability of regression models for out-of-distribution inferences.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#33410;&#28857;&#35782;&#21035;&#12289;&#23494;&#38598;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#28040;&#24687;&#20256;&#36882;&#23618;&#26469;&#23454;&#29616;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#39034;&#24207;&#27169;&#22359;&#12290;&#35813;&#27169;&#22411;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01482</link><description>&lt;p&gt;
Nexus sine qua non&#65306;&#22522;&#20110;&#33410;&#28857;&#35782;&#21035;&#30340;&#31070;&#32463;&#32593;&#32476;&#36830;&#25509;&#30340;&#26102;&#31354;&#39044;&#27979;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Nexus sine qua non: Essentially connected neural networks for spatial-temporal forecasting of multivariate time series. (arXiv:2307.01482v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01482
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#33410;&#28857;&#35782;&#21035;&#12289;&#23494;&#38598;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#28040;&#24687;&#20256;&#36882;&#23618;&#26469;&#23454;&#29616;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#39034;&#24207;&#27169;&#22359;&#12290;&#35813;&#27169;&#22411;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#21644;&#39044;&#27979;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#19981;&#20165;&#26377;&#21161;&#20110;&#20174;&#19994;&#32773;&#30340;&#20915;&#31574;&#65292;&#36824;&#21152;&#28145;&#25105;&#20204;&#23545;&#24213;&#23618;&#21160;&#24577;&#31995;&#32479;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STGNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#24378;&#22823;&#30340;&#39044;&#27979;&#22120;&#65292;&#24182;&#25104;&#20026;&#23398;&#20064;&#26102;&#31354;&#34920;&#31034;&#30340;&#20107;&#23454;&#26631;&#20934;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;STGNNs&#30340;&#26550;&#26500;&#24448;&#24448;&#36890;&#36807;&#22534;&#21472;&#19968;&#31995;&#21015;&#22797;&#26434;&#30340;&#23618;&#27425;&#32780;&#21464;&#24471;&#22797;&#26434;&#12290;&#35774;&#35745;&#30340;&#27169;&#22411;&#21487;&#33021;&#22810;&#20313;&#25110;&#38590;&#20197;&#29702;&#35299;&#65292;&#36825;&#32473;&#22797;&#26434;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#36825;&#20123;&#38382;&#39064;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#29616;&#20195;STGNNs&#30340;&#35774;&#35745;&#65292;&#24182;&#30830;&#23450;&#23545;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;&#31070;&#32463;&#39044;&#27979;&#22120;&#26377;&#25152;&#36129;&#29486;&#30340;&#26680;&#24515;&#21407;&#21017;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32039;&#20945;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#23436;&#20840;&#30001;&#23494;&#38598;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#28040;&#24687;&#20256;&#36882;&#23618;&#26469;&#23450;&#20041;&#65292;&#22522;&#20110;&#33410;&#28857;&#35782;&#21035;&#65292;&#27809;&#26377;&#20219;&#20309;&#22797;&#26434;&#30340;&#39034;&#24207;&#27169;&#22359;&#65292;&#20363;&#22914;TCNs&#65292;RNNs&#21644;Transformers&#12290;&#36890;&#36807;&#23454;&#35777;&#37325;&#26032;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling and forecasting multivariate time series not only facilitates the decision making of practitioners, but also deepens our scientific understanding of the underlying dynamical systems. Spatial-temporal graph neural networks (STGNNs) are emerged as powerful predictors and have become the de facto models for learning spatiotemporal representations in recent years. However, existing architectures of STGNNs tend to be complicated by stacking a series of fancy layers. The designed models could be either redundant or enigmatic, which pose great challenges on their complexity and scalability. Such concerns prompt us to re-examine the designs of modern STGNNs and identify core principles that contribute to a powerful and efficient neural predictor. Here we present a compact predictive model that is fully defined by a dense encoder-decoder and a message-passing layer, powered by node identifications, without any complex sequential modules, e.g., TCNs, RNNs, and Transformers. Empirical re
&lt;/p&gt;</description></item><item><title>ManimML&#26159;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#65292;&#36890;&#36807;&#21160;&#30011;&#28436;&#31034;&#33258;&#21160;&#29983;&#25104;&#30340;ML&#31639;&#27861;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#29087;&#24713;&#30340;&#26041;&#24335;&#26469;&#27807;&#36890;&#21644;&#21487;&#35270;&#21270;ML&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.17108</link><description>&lt;p&gt;
ManimML&#65306;&#29992;&#21160;&#30011;&#28436;&#31034;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
ManimML: Communicating Machine Learning Architectures with Animation. (arXiv:2306.17108v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17108
&lt;/p&gt;
&lt;p&gt;
ManimML&#26159;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#65292;&#36890;&#36807;&#21160;&#30011;&#28436;&#31034;&#33258;&#21160;&#29983;&#25104;&#30340;ML&#31639;&#27861;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#29087;&#24713;&#30340;&#26041;&#24335;&#26469;&#27807;&#36890;&#21644;&#21487;&#35270;&#21270;ML&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#20852;&#36259;&#28608;&#22686;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;ML&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#35299;&#37322;&#21644;&#21487;&#35270;&#21270;&#26032;&#39062;&#30340;ML&#31639;&#27861;&#30340;&#24037;&#20855;&#36824;&#36828;&#36828;&#33853;&#21518;&#12290;&#21160;&#30011;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#21046;&#20316;&#20986;&#38543;&#26102;&#38388;&#21160;&#24577;&#21464;&#21270;&#30340;&#31995;&#32479;&#30340;&#21560;&#24341;&#20154;&#21487;&#35270;&#21270;&#25928;&#26524;&#65292;&#38750;&#24120;&#36866;&#21512;&#29992;&#20110;&#27807;&#36890;ML&#31639;&#27861;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21160;&#30011;&#21270;ML&#31639;&#27861;&#30340;&#26041;&#27861;&#26159;&#25163;&#24037;&#21046;&#20316;&#31361;&#20986;&#29305;&#23450;&#31639;&#27861;&#25110;&#20351;&#29992;&#22797;&#26434;&#30340;&#36890;&#29992;&#21160;&#30011;&#36719;&#20214;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;ManimML&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#20195;&#30721;&#20013;&#36731;&#26494;&#29983;&#25104;ML&#31639;&#27861;&#30340;&#21160;&#30011;&#12290;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;ML&#20174;&#19994;&#32773;&#23545;&#32534;&#31243;&#30340;&#29616;&#26377;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#35201;&#27714;&#20182;&#20204;&#23398;&#20064;&#22797;&#26434;&#30340;&#21160;&#30011;&#36719;&#20214;&#12290;ManimML&#20855;&#26377;&#29087;&#24713;&#30340;&#35821;&#27861;&#65292;&#29992;&#20110;&#25351;&#23450;&#27169;&#20223;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#22914;Pytorch&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been an explosion in interest in machine learning (ML) in recent years due to its applications to science and engineering. However, as ML techniques have advanced, tools for explaining and visualizing novel ML algorithms have lagged behind. Animation has been shown to be a powerful tool for making engaging visualizations of systems that dynamically change over time, which makes it well suited to the task of communicating ML algorithms. However, the current approach to animating ML algorithms is to handcraft applications that highlight specific algorithms or use complex generalized animation software. We developed ManimML, an open-source Python library for easily generating animations of ML algorithms directly from code. We sought to leverage ML practitioners' preexisting knowledge of programming rather than requiring them to learn complex animation software. ManimML has a familiar syntax for specifying neural networks that mimics popular deep learning frameworks like Pytorch.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#27169;&#25311;&#21452;&#20998;&#23700;&#29992;&#20110;&#22823;&#35268;&#27169;MIMO&#20449;&#21495;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20462;&#25913;&#31639;&#27861;&#21644;&#20351;&#29992;&#28145;&#24230;&#23637;&#24320;&#25216;&#26415;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.16264</link><description>&lt;p&gt;
&#28145;&#24230;&#23637;&#24320;&#27169;&#25311;&#30340;&#21452;&#20998;&#23700;&#29992;&#20110;&#22823;&#35268;&#27169;MIMO&#20449;&#21495;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Unfolded Simulated Bifurcation for Massive MIMO Signal Detection. (arXiv:2306.16264v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#27169;&#25311;&#21452;&#20998;&#23700;&#29992;&#20110;&#22823;&#35268;&#27169;MIMO&#20449;&#21495;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20462;&#25913;&#31639;&#27861;&#21644;&#20351;&#29992;&#28145;&#24230;&#23637;&#24320;&#25216;&#26415;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#26159;&#19979;&#19968;&#20195;&#26080;&#32447;&#36890;&#20449;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#37327;&#23376;&#65288;&#21551;&#21457;&#24335;&#65289;&#31639;&#27861;&#30340;&#21508;&#31181;MIMO&#20449;&#21495;&#26816;&#27979;&#22120;&#24050;&#34987;&#25552;&#20986;&#65292;&#20197;&#25913;&#21892;&#19982;&#20256;&#32479;&#26816;&#27979;&#22120;&#30456;&#27604;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#26412;&#25991;&#20851;&#27880;&#37327;&#23376;&#21551;&#21457;&#24335;&#31639;&#27861;&#20013;&#30340;&#27169;&#25311;&#21452;&#20998;&#23700;&#65288;SB&#65289;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25216;&#26415;&#26469;&#25913;&#21892;&#20854;&#26816;&#27979;&#24615;&#33021;&#12290;&#31532;&#19968;&#31181;&#26159;&#20462;&#25913;&#21463;Levenberg-Marquardt&#31639;&#27861;&#21551;&#21457;&#30340;&#31639;&#27861;&#65292;&#20197;&#28040;&#38500;&#26368;&#22823;&#20284;&#28982;&#26816;&#27979;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#31532;&#20108;&#31181;&#26159;&#20351;&#29992;&#28145;&#24230;&#23637;&#24320;&#65292;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#26469;&#35757;&#32451;&#36845;&#20195;&#31639;&#27861;&#30340;&#20869;&#37096;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23637;&#24320;&#30340;SB&#65292;&#36890;&#36807;&#20351;SB&#30340;&#26356;&#26032;&#35268;&#21017;&#21487;&#24494;&#20998;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#25552;&#20986;&#30340;&#26816;&#27979;&#22120;&#26174;&#33879;&#25913;&#21892;&#20102;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#30340;&#20449;&#21495;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple-input multiple-output (MIMO) is a key ingredient of next-generation wireless communications. Recently, various MIMO signal detectors based on deep learning techniques and quantum(-inspired) algorithms have been proposed to improve the detection performance compared with conventional detectors. This paper focuses on the simulated bifurcation (SB) algorithm, a quantum-inspired algorithm. This paper proposes two techniques to improve its detection performance. The first is modifying the algorithm inspired by the Levenberg-Marquardt algorithm to eliminate local minima of maximum likelihood detection. The second is the use of deep unfolding, a deep learning technique to train the internal parameters of an iterative algorithm. We propose a deep-unfolded SB by making the update rule of SB differentiable. The numerical results show that these proposed detectors significantly improve the signal detection performance in massive MIMO systems.
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#31185;&#23398;&#26159;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#28508;&#21147;&#21644;&#24212;&#29992;&#24191;&#27867;&#24615;&#65292;&#22312;40&#22810;&#20010;&#23398;&#31185;&#12289;&#25968;&#30334;&#20010;&#30740;&#31350;&#39046;&#22495;&#21644;&#25104;&#21315;&#19978;&#19975;&#20010;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36215;&#27493;&#38454;&#27573;&#65292;&#30446;&#21069;&#23384;&#22312;&#35768;&#22810;&#23450;&#20041;&#30340;&#20887;&#20313;&#21644;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16177</link><description>&lt;p&gt;
&#23450;&#20041;&#25968;&#25454;&#31185;&#23398;&#65306;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Defining data science: a new field of inquiry. (arXiv:2306.16177v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16177
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#26159;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#28508;&#21147;&#21644;&#24212;&#29992;&#24191;&#27867;&#24615;&#65292;&#22312;40&#22810;&#20010;&#23398;&#31185;&#12289;&#25968;&#30334;&#20010;&#30740;&#31350;&#39046;&#22495;&#21644;&#25104;&#21315;&#19978;&#19975;&#20010;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36215;&#27493;&#38454;&#27573;&#65292;&#30446;&#21069;&#23384;&#22312;&#35768;&#22810;&#23450;&#20041;&#30340;&#20887;&#20313;&#21644;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#19981;&#26159;&#19968;&#38376;&#31185;&#23398;&#65292;&#32780;&#26159;&#19968;&#31181;&#30740;&#31350;&#33539;&#24335;&#12290;&#23427;&#30340;&#21147;&#37327;&#12289;&#33539;&#22260;&#21644;&#35268;&#27169;&#23558;&#36229;&#36234;&#31185;&#23398;&#65292;&#25104;&#20026;&#20419;&#20351;&#30693;&#35782;&#21457;&#29616;&#24182;&#25913;&#21464;&#19990;&#30028;&#30340;&#37325;&#35201;&#25163;&#27573;&#12290;&#25105;&#20204;&#23578;&#26410;&#29702;&#35299;&#21644;&#23450;&#20041;&#23427;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#20854;&#28508;&#21147;&#21644;&#31649;&#29702;&#20854;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#12290;&#33258;1962&#24180;&#20197;&#26469;&#32531;&#24930;&#21457;&#23637;&#65292;&#24182;&#19988;&#33258;2000&#24180;&#20197;&#26469;&#21457;&#23637;&#36805;&#36895;&#65292;&#23427;&#26159;&#19968;&#31181;&#26681;&#26412;&#24615;&#30340;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26159;21&#19990;&#32426;&#26368;&#27963;&#36291;&#12289;&#26368;&#24378;&#22823;&#21644;&#21457;&#23637;&#26368;&#24555;&#30340;&#21019;&#26032;&#20043;&#19968;&#12290;&#30001;&#20110;&#20854;&#20215;&#20540;&#12289;&#21147;&#37327;&#21644;&#36866;&#29992;&#24615;&#65292;&#23427;&#27491;&#22312;40&#22810;&#20010;&#23398;&#31185;&#12289;&#25968;&#30334;&#20010;&#30740;&#31350;&#39046;&#22495;&#21644;&#25104;&#21315;&#19978;&#19975;&#20010;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#25968;&#20197;&#30334;&#19975;&#35745;&#30340;&#25968;&#25454;&#31185;&#23398;&#20986;&#29256;&#29289;&#20013;&#21253;&#21547;&#20102;&#26080;&#25968;&#20851;&#20110;&#25968;&#25454;&#31185;&#23398;&#21644;&#25968;&#25454;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#30340;&#23450;&#20041;&#12290;&#30001;&#20110;&#20854;&#36215;&#27493;&#38454;&#27573;&#65292;&#35768;&#22810;&#23450;&#20041;&#26159;&#29420;&#31435;&#30340;&#12289;&#24212;&#29992;&#29305;&#23450;&#30340;&#12289;&#30456;&#20114;&#19981;&#23436;&#25972;&#30340;&#12289;&#20887;&#20313;&#30340;&#25110;&#19981;&#19968;&#33268;&#30340;&#65292;&#22240;&#27492;&#25968;&#25454;&#31185;&#23398;&#20063;&#26159;&#22914;&#27492;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#22810;&#37325;&#23450;&#20041;&#25361;&#25112;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data science is not a science. It is a research paradigm. Its power, scope, and scale will surpass science, our most powerful research paradigm, to enable knowledge discovery and change our world. We have yet to understand and define it, vital to realizing its potential and managing its risks. Modern data science is in its infancy. Emerging slowly since 1962 and rapidly since 2000, it is a fundamentally new field of inquiry, one of the most active, powerful, and rapidly evolving 21st century innovations. Due to its value, power, and applicability, it is emerging in 40+ disciplines, hundreds of research areas, and thousands of applications. Millions of data science publications contain myriad definitions of data science and data science problem solving. Due to its infancy, many definitions are independent, application-specific, mutually incomplete, redundant, or inconsistent, hence so is data science. This research addresses this data science multiple definitions challenge by proposing 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20195;&#29702;&#27169;&#22411;&#65292;&#24555;&#36895;&#39044;&#27979;&#21335;&#20315;&#32599;&#37324;&#36798;&#24030;&#36808;&#38463;&#23494;&#27827;&#19979;&#28216;&#30340;&#27700;&#20301;&#65292;&#24182;&#19982;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2306.15907</link><description>&lt;p&gt;
&#21335;&#20315;&#32599;&#37324;&#36798;&#24030;&#27700;&#20301;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Models for Water Stage Predictions in South Florida. (arXiv:2306.15907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20195;&#29702;&#27169;&#22411;&#65292;&#24555;&#36895;&#39044;&#27979;&#21335;&#20315;&#32599;&#37324;&#36798;&#24030;&#36808;&#38463;&#23494;&#27827;&#19979;&#28216;&#30340;&#27700;&#20301;&#65292;&#24182;&#19982;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#21644;&#39044;&#27979;&#27827;&#27969;&#31995;&#32479;&#30340;&#27700;&#20301;&#23545;&#20110;&#27946;&#27700;&#35686;&#25253;&#12289;&#27700;&#21147;&#25805;&#20316;&#21644;&#27946;&#27700;&#20943;&#36731;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#24037;&#31243;&#39046;&#22495;&#20013;&#65292;&#20351;&#29992;HEC-RAS&#12289;MIKE&#21644;SWMM&#31561;&#24037;&#20855;&#24314;&#31435;&#35814;&#32454;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#27700;&#25991;&#21644;&#27700;&#21147;&#35745;&#31639;&#27169;&#22411;&#26469;&#27169;&#25311;&#25972;&#20010;&#27969;&#22495;&#65292;&#20174;&#32780;&#39044;&#27979;&#31995;&#32479;&#20013;&#20219;&#24847;&#28857;&#30340;&#27700;&#20301;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#35745;&#31639;&#37327;&#22823;&#65292;&#23588;&#20854;&#23545;&#20110;&#22823;&#27969;&#22495;&#21644;&#38271;&#26102;&#38388;&#27169;&#25311;&#26469;&#35828;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#20960;&#20010;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#65292;&#24555;&#36895;&#39044;&#27979;&#27700;&#20301;&#12290;&#26412;&#25991;&#20197;&#21335;&#20315;&#32599;&#37324;&#36798;&#24030;&#36808;&#38463;&#23494;&#27827;&#30340;&#19979;&#28216;&#27700;&#20301;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25968;&#25454;&#38598;&#26469;&#33258;&#21335;&#20315;&#32599;&#37324;&#36798;&#27700;&#31649;&#29702;&#21306;&#65288;SFWMD&#65289;&#30340;DBHYDRO&#25968;&#25454;&#24211;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;2010&#24180;1&#26376;1&#26085;&#33267;2020&#24180;12&#26376;31&#26085;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DL&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating and predicting water levels in river systems is essential for flood warnings, hydraulic operations, and flood mitigations. In the engineering field, tools such as HEC-RAS, MIKE, and SWMM are used to build detailed physics-based hydrological and hydraulic computational models to simulate the entire watershed, thereby predicting the water stage at any point in the system. However, these physics-based models are computationally intensive, especially for large watersheds and for longer simulations. To overcome this problem, we train several deep learning (DL) models for use as surrogate models to rapidly predict the water stage. The downstream stage of the Miami River in South Florida is chosen as a case study for this paper. The dataset is from January 1, 2010, to December 31, 2020, downloaded from the DBHYDRO database of the South Florida Water Management District (SFWMD). Extensive experiments show that the performance of the DL models is comparable to that of the physics-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;P5&#27169;&#22411;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#22686;&#24378;&#20102;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#36890;&#36807;&#20248;&#21270;&#30446;&#26631;&#32858;&#21512;&#29289;&#38142;&#26500;&#35937;&#30340;&#37319;&#26679;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36229;&#36807;37.1%&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;&#24378;&#21270;&#23398;&#20064;&#20135;&#29983;&#30340;&#25511;&#21046;&#31574;&#30053;&#25193;&#23637;&#20102;&#20256;&#32479;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;&#37197;&#32622;&#31354;&#38388;&#30340;&#25506;&#32034;&#33539;&#22260;&#65292;&#29983;&#25104;&#20102;&#26356;&#22810;&#26679;&#21270;&#30340;&#26500;&#35937;&#38598;&#21512;&#65292;&#24182;&#38024;&#23545;&#29305;&#23450;&#29305;&#24615;&#36827;&#34892;&#30446;&#26631;&#21270;&#12290;&#36825;&#19968;&#25216;&#26415;&#22312;&#30740;&#31350;&#26032;&#31995;&#32479;&#26102;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#24182;&#20026;&#22797;&#26434;&#27169;&#25311;&#38382;&#39064;&#30340;&#35299;&#20915;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#23398;&#12290;</title><link>http://arxiv.org/abs/2306.14705</link><description>&lt;p&gt;
&#25193;&#22823;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#22120;&#25506;&#32034;&#31354;&#38388;&#30340;&#25511;&#21046;&#20197;&#36890;&#36807;&#29983;&#25104;&#25511;&#21046;&#31574;&#30053;&#31616;&#21270;De Novo&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Augmenting Control over Exploration Space in Molecular Dynamics Simulators to Streamline De Novo Analysis through Generative Control Policies. (arXiv:2306.14705v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;P5&#27169;&#22411;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#22686;&#24378;&#20102;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#36890;&#36807;&#20248;&#21270;&#30446;&#26631;&#32858;&#21512;&#29289;&#38142;&#26500;&#35937;&#30340;&#37319;&#26679;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36229;&#36807;37.1%&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;&#24378;&#21270;&#23398;&#20064;&#20135;&#29983;&#30340;&#25511;&#21046;&#31574;&#30053;&#25193;&#23637;&#20102;&#20256;&#32479;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;&#37197;&#32622;&#31354;&#38388;&#30340;&#25506;&#32034;&#33539;&#22260;&#65292;&#29983;&#25104;&#20102;&#26356;&#22810;&#26679;&#21270;&#30340;&#26500;&#35937;&#38598;&#21512;&#65292;&#24182;&#38024;&#23545;&#29305;&#23450;&#29305;&#24615;&#36827;&#34892;&#30446;&#26631;&#21270;&#12290;&#36825;&#19968;&#25216;&#26415;&#22312;&#30740;&#31350;&#26032;&#31995;&#32479;&#26102;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#24182;&#20026;&#22797;&#26434;&#27169;&#25311;&#38382;&#39064;&#30340;&#35299;&#20915;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;P5&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#22686;&#24378;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#30340;&#25511;&#21046;&#12289;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#31574;&#30053;&#20248;&#21270;&#20102;&#30446;&#26631;&#32858;&#21512;&#29289;&#38142;&#26500;&#35937;&#30340;&#37319;&#26679;&#65292;&#20351;&#25928;&#29575;&#25552;&#39640;&#20102;37.1%&#20197;&#19978;&#12290;&#24378;&#21270;&#23398;&#20064;&#24341;&#23548;&#30340;&#25511;&#21046;&#31574;&#30053;&#20316;&#20026;&#24402;&#32435;&#20559;&#22909;&#65292;&#35843;&#33410;&#24067;&#26391;&#21147;&#20197;&#23558;&#31995;&#32479;&#24341;&#23548;&#21040;&#20248;&#36873;&#29366;&#24577;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#20256;&#32479;&#20998;&#23376;&#21160;&#21147;&#23398;&#25152;&#20801;&#35768;&#30340;&#37197;&#32622;&#31354;&#38388;&#30340;&#25506;&#32034;&#33539;&#22260;&#12290;&#36825;&#31181;&#25193;&#22823;&#30340;&#25506;&#32034;&#29983;&#25104;&#20102;&#26356;&#22810;&#26679;&#21270;&#30340;&#26500;&#35937;&#38598;&#21512;&#65292;&#24182;&#38024;&#23545;&#29305;&#23450;&#29305;&#24615;&#36827;&#34892;&#30446;&#26631;&#21270;&#65292;&#36825;&#23545;&#20110;&#32858;&#21512;&#29289;&#24320;&#21457;&#12289;&#33647;&#29289;&#21457;&#29616;&#21644;&#26448;&#26009;&#35774;&#35745;&#30340;&#36827;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#30740;&#31350;&#20855;&#26377;&#26377;&#38480;&#20808;&#39564;&#30693;&#35782;&#30340;&#26032;&#31995;&#32479;&#26102;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#20026;&#21033;&#29992;&#29983;&#25104;&#25216;&#26415;&#35299;&#20915;&#22797;&#26434;&#30340;&#27169;&#25311;&#38382;&#39064;&#24320;&#36767;&#20102;&#26032;&#30340;&#26041;&#27861;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces the P5 model - a foundational method that utilizes reinforcement learning (RL) to augment control, effectiveness, and scalability in molecular dynamics simulations (MD). Our innovative strategy optimizes the sampling of target polymer chain conformations, marking an efficiency improvement of over 37.1%. The RL-induced control policies function as an inductive bias, modulating Brownian forces to steer the system towards the preferred state, thereby expanding the exploration of the configuration space beyond what traditional MD allows. This broadened exploration generates a more varied set of conformations and targets specific properties, a feature pivotal for progress in polymer development, drug discovery, and material design. Our technique offers significant advantages when investigating new systems with limited prior knowledge, opening up new methodologies for tackling complex simulation problems with generative techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19987;&#23478;&#35266;&#23519;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20687;&#32032;&#35266;&#27979;&#20013;&#36827;&#34892;&#31232;&#30095;&#22870;&#21169;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#35266;&#23519;&#25968;&#25454;&#20316;&#20026;&#30446;&#26631;&#26465;&#20214;&#65292;&#35813;&#26041;&#27861;&#33021;&#26174;&#33879;&#25913;&#36827;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#35757;&#32451;&#36807;&#31243;&#20013;&#25152;&#38656;&#30340;&#19987;&#23478;&#34892;&#21160;&#27425;&#25968;&#20943;&#23569;&#20102;4-20&#20493;&#65292;&#24182;&#19988;&#20248;&#20110;&#20998;&#23618;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.13872</link><description>&lt;p&gt;
&#20351;&#29992;&#19987;&#23478;&#35266;&#23519;&#25968;&#25454;&#36827;&#34892;&#20687;&#32032;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Pixels with Expert Observations. (arXiv:2306.13872v2 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19987;&#23478;&#35266;&#23519;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20687;&#32032;&#35266;&#27979;&#20013;&#36827;&#34892;&#31232;&#30095;&#22870;&#21169;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#35266;&#23519;&#25968;&#25454;&#20316;&#20026;&#30446;&#26631;&#26465;&#20214;&#65292;&#35813;&#26041;&#27861;&#33021;&#26174;&#33879;&#25913;&#36827;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#35757;&#32451;&#36807;&#31243;&#20013;&#25152;&#38656;&#30340;&#19987;&#23478;&#34892;&#21160;&#27425;&#25968;&#20943;&#23569;&#20102;4-20&#20493;&#65292;&#24182;&#19988;&#20248;&#20110;&#20998;&#23618;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#31232;&#30095;&#22870;&#21169;&#21487;&#33021;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#21487;&#20197;&#21033;&#29992;&#19987;&#23478;&#34892;&#20026;&#26469;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#26126;&#30830;&#30340;&#19987;&#23478;&#34892;&#21160;&#21487;&#33021;&#26159;&#26114;&#36149;&#30340;&#65292;&#32780;&#19987;&#23478;&#35266;&#23519;&#25968;&#25454;&#36890;&#24120;&#26356;&#23481;&#26131;&#33719;&#24471;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#35266;&#23519;&#25968;&#25454;&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#20687;&#32032;&#35266;&#27979;&#20013;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#23558;&#19987;&#23478;&#35266;&#23519;&#25968;&#25454;&#20316;&#20026;&#30446;&#26631;&#26465;&#20214;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#20013;&#38388;&#35270;&#35273;&#30446;&#26631;&#65292;&#20351;&#20854;&#36890;&#36807;&#36830;&#32493;&#36798;&#21040;&#19968;&#31995;&#21015;&#30446;&#26631;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#20116;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22359;&#22534;&#21472;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#19982;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#26234;&#33021;&#20307;&#30456;&#32467;&#21512;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23427;&#20204;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38656;&#35201;&#30340;&#19987;&#23478;&#34892;&#21160;&#27425;&#25968;&#20943;&#23569;&#20102;4-20&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#20248;&#20110;&#19968;&#20010;&#20998;&#23618;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning (RL), sparse rewards can present a significant challenge. Fortunately, expert actions can be utilized to overcome this issue. However, acquiring explicit expert actions can be costly, and expert observations are often more readily available. This paper presents a new approach that uses expert observations for learning in robot manipulation tasks with sparse rewards from pixel observations. Specifically, our technique involves using expert observations as intermediate visual goals for a goal-conditioned RL agent, enabling it to complete a task by successively reaching a series of goals. We demonstrate the efficacy of our method in five challenging block construction tasks in simulation and show that when combined with two state-of-the-art agents, our approach can significantly improve their performance while requiring 4-20 times fewer expert actions during training. Moreover, our method is also superior to a hierarchical baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSelect&#30340;&#26032;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23547;&#25214;&#26368;&#20339;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20840;&#23616;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13264</link><description>&lt;p&gt;
FedSelect: &#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#21442;&#25968;&#33258;&#23450;&#20041;&#36873;&#25321;&#30340;&#32454;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedSelect: Customized Selection of Parameters for Fine-Tuning during Personalized Federated Learning. (arXiv:2306.13264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSelect&#30340;&#26032;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23547;&#25214;&#26368;&#20339;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20840;&#23616;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#22312;&#26412;&#22320;&#25968;&#25454;&#19978;&#24494;&#35843;&#23458;&#25143;&#31471;&#21442;&#25968;&#25110;&#38024;&#23545;&#26412;&#22320;&#20219;&#21153;&#20010;&#24615;&#21270;&#26550;&#26500;&#26469;&#25552;&#39640;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#22312;&#29306;&#29298;&#37325;&#35201;&#30340;&#20840;&#23616;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20010;&#24615;&#21270;&#65292;&#35201;&#20040;&#22312;&#39044;&#20808;&#30830;&#23450;&#32593;&#32476;&#23618;&#20197;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23548;&#33268;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#20840;&#23616;&#30693;&#35782;&#20648;&#23384;&#30340;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;FedSelect&#65292;&#36890;&#36807;&#21516;&#26102;&#25628;&#32034;&#24182;&#33719;&#24471;&#20010;&#24615;&#21270;&#26368;&#20339;&#21442;&#25968;&#21644;&#29992;&#20110;&#20840;&#23616;&#32858;&#21512;&#30340;&#20854;&#20313;&#21442;&#25968;&#65292;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in federated learning (FL) seek to increase client-level performance by fine-tuning client parameters on local data or personalizing architectures for the local task. Existing methods for such personalization either prune a global model or fine-tune a global model on a local client distribution. However, these existing methods either personalize at the expense of retaining important global knowledge, or predetermine network layers for fine-tuning, resulting in suboptimal storage of global knowledge within client models. Enlightened by the lottery ticket hypothesis, we first introduce a hypothesis for finding optimal client subnetworks to locally fine-tune while leaving the rest of the parameters frozen. We then propose a novel FL framework, FedSelect, using this procedure that directly personalizes both client subnetwork structure and parameters, via the simultaneous discovery of optimal parameters for personalization and the rest of parameters for global aggregatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#34507;&#30333;&#36136;&#21464;&#20307;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#25552;&#20986;&#30340;&#32467;&#26500;&#26041;&#27861;&#22312;&#35757;&#32451;&#26356;&#23569;&#30340;&#20998;&#23376;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#36798;&#21040;&#19982;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12231</link><description>&lt;p&gt;
&#21033;&#29992;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#34507;&#30333;&#36136;&#21464;&#20307;
&lt;/p&gt;
&lt;p&gt;
Predicting protein variants with equivariant graph neural networks. (arXiv:2306.12231v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#34507;&#30333;&#36136;&#21464;&#20307;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#25552;&#20986;&#30340;&#32467;&#26500;&#26041;&#27861;&#22312;&#35757;&#32451;&#26356;&#23569;&#30340;&#20998;&#23376;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#36798;&#21040;&#19982;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#35768;&#22810;&#34507;&#30333;&#36136;&#24037;&#31243;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#24207;&#21015;&#30340;&#27169;&#22411;&#22312;&#34507;&#30333;&#36136;&#36866;&#24212;&#24615;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#32780;&#22522;&#20110;&#32467;&#26500;&#30340;&#27169;&#22411;&#21017;&#24050;&#34987;&#23454;&#39564;&#24615;&#22320;&#29992;&#20110;&#24320;&#21457;&#20855;&#26377;&#22686;&#24378;&#21151;&#33021;&#30340;&#34507;&#30333;&#36136;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#27979;&#20248;&#20110;&#37326;&#29983;&#22411;&#34507;&#30333;&#36136;&#30340;&#34507;&#30333;&#36136;&#21464;&#20307;&#26041;&#38754;&#65292;&#22522;&#20110;&#32467;&#26500;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;&#27604;&#36739;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#23384;&#22312;&#30740;&#31350;&#31354;&#30333;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#27604;&#36739;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;EGNN&#65289;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;&#35782;&#21035;&#26377;&#21069;&#36884;&#30340;&#27688;&#22522;&#37240;&#31361;&#21464;&#30340;&#33021;&#21147;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#31354;&#30333;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#32467;&#26500;&#26041;&#27861;&#22312;&#35757;&#32451;&#26356;&#23569;&#30340;&#20998;&#23376;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#23558;&#27979;&#23450;&#26631;&#35760;&#25968;&#25454;&#19982;&#32467;&#26500;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#19982;&#24207;&#21015;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25928;&#26524;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained models have been successful in many protein engineering tasks. Most notably, sequence-based models have achieved state-of-the-art performance on protein fitness prediction while structure-based models have been used experimentally to develop proteins with enhanced functions. However, there is a research gap in comparing structure- and sequence-based methods for predicting protein variants that are better than the wildtype protein. This paper aims to address this gap by conducting a comparative study between the abilities of equivariant graph neural networks (EGNNs) and sequence-based approaches to identify promising amino-acid mutations. The results show that our proposed structural approach achieves a competitive performance to sequence-based methods while being trained on significantly fewer molecules. Additionally, we find that combining assay labelled data with structure pre-trained models yields similar trends as with sequence pre-trained models.
&lt;/p&gt;</description></item><item><title>&#36716;&#25442;&#22120;&#27169;&#22411;&#22312;&#39044;&#27979;&#22810;&#36127;&#36733;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#20351;&#29992;&#20840;&#23616;&#35757;&#32451;&#31574;&#30053;&#27604;&#22810;&#21464;&#37327;&#21644;&#26412;&#22320;&#35757;&#32451;&#31574;&#30053;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#38477;&#20302;&#20102;21.8%&#21644;12.8%&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.10891</link><description>&lt;p&gt;
&#36716;&#25442;&#22120;&#35757;&#32451;&#31574;&#30053;&#29992;&#20110;&#39044;&#27979;&#22810;&#20010;&#36127;&#36733;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Transformer Training Strategies for Forecasting Multiple Load Time Series. (arXiv:2306.10891v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10891
&lt;/p&gt;
&lt;p&gt;
&#36716;&#25442;&#22120;&#27169;&#22411;&#22312;&#39044;&#27979;&#22810;&#36127;&#36733;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#20351;&#29992;&#20840;&#23616;&#35757;&#32451;&#31574;&#30053;&#27604;&#22810;&#21464;&#37327;&#21644;&#26412;&#22320;&#35757;&#32451;&#31574;&#30053;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#38477;&#20302;&#20102;21.8%&#21644;12.8%&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26410;&#26469;&#30340;&#26234;&#33021;&#30005;&#32593;&#20013;&#65292;&#20934;&#30830;&#30340;&#36127;&#36733;&#39044;&#27979;&#21487;&#20197;&#24110;&#21161;&#22312;&#26412;&#22320;&#24179;&#34913;&#20379;&#38656;&#65292;&#24182;&#38450;&#27490;&#30005;&#32593;&#25925;&#38556;&#12290;&#23613;&#31649;&#34987;&#30417;&#27979;&#30340;&#23458;&#25143;&#25968;&#37327;&#23558;&#38543;&#30528;&#19981;&#26029;&#25512;&#36827;&#30340;&#26234;&#33021;&#30005;&#34920;&#23433;&#35013;&#32780;&#22686;&#21152;&#65292;&#20294;&#27599;&#20010;&#23458;&#25143;&#30340;&#25968;&#25454;&#37327;&#22987;&#32456;&#26159;&#26377;&#38480;&#30340;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36716;&#25442;&#22120;&#36127;&#36733;&#39044;&#27979;&#27169;&#22411;&#26159;&#21542;&#21463;&#30410;&#20110;&#36716;&#31227;&#23398;&#20064;&#31574;&#30053;&#65292;&#21363;&#22312;&#22810;&#20010;&#23458;&#25143;&#30340;&#36127;&#36733;&#26102;&#38388;&#24207;&#21015;&#19978;&#35757;&#32451;&#20840;&#23616;&#30340;&#21333;&#21464;&#37327;&#27169;&#22411;&#12290;&#22312;&#20351;&#29992;&#20004;&#20010;&#21253;&#21547;&#25968;&#30334;&#20010;&#23458;&#25143;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20840;&#23616;&#35757;&#32451;&#31574;&#30053;&#20248;&#20110;&#30456;&#20851;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#22810;&#21464;&#37327;&#21644;&#26412;&#22320;&#35757;&#32451;&#31574;&#30053;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;&#19982;&#20854;&#20182;&#20004;&#31181;&#31574;&#30053;&#30456;&#27604;&#65292;&#20840;&#23616;&#35757;&#32451;&#31574;&#30053;&#22312;&#20174;&#26410;&#26469;&#19968;&#22825;&#21040;&#19968;&#20010;&#26376;&#30340;&#39044;&#27979;&#26102;&#38388;&#33539;&#22260;&#20869;&#65292;&#39044;&#27979;&#35823;&#24046;&#38477;&#20302;&#20102;21.8%&#21644;12.8%&#12290;&#19982;&#32447;&#24615;&#27169;&#22411;&#12289;&#22810;&#23618;&#24863;&#30693;&#26426;&#21644;LSTM&#27169;&#22411;&#30340;&#27604;&#36739;&#26174;&#31034;&#65292;&#36716;&#25442;&#22120;&#35757;&#32451;&#31574;&#30053;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the smart grid of the future, accurate load forecasts on the level of individual clients can help to balance supply and demand locally and to prevent grid outages. While the number of monitored clients will increase with the ongoing smart meter rollout, the amount of data per client will always be limited. We evaluate whether a Transformer load forecasting model benefits from a transfer learning strategy, where a global univariate model is trained on the load time series from multiple clients. In experiments with two datasets containing load time series from several hundred clients, we find that the global training strategy is superior to the multivariate and local training strategies used in related work. On average, the global training strategy results in 21.8% and 12.8% lower forecasting errors than the two other strategies, measured across forecasting horizons from one day to one month into the future. A comparison to linear models, multi-layer perceptrons and LSTMs shows that T
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#65292;&#36890;&#36807;&#20943;&#23569;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#65292;&#20063;&#33021;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.10125</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#20998;&#31867;&#12289;&#36827;&#23637;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects. (arXiv:2306.10125v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10125
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#65292;&#36890;&#36807;&#20943;&#23569;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#65292;&#20063;&#33021;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#12290;SSL&#26368;&#31361;&#20986;&#30340;&#20248;&#21183;&#26159;&#20943;&#23569;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#22522;&#20110;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;&#19982;&#35768;&#22810;&#20851;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#32508;&#36848;&#30456;&#27604;&#65292;&#30446;&#21069;&#36824;&#32570;&#20047;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;SSL&#30340;&#32508;&#36848;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#22238;&#39038;&#20102;&#24403;&#21069;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20840;&#38754;&#22238;&#39038;&#20102;&#19982;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;&#26102;&#38388;&#24207;&#21015;&#30456;&#20851;&#30340;&#29616;&#26377;&#32508;&#36848;&#65292;&#28982;&#21518;&#36890;&#36807;&#24635;&#32467;&#20174;&#29983;&#25104;&#22411;&#12289;&#23545;&#27604;&#22411;&#21644;&#23545;&#25239;&#22411;&#19977;&#20010;&#35282;&#24230;&#23545;&#29616;&#26377;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#26032;&#30340;&#20998;&#31867;&#12290;&#36825;&#20123;&#26041;&#27861;&#36827;&#19968;&#27493;&#32454;&#20998;&#20026;&#21313;&#20010;&#23376;&#31867;&#65292;&#35814;&#32454;&#22238;&#39038;&#21644;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20851;&#38190;&#30452;&#35273;&#12289;&#20027;&#35201;&#26694;&#26550;&#12289;&#20248;&#21183;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently achieved impressive performance on various time series tasks. The most prominent advantage of SSL is that it reduces the dependence on labeled data. Based on the pre-training and fine-tuning strategy, even a small amount of labeled data can achieve high performance. Compared with many published self-supervised surveys on computer vision and natural language processing, a comprehensive survey for time series SSL is still missing. To fill this gap, we review current state-of-the-art SSL methods for time series data in this article. To this end, we first comprehensively review existing surveys related to SSL and time series, and then provide a new taxonomy of existing time series SSL methods by summarizing them from three perspectives: generative-based, contrastive-based, and adversarial-based. These methods are further divided into ten subcategories with detailed reviews and discussions about their key intuitions, main frameworks, advantages an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26410;&#26631;&#27880;&#35270;&#39057;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21512;&#25104;&#38899;&#39057;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#27169;&#24577;&#20316;&#20026;&#26725;&#26753;&#26469;&#23398;&#20064;&#25991;&#26412;-&#38899;&#39057;&#23545;&#24212;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#26377;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29983;&#25104;&#35270;&#39057;&#30340;&#38899;&#36712;&#12290;&#20351;&#29992;CLIP&#22270;&#20687;&#26597;&#35810;&#26465;&#20214;&#36827;&#34892;&#38646;&#26679;&#26412;&#27169;&#24577;&#36716;&#25442;&#12290;&#24182;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#20808;&#39564;&#27169;&#22411;&#65292;&#29983;&#25104;&#23545;&#24212;&#20110;CLIP&#25991;&#26412;&#23884;&#20837;&#30340;CLIP&#22270;&#20687;&#23884;&#20837;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09635</link><description>&lt;p&gt;
CLIPSonic&#65306;&#20351;&#29992;&#26410;&#26631;&#27880;&#35270;&#39057;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#30340;&#25991;&#26412;&#21512;&#25104;&#38899;&#39057;
&lt;/p&gt;
&lt;p&gt;
CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained Language-Vision Models. (arXiv:2306.09635v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26410;&#26631;&#27880;&#35270;&#39057;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21512;&#25104;&#38899;&#39057;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#27169;&#24577;&#20316;&#20026;&#26725;&#26753;&#26469;&#23398;&#20064;&#25991;&#26412;-&#38899;&#39057;&#23545;&#24212;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#26377;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29983;&#25104;&#35270;&#39057;&#30340;&#38899;&#36712;&#12290;&#20351;&#29992;CLIP&#22270;&#20687;&#26597;&#35810;&#26465;&#20214;&#36827;&#34892;&#38646;&#26679;&#26412;&#27169;&#24577;&#36716;&#25442;&#12290;&#24182;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#20808;&#39564;&#27169;&#22411;&#65292;&#29983;&#25104;&#23545;&#24212;&#20110;CLIP&#25991;&#26412;&#23884;&#20837;&#30340;CLIP&#22270;&#20687;&#23884;&#20837;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#37327;&#25104;&#23545;&#30340;&#25991;&#26412;&#21644;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#25991;&#26412;&#21512;&#25104;&#38899;&#39057;&#12290; &#28982;&#32780;&#65292;&#24102;&#26377;&#39640;&#36136;&#37327;&#25991;&#26412;&#27880;&#37322;&#30340;&#38899;&#39057;&#24405;&#38899;&#21487;&#33021;&#38590;&#20197;&#33719;&#21462;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#26469;&#36827;&#34892;&#25991;&#26412;&#21512;&#25104;&#38899;&#39057;&#30340;&#30740;&#31350;&#12290; &#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#35270;&#35273;&#27169;&#24577;&#20316;&#20026;&#26725;&#26753;&#26469;&#23398;&#20064;&#25152;&#38656;&#30340;&#25991;&#26412;-&#38899;&#39057;&#23545;&#24212;&#20851;&#31995;&#12290; &#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#26377;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#35270;&#39057;&#30340;&#38899;&#39057;&#36712;&#36947;&#65292;&#32473;&#23450;&#30001;&#39044;&#35757;&#32451;&#30340;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27169;&#22411;&#32534;&#30721;&#30340;&#35270;&#39057;&#24103;&#12290; &#22312;&#27979;&#35797;&#26102;&#38388;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#32034;&#25191;&#34892;&#38646;&#26679;&#26412;&#27169;&#24577;&#36716;&#25442;&#65292;&#24182;&#20351;&#29992;CLIP&#32534;&#30721;&#30340;&#25991;&#26412;&#26597;&#35810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#12290; &#20294;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19982;&#22270;&#20687;&#26597;&#35810;&#30456;&#27604;&#23384;&#22312;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290; &#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#20808;&#39564;&#27169;&#22411;&#65292;&#29983;&#25104;&#32473;&#23450;CLIP&#25991;&#26412;&#23884;&#20837;&#30340;CLIP&#22270;&#20687;&#23884;&#20837;&#12290; &#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;
&lt;/p&gt;
&lt;p&gt;
Recent work has studied text-to-audio synthesis using large amounts of paired text-audio data. However, audio recordings with high-quality text annotations can be difficult to acquire. In this work, we approach text-to-audio synthesis using unlabeled videos and pretrained language-vision models. We propose to learn the desired text-audio correspondence by leveraging the visual modality as a bridge. We train a conditional diffusion model to generate the audio track of a video, given a video frame encoded by a pretrained contrastive language-image pretraining (CLIP) model. At test time, we first explore performing a zero-shot modality transfer and condition the diffusion model with a CLIP-encoded text query. However, we observe a noticeable performance drop with respect to image queries. To close this gap, we further adopt a pretrained diffusion prior model to generate a CLIP image embedding given a CLIP text embedding. Our results show the effectiveness of the proposed method, and that 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#19968;&#20123;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#25216;&#26415;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#12289;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#19978;&#25366;&#25496;&#27169;&#24335;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09256</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#19968;&#20123;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#25216;&#26415;&#30340;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Survey of Some Density Based Clustering Techniques. (arXiv:2306.09256v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09256
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#19968;&#20123;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#25216;&#26415;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#12289;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#19978;&#25366;&#25496;&#27169;&#24335;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#26159;&#25968;&#25454;&#25366;&#25496;&#20013;&#20351;&#29992;&#30340;&#19968;&#31181;&#32858;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20197;&#21069;&#26410;&#30693;&#30340;&#27169;&#24335;&#12290;&#26377;&#35768;&#22810;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#22914;DBSCAN&#65292;OPTICS&#65292;DENCLUE&#65292;VDBSCAN&#65292;DVBSCAN&#65292;DBCLASD&#21644;ST-DBSCAN&#12290;&#26412;&#25991;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#12289;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#26368;&#37325;&#35201;&#30340;&#26159;&#23427;&#20204;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#25366;&#25496;&#26377;&#29992;&#21644;&#36866;&#24403;&#30340;&#27169;&#24335;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Density Based Clustering are a type of Clustering methods using in data mining for extracting previously unknown patterns from data sets. There are a number of density based clustering methods such as DBSCAN, OPTICS, DENCLUE, VDBSCAN, DVBSCAN, DBCLASD and ST-DBSCAN. In this paper, a study of these methods is done along with their characteristics, advantages and disadvantages and most importantly, their applicability to different types of data sets to mine useful and appropriate patterns.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#22312;&#39640;&#32500;&#35774;&#35745;&#31354;&#38388;&#20013;&#21516;&#26102;&#20248;&#21270;&#20102;&#24322;&#27493;&#30005;&#26426;&#21644;&#27704;&#30913;&#21516;&#27493;&#30005;&#26426;&#20004;&#31181;&#19981;&#21516;&#30340;&#30005;&#26426;&#25216;&#26415;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#35299;&#30721;&#22120;&#20316;&#20026;&#20803;&#27169;&#22411;&#65292;&#39044;&#27979;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#24182;&#29983;&#25104;&#26032;&#30340;&#35774;&#35745;&#12290;&#19982;&#32463;&#20856;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30452;&#25509;&#26041;&#27861;&#30456;&#27604;&#65292;VAE&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09087</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#30446;&#26631;&#30005;&#26426;&#25216;&#26415;&#20248;&#21270;&#30340;&#20803;&#27169;&#22411;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Deep learning based Meta-modeling for Multi-objective Technology Optimization of Electrical Machines. (arXiv:2306.09087v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#22312;&#39640;&#32500;&#35774;&#35745;&#31354;&#38388;&#20013;&#21516;&#26102;&#20248;&#21270;&#20102;&#24322;&#27493;&#30005;&#26426;&#21644;&#27704;&#30913;&#21516;&#27493;&#30005;&#26426;&#20004;&#31181;&#19981;&#21516;&#30340;&#30005;&#26426;&#25216;&#26415;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#35299;&#30721;&#22120;&#20316;&#20026;&#20803;&#27169;&#22411;&#65292;&#39044;&#27979;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#24182;&#29983;&#25104;&#26032;&#30340;&#35774;&#35745;&#12290;&#19982;&#32463;&#20856;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30452;&#25509;&#26041;&#27861;&#30456;&#27604;&#65292;VAE&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26059;&#36716;&#30005;&#26426;&#30340;&#20248;&#21270;&#26082;&#32791;&#26102;&#21448;&#38656;&#35201;&#35745;&#31639;&#36164;&#28304;&#12290;&#30001;&#20110;&#21442;&#25968;&#21270;&#19981;&#21516;&#65292;&#35774;&#35745;&#20248;&#21270;&#36890;&#24120;&#23545;&#27599;&#31181;&#30005;&#26426;&#25216;&#26415;&#20998;&#21035;&#25191;&#34892;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#21516;&#26102;&#20248;&#21270;&#20004;&#31181;&#19981;&#21516;&#30340;&#30005;&#26426;&#25216;&#26415;&#65292;&#21363;&#24322;&#27493;&#30005;&#26426;&#21644;&#27704;&#30913;&#21516;&#27493;&#30005;&#26426;&#12290;&#22312;&#35757;&#32451;&#20043;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#35299;&#30721;&#22120;&#20316;&#20026;&#20803;&#27169;&#22411;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#28508;&#22312;&#31354;&#38388;&#22312;&#20248;&#21270;&#24490;&#29615;&#20013;&#39044;&#27979;&#20840;&#23616;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#65288;KPI&#65289;&#24182;&#29983;&#25104;&#30456;&#20851;&#26032;&#35774;&#35745;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#20102;&#22312;&#39640;&#32500;&#35774;&#35745;&#31354;&#38388;&#20013;&#30340;&#24182;&#34892;&#21442;&#25968;&#21270;&#22810;&#30446;&#26631;&#25216;&#26415;&#20248;&#21270;&#12290;VAE&#26041;&#27861;&#22312;KPI&#39044;&#27979;&#19978;&#19982;&#32463;&#20856;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30452;&#25509;&#26041;&#27861;&#36827;&#34892;&#20102;&#23450;&#37327;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization of rotating electrical machines is both time- and computationally expensive. Because of the different parametrization, design optimization is commonly executed separately for each machine technology. In this paper, we present the application of a variational auto-encoder (VAE) to optimize two different machine technologies simultaneously, namely an asynchronous machine and a permanent magnet synchronous machine. After training, we employ a deep neural network and a decoder as meta-models to predict global key performance indicators (KPIs) and generate associated new designs, respectively, through unified latent space in the optimization loop. Numerical results demonstrate concurrent parametric multi-objective technology optimization in the high-dimensional design space. The VAE-based approach is quantitatively compared to a classical deep learning-based direct approach for KPIs prediction.
&lt;/p&gt;</description></item><item><title>NuCLR&#26159;&#19968;&#31181;&#21487;&#20197;&#39044;&#27979;&#21508;&#31181;&#26680;&#21487;&#35266;&#27979;&#37327;&#65292;&#21253;&#25324;&#32467;&#21512;&#33021;&#12289;&#34928;&#21464;&#33021;&#21644;&#26680;&#30005;&#33655;&#21322;&#24452;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#20849;&#20139;&#34920;&#31034;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#25429;&#25417;&#21040;&#26680;&#29289;&#29702;&#30340;&#22522;&#30784;&#29289;&#29702;&#21407;&#29702;&#65292;&#24182;&#26377;&#28508;&#21147;&#20026;&#26680;&#29702;&#35770;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.06099</link><description>&lt;p&gt;
NuCLR&#65306;&#26680;&#20849;&#23398;&#20064;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
NuCLR: Nuclear Co-Learned Representations. (arXiv:2306.06099v1 [nucl-th])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06099
&lt;/p&gt;
&lt;p&gt;
NuCLR&#26159;&#19968;&#31181;&#21487;&#20197;&#39044;&#27979;&#21508;&#31181;&#26680;&#21487;&#35266;&#27979;&#37327;&#65292;&#21253;&#25324;&#32467;&#21512;&#33021;&#12289;&#34928;&#21464;&#33021;&#21644;&#26680;&#30005;&#33655;&#21322;&#24452;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#20849;&#20139;&#34920;&#31034;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#25429;&#25417;&#21040;&#26680;&#29289;&#29702;&#30340;&#22522;&#30784;&#29289;&#29702;&#21407;&#29702;&#65292;&#24182;&#26377;&#28508;&#21147;&#20026;&#26680;&#29702;&#35770;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#20849;&#23398;&#20064;&#34920;&#31034;&#65288;NuCLR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#21508;&#31181;&#26680;&#21487;&#35266;&#27979;&#37327;&#65292;&#21253;&#25324;&#32467;&#21512;&#33021;&#12289;&#34928;&#21464;&#33021;&#21644;&#26680;&#30005;&#33655;&#21322;&#24452;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#20849;&#20139;&#34920;&#31034;&#27861;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#29702;&#35299;&#26680;&#65288;&#22825;&#20307;&#65289;&#29289;&#29702;&#22522;&#26412;&#29616;&#35937;&#25152;&#24517;&#38656;&#30340;&#31934;&#24230;&#27700;&#24179;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#21457;&#29616;&#65292;&#21363;NuCLR&#23398;&#20064;&#30340;&#34920;&#31034;&#23637;&#29616;&#20986;&#26680;&#22771;&#27169;&#22411;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#22771;&#23618;&#32467;&#26500;&#65292;&#21253;&#25324;&#20247;&#25152;&#21608;&#30693;&#30340;&#39764;&#25968;&#21644;&#27873;&#21033;&#25490;&#26021;&#21407;&#29702;&#12290;&#36825;&#34920;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#21040;&#22522;&#30784;&#29289;&#29702;&#21407;&#29702;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#28508;&#21147;&#20026;&#26680;&#29702;&#35770;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Nuclear Co-Learned Representations (NuCLR), a deep learning model that predicts various nuclear observables, including binding and decay energies, and nuclear charge radii. The model is trained using a multi-task approach with shared representations and obtains state-of-the-art performance, achieving levels of precision that are crucial for understanding fundamental phenomena in nuclear (astro)physics. We also report an intriguing finding that the learned representation of NuCLR exhibits the prominent emergence of crucial aspects of the nuclear shell model, namely the shell structure, including the well-known magic numbers, and the Pauli Exclusion Principle. This suggests that the model is capable of capturing the underlying physical principles and that our approach has the potential to offer valuable insights into nuclear theory.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.03341</link><description>&lt;p&gt;
&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65306;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#23548;&#20986;&#30495;&#23454;&#30340;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30495;&#23454;&#24615;&#12290;ITI&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#27839;&#30528;&#19968;&#32452;&#26041;&#21521;&#31227;&#21160;&#27169;&#22411;&#28608;&#27963;&#65292;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#12290;&#36825;&#31181;&#24178;&#39044;&#26174;&#30528;&#25552;&#39640;&#20102;LLaMA&#27169;&#22411;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#22312;&#25351;&#20196;&#24494;&#35843;&#30340;LLaMA Alpaca&#19978;&#65292;ITI&#23558;&#20854;&#30495;&#23454;&#24615;&#20174;32.5&#65285;&#25552;&#39640;&#21040;65.1&#65285;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#30495;&#23454;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35843;&#25972;&#24178;&#39044;&#24378;&#24230;&#26469;&#24179;&#34913;&#23427;&#12290;ITI &#21462;&#24471;&#20102;&#26368;&#20302;&#31243;&#24230;&#30340;&#24178;&#25200;&#19988;&#35745;&#31639;&#24265;&#20215;&#12290;&#27492;&#22806;&#65292;&#35813;&#25216;&#26415;&#22312;&#25968;&#25454;&#25928;&#29575;&#19978;&#34920;&#29616;&#20248;&#24322;&#65306;&#34429;&#28982;&#20687;RLHF&#36825;&#26679;&#30340;&#26041;&#27861;&#38656;&#35201;&#24191;&#27867;&#27880;&#37322;&#65292;&#20294;&#26159;ITI&#20165;&#20351;&#29992;&#20102;&#20960;&#30334;&#20010;&#20363;&#23376;&#23601;&#33021;&#23450;&#20301;&#30495;&#23454;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#33021;&#20855;&#26377;&#26576;&#31181;&#20869;&#37096;&#34920;&#31034;&#26041;&#27861;&#26469;&#34920;&#31034;&#26576;&#20107;&#26159;&#30495;&#23454;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#20351;&#23427;&#20204;&#22312;&#34920;&#38754;&#19978;&#20135;&#29983;&#20102;&#34394;&#20551;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;ConvNets&#38656;&#35201;&#20381;&#36182;&#22270;&#20687;&#20142;&#24230;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#21487;&#35270;&#21270;&#36827;&#34892;&#20102;&#23637;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.00360</link><description>&lt;p&gt;
ConvNets&#26159;&#22914;&#20309;&#29702;&#35299;&#22270;&#20687;&#20142;&#24230;&#30340;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Do ConvNets Understand Image Intensity?. (arXiv:2306.00360v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00360
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;ConvNets&#38656;&#35201;&#20381;&#36182;&#22270;&#20687;&#20142;&#24230;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#21487;&#35270;&#21270;&#36827;&#34892;&#20102;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(ConvNets)&#36890;&#24120;&#20381;&#36182;&#36793;&#32536;/&#24418;&#29366;&#20449;&#24687;&#26469;&#20998;&#31867;&#22270;&#20687;&#12290;&#36807;&#21435;&#21313;&#24180;&#38388;&#24320;&#21457;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#30830;&#35748;ConvNets&#20381;&#38752;&#36793;&#32536;&#20449;&#24687;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;ConvNets&#38656;&#35201;&#38500;&#24418;&#29366;&#20449;&#24687;&#22806;&#36824;&#38656;&#20381;&#36182;&#22270;&#20687;&#20142;&#24230;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#36890;&#36807;&#21487;&#35270;&#21270;&#23637;&#31034;&#20102;ConvNets&#20381;&#36182;&#22270;&#20687;&#20142;&#24230;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks (ConvNets) usually rely on edge/shape information to classify images. Visualization methods developed over the last decade confirm that ConvNets rely on edge information. We investigate situations where the ConvNet needs to rely on image intensity in addition to shape. We show that the ConvNet relies on image intensity information using visualization.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20998;&#23376;&#23545;&#25509;&#21644;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#26041;&#27861;&#65292;&#31579;&#36873;&#20986;&#38024;&#23545; SARS-CoV-2&#30340;&#20027;&#35201;&#34507;&#30333;&#37238;3CL&#28508;&#22312;&#27835;&#30103;&#33647;&#29289;&#12290;&#20854;&#20013;&#65292;&#20915;&#31574;&#26641;&#22238;&#24402;&#65288;DTR&#65289;&#27169;&#22411;&#20855;&#26377;&#25913;&#36827;&#30340;&#32479;&#35745;&#25514;&#26045;R2&#21644;RMSE&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#20855;&#26377;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#26377;&#21033;&#30340;&#32467;&#21512;&#33021;&#30340;&#33647;&#29289;&#12290;</title><link>http://arxiv.org/abs/2305.18088</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#23376;&#23545;&#25509;&#21644;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#26041;&#27861;&#30340;&#33647;&#29289;&#37325;&#29992;&#20197;&#38774;&#21521;COVID-19 3CL Protease
&lt;/p&gt;
&lt;p&gt;
Drug Repurposing Targeting COVID-19 3CL Protease using Molecular Docking and Machine Learning Regression Approach. (arXiv:2305.18088v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20998;&#23376;&#23545;&#25509;&#21644;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#26041;&#27861;&#65292;&#31579;&#36873;&#20986;&#38024;&#23545; SARS-CoV-2&#30340;&#20027;&#35201;&#34507;&#30333;&#37238;3CL&#28508;&#22312;&#27835;&#30103;&#33647;&#29289;&#12290;&#20854;&#20013;&#65292;&#20915;&#31574;&#26641;&#22238;&#24402;&#65288;DTR&#65289;&#27169;&#22411;&#20855;&#26377;&#25913;&#36827;&#30340;&#32479;&#35745;&#25514;&#26045;R2&#21644;RMSE&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#20855;&#26377;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#26377;&#21033;&#30340;&#32467;&#21512;&#33021;&#30340;&#33647;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#24050;&#32463;&#25104;&#20026;&#20840;&#29699;&#20581;&#24247;&#21361;&#26426;&#65292;&#36843;&#20999;&#38656;&#35201;&#24555;&#36895;&#37492;&#23450;&#28508;&#22312;&#30340;&#27835;&#30103;&#33647;&#29289;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#33647;&#29289;&#37325;&#29992;&#26159;&#30465;&#26102;&#30465;&#21147;&#30340;&#21807;&#19968;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;Zinc&#25968;&#25454;&#24211;&#23545;&#20840;&#29699;&#24050;&#25209;&#20934;&#65288;&#21253;&#25324;FDA&#25209;&#20934;&#65289;&#30340;5903&#31181;&#33647;&#29289;&#36827;&#34892;&#31579;&#36873;&#65292;&#20316;&#20026;&#28508;&#22312;&#30340;COVID-19&#27835;&#30103;&#33647;&#29289;&#65292;&#20197;&#38774;&#21521;SARS-CoV-2&#30340;&#20027;&#35201;&#34507;&#30333;&#37238;3CL&#12290;&#25105;&#20204;&#20351;&#29992;Autodock-Vina&#36827;&#34892;&#20998;&#23376;&#23545;&#25509;&#65292;&#26816;&#26597;&#33647;&#29289;&#20998;&#23376;&#30340;&#21151;&#25928;&#12290;&#20026;&#20102;&#25552;&#39640;&#33647;&#29289;&#37325;&#29992;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#37319;&#29992;&#20915;&#31574;&#26641;&#12289;&#39069;&#22806;&#26641;&#12289;MLP&#12289;KNN&#12289;XGBoost&#21644;&#26799;&#24230;&#25552;&#21319;&#31561;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#26041;&#27861;&#24314;&#27169;&#32467;&#21512;&#33647;&#29289;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;&#35745;&#31639;&#32467;&#26524;&#34920;&#26126;&#65292;&#20915;&#31574;&#26641;&#22238;&#24402;&#65288;DTR&#65289;&#27169;&#22411;&#20855;&#26377;&#25913;&#36827;&#30340;&#32479;&#35745;&#25514;&#26045;R2&#21644;RMSE&#12290;&#36825;&#20123;&#27169;&#25311;&#32467;&#26524;&#26377;&#21161;&#20110;&#35782;&#21035;&#20855;&#26377;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#26377;&#21033;&#30340;&#32467;&#21512;&#33021;&#30340;&#33647;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has created a global health crisis, driving the need for the rapid identification of potential therapeutics. To meet this challenge, drug repurposing is the only solution with saving cost and time. In this study, we used the Zinc database to screen the world-approved including FDA-approved 5903 drugs for repurposing as potential COVID-19 treatments targeting the main protease 3CL of SARS-CoV-2. We performed molecular docking using Autodock-Vina to check the efficacy of drug molecules. To enhance the efficiency of drug repurposing approach, we modeled the binding affinities using several machine learning regression approaches for QSAR modeling such as decision tree, extra trees, MLP, KNN, XGBoost, and gradient boosting. The computational results demonstrated that Decision Tree Regression (DTR) model has improved statistical measures of R2 and RMSE. These simulated results helped to identify drugs with high binding affinity and favorable binding energies. From the s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#28789;&#27963;&#30340;PFN&#20316;&#20026;BO&#20195;&#29702;&#24314;&#27169;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20801;&#35768;&#36827;&#19968;&#27493;&#20449;&#24687;&#32435;&#20837;&#20197;&#36827;&#34892;&#38750;&#36828;&#35270;BO&#12290;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17535</link><description>&lt;p&gt;
PFN&#26159;&#36866;&#29992;&#20110;&#23454;&#38469;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#28789;&#27963;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
PFNs Are Flexible Models for Real-World Bayesian Optimization. (arXiv:2305.17535v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#28789;&#27963;&#30340;PFN&#20316;&#20026;BO&#20195;&#29702;&#24314;&#27169;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20801;&#35768;&#36827;&#19968;&#27493;&#20449;&#24687;&#32435;&#20837;&#20197;&#36827;&#34892;&#38750;&#36828;&#35270;BO&#12290;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20808;&#39564;&#25968;&#25454;&#25311;&#21512;&#32593;&#32476;(PFNs)&#20316;&#20026;&#36125;&#21494;&#26031;&#20248;&#21270;(BO)&#30340;&#28789;&#27963;&#20195;&#29702;&#12290;PFN&#26159;&#19968;&#31181;&#31070;&#32463;&#36807;&#31243;&#65292;&#34987;&#35757;&#32451;&#29992;&#20110;&#36817;&#20284;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;(PPD)&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#21487;&#26377;&#25928;&#37319;&#26679;&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#31181;&#28789;&#27963;&#24615;&#26469;&#36827;&#34892;BO&#30340;&#20195;&#29702;&#24314;&#27169;&#12290;&#25105;&#20204;&#20351;&#29992;PFN&#26469;&#27169;&#25311;&#19968;&#20010;&#26420;&#32032;&#39640;&#26031;&#36807;&#31243;(GP)&#65292;&#19968;&#20010;&#20808;&#36827;&#30340;GP&#21644;&#19968;&#20010;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(BNN)&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#36827;&#19968;&#27493;&#30340;&#20449;&#24687;&#32435;&#20837;&#20808;&#39564;&#65292;&#20363;&#22914;&#20801;&#35768;&#26377;&#20851;&#26368;&#20248;&#20301;&#32622;&#30340;&#25552;&#31034;(&#29992;&#25143;&#20808;&#39564;)&#65292;&#24573;&#30053;&#19981;&#30456;&#20851;&#30340;&#32500;&#24230;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#33719;&#21462;&#20989;&#25968;&#26469;&#25191;&#34892;&#38750;&#36828;&#35270;BO&#12290;&#36825;&#20123;&#25193;&#23637;&#30340;&#28789;&#27963;&#24615;&#20026;&#20351;&#29992;PFN&#36827;&#34892;BO&#24320;&#36767;&#20102;&#24191;&#38420;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#22312;&#20154;&#24037;&#39640;&#26031;&#36807;&#31243;&#26679;&#26412;&#21644;&#19977;&#20010;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#27979;&#35797;&#24179;&#21488;&#19978;&#23637;&#31034;&#20102;PFN&#23545;BO&#30340;&#26377;&#29992;&#24615;&#65306;HPO-B&#12289;Bayesmark&#21644;PD1&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we use Prior-data Fitted Networks (PFNs) as a flexible surrogate for Bayesian Optimization (BO). PFNs are neural processes that are trained to approximate the posterior predictive distribution (PPD) for any prior distribution that can be efficiently sampled from. We describe how this flexibility can be exploited for surrogate modeling in BO. We use PFNs to mimic a naive Gaussian process (GP), an advanced GP, and a Bayesian Neural Network (BNN). In addition, we show how to incorporate further information into the prior, such as allowing hints about the position of optima (user priors), ignoring irrelevant dimensions, and performing non-myopic BO by learning the acquisition function. The flexibility underlying these extensions opens up vast possibilities for using PFNs for BO. We demonstrate the usefulness of PFNs for BO in a large-scale evaluation on artificial GP samples and three different hyperparameter optimization testbeds: HPO-B, Bayesmark, and PD1. We publish code 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#22312;&#20840;&#36830;&#25509;&#32447;&#24615;&#23618;&#20043;&#21069;&#28155;&#21152;&#19968;&#20010;dropout&#23618;&#65292;&#21487;&#20197;&#32531;&#35299;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16179</link><description>&lt;p&gt;
Dropout&#21487;&#20197;&#32531;&#35299;&#21452;&#19979;&#38477;&#29616;&#35937;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dropout Drops Double Descent. (arXiv:2305.16179v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#22312;&#20840;&#36830;&#25509;&#32447;&#24615;&#23618;&#20043;&#21069;&#28155;&#21152;&#19968;&#20010;dropout&#23618;&#65292;&#21487;&#20197;&#32531;&#35299;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#24182;&#20998;&#26512;&#65292;&#36890;&#36807;&#22312;&#20840;&#36830;&#25509;&#32447;&#24615;&#23618;&#20043;&#21069;&#28155;&#21152;&#19968;&#20010;dropout&#23618;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#32531;&#35299;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#20844;&#20247;&#30340;&#20851;&#27880;&#65292;&#21363;&#38543;&#30528;&#26679;&#26412;&#25110;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#39044;&#27979;&#35823;&#24046;&#20250;&#20808;&#19978;&#21319;&#20877;&#19979;&#38477;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#20004;&#20010;&#26041;&#38754;&#35777;&#26126;&#65292;&#36890;&#36807;&#22312;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#21644;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#22238;&#24402;&#20013;&#20351;&#29992;&#26368;&#20339;&#30340;dropout&#65292;&#21487;&#20197;&#32531;&#35299;&#36825;&#20123;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we find and analyze that we can easily drop the double descent by only adding one dropout layer before the fully-connected linear layer. The surprising double-descent phenomenon has drawn public attention in recent years, making the prediction error rise and drop as we increase either sample or model size. The current paper shows that it is possible to alleviate these phenomena by using optimal dropout in the linear regression model and the nonlinear random feature regression, both theoretically and empirically. % ${y}=X{\beta}^0+{\epsilon}$ with $X\in\mathbb{R}^{n\times p}$. We obtain the optimal dropout hyperparameter by estimating the ground truth ${\beta}^0$ with generalized ridge typed estimator $\hat{{\beta}}=(X^TX+\alpha\cdot\mathrm{diag}(X^TX))^{-1}X^T{y}$. Moreover, we empirically show that optimal dropout can achieve a monotonic test error curve in nonlinear neural networks using Fashion-MNIST and CIFAR-10. Our results suggest considering dropout for risk curve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;MLP&#39034;&#24207;&#25512;&#33616;&#26550;&#26500;TriMLP&#65292;&#20854;&#20013;&#21152;&#20837;&#20102;&#26032;&#39062;&#30340;&#19977;&#35282;&#24418;&#28151;&#21512;&#22120;&#20197;&#23454;&#29616;&#26631;&#35760;&#26377;&#24207;&#30340;&#20132;&#20114;&#65292;&#20197;&#25552;&#39640;&#39034;&#24207;&#25512;&#33616;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14675</link><description>&lt;p&gt;
MLP&#22312;&#39034;&#24207;&#25512;&#33616;&#20013;&#30340;&#22797;&#20167;
&lt;/p&gt;
&lt;p&gt;
Revenge of MLP in Sequential Recommendation. (arXiv:2305.14675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;MLP&#39034;&#24207;&#25512;&#33616;&#26550;&#26500;TriMLP&#65292;&#20854;&#20013;&#21152;&#20837;&#20102;&#26032;&#39062;&#30340;&#19977;&#35282;&#24418;&#28151;&#21512;&#22120;&#20197;&#23454;&#29616;&#26631;&#35760;&#26377;&#24207;&#30340;&#20132;&#20114;&#65292;&#20197;&#25552;&#39640;&#39034;&#24207;&#25512;&#33616;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#23545;&#21382;&#21490;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#34892;&#20026;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#26356;&#22909;&#22320;&#25512;&#26029;&#21160;&#24577;&#20559;&#22909;&#12290;&#36817;&#24180;&#26469;&#65292;&#24471;&#30410;&#20110;&#25913;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22914;RNN&#12289;CNN&#21644;Transformer&#65292;&#36825;&#20010;&#39046;&#22495;&#24050;&#32463;&#36814;&#26469;&#20102;&#24555;&#36895;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26368;&#36817;&#65292;&#20840;MLP&#27169;&#22411;&#30340;&#30740;&#31350;&#25104;&#26524;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#30340;&#27880;&#24847;&#65292;&#21363;&#36890;&#36807;&#28151;&#21512;&#21382;&#21490;&#34892;&#20026;&#30340;MLP&#23398;&#20064;&#36716;&#25442;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#31181;&#20840;&#36830;&#25509;&#32467;&#26500;&#20801;&#35768;&#19981;&#21463;&#38480;&#21046;&#30340;&#36328;&#34892;&#20026;&#38388;&#36890;&#20449;&#24182;&#24573;&#30053;&#20102;&#26102;&#38388;&#39034;&#24207;&#65292;&#25105;&#20204;&#21457;&#29616;&#30452;&#25509;&#23558;&#28151;&#21512;MLP&#24212;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#20250;&#23548;&#33268;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;MLP&#39034;&#24207;&#25512;&#33616;&#26550;&#26500;TriMLP&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#35282;&#24418;&#28151;&#21512;&#22120;&#65292;&#25913;&#36827;&#21518;&#30340;MLP&#36171;&#20104;&#20102;&#26631;&#35760;&#26377;&#24207;&#30340;&#20132;&#20114;&#12290;&#30001;&#20110;MLP&#20013;&#30340;&#36328;&#26631;&#35760;&#20132;&#20114;&#23454;&#38469;&#19978;&#26159;&#30697;&#38453;...
&lt;/p&gt;
&lt;p&gt;
Sequential recommendation models sequences of historical user-item interactive behaviors (or referred as token) to better infer dynamic preferences. Fueled by the improved neural network architectures such as RNN, CNN and Transformer, this field has enjoyed rapid performance boost in the past years. Recent progress on all-MLP models lights on an efficient method with less intensive computation, token-mixing MLP, to learn the transformation patterns among historical behaviors. However, due to the inherent fully-connection design that allows the unrestricted cross-token communication and ignores the chronological order, we find that directly applying token-mixing MLP into sequential recommendation leads to subpar performance. In this paper, we present a purely MLP-based sequential recommendation architecture TriMLP with a novel \underline{Tri}angular Mixer where the modified \underline{MLP} endows tokens with ordered interactions. As the cross-token interaction in MLP is actually matrix 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;OL-Transformer&#29992;&#20110;&#20809;&#23398;&#22810;&#23618;&#34180;&#33180;&#32467;&#26500;&#65292;&#21487;&#20197;&#39044;&#27979;&#22810;&#36798;$10^{25}$&#31181;&#19981;&#21516;&#22810;&#23618;&#32467;&#26500;&#30340;&#31934;&#30830;&#21453;&#23556;&#21644;&#36879;&#23556;&#20809;&#35889;&#65292;&#21516;&#26102;&#20855;&#26377;&#24555;&#36895;&#30340;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.11984</link><description>&lt;p&gt;
OL-Transformer&#65306;&#29992;&#20110;&#20809;&#23398;&#22810;&#23618;&#34180;&#33180;&#32467;&#26500;&#30340;&#24555;&#36895;&#36890;&#29992;&#20195;&#29702;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
OL-Transformer: A Fast and Universal Surrogate Simulator for Optical Multilayer Thin Film Structures. (arXiv:2305.11984v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11984
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;OL-Transformer&#29992;&#20110;&#20809;&#23398;&#22810;&#23618;&#34180;&#33180;&#32467;&#26500;&#65292;&#21487;&#20197;&#39044;&#27979;&#22810;&#36798;$10^{25}$&#31181;&#19981;&#21516;&#22810;&#23618;&#32467;&#26500;&#30340;&#31934;&#30830;&#21453;&#23556;&#21644;&#36879;&#23556;&#20809;&#35889;&#65292;&#21516;&#26102;&#20855;&#26377;&#24555;&#36895;&#30340;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26368;&#36817;&#34987;&#35777;&#26126;&#26159;&#29992;&#20110;&#20809;&#23398;&#22810;&#23618;&#34180;&#33180;&#32467;&#26500;&#30340;&#24555;&#36895;&#20934;&#30830;&#30340;&#20195;&#29702;&#27169;&#25311;&#22120;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20165;&#36866;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#26448;&#26009;&#25490;&#21015;&#26041;&#24335;&#30340;&#26377;&#38480;&#31867;&#22411;&#30340;&#32467;&#26500;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#21521;&#22810;&#26679;&#21270;&#21644;&#36890;&#29992;&#21270;&#32467;&#26500;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Opto-Layer&#65288;OL&#65289;Transformer&#20316;&#20026;&#24040;&#37327;&#32467;&#26500;&#30340;&#36890;&#29992;&#26367;&#20195;&#27169;&#25311;&#22120;&#12290;&#32467;&#21512;&#32467;&#26500;&#24207;&#21015;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#22810;&#36798;$10^{25}$&#31181;&#19981;&#21516;&#22810;&#23618;&#32467;&#26500;&#30340;&#31934;&#30830;&#21453;&#23556;&#21644;&#36879;&#23556;&#20809;&#35889;&#65292;&#21516;&#26102;&#30456;&#36739;&#20110;&#29289;&#29702;&#27714;&#35299;&#22120;&#20173;&#28982;&#23454;&#29616;&#20102;6&#20493;&#26102;&#38388;&#21152;&#36895;&#12290;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26222;&#36941;&#30340;&#23398;&#20064;&#33021;&#21147;&#26469;&#33258;&#20110;&#25105;&#20204;&#30340;&#27169;&#22411;&#39318;&#20808;&#23398;&#20064;&#29289;&#29702;&#23884;&#20837;&#65292;&#28982;&#21518;&#20351;&#29992;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#27599;&#23618;&#20043;&#38388;&#30340;&#20809;&#29289;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#38544;&#34255;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based methods have recently been established as fast and accurate surrogate simulators for optical multilayer thin film structures. However, existing methods only work for limited types of structures with different material arrangements, preventing their applications towards diverse and universal structures. Here, we propose the Opto-Layer (OL) Transformer to act as a universal surrogate simulator for enormous types of structures. Combined with the technique of structure serialization, our model can predict accurate reflection and transmission spectra for up to $10^{25}$ different multilayer structures, while still achieving a six-fold time speedup compared to physical solvers. Further investigation reveals that the general learning ability comes from the fact that our model first learns the physical embeddings and then uses the self-attention mechanism to capture the hidden relationship of light-matter interaction between each layer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#38750;&#20108;&#20803;&#29305;&#24449;&#30340;&#20998;&#31867;&#22120;&#30340;&#26032;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#20915;&#31574;&#21644;&#22522;&#30784;&#20998;&#31867;&#22120;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.14760</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#20108;&#20803;&#29305;&#24449;&#30340;&#20998;&#31867;&#22120;&#30340;&#26032;&#22411;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Class of Explanations for Classifiers with Non-Binary Features. (arXiv:2304.14760v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#38750;&#20108;&#20803;&#29305;&#24449;&#30340;&#20998;&#31867;&#22120;&#30340;&#26032;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#20915;&#31574;&#21644;&#22522;&#30784;&#20998;&#31867;&#22120;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#24403;&#20998;&#26512;&#20998;&#31867;&#22120;&#20915;&#31574;&#26102;&#65292;&#24050;&#32463;&#26377;&#20004;&#31181;&#31867;&#22411;&#30340;&#35299;&#37322;&#21463;&#21040;&#20102;&#25991;&#29486;&#20013;&#30340;&#37325;&#35270;&#12290;&#31532;&#19968;&#31181;&#35299;&#37322;&#26159;&#20026;&#20915;&#31574;&#25552;&#20379;&#20805;&#20998;&#29702;&#30001;&#30340;&#35299;&#37322;&#65292;&#21363;&#32553;&#20889;&#20026;PI&#35299;&#37322;&#30340;&#35825;&#23548;&#24335;&#35299;&#37322;&#65307;&#31532;&#20108;&#31181;&#35299;&#37322;&#26159;&#20026;&#20309;&#19981;&#20570;&#20986;&#20854;&#20182;&#20915;&#31574;&#30340;&#35299;&#37322;&#65292;&#21363;&#23545;&#29031;&#24335;&#25110;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24517;&#35201;&#29702;&#30001;&#12290;&#36825;&#20123;&#35299;&#37322;&#26159;&#20026;&#20108;&#20803;&#12289;&#31163;&#25955;&#21644;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20026;&#36830;&#32493;&#29305;&#24449;&#30340;&#20998;&#31867;&#22120;&#23450;&#20041;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#23384;&#22312;&#38750;&#20108;&#20803;&#29305;&#24449;&#26102;&#65292;&#36825;&#20123;&#35299;&#37322;&#21487;&#20197;&#24471;&#21040;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#19968;&#31867;&#26032;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#20915;&#31574;&#21644;&#22522;&#30784;&#20998;&#31867;&#22120;&#30340;&#20449;&#24687;&#12290;&#24517;&#35201;&#21644;&#20805;&#20998;&#21407;&#22240;&#20063;&#34987;&#35777;&#26126;&#26159;&#23436;&#25972;&#21407;&#22240;&#30340;&#20027;&#35201;&#34164;&#21547;&#39033;&#21644;&#34987;&#34164;&#21547;&#39033;&#65292;&#21487;&#20197;&#20351;&#29992;&#37327;&#21270;&#31639;&#23376;&#33719;&#24471;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25913;&#36827;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#21407;&#22240;&#30340;&#27010;&#24565;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#36866;&#29992;&#20110;&#20855;&#26377;&#38750;&#20108;&#20803;&#29305;&#24449;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two types of explanations have received significant attention in the literature recently when analyzing the decisions made by classifiers. The first type explains why a decision was made and is known as a sufficient reason for the decision, also an abductive or PI-explanation. The second type explains why some other decision was not made and is known as a necessary reason for the decision, also a contrastive or counterfactual explanation. These explanations were defined for classifiers with binary, discrete and, in some cases, continuous features. We show that these explanations can be significantly improved in the presence of non-binary features, leading to a new class of explanations that relay more information about decisions and the underlying classifiers. Necessary and sufficient reasons were also shown to be the prime implicates and implicants of the complete reason for a decision, which can be obtained using a quantification operator. We show that our improved notions of necessa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20108;&#20803;&#21644;&#36830;&#32493;&#30340;&#36127;&#36793;&#36317;&#24863;&#30693;&#22120;&#20316;&#20026;&#38750;&#20984;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21457;&#29616;&#23427;&#20204;&#37117;&#23384;&#22312;&#30528;&#26497;&#20026;&#24179;&#22374;&#21644;&#23485;&#24191;&#30340;&#20122;&#20248;&#35299;&#65292;&#36825;&#23545;&#20110;&#20108;&#20803;&#24773;&#20917;&#20013;&#30340;&#31639;&#27861;&#34892;&#20026;&#26377;&#30528;&#24456;&#24378;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.13871</link><description>&lt;p&gt;
&#36830;&#32493;&#21644;&#31163;&#25955;&#26435;&#37325;&#19979;&#38750;&#20984;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20856;&#22411;&#19982;&#38750;&#20856;&#22411;&#35299;&#26512;&#35299;
&lt;/p&gt;
&lt;p&gt;
Typical and atypical solutions in non-convex neural networks with discrete and continuous weights. (arXiv:2304.13871v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20108;&#20803;&#21644;&#36830;&#32493;&#30340;&#36127;&#36793;&#36317;&#24863;&#30693;&#22120;&#20316;&#20026;&#38750;&#20984;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21457;&#29616;&#23427;&#20204;&#37117;&#23384;&#22312;&#30528;&#26497;&#20026;&#24179;&#22374;&#21644;&#23485;&#24191;&#30340;&#20122;&#20248;&#35299;&#65292;&#36825;&#23545;&#20110;&#20108;&#20803;&#24773;&#20917;&#20013;&#30340;&#31639;&#27861;&#34892;&#20026;&#26377;&#30528;&#24456;&#24378;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20108;&#20803;&#21644;&#36830;&#32493;&#30340;&#36127;&#36793;&#36317;&#24863;&#30693;&#22120;&#20316;&#20026;&#31616;&#21333;&#30340;&#38750;&#20984;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#23398;&#20064;&#38543;&#26426;&#35268;&#21017;&#21644;&#20851;&#32852;&#26102;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#27169;&#22411;&#30340;&#35299;&#31354;&#38388;&#20960;&#20309;&#24418;&#24577;&#65292;&#24182;&#25214;&#21040;&#20102;&#37325;&#35201;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#12290;&#36825;&#20004;&#31181;&#27169;&#22411;&#37117;&#34920;&#29616;&#20986;&#26497;&#20026;&#24179;&#22374;&#21644;&#23485;&#24191;&#30340;&#20122;&#20248;&#35299;&#12290;&#36825;&#20123;&#20122;&#20248;&#35299;&#19982;&#20108;&#20803;&#24773;&#20917;&#19979;&#26080;&#27861;&#36882;&#24402;&#31639;&#27861;&#35775;&#38382;&#30340;&#22823;&#37327;&#34067;&#24310;&#23567;&#38598;&#32676;&#65288;&#20923;&#32467;1-RSB&#30456;&#65289;&#32452;&#25104;&#30340;&#20027;&#35201;&#35299;&#38598;&#20849;&#23384;&#65292;&#25110;&#32773;&#29699;&#38754;&#24773;&#20917;&#19979;&#19981;&#21516;&#22823;&#23567;&#32858;&#31867;&#30340;&#20998;&#23618;&#32467;&#26500;&#65288;&#23436;&#20840;RSB&#30456;&#65289;&#12290;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#24403;&#20132;&#21449;&#19968;&#23450;&#23494;&#24230;&#32422;&#26463;&#30340;&#38408;&#20540;&#26102;&#65292;&#23485;&#24191;&#24179;&#22374;&#26497;&#23567;&#20540;&#30340;&#23616;&#37096;&#29109;&#21464;&#24471;&#38750;&#21333;&#35843;&#65292;&#34920;&#26126;&#40065;&#26834;&#35299;&#30340;&#31354;&#38388;&#34987;&#20998;&#25104;&#20102;&#19981;&#36830;&#36890;&#30340;&#32452;&#20214;&#12290;&#36825;&#23545;&#20108;&#20803;&#27169;&#22411;&#20013;&#31639;&#27861;&#30340;&#34892;&#20026;&#20135;&#29983;&#20102;&#24456;&#24378;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#23427;&#26080;&#27861;&#35775;&#38382;&#21097;&#20313;&#30340;&#23396;&#31435;&#38598;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the binary and continuous negative-margin perceptrons as simple non-convex neural network models learning random rules and associations. We analyze the geometry of the landscape of solutions in both models and find important similarities and differences. Both models exhibit subdominant minimizers which are extremely flat and wide. These minimizers coexist with a background of dominant solutions which are composed by an exponential number of algorithmically inaccessible small clusters for the binary case (the frozen 1-RSB phase) or a hierarchical structure of clusters of different sizes for the spherical case (the full RSB phase). In both cases, when a certain threshold in constraint density is crossed, the local entropy of the wide flat minima becomes non-monotonic, indicating a break-up of the space of robust solutions into disconnected components. This has a strong impact on the behavior of algorithms in binary models, which cannot access the remaining isolated clusters. For
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#38656;&#27714;&#39044;&#27979;&#30340;&#38543;&#26426;MPC&#25511;&#21046;&#22120;&#65292;&#37319;&#29992;&#26426;&#20250;&#32422;&#26463;&#26469;&#26368;&#23567;&#21270;&#19981;&#30830;&#23450;&#30340;&#30005;&#21147;&#21644;&#28909;&#38656;&#27714;&#65292;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24773;&#26223;&#27861;&#37319;&#26679;&#22810;&#27493;&#38656;&#27714;&#36712;&#36857;&#12290;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#39564;&#35777;&#20102;&#35813;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12438</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#38656;&#27714;&#39044;&#27979;&#30340;&#33021;&#28304;&#20013;&#24515;&#38543;&#26426;MPC
&lt;/p&gt;
&lt;p&gt;
Stochastic MPC for energy hubs using data driven demand forecasting. (arXiv:2304.12438v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#38656;&#27714;&#39044;&#27979;&#30340;&#38543;&#26426;MPC&#25511;&#21046;&#22120;&#65292;&#37319;&#29992;&#26426;&#20250;&#32422;&#26463;&#26469;&#26368;&#23567;&#21270;&#19981;&#30830;&#23450;&#30340;&#30005;&#21147;&#21644;&#28909;&#38656;&#27714;&#65292;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24773;&#26223;&#27861;&#37319;&#26679;&#22810;&#27493;&#38656;&#27714;&#36712;&#36857;&#12290;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#39564;&#35777;&#20102;&#35813;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#28304;&#20013;&#24515;&#36890;&#36807;&#22810;&#31181;&#36716;&#25442;&#21644;&#20648;&#33021;&#32452;&#20214;&#23558;&#33021;&#28304;&#36164;&#28304;&#36827;&#34892;&#36716;&#25442;&#21644;&#20998;&#37197;&#12290;&#33021;&#28304;&#20013;&#24515;&#30340;&#26368;&#20339;&#36816;&#34892;&#21033;&#29992;&#20854;&#28789;&#27963;&#24615;&#26469;&#22686;&#21152;&#33021;&#28304;&#25928;&#29575;&#24182;&#20943;&#23569;&#36816;&#33829;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#38656;&#27714;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#32473;&#33021;&#28304;&#20013;&#24515;&#30340;&#20248;&#21270;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;MPC&#25511;&#21046;&#22120;&#65292;&#20351;&#29992;&#26426;&#20250;&#32422;&#26463;&#26469;&#26368;&#23567;&#21270;&#19981;&#30830;&#23450;&#30340;&#30005;&#21147;&#21644;&#28909;&#38656;&#27714;&#65292;&#37319;&#29992;&#21382;&#21490;&#25968;&#25454;&#26500;&#24314;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#29983;&#25104;&#26410;&#26469;&#30005;&#21147;&#21644;&#28909;&#38656;&#27714;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#20174;&#39044;&#27979;&#27169;&#22411;&#20013;&#37319;&#26679;&#22810;&#27493;&#38656;&#27714;&#36712;&#36857;&#65292;&#37319;&#29992;&#8220;&#24773;&#26223;&#27861;&#8221;&#35299;&#20915;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#27169;&#25311;&#33021;&#28304;&#20013;&#24515;&#27169;&#22411;&#21644;&#26469;&#33258;&#23454;&#38469;&#24314;&#31569;&#30340;&#38656;&#27714;&#25968;&#25454;&#19978;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#39044;&#27979;&#22120;&#21644;&#38543;&#26426;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy hubs convert and distribute energy resources by combining different energy inputs through multiple conversion and storage components. The optimal operation of the energy hub exploits its flexibility to increase the energy efficiency and reduce the operational costs. However, uncertainties in the demand present challenges to energy hub optimization. In this paper, we propose a stochastic MPC controller to minimize energy costs using chance constraints for the uncertain electricity and thermal demands. Historical data is used to build a demand prediction model based on Gaussian processes to generate a forecast of the future electricity and heat demands. The stochastic optimization problem is solved via the Scenario Approach by sampling multi-step demand trajectories from the derived prediction model. The performance of the proposed predictor and of the stochastic controller is verified on a simulated energy hub model and demand data from a real building.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#36890;&#29992;&#36924;&#36817;&#22120;&#20026;&#26500;&#24314;&#22359;&#65292;&#26500;&#24314;&#20102;&#22312;&#20219;&#24847;&#27874;&#20848;&#24230;&#37327;&#31354;&#38388; $\mathcal{X}$ &#21644; $\mathcal{Y}$ &#20043;&#38388;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#21270;&#36755;&#20986;&#31163;&#25955;&#27010;&#29575;&#27979;&#24230;&#26469;&#20811;&#26381;&#26576;&#20123;&#38480;&#21046;&#12290;&#22312;&#36866;&#24403;&#30340;&#32467;&#26500;&#19979;&#25552;&#20379;&#20102;&#27010;&#29575;&#21644;&#23450;&#37327;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.12231</link><description>&lt;p&gt;
&#19968;&#31181;&#36716;&#31227;&#21407;&#29702;&#65306;&#20174;&#27431;&#20960;&#37324;&#24471;&#36890;&#29992;&#36924;&#36817;&#22120;&#21040;&#24230;&#37327;&#31354;&#38388;&#20043;&#38388;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Transfer Principle: Universal Approximators Between Metric Spaces From Euclidean Universal Approximators. (arXiv:2304.12231v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#36890;&#29992;&#36924;&#36817;&#22120;&#20026;&#26500;&#24314;&#22359;&#65292;&#26500;&#24314;&#20102;&#22312;&#20219;&#24847;&#27874;&#20848;&#24230;&#37327;&#31354;&#38388; $\mathcal{X}$ &#21644; $\mathcal{Y}$ &#20043;&#38388;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#21270;&#36755;&#20986;&#31163;&#25955;&#27010;&#29575;&#27979;&#24230;&#26469;&#20811;&#26381;&#26576;&#20123;&#38480;&#21046;&#12290;&#22312;&#36866;&#24403;&#30340;&#32467;&#26500;&#19979;&#25552;&#20379;&#20102;&#27010;&#29575;&#21644;&#23450;&#37327;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#36890;&#29992;&#36924;&#36817;&#22120;&#20316;&#20026;&#26500;&#24314;&#22359;&#65292;&#26500;&#24314;&#20102;&#36830;&#32493;&#26144;&#23556;&#30340;&#24230;&#37327;&#31354;&#38388;&#20043;&#38388;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;&#26089;&#26399;&#32467;&#26524;&#20551;&#23450;&#36755;&#20986;&#31354;&#38388; $\mathcal{Y}$ &#26159;&#25299;&#25169;&#21521;&#37327;&#31354;&#38388;&#12290;&#25105;&#20204;&#36890;&#36807;&#8220;&#38543;&#26426;&#21270;&#8221;&#26469;&#20811;&#26381;&#36825;&#31181;&#38480;&#21046;&#65306;&#25105;&#20204;&#30340;&#36924;&#36817;&#22120;&#36755;&#20986; $\mathcal{Y}$ &#19978;&#30340;&#31163;&#25955;&#27010;&#29575;&#27979;&#24230;&#12290;&#24403; $\mathcal{X}$ &#21644; $\mathcal{Y}$ &#27809;&#26377;&#38468;&#21152;&#32467;&#26500;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#38750;&#24120;&#36890;&#29992;&#30340;&#23450;&#24615;&#20445;&#35777;&#65307;&#24403;&#23427;&#20204;&#20855;&#26377;&#36866;&#24403;&#30340;&#32452;&#21512;&#32467;&#26500;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102; H\"older &#31867;&#26144;&#23556;&#30340;&#23450;&#37327;&#20445;&#35777;&#65292;&#21253;&#25324;&#26377;&#38480;&#22270;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#22312;&#26576;&#20123; Carnot &#32676;&#20043;&#38388;&#30340;&#31895;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#23376;&#20197;&#21450;&#21453;&#38382;&#39064;&#20013;&#20986;&#29616;&#30340; Banach &#31354;&#38388;&#20043;&#38388;&#30340;&#36830;&#32493;&#38750;&#32447;&#24615;&#31639;&#23376;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#38656;&#30340; Dirac &#27979;&#24230;&#25968;&#37327;&#30001; $\mathcal{X}$ &#21644; $\mathcal{Y}$ &#30340;&#32452;&#21512;&#32467;&#26500;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
We build universal approximators of continuous maps between arbitrary Polish metric spaces $\mathcal{X}$ and $\mathcal{Y}$ using universal approximators between Euclidean spaces as building blocks. Earlier results assume that the output space $\mathcal{Y}$ is a topological vector space. We overcome this limitation by "randomization": our approximators output discrete probability measures over $\mathcal{Y}$. When $\mathcal{X}$ and $\mathcal{Y}$ are Polish without additional structure, we prove very general qualitative guarantees; when they have suitable combinatorial structure, we prove quantitative guarantees for H\"older-like maps, including maps between finite graphs, solution operators to rough differential equations between certain Carnot groups, and continuous non-linear operators between Banach spaces arising in inverse problems. In particular, we show that the required number of Dirac measures is determined by the combinatorial structure of $\mathcal{X}$ and $\mathcal{Y}$. For b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;BERT&#25216;&#26415;&#25506;&#31350;&#20102;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#20351;&#29992;BERT&#27169;&#22411;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#32456;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;80%&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;60%&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.08649</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#30340;&#25216;&#26415;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of US Supreme Court Cases using BERT-Based Techniques. (arXiv:2304.08649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;BERT&#25216;&#26415;&#25506;&#31350;&#20102;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#20351;&#29992;BERT&#27169;&#22411;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#32456;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;80%&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;60%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#65288;BERT&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#65288;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#65292;&#35789;&#24615;&#65288;POS&#65289;&#26631;&#35760;&#31561;&#65289;&#19978;&#20135;&#29983;&#20102;&#26368;&#26032;&#25216;&#26415;&#65288;SOTA&#65289;&#32467;&#26524;&#12290;&#24403;&#20998;&#31867;&#38271;&#25991;&#26723;&#65288;&#20363;&#22914;&#26469;&#33258;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#30340;&#25991;&#26723;&#65289;&#26102;&#65292;&#20351;&#29992;BERT&#27169;&#22411;&#21487;&#33021;&#27604;&#36739;&#22256;&#38590;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#20960;&#31181;&#22522;&#20110;BERT&#30340;&#20998;&#31867;&#25216;&#26415;&#65292;&#29992;&#20110;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#20915;&#23450;&#25110;&#26368;&#39640;&#27861;&#38498;&#25968;&#25454;&#24211;&#65288;SCDB&#65289;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#19982;&#20808;&#21069;&#30340;SOTA&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;&#38024;&#23545;&#38271;&#25991;&#26723;&#30340;SOTA&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;&#65288;1&#65289;&#24191;&#27867;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#20855;&#26377;15&#20010;&#31867;&#21035;&#65307;&#65288;2&#65289;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#20855;&#26377;279&#20010;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#32467;&#26524;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#20135;&#29983;80&#65285;&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#20135;&#29983;60&#65285;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models based on bidirectional encoder representations from transformers (BERT) produce state of the art (SOTA) results on many natural language processing (NLP) tasks such as named entity recognition (NER), part-of-speech (POS) tagging etc. An interesting phenomenon occurs when classifying long documents such as those from the US supreme court where BERT-based models can be considered difficult to use on a first-pass or out-of-the-box basis. In this paper, we experiment with several BERT-based classification techniques for US supreme court decisions or supreme court database (SCDB) and compare them with the previous SOTA results. We then compare our results specifically with SOTA models for long documents. We compare our results for two classification tasks: (1) a broad classification task with 15 categories and (2) a fine-grained classification task with 279 categories. Our best result produces an accuracy of 80\% on the 15 broad categories and 60\% on the fine-grained 279 categories 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#65292;&#26469;&#25913;&#21892;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#20195;&#30721;&#23457;&#26597;&#21644;&#20195;&#30721;&#26356;&#25913;&#30340;&#25968;&#25454;&#38598;&#24494;&#35843;&#65292;&#33021;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.07840</link><description>&lt;p&gt;
&#22522;&#20110;&#20195;&#30721;&#23457;&#26597;&#30340;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;: &#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#34920;&#29616;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
Automated Program Repair Based on Code Review: How do Pre-trained Transformer Models Perform?. (arXiv:2304.07840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#65292;&#26469;&#25913;&#21892;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#20195;&#30721;&#23457;&#26597;&#21644;&#20195;&#30721;&#26356;&#25913;&#30340;&#25968;&#25454;&#38598;&#24494;&#35843;&#65292;&#33021;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#23558;&#38169;&#35823;&#30340;&#31243;&#24207;&#36716;&#25442;&#20026;&#27491;&#30830;&#30340;&#31243;&#24207;&#65292;&#24403;&#23427;&#20204;&#34987;&#35757;&#32451;&#20351;&#29992;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#38598;&#26102;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#20063;&#35777;&#26126;&#20102;&#20195;&#30721;&#23457;&#26597;(&#20851;&#20110;&#20195;&#30721;&#20013;&#24314;&#35758;&#24615;&#26356;&#25913;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;)&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#31243;&#24207;&#20462;&#22797;&#12290;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21644;&#35745;&#31639;&#26426;&#31243;&#24207;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21253;&#21547;&#20004;&#31181;&#35821;&#35328;&#30340;&#22266;&#26377;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#20195;&#30721;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#22266;&#26377;&#30693;&#35782;&#26469;&#25552;&#39640;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#23558;PLBART&#21644;CodeT5&#36825;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#20004;&#20010;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#31243;&#24207;&#20462;&#22797;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#21253;&#21547;&#20195;&#30721;&#23457;&#26597;&#21644;&#38543;&#21518;&#20195;&#30721;&#26356;&#25913;&#30340;&#25968;&#25454;&#38598;&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#26126;&#26174;&#20248;&#20110;&#20043;&#21069;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#20174;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20462;&#22797;&#33021;&#21147;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;SOTA&#25216;&#26415;&#65292;CODET5&#21462;&#24471;&#20102;3%&#24038;&#21491;&#30340;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-sequence models have been used to transform erroneous programs into correct ones when trained with a large enough dataset. Some recent studies also demonstrated strong empirical evidence that code review (natural language instruction about suggestive changes in code) can improve the program repair further. Large language models, trained with Natural Language (NL) and computer program corpora, have the capacity to contain inherent knowledge of both. In this study, we investigate if this inherent knowledge of code and NL can be utilized to improve automated program repair. We applied PLBART and CodeT5, two state-of-the-art language models that are pre-trained with both Programming Language (PL) and Natural Language (NL), on two such natural language-based program repair datasets and found that the pre-trained language models fine-tuned with datasets containing both code review and subsequent code changes notably outperform each of the previous models. We observed that the pre
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#24314;&#31435;&#32479;&#35745;&#24418;&#29366;&#27169;&#22411;&#65292;&#20351;&#29992;&#28145;&#24230;&#20960;&#20309;&#29305;&#24449;&#21644;&#21151;&#33021;&#23545;&#24212;&#26469;&#21516;&#26102;&#23398;&#20064;&#22797;&#26434;&#35299;&#21078;&#32467;&#26500;&#20013;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#24418;&#29366;&#32467;&#26500;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#36275;&#22815;&#20581;&#22766;&#65292;&#33021;&#22815;&#20174;&#22122;&#22768;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20013;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.07515</link><description>&lt;p&gt;
S3M&#65306;&#36890;&#36807;&#26080;&#30417;&#30563;&#23545;&#24212;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#32479;&#35745;&#24418;&#29366;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
S3M: Scalable Statistical Shape Modeling through Unsupervised Correspondences. (arXiv:2304.07515v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07515
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#24314;&#31435;&#32479;&#35745;&#24418;&#29366;&#27169;&#22411;&#65292;&#20351;&#29992;&#28145;&#24230;&#20960;&#20309;&#29305;&#24449;&#21644;&#21151;&#33021;&#23545;&#24212;&#26469;&#21516;&#26102;&#23398;&#20064;&#22797;&#26434;&#35299;&#21078;&#32467;&#26500;&#20013;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#24418;&#29366;&#32467;&#26500;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#36275;&#22815;&#20581;&#22766;&#65292;&#33021;&#22815;&#20174;&#22122;&#22768;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20013;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#24418;&#29366;&#27169;&#22411;(SSM)&#26159;&#19968;&#31181;&#20960;&#20309;&#19978;&#34920;&#31034;&#20154;&#32676;&#35299;&#21078;&#23398;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#35786;&#26029;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#20294;&#26159;&#65292;&#36890;&#24120;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#36153;&#21147;&#30340;&#25163;&#21160;&#20998;&#21106;&#25110;&#22320;&#26631;&#27880;&#37322;&#26469;&#29983;&#25104;&#12290;&#23545;&#20110;SSM&#30340;&#23545;&#24212;&#20272;&#35745;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#36825;&#20123;&#26631;&#31614;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#20960;&#20309;&#29305;&#24449;&#21644;&#21151;&#33021;&#23545;&#24212;&#26469;&#21516;&#26102;&#23398;&#20064;&#22797;&#26434;&#35299;&#21078;&#32467;&#26500;&#20013;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#24418;&#29366;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#27969;&#31243;&#26174;&#33879;&#25913;&#36827;&#20102;SSM&#30340;&#26080;&#30417;&#30563;&#23545;&#24212;&#20272;&#35745;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#39640;&#24230;&#19981;&#35268;&#21017;&#30340;&#34920;&#38754;&#25299;&#25169;&#19978;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#29992;&#30002;&#29366;&#33146;&#21644;&#22810;&#23460;&#24515;&#33039;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#36825;&#19968;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36275;&#22815;&#20581;&#22766;&#65292;&#33021;&#22815;&#20174;&#22122;&#22768;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20013;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#25193;&#22823;SSMs&#35268;&#27169;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical shape models (SSMs) are an established way to geometrically represent the anatomy of a population with various clinically relevant applications. However, they typically require domain expertise and labor-intensive manual segmentations or landmark annotations to generate. Methods to estimate correspondences for SSMs typically learn with such labels as supervision signals. We address these shortcomings by proposing an unsupervised method that leverages deep geometric features and functional correspondences to learn local and global shape structures across complex anatomies simultaneously. Our pipeline significantly improves unsupervised correspondence estimation for SSMs compared to baseline methods, even on highly irregular surface topologies. We demonstrate this for two different anatomical structures: the thyroid and a multi-chamber heart dataset. Furthermore, our method is robust enough to learn from noisy neural network predictions, enabling scaling SSMs to larger patien
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#65292;&#24403;&#27169;&#22411;&#31867;&#36275;&#22815;&#20016;&#23500;&#20197;&#28085;&#30422;&#30495;&#23454;&#24773;&#20917;&#26102;&#65292;&#38750;&#32447;&#24615;&#38382;&#39064;&#30340;&#8220;&#20808;&#20272;&#35745;&#20877;&#20248;&#21270;&#8221;&#26041;&#27861;&#20248;&#20110;&#38598;&#25104;&#26041;&#27861;&#65292;&#21253;&#25324;&#20248;&#21270;&#38388;&#38553;&#30340;&#28176;&#36827;&#20248;&#21183;&#30340;&#22343;&#20540;&#65292;&#25152;&#26377;&#20854;&#20182;&#26102;&#21051;&#21644;&#25972;&#20010;&#28176;&#36827;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2304.06833</link><description>&lt;p&gt;
&#35780;&#20272;-&#20248;&#21270;&#26041;&#27861;&#19982;&#38598;&#25104;&#35780;&#20272;&#20248;&#21270;&#27861;&#65306;&#22522;&#20110;&#38543;&#26426;&#20248;&#21183;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Estimate-Then-Optimize Versus Integrated-Estimation-Optimization: A Stochastic Dominance Perspective. (arXiv:2304.06833v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#65292;&#24403;&#27169;&#22411;&#31867;&#36275;&#22815;&#20016;&#23500;&#20197;&#28085;&#30422;&#30495;&#23454;&#24773;&#20917;&#26102;&#65292;&#38750;&#32447;&#24615;&#38382;&#39064;&#30340;&#8220;&#20808;&#20272;&#35745;&#20877;&#20248;&#21270;&#8221;&#26041;&#27861;&#20248;&#20110;&#38598;&#25104;&#26041;&#27861;&#65292;&#21253;&#25324;&#20248;&#21270;&#38388;&#38553;&#30340;&#28176;&#36827;&#20248;&#21183;&#30340;&#22343;&#20540;&#65292;&#25152;&#26377;&#20854;&#20182;&#26102;&#21051;&#21644;&#25972;&#20010;&#28176;&#36827;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#38543;&#26426;&#20248;&#21270;&#20013;&#65292;&#38500;&#20102;&#38656;&#35201;&#20248;&#21270;&#20219;&#21153;&#65292;&#36824;&#38656;&#35201;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#28508;&#22312;&#20998;&#24067;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;&#36890;&#36807;&#36873;&#25321;&#23548;&#33268;&#26368;&#20339;&#32463;&#39564;&#30446;&#26631;&#24615;&#33021;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#21487;&#20197;&#38598;&#25104;&#20272;&#35745;&#21644;&#20248;&#21270;&#36807;&#31243;&#12290;&#24403;&#27169;&#22411;&#34987;&#38169;&#35823;&#22320;&#25351;&#23450;&#26102;&#65292;&#36825;&#31181;&#38598;&#25104;&#26041;&#27861;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#26174;&#31034;&#20986;&#20248;&#20110;&#31616;&#21333;&#30340;&#8220;&#20808;&#20272;&#35745;&#20877;&#20248;&#21270;&#8221;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#22312;&#27169;&#22411;&#31867;&#36275;&#22815;&#20016;&#23500;&#20197;&#28085;&#30422;&#30495;&#23454;&#24773;&#20917;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#38750;&#32447;&#24615;&#38382;&#39064;&#65292;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#25490;&#24207;&#22312;&#24378;&#28872;&#30340;&#24847;&#20041;&#19979;&#34987;&#39072;&#20498;&#12290;&#22312;&#21463;&#38480;&#26465;&#20214;&#21644;&#24403;&#19978;&#19979;&#25991;&#29305;&#24449;&#21487;&#29992;&#26102;&#65292;&#31867;&#20284;&#30340;&#32467;&#26524;&#20063;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
In data-driven stochastic optimization, model parameters of the underlying distribution need to be estimated from data in addition to the optimization task. Recent literature suggests the integration of the estimation and optimization processes, by selecting model parameters that lead to the best empirical objective performance. Such an integrated approach can be readily shown to outperform simple ``estimate then optimize" when the model is misspecified. In this paper, we argue that when the model class is rich enough to cover the ground truth, the performance ordering between the two approaches is reversed for nonlinear problems in a strong sense. Simple ``estimate then optimize" outperforms the integrated approach in terms of stochastic dominance of the asymptotic optimality gap, i,e, the mean, all other moments, and the entire asymptotic distribution of the optimality gap is always better. Analogous results also hold under constrained settings and when contextual features are availa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#24320;&#25918;&#38598;(UIOS)&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#35757;&#32451;&#20013;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#26679;&#26412;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#35745;&#31639;&#19981;&#30830;&#23450;&#24615;&#24471;&#20998;&#26469;&#34920;&#36798;&#20854;&#32622;&#20449;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20026;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#32593;&#33180;&#24322;&#24120;&#31579;&#26597;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.03981</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#24320;&#25918;&#38598;&#23398;&#20064;&#29992;&#20110;&#35270;&#32593;&#33180;&#24322;&#24120;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-inspired Open Set Learning for Retinal Anomaly Identification. (arXiv:2304.03981v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03981
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#24320;&#25918;&#38598;(UIOS)&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#35757;&#32451;&#20013;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#26679;&#26412;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#35745;&#31639;&#19981;&#30830;&#23450;&#24615;&#24471;&#20998;&#26469;&#34920;&#36798;&#20854;&#32622;&#20449;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20026;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#32593;&#33180;&#24322;&#24120;&#31579;&#26597;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#23454;&#38469;&#35270;&#32593;&#33180;&#24322;&#24120;&#20998;&#31867;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#26080;&#27861;&#35782;&#21035;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#26679;&#26412;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#24320;&#25918;&#38598;(UIOS)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;9&#20010;&#24120;&#35265;&#35270;&#32593;&#33180;&#30149;&#21464;&#30340;&#30524;&#24213;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#12290;&#38500;&#20102;&#27599;&#20010;&#31867;&#21035;&#30340;&#27010;&#29575;&#65292;UIOS&#36824;&#35745;&#31639;&#20102;&#19981;&#30830;&#23450;&#24615;&#24471;&#20998;&#26469;&#34920;&#36798;&#20854;&#32622;&#20449;&#24230;&#12290;&#25105;&#20204;&#30340;UIOS&#27169;&#22411;&#36890;&#36807;&#35774;&#32622;&#38408;&#20540;&#31574;&#30053;&#65292;&#22312;&#20869;&#37096;&#27979;&#35797;&#25968;&#25454;&#38598;&#12289;&#22806;&#37096;&#27979;&#35797;&#25968;&#25454;&#38598;&#21644;&#38750;&#20856;&#22411;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#30340;F1&#20998;&#21035;&#36798;&#21040;&#20102;99.55&#65285;&#12289;97.01&#65285;&#21644;91.91&#65285;&#65292;&#30456;&#27604;&#26631;&#20934;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;F1&#20998;&#21035;&#20026;92.20&#65285;&#12289;80.69&#65285;&#21644;64.74&#65285;&#12290;&#27492;&#22806;&#65292;UIOS&#27491;&#30830;&#39044;&#27979;&#20102;&#32597;&#35265;&#35270;&#32593;&#33180;&#30142;&#30149;&#12289;&#20302;&#36136;&#37327;&#30524;&#24213;&#22270;&#20687;&#21644;&#38750;&#30524;&#24213;&#22270;&#20687;&#31561;&#25968;&#25454;&#38598;&#20013;&#30340;&#39640;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#65292;&#25552;&#31034;&#38656;&#35201;&#25163;&#21160;&#26816;&#26597;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#32593;&#33180;&#24322;&#24120;&#31579;&#26597;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Failure to recognize samples from the classes unseen during training is a major limit of artificial intelligence (AI) in real-world implementation of retinal anomaly classification. To resolve this obstacle, we propose an uncertainty-inspired open-set (UIOS) model which was trained with fundus images of 9 common retinal conditions. Besides the probability of each category, UIOS also calculates an uncertainty score to express its confidence. Our UIOS model with thresholding strategy achieved an F1 score of 99.55%, 97.01% and 91.91% for the internal testing set, external testing set and non-typical testing set, respectively, compared to the F1 score of 92.20%, 80.69% and 64.74% by the standard AI model. Furthermore, UIOS correctly predicted high uncertainty scores, which prompted the need for a manual check, in the datasets of rare retinal diseases, low-quality fundus images, and non-fundus images. This work provides a robust method for real-world screening of retinal anomalies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RED-PSM&#30340;&#26041;&#27861;&#65292;&#23558;&#37096;&#20998;&#21487;&#20998;&#27169;&#22411;&#19982;&#21435;&#22122;&#27491;&#21017;&#21270;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#25104;&#20687;&#38382;&#39064;&#65292;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03483</link><description>&lt;p&gt;
RED-PSM: &#24102;&#21435;&#22122;&#27491;&#21017;&#21270;&#30340;&#37096;&#20998;&#21487;&#20998;&#27169;&#22411;&#29992;&#20110;&#21160;&#24577;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
RED-PSM: Regularization by Denoising of Partially Separable Models for Dynamic Imaging. (arXiv:2304.03483v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RED-PSM&#30340;&#26041;&#27861;&#65292;&#23558;&#37096;&#20998;&#21487;&#20998;&#27169;&#22411;&#19982;&#21435;&#22122;&#27491;&#21017;&#21270;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#25104;&#20687;&#38382;&#39064;&#65292;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#25104;&#20687;&#26159;&#25351;&#21033;&#29992;&#34987;&#27424;&#37319;&#26679;&#30340;&#27979;&#37327;&#25968;&#25454;&#24674;&#22797;&#27599;&#20010;&#26102;&#38388;&#28857;&#19978;&#30340;&#26102;&#21464;&#20108;&#32500;&#25110;&#19977;&#32500;&#29289;&#20307;&#12290;&#23588;&#20854;&#26159;&#22312;&#21160;&#24577;&#26029;&#23618;&#25195;&#25551;&#20013;&#65292;&#27599;&#20010;&#26102;&#38388;&#28857;&#19978;&#21482;&#26377;&#19968;&#20010;&#35270;&#35282;&#19979;&#30340;&#21333;&#20010;&#25237;&#24433;&#21487;&#29992;&#65292;&#20351;&#24471;&#38382;&#39064;&#20005;&#37325;&#31639;&#19981;&#21487;&#36870;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RED-PSM&#30340;&#26041;&#27861;&#65292;&#39318;&#27425;&#23558;&#20004;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25104;&#20687;&#38382;&#39064;&#12290;&#31532;&#19968;&#31181;&#25216;&#26415;&#26159;&#37096;&#20998;&#21487;&#20998;&#27169;&#22411;&#65292;&#24050;&#32463;&#29992;&#20110;&#39640;&#25928;&#22320;&#20026;&#26102;&#31354;&#30446;&#26631;&#24341;&#20837;&#20302;&#31209;&#20808;&#39564;&#12290;&#31532;&#20108;&#31181;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#21435;&#22122;&#27491;&#21017;&#21270;(RED)&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#21435;&#22122;&#31639;&#27861;&#22788;&#29702;&#21508;&#31181;&#21453;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#27491;&#21017;&#21270;&#30340;&#37096;&#20998;&#21487;&#20998;&#30446;&#26631;&#65292;&#36890;&#36807;&#21464;&#37327;&#20998;&#35010;&#21644;ADMM&#20248;&#21270;&#26041;&#26696;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30446;&#26631;&#25910;&#25947;&#20110;&#19968;&#20010;&#28385;&#36275;&#20248;&#21270;&#38382;&#39064;&#30340;&#31283;&#23450;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21160;&#24577;&#26029;&#23618;&#25195;&#25551;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;RED-PSM&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#21160;&#24577;&#25104;&#20687;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic imaging addresses the recovery of a time-varying 2D or 3D object at each time instant using its undersampled measurements. In particular, in the case of dynamic tomography, only a single projection at a single view angle may be available at a time, making the problem severely ill-posed. In this work, we propose an approach, RED-PSM, which combines for the first time two powerful techniques to address this challenging imaging problem. The first, are partially separable models, which have been used to efficiently introduce a low-rank prior for the spatio-temporal object. The second is the recent Regularization by Denoising (RED), which provides a flexible framework to exploit the impressive performance of state-of-the-art image denoising algorithms, for various inverse problems. We propose a partially separable objective with RED and an optimization scheme with variable splitting and ADMM, and prove convergence of our objective to a value corresponding to a stationary point satis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31934;&#30830;&#22320;&#21051;&#30011;&#20102;&#20855;&#26377;&#26377;&#30028;&#25200;&#21160;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#20026;&#19968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#30340;&#20984;&#21253;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36807;&#36924;&#36817;&#21487;&#36798;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.17674</link><description>&lt;p&gt;
&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#30340;&#31934;&#30830;&#21051;&#30011;
&lt;/p&gt;
&lt;p&gt;
Exact Characterization of the Convex Hulls of Reachable Sets. (arXiv:2303.17674v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31934;&#30830;&#22320;&#21051;&#30011;&#20102;&#20855;&#26377;&#26377;&#30028;&#25200;&#21160;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#20026;&#19968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#30340;&#20984;&#21253;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36807;&#36924;&#36817;&#21487;&#36798;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#30028;&#25200;&#21160;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#12290;&#21487;&#36798;&#38598;&#22312;&#25511;&#21046;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20294;&#35745;&#31639;&#36215;&#26469;&#20173;&#28982;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29616;&#26377;&#30340;&#36807;&#36924;&#36817;&#24037;&#20855;&#24448;&#24448;&#36807;&#20110;&#20445;&#23432;&#25110;&#35745;&#31639;&#20195;&#20215;&#39640;&#26114;&#12290;&#26412;&#25991;&#31934;&#30830;&#22320;&#21051;&#30011;&#20102;&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#65292;&#23558;&#20854;&#34920;&#31034;&#25104;&#19968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#30340;&#20984;&#21253;&#65292;&#36825;&#20010;&#26377;&#38480;&#32500;&#30340;&#21051;&#30011;&#24320;&#21551;&#20102;&#19968;&#31181;&#32039;&#23494;&#30340;&#20272;&#35745;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36807;&#36924;&#36817;&#21487;&#36798;&#38598;&#65292;&#19988;&#25104;&#26412;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20302;&#12289;&#26356;&#31934;&#20934;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#31070;&#32463;&#21453;&#39304;&#29615;&#20998;&#26512;&#21644;&#40065;&#26834;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the convex hulls of reachable sets of nonlinear systems with bounded disturbances. Reachable sets play a critical role in control, but remain notoriously challenging to compute, and existing over-approximation tools tend to be conservative or computationally expensive. In this work, we exactly characterize the convex hulls of reachable sets as the convex hulls of solutions of an ordinary differential equation from all possible initial values of the disturbances. This finite-dimensional characterization unlocks a tight estimation algorithm to over-approximate reachable sets that is significantly faster and more accurate than existing methods. We present applications to neural feedback loop analysis and robust model predictive control.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;NeRF-GAN&#20013;&#25552;&#21462;3D&#30693;&#35782;&#36827;&#34892;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#23039;&#24577;&#30340;2D GAN&#30340;&#39640;&#25928;3D&#24863;&#30693;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2303.12865</link><description>&lt;p&gt;
NeRF-GAN&#33976;&#39311;&#65306;&#22522;&#20110;&#21367;&#31215;&#30340;&#39640;&#25928;3D&#24863;&#30693;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
NeRF-GAN Distillation for Efficient 3D-Aware Generation with Convolutions. (arXiv:2303.12865v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;NeRF-GAN&#20013;&#25552;&#21462;3D&#30693;&#35782;&#36827;&#34892;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#23039;&#24577;&#30340;2D GAN&#30340;&#39640;&#25928;3D&#24863;&#30693;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23039;&#24577;&#30340;&#21367;&#31215;&#29983;&#25104;&#27169;&#22411;&#22312;&#20174;&#21333;&#35270;&#22270;&#25968;&#25454;&#38598;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;3D&#19968;&#33268;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#36275;&#22815;&#30340;3D&#20808;&#39564;&#30693;&#35782;&#12290;&#26368;&#36817;&#65292;&#23558;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#31561;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20174;&#21333;&#35270;&#22270;&#22270;&#20687;&#20013;&#29983;&#25104;3D&#24863;&#30693;&#22270;&#20687;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;NeRF-GAN&#21033;&#29992;&#20102;&#19977;&#32500;&#31070;&#32463;&#34920;&#31034;&#21644;&#20307;&#31215;&#28210;&#26579;&#30340;&#24378;&#24402;&#32435;&#20559;&#24046;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#26356;&#39640;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;NeRF-GAN&#20013;&#25552;&#21462;3D&#30693;&#35782;&#36827;&#34892;&#33976;&#39311;&#65292;&#37325;&#23457;&#22522;&#20110;&#23039;&#24577;&#30340;&#20108;&#32500;GAN&#65292;&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#23454;&#29616;&#39640;&#25928;&#30340;3D&#24863;&#30693;&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#22312;&#22522;&#20110;&#23039;&#24577;&#30340;&#21367;&#31215;&#32593;&#32476;&#20013;&#37325;&#29992;&#39044;&#35757;&#32451;&#30340;NeRF-GAN&#30340;&#33391;&#22909;&#35299;&#32806;&#28508;&#22312;&#31354;&#38388;&#65292;&#30452;&#25509;&#29983;&#25104;&#19982;&#28508;&#22312;&#30340;3D&#34920;&#36798;&#30456;&#23545;&#24212;&#30340;3D&#19968;&#33268;&#22270;&#29255;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;3D&#24863;&#30693;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pose-conditioned convolutional generative models struggle with high-quality 3D-consistent image generation from single-view datasets, due to their lack of sufficient 3D priors. Recently, the integration of Neural Radiance Fields (NeRFs) and generative models, such as Generative Adversarial Networks (GANs), has transformed 3D-aware generation from single-view images. NeRF-GANs exploit the strong inductive bias of 3D neural representations and volumetric rendering at the cost of higher computational complexity. This study aims at revisiting pose-conditioned 2D GANs for efficient 3D-aware generation at inference time by distilling 3D knowledge from pretrained NeRF-GANS. We propose a simple and effective method, based on re-using the well-disentangled latent space of a pre-trained NeRF-GAN in a pose-conditioned convolutional network to directly generate 3D-consistent images corresponding to the underlying 3D representations. Experiments on several datasets demonstrate that the proposed met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#27969;&#34892;&#30149;&#21464;&#21270;&#23545;&#31639;&#27861;&#30340;&#37096;&#32626;&#25928;&#26524;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.12540</link><description>&lt;p&gt;
&#8220;&#22270;&#20687;&#20998;&#26512;&#31639;&#27861;&#22312;&#27969;&#34892;&#30149;&#21464;&#21270;&#19979;&#30340;&#37096;&#32626;&#8221;
&lt;/p&gt;
&lt;p&gt;
Deployment of Image Analysis Algorithms under Prevalence Shifts. (arXiv:2303.12540v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#27969;&#34892;&#30149;&#21464;&#21270;&#23545;&#31639;&#27861;&#30340;&#37096;&#32626;&#25928;&#26524;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#24046;&#36317;&#26159;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20020;&#24202;&#36716;&#21270;&#20013;&#26368;&#37325;&#35201;&#30340;&#38556;&#30861;&#20043;&#19968;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;&#21644;&#32593;&#32476;&#26550;&#26500;&#65292;&#20294;&#24456;&#23569;&#20851;&#27880;&#31639;&#27861;&#22312;&#23454;&#36341;&#20013;&#37096;&#32626;&#26102;&#27969;&#34892;&#30149;&#21457;&#29983;&#29575;&#21464;&#21270;&#30340;&#29305;&#23450;&#24433;&#21709;&#12290;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#27665;&#20027;&#21270;&#30340;&#32972;&#26223;&#65292;&#30001;&#20110;&#30142;&#30149;&#27969;&#34892;&#29575;&#21487;&#33021;&#22312;&#26102;&#38388;&#21644;&#22320;&#28857;&#19978;&#21464;&#21270;&#24456;&#22823;&#65292;&#22240;&#27492;&#24320;&#21457;/&#39564;&#35777;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#21644;&#37096;&#32626;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#30340;&#31867;&#21035;&#39057;&#29575;&#20043;&#38388;&#30340;&#24046;&#24322;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#65288;i&#65289;&#26657;&#20934;&#30340;&#31243;&#24230;&#65292;&#65288;ii&#65289;&#20915;&#31574;&#38408;&#20540;&#19982;&#26368;&#20248;&#20540;&#30340;&#20559;&#24046;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#39564;&#35777;&#25351;&#26631;&#21453;&#26144;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#22312;&#37096;&#32626;&#20154;&#21475;&#30340;&#33021;&#21147;&#65292;&#23454;&#35777;&#22320;&#35777;&#26126;&#20102;&#32570;&#23569;&#27969;&#34892;&#30149;&#22788;&#29702;&#30340;&#28508;&#22312;&#20005;&#37325;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain gaps are among the most relevant roadblocks in the clinical translation of machine learning (ML)-based solutions for medical image analysis. While current research focuses on new training paradigms and network architectures, little attention is given to the specific effect of prevalence shifts on an algorithm deployed in practice. Such discrepancies between class frequencies in the data used for a method's development/validation and that in its deployment environment(s) are of great importance, for example in the context of artificial intelligence (AI) democratization, as disease prevalences may vary widely across time and location. Our contribution is twofold. First, we empirically demonstrate the potentially severe consequences of missing prevalence handling by analyzing (i) the extent of miscalibration, (ii) the deviation of the decision threshold from the optimum, and (iii) the ability of validation metrics to reflect neural network performance on the deployment population a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23558;&#25512;&#26029;&#31639;&#27861;&#31616;&#21270;&#24182;&#23884;&#20837;&#20135;&#21697;&#20195;&#30721;&#20013;&#65292;&#20197;&#20943;&#23569;&#32593;&#32476;&#36890;&#20449;&#65292;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#30340;&#23454;&#26102;&#24179;&#21488;&#19978;&#21487;&#23558;&#25512;&#26029;&#24310;&#36831;&#38477;&#20302;1.3&#20493;&#65292;CPU&#36164;&#28304;&#20943;&#23569;30&#65285;&#65292;&#24182;&#23558;&#24212;&#29992;&#31243;&#24207;&#21069;&#31471;&#21644;&#21518;&#31471;&#20043;&#38388;&#30340;&#32593;&#32476;&#36890;&#20449;&#20943;&#23569;60&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.11580</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#39640;&#25928;&#22810;&#32423;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Efficient Multi-stage Inference on Tabular Data. (arXiv:2303.11580v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11580
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23558;&#25512;&#26029;&#31639;&#27861;&#31616;&#21270;&#24182;&#23884;&#20837;&#20135;&#21697;&#20195;&#30721;&#20013;&#65292;&#20197;&#20943;&#23569;&#32593;&#32476;&#36890;&#20449;&#65292;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#30340;&#23454;&#26102;&#24179;&#21488;&#19978;&#21487;&#23558;&#25512;&#26029;&#24310;&#36831;&#38477;&#20302;1.3&#20493;&#65292;CPU&#36164;&#28304;&#20943;&#23569;30&#65285;&#65292;&#24182;&#23558;&#24212;&#29992;&#31243;&#24207;&#21069;&#31471;&#21644;&#21518;&#31471;&#20043;&#38388;&#30340;&#32593;&#32476;&#36890;&#20449;&#20943;&#23569;60&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#21644;&#20135;&#21697;&#36890;&#36807;&#20013;&#31561;&#25968;&#37327;&#30340;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#22312;&#23454;&#26102;&#25512;&#26029;&#26102;&#34987;&#29942;&#39048;&#25152;&#22256;&#12290;&#20256;&#32479;&#26234;&#24935;&#22312;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26102;&#65292;&#20542;&#21521;&#20110;&#23558;ML&#20195;&#30721;&#20998;&#21106;&#25104;&#26381;&#21153;&#65292;&#24182;&#36890;&#36807;&#36828;&#31243;&#36807;&#31243;&#35843;&#29992;&#65288;RPC&#65289;API&#34987;&#20135;&#21697;&#20195;&#30721;&#26597;&#35810;&#12290;&#36825;&#31181;&#26041;&#27861;&#28548;&#28165;&#20102;&#25972;&#20307;&#36719;&#20214;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#25277;&#35937;ML&#20869;&#37096;&#31616;&#21270;&#20102;&#20135;&#21697;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20998;&#31163;&#22686;&#21152;&#20102;&#32593;&#32476;&#24310;&#36831;&#65292;&#24182;&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;CPU&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#31616;&#21270;&#25512;&#26029;&#31639;&#27861;&#24182;&#23558;&#20854;&#23884;&#20837;&#20135;&#21697;&#20195;&#30721;&#20013;&#65292;&#20197;&#20943;&#23569;&#32593;&#32476;&#36890;&#20449;&#12290;&#38024;&#23545;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#30340;&#39640;&#24615;&#33021;&#23454;&#26102;&#24179;&#21488;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#24120;&#26377;&#36229;&#36807;&#19968;&#21322;&#30340;&#36755;&#20837;&#21487;&#20197;&#36866;&#24212;&#36825;&#31181;&#20248;&#21270;&#65292;&#32780;&#20854;&#20313;&#37096;&#20998;&#21487;&#20197;&#30001;&#21407;&#22987;&#27169;&#22411;&#22788;&#29702;&#12290;&#36890;&#36807;&#23558;AutoML&#24212;&#29992;&#20110;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#25105;&#20204;&#23558;&#25512;&#26029;&#24310;&#36831;&#38477;&#20302;&#20102;1.3&#20493;&#65292;CPU&#36164;&#28304;&#20943;&#23569;&#20102;30&#65285;&#65292;&#24212;&#29992;&#31243;&#24207;&#21069;&#31471;&#21644;&#21518;&#31471;&#20043;&#38388;&#30340;&#32593;&#32476;&#36890;&#20449;&#20943;&#23569;&#20102;60&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many ML applications and products train on medium amounts of input data but get bottlenecked in real-time inference. When implementing ML systems, conventional wisdom favors segregating ML code into services queried by product code via Remote Procedure Call (RPC) APIs. This approach clarifies the overall software architecture and simplifies product code by abstracting away ML internals. However, the separation adds network latency and entails additional CPU overhead. Hence, we simplify inference algorithms and embed them into the product code to reduce network communication. For public datasets and a high-performance real-time platform that deals with tabular data, we show that over half of the inputs are often amenable to such optimization, while the remainder can be handled by the original model. By applying our optimization with AutoML to both training and inference, we reduce inference latency by 1.3x, CPU resources by 30%, and network communication between application front-end an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2303.11413</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25391;&#21160;&#20449;&#21495;&#21435;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vibration Signal Denoising Using Deep Learning. (arXiv:2303.11413v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#33050;&#27493;&#24341;&#36215;&#30340;&#32467;&#26500;&#25391;&#21160;&#20449;&#21495;&#34987;&#24191;&#27867;&#29992;&#20110;&#20154;&#21592;&#35782;&#21035;&#12289;&#23450;&#20301;&#12289;&#20154;&#31867;&#27963;&#21160;&#25512;&#26029;&#12289;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29615;&#22659;&#22122;&#22768;&#12289;&#30005;&#30913;&#24178;&#25200;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#23454;&#38469;&#37319;&#38598;&#30340;&#20449;&#21495;&#36890;&#24120;&#20250;&#24102;&#26377;&#22122;&#22768;&#12290;&#22122;&#22768;&#30340;&#23384;&#22312;&#24433;&#21709;&#20102;&#20449;&#21495;&#22788;&#29702;&#36807;&#31243;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#26368;&#32456;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;&#35823;&#24046;&#12290;&#26412;&#25991;&#20027;&#35201;&#25506;&#35752;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#22122;&#22768;&#65292;&#21253;&#25324;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure vibration signals induced by footsteps are widely used for tasks like occupant identification, localization, human activity inference, structure health monitoring and so on. The vibration signals are collected as time series with amplitude values. However, the collected signals are always noisy in practice due to the influence of environmental noise, electromagnetic interference and other factors. The presence of noise affects the process of signal analysis, thus affecting the accuracy and error of the final tasks. In this paper, we mainly explore the denoising methods for footstep-induced vibration signals. We have considered different kinds of noise including stationary noises such as gaussian noises and non-stationary noises such as item-dropping vibration noise and music noises.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20266;&#24433;&#38477;&#22122;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#31232;&#30095;&#35270;&#22270;&#19979;&#33258;&#21160;&#20986;&#34880;&#26816;&#27979;&#30340;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#19982;&#23436;&#20840;&#37319;&#26679;&#30340;&#22270;&#20687;&#36827;&#34892;&#21516;&#31561;&#31934;&#30830;&#24230;&#30340;&#20998;&#31867;&#21644;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.09340</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20266;&#24433;&#38477;&#22122;&#30340;&#31232;&#30095;&#35270;&#22270;CT&#22270;&#20687;&#33258;&#21160;&#20986;&#34880;&#26816;&#27979;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Automated Hemorrhage Detection in Sparse-view Computed Tomography via Deep Convolutional Neural Network based Artifact Reduction. (arXiv:2303.09340v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20266;&#24433;&#38477;&#22122;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#31232;&#30095;&#35270;&#22270;&#19979;&#33258;&#21160;&#20986;&#34880;&#26816;&#27979;&#30340;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#19982;&#23436;&#20840;&#37319;&#26679;&#30340;&#22270;&#20687;&#36827;&#34892;&#21516;&#31561;&#31934;&#30830;&#24230;&#30340;&#20998;&#31867;&#21644;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39045;&#20869;&#20986;&#34880;&#26159;&#19968;&#31181;&#20005;&#37325;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#38656;&#35201;&#24555;&#36895;&#19988;&#24120;&#24120;&#38750;&#24120;&#23494;&#38598;&#30340;&#21307;&#30103;&#27835;&#30103;&#12290;&#20026;&#20102;&#35786;&#26029;&#65292;&#36890;&#24120;&#35201;&#36827;&#34892;&#39045;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CCT&#65289;&#25195;&#25551;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36752;&#23556;&#24341;&#36215;&#30340;&#22686;&#21152;&#30340;&#20581;&#24247;&#39118;&#38505;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#38477;&#20302;&#36825;&#31181;&#28508;&#22312;&#39118;&#38505;&#30340;&#26368;&#37325;&#35201;&#31574;&#30053;&#26159;&#23613;&#21487;&#33021;&#20445;&#25345;&#36752;&#23556;&#21058;&#37327;&#20302;&#65292;&#24182;&#19982;&#35786;&#26029;&#20219;&#21153;&#19968;&#33268;&#12290; &#31232;&#30095;&#35270;&#22270;CT&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#25152;&#37319;&#38598;&#30340;&#35270;&#22270;&#24635;&#25968;&#65292;&#20174;&#32780;&#38477;&#20302;&#21058;&#37327;&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#20294;&#20195;&#20215;&#26159;&#38477;&#20302;&#22270;&#20687;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;U-Net&#26550;&#26500;&#26469;&#20943;&#23569;&#31232;&#30095;&#35270;&#22270;CCT&#30340;&#20266;&#24433;&#65292;&#20174;&#31232;&#30095;&#35270;&#22270;&#20013;&#39044;&#27979;&#23436;&#20840;&#37319;&#26679;&#30340;&#37325;&#24314;&#22270;&#20687;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#20986;&#34880;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#23436;&#20840;&#37319;&#26679;&#30340;CCT&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20266;&#24433;&#38477;&#22122;&#21518;&#30340;CCT&#22270;&#20687;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#21644;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#19982;&#23436;&#20840;&#37319;&#26679;&#30340;CCT&#22270;&#20687;&#27809;&#26377;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intracranial hemorrhage poses a serious health problem requiring rapid and often intensive medical treatment. For diagnosis, a Cranial Computed Tomography (CCT) scan is usually performed. However, the increased health risk caused by radiation is a concern. The most important strategy to reduce this potential risk is to keep the radiation dose as low as possible and consistent with the diagnostic task. Sparse-view CT can be an effective strategy to reduce dose by reducing the total number of views acquired, albeit at the expense of image quality. In this work, we use a U-Net architecture to reduce artifacts from sparse-view CCTs, predicting fully sampled reconstructions from sparse-view ones. We evaluate the hemorrhage detectability in the predicted CCTs with a hemorrhage classification convolutional neural network, trained on fully sampled CCTs to detect and classify different sub-types of hemorrhages. Our results suggest that the automated classification and detection accuracy of hemo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#33258;&#21160;&#21270;&#31649;&#36947;&#65292;&#20351;&#29992;&#19987;&#21033;&#25968;&#25454;&#28304;&#35757;&#32451;&#39046;&#22495;&#29305;&#23450;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#19987;&#21033;&#20013;&#30340;&#24369;&#26631;&#35760;&#24212;&#29992;&#31867;&#21035;&#20013;&#23613;&#21487;&#33021;&#22810;&#30340;&#20449;&#24687;&#23454;&#29616;&#21270;&#23398;&#31354;&#38388;&#20869;&#29983;&#25104;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2303.08272</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#19987;&#21033;&#25552;&#21462;&#25903;&#25345;&#32858;&#28966;&#21270;&#23398;&#31354;&#38388;&#20869;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Automated patent extraction powers generative modeling in focused chemical spaces. (arXiv:2303.08272v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#33258;&#21160;&#21270;&#31649;&#36947;&#65292;&#20351;&#29992;&#19987;&#21033;&#25968;&#25454;&#28304;&#35757;&#32451;&#39046;&#22495;&#29305;&#23450;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#19987;&#21033;&#20013;&#30340;&#24369;&#26631;&#35760;&#24212;&#29992;&#31867;&#21035;&#20013;&#23613;&#21487;&#33021;&#22810;&#30340;&#20449;&#24687;&#23454;&#29616;&#21270;&#23398;&#31354;&#38388;&#20869;&#29983;&#25104;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24050;&#25104;&#20026;&#21453;&#21521;&#20998;&#23376;&#35774;&#35745;&#30340;&#19968;&#31181;&#20196;&#20154;&#20852;&#22859;&#30340;&#25163;&#27573;&#65292;&#20854;&#36827;&#23637;&#26469;&#33258;&#20110;&#35757;&#32451;&#31639;&#27861;&#21644;&#20998;&#23376;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#24212;&#29992;&#20110;&#26448;&#26009;&#31185;&#23398;&#21644;&#21270;&#23398;&#39046;&#22495;&#26102;&#65292;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#20855;&#26377;&#23646;&#24615;&#26631;&#31614;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#24050;&#21457;&#24067;&#30340;&#19987;&#21033;&#21253;&#21547;&#22312;&#20854;&#22312;&#26399;&#21002;&#19978;&#21457;&#34920;&#20043;&#21069;&#25259;&#38706;&#26032;&#26448;&#26009;&#30340;&#20449;&#24687;&#65292;&#26159;&#19968;&#31181;&#30456;&#23545;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#30340;&#31185;&#23398;&#30693;&#35782;&#24191;&#27867;&#26469;&#28304;&#12290;&#30001;&#20110;&#19987;&#21033;&#34987;&#25552;&#20132;&#26159;&#20026;&#20102;&#20445;&#25252;&#29305;&#23450;&#29992;&#36884;&#65292;&#22240;&#27492;&#19987;&#21033;&#20013;&#30340;&#20998;&#23376;&#21487;&#20197;&#34987;&#35270;&#20026;&#24369;&#26631;&#35760;&#30340;&#24212;&#29992;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#30001;&#32654;&#22269;&#19987;&#21033;&#19982;&#21830;&#26631;&#23616;&#65288;USPTO&#65289;&#21457;&#24067;&#30340;&#19987;&#21033;&#20855;&#26377;&#21487;&#19979;&#36733;&#30340;&#26426;&#22120;&#21487;&#35835;&#25991;&#26412;&#21644;&#20998;&#23376;&#32467;&#26500;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#33258;&#21160;&#21270;&#31649;&#36947;&#65292;&#20351;&#29992;&#19987;&#21033;&#25968;&#25454;&#28304;&#35757;&#32451;&#39046;&#22495;&#29305;&#23450;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models have emerged as an exciting avenue for inverse molecular design, with progress coming from the interplay between training algorithms and molecular representations. One of the key challenges in their applicability to materials science and chemistry has been the lack of access to sizeable training datasets with property labels. Published patents contain the first disclosure of new materials prior to their publication in journals, and are a vast source of scientific knowledge that has remained relatively untapped in the field of data-driven molecular design. Because patents are filed seeking to protect specific uses, molecules in patents can be considered to be weakly labeled into application classes. Furthermore, patents published by the US Patent and Trademark Office (USPTO) are downloadable and have machine-readable text and molecular structures. In this work, we train domain-specific generative models using patent data sources by developing an automated pipeline
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#31867;&#20013;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#24067;&#23572;&#30005;&#36335;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#65292;&#23558;&#30005;&#36335;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#65292;&#30456;&#27604;&#20110;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2303.06516</link><description>&lt;p&gt;
&#25171;&#24320;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20197;&#35745;&#31639;Shap&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Opening Up the Neural Network Classifier for Shap Score Computation. (arXiv:2303.06516v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#31867;&#20013;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#24067;&#23572;&#30005;&#36335;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#65292;&#23558;&#30005;&#36335;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#65292;&#30456;&#27604;&#20110;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an efficient method for computing Shap explanation scores in machine learning model classification by transforming binary neural networks into Boolean circuits and treating the resulting circuit as an open-box model, which leads to a significant improvement in performance compared to computing Shap directly on the BNN treated as a black-box model.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#30340;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#39640;&#25928;&#35745;&#31639;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#36716;&#25442;&#20026;&#30830;&#23450;&#24615;&#21644;&#21487;&#20998;&#35299;&#30340;&#24067;&#23572;&#30005;&#36335;&#65292;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#12290;&#25152;&#24471;&#21040;&#30340;&#30005;&#36335;&#34987;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#12290;&#35814;&#32454;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#30456;&#27604;&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of efficiently computing Shap explanation scores for classifications with machine learning models. With this goal, we show the transformation of binary neural networks (BNNs) for classification into deterministic and decomposable Boolean circuits, for which knowledge compilation techniques are used. The resulting circuit is treated as an open-box model, to compute Shap scores by means of a recent efficient algorithm for this class of circuits. Detailed experiments show a considerable gain in performance in comparison with computing Shap directly on the BNN treated as a black-box model.
&lt;/p&gt;</description></item><item><title>Exphormer&#26159;&#19968;&#20010;&#31232;&#30095;Transformer&#26694;&#26550;&#65292;&#36890;&#36807;&#34394;&#25311;&#20840;&#23616;&#33410;&#28857;&#21644;&#25193;&#24352;&#22270;&#30340;&#31232;&#30095;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#22411;&#22270;&#24418;&#65292;&#24182;&#35777;&#26126;&#20854;&#25317;&#26377;&#29702;&#24819;&#30340;&#29702;&#35770;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06147</link><description>&lt;p&gt;
Exphormer: &#31232;&#30095;Transformer&#29992;&#20110;&#22270;&#24418;
&lt;/p&gt;
&lt;p&gt;
Exphormer: Sparse Transformers for Graphs. (arXiv:2303.06147v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06147
&lt;/p&gt;
&lt;p&gt;
Exphormer&#26159;&#19968;&#20010;&#31232;&#30095;Transformer&#26694;&#26550;&#65292;&#36890;&#36807;&#34394;&#25311;&#20840;&#23616;&#33410;&#28857;&#21644;&#25193;&#24352;&#22270;&#30340;&#31232;&#30095;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#22411;&#22270;&#24418;&#65292;&#24182;&#35777;&#26126;&#20854;&#25317;&#26377;&#29702;&#24819;&#30340;&#29702;&#35770;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#36716;&#25442;&#22120;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#22270;&#23398;&#20064;&#21644;&#34920;&#31034;&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26550;&#26500;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26159;&#23558;&#22270;&#24418;&#36716;&#25442;&#22120;&#25193;&#23637;&#21040;&#22823;&#22411;&#22270;&#24418;&#24182;&#21516;&#26102;&#20445;&#25345;&#19982;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#30456;&#23218;&#32654;&#30340;&#20934;&#30830;&#24615;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Exphormer&#65292;&#19968;&#20010;&#26500;&#24314;&#24378;&#22823;&#19988;&#21487;&#25193;&#23637;&#30340;&#22270;&#24418;&#36716;&#25442;&#22120;&#30340;&#26694;&#26550;&#12290;Exphormer&#21253;&#25324;&#22522;&#20110;&#20004;&#20010;&#26426;&#21046;&#30340;&#31232;&#30095;&#27880;&#24847;&#26426;&#21046;&#65306;&#34394;&#25311;&#20840;&#23616;&#33410;&#28857;&#21644;&#25193;&#24352;&#22270;&#65292;&#20854;&#25968;&#23398;&#29305;&#24615;&#65288;&#22914;&#35889;&#25193;&#23637;&#12289;&#20266;&#38543;&#26426;&#24615;&#21644;&#31232;&#30095;&#24615;&#65289;&#20351;&#24471;&#22270;&#24418;&#36716;&#25442;&#22120;&#30340;&#22797;&#26434;&#24230;&#20165;&#19982;&#22270;&#24418;&#22823;&#23567;&#32447;&#24615;&#30456;&#20851;&#65292;&#24182;&#19988;&#33021;&#22815;&#35777;&#26126;&#29983;&#25104;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#20855;&#26377;&#29702;&#24819;&#30340;&#29702;&#35770;&#29305;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;Exphormer&#32435;&#20837;&#26368;&#36817;&#25552;&#20986;&#30340;GraphGPS&#26694;&#26550;&#20013;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#22270;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#21253;&#25324;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph transformers have emerged as a promising architecture for a variety of graph learning and representation tasks. Despite their successes, though, it remains challenging to scale graph transformers to large graphs while maintaining accuracy competitive with message-passing networks. In this paper, we introduce Exphormer, a framework for building powerful and scalable graph transformers. Exphormer consists of a sparse attention mechanism based on two mechanisms: virtual global nodes and expander graphs, whose mathematical characteristics, such as spectral expansion, pseduorandomness, and sparsity, yield graph transformers with complexity only linear in the size of the graph, while allowing us to prove desirable theoretical properties of the resulting transformer models. We show that incorporating Exphormer into the recently-proposed GraphGPS framework produces models with competitive empirical results on a wide variety of graph datasets, including state-of-the-art results on three d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;Transformer&#23398;&#20064;&#35821;&#20041;&#32467;&#26500;&#30340;&#26426;&#21046;&#24615;&#29702;&#35299;&#65292;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#23884;&#20837;&#23618;&#21644;&#33258;&#27880;&#24847;&#21147;&#23618;&#22914;&#20309;&#23545;&#35789;&#27719;&#30340;&#20849;&#29616;&#32467;&#26500;&#36827;&#34892;&#32534;&#30721;&#12290;</title><link>http://arxiv.org/abs/2303.04245</link><description>&lt;p&gt;
Transformers&#22914;&#20309;&#23398;&#20064;&#20027;&#39064;&#32467;&#26500;&#65306;&#36208;&#21521;&#23545;&#20854;&#26426;&#21046;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding. (arXiv:2303.04245v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;Transformer&#23398;&#20064;&#35821;&#20041;&#32467;&#26500;&#30340;&#26426;&#21046;&#24615;&#29702;&#35299;&#65292;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#23884;&#20837;&#23618;&#21644;&#33258;&#27880;&#24847;&#21147;&#23618;&#22914;&#20309;&#23545;&#35789;&#27719;&#30340;&#20849;&#29616;&#32467;&#26500;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Transformer&#22312;&#35768;&#22810;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23545;&#20854;&#23398;&#20064;&#26426;&#21046;&#30340;&#20934;&#30830;&#29702;&#35299;&#20173;&#28982;&#23384;&#22312;&#36739;&#22823;&#30340;&#32570;&#20047;&#12290;&#34429;&#28982;&#23427;&#20204;&#22312;&#21253;&#25324;&#21508;&#31181;&#32467;&#26500;&#21270;&#21644;&#25512;&#29702;&#20219;&#21153;&#22312;&#20869;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#23545;&#25968;&#23398;&#29702;&#35299;&#30340;&#30740;&#31350;&#20173;&#28982;&#28382;&#21518;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#22987;&#20174;&#34920;&#31034;&#26041;&#38754;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#21363;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32593;&#32476;&#30340;&#22823;&#23567;/&#28145;&#24230;/&#22797;&#26434;&#24615;&#29992;&#20110;&#25191;&#34892;&#26576;&#20123;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24182;&#19981;&#33021;&#20445;&#35777;&#23398;&#20064;&#21160;&#24577;&#20250;&#25910;&#25947;&#21040;&#25152;&#25552;&#20986;&#30340;&#32467;&#26500;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#32454;&#33268;&#20837;&#24494;&#30340;&#26426;&#21046;&#29702;&#35299;&#65292;&#38416;&#26126;&#20102;Transformer&#22914;&#20309;&#23398;&#20064;&#8220;&#35821;&#20041;&#32467;&#26500;&#8221;&#65292;&#21363;&#25429;&#25417;&#35789;&#27719;&#30340;&#20849;&#29616;&#32467;&#26500;&#12290;&#20934;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#21644;&#23545;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#20197;&#21450;&#30001;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#24314;&#27169;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#23884;&#20837;&#23618;&#21644;&#33258;&#27880;&#24847;&#21147;&#23618;&#22914;&#20309;&#23545;&#20027;&#39064;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the successes of transformers across many domains are indisputable, accurate understanding of the learning mechanics is still largely lacking. Their capabilities have been probed on benchmarks which include a variety of structured and reasoning tasks -- but mathematical understanding is lagging substantially behind. Recent lines of work have begun studying representational aspects of this question: that is, the size/depth/complexity of attention-based networks to perform certain tasks. However, there is no guarantee the learning dynamics will converge to the constructions proposed. In our paper, we provide fine-grained mechanistic understanding of how transformers learn "semantic structure", understood as capturing co-occurrence structure of words. Precisely, we show, through a combination of mathematical analysis and experiments on Wikipedia data and synthetic data modeled by Latent Dirichlet Allocation (LDA), that the embedding layer and the self-attention layer encode the topi
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#36890;&#36807;&#23545;&#29289;&#20307;&#36755;&#20837;&#34920;&#31034;&#36827;&#34892;&#36523;&#20221;&#20445;&#25345;&#30340;&#21464;&#25442;&#65292;&#19981;&#20165;&#26377;&#21161;&#20110;&#29289;&#20307;&#30340;&#20998;&#31867;&#65292;&#36824;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#23646;&#24615;&#30340;&#26377;&#26080;&#20915;&#31574;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2302.10763</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#19982;&#23646;&#24615;&#20851;&#32852;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning and the Emergence of Attributes Associations. (arXiv:2302.10763v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10763
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#36890;&#36807;&#23545;&#29289;&#20307;&#36755;&#20837;&#34920;&#31034;&#36827;&#34892;&#36523;&#20221;&#20445;&#25345;&#30340;&#21464;&#25442;&#65292;&#19981;&#20165;&#26377;&#21161;&#20110;&#29289;&#20307;&#30340;&#20998;&#31867;&#65292;&#36824;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#23646;&#24615;&#30340;&#26377;&#26080;&#20915;&#31574;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29289;&#20307;&#21576;&#29616;&#65292;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#36890;&#24120;&#20250;&#32473;&#20986;&#19968;&#20010;&#31616;&#27905;&#30340;&#26631;&#31614;&#12290;&#32780;&#20154;&#31867;&#22312;&#31867;&#20284;&#30340;&#21576;&#29616;&#19979;&#65292;&#38500;&#20102;&#32473;&#20986;&#19968;&#20010;&#26631;&#31614;&#22806;&#65292;&#36824;&#20250;&#34987;&#22823;&#37327;&#30340;&#20851;&#32852;&#20449;&#24687;&#25152;&#28153;&#27809;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#21576;&#29616;&#29289;&#20307;&#30340;&#23646;&#24615;&#12290;&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#65292;&#22522;&#20110;&#23545;&#29289;&#20307;&#36755;&#20837;&#34920;&#31034;&#36827;&#34892;&#20445;&#25345;&#36523;&#20221;&#30340;&#21464;&#25442;&#12290;&#26412;&#30740;&#31350;&#25512;&#27979;&#65292;&#36825;&#20123;&#21464;&#25442;&#19981;&#20165;&#21487;&#20197;&#20445;&#25345;&#21576;&#29616;&#29289;&#20307;&#30340;&#36523;&#20221;&#65292;&#36824;&#21487;&#20197;&#20445;&#25345;&#20854;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#23646;&#24615;&#30340;&#36523;&#20221;&#12290;&#36825;&#24847;&#21619;&#30528;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#30340;&#36755;&#20986;&#34920;&#31034;&#19981;&#20165;&#23545;&#20110;&#21576;&#29616;&#29289;&#20307;&#30340;&#20998;&#31867;&#26377;&#20215;&#20540;&#65292;&#36824;&#23545;&#20110;&#20219;&#20309;&#24863;&#20852;&#36259;&#23646;&#24615;&#30340;&#26377;&#26080;&#20915;&#31574;&#26377;&#20215;&#20540;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#35266;&#28857;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to an object presentation, supervised learning schemes generally respond with a parsimonious label. Upon a similar presentation we humans respond again with a label, but are flooded, in addition, by a myriad of associations. A significant portion of these consist of the presented object attributes. Contrastive learning is a semi-supervised learning scheme based on the application of identity preserving transformations on the object input representations. It is conjectured in this work that these same applied transformations preserve, in addition to the identity of the presented object, also the identity of its semantically meaningful attributes. The corollary of this is that the output representations of such a contrastive learning scheme contain valuable information not only for the classification of the presented object, but also for the presence or absence decision of any attribute of interest. Simulation results which demonstrate this idea and the feasibility of this co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BiofilmScanner&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;Yolact&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;&#19981;&#21464;&#30697;&#26469;&#39640;&#25928;&#22320;&#26816;&#27979;&#21644;&#20998;&#21106;SRB&#22270;&#20687;&#20013;&#30340;&#32454;&#33740;&#32454;&#32990;&#65292;&#24182;&#20934;&#30830;&#22320;&#25552;&#21462;&#20854;&#20960;&#20309;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.09629</link><description>&lt;p&gt;
BiofilmScanner: &#19968;&#31181;&#20174;&#29983;&#29289;&#34987;&#33180;&#22270;&#20687;&#20013;&#33719;&#21462;&#32454;&#33740;&#32454;&#32990;&#24418;&#24577;&#23646;&#24615;&#30340;&#35745;&#31639;&#26234;&#33021;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BiofilmScanner: A Computational Intelligence Approach to Obtain Bacterial Cell Morphological Attributes from Biofilm Image. (arXiv:2302.09629v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BiofilmScanner&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;Yolact&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;&#19981;&#21464;&#30697;&#26469;&#39640;&#25928;&#22320;&#26816;&#27979;&#21644;&#20998;&#21106;SRB&#22270;&#20687;&#20013;&#30340;&#32454;&#33740;&#32454;&#32990;&#65292;&#24182;&#20934;&#30830;&#22320;&#25552;&#21462;&#20854;&#20960;&#20309;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Desulfovibrio alaskensis G20 (DA-G20)&#34987;&#29992;&#20316;&#30827;&#37240;&#30416;&#36824;&#21407;&#32454;&#33740;&#65288;SRB&#65289;&#30340;&#27169;&#22411;&#65292;SRB&#19982;&#30001;&#24494;&#29983;&#29289;&#24341;&#36215;&#30340;&#33104;&#34432;&#38382;&#39064;&#26377;&#20851;&#12290;SRB&#22522;&#29983;&#29289;&#33180;&#34987;&#35748;&#20026;&#26159;&#23548;&#33268;&#37329;&#23646;&#22522;&#30784;&#35774;&#26045;&#27599;&#24180;&#25439;&#22833;&#25968;&#21313;&#20159;&#32654;&#20803;&#30340;&#29983;&#29289;&#33104;&#34432;&#30340;&#21407;&#22240;&#12290;&#20102;&#35299;SRB&#29983;&#29289;&#33180;&#20013;&#19981;&#21516;&#29983;&#38271;&#38454;&#27573;&#20013;&#32454;&#33740;&#32454;&#32990;&#30340;&#24418;&#29366;&#21644;&#22823;&#23567;&#23646;&#24615;&#30340;&#25552;&#21462;&#23558;&#26377;&#21161;&#20110;&#35774;&#35745;&#38450;&#33104;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#23384;&#22312;&#35768;&#22810;&#38382;&#39064;&#65292;&#21253;&#25324;&#32791;&#26102;&#30340;&#20960;&#20309;&#29305;&#24615;&#25552;&#21462;&#12289;&#20302;&#25928;&#29575;&#21644;&#39640;&#35823;&#24046;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BiofilmScanner&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#22522;&#20110;Yolact&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#20102;&#19981;&#21464;&#30697;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#26816;&#27979;&#21644;&#20998;&#21106;SRB&#22270;&#20687;&#20013;&#30340;&#32454;&#33740;&#32454;&#32990;&#65292;&#21516;&#26102;&#21033;&#29992;&#19981;&#21464;&#30697;&#23545;&#20998;&#21106;&#30340;&#32454;&#32990;&#30340;&#20960;&#20309;&#29305;&#24615;&#36827;&#34892;&#27979;&#37327;&#65292;&#35823;&#24046;&#36739;&#20302;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Desulfovibrio alaskensis G20 (DA-G20) is utilized as a model for sulfate-reducing bacteria (SRB) that are associated with corrosion issues caused by microorganisms. SRB-based biofilms are thought to be responsible for the billion-dollar-per-year bio-corrosion of metal infrastructure. Understanding the extraction of the bacterial cells' shape and size properties in the SRB-biofilm at different growth stages will assist with the design of anti-corrosion techniques. However, numerous issues affect current approaches, including time-consuming geometric property extraction, low efficiency, and high error rates. This paper proposes BiofilScanner, a Yolact-based deep learning method integrated with invariant moments to address these problems. Our approach efficiently detects and segments bacterial cells in an SRB image while simultaneously invariant moments measure the geometric characteristics of the segmented cells with low errors. The numerical experiments of the proposed method demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#20102;&#32473;&#23450;&#22270;&#30340;&#31867;&#26631;&#31614;&#27010;&#29575;&#30340;&#20998;&#31867;&#20844;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214; ELBO &#29992;&#20110;&#35757;&#32451;&#29983;&#25104;&#22270;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#12290;&#36825;&#26159;&#19968;&#31181;&#22312;&#22270;&#20998;&#31867;&#20013;&#20855;&#26377;&#21019;&#26032;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.07989</link><description>&lt;p&gt;
&#20174;&#22270;&#29983;&#25104;&#21040;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
From Graph Generation to Graph Classification. (arXiv:2302.07989v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#20102;&#32473;&#23450;&#22270;&#30340;&#31867;&#26631;&#31614;&#27010;&#29575;&#30340;&#20998;&#31867;&#20844;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214; ELBO &#29992;&#20110;&#35757;&#32451;&#29983;&#25104;&#22270;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#12290;&#36825;&#26159;&#19968;&#31181;&#22312;&#22270;&#20998;&#31867;&#20013;&#20855;&#26377;&#21019;&#26032;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#29983;&#25104;&#27169;&#22411; (GGM) &#36827;&#34892;&#22270;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#12290;&#20551;&#35774;&#19968;&#20010;&#23450;&#20041;&#20102;&#22270;&#21450;&#20854;&#31867;&#26631;&#31614;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#30340; GGM&#65292;&#25105;&#25512;&#23548;&#20102;&#35745;&#31639;&#32473;&#23450;&#22270;&#30340;&#31867;&#26631;&#31614;&#27010;&#29575;&#30340;&#20998;&#31867;&#20844;&#24335;&#12290;&#21487;&#20197;&#20351;&#29992;&#26032;&#30340;&#26465;&#20214; ELBO &#26469;&#35757;&#32451;&#29983;&#25104;&#22270;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#21306;&#20998;&#12290;&#34429;&#28982;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#22312;&#38750;&#20851;&#31995; i.i.d. &#25968;&#25454;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#19968;&#31181;&#22270;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This note describes a new approach to classifying graphs that leverages graph generative models (GGM). Assuming a GGM that defines a joint probability distribution over graphs and their class labels, I derive classification formulas for the probability of a class label given a graph. A new conditional ELBO can be used to train a generative graph auto-encoder model for discrimination. While leveraging generative models for classification has been well explored for non-relational i.i.d. data, to our knowledge it is a novel approach to graph classification.
&lt;/p&gt;</description></item><item><title>DNArch&#26159;&#19968;&#31181;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#21516;&#26102;&#23398;&#20064;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#23398;&#20064;&#27599;&#19968;&#23618;&#30340;&#21367;&#31215;&#26680;&#22823;&#23567;&#21644;&#36890;&#36947;&#25968;&#65292;&#36824;&#21487;&#20197;&#23398;&#20064;&#32593;&#32476;&#30340;&#28145;&#24230;&#21644;&#19979;&#37319;&#26679;&#23618;&#30340;&#20301;&#32622;&#21644;&#20540;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;DNArch&#19981;&#38480;&#20110;&#39044;&#23450;&#20041;&#30340;&#31070;&#32463;&#32452;&#20214;&#65292;&#33021;&#22815;&#21457;&#29616;&#21508;&#31181;&#26680;&#22823;&#23567;&#12289;&#23485;&#24230;&#12289;&#28145;&#24230;&#21644;&#19979;&#37319;&#26679;&#32452;&#21512;&#20013;&#30340;&#25972;&#20010;CNN&#26550;&#26500;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;DNArch&#22312;&#22810;&#20010;&#20998;&#31867;&#21644;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#19978;&#25214;&#21040;&#20102;&#39640;&#24615;&#33021;&#30340;CNN&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2302.05400</link><description>&lt;p&gt;
DNArch: &#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#23398;&#20064;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
DNArch: Learning Convolutional Neural Architectures by Backpropagation. (arXiv:2302.05400v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05400
&lt;/p&gt;
&lt;p&gt;
DNArch&#26159;&#19968;&#31181;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#21516;&#26102;&#23398;&#20064;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#23398;&#20064;&#27599;&#19968;&#23618;&#30340;&#21367;&#31215;&#26680;&#22823;&#23567;&#21644;&#36890;&#36947;&#25968;&#65292;&#36824;&#21487;&#20197;&#23398;&#20064;&#32593;&#32476;&#30340;&#28145;&#24230;&#21644;&#19979;&#37319;&#26679;&#23618;&#30340;&#20301;&#32622;&#21644;&#20540;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;DNArch&#19981;&#38480;&#20110;&#39044;&#23450;&#20041;&#30340;&#31070;&#32463;&#32452;&#20214;&#65292;&#33021;&#22815;&#21457;&#29616;&#21508;&#31181;&#26680;&#22823;&#23567;&#12289;&#23485;&#24230;&#12289;&#28145;&#24230;&#21644;&#19979;&#37319;&#26679;&#32452;&#21512;&#20013;&#30340;&#25972;&#20010;CNN&#26550;&#26500;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;DNArch&#22312;&#22810;&#20010;&#20998;&#31867;&#21644;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#19978;&#25214;&#21040;&#20102;&#39640;&#24615;&#33021;&#30340;CNN&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Differentiable Neural Architectures (DNArch)&#65292;&#19968;&#31181;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#21516;&#26102;&#23398;&#20064;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#30340;&#26435;&#37325;&#21644;&#26550;&#26500;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DNArch&#20801;&#35768;&#23398;&#20064;(i)&#27599;&#19968;&#23618;&#30340;&#21367;&#31215;&#26680;&#22823;&#23567;&#65292;(ii)&#27599;&#19968;&#23618;&#30340;&#36890;&#36947;&#25968;&#65292;(iii)&#19979;&#37319;&#26679;&#23618;&#30340;&#20301;&#32622;&#21644;&#20540;&#65292;&#20197;&#21450;(iv)&#32593;&#32476;&#30340;&#28145;&#24230;&#12290;&#20026;&#27492;&#65292;DNArch&#23558;&#31070;&#32463;&#26550;&#26500;&#35270;&#20026;&#36830;&#32493;&#30340;&#22810;&#32500;&#23454;&#20307;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#21487;&#24494;&#25513;&#30721;&#26469;&#25511;&#21046;&#20854;&#22823;&#23567;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;DNArch&#19981;&#38480;&#20110;&#39044;&#23450;&#20041;&#30340;&#21487;&#33021;&#31070;&#32463;&#32452;&#20214;&#38598;&#65292;&#32780;&#26159;&#33021;&#22815;&#21457;&#29616;&#22312;&#25152;&#26377;&#21487;&#34892;&#30340;&#26680;&#22823;&#23567;&#12289;&#23485;&#24230;&#12289;&#28145;&#24230;&#21644;&#19979;&#37319;&#26679;&#32452;&#21512;&#20013;&#30340;&#25972;&#20010;CNN&#26550;&#26500;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DNArch&#33021;&#22815;&#20026;&#39034;&#24207;&#21644;&#22270;&#20687;&#25968;&#25454;&#30340;&#22810;&#20010;&#20998;&#31867;&#21644;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#25214;&#21040;&#26377;&#25928;&#30340;CNN&#26550;&#26500;&#12290;&#24403;&#19982;&#25511;&#21046;&#26550;&#26500;&#22823;&#23567;&#30340;&#25439;&#22833;&#39033;&#30456;&#32467;&#21512;&#26102;&#65292;DNArch&#36824;&#33021;&#22312;&#36816;&#34892;&#26102;&#38388;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#23454;&#29616;&#26377;&#25928;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Differentiable Neural Architectures (DNArch), a method that jointly learns the weights and the architecture of Convolutional Neural Networks (CNNs) by backpropagation. In particular, DNArch allows learning (i) the size of convolutional kernels at each layer, (ii) the number of channels at each layer, (iii) the position and values of downsampling layers, and (iv) the depth of the network. To this end, DNArch views neural architectures as continuous multidimensional entities, and uses learnable differentiable masks along each dimension to control their size. Unlike existing methods, DNArch is not limited to a predefined set of possible neural components, but instead it is able to discover entire CNN architectures across all feasible combinations of kernel sizes, widths, depths and downsampling. Empirically, DNArch finds performant CNN architectures for several classification and dense prediction tasks on sequential and image data. When combined with a loss term that controls t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#26032;&#38395;&#25991;&#31456;&#20013;&#26816;&#27979;&#26377;&#23475;&#35758;&#31243;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#38395;&#25991;&#31456;&#27880;&#37322;&#25968;&#25454;&#38598;&#20197;&#20379;&#30740;&#31350;&#20351;&#29992;&#12290;&#30740;&#31350;&#32773;&#23637;&#31034;&#20102;&#21487;&#35299;&#37322;&#31995;&#32479;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#21644;&#40657;&#30418;&#27169;&#22411;&#26377;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.00102</link><description>&lt;p&gt;
&#22312;&#26032;&#38395;&#25991;&#31456;&#20013;&#26816;&#27979;&#26377;&#23475;&#35758;&#31243;
&lt;/p&gt;
&lt;p&gt;
Detecting Harmful Agendas in News Articles. (arXiv:2302.00102v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00102
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#26032;&#38395;&#25991;&#31456;&#20013;&#26816;&#27979;&#26377;&#23475;&#35758;&#31243;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#38395;&#25991;&#31456;&#27880;&#37322;&#25968;&#25454;&#38598;&#20197;&#20379;&#30740;&#31350;&#20351;&#29992;&#12290;&#30740;&#31350;&#32773;&#23637;&#31034;&#20102;&#21487;&#35299;&#37322;&#31995;&#32479;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#21644;&#40657;&#30418;&#27169;&#22411;&#26377;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#19978;&#25805;&#32437;&#26032;&#38395;&#26159;&#19968;&#20010;&#26085;&#30410;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#20351;&#29992;&#33258;&#21160;&#21270;&#31995;&#32479;&#26469;&#36943;&#21046;&#20854;&#20256;&#25773;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#34429;&#28982;&#35823;&#23548;&#20449;&#24687;&#21644;&#34394;&#20551;&#20449;&#24687;&#30340;&#26816;&#27979;&#24050;&#32463;&#24471;&#21040;&#30740;&#31350;&#65292;&#20294;&#22312;&#26816;&#27979;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#26377;&#23475;&#35758;&#31243;&#36825;&#19968;&#37325;&#35201;&#25361;&#25112;&#26041;&#38754;&#32570;&#20047;&#25237;&#36164;&#65307;&#35782;&#21035;&#26377;&#23475;&#35758;&#31243;&#23545;&#20110;&#35782;&#21035;&#20855;&#26377;&#26368;&#22823;&#28508;&#22312;&#29616;&#23454;&#21361;&#23475;&#30340;&#26032;&#38395;&#36816;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23545;&#23457;&#26597;&#21046;&#24230;&#23384;&#22312;&#30495;&#23454;&#30340;&#25285;&#24551;&#65292;&#26377;&#23475;&#35758;&#31243;&#26816;&#27979;&#22120;&#24517;&#39035;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#25165;&#33021;&#21457;&#25381;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#19968;&#20840;&#26032;&#30340;&#20219;&#21153;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;NewsAgendas&#30340;&#26032;&#38395;&#25991;&#31456;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35758;&#31243;&#35782;&#21035;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#35299;&#37322;&#31995;&#32479;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#19982;&#40657;&#30418;&#27169;&#22411;&#20855;&#26377;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manipulated news online is a growing problem which necessitates the use of automated systems to curtail its spread. We argue that while misinformation and disinformation detection have been studied, there has been a lack of investment in the important open challenge of detecting harmful agendas in news articles; identifying harmful agendas is critical to flag news campaigns with the greatest potential for real world harm. Moreover, due to real concerns around censorship, harmful agenda detectors must be interpretable to be effective. In this work, we propose this new task and release a dataset, NewsAgendas, of annotated news articles for agenda identification. We show how interpretable systems can be effective on this task and demonstrate that they can perform comparably to black-box models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#26799;&#24230;&#20026;&#22522;&#30784;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#24930;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#20197;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#24102;&#26469;&#30340;&#19981;&#33391;&#24433;&#21709;&#65292;&#35813;&#26041;&#27861;&#22312;&#25928;&#29575;&#19978;&#27604;&#21097;&#20313;&#26799;&#24230;&#26041;&#27861;&#26356;&#24555;&#65292;&#20960;&#20046;&#20855;&#26377;&#30456;&#21516;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#32463;&#20856;&#38382;&#39064;&#19978;&#19982;TD&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.13757</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#25928;&#26799;&#24230;&#20026;&#22522;&#30784;&#30340;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Toward Efficient Gradient-Based Value Estimation. (arXiv:2301.13757v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#26799;&#24230;&#20026;&#22522;&#30784;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#24930;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#20197;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#24102;&#26469;&#30340;&#19981;&#33391;&#24433;&#21709;&#65292;&#35813;&#26041;&#27861;&#22312;&#25928;&#29575;&#19978;&#27604;&#21097;&#20313;&#26799;&#24230;&#26041;&#27861;&#26356;&#24555;&#65292;&#20960;&#20046;&#20855;&#26377;&#30456;&#21516;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#32463;&#20856;&#38382;&#39064;&#19978;&#19982;TD&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#26799;&#24230;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#31283;&#23450;&#24615;&#65292;&#20294;&#36890;&#24120;&#27604;&#26102;&#38388;&#24046;&#24322;&#65288;TD&#65289;&#23398;&#20064;&#26041;&#27861;&#24930;&#24471;&#22810;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#32531;&#24930;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#34920;&#26126;&#22343;&#26041;&#36125;&#23572;&#26364;&#35823;&#24046;&#65288;MSBE&#65289;&#26159;&#19968;&#31181;&#30149;&#24577;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20854;&#40657;&#22622;&#30697;&#38453;&#20855;&#26377;&#36739;&#22823;&#30340;&#26465;&#20214;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;MSBE&#30340;&#19981;&#33391;&#26465;&#20214;&#23545;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#22797;&#26434;&#24230;&#30340;&#26080;&#25209;&#22788;&#29702;&#36817;&#31471;&#26041;&#27861;&#65292;&#23427;&#36817;&#20284;&#36981;&#24490;&#39640;&#26031;&#29275;&#39039;&#26041;&#21521;&#65292;&#24182;&#22312;&#21442;&#25968;&#21270;&#26041;&#38754;&#28176;&#36817;&#40065;&#26834;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#31639;&#27861;&#31216;&#20026;RANS&#65292;&#23427;&#22312;&#25928;&#29575;&#19978;&#27604;&#21097;&#20313;&#26799;&#24230;&#26041;&#27861;&#26356;&#24555;&#65292;&#20960;&#20046;&#20855;&#26377;&#30456;&#21516;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#25105;&#20204;&#27979;&#35797;&#30340;&#32463;&#20856;&#38382;&#39064;&#19978;&#19982;TD&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-based methods for value estimation in reinforcement learning have favorable stability properties, but they are typically much slower than Temporal Difference (TD) learning methods. We study the root causes of this slowness and show that Mean Square Bellman Error (MSBE) is an ill-conditioned loss function in the sense that its Hessian has large condition-number. To resolve the adverse effect of poor conditioning of MSBE on gradient based methods, we propose a low complexity batch-free proximal method that approximately follows the Gauss-Newton direction and is asymptotically robust to parameterization. Our main algorithm, called RANS, is efficient in the sense that it is significantly faster than the residual gradient methods while having almost the same computational complexity, and is competitive with TD on the classic problems that we tested.
&lt;/p&gt;</description></item><item><title>ScaDLES&#26159;&#19968;&#31181;&#29992;&#20110;&#36793;&#32536;&#31471;&#27969;&#24335;&#25968;&#25454;&#19978;&#30340;&#21487;&#25193;&#23637;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31995;&#32479;&#21644;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2301.08897</link><description>&lt;p&gt;
ScaDLES: &#36793;&#32536;&#31471;&#27969;&#24335;&#25968;&#25454;&#19978;&#30340;&#21487;&#25193;&#23637;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ScaDLES: Scalable Deep Learning over Streaming data at the Edge. (arXiv:2301.08897v1 [cs.DC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08897
&lt;/p&gt;
&lt;p&gt;
ScaDLES&#26159;&#19968;&#31181;&#29992;&#20110;&#36793;&#32536;&#31471;&#27969;&#24335;&#25968;&#25454;&#19978;&#30340;&#21487;&#25193;&#23637;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31995;&#32479;&#21644;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#65288;DDL&#65289;&#35757;&#32451;&#31995;&#32479;&#35774;&#35745;&#29992;&#20110;&#20113;&#21644;&#25968;&#25454;&#20013;&#24515;&#29615;&#22659;&#65292;&#20551;&#35774;&#20855;&#26377;&#22343;&#21248;&#35745;&#31639;&#36164;&#28304;&#12289;&#39640;&#32593;&#32476;&#24102;&#23485;&#12289;&#36275;&#22815;&#30340;&#20869;&#23384;&#21644;&#23384;&#20648;&#65292;&#20197;&#21450;&#22312;&#25152;&#26377;&#33410;&#28857;&#19978;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20551;&#35774;&#22312;&#36793;&#32536;&#31471;&#19981;&#19968;&#23450;&#36866;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#22312;&#32447;&#26041;&#24335;&#19979;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#12290;&#36793;&#32536;&#35745;&#31639;&#38754;&#20020;&#31995;&#32479;&#21644;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;&#31995;&#32479;&#24322;&#36136;&#24615;&#24402;&#22240;&#20110;&#27599;&#20010;&#35774;&#22791;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#24102;&#23485;&#30340;&#24046;&#24322;&#65292;&#32780;&#32479;&#35745;&#24322;&#36136;&#24615;&#21017;&#26469;&#33258;&#20110;&#36793;&#32536;&#31471;&#30340;&#19981;&#24179;&#34913;&#21644;&#20559;&#26012;&#25968;&#25454;&#12290;&#22312;&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#26102;&#65292;&#35774;&#22791;&#20043;&#38388;&#30340;&#19981;&#21516;&#27969;&#36895;&#20063;&#21487;&#20197;&#25104;&#20026;&#24322;&#36136;&#24615;&#30340;&#21478;&#19968;&#20010;&#26469;&#28304;&#12290;&#22914;&#26524;&#27969;&#36895;&#20302;&#20110;&#35757;&#32451;&#25209;&#37327;&#22823;&#23567;&#65292;&#35774;&#22791;&#38656;&#35201;&#31561;&#24453;&#36275;&#22815;&#30340;&#26679;&#26412;&#27969;&#20837;&#21518;&#25165;&#33021;&#25191;&#34892;&#19968;&#27425;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#36845;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed deep learning (DDL) training systems are designed for cloud and data-center environments that assumes homogeneous compute resources, high network bandwidth, sufficient memory and storage, as well as independent and identically distributed (IID) data across all nodes. However, these assumptions don't necessarily apply on the edge, especially when training neural networks on streaming data in an online manner. Computing on the edge suffers from both systems and statistical heterogeneity. Systems heterogeneity is attributed to differences in compute resources and bandwidth specific to each device, while statistical heterogeneity comes from unbalanced and skewed data on the edge. Different streaming-rates among devices can be another source of heterogeneity when dealing with streaming data. If the streaming rate is lower than training batch-size, device needs to wait until enough samples have streamed in before performing a single iteration of stochastic gradient descent (SGD).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CLIPTER&#65288;CLIP&#25991;&#26412;&#35782;&#21035;&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25972;&#20010;&#22270;&#20687;&#30340;&#20449;&#24687;&#65292;&#36890;&#36807;&#38376;&#25511;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#34701;&#21512;&#21040;&#35009;&#21098;&#30340;&#25991;&#26412;&#22270;&#20687;&#35782;&#21035;&#22120;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.07464</link><description>&lt;p&gt;
CLIPTER: &#22312;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#20013;&#20851;&#27880;&#26356;&#22823;&#30340;&#32972;&#26223;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
CLIPTER: Looking at the Bigger Picture in Scene Text Recognition. (arXiv:2301.07464v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CLIPTER&#65288;CLIP&#25991;&#26412;&#35782;&#21035;&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25972;&#20010;&#22270;&#20687;&#30340;&#20449;&#24687;&#65292;&#36890;&#36807;&#38376;&#25511;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#34701;&#21512;&#21040;&#35009;&#21098;&#30340;&#25991;&#26412;&#22270;&#20687;&#35782;&#21035;&#22120;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#38405;&#35835;&#25991;&#26412;&#36890;&#24120;&#38656;&#35201;&#29702;&#35299;&#20854;&#21608;&#22260;&#30340;&#19978;&#19979;&#25991;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#36136;&#37327;&#36739;&#24046;&#30340;&#25991;&#26412;&#26102;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#22120;&#22312;&#22788;&#29702;&#35009;&#21098;&#30340;&#25991;&#26412;&#22270;&#20687;&#26102;&#24182;&#19981;&#20102;&#35299;&#26356;&#22823;&#30340;&#32972;&#26223;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#30340;&#20195;&#34920;&#24615;&#33021;&#21147;&#65292;&#23558;&#22330;&#26223;&#32423;&#21035;&#30340;&#20449;&#24687;&#25552;&#20379;&#32473;&#22522;&#20110;&#35009;&#21098;&#30340;&#35782;&#21035;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#38376;&#25511;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#65292;&#23558;&#25972;&#20010;&#22270;&#20687;&#30340;&#20016;&#23500;&#34920;&#31034;&#65288;&#26469;&#33258;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65289;&#19982;&#35782;&#21035;&#22120;&#30340;&#21333;&#35789;&#32423;&#29305;&#24449;&#34701;&#21512;&#12290;&#35813;&#32452;&#20214;&#36880;&#28176;&#36716;&#21521;&#22686;&#24378;&#19978;&#19979;&#25991;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#39044;&#35757;&#32451;&#35782;&#21035;&#22120;&#30340;&#31283;&#23450;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#39046;&#20808;&#30340;&#25991;&#26412;&#35782;&#21035;&#26550;&#26500;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;CLIPTER&#65288;CLIP&#25991;&#26412;&#35782;&#21035;&#65289;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#20986;&#20102;&#25913;&#36827;&#20102;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Reading text in real-world scenarios often requires understanding the context surrounding it, especially when dealing with poor-quality text. However, current scene text recognizers are unaware of the bigger picture as they operate on cropped text images. In this study, we harness the representative capabilities of modern vision-language models, such as CLIP, to provide scene-level information to the crop-based recognizer. We achieve this by fusing a rich representation of the entire image, obtained from the vision-language model, with the recognizer word-level features via a gated cross-attention mechanism. This component gradually shifts to the context-enhanced representation, allowing for stable fine-tuning of a pretrained recognizer. We demonstrate the effectiveness of our model-agnostic framework, CLIPTER (CLIP TExt Recognition), on leading text recognition architectures and achieve state-of-the-art results across multiple benchmarks. Furthermore, our analysis highlights improved 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#21453;&#25955;&#23556;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#20302;&#32500;&#27969;&#24418;&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#21482;&#38656;&#35201;&#30446;&#26631;&#20171;&#30005;&#24120;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#26694;&#26550;&#26469;&#36817;&#20284;&#21518;&#39564;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2301.03092</link><description>&lt;p&gt;
&#28145;&#24230;&#27880;&#20837;&#20808;&#39564;&#29992;&#20110;&#21453;&#25955;&#23556;
&lt;/p&gt;
&lt;p&gt;
Deep Injective Prior for Inverse Scattering. (arXiv:2301.03092v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#21453;&#25955;&#23556;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#20302;&#32500;&#27969;&#24418;&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#21482;&#38656;&#35201;&#30446;&#26631;&#20171;&#30005;&#24120;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#26694;&#26550;&#26469;&#36817;&#20284;&#21518;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#30913;&#21453;&#25955;&#23556;&#20013;&#65292;&#30446;&#26631;&#26159;&#21033;&#29992;&#25955;&#23556;&#27874;&#26469;&#37325;&#24314;&#29289;&#20307;&#20171;&#30005;&#24120;&#25968;&#12290;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#20316;&#20026;&#36845;&#20195;&#27714;&#35299;&#22120;&#26367;&#20195;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20027;&#35201;&#24212;&#29992;&#20110;&#25955;&#23556;&#22330;&#30340;&#20998;&#24067;&#28418;&#31227;&#25935;&#24863;&#30340;&#30417;&#30563;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#21482;&#25552;&#20379;&#19968;&#20010;&#20171;&#30005;&#24120;&#25968;&#27169;&#24335;&#30340;&#20272;&#35745;&#65292;&#21487;&#33021;&#22240;&#22122;&#22768;&#21644;&#38382;&#39064;&#30340;&#30149;&#24577;&#24615;&#32780;&#19981;&#36275;&#25110;&#24341;&#23548;&#38169;&#35823;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#21453;&#25955;&#23556;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#19968;&#20010;&#20302;&#32500;&#27969;&#24418;&#20316;&#20026;&#24674;&#22797;&#30446;&#26631;&#20171;&#30005;&#24120;&#25968;&#30340;&#27491;&#21017;&#21270;&#22120;&#12290;&#19982;&#38656;&#35201;&#25955;&#23556;&#22330;&#21644;&#30446;&#26631;&#20171;&#30005;&#24120;&#25968;&#20316;&#20026;&#35757;&#32451;&#36755;&#20837;&#30340;&#30417;&#30563;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#35201;&#30446;&#26631;&#20171;&#30005;&#24120;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#21487;&#20197;&#19982;&#20219;&#20309;&#23454;&#39564;&#35774;&#32622;&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26694;&#26550;&#26469;&#36817;&#20284;&#21518;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
In electromagnetic inverse scattering, the goal is to reconstruct object permittivity using scattered waves. While deep learning has shown promise as an alternative to iterative solvers, it is primarily used in supervised frameworks which are sensitive to distribution drift of the scattered fields, common in practice. Moreover, these methods typically provide a single estimate of the permittivity pattern, which may be inadequate or misleading due to noise and the ill-posedness of the problem. In this paper, we propose a data-driven framework for inverse scattering based on deep generative models. Our approach learns a low-dimensional manifold as a regularizer for recovering target permittivities. Unlike supervised methods that necessitate both scattered fields and target permittivities, our method only requires the target permittivities for training; it can then be used with any experimental setup. We also introduce a Bayesian framework for approximating the posterior distribution of t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26080;&#25034;&#24724;&#35268;&#32422;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#37325;&#32622;&#26426;&#21046;&#24102;&#26469;&#30340;&#23454;&#38469;&#24212;&#29992;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#35777;&#26126;&#27491;&#30830;&#24615;&#30340;&#26080;&#37325;&#32622;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.02389</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#30340;&#26080;&#37325;&#32622;&#24378;&#21270;&#23398;&#20064;&#8212;&#8212;&#22522;&#20110;&#26080;&#25034;&#24724;&#35268;&#32422;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Provable Reset-free Reinforcement Learning by No-Regret Reduction. (arXiv:2301.02389v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02389
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26080;&#25034;&#24724;&#35268;&#32422;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#37325;&#32622;&#26426;&#21046;&#24102;&#26469;&#30340;&#23454;&#38469;&#24212;&#29992;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#35777;&#26126;&#27491;&#30830;&#24615;&#30340;&#26080;&#37325;&#32622;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36804;&#20170;&#20026;&#27490;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#38750;&#24120;&#26377;&#38480;&#12290;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#65292;&#20856;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#37325;&#32622;&#26426;&#21046;&#26469;&#37319;&#26679;&#21512;&#36866;&#30340;&#21021;&#22987;&#29366;&#24577;&#65307;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#25110;&#32773;&#22797;&#26434;&#30340;&#29615;&#22659;&#24037;&#31243;&#65292;&#36825;&#31181;&#37325;&#32622;&#26426;&#21046;&#30340;&#23454;&#29616;&#25104;&#26412;&#36739;&#39640;&#12290;&#20026;&#20102;&#20351;&#23398;&#20064;&#26356;&#21152;&#23454;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26080;&#25034;&#24724;&#35268;&#32422;&#26041;&#27861;&#26469;&#31995;&#32479;&#22320;&#35774;&#35745;&#26080;&#37325;&#32622;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#35268;&#32422;&#23558;&#26080;&#37325;&#32622;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#20004;&#20154;&#21338;&#24328;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36825;&#20010;&#20004;&#20154;&#21338;&#24328;&#20013;&#23454;&#29616;&#20122;&#32447;&#24615;&#25034;&#24724;&#23558;&#24847;&#21619;&#30528;&#22312;&#21407;&#22987;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#23398;&#20064;&#21040;&#20855;&#26377;&#20122;&#32447;&#24615;&#24615;&#33021;&#24724;&#24680;&#21644;&#20122;&#32447;&#24615;&#37325;&#32622;&#24635;&#27425;&#25968;&#30340;&#31574;&#30053;&#12290;&#36825;&#24847;&#21619;&#30528;&#26234;&#33021;&#20307;&#26368;&#32456;&#23398;&#20250;&#20102;&#26368;&#20248;&#21270;&#34892;&#20026;&#24182;&#36991;&#20813;&#37325;&#32622;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#20010;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#23454;&#20363;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#21487;&#35777;&#26126;&#27491;&#30830;&#24615;&#30340;&#26080;&#37325;&#32622;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) so far has limited real-world applications. One key challenge is that typical RL algorithms heavily rely on a reset mechanism to sample proper initial states; these reset mechanisms, in practice, are expensive to implement due to the need for human intervention or heavily engineered environments. To make learning more practical, we propose a generic no-regret reduction to systematically design reset-free RL algorithms. Our reduction turns the reset-free RL problem into a two-player game. We show that achieving sublinear regret in this two-player game would imply learning a policy that has both sublinear performance regret and sublinear total number of resets in the original RL problem. This means that the agent eventually learns to perform optimally and avoid resets. To demonstrate the effectiveness of this reduction, we design an instantiation for linear Markov decision processes, which is the first provably correct reset-free RL algorithm.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22238;&#25918;&#36712;&#36857;&#32780;&#19981;&#26159;&#21407;&#22987;&#30340;&#21069;&#21521;&#36712;&#36857;&#26469;&#35757;&#32451;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;&#22870;&#21169;&#20449;&#21495;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#65292;&#32780;&#19988;&#21482;&#38656;&#35201;&#36827;&#34892;&#24494;&#23567;&#30340;&#31639;&#27861;&#25913;&#21464;&#12290;</title><link>http://arxiv.org/abs/2212.14214</link><description>&lt;p&gt;
&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Backward Curriculum Reinforcement Learning. (arXiv:2212.14214v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14214
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22238;&#25918;&#36712;&#36857;&#32780;&#19981;&#26159;&#21407;&#22987;&#30340;&#21069;&#21521;&#36712;&#36857;&#26469;&#35757;&#32451;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;&#22870;&#21169;&#20449;&#21495;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#65292;&#32780;&#19988;&#21482;&#38656;&#35201;&#36827;&#34892;&#24494;&#23567;&#30340;&#31639;&#27861;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;&#21069;&#21521;&#29983;&#25104;&#36712;&#36857;&#26469;&#35757;&#32451;&#26234;&#33021;&#20307;&#65292;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#30340;&#25351;&#23548;&#19981;&#36275;&#20197;&#20351;&#26234;&#33021;&#20307;&#36827;&#34892;&#23613;&#21487;&#33021;&#22810;&#30340;&#25506;&#32034;&#12290;&#23613;&#31649;&#25105;&#20204;&#35748;&#35782;&#21040;&#24378;&#21270;&#23398;&#20064;&#32467;&#26524;&#26469;&#33258;&#20805;&#20998;&#30340;&#25506;&#32034;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#23384;&#22312;&#25240;&#34935;&#65292;&#36825;&#26159;&#24433;&#21709;&#31639;&#27861;&#24615;&#33021;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#20351;&#29992;&#22870;&#21169;&#22609;&#36896;&#25216;&#26415;&#21644;&#32593;&#32476;&#32467;&#26500;&#20462;&#25913;&#26469;&#22686;&#21152;&#26679;&#26412;&#25928;&#29575;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#24456;&#22810;&#27493;&#39588;&#26469;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#20351;&#29992;&#22238;&#25918;&#36712;&#36857;&#32780;&#19981;&#26159;&#21407;&#22987;&#30340;&#21069;&#21521;&#36712;&#36857;&#26469;&#35757;&#32451;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#22870;&#21169;&#20449;&#21495;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#35201;&#22312;&#26234;&#33021;&#20307;&#35757;&#32451;&#20043;&#21069;&#23545;&#36712;&#36857;&#30340;&#39034;&#24207;&#36827;&#34892;&#24494;&#23567;&#30340;&#25913;&#21464;&#65292;&#20351;&#24471;&#23454;&#29616;&#36215;&#26469;&#26356;&#21152;&#30452;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current reinforcement learning algorithms train an agent using forward-generated trajectories, which provide little guidance so that the agent can explore as much as possible. While realizing the value of reinforcement learning results from sufficient exploration, this approach leads to a trade-off in losing sample efficiency, an essential factor impacting algorithm performance. Previous tasks use reward-shaping techniques and network structure modification to increase sample efficiency. However, these methods require many steps to implement. In this work, we propose novel backward curriculum reinforcement learning that begins training the agent using the backward trajectory of the episode instead of the original forward trajectory. This approach provides the agent with a strong reward signal, enabling more sample-efficient learning. Moreover, our method only requires a minor change in the algorithm of reversing the order of the trajectory before agent training, allowing a straightforw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28151;&#21512;&#22810;&#36890;&#36947;&#31232;&#30095;&#20449;&#21495;&#24674;&#22797;&#38382;&#39064;&#65292;&#23558;&#20854;&#35270;&#20026;&#20449;&#21495;&#37325;&#24314;&#38382;&#39064;&#24182;&#24314;&#31435;&#20102;&#21807;&#19968;&#24674;&#22797;&#30340;&#26465;&#20214;&#12290;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#65292;&#23578;&#26410;&#32771;&#34385;&#36807;&#30456;&#20851;&#30340;&#37319;&#26679;&#32467;&#26524;&#65292;&#24182;&#19988;&#29616;&#26377;&#30340;&#26080;&#26631;&#31614;&#24863;&#30693;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2212.07368</link><description>&lt;p&gt;
&#28151;&#21512;&#22810;&#36890;&#36947;&#31232;&#30095;&#20449;&#21495;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Shuffled Multi-Channel Sparse Signal Recovery. (arXiv:2212.07368v3 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28151;&#21512;&#22810;&#36890;&#36947;&#31232;&#30095;&#20449;&#21495;&#24674;&#22797;&#38382;&#39064;&#65292;&#23558;&#20854;&#35270;&#20026;&#20449;&#21495;&#37325;&#24314;&#38382;&#39064;&#24182;&#24314;&#31435;&#20102;&#21807;&#19968;&#24674;&#22797;&#30340;&#26465;&#20214;&#12290;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#65292;&#23578;&#26410;&#32771;&#34385;&#36807;&#30456;&#20851;&#30340;&#37319;&#26679;&#32467;&#26524;&#65292;&#24182;&#19988;&#29616;&#26377;&#30340;&#26080;&#26631;&#31614;&#24863;&#30693;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20010;&#30495;&#23454;&#24212;&#29992;&#20013;&#24120;&#24120;&#23384;&#22312;&#26679;&#26412;&#19982;&#20854;&#23545;&#24212;&#30340;&#36890;&#36947;&#25110;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20854;&#20316;&#20026;&#19968;&#20010;&#20449;&#21495;&#37325;&#24314;&#38382;&#39064;&#26469;&#25552;&#20986;&#65292;&#20854;&#20013;&#26679;&#26412;&#19982;&#20854;&#23545;&#24212;&#30340;&#36890;&#36947;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#20002;&#22833;&#20102;&#12290;&#22312;&#20551;&#35774;&#25105;&#20204;&#26377;&#19968;&#20010;&#24863;&#30693;&#30697;&#38453;&#29992;&#20110;&#24213;&#23618;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#38382;&#39064;&#31561;&#20215;&#20110;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#26080;&#26631;&#31614;&#24863;&#30693;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#21807;&#19968;&#24674;&#22797;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25991;&#29486;&#20013;&#23578;&#26410;&#32771;&#34385;&#36807;&#28151;&#21512;&#22810;&#36890;&#36947;&#20449;&#21495;&#37325;&#24314;&#30340;&#37319;&#26679;&#32467;&#26524;&#65292;&#29616;&#26377;&#30340;&#26080;&#26631;&#31614;&#24863;&#30693;&#26041;&#27861;&#20063;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;&#20449;&#21495;&#20801;&#35768;&#31232;&#30095;&#34920;&#31034;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mismatches between samples and their respective channel or target commonly arise in several real-world applications. For instance, whole-brain calcium imaging of freely moving organisms, multiple-target tracking or multi-person contactless vital sign monitoring may be severely affected by mismatched sample-channel assignments. To systematically address this fundamental problem, we pose it as a signal reconstruction problem where we have lost correspondences between the samples and their respective channels. Assuming that we have a sensing matrix for the underlying signals, we show that the problem is equivalent to a structured unlabeled sensing problem, and establish sufficient conditions for unique recovery. To the best of our knowledge, a sampling result for the reconstruction of shuffled multi-channel signals has not been considered in the literature and existing methods for unlabeled sensing cannot be directly applied. We extend our results to the case where the signals admit a spa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#36827;&#21270;&#32858;&#31867;&#31639;&#27861; GSEMO &#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#20445;&#35777;&#35299;&#20915;&#22235;&#31181;&#32858;&#31867;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#31163;&#25955; k-&#20013;&#20301;&#25968;&#32858;&#31867;&#20013;&#30340;&#20844;&#24179;&#32858;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.01771</link><description>&lt;p&gt;
&#36827;&#21270;&#32858;&#31867;&#26159;&#21542;&#26377;&#29702;&#35770;&#20445;&#35777;?
&lt;/p&gt;
&lt;p&gt;
Can Evolutionary Clustering Have Theoretical Guarantees?. (arXiv:2212.01771v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#36827;&#21270;&#32858;&#31867;&#31639;&#27861; GSEMO &#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#20445;&#35777;&#35299;&#20915;&#22235;&#31181;&#32858;&#31867;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#31163;&#25955; k-&#20013;&#20301;&#25968;&#32858;&#31867;&#20013;&#30340;&#20844;&#24179;&#32858;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26159;&#35768;&#22810;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#22522;&#20110;&#26576;&#31181;&#36317;&#31163;&#24230;&#37327;&#23558;&#32473;&#23450;&#30340;&#25968;&#25454;&#38598;&#20998;&#25104;&#19981;&#21516;&#30340;&#32452;&#65292;&#20351;&#24471;&#21516;&#19968;&#32452;&#20013;&#30340;&#25968;&#25454;&#28857;&#30456;&#20284;&#32780;&#19981;&#21516;&#32452;&#20013;&#30340;&#25968;&#25454;&#28857;&#19981;&#30456;&#20284;&#12290;&#30001;&#20110;&#20854;&#37325;&#35201;&#24615;&#21644; NP &#22256;&#38590;&#24615;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20854;&#20013;&#36827;&#21270;&#31639;&#27861;&#26159;&#19968;&#31867;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;&#36827;&#21270;&#32858;&#31867;&#24050;&#32463;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25152;&#26377;&#30340;&#32467;&#26524;&#37117;&#26159;&#32463;&#39564;&#20027;&#20041;&#30340;&#65292;&#32570;&#20047;&#29702;&#35770;&#25903;&#25345;&#12290;&#26412;&#25991;&#36890;&#36807;&#35777;&#26126; GSEMO&#65288;&#19968;&#31181;&#31616;&#21333;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65289;&#22312;&#35299;&#20915;&#22235;&#31181;&#32858;&#31867;&#24418;&#24335;&#65306;k-&#24635;&#36317;&#31163;&#26368;&#23567;&#21270;&#12289;k-&#20013;&#24515;&#12289;&#31163;&#25955; k-&#20013;&#20301;&#25968;&#21644; k-&#22343;&#20540;&#26102;&#65292;&#20854;&#36817;&#20284;&#24615;&#33021;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20844;&#24179;&#30340;&#32858;&#31867;&#38382;&#39064;&#65292;&#21363;&#23613;&#37327;&#36991;&#20813;&#31639;&#27861;&#30340;&#20559;&#35265;&#65292;&#36825;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#36817;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#31163;&#25955; k-&#20013;&#20301;&#25968;&#32858;&#31867;&#19979;&#30340;&#20844;&#24179;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering is a fundamental problem in many areas, which aims to partition a given data set into groups based on some distance measure, such that the data points in the same group are similar while that in different groups are dissimilar. Due to its importance and NP-hardness, a lot of methods have been proposed, among which evolutionary algorithms are a class of popular ones. Evolutionary clustering has found many successful applications, but all the results are empirical, lacking theoretical support. This paper fills this gap by proving that the approximation performance of the GSEMO (a simple multi-objective evolutionary algorithm) for solving four formulations of clustering, i.e., $k$-tMM, $k$-center, discrete $k$-median and $k$-means, can be theoretically guaranteed. Furthermore, we consider clustering under fairness, which tries to avoid algorithmic bias, and has recently been an important research topic in machine learning. We prove that for discrete $k$-median clustering under 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#34892;&#20026;&#32422;&#26463;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38381;&#21512;&#24418;&#24335;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#65292;&#35813;&#31639;&#23376;&#23558;&#34892;&#20026;&#31574;&#30053;&#24314;&#27169;&#20026;&#39640;&#26031;&#28151;&#21512;&#65292;&#21033;&#29992;LogSumExp&#30340;&#19979;&#30028;&#21644;Jensen&#19981;&#31561;&#24335;&#20811;&#26381;&#20102;&#20248;&#21270;&#22256;&#38590;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#30340;&#24322;&#26500;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2211.15956</link><description>&lt;p&gt;
&#22522;&#20110;&#38381;&#21512;&#24418;&#24335;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning with Closed-Form Policy Improvement Operators. (arXiv:2211.15956v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#34892;&#20026;&#32422;&#26463;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38381;&#21512;&#24418;&#24335;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#65292;&#35813;&#31639;&#23376;&#23558;&#34892;&#20026;&#31574;&#30053;&#24314;&#27169;&#20026;&#39640;&#26031;&#28151;&#21512;&#65292;&#21033;&#29992;LogSumExp&#30340;&#19979;&#30028;&#21644;Jensen&#19981;&#31561;&#24335;&#20811;&#26381;&#20102;&#20248;&#21270;&#22256;&#38590;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#30340;&#24322;&#26500;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#32422;&#26463;&#31574;&#30053;&#20248;&#21270;&#24050;&#34987;&#35777;&#26126;&#26159;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#31181;&#25104;&#21151;&#30340;&#33539;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#38381;&#21512;&#24418;&#24335;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34892;&#20026;&#32422;&#26463;&#33258;&#28982;&#22320;&#28608;&#21169;&#20102;&#20351;&#29992;&#19968;&#38454;&#27888;&#21202;&#36817;&#20284;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#31574;&#30053;&#30446;&#26631;&#30340;&#32447;&#24615;&#36817;&#20284;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23454;&#38469;&#25968;&#25454;&#38598;&#36890;&#24120;&#30001;&#24322;&#26500;&#31574;&#30053;&#25910;&#38598;&#32780;&#26469;&#65292;&#25105;&#20204;&#23558;&#34892;&#20026;&#31574;&#30053;&#24314;&#27169;&#20026;&#39640;&#26031;&#28151;&#21512;&#65292;&#24182;&#21033;&#29992;LogSumExp&#30340;&#19979;&#30028;&#21644;Jensen&#19981;&#31561;&#24335;&#20811;&#26381;&#20102;&#24341;&#36215;&#20248;&#21270;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#38381;&#24335;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#39062;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#26469;&#23454;&#20363;&#21270;&#31163;&#32447;RL&#31639;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Behavior constrained policy optimization has been demonstrated to be a successful paradigm for tackling Offline Reinforcement Learning. By exploiting historical transitions, a policy is trained to maximize a learned value function while constrained by the behavior policy to avoid a significant distributional shift. In this paper, we propose our closed-form policy improvement operators. We make a novel observation that the behavior constraint naturally motivates the use of first-order Taylor approximation, leading to a linear approximation of the policy objective. Additionally, as practical datasets are usually collected by heterogeneous policies, we model the behavior policies as a Gaussian Mixture and overcome the induced optimization difficulties by leveraging the LogSumExp's lower bound and Jensen's Inequality, giving rise to a closed-form policy improvement operator. We instantiate offline RL algorithms with our novel policy improvement operators and empirically demonstrate their e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#35299;&#30721;&#26041;&#27861;&#26469;&#29983;&#25104;&#22270;&#20687;&#12290;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#21367;&#31215;&#23618;&#24314;&#27169;&#36712;&#36857;&#19978;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2211.13449</link><description>&lt;p&gt;
&#24555;&#36895;&#37319;&#26679;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#31639;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast Sampling of Diffusion Models via Operator Learning. (arXiv:2211.13449v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#35299;&#30721;&#26041;&#27861;&#26469;&#29983;&#25104;&#22270;&#20687;&#12290;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#21367;&#31215;&#23618;&#24314;&#27169;&#36712;&#36857;&#19978;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25193;&#25955;&#27169;&#22411;&#38656;&#35201;&#36890;&#36807;&#25968;&#30334;&#21040;&#25968;&#21315;&#27425;&#32593;&#32476;&#35780;&#20272;&#26469;&#27169;&#25311;&#30001;&#24494;&#20998;&#26041;&#31243;&#23450;&#20041;&#30340;&#36830;&#32493;&#36807;&#31243;&#65292;&#25152;&#20197;&#20854;&#37319;&#26679;&#36807;&#31243;&#36739;&#24930;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#65292;&#19968;&#31181;&#39640;&#25928;&#35299;&#20915;&#27010;&#29575;&#27969;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#26469;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#36807;&#31243;&#12290;&#19982;&#20854;&#20182;&#20855;&#26377;&#39034;&#24207;&#24615;&#36136;&#30340;&#24555;&#36895;&#37319;&#26679;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#35299;&#30721;&#26041;&#27861;&#65292;&#21482;&#38656;&#36827;&#34892;&#19968;&#27425;&#27169;&#22411;&#21069;&#21521;&#20256;&#36882;&#21363;&#21487;&#29983;&#25104;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#19982;&#31070;&#32463;&#31639;&#23376;&#65288;DSNO&#65289;&#65292;&#23558;&#21021;&#22987;&#26465;&#20214;&#65288;&#21363;&#39640;&#26031;&#20998;&#24067;&#65289;&#26144;&#23556;&#21040;&#36870;&#25193;&#25955;&#36807;&#31243;&#30340;&#36830;&#32493;&#26102;&#38388;&#35299;&#36712;&#36857;&#12290;&#20026;&#20102;&#24314;&#27169;&#36712;&#36857;&#19978;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#23558;&#21442;&#25968;&#21270;&#20613;&#31435;&#21494;&#31354;&#38388;&#20013;&#30340;&#26102;&#38388;&#21367;&#31215;&#23618;&#24341;&#20837;&#32473;&#23450;&#25193;&#25955;&#27169;&#22411;&#30340;&#20027;&#24178;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have found widespread adoption in various areas. However, their sampling process is slow because it requires hundreds to thousands of network evaluations to emulate a continuous process defined by differential equations. In this work, we use neural operators, an efficient method to solve the probability flow differential equations, to accelerate the sampling process of diffusion models. Compared to other fast sampling methods that have a sequential nature, we are the first to propose a parallel decoding method that generates images with only one model forward pass. We propose diffusion model sampling with neural operator (DSNO) that maps the initial condition, i.e., Gaussian distribution, to the continuous-time solution trajectory of the reverse diffusion process. To model the temporal correlations along the trajectory, we introduce temporal convolution layers that are parameterized in the Fourier space into the given diffusion model backbone. We show our method achiev
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21333;&#27425;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#65292;&#24182;&#32473;&#20986;&#20102;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2211.10890</link><description>&lt;p&gt;
&#21333;&#27425;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#36866;&#29992;&#20110;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;
&lt;/p&gt;
&lt;p&gt;
Single-Pass Contrastive Learning Can Work for Both Homophilic and Heterophilic Graph. (arXiv:2211.10890v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10890
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21333;&#27425;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#65292;&#24182;&#32473;&#20986;&#20102;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#25216;&#26415;&#36890;&#24120;&#38656;&#35201;&#20004;&#27425;&#21069;&#21521;&#20256;&#36882;&#25165;&#33021;&#26500;&#24314;&#20135;&#29983;&#23545;&#27604;&#30340;&#25439;&#22833;&#65292;&#36825;&#23545;&#20110;&#25429;&#25417;&#33410;&#28857;&#29305;&#24449;&#30340;&#20302;&#39057;&#20449;&#21495;&#26159;&#26377;&#25928;&#30340;&#12290;&#36825;&#31181;&#21452;&#36890;&#36947;&#35774;&#35745;&#24050;&#32463;&#22312;&#21516;&#26500;&#22270;&#19978;&#34920;&#29616;&#20986;&#23454;&#35777;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#22312;&#24322;&#26500;&#22270;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#30452;&#25509;&#36830;&#25509;&#30340;&#33410;&#28857;&#36890;&#24120;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#65292;&#23578;&#19981;&#28165;&#26970;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;GCL&#26041;&#27861;&#26080;&#27861;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#21463;&#21040;GCL&#26041;&#27861;&#22312;&#24322;&#26500;&#22270;&#19978;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#30340;&#24433;&#21709;&#65292;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#20110;&#26159;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#23601;&#20986;&#29616;&#20102;&#65306;&#25105;&#20204;&#33021;&#21542;&#35774;&#35745;&#19968;&#31181;&#36866;&#29992;&#20110;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#30340;GCL&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#22312;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#19978;&#36890;&#36807;&#37051;&#22495;&#32858;&#21512;&#24471;&#21040;&#30340;&#29305;&#24449;&#30340;&#38598;&#20013;&#24615;&#36136;&#65292;&#24341;&#20837;&#20102;&#21333;&#36890;&#36947;&#22270;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing graph contrastive learning (GCL) techniques typically require two forward passes for a single instance to construct the contrastive loss, which is effective for capturing the low-frequency signals of node features. Such a dual-pass design has shown empirical success on homophilic graphs, but its effectiveness on heterophilic graphs, where directly connected nodes typically have different labels, is unknown. In addition, existing GCL approaches fail to provide strong performance guarantees. Coupled with the unpredictability of GCL approaches on heterophilic graphs, their applicability in real-world contexts is limited. Then, a natural question arises: Can we design a GCL method that works for both homophilic and heterophilic graphs with a performance guarantee? To answer this question, we theoretically study the concentration property of features obtained by neighborhood aggregation on homophilic and heterophilic graphs, introduce the single-pass graph contrastive learning loss
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25289;&#27604;&#25991;&#23398;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#20854;&#39118;&#26684;&#26469;&#26816;&#27979;Midrash Tanhuma&#20013;&#30340;&#22833;&#33853;&#26448;&#26009;&#12290;</title><link>http://arxiv.org/abs/2211.09710</link><description>&lt;p&gt;
&#32763;&#35793;&#65306;&#21033;&#29992;&#39118;&#26684;&#20998;&#31867;&#26469;&#26816;&#27979;&#22833;&#33853;&#30340;&#12298;Midrash Tanhuma&#12299;&#26448;&#26009;
&lt;/p&gt;
&lt;p&gt;
Style Classification of Rabbinic Literature for Detection of Lost Midrash Tanhuma Material. (arXiv:2211.09710v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25289;&#27604;&#25991;&#23398;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#20854;&#39118;&#26684;&#26469;&#26816;&#27979;Midrash Tanhuma&#20013;&#30340;&#22833;&#33853;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Midrash&#38598;&#21512;&#26159;&#22797;&#26434;&#30340;&#25289;&#27604;&#25991;&#29486;&#20316;&#21697;&#65292;&#30001;&#22810;&#31181;&#35821;&#35328;&#30340;&#25991;&#26412;&#32452;&#25104;&#65292;&#32463;&#36807;&#19981;&#31283;&#23450;&#30340;&#21475;&#22836;&#21644;&#20070;&#38754;&#20256;&#36882;&#36807;&#31243;&#28436;&#21464;&#32780;&#26469;&#12290;&#30830;&#23450;&#36825;&#31181;&#21512;&#38598;&#20013;&#30340;&#19968;&#20010;&#32473;&#23450;&#27573;&#33853;&#30340;&#36215;&#28304;&#24182;&#19981;&#24635;&#26159;&#30452;&#35266;&#30340;&#65292;&#24120;&#24120;&#26159;&#23398;&#32773;&#20043;&#38388;&#30340;&#20105;&#35758;&#65292;&#28982;&#32780;&#23545;&#20110;&#23398;&#32773;&#20204;&#29702;&#35299;&#27573;&#33853;&#21450;&#20854;&#19982;&#25289;&#27604;&#25991;&#38598;&#20013;&#20854;&#20182;&#25991;&#26412;&#30340;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39118;&#26684;&#30340;&#25289;&#27604;&#25991;&#23398;&#20998;&#31867;&#31995;&#32479;&#65292;&#21033;&#29992;&#26368;&#36817;&#21457;&#24067;&#30340;&#38024;&#23545;&#24076;&#20271;&#26469;&#35821;&#30340;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#22833;&#33853;&#30340;Midrash Tanhuma&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
Midrash collections are complex rabbinic works that consist of text in multiple languages, which evolved through long processes of unstable oral and written transmission. Determining the origin of a given passage in such a compilation is not always straightforward and is often a matter of dispute among scholars, yet it is essential for scholars' understanding of the passage and its relationship to other texts in the rabbinic corpus.  To help solve this problem, we propose a system for classification of rabbinic literature based on its style, leveraging recently released pretrained Transformer models for Hebrew. Additionally, we demonstrate how our method can be applied to uncover lost material from Midrash Tanhuma.
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#23398;&#20064;&#22686;&#24378;&#30340;B&#26641;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22797;&#21512;&#20248;&#20808;&#32423;&#30340;Treaps&#65292;&#27599;&#20010;&#39033;&#30446;&#30340;&#28145;&#24230;&#30001;&#20854;&#39044;&#27979;&#26435;&#37325;&#30830;&#23450;&#65292;&#25512;&#24191;&#20102;&#26368;&#36817;&#30340;&#23398;&#20064;&#22686;&#24378;BST&#65292;&#24182;&#19988;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#35775;&#38382;&#24207;&#21015;&#20013;&#30340;&#23616;&#37096;&#24615;&#30340;B&#26641;&#25968;&#25454;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2211.09251</link><description>&lt;p&gt;
&#23398;&#20064;&#22686;&#24378;&#30340;B&#26641;
&lt;/p&gt;
&lt;p&gt;
Learning-Augmented B-Trees. (arXiv:2211.09251v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09251
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#23398;&#20064;&#22686;&#24378;&#30340;B&#26641;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22797;&#21512;&#20248;&#20808;&#32423;&#30340;Treaps&#65292;&#27599;&#20010;&#39033;&#30446;&#30340;&#28145;&#24230;&#30001;&#20854;&#39044;&#27979;&#26435;&#37325;&#30830;&#23450;&#65292;&#25512;&#24191;&#20102;&#26368;&#36817;&#30340;&#23398;&#20064;&#22686;&#24378;BST&#65292;&#24182;&#19988;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#35775;&#38382;&#24207;&#21015;&#20013;&#30340;&#23616;&#37096;&#24615;&#30340;B&#26641;&#25968;&#25454;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22797;&#21512;&#20248;&#20808;&#32423;&#30340;Treaps&#26469;&#30740;&#31350;&#23398;&#20064;&#22686;&#24378;&#30340;&#20108;&#21449;&#25628;&#32034;&#26641;&#65288;BST&#65289;&#21644;B&#26641;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#25628;&#32034;&#26641;&#65292;&#20854;&#20013;&#27599;&#20010;&#39033;&#30446;&#30340;&#28145;&#24230;&#30001;&#20854;&#39044;&#27979;&#26435;&#37325;$w_x$&#30830;&#23450;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#32467;&#26524;&#65292;&#27599;&#20010;&#39033;&#30446;$x$&#37117;&#26377;&#20854;&#22797;&#21512;&#20248;&#20808;&#32423;$-\lfloor\log\log(1/w_x)\rfloor + U(0, 1)$&#65292;&#20854;&#20013;$U(0, 1)$&#26159;&#22343;&#21248;&#20998;&#24067;&#30340;&#38543;&#26426;&#21464;&#37327;&#12290;&#36825;&#23558;&#26368;&#36817;&#30340;&#23398;&#20064;&#22686;&#24378;BST&#65288;Lin-Luo-Woodruff ICML`22&#65289;&#25512;&#24191;&#21040;&#20219;&#24847;&#36755;&#20837;&#21644;&#39044;&#27979;&#65292;&#32780;&#19981;&#20165;&#20165;&#36866;&#29992;&#20110;Zipfian&#20998;&#24067;&#12290;&#23427;&#36824;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#21487;&#20197;&#26681;&#25454;&#35775;&#38382;&#24207;&#21015;&#20013;&#30340;&#23616;&#37096;&#24615;&#36827;&#34892;&#22312;&#32447;&#33258;&#25105;&#37325;&#32452;&#30340;B&#26641;&#25968;&#25454;&#32467;&#26500;&#12290;&#35813;&#25968;&#25454;&#32467;&#26500;&#23545;&#20110;&#39044;&#27979;&#38169;&#35823;&#26159;&#20581;&#22766;&#30340;&#65292;&#21487;&#20197;&#22788;&#29702;&#25554;&#20837;&#12289;&#21024;&#38500;&#20197;&#21450;&#39044;&#27979;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study learning-augmented binary search trees (BSTs) and B-Trees via Treaps with composite priorities. The result is a simple search tree where the depth of each item is determined by its predicted weight $w_x$. To achieve the result, each item $x$ has its composite priority $-\lfloor\log\log(1/w_x)\rfloor + U(0, 1)$ where $U(0, 1)$ is the uniform random variable. This generalizes the recent learning-augmented BSTs [Lin-Luo-Woodruff ICML`22], which only work for Zipfian distributions, to arbitrary inputs and predictions. It also gives the first B-Tree data structure that can provably take advantage of localities in the access sequence via online self-reorganization. The data structure is robust to prediction errors and handles insertions, deletions, as well as prediction updates.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;MIMIC-RRS&#65292;&#21253;&#21547;&#22810;&#20010;&#35299;&#21078;&#23398;&#21644;&#27169;&#24577;&#12290;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#21644;&#20020;&#24202;&#35780;&#20272;&#65292;&#25105;&#20204;&#26088;&#22312;&#25193;&#22823;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2211.08584</link><description>&lt;p&gt;
&#25193;&#22823;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#33539;&#22260;&#65306;&#22810;&#20010;&#35299;&#21078;&#23398;&#21644;&#27169;&#24577;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Toward expanding the scope of radiology report summarization to multiple anatomies and modalities. (arXiv:2211.08584v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;MIMIC-RRS&#65292;&#21253;&#21547;&#22810;&#20010;&#35299;&#21078;&#23398;&#21644;&#27169;&#24577;&#12290;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#21644;&#20020;&#24202;&#35780;&#20272;&#65292;&#25105;&#20204;&#26088;&#22312;&#25193;&#22823;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#65288;RRS&#65289;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#32473;&#23450;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#21457;&#29616;&#37096;&#20998;&#65292;&#30446;&#26631;&#26159;&#29983;&#25104;&#19968;&#20010;&#27010;&#36848;&#65288;&#31216;&#20026;&#21360;&#35937;&#37096;&#20998;&#65289;&#65292;&#31361;&#20986;&#25918;&#23556;&#23398;&#30740;&#31350;&#30340;&#20851;&#38190;&#35266;&#23519;&#21644;&#32467;&#35770;&#12290;&#28982;&#32780;&#65292;RRS&#30446;&#21069;&#38754;&#20020;&#30528;&#37325;&#35201;&#30340;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#31169;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#26080;&#27861;&#37325;&#29616;&#32467;&#26524;&#24182;&#22312;&#19981;&#21516;&#31995;&#32479;&#21644;&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#12290;&#20854;&#27425;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26041;&#27861;&#20165;&#22312;&#33016;&#37096;X&#23556;&#32447;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65288;MIMIC-RRS&#65289;&#65292;&#28041;&#21450;MIMIC-III&#21644;MIMIC-CXR&#25968;&#25454;&#38598;&#30340;&#19977;&#31181;&#26032;&#30340;&#27169;&#24577;&#21644;&#19971;&#31181;&#26032;&#30340;&#35299;&#21078;&#23398;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;MIMIC-RRS&#20013;&#30340;&#27169;&#24577;-&#35299;&#21078;&#23398;&#23545;&#20869;&#21644;&#23545;&#22806;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;RadGraph&#35780;&#20272;&#23427;&#20204;&#30340;&#20020;&#24202;&#21151;&#25928;&#65292;&#36825;&#26159;&#19968;&#20010;&#20107;&#23454;&#27491;&#30830;&#24615;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radiology report summarization (RRS) is a growing area of research. Given the Findings section of a radiology report, the goal is to generate a summary (called an Impression section) that highlights the key observations and conclusions of the radiology study. However, RRS currently faces essential limitations.First, many prior studies conduct experiments on private datasets, preventing reproduction of results and fair comparisons across different systems and solutions. Second, most prior approaches are evaluated solely on chest X-rays. To address these limitations, we propose a dataset (MIMIC-RRS) involving three new modalities and seven new anatomies based on the MIMIC-III and MIMIC-CXR datasets. We then conduct extensive experiments to evaluate the performance of models both within and across modality-anatomy pairs in MIMIC-RRS. In addition, we evaluate their clinical efficacy via RadGraph, a factual correctness metric.
&lt;/p&gt;</description></item><item><title>C3&#26159;&#19968;&#31181;&#24341;&#20837;&#20102;&#20132;&#21449;&#23454;&#20363;&#20851;&#31995;&#30340;&#23545;&#27604;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#23454;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#22686;&#21152;&#20102;&#27491;&#23545;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32858;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.07136</link><description>&lt;p&gt;
C3: &#36328;&#23454;&#20363;&#24341;&#23548;&#23545;&#27604;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
C3: Cross-instance guided Contrastive Clustering. (arXiv:2211.07136v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07136
&lt;/p&gt;
&lt;p&gt;
C3&#26159;&#19968;&#31181;&#24341;&#20837;&#20102;&#20132;&#21449;&#23454;&#20363;&#20851;&#31995;&#30340;&#23545;&#27604;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#23454;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#22686;&#21152;&#20102;&#27491;&#23545;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26159;&#23558;&#30456;&#20284;&#30340;&#25968;&#25454;&#26679;&#26412;&#32858;&#38598;&#21040;&#26080;&#38656;&#39044;&#23450;&#20041;&#26631;&#31614;&#30340;&#31751;&#20013;&#30340;&#20219;&#21153;&#12290;&#23427;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#37325;&#26032;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#35813;&#39046;&#22495;&#30340;&#20852;&#36259;&#12290;&#23545;&#27604;&#32858;&#31867;&#65288;CC&#65289;&#27169;&#22411;&#26159;&#28145;&#24230;&#32858;&#31867;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#29983;&#25104;&#27599;&#20010;&#25968;&#25454;&#23454;&#20363;&#30340;&#27491;&#36127;&#23545;&#12290;CC&#27169;&#22411;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#23558;&#27491;&#23545;&#23454;&#20363;&#30340;&#23454;&#20363;&#32423;&#21644;&#32858;&#31867;&#32423;&#34920;&#31034;&#20998;&#32452;&#22312;&#19968;&#36215;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;&#23613;&#31649;&#25552;&#39640;&#20102;SOTA&#65292;&#20294;&#36825;&#20123;&#31639;&#27861;&#24573;&#30053;&#20102;&#25658;&#24102;&#26377;&#25913;&#36827;&#32858;&#31867;&#24615;&#33021;&#30340;&#20132;&#21449;&#23454;&#20363;&#27169;&#24335;&#12290;&#36825;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#38169;&#35823;&#36127;&#23545;&#25968;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#23427;&#30340;&#30495;&#27491;&#27491;&#23545;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#32858;&#31867;&#26041;&#27861;&#65292;Cross-instance guided Contrastive Clustering (C3)&#65292;&#32771;&#34385;&#20102;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#22686;&#21152;&#27491;&#23545;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering is the task of gathering similar data samples into clusters without using any predefined labels. It has been widely studied in machine learning literature, and recent advancements in deep learning have revived interest in this field. Contrastive clustering (CC) models are a staple of deep clustering in which positive and negative pairs of each data instance are generated through data augmentation. CC models aim to learn a feature space where instance-level and cluster-level representations of positive pairs are grouped together. Despite improving the SOTA, these algorithms ignore the cross-instance patterns, which carry essential information for improving clustering performance. This increases the false-negative-pair rate of the model while decreasing its true-positive-pair rate. In this paper, we propose a novel contrastive clustering method, Cross-instance guided Contrastive Clustering (C3), that considers the cross-sample relationships to increase the number of positive p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#39640;&#32500;&#36125;&#21494;&#26031;&#32447;&#24615;&#27169;&#22411;&#30340;&#22810;&#37325;&#26816;&#39564;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#36817;&#20284;&#26368;&#20248;&#30340;&#22810;&#37325;&#26816;&#39564;&#31243;&#24207;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#25511;&#21046;&#39057;&#29575;FDR&#65292;&#24182;&#33021;&#36798;&#21040;&#36817;&#20284;&#26368;&#20248;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.02778</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#32447;&#24615;&#27169;&#22411;&#20013;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;FDR&#25511;&#21046;&#30340;&#36817;&#20284;&#26368;&#20248;&#22810;&#37325;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Near-optimal multiple testing in Bayesian linear models with finite-sample FDR control. (arXiv:2211.02778v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#39640;&#32500;&#36125;&#21494;&#26031;&#32447;&#24615;&#27169;&#22411;&#30340;&#22810;&#37325;&#26816;&#39564;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#36817;&#20284;&#26368;&#20248;&#30340;&#22810;&#37325;&#26816;&#39564;&#31243;&#24207;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#25511;&#21046;&#39057;&#29575;FDR&#65292;&#24182;&#33021;&#36798;&#21040;&#36817;&#20284;&#26368;&#20248;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#21464;&#37327;&#36873;&#25321;&#38382;&#39064;&#20013;&#65292;&#32479;&#35745;&#23398;&#23478;&#36890;&#24120;&#33268;&#21147;&#20110;&#35774;&#35745;&#33021;&#25511;&#21046;&#34394;&#35686;&#21457;&#29616;&#29575;&#65288;FDR&#65289;&#30340;&#22810;&#37325;&#26816;&#39564;&#36807;&#31243;&#65292;&#21516;&#26102;&#35782;&#21035;&#26356;&#22810;&#30456;&#20851;&#21464;&#37327;&#12290;&#27169;&#22411;-X&#26041;&#27861;&#65288;&#22914;Knockoffs&#21644;&#26465;&#20214;&#38543;&#26426;&#21270;&#26816;&#39564;&#65289;&#22312;&#20551;&#35774;&#24050;&#30693;&#21327;&#21464;&#37327;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#26377;&#38480;&#26679;&#26412;FDR&#25511;&#21046;&#30340;&#20027;&#35201;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26159;&#21542;&#36824;&#33021;&#23454;&#29616;&#26368;&#22823;&#21270;&#21457;&#29616;&#30340;&#27425;&#35201;&#30446;&#26631;&#20173;&#19981;&#30830;&#23450;&#12290;&#20107;&#23454;&#19978;&#65292;&#35774;&#35745;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;FDR&#25511;&#21046;&#24182;&#21457;&#29616;&#26356;&#22810;&#30456;&#20851;&#21464;&#37327;&#30340;&#31243;&#24207;&#65292;&#22312;&#21487;&#33021;&#26159;&#26368;&#31616;&#21333;&#30340;&#32447;&#24615;&#27169;&#22411;&#20013;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#22522;&#26412;&#24320;&#25918;&#38382;&#39064;&#12290;&#26412;&#25991;&#38024;&#23545;&#39640;&#32500;&#36125;&#21494;&#26031;&#32447;&#24615;&#27169;&#22411;&#19982;&#21508;&#21521;&#21516;&#24615;&#21327;&#21464;&#37327;&#65292;&#24320;&#21457;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#22810;&#37325;&#26816;&#39564;&#31243;&#24207;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#33021;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#20445;&#35777;&#39057;&#29575;FDR&#25511;&#21046;&#30340;&#27169;&#22411;-X&#31243;&#24207;&#65292;&#21363;&#20351;&#27169;&#22411;&#34987;&#38169;&#35823;&#25351;&#23450;&#65292;&#20063;&#33021;&#36798;&#21040;&#36817;&#20284;&#26368;&#20248;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In high dimensional variable selection problems, statisticians often seek to design multiple testing procedures that control the False Discovery Rate (FDR), while concurrently identifying a greater number of relevant variables. Model-X methods, such as Knockoffs and conditional randomization tests, achieve the primary goal of finite-sample FDR control, assuming a known distribution of covariates. However, whether these methods can also achieve the secondary goal of maximizing discoveries remains uncertain. In fact, designing procedures to discover more relevant variables with finite-sample FDR control is a largely open question, even within the arguably simplest linear models.  In this paper, we develop near-optimal multiple testing procedures for high dimensional Bayesian linear models with isotropic covariates. We introduce Model-X procedures that provably control the frequentist FDR from finite samples, even when the model is misspecified, and conjecturally achieve near-optimal powe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#24322;&#26041;&#24046;&#20998;&#24067;&#19978;&#36827;&#34892;&#31070;&#32463;&#20027;&#21160;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#22833;&#36133;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24494;&#35843;&#26469;&#20943;&#36731;&#36825;&#31181;&#22833;&#36133;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.00928</link><description>&lt;p&gt;
&#24322;&#26041;&#24046;&#20998;&#24067;&#19978;&#30340;&#31070;&#32463;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neural Active Learning on Heteroskedastic Distributions. (arXiv:2211.00928v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00928
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#24322;&#26041;&#24046;&#20998;&#24067;&#19978;&#36827;&#34892;&#31070;&#32463;&#20027;&#21160;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#22833;&#36133;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24494;&#35843;&#26469;&#20943;&#36731;&#36825;&#31181;&#22833;&#36133;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#20027;&#21160;&#23547;&#25214;&#26368;&#20339;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#27169;&#22411;&#25215;&#35834;&#30528;&#26356;&#20934;&#30830;&#12289;&#36866;&#24212;&#24615;&#24378;&#21644;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#36890;&#24120;&#20542;&#21521;&#20110;&#36873;&#25321;&#26368;&#38590;&#20998;&#31867;&#30340;&#20363;&#23376;&#12290;&#23613;&#31649;&#36825;&#22312;&#21516;&#36136;&#25968;&#25454;&#38598;&#19978;&#25928;&#26524;&#33391;&#22909;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#22312;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#26631;&#31614;&#22122;&#22768;&#25110;&#24322;&#26041;&#24046;&#24615;&#30340;&#20998;&#24067;&#19978;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#21487;&#33021;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#22833;&#36133;&#12290;&#36825;&#20123;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#24378;&#28872;&#20542;&#21521;&#20110;&#20174;&#22122;&#22768;&#26356;&#22823;&#30340;&#20998;&#24067;&#20013;&#36873;&#25321;&#65292;&#21363;&#20351;&#36825;&#20123;&#20363;&#23376;&#27809;&#26377;&#20449;&#24687;&#32467;&#26500;&#65288;&#20363;&#22914;&#20855;&#26377;&#38543;&#26426;&#26631;&#31614;&#30340;&#32431;&#33394;&#22270;&#20687;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#22312;&#24322;&#26041;&#24046;&#20998;&#24067;&#19978;&#30340;&#28798;&#38590;&#24615;&#22833;&#36133;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#35843;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#20123;&#22833;&#36133;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#24341;&#20837;&#20102;&#27169;&#22411;&#24046;&#24322;&#35780;&#20998;&#20989;&#25968;&#65292;&#29992;&#20110;&#21435;&#38500;&#22122;&#22768;&#20363;&#23376;&#24182;&#37319;&#26679;&#28165;&#26224;&#30340;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models that can actively seek out the best quality training data hold the promise of more accurate, adaptable, and efficient machine learning. Active learning techniques often tend to prefer examples that are the most difficult to classify. While this works well on homogeneous datasets, we find that it can lead to catastrophic failures when performed on multiple distributions with different degrees of label noise or heteroskedasticity. These active learning algorithms strongly prefer to draw from the distribution with more noise, even if their examples have no informative structure (such as solid color images with random labels). To this end, we demonstrate the catastrophic failure of these active learning algorithms on heteroskedastic distributions and propose a fine-tuning-based approach to mitigate these failures. Further, we propose a new algorithm that incorporates a model difference scoring function for each data point to filter out the noisy examples and sample clean examples th
&lt;/p&gt;</description></item><item><title>&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#31639;&#27861;&#31867;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#31232;&#32570;&#39640;&#32500;&#25968;&#25454;&#30340;&#20219;&#24847;&#30446;&#26631;&#20998;&#24067;&#24182;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25968;&#25454;&#25972;&#21512;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.17230</link><description>&lt;p&gt;
Lipschitz&#27491;&#21017;&#21270;&#26799;&#24230;&#27969;&#21644;&#39640;&#32500;&#31232;&#32570;&#25968;&#25454;&#30340;&#29983;&#25104;&#31890;&#23376;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Lipschitz-regularized gradient flows and generative particle algorithms for high-dimensional scarce data. (arXiv:2210.17230v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17230
&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#31639;&#27861;&#31867;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#31232;&#32570;&#39640;&#32500;&#25968;&#25454;&#30340;&#20219;&#24847;&#30446;&#26631;&#20998;&#24067;&#24182;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25968;&#25454;&#25972;&#21512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#31639;&#27861;&#31867;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#21487;&#33021;&#31232;&#32570;&#12289;&#39640;&#32500;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#20219;&#24847;&#30446;&#26631;&#20998;&#24067;&#65292;&#24182;&#29983;&#25104;&#26032;&#30340;&#26679;&#26412;&#12290;&#36825;&#20123;&#29983;&#25104;&#31639;&#27861;&#26159;&#22522;&#20110;&#31890;&#23376;&#30340;&#65292;&#24182;&#19988;&#26159;&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;Kullback-Leibler&#25110;&#20854;&#20182;f-&#25955;&#24230;&#30340;&#26799;&#24230;&#27969;&#26469;&#26500;&#36896;&#30340;&#65292;&#20854;&#20013;&#26469;&#33258;&#28304;&#20998;&#24067;&#30340;&#25968;&#25454;&#21487;&#20197;&#31283;&#23450;&#22320;&#20316;&#20026;&#31890;&#23376;&#20256;&#36755;&#21040;&#30446;&#26631;&#20998;&#24067;&#30340;&#38468;&#36817;&#12290;&#20316;&#20026;&#25968;&#25454;&#25972;&#21512;&#30340;&#19968;&#20010;&#31361;&#20986;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#27491;&#30830;&#20256;&#36755;&#32500;&#25968;&#36229;&#36807;54K&#30340;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#28857;&#65292;&#32780;&#26679;&#26412;&#37327;&#36890;&#24120;&#21482;&#26377;&#20960;&#30334;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
We build a new class of generative algorithms capable of efficiently learning an arbitrary target distribution from possibly scarce, high-dimensional data and subsequently generate new samples. These generative algorithms are particle-based and are constructed as gradient flows of Lipschitz-regularized Kullback-Leibler or other $f$-divergences, where data from a source distribution can be stably transported as particles, towards the vicinity of the target distribution. As a highlighted result in data integration, we demonstrate that the proposed algorithms correctly transport gene expression data points with dimension exceeding 54K, while the sample size is typically only in the hundreds.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20351;&#29992;&#20449;&#24687;&#20960;&#20309;&#25216;&#26415;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#28145;&#24230;&#32593;&#32476;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#65292;&#21457;&#29616;&#20219;&#21153;&#31354;&#38388;&#30340;&#32467;&#26500;&#19982;Wordnet&#31995;&#32479;&#36827;&#21270;&#26641;&#30340;&#26576;&#20123;&#37096;&#20998;&#19968;&#33268;&#65292;&#24182;&#19988;&#30417;&#30563;&#23398;&#20064;&#22312;&#19968;&#20010;&#20219;&#21153;&#19978;&#30340;&#36827;&#23637;&#21487;&#20197;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#20135;&#29983;&#19968;&#23450;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2210.17011</link><description>&lt;p&gt;
&#20856;&#22411;&#21487;&#23398;&#20064;&#20219;&#21153;&#31354;&#38388;&#30340;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
A picture of the space of typical learnable tasks. (arXiv:2210.17011v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17011
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#20449;&#24687;&#20960;&#20309;&#25216;&#26415;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#28145;&#24230;&#32593;&#32476;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#65292;&#21457;&#29616;&#20219;&#21153;&#31354;&#38388;&#30340;&#32467;&#26500;&#19982;Wordnet&#31995;&#32479;&#36827;&#21270;&#26641;&#30340;&#26576;&#20123;&#37096;&#20998;&#19968;&#33268;&#65292;&#24182;&#19988;&#30417;&#30563;&#23398;&#20064;&#22312;&#19968;&#20010;&#20219;&#21153;&#19978;&#30340;&#36827;&#23637;&#21487;&#20197;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#20135;&#29983;&#19968;&#23450;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#20449;&#24687;&#20960;&#20309;&#25216;&#26415;&#26469;&#29702;&#35299;&#28145;&#24230;&#32593;&#32476;&#22312;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#12289;&#20803;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23398;&#21040;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#19982;&#20219;&#21153;&#31354;&#38388;&#32467;&#26500;&#30456;&#20851;&#30340;&#20197;&#19979;&#29616;&#35937;&#65306;(1)&#20351;&#29992;&#19981;&#21516;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27010;&#29575;&#27169;&#22411;&#27969;&#24418;&#23454;&#38469;&#19978;&#26159;&#20302;&#32500;&#30340;&#65307;(2)&#22312;&#19968;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#21363;&#20351;&#22312;&#34920;&#38754;&#19978;&#30475;&#36215;&#26469;&#26159;&#19981;&#30456;&#20284;&#30340;&#20219;&#21153;&#19978;&#20063;&#33021;&#21462;&#24471;&#20986;&#20046;&#24847;&#26009;&#30340;&#36827;&#23637;&#65307;&#22914;&#26524;&#35757;&#32451;&#20219;&#21153;&#20855;&#26377;&#22810;&#26679;&#30340;&#31867;&#21035;&#65292;&#21017;&#20854;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#30340;&#36827;&#23637;&#26356;&#22823;&#65307;(3)&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#25152;&#25351;&#31034;&#30340;&#20219;&#21153;&#31354;&#38388;&#32467;&#26500;&#19982;Wordnet&#31995;&#32479;&#36827;&#21270;&#26641;&#20013;&#30340;&#26576;&#20123;&#37096;&#20998;&#19968;&#33268;&#65307;(4)&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#24773;&#22659;&#20803;&#23398;&#20064;&#31639;&#27861;&#21644;&#30417;&#30563;&#23398;&#20064;&#36981;&#24490;&#19981;&#21516;&#30340;&#36712;&#36857;&#65292;&#20294;&#26368;&#32456;&#36866;&#24212;&#30456;&#20284;&#30340;&#27169;&#22411;&#65307;(5)&#23545;&#27604;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36981;&#24490;&#31867;&#20284;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop information geometric techniques to understand the representations learned by deep networks when they are trained on different tasks using supervised, meta-, semi-supervised and contrastive learning. We shed light on the following phenomena that relate to the structure of the space of tasks: (1) the manifold of probabilistic models trained on different tasks using different representation learning methods is effectively low-dimensional; (2) supervised learning on one task results in a surprising amount of progress even on seemingly dissimilar tasks; progress on other tasks is larger if the training task has diverse classes; (3) the structure of the space of tasks indicated by our analysis is consistent with parts of the Wordnet phylogenetic tree; (4) episodic meta-learning algorithms and supervised learning traverse different trajectories during training but they fit similar models eventually; (5) contrastive and semi-supervised learning methods traverse trajectories similar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35780;&#20998;&#20989;&#25968;PaxNet&#65292;&#36890;&#36807;&#35757;&#32451;&#24050;&#35299;&#20915;&#30340;RNA 3D&#32467;&#26500;&#19978;&#30340;&#21407;&#23376;&#31867;&#22411;&#21644;&#22352;&#26631;&#26469;&#39044;&#27979;&#20934;&#30830;&#30340;&#32467;&#26500;&#27169;&#22411;&#12290;PaxNet&#27169;&#25311;&#20102;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21253;&#21547;&#27880;&#24847;&#21147;&#34701;&#21512;&#27169;&#22359;&#65292;&#23427;&#33021;&#23398;&#20064;&#27599;&#31181;&#30456;&#20114;&#20316;&#29992;&#31867;&#22411;&#30340;&#20010;&#20307;&#36129;&#29486;&#20197;&#36827;&#34892;&#26368;&#32456;&#39044;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;PaxNet&#22312;RNA 3D&#32467;&#26500;&#39044;&#27979;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.16392</link><description>&lt;p&gt;
&#29289;&#29702;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#31934;&#30830;&#30340;RNA 3D&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Physics-aware Graph Neural Network for Accurate RNA 3D Structure Prediction. (arXiv:2210.16392v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35780;&#20998;&#20989;&#25968;PaxNet&#65292;&#36890;&#36807;&#35757;&#32451;&#24050;&#35299;&#20915;&#30340;RNA 3D&#32467;&#26500;&#19978;&#30340;&#21407;&#23376;&#31867;&#22411;&#21644;&#22352;&#26631;&#26469;&#39044;&#27979;&#20934;&#30830;&#30340;&#32467;&#26500;&#27169;&#22411;&#12290;PaxNet&#27169;&#25311;&#20102;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21253;&#21547;&#27880;&#24847;&#21147;&#34701;&#21512;&#27169;&#22359;&#65292;&#23427;&#33021;&#23398;&#20064;&#27599;&#31181;&#30456;&#20114;&#20316;&#29992;&#31867;&#22411;&#30340;&#20010;&#20307;&#36129;&#29486;&#20197;&#36827;&#34892;&#26368;&#32456;&#39044;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;PaxNet&#22312;RNA 3D&#32467;&#26500;&#39044;&#27979;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
RNA&#30340;&#29983;&#29289;&#21151;&#33021;&#30001;&#20854;&#19977;&#32500;&#65288;3D&#65289;&#32467;&#26500;&#20915;&#23450;&#12290;&#22240;&#27492;&#65292;&#37492;&#20110;&#23454;&#39564;&#30830;&#23450;&#30340;RNA&#32467;&#26500;&#26377;&#38480;&#65292;&#39044;&#27979;RNA&#32467;&#26500;&#23558;&#26377;&#21161;&#20110;&#38416;&#26126;RNA&#21151;&#33021;&#21644;RNA&#38774;&#21521;&#33647;&#29289;&#21457;&#29616;&#65292;&#20294;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#20165;&#36890;&#36807;&#24050;&#35299;&#20915;&#30340;RNA 3D&#32467;&#26500;&#19978;&#30340;&#21407;&#23376;&#31867;&#22411;&#21644;&#22352;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#21306;&#20998;&#20934;&#30830;&#30340;&#32467;&#26500;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#29289;&#29702;&#24863;&#30693;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;PaxNet&#65289;&#20998;&#21035;&#27169;&#25311;&#20102;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#65292;&#21463;&#20998;&#23376;&#21147;&#23398;&#21551;&#21457;&#12290;&#27492;&#22806;&#65292;PaxNet&#36824;&#21253;&#21547;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#34701;&#21512;&#27169;&#22359;&#65292;&#29992;&#20110;&#23398;&#20064;&#27599;&#31181;&#30456;&#20114;&#20316;&#29992;&#31867;&#22411;&#30340;&#20010;&#20307;&#36129;&#29486;&#65292;&#20197;&#36827;&#34892;&#26368;&#32456;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#23545;PaxNet&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#24182;&#23558;&#20854;&#19982;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;PaxNet&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biological functions of RNAs are determined by their three-dimensional (3D) structures. Thus, given the limited number of experimentally determined RNA structures, the prediction of RNA structures will facilitate elucidating RNA functions and RNA-targeted drug discovery, but remains a challenging task. In this work, we propose a Graph Neural Network (GNN)-based scoring function trained only with the atomic types and coordinates on limited solved RNA 3D structures for distinguishing accurate structural models. The proposed Physics-aware Multiplex Graph Neural Network (PaxNet) separately models the local and non-local interactions inspired by molecular mechanics. Furthermore, PaxNet contains an attention-based fusion module that learns the individual contribution of each interaction type for the final prediction. We rigorously evaluate the performance of PaxNet on two benchmarks and compare it with several state-of-the-art baselines. The results show that PaxNet significantly outperforms
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.14891</link><description>&lt;p&gt;
&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65288;BNSL&#65289;&#65289;&#65292;&#23427;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#35780;&#20272;&#25351;&#26631;&#38543;&#29992;&#20110;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#12289;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#19978;&#28216;&#24615;&#33021;&#21464;&#21270;&#32780;&#21464;&#21270;&#65289;&#23545;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25193;&#25955;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;AI&#23545;&#40784;&#12289;&#26426;&#22120;&#20154;&#12289;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#33976;&#39311;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#38750;&#32447;&#24615;&#26426;&#22120;&#20154;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#20013;&#20934;&#30830;&#25512;&#26029;&#27169;&#22411;&#21160;&#21147;&#23398;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#12290;</title><link>http://arxiv.org/abs/2210.12583</link><description>&lt;p&gt;
&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#31163;&#25955;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning of Discrete-Time Dynamics for Uncertainty-Aware Model Predictive Control. (arXiv:2210.12583v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#38750;&#32447;&#24615;&#26426;&#22120;&#20154;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#20013;&#20934;&#30830;&#25512;&#26029;&#27169;&#22411;&#21160;&#21147;&#23398;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#39537;&#21160;&#30340;&#25511;&#21046;&#38656;&#35201;&#23545;&#31995;&#32479;&#21160;&#21147;&#23398;&#36827;&#34892;&#20934;&#30830;&#24314;&#27169;&#65292;&#20197;&#20415;&#22312;&#22797;&#26434;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#31934;&#30830;&#19988;&#23433;&#20840;&#22320;&#25511;&#21046;&#26426;&#22120;&#20154;&#12290;&#27492;&#22806;&#65292;&#22312;&#25805;&#20316;&#26465;&#20214;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#24212;&#35813;&#19981;&#26029;&#35843;&#25972;&#20197;&#24357;&#34917;&#21160;&#21147;&#23398;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#20027;&#21160;&#24314;&#27169;&#38750;&#32447;&#24615;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#31163;&#32447;&#23398;&#20064;&#20197;&#24448;&#32463;&#39564;&#21644;&#22312;&#32447;&#23398;&#20064;&#24403;&#21069;&#26426;&#22120;&#20154;&#19982;&#26410;&#30693;&#29615;&#22659;&#30340;&#20132;&#20114;&#12290;&#36825;&#20004;&#20010;&#22240;&#32032;&#20351;&#24471;&#23398;&#20064;&#36807;&#31243;&#39640;&#25928;&#19988;&#33258;&#36866;&#24212;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#20013;&#20934;&#30830;&#25512;&#26029;&#27169;&#22411;&#21160;&#21147;&#23398;&#65292;&#21363;&#20351;&#22312;&#22823;&#22823;&#19981;&#21516;&#20110;&#35757;&#32451;&#20998;&#24067;&#30340;&#25805;&#20316;&#33539;&#22260;&#20869;&#20063;&#21487;&#34892;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23545;&#23398;&#20064;&#21040;&#30340;&#21160;&#21147;&#23398;&#30340;aleatoric&#65288;&#25968;&#25454;&#65289;&#19981;&#30830;&#23450;&#24615;&#21551;&#21457;&#24335;&#26465;&#20214;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#12290;&#35813;&#25511;&#21046;&#22120;&#21487;&#20197;&#20027;&#21160;&#36873;&#25321;&#26368;&#20248;&#30340;&#25511;&#21046;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based control requires an accurate model of the system dynamics for precisely and safely controlling the robot in complex and dynamic environments. Moreover, in the presence of variations in the operating conditions, the model should be continuously refined to compensate for dynamics changes. In this paper, we present a self-supervised learning approach that actively models the dynamics of nonlinear robotic systems. We combine offline learning from past experience and online learning from current robot interaction with the unknown environment. These two ingredients enable a highly sample-efficient and adaptive learning process, capable of accurately inferring model dynamics in real-time even in operating regimes that greatly differ from the training distribution. Moreover, we design an uncertainty-aware model predictive controller that is heuristically conditioned to the aleatoric (data) uncertainty of the learned dynamics. This controller actively chooses the optimal control act
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#21450;&#20854;&#23545;&#19979;&#28216;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#22312;&#20108;&#20803;&#24773;&#20917;&#19979;&#65292;&#19979;&#28216;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#26080;&#27861;&#24674;&#22797;&#34987;&#21024;&#38500;&#30340;&#27010;&#24565;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#22810;&#31867;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#38388;&#25509;&#24674;&#22797;&#27010;&#24565;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#32447;&#24615;&#21024;&#38500;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2210.10012</link><description>&lt;p&gt;
&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#21450;&#20854;&#24433;&#21709;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Log-linear Guardedness and its Implications. (arXiv:2210.10012v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#21450;&#20854;&#23545;&#19979;&#28216;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#22312;&#20108;&#20803;&#24773;&#20917;&#19979;&#65292;&#19979;&#28216;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#26080;&#27861;&#24674;&#22797;&#34987;&#21024;&#38500;&#30340;&#27010;&#24565;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#22810;&#31867;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#38388;&#25509;&#24674;&#22797;&#27010;&#24565;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#32447;&#24615;&#21024;&#38500;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#21457;&#29616;&#65292;&#22312;&#20551;&#35774;&#21487;&#32447;&#24615;&#30340;&#31070;&#32463;&#34920;&#31034;&#20013;&#65292;&#20174;&#20013;&#21024;&#38500;&#21487;&#20154;&#35299;&#37322;&#30340;&#27010;&#24565;&#30340;&#26041;&#27861;&#26159;&#21487;&#34892;&#21644;&#26377;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21024;&#38500;&#23545;&#20110;&#22522;&#20110;&#20462;&#25913;&#21518;&#34920;&#31034;&#36827;&#34892;&#35757;&#32451;&#30340;&#19979;&#28216;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#24433;&#21709;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#30340;&#27010;&#24565;&#65292;&#21363;&#23545;&#25163;&#26080;&#27861;&#30452;&#25509;&#20174;&#34920;&#31034;&#20013;&#39044;&#27979;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#24182;&#30740;&#31350;&#20854;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20108;&#20803;&#24773;&#20917;&#19979;&#65292;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#19979;&#28216;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#26080;&#27861;&#24674;&#22797;&#34987;&#21024;&#38500;&#30340;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#22810;&#31867;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#65292;&#38388;&#25509;&#24674;&#22797;&#27010;&#24565;&#65292;&#36825;&#25351;&#20986;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#20316;&#20026;&#19979;&#28216;&#20559;&#24046;&#32531;&#35299;&#25216;&#26415;&#30340;&#20869;&#22312;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#32447;&#24615;&#21024;&#38500;&#26041;&#27861;&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#21487;&#35299;&#37322;&#31070;&#32463;&#34920;&#31034;&#19982;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#32852;&#31995;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for erasing human-interpretable concepts from neural representations that assume linearity have been found to be tractable and useful. However, the impact of this removal on the behavior of downstream classifiers trained on the modified representations is not fully understood. In this work, we formally define the notion of log-linear guardedness as the inability of an adversary to predict the concept directly from the representation, and study its implications. We show that, in the binary case, under certain assumptions, a downstream log-linear model cannot recover the erased concept. However, we demonstrate that a multiclass log-linear model \emph{can} be constructed that indirectly recovers the concept in some cases, pointing to the inherent limitations of log-linear guardedness as a downstream bias mitigation technique. These findings shed light on the theoretical limitations of linear erasure methods and highlight the need for further research on the connections between int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20004;&#20010;&#26080;&#31351;&#32500;Sobolev&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20043;&#38388;&#30340;Hilbert-Schmidt&#31639;&#23376;&#30340;&#32479;&#35745;&#26497;&#38480;&#65292;&#22312;&#22810;&#23618;&#32423;&#35757;&#32451;&#26041;&#27861;&#19979;&#65292;&#36890;&#36807;&#23398;&#20064;&#20559;&#24046;&#20197;&#19979;&#30340;&#35889;&#20998;&#37327;&#21644;&#24573;&#30053;&#26041;&#24046;&#20197;&#19978;&#30340;&#20998;&#37327;&#65292;&#21487;&#20197;&#36798;&#21040;&#26368;&#20248;&#30340;&#23398;&#20064;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2209.14430</link><description>&lt;p&gt;
&#26368;&#23567;&#21270;&#25439;&#22833;&#30340;&#22810;&#23618;&#35757;&#32451;&#26041;&#27861;&#19979;&#30340;&#26368;&#20248;&#26680;&#31639;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Minimax Optimal Kernel Operator Learning via Multilevel Training. (arXiv:2209.14430v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20004;&#20010;&#26080;&#31351;&#32500;Sobolev&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20043;&#38388;&#30340;Hilbert-Schmidt&#31639;&#23376;&#30340;&#32479;&#35745;&#26497;&#38480;&#65292;&#22312;&#22810;&#23618;&#32423;&#35757;&#32451;&#26041;&#27861;&#19979;&#65292;&#36890;&#36807;&#23398;&#20064;&#20559;&#24046;&#20197;&#19979;&#30340;&#35889;&#20998;&#37327;&#21644;&#24573;&#30053;&#26041;&#24046;&#20197;&#19978;&#30340;&#20998;&#37327;&#65292;&#21487;&#20197;&#36798;&#21040;&#26368;&#20248;&#30340;&#23398;&#20064;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#31351;&#32500;&#20989;&#25968;&#31354;&#38388;&#20013;&#23398;&#20064;&#26144;&#23556;&#24050;&#32463;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#35768;&#22810;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#32463;&#39564;&#19978;&#30340;&#25104;&#21151;&#65292;&#21253;&#25324;&#29983;&#25104;&#27169;&#22411;&#12289;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#12289;&#22240;&#26524;&#25512;&#26029;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20004;&#20010;&#26080;&#31351;&#32500;Sobolev&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20043;&#38388;&#30340;Hilbert-Schmidt&#31639;&#23376;&#30340;&#32479;&#35745;&#26497;&#38480;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20351;&#29992;Sobolev Hilbert-Schmidt&#33539;&#25968;&#30340;&#20449;&#24687;&#29702;&#35770;&#19979;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#35268;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20559;&#24046;&#36718;&#24275;&#20197;&#19979;&#30340;&#35889;&#20998;&#37327;&#24182;&#24573;&#30053;&#26041;&#24046;&#36718;&#24275;&#20197;&#19978;&#30340;&#20998;&#37327;&#65292;&#21487;&#20197;&#36798;&#21040;&#26368;&#20248;&#30340;&#23398;&#20064;&#36895;&#29575;&#12290;&#21516;&#26102;&#65292;&#20559;&#24046;&#21644;&#26041;&#24046;&#36718;&#24275;&#20043;&#38388;&#30340;&#35889;&#20998;&#37327;&#32473;&#25105;&#20204;&#22312;&#35774;&#35745;&#35745;&#31639;&#19978;&#21487;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26102;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#23618;&#32423;&#30340;&#26680;&#31639;&#23376;&#23398;&#20064;&#31639;&#27861;&#65292;&#24403;&#23398;&#20064;&#32447;&#24615;&#31639;&#23376;&#26102;&#23427;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning mappings between infinite-dimensional function spaces has achieved empirical success in many disciplines of machine learning, including generative modeling, functional data analysis, causal inference, and multi-agent reinforcement learning. In this paper, we study the statistical limit of learning a Hilbert-Schmidt operator between two infinite-dimensional Sobolev reproducing kernel Hilbert spaces. We establish the information-theoretic lower bound in terms of the Sobolev Hilbert-Schmidt norm and show that a regularization that learns the spectral components below the bias contour and ignores the ones that are above the variance contour can achieve the optimal learning rate. At the same time, the spectral components between the bias and variance contours give us flexibility in designing computationally feasible machine learning algorithms. Based on this observation, we develop a multilevel kernel operator learning algorithm that is optimal when learning linear operators betwee
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Pareto Actor-Critic&#65288;Pareto-AC&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22343;&#34913;&#36873;&#25321;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#26080;&#20914;&#31361;&#28216;&#25103;&#30340;&#24615;&#36136;&#65292;&#21363;Pareto&#26368;&#20248;&#22343;&#34913;&#26368;&#22823;&#21270;&#20102;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#22238;&#25253;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;Pareto-AC&#30456;&#27604;&#20854;&#20182;&#19971;&#31181;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#26356;&#33021;&#25910;&#25947;&#21040;&#26356;&#39640;&#30340;&#22238;&#21512;&#22238;&#25253;&#12290;</title><link>http://arxiv.org/abs/2209.14344</link><description>&lt;p&gt;
Pareto Actor-Critic&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22343;&#34913;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Pareto Actor-Critic for Equilibrium Selection in Multi-Agent Reinforcement Learning. (arXiv:2209.14344v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Pareto Actor-Critic&#65288;Pareto-AC&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22343;&#34913;&#36873;&#25321;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#26080;&#20914;&#31361;&#28216;&#25103;&#30340;&#24615;&#36136;&#65292;&#21363;Pareto&#26368;&#20248;&#22343;&#34913;&#26368;&#22823;&#21270;&#20102;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#22238;&#25253;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;Pareto-AC&#30456;&#27604;&#20854;&#20182;&#19971;&#31181;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#26356;&#33021;&#25910;&#25947;&#21040;&#26356;&#39640;&#30340;&#22238;&#21512;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#22312;&#26080;&#20914;&#31361;&#22810;&#26234;&#33021;&#20307;&#21338;&#24328;&#20013;&#30340;&#22343;&#34913;&#36873;&#25321;&#38382;&#39064;&#65292;&#20855;&#20307;&#30740;&#31350;&#20102;&#22312;&#22810;&#20010;&#29616;&#26377;&#22343;&#34913;&#20013;&#36873;&#25321;Pareto&#26368;&#20248;&#22343;&#34913;&#30340;&#38382;&#39064;&#12290;&#24050;&#32463;&#34920;&#26126;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30001;&#20110;&#27599;&#20010;&#26234;&#33021;&#20307;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#25919;&#31574;&#30340;&#19981;&#30830;&#23450;&#24615;&#32780;&#23481;&#26131;&#25910;&#25947;&#21040;Pareto&#25903;&#37197;&#30340;&#22343;&#34913;&#29366;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#27425;&#20248;&#22343;&#34913;&#36873;&#25321;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Pareto Actor-Critic&#65288;Pareto-AC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#26080;&#20914;&#31361;&#28216;&#25103;&#65288;&#21512;&#20316;&#28216;&#25103;&#30340;&#36229;&#38598;&#65289;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#20854;&#21033;&#29992;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#24615;&#36136;&#65306;&#26080;&#20914;&#31361;&#28216;&#25103;&#20013;&#30340;Pareto&#26368;&#20248;&#22343;&#34913;&#26368;&#22823;&#21270;&#20102;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#22238;&#25253;&#65292;&#22240;&#27492;&#23545;&#20110;&#25152;&#26377;&#26234;&#33021;&#20307;&#26469;&#35828;&#26159;&#39318;&#36873;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22810;&#26234;&#33021;&#20307;&#21338;&#24328;&#20013;&#35780;&#20272;&#20102;Pareto-AC&#65292;&#24182;&#26174;&#31034;&#23427;&#25910;&#25947;&#21040;&#26356;&#39640;&#30340;&#22238;&#21512;&#22238;&#25253;&#65292;&#19982;&#19971;&#31181;&#26368;&#20808;&#36827;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30456;&#27604;&#65292;Pareto-AC&#25104;&#21151;&#22320;&#25910;&#25947;&#21040;&#20102;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
This work focuses on equilibrium selection in no-conflict multi-agent games, where we specifically study the problem of selecting a Pareto-optimal equilibrium among several existing equilibria. It has been shown that many state-of-the-art multi-agent reinforcement learning (MARL) algorithms are prone to converging to Pareto-dominated equilibria due to the uncertainty each agent has about the policy of the other agents during training. To address sub-optimal equilibrium selection, we propose Pareto Actor-Critic (Pareto-AC), which is an actor-critic algorithm that utilises a simple property of no-conflict games (a superset of cooperative games): the Pareto-optimal equilibrium in a no-conflict game maximises the returns of all agents and therefore is the preferred outcome for all agents. We evaluate Pareto-AC in a diverse set of multi-agent games and show that it converges to higher episodic returns compared to seven state-of-the-art MARL algorithms and that it successfully converges to a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33016;&#37096;X&#23556;&#32447;&#21311;&#21517;&#21270;&#26041;&#27861;&#65288;PriCheXy-Net&#65289;&#65292;&#21487;&#20197;&#26377;&#38024;&#23545;&#24615;&#22320;&#21435;&#38500;&#29983;&#29289;&#29305;&#24449;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#25968;&#25454;&#30340;&#35786;&#26029;&#21644;&#26426;&#22120;&#23398;&#20064;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2209.11531</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#33016;&#37096;X&#23556;&#32447;&#21311;&#21517;&#21270;&#65306;&#32500;&#25252;&#24739;&#32773;&#38544;&#31169;&#30340;&#25928;&#29992;&#20445;&#25252;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-based Anonymization of Chest Radiographs: A Utility-preserving Measure for Patient Privacy. (arXiv:2209.11531v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11531
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33016;&#37096;X&#23556;&#32447;&#21311;&#21517;&#21270;&#26041;&#27861;&#65288;PriCheXy-Net&#65289;&#65292;&#21487;&#20197;&#26377;&#38024;&#23545;&#24615;&#22320;&#21435;&#38500;&#29983;&#29289;&#29305;&#24449;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#25968;&#25454;&#30340;&#35786;&#26029;&#21644;&#26426;&#22120;&#23398;&#20064;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21457;&#24067;&#29992;&#20110;&#30740;&#31350;&#30446;&#30340;&#30340;&#22823;&#22411;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#38598;&#20043;&#21069;&#65292;&#24378;&#22823;&#19988;&#21487;&#38752;&#30340;&#33016;&#37096;X&#23556;&#32447;&#21311;&#21517;&#21270;&#26159;&#19968;&#20010;&#24517;&#35201;&#30340;&#27493;&#39588;&#12290;&#20256;&#32479;&#30340;&#21311;&#21517;&#21270;&#36807;&#31243;&#26159;&#36890;&#36807;&#22312;&#22270;&#20687;&#20013;&#27169;&#31946;&#20010;&#20154;&#20449;&#24687;&#24182;&#21024;&#38500;&#25110;&#26367;&#25442;&#20803;&#20449;&#24687;&#26469;&#23436;&#25104;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31616;&#21333;&#30340;&#25514;&#26045;&#20173;&#28982;&#20445;&#30041;&#20102;&#33016;&#37096;X&#23556;&#32447;&#20013;&#30340;&#29983;&#29289;&#29305;&#24449;&#20449;&#24687;&#65292;&#20351;&#24471;&#24739;&#32773;&#21487;&#20197;&#36890;&#36807;&#20851;&#32852;&#25915;&#20987;&#36827;&#34892;&#37325;&#26032;&#35782;&#21035;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#28151;&#28102;&#22270;&#20687;&#20013;&#20986;&#29616;&#30340;&#29983;&#29289;&#29305;&#24449;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;PriCheXy-Net&#65289;&#65292;&#21487;&#20197;&#26377;&#38024;&#23545;&#24615;&#22320;&#23545;&#33016;&#37096;X&#23556;&#32447;&#36827;&#34892;&#21311;&#21517;&#21270;&#65292;&#21516;&#26102;&#20445;&#25345;&#35786;&#26029;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#25928;&#29992;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26550;&#26500;&#30001;&#19977;&#20010;&#29420;&#31435;&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#65292;&#24403;&#38598;&#20307;&#20351;&#29992;&#26102;&#65292;&#21487;&#20197;&#23398;&#20064;&#19968;&#20010;&#21464;&#24418;&#22330;&#65292;&#20197;&#38450;&#27490;&#24739;&#32773;&#37325;&#26032;&#35782;&#21035;&#12290;&#22312;ChestX-ray14&#25968;&#25454;&#38598;&#19978;&#30340;&#23450;&#37327;&#32467;&#26524;&#26174;&#31034;&#20102;&#19968;&#23450;&#30340;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust and reliable anonymization of chest radiographs constitutes an essential step before publishing large datasets of such for research purposes. The conventional anonymization process is carried out by obscuring personal information in the images with black boxes and removing or replacing meta-information. However, such simple measures retain biometric information in the chest radiographs, allowing patients to be re-identified by a linkage attack. Therefore, there is an urgent need to obfuscate the biometric information appearing in the images. We propose the first deep learning-based approach (PriCheXy-Net) to targetedly anonymize chest radiographs while maintaining data utility for diagnostic and machine learning purposes. Our model architecture is a composition of three independent neural networks that, when collectively used, allow for learning a deformation field that is able to impede patient re-identification. Quantitative results on the ChestX-ray14 dataset show a reduction
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26412;&#25991;&#38024;&#23545;&#22270;&#24322;&#24120;&#26816;&#27979;&#20013;&#23384;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#26032;&#39062;&#30340;&#30740;&#31350;&#38382;&#39064;&#65306;&#24191;&#20041;&#22270;&#24322;&#24120;&#26816;&#27979;&#65292;&#26088;&#22312;&#21516;&#26102;&#26377;&#25928;&#35782;&#21035;&#35757;&#32451;&#39046;&#22495;&#22270;&#21644;&#26410;&#35757;&#32451;&#39046;&#22495;&#22270;&#19978;&#30340;&#24322;&#24120;&#12290;</title><link>http://arxiv.org/abs/2209.10168</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25552;&#39640;&#22270;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Generalizability of Graph Anomaly Detection Models via Data Augmentation. (arXiv:2209.10168v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10168
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26412;&#25991;&#38024;&#23545;&#22270;&#24322;&#24120;&#26816;&#27979;&#20013;&#23384;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#26032;&#39062;&#30340;&#30740;&#31350;&#38382;&#39064;&#65306;&#24191;&#20041;&#22270;&#24322;&#24120;&#26816;&#27979;&#65292;&#26088;&#22312;&#21516;&#26102;&#26377;&#25928;&#35782;&#21035;&#35757;&#32451;&#39046;&#22495;&#22270;&#21644;&#26410;&#35757;&#32451;&#39046;&#22495;&#22270;&#19978;&#30340;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#21363;&#20351;&#23569;&#25968;&#24322;&#24120;&#37117;&#21487;&#33021;&#23545;&#33391;&#24615;&#29992;&#25143;&#36896;&#25104;&#24040;&#22823;&#23041;&#32961;&#12290;&#26368;&#36817;&#30340;&#21322;&#30417;&#30563;GAD&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#21487;&#29992;&#30340;&#26631;&#31614;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#27604;&#26080;&#30417;&#30563;&#26041;&#27861;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#20154;&#20204;&#36890;&#24120;&#38656;&#35201;&#22312;&#26032;&#30340;&#65288;&#23376;&#65289;&#22270;&#19978;&#35782;&#21035;&#24322;&#24120;&#20197;&#30830;&#20445;&#19994;&#21153;&#23433;&#20840;&#65292;&#20294;&#20182;&#20204;&#21487;&#33021;&#32570;&#20047;&#26631;&#31614;&#26469;&#35757;&#32451;&#26377;&#25928;&#30340;&#26816;&#27979;&#27169;&#22411;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#24819;&#27861;&#26159;&#30452;&#25509;&#23558;&#35757;&#32451;&#22909;&#30340;GAD&#27169;&#22411;&#24212;&#29992;&#20110;&#26032;&#30340;&#65288;&#23376;&#65289;&#22270;&#36827;&#34892;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;GAD&#26041;&#27861;&#23384;&#22312;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#65292;&#21363;&#35757;&#32451;&#33391;&#22909;&#30340;&#27169;&#22411;&#22312;&#21516;&#19968;&#22270;&#30340;&#26410;&#35757;&#32451;&#21306;&#22495;&#65288;&#21363;&#35757;&#32451;&#20013;&#26080;&#27861;&#35775;&#38382;&#30340;&#21306;&#22495;&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#21487;&#33021;&#20250;&#24102;&#26469;&#24456;&#22823;&#30340;&#40635;&#28902;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#36825;&#19968;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#26032;&#39062;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#21363;&#24191;&#20041;&#22270;&#24322;&#24120;&#26816;&#27979;&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#35782;&#21035;&#35757;&#32451;&#39046;&#22495;&#22270;&#21644;&#26410;&#35757;&#32451;&#39046;&#22495;&#22270;&#19978;&#30340;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph anomaly detection (GAD) is a vital task since even a few anomalies can pose huge threats to benign users. Recent semi-supervised GAD methods, which can effectively leverage the available labels as prior knowledge, have achieved superior performances than unsupervised methods. In practice, people usually need to identify anomalies on new (sub)graphs to secure their business, but they may lack labels to train an effective detection model. One natural idea is to directly adopt a trained GAD model to the new (sub)graph for testing. However, we find that existing semi-supervised GAD methods suffer from poor generalization issue, i.e., well-trained models could not perform well on an unseen area (i.e., not accessible in training) of the same graph. It may cause great troubles. In this paper, we base on the phenomenon and propose a general and novel research problem of generalized graph anomaly detection that aims to effectively identify anomalies on both the training-domain graph and u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38024;&#23545;Twenty20&#26495;&#29699;&#27604;&#36187;&#30340;&#32467;&#26524;&#36827;&#34892;&#39044;&#27979;&#65292;&#36890;&#36807;&#32771;&#34385;&#29699;&#21592;&#30340;&#21382;&#21490;&#34920;&#29616;&#32479;&#35745;&#12289;&#35780;&#32423;&#21644;&#32858;&#31867;&#31561;&#22240;&#32032;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ELO&#26041;&#27861;&#30340;&#29699;&#21592;&#35780;&#32423;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2209.06346</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#27979;Twenty20&#26495;&#29699;&#27604;&#36187;&#32467;&#26524;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prediction of the outcome of a Twenty-20 Cricket Match : A Machine Learning Approach. (arXiv:2209.06346v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38024;&#23545;Twenty20&#26495;&#29699;&#27604;&#36187;&#30340;&#32467;&#26524;&#36827;&#34892;&#39044;&#27979;&#65292;&#36890;&#36807;&#32771;&#34385;&#29699;&#21592;&#30340;&#21382;&#21490;&#34920;&#29616;&#32479;&#35745;&#12289;&#35780;&#32423;&#21644;&#32858;&#31867;&#31561;&#22240;&#32032;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ELO&#26041;&#27861;&#30340;&#29699;&#21592;&#35780;&#32423;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Twenty20&#26495;&#29699;&#26159;&#26495;&#29699;&#30340;&#19968;&#31181;&#30701;&#24335;&#24418;&#24335;&#65292;&#27599;&#20010;&#38431;&#20237;&#26377;&#19968;&#20010;&#23616;&#65292;&#23616;&#38480;&#20110;&#26368;&#22810;20&#20010;&#22238;&#21512;&#12290;&#26412;&#25991;&#23581;&#35797;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;Twenty20&#26495;&#29699;&#27604;&#36187;&#30340;&#32467;&#26524;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#21442;&#19982;&#31454;&#20105;&#38431;&#20237;&#30340;&#29699;&#21592;&#30340;&#20197;&#24448;&#34920;&#29616;&#32479;&#35745;&#12289;&#20174;&#33879;&#21517;&#26495;&#29699;&#32479;&#35745;&#32593;&#31449;&#33719;&#24471;&#30340;&#29699;&#21592;&#35780;&#32423;&#12289;&#23558;&#20855;&#26377;&#31867;&#20284;&#34920;&#29616;&#32479;&#35745;&#30340;&#29699;&#21592;&#36827;&#34892;&#20998;&#31867;&#20197;&#21450;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ELO&#26041;&#27861;&#30340;&#26032;&#39062;&#35780;&#32423;&#29699;&#21592;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21253;&#25324;&#36923;&#36753;&#22238;&#24402;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#36125;&#21494;&#26031;&#32593;&#32476;&#12289;&#20915;&#31574;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#65292;&#27604;&#36739;&#20102;&#27599;&#31181;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Twenty20 cricket, sometimes written Twenty-20, and often abbreviated to T20, is a short form of cricket. In a Twenty20 game the two teams of 11 players have a single innings each, which is restricted to a maximum of 20 overs. This version of cricket is especially unpredictable and is one of the reasons it has gained popularity over recent times. However, in this paper we try four different machine learning approaches for predicting the results of T20 Cricket Matches. Specifically we take in to account: previous performance statistics of the players involved in the competing teams, ratings of players obtained from reputed cricket statistics websites, clustering the players' with similar performance statistics and propose a novel method using an ELO based approach to rate players. We compare the performances of each of these feature engineering approaches by using different ML algorithms, including logistic regression, support vector machines, bayes network, decision tree, random forest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;&#20013;&#20351;&#29992;R\'{e}nyi&#25955;&#24230;&#65292;&#23427;&#33021;&#22815;&#22312;&#19981;&#24341;&#20837;&#22823;&#37327;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#65292;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#32467;&#26524;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2209.05732</link><description>&lt;p&gt;
R\'{e}nyi&#25955;&#24230;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
R\'{e}nyi Divergence Deep Mutual Learning. (arXiv:2209.05732v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;&#20013;&#20351;&#29992;R\'{e}nyi&#25955;&#24230;&#65292;&#23427;&#33021;&#22815;&#22312;&#19981;&#24341;&#20837;&#22823;&#37327;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#65292;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#32467;&#26524;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#23457;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35745;&#31639;&#33539;&#24335;&#8212;&#8212;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;&#65288;DML&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;R\'{e}nyi&#25955;&#24230;&#32780;&#19981;&#26159;KL&#25955;&#24230;&#65292;&#36825;&#31181;&#20570;&#27861;&#26356;&#21152;&#28789;&#27963;&#12289;&#21487;&#35843;&#65292;&#20197;&#25913;&#21892;vanilla DML&#12290;&#36825;&#31181;&#20462;&#25913;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#38468;&#21152;&#22797;&#26434;&#24615;&#19979;&#19981;&#26029;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#33539;&#20363;&#30340;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#19988;&#34920;&#26126;&#20855;&#26377;&#24658;&#23450;&#23398;&#20064;&#29575;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#38750;&#20984;&#20248;&#21270;&#20219;&#21153;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#25910;&#25947;&#30340;&#20559;&#24046;&#20026;$\mathcal{O}(1)$&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits Deep Mutual Learning (DML), a simple yet effective computing paradigm. We propose using R\'{e}nyi divergence instead of the KL divergence, which is more flexible and tunable, to improve vanilla DML. This modification is able to consistently improve performance over vanilla DML with limited additional complexity. The convergence properties of the proposed paradigm are analyzed theoretically, and Stochastic Gradient Descent with a constant learning rate is shown to converge with $\mathcal{O}(1)$-bias in the worst case scenario for nonconvex optimization tasks. That is, learning will reach nearby local optima but continue searching within a bounded scope, which may help mitigate overfitting. Finally, our extensive empirical results demonstrate the advantage of combining DML and R\'{e}nyi divergence, which further improves generalized models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#25972;&#20307;&#20998;&#21106;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#24050;&#30693;&#30340;&#29289;&#20307;&#21644;&#26410;&#30693;&#30340;&#29289;&#20307;&#36827;&#34892;&#20934;&#30830;&#20998;&#21106;&#65292;&#24182;&#22312;&#22788;&#29702;&#24050;&#30693;&#31867;&#21035;&#26102;&#23545;&#26410;&#30693;&#31867;&#21035;&#36827;&#34892;&#40065;&#26834;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2209.05407</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#20998;&#21106;&#24050;&#30693;&#29289;&#20307;&#21644;&#26410;&#30693;&#29289;&#20307;
&lt;/p&gt;
&lt;p&gt;
Segmenting Known Objects and Unseen Unknowns without Prior Knowledge. (arXiv:2209.05407v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#25972;&#20307;&#20998;&#21106;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#24050;&#30693;&#30340;&#29289;&#20307;&#21644;&#26410;&#30693;&#30340;&#29289;&#20307;&#36827;&#34892;&#20934;&#30830;&#20998;&#21106;&#65292;&#24182;&#22312;&#22788;&#29702;&#24050;&#30693;&#31867;&#21035;&#26102;&#23545;&#26410;&#30693;&#31867;&#21035;&#36827;&#34892;&#40065;&#26834;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#35270;&#22495;&#20998;&#21106;&#26041;&#27861;&#20250;&#26681;&#25454;&#36755;&#20837;&#23558;&#24050;&#30693;&#31867;&#21035;&#20998;&#37197;&#32473;&#27599;&#20010;&#20687;&#32032;&#12290;&#21363;&#20351;&#22312;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20013;&#65292;&#36825;&#24517;&#28982;&#20250;&#23548;&#33268;&#22312;&#35757;&#32451;&#31867;&#21035;&#20043;&#22806;&#30340;&#29289;&#20307;&#19978;&#20135;&#29983;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#65292;&#23545;&#20110;&#26410;&#30693;&#26679;&#26412;&#21644;&#26497;&#31471;&#24773;&#20917;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#36991;&#20813;&#21361;&#38505;&#21518;&#26524;&#12290;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#26080;&#27861;&#21253;&#21547;&#36275;&#22815;&#30340;&#25968;&#25454;&#28857;&#26469;&#20805;&#20998;&#37319;&#26679;&#24213;&#23618;&#20998;&#24067;&#30340;&#38271;&#23614;&#37096;&#20998;&#65292;&#27169;&#22411;&#24517;&#39035;&#33021;&#22815;&#22788;&#29702;&#26410;&#35265;&#36807;&#21644;&#26410;&#30693;&#30340;&#24773;&#20917;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#37325;&#26032;&#35782;&#21035;&#24050;&#30475;&#21040;&#30340;&#26410;&#26631;&#35760;&#23545;&#35937;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#23637;&#20998;&#21106;&#30340;&#24517;&#35201;&#27493;&#39588;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#25972;&#20307;&#20998;&#21106;&#12290;&#25972;&#20307;&#20998;&#21106;&#26088;&#22312;&#20174;&#26410;&#35265;&#36807;&#30340;&#26410;&#30693;&#31867;&#21035;&#20013;&#35782;&#21035;&#21644;&#20998;&#31163;&#23545;&#35937;&#23454;&#20363;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#65292;&#21516;&#26102;&#25191;&#34892;&#24050;&#30693;&#31867;&#21035;&#30340;&#27867;&#35270;&#22495;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Panoptic segmentation methods assign a known class to each pixel given in input. Even for state-of-the-art approaches, this inevitably enforces decisions that systematically lead to wrong predictions for objects outside the training categories. However, robustness against out-of-distribution samples and corner cases is crucial in safety-critical settings to avoid dangerous consequences. Since real-world datasets cannot contain enough data points to adequately sample the long tail of the underlying distribution, models must be able to deal with unseen and unknown scenarios as well. Previous methods targeted this by re-identifying already-seen unlabeled objects. In this work, we propose the necessary step to extend segmentation with a new setting which we term holistic segmentation. Holistic segmentation aims to identify and separate objects of unseen unknown categories into instances, without any prior knowledge about them, while performing panoptic segmentation of known classes. We tac
&lt;/p&gt;</description></item><item><title>EchoGNN&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#36229;&#22768;&#24515;&#21160;&#22270;&#35270;&#39057;&#20013;&#21487;&#35299;&#37322;&#22320;&#20272;&#35745;&#23556;&#34880;&#20998;&#25968;&#12290;&#23427;&#25512;&#26029;&#20986;&#19968;&#20010;&#28508;&#22312;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#65292;&#24182;&#36890;&#36807;&#33410;&#28857;&#21644;&#36793;&#30340;&#26435;&#37325;&#26469;&#30830;&#23450;&#24103;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#24110;&#21161;&#20272;&#35745;&#23556;&#34880;&#20998;&#25968;&#12290;&#35813;&#27169;&#22411;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#26041;&#38754;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.14003</link><description>&lt;p&gt;
EchoGNN: &#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#23556;&#34880;&#20998;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
EchoGNN: Explainable Ejection Fraction Estimation with Graph Neural Networks. (arXiv:2208.14003v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14003
&lt;/p&gt;
&lt;p&gt;
EchoGNN&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#36229;&#22768;&#24515;&#21160;&#22270;&#35270;&#39057;&#20013;&#21487;&#35299;&#37322;&#22320;&#20272;&#35745;&#23556;&#34880;&#20998;&#25968;&#12290;&#23427;&#25512;&#26029;&#20986;&#19968;&#20010;&#28508;&#22312;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#65292;&#24182;&#36890;&#36807;&#33410;&#28857;&#21644;&#36793;&#30340;&#26435;&#37325;&#26469;&#30830;&#23450;&#24103;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#24110;&#21161;&#20272;&#35745;&#23556;&#34880;&#20998;&#25968;&#12290;&#35813;&#27169;&#22411;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#26041;&#38754;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23556;&#34880;&#20998;&#25968;&#65288;EF&#65289;&#26159;&#24515;&#33039;&#21151;&#33021;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#21487;&#29992;&#20110;&#35782;&#21035;&#23481;&#26131;&#20986;&#29616;&#24515;&#21151;&#33021;&#38556;&#30861;&#65288;&#22914;&#24515;&#34928;&#65289;&#30340;&#24739;&#32773;&#12290;EF&#26159;&#36890;&#36807;&#25163;&#21160;&#36861;&#36394;&#24038;&#24515;&#23460;&#24182;&#20272;&#35745;&#29305;&#23450;&#24103;&#19978;&#20854;&#20307;&#31215;&#30340;&#24515;&#33039;&#36229;&#22768;&#35270;&#39057;&#65288;&#36229;&#22768;&#24515;&#21160;&#22270;&#65289;&#26469;&#20272;&#35745;&#30340;&#12290;&#30001;&#20110;&#25163;&#21160;&#36807;&#31243;&#21644;&#35270;&#39057;&#36136;&#37327;&#30340;&#24046;&#24322;&#65292;&#36825;&#20123;&#20272;&#35745;&#23384;&#22312;&#39640;&#24230;&#30340;&#35266;&#23519;&#32773;&#24046;&#24322;&#12290;&#36825;&#31181;&#19981;&#20934;&#30830;&#24615;&#21644;&#24555;&#36895;&#35780;&#20272;&#30340;&#38656;&#27714;&#38656;&#35201;&#21487;&#38752;&#19988;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EchoGNN&#65292;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#36229;&#22768;&#24515;&#21160;&#22270;&#35270;&#39057;&#20013;&#20272;&#35745;EF&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#39318;&#20808;&#20174;&#19968;&#20010;&#25110;&#22810;&#20010;&#36229;&#22768;&#24515;&#21160;&#22270;&#24207;&#21015;&#30340;&#24103;&#20013;&#25512;&#26029;&#20986;&#28508;&#22312;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#12290;&#28982;&#21518;&#65292;&#23427;&#20272;&#35745;&#36825;&#20010;&#22270;&#30340;&#33410;&#28857;&#21644;&#36793;&#30340;&#26435;&#37325;&#65292;&#34920;&#31034;&#23545;&#20110;&#24110;&#21161;&#20272;&#35745;EF&#30340;&#20010;&#21035;&#24103;&#30340;&#37325;&#35201;&#24615;&#12290;&#19968;&#20010;GNN&#22238;&#24402;&#22120;&#20351;&#29992;&#36825;&#20010;&#21152;&#26435;&#22270;&#26469;&#39044;&#27979;EF&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;EchoGNN&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ejection fraction (EF) is a key indicator of cardiac function, allowing identification of patients prone to heart dysfunctions such as heart failure. EF is estimated from cardiac ultrasound videos known as echocardiograms (echo) by manually tracing the left ventricle and estimating its volume on certain frames. These estimations exhibit high inter-observer variability due to the manual process and varying video quality. Such sources of inaccuracy and the need for rapid assessment necessitate reliable and explainable machine learning techniques. In this work, we introduce EchoGNN, a model based on graph neural networks (GNNs) to estimate EF from echo videos. Our model first infers a latent echo-graph from the frames of one or multiple echo cine series. It then estimates weights over nodes and edges of this graph, indicating the importance of individual frames that aid EF estimation. A GNN regressor uses this weighted graph to predict EF. We show, qualitatively and quantitatively, that t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#38459;&#22622;Gibbs&#37319;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#21487;&#34892;&#22320;&#36827;&#34892;&#23567;&#25209;&#37327;MCMC&#37319;&#26679;&#65292;&#25552;&#39640;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2208.11389</link><description>&lt;p&gt;
Bayesian&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36817;&#20284;&#38459;&#22622;Gibbs&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Approximate blocked Gibbs sampling for Bayesian neural networks. (arXiv:2208.11389v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#38459;&#22622;Gibbs&#37319;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#21487;&#34892;&#22320;&#36827;&#34892;&#23567;&#25209;&#37327;MCMC&#37319;&#26679;&#65292;&#25552;&#39640;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21487;&#34892;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#23567;&#25209;&#37327;MCMC&#37319;&#26679;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25991;&#20013;&#25552;&#20986;&#20102;&#36890;&#36807;&#38459;&#22622;Gibbs&#37319;&#26679;&#26041;&#26696;&#23545;&#21442;&#25968;&#36827;&#34892;&#23376;&#32452;&#25277;&#26679;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#21442;&#25968;&#31354;&#38388;&#36827;&#34892;&#21010;&#20998;&#65292;&#26080;&#35770;&#23618;&#23485;&#22914;&#20309;&#65292;&#37117;&#33021;&#36827;&#34892;&#37319;&#26679;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#22312;&#28145;&#23618;&#20943;&#23567;&#24314;&#35758;&#26041;&#24046;&#65292;&#21487;&#20197;&#20943;&#36731;&#36882;&#22686;&#28145;&#24230;&#26102;&#28040;&#22833;&#30340;&#25509;&#21463;&#29575;&#38382;&#39064;&#12290;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#22686;&#21152;&#38750;&#25910;&#25947;&#38142;&#30340;&#38271;&#24230;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#22240;&#27492;&#36991;&#20813;&#28040;&#22833;&#30340;&#25509;&#21463;&#29575;&#21644;&#20801;&#35768;&#26356;&#38271;&#30340;&#38142;&#36816;&#34892;&#20855;&#26377;&#23454;&#38469;&#22909;&#22788;&#12290;&#27492;&#22806;&#65292;&#38750;&#25910;&#25947;&#38142;&#30340;&#23454;&#29616;&#26377;&#21161;&#20110;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#26159;&#22312;&#23384;&#22312;&#22686;&#24191;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#36827;&#34892;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#23567;&#25209;&#37327;MCMC&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, minibatch MCMC sampling for feedforward neural networks is made more feasible. To this end, it is proposed to sample subgroups of parameters via a blocked Gibbs sampling scheme. By partitioning the parameter space, sampling is possible irrespective of layer width. It is also possible to alleviate vanishing acceptance rates for increasing depth by reducing the proposal variance in deeper layers. Increasing the length of a non-convergent chain increases the predictive accuracy in classification tasks, so avoiding vanishing acceptance rates and consequently enabling longer chain runs have practical benefits. Moreover, non-convergent chain realizations aid in the quantification of predictive uncertainty. An open problem is how to perform minibatch MCMC sampling for feedforward neural networks in the presence of augmented data.
&lt;/p&gt;</description></item><item><title>Frouros&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#65292;&#21487;&#20197;&#26816;&#27979;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#27010;&#24565;&#21644;&#25968;&#25454;&#28418;&#31227;&#65292;&#26131;&#20110;&#32500;&#25252;&#21644;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2208.06868</link><description>&lt;p&gt;
Frouros: &#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#28418;&#31227;&#26816;&#27979;&#30340;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
Frouros: A Python library for drift detection in machine learning systems. (arXiv:2208.06868v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06868
&lt;/p&gt;
&lt;p&gt;
Frouros&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#65292;&#21487;&#20197;&#26816;&#27979;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#27010;&#24565;&#21644;&#25968;&#25454;&#28418;&#31227;&#65292;&#26131;&#20110;&#32500;&#25252;&#21644;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Frouros&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#65292;&#33021;&#22815;&#26816;&#27979;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#28418;&#31227;&#12290;&#23427;&#25552;&#20379;&#20102;&#20256;&#32479;&#21644;&#26368;&#36817;&#31639;&#27861;&#30340;&#32452;&#21512;&#26469;&#26816;&#27979;&#27010;&#24565;&#21644;&#25968;&#25454;&#28418;&#31227;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#30446;&#26631;&#26159;&#20351;&#23427;&#19982;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20860;&#23481;&#65292;&#24182;&#36731;&#26494;&#36866;&#24212;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#12290;&#35813;&#24211;&#36981;&#24490;&#19968;&#31995;&#21015;&#26368;&#20339;&#24320;&#21457;&#21644;&#25345;&#32493;&#38598;&#25104;&#23454;&#36341;&#65292;&#20197;&#30830;&#20445;&#26131;&#20110;&#32500;&#25252;&#21644;&#25193;&#23637;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/IFCA/frouros&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Frouros is an open-source Python library capable of detecting drift in machine learning systems. It provides a combination of classical and more recent algorithms for drift detection: both concept and data drift. We have designed it with the objective of making it compatible with any machine learning framework and easily adaptable to real-world use cases. The library is developed following a set of best development and continuous integration practices to ensure ease of maintenance and extensibility. The source code is available at https://github.com/IFCA/frouros.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#20998;&#31163;&#26465;&#20214;&#30340;&#31232;&#30095;&#30697;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#24341;&#20837;&#20102;&#20840;&#38754;&#19988;&#32039;&#23494;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;Vandermonde&#30697;&#38453;&#21644;Schur&#22810;&#39033;&#24335;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20379;&#20102;&#19982;&#20998;&#31163;&#26080;&#20851;&#30340;&#32039;&#23494;&#25668;&#21160;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2207.13008</link><description>&lt;p&gt;
&#26080;&#38656;&#20998;&#31163;&#30340;&#31232;&#30095;&#30697;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Algorithms for Sparse Moment Problems without Separation. (arXiv:2207.13008v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.13008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#20998;&#31163;&#26465;&#20214;&#30340;&#31232;&#30095;&#30697;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#24341;&#20837;&#20102;&#20840;&#38754;&#19988;&#32039;&#23494;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;Vandermonde&#30697;&#38453;&#21644;Schur&#22810;&#39033;&#24335;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20379;&#20102;&#19982;&#20998;&#31163;&#26080;&#20851;&#30340;&#32039;&#23494;&#25668;&#21160;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#39640;&#32500;&#31354;&#38388;&#20013;&#20174;&#21547;&#22122;&#22768;&#30340;&#30697;&#20449;&#24687;&#20013;&#23398;&#20064;$k$-&#23574;&#23792;&#28151;&#21512;&#30340;&#31232;&#30095;&#30697;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#36816;&#36755;&#36317;&#31163;&#26469;&#34913;&#37327;&#23398;&#20064;&#28151;&#21512;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20808;&#21069;&#30340;&#31639;&#27861;&#35201;&#20040;&#20551;&#35774;&#20102;&#29305;&#23450;&#30340;&#20998;&#31163;&#26465;&#20214;&#65292;&#35201;&#20040;&#20351;&#29992;&#26356;&#22810;&#30340;&#24674;&#22797;&#30697;&#65292;&#35201;&#20040;&#36816;&#34892;&#22312;&#65288;&#36229;&#65289;&#25351;&#25968;&#26102;&#38388;&#12290;&#25105;&#20204;&#38024;&#23545;&#19968;&#32500;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#31639;&#27861;&#65288;&#20063;&#31216;&#20026;&#31232;&#30095;Hausdorff&#30697;&#38382;&#39064;&#65289;&#65292;&#20854;&#22522;&#30784;&#26159;&#32463;&#20856;&#30340;Prony&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#20027;&#35201;&#22312;&#20110;&#20998;&#26512;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#27604;&#20808;&#21069;&#24037;&#20316;&#26356;&#20840;&#38754;&#19988;&#26356;&#32039;&#23494;&#30340;&#20998;&#26512;&#65288;&#20808;&#21069;&#24037;&#20316;&#20998;&#26512;&#20102;Prony&#26041;&#27861;&#20013;&#38388;&#32467;&#26524;&#30340;&#25668;&#21160;&#65289;&#12290;&#19968;&#20010;&#26377;&#29992;&#30340;&#25216;&#26415;&#35201;&#32032;&#26159;Vandermonde&#30697;&#38453;&#23450;&#20041;&#30340;&#32447;&#24615;&#31995;&#32479;&#19982;Schur&#22810;&#39033;&#24335;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#23427;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#19982;&#20998;&#31163;&#26080;&#20851;&#30340;&#32039;&#23494;&#25668;&#21160;&#30028;&#38480;&#65292;&#24182;&#19988;&#22312;&#20854;&#20182;&#24773;&#22659;&#20013;&#20063;&#21487;&#33021;&#26377;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#39640;&#32500;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#27714;&#35299;&#20102;&#19968;&#32500;&#38382;&#39064;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the sparse moment problem of learning a $k$-spike mixture in high-dimensional space from its noisy moment information in any dimension. We measure the accuracy of the learned mixtures using transportation distance. Previous algorithms either assume certain separation assumptions, use more recovery moments, or run in (super) exponential time. Our algorithm for the one-dimensional problem (also called the sparse Hausdorff moment problem) is a robust version of the classic Prony's method, and our contribution mainly lies in the analysis. We adopt a global and much tighter analysis than previous work (which analyzes the perturbation of the intermediate results of Prony's method). A useful technical ingredient is a connection between the linear system defined by the Vandermonde matrix and the Schur polynomial, which allows us to provide tight perturbation bound independent of the separation and may be useful in other contexts. To tackle the high-dimensional problem, we first sol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#22270;&#25299;&#25169;&#32467;&#26500;&#65292;&#29992;&#20110;&#36328;&#25968;&#25454;&#20179;&#24211;&#32852;&#37030;&#23398;&#20064;&#65292;&#36890;&#36807;&#23396;&#31435;&#33410;&#28857;&#23454;&#29616;&#27169;&#22411;&#32858;&#21512;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2207.09657</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#22270;&#25299;&#25169;&#22312;&#36328;&#25968;&#25454;&#20179;&#24211;&#32852;&#37030;&#23398;&#20064;&#20013;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
Reducing Training Time in Cross-Silo Federated Learning using Multigraph Topology. (arXiv:2207.09657v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#22270;&#25299;&#25169;&#32467;&#26500;&#65292;&#29992;&#20110;&#36328;&#25968;&#25454;&#20179;&#24211;&#32852;&#37030;&#23398;&#20064;&#65292;&#36890;&#36807;&#23396;&#31435;&#33410;&#28857;&#23454;&#29616;&#27169;&#22411;&#32858;&#21512;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#22240;&#20026;&#23427;&#20351;&#24471;&#22810;&#20010;&#21442;&#19982;&#32773;&#33021;&#22815;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#32780;&#26080;&#38656;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#12290;&#30446;&#21069;&#65292;&#36328;&#25968;&#25454;&#20179;&#24211;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35757;&#32451;&#35774;&#32622;&#65292;&#23427;&#21033;&#29992;&#20960;&#30334;&#20010;&#21487;&#38752;&#30340;&#25968;&#25454;&#20179;&#24211;&#21644;&#39640;&#36895;&#35775;&#38382;&#38142;&#36335;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#35774;&#35745;&#19968;&#20010;&#31283;&#20581;&#30340;&#25299;&#25169;&#32467;&#26500;&#20197;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#36328;&#25968;&#25454;&#20179;&#24211;&#32852;&#37030;&#23398;&#20064;&#30340;&#22810;&#22270;&#25299;&#25169;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#35206;&#30422;&#22270;&#26500;&#24314;&#22810;&#22270;&#65292;&#28982;&#21518;&#23558;&#36825;&#20010;&#22810;&#22270;&#35299;&#26512;&#25104;&#20855;&#26377;&#23396;&#31435;&#33410;&#28857;&#30340;&#19981;&#21516;&#31616;&#21333;&#22270;&#12290;&#23396;&#31435;&#33410;&#28857;&#30340;&#23384;&#22312;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#22312;&#31561;&#24453;&#20854;&#20182;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27169;&#22411;&#32858;&#21512;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;&#23545;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#26368;&#26032;&#30340;&#25299;&#25169;&#32467;&#26500;&#30456;&#27604;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is an active research topic since it enables several participants to jointly train a model without sharing local data. Currently, cross-silo federated learning is a popular training setting that utilizes a few hundred reliable data silos with high-speed access links to training a model. While this approach has been widely applied in real-world scenarios, designing a robust topology to reduce the training time remains an open problem. In this paper, we present a new multigraph topology for cross-silo federated learning. We first construct the multigraph using the overlay graph. We then parse this multigraph into different simple graphs with isolated nodes. The existence of isolated nodes allows us to perform model aggregation without waiting for other nodes, hence effectively reducing the training time. Intensive experiments on three public datasets show that our proposed method significantly reduces the training time compared with recent state-of-the-art topologies w
&lt;/p&gt;</description></item><item><title>GANDALF&#26159;&#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#39640;&#24615;&#33021;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#29305;&#28857;&#12290;&#36890;&#36807;&#24341;&#20837;&#38376;&#25511;&#29305;&#24449;&#23398;&#20064;&#21333;&#20803;&#65292;GANDALF&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#20844;&#24320;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#25110;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25345;&#24179;&#12290;</title><link>http://arxiv.org/abs/2207.08548</link><description>&lt;p&gt;
GANDALF: &#29992;&#20110;&#28145;&#24230;&#33258;&#21160;&#21270;&#29305;&#24449;&#23398;&#20064;&#30340;&#38376;&#25511;&#33258;&#36866;&#24212;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GANDALF: Gated Adaptive Network for Deep Automated Learning of Features. (arXiv:2207.08548v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08548
&lt;/p&gt;
&lt;p&gt;
GANDALF&#26159;&#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#39640;&#24615;&#33021;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#29305;&#28857;&#12290;&#36890;&#36807;&#24341;&#20837;&#38376;&#25511;&#29305;&#24449;&#23398;&#20064;&#21333;&#20803;&#65292;GANDALF&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#20844;&#24320;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#25110;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#12289;&#21442;&#25968;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#65292;&#31216;&#20026;GANDALF&#65288;Gated Adaptive Network for Deep Automated Learning of Features&#65289;&#12290;GANDALF&#20381;&#36182;&#20110;&#19968;&#20010;&#20855;&#26377;&#38376;&#25511;&#26426;&#21046;&#21644;&#20869;&#32622;&#29305;&#24449;&#36873;&#25321;&#30340;&#26032;&#30340;&#34920;&#26684;&#22788;&#29702;&#21333;&#20803;&#65292;&#31216;&#20026;&#38376;&#25511;&#29305;&#24449;&#23398;&#20064;&#21333;&#20803;&#65288;GFLU&#65289;&#65292;&#20316;&#20026;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#21333;&#20803;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#20844;&#24320;&#30340;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;GANDALF&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#25110;&#19982;XGBoost&#12289;SAINT&#12289;FT-Transformers&#31561;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25345;&#24179;&#12290;&#25105;&#20204;&#24050;&#32463;&#23558;&#20195;&#30721;&#22312;github.com/manujosephv/pytorch_tabular&#19978;&#20197;MIT&#35768;&#21487;&#35777;&#30340;&#24418;&#24335;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel high-performance, interpretable, and parameter \&amp; computationally efficient deep learning architecture for tabular data, Gated Adaptive Network for Deep Automated Learning of Features (GANDALF). GANDALF relies on a new tabular processing unit with a gating mechanism and in-built feature selection called Gated Feature Learning Unit (GFLU) as a feature representation learning unit. We demonstrate that GANDALF outperforms or stays at-par with SOTA approaches like XGBoost, SAINT, FT-Transformers, etc. by experiments on multiple established public benchmarks. We have made available the code at github.com/manujosephv/pytorch_tabular under MIT License.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;SEADS&#65292;&#23558;&#36830;&#32493;&#25511;&#21046;&#30340;&#26102;&#38388;&#24310;&#23637;&#25216;&#33021;&#19982;&#29615;&#22659;&#29366;&#24577;&#30340;&#31526;&#21495;&#31163;&#25955;&#25277;&#35937;&#30340;&#21069;&#21521;&#27169;&#22411;&#30456;&#36830;&#25509;&#65292;&#23454;&#29616;&#20102;&#22312;&#36830;&#32493;&#39046;&#22495;&#20013;&#30340;&#38271;&#31243;&#35268;&#21010;&#21644;&#25511;&#21046;&#33021;&#21147;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2207.05018</link><description>&lt;p&gt;
&#22312;&#36830;&#32493;&#39046;&#22495;&#20013;&#23558;&#26102;&#38388;&#24310;&#23637;&#25216;&#33021;&#20316;&#20026;&#31526;&#21495;&#21160;&#20316;&#36827;&#34892;&#35268;&#21010;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Temporally Extended Skills in Continuous Domains as Symbolic Actions for Planning. (arXiv:2207.05018v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;SEADS&#65292;&#23558;&#36830;&#32493;&#25511;&#21046;&#30340;&#26102;&#38388;&#24310;&#23637;&#25216;&#33021;&#19982;&#29615;&#22659;&#29366;&#24577;&#30340;&#31526;&#21495;&#31163;&#25955;&#25277;&#35937;&#30340;&#21069;&#21521;&#27169;&#22411;&#30456;&#36830;&#25509;&#65292;&#23454;&#29616;&#20102;&#22312;&#36830;&#32493;&#39046;&#22495;&#20013;&#30340;&#38271;&#31243;&#35268;&#21010;&#21644;&#25511;&#21046;&#33021;&#21147;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26082;&#38656;&#35201;&#38271;&#31243;&#35268;&#21010;&#21448;&#38656;&#35201;&#36830;&#32493;&#25511;&#21046;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#23427;&#23558;&#36830;&#32493;&#25511;&#21046;&#30340;&#26102;&#38388;&#24310;&#23637;&#25216;&#33021;&#19982;&#29615;&#22659;&#29366;&#24577;&#30340;&#31526;&#21495;&#31163;&#25955;&#25277;&#35937;&#30340;&#21069;&#21521;&#27169;&#22411;&#30456;&#36830;&#25509;&#65292;&#29992;&#20110;&#35268;&#21010;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20195;&#29702;&#31216;&#20026;SEADS&#65292;&#21363;Symbolic Effect-Aware Diverse Skills&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#30446;&#26631;&#21644;&#30456;&#24212;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20869;&#22312;&#21160;&#26426;&#22312;&#24050;&#30693;&#29366;&#24577;&#25277;&#35937;&#19979;&#36827;&#34892;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#20174;&#32780;&#23398;&#20064;&#21040;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#25216;&#33021;&#12290;&#36825;&#20123;&#25216;&#33021;&#19982;&#25429;&#25417;&#25216;&#33021;&#25191;&#34892;&#22312;&#29366;&#24577;&#25277;&#35937;&#20013;&#30340;&#24433;&#21709;&#30340;&#31526;&#21495;&#21069;&#21521;&#27169;&#22411;&#20849;&#21516;&#23398;&#20064;&#12290;&#35757;&#32451;&#23436;&#25104;&#21518;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#25216;&#33021;&#20316;&#20026;&#31526;&#21495;&#21160;&#20316;&#20351;&#29992;&#21069;&#21521;&#27169;&#22411;&#36827;&#34892;&#38271;&#31243;&#35268;&#21010;&#65292;&#24182;&#38543;&#21518;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#36830;&#32493;&#21160;&#20316;&#25511;&#21046;&#25216;&#33021;&#25191;&#34892;&#35745;&#21010;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#25216;&#33021;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Problems which require both long-horizon planning and continuous control capabilities pose significant challenges to existing reinforcement learning agents. In this paper we introduce a novel hierarchical reinforcement learning agent which links temporally extended skills for continuous control with a forward model in a symbolic discrete abstraction of the environment's state for planning. We term our agent SEADS for Symbolic Effect-Aware Diverse Skills. We formulate an objective and corresponding algorithm which leads to unsupervised learning of a diverse set of skills through intrinsic motivation given a known state abstraction. The skills are jointly learned with the symbolic forward model which captures the effect of skill execution in the state abstraction. After training, we can leverage the skills as symbolic actions using the forward model for long-horizon planning and subsequently execute the plan using the learned continuous-action control skills. The proposed algorithm learn
&lt;/p&gt;</description></item><item><title>TF-GNN&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340; TensorFlow &#22270;&#31070;&#32463;&#32593;&#32476;&#24211;&#65292;&#29992;&#20110;&#25903;&#25345;&#20016;&#23500;&#30340;&#24322;&#26500;&#22270;&#25968;&#25454;&#12290;&#23427;&#25552;&#20379;&#20102;&#20302;&#20195;&#30721;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24191;&#27867;&#24212;&#29992;&#20110;&#35895;&#27468;&#30340;&#29983;&#20135;&#27169;&#22411;&#20013;&#12290;&#36825;&#20010;&#24211;&#26368;&#36817;&#20316;&#20026;&#24320;&#28304;&#39033;&#30446;&#21457;&#24067;&#65292;&#20026;&#22270;&#23398;&#20064;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2207.03522</link><description>&lt;p&gt;
TF-GNN: TensorFlow&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TF-GNN: Graph Neural Networks in TensorFlow. (arXiv:2207.03522v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03522
&lt;/p&gt;
&lt;p&gt;
TF-GNN&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340; TensorFlow &#22270;&#31070;&#32463;&#32593;&#32476;&#24211;&#65292;&#29992;&#20110;&#25903;&#25345;&#20016;&#23500;&#30340;&#24322;&#26500;&#22270;&#25968;&#25454;&#12290;&#23427;&#25552;&#20379;&#20102;&#20302;&#20195;&#30721;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24191;&#27867;&#24212;&#29992;&#20110;&#35895;&#27468;&#30340;&#29983;&#20135;&#27169;&#22411;&#20013;&#12290;&#36825;&#20010;&#24211;&#26368;&#36817;&#20316;&#20026;&#24320;&#28304;&#39033;&#30446;&#21457;&#24067;&#65292;&#20026;&#22270;&#23398;&#20064;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TensorFlow-GNN (TF-GNN) &#26159;&#19968;&#20010;&#22312; TensorFlow &#20013;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#24211;&#12290;&#23427;&#20174;&#24213;&#23618;&#35774;&#35745;&#26469;&#25903;&#25345;&#24403;&#20170;&#20449;&#24687;&#29983;&#24577;&#31995;&#32479;&#20013;&#20986;&#29616;&#30340;&#21508;&#31181;&#20016;&#23500;&#30340;&#24322;&#26500;&#22270;&#25968;&#25454;&#12290;&#38500;&#20102;&#20026;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#21644;&#39640;&#32423;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#25903;&#25345;&#65292;TF-GNN&#36824;&#25552;&#20379;&#20302;&#20195;&#30721;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#36171;&#33021;&#26356;&#24191;&#27867;&#30340;&#24320;&#21457;&#32773;&#31038;&#21306;&#36827;&#34892;&#22270;&#23398;&#20064;&#12290;&#35895;&#27468;&#30340;&#35768;&#22810;&#29983;&#20135;&#27169;&#22411;&#37117;&#22312;&#20351;&#29992; TF-GNN&#65292;&#24182;&#19988;&#23427;&#26368;&#36817;&#20316;&#20026;&#19968;&#20010;&#24320;&#28304;&#39033;&#30446;&#21457;&#24067;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102; TF-GNN &#30340;&#25968;&#25454;&#27169;&#22411;&#12289;&#20854; Keras &#28040;&#24687;&#20256;&#36882; API &#20197;&#21450;&#30456;&#20851;&#30340;&#33021;&#21147;&#65292;&#22914;&#22270;&#37319;&#26679;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
TensorFlow-GNN (TF-GNN) is a scalable library for Graph Neural Networks in TensorFlow. It is designed from the bottom up to support the kinds of rich heterogeneous graph data that occurs in today's information ecosystems. In addition to enabling machine learning researchers and advanced developers, TF-GNN offers low-code solutions to empower the broader developer community in graph learning. Many production models at Google use TF-GNN, and it has been recently released as an open source project. In this paper we describe the TF-GNN data model, its Keras message passing API, and relevant capabilities such as graph sampling and distributed training.
&lt;/p&gt;</description></item><item><title>TabText&#26159;&#19968;&#31181;&#22788;&#29702;&#21644;&#29305;&#24449;&#25552;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#36716;&#25442;&#20869;&#23481;&#20026;&#35821;&#35328;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#36890;&#36807;&#24212;&#29992;TabText&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#39640;&#24615;&#33021;&#19988;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27169;&#22411;&#65292;&#20943;&#23569;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#24037;&#20316;&#37327;&#12290;&#35813;&#26694;&#26550;&#22312;&#21307;&#30103;&#39044;&#27979;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.10381</link><description>&lt;p&gt;
TabText:&#19968;&#31181;&#28789;&#27963;&#21644;&#19978;&#19979;&#25991;&#21270;&#30340;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TabText: A Flexible and Contextual Approach to Tabular Data Representation. (arXiv:2206.10381v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10381
&lt;/p&gt;
&lt;p&gt;
TabText&#26159;&#19968;&#31181;&#22788;&#29702;&#21644;&#29305;&#24449;&#25552;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#36716;&#25442;&#20869;&#23481;&#20026;&#35821;&#35328;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#36890;&#36807;&#24212;&#29992;TabText&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#39640;&#24615;&#33021;&#19988;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27169;&#22411;&#65292;&#20943;&#23569;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#24037;&#20316;&#37327;&#12290;&#35813;&#26694;&#26550;&#22312;&#21307;&#30103;&#39044;&#27979;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#23545;&#20110;&#22312;&#21508;&#20010;&#34892;&#19994;&#20013;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#24182;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#34920;&#26684;&#20013;&#25152;&#26377;&#21487;&#29992;&#30340;&#20449;&#24687;&#65292;&#24573;&#35270;&#20102;&#37325;&#35201;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#22914;&#21015;&#26631;&#39064;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#23558;&#25968;&#25454;&#39044;&#22788;&#29702;&#25104;&#34920;&#26684;&#26684;&#24335;&#20173;&#28982;&#26159;&#27169;&#22411;&#24320;&#21457;&#20013;&#19968;&#39033;&#32791;&#26102;&#30340;&#29942;&#39048;&#12290;&#26412;&#24037;&#20316;&#24341;&#20837;&#20102;TabText&#65292;&#19968;&#31181;&#22788;&#29702;&#21644;&#29305;&#24449;&#25552;&#21462;&#26694;&#26550;&#65292;&#23558;&#19978;&#19979;&#25991;&#20449;&#24687;&#20174;&#34920;&#26684;&#25968;&#25454;&#32467;&#26500;&#20013;&#25552;&#21462;&#20986;&#26469;&#12290;TabText&#36890;&#36807;&#23558;&#20869;&#23481;&#36716;&#25442;&#20026;&#35821;&#35328;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#35299;&#20915;&#22788;&#29702;&#22256;&#38590;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;&#24739;&#32773;&#20986;&#38498;&#12289;ICU&#20837;&#38498;&#21644;&#27515;&#20129;&#31561;&#20061;&#20010;&#21307;&#30103;&#39044;&#27979;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65306;1) &#24212;&#29992;&#25105;&#20204;&#30340;TabText&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#24615;&#33021;&#20248;&#31168;&#19988;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27169;&#22411;&#65292;&#21482;&#38656;&#26368;&#23569;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#65307;2) &#22686;&#24378;&#39044;&#22788;&#29702;&#21518;&#30340;&#25968;&#25454;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25552;&#21319;&#27169;&#22411;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Tabular data is essential for applying machine learning tasks across various industries. However, traditional data processing methods do not fully utilize all the information available in the tables, ignoring important contextual information such as column header descriptions. In addition, pre-processing data into a tabular format can remain a labor-intensive bottleneck in model development. This work introduces TabText, a processing and feature extraction framework that extracts contextual information from tabular data structures. TabText addresses processing difficulties by converting the content into language and utilizing pre-trained large language models (LLMs). We evaluate our framework on nine healthcare prediction tasks ranging from patient discharge, ICU admission, and mortality. We show that 1) applying our TabText framework enables the generation of high-performing and simple machine learning baseline models with minimal data pre-processing, and 2) augmenting pre-processed t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;700,000&#20154;&#26085;&#30340;&#26410;&#26631;&#35760;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27867;&#21270;&#19988;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#26377;&#26395;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#24320;&#21457;&#39640;&#24615;&#33021;&#30340;&#21487;&#23450;&#21046;&#21644;&#27867;&#21270;&#30340;&#27963;&#21160;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2206.02909</link><description>&lt;p&gt;
&#20351;&#29992;70&#19975;&#20154;&#26085;&#30340;&#21487;&#31359;&#25140;&#25968;&#25454;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Learning for Human Activity Recognition Using 700,000 Person-days of Wearable Data. (arXiv:2206.02909v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;700,000&#20154;&#26085;&#30340;&#26410;&#26631;&#35760;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27867;&#21270;&#19988;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#26377;&#26395;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#24320;&#21457;&#39640;&#24615;&#33021;&#30340;&#21487;&#23450;&#21046;&#21644;&#27867;&#21270;&#30340;&#27963;&#21160;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26041;&#38754;&#30340;&#36827;&#23637;&#30456;&#23545;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#23545;UK-Biobank&#27963;&#21160;&#36861;&#36394;&#22120;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;70&#19975;&#20154;&#26085;&#30340;&#26410;&#26631;&#35760;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#25105;&#20204;&#24471;&#21040;&#30340;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#22312;&#19971;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22987;&#32456;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#65292;F1&#30456;&#23545;&#25913;&#21892;2.5%-100%&#65288;&#20013;&#20301;&#25968;18.4%&#65289;&#65292;&#25913;&#36827;&#26368;&#22823;&#30340;&#26159;&#35268;&#27169;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#20197;&#27867;&#21270;&#21040;&#22806;&#37096;&#25968;&#25454;&#38598;&#12289;&#35774;&#22791;&#21644;&#29615;&#22659;&#12290;&#25105;&#20204;&#30340;&#24320;&#28304;&#27169;&#22411;&#23558;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#26500;&#24314;&#20855;&#26377;&#39640;&#24615;&#33021;&#30340;&#21487;&#23450;&#21046;&#21644;&#27867;&#21270;&#30340;&#27963;&#21160;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in deep learning for human activity recognition have been relatively limited due to the lack of large labelled datasets. In this study, we leverage self-supervised learning techniques on the UK-Biobank activity tracker dataset--the largest of its kind to date--containing more than 700,000 person-days of unlabelled wearable sensor data. Our resulting activity recognition model consistently outperformed strong baselines across seven benchmark datasets, with an F1 relative improvement of 2.5%-100% (median 18.4%), the largest improvements occurring in the smaller datasets. In contrast to previous studies, our results generalise across external datasets, devices, and environments. Our open-source model will help researchers and developers to build customisable and generalisable activity classifiers with high performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#32676;&#20307;&#23646;&#24615;&#20010;&#24615;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#38477;&#20302;&#32676;&#20307;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#30830;&#20445;&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#20844;&#24179;&#20351;&#29992;&#32676;&#20307;&#23646;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24418;&#24335;&#21270;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#20013;&#26222;&#36941;&#23384;&#22312;&#20844;&#24179;&#20351;&#29992;&#36829;&#35268;&#30340;&#24773;&#20917;&#65292;&#20294;&#25105;&#20204;&#20063;&#25214;&#21040;&#20102;&#31616;&#21333;&#24178;&#39044;&#25163;&#27573;&#26469;&#20943;&#36731;&#20854;&#20260;&#23475;&#12290;</title><link>http://arxiv.org/abs/2206.02058</link><description>&lt;p&gt;
&#24403;&#20010;&#24615;&#21270;&#36896;&#25104;&#20260;&#23475;&#26102;&#65306;&#37325;&#26032;&#32771;&#34385;&#22312;&#39044;&#27979;&#20013;&#20351;&#29992;&#32676;&#20307;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
When Personalization Harms: Reconsidering the Use of Group Attributes in Prediction. (arXiv:2206.02058v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#32676;&#20307;&#23646;&#24615;&#20010;&#24615;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#38477;&#20302;&#32676;&#20307;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#30830;&#20445;&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#20844;&#24179;&#20351;&#29992;&#32676;&#20307;&#23646;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24418;&#24335;&#21270;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#20013;&#26222;&#36941;&#23384;&#22312;&#20844;&#24179;&#20351;&#29992;&#36829;&#35268;&#30340;&#24773;&#20917;&#65292;&#20294;&#25105;&#20204;&#20063;&#25214;&#21040;&#20102;&#31616;&#21333;&#24178;&#39044;&#25163;&#27573;&#26469;&#20943;&#36731;&#20854;&#20260;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#20250;&#20351;&#29992;&#21463;&#20445;&#25252;&#12289;&#25935;&#24863;&#12289;&#33258;&#25253;&#21578;&#25110;&#32773;&#26114;&#36149;&#30340;&#20998;&#31867;&#23646;&#24615;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#26412;&#30740;&#31350;&#25351;&#20986;&#65292;&#20351;&#29992;&#32676;&#20307;&#23646;&#24615;&#36827;&#34892;&#20010;&#24615;&#21270;&#20250;&#38477;&#20302;&#32676;&#20307;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#8220;&#20844;&#24179;&#20351;&#29992;&#8221;&#32676;&#20307;&#23646;&#24615;&#65292;&#26041;&#27861;&#26159;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#39069;&#22806;&#30340;&#27169;&#22411;&#65292;&#21363;&#20445;&#35777;&#27599;&#20010;&#25552;&#20379;&#20010;&#20154;&#25968;&#25454;&#30340;&#32676;&#20307;&#20250;&#33719;&#24471;&#30456;&#23545;&#24212;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36275;&#22815;&#30340;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;&#22312;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#20013;&#30340;&#20844;&#24179;&#20351;&#29992;&#65292;&#24182;&#25551;&#36848;&#20102;&#23548;&#33268;&#20844;&#24179;&#20351;&#29992;&#36829;&#35268;&#30340;&#25925;&#38556;&#27169;&#24335;&#65292;&#36825;&#26159;&#30001;&#20110;&#27169;&#22411;&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#30340;&#26631;&#20934;&#20570;&#27861;&#25152;&#23548;&#33268;&#30340;&#12290;&#25105;&#20204;&#23545;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23454;&#36341;&#20013;&#26222;&#36941;&#23384;&#22312;&#20844;&#24179;&#20351;&#29992;&#36829;&#35268;&#65292;&#24182;&#35828;&#26126;&#20102;&#20943;&#36731;&#20854;&#20260;&#23475;&#30340;&#31616;&#21333;&#24178;&#39044;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are often personalized with categorical attributes that are protected, sensitive, self-reported, or costly to acquire. In this work, we show models that are personalized with group attributes can reduce performance at a group level. We propose formal conditions to ensure the "fair use" of group attributes in prediction tasks by training one additional model -- i.e., collective preference guarantees to ensure that each group who provides personal data will receive a tailored gain in performance in return. We present sufficient conditions to ensure fair use in empirical risk minimization and characterize failure modes that lead to fair use violations due to standard practices in model development and deployment. We present a comprehensive empirical study of fair use in clinical prediction tasks. Our results demonstrate the prevalence of fair use violations in practice and illustrate simple interventions to mitigate their harm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20195;&#25968;&#20998;&#26512;&#25913;&#36827;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;GNN&#33021;&#22815;&#27604;Weisfeiler-Lehman&#65288;WL&#65289;&#31639;&#27861;&#26356;&#22909;&#22320;&#20135;&#29983;&#21306;&#20998;&#24615;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#20540;&#30340;&#22270;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#31616;&#21333;&#30340;&#21367;&#31215;&#32467;&#26500;&#19982;&#26080;&#20449;&#24687;&#36755;&#20837;&#20135;&#29983;&#30340;&#31561;&#21464;&#29305;&#24449;&#27604;WL&#34920;&#31034;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2205.09801</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65306;&#36890;&#36807;&#20195;&#25968;&#20998;&#26512;&#25913;&#36827;&#34920;&#36798;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Representation Power of Graph Neural Networks: Improved Expressivity via Algebraic Analysis. (arXiv:2205.09801v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20195;&#25968;&#20998;&#26512;&#25913;&#36827;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;GNN&#33021;&#22815;&#27604;Weisfeiler-Lehman&#65288;WL&#65289;&#31639;&#27861;&#26356;&#22909;&#22320;&#20135;&#29983;&#21306;&#20998;&#24615;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#20540;&#30340;&#22270;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#31616;&#21333;&#30340;&#21367;&#31215;&#32467;&#26500;&#19982;&#26080;&#20449;&#24687;&#36755;&#20837;&#20135;&#29983;&#30340;&#31561;&#21464;&#29305;&#24449;&#27604;WL&#34920;&#31034;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#26222;&#36941;&#35748;&#20026;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#65292;&#24182;&#19988;&#23427;&#20204;&#26368;&#22810;&#19982;Weisfeiler-Lehman&#65288;WL&#65289;&#31639;&#27861;&#19968;&#26679;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#12290;&#26412;&#25991;&#19982;&#27492;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26631;&#20934;&#30340;GNN&#65288;&#21311;&#21517;&#36755;&#20837;&#65289;&#20135;&#29983;&#30340;&#34920;&#31034;&#27604;WL&#31639;&#27861;&#26356;&#20855;&#26377;&#21306;&#20998;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#32447;&#24615;&#20195;&#25968;&#24037;&#20855;&#23545;GNN&#30340;&#34920;&#31034;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#26032;&#30340;&#20998;&#26512;&#65292;&#24182;&#23558;&#20854;&#19982;&#22270;&#25805;&#20316;&#31526;&#30340;&#29305;&#24449;&#20540;&#20998;&#35299;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GNN&#33021;&#22815;&#20174;&#26080;&#20449;&#24687;&#36755;&#20837;&#20135;&#29983;&#29420;&#29305;&#30340;&#36755;&#20986;&#65292;&#33267;&#23569;&#23545;&#20110;&#25152;&#26377;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#20540;&#30340;&#22270;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#21367;&#31215;&#32467;&#26500;&#19982;&#26080;&#20449;&#24687;&#36755;&#20837;&#20135;&#29983;&#30340;&#31561;&#21464;&#29305;&#24449;&#65292;&#23427;&#20204;&#35745;&#31639;&#22270;&#20013;&#30340;&#38381;&#21512;&#36335;&#24452;&#24182;&#19988;&#26126;&#26174;&#27604;WL&#34920;&#31034;&#20855;&#26377;&#26356;&#39640;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#22312;&#22270;&#21516;&#26500;&#21644;&#22270;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#20998;&#26512;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable success of Graph Neural Networks (GNNs), the common belief is that their representation power is limited and that they are at most as expressive as the Weisfeiler-Lehman (WL) algorithm. In this paper, we argue the opposite and show that standard GNNs, with anonymous inputs, produce more discriminative representations than the WL algorithm. Our novel analysis employs linear algebraic tools and characterizes the representation power of GNNs with respect to the eigenvalue decomposition of the graph operators. We prove that GNNs are able to generate distinctive outputs from white uninformative inputs, for, at least, all graphs that have different eigenvalues. We also show that simple convolutional architectures with white inputs, produce equivariant features that count the closed paths in the graph and are provably more expressive than the WL representations. Thorough experimental analysis on graph isomorphism and graph classification datasets corroborates our theore
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20116;&#20010;&#26032;&#30340;&#29983;&#29289;&#21307;&#23398;&#26412;&#20307;&#21305;&#37197;&#20219;&#21153;&#65292;&#36890;&#36807;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24182;&#35299;&#20915;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.03447</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21451;&#22909;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#29992;&#20110;&#31561;&#20215;&#21644;&#21253;&#21547;&#20851;&#31995;&#26412;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching. (arXiv:2205.03447v7 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.03447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20116;&#20010;&#26032;&#30340;&#29983;&#29289;&#21307;&#23398;&#26412;&#20307;&#21305;&#37197;&#20219;&#21153;&#65292;&#36890;&#36807;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24182;&#35299;&#20915;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#21305;&#37197;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#21644;&#35821;&#20041;&#32593;&#31561;&#35768;&#22810;&#39046;&#22495;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#20854;&#30740;&#31350;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26412;&#20307;&#21305;&#37197;&#35780;&#20272;&#26041;&#27861;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#23545;&#21253;&#21547;&#20851;&#31995;&#26144;&#23556;&#30340;&#26377;&#38480;&#35780;&#20272;&#12289;&#21442;&#32771;&#26144;&#23556;&#30340;&#20122;&#20248;&#35299;&#20197;&#21450;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#35780;&#20272;&#30340;&#26377;&#38480;&#25903;&#25345;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20116;&#20010;&#26032;&#30340;&#29983;&#29289;&#21307;&#23398;&#26412;&#20307;&#21305;&#37197;&#20219;&#21153;&#65292;&#28041;&#21450;&#20174;Mondo&#21644;UMLS&#20013;&#25552;&#21462;&#30340;&#26412;&#20307;&#12290;&#27599;&#20010;&#20219;&#21153;&#21253;&#25324;&#31561;&#20215;&#21644;&#21253;&#21547;&#20851;&#31995;&#21305;&#37197;&#65292;&#24182;&#36890;&#36807;&#20154;&#24037;&#31579;&#36873;&#12289;&#26412;&#20307;&#20462;&#21098;&#31561;&#26041;&#24335;&#30830;&#20445;&#21442;&#32771;&#26144;&#23556;&#30340;&#36136;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#26469;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#35780;&#20272;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#38750;&#26426;&#22120;&#23398;&#20064;&#30340;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#24615;&#33021;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ontology Matching (OM) plays an important role in many domains such as bioinformatics and the Semantic Web, and its research is becoming increasingly popular, especially with the application of machine learning (ML) techniques. Although the Ontology Alignment Evaluation Initiative (OAEI) represents an impressive effort for the systematic evaluation of OM systems, it still suffers from several limitations including limited evaluation of subsumption mappings, suboptimal reference mappings, and limited support for the evaluation of ML-based systems. To tackle these limitations, we introduce five new biomedical OM tasks involving ontologies extracted from Mondo and UMLS. Each task includes both equivalence and subsumption matching; the quality of reference mappings is ensured by human curation, ontology pruning, etc.; and a comprehensive evaluation framework is proposed to measure OM performance from various perspectives for both ML-based and non-ML-based OM systems. We report evaluation r
&lt;/p&gt;</description></item><item><title>AdaBest&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#31471;&#28418;&#31227;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;AdaBest&#25152;&#38656;&#30340;&#23384;&#20648;&#21644;&#36890;&#20449;&#24102;&#23485;&#36739;&#23569;&#65292;&#35745;&#31639;&#25104;&#26412;&#20063;&#36739;&#20302;&#12290;&#27492;&#22806;&#65292;AdaBest&#36890;&#36807;&#38480;&#21046;&#20272;&#35745;&#20540;&#30340;&#33539;&#25968;&#26469;&#25552;&#20379;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.13170</link><description>&lt;p&gt;
AdaBest: &#36890;&#36807;&#33258;&#36866;&#24212;&#20559;&#24046;&#20272;&#35745;&#26368;&#23567;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
AdaBest: Minimizing Client Drift in Federated Learning via Adaptive Bias Estimation. (arXiv:2204.13170v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.13170
&lt;/p&gt;
&lt;p&gt;
AdaBest&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#31471;&#28418;&#31227;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;AdaBest&#25152;&#38656;&#30340;&#23384;&#20648;&#21644;&#36890;&#20449;&#24102;&#23485;&#36739;&#23569;&#65292;&#35745;&#31639;&#25104;&#26412;&#20063;&#36739;&#20302;&#12290;&#27492;&#22806;&#65292;AdaBest&#36890;&#36807;&#38480;&#21046;&#20272;&#35745;&#20540;&#30340;&#33539;&#25968;&#26469;&#25552;&#20379;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#35768;&#22810;&#23458;&#25143;&#31471;&#25110;&#35774;&#22791;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#20316;&#35757;&#32451;&#27169;&#22411;&#12290;&#27169;&#22411;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#36827;&#34892;&#26412;&#22320;&#20248;&#21270;&#65292;&#28982;&#21518;&#20256;&#36755;&#21040;&#38598;&#20013;&#20013;&#24515;&#36827;&#34892;&#32858;&#21512;&#12290;&#23613;&#31649;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#20998;&#25955;&#24335;&#35757;&#32451;&#33539;&#24335;&#65292;&#20294;&#26469;&#33258;&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#21487;&#33021;&#23548;&#33268;&#23616;&#37096;&#20248;&#21270;&#20559;&#31163;&#20840;&#23616;&#30446;&#26631;&#12290;&#20026;&#20102;&#20272;&#35745;&#21644;&#28040;&#38500;&#36825;&#31181;&#20559;&#31163;&#65292;&#36817;&#26399;&#22312;&#32852;&#37030;&#23398;&#20064;&#20248;&#21270;&#20013;&#24341;&#20837;&#20102;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#23458;&#25143;&#31471;&#28418;&#31227;&#36827;&#34892;&#20102;&#19981;&#20934;&#30830;&#30340;&#20272;&#35745;&#65292;&#24182;&#26368;&#32456;&#26410;&#33021;&#27491;&#30830;&#22320;&#28040;&#38500;&#23427;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#30830;&#20272;&#35745;&#23458;&#25143;&#31471;&#28418;&#31227;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#23384;&#20648;&#21644;&#36890;&#20449;&#24102;&#23485;&#65292;&#20197;&#21450;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#35758;&#30340;&#26041;&#27861;&#36890;&#36807;&#38480;&#21046;&#23458;&#25143;&#31471;&#28418;&#31227;&#20272;&#35745;&#30340;&#33539;&#25968;&#26469;&#24341;&#20837;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Federated Learning (FL), a number of clients or devices collaborate to train a model without sharing their data. Models are optimized locally at each client and further communicated to a central hub for aggregation. While FL is an appealing decentralized training paradigm, heterogeneity among data from different clients can cause the local optimization to drift away from the global objective. In order to estimate and therefore remove this drift, variance reduction techniques have been incorporated into FL optimization recently. However, these approaches inaccurately estimate the clients' drift and ultimately fail to remove it properly. In this work, we propose an adaptive algorithm that accurately estimates drift across clients. In comparison to previous works, our approach necessitates less storage and communication bandwidth, as well as lower compute costs. Additionally, our proposed methodology induces stability by constraining the norm of estimates for client drift, making it mo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#25968;&#25454;&#23494;&#24230;&#12289;&#22122;&#22768;&#21644;&#30456;&#20284;&#24615;&#23398;&#20064;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#35777;&#26126;&#20102;&#25968;&#25454;&#23545;&#30340;&#23494;&#24230;&#23545;&#20110;&#27867;&#21270;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#31181;&#22312;&#23494;&#38598;&#25968;&#25454;&#38598;&#19978;&#27604;&#23545;&#31216;&#26631;&#31614;&#22122;&#22768;&#26356;&#24046;&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#29616;&#35937;&#65292;&#31216;&#20026;&#23494;&#24230;&#35825;&#23548;&#30340;&#30456;&#20284;&#24615;&#30772;&#22351;&#65288;DIBS&#65289;&#12290;</title><link>http://arxiv.org/abs/2201.12803</link><description>&lt;p&gt;
&#22122;&#22768;&#35774;&#32622;&#20013;&#30340;&#30456;&#20284;&#24615;&#27867;&#21270;&#65306;DIBS&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Generalizing similarity in noisy setups: the DIBS phenomenon. (arXiv:2201.12803v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#25968;&#25454;&#23494;&#24230;&#12289;&#22122;&#22768;&#21644;&#30456;&#20284;&#24615;&#23398;&#20064;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#35777;&#26126;&#20102;&#25968;&#25454;&#23545;&#30340;&#23494;&#24230;&#23545;&#20110;&#27867;&#21270;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#31181;&#22312;&#23494;&#38598;&#25968;&#25454;&#38598;&#19978;&#27604;&#23545;&#31216;&#26631;&#31614;&#22122;&#22768;&#26356;&#24046;&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#29616;&#35937;&#65292;&#31216;&#20026;&#23494;&#24230;&#35825;&#23548;&#30340;&#30456;&#20284;&#24615;&#30772;&#22351;&#65288;DIBS&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#25968;&#25454;&#23494;&#24230;&#12289;&#22122;&#22768;&#21644;&#30456;&#20284;&#24615;&#23398;&#20064;&#30340;&#26222;&#36866;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26297;&#32599;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#65292;&#36825;&#26159;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#26412;&#24418;&#24335;&#65292;&#24182;&#25506;&#32034;&#20102;&#20004;&#31181;&#21487;&#33021;&#24433;&#21709;SNNs&#30340;&#22122;&#22768;&#31867;&#22411;&#65292;&#21363;&#23545;&#27604;&#26631;&#31614;&#22122;&#22768;&#65288;PLN&#65289;&#21644;&#21333;&#26631;&#31614;&#22122;&#22768;&#65288;SLN&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#19981;&#35770;&#35757;&#32451;&#35774;&#32622;&#22914;&#20309;&#65292;SNNs&#37117;&#34920;&#29616;&#20986;&#21452;&#38477;&#34892;&#20026;&#65292;&#24182;&#19988;&#22122;&#22768;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#31181;&#34892;&#20026;&#12290;&#25105;&#20204;&#35777;&#26126;&#25968;&#25454;&#23545;&#30340;&#23494;&#24230;&#23545;&#20110;&#27867;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;SNNs&#22312;&#31232;&#30095;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#20855;&#26377;&#30456;&#21516;&#25968;&#37327;&#30340;PLN&#25110;SLN&#65292;&#23427;&#20204;&#30340;&#27867;&#21270;&#24615;&#33021;&#26159;&#21487;&#27604;&#36739;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#23494;&#38598;&#25968;&#25454;&#38598;&#26102;&#65292;&#22312;&#36807;&#21442;&#25968;&#21270;&#21306;&#22495;&#20013;&#65292;PLN&#26696;&#20363;&#30340;&#27867;&#21270;&#24615;&#33021;&#36739;&#24046;&#65292;&#30456;&#23545;&#20110;SLN&#26696;&#20363;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#25105;&#20204;&#31216;&#20026;&#23494;&#24230;&#35825;&#23548;&#30340;&#30456;&#20284;&#24615;&#30772;&#22351;&#65288;DIBS&#65289;&#30340;&#29616;&#35937;&#12290;&#22312;&#36825;&#20010;&#24773;&#20917;&#19979;&#65292;PLN&#30456;&#20284;&#24615;&#36829;&#35268;&#21464;&#24471;&#23439;&#35266;&#21270;&#65292;&#20351;&#24471;&#25968;&#25454;&#38598;&#34987;&#25439;&#22351;&#21040;&#26080;&#27861;&#23454;&#29616;&#23436;&#20840;&#25554;&#20540;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work uncovers an interplay among data density, noise, and the generalization ability in similarity learning. We consider Siamese Neural Networks (SNNs), which are the basic form of contrastive learning, and explore two types of noise that can impact SNNs, Pair Label Noise (PLN) and Single Label Noise (SLN). Our investigation reveals that SNNs exhibit double descent behaviour regardless of the training setup and that it is further exacerbated by noise. We demonstrate that the density of data pairs is crucial for generalization. When SNNs are trained on sparse datasets with the same amount of PLN or SLN, they exhibit comparable generalization properties. However, when using dense datasets, PLN cases generalize worse than SLN ones in the overparametrized region, leading to a phenomenon we call Density-Induced Break of Similarity (DIBS). In this regime, PLN similarity violation becomes macroscopical, corrupting the dataset to the point where complete interpolation cannot be achieved, 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LAnoBERT&#30340;&#26080;&#35299;&#26512;&#22120;&#31995;&#32479;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20102;BERT&#27169;&#22411;&#36827;&#34892;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26080;&#20154;&#24037;&#24178;&#39044;&#30340;&#39640;&#25928;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2111.09564</link><description>&lt;p&gt;
LAnoBERT: &#22522;&#20110;BERT&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#31995;&#32479;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LAnoBERT: System Log Anomaly Detection based on BERT Masked Language Model. (arXiv:2111.09564v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.09564
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LAnoBERT&#30340;&#26080;&#35299;&#26512;&#22120;&#31995;&#32479;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20102;BERT&#27169;&#22411;&#36827;&#34892;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26080;&#20154;&#24037;&#24178;&#39044;&#30340;&#39640;&#25928;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#31995;&#32479;&#20013;&#29983;&#25104;&#30340;&#31995;&#32479;&#26085;&#24535;&#25351;&#30340;&#26159;&#22823;&#35268;&#27169;&#21516;&#26102;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#29992;&#20316;&#30830;&#23450;&#38169;&#35823;&#12289;&#20837;&#20405;&#21644;&#24322;&#24120;&#34892;&#20026;&#30340;&#22522;&#30784;&#25968;&#25454;&#12290;&#31995;&#32479;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#30340;&#30446;&#26631;&#26159;&#21450;&#26102;&#35782;&#21035;&#24322;&#24120;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20154;&#24037;&#24178;&#39044;&#65292;&#36825;&#26159;&#34892;&#19994;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#23558;&#21508;&#31181;&#24418;&#24335;&#30340;&#26085;&#24535;&#25968;&#25454;&#36716;&#25442;&#20026;&#26631;&#20934;&#21270;&#27169;&#26495;&#65292;&#20351;&#29992;&#35299;&#26512;&#22120;&#36827;&#34892;&#31639;&#27861;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#29305;&#21035;&#26159;&#65292;&#24212;&#39044;&#20808;&#23450;&#20041;&#19982;&#29305;&#23450;&#20107;&#20214;&#23545;&#24212;&#30340;&#27169;&#26495;&#65292;&#20197;&#20415;&#25152;&#26377;&#26085;&#24535;&#25968;&#25454;&#20351;&#29992;&#35813;&#27169;&#26495;&#65292;&#20854;&#20013;&#26085;&#24535;&#20851;&#38190;&#20449;&#24687;&#21487;&#33021;&#20250;&#20002;&#22833;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LAnoBERT&#30340;&#26080;&#35299;&#26512;&#22120;&#31995;&#32479;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;BERT&#27169;&#22411;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;LAnoBERT&#36890;&#36807;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#36827;&#34892;&#27169;&#22411;&#23398;&#20064;&#65292;&#36827;&#32780;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The system log generated in a computer system refers to large-scale data that are collected simultaneously and used as the basic data for determining errors, intrusion and abnormal behaviors. The aim of system log anomaly detection is to promptly identify anomalies while minimizing human intervention, which is a critical problem in the industry. Previous studies performed anomaly detection through algorithms after converting various forms of log data into a standardized template using a parser. Particularly, a template corresponding to a specific event should be defined in advance for all the log data using which the information within the log key may get lost. In this study, we propose LAnoBERT, a parser free system log anomaly detection method that uses the BERT model, exhibiting excellent natural language processing performance. The proposed method, LAnoBERT, learns the model through masked language modeling, which is a BERT-based pre-training method, and proceeds with unsupervised 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#24179;&#26041;&#26681;&#36895;&#24230;&#20989;&#25968;&#30340;&#26032;&#34920;&#31034;&#26041;&#27861;&#21644;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#27604;&#36739;&#26641;&#29366;&#19977;&#32500;&#29289;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#29289;&#20307;&#24418;&#29366;&#24046;&#24322;&#35745;&#31639;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2110.08693</link><description>&lt;p&gt;
&#23398;&#20064;&#26641;&#29366;&#19977;&#32500;&#29289;&#20307;&#30340;&#20960;&#20309;&#21644;&#25299;&#25169;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Generative Models of the Geometry and Topology of Tree-like 3D Objects. (arXiv:2110.08693v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.08693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#24179;&#26041;&#26681;&#36895;&#24230;&#20989;&#25968;&#30340;&#26032;&#34920;&#31034;&#26041;&#27861;&#21644;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#27604;&#36739;&#26641;&#29366;&#19977;&#32500;&#29289;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#29289;&#20307;&#24418;&#29366;&#24046;&#24322;&#35745;&#31639;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#20998;&#26512;&#23637;&#29616;&#20986;&#22797;&#26434;&#20960;&#20309;&#21644;&#25299;&#25169;&#21464;&#21270;&#30340;&#35814;&#32454;&#19977;&#32500;&#29983;&#29289;&#29289;&#20307;&#65292;&#20363;&#22914;&#31070;&#32463;&#20803;&#21644;&#26893;&#29289;&#26641;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#34920;&#31034;&#12289;&#27604;&#36739;&#21644;&#35745;&#31639;&#36825;&#20123;&#26641;&#29366;&#19977;&#32500;&#23545;&#35937;&#30340;&#24418;&#29366;&#24046;&#24322;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#26469;&#37327;&#21270;&#23558;&#19968;&#20010;&#26641;&#29366;&#29289;&#20307;&#21464;&#24418;&#20026;&#21478;&#19968;&#20010;&#29289;&#20307;&#25152;&#38656;&#30340;&#24367;&#26354;&#12289;&#25289;&#20280;&#21644;&#20998;&#25903;&#28369;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can one analyze detailed 3D biological objects, such as neurons and botanical trees, that exhibit complex geometrical and topological variation? In this paper, we develop a novel mathematical framework for representing, comparing, and computing geodesic deformations between the shapes of such tree-like 3D objects. A hierarchical organization of subtrees characterizes these objects -- each subtree has the main branch with some side branches attached -- and one needs to match these structures across objects for meaningful comparisons. We propose a novel representation that extends the Square-Root Velocity Function (SRVF), initially developed for Euclidean curves, to tree-shaped 3D objects. We then define a new metric that quantifies the bending, stretching, and branch sliding needed to deform one tree-shaped object into the other. Compared to the current metrics, such as the Quotient Euclidean Distance (QED) and the Tree Edit Distance (TED), the proposed representation and metric cap
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#32858;&#31867;&#20219;&#21153;&#65292;&#38024;&#23545;&#20855;&#26377;&#32452;&#21512;&#20851;&#31995;&#30340;&#32858;&#31867;&#30740;&#31350;&#65292;&#36890;&#36807;&#25552;&#20986;&#19977;&#31181;&#26032;&#31639;&#27861;&#21487;&#20197;&#23558;&#31034;&#20363;&#20998;&#25104;&#36830;&#36143;&#30340;&#32676;&#32452;&#24182;&#25512;&#26029;&#20854;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;&#12290;&#19982;&#20256;&#32479;&#30340;&#23618;&#27425;&#32858;&#31867;&#19981;&#21516;&#65292;&#35813;&#20219;&#21153;&#20851;&#27880;&#30340;&#26159;&#25214;&#21040;&#34920;&#31034;&#32452;&#25104;&#32858;&#31867;&#30340;&#23646;&#24615;&#24182;&#38598;&#30340;&#32452;&#21512;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2109.04160</link><description>&lt;p&gt;
&#32452;&#21512;&#32858;&#31867;&#65306;&#22312;&#22810;&#26631;&#31614;&#29289;&#20307;&#35782;&#21035;&#21644;&#35828;&#35805;&#32773;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Compositional Clustering: Applications to Multi-Label Object Recognition and Speaker Identification. (arXiv:2109.04160v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.04160
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#32858;&#31867;&#20219;&#21153;&#65292;&#38024;&#23545;&#20855;&#26377;&#32452;&#21512;&#20851;&#31995;&#30340;&#32858;&#31867;&#30740;&#31350;&#65292;&#36890;&#36807;&#25552;&#20986;&#19977;&#31181;&#26032;&#31639;&#27861;&#21487;&#20197;&#23558;&#31034;&#20363;&#20998;&#25104;&#36830;&#36143;&#30340;&#32676;&#32452;&#24182;&#25512;&#26029;&#20854;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;&#12290;&#19982;&#20256;&#32479;&#30340;&#23618;&#27425;&#32858;&#31867;&#19981;&#21516;&#65292;&#35813;&#20219;&#21153;&#20851;&#27880;&#30340;&#26159;&#25214;&#21040;&#34920;&#31034;&#32452;&#25104;&#32858;&#31867;&#30340;&#23646;&#24615;&#24182;&#38598;&#30340;&#32452;&#21512;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#26032;&#39062;&#30340;&#32858;&#31867;&#20219;&#21153;&#65292;&#20854;&#20013;&#32858;&#31867;&#21487;&#20197;&#20855;&#26377;&#32452;&#21512;&#20851;&#31995;&#65292;&#20363;&#22914;&#65292;&#19968;&#20010;&#32858;&#31867;&#21253;&#21547;&#30697;&#24418;&#22270;&#20687;&#65292;&#19968;&#20010;&#32858;&#31867;&#21253;&#21547;&#22278;&#24418;&#22270;&#20687;&#65292;&#19968;&#20010;&#65288;&#32452;&#21512;&#24615;&#65289;&#32858;&#31867;&#21253;&#21547;&#21516;&#26102;&#25317;&#26377;&#20004;&#31181;&#23545;&#35937;&#30340;&#22270;&#20687;&#12290;&#19982;&#23618;&#27425;&#32858;&#31867;&#19981;&#21516;&#65292;&#20854;&#20013;&#29238;&#32858;&#31867;&#34920;&#31034;&#23376;&#32858;&#31867;&#23646;&#24615;&#30340;&#20132;&#38598;&#65292;&#25105;&#20204;&#30340;&#38382;&#39064;&#26159;&#25214;&#21040;&#34920;&#31034;&#32452;&#25104;&#32858;&#31867;&#30340;&#23646;&#24615;&#24182;&#38598;&#30340;&#32452;&#21512;&#32858;&#31867;&#12290;&#36825;&#20010;&#20219;&#21153;&#30340;&#21160;&#26426;&#26469;&#33258;&#20110;&#26368;&#36817;&#24320;&#21457;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#23884;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#21306;&#20998;&#20998;&#37197;&#32473;&#31034;&#20363;&#30340;&#26631;&#31614;&#38598;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#21333;&#20010;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#31639;&#27861;--&#32452;&#21512;&#20146;&#21644;&#20256;&#25773;&#65288;CAP&#65289;&#12289;&#32452;&#21512;k&#22343;&#20540;&#65288;CKM&#65289;&#21644;&#36138;&#23146;&#32452;&#21512;&#37325;&#26032;&#20998;&#37197;&#65288;GCR&#65289;--&#21487;&#20197;&#23558;&#31034;&#20363;&#20998;&#25104;&#36830;&#36143;&#30340;&#32676;&#32452;&#24182;&#25512;&#26029;&#20854;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#19982;&#27969;&#34892;&#30340;&#31639;&#27861;&#65288;&#22914;&#39640;&#26031;m&#65289;
&lt;/p&gt;
&lt;p&gt;
We consider a novel clustering task in which clusters can have compositional relationships, e.g., one cluster contains images of rectangles, one contains images of circles, and a third (compositional) cluster contains images with both objects. In contrast to hierarchical clustering in which a parent cluster represents the intersection of properties of the child clusters, our problem is about finding compositional clusters that represent the union of the properties of the constituent clusters. This task is motivated by recently developed few-shot learning and embedding models can distinguish the label sets, not just the individual labels, assigned to the examples. We propose three new algorithms -- Compositional Affinity Propagation (CAP), Compositional k-means (CKM), and Greedy Compositional Reassignment (GCR) -- that can partition examples into coherent groups and infer the compositional structure among them. We show promising results, compared to popular algorithms such as Gaussian m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#23398;&#20064;&#26368;&#20248;&#22788;&#26041;&#26641;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#25968;&#25454;&#38543;&#26426;&#21270;&#21644;&#23545;&#26641;&#26377;&#20005;&#26684;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#25216;&#26415;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#20855;&#26377;&#24314;&#27169;&#39046;&#22495;&#29305;&#23450;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2108.13628</link><description>&lt;p&gt;
&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#23398;&#20064;&#26368;&#20248;&#30340;&#22788;&#26041;&#26641;
&lt;/p&gt;
&lt;p&gt;
Learning Optimal Prescriptive Trees from Observational Data. (arXiv:2108.13628v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.13628
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#23398;&#20064;&#26368;&#20248;&#22788;&#26041;&#26641;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#25968;&#25454;&#38543;&#26426;&#21270;&#21644;&#23545;&#26641;&#26377;&#20005;&#26684;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#25216;&#26415;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#20855;&#26377;&#24314;&#27169;&#39046;&#22495;&#29305;&#23450;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#36866;&#24230;&#28145;&#24230;&#30340;&#26368;&#20248;&#22788;&#26041;&#26641;&#65288;&#21363;&#65292;&#20197;&#20108;&#21449;&#26641;&#24418;&#24335;&#34920;&#31034;&#30340;&#21487;&#35299;&#37322;&#30340;&#27835;&#30103;&#20998;&#37197;&#31574;&#30053;&#65289;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#35768;&#22810;&#31038;&#20250;&#37325;&#35201;&#39046;&#22495;&#65288;&#22914;&#20844;&#20849;&#21355;&#29983;&#21644;&#20010;&#24615;&#21270;&#21307;&#23398;&#65289;&#20013;&#26159;&#23384;&#22312;&#30340;&#65292;&#36825;&#20123;&#39046;&#22495;&#20013;&#36890;&#36807;&#34987;&#21160;&#25910;&#38598;&#25968;&#25454;&#26469;&#23547;&#25214;&#22522;&#20110;&#25968;&#25454;&#30340;&#21487;&#35299;&#37322;&#21644;&#25968;&#25454;&#39537;&#21160;&#24178;&#39044;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#38543;&#26426;&#35797;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#65288;MIO&#65289;&#25216;&#26415;&#26469;&#23398;&#20064;&#26368;&#20248;&#22788;&#26041;&#26641;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#28176;&#36817;&#31934;&#30830;&#30340;&#65292;&#21363;&#38543;&#30528;&#21382;&#21490;&#25968;&#25454;&#26679;&#26412;&#30340;&#25968;&#37327;&#36235;&#21521;&#20110;&#26080;&#31351;&#22823;&#65292;&#23427;&#25910;&#25947;&#21040;&#19968;&#20010;&#26368;&#20248;&#30340;&#26679;&#26412;&#22806;&#27835;&#30103;&#20998;&#37197;&#31574;&#30053;&#12290;&#19982;&#29616;&#26377;&#25991;&#29486;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;1&#65289;&#19981;&#38656;&#35201;&#25968;&#25454;&#38543;&#26426;&#21270;&#65292;2&#65289;&#19981;&#23545;&#23398;&#20064;&#21040;&#30340;&#26641;&#26045;&#21152;&#20005;&#26684;&#30340;&#20551;&#35774;&#65292;3&#65289;&#20855;&#26377;&#24314;&#27169;&#39046;&#22495;&#29305;&#23450;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning an optimal prescriptive tree (i.e., an interpretable treatment assignment policy in the form of a binary tree) of moderate depth, from observational data. This problem arises in numerous socially important domains such as public health and personalized medicine, where interpretable and data-driven interventions are sought based on data gathered in deployment -- through passive collection of data -- rather than from randomized trials. We propose a method for learning optimal prescriptive trees using mixed-integer optimization (MIO) technology. We show that under mild conditions our method is asymptotically exact in the sense that it converges to an optimal out-of-sample treatment assignment policy as the number of historical data samples tends to infinity. Contrary to existing literature, our approach: 1) does not require data to be randomized, 2) does not impose stringent assumptions on the learned trees, and 3) has the ability to model domain specif
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#30456;&#20851;&#20998;&#24067;&#19979;&#30340;&#28504;&#22810;&#25289;&#39764;&#30418;&#38382;&#39064;&#65292;&#23558;&#20854;&#19982;&#22343;&#21248;&#20915;&#31574;&#26641;&#21644;&#26368;&#23567;&#21644;&#38598;&#35206;&#30422;&#38382;&#39064;&#36827;&#34892;&#20102;&#36817;&#20284;&#36716;&#21270;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#38382;&#39064;&#37117;&#21487;&#20197;&#22312;&#20122;&#25351;&#25968;&#26102;&#38388;&#20869;&#23454;&#29616;&#24120;&#25968;&#36817;&#20284;&#27604;&#12290;</title><link>http://arxiv.org/abs/2108.12976</link><description>&lt;p&gt;
&#29992;&#30456;&#20851;&#24615;&#36924;&#36817;&#28504;&#22810;&#25289;&#39764;&#30418;
&lt;/p&gt;
&lt;p&gt;
Approximating Pandora's Box with Correlations. (arXiv:2108.12976v3 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.12976
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#30456;&#20851;&#20998;&#24067;&#19979;&#30340;&#28504;&#22810;&#25289;&#39764;&#30418;&#38382;&#39064;&#65292;&#23558;&#20854;&#19982;&#22343;&#21248;&#20915;&#31574;&#26641;&#21644;&#26368;&#23567;&#21644;&#38598;&#35206;&#30422;&#38382;&#39064;&#36827;&#34892;&#20102;&#36817;&#20284;&#36716;&#21270;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#38382;&#39064;&#37117;&#21487;&#20197;&#22312;&#20122;&#25351;&#25968;&#26102;&#38388;&#20869;&#23454;&#29616;&#24120;&#25968;&#36817;&#20284;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#30456;&#20851;&#20998;&#24067;&#19979;&#30340;&#32463;&#20856;&#28504;&#22810;&#25289;&#39764;&#30418;&#65288;PB&#65289;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24471;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38382;&#39064;&#20013;&#25353;&#22266;&#23450;&#39034;&#24207;&#35775;&#38382;&#30418;&#23376;&#30340;&#19968;&#31867;&#38480;&#21046;&#31574;&#30053;&#30340;&#24120;&#25968;&#36817;&#20284;&#31639;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#30340;&#22797;&#26434;&#24615;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#26681;&#25454;&#36804;&#20170;&#20026;&#27490;&#30475;&#21040;&#30340;&#25968;&#20540;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#19979;&#19968;&#20010;&#35201;&#35775;&#38382;&#30340;&#30418;&#23376;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#23558;PB&#19982;&#32463;&#20856;&#30340;&#22343;&#21248;&#20915;&#31574;&#26641;&#65288;UDT&#65289;&#38382;&#39064;&#20197;&#21450;&#21464;&#20307;&#30340;&#26368;&#23567;&#21644;&#38598;&#35206;&#30422;&#65288;MSSC_f&#65289;&#38382;&#39064;&#36827;&#34892;&#20102;&#36817;&#20284;&#36716;&#21270;&#12290;&#23545;&#20110;&#20855;&#26377;&#25903;&#25345;&#24230;$m$&#30340;&#20998;&#24067;&#65292;UDT&#20855;&#26377;$\log m$&#30340;&#36817;&#20284;&#27604;&#65292;&#23613;&#31649;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23454;&#29616;&#24120;&#25968;&#36817;&#20284;&#27604;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#20294;&#22312;&#20122;&#25351;&#25968;&#26102;&#38388;&#20869;&#23454;&#29616;&#24120;&#25968;&#36817;&#20284;&#27604;&#26159;&#21487;&#33021;&#30340;&#65288;arXiv:1906.11385&#65289;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;PB&#21644;MSSC_f&#20063;&#20855;&#26377;&#30456;&#21516;&#30340;&#24615;&#36136;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;c
&lt;/p&gt;
&lt;p&gt;
We revisit the classic Pandora's Box (PB) problem under correlated distributions on the box values. Recent work of arXiv:1911.01632 obtained constant approximate algorithms for a restricted class of policies for the problem that visit boxes in a fixed order. In this work, we study the complexity of approximating the optimal policy which may adaptively choose which box to visit next based on the values seen so far.  Our main result establishes an approximation-preserving equivalence of PB to the well studied Uniform Decision Tree (UDT) problem from stochastic optimization and a variant of the Min-Sum Set Cover ($\text{MSSC}_f$) problem. For distributions of support $m$, UDT admits a $\log m$ approximation, and while a constant factor approximation in polynomial time is a long-standing open problem, constant factor approximations are achievable in subexponential time (arXiv:1906.11385). Our main result implies that the same properties hold for PB and $\text{MSSC}_f$.  We also study the c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25972;&#20307;&#25351;&#23548;&#26041;&#27861;&#26469;&#35299;&#20915;&#36974;&#25377;&#34892;&#20154;&#20877;&#35782;&#21035;&#38382;&#39064;&#65292;&#36890;&#36807;&#21305;&#37197;&#36974;&#25377;&#26679;&#26412;&#19982;&#25972;&#20307;&#26679;&#26412;&#30340;&#36317;&#31163;&#20998;&#24067;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#12290;</title><link>http://arxiv.org/abs/2104.06524</link><description>&lt;p&gt;
&#36974;&#25377;&#34892;&#20154;&#20877;&#35782;&#21035;&#30340;&#25972;&#20307;&#25351;&#23548;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Holistic Guidance for Occluded Person Re-Identification. (arXiv:2104.06524v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.06524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25972;&#20307;&#25351;&#23548;&#26041;&#27861;&#26469;&#35299;&#20915;&#36974;&#25377;&#34892;&#20154;&#20877;&#35782;&#21035;&#38382;&#39064;&#65292;&#36890;&#36807;&#21305;&#37197;&#36974;&#25377;&#26679;&#26412;&#19982;&#25972;&#20307;&#26679;&#26412;&#30340;&#36317;&#31163;&#20998;&#24067;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#35270;&#39057;&#30417;&#25511;&#24212;&#29992;&#20013;&#65292;&#20154;&#21592;&#20877;&#35782;&#21035;&#65288;ReID&#65289;&#21463;&#21040;&#36974;&#25377;&#21644;&#26816;&#27979;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#36974;&#25377;&#20173;&#28982;&#20250;&#30772;&#22351;&#26368;&#20808;&#36827;&#30340;CNN&#39592;&#24178;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#24182;&#22240;&#27492;&#38477;&#20302;&#20102;ReID&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#20351;&#29992;&#39069;&#22806;&#30340;&#26114;&#36149;&#36807;&#31243;&#65292;&#22914;&#23039;&#24577;&#20272;&#35745;&#65292;&#20854;&#20013;&#23039;&#24577;&#22270;&#25552;&#20379;&#30417;&#30563;&#26469;&#25490;&#38500;&#36974;&#25377;&#30340;&#21306;&#22495;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25972;&#20307;&#25351;&#23548;&#65288;HG&#65289;&#26041;&#27861;&#65292;&#21482;&#20381;&#36182;&#20110;&#20154;&#30340;&#36523;&#20221;&#26631;&#31614;&#21644;&#25968;&#25454;&#38598;&#30340;&#37197;&#23545;&#21305;&#37197;&#36317;&#31163;&#20998;&#24067;&#65292;&#20197;&#20943;&#36731;&#36974;&#25377;&#38382;&#39064;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#30417;&#30563;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#23398;&#29983;-&#25945;&#24072;&#26694;&#26550;&#36890;&#36807;&#23558;&#36974;&#25377;&#26679;&#26412;&#30340;&#36317;&#31163;&#20998;&#24067;&#65288;DCD&#65289;&#19982;&#25972;&#20307;&#65288;&#38750;&#36974;&#25377;&#65289;&#26679;&#26412;&#30340;&#36317;&#31163;&#20998;&#24067;&#21305;&#37197;&#26469;&#35757;&#32451;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36974;&#25377;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#21518;&#32773;&#20316;&#20026;&#36719;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world video surveillance applications, person re-identification (ReID) suffers from the effects of occlusions and detection errors. Despite recent advances, occlusions continue to corrupt the features extracted by state-of-art CNN backbones, and thereby deteriorate the accuracy of ReID systems. To address this issue, methods in the literature use an additional costly process such as pose estimation, where pose maps provide supervision to exclude occluded regions. In contrast, we introduce a novel Holistic Guidance (HG) method that relies only on person identity labels, and on the distribution of pairwise matching distances of datasets to alleviate the problem of occlusion, without requiring additional supervision. Hence, our proposed student-teacher framework is trained to address the occlusion problem by matching the distributions of between- and within-class distances (DCDs) of occluded samples with that of holistic (non-occluded) samples, thereby using the latter as a soft l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20849;&#21516;&#27169;&#20223;&#23398;&#20064;&#65288;CoIL&#65289;&#30340;&#26032;&#22411;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20195;&#29702;&#33258;&#36523;&#30340;&#33391;&#22909;&#32463;&#39564;&#65292;&#32780;&#26080;&#38656;&#19987;&#23478;&#31034;&#33539;&#65292;&#26469;&#25913;&#21892;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#37117;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2103.14823</link><description>&lt;p&gt;
&#26080;&#38656;&#19987;&#23478;&#31034;&#33539;&#30340;&#20849;&#21516;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Co-Imitation Learning without Expert Demonstration. (arXiv:2103.14823v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.14823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20849;&#21516;&#27169;&#20223;&#23398;&#20064;&#65288;CoIL&#65289;&#30340;&#26032;&#22411;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20195;&#29702;&#33258;&#36523;&#30340;&#33391;&#22909;&#32463;&#39564;&#65292;&#32780;&#26080;&#38656;&#19987;&#23478;&#31034;&#33539;&#65292;&#26469;&#25913;&#21892;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#37117;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#26159;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#31034;&#33539;&#26469;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#25928;&#29575;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#33719;&#21462;&#19987;&#23478;&#31034;&#33539;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#29978;&#33267;&#19981;&#21487;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20849;&#21516;&#27169;&#20223;&#23398;&#20064;&#65288;CoIL&#65289;&#30340;&#26032;&#22411;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#21033;&#29992;&#20195;&#29702;&#36873;&#25321;&#25506;&#32034;&#29615;&#22659;&#24182;&#21033;&#29992;&#21516;&#20276;&#20195;&#29702;&#30340;&#32463;&#39564;&#12290;&#23613;&#31649;&#36825;&#20123;&#32463;&#39564;&#21487;&#33021;&#26377;&#20215;&#20540;&#65292;&#20063;&#21487;&#33021;&#35823;&#23548;&#20154;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#26399;&#26395;&#22686;&#30410;&#30340;&#20215;&#20540;&#20989;&#25968;&#26469;&#20272;&#35745;&#27599;&#20010;&#32463;&#39564;&#30340;&#28508;&#22312;&#25928;&#29992;&#12290;&#22240;&#27492;&#65292;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#24378;&#35843;&#26356;&#26377;&#29992;&#30340;&#32463;&#39564;&#24182;&#36807;&#28388;&#25481;&#22122;&#22768;&#26469;&#36873;&#25321;&#24615;&#22320;&#20114;&#30456;&#27169;&#20223;&#12290;&#21508;&#31181;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#20849;&#21516;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#30340;&#26174;&#33879;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning is a primary approach to improve the efficiency of reinforcement learning by exploiting the expert demonstrations. However, in many real scenarios, obtaining expert demonstrations could be extremely expensive or even impossible. To overcome this challenge, in this paper, we propose a novel learning framework called Co-Imitation Learning (CoIL) to exploit the past good experiences of the agents themselves without expert demonstration. Specifically, we train two different agents via letting each of them alternately explore the environment and exploit the peer agent's experience. While the experiences could be valuable or misleading, we propose to estimate the potential utility of each piece of experience with the expected gain of the value function. Thus the agents can selectively imitate from each other by emphasizing the more useful experiences while filtering out noisy ones. Experimental results on various tasks show significant superiority of the proposed Co-Imitat
&lt;/p&gt;</description></item><item><title>&#36866;&#24230;&#30417;&#30563;&#23398;&#20064;&#21253;&#25324;&#26631;&#20934;&#26377;&#30417;&#30563;&#23398;&#20064;&#21644;&#24369;&#30417;&#30563;&#23398;&#20064;&#65292;&#24050;&#26377;&#30340;&#19968;&#20123;&#26631;&#20934;&#26377;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#34920;&#26126;&#65292;&#32473;&#23450;&#30340;&#26631;&#31614;&#24182;&#19981;&#23481;&#26131;&#23398;&#20064;&#65292;&#20174;&#32473;&#23450;&#30340;&#26631;&#31614;&#36716;&#25442;&#20026;&#26131;&#20110;&#23398;&#20064;&#30340;&#30446;&#26631;&#30340;&#36807;&#31243;&#20250;&#26174;&#33879;&#24433;&#21709;&#26368;&#32456;&#30340;&#24615;&#33021;&#65292;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;&#23450;&#20041;&#38544;&#34255;&#20102;&#21487;&#33021;&#23545;&#35299;&#20915;&#26041;&#26696;&#24433;&#21709;&#37325;&#22823;&#30340;&#32454;&#33410;&#12290;</title><link>http://arxiv.org/abs/2008.11945</link><description>&lt;p&gt;
&#36866;&#24230;&#30417;&#30563;&#23398;&#20064;&#65306;&#23450;&#20041;&#12289;&#26694;&#26550;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Moderately Supervised Learning: Definition, Framework and Generality. (arXiv:2008.11945v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.11945
&lt;/p&gt;
&lt;p&gt;
&#36866;&#24230;&#30417;&#30563;&#23398;&#20064;&#21253;&#25324;&#26631;&#20934;&#26377;&#30417;&#30563;&#23398;&#20064;&#21644;&#24369;&#30417;&#30563;&#23398;&#20064;&#65292;&#24050;&#26377;&#30340;&#19968;&#20123;&#26631;&#20934;&#26377;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#34920;&#26126;&#65292;&#32473;&#23450;&#30340;&#26631;&#31614;&#24182;&#19981;&#23481;&#26131;&#23398;&#20064;&#65292;&#20174;&#32473;&#23450;&#30340;&#26631;&#31614;&#36716;&#25442;&#20026;&#26131;&#20110;&#23398;&#20064;&#30340;&#30446;&#26631;&#30340;&#36807;&#31243;&#20250;&#26174;&#33879;&#24433;&#21709;&#26368;&#32456;&#30340;&#24615;&#33021;&#65292;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;&#23450;&#20041;&#38544;&#34255;&#20102;&#21487;&#33021;&#23545;&#35299;&#20915;&#26041;&#26696;&#24433;&#21709;&#37325;&#22823;&#30340;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24230;&#30417;&#30563;&#23398;&#20064;&#21253;&#25324;&#26631;&#20934;&#26377;&#30417;&#30563;&#23398;&#20064;&#21644;&#24369;&#30417;&#30563;&#23398;&#20064;&#65292;&#21069;&#32773;&#35757;&#32451;&#25968;&#25454;&#38598;&#26631;&#31614;&#26159;&#29702;&#24819;&#30340;&#65288;&#23436;&#20840;&#12289;&#31934;&#30830;&#21644;&#20934;&#30830;&#65289;&#65292;&#21518;&#32773;&#35757;&#32451;&#25968;&#25454;&#38598;&#26631;&#31614;&#19981;&#23436;&#32654;&#65288;&#19981;&#23436;&#25972;&#65292;&#19981;&#31934;&#30830;&#25110;&#19981;&#20934;&#30830;&#65289;&#12290;&#32463;&#36807;&#22810;&#27425;&#23454;&#39564;&#65292;&#24050;&#26377;&#30340;&#19968;&#20123;&#26631;&#20934;&#26377;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#34920;&#26126;&#65292;&#26377;&#26102;&#20505;&#32473;&#23450;&#30340;&#26631;&#31614;&#24182;&#19981;&#23481;&#26131;&#23398;&#20064;&#65292;&#20174;&#32473;&#23450;&#30340;&#26631;&#31614;&#36716;&#25442;&#20026;&#26131;&#20110;&#23398;&#20064;&#30340;&#30446;&#26631;&#30340;&#36807;&#31243;&#20250;&#26174;&#33879;&#24433;&#21709;&#26368;&#32456;&#26631;&#20934;&#26377;&#30417;&#30563;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22312;&#26410;&#32771;&#34385;&#20174;&#32473;&#23450;&#26631;&#31614;&#21040;&#26131;&#20110;&#23398;&#20064;&#30340;&#30446;&#26631;&#30340;&#36716;&#21270;&#36807;&#31243;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;&#23450;&#20041;&#38544;&#34255;&#20102;&#19968;&#20123;&#21487;&#33021;&#23545;&#24314;&#31435;&#35299;&#20915;&#26041;&#26696;&#24433;&#21709;&#37325;&#22823;&#30340;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning with supervision has achieved remarkable success in numerous artificial intelligence (AI) applications. In the current literature, by referring to the properties of the labels prepared for the training dataset, learning with supervision is categorized as supervised learning (SL) and weakly supervised learning (WSL). SL concerns the situation where the training data set is assigned with ideal (complete, exact and accurate) labels, while WSL concerns the situation where the training data set is assigned with non-ideal (incomplete, inexact or inaccurate) labels. However, various solutions for SL tasks have shown that the given labels are not always easy to learn, and the transformation from the given labels to easy-to-learn targets can significantly affect the performance of the final SL solutions. Without considering the properties of the transformation from the given labels to easy-to-learn targets, the definition of SL conceals some details that can be critical to building the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#19968;&#31181;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#30340;&#20851;&#27880;&#65292;&#24182;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/1912.13122</link><description>&lt;p&gt;
&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Declarative Mechanism Design. (arXiv:1912.13122v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.13122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#19968;&#31181;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#30340;&#20851;&#27880;&#65292;&#24182;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65288;MAS&#65289;&#21644;&#22768;&#26126;&#24615;&#30005;&#23376;&#26426;&#26500;&#65288;DEIs&#65289;&#30340;&#35843;&#25511;&#26159;&#36807;&#21435;&#21313;&#24180;&#28041;&#21450;&#29289;&#29702;&#21644;&#36719;&#20214;&#26234;&#33021;&#20307;&#20197;&#21450;&#27861;&#24459;&#30340;&#22810;&#23398;&#31185;&#30740;&#31350;&#35838;&#39064;&#65292;&#20294;&#36817;&#24180;&#26469;&#36880;&#28176;&#28436;&#21464;&#20026;2016&#24180;&#36215;&#34987;&#31216;&#20026;&#26032;&#38395;&#30340;&#26426;&#22120;&#24459;&#24072;&#12290;&#20854;&#20013;&#19968;&#31181;&#39318;&#27425;&#25552;&#20986;&#38480;&#21046;&#36719;&#20214;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#26041;&#26696;&#26159;&#30005;&#23376;&#26426;&#26500;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#34987;&#37325;&#26032;&#23450;&#20041;&#20026;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#65292;&#26377;&#20851;DL&#20351;&#29992;&#30340;&#23433;&#20840;&#12289;&#38544;&#31169;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#38382;&#39064;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31038;&#21306;&#30340;&#20851;&#27880;&#12290;&#29616;&#22312;&#65292;MAS&#30340;&#35268;&#33539;&#20960;&#20046;&#24471;&#21040;&#27491;&#30830;&#22788;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#33539;&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20043;&#20026;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#65288;INN&#65289;&#12290;&#26412;&#25991;&#30340;&#20027;&#26088;&#26159;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#65288;AT&#65289;&#30340;&#20851;&#27880;&#65292;&#24182;&#32473;&#20986;&#19968;&#20010;&#21021;&#27493;&#30340;&#31572;&#26696;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#35777;&#26126;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regulation of Multi-Agent Systems (MAS) and Declarative Electronic Institutions (DEIs) was a multidisciplinary research topic of the past decade involving (Physical and Software) Agents and Law since the beginning, but recently evolved towards News-claimed Robot Lawyer since 2016. One of these first proposals of restricting the behaviour of Software Agentswas Electronic Institutions.However, with the recent reformulation of Artificial Neural Networks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legal issues regarding the use of DL has raised concerns in the Artificial Intelligence (AI) Community. Now that the Regulation of MAS is almost correctly addressed, we propose the Regulation of Artificial Neural Networks as Agent-based Training of a special type of regulated Artificial Neural Network that we call Institutional Neural Network (INN).The main purpose of this paper is to bring attention to Artificial Teaching (AT) and to give a tentative answer showing a proof-of-con
&lt;/p&gt;</description></item><item><title>Transformer&#26159;&#19968;&#31181;&#26032;&#30340;&#31616;&#21333;&#32593;&#32476;&#26550;&#26500;&#65292;&#23436;&#20840;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21462;&#20195;&#20102;&#22797;&#26434;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#23454;&#39564;&#35777;&#26126;Transformer&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#36136;&#37327;&#26356;&#22909;&#12289;&#24182;&#34892;&#21270;&#25928;&#26524;&#26356;&#20339;&#65292;&#19988;&#35757;&#32451;&#26102;&#38388;&#26356;&#30701;&#12290;&#23427;&#22312;&#33521;&#35793;&#24503;&#21644;&#33521;&#35793;&#27861;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/1706.03762</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#23601;&#26159;&#19968;&#20999;&#65288;arXiv:1706.03762v6 [cs.CL]&#24050;&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Attention Is All You Need. (arXiv:1706.03762v6 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1706.03762
&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#26032;&#30340;&#31616;&#21333;&#32593;&#32476;&#26550;&#26500;&#65292;&#23436;&#20840;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21462;&#20195;&#20102;&#22797;&#26434;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#23454;&#39564;&#35777;&#26126;Transformer&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#36136;&#37327;&#26356;&#22909;&#12289;&#24182;&#34892;&#21270;&#25928;&#26524;&#26356;&#20339;&#65292;&#19988;&#35757;&#32451;&#26102;&#38388;&#26356;&#30701;&#12290;&#23427;&#22312;&#33521;&#35793;&#24503;&#21644;&#33521;&#35793;&#27861;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20027;&#35201;&#30340;&#24207;&#21015;&#36716;&#25442;&#27169;&#22411;&#22522;&#20110;&#22797;&#26434;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#37197;&#32622;&#12290;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#36824;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#36830;&#25509;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31616;&#21333;&#32593;&#32476;&#26550;&#26500;&#65292;Transformer&#65292;&#23436;&#20840;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#19981;&#20877;&#20351;&#29992;&#24490;&#29615;&#21644;&#21367;&#31215;&#12290;&#22312;&#20004;&#20010;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#21516;&#26102;&#26356;&#26131;&#20110;&#24182;&#34892;&#21270;&#65292;&#35757;&#32451;&#26102;&#38388;&#26174;&#33879;&#20943;&#23569;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;WMT 2014&#33521;&#35793;&#24503;&#20219;&#21153;&#19978;&#36798;&#21040;28.4&#30340;BLEU&#20998;&#25968;&#65292;&#27604;&#29616;&#26377;&#26368;&#22909;&#32467;&#26524;&#65288;&#21253;&#25324;&#38598;&#25104;&#27169;&#22411;&#65289;&#25552;&#39640;&#20102;2&#20010;BLEU&#20998;&#12290;&#22312;WMT 2014&#33521;&#35793;&#27861;&#20219;&#21153;&#19978;&#65292;&#22312;8&#20010;GPU&#19978;&#35757;&#32451;&#20102;3.5&#22825;&#21518;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33719;&#24471;&#20102;41.8&#30340;&#21333;&#27169;&#22411;&#26368;&#26032;BLEU&#20998;&#25968;&#65292;&#35757;&#32451;&#25104;&#26412;&#20165;&#20026;&#25991;&#29486;&#20013;&#26368;&#22909;&#27169;&#22411;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Transformer&#26550;&#26500;&#30340;&#20248;&#21183;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transforme
&lt;/p&gt;</description></item></channel></rss>