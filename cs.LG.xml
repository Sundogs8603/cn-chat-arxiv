<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#22312;M-pox&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;COVID-19&#27169;&#22411;&#22312;&#21335;&#38750;&#30123;&#33495;&#29369;&#35947;&#26041;&#38754;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2310.04453</link><description>&lt;p&gt;
COVID-19&#21335;&#38750;&#30123;&#33495;&#29369;&#35947;&#27169;&#22411;&#22312;&#24494;&#35843;M-pox&#25512;&#25991;&#19978;&#23637;&#29616;&#20986;&#24615;&#33021;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
COVID-19 South African Vaccine Hesitancy Models Show Boost in Performance Upon Fine-Tuning on M-pox Tweets. (arXiv:2310.04453v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04453
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;M-pox&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;COVID-19&#27169;&#22411;&#22312;&#21335;&#38750;&#30123;&#33495;&#29369;&#35947;&#26041;&#38754;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;2022&#24180;5&#26376;&#24320;&#22987;&#65292;&#38750;&#22320;&#21306;&#24615;&#22269;&#23478;&#25253;&#21578;&#20102;&#22823;&#37327;M-pox&#30149;&#20363;&#65292;&#35753;&#24456;&#22810;&#20154;&#25285;&#24515;M-pox&#30123;&#24773;&#23558;&#36805;&#36895;&#36716;&#21464;&#20026;&#21478;&#19968;&#20010;&#22823;&#27969;&#34892;&#65292;&#32780;COVID-19&#30123;&#24773;&#20173;&#22312;&#32902;&#34384;&#12290;&#37492;&#20110;M-pox&#19982;COVID-19&#30340;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#36873;&#25321;&#27979;&#35797;&#22312;&#21335;&#38750;Twitter&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;COVID-19&#27169;&#22411;&#22312;&#25163;&#24037;&#26631;&#35760;&#30340;M-pox&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#24494;&#35843;&#21069;&#21644;&#24494;&#35843;&#21518;&#12290;&#36229;&#36807;20k&#26465;&#26469;&#33258;&#21335;&#38750;&#30340;M-pox&#30456;&#20851;&#25512;&#25991;&#34987;&#25163;&#24037;&#26631;&#35760;&#20026;&#31215;&#26497;&#12289;&#28040;&#26497;&#25110;&#20013;&#24615;&#12290;&#22312;&#23558;&#36825;&#20123;COVID-19&#27169;&#22411;&#24494;&#35843;&#21040;M-pox&#25968;&#25454;&#38598;&#21518;&#65292;F1-score&#25552;&#39640;&#20102;&#36229;&#36807;8%&#65292;&#25509;&#36817;70%&#65292;&#20294;&#20173;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#21644;&#20247;&#25152;&#21608;&#30693;&#30340;&#20998;&#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;LDA&#30340;&#20027;&#39064;&#24314;&#27169;&#31243;&#24207;&#23558;&#21407;&#22987;COVID-19 RoBERTa&#27169;&#22411;&#21644;&#20854;&#32463;&#36807;&#24494;&#35843;&#30340;&#29256;&#26412;&#30340;&#38169;&#20998;M-pox&#25512;&#25991;&#36827;&#34892;&#27604;&#36739;&#65292;&#36890;&#36807;&#36825;&#20010;&#20998;&#26512;&#65292;&#25105;&#20204;&#33021;&#24471;&#20986;&#20851;&#20110;&#29369;&#35947;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Very large numbers of M-pox cases have, since the start of May 2022, been reported in non-endemic countries leading many to fear that the M-pox Outbreak would rapidly transition into another pandemic, while the COVID-19 pandemic ravages on. Given the similarities of M-pox with COVID-19, we chose to test the performance of COVID-19 models trained on South African twitter data on a hand-labelled M-pox dataset before and after fine-tuning. More than 20k M-pox-related tweets from South Africa were hand-labelled as being either positive, negative or neutral. After fine-tuning these COVID-19 models on the M-pox dataset, the F1-scores increased by more than 8% falling just short of 70%, but still outperforming state-of-the-art models and well-known classification algorithms. An LDA-based topic modelling procedure was used to compare the miss-classified M-pox tweets of the original COVID-19 RoBERTa model with its fine-tuned version, and from this analysis, we were able to draw conclusions on h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#30701;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#23545;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#12290;&#20197;Twitter&#19978;&#30340;&#27668;&#20505;&#21464;&#21270;&#35805;&#39064;&#20026;&#20363;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;5750&#20010;&#25512;&#25991;&#30340;&#26032;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.04452</link><description>&lt;p&gt;
&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#30701;&#25991;&#26412;&#20998;&#31867;&#65306;&#20197;Twitter&#19978;&#30340;&#27668;&#20505;&#21464;&#21270;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Short text classification with machine learning in the social sciences: The case of climate change on Twitter. (arXiv:2310.04452v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#30701;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#23545;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#12290;&#20197;Twitter&#19978;&#30340;&#27668;&#20505;&#21464;&#21270;&#35805;&#39064;&#20026;&#20363;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;5750&#20010;&#25512;&#25991;&#30340;&#26032;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20998;&#26512;&#22823;&#37327;&#30340;&#25991;&#26412;&#65292;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#32773;&#36234;&#26469;&#36234;&#38754;&#20020;&#25991;&#26412;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;&#24403;&#26080;&#27861;&#36827;&#34892;&#25163;&#21160;&#26631;&#27880;&#26102;&#65292;&#30740;&#31350;&#32773;&#24517;&#39035;&#25214;&#21040;&#33258;&#21160;&#21270;&#20998;&#31867;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#35745;&#31639;&#26426;&#31185;&#23398;&#20026;&#31038;&#20250;&#31185;&#23398;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24037;&#20855;&#31665;&#65292;&#20854;&#24615;&#33021;&#22312;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#26368;&#24120;&#29992;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#20856;&#22411;&#30740;&#31350;&#22330;&#26223;&#65306;&#19968;&#20010;&#30456;&#23545;&#36739;&#23567;&#30340;&#34987;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#24863;&#20852;&#36259;&#30340;&#31867;&#21035;&#24456;&#23569;&#20986;&#29616;&#65292;&#24182;&#19988;&#20316;&#20026;&#19968;&#20010;&#22823;&#22411;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#12290;&#20197;&#27668;&#20505;&#21464;&#21270;&#30340;Twitter&#27807;&#36890;&#20026;&#20363;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#20132;&#21449;&#23398;&#31185;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#30340;&#35805;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;&#21508;&#31181;&#22269;&#38469;&#32452;&#32455;&#30340;5750&#20010;&#20851;&#20110;&#27668;&#20505;&#21464;&#21270;&#36825;&#20010;&#39640;&#24230;&#27169;&#31946;&#27010;&#24565;&#30340;&#25512;&#25991;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
To analyse large numbers of texts, social science researchers are increasingly confronting the challenge of text classification. When manual labeling is not possible and researchers have to find automatized ways to classify texts, computer science provides a useful toolbox of machine-learning methods whose performance remains understudied in the social sciences. In this article, we compare the performance of the most widely used text classifiers by applying them to a typical research scenario in social science research: a relatively small labeled dataset with infrequent occurrence of categories of interest, which is a part of a large unlabeled dataset. As an example case, we look at Twitter communication regarding climate change, a topic of increasing scholarly interest in interdisciplinary social science research. Using a novel dataset including 5,750 tweets from various international organizations regarding the highly ambiguous concept of climate change, we evaluate the performance o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoFT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19982;&#26377;&#23475;&#26597;&#35810;&#22788;&#20110;&#35789;&#27719;-&#35821;&#20041;&#37051;&#22495;&#30340;&#30456;&#20284;&#26597;&#35810;&#19978;&#36827;&#34892;&#20195;&#29702;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#26469;&#25913;&#21892;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#21487;&#20256;&#36882;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04445</link><description>&lt;p&gt;
LoFT: &#29992;&#20110;&#25913;&#36827;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#21487;&#20256;&#36882;&#24615;&#30340;&#26412;&#22320;&#20195;&#29702;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LoFT: Local Proxy Fine-tuning For Improving Transferability Of Adversarial Attacks Against Large Language Model. (arXiv:2310.04445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoFT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19982;&#26377;&#23475;&#26597;&#35810;&#22788;&#20110;&#35789;&#27719;-&#35821;&#20041;&#37051;&#22495;&#30340;&#30456;&#20284;&#26597;&#35810;&#19978;&#36827;&#34892;&#20195;&#29702;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#26469;&#25913;&#21892;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#21487;&#20256;&#36882;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#40784;&#21487;&#20197;&#36890;&#36807;&#38468;&#21152;&#29305;&#21046;&#30340;&#25915;&#20987;&#21518;&#32512;&#21644;&#26377;&#23475;&#26597;&#35810;&#26469;&#35268;&#36991;&#65292;&#20197;&#24341;&#21457;&#26377;&#23475;&#30340;&#21709;&#24212;&#12290;&#20026;&#20102;&#23545;&#26410;&#30693;&#29305;&#24449;&#30340;&#31169;&#26377;&#30446;&#26631;&#27169;&#22411;&#36827;&#34892;&#25915;&#20987;&#65292;&#21487;&#20197;&#20351;&#29992;&#20844;&#20849;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#26469;&#26500;&#24314;&#25915;&#20987;&#65292;&#24182;&#23558;&#25104;&#21151;&#30340;&#25915;&#20987;&#20174;&#20844;&#20849;&#20195;&#29702;&#20256;&#36882;&#21040;&#31169;&#26377;&#30446;&#26631;&#27169;&#22411;&#12290;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#21462;&#20915;&#20110;&#20195;&#29702;&#27169;&#22411;&#33021;&#22815;&#22810;&#22823;&#31243;&#24230;&#19978;&#36924;&#36817;&#31169;&#26377;&#27169;&#22411;&#12290;&#25105;&#20204;&#20551;&#35774;&#65292;&#23545;&#20110;&#25915;&#20987;&#21487;&#20256;&#36882;&#24615;&#26469;&#35828;&#65292;&#21482;&#35201;&#20195;&#29702;&#33021;&#22815;&#22312;&#26377;&#23475;&#26597;&#35810;&#30340;&#35789;&#27719;-&#35821;&#20041;&#37051;&#22495;&#20869;&#36924;&#36817;&#30446;&#26631;&#27169;&#22411;&#21363;&#21487;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26412;&#22320;&#24494;&#35843;&#65288;LoFT&#65289;&#8221;&#65292;&#21363;&#22312;&#19982;&#26377;&#23475;&#26597;&#35810;&#22788;&#20110;&#35789;&#27719;-&#35821;&#20041;&#37051;&#22495;&#30340;&#30456;&#20284;&#26597;&#35810;&#19978;&#36827;&#34892;&#20195;&#29702;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#20197;&#20943;&#23567;&#20195;&#29702;&#21644;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#19977;&#31181;&#20419;&#20351;&#31169;&#26377;&#30446;&#26631;&#27169;&#22411;&#21464;&#24471;&#26131;&#21463;&#25915;&#20987;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been shown that Large Language Model (LLM) alignments can be circumvented by appending specially crafted attack suffixes with harmful queries to elicit harmful responses. To conduct attacks against private target models whose characterization is unknown, public models can be used as proxies to fashion the attack, with successful attacks being transferred from public proxies to private target models. The success rate of attack depends on how closely the proxy model approximates the private model. We hypothesize that for attacks to be transferrable, it is sufficient if the proxy can approximate the target model in the neighborhood of the harmful query. Therefore, in this paper, we propose \emph{Local Fine-Tuning (LoFT)}, \textit{i.e.}, fine-tuning proxy models on similar queries that lie in the lexico-semantic neighborhood of harmful queries to decrease the divergence between the proxy and target models. First, we demonstrate three approaches to prompt private target models to obt
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04444</link><description>&lt;p&gt;
&#39764;&#27861;&#35789;&#26159;&#20160;&#20040;&#65311;LLM&#25552;&#31034;&#30340;&#25511;&#21046;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
What's the Magic Word? A Control Theory of LLM Prompting. (arXiv:2310.04444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#22312;LLM&#30340;&#37096;&#32626;&#20013;&#26159;&#26377;&#25928;&#21644;&#37325;&#35201;&#30340;&#65292;&#20294;&#22312;&#25968;&#23398;&#19978;&#29702;&#35299;&#19981;&#36275;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#25552;&#31034;&#34987;&#35748;&#20026;&#26159;&#35843;&#33410;LLM&#36755;&#20986;&#20998;&#24067;&#30340;&#25511;&#21046;&#21464;&#37327;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;token&#24207;&#21015;&#65292;&#26159;&#21542;&#24635;&#23384;&#22312;&#19968;&#20010;&#25105;&#20204;&#21487;&#20197;&#28155;&#21152;&#30340;&#25552;&#31034;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65311;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#26368;&#20248;&#25552;&#31034;&#31216;&#20026;&#39764;&#27861;&#35789;&#65292;&#22240;&#20026;&#28155;&#21152;&#25552;&#31034;&#20250;&#23548;&#33268;LLM&#36755;&#20986;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#22914;&#26524;&#23384;&#22312;&#39764;&#27861;&#35789;&#65292;&#25105;&#20204;&#33021;&#21542;&#25214;&#21040;&#23427;&#20204;&#65311;&#22914;&#26524;&#21487;&#20197;&#65292;&#23427;&#20204;&#30340;&#29305;&#24615;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#25552;&#20379;&#20102;&#23558;&#25511;&#21046;&#29702;&#35770;&#24212;&#29992;&#20110;&#33258;&#27880;&#24847;&#21147;&#22836;&#30340;&#20998;&#26512;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#20854;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#20989;&#25968;&#20026;&#21487;&#25511;&#21046;&#24615;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#20511;&#37492;&#25511;&#21046;&#29702;&#35770;&#26469;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;$k-\epsilon$&#21487;&#25511;&#21046;&#24615;&#30340;&#25351;&#26631;&#65292;&#29992;&#20110;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt engineering is effective and important in the deployment of LLMs but is poorly understood mathematically. Here, we formalize prompt engineering as an optimal control problem on LLMs -- where the prompt is considered a control variable for modulating the output distribution of the LLM. Within this framework, we ask a simple question: given a sequence of tokens, does there always exist a prompt we can prepend that will steer the LLM toward accurately predicting the final token? We call such an optimal prompt the magic word since prepending the prompt causes the LLM to output the correct answer. If magic words exist, can we find them? If so, what are their properties? We offer analytic analysis on the controllability of the self-attention head where we prove a bound on controllability as a function of the singular values of its weight matrices. We take inspiration from control theory to propose a metric called $k-\epsilon$ controllability to characterize LLM steerability. We comput
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#37327;&#33021;&#22120;&#20998;&#21106;&#23545;&#33021;&#37327;&#37325;&#24314;&#30340;&#24433;&#21709;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#37325;&#24314;&#65292;&#24182;&#21457;&#29616;&#30456;&#23545;&#32454;&#33268;&#30340;&#32437;&#21521;&#20998;&#21106;&#23545;&#20110;&#33021;&#37327;&#20272;&#35745;&#33267;&#20851;&#37325;&#35201;&#65292;&#25552;&#20379;&#20102;EIC&#25506;&#27979;&#22120;&#20248;&#21270;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2310.04442</link><description>&lt;p&gt;
&#20998;&#21106;&#24615;&#33021;&#29992;&#20110;&#37319;&#26679;&#37327;&#33021;&#22120;&#30340;&#26368;&#20339;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Optimal use of Segmentation for Sampling Calorimeters. (arXiv:2310.04442v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04442
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#37327;&#33021;&#22120;&#20998;&#21106;&#23545;&#33021;&#37327;&#37325;&#24314;&#30340;&#24433;&#21709;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#37325;&#24314;&#65292;&#24182;&#21457;&#29616;&#30456;&#23545;&#32454;&#33268;&#30340;&#32437;&#21521;&#20998;&#21106;&#23545;&#20110;&#33021;&#37327;&#20272;&#35745;&#33267;&#20851;&#37325;&#35201;&#65292;&#25552;&#20379;&#20102;EIC&#25506;&#27979;&#22120;&#20248;&#21270;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#20309;&#37319;&#26679;&#37327;&#33021;&#22120;&#30340;&#20851;&#38190;&#35774;&#35745;&#36873;&#25321;&#20043;&#19968;&#26159;&#32437;&#21521;&#21644;&#27178;&#21521;&#20998;&#21106;&#30340;&#23494;&#24230;&#12290;&#20026;&#20102;&#30830;&#23450;&#36825;&#20010;&#36873;&#25321;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#37327;&#33021;&#22120;&#20998;&#21106;&#23545;&#33021;&#37327;&#37325;&#24314;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#30830;&#20445;&#36235;&#21183;&#23436;&#20840;&#30001;&#30828;&#20214;&#20915;&#23450;&#65292;&#32780;&#19981;&#26159;&#30001;&#20110;&#20998;&#21106;&#30340;&#27425;&#20248;&#24212;&#29992;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#37325;&#24314;&#12290;&#36825;&#20123;&#32593;&#32476;&#23558;&#37327;&#33021;&#22120;&#34920;&#31034;&#20026;&#19968;&#20010;&#28857;&#20113;&#65292;&#24182;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#27169;&#25311;&#20102;&#19968;&#20010;&#31867;&#20284;&#20110;&#21069;&#21521;&#37327;&#33021;&#22120;&#31995;&#32479;&#30340;&#25506;&#27979;&#22120;&#65292;&#35813;&#31995;&#32479;&#26088;&#22312;&#29992;&#20110;&#21363;&#23558;&#21040;&#26469;&#30340;&#30005;&#23376;&#31163;&#23376;&#23545;&#25758;&#26426;&#19978;&#30340;ePIC&#25506;&#27979;&#22120;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#23396;&#31435;&#24102;&#30005;&#960;&#20171;&#23376;&#31751;&#30340;&#33021;&#37327;&#20272;&#35745;&#65292;&#30456;&#23545;&#32454;&#33268;&#30340;&#32437;&#21521;&#20998;&#21106;&#23545;&#20110;&#22312;&#25972;&#20010;&#30456;&#31354;&#38388;&#20869;&#23454;&#29616;&#33021;&#37327;&#20998;&#36776;&#29575;&#20248;&#20110;10%&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;&#27491;&#22312;&#36827;&#34892;&#30340;EIC&#25506;&#27979;&#22120;&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#22522;&#20934;&#65292;&#24182;&#19988;&#36824;&#21487;&#33021;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the key design choices of any sampling calorimeter is how fine to make the longitudinal and transverse segmentation. To inform this choice, we study the impact of calorimeter segmentation on energy reconstruction. To ensure that the trends are due entirely to hardware and not to a sub-optimal use of segmentation, we deploy deep neural networks to perform the reconstruction. These networks make use of all available information by representing the calorimeter as a point cloud. To demonstrate our approach, we simulate a detector similar to the forward calorimeter system intended for use in the ePIC detector, which will operate at the upcoming Electron Ion Collider. We find that for the energy estimation of isolated charged pion showers, relatively fine longitudinal segmentation is key to achieving an energy resolution that is better than 10% across the full phase space. These results provide a valuable benchmark for ongoing EIC detector optimizations and may also inform future stud
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#32447;&#24615;&#22270;&#20687;&#21453;&#28436;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#27969;&#27169;&#22411;&#65292;&#22312;&#20943;&#23569;&#25163;&#21160;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36870;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04432</link><description>&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#30340;&#32447;&#24615;&#22270;&#20687;&#21453;&#28436;&#26041;&#27861;&#65306;&#36890;&#36807;&#27969;&#36827;&#34892;
&lt;/p&gt;
&lt;p&gt;
Training-free Linear Image Inversion via Flows. (arXiv:2310.04432v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04432
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#32447;&#24615;&#22270;&#20687;&#21453;&#28436;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#27969;&#27169;&#22411;&#65292;&#22312;&#20943;&#23569;&#25163;&#21160;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36870;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#30340;&#32447;&#24615;&#21453;&#28436;&#26041;&#27861;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23545;&#29983;&#25104;&#36807;&#31243;&#30340;&#36866;&#24403;&#20462;&#25913;&#26469;&#35299;&#20915;&#36870;&#38382;&#39064;&#65292;&#32780;&#26080;&#38656;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35843;&#20248;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#20808;&#21069;&#26041;&#27861;&#24050;&#32463;&#25506;&#32034;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#20294;&#20173;&#38656;&#35201;&#25163;&#21160;&#35843;&#25972;&#35768;&#22810;&#36229;&#21442;&#25968;&#26469;&#24212;&#23545;&#19981;&#21516;&#30340;&#36870;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#27969;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#21453;&#28436;&#30340;&#26080;&#38656;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#27969;&#21305;&#37197;&#27169;&#22411;&#30340;&#31616;&#27905;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#20351;&#29992;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#21152;&#26435;&#26041;&#26696;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#25163;&#21160;&#35843;&#25972;&#30340;&#24037;&#20316;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#20027;&#35201;&#28304;&#22836;&#27762;&#21462;&#28789;&#24863;&#65306;&#23558;&#20808;&#21069;&#30340;&#26799;&#24230;&#26657;&#27491;&#26041;&#27861;&#24212;&#29992;&#20110;&#27969;&#39046;&#22495;&#65292;&#20197;&#21450;&#22522;&#20110;&#26465;&#20214;&#26368;&#20248;&#20256;&#36755;&#36335;&#24452;&#30340;&#27714;&#35299;&#22120;&#26041;&#26696;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#24191;&#27867;&#21487;&#29992;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#25193;&#25955;&#27169;&#22411;&#23454;&#38469;&#24212;&#29992;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#36870;&#38382;&#39064;&#19978;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training-free linear inversion involves the use of a pretrained generative model and -- through appropriate modifications to the generation process -solving inverse problems without any finetuning of the generative model. While recent prior methods have explored the use of diffusion models, they still require the manual tuning of many hyperparameters for different inverse problems. In this work, we propose a training-free method for image inversion using pretrained flow models, leveraging the simplicity and efficiency of Flow Matching models, using theoretically-justified weighting schemes and thereby significantly reducing the amount of manual tuning. In particular, we draw inspiration from two main sources: adopting prior gradient correction methods to the flow regime, and a solver scheme based on conditional Optimal Transport paths. As pretrained diffusion models are widely accessible, we also show how to practically adapt diffusion models for our method. Empirically, our approach
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#27604;&#36739;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#25968;&#23383;&#39057;&#29575;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.04431</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#33021;&#21542;&#35745;&#31639;&#25968;&#23383;&#39057;&#29575;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can neural networks count digit frequency?. (arXiv:2310.04431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#27604;&#36739;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#25968;&#23383;&#39057;&#29575;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#27604;&#36739;&#19981;&#21516;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#32473;&#23450;&#25968;&#23383;&#20013;&#27599;&#20010;&#25968;&#23383;&#20986;&#29616;&#39057;&#29575;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#23427;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#26377;&#22810;&#31181;&#24212;&#29992;&#65292;&#20363;&#22914;&#22312;&#35270;&#35273;&#22330;&#26223;&#20013;&#33719;&#21462;&#30446;&#26631;&#23545;&#35937;&#30340;&#39057;&#29575;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#35270;&#20026;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;&#28151;&#21512;&#12290;&#25105;&#20204;&#31934;&#24515;&#21019;&#24314;&#20102;&#33258;&#24049;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#35266;&#23519;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#31995;&#32479;&#24046;&#24322;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#25351;&#26631;&#35780;&#20272;&#27599;&#31181;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#25152;&#20351;&#29992;&#30340;&#24615;&#33021;&#25351;&#26631;&#21253;&#25324;&#22238;&#24402;&#35780;&#20272;&#20013;&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65292;&#20197;&#21450;&#20998;&#31867;&#24615;&#33021;&#35780;&#20272;&#20013;&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20915;&#31574;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#20250;&#36807;&#25311;&#21512;&#21040;&#25968;&#25454;&#38598;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#22266;&#26377;&#20559;&#24046;&#65292;&#26080;&#27861;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#31070;&#32463;&#32593;&#32476;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research, we aim to compare the performance of different classical machine learning models and neural networks in identifying the frequency of occurrence of each digit in a given number. It has various applications in machine learning and computer vision, e.g. for obtaining the frequency of a target object in a visual scene. We considered this problem as a hybrid of classification and regression tasks. We carefully create our own datasets to observe systematic differences between different methods. We evaluate each of the methods using different metrics across multiple datasets.The metrics of performance used were the root mean squared error and mean absolute error for regression evaluation, and accuracy for classification performance evaluation. We observe that decision trees and random forests overfit to the dataset, due to their inherent bias, and are not able to generalize well. We also observe that the neural networks significantly outperform the classical machine learning
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#22320;&#34920;&#37325;&#21147;&#19982;&#22320;&#38663;&#25968;&#25454;&#32852;&#21512;&#21453;&#28436;&#21487;&#20197;&#26377;&#25928;&#30417;&#27979;CO2&#23384;&#20648;&#65292;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#23494;&#24230;&#21644;&#36895;&#24230;&#37325;&#24314;&#12289;&#20934;&#30830;&#30340;&#20998;&#21106;&#21644;&#26356;&#39640;&#30340;R&#26041;&#31995;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.04430</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#22320;&#34920;&#37325;&#21147;&#19982;&#22320;&#38663;&#25968;&#25454;&#32852;&#21512;&#21453;&#28436;&#30417;&#27979;3D CO2&#26001;&#22359;
&lt;/p&gt;
&lt;p&gt;
Joint inversion of Time-Lapse Surface Gravity and Seismic Data for Monitoring of 3D CO$_2$ Plumes via Deep Learning. (arXiv:2310.04430v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04430
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#22320;&#34920;&#37325;&#21147;&#19982;&#22320;&#38663;&#25968;&#25454;&#32852;&#21512;&#21453;&#28436;&#21487;&#20197;&#26377;&#25928;&#30417;&#27979;CO2&#23384;&#20648;&#65292;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#23494;&#24230;&#21644;&#36895;&#24230;&#37325;&#24314;&#12289;&#20934;&#30830;&#30340;&#20998;&#21106;&#21644;&#26356;&#39640;&#30340;R&#26041;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23436;&#20840;&#19977;&#32500;&#32852;&#21512;&#21453;&#28436;&#26041;&#27861;&#65292;&#29992;&#20110;&#37325;&#24314;&#22320;&#19979;&#23494;&#24230;&#21644;&#36895;&#24230;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#30340;&#30446;&#26631;&#24212;&#29992;&#26159;&#39044;&#27979;&#22320;&#19979;CO2&#26001;&#22359;&#65292;&#20316;&#20026;&#30417;&#27979;CO2&#23553;&#23384;&#37096;&#32626;&#30340;&#34917;&#20805;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#32852;&#21512;&#21453;&#28436;&#25216;&#26415;&#20248;&#20110;&#20165;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#37325;&#21147;&#25110;&#22320;&#38663;&#21453;&#28436;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#23494;&#24230;&#21644;&#36895;&#24230;&#37325;&#24314;&#12289;&#20934;&#30830;&#30340;&#20998;&#21106;&#21644;&#26356;&#39640;&#30340;R&#26041;&#31995;&#25968;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32852;&#21512;&#21453;&#28436;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;CO2&#23384;&#20648;&#30417;&#27979;&#24037;&#20855;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#23558;&#30528;&#37325;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#26356;&#22823;&#25968;&#25454;&#38598;&#12289;&#27169;&#25311;&#20854;&#20182;&#22320;&#36136;&#23553;&#23384;&#22320;&#28857;&#30340;&#24773;&#20917;&#65292;&#26368;&#32456;&#39564;&#35777;&#37326;&#22806;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a fully 3D, deep learning-based approach for the joint inversion of time-lapse surface gravity and seismic data for reconstructing subsurface density and velocity models. The target application of this proposed inversion approach is the prediction of subsurface CO2 plumes as a complementary tool for monitoring CO2 sequestration deployments. Our joint inversion technique outperforms deep learning-based gravity-only and seismic-only inversion models, achieving improved density and velocity reconstruction, accurate segmentation, and higher R-squared coefficients. These results indicate that deep learning-based joint inversion is an effective tool for CO$_2$ storage monitoring. Future work will focus on validating our approach with larger datasets, simulations with other geological storage sites, and ultimately field data.
&lt;/p&gt;</description></item><item><title>NetDiffus&#26159;&#19968;&#20010;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#32593;&#32476;&#27969;&#37327;&#25968;&#25454;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22312;&#29983;&#25104;&#25968;&#25454;&#30340;&#20445;&#30495;&#24230;&#21644;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2310.04429</link><description>&lt;p&gt;
NetDiffus&#65306;&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;&#25104;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#32593;&#32476;&#27969;&#37327;
&lt;/p&gt;
&lt;p&gt;
NetDiffus: Network Traffic Generation by Diffusion Models through Time-Series Imaging. (arXiv:2310.04429v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04429
&lt;/p&gt;
&lt;p&gt;
NetDiffus&#26159;&#19968;&#20010;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#32593;&#32476;&#27969;&#37327;&#25968;&#25454;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22312;&#29983;&#25104;&#25968;&#25454;&#30340;&#20445;&#30495;&#24230;&#21644;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#25968;&#25454;&#20998;&#26512;&#29616;&#22312;&#26159;&#20960;&#20046;&#27599;&#20010;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#30340;&#26680;&#24515;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#20195;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#12289;&#21830;&#19994;&#25935;&#24863;&#24615;&#12289;&#38544;&#31169;&#21644;&#30417;&#31649;&#38480;&#21046;&#31561;&#22810;&#31181;&#21407;&#22240;&#65292;&#33719;&#24471;&#32593;&#32476;&#25968;&#25454;&#30340;&#35775;&#38382;&#21463;&#21040;&#20102;&#38271;&#26399;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#26469;&#29983;&#25104;&#21512;&#25104;&#32593;&#32476;&#27969;&#37327;&#25968;&#25454;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26694;&#26550;NetDiffus&#65292;&#35813;&#26694;&#26550;&#39318;&#20808;&#23558;&#19968;&#32500;&#26102;&#38388;&#24207;&#21015;&#32593;&#32476;&#27969;&#37327;&#36716;&#25442;&#20026;&#20108;&#32500;&#22270;&#20687;&#65292;&#28982;&#21518;&#21512;&#25104;&#21407;&#22987;&#25968;&#25454;&#30340;&#20195;&#34920;&#24615;&#22270;&#20687;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;NetDiffus&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#20445;&#30495;&#24230;&#26041;&#38754;&#27604;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#26368;&#20808;&#36827;&#30340;&#27969;&#37327;&#29983;&#25104;&#26041;&#27861;&#25552;&#20379;&#20102;66.4%&#30340;&#25552;&#39640;&#65292;&#24182;&#25552;&#39640;&#20102;18.1%&#30340;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#19971;&#20010;&#19981;&#21516;&#30340;&#27969;&#37327;&#36319;&#36394;&#19978;&#35780;&#20272;&#20102;NetDiffus&#65292;&#24182;&#26174;&#31034;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27969;&#37327;&#20998;&#26512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network data analytics are now at the core of almost every networking solution. Nonetheless, limited access to networking data has been an enduring challenge due to many reasons including complexity of modern networks, commercial sensitivity, privacy and regulatory constraints. In this work, we explore how to leverage recent advancements in Diffusion Models (DM) to generate synthetic network traffic data. We develop an end-to-end framework NetDiffus that first converts one-dimensional time-series network traffic into two-dimensional images, and then synthesizes representative images for the original data. We demonstrate that NetDiffus outperforms the state-of-the-art traffic generation methods based on Generative Adversarial Networks (GANs) by providing 66.4% increase in fidelity of the generated data and 18.1% increase in downstream machine learning tasks. We evaluate NetDiffus on seven diverse traffic traces and show that utilizing synthetic data significantly improves traffic fing
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25351;&#20986;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#22312;&#24314;&#31569;&#34892;&#19994;&#20013;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#30340;&#30693;&#35782;&#31354;&#30333;&#65292;&#24182;&#24378;&#35843;&#20102;GenAI&#26089;&#26399;&#37319;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04427</link><description>&lt;p&gt;
&#24314;&#31569;&#34892;&#19994;&#20013;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65306;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Generative AI in the Construction Industry: Opportunities &amp; Challenges. (arXiv:2310.04427v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04427
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25351;&#20986;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#22312;&#24314;&#31569;&#34892;&#19994;&#20013;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#30340;&#30693;&#35782;&#31354;&#30333;&#65292;&#24182;&#24378;&#35843;&#20102;GenAI&#26089;&#26399;&#37319;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#34429;&#28982;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#25913;&#21464;&#20102;&#35768;&#22810;&#34892;&#19994;&#30340;&#20570;&#27861;&#65292;&#20294;&#24314;&#31569;&#34892;&#19994;&#30340;&#37319;&#29992;&#36828;&#36828;&#28382;&#21518;&#12290;&#26368;&#36817;&#65292;&#20687;OpenAI&#30340;GPT&#12289;Google&#30340;PaLM&#21644;Meta&#30340;Llama&#36825;&#26679;&#30340;&#20808;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#21644;&#36805;&#36895;&#37319;&#29992;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#24182;&#24341;&#36215;&#20102;&#20840;&#29699;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#28608;&#22686;&#32570;&#20047;&#30740;&#31350;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#22312;&#24314;&#31569;&#39046;&#22495;&#23454;&#26045;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#21019;&#24314;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#30693;&#35782;&#31354;&#30333;&#12290;&#36825;&#20984;&#26174;&#20102;&#25506;&#32034;GenAI&#25972;&#21512;&#21069;&#26223;&#21644;&#22797;&#26434;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#23545;&#20110;&#20248;&#21270;&#24314;&#31569;&#34892;&#19994;&#26089;&#26399;&#37319;&#29992;GenAI&#33267;&#20851;&#37325;&#35201;&#12290;&#37492;&#20110;GenAI&#26681;&#25454;&#23545;&#29616;&#26377;&#20869;&#23481;&#30340;&#23398;&#20064;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#20869;&#23481;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#20004;&#20010;&#25351;&#23548;&#24615;&#38382;&#39064;&#65306;GenAI&#22312;&#24314;&#31569;&#34892;&#19994;&#23558;&#20250;&#24102;&#26469;&#20160;&#20040;&#26410;&#26469;&#65311;
&lt;/p&gt;
&lt;p&gt;
In the last decade, despite rapid advancements in artificial intelligence (AI) transforming many industry practices, construction largely lags in adoption. Recently, the emergence and rapid adoption of advanced large language models (LLM) like OpenAI's GPT, Google's PaLM, and Meta's Llama have shown great potential and sparked considerable global interest. However, the current surge lacks a study investigating the opportunities and challenges of implementing Generative AI (GenAI) in the construction sector, creating a critical knowledge gap for researchers and practitioners. This underlines the necessity to explore the prospects and complexities of GenAI integration. Bridging this gap is fundamental to optimizing GenAI's early-stage adoption within the construction sector. Given GenAI's unprecedented capabilities to generate human-like content based on learning from existing content, we reflect on two guiding questions: What will the future bring for GenAI in the construction industry?
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32418;&#38431;&#26041;&#27861;&#27169;&#25311;&#28508;&#22312;&#30340;&#32593;&#32476;&#25915;&#20987;&#65292;&#35780;&#20272;&#20102;&#29983;&#25104;AI/NLP&#27169;&#22411;&#21644;BB84&#37327;&#23376;&#23494;&#30721;&#21327;&#35758;&#30340;&#20581;&#22766;&#24615;&#65292;&#24182;&#26088;&#22312;&#23558;&#29702;&#35770;&#27934;&#23519;&#21147;&#36716;&#21270;&#20026;&#21487;&#34892;&#30340;&#32593;&#32476;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.04425</link><description>&lt;p&gt;
Red Teaming&#29983;&#25104;AI/NLP&#12289;BB84&#37327;&#23376;&#23494;&#30721;&#21327;&#35758;&#21644;NIST&#35748;&#21487;&#30340;&#25239;&#37327;&#23376;&#25915;&#20987;&#21152;&#23494;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Red Teaming Generative AI/NLP, the BB84 quantum cryptography protocol and the NIST-approved Quantum-Resistant Cryptographic Algorithms. (arXiv:2310.04425v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32418;&#38431;&#26041;&#27861;&#27169;&#25311;&#28508;&#22312;&#30340;&#32593;&#32476;&#25915;&#20987;&#65292;&#35780;&#20272;&#20102;&#29983;&#25104;AI/NLP&#27169;&#22411;&#21644;BB84&#37327;&#23376;&#23494;&#30721;&#21327;&#35758;&#30340;&#20581;&#22766;&#24615;&#65292;&#24182;&#26088;&#22312;&#23558;&#29702;&#35770;&#27934;&#23519;&#21147;&#36716;&#21270;&#20026;&#21487;&#34892;&#30340;&#32593;&#32476;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20195;&#25968;&#23383;&#26102;&#20195;&#65292;&#37327;&#23376;&#35745;&#31639;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#34701;&#21512;&#27491;&#22312;&#37325;&#22609;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#65292;&#24341;&#20837;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#21644;&#28508;&#22312;&#28431;&#27934;&#12290;&#36825;&#39033;&#30740;&#31350;&#21382;&#26102;&#20116;&#24180;&#65292;&#25506;&#35752;&#20102;&#36825;&#31181;&#34701;&#21512;&#23545;&#32593;&#32476;&#23433;&#20840;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#20851;&#27880;AI/&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#21644;&#37327;&#23376;&#23494;&#30721;&#21327;&#35758;&#65292;&#23588;&#20854;&#26159;BB84&#26041;&#27861;&#21644;&#29305;&#23450;&#30340;NIST&#35748;&#21487;&#31639;&#27861;&#12290;&#30740;&#31350;&#37319;&#29992;Python&#21644;C++&#20316;&#20026;&#20027;&#35201;&#35745;&#31639;&#24037;&#20855;&#65292;&#37319;&#29992;&#8220;&#32418;&#38431;&#8221;&#26041;&#27861;&#36827;&#34892;&#27169;&#25311;&#28508;&#22312;&#30340;&#32593;&#32476;&#25915;&#20987;&#65292;&#35780;&#20272;&#37327;&#23376;&#23433;&#20840;&#25514;&#26045;&#30340;&#20581;&#22766;&#24615;&#12290;&#21021;&#27493;&#30740;&#31350;&#25345;&#32493;&#20102;12&#20010;&#26376;&#65292;&#20026;&#26412;&#30740;&#31350;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#26088;&#22312;&#23558;&#29702;&#35770;&#27934;&#23519;&#21147;&#36716;&#21270;&#20026;&#21487;&#34892;&#30340;&#29616;&#23454;&#19990;&#30028;&#30340;&#32593;&#32476;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#30740;&#31350;&#20301;&#20110;&#29275;&#27941;&#22823;&#23398;&#25216;&#26415;&#22253;&#21306;&#65292;&#25317;&#26377;&#20808;&#36827;&#30340;&#22522;&#30784;&#35774;&#26045;&#21644;&#20016;&#23500;&#30340;&#21512;&#20316;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the contemporary digital age, Quantum Computing and Artificial Intelligence (AI) convergence is reshaping the cyber landscape, introducing unprecedented opportunities and potential vulnerabilities.This research, conducted over five years, delves into the cybersecurity implications of this convergence, with a particular focus on AI/Natural Language Processing (NLP) models and quantum cryptographic protocols, notably the BB84 method and specific NIST-approved algorithms. Utilising Python and C++ as primary computational tools, the study employs a "red teaming" approach, simulating potential cyber-attacks to assess the robustness of quantum security measures. Preliminary research over 12 months laid the groundwork, which this study seeks to expand upon, aiming to translate theoretical insights into actionable, real-world cybersecurity solutions. Located at the University of Oxford's technology precinct, the research benefits from state-of-the-art infrastructure and a rich collaborative
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#22522;&#22240;&#35843;&#25511;&#31070;&#32463;&#32593;&#32476;&#65288;GRNN&#65289;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#65292;&#29992;&#20110;&#31283;&#23450;&#24615;&#20998;&#26512;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#24212;&#29992;&#22312;&#29983;&#29289;&#20154;&#24037;&#26234;&#33021;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.04424</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#22240;&#35843;&#25511;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#31283;&#23450;&#24615;&#20998;&#26512;&#22312;&#29983;&#29289;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Stability Analysis of Non-Linear Classifiers using Gene Regulatory Neural Network for Biological AI. (arXiv:2310.04424v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#22522;&#22240;&#35843;&#25511;&#31070;&#32463;&#32593;&#32476;&#65288;GRNN&#65289;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#65292;&#29992;&#20110;&#31283;&#23450;&#24615;&#20998;&#26512;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#24212;&#29992;&#22312;&#29983;&#29289;&#20154;&#24037;&#26234;&#33021;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#32454;&#32990;&#30340;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#65288;GRN&#65289;&#31649;&#29702;&#30528;&#35768;&#22810;&#20851;&#38190;&#21151;&#33021;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#36866;&#24212;&#21644;&#22312;&#19981;&#21516;&#29615;&#22659;&#26465;&#20214;&#19979;&#29983;&#23384;&#12290;&#23545;GRN&#30340;&#20180;&#32454;&#35266;&#23519;&#34920;&#26126;&#65292;&#20854;&#32467;&#26500;&#21644;&#25805;&#20316;&#21407;&#21017;&#31867;&#20284;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#65292;&#20026;&#29983;&#29289;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#22522;&#22240;&#30340;&#36716;&#24405;&#21644;&#32763;&#35793;&#36807;&#31243;&#20013;&#65292;&#22522;&#20110;&#36716;&#24405;&#22240;&#23376;&#36755;&#20837;&#30340;&#29305;&#24615;&#31867;&#20284;&#20110;S&#22411;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21452;&#23618;&#36716;&#24405;-&#32763;&#35793;&#21270;&#23398;&#21453;&#24212;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#22240;-&#24863;&#30693;&#22120;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;GRN&#36716;&#25442;&#20026;&#22522;&#22240;&#35843;&#25511;&#31070;&#32463;&#32593;&#32476;&#65288;GRNN&#65289;&#12290;&#25105;&#20204;&#23545;&#23436;&#20840;&#36830;&#25509;&#30340;GRNN&#23376;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#22522;&#22240;-&#24863;&#30693;&#22120;&#36827;&#34892;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#20250;&#20135;&#29983;&#21487;&#38752;&#35745;&#31639;&#24615;&#33021;&#30340;&#26102;&#38388;&#21644;&#31283;&#23450;&#27987;&#24230;&#36755;&#20986;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Gene Regulatory Network (GRN) of biological cells governs a number of key functionalities that enables them to adapt and survive through different environmental conditions. Close observation of the GRN shows that the structure and operational principles resembles an Artificial Neural Network (ANN), which can pave the way for the development of Biological Artificial Intelligence. In particular, a gene's transcription and translation process resembles a sigmoidal-like property based on transcription factor inputs. In this paper, we develop a mathematical model of gene-perceptron using a dual-layered transcription-translation chemical reaction model, enabling us to transform a GRN into a Gene Regulatory Neural Network (GRNN). We perform stability analysis for each gene-perceptron within the fully-connected GRNN sub network to determine temporal as well as stable concentration outputs that will result in reliable computing performance. We focus on a non-linear classifier application fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25193;&#25955;&#27169;&#22411;&#20026;&#28789;&#24863;&#30340;&#28145;&#24230;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#24182;&#21487;&#22312;&#25968;&#37327;&#30456;&#21516;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#19979;&#19982;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#21487;&#27604;&#36739;&#30340;&#25968;&#20540;&#32467;&#26524;&#12290;&#36890;&#36807;&#25512;&#23548;&#24471;&#20998;&#21305;&#37197;&#30340;&#23646;&#24615;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#38543;&#26426;&#29305;&#24449;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#26679;&#26412;&#25968;&#25454;&#20998;&#24067;&#19982;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;&#27867;&#21270;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.04417</link><description>&lt;p&gt;
&#25193;&#25955;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Random Feature Model. (arXiv:2310.04417v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25193;&#25955;&#27169;&#22411;&#20026;&#28789;&#24863;&#30340;&#28145;&#24230;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#24182;&#21487;&#22312;&#25968;&#37327;&#30456;&#21516;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#19979;&#19982;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#21487;&#27604;&#36739;&#30340;&#25968;&#20540;&#32467;&#26524;&#12290;&#36890;&#36807;&#25512;&#23548;&#24471;&#20998;&#21305;&#37197;&#30340;&#23646;&#24615;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#38543;&#26426;&#29305;&#24449;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#26679;&#26412;&#25968;&#25454;&#20998;&#24067;&#19982;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;&#27867;&#21270;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#24050;&#25104;&#21151;&#29992;&#20110;&#29983;&#25104;&#20174;&#22122;&#22768;&#20013;&#20135;&#29983;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25193;&#25955;&#27169;&#22411;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#38590;&#20197;&#35299;&#37322;&#65292;&#32570;&#20047;&#29702;&#35770;&#20381;&#25454;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#20854;&#21487;&#35299;&#37322;&#24615;&#65292;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#22312;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#25193;&#25955;&#27169;&#22411;&#21551;&#21457;&#30340;&#28145;&#24230;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#65292;&#23427;&#26082;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#21448;&#33021;&#32473;&#20986;&#19982;&#20855;&#26377;&#30456;&#21516;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30456;&#24403;&#30340;&#25968;&#20540;&#32467;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#38543;&#26426;&#29305;&#24449;&#32467;&#26524;&#65292;&#21033;&#29992;&#24471;&#20998;&#21305;&#37197;&#30340;&#23646;&#24615;&#23548;&#20986;&#20102;&#26679;&#26412;&#25968;&#25454;&#20998;&#24067;&#19982;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;&#27867;&#21270;&#36793;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#26102;&#23578;MNIST&#25968;&#25454;&#38598;&#21644;&#20048;&#22120;&#38899;&#39057;&#25968;&#25454;&#19978;&#29983;&#25104;&#26679;&#26412;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models have been successfully used to generate data from noise. However, most diffusion models are computationally expensive and difficult to interpret with a lack of theoretical justification. Random feature models on the other hand have gained popularity due to their interpretability but their application to complex machine learning tasks remains limited. In this work, we present a diffusion model-inspired deep random feature model that is interpretable and gives comparable numerical results to a fully connected neural network having the same number of trainable parameters. Specifically, we extend existing results for random features and derive generalization bounds between the distribution of sampled data and the true distribution using properties of score matching. We validate our findings by generating samples on the fashion MNIST dataset and instrumental audio data.
&lt;/p&gt;</description></item><item><title>MoatPlus&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#20215;&#26684;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#30417;&#30563;&#32479;&#35745;&#29305;&#24449;&#21644;&#21382;&#21490;&#20215;&#26684;&#36235;&#21183;&#29983;&#25104;&#19978;&#38480;&#20215;&#26684;&#36793;&#30028;&#65292;&#20197;&#35299;&#20915;&#22312;&#32447;&#24066;&#22330;&#20013;&#30340;&#25968;&#25454;&#36136;&#37327;&#21644;&#38169;&#35823;&#20215;&#26684;&#21457;&#24067;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04367</link><description>&lt;p&gt;
&#19968;&#20010;&#22823;&#35268;&#27169;&#24066;&#22330;&#20215;&#26684;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Marketplace Price Anomaly Detection System at Scale. (arXiv:2310.04367v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04367
&lt;/p&gt;
&lt;p&gt;
MoatPlus&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#20215;&#26684;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#30417;&#30563;&#32479;&#35745;&#29305;&#24449;&#21644;&#21382;&#21490;&#20215;&#26684;&#36235;&#21183;&#29983;&#25104;&#19978;&#38480;&#20215;&#26684;&#36793;&#30028;&#65292;&#20197;&#35299;&#20915;&#22312;&#32447;&#24066;&#22330;&#20013;&#30340;&#25968;&#25454;&#36136;&#37327;&#21644;&#38169;&#35823;&#20215;&#26684;&#21457;&#24067;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24066;&#22330;&#27599;&#22825;&#22312;&#24179;&#21488;&#19978;&#25191;&#34892;&#22823;&#37327;&#30340;&#20215;&#26684;&#26356;&#26032;&#65292;&#36825;&#20123;&#26356;&#26032;&#30001;&#20010;&#20307;&#24066;&#22330;&#21334;&#23478;&#21457;&#36215;&#12290;&#36825;&#31181;&#20215;&#26684;&#27665;&#20027;&#21270;&#38543;&#30528;&#25968;&#25454;&#36136;&#37327;&#30340;&#25361;&#25112;&#32780;&#22686;&#21152;&#12290;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#22312;&#32447;&#38646;&#21806;&#21830;&#65292;&#32570;&#20047;&#38598;&#20013;&#30340;&#38450;&#25252;&#25514;&#26045;&#20250;&#23548;&#33268;&#26356;&#39640;&#30340;&#38169;&#35823;&#20215;&#26684;&#22312;&#32593;&#31449;&#19978;&#21457;&#24067;&#65292;&#20174;&#32780;&#32473;&#39038;&#23458;&#20307;&#39564;&#24102;&#26469;&#24046;&#35780;&#21644;&#28508;&#22312;&#30340;&#25910;&#20837;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MoatPlus&#65288;&#20351;&#29992;&#26641;&#12289;&#22522;&#20110;&#37051;&#36817;&#24230;&#30340;&#26631;&#31614;&#20197;&#21450;&#26080;&#30417;&#30563;&#32479;&#35745;&#29305;&#24449;&#30340;&#33945;&#38754;&#26368;&#20248;&#38170;&#28857;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#19981;&#26029;&#22686;&#38271;&#30340;&#24066;&#22330;&#24179;&#21488;&#30340;&#21487;&#25193;&#23637;&#20215;&#26684;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#12290;&#30446;&#26631;&#26159;&#21033;&#29992;&#37051;&#36817;&#24230;&#21644;&#21382;&#21490;&#20215;&#26684;&#36235;&#21183;&#30340;&#26080;&#30417;&#30563;&#32479;&#35745;&#29305;&#24449;&#26469;&#29983;&#25104;&#19978;&#38480;&#20215;&#26684;&#36793;&#30028;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#38598;&#21512;&#26469;&#26816;&#27979;&#22522;&#20110;&#20215;&#26684;&#30340;&#29305;&#24449;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#65292;&#25490;&#38500;&#24322;&#24120;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#20248;&#21270;&#30340;&#21152;&#26435;&#26041;&#26696;&#26469;&#26500;&#24314;&#23454;&#26102;&#23450;&#20215;&#31649;&#36947;&#20013;&#21487;&#38752;&#30340;&#20215;&#26684;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online marketplaces execute large volume of price updates that are initiated by individual marketplace sellers each day on the platform. This price democratization comes with increasing challenges with data quality. Lack of centralized guardrails that are available for a traditional online retailer causes a higher likelihood for inaccurate prices to get published on the website, leading to poor customer experience and potential for revenue loss. We present MoatPlus (Masked Optimal Anchors using Trees, Proximity-based Labeling and Unsupervised Statistical-features), a scalable price anomaly detection framework for a growing marketplace platform. The goal is to leverage proximity and historical price trends from unsupervised statistical features to generate an upper price bound. We build an ensemble of models to detect irregularities in price-based features, exclude irregular features and use optimized weighting scheme to build a reliable price bound in real-time pricing pipeline. We obs
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;NAEPro&#27169;&#22411;&#65292;&#36890;&#36807;&#20989;&#25968;&#20960;&#20309;&#24341;&#23548;&#30340;&#26041;&#27861;&#20849;&#21516;&#35774;&#35745;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#27688;&#22522;&#37240;&#24674;&#22797;&#29575;&#12289;TM&#20998;&#25968;&#21644;RMSD&#31561;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.04343</link><description>&lt;p&gt;
&#20989;&#25968;&#20960;&#20309;&#24341;&#23548;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#39592;&#26550;&#32467;&#26500;&#20849;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design. (arXiv:2310.04343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;NAEPro&#27169;&#22411;&#65292;&#36890;&#36807;&#20989;&#25968;&#20960;&#20309;&#24341;&#23548;&#30340;&#26041;&#27861;&#20849;&#21516;&#35774;&#35745;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#27688;&#22522;&#37240;&#24674;&#22797;&#29575;&#12289;TM&#20998;&#25968;&#21644;RMSD&#31561;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#26159;&#20960;&#20046;&#25152;&#26377;&#29983;&#29289;&#20307;&#20013;&#36127;&#36131;&#22522;&#26412;&#21151;&#33021;&#30340;&#22823;&#20998;&#23376;&#12290;&#35774;&#35745;&#21512;&#29702;&#30340;&#20855;&#26377;&#26399;&#26395;&#21151;&#33021;&#30340;&#34507;&#30333;&#36136;&#33267;&#20851;&#37325;&#35201;&#12290;&#34507;&#30333;&#36136;&#30340;&#24207;&#21015;&#21644;&#32467;&#26500;&#23494;&#20999;&#30456;&#20851;&#65292;&#23427;&#20204;&#20849;&#21516;&#20915;&#23450;&#20102;&#20854;&#21151;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NAEPro&#65292;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#26816;&#27979;&#21040;&#30340;&#21151;&#33021;&#20301;&#28857;&#20849;&#21516;&#35774;&#35745;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#30340;&#27169;&#22411;&#12290;NAEPro&#37319;&#29992;&#20102;&#27880;&#24847;&#21147;&#21644;&#31561;&#21464;&#23618;&#30340;&#20132;&#38169;&#32593;&#32476;&#65292;&#21487;&#20197;&#25429;&#25417;&#24207;&#21015;&#20013;&#30340;&#20840;&#23616;&#30456;&#20851;&#24615;&#20197;&#21450;&#19977;&#32500;&#31354;&#38388;&#20013;&#26368;&#36817;&#27688;&#22522;&#37240;&#30340;&#23616;&#37096;&#24433;&#21709;&#12290;&#36825;&#31181;&#26550;&#26500;&#22312;&#20004;&#20010;&#23618;&#38754;&#19978;&#20419;&#36827;&#20102;&#26377;&#25928;&#32780;&#32463;&#27982;&#30340;&#20449;&#24687;&#20256;&#36882;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#34507;&#30333;&#36136;&#25968;&#25454;&#38598;&#65288;&#946;-&#20869;&#37232;&#33018;&#37238;&#21644;&#32908;&#32418;&#34507;&#30333;&#65289;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21644;&#20960;&#20010;&#24378;&#31454;&#20105;&#22522;&#32447;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#31454;&#20105;&#23545;&#25163;&#20013;&#22987;&#32456;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#27688;&#22522;&#37240;&#24674;&#22797;&#29575;&#12289;TM&#20998;&#25968;&#21644;&#26368;&#20302;&#30340;RMSD&#12290;&#36825;&#20123;&#21457;&#29616;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proteins are macromolecules responsible for essential functions in almost all living organisms. Designing reasonable proteins with desired functions is crucial. A protein's sequence and structure are strongly correlated and they together determine its function. In this paper, we propose NAEPro, a model to jointly design Protein sequence and structure based on automatically detected functional sites. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from nearest amino acids in three dimensional (3D) space. Such an architecture facilitates effective yet economic message passing at two levels. We evaluate our model and several strong baselines on two protein datasets, $\beta$-lactamase and myoglobin. Experimental results show that our model consistently achieves the highest amino acid recovery rate, TM-score, and the lowest RMSD among all competitors. These findings prove the capab
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19971;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#26159;ToyMix&#12289;LargeMix&#21644;UltraLarge&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#21644;&#26377;&#30417;&#30563;&#26631;&#31614;&#30340;&#22810;&#26679;&#24615;&#26041;&#38754;&#31361;&#30772;&#20102;&#30028;&#38480;&#65292;&#28085;&#30422;&#20102;&#36817;1&#20159;&#20010;&#20998;&#23376;&#21644;3000&#22810;&#20010;&#31232;&#30095;&#23450;&#20041;&#30340;&#20219;&#21153;&#65292;&#24635;&#35745;&#36229;&#36807;130&#20159;&#20010;&#20851;&#20110;&#37327;&#23376;&#21644;&#29983;&#29289;&#24615;&#36136;&#30340;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2310.04292</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#20998;&#23376;&#23398;&#20064;&#22522;&#30784;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets. (arXiv:2310.04292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19971;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#26159;ToyMix&#12289;LargeMix&#21644;UltraLarge&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#21644;&#26377;&#30417;&#30563;&#26631;&#31614;&#30340;&#22810;&#26679;&#24615;&#26041;&#38754;&#31361;&#30772;&#20102;&#30028;&#38480;&#65292;&#28085;&#30422;&#20102;&#36817;1&#20159;&#20010;&#20998;&#23376;&#21644;3000&#22810;&#20010;&#31232;&#30095;&#23450;&#20041;&#30340;&#20219;&#21153;&#65292;&#24635;&#35745;&#36229;&#36807;130&#20159;&#20010;&#20851;&#20110;&#37327;&#23376;&#21644;&#29983;&#29289;&#24615;&#36136;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#25163;&#24037;&#31574;&#21010;&#30340;&#65292;&#22240;&#27492;&#35268;&#27169;&#36739;&#23567;&#65292;&#32570;&#20047;&#24102;&#26377;&#26631;&#35760;&#29305;&#24449;&#21644;&#31649;&#29702;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#20195;&#30721;&#24211;&#65292;&#21046;&#32422;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19971;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#20998;&#20026;&#19977;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65306;ToyMix&#12289;LargeMix&#21644;UltraLarge&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#21644;&#26377;&#30417;&#30563;&#26631;&#31614;&#30340;&#22810;&#26679;&#24615;&#26041;&#38754;&#31361;&#30772;&#20102;&#30028;&#38480;&#12290;&#23427;&#20204;&#28085;&#30422;&#20102;&#36817;1&#20159;&#20010;&#20998;&#23376;&#21644;3000&#22810;&#20010;&#31232;&#30095;&#23450;&#20041;&#30340;&#20219;&#21153;&#65292;&#24635;&#35745;&#36229;&#36807;130&#20159;&#20010;&#20851;&#20110;&#37327;&#23376;&#21644;&#29983;&#29289;&#24615;&#36136;&#30340;&#26631;&#31614;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;OGB-LSC PCQM4Mv2&#25968;&#25454;&#38598;&#30340;300&#20493;&#65292;&#20063;&#26159;&#20165;&#21253;&#21547;&#37327;&#23376;&#25968;&#25454;&#30340;QM1B&#25968;&#25454;&#38598;&#30340;13&#20493;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25903;&#25345;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recently, pre-trained foundation models have enabled significant advancements in multiple fields. In molecular machine learning, however, where datasets are often hand-curated, and hence typically small, the lack of datasets with labeled features, and codebases to manage those datasets, has hindered the development of foundation models. In this work, we present seven novel datasets categorized by size into three distinct categories: ToyMix, LargeMix and UltraLarge. These datasets push the boundaries in both the scale and the diversity of supervised labels for molecular learning. They cover nearly 100 million molecules and over 3000 sparsely defined tasks, totaling more than 13 billion individual labels of both quantum and biological nature. In comparison, our datasets contain 300 times more data points than the widely used OGB-LSC PCQM4Mv2 dataset, and 13 times more than the quantum-only QM1B dataset. In addition, to support the development of foundational models based on our proposed 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#29992;&#20110;&#23398;&#20064;&#34920;&#31034;&#30340;&#19981;&#21516;&#36741;&#21161;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#29615;&#22659;&#19978;&#35757;&#32451;&#25968;&#30334;&#20010;&#26234;&#33021;&#20307;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#20351;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#23545;&#29615;&#22659;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#22238;&#25253;&#26377;&#30410;&#12290;</title><link>http://arxiv.org/abs/2310.04241</link><description>&lt;p&gt;
&#27604;&#36739;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36741;&#21161;&#20219;&#21153;&#30340;&#23398;&#20064;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Comparing Auxiliary Tasks for Learning Representations for Reinforcement Learning. (arXiv:2310.04241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#29992;&#20110;&#23398;&#20064;&#34920;&#31034;&#30340;&#19981;&#21516;&#36741;&#21161;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#29615;&#22659;&#19978;&#35757;&#32451;&#25968;&#30334;&#20010;&#26234;&#33021;&#20307;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#20351;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#23545;&#29615;&#22659;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#22238;&#25253;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33021;&#22815;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#29615;&#22659;&#22238;&#25253;&#65292;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#19968;&#31181;&#30452;&#25509;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#19968;&#20010;&#19982;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19981;&#21516;&#30340;&#36741;&#21161;&#20219;&#21153;&#35757;&#32451;&#19968;&#20010;&#29420;&#31435;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#34920;&#31034;&#12290;&#34429;&#28982;&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#36825;&#26679;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#20294;&#22312;&#20856;&#22411;&#30340;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#29615;&#22659;&#19978;&#36827;&#34892;&#27604;&#36739;&#35745;&#31639;&#37327;&#22823;&#19988;&#25454;&#25105;&#20204;&#25152;&#30693;&#20197;&#21069;&#26410;&#36827;&#34892;&#36807;&#12290;&#26412;&#25991;&#22312;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#30340;&#25968;&#30334;&#20010;&#26234;&#33021;&#20307;&#19978;&#36827;&#34892;&#20102;&#36825;&#26679;&#30340;&#36741;&#21161;&#20219;&#21153;&#27604;&#36739;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20174;&#31616;&#21333;&#25670;&#32447;&#21040;&#22797;&#26434;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#22238;&#25253;&#30340;&#21487;&#33021;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#23545;&#29615;&#22659;&#26159;&#26377;&#30410;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning state representations has gained steady popularity in reinforcement learning (RL) due to its potential to improve both sample efficiency and returns on many environments. A straightforward and efficient method is to generate representations with a distinct neural network trained on an auxiliary task, i.e. a task that differs from the actual RL task. While a whole range of such auxiliary tasks has been proposed in the literature, a comparison on typical continuous control benchmark environments is computationally expensive and has, to the best of our knowledge, not been performed before. This paper presents such a comparison of common auxiliary tasks, based on hundreds of agents trained with state-of-the-art off-policy RL algorithms. We compare possible improvements in both sample efficiency and returns for environments ranging from simple pendulum to a complex simulated robotics task. Our findings show that representation learning with auxiliary tasks is beneficial for environ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#32858;&#21512;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23398;&#20064;&#27599;&#20010;&#20851;&#31995;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#27880;&#24847;&#20989;&#25968;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#30340;&#32858;&#21512;&#12290;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#32771;&#34385;&#30446;&#26631;&#33410;&#28857;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35745;&#31639;&#20851;&#31995;&#20043;&#38388;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.04171</link><description>&lt;p&gt;
&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#21147;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dynamic Relation-Attentive Graph Neural Networks for Fraud Detection. (arXiv:2310.04171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#32858;&#21512;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23398;&#20064;&#27599;&#20010;&#20851;&#31995;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#27880;&#24847;&#20989;&#25968;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#30340;&#32858;&#21512;&#12290;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#32771;&#34385;&#30446;&#26631;&#33410;&#28857;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35745;&#31639;&#20851;&#31995;&#20043;&#38388;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27450;&#35784;&#26816;&#27979;&#26088;&#22312;&#21457;&#29616;&#27450;&#35784;&#32773;&#36890;&#36807;&#30041;&#19979;&#20551;&#35780;&#35770;&#25110;&#36827;&#34892;&#24322;&#24120;&#20132;&#26131;&#27450;&#39575;&#20854;&#20182;&#29992;&#25143;&#12290;&#22522;&#20110;&#22270;&#30340;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#23558;&#36825;&#20010;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#21253;&#21547;&#20004;&#20010;&#31867;&#21035;&#65288;&#27450;&#35784;&#25110;&#27491;&#24120;&#65289;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#32858;&#21512;&#26426;&#21046;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22522;&#20110;&#23454;&#38469;&#19990;&#30028;&#22270;&#34920;&#20013;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#30340;&#20851;&#31995;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#24314;&#35758;&#23398;&#20064;&#27599;&#20010;&#20851;&#31995;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#27880;&#24847;&#20989;&#25968;&#32858;&#21512;&#33410;&#28857;&#34920;&#31034;&#65292;&#35813;&#20989;&#25968;&#20026;&#27599;&#20010;&#20851;&#31995;&#20998;&#37197;&#19981;&#21516;&#30340;&#27880;&#24847;&#31995;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32467;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#20197;&#32771;&#34385;&#30446;&#26631;&#33410;&#28857;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#36825;&#26377;&#21161;&#20110;&#25552;&#39640;&#22312;&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#19978;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#25152;&#26377;&#32858;&#21512;&#36807;&#31243;&#20013;&#37319;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35745;&#31639;&#20851;&#31995;&#20043;&#38388;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fraud detection aims to discover fraudsters deceiving other users by, for example, leaving fake reviews or making abnormal transactions. Graph-based fraud detection methods consider this task as a classification problem with two classes: frauds or normal. We address this problem using Graph Neural Networks (GNNs) by proposing a dynamic relation-attentive aggregation mechanism. Based on the observation that many real-world graphs include different types of relations, we propose to learn a node representation per relation and aggregate the node representations using a learnable attention function that assigns a different attention coefficient to each relation. Furthermore, we combine the node representations from different layers to consider both the local and global structures of a target node, which is beneficial to improving the performance of fraud detection on graphs with heterophily. By employing dynamic graph attention in all the aggregation processes, our method adaptively comput
&lt;/p&gt;</description></item><item><title>AUTOPARLLM&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#24182;&#29983;&#25104;&#39034;&#24207;&#32534;&#20889;&#31243;&#24207;&#24182;&#34892;&#29256;&#26412;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;GNN&#30340;&#24182;&#34892;&#24615;&#21457;&#29616;&#27169;&#22359;&#21644;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#22120;&#12290;</title><link>http://arxiv.org/abs/2310.04047</link><description>&lt;p&gt;
AUTOPARLLM&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;GNN&#24341;&#23548;&#30340;&#33258;&#21160;&#20195;&#30721;&#24182;&#34892;&#21270;
&lt;/p&gt;
&lt;p&gt;
AUTOPARLLM: GNN-Guided Automatic Code Parallelization using Large Language Models. (arXiv:2310.04047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04047
&lt;/p&gt;
&lt;p&gt;
AUTOPARLLM&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#24182;&#29983;&#25104;&#39034;&#24207;&#32534;&#20889;&#31243;&#24207;&#24182;&#34892;&#29256;&#26412;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;GNN&#30340;&#24182;&#34892;&#24615;&#21457;&#29616;&#27169;&#22359;&#21644;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24182;&#34892;&#21270;&#39034;&#24207;&#32534;&#20889;&#30340;&#31243;&#24207;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#21363;&#20351;&#26159;&#32463;&#39564;&#20016;&#23500;&#30340;&#24320;&#21457;&#20154;&#21592;&#20063;&#38656;&#35201;&#33457;&#36153;&#30456;&#24403;&#22810;&#30340;&#26102;&#38388;&#26469;&#23547;&#25214;&#24182;&#34892;&#24615;&#26426;&#20250;&#65292;&#28982;&#21518;&#23454;&#38469;&#32534;&#20889;&#39034;&#24207;&#32534;&#20889;&#31243;&#24207;&#30340;&#24182;&#34892;&#29256;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AUTOPARLLM&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#24182;&#34892;&#24615;&#24182;&#29983;&#25104;&#39034;&#24207;&#32534;&#20889;&#31243;&#24207;&#30340;&#24182;&#34892;&#29256;&#26412;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;i&#65289;&#22522;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24182;&#34892;&#24615;&#21457;&#29616;&#21644;&#24182;&#34892;&#27169;&#24335;&#26816;&#27979;&#27169;&#22359;&#65292;&#20197;&#21450;ii&#65289;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#29983;&#25104;&#39034;&#24207;&#31243;&#24207;&#30340;&#24182;&#34892;&#23545;&#24212;&#29256;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;GNN&#23398;&#20064;&#31243;&#24207;&#30340;&#27969;&#25935;&#24863;&#29305;&#24449;&#65292;&#20197;&#35782;&#21035;&#39034;&#24207;&#31243;&#24207;&#20013;&#30340;&#24182;&#34892;&#21306;&#22495;&#65292;&#24182;&#20351;&#29992;GNN&#30340;&#32467;&#26524;&#26500;&#24314;&#22686;&#24378;&#25552;&#31034;&#65292;&#20197;&#20379;LLM&#22522;&#30784;&#29983;&#25104;&#22120;&#26368;&#32456;&#20135;&#29983;&#39034;&#24207;&#31243;&#24207;&#30340;&#24182;&#34892;&#23545;&#24212;&#29256;&#26412;&#12290;&#25105;&#20204;&#22312;11&#20010;&#24212;&#29992;&#19978;&#35780;&#20272;&#20102;AUTOPARLLM
&lt;/p&gt;
&lt;p&gt;
Parallelizing sequentially written programs is a challenging task. Even experienced developers need to spend considerable time finding parallelism opportunities and then actually writing parallel versions of sequentially written programs. To address this issue, we present AUTOPARLLM, a framework for automatically discovering parallelism and generating the parallel version of the sequentially written program. Our framework consists of two major components: i) a heterogeneous Graph Neural Network (GNN) based parallelism discovery and parallel pattern detection module, and ii) an LLM-based code generator to generate the parallel counterpart of the sequential programs. We use the GNN to learn the flow-aware characteristics of the programs to identify parallel regions in sequential programs and then construct an enhanced prompt using the GNN's results for the LLM-based generator to finally produce the parallel counterparts of the sequential programs. We evaluate AUTOPARLLM on 11 application
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31867;&#20284;&#26679;&#26412;&#32858;&#31867;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26367;&#25442;&#20010;&#20307;&#30340;&#25935;&#24863;&#29305;&#24449;&#20026;&#32858;&#31867;&#30340;&#24179;&#22343;&#20540;&#26469;&#22686;&#24378;&#38544;&#31169;&#12290;&#36890;&#36807;&#23545;&#20351;&#29992;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#35757;&#32451;&#27169;&#22411;&#30340;&#31934;&#30830;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19981;&#21516;&#27169;&#22411;&#32452;&#25104;&#37096;&#20998;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#22312;&#26576;&#20123;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.04015</link><description>&lt;p&gt;
&#36890;&#36807;&#31867;&#20284;&#26679;&#26412;&#32858;&#31867;&#23398;&#20064;&#65306;&#23545;&#27169;&#22411;&#27867;&#21270;&#30340;&#31934;&#30830;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Learning via Look-Alike Clustering: A Precise Analysis of Model Generalization. (arXiv:2310.04015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31867;&#20284;&#26679;&#26412;&#32858;&#31867;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26367;&#25442;&#20010;&#20307;&#30340;&#25935;&#24863;&#29305;&#24449;&#20026;&#32858;&#31867;&#30340;&#24179;&#22343;&#20540;&#26469;&#22686;&#24378;&#38544;&#31169;&#12290;&#36890;&#36807;&#23545;&#20351;&#29992;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#35757;&#32451;&#27169;&#22411;&#30340;&#31934;&#30830;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19981;&#21516;&#27169;&#22411;&#32452;&#25104;&#37096;&#20998;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#22312;&#26576;&#20123;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20294;&#30830;&#20445;&#29992;&#25143;&#25968;&#25454;&#30340;&#20445;&#25252;&#20173;&#28982;&#26159;&#36825;&#20123;&#23398;&#20064;&#31995;&#32479;&#24320;&#21457;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#22686;&#24378;&#38544;&#31169;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#21311;&#21517;&#25968;&#25454;&#32780;&#19981;&#26159;&#20010;&#20307;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31867;&#20284;&#26679;&#26412;&#32858;&#31867;&#8221;&#30340;&#33258;&#28982;&#25216;&#26415;&#65292;&#23427;&#28041;&#21450;&#23558;&#20010;&#20307;&#30340;&#25935;&#24863;&#29305;&#24449;&#26367;&#25442;&#20026;&#32858;&#31867;&#30340;&#24179;&#22343;&#20540;&#12290;&#25105;&#20204;&#23545;&#20351;&#29992;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#35757;&#32451;&#27169;&#22411;&#22914;&#20309;&#24433;&#21709;&#20854;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#20102;&#31934;&#30830;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#20851;&#27880;&#19968;&#20010;&#28176;&#36817;&#24773;&#20917;&#65292;&#21363;&#35757;&#32451;&#38598;&#30340;&#22823;&#23567;&#19982;&#29305;&#24449;&#32500;&#24230;&#25104;&#27604;&#20363;&#22686;&#38271;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;&#20984;&#39640;&#26031;&#26497;&#23567;&#21270;&#26497;&#22823;&#23450;&#29702;&#65288;Convex Gaussian Minimax Theorem&#65292;CGMT&#65289;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#29702;&#35770;&#19978;&#29702;&#35299;&#19981;&#21516;&#27169;&#22411;&#32452;&#25104;&#37096;&#20998;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#26576;&#20123;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#36827;&#34892;&#35757;&#32451;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While personalized recommendations systems have become increasingly popular, ensuring user data protection remains a paramount concern in the development of these learning systems. A common approach to enhancing privacy involves training models using anonymous data rather than individual data. In this paper, we explore a natural technique called \emph{look-alike clustering}, which involves replacing sensitive features of individuals with the cluster's average values. We provide a precise analysis of how training models using anonymous cluster centers affects their generalization capabilities. We focus on an asymptotic regime where the size of the training set grows in proportion to the features dimension. Our analysis is based on the Convex Gaussian Minimax Theorem (CGMT) and allows us to theoretically understand the role of different model components on the generalization error. In addition, we demonstrate that in certain high-dimensional regimes, training over anonymous cluster cente
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#39640;&#36125;&#21494;&#26031;&#20248;&#21270;&#27169;&#22411;&#24314;&#27169;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;&#28145;&#24230;&#26680;&#23398;&#20064;&#20013;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03912</link><description>&lt;p&gt;
RTDK-BO&#65306;&#20855;&#26377;Reinforced Transformer&#28145;&#24230;&#26680;&#20989;&#25968;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
RTDK-BO: High Dimensional Bayesian Optimization with Reinforced Transformer Deep kernels. (arXiv:2310.03912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#39640;&#36125;&#21494;&#26031;&#20248;&#21270;&#27169;&#22411;&#24314;&#27169;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;&#28145;&#24230;&#26680;&#23398;&#20064;&#20013;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20195;&#29702;&#25351;&#23548;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#23545;&#20110;&#39640;&#32500;&#40657;&#30418;&#20248;&#21270;&#38750;&#24120;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#22312;&#24037;&#19994;&#35774;&#35745;&#21644;&#31185;&#23398;&#35745;&#31639;&#31561;&#35768;&#22810;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#21333;&#20989;&#25968;&#20248;&#21270;&#21644;&#23569;&#26679;&#26412;&#22810;&#30446;&#26631;&#20248;&#21270;&#19978;&#24341;&#20837;&#20102;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#23569;&#26679;&#26412;&#25216;&#26415;&#20063;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#32039;&#23494;&#30456;&#20851;&#30446;&#26631;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#26412;&#25991;&#32467;&#21512;&#20102;&#28145;&#24230;&#26680;&#23398;&#20064;&#65288;DKL&#65289;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;Transformer&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25913;&#36827;&#20102;GP&#20195;&#29702;&#30340;&#24314;&#27169;&#33021;&#21147;&#19982;&#20803;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;DKL&#20013;&#26469;&#25913;&#36827;&#20803;&#23398;&#20064;BO&#20195;&#29702;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;BO&#36807;&#31243;&#20013;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;Transformer&#28145;&#24230;&#26680;&#26041;&#27861;&#19982;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#26469;&#25552;&#39640;BO&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO), guided by Gaussian process (GP) surrogates, has proven to be an invaluable technique for efficient, high-dimensional, black-box optimization, a critical problem inherent to many applications such as industrial design and scientific computing. Recent contributions have introduced reinforcement learning (RL) to improve the optimization performance on both single function optimization and \textit{few-shot} multi-objective optimization. However, even few-shot techniques fail to exploit similarities shared between closely related objectives. In this paper, we combine recent developments in Deep Kernel Learning (DKL) and attention-based Transformer models to improve the modeling powers of GP surrogates with meta-learning. We propose a novel method for improving meta-learning BO surrogates by incorporating attention mechanisms into DKL, empowering the surrogates to adapt to contextual information gathered during the BO process. We combine this Transformer Deep Kern
&lt;/p&gt;</description></item><item><title>PyDCM&#26159;&#19968;&#20010;&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;&#21487;&#23450;&#21046;&#30340;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#23450;&#20041;&#37197;&#32622;&#21644;&#21521;&#37327;&#21270;&#30340;&#28909;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#20013;&#24515;&#30340;&#20248;&#21270;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.03906</link><description>&lt;p&gt;
&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;PyDCM&#65306;&#20026;&#21487;&#25345;&#32493;&#24615;&#23450;&#21046;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PyDCM: Custom Data Center Models with Reinforcement Learning for Sustainability. (arXiv:2310.03906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03906
&lt;/p&gt;
&lt;p&gt;
PyDCM&#26159;&#19968;&#20010;&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;&#21487;&#23450;&#21046;&#30340;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#23450;&#20041;&#37197;&#32622;&#21644;&#21521;&#37327;&#21270;&#30340;&#28909;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#20013;&#24515;&#30340;&#20248;&#21270;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#23545;&#21487;&#25345;&#32493;&#24615;&#21644;&#20943;&#23569;&#30899;&#25490;&#25918;&#30340;&#24378;&#35843;&#26085;&#30410;&#22686;&#21152;&#65292;&#20419;&#20351;&#25919;&#24220;&#21644;&#20225;&#19994;&#37325;&#26032;&#24605;&#32771;&#25968;&#25454;&#20013;&#24515;&#30340;&#35774;&#35745;&#21644;&#36816;&#33829;&#26041;&#27861;&#12290;&#37492;&#20110;&#25968;&#25454;&#20013;&#24515;&#30340;&#39640;&#33021;&#32791;&#21644;&#25351;&#25968;&#32423;&#35745;&#31639;&#24037;&#20316;&#37327;&#65292;&#20248;&#21270;&#33021;&#32791;&#29305;&#21035;&#26159;&#22312;&#20919;&#21364;&#21644;IT&#33021;&#28304;&#20351;&#29992;&#26041;&#38754;&#65292;&#25968;&#25454;&#20013;&#24515;&#26159;&#20248;&#21270;&#30005;&#21147;&#28040;&#32791;&#30340;&#29702;&#24819;&#20505;&#36873;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#21487;&#37197;&#32622;&#21644;&#21487;&#25193;&#23637;&#30340;&#28909;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31649;&#36947;&#12290;&#25968;&#25454;&#20013;&#24515;&#30001;&#22810;&#20010;IT&#32452;&#20214;&#32452;&#25104;&#65292;&#20854;&#20960;&#20309;&#37197;&#32622;&#21644;&#25955;&#28909;&#20351;&#24471;&#28909;&#24314;&#27169;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PyDCM&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;Python&#23454;&#29616;&#30340;&#21487;&#23450;&#21046;&#30340;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#33258;&#23450;&#20041;&#30340;&#26381;&#21153;&#22120;&#35268;&#26684;&#21644;IT&#26426;&#26588;&#30340;&#20960;&#20309;&#24067;&#32622;&#21019;&#24314;&#29420;&#29305;&#30340;&#37197;&#32622;&#12290;&#20351;&#29992;&#21521;&#37327;&#21270;&#30340;&#28909;&#35745;&#31639;&#20351;&#24471;PyDCM&#27604;&#24403;&#21069;&#26041;&#27861;&#24555;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#65288;30&#20493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing global emphasis on sustainability and reducing carbon emissions is pushing governments and corporations to rethink their approach to data center design and operation. Given their high energy consumption and exponentially large computational workloads, data centers are prime candidates for optimizing power consumption, especially in areas such as cooling and IT energy usage. A significant challenge in this pursuit is the lack of a configurable and scalable thermal data center model that offers an end-to-end pipeline. Data centers consist of multiple IT components whose geometric configuration and heat dissipation make thermal modeling difficult. This paper presents PyDCM, a customizable Data Center Model implemented in Python, that allows users to create unique configurations of IT equipment with custom server specifications and geometric arrangements of IT cabinets. The use of vectorized thermal calculations makes PyDCM orders of magnitude faster (30 times) than current 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36864;&#28779;&#26041;&#27861;&#20272;&#35745;&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#12290;&#36890;&#36807;&#35780;&#20272;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#23545;&#20272;&#35745;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#36864;&#28779;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#22120;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#20351;&#29992;&#20960;&#20309;&#36335;&#24452;&#21487;&#20197;&#38477;&#20302;&#20272;&#35745;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.03902</link><description>&lt;p&gt;
&#36890;&#36807;&#36864;&#28779;&#26469;&#20272;&#35745;&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#21487;&#35777;&#26126;&#30340;&#30410;&#22788;&#65306;&#37325;&#35201;&#24615;&#25277;&#26679;&#65292;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#65292;&#20197;&#21450;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
Provable benefits of annealing for estimating normalizing constants: Importance Sampling, Noise-Contrastive Estimation, and beyond. (arXiv:2310.03902v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03902
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36864;&#28779;&#26041;&#27861;&#20272;&#35745;&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#12290;&#36890;&#36807;&#35780;&#20272;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#23545;&#20272;&#35745;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#36864;&#28779;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#22120;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#20351;&#29992;&#20960;&#20309;&#36335;&#24452;&#21487;&#20197;&#38477;&#20302;&#20272;&#35745;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#23637;&#20102;&#20960;&#31181;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#26469;&#20272;&#35745;&#24402;&#19968;&#21270;&#24120;&#25968;&#65288;&#37197;&#20998;&#20989;&#25968;&#65289;&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#36864;&#28779;&#30340;&#24605;&#24819;&#12290;&#21363;&#20174;&#21487;&#35745;&#31639;&#30340;&#8220;&#25552;&#35758;&#8221;&#20998;&#24067;&#21644;&#26410;&#24402;&#19968;&#21270;&#30340;&#8220;&#30446;&#26631;&#8221;&#20998;&#24067;&#20043;&#38388;&#30340;&#36335;&#24452;&#36880;&#27493;&#37319;&#26679;&#12290;&#36825;&#20123;&#23478;&#26063;&#20013;&#30340;&#37325;&#35201;&#20272;&#35745;&#22120;&#21253;&#25324;&#36864;&#28779;&#37325;&#35201;&#24615;&#25277;&#26679;&#21644;&#36864;&#28779;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#12290;&#36825;&#26679;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#35768;&#22810;&#35774;&#35745;&#36873;&#25321;&#65306;&#20351;&#29992;&#21738;&#20010;&#20272;&#35745;&#22120;&#12289;&#20351;&#29992;&#21738;&#20010;&#20998;&#24067;&#36335;&#24452;&#20197;&#21450;&#26159;&#21542;&#20351;&#29992;&#20998;&#24067;&#36335;&#24452;&#65307;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#23545;&#20110;&#21738;&#20123;&#36873;&#25321;&#26159;&#26377;&#25928;&#30340;&#36824;&#27809;&#26377;&#26126;&#30830;&#30340;&#29702;&#35770;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#20135;&#29983;&#30340;&#28176;&#36817;&#20272;&#35745;&#35823;&#24046;&#26469;&#35780;&#20272;&#27599;&#20010;&#35774;&#35745;&#36873;&#25321;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;NCE&#27604;&#37325;&#35201;&#24615;&#25277;&#26679;&#20272;&#35745;&#22120;&#26356;&#26377;&#25928;&#65292;&#20294;&#22312;&#26080;&#38480;&#23567;&#30340;&#36335;&#24452;&#27493;&#38271;&#30340;&#26497;&#38480;&#19979;&#65292;&#24046;&#24322;&#28040;&#22833;&#20102;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#20960;&#20309;&#36335;&#24452;&#23558;&#20272;&#35745;&#35823;&#24046;&#20174;&#25351;&#25968;&#32423;&#38477;&#20302;&#21040;...
&lt;/p&gt;
&lt;p&gt;
Recent research has developed several Monte Carlo methods for estimating the normalization constant (partition function) based on the idea of annealing. This means sampling successively from a path of distributions that interpolate between a tractable "proposal" distribution and the unnormalized "target" distribution. Prominent estimators in this family include annealed importance sampling and annealed noise-contrastive estimation (NCE). Such methods hinge on a number of design choices: which estimator to use, which path of distributions to use and whether to use a path at all; so far, there is no definitive theory on which choices are efficient. Here, we evaluate each design choice by the asymptotic estimation error it produces. First, we show that using NCE is more efficient than the importance sampling estimator, but in the limit of infinitesimal path steps, the difference vanishes. Second, we find that using the geometric path brings down the estimation error from an exponential to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#38750;&#32447;&#24615;&#29983;&#25104;&#24335;&#21387;&#32553;&#24863;&#30693;&#20013;&#23454;&#29616;&#32479;&#19968;&#30340;&#20449;&#21495;&#24674;&#22797;&#12290;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#38750;&#32447;&#24615;&#21644;&#21487;&#33021;&#38750;&#36830;&#32493;&#25110;&#26410;&#30693;&#30340;&#35266;&#27979;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#24674;&#22797;&#29983;&#25104;&#27169;&#22411;&#20013;&#25152;&#26377;&#21487;&#33021;&#30340;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2310.03758</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#29983;&#25104;&#24335;&#21387;&#32553;&#24863;&#30693;&#20013;&#32479;&#19968;&#20449;&#21495;&#24674;&#22797;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Uniform Signal Recovery in Nonlinear Generative Compressed Sensing. (arXiv:2310.03758v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03758
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#38750;&#32447;&#24615;&#29983;&#25104;&#24335;&#21387;&#32553;&#24863;&#30693;&#20013;&#23454;&#29616;&#32479;&#19968;&#30340;&#20449;&#21495;&#24674;&#22797;&#12290;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#38750;&#32447;&#24615;&#21644;&#21487;&#33021;&#38750;&#36830;&#32493;&#25110;&#26410;&#30693;&#30340;&#35266;&#27979;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#24674;&#22797;&#29983;&#25104;&#27169;&#22411;&#20013;&#25152;&#26377;&#21487;&#33021;&#30340;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#24335;&#21387;&#32553;&#24863;&#30693;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#20351;&#29992;&#29983;&#25104;&#20808;&#39564;&#20174;m&#20010;&#27979;&#37327;&#20013;&#65288;m&#8810;n&#65289;&#24674;&#22797;&#19968;&#20010;&#20449;&#21495;x&#8727;&#8712;Rn&#65292;&#20854;&#20013;G&#36890;&#24120;&#26159;&#19968;&#20010;L-Lipschitz&#36830;&#32493;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;B2k(r)&#34920;&#31034;Rk&#20013;&#30340;&#21322;&#24452;&#20026;r&#30340;&#8467;2&#29699;&#12290;&#22312;&#38750;&#32447;&#24615;&#27979;&#37327;&#19979;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#32467;&#26524;&#26159;&#38750;&#22343;&#21248;&#30340;&#65292;&#21363;&#23427;&#20204;&#23545;&#20110;&#22266;&#23450;&#30340;x&#8727;&#20855;&#26377;&#39640;&#27010;&#29575;&#65292;&#32780;&#19981;&#26159;&#23545;&#20110;&#25152;&#26377;&#30340;x&#8727;&#21516;&#26102;&#25104;&#31435;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#25512;&#23548;&#38750;&#32447;&#24615;&#29983;&#25104;&#24335;&#21387;&#32553;&#24863;&#30693;&#20013;&#30340;&#22343;&#21248;&#24674;&#22797;&#20445;&#35777;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#38750;&#32447;&#24615;&#21644;&#21487;&#33021;&#38750;&#36830;&#32493;&#25110;&#26410;&#30693;&#30340;&#35266;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20102;1&#20301;/&#22343;&#21248;&#37327;&#21270;&#35266;&#27979;&#21644;&#21333;&#32034;&#24341;&#27169;&#22411;&#20316;&#20026;&#35268;&#33539;&#31034;&#20363;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20351;&#29992;&#24863;&#30693;&#38598;&#21512;&#30340;&#21333;&#20010;&#23454;&#29616;&#21644;&#24191;&#20041;Lasso&#65292;&#25152;&#26377;&#30340;x&#8727;&#8712;G(B2k(r))&#21487;&#20197;&#24674;&#22797;&#21040;&#19968;&#20010;el
&lt;/p&gt;
&lt;p&gt;
In generative compressed sensing (GCS), we want to recover a signal $\mathbf{x}^* \in \mathbb{R}^n$ from $m$ measurements ($m\ll n$) using a generative prior $\mathbf{x}^*\in G(\mathbb{B}_2^k(r))$, where $G$ is typically an $L$-Lipschitz continuous generative model and $\mathbb{B}_2^k(r)$ represents the radius-$r$ $\ell_2$-ball in $\mathbb{R}^k$. Under nonlinear measurements, most prior results are non-uniform, i.e., they hold with high probability for a fixed $\mathbf{x}^*$ rather than for all $\mathbf{x}^*$ simultaneously. In this paper, we build a unified framework to derive uniform recovery guarantees for nonlinear GCS where the observation model is nonlinear and possibly discontinuous or unknown. Our framework accommodates GCS with 1-bit/uniformly quantized observations and single index models as canonical examples. Specifically, using a single realization of the sensing ensemble and generalized Lasso, {\em all} $\mathbf{x}^*\in G(\mathbb{B}_2^k(r))$ can be recovered up to an $\el
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#8220;e-process&#8221;&#21644;&#32622;&#20449;&#24207;&#21015;&#26041;&#27861;&#65292;&#20998;&#21035;&#36890;&#36807;&#26367;&#25442;Lai&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#25152;&#24471;&#32467;&#26524;&#30340;&#23485;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.03722</link><description>&lt;p&gt;
&#26410;&#30693;&#26041;&#24046;&#19979;&#30340;&#39640;&#26031;&#22343;&#20540;&#30340;&#20219;&#24847;&#26377;&#25928;T&#26816;&#39564;&#21644;&#32622;&#20449;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Anytime-valid t-tests and confidence sequences for Gaussian means with unknown variance. (arXiv:2310.03722v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#8220;e-process&#8221;&#21644;&#32622;&#20449;&#24207;&#21015;&#26041;&#27861;&#65292;&#20998;&#21035;&#36890;&#36807;&#26367;&#25442;Lai&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#25152;&#24471;&#32467;&#26524;&#30340;&#23485;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;1976&#24180;&#65292;Lai&#26500;&#36896;&#20102;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#22343;&#20540;$\mu$&#30340;&#39640;&#26031;&#20998;&#24067;&#30340;&#32622;&#20449;&#24207;&#21015;&#65292;&#35813;&#20998;&#24067;&#30340;&#26041;&#24046;$\sigma$&#26159;&#26410;&#30693;&#30340;&#12290;&#20182;&#20351;&#29992;&#20102;&#20851;&#20110;$\sigma$&#30340;&#19981;&#36866;&#24403;&#65288;&#21491;Haar&#65289;&#28151;&#21512;&#21644;&#20851;&#20110;$\mu$&#30340;&#19981;&#36866;&#24403;&#65288;&#24179;&#22374;&#65289;&#28151;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;&#20182;&#26500;&#24314;&#30340;&#32454;&#33410;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#24191;&#20041;&#30340;&#19981;&#21487;&#31215;&#20998;&#38789;&#21644;&#25193;&#23637;&#30340;&#32500;&#23572;&#19981;&#31561;&#24335;&#12290;&#23613;&#31649;&#36825;&#30830;&#23454;&#20135;&#29983;&#20102;&#19968;&#20010;&#39034;&#24207;T&#26816;&#39564;&#65292;&#20294;&#30001;&#20110;&#20182;&#30340;&#38789;&#19981;&#21487;&#31215;&#20998;&#65292;&#23427;&#24182;&#27809;&#26377;&#20135;&#29983;&#19968;&#20010;&#8220;e-process&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#30456;&#21516;&#30340;&#35774;&#32622;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#8220;e-process&#8221;&#21644;&#32622;&#20449;&#24207;&#21015;&#65306;&#19968;&#20010;&#26159;&#22312;&#32553;&#20943;&#28388;&#27874;&#22120;&#20013;&#30340;&#27979;&#35797;&#38789;&#65292;&#21478;&#19968;&#20010;&#26159;&#22312;&#35268;&#33539;&#25968;&#25454;&#28388;&#27874;&#22120;&#20013;&#30340;&#8220;e-process&#8221;&#12290;&#36825;&#20123;&#20998;&#21035;&#26159;&#36890;&#36807;&#23558;Lai&#30340;&#24179;&#22374;&#28151;&#21512;&#26367;&#25442;&#20026;&#39640;&#26031;&#28151;&#21512;&#65292;&#24182;&#23558;&#23545;$\sigma$&#30340;&#21491;Haar&#28151;&#21512;&#26367;&#25442;&#20026;&#22312;&#38646;&#31354;&#38388;&#19979;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#23601;&#20687;&#22312;&#36890;&#29992;&#25512;&#26029;&#20013;&#19968;&#26679;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25152;&#24471;&#32467;&#26524;&#30340;&#23485;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 1976, Lai constructed a nontrivial confidence sequence for the mean $\mu$ of a Gaussian distribution with unknown variance $\sigma$. Curiously, he employed both an improper (right Haar) mixture over $\sigma$ and an improper (flat) mixture over $\mu$. Here, we elaborate carefully on the details of his construction, which use generalized nonintegrable martingales and an extended Ville's inequality. While this does yield a sequential t-test, it does not yield an ``e-process'' (due to the nonintegrability of his martingale). In this paper, we develop two new e-processes and confidence sequences for the same setting: one is a test martingale in a reduced filtration, while the other is an e-process in the canonical data filtration. These are respectively obtained by swapping Lai's flat mixture for a Gaussian mixture, and swapping the right Haar mixture over $\sigma$ with the maximum likelihood estimate under the null, as done in universal inference. We also analyze the width of resulting 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BTDNet&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#21442;&#37327;MRI&#25195;&#25551;&#39044;&#27979;MGMT&#21551;&#21160;&#23376;&#30002;&#22522;&#21270;&#29366;&#24577;&#12290;BTDNet&#35299;&#20915;&#20102;&#21487;&#21464;&#30340;&#20307;&#31215;&#38271;&#24230;&#21644;&#20307;&#31215;&#32423;&#27880;&#37322;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#25968;&#25454;&#22686;&#24378;&#12289;3D&#20998;&#26512;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#20999;&#29255;&#32423;&#20998;&#31867;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.03485</link><description>&lt;p&gt;
BTDNet:&#19968;&#31181;&#29992;&#20110;&#33041;&#32959;&#30244;&#25918;&#23556;&#22522;&#22240;&#32452;&#20998;&#31867;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BTDNet: a Multi-Modal Approach for Brain Tumor Radiogenomic Classification. (arXiv:2310.03485v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BTDNet&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#21442;&#37327;MRI&#25195;&#25551;&#39044;&#27979;MGMT&#21551;&#21160;&#23376;&#30002;&#22522;&#21270;&#29366;&#24577;&#12290;BTDNet&#35299;&#20915;&#20102;&#21487;&#21464;&#30340;&#20307;&#31215;&#38271;&#24230;&#21644;&#20307;&#31215;&#32423;&#27880;&#37322;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#25968;&#25454;&#22686;&#24378;&#12289;3D&#20998;&#26512;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#20999;&#29255;&#32423;&#20998;&#31867;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#32959;&#30244;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#20581;&#24247;&#25361;&#25112;&#65292;&#20854;&#20013;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#26159;&#26368;&#20855;&#20405;&#34989;&#24615;&#30340;&#24418;&#24335;&#20043;&#19968;&#12290;&#20934;&#30830;&#30830;&#23450;O6-&#30002;&#22522;&#40479;&#22028;&#21604;-DNA&#30002;&#22522;&#36716;&#31227;&#37238;(MGMT)&#21551;&#21160;&#23376;&#30002;&#22522;&#21270;&#29366;&#24577;&#23545;&#20110;&#20010;&#20307;&#21270;&#27835;&#30103;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#36153;&#26102;&#36153;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;BTDNet&#65292;&#21033;&#29992;&#22810;&#21442;&#37327;MRI&#25195;&#25551;&#65292;&#21253;&#25324;FLAIR&#12289;T1w&#12289;T1wCE&#21644;T2 3D&#20307;&#31215;&#65292;&#39044;&#27979;MGMT&#21551;&#21160;&#23376;&#30002;&#22522;&#21270;&#29366;&#24577;&#12290;BTDNet&#35299;&#20915;&#20102;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#21487;&#21464;&#30340;&#20307;&#31215;&#38271;&#24230;&#65288;&#21363;&#27599;&#20010;&#20307;&#31215;&#21253;&#21547;&#19981;&#21516;&#25968;&#37327;&#30340;&#20999;&#29255;&#65289;&#21644;&#20307;&#31215;&#32423;&#27880;&#37322;&#65288;&#21363;&#25972;&#20010;3D&#20307;&#31215;&#34987;&#27880;&#37322;&#65292;&#32780;&#19981;&#26159;&#23427;&#21253;&#21547;&#30340;&#29420;&#31435;&#20999;&#29255;&#65289;&#12290;BTDNet&#30001;&#22235;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;i&#65289;&#25968;&#25454;&#22686;&#24378;&#65288;&#25191;&#34892;&#20960;&#20309;&#21464;&#25442;&#65292;&#25968;&#25454;&#23545;&#30340;&#20984;&#32452;&#21512;&#21644;&#27979;&#35797;&#26102;&#25968;&#25454;&#22686;&#24378;&#65289;&#65307;ii&#65289;3D&#20998;&#26512;&#65288;&#25191;&#34892;&#20840;&#23616;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#65289;&#65307;iii&#65289;&#29305;&#24449;&#25552;&#21462;&#65288;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#20999;&#29255;&#32423;&#29305;&#24449;&#65289;&#65307;iv&#65289;&#20999;&#29255;&#32423;&#20998;&#31867;&#65288;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#20999;&#29255;&#32423;&#20998;&#31867;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain tumors pose significant health challenges worldwide, with glioblastoma being one of the most aggressive forms. Accurate determination of the O6-methylguanine-DNA methyltransferase (MGMT) promoter methylation status is crucial for personalized treatment strategies. However, traditional methods are labor-intensive and time-consuming. This paper proposes a novel multi-modal approach, BTDNet, leveraging multi-parametric MRI scans, including FLAIR, T1w, T1wCE, and T2 3D volumes, to predict MGMT promoter methylation status. BTDNet addresses two main challenges: the variable volume lengths (i.e., each volume consists of a different number of slices) and the volume-level annotations (i.e., the whole 3D volume is annotated and not the independent slices that it consists of). BTDNet consists of four components: i) the data augmentation one (that performs geometric transformations, convex combinations of data pairs and test-time data augmentation); ii) the 3D analysis one (that performs glo
&lt;/p&gt;</description></item><item><title>OpenPatch&#26159;&#19968;&#20010;3D&#25340;&#36148;&#65292;&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#21462;&#20013;&#38388;&#29305;&#24449;&#26469;&#25551;&#36848;&#27599;&#20010;&#24050;&#30693;&#31867;&#21035;&#30340;&#34917;&#19969;&#34920;&#31034;&#65292;&#22312;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.03388</link><description>&lt;p&gt;
OpenPatch:&#19968;&#20010;&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#30340;3D&#25340;&#36148;
&lt;/p&gt;
&lt;p&gt;
OpenPatch: a 3D patchwork for Out-Of-Distribution detectionpdf icon. (arXiv:2310.03388v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03388
&lt;/p&gt;
&lt;p&gt;
OpenPatch&#26159;&#19968;&#20010;3D&#25340;&#36148;&#65292;&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#21462;&#20013;&#38388;&#29305;&#24449;&#26469;&#25551;&#36848;&#27599;&#20010;&#24050;&#30693;&#31867;&#21035;&#30340;&#34917;&#19969;&#34920;&#31034;&#65292;&#22312;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#23454;&#39564;&#23460;&#29615;&#22659;&#36801;&#31227;&#21040;&#24320;&#25918;&#19990;&#30028;&#65292;&#38656;&#35201;&#20351;&#23427;&#20204;&#33021;&#22815;&#22788;&#29702;&#26410;&#30693;&#26465;&#20214;&#12290;&#22312;&#19968;&#20123;&#24212;&#29992;&#20013;&#65292;&#37096;&#32626;&#36807;&#31243;&#20013;&#20986;&#29616;&#26032;&#30340;&#31867;&#21035;&#21487;&#33021;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#65292;&#22240;&#27492;&#26377;&#25928;&#22320;&#26816;&#27979;&#23427;&#20204;&#33267;&#20851;&#37325;&#35201;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#33021;&#21147;&#24212;&#35813;&#22312;&#38656;&#35201;&#26102;&#20351;&#29992;&#65292;&#32780;&#19981;&#38656;&#35201;&#22312;&#27599;&#20010;&#26032;&#20219;&#21153;&#20013;&#36827;&#34892;&#20219;&#20309;&#36827;&#19968;&#27493;&#30340;&#35745;&#31639;&#35757;&#32451;&#12290;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#28982;&#32780;&#32477;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#22788;&#29702;2D&#22270;&#20687;&#65292;&#24573;&#35270;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#22266;&#26377;&#30340;3D&#29305;&#24615;&#65292;&#24182;&#32463;&#24120;&#28151;&#28102;&#39046;&#22495;&#21644;&#35821;&#20041;&#30340;&#26032;&#39062;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#21518;&#32773;&#65292;&#32771;&#34385;&#30001;3D&#28857;&#20113;&#25429;&#25417;&#30340;&#29289;&#20307;&#20960;&#20309;&#32467;&#26500;&#65292;&#32780;&#19981;&#32771;&#34385;&#29305;&#23450;&#39046;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;OpenPatch&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#23427;&#24314;&#31435;&#22312;&#19968;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#31616;&#21333;&#22320;&#25552;&#21462;&#20174;&#20854;&#20013;&#38388;&#29305;&#24449;&#20013;&#25551;&#36848;&#27599;&#20010;&#24050;&#30693;&#31867;&#21035;&#30340;&#19968;&#32452;&#34917;&#19969;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Moving deep learning models from the laboratory setting to the open world entails preparing them to handle unforeseen conditions. In several applications the occurrence of novel classes during deployment poses a significant threat, thus it is crucial to effectively detect them. Ideally, this skill should be used when needed without requiring any further computational training effort at every new task. Out-of-distribution detection has attracted significant attention in the last years, however the majority of the studies deal with 2D images ignoring the inherent 3D nature of the real-world and often confusing between domain and semantic novelty. In this work, we focus on the latter, considering the objects geometric structure captured by 3D point clouds regardless of the specific domain. We advance the field by introducing OpenPatch that builds on a large pre-trained model and simply extracts from its intermediate features a set of patch representations that describe each known class. F
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24037;&#20316;&#37325;&#28857;&#30740;&#31350;&#38750;&#23450;&#21521;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;&#22312;&#23454;&#26102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#37319;&#29992;&#21551;&#21457;&#24335;&#38450;&#24481;&#26041;&#27861;&#26469;&#20445;&#25252;&#35745;&#31639;&#26426;&#32593;&#32476;&#20813;&#21463;&#21508;&#31181;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2310.03334</link><description>&lt;p&gt;
&#29992;&#21551;&#21457;&#24335;&#38450;&#24481;&#26041;&#27861;&#30340;&#23454;&#26102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#38750;&#23450;&#21521;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Untargeted White-box Adversarial Attack with Heuristic Defence Methods in Real-time Deep Learning based Network Intrusion Detection System. (arXiv:2310.03334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24037;&#20316;&#37325;&#28857;&#30740;&#31350;&#38750;&#23450;&#21521;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;&#22312;&#23454;&#26102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#37319;&#29992;&#21551;&#21457;&#24335;&#38450;&#24481;&#26041;&#27861;&#26469;&#20445;&#25252;&#35745;&#31639;&#26426;&#32593;&#32476;&#20813;&#21463;&#21508;&#31181;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#26159;&#20445;&#25252;&#35745;&#31639;&#26426;&#32593;&#32476;&#20813;&#21463;&#21508;&#31181;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#21644;&#32593;&#32476;&#25915;&#20987;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#19968;&#31181;&#19981;&#24184;&#30340;&#24773;&#20917;&#65292;&#21363;NIDS&#26412;&#36523;&#21463;&#21040;&#25915;&#20987;&#24182;&#26131;&#21463;&#25915;&#20987;&#65292;&#25105;&#20204;&#21487;&#20197;&#35828;&#65292;&#22914;&#20309;&#20445;&#21355;&#25421;&#21355;&#32773;&#65311;&#22312;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#65288;AML&#65289;&#20013;&#65292;&#24694;&#24847;&#34892;&#20026;&#32773;&#26088;&#22312;&#36890;&#36807;&#29305;&#24847;&#21046;&#20316;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#26469;&#27450;&#39575;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65292;&#20135;&#29983;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#36825;&#20123;&#23545;&#25239;&#24615;&#26679;&#26412;&#24050;&#25104;&#20026;ML&#21644;DL&#31995;&#32479;&#30340;&#26368;&#22823;&#28431;&#27934;&#65292;&#24182;&#19988;&#26159;&#22312;&#23454;&#26102;&#21644;&#20851;&#38190;&#20219;&#21153;&#24212;&#29992;&#65288;&#20363;&#22914;NIDS&#65289;&#20013;&#37319;&#29992;&#23427;&#20204;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;AML&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#21450;&#20854;&#38450;&#24481;&#31574;&#30053;&#30340;&#28145;&#20837;&#30740;&#31350;&#24050;&#25104;&#20026;&#20445;&#25252;&#35745;&#31639;&#26426;&#32593;&#32476;&#20813;&#21463;&#21508;&#31181;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#30340;&#24517;&#35201;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#28085;&#30422;&#19982;NI&#30456;&#20851;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network Intrusion Detection System (NIDS) is a key component in securing the computer network from various cyber security threats and network attacks. However, consider an unfortunate situation where the NIDS is itself attacked and vulnerable more specifically, we can say, How to defend the defender?. In Adversarial Machine Learning (AML), the malicious actors aim to fool the Machine Learning (ML) and Deep Learning (DL) models to produce incorrect predictions with intentionally crafted adversarial examples. These adversarial perturbed examples have become the biggest vulnerability of ML and DL based systems and are major obstacles to their adoption in real-time and mission-critical applications such as NIDS. AML is an emerging research domain, and it has become a necessity for the in-depth study of adversarial attacks and their defence strategies to safeguard the computer network from various cyber security threads. In this research work, we aim to cover important aspects related to NI
&lt;/p&gt;</description></item><item><title>Know2BIO&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#21452;&#35270;&#22270;&#28436;&#21464;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#22522;&#20934;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#26679;&#21270;&#25968;&#25454;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;KG&#30340;&#23454;&#20307;&#23545;&#40784;&#12289;&#25193;&#23637;&#24615;&#21644;&#26356;&#26032;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.03221</link><description>&lt;p&gt;
Know2BIO: &#19968;&#20010;&#20840;&#38754;&#30340;&#21452;&#35270;&#22270;&#28436;&#21464;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Know2BIO: A Comprehensive Dual-View Benchmark for Evolving Biomedical Knowledge Graphs. (arXiv:2310.03221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03221
&lt;/p&gt;
&lt;p&gt;
Know2BIO&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#21452;&#35270;&#22270;&#28436;&#21464;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#22522;&#20934;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#26679;&#21270;&#25968;&#25454;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;KG&#30340;&#23454;&#20307;&#23545;&#40784;&#12289;&#25193;&#23637;&#24615;&#21644;&#26356;&#26032;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#24050;&#32463;&#25104;&#20026;&#34920;&#31034;&#21644;&#38598;&#25104;&#22797;&#26434;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#20174;&#22810;&#26679;&#21270;&#30340;&#26469;&#28304;&#32452;&#35013;KG&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#21253;&#25324;&#23454;&#20307;&#23545;&#40784;&#65292;&#21487;&#25193;&#23637;&#24615;&#20197;&#21450;&#38656;&#35201;&#19981;&#26029;&#26356;&#26032;&#20197;&#36319;&#19978;&#31185;&#23398;&#36827;&#23637;&#12290;&#27492;&#22806;&#65292;&#30693;&#35782;&#22270;&#35889;&#30340;&#20195;&#34920;&#33021;&#21147;&#36890;&#24120;&#21463;&#21040;&#22810;&#27169;&#24577;&#25968;&#25454;&#25972;&#21512;&#30340;&#31232;&#32570;&#24615;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Know2BIO&#65292;&#19968;&#20010;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#36890;&#29992;&#24322;&#26500;KG&#22522;&#20934;&#12290;Know2BIO&#25972;&#21512;&#20102;&#26469;&#33258;30&#20010;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#25429;&#25417;&#20102;11&#20010;&#29983;&#29289;&#21307;&#23398;&#31867;&#21035;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#23427;&#30446;&#21069;&#21253;&#21547;&#32422;219,000&#20010;&#33410;&#28857;&#21644;&#32422;6,200,000&#20010;&#36793;&#12290;Know2BIO&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#25351;&#31034;&#33258;&#21160;&#26356;&#26032;&#20197;&#21453;&#26144;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#20013;&#30340;&#26368;&#26032;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;Know2BIO&#36824;&#38468;&#24102;&#22810;&#27169;&#24577;&#25968;&#25454;&#65306;&#21253;&#25324;&#25991;&#26412;&#25551;&#36848;&#12289;&#34507;&#30333;&#36136;&#21644;&#21270;&#21512;&#29289;&#24207;&#21015;&#31561;&#33410;&#28857;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) have emerged as a powerful framework for representing and integrating complex biomedical information. However, assembling KGs from diverse sources remains a significant challenge in several aspects, including entity alignment, scalability, and the need for continuous updates to keep pace with scientific advancements. Moreover, the representative power of KGs is often limited by the scarcity of multi-modal data integration. To overcome these challenges, we propose Know2BIO, a general-purpose heterogeneous KG benchmark for the biomedical domain. Know2BIO integrates data from 30 diverse sources, capturing intricate relationships across 11 biomedical categories. It currently consists of ~219,000 nodes and ~6,200,000 edges. Know2BIO is capable of user-directed automated updating to reflect the latest knowledge in biomedical science. Furthermore, Know2BIO is accompanied by multi-modal data: node features including text descriptions, protein and compound sequences and s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#26041;&#27861;&#65292;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#32771;&#34385;&#26356;&#24369;&#27169;&#22411;&#30340;&#31572;&#26696;&#19968;&#33268;&#24615;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#38382;&#39064;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#33410;&#32422;&#20351;&#29992;&#26356;&#24378;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.03094</link><description>&lt;p&gt;
&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning. (arXiv:2310.03094v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#26041;&#27861;&#65292;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#32771;&#34385;&#26356;&#24369;&#27169;&#22411;&#30340;&#31572;&#26696;&#19968;&#33268;&#24615;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#38382;&#39064;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#33410;&#32422;&#20351;&#29992;&#26356;&#24378;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#31181;&#24378;&#22823;&#30340;&#24615;&#33021;&#36890;&#24120;&#20276;&#38543;&#30528;&#20351;&#29992;&#20184;&#36153;API&#26381;&#21153;&#30340;&#39640;&#26114;&#36153;&#29992;&#12290;&#26412;&#25991;&#30340;&#30740;&#31350;&#21160;&#26426;&#26159;&#20026;&#20102;&#30740;&#31350;&#26500;&#24314;LLM&#32423;&#32852;&#20197;&#33410;&#32422;&#20351;&#29992;LLM&#30340;&#25104;&#26412;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#36827;&#34892;&#25512;&#29702;&#65288;&#20363;&#22914;&#25968;&#23398;&#12289;&#22240;&#26524;&#25512;&#29702;&#65289;&#20219;&#21153;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#32423;&#32852;&#31649;&#36947;&#36981;&#24490;&#19968;&#20010;&#30452;&#35266;&#30340;&#24605;&#24819;&#65292;&#21363;&#31616;&#21333;&#30340;&#38382;&#39064;&#21487;&#20197;&#30001;&#19968;&#20010;&#26356;&#24369;&#20294;&#26356;&#23454;&#24800;&#30340;LLM&#26469;&#35299;&#20915;&#65292;&#32780;&#21482;&#26377;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#25165;&#38656;&#35201;&#26356;&#24378;&#22823;&#12289;&#26356;&#26114;&#36149;&#30340;LLM&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#20915;&#31574;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#26356;&#24369;&#30340;LLM&#30340;&#8220;&#31572;&#26696;&#19968;&#33268;&#24615;&#8221;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#31572;&#26696;&#37319;&#26679;&#21644;&#19968;&#33268;&#24615;&#26816;&#26597;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#20004;&#31181;&#24605;&#32500;&#34920;&#31034;&#65288;&#21363;&#36830;&#32493;&#24605;&#32500;&#21644;&#31243;&#24207;&#24605;&#32500;&#65289;&#30340;&#28151;&#21512;&#12290;&#36890;&#36807;&#22312;&#20845;&#20010;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-3.5-turbo&#21644;GPT-4&#20316;&#20026;&#36739;&#24369;&#30340;&#27169;&#22411;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the "answer consistency" of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24369;&#20998;&#24067;&#19981;&#21464;&#24615;&#36827;&#34892;&#22810;&#39046;&#22495;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#34701;&#20837;&#36825;&#31181;&#19981;&#21464;&#24615;&#30340;&#33258;&#32534;&#30721;&#22120;&#33021;&#22815;&#21487;&#38752;&#22320;&#35782;&#21035;&#20986;&#31283;&#23450;&#30340;&#21464;&#37327;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.02854</link><description>&lt;p&gt;
&#36890;&#36807;&#24369;&#20998;&#24067;&#19981;&#21464;&#24615;&#23454;&#29616;&#22810;&#39046;&#22495;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Domain Causal Representation Learning via Weak Distributional Invariances. (arXiv:2310.02854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24369;&#20998;&#24067;&#19981;&#21464;&#24615;&#36827;&#34892;&#22810;&#39046;&#22495;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#34701;&#20837;&#36825;&#31181;&#19981;&#21464;&#24615;&#30340;&#33258;&#32534;&#30721;&#22120;&#33021;&#22815;&#21487;&#38752;&#22320;&#35782;&#21035;&#20986;&#31283;&#23450;&#30340;&#21464;&#37327;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#24050;&#25104;&#20026;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#26680;&#24515;&#12290;&#29305;&#21035;&#26159;&#65292;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#20026;&#23637;&#31034;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30456;&#23545;&#20110;&#26631;&#20934;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#20248;&#21183;&#25552;&#20379;&#20102;&#33258;&#28982;&#26426;&#20250;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#36807;&#20110;&#31616;&#21270;&#25968;&#25454;&#30340;&#20551;&#35774;&#65292;&#23427;&#20204;&#24448;&#24448;&#19981;&#33021;&#36866;&#29992;&#20110;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#65307;&#20363;&#22914;&#65292;&#27599;&#20010;&#39046;&#22495;&#37117;&#26469;&#33258;&#19981;&#21516;&#30340;&#21333;&#33410;&#28857;&#23436;&#32654;&#24178;&#39044;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25918;&#23485;&#20102;&#36825;&#20123;&#20551;&#35774;&#65292;&#24182;&#21033;&#29992;&#20197;&#19979;&#35266;&#23519;&#32467;&#26524;&#65306;&#22312;&#22810;&#39046;&#22495;&#25968;&#25454;&#20013;&#65292;&#24448;&#24448;&#23384;&#22312;&#19968;&#37096;&#20998;&#28508;&#21464;&#37327;&#30340;&#26576;&#20123;&#20998;&#24067;&#23646;&#24615;&#65288;&#20363;&#22914;&#25903;&#25345;&#24230;&#12289;&#26041;&#24046;&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#20445;&#25345;&#31283;&#23450;&#65307;&#24403;&#27599;&#20010;&#39046;&#22495;&#26469;&#33258;&#22810;&#33410;&#28857;&#19981;&#23436;&#32654;&#24178;&#39044;&#26102;&#65292;&#36825;&#20010;&#23646;&#24615;&#25104;&#31435;&#12290;&#21033;&#29992;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#34701;&#20837;&#36825;&#31181;&#19981;&#21464;&#24615;&#30340;&#33258;&#32534;&#30721;&#22120;&#33021;&#22815;&#21487;&#38752;&#22320;&#35782;&#21035;&#20986;&#31283;&#23450;&#30340;&#21464;&#37327;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal representation learning has emerged as the center of action in causal machine learning research. In particular, multi-domain datasets present a natural opportunity for showcasing the advantages of causal representation learning over standard unsupervised representation learning. While recent works have taken crucial steps towards learning causal representations, they often lack applicability to multi-domain datasets due to over-simplifying assumptions about the data; e.g. each domain comes from a different single-node perfect intervention. In this work, we relax these assumptions and capitalize on the following observation: there often exists a subset of latents whose certain distributional properties (e.g., support, variance) remain stable across domains; this property holds when, for example, each domain comes from a multi-node imperfect intervention. Leveraging this observation, we show that autoencoders that incorporate such invariances can provably identify the stable set o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24335;&#24314;&#27169;&#21644;&#23398;&#20064;&#38544;&#34255;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#31227;&#27010;&#29575;&#65292;&#32780;&#19981;&#26159;&#38544;&#21547;&#22320;&#32534;&#30721;&#25345;&#32493;&#26102;&#38388;&#32479;&#35745;&#30340;&#31354;&#26631;&#31614;&#12290;&#34429;&#28982;&#36716;&#31227;&#27169;&#22411;&#30340;&#35757;&#32451;&#19981;&#20250;&#25913;&#21892;&#35782;&#21035;&#24615;&#33021;&#65292;&#20294;&#23545;&#40784;&#36136;&#37327;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.02724</link><description>&lt;p&gt;
&#31070;&#32463;HMM&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#65306;&#22312;&#26631;&#31614;&#21644;&#36716;&#31227;&#27010;&#29575;&#19978;
&lt;/p&gt;
&lt;p&gt;
End-to-End Training of a Neural HMM with Label and Transition Probabilities. (arXiv:2310.02724v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24335;&#24314;&#27169;&#21644;&#23398;&#20064;&#38544;&#34255;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#31227;&#27010;&#29575;&#65292;&#32780;&#19981;&#26159;&#38544;&#21547;&#22320;&#32534;&#30721;&#25345;&#32493;&#26102;&#38388;&#32479;&#35745;&#30340;&#31354;&#26631;&#31614;&#12290;&#34429;&#28982;&#36716;&#31227;&#27169;&#22411;&#30340;&#35757;&#32451;&#19981;&#20250;&#25913;&#21892;&#35782;&#21035;&#24615;&#33021;&#65292;&#20294;&#23545;&#40784;&#36136;&#37327;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;(HMM)&#36827;&#34892;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#22312;&#38544;&#34255;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#31227;&#27010;&#29575;&#34987;&#26174;&#24335;&#22320;&#24314;&#27169;&#21644;&#23398;&#20064;&#12290;&#22823;&#22810;&#25968;&#24403;&#20195;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20801;&#35768;&#36890;&#36807;&#22312;&#32473;&#23450;&#25299;&#25169;&#32467;&#26500;&#20013;&#23545;&#25152;&#26377;&#21487;&#33021;&#30340;&#26631;&#31614;&#20998;&#21106;&#36827;&#34892;&#27714;&#21644;&#26469;&#36827;&#34892;&#20174;&#22836;&#35757;&#32451;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#23384;&#22312;&#30528;&#26174;&#24335;&#30340;&#12289;&#21487;&#23398;&#20064;&#30340;&#20998;&#21106;&#20043;&#38388;&#30340;&#36716;&#31227;&#27010;&#29575;&#65292;&#32780;&#19981;&#26159;&#38544;&#21547;&#22320;&#32534;&#30721;&#25345;&#32493;&#26102;&#38388;&#32479;&#35745;&#30340;&#31354;&#26631;&#31614;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#22522;&#20110;GPU&#30340;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#35757;&#32451;&#26631;&#31614;&#21644;&#36716;&#31227;&#27010;&#29575;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#35782;&#21035;&#32467;&#26524;&#21644;Viterbi&#23545;&#40784;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36716;&#31227;&#27169;&#22411;&#30340;&#35757;&#32451;&#34429;&#28982;&#26080;&#27861;&#25913;&#21892;&#35782;&#21035;&#24615;&#33021;&#65292;&#20294;&#23545;&#40784;&#36136;&#37327;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#29983;&#25104;&#30340;&#23545;&#40784;&#22312;&#26368;&#20808;&#36827;&#30340; Viterbi &#35757;&#32451;&#20013;&#34987;&#35777;&#26126;&#26159;&#21487;&#34892;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate a novel modeling approach for end-to-end neural network training using hidden Markov models (HMM) where the transition probabilities between hidden states are modeled and learned explicitly. Most contemporary sequence-to-sequence models allow for from-scratch training by summing over all possible label segmentations in a given topology. In our approach there are explicit, learnable probabilities for transitions between segments as opposed to a blank label that implicitly encodes duration statistics. We implement a GPU-based forward-backward algorithm that enables the simultaneous training of label and transition probabilities. We investigate recognition results and additionally Viterbi alignments of our models. We find that while the transition model training does not improve recognition performance, it has a positive impact on the alignment quality. The generated alignments are shown to be viable targets in state-of-the-art Viterbi trainings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#20984;&#25903;&#37197;&#65288;PCM&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25674;&#38144;&#20248;&#21270;&#20013;&#36924;&#36817;&#30446;&#26631;&#20989;&#25968;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#36890;&#29992;&#36924;&#36817;&#24615;&#33021;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#20984;&#20248;&#21270;&#33719;&#24471;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.02519</link><description>&lt;p&gt;
&#21442;&#21442;&#25968;&#21270;&#20984;&#25903;&#37197;&#27861;&#29992;&#20110;&#25674;&#38144;&#20248;&#21270;&#20013;&#30446;&#26631;&#20989;&#25968;&#30340;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Parameterized Convex Minorant for Objective Function Approximation in Amortized Optimization. (arXiv:2310.02519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02519
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#20984;&#25903;&#37197;&#65288;PCM&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25674;&#38144;&#20248;&#21270;&#20013;&#36924;&#36817;&#30446;&#26631;&#20989;&#25968;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#36890;&#29992;&#36924;&#36817;&#24615;&#33021;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#20984;&#20248;&#21270;&#33719;&#24471;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#20984;&#25903;&#37197;&#65288;PCM&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25674;&#38144;&#20248;&#21270;&#20013;&#36924;&#36817;&#30446;&#26631;&#20989;&#25968;&#12290;&#22312;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#30446;&#26631;&#20989;&#25968;&#36924;&#36817;&#22120;&#30001;PCM&#21644;&#38750;&#36127;&#38388;&#38553;&#20989;&#25968;&#20043;&#21644;&#34920;&#31034;&#65292;&#20854;&#20013;&#30446;&#26631;&#20989;&#25968;&#36924;&#36817;&#22120;&#22312;&#20248;&#21270;&#21464;&#37327;&#19978;&#30001;PCM&#20984;&#20989;&#25968;&#19979;&#30028;&#32422;&#26463;&#12290;&#25152;&#25552;&#20986;&#30340;&#30446;&#26631;&#20989;&#25968;&#36924;&#36817;&#22120;&#26159;&#36830;&#32493;&#20989;&#25968;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#65292;PCM&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#36798;&#21040;&#30446;&#26631;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#20984;&#20248;&#21270;&#33719;&#21462;&#30446;&#26631;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;&#20316;&#20026;&#25152;&#25552;&#26041;&#27861;&#30340;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#25193;&#23637;&#30340;&#21442;&#25968;&#21270;&#23545;&#25968;&#21644;&#25351;&#25968;&#32593;&#32476;&#65292;&#21033;&#29992;&#21442;&#25968;&#21270;&#30340;&#23545;&#25968;&#21644;&#25351;&#25968;&#32593;&#32476;&#20316;&#20026;PCM&#12290;&#23545;&#20110;&#38750;&#21442;&#25968;&#21270;&#20984;&#30446;&#26631;&#20989;&#25968;&#36924;&#36817;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#36827;&#34892;&#20102;&#25968;&#20540;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameterized convex minorant (PCM) method is proposed for the approximation of the objective function in amortized optimization. In the proposed method, the objective function approximator is expressed by the sum of a PCM and a nonnegative gap function, where the objective function approximator is bounded from below by the PCM convex in the optimization variable. The proposed objective function approximator is a universal approximator for continuous functions, and the global minimizer of the PCM attains the global minimum of the objective function approximator. Therefore, the global minimizer of the objective function approximator can be obtained by a single convex optimization. As a realization of the proposed method, extended parameterized log-sum-exp network is proposed by utilizing a parameterized log-sum-exp network as the PCM. Numerical simulation is performed for non-parameterized-convex objective function approximation and for learning-based nonlinear model predictive control 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#38544;&#31169;&#20445;&#25252;&#35821;&#35328;&#27169;&#22411;&#65288;PPLM&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#26377;&#25928;&#27880;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#35774;&#35745;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#19981;&#21516;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#20351;&#29992;&#27491;&#21521;&#21644;&#36127;&#21521;&#31034;&#20363;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#30340;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02469</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#33391;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Be Good Privacy Protection Learners. (arXiv:2310.02469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#38544;&#31169;&#20445;&#25252;&#35821;&#35328;&#27169;&#22411;&#65288;PPLM&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#26377;&#25928;&#27880;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#35774;&#35745;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#19981;&#21516;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#20351;&#29992;&#27491;&#21521;&#21644;&#36127;&#21521;&#31034;&#20363;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#30340;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20351;&#29992;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#65292;&#21019;&#24314;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#29305;&#23450;&#39046;&#22495;&#30340;&#24494;&#35843;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#25935;&#24863;&#30340;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#65288;PII&#65289;&#12290;&#22312;&#27809;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#24773;&#20917;&#19979;&#30452;&#25509;&#24494;&#35843; LLMs &#20250;&#23384;&#22312;&#20449;&#24687;&#27844;&#38706;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38544;&#31169;&#20445;&#25252;&#35821;&#35328;&#27169;&#22411;&#65288;PPLM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#26377;&#25928;&#27880;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#26032;&#33539;&#24335;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#27169;&#22411;&#35774;&#35745;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#27604;&#22914;&#35821;&#26009;&#24211;&#31574;&#23637;&#12289;&#22522;&#20110;&#24809;&#32602;&#30340;&#38750;&#27010;&#28982;&#24615;&#35757;&#32451;&#25439;&#22833;&#20197;&#21450;&#22522;&#20110;&#25351;&#20196;&#30340;&#24494;&#35843;&#31561;&#31561;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#22330;&#26223;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#20351;&#29992;&#27491;&#21521;&#21644;&#36127;&#21521;&#31034;&#20363;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#65292;&#26174;&#31034;&#20986;&#24456;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of Large Language Models (LLMs) has driven considerable interest in fine-tuning them with domain-specific data to create specialized language models. Nevertheless, such domain-specific fine-tuning data often contains sensitive personally identifiable information (PII). Direct fine-tuning LLMs on this data without privacy protection poses a risk of leakage. To address this challenge, we introduce Privacy Protection Language Models (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects domain-specific knowledge while safeguarding data privacy. Our work offers a theoretical analysis for model design and delves into various techniques such as corpus curation, penalty-based unlikelihood in training loss, and instruction-based tuning, etc. Extensive experiments across diverse datasets and scenarios demonstrate the effectiveness of our approaches. In particular, instruction tuning with both positive and negative examples, stands out as a promising method, eff
&lt;/p&gt;</description></item><item><title>&#22270;&#21435;&#23398;&#20064;&#26159;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#21024;&#38500;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#25935;&#24863;&#25968;&#25454;&#30165;&#36857;&#26469;&#32500;&#25252;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#12290;&#36825;&#31687;&#32508;&#36848;&#24615;&#35770;&#25991;&#39318;&#27425;&#31995;&#32479;&#22238;&#39038;&#20102;&#22270;&#21435;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20102;&#21508;&#31181;&#26041;&#27861;&#23398;&#65292;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#20998;&#31867;&#21644;&#26368;&#26032;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#20197;&#24110;&#21161;&#26032;&#36827;&#20837;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;&#12290;&#19982;&#24046;&#20998;&#38544;&#31169;&#30340;&#20851;&#31995;&#21152;&#28145;&#20102;&#23545;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.02164</link><description>&lt;p&gt;
&#22270;&#21435;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Graph Unlearning. (arXiv:2310.02164v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02164
&lt;/p&gt;
&lt;p&gt;
&#22270;&#21435;&#23398;&#20064;&#26159;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#21024;&#38500;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#25935;&#24863;&#25968;&#25454;&#30165;&#36857;&#26469;&#32500;&#25252;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#12290;&#36825;&#31687;&#32508;&#36848;&#24615;&#35770;&#25991;&#39318;&#27425;&#31995;&#32479;&#22238;&#39038;&#20102;&#22270;&#21435;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20102;&#21508;&#31181;&#26041;&#27861;&#23398;&#65292;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#20998;&#31867;&#21644;&#26368;&#26032;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#20197;&#24110;&#21161;&#26032;&#36827;&#20837;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;&#12290;&#19982;&#24046;&#20998;&#38544;&#31169;&#30340;&#20851;&#31995;&#21152;&#28145;&#20102;&#23545;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21435;&#23398;&#20064;&#26159;&#22312;&#36861;&#27714;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#23427;&#25552;&#20379;&#20102;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21024;&#38500;&#25935;&#24863;&#25968;&#25454;&#30165;&#36857;&#30340;&#26041;&#27861;&#65292;&#20197;&#32500;&#25252;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#12290;&#26174;&#28982;&#65292;&#22270;&#26426;&#22120;&#23398;&#20064;&#23545;&#25968;&#25454;&#38544;&#31169;&#21644;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#25935;&#24863;&#24615;&#65292;&#22240;&#27492;&#38656;&#35201;&#24212;&#29992;&#22270;&#21435;&#23398;&#20064;&#25216;&#26415;&#26469;&#26377;&#25928;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#24615;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#22270;&#21435;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26041;&#27861;&#23398;&#65292;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#20998;&#31867;&#21644;&#26368;&#26032;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#20197;&#24110;&#21161;&#26032;&#36827;&#20837;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#22270;&#21435;&#23398;&#20064;&#19982;&#24046;&#20998;&#38544;&#31169;&#20043;&#38388;&#30340;&#37325;&#35201;&#32852;&#31995;&#65292;&#22686;&#24378;&#20102;&#25105;&#20204;&#23545;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#30456;&#20851;&#24615;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#20445;&#35777;&#28165;&#26224;&#24230;&#65292;&#25105;&#20204;&#23545;&#22270;&#21435;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#20102;&#31616;&#26126;&#25212;&#35201;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph unlearning emerges as a crucial advancement in the pursuit of responsible AI, providing the means to remove sensitive data traces from trained models, thereby upholding the right to be forgotten. It is evident that graph machine learning exhibits sensitivity to data privacy and adversarial attacks, necessitating the application of graph unlearning techniques to address these concerns effectively. In this comprehensive survey paper, we present the first systematic review of graph unlearning approaches, encompassing a diverse array of methodologies and offering a detailed taxonomy and up-to-date literature overview to facilitate the understanding of researchers new to this field. Additionally, we establish the vital connections between graph unlearning and differential privacy, augmenting our understanding of the relevance of privacy-preserving techniques in this context. To ensure clarity, we provide lucid explanations of the fundamental concepts and evaluation measures used in gr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#19981;&#21516;&#25915;&#20987;&#22330;&#26223;&#19979;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#21518;&#38376;&#20928;&#21270;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20302;&#27745;&#26579;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#21518;&#38376;&#21644;&#24178;&#20928;&#29305;&#24449;&#20043;&#38388;&#30340;&#32416;&#32544;&#20250;&#21066;&#24369;&#35843;&#25972;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.01875</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#28418;&#31227;&#35843;&#25972;&#23454;&#29616;&#31283;&#23450;&#30340;&#21518;&#38376;&#20928;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Stable Backdoor Purification through Feature Shift Tuning. (arXiv:2310.01875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#19981;&#21516;&#25915;&#20987;&#22330;&#26223;&#19979;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#21518;&#38376;&#20928;&#21270;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20302;&#27745;&#26579;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#21518;&#38376;&#21644;&#24178;&#20928;&#29305;&#24449;&#20043;&#38388;&#30340;&#32416;&#32544;&#20250;&#21066;&#24369;&#35843;&#25972;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#31713;&#25913;&#19968;&#23567;&#32452;&#35757;&#32451;&#26679;&#26412;&#26469;&#24694;&#24847;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#38450;&#24481;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#23041;&#32961;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#38656;&#35201;&#23545;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#22797;&#26434;&#20462;&#25913;&#65292;&#35201;&#20040;&#20005;&#37325;&#20381;&#36182;&#29305;&#23450;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#24494;&#35843;&#24320;&#22987;&#65292;&#36890;&#36807;&#23545;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#30340;&#20840;&#38754;&#35780;&#20272;&#26469;&#25506;&#32034;&#26368;&#24120;&#35265;&#21644;&#26131;&#20110;&#37096;&#32626;&#30340;&#21518;&#38376;&#38450;&#24481;&#26041;&#27861;&#12290;&#36890;&#36807;&#21021;&#27493;&#23454;&#39564;&#35266;&#23519;&#21457;&#29616;&#65292;&#19982;&#39640;&#27745;&#26579;&#29575;&#30340;&#26377;&#24076;&#26395;&#30340;&#38450;&#24481;&#32467;&#26524;&#30456;&#27604;&#65292;&#26222;&#36890;&#30340;&#35843;&#25972;&#26041;&#27861;&#22312;&#20302;&#27745;&#26579;&#29575;&#22330;&#26223;&#19979;&#23436;&#20840;&#22833;&#25928;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#20302;&#27745;&#26579;&#29575;&#19979;&#65292;&#21518;&#38376;&#21644;&#24178;&#20928;&#29305;&#24449;&#20043;&#38388;&#30340;&#32416;&#32544;&#30772;&#22351;&#20102;&#22522;&#20110;&#35843;&#25972;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been widely observed that deep neural networks (DNN) are vulnerable to backdoor attacks where attackers could manipulate the model behavior maliciously by tampering with a small set of training samples. Although a line of defense methods is proposed to mitigate this threat, they either require complicated modifications to the training process or heavily rely on the specific model architecture, which makes them hard to deploy into real-world applications. Therefore, in this paper, we instead start with fine-tuning, one of the most common and easy-to-deploy backdoor defenses, through comprehensive evaluations against diverse attack scenarios. Observations made through initial experiments show that in contrast to the promising defensive results on high poisoning rates, vanilla tuning methods completely fail at low poisoning rate scenarios. Our analysis shows that with the low poisoning rate, the entanglement between backdoor and clean features undermines the effect of tuning-based 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36807;&#21442;&#25968;&#21270;&#22914;&#20309;&#24433;&#21709;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#34892;&#20026;&#65292;&#22312;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#35774;&#32622;&#19979;&#32473;&#20986;&#20102;&#19981;&#21516;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.01769</link><description>&lt;p&gt;
&#36807;&#21442;&#25968;&#21270;&#22914;&#20309;&#20943;&#32531;&#30697;&#38453;&#24863;&#30693;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#65306;&#23545;&#31216;&#24615;&#21644;&#21021;&#22987;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing: The Curses of Symmetry and Initialization. (arXiv:2310.01769v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01769
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36807;&#21442;&#25968;&#21270;&#22914;&#20309;&#24433;&#21709;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#34892;&#20026;&#65292;&#22312;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#35774;&#32622;&#19979;&#32473;&#20986;&#20102;&#19981;&#21516;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#38416;&#36848;&#20102;&#36807;&#21442;&#25968;&#21270;&#22914;&#20309;&#25913;&#21464;&#26799;&#24230;&#19979;&#38477;&#22312;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#22312;&#23545;&#31216;&#35774;&#32622;&#20013;&#65292;&#36890;&#36807;&#23545;&#31216;&#21442;&#25968;&#21270;&#23398;&#20064;&#26410;&#30693;&#30340;&#21322;&#27491;&#23450;&#30697;&#38453;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#65288;$k&gt;r$&#65289;&#38543;&#26426;&#21021;&#22987;&#21270;&#26799;&#24230;&#19979;&#38477;&#30340;&#26032;&#22411;$\Omega (1/T^2)$&#19979;&#30028;&#65292;&#19982;&#31934;&#30830;&#21442;&#25968;&#21270;&#24773;&#20917;&#65288;$k=r$&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;$\exp (-\Omega (T))$&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#23545;&#31216;&#35774;&#32622;&#65292;&#20854;&#20013;$M^* \in \mathbb{R}^{n_1 \times n_2}$&#26159;&#26410;&#30693;&#30697;&#38453;&#65292;&#37319;&#29992;&#38750;&#23545;&#31216;&#21442;&#25968;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper rigorously shows how over-parameterization changes the convergence behaviors of gradient descent (GD) for the matrix sensing problem, where the goal is to recover an unknown low-rank ground-truth matrix from near-isotropic linear measurements. First, we consider the symmetric setting with the symmetric parameterization where $M^* \in \mathbb{R}^{n \times n}$ is a positive semi-definite unknown matrix of rank $r \ll n$, and one uses a symmetric parameterization $XX^\top$ to learn $M^*$. Here $X \in \mathbb{R}^{n \times k}$ with $k &gt; r$ is the factor matrix. We give a novel $\Omega (1/T^2)$ lower bound of randomly initialized GD for the over-parameterized case ($k &gt;r$) where $T$ is the number of iterations. This is in stark contrast to the exact-parameterization scenario ($k=r$) where the convergence rate is $\exp (-\Omega (T))$. Next, we study asymmetric setting where $M^* \in \mathbb{R}^{n_1 \times n_2}$ is the unknown matrix of rank $r \ll \min\{n_1,n_2\}$, and one uses an 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#31216;&#20026;&#31867;&#27604;&#25552;&#31034;&#65292;&#29992;&#20110;&#33258;&#21160;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#33258;&#21160;&#29983;&#25104;&#30456;&#20851;&#23454;&#20363;&#25110;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01714</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#31867;&#27604;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Analogical Reasoners. (arXiv:2310.01714v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#31216;&#20026;&#31867;&#27604;&#25552;&#31034;&#65292;&#29992;&#20110;&#33258;&#21160;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#33258;&#21160;&#29983;&#25104;&#30456;&#20851;&#23454;&#20363;&#25110;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#26377;&#26631;&#35760;&#30340;&#25512;&#29702;&#36807;&#31243;&#31034;&#20363;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#31216;&#20026;&#31867;&#27604;&#25552;&#31034;&#65292;&#26088;&#22312;&#33258;&#21160;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#21463;&#31867;&#27604;&#25512;&#29702;&#30340;&#21551;&#21457;&#65292;&#31867;&#27604;&#25512;&#29702;&#26159;&#19968;&#31181;&#35748;&#30693;&#36807;&#31243;&#65292;&#20154;&#31867;&#20174;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#20013;&#33719;&#21462;&#30693;&#35782;&#26469;&#35299;&#20915;&#26032;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20419;&#20351;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#19978;&#19979;&#25991;&#20013;&#30340;&#30456;&#20851;&#23454;&#20363;&#25110;&#30693;&#35782;&#65292;&#28982;&#21518;&#35299;&#20915;&#32473;&#23450;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#20197;&#19979;&#20960;&#20010;&#20248;&#28857;&#65306;&#23427;&#30465;&#21435;&#20102;&#26631;&#35760;&#25110;&#26816;&#32034;&#23454;&#20363;&#30340;&#38656;&#27714;&#65292;&#25552;&#20379;&#20102;&#26222;&#36866;&#24615;&#21644;&#20415;&#21033;&#24615;&#65307;&#23427;&#36824;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#38382;&#39064;&#23450;&#21046;&#29983;&#25104;&#30340;&#31034;&#20363;&#21644;&#30693;&#35782;&#65292;&#25552;&#20379;&#20102;&#36866;&#24212;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#20248;&#20110;0-shot CoT&#21644;&#25163;&#21160;few-shot CoT&#65292;&#21253;&#25324;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, Analogical Prompting, designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving i
&lt;/p&gt;</description></item><item><title>GRID&#26159;&#19968;&#20010;&#26500;&#24314;&#36890;&#29992;&#26426;&#22120;&#20154;&#26234;&#33021;&#30340;&#24320;&#21457;&#24179;&#21488;&#65292;&#36890;&#36807;&#22522;&#30784;&#27169;&#22411;&#21644;&#25193;&#23637;&#24615;&#35774;&#35745;&#26469;&#35299;&#20915;&#29305;&#23450;&#24212;&#29992;&#21644;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00887</link><description>&lt;p&gt;
GRID: &#36890;&#29992;&#26426;&#22120;&#20154;&#26234;&#33021;&#24320;&#21457;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
GRID: A Platform for General Robot Intelligence Development. (arXiv:2310.00887v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00887
&lt;/p&gt;
&lt;p&gt;
GRID&#26159;&#19968;&#20010;&#26500;&#24314;&#36890;&#29992;&#26426;&#22120;&#20154;&#26234;&#33021;&#30340;&#24320;&#21457;&#24179;&#21488;&#65292;&#36890;&#36807;&#22522;&#30784;&#27169;&#22411;&#21644;&#25193;&#23637;&#24615;&#35774;&#35745;&#26469;&#35299;&#20915;&#29305;&#23450;&#24212;&#29992;&#21644;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#21644;&#33258;&#20027;&#31995;&#32479;&#20013;&#24320;&#21457;&#26426;&#22120;&#26234;&#33021;&#33021;&#21147;&#26159;&#19968;&#39033;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#65292;&#38590;&#20197;&#25512;&#24191;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#22686;&#21152;&#20102;&#37096;&#32626;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36890;&#29992;&#26426;&#22120;&#20154;&#26234;&#33021;&#24320;&#21457;&#24179;&#21488;&#65288;GRID&#65289;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#35813;&#24179;&#21488;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#23398;&#20064;&#12289;&#32452;&#21512;&#21644;&#36866;&#24212;&#20854;&#29289;&#29702;&#33021;&#21147;&#12289;&#29615;&#22659;&#38480;&#21046;&#21644;&#30446;&#26631;&#12290;&#35813;&#24179;&#21488;&#36890;&#36807;&#20102;&#35299;&#29289;&#29702;&#19990;&#30028;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#35299;&#20915;&#26426;&#22120;&#20154;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#12290;GRID&#20174;&#26681;&#26412;&#19978;&#35774;&#35745;&#25104;&#21487;&#25193;&#23637;&#30340;&#65292;&#20197;&#36866;&#24212;&#26032;&#31867;&#22411;&#30340;&#26426;&#22120;&#20154;&#12289;&#36710;&#36742;&#12289;&#30828;&#20214;&#24179;&#21488;&#21644;&#36719;&#20214;&#21327;&#35758;&#12290;&#27492;&#22806;&#65292;&#27169;&#22359;&#21270;&#35774;&#35745;&#20351;&#21508;&#31181;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#21644;&#29616;&#26377;&#22522;&#30784;&#27169;&#22411;&#22312;&#26356;&#24191;&#27867;&#30340;&#20197;&#26426;&#22120;&#20154;&#20026;&#20013;&#24515;&#30340;&#38382;&#39064;&#20013;&#23481;&#26131;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#23637;&#31034;&#20102;&#36825;&#20010;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing machine intelligence abilities in robots and autonomous systems is an expensive and time consuming process. Existing solutions are tailored to specific applications and are harder to generalize. Furthermore, scarcity of training data adds a layer of complexity in deploying deep machine learning models. We present a new platform for General Robot Intelligence Development (GRID) to address both of these issues. The platform enables robots to learn, compose and adapt skills to their physical capabilities, environmental constraints and goals. The platform addresses AI problems in robotics via foundation models that know the physical world. GRID is designed from the ground up to be extensible to accommodate new types of robots, vehicles, hardware platforms and software protocols. In addition, the modular design enables various deep ML components and existing foundation models to be easily usable in a wider variety of robot-centric problems. We demonstrate the platform in various 
&lt;/p&gt;</description></item><item><title>GeRA&#26159;&#19968;&#31181;&#26631;&#31614;&#25928;&#29575;&#30340;&#20960;&#20309;&#27491;&#21017;&#21270;&#23545;&#40784;&#26041;&#27861;&#65292;&#21033;&#29992;&#26410;&#37197;&#23545;&#25968;&#25454;&#30340;&#27969;&#24418;&#20960;&#20309;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#23545;&#40784;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00672</link><description>&lt;p&gt;
GeRA: &#26631;&#31614;&#25928;&#29575;&#30340;&#20960;&#20309;&#27491;&#21017;&#21270;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
GeRA: Label-Efficient Geometrically Regularized Alignment. (arXiv:2310.00672v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00672
&lt;/p&gt;
&lt;p&gt;
GeRA&#26159;&#19968;&#31181;&#26631;&#31614;&#25928;&#29575;&#30340;&#20960;&#20309;&#27491;&#21017;&#21270;&#23545;&#40784;&#26041;&#27861;&#65292;&#21033;&#29992;&#26410;&#37197;&#23545;&#25968;&#25454;&#30340;&#27969;&#24418;&#20960;&#20309;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#23545;&#40784;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#23558;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#34701;&#20837;&#23884;&#20837;&#31354;&#38388;&#32467;&#26500;&#20013;&#12290;&#20026;&#20102;&#20855;&#26377;&#31867;&#20284;&#30340;&#20449;&#24687;&#37327;&#65292;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#25104;&#23545;&#25968;&#25454;&#36827;&#34892;&#23545;&#40784;&#21644;&#35757;&#32451;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#20960;&#20309;&#27491;&#21017;&#21270;&#23545;&#40784;&#26041;&#27861;&#65288;GeRA&#65289;&#65292;&#20197;&#26631;&#31614;&#39640;&#25928;&#30340;&#26041;&#24335;&#23545;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26410;&#37197;&#23545;&#65288;&#26410;&#26631;&#35760;&#65289;&#25968;&#25454;&#30340;&#27969;&#24418;&#20960;&#20309;&#26469;&#25552;&#39640;&#23545;&#40784;&#24615;&#33021;&#12290;&#20026;&#20102;&#38450;&#27490;&#22312;&#23545;&#40784;&#36807;&#31243;&#20013;&#23545;&#23616;&#37096;&#20960;&#20309;&#36896;&#25104;&#22833;&#30495;&#65292;&#21487;&#33021;&#30772;&#22351;&#35821;&#20041;&#37051;&#22495;&#32467;&#26500;&#24182;&#23548;&#33268;&#26410;&#35266;&#23519;&#21040;&#30340;&#23545;&#20135;&#29983;&#38169;&#20301;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20309;&#25439;&#22833;&#39033;&#12290;&#35813;&#39033;&#22522;&#20110;&#19968;&#20010;&#25429;&#33719;&#21333;&#27169;&#24577;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#23616;&#37096;&#27969;&#24418;&#20960;&#20309;&#30340;&#25193;&#25955;&#31639;&#23376;&#26500;&#24314;&#12290;GeRA&#26159;&#27169;&#24577;&#26080;&#20851;&#30340;&#65292;&#22240;&#27492;&#21487;&#20197;&#29992;&#20110;&#23545;&#40784;&#20219;&#20309;&#25968;&#25454;&#27169;&#24577;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#26469;&#25903;&#25345;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained unimodal encoders incorporate rich semantic information into embedding space structures. To be similarly informative, multi-modal encoders typically require massive amounts of paired data for alignment and training. We introduce a semi-supervised Geometrically Regularized Alignment (GeRA) method to align the embedding spaces of pretrained unimodal encoders in a label-efficient way. Our method leverages the manifold geometry of unpaired (unlabeled) data to improve alignment performance. To prevent distortions to local geometry during the alignment process, potentially disrupting semantic neighborhood structures and causing misalignment of unobserved pairs, we introduce a geometric loss term. This term is built upon a diffusion operator that captures the local manifold geometry of the unimodal pretrained encoders. GeRA is modality-agnostic and thus can be used to align pretrained encoders from any data modalities. We provide empirical evidence to the effectiveness of our metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;OptGNN&#65292;&#21033;&#29992;&#21322;&#23450;&#35268;&#21010;&#24037;&#20855;&#33719;&#24471;&#22823;&#31867;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#12290;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#31639;&#27861;&#21644;&#20256;&#32479;&#31639;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;OptGNN&#30340;&#33021;&#21147;&#35774;&#35745;&#20102;&#19968;&#20010;&#20135;&#29983;&#20248;&#21270;&#30340;&#23545;&#20598;&#35777;&#20070;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.00526</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#21542;&#20316;&#20026;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Graph Neural Networks Optimal Approximation Algorithms?. (arXiv:2310.00526v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;OptGNN&#65292;&#21033;&#29992;&#21322;&#23450;&#35268;&#21010;&#24037;&#20855;&#33719;&#24471;&#22823;&#31867;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#12290;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#31639;&#27861;&#21644;&#20256;&#32479;&#31639;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;OptGNN&#30340;&#33021;&#21147;&#35774;&#35745;&#20102;&#19968;&#20010;&#20135;&#29983;&#20248;&#21270;&#30340;&#23545;&#20598;&#35777;&#20070;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33021;&#22815;&#20351;&#29992;&#21322;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#24378;&#22823;&#30340;&#31639;&#27861;&#24037;&#20855;&#26469;&#33719;&#24471;&#22823;&#31867;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#21487;&#20197;&#34920;&#31034;&#26368;&#24378;&#22823;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#21069;&#25552;&#26159;&#20551;&#35774;&#21807;&#19968;&#28216;&#25103;&#29468;&#24819;&#25104;&#31435;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#32467;&#26524;&#26500;&#24314;&#20102;&#39640;&#25928;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;OptGNN&#65292;&#23427;&#22312;&#35832;&#22914;&#26368;&#22823;&#21106;&#21644;&#26368;&#22823;&#29420;&#31435;&#38598;&#31561;&#37325;&#35201;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#33719;&#24471;&#20102;&#39640;&#36136;&#37327;&#30340;&#36817;&#20284;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#19981;&#20165;&#36229;&#36807;&#20102;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#31639;&#27861;&#65292;&#36824;&#36229;&#36807;&#20102;&#20256;&#32479;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;OptGNN&#25429;&#25417;&#20984;&#26494;&#24347;&#30340;&#33021;&#21147;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#20135;&#29983;&#20248;&#21270;&#30340;&#23545;&#20598;&#35777;&#20070;&#65288;&#30830;&#23450;&#24615;&#19978;&#30028;&#65289;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we design graph neural network architectures that can be used to obtain optimal approximation algorithms for a large class of combinatorial optimization problems using powerful algorithmic tools from semidefinite programming (SDP). Concretely, we prove that polynomial-sized message passing algorithms can represent the most powerful polynomial time algorithms for Max Constraint Satisfaction Problems assuming the Unique Games Conjecture. We leverage this result to construct efficient graph neural network architectures, OptGNN, that obtain high-quality approximate solutions on landmark combinatorial optimization problems such as Max Cut and maximum independent set. Our approach achieves strong empirical results across a wide range of real-world and synthetic datasets against both neural baselines and classical algorithms. Finally, we take advantage of OptGNN's ability to capture convex relaxations to design an algorithm for producing dual certificates of optimality (bounds on
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20223;&#30495;&#30340;&#33258;&#21160;&#29983;&#25104;&#26580;&#24615;&#26426;&#22120;&#20154;&#22235;&#36275;&#34892;&#36208;&#27493;&#24577;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#35745;&#31639;&#37327;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.00498</link><description>&lt;p&gt;
&#26580;&#24615;&#26426;&#22120;&#20154;&#22235;&#36275;&#34892;&#36208;&#30340;&#33258;&#21160;&#29983;&#25104;&#27493;&#24577;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Automated Gait Generation For Walking, Soft Robotic Quadrupeds. (arXiv:2310.00498v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00498
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20223;&#30495;&#30340;&#33258;&#21160;&#29983;&#25104;&#26580;&#24615;&#26426;&#22120;&#20154;&#22235;&#36275;&#34892;&#36208;&#27493;&#24577;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#35745;&#31639;&#37327;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36719;&#24335;&#25191;&#34892;&#22120;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#21644;&#39640;&#32500;&#36755;&#20837;&#31354;&#38388;&#65292;&#26580;&#24615;&#26426;&#22120;&#20154;&#30340;&#27493;&#24577;&#29983;&#25104;&#23384;&#22312;&#19968;&#23450;&#30340;&#25361;&#25112;&#12290;&#36719;&#24335;&#26426;&#22120;&#20154;&#25511;&#21046;&#21644;&#24863;&#30693;&#30340;&#38480;&#21046;&#36843;&#20351;&#30740;&#31350;&#20154;&#21592;&#25163;&#24037;&#35774;&#35745;&#24320;&#29615;&#25511;&#21046;&#22120;&#26469;&#29983;&#25104;&#27493;&#24577;&#24207;&#21015;&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#36719;&#24335;&#25191;&#34892;&#22120;&#30340;&#23551;&#21629;&#30701;&#21644;&#25191;&#34892;&#22120;&#34892;&#20026;&#30340;&#33258;&#28982;&#21464;&#21270;&#38480;&#21046;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#19982;&#26426;&#22120;&#20154;&#37096;&#32626;&#26102;&#38388;&#30456;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#30001;&#20110;&#26580;&#24615;&#26426;&#22120;&#20154;&#26448;&#26009;&#30340;&#24322;&#36136;&#24615;&#21644;&#38750;&#32447;&#24615;&#65292;&#27169;&#25311;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#30340;&#65292;&#32780;&#19988;&#26448;&#26009;&#30340;&#21160;&#21147;&#23398;&#22240;&#30952;&#25439;&#32780;&#21457;&#29983;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26080;&#38656;&#20223;&#30495;&#30340;&#33258;&#21160;&#29983;&#25104;&#36719;&#24335;&#26426;&#22120;&#20154;&#27493;&#24577;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#38750;&#24120;&#23569;&#30340;&#35745;&#31639;&#12290;&#35813;&#25216;&#26415;&#22312;&#19968;&#20010;&#30001;16&#20010;&#8220;&#20132;&#38169;&#21098;&#20999;&#30340;&#22855;&#24322;&#21152;&#20493;&#8221;&#65288;HSA&#65289;&#25191;&#34892;&#22120;&#26500;&#25104;&#30340;&#22235;&#36275;&#30005;&#21160;&#36719;&#24335;&#26426;&#22120;&#20154;&#19978;&#24471;&#21040;&#20102;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gait generation for soft robots is challenging due to the nonlinear dynamics and high dimensional input spaces of soft actuators. Limitations in soft robotic control and perception force researchers to hand-craft open loop controllers for gait sequences, which is a non-trivial process. Moreover, short soft actuator lifespans and natural variations in actuator behavior limit machine learning techniques to settings that can be learned on the same time scales as robot deployment. Lastly, simulation is not always possible, due to heterogeneity and nonlinearity in soft robotic materials and their dynamics change due to wear. We present a sample-efficient, simulation free, method for self-generating soft robot gaits, using very minimal computation. This technique is demonstrated on a motorized soft robotic quadruped that walks using four legs constructed from 16 "handed shearing auxetic" (HSA) actuators. To manage the dimension of the search space, gaits are composed of two sequential sets o
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#39318;&#27425;&#25506;&#32034;&#20102;&#22312;&#20803;&#23398;&#20064;&#26694;&#26550;&#20013;&#31070;&#32463;&#23849;&#28291;&#23646;&#24615;&#30340;&#29305;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#31070;&#32463;&#23849;&#28291;&#29616;&#35937;&#20013;&#36755;&#20837;&#29305;&#24449;&#12289;&#31867;&#21035;&#24179;&#22343;&#20540;&#21644;&#20998;&#31867;&#22120;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.00451</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#23849;&#28291;&#22312;&#20803;&#23398;&#20064;&#27169;&#22411;&#20013;&#22312;Few-shot&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Role of Neural Collapse in Meta Learning Models for Few-shot Learning. (arXiv:2310.00451v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00451
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#39318;&#27425;&#25506;&#32034;&#20102;&#22312;&#20803;&#23398;&#20064;&#26694;&#26550;&#20013;&#31070;&#32463;&#23849;&#28291;&#23646;&#24615;&#30340;&#29305;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#31070;&#32463;&#23849;&#28291;&#29616;&#35937;&#20013;&#36755;&#20837;&#29305;&#24449;&#12289;&#31867;&#21035;&#24179;&#22343;&#20540;&#21644;&#20998;&#31867;&#22120;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Few-shot&#23398;&#20064;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#26088;&#22312;&#23398;&#20064;&#33021;&#22815;&#22312;&#24456;&#23569;&#30340;&#35757;&#32451;&#26679;&#26412;&#19979;&#36805;&#36895;&#23398;&#20064;&#26032;&#25216;&#33021;&#25110;&#36866;&#24212;&#26032;&#29615;&#22659;&#30340;&#27169;&#22411;&#12290;&#36825;&#23548;&#33268;&#25152;&#24320;&#21457;&#30340;&#27169;&#22411;&#20855;&#26377;&#21521;&#26032;&#31867;&#21035;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21482;&#38656;&#24456;&#23569;&#30340;&#26631;&#35760;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32593;&#32476;&#34987;&#30475;&#20316;&#26159;&#40657;&#30418;&#27169;&#22411;&#65292;&#29702;&#35299;&#22312;&#19981;&#21516;&#23398;&#20064;&#22330;&#26223;&#19979;&#23398;&#21040;&#30340;&#34920;&#31034;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#31070;&#32463;&#23849;&#28291;&#65288;NC&#65289;&#26159;&#26368;&#36817;&#21457;&#29616;&#30340;&#19968;&#31181;&#29616;&#35937;&#65292;&#23427;&#23637;&#31034;&#20102;&#32593;&#32476;&#22312;&#36235;&#36817;&#20110;&#38646;&#25439;&#22833;&#26102;&#30340;&#29420;&#29305;&#29305;&#24615;&#12290;&#36755;&#20837;&#29305;&#24449;&#20250;&#23849;&#28291;&#21040;&#21508;&#33258;&#30340;&#31867;&#21035;&#24179;&#22343;&#20540;&#65292;&#31867;&#21035;&#24179;&#22343;&#20540;&#20250;&#24418;&#25104;&#19968;&#20010;&#31616;&#21333;&#30340;&#31561;&#35282;&#32039;&#26694;&#26550;&#65288;ETF&#65289;&#65292;&#20854;&#20013;&#31867;&#21035;&#24179;&#22343;&#20540;&#26368;&#22823;&#22320;&#20998;&#31163;&#19988;&#32447;&#24615;&#21487;&#20998;&#65292;&#20998;&#31867;&#22120;&#20805;&#24403;&#19968;&#20010;&#31616;&#21333;&#30340;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#12290;&#23613;&#31649;&#36825;&#20123;&#29616;&#35937;&#22312;&#31616;&#21333;&#30340;&#20998;&#31867;&#32593;&#32476;&#20013;&#24050;&#32463;&#35266;&#23519;&#21040;&#65292;&#20294;&#36825;&#20010;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#25506;&#32034;&#21644;&#29702;&#35299;&#20803;&#23398;&#20064;&#26694;&#26550;&#20013;&#31070;&#32463;&#23849;&#28291;&#23646;&#24615;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning frameworks for few-shot learning aims to learn models that can learn new skills or adapt to new environments rapidly with a few training examples. This has led to the generalizability of the developed model towards new classes with just a few labelled samples. However these networks are seen as black-box models and understanding the representations learnt under different learning scenarios is crucial. Neural collapse ($\mathcal{NC}$) is a recently discovered phenomenon which showcases unique properties at the network proceeds towards zero loss. The input features collapse to their respective class means, the class means form a Simplex equiangular tight frame (ETF) where the class means are maximally distant and linearly separable, and the classifier acts as a simple nearest neighbor classifier. While these phenomena have been observed in simple classification networks, this study is the first to explore and understand the properties of neural collapse in meta learning fra
&lt;/p&gt;</description></item><item><title>AdaptNet&#26159;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#25511;&#21046;&#30340;&#31574;&#30053;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#29616;&#26377;&#31574;&#30053;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#21487;&#20197;&#20174;&#31867;&#20284;&#20219;&#21153;&#20013;&#24555;&#36895;&#23398;&#20064;&#21040;&#26032;&#30340;&#34892;&#20026;&#65292;&#26174;&#33879;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.00239</link><description>&lt;p&gt;
AdaptNet: &#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#25511;&#21046;&#30340;&#31574;&#30053;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
AdaptNet: Policy Adaptation for Physics-Based Character Control. (arXiv:2310.00239v2 [cs.GR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00239
&lt;/p&gt;
&lt;p&gt;
AdaptNet&#26159;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#25511;&#21046;&#30340;&#31574;&#30053;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#29616;&#26377;&#31574;&#30053;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#21487;&#20197;&#20174;&#31867;&#20284;&#20219;&#21153;&#20013;&#24555;&#36895;&#23398;&#20064;&#21040;&#26032;&#30340;&#34892;&#20026;&#65292;&#26174;&#33879;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20154;&#31867;&#22312;&#23398;&#20064;&#26032;&#25216;&#33021;&#26102;&#33021;&#22815;&#36866;&#24212;&#29616;&#26377;&#25216;&#33021;&#30340;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaptNet&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20462;&#25913;&#29616;&#26377;&#31574;&#30053;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#20854;&#33021;&#22815;&#20174;&#31867;&#20284;&#20219;&#21153;&#20013;&#24555;&#36895;&#23398;&#20064;&#21040;&#26032;&#30340;&#34892;&#20026;&#65292;&#30456;&#27604;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#12290;AdaptNet&#22312;&#32473;&#23450;&#30340;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22120;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#20004;&#23618;&#27425;&#32467;&#26500;&#65292;&#36890;&#36807;&#22686;&#21152;&#21407;&#22987;&#29366;&#24577;&#23884;&#20837;&#26469;&#25903;&#25345;&#34892;&#20026;&#30340;&#36866;&#24230;&#21464;&#21270;&#65292;&#24182;&#36827;&#19968;&#27493;&#20462;&#25913;&#31574;&#30053;&#32593;&#32476;&#23618;&#26469;&#23454;&#29616;&#26356;&#28145;&#36828;&#30340;&#21464;&#21270;&#12290;&#35813;&#25216;&#26415;&#34987;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#36866;&#24212;&#29616;&#26377;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#25511;&#21046;&#22120;&#20197;&#36866;&#24212;&#24191;&#27867;&#30340;&#26032;&#30340;&#36816;&#21160;&#39118;&#26684;&#12289;&#26032;&#30340;&#20219;&#21153;&#30446;&#26631;&#12289;&#35282;&#33394;&#24418;&#24577;&#30340;&#21464;&#21270;&#20197;&#21450;&#29615;&#22659;&#30340;&#24191;&#27867;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#25110;&#20351;&#29992;&#20854;&#20182;&#20462;&#25913;&#29616;&#26377;&#31574;&#30053;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#26174;&#31034;&#20986;&#26174;&#33879;&#25552;&#39640;&#30340;&#23398;&#20064;&#25928;&#29575;&#65292;&#34920;&#29616;&#20026;&#22823;&#22823;&#32553;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#20195;&#30721;&#21487;&#22312;https://motion-&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by humans' ability to adapt skills in the learning of new ones, this paper presents AdaptNet, an approach for modifying the latent space of existing policies to allow new behaviors to be quickly learned from like tasks in comparison to learning from scratch. Building on top of a given reinforcement learning controller, AdaptNet uses a two-tier hierarchy that augments the original state embedding to support modest changes in a behavior and further modifies the policy network layers to make more substantive changes. The technique is shown to be effective for adapting existing physics-based controllers to a wide range of new styles for locomotion, new task targets, changes in character morphology and extensive changes in environment. Furthermore, it exhibits significant increase in learning efficiency, as indicated by greatly reduced training times when compared to training from scratch or using other approaches that modify existing policies. Code is available at https://motion-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#30456;&#23545;&#21453;&#39304;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20248;&#21270;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#31639;&#27861;&#35774;&#35745;&#21644;&#20989;&#25968;&#36924;&#36817;&#12290;</title><link>http://arxiv.org/abs/2310.00212</link><description>&lt;p&gt;
&#20004;&#20004;&#37051;&#36817;&#31574;&#30053;&#20248;&#21270;: &#21033;&#29992;&#30456;&#23545;&#21453;&#39304;&#36827;&#34892;LLM&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment. (arXiv:2310.00212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00212
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#30456;&#23545;&#21453;&#39304;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20248;&#21270;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#31639;&#27861;&#35774;&#35745;&#21644;&#20989;&#25968;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#39044;&#20808;&#35757;&#32451;&#26469;&#33719;&#21462;&#24191;&#27867;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25509;&#35302;&#21040;&#20302;&#36136;&#37327;&#25968;&#25454;&#65292;LLMs&#21487;&#33021;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#19981;&#19968;&#33268;&#30340;&#26377;&#23475;&#34892;&#20026;&#12290;&#24341;&#23548;LLMs&#26397;&#30528;&#26377;&#30410;&#34892;&#20026;&#26041;&#21521;&#21457;&#23637;&#30340;&#20027;&#23548;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#20854;&#20013;Proximal Policy Optimization&#65288;PPO&#65289;&#26159;&#40664;&#35748;&#30340;RL&#20248;&#21270;&#22120;&#12290;&#23613;&#31649;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;PPO&#22312;&#20248;&#21270;&#22522;&#20110;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20027;&#35201;&#38382;&#39064;&#26159;&#65292;&#30001;&#20110;&#38656;&#35201;&#26657;&#20934;&#22870;&#21169;&#23610;&#24230;&#65292;PPO&#23545;&#20110;&#21253;&#21547;&#30456;&#21516;&#20559;&#22909;&#20449;&#24687;&#30340;&#31561;&#20215;&#22870;&#21169;&#20989;&#25968;&#19981;&#20855;&#22791;&#19981;&#21464;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#22522;&#20110;&#36712;&#36857;&#30340;&#20248;&#21270;&#30456;&#27604;&#65292;PPO&#23545;&#20110;&#22522;&#20110;&#20196;&#29260;&#30340;&#26356;&#26032;&#30340;&#38656;&#27714;&#24341;&#20837;&#20102;&#20989;&#25968;&#36924;&#36817;&#21644;&#31639;&#27861;&#35774;&#35745;&#26041;&#38754;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#30456;&#23545;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;Pairwise Proximal Policy Optimization&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) can acquire extensive world knowledge through pre-training on large corpora. However, due to exposure to low-quality data, LLMs may exhibit harmful behavior without aligning with human values. The dominant approach for steering LLMs towards beneficial behavior involves Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy Optimization (PPO) serving as the default RL optimizer. Despite its effectiveness, PPO has limitations when optimizing rewards trained from comparison-based loss. Primarily, PPO is not invariant to equivalent reward functions containing identical preference information due to the need to calibrate the reward scale. Additionally, PPO's necessity for token-wise updates introduces complexity in both function approximation and algorithm design compared to trajectory-wise optimization. This paper proposes a new framework, reinforcement learning with relative feedback, and a novel trajectory-wise policy gradient algorithm, Pair
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#21644;&#40657;&#30418;&#20248;&#21270;&#20043;&#38388;&#30340;&#20132;&#21449;&#24212;&#29992;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#23454;&#39564;&#35780;&#20272;&#20102;&#20302;&#25104;&#26412;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00077</link><description>&lt;p&gt;
&#20302;&#25104;&#26412;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#22312;BBOB&#21644;OpenAI Gym&#19978;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Low-budget Black-box Optimization Algorithms Evaluated on BBOB and OpenAI Gym. (arXiv:2310.00077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00077
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#21644;&#40657;&#30418;&#20248;&#21270;&#20043;&#38388;&#30340;&#20132;&#21449;&#24212;&#29992;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#23454;&#39564;&#35780;&#20272;&#20102;&#20302;&#25104;&#26412;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#40657;&#30418;&#20248;&#21270;&#65288;BBO&#65289;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#29305;&#21035;&#20851;&#27880;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#12290;&#22522;&#20110;BO&#30340;&#31639;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#20204;&#29992;&#20110;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#31639;&#27861;&#37197;&#32622;&#31561;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#38382;&#39064;&#32500;&#24230;&#21644;&#35780;&#20272;&#39044;&#31639;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#30340;&#25928;&#29575;&#20250;&#38477;&#20302;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#22312;&#20248;&#21270;&#31038;&#21306;&#20013;&#29420;&#31435;&#21457;&#23637;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36843;&#20999;&#38656;&#35201;&#20102;&#35299;&#26159;&#21542;&#21487;&#20197;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;BBO&#20043;&#38388;&#36827;&#34892;&#20132;&#21449;&#21463;&#31934;&#65292;&#21363;&#26426;&#22120;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#22312;BBO&#20013;&#26159;&#21542;&#21516;&#26679;&#26377;&#25928;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#27604;&#36739;&#23454;&#39564;&#36890;&#24120;&#28041;&#21450;&#30456;&#23545;&#36739;&#23567;&#30340;&#22522;&#20934;&#21644;&#23454;&#39564;&#35774;&#32622;&#20013;&#30340;&#21487;&#35265;&#38382;&#39064;&#65292;&#22914;&#22522;&#32447;&#21021;&#22987;&#21270;&#19981;&#33391;&#12289;&#36807;&#24230;&#25311;&#21512;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing ubiquity of machine learning (ML) has led it to enter various areas of computer science, including black-box optimization (BBO). Recent research is particularly concerned with Bayesian optimization (BO). BO-based algorithms are popular in the ML community, as they are used for hyperparameter optimization and more generally for algorithm configuration. However, their efficiency decreases as the dimensionality of the problem and the budget of evaluations increase. Meanwhile, derivative-free optimization methods have evolved independently in the optimization community. Therefore, we urge to understand whether cross-fertilization is possible between the two communities, ML and BBO, i.e., whether algorithms that are heavily used in ML also work well in BBO and vice versa. Comparative experiments often involve rather small benchmarks and show visible problems in the experimental setup, such as poor initialization of baselines, overfitting due to problem-specific setting of hyperp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#36776;&#29575;&#20027;&#21160;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;MRA-FNO&#65289;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#36755;&#20837;&#20989;&#25968;&#21644;&#20998;&#36776;&#29575;&#26469;&#38477;&#20302;&#25968;&#25454;&#25104;&#26412;&#24182;&#20248;&#21270;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.16971</link><description>&lt;p&gt;
&#22810;&#20998;&#36776;&#29575;&#20027;&#21160;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Multi-Resolution Active Learning of Fourier Neural Operators. (arXiv:2309.16971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16971
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#36776;&#29575;&#20027;&#21160;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;MRA-FNO&#65289;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#36755;&#20837;&#20989;&#25968;&#21644;&#20998;&#36776;&#29575;&#26469;&#38477;&#20302;&#25968;&#25454;&#25104;&#26412;&#24182;&#20248;&#21270;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#19981;&#20165;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#22312;&#35757;&#32451;&#21644;&#39044;&#27979;&#26041;&#38754;&#39640;&#25928;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#20026;FNO&#25910;&#38598;&#35757;&#32451;&#25968;&#25454;&#26159;&#19968;&#20010;&#26114;&#36149;&#30340;&#29942;&#39048;&#65292;&#22240;&#20026;&#23427;&#32463;&#24120;&#38656;&#35201;&#36827;&#34892;&#26114;&#36149;&#30340;&#29289;&#29702;&#27169;&#25311;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#20998;&#36776;&#29575;&#20027;&#21160;&#23398;&#20064;&#30340;FNO&#65288;MRA-FNO&#65289;&#65292;&#23427;&#33021;&#22815;&#21160;&#24577;&#36873;&#25321;&#36755;&#20837;&#20989;&#25968;&#21644;&#20998;&#36776;&#29575;&#65292;&#23613;&#37327;&#38477;&#20302;&#25968;&#25454;&#25104;&#26412;&#65292;&#21516;&#26102;&#20248;&#21270;&#23398;&#20064;&#25928;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27010;&#29575;&#22810;&#20998;&#36776;&#29575;FNO&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#21518;&#39564;&#25512;&#29702;&#31639;&#27861;&#12290;&#20026;&#20102;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#65292;&#25105;&#20204;&#26368;&#22823;&#21270;&#25928;&#29992;&#25104;&#26412;&#27604;&#20316;&#20026;&#33719;&#21462;&#20989;&#25968;&#65292;&#22312;&#27599;&#19968;&#27493;&#33719;&#21462;&#26032;&#30340;&#26679;&#26412;&#21644;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;&#30697;&#21305;&#37197;&#21644;&#30697;&#38453;&#34892;&#21015;&#24335;&#24341;&#29702;&#23454;&#29616;&#20102;&#21487;&#34892;&#65292;&#39640;&#25928;&#30340;&#25928;&#29992;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fourier Neural Operator (FNO) is a popular operator learning framework, which not only achieves the state-of-the-art performance in many tasks, but also is highly efficient in training and prediction. However, collecting training data for the FNO is a costly bottleneck in practice, because it often demands expensive physical simulations. To overcome this problem, we propose Multi-Resolution Active learning of FNO (MRA-FNO), which can dynamically select the input functions and resolutions to lower the data cost as much as possible while optimizing the learning efficiency. Specifically, we propose a probabilistic multi-resolution FNO and use ensemble Monte-Carlo to develop an effective posterior inference algorithm. To conduct active learning, we maximize a utility-cost ratio as the acquisition function to acquire new examples and resolutions at each step. We use moment matching and the matrix determinant lemma to enable tractable, efficient utility computation. Furthermore, we develop a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#32447;&#24615;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#12289;LightGBM&#21644;MLP&#31070;&#32463;&#32593;&#32476;&#20116;&#31181;&#27169;&#22411;&#22312;&#20304;&#27835;&#20122;&#24030;&#39044;&#27979;&#27700;&#36136;pH&#20540;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;LightGBM&#34920;&#29616;&#26368;&#22909;&#12290;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#20248;&#21183;&#26174;&#33879;&#65292;&#32780;MLP&#31070;&#32463;&#32593;&#32476;&#23545;&#29305;&#24449;&#32553;&#25918;&#25935;&#24863;&#12290;&#21516;&#26102;&#65292;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#19982;&#21407;&#30740;&#31350;&#30456;&#27604;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2309.16951</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22312;&#27700;&#36136;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Water quality prediction using machine learning and neural network approaches. (arXiv:2309.16951v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#32447;&#24615;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#12289;LightGBM&#21644;MLP&#31070;&#32463;&#32593;&#32476;&#20116;&#31181;&#27169;&#22411;&#22312;&#20304;&#27835;&#20122;&#24030;&#39044;&#27979;&#27700;&#36136;pH&#20540;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;LightGBM&#34920;&#29616;&#26368;&#22909;&#12290;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#20248;&#21183;&#26174;&#33879;&#65292;&#32780;MLP&#31070;&#32463;&#32593;&#32476;&#23545;&#29305;&#24449;&#32553;&#25918;&#25935;&#24863;&#12290;&#21516;&#26102;&#65292;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#19982;&#21407;&#30740;&#31350;&#30456;&#27604;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#36164;&#28304;&#26159;&#20154;&#31867;&#29983;&#35745;&#21644;&#32463;&#27982;&#36827;&#27493;&#30340;&#22522;&#30784;&#65292;&#19982;&#20844;&#20849;&#20581;&#24247;&#21644;&#29615;&#22659;&#31119;&#31049;&#26377;&#30528;&#20869;&#22312;&#30340;&#32852;&#31995;&#12290;&#20934;&#30830;&#39044;&#27979;&#27700;&#36136;&#26159;&#25913;&#21892;&#27700;&#36164;&#28304;&#31649;&#29702;&#21644;&#23545;&#25239;&#27745;&#26579;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#22810;&#31181;&#24615;&#33021;&#25351;&#26631;&#65292;&#35780;&#20272;&#20102;&#20116;&#31181;&#19981;&#21516;&#27169;&#22411;&#65288;&#32447;&#24615;&#22238;&#24402;&#65292;&#38543;&#26426;&#26862;&#26519;&#65292;XGBoost&#65292;LightGBM&#21644;MLP&#31070;&#32463;&#32593;&#32476;&#65289;&#22312;&#32654;&#22269;&#20304;&#27835;&#20122;&#24030;&#39044;&#27979;pH&#20540;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;LightGBM&#22312;&#25152;&#26377;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#24179;&#22343;&#31934;&#24230;&#12290;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#20984;&#26174;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;MLP&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#23545;&#29305;&#24449;&#32553;&#25918;&#20855;&#26377;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#36824;&#35814;&#32454;&#38416;&#36848;&#24182;&#20998;&#26512;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#31354;&#38388;&#32771;&#34385;&#22240;&#32032;&#26041;&#38754;&#19982;&#21407;&#30740;&#31350;&#30456;&#27604;&#25152;&#21462;&#24471;&#30340;&#20248;&#36234;&#24615;&#33021;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Water resources serve as the cornerstone of human livelihoods and economic progress, with intrinsic links to both public health and environmental well-being. The accurate prediction of water quality stands as a pivotal factor in enhancing water resource management and combating pollution. This research, employing diverse performance metrics, assesses the efficacy of five distinct models, namely, linear regression, Random Forest, XGBoost, LightGBM, and MLP neural network, in forecasting pH values within Georgia, USA. Concurrently, LightGBM attains the highest average precision among all models examined. Tree-based models underscore their supremacy in addressing regression challenges. Furthermore, the performance of MLP neural network is sensitive to feature scaling. Additionally, we expound upon and dissect the reasons behind the superior precision of the machine learning models when they are compared to the original study, which factors in time dependencies and spatial considerations. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#23545;184&#26465;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24471;&#20986;&#20102;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16742</link><description>&lt;p&gt;
&#26089;&#26399;&#26816;&#27979;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#39118;&#38505;&#30340;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Supervised Learning Models for Early Detection of Albuminuria Risk in Type-2 Diabetes Mellitus Patients. (arXiv:2309.16742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#23545;184&#26465;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24471;&#20986;&#20102;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#65292;&#23588;&#20854;&#26159;2&#22411;&#31958;&#23615;&#30149;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#20581;&#24247;&#38382;&#39064;&#12290;&#19982;&#31958;&#23615;&#30149;&#30456;&#20851;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#20854;&#24182;&#21457;&#30151;&#30340;&#21457;&#23637;&#12290;&#31958;&#23615;&#30149;&#32958;&#30149;&#26159;&#31958;&#23615;&#30149;&#30340;&#19968;&#31181;&#24930;&#24615;&#24182;&#21457;&#30151;&#65292;&#19981;&#21033;&#22320;&#24433;&#21709;&#32958;&#33039;&#65292;&#23548;&#33268;&#32958;&#33039;&#25439;&#20260;&#12290;&#35786;&#26029;&#31958;&#23615;&#30149;&#32958;&#30149;&#28041;&#21450;&#32771;&#34385;&#21508;&#31181;&#26631;&#20934;&#20043;&#19968;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;&#23615;&#28082;&#20013;&#30333;&#34507;&#30333;&#30340;&#30149;&#29702;&#23398;&#30149;&#29702;&#23398;&#25968;&#37327;&#65292;&#31216;&#20026;&#30333;&#34507;&#30333;&#23615;&#12290;&#22240;&#27492;&#65292;&#23545;&#31958;&#23615;&#30149;&#24739;&#32773;&#23615;&#28082;&#20013;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#20855;&#26377;&#21450;&#26102;&#39044;&#38450;&#25514;&#26045;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#24739;&#26377;&#30333;&#34507;&#30333;&#23615;&#30340;&#39118;&#38505;&#12290;&#25152;&#36873;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#21253;&#25324;&#26420;&#32032;&#36125;&#21494;&#26031;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#20915;&#31574;&#26641;&#65292;&#38543;&#26426;&#26862;&#26519;&#65292;AdaBoost&#65292;XGBoost&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#12290;&#25105;&#20204;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#21253;&#25324;184&#26465;&#31958;&#23615;&#30149;&#24182;&#21457;&#30151;&#39118;&#38505;&#22240;&#32032;&#30340;&#26465;&#30446;&#34987;&#29992;&#26469;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Diabetes, especially T2DM, continues to be a significant health problem. One of the major concerns associated with diabetes is the development of its complications. Diabetic nephropathy, one of the chronic complication of diabetes, adversely affects the kidneys, leading to kidney damage. Diagnosing diabetic nephropathy involves considering various criteria, one of which is the presence of a pathologically significant quantity of albumin in urine, known as albuminuria. Thus, early prediction of albuminuria in diabetic patients holds the potential for timely preventive measures. This study aimed to develop a supervised learning model to predict the risk of developing albuminuria in T2DM patients. The selected supervised learning algorithms included Na\"ive Bayes, Support Vector Machine (SVM), decision tree, random forest, AdaBoost, XGBoost, and Multi-Layer Perceptron (MLP). Our private dataset, comprising 184 entries of diabetes complications risk factors, was used to train the algorithm
&lt;/p&gt;</description></item><item><title>D$^3$Fields&#26159;&#19968;&#20010;&#21160;&#24577;&#30340;&#19977;&#32500;&#25551;&#36848;&#31526;&#22330;&#65292;&#23558;&#24213;&#23618;&#19977;&#32500;&#29615;&#22659;&#30340;&#21160;&#24577;&#29305;&#24615;&#20197;&#21450;&#35821;&#20041;&#29305;&#24449;&#21644;&#23454;&#20363;&#25513;&#27169;&#32534;&#30721;&#36215;&#26469;&#12290;&#23427;&#21487;&#20197;&#28789;&#27963;&#22320;&#20351;&#29992;&#19981;&#21516;&#32972;&#26223;&#12289;&#39118;&#26684;&#21644;&#23454;&#20363;&#30340;&#20108;&#32500;&#22270;&#20687;&#25351;&#23450;&#30446;&#26631;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#21487;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.16118</link><description>&lt;p&gt;
D$^3$Fields: &#21160;&#24577;&#19977;&#32500;&#25551;&#36848;&#31526;&#22330;&#29992;&#20110;&#38646;&#26679;&#26412;&#21487;&#27867;&#21270;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
D$^3$Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Robotic Manipulation. (arXiv:2309.16118v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16118
&lt;/p&gt;
&lt;p&gt;
D$^3$Fields&#26159;&#19968;&#20010;&#21160;&#24577;&#30340;&#19977;&#32500;&#25551;&#36848;&#31526;&#22330;&#65292;&#23558;&#24213;&#23618;&#19977;&#32500;&#29615;&#22659;&#30340;&#21160;&#24577;&#29305;&#24615;&#20197;&#21450;&#35821;&#20041;&#29305;&#24449;&#21644;&#23454;&#20363;&#25513;&#27169;&#32534;&#30721;&#36215;&#26469;&#12290;&#23427;&#21487;&#20197;&#28789;&#27963;&#22320;&#20351;&#29992;&#19981;&#21516;&#32972;&#26223;&#12289;&#39118;&#26684;&#21644;&#23454;&#20363;&#30340;&#20108;&#32500;&#22270;&#20687;&#25351;&#23450;&#30446;&#26631;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#21487;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#34920;&#31034;&#26159;&#26426;&#22120;&#20154;&#25805;&#20316;&#31995;&#32479;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#19968;&#20010;&#29702;&#24819;&#30340;&#34920;&#31034;&#24212;&#35813;&#26159;&#19977;&#32500;&#30340;&#12289;&#21160;&#24577;&#30340;&#21644;&#35821;&#20041;&#21270;&#30340;&#65292;&#20197;&#28385;&#36275;&#19981;&#21516;&#25805;&#20316;&#20219;&#21153;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#24448;&#24448;&#21516;&#26102;&#32570;&#20047;&#36825;&#19977;&#20010;&#23646;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;D$^3$Fields&#21160;&#24577;&#19977;&#32500;&#25551;&#36848;&#31526;&#22330;&#12290;&#36825;&#20123;&#22330;&#25429;&#25417;&#20102;&#24213;&#23618;&#19977;&#32500;&#29615;&#22659;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#32534;&#30721;&#20102;&#35821;&#20041;&#29305;&#24449;&#21644;&#23454;&#20363;&#25513;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#24037;&#20316;&#21306;&#22495;&#20013;&#30340;&#20219;&#24847;&#19977;&#32500;&#28857;&#25237;&#24433;&#21040;&#22810;&#35270;&#35282;&#30340;&#20108;&#32500;&#35270;&#35273;&#35266;&#23519;&#20013;&#65292;&#24182;&#25554;&#20540;&#20174;&#22522;&#26412;&#27169;&#22411;&#20013;&#24471;&#21040;&#30340;&#29305;&#24449;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#34701;&#21512;&#25551;&#36848;&#31526;&#22330;&#21487;&#20197;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#32972;&#26223;&#12289;&#39118;&#26684;&#21644;&#23454;&#20363;&#30340;&#20108;&#32500;&#22270;&#20687;&#28789;&#27963;&#22320;&#25351;&#23450;&#30446;&#26631;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#25551;&#36848;&#31526;&#22330;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#23558;&#25105;&#20204;&#30340;&#34920;&#31034;&#24212;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#22330;&#26223;&#21644;&#27169;&#25311;&#20013;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scene representation has been a crucial design choice in robotic manipulation systems. An ideal representation should be 3D, dynamic, and semantic to meet the demands of diverse manipulation tasks. However, previous works often lack all three properties simultaneously. In this work, we introduce D$^3$Fields dynamic 3D descriptor fields. These fields capture the dynamics of the underlying 3D environment and encode both semantic features and instance masks. Specifically, we project arbitrary 3D points in the workspace onto multi-view 2D visual observations and interpolate features derived from foundational models. The resulting fused descriptor fields allow for flexible goal specifications using 2D images with varied contexts, styles, and instances. To evaluate the effectiveness of these descriptor fields, we apply our representation to a wide range of robotic manipulation tasks in a zero-shot manner. Through extensive evaluation in both real-world scenarios and simulations, we demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#32852;&#21512;&#28304;-&#20449;&#36947;&#32534;&#30721;&#21644;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#22122;&#22768;&#26080;&#32447;&#20449;&#36947;&#19978;&#36827;&#34892;&#22270;&#20687;&#20256;&#36755;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#33539;&#22260;-&#38646;&#31354;&#38388;&#20998;&#35299;&#21644;&#36880;&#27493;&#20248;&#21270;&#38646;&#31354;&#38388;&#20869;&#23481;&#65292;&#23454;&#29616;&#20102;&#22312;&#22833;&#30495;&#21644;&#24863;&#30693;&#36136;&#37327;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.15889</link><description>&lt;p&gt;
&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#39640;&#24863;&#30693;&#36136;&#37327;&#30340;&#26080;&#32447;&#22270;&#20687;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
High Perceptual Quality Wireless Image Delivery with Denoising Diffusion Models. (arXiv:2309.15889v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#32852;&#21512;&#28304;-&#20449;&#36947;&#32534;&#30721;&#21644;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#22122;&#22768;&#26080;&#32447;&#20449;&#36947;&#19978;&#36827;&#34892;&#22270;&#20687;&#20256;&#36755;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#33539;&#22260;-&#38646;&#31354;&#38388;&#20998;&#35299;&#21644;&#36880;&#27493;&#20248;&#21270;&#38646;&#31354;&#38388;&#20869;&#23481;&#65292;&#23454;&#29616;&#20102;&#22312;&#22833;&#30495;&#21644;&#24863;&#30693;&#36136;&#37327;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32852;&#21512;&#28304;-&#20449;&#36947;&#32534;&#30721;&#65288;DeepJSCC&#65289;&#20197;&#21450;&#25509;&#25910;&#31471;&#30340;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#22312;&#22122;&#22768;&#26080;&#32447;&#20449;&#36947;&#19978;&#36827;&#34892;&#22270;&#20687;&#20256;&#36755;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#22312;&#23454;&#38469;&#26377;&#38480;&#22359;&#38271;&#24230;&#30340;&#24773;&#20917;&#19979;&#30340;&#24863;&#30693;&#22833;&#30495;&#26435;&#34913;&#38382;&#39064;&#65292;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20998;&#31163;&#30340;&#28304;&#32534;&#30721;&#21644;&#20449;&#36947;&#32534;&#30721;&#21487;&#33021;&#20250;&#39640;&#24230;&#19981;&#29702;&#24819;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#30446;&#26631;&#22270;&#20687;&#30340;&#33539;&#22260;-&#38646;&#31354;&#38388;&#20998;&#35299;&#30340;&#26032;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;&#32534;&#30721;&#21518;&#20256;&#36755;&#22270;&#20687;&#30340;&#33539;&#22260;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;DDPM&#36880;&#27493;&#20248;&#21270;&#20854;&#38646;&#31354;&#38388;&#20869;&#23481;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#26631;&#20934;&#30340;DeepJSCC&#21644;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#24335;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#37325;&#26500;&#22270;&#20687;&#30340;&#22833;&#30495;&#21644;&#24863;&#30693;&#36136;&#37327;&#26041;&#38754;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#20026;&#20102;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#21487;&#37325;&#29616;&#24615;&#65292;&#25105;&#20204;&#23558;&#20844;&#24320;&#20998;&#20139;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the image transmission problem over a noisy wireless channel via deep learning-based joint source-channel coding (DeepJSCC) along with a denoising diffusion probabilistic model (DDPM) at the receiver. Specifically, we are interested in the perception-distortion trade-off in the practical finite block length regime, in which separate source and channel coding can be highly suboptimal. We introduce a novel scheme that utilizes the range-null space decomposition of the target image. We transmit the range-space of the image after encoding and employ DDPM to progressively refine its null space contents. Through extensive experiments, we demonstrate significant improvements in distortion and perceptual quality of reconstructed images compared to standard DeepJSCC and the state-of-the-art generative learning-based method. We will publicly share our source code to facilitate further research and reproducibility.
&lt;/p&gt;</description></item><item><title>&#36882;&#24402;&#36229;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#26356;&#20026;&#31616;&#21333;&#20294;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.14970</link><description>&lt;p&gt;
&#36882;&#24402;&#36229;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Recurrent Hypernetworks are Surprisingly Strong in Meta-RL. (arXiv:2309.14970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14970
&lt;/p&gt;
&lt;p&gt;
&#36882;&#24402;&#36229;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#26356;&#20026;&#31616;&#21333;&#20294;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#26102;&#22240;&#26679;&#26412;&#25928;&#29575;&#20302;&#32780;&#19981;&#26131;&#37096;&#32626;&#12290;&#20803;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#23398;&#20064;&#22312;&#20803;&#35757;&#32451;&#26102;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#20998;&#24067;&#26469;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#30452;&#25509;&#35299;&#20915;&#20102;&#36825;&#20010;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#19987;&#38376;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#19982;&#19968;&#20010;&#36890;&#29992;&#30340;&#24207;&#21015;&#27169;&#22411;&#65288;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65289;&#32467;&#21512;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26159;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#24378;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35266;&#28857;&#30001;&#20110;&#26377;&#38480;&#30340;&#25903;&#25345;&#35777;&#25454;&#32780;&#24341;&#36215;&#20102;&#20105;&#35758;&#65292;&#29305;&#21035;&#26159;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#30830;&#31435;&#20102;&#23436;&#20840;&#30456;&#21453;&#30340;&#35266;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#34429;&#28982;&#25105;&#20204;&#21516;&#26679;&#21457;&#29616;&#24490;&#29615;&#32593;&#32476;&#21487;&#20197;&#36798;&#21040;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#36229;&#32593;&#32476;&#30340;&#20351;&#29992;&#23545;&#20110;&#21457;&#25381;&#24490;&#29615;&#22522;&#32447;&#30340;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#19982;&#36229;&#32593;&#32476;&#30456;&#32467;&#21512;&#26102;&#65292;&#36825;&#31181;&#36828;&#27604;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#31616;&#21333;&#30340;&#24490;&#29615;&#22522;&#20934;&#23454;&#38469;&#19978;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (RL) is notoriously impractical to deploy due to sample inefficiency. Meta-RL directly addresses this sample inefficiency by learning to perform few-shot learning when a distribution of related tasks is available for meta-training. While many specialized meta-RL methods have been proposed, recent work suggests that end-to-end learning in conjunction with an off-the-shelf sequential model, such as a recurrent network, is a surprisingly strong baseline. However, such claims have been controversial due to limited supporting evidence, particularly in the face of prior work establishing precisely the opposite. In this paper, we conduct an empirical investigation. While we likewise find that a recurrent network can achieve strong performance, we demonstrate that the use of hypernetworks is crucial to maximizing their potential. Surprisingly, when combined with hypernetworks, the recurrent baselines that are far simpler than existing specialized methods actually ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;QA-LoRA&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#37327;&#21270;&#24847;&#35782;&#20197;&#21450;&#32452;&#20869;&#36816;&#31639;&#31526;&#26469;&#23454;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20302;&#31209;&#36866;&#24212;&#12290;QA-LoRA&#33021;&#22815;&#23558;&#27169;&#22411;&#26435;&#37325;&#37327;&#21270;&#20197;&#20943;&#23569;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23558;&#27169;&#22411;&#38598;&#25104;&#20026;&#19968;&#20010;&#37327;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14717</link><description>&lt;p&gt;
QA-LoRA: &#22522;&#20110;&#37327;&#21270;&#24847;&#35782;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#20302;&#31209;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models. (arXiv:2309.14717v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;QA-LoRA&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#37327;&#21270;&#24847;&#35782;&#20197;&#21450;&#32452;&#20869;&#36816;&#31639;&#31526;&#26469;&#23454;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20302;&#31209;&#36866;&#24212;&#12290;QA-LoRA&#33021;&#22815;&#23558;&#27169;&#22411;&#26435;&#37325;&#37327;&#21270;&#20197;&#20943;&#23569;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23558;&#27169;&#22411;&#38598;&#25104;&#20026;&#19968;&#20010;&#37327;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#27785;&#37325;&#30340;&#35745;&#31639;&#36127;&#25285;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#38480;&#21046;&#20102;LLMs&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#24403;&#38656;&#35201;&#23558;&#23427;&#20204;&#37096;&#32626;&#21040;&#36793;&#32536;&#35774;&#22791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#21270;&#24847;&#35782;&#30340;&#20302;&#31209;&#36866;&#24212;&#65288;QA-LoRA&#65289;&#31639;&#27861;&#12290;&#21160;&#26426;&#22312;&#20110;&#37327;&#21270;&#21644;&#36866;&#24212;&#30340;&#33258;&#30001;&#24230;&#19981;&#24179;&#34913;&#65292;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#32452;&#20869;&#36816;&#31639;&#31526;&#65292;&#22686;&#21152;&#37327;&#21270;&#30340;&#33258;&#30001;&#24230;&#65292;&#21516;&#26102;&#20943;&#23569;&#36866;&#24212;&#30340;&#33258;&#30001;&#24230;&#12290;QA-LoRA&#21487;&#20197;&#29992;&#20960;&#34892;&#20195;&#30721;&#36731;&#26494;&#23454;&#29616;&#65292;&#24182;&#20351;&#21407;&#22987;&#30340;LoRA&#20855;&#22791;&#20102;&#20004;&#20010;&#33021;&#21147;&#65306;&#65288;i&#65289;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;LLM&#30340;&#26435;&#37325;&#34987;&#37327;&#21270;&#65288;&#20363;&#22914;&#36716;&#25442;&#20026;INT4&#65289;&#65292;&#20197;&#20943;&#23569;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#20351;&#29992;&#65307;&#65288;ii&#65289;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;LLM&#21644;&#36741;&#21161;&#26435;&#37325;&#33258;&#28982;&#22320;&#38598;&#25104;&#21040;&#19968;&#20010;&#37327;&#21270;&#27169;&#22411;&#20013;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23558;QA-LoRA&#24212;&#29992;&#21040;LLaMA&#21644;LLaMA2&#27169;&#22411;&#23478;&#26063;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently years have witnessed a rapid development of large language models (LLMs). Despite the strong ability in many language-understanding tasks, the heavy computational burden largely restricts the application of LLMs especially when one needs to deploy them onto edge devices. In this paper, we propose a quantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies in the imbalanced degrees of freedom of quantization and adaptation, and the solution is to use group-wise operators which increase the degree of freedom of quantization meanwhile decreasing that of adaptation. QA-LoRA is easily implemented with a few lines of code, and it equips the original LoRA with two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized (e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model famil
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Stackelberg&#39550;&#39542;&#21592;&#27169;&#22411;&#30340;&#25345;&#32493;&#25919;&#31574;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38381;&#29615;&#33258;&#21160;&#39550;&#39542;&#20013;&#24341;&#20837;&#32972;&#26223;&#36710;&#36742;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20043;&#38388;&#30340;&#21338;&#24328;&#24335;&#20132;&#20114;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#38271;&#23614;&#20998;&#24067;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.14235</link><description>&lt;p&gt;
Stackelberg&#39550;&#39542;&#21592;&#27169;&#22411;&#29992;&#20110;&#22522;&#20110;&#22330;&#26223;&#30340;&#38381;&#29615;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#25345;&#32493;&#25919;&#31574;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Stackelberg Driver Model for Continual Policy Improvement in Scenario-Based Closed-Loop Autonomous Driving. (arXiv:2309.14235v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14235
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Stackelberg&#39550;&#39542;&#21592;&#27169;&#22411;&#30340;&#25345;&#32493;&#25919;&#31574;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38381;&#29615;&#33258;&#21160;&#39550;&#39542;&#20013;&#24341;&#20837;&#32972;&#26223;&#36710;&#36742;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20043;&#38388;&#30340;&#21338;&#24328;&#24335;&#20132;&#20114;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#38271;&#23614;&#20998;&#24067;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#22256;&#38590;&#65292;&#22240;&#20026;&#38271;&#23614;&#20998;&#24067;&#30340;&#39550;&#39542;&#22330;&#26223;&#20013;&#23384;&#22312;&#32597;&#35265;&#20294;&#20851;&#38190;&#30340;&#36793;&#38469;&#24773;&#20917;&#65292;&#36825;&#20250;&#23545;&#23427;&#20204;&#30340;&#25972;&#20307;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#23545;&#25239;&#24615;&#29983;&#25104;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#19968;&#31867;&#26377;&#25928;&#30340;&#36884;&#24452;&#65292;&#29992;&#20110;&#21512;&#25104;AV&#27979;&#35797;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29983;&#25104;&#30340;&#22330;&#26223;&#36890;&#24120;&#34987;&#29992;&#20110;AV&#35757;&#32451;&#30340;&#26426;&#20250;&#26377;&#38480;&#65292;&#36896;&#25104;&#20102;&#25345;&#32493;AV&#25919;&#31574;&#25913;&#36827;&#30340;&#28508;&#21147;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#65292;&#21516;&#26102;&#20063;&#32570;&#20047;&#38381;&#29615;&#35774;&#35745;&#26469;&#23454;&#29616;&#36825;&#19968;&#25913;&#36827;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;Stackelberg&#39550;&#39542;&#21592;&#27169;&#22411;&#65288;SDM&#65289;&#36827;&#34892;&#35843;&#25972;&#65292;&#20197;&#20934;&#30830;&#25551;&#36848;&#36710;&#36742;&#20132;&#20114;&#21160;&#21147;&#23398;&#30340;&#23618;&#27425;&#24615;&#36136;&#65292;&#36890;&#36807;&#23558;&#32972;&#26223;&#36710;&#36742;&#65288;BVs&#65289;&#21644;AV&#22312;&#19968;&#31181;&#39034;&#24207;&#21338;&#24328;&#24335;&#30340;&#20132;&#20114;&#33539; Paradigm&#20869;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;&#36890;&#36807;AV&#20805;&#24403;&#39046;&#23548;&#32773;&#65292;BVs&#20316;&#20026;&#36861;&#38543;&#32773;&#65292;&#36825;&#31181;&#39046;&#23548;&#32773;-&#36861;&#38543;&#32773;&#27169;&#22411;&#30830;&#20445;&#20102;AV&#22987;&#32456;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment of autonomous vehicles (AVs) has faced hurdles due to the dominance of rare but critical corner cases within the long-tail distribution of driving scenarios, which negatively affects their overall performance. To address this challenge, adversarial generation methods have emerged as a class of efficient approaches to synthesize safety-critical scenarios for AV testing. However, these generated scenarios are often underutilized for AV training, resulting in the potential for continual AV policy improvement remaining untapped, along with a deficiency in the closed-loop design needed to achieve it. Therefore, we tailor the Stackelberg Driver Model (SDM) to accurately characterize the hierarchical nature of vehicle interaction dynamics, facilitating iterative improvement by engaging background vehicles (BVs) and AV in a sequential game-like interaction paradigm. With AV acting as the leader and BVs as followers, this leader-follower modeling ensures that AV would consistentl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;DFRD&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#21033;&#29992;&#26381;&#21153;&#22120;&#19978;&#30340;&#26465;&#20214;&#29983;&#25104;&#22120;&#36817;&#20284;&#23616;&#37096;&#27169;&#22411;&#30340;&#35757;&#32451;&#31354;&#38388;&#65292;&#24182;&#37319;&#29992;&#21160;&#24577;&#21152;&#26435;&#21644;&#26631;&#31614;&#25277;&#26679;&#26469;&#20934;&#30830;&#25552;&#21462;&#30693;&#35782;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13546</link><description>&lt;p&gt;
DFRD: &#26080;&#25968;&#25454;&#40065;&#26834;&#24615;&#33976;&#39311;&#25216;&#26415;&#29992;&#20110;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DFRD: Data-Free Robustness Distillation for Heterogeneous Federated Learning. (arXiv:2309.13546v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13546
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;DFRD&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#21033;&#29992;&#26381;&#21153;&#22120;&#19978;&#30340;&#26465;&#20214;&#29983;&#25104;&#22120;&#36817;&#20284;&#23616;&#37096;&#27169;&#22411;&#30340;&#35757;&#32451;&#31354;&#38388;&#65292;&#24182;&#37319;&#29992;&#21160;&#24577;&#21152;&#26435;&#21644;&#26631;&#31614;&#25277;&#26679;&#26469;&#20934;&#30830;&#25552;&#21462;&#30693;&#35782;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#38544;&#31169;&#21463;&#38480;&#30340;&#20998;&#25955;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23458;&#25143;&#31471;&#21487;&#20197;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#32780;&#19981;&#27844;&#38706;&#31169;&#23494;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#25454;&#24322;&#26500;&#21644;&#27169;&#22411;&#24322;&#26500;&#30340;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#23398;&#20064;&#19968;&#20010;&#40065;&#26834;&#30340;&#20840;&#23616;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;DFRD&#65289;&#12290;DFRD&#22312;&#26381;&#21153;&#22120;&#19978;&#37197;&#22791;&#19968;&#20010;&#26465;&#20214;&#29983;&#25104;&#22120;&#65292;&#20197;&#36817;&#20284;&#23458;&#25143;&#31471;&#19978;&#20256;&#30340;&#23616;&#37096;&#27169;&#22411;&#30340;&#35757;&#32451;&#31354;&#38388;&#65292;&#24182;&#22312;&#29983;&#25104;&#22120;&#30340;&#20998;&#24067;&#36716;&#31227;&#24341;&#36215;&#30340;&#27807;&#36890;&#36718;&#27425;&#20013;&#20445;&#25345;&#26381;&#21153;&#22120;&#19978;&#30340;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#21103;&#26412;&#65292;&#20197;&#20811;&#26381;&#20840;&#23616;&#27169;&#22411;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21160;&#24577;&#21152;&#26435;&#21644;&#26631;&#31614;&#25277;&#26679;&#65292;&#20197;&#20934;&#30830;&#22320;&#25552;&#21462;&#23616;&#37096;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#26368;&#21518;&#65292;&#22312;&#19981;&#21516;&#30340;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a privacy-constrained decentralized machine learning paradigm in which clients enable collaborative training without compromising private data. However, how to learn a robust global model in the data-heterogeneous and model-heterogeneous FL scenarios is challenging. To address it, we resort to data-free knowledge distillation to propose a new FL method (namely DFRD). DFRD equips a conditional generator on the server to approximate the training space of the local models uploaded by clients, and systematically investigates its training in terms of fidelity, transferability} and diversity. To overcome the catastrophic forgetting of the global model caused by the distribution shifts of the generator across communication rounds, we maintain an exponential moving average copy of the generator on the server. Additionally, we propose dynamic weighting and label sampling to accurately extract knowledge from local models. Finally, our extensive experiments on various i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20122;&#39532;&#36874;&#26426;&#22120;&#20154;&#20844;&#21496;&#30340;Robot Induction&#65288;Robin&#65289;&#33328;&#38431;&#20013;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#32437;&#65292;&#36890;&#36807;&#20351;&#29992;&#25315;&#36873;&#25104;&#21151;&#39044;&#27979;&#22120;&#20197;&#21450;&#35757;&#32451;&#30340;&#25315;&#36873;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#65292;&#22312;&#30495;&#23454;&#29983;&#20135;&#31995;&#32479;&#20013;&#36827;&#34892;&#33258;&#21160;&#21270;&#20179;&#20648;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.13224</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#32437;&#30340;&#25315;&#36873;&#35745;&#21010;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Pick Planning Strategies for Large-Scale Package Manipulation. (arXiv:2309.13224v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20122;&#39532;&#36874;&#26426;&#22120;&#20154;&#20844;&#21496;&#30340;Robot Induction&#65288;Robin&#65289;&#33328;&#38431;&#20013;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#32437;&#65292;&#36890;&#36807;&#20351;&#29992;&#25315;&#36873;&#25104;&#21151;&#39044;&#27979;&#22120;&#20197;&#21450;&#35757;&#32451;&#30340;&#25315;&#36873;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#65292;&#22312;&#30495;&#23454;&#29983;&#20135;&#31995;&#32479;&#20013;&#36827;&#34892;&#33258;&#21160;&#21270;&#20179;&#20648;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#20179;&#20648;&#25805;&#20316;&#21487;&#20197;&#38477;&#20302;&#29289;&#27969;&#25104;&#26412;&#65292;&#26368;&#32456;&#38477;&#20302;&#28040;&#36153;&#32773;&#30340;&#20215;&#26684;&#65292;&#21152;&#24555;&#20132;&#36135;&#36895;&#24230;&#65292;&#24182;&#22686;&#24378;&#23545;&#24066;&#22330;&#27874;&#21160;&#30340;&#36866;&#24212;&#21147;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20122;&#39532;&#36874;&#26426;&#22120;&#20154;&#20844;&#21496;&#30340;&#26426;&#22120;&#20154;&#24341;&#23548;&#65288;Robin&#65289;&#33328;&#38431;&#20013;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#32437;&#65292;&#29992;&#20110;&#27599;&#22825;&#25315;&#36873;&#21644;&#21333;&#29420;&#22788;&#29702;600&#19975;&#20010;&#21253;&#35065;&#65292;&#24182;&#19988;&#30446;&#21069;&#24050;&#32463;&#22788;&#29702;&#20102;20&#20159;&#20010;&#21253;&#35065;&#12290;&#23427;&#25551;&#36848;&#20102;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#24320;&#21457;&#30340;&#21508;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#21450;&#20854;&#21518;&#32487;&#26041;&#27861;&#65292;&#21518;&#32487;&#26041;&#27861;&#21033;&#29992;&#20102;&#22312;&#30495;&#23454;&#29983;&#20135;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#25315;&#36873;&#25104;&#21151;&#39044;&#27979;&#22120;&#12290;&#25454;&#20316;&#32773;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#22312;&#30495;&#23454;&#29983;&#20135;&#31995;&#32479;&#20013;&#39318;&#27425;&#22823;&#35268;&#27169;&#37096;&#32626;&#23398;&#20064;&#30340;&#25315;&#36873;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automating warehouse operations can reduce logistics overhead costs, ultimately driving down the final price for consumers, increasing the speed of delivery, and enhancing the resiliency to market fluctuations.  This extended abstract showcases a large-scale package manipulation from unstructured piles in Amazon Robotics' Robot Induction (Robin) fleet, which is used for picking and singulating up to 6 million packages per day and so far has manipulated over 2 billion packages. It describes the various heuristic methods developed over time and their successor, which utilizes a pick success predictor trained on real production data.  To the best of the authors' knowledge, this work is the first large-scale deployment of learned pick quality estimation methods in a real production system.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#22810;&#31181;&#31639;&#27861;&#65292;&#21457;&#29616;&#24046;&#20998;&#36827;&#21270;&#65288;DE&#65289;&#31639;&#27861;&#22312;&#20248;&#21270;&#24211;&#23384;&#31649;&#29702;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#26377;&#25928;&#38477;&#20302;&#24211;&#23384;&#25104;&#26412;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#19981;&#30830;&#23450;&#38656;&#27714;&#27169;&#24335;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2309.13095</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20010;&#29420;&#31435;&#30340;&#24046;&#20998;&#36827;&#21270;&#20248;&#21270;&#26041;&#27861;&#35299;&#20915;&#24211;&#23384;&#31649;&#29702;&#20013;&#30340;&#38656;&#27714;&#19981;&#30830;&#23450;&#24615;&#21644;&#21464;&#24322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multiple Independent DE Optimizations to Tackle Uncertainty and Variability in Demand in Inventory Management. (arXiv:2309.13095v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#22810;&#31181;&#31639;&#27861;&#65292;&#21457;&#29616;&#24046;&#20998;&#36827;&#21270;&#65288;DE&#65289;&#31639;&#27861;&#22312;&#20248;&#21270;&#24211;&#23384;&#31649;&#29702;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#26377;&#25928;&#38477;&#20302;&#24211;&#23384;&#25104;&#26412;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#19981;&#30830;&#23450;&#38656;&#27714;&#27169;&#24335;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#23450;&#20803;&#21551;&#21457;&#24335;&#24046;&#20998;&#36827;&#21270;&#20248;&#21270;&#31574;&#30053;&#22312;&#38543;&#26426;&#38656;&#27714;&#24773;&#20917;&#19979;&#23545;&#24211;&#23384;&#31649;&#29702;&#30340;&#26377;&#25928;&#24615;&#65292;&#26412;&#23454;&#35777;&#30740;&#31350;&#36827;&#34892;&#20102;&#28145;&#20837;&#35843;&#26597;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#19981;&#30830;&#23450;&#38656;&#27714;&#27169;&#24335;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#20943;&#23569;&#24211;&#23384;&#25104;&#26412;&#31574;&#30053;&#12290;&#24211;&#23384;&#25104;&#26412;&#26159;&#25351;&#20225;&#19994;&#25345;&#26377;&#21644;&#31649;&#29702;&#24211;&#23384;&#25152;&#20135;&#29983;&#30340;&#36153;&#29992;&#12290;&#35813;&#26041;&#27861;&#23558;&#36830;&#32493;&#23457;&#26597;&#24211;&#23384;&#31649;&#29702;&#25919;&#31574;&#19982;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#65288;MCS&#65289;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#25214;&#21040;&#26368;&#20248;&#35299;&#65292;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#22810;&#20010;&#31639;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24046;&#20998;&#36827;&#21270;&#65288;DE&#65289;&#31639;&#27861;&#22312;&#20248;&#21270;&#24211;&#23384;&#31649;&#29702;&#26041;&#38754;&#20248;&#20110;&#20854;&#31454;&#20105;&#23545;&#25163;&#12290;&#20026;&#20102;&#35843;&#25972;&#21442;&#25968;&#65292;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#25289;&#19969;&#36229;&#31435;&#26041;&#37319;&#26679;&#65288;LHS&#65289;&#32479;&#35745;&#26041;&#27861;&#12290;&#20026;&#20102;&#30830;&#23450;&#26368;&#32456;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22810;&#20010;&#29420;&#31435;&#20248;&#21270;&#32467;&#26524;&#32508;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
To determine the effectiveness of metaheuristic Differential Evolution optimization strategy for inventory management (IM) in the context of stochastic demand, this empirical study undertakes a thorough investigation. The primary objective is to discern the most effective strategy for minimizing inventory costs within the context of uncertain demand patterns. Inventory costs refer to the expenses associated with holding and managing inventory within a business. The approach combines a continuous review of IM policies with a Monte Carlo Simulation (MCS). To find the optimal solution, the study focuses on meta-heuristic approaches and compares multiple algorithms. The outcomes reveal that the Differential Evolution (DE) algorithm outperforms its counterparts in optimizing IM. To fine-tune the parameters, the study employs the Latin Hypercube Sampling (LHS) statistical method. To determine the final solution, a method is employed in this study which combines the outcomes of multiple indep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#26410;&#35780;&#20272;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#25552;&#39640;&#20195;&#29702;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#65288;SAEAs&#65289;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#20195;&#29702;&#27169;&#22411;&#35782;&#21035;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#30452;&#25509;&#29983;&#25104;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.11994</link><description>&lt;p&gt;
&#29992;&#26410;&#35780;&#20272;&#30340;&#35299;&#20915;&#26041;&#26696;&#22686;&#24378;SAEAs&#65306;&#26114;&#36149;&#20248;&#21270;&#38382;&#39064;&#30340;&#20851;&#31995;&#27169;&#22411;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing SAEAs with Unevaluated Solutions: A Case Study of Relation Model for Expensive Optimization. (arXiv:2309.11994v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#26410;&#35780;&#20272;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#25552;&#39640;&#20195;&#29702;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#65288;SAEAs&#65289;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#20195;&#29702;&#27169;&#22411;&#35782;&#21035;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#30452;&#25509;&#29983;&#25104;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20195;&#29702;&#30340;&#36827;&#21270;&#31639;&#27861;&#65288;SAEAs&#65289;&#22312;&#35299;&#20915;&#26114;&#36149;&#20248;&#21270;&#38382;&#39064;&#65288;EOPs&#65289;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36890;&#36807;&#24320;&#21457;&#39640;&#25928;&#30340;&#27169;&#22411;&#36741;&#21161;&#36873;&#25321;&#26041;&#27861;&#65292;&#20154;&#20204;&#24050;&#32463;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#26469;&#25552;&#39640;SAEAs&#30340;&#25928;&#33021;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#36873;&#25321;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#22312;SAEAs&#30340;&#27599;&#19968;&#20195;&#20013;&#20165;&#35780;&#20272;&#26377;&#38480;&#25968;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#22522;&#26412;&#33539;&#24335;&#20943;&#23569;&#20102;&#30456;&#37051;&#31181;&#32676;&#30340;&#26041;&#24046;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#21518;&#20195;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#12290;&#36825;&#26159;&#19968;&#20010;&#32463;&#24120;&#36935;&#21040;&#30340;&#38382;&#39064;&#65292;&#20294;&#36824;&#27809;&#26377;&#24471;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#26410;&#35780;&#20272;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#25552;&#39640;SAEAs&#30340;&#25928;&#29575;&#12290;&#20195;&#29702;&#27169;&#22411;&#34987;&#29992;&#26469;&#35782;&#21035;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#65292;&#30452;&#25509;&#29983;&#25104;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#35780;&#20272;&#12290;&#20026;&#20102;&#30830;&#20445;&#21487;&#38752;&#30340;&#36873;&#25321;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#23450;&#21046;&#30340;&#20851;&#31995;&#27169;&#22411;&#65292;&#29992;&#20110;&#36873;&#25321;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#21644;&#26410;&#35780;&#20272;&#31181;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surrogate-assisted evolutionary algorithms (SAEAs) hold significant importance in resolving expensive optimization problems~(EOPs). Extensive efforts have been devoted to improving the efficacy of SAEAs through the development of proficient model-assisted selection methods. However, generating high-quality solutions is a prerequisite for selection. The fundamental paradigm of evaluating a limited number of solutions in each generation within SAEAs reduces the variance of adjacent populations, thus impacting the quality of offspring solutions. This is a frequently encountered issue, yet it has not gained widespread attention. This paper presents a framework using unevaluated solutions to enhance the efficiency of SAEAs. The surrogate model is employed to identify high-quality solutions for direct generation of new solutions without evaluation. To ensure dependable selection, we have introduced two tailored relation models for the selection of the optimal solution and the unevaluated pop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#19979;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20219;&#21153;&#22270;&#31163;&#36733;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#24448;&#24448;&#26080;&#27861;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#65292;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#19979;&#38477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20219;&#21153;&#22270;&#35843;&#24230;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#20197;&#36866;&#24212;&#35745;&#31639;&#33021;&#21147;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.10569</link><description>&lt;p&gt;
&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20219;&#21153;&#22270;&#31163;&#36733;
&lt;/p&gt;
&lt;p&gt;
Task Graph offloading via Deep Reinforcement Learning in Mobile Edge Computing. (arXiv:2309.10569v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#19979;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20219;&#21153;&#22270;&#31163;&#36733;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#24448;&#24448;&#26080;&#27861;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#65292;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#19979;&#38477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20219;&#21153;&#22270;&#35843;&#24230;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#20197;&#36866;&#24212;&#35745;&#31639;&#33021;&#21147;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#20854;&#20013;&#21253;&#21547;&#30340;&#20381;&#36182;&#20219;&#21153;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#36890;&#24120;&#20855;&#26377;&#20302;&#24310;&#36831;&#35201;&#27714;&#65292;&#20174;&#32780;&#23548;&#33268;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#38656;&#27714;&#24613;&#21095;&#22686;&#21152;&#12290;&#38543;&#30528;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#30340;&#20986;&#29616;&#65292;&#23558;&#24212;&#29992;&#31243;&#24207;&#20219;&#21153;&#21368;&#36733;&#21040;&#37096;&#32626;&#22312;&#31227;&#21160;&#32593;&#32476;&#36793;&#32536;&#30340;&#23567;&#22411;&#35774;&#22791;&#19978;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#29992;&#25143;&#20307;&#39564;&#25104;&#20026;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;MEC&#29615;&#22659;&#26159;&#21160;&#24577;&#30340;&#65292;&#22823;&#22810;&#25968;&#20381;&#36182;&#19987;&#23478;&#30693;&#35782;&#25110;&#20934;&#30830;&#30340;&#20998;&#26512;&#27169;&#22411;&#30340;&#29616;&#26377;&#24037;&#20316;&#22312;&#20219;&#21153;&#22270;&#31163;&#36733;&#26041;&#38754;&#26080;&#27861;&#23436;&#20840;&#36866;&#24212;&#36825;&#31181;&#29615;&#22659;&#21464;&#21270;&#65292;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#38477;&#20302;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;MEC&#20013;&#30340;&#20219;&#21153;&#22270;&#31163;&#36733;&#65292;&#32771;&#34385;&#21040;&#36793;&#32536;&#35745;&#31639;&#35774;&#22791;&#30340;&#35745;&#31639;&#33021;&#21147;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#20026;&#20102;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#65292;&#25105;&#20204;&#23558;&#35745;&#31639;&#31163;&#36733;&#30340;&#20219;&#21153;&#22270;&#35843;&#24230;&#24314;&#27169;&#20026;&#19968;&#20010;Markov&#20915;&#31574;&#36807;&#31243;&#65288;Markov Decision Process&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various mobile applications that comprise dependent tasks are gaining widespread popularity and are increasingly complex. These applications often have low-latency requirements, resulting in a significant surge in demand for computing resources. With the emergence of mobile edge computing (MEC), it becomes the most significant issue to offload the application tasks onto small-scale devices deployed at the edge of the mobile network for obtaining a high-quality user experience. However, since the environment of MEC is dynamic, most existing works focusing on task graph offloading, which rely heavily on expert knowledge or accurate analytical models, fail to fully adapt to such environmental changes, resulting in the reduction of user experience. This paper investigates the task graph offloading in MEC, considering the time-varying computation capabilities of edge computing devices. To adapt to environmental changes, we model the task graph scheduling for computation offloading as a Mark
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;Attention Loss Adjusted Prioritized Experience Replay (ALAP)&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25913;&#36827;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#21452;&#37319;&#26679;&#26426;&#21046;&#65292;&#35843;&#33410;&#37325;&#35201;&#24615;&#37319;&#26679;&#26435;&#37325;&#65292;&#28040;&#38500;&#20102;&#20808;&#36827;&#30340;&#32463;&#39564;&#22238;&#25918;&#31639;&#27861;&#20013;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;&#22312;OPENAI gym&#29615;&#22659;&#20013;&#30340;&#27979;&#35797;&#21644;&#23545;&#27604;&#30740;&#31350;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#30340;&#20248;&#21183;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.06684</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;Attention Loss Adjusted Prioritized Experience Replay&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Attention Loss Adjusted Prioritized Experience Replay. (arXiv:2309.06684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;Attention Loss Adjusted Prioritized Experience Replay (ALAP)&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25913;&#36827;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#21452;&#37319;&#26679;&#26426;&#21046;&#65292;&#35843;&#33410;&#37325;&#35201;&#24615;&#37319;&#26679;&#26435;&#37325;&#65292;&#28040;&#38500;&#20102;&#20808;&#36827;&#30340;&#32463;&#39564;&#22238;&#25918;&#31639;&#27861;&#20013;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;&#22312;OPENAI gym&#29615;&#22659;&#20013;&#30340;&#27979;&#35797;&#21644;&#23545;&#27604;&#30740;&#31350;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#30340;&#20248;&#21183;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#32463;&#39564;&#22238;&#25918;&#31639;&#27861;(Prioritized Experience Replay, PER)&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#26356;&#22810;&#30693;&#35782;&#37327;&#30340;&#32463;&#39564;&#26679;&#26412;&#26469;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;PER&#20013;&#20351;&#29992;&#30340;&#38750;&#22343;&#21248;&#37319;&#26679;&#19981;&#21487;&#36991;&#20813;&#22320;&#20351;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#24102;&#26469;Q&#20540;&#20989;&#25968;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Attention Loss Adjusted Prioritized (ALAP) Experience Replay&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#25913;&#36827;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#21452;&#37319;&#26679;&#26426;&#21046;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#36866;&#24212;&#33021;&#22815;&#35843;&#33410;&#37325;&#35201;&#24615;&#37319;&#26679;&#26435;&#37325;&#30340;&#36229;&#21442;&#25968;&#65292;&#20174;&#32780;&#28040;&#38500;&#22240;PER&#24341;&#36215;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;&#20026;&#20102;&#39564;&#35777;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#36890;&#29992;&#24615;&#65292;&#25105;&#20204;&#22312;OPENAI gym&#29615;&#22659;&#20013;&#23545;&#22522;&#20110;&#20540;&#20989;&#25968;&#12289;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#21644;&#22810;&#20027;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#36827;&#34892;&#20102;&#23545;&#27604;&#30740;&#31350;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26694;&#26550;&#30340;&#20248;&#21183;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prioritized Experience Replay (PER) is a technical means of deep reinforcement learning by selecting experience samples with more knowledge quantity to improve the training rate of neural network. However, the non-uniform sampling used in PER inevitably shifts the state-action space distribution and brings the estimation error of Q-value function. In this paper, an Attention Loss Adjusted Prioritized (ALAP) Experience Replay algorithm is proposed, which integrates the improved Self-Attention network with Double-Sampling mechanism to fit the hyperparameter that can regulate the importance sampling weights to eliminate the estimation error caused by PER. In order to verify the effectiveness and generality of the algorithm, the ALAP is tested with value-function based, policy-gradient based and multi-agent reinforcement learning algorithms in OPENAI gym, and comparison studies verify the advantage and efficiency of the proposed training framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20004;&#31181;&#26426;&#21046;&#65292;&#28789;&#27963;&#30340;&#25513;&#27169;&#21644;&#29420;&#29305;&#30340;&#32593;&#32476;&#21098;&#26525;&#65292;&#20351;&#24471;&#19968;&#20010;&#21069;&#39304;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#30697;&#38453;&#21521;&#37327;&#20056;&#27861;&#65292;&#24182;&#19988;&#22312;&#22270;&#24418;&#27169;&#22411;&#20013;&#21487;&#20197;&#29992;&#26469;&#27979;&#35797;&#20381;&#36182;&#20851;&#31995;&#25110;&#20132;&#20114;&#39034;&#24207;&#12290;</title><link>http://arxiv.org/abs/2309.06382</link><description>&lt;p&gt;
&#38598;&#25104;&#25513;&#27169;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Ensemble Mask Networks. (arXiv:2309.06382v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20004;&#31181;&#26426;&#21046;&#65292;&#28789;&#27963;&#30340;&#25513;&#27169;&#21644;&#29420;&#29305;&#30340;&#32593;&#32476;&#21098;&#26525;&#65292;&#20351;&#24471;&#19968;&#20010;&#21069;&#39304;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#30697;&#38453;&#21521;&#37327;&#20056;&#27861;&#65292;&#24182;&#19988;&#22312;&#22270;&#24418;&#27169;&#22411;&#20013;&#21487;&#20197;&#29992;&#26469;&#27979;&#35797;&#20381;&#36182;&#20851;&#31995;&#25110;&#20132;&#20114;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;$\mathbb{R}^n\rightarrow \mathbb{R}^n$&#30340;&#21069;&#39304;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#30697;&#38453;&#21521;&#37327;&#20056;&#27861;&#21527;&#65311;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20004;&#31181;&#26426;&#21046;&#65306;&#28789;&#27963;&#30340;&#25513;&#27169;&#29992;&#20110;&#25509;&#25910;&#30697;&#38453;&#36755;&#20837;&#65292;&#20197;&#21450;&#19968;&#31181;&#29420;&#29305;&#30340;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#20197;&#23562;&#37325;&#25513;&#27169;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;&#32593;&#32476;&#21487;&#20197;&#36817;&#20284;&#22266;&#23450;&#25805;&#20316;&#65292;&#22914;&#30697;&#38453;&#21521;&#37327;&#20056;&#27861;$\phi(A,x) \rightarrow Ax$&#65292;&#36825;&#28608;&#21457;&#20102;&#24341;&#20837;&#30340;&#26426;&#21046;&#22312;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#20013;&#27979;&#35797;&#20381;&#36182;&#20851;&#31995;&#25110;&#20132;&#20114;&#39034;&#24207;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can an $\mathbb{R}^n\rightarrow \mathbb{R}^n$ feedforward network learn matrix-vector multiplication? This study introduces two mechanisms - flexible masking to take matrix inputs, and a unique network pruning to respect the mask's dependency structure. Networks can approximate fixed operations such as matrix-vector multiplication $\phi(A,x) \rightarrow Ax$, motivating the mechanisms introduced with applications towards litmus-testing dependencies or interaction order in graph-based models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22270;&#35770;&#31639;&#27861;&#22312;&#22320;&#29702;&#31354;&#38388;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#23427;&#20204;&#22312;&#32593;&#32476;&#20998;&#26512;&#12289;&#31354;&#38388;&#36830;&#25509;&#24615;&#12289;&#22320;&#29702;&#20449;&#24687;&#31995;&#32479;&#20197;&#21450;&#21508;&#31181;&#31354;&#38388;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21015;&#20030;&#20102;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#23454;&#26045;&#30340;&#24191;&#27867;&#30740;&#31350;&#12289;&#21019;&#26032;&#25216;&#26415;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03249</link><description>&lt;p&gt;
&#22270;&#35770;&#22312;&#39640;&#32423;&#22320;&#29702;&#31354;&#38388;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Graph Theory Applications in Advanced Geospatial Research. (arXiv:2309.03249v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22270;&#35770;&#31639;&#27861;&#22312;&#22320;&#29702;&#31354;&#38388;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#23427;&#20204;&#22312;&#32593;&#32476;&#20998;&#26512;&#12289;&#31354;&#38388;&#36830;&#25509;&#24615;&#12289;&#22320;&#29702;&#20449;&#24687;&#31995;&#32479;&#20197;&#21450;&#21508;&#31181;&#31354;&#38388;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21015;&#20030;&#20102;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#23454;&#26045;&#30340;&#24191;&#27867;&#30740;&#31350;&#12289;&#21019;&#26032;&#25216;&#26415;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29702;&#31354;&#38388;&#31185;&#23398;&#28085;&#30422;&#20102;&#20174;&#29615;&#22659;&#30417;&#27979;&#12289;&#20132;&#36890;&#21040;&#22522;&#30784;&#35774;&#26045;&#35268;&#21010;&#12289;&#20197;&#21450;&#22522;&#20110;&#20301;&#32622;&#30340;&#20998;&#26512;&#21644;&#26381;&#21153;&#31561;&#24191;&#27867;&#24212;&#29992;&#12290;&#25968;&#23398;&#20013;&#30340;&#22270;&#35770;&#31639;&#27861;&#30001;&#20110;&#20854;&#39640;&#25928;&#22320;&#24314;&#27169;&#21644;&#20998;&#26512;&#31354;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#24050;&#32463;&#25104;&#20026;&#19981;&#21487;&#25110;&#32570;&#30340;&#24037;&#20855;&#12290;&#26412;&#25216;&#26415;&#25253;&#21578;&#25506;&#35752;&#20102;&#22270;&#35770;&#31639;&#27861;&#22312;&#22320;&#29702;&#31354;&#38388;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#23427;&#20204;&#22312;&#32593;&#32476;&#20998;&#26512;&#12289;&#31354;&#38388;&#36830;&#25509;&#24615;&#12289;&#22320;&#29702;&#20449;&#24687;&#31995;&#32479;&#20197;&#21450;&#21508;&#31181;&#31354;&#38388;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#20316;&#29992;&#12290;&#23427;&#25552;&#20379;&#20102;&#23545;&#22270;&#35770;&#30340;&#20851;&#38190;&#27010;&#24565;&#21644;&#31639;&#27861;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#20197;&#21161;&#20110;&#24314;&#27169;&#36807;&#31243;&#12290;&#35813;&#25253;&#21578;&#28145;&#20837;&#20998;&#26512;&#20102;&#22270;&#35770;&#22312;&#24212;&#23545;&#29616;&#23454;&#19990;&#30028;&#22320;&#29702;&#31354;&#38388;&#25361;&#25112;&#21644;&#26426;&#36935;&#20013;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;&#23427;&#36824;&#21015;&#20030;&#20102;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#23454;&#26045;&#30340;&#24191;&#27867;&#30740;&#31350;&#12289;&#21019;&#26032;&#25216;&#26415;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geospatial sciences include a wide range of applications, from environmental monitoring transportation to infrastructure planning, as well as location-based analysis and services. Graph theory algorithms in mathematics have emerged as indispensable tools in these domains due to their capability to model and analyse spatial relationships efficiently. This technical report explores the applications of graph theory algorithms in geospatial sciences, highlighting their role in network analysis, spatial connectivity, geographic information systems, and various other spatial problem-solving scenarios. It provides a comprehensive idea about the key concepts and algorithms of graph theory that assist the modelling processes. The report provides insights into the practical significance of graph theory in addressing real-world geospatial challenges and opportunities. It lists the extensive research, innovative technologies and methodologies implemented in this field.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#32593;&#32476;&#26550;&#26500;&#26377;&#25928;&#22320;&#25972;&#21512;&#32467;&#26500;&#21644;&#27969;&#20307;&#20449;&#24687;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#30340;&#32467;&#26524;&#20998;&#26512;&#34920;&#26126;&#20854;&#22312;&#21306;&#20998;&#21547;&#30719;&#23454;&#20363;&#21644;&#39044;&#27979;&#30719;&#20135;&#21069;&#26223;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65307;&#28040;&#34701;&#30740;&#31350;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#32852;&#21512;&#29305;&#24449;&#21033;&#29992;&#21644;CCA&#34701;&#21512;&#30340;&#30410;&#22788;&#12290;&#36825;&#23545;&#20110;&#22686;&#24378;&#21208;&#25506;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.02911</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#32508;&#21512;&#19977;&#32500;&#30719;&#20135;&#21069;&#26223;&#24314;&#27169;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;&#20197;&#21450;&#32852;&#21512;&#23398;&#20064;&#30340;&#32467;&#26500;-&#27969;&#20307;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
A Multimodal Learning Framework for Comprehensive 3D Mineral Prospectivity Modeling with Jointly Learned Structure-Fluid Relationships. (arXiv:2309.02911v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#32593;&#32476;&#26550;&#26500;&#26377;&#25928;&#22320;&#25972;&#21512;&#32467;&#26500;&#21644;&#27969;&#20307;&#20449;&#24687;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#30340;&#32467;&#26524;&#20998;&#26512;&#34920;&#26126;&#20854;&#22312;&#21306;&#20998;&#21547;&#30719;&#23454;&#20363;&#21644;&#39044;&#27979;&#30719;&#20135;&#21069;&#26223;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65307;&#28040;&#34701;&#30740;&#31350;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#32852;&#21512;&#29305;&#24449;&#21033;&#29992;&#21644;CCA&#34701;&#21512;&#30340;&#30410;&#22788;&#12290;&#36825;&#23545;&#20110;&#22686;&#24378;&#21208;&#25506;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#19977;&#32500;&#30719;&#20135;&#21069;&#26223;&#26144;&#23556;&#65288;3D MPM&#65289;&#65292;&#36890;&#36807;&#28145;&#24230;&#32593;&#32476;&#26550;&#26500;&#26377;&#25928;&#22320;&#25972;&#21512;&#32467;&#26500;&#21644;&#27969;&#20307;&#20449;&#24687;&#12290;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#26469;&#23545;&#40784;&#21644;&#34701;&#21512;&#22810;&#27169;&#24577;&#29305;&#24449;&#12290;&#23545;&#33014;&#33014;&#37329;&#30719;&#24202;&#25968;&#25454;&#38598;&#30340;&#20005;&#26684;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#21306;&#20998;&#21547;&#30719;&#23454;&#20363;&#21644;&#39044;&#27979;&#30719;&#20135;&#21069;&#26223;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#30340;&#32467;&#26524;&#20998;&#26512;&#12290;&#28040;&#34701;&#30740;&#31350;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#32852;&#21512;&#29305;&#24449;&#21033;&#29992;&#21644;CCA&#34701;&#21512;&#30340;&#30410;&#22788;&#12290;&#26412;&#30740;&#31350;&#19981;&#20165;&#25512;&#36827;&#20102;&#30719;&#20135;&#21069;&#26223;&#24314;&#27169;&#65292;&#36824;&#24378;&#35843;&#20102;&#25968;&#25454;&#25972;&#21512;&#21644;&#29305;&#24449;&#23545;&#40784;&#22312;&#22686;&#24378;&#21208;&#25506;&#20915;&#31574;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a novel multimodal fusion model for three-dimensional mineral prospectivity mapping (3D MPM), effectively integrating structural and fluid information through a deep network architecture. Leveraging Convolutional Neural Networks (CNN) and Multilayer Perceptrons (MLP), the model employs canonical correlation analysis (CCA) to align and fuse multimodal features. Rigorous evaluation on the Jiaojia gold deposit dataset demonstrates the model's superior performance in distinguishing ore-bearing instances and predicting mineral prospectivity, outperforming other models in result analyses. Ablation studies further reveal the benefits of joint feature utilization and CCA incorporation. This research not only advances mineral prospectivity modeling but also highlights the pivotal role of data integration and feature alignment for enhanced exploration decision-making.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#20851;&#31995;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#20107;&#20214;&#24207;&#21015;&#24314;&#27169;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20851;&#31995;&#22270;&#24182;&#22312;&#35266;&#27979;&#25968;&#25454;&#20013;&#25429;&#25417;&#21160;&#24577;&#27169;&#24335;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#25512;&#26029;&#20107;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#22312;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#30340;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.02868</link><description>&lt;p&gt;
&#20511;&#21161;&#23545;&#27604;&#20851;&#31995;&#25512;&#29702;&#26469;&#22686;&#24378;&#20107;&#20214;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Enhancing Event Sequence Modeling with Contrastive Relational Inference. (arXiv:2309.02868v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#20851;&#31995;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#20107;&#20214;&#24207;&#21015;&#24314;&#27169;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20851;&#31995;&#22270;&#24182;&#22312;&#35266;&#27979;&#25968;&#25454;&#20013;&#25429;&#25417;&#21160;&#24577;&#27169;&#24335;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#25512;&#26029;&#20107;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#22312;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#30340;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26102;&#24207;&#28857;&#36807;&#31243;&#65288;TPPs&#65289;&#22312;&#24314;&#27169;&#36830;&#32493;&#26102;&#38388;&#20107;&#20214;&#24207;&#21015;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25429;&#25417;&#20107;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#20110;&#25191;&#34892;&#39044;&#27979;&#31561;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#30340;&#25512;&#29702;&#20219;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#20851;&#38190;&#24615;&#30340;&#12290;&#29616;&#26377;&#30340;TPP&#27169;&#22411;&#20391;&#37325;&#20110;&#21442;&#25968;&#21270;&#26410;&#26469;&#20107;&#20214;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#20294;&#38590;&#20197;&#24314;&#27169;&#20107;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#20851;&#31995;&#25512;&#29702;&#65288;NRI&#65289;&#26469;&#23398;&#20064;&#19968;&#20010;&#20851;&#31995;&#22270;&#65292;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#21516;&#26102;&#23398;&#20064;&#21160;&#24577;&#27169;&#24335;&#21644;&#25512;&#26029;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23545;&#27604;&#20851;&#31995;&#25512;&#29702;&#39537;&#21160;&#30340;Hawkes&#36807;&#31243;&#65288;CRIHP&#65289;&#65292;&#22312;&#21464;&#20998;&#25512;&#29702;&#26694;&#26550;&#19979;&#25512;&#29702;&#20107;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#23427;&#21033;&#29992;&#22522;&#20110;&#24378;&#24230;&#30340;&#23398;&#20064;&#26469;&#25628;&#32034;&#23545;&#27604;&#20851;&#31995;&#32422;&#26463;&#30340;&#21407;&#22411;&#36335;&#24452;&#12290;&#22312;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#22312;&#25429;&#25417;&#20107;&#20214;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural temporal point processes(TPPs) have shown promise for modeling continuous-time event sequences. However, capturing the interactions between events is challenging yet critical for performing inference tasks like forecasting on event sequence data. Existing TPP models have focused on parameterizing the conditional distribution of future events but struggle to model event interactions. In this paper, we propose a novel approach that leverages Neural Relational Inference (NRI) to learn a relation graph that infers interactions while simultaneously learning the dynamics patterns from observational data. Our approach, the Contrastive Relational Inference-based Hawkes Process (CRIHP), reasons about event interactions under a variational inference framework. It utilizes intensity-based learning to search for prototype paths to contrast relationship constraints. Extensive experiments on three real-world datasets demonstrate the effectiveness of our model in capturing event interactions f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#24352;&#37327;&#21270;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;&#24352;&#37327;&#21270;&#26725;&#26753;&#20102;&#25968;&#25454;&#30340;&#22810;&#32500;&#29305;&#24615;&#19982;&#20256;&#32479;&#32447;&#24615;&#20195;&#25968;&#26041;&#27861;&#20013;&#30340;&#20108;&#32500;&#30697;&#38453;&#34920;&#31034;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#22810;&#32500;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#34920;&#36798;&#21147;&#21644;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.02428</link><description>&lt;p&gt;
&#36890;&#36807;&#24352;&#37327;&#21270;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65306;&#32508;&#36848;&#19982;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Enhancing Deep Learning Models through Tensorization: A Comprehensive Survey and Framework. (arXiv:2309.02428v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#24352;&#37327;&#21270;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;&#24352;&#37327;&#21270;&#26725;&#26753;&#20102;&#25968;&#25454;&#30340;&#22810;&#32500;&#29305;&#24615;&#19982;&#20256;&#32479;&#32447;&#24615;&#20195;&#25968;&#26041;&#27861;&#20013;&#30340;&#20108;&#32500;&#30697;&#38453;&#34920;&#31034;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#22810;&#32500;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#34920;&#36798;&#21147;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#39046;&#22495;&#25968;&#25454;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;&#30340;&#26085;&#30410;&#22797;&#26434;&#65292;&#20984;&#26174;&#20102;&#23545;&#26356;&#39640;&#25928;&#30340;&#25968;&#25454;&#34920;&#31034;&#21644;&#20998;&#26512;&#25216;&#26415;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#21463;&#21040;Helal&#65288;2023&#65289;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#26088;&#22312;&#20840;&#38754;&#20171;&#32461;&#24352;&#37327;&#21270;&#12290;&#36825;&#31181;&#36716;&#21270;&#24615;&#30340;&#26041;&#27861;&#24357;&#21512;&#20102;&#25968;&#25454;&#30340;&#22266;&#26377;&#22810;&#32500;&#29305;&#24615;&#19982;&#24120;&#29992;&#30340;&#32447;&#24615;&#20195;&#25968;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#31616;&#21270;&#30340;&#20108;&#32500;&#30697;&#38453;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24352;&#37327;&#21270;&#30340;&#27493;&#39588;&#12289;&#22810;&#32500;&#25968;&#25454;&#28304;&#12289;&#21508;&#31181;&#22810;&#26041;&#38754;&#20998;&#26512;&#26041;&#27861;&#30340;&#24212;&#29992;&#20197;&#21450;&#36825;&#20123;&#26041;&#27861;&#30340;&#22909;&#22788;&#12290;&#25991;&#31456;&#36890;&#36807;&#27604;&#36739;&#22312;Python&#20013;&#20351;&#29992;&#20108;&#32500;&#31639;&#27861;&#21644;&#22810;&#26041;&#38754;&#31639;&#27861;&#30340;&#30450;&#28304;&#20998;&#31163;&#65288;BSS&#65289;&#30340;&#23567;&#20363;&#23376;&#65292;&#32467;&#26524;&#34920;&#26126;&#22810;&#26041;&#38754;&#20998;&#26512;&#26356;&#20855;&#34920;&#36798;&#21147;&#12290;&#19982;&#32500;&#24230;&#35781;&#21650;&#30340;&#30452;&#35273;&#30456;&#21453;&#65292;&#21033;&#29992;&#22810;&#32500;&#25968;&#25454;&#38598;&#30340;&#21407;&#22987;&#24418;&#24335;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The burgeoning growth of public domain data and the increasing complexity of deep learning model architectures have underscored the need for more efficient data representation and analysis techniques. This paper is motivated by the work of Helal (2023) and aims to present a comprehensive overview of tensorization. This transformative approach bridges the gap between the inherently multidimensional nature of data and the simplified 2-dimensional matrices commonly used in linear algebra-based machine learning algorithms. This paper explores the steps involved in tensorization, multidimensional data sources, various multiway analysis methods employed, and the benefits of these approaches. A small example of Blind Source Separation (BSS) is presented comparing 2-dimensional algorithms and a multiway algorithm in Python. Results indicate that multiway analysis is more expressive. Contrary to the intuition of the dimensionality curse, utilising multidimensional datasets in their native form 
&lt;/p&gt;</description></item><item><title>Data-Juicer&#26159;&#19968;&#20010;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19968;&#31449;&#24335;&#25968;&#25454;&#22788;&#29702;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#30340;&#25968;&#25454;&#37197;&#26041;&#65292;&#25506;&#32034;&#19981;&#21516;&#30340;&#25968;&#25454;&#28151;&#21512;&#26041;&#24335;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.02033</link><description>&lt;p&gt;
Data-Juicer: &#19968;&#20010;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19968;&#31449;&#24335;&#25968;&#25454;&#22788;&#29702;&#31995;&#32479;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Data-Juicer: A One-Stop Data Processing System for Large Language Models. (arXiv:2309.02033v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02033
&lt;/p&gt;
&lt;p&gt;
Data-Juicer&#26159;&#19968;&#20010;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19968;&#31449;&#24335;&#25968;&#25454;&#22788;&#29702;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#30340;&#25968;&#25454;&#37197;&#26041;&#65292;&#25506;&#32034;&#19981;&#21516;&#30340;&#25968;&#25454;&#28151;&#21512;&#26041;&#24335;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24040;&#22823;&#21457;&#23637;&#20984;&#26174;&#20102;&#22823;&#35268;&#27169;&#12289;&#24322;&#26500;&#21644;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;&#25968;&#25454;&#37197;&#26041;&#26159;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#28151;&#21512;&#29289;&#65292;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#24320;&#28304;&#24037;&#20855;&#20027;&#35201;&#38024;&#23545;&#29305;&#23450;&#25968;&#25454;&#37197;&#26041;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#20026;&#20102;&#19981;&#26029;&#25366;&#25496;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#24341;&#20837;&#26469;&#33258;&#26032;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Data-Juicer&#30340;&#26032;&#31995;&#32479;&#65292;&#36825;&#20010;&#31995;&#32479;&#21487;&#20197;&#39640;&#25928;&#22320;&#29983;&#25104;&#22810;&#26679;&#30340;&#25968;&#25454;&#37197;&#26041;&#65292;&#25506;&#32034;&#19981;&#21516;&#30340;&#25968;&#25454;&#28151;&#21512;&#26041;&#24335;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#19982;&#20256;&#32479;&#30340;&#25968;&#25454;&#20998;&#26512;&#27969;&#31243;&#19981;&#21516;&#65292;Data-Juicer&#38754;&#20020;&#30528;&#19968;&#20123;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#29992;&#20110;&#29983;&#25104;&#25968;&#25454;&#37197;&#26041;&#30340;&#21487;&#33021;&#25968;&#25454;&#28304;&#26159;&#30495;&#27491;&#30340;&#24322;&#26500;&#21644;&#22823;&#35268;&#27169;&#30340;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#36136;&#37327;&#12290;&#20854;&#27425;&#65292;&#31934;&#30830;&#35780;&#20272;&#25968;&#25454;&#37197;&#26041;&#23545;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#38750;&#24120;&#26114;&#36149;&#12290;
&lt;/p&gt;
&lt;p&gt;
The immense evolution in Large Language Models (LLMs) has underscored the importance of massive, heterogeneous, and high-quality data. A data recipe is a mixture of data from different sources for training LLMs, which plays a vital role in LLMs' performance. Existing open-source tools for LLM data processing are mostly tailored for specific data recipes. To continuously uncover the potential of LLMs, incorporate data from new sources, and improve LLMs' performance, we build a new system named Data-Juicer, with which we can efficiently generate diverse data recipes, explore different possibilities in forming data mixtures, and evaluate their effects on model performance. Different from traditional data-analytics pipelines, Data-Juicer faces some unique challenges. Firstly, the possible data sources for forming data recipes are truly heterogeneous and massive with various qualities. Secondly, it is extremely expensive to precisely evaluate data recipes' impact on LLMs' performance. Third
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32034;&#24341;&#24863;&#30693;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#30005;&#36335;&#36827;&#34892;&#35299;&#21078;&#27010;&#24565;&#30340;&#24212;&#29992;&#65292;&#23558;&#24046;&#20998;&#20195;&#25968;&#26041;&#31243;&#32452;&#20998;&#35299;&#20026;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#20195;&#25968;&#26041;&#31243;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#29616;&#26377;&#30693;&#35782;&#26469;&#37327;&#21270;&#30005;&#36335;&#35774;&#35745;&#20013;&#21487;&#35843;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.00958</link><description>&lt;p&gt;
&#30005;&#36335;&#30340;&#32034;&#24341;&#24863;&#30693;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Index-aware learning of circuits. (arXiv:2309.00958v2 [cs.CE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00958
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32034;&#24341;&#24863;&#30693;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#30005;&#36335;&#36827;&#34892;&#35299;&#21078;&#27010;&#24565;&#30340;&#24212;&#29992;&#65292;&#23558;&#24046;&#20998;&#20195;&#25968;&#26041;&#31243;&#32452;&#20998;&#35299;&#20026;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#20195;&#25968;&#26041;&#31243;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#29616;&#26377;&#30693;&#35782;&#26469;&#37327;&#21270;&#30005;&#36335;&#35774;&#35745;&#20013;&#21487;&#35843;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#30005;&#36335;&#22312;&#21508;&#31181;&#25216;&#26415;&#20013;&#37117;&#23384;&#22312;&#65292;&#20854;&#35774;&#35745;&#26159;&#35745;&#31639;&#26426;&#36741;&#21161;&#24037;&#31243;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#24433;&#21709;&#26368;&#32456;&#35774;&#35745;&#30340;&#21487;&#35843;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#23548;&#33268;&#20102;&#23545;&#37327;&#21270;&#20854;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#22312;&#36825;&#26041;&#38754;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#65292;&#28982;&#32780;&#30446;&#21069;&#30340;&#26041;&#27861;&#24448;&#24448;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#26377;&#20851;&#29616;&#26377;&#31995;&#32479;&#30340;&#30693;&#35782;&#12290;&#23601;&#30005;&#36335;&#32780;&#35328;&#65292;&#36890;&#36807;&#20462;&#25913;&#33410;&#28857;&#20998;&#26512;&#23545;&#20854;&#36827;&#34892;&#25551;&#36848;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#12290;&#36825;&#31181;&#29305;&#23450;&#30340;&#34920;&#36798;&#24418;&#24335;&#23548;&#33268;&#20102;&#24046;&#20998;&#20195;&#25968;&#26041;&#31243;&#32452;&#65288;DAEs&#65289;&#65292;&#20854;&#20013;&#23384;&#22312;&#19968;&#20123;&#29305;&#27530;&#24615;&#36136;&#65292;&#20363;&#22914;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#28385;&#36275;&#30340;&#38544;&#34255;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20351;&#29992;&#26368;&#36817;&#24341;&#20837;&#30340;&#29992;&#20110;DAEs&#30340;&#35299;&#21078;&#27010;&#24565;&#65292;&#21487;&#20197;&#23558;&#32473;&#23450;&#31995;&#32479;&#20998;&#35299;&#20026;&#20165;&#20381;&#36182;&#20110;&#24046;&#20998;&#21464;&#37327;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#20165;&#25551;&#36848;&#24046;&#20998;&#21644;&#20195;&#25968;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#20195;&#25968;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrical circuits are present in a variety of technologies, making their design an important part of computer aided engineering. The growing number of tunable parameters that affect the final design leads to a need for new approaches of quantifying their impact. Machine learning may play a key role in this regard, however current approaches often make suboptimal use of existing knowledge about the system at hand. In terms of circuits, their description via modified nodal analysis is well-understood. This particular formulation leads to systems of differential-algebraic equations (DAEs) which bring with them a number of peculiarities, e.g. hidden constraints that the solution needs to fulfill. We aim to use the recently introduced dissection concept for DAEs that can decouple a given system into ordinary differential equations, only depending on differential variables, and purely algebraic equations that describe the relations between differential and algebraic variables. The idea the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#22810;&#20307;&#37327;&#23376;&#21160;&#21147;&#23398;&#30340;&#39640;&#25928;&#37327;&#23376;&#31639;&#27861;&#65292;&#36890;&#36807;&#22359;&#32534;&#30721;&#25216;&#26415;&#23454;&#29616;&#20102;&#37327;&#23376;&#35745;&#31639;&#30340;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2308.14239</link><description>&lt;p&gt;
&#37327;&#23376;&#19979;&#19968;&#20195;&#27833;&#34255;&#35745;&#31639;&#65306;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#37327;&#23376;&#21160;&#21147;&#23398;&#30340;&#39640;&#25928;&#37327;&#23376;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantum Next Generation Reservoir Computing: An Efficient Quantum Algorithm for Forecasting Quantum Dynamics. (arXiv:2308.14239v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#22810;&#20307;&#37327;&#23376;&#21160;&#21147;&#23398;&#30340;&#39640;&#25928;&#37327;&#23376;&#31639;&#27861;&#65292;&#36890;&#36807;&#22359;&#32534;&#30721;&#25216;&#26415;&#23454;&#29616;&#20102;&#37327;&#23376;&#35745;&#31639;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20195;&#27833;&#34255;&#35745;&#31639;&#26159;&#19968;&#31181;&#29616;&#20195;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#30001;&#21160;&#21147;&#31995;&#32479;&#20135;&#29983;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19979;&#19968;&#20195;&#27833;&#34255;&#35745;&#31639;&#21487;&#20197;&#22312;&#21487;&#31215;&#21644;&#28151;&#27788;&#31995;&#32479;&#20013;&#20934;&#30830;&#39044;&#27979;&#25972;&#20307;&#22810;&#20307;&#37327;&#23376;&#21160;&#21147;&#23398;&#65292;&#19982;&#20256;&#32479;&#30340;&#27833;&#34255;&#35745;&#31639;&#26041;&#27861;&#30456;&#27604;&#65292;&#21518;&#32773;&#21482;&#30528;&#37325;&#20110;&#39044;&#27979;&#21487;&#35266;&#23519;&#37327;&#30340;&#21160;&#21147;&#23398;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#31181;&#31216;&#20026;"&#36339;&#20808;"&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#36828;&#26410;&#26469;&#30340;&#29366;&#24577;&#65292;&#32780;&#26080;&#38656;&#25552;&#21462;&#20013;&#38388;&#29366;&#24577;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#37319;&#29992;&#32463;&#20856;&#30340;&#19979;&#19968;&#20195;&#27833;&#34255;&#35745;&#31639;&#26041;&#27861;&#26469;&#39044;&#27979;&#22810;&#20307;&#37327;&#23376;&#21160;&#21147;&#23398;&#26159;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#26679;&#26412;&#36755;&#20837;&#25968;&#25454;&#30340;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#38750;&#24120;&#22823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#29992;&#20110;&#22810;&#20307;&#37327;&#23376;&#21160;&#21147;&#23398;&#39044;&#27979;&#30340;&#37327;&#23376;&#31639;&#27861;&#65292;&#36890;&#36807;&#22359;&#32534;&#30721;&#25216;&#26415;&#23454;&#29616;&#37327;&#23376;&#35745;&#31639;&#30340;&#21152;&#36895;&#12290;&#36825;&#20010;&#26041;&#26696;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Next Generation Reservoir Computing (NG-RC) is a modern class of model-free machine learning that enables an accurate forecasting of time series data generated by dynamical systems. We demonstrate that NG-RC can accurately predict full many-body quantum dynamics in both integrable and chaotic systems. This is in contrast to the conventional application of reservoir computing that concentrates on the prediction of the dynamics of observables. In addition, we apply a technique which we refer to as skipping ahead to predict far future states accurately without the need to extract information about the intermediate states. However, adopting a classical NG-RC for many-body quantum dynamics prediction is computationally prohibitive due to the large Hilbert space of sample input data. In this work, we propose an end-to-end quantum algorithm for many-body quantum dynamics forecasting with a quantum computational speedup via the block-encoding technique. This proposal presents an efficient mode
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#27169;&#22359;&#21270;&#26435;&#37325;&#21644;&#39046;&#22495;&#36801;&#31227;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#26435;&#37325;&#25513;&#30721;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#22312;&#20445;&#25345;&#28304;&#20219;&#21153;&#30693;&#35782;&#30340;&#21516;&#26102;&#20801;&#35768;&#39640;&#25928;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13957</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#26435;&#37325;&#25513;&#30721;&#29992;&#20110;&#39046;&#22495;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Differentiable Weight Masks for Domain Transfer. (arXiv:2308.13957v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#27169;&#22359;&#21270;&#26435;&#37325;&#21644;&#39046;&#22495;&#36801;&#31227;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#26435;&#37325;&#25513;&#30721;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#22312;&#20445;&#25345;&#28304;&#20219;&#21153;&#30693;&#35782;&#30340;&#21516;&#26102;&#20801;&#35768;&#39640;&#25928;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#23427;&#20204;&#26080;&#27861;&#20197;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#20445;&#30041;&#22810;&#20010;&#20449;&#24687;&#28304;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#19968;&#20010;&#22312;&#28304;&#20219;&#21153;&#19978;&#35757;&#32451;&#36807;&#30340;&#32593;&#32476;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#20445;&#25345;&#20854;&#22312;&#28304;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#23558;&#20854;&#37325;&#26032;&#35757;&#32451;&#21040;&#19968;&#20010;&#30456;&#20284;&#20294;&#19981;&#21516;&#30340;&#30446;&#26631;&#20219;&#21153;&#19978;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#32593;&#32476;&#26435;&#37325;&#30340;&#27169;&#22359;&#21270;&#65292;&#20197;&#23450;&#20301;&#21644;&#30830;&#23450;&#23545;&#20110;&#35302;&#21457;&#32473;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#30340;&#26435;&#37325;&#38598;&#21512;&#12290;&#19968;&#20123;&#24037;&#20316;&#30740;&#31350;&#20102;&#36890;&#36807;&#23398;&#20064;&#21644;&#20998;&#26512;&#26435;&#37325;&#25513;&#30721;&#24341;&#20837;&#30340;&#32593;&#32476;&#26435;&#37325;&#30340;&#27169;&#22359;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#39046;&#22495;&#32467;&#21512;&#36215;&#26469;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#26435;&#37325;&#25513;&#30721;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#22312;&#32531;&#35299;&#28304;&#20219;&#21153;&#30340;&#8220;&#36951;&#24536;&#8221;&#21516;&#26102;&#20801;&#35768;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#36827;&#34892;&#39640;&#25928;&#24494;&#35843;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;&#25513;&#30721;&#25216;&#26415;&#22312;&#20445;&#30041;&#28304;&#20219;&#21153;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the major drawbacks of deep learning models for computer vision has been their inability to retain multiple sources of information in a modular fashion. For instance, given a network that has been trained on a source task, we would like to re-train this network on a similar, yet different, target task while maintaining its performance on the source task. Simultaneously, researchers have extensively studied modularization of network weights to localize and identify the set of weights culpable for eliciting the observed performance on a given task. One set of works studies the modularization induced in the weights of a neural network by learning and analysing weight masks. In this work, we combine these fields to study three such weight masking methods and analyse their ability to mitigate "forgetting'' on the source task while also allowing for efficient finetuning on the target task. We find that different masking techniques have trade-offs in retaining knowledge in the source t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21097;&#20313;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;RDDM&#65289;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#39044;&#27979;&#27531;&#24046;&#26469;&#34920;&#31034;&#20174;&#30446;&#26631;&#22495;&#21040;&#36755;&#20837;&#22495;&#30340;&#26041;&#21521;&#24615;&#25193;&#25955;&#65292;&#24182;&#21516;&#26102;&#20272;&#35745;&#22122;&#22768;&#26469;&#32771;&#34385;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2308.13712</link><description>&lt;p&gt;
&#21097;&#20313;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Residual Denoising Diffusion Models. (arXiv:2308.13712v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13712
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21097;&#20313;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;RDDM&#65289;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#39044;&#27979;&#27531;&#24046;&#26469;&#34920;&#31034;&#20174;&#30446;&#26631;&#22495;&#21040;&#36755;&#20837;&#22495;&#30340;&#26041;&#21521;&#24615;&#25193;&#25955;&#65292;&#24182;&#21516;&#26102;&#20272;&#35745;&#22122;&#22768;&#26469;&#32771;&#34385;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#26041;&#27861;&#23558;&#36864;&#21270;&#30340;&#36755;&#20837;&#22270;&#20687;&#20316;&#20026;&#26465;&#20214;&#36755;&#20837;&#21040;&#22122;&#22768;&#20272;&#35745;&#32593;&#32476;&#20013;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#36825;&#20010;&#25193;&#25955;&#36807;&#31243;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#22522;&#26412;&#19978;&#26159;&#20174;&#22122;&#22768;&#29983;&#25104;&#30446;&#26631;&#22270;&#20687;&#12290;&#20026;&#20102;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#19988;&#26356;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#24674;&#22797;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21097;&#20313;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;RDDM&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#65288;&#20363;&#22914;DDPM&#25110;DDIM&#65289;&#20165;&#19987;&#27880;&#20110;&#22122;&#22768;&#20272;&#35745;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;RDDM&#39044;&#27979;&#27531;&#24046;&#26469;&#34920;&#31034;&#20174;&#30446;&#26631;&#22495;&#21040;&#36755;&#20837;&#22495;&#30340;&#26041;&#21521;&#24615;&#25193;&#25955;&#65292;&#24182;&#21516;&#26102;&#20272;&#35745;&#22122;&#22768;&#26469;&#32771;&#34385;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#25200;&#21160;&#12290;&#27531;&#24046;&#30340;&#24341;&#20837;&#20351;&#25105;&#20204;&#33021;&#22815;&#37325;&#26032;&#23450;&#20041;&#27491;&#21521;&#25193;&#25955;&#36807;&#31243;&#65292;&#20854;&#20013;&#30446;&#26631;&#22270;&#20687;&#36880;&#28176;&#25193;&#25955;&#25104;&#19968;&#20010;&#32431;&#22122;&#22768;&#22270;&#20687;&#25110;&#25658;&#24102;&#22122;&#22768;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#20174;&#32780;&#32479;&#19968;&#20102;&#22270;&#20687;&#29983;&#25104;&#21644;&#24674;&#22797;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#37319;&#26679;&#36807;&#31243;&#19982;&#30495;&#23454;&#30340;&#30446;&#26631;&#22270;&#20687;&#20998;&#24067;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current diffusion-based image restoration methods feed degraded input images as conditions into the noise estimation network. However, interpreting this diffusion process is challenging since it essentially generates the target image from the noise. To establish a unified and more interpretable model for image generation and restoration, we propose residual denoising diffusion models (RDDM). In contrast to existing diffusion models (e.g., DDPM or DDIM) that focus solely on noise estimation, our RDDM predicts residuals to represent directional diffusion from the target domain to the input domain, while concurrently estimating noise to account for random perturbations in the diffusion process. The introduction of residuals allows us to redefine the forward diffusion process, wherein the target image progressively diffuses into a purely noisy image or a noise-carrying input image, thus unifying image generation and restoration. We demonstrate that our sampling process is consistent with t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13570</link><description>&lt;p&gt;
&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38543;&#26426;&#37197;&#32622;&#26426;
&lt;/p&gt;
&lt;p&gt;
Stochastic Configuration Machines for Industrial Artificial Intelligence. (arXiv:2308.13570v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#65288;IAI&#65289;&#20013;&#65292;&#38656;&#35201;&#23454;&#26102;&#12289;&#20934;&#30830;&#30340;&#39044;&#27979;&#24314;&#27169;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#20854;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#24378;&#22823;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#35774;&#22791;&#26469;&#22788;&#29702;&#22823;&#37327;&#30340;&#28014;&#28857;&#25968;&#25454;&#12290;&#26412;&#25991;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20197;&#24378;&#35843;&#23545;&#20110;&#24037;&#19994;&#24212;&#29992;&#38750;&#24120;&#26377;&#29992;&#21644;&#26377;&#20215;&#20540;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;&#19982;&#20855;&#26377;&#20108;&#20540;&#21270;&#23454;&#29616;&#30340;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#65288;RVFL&#65289;&#32593;&#32476;&#30456;&#27604;&#65292;SCMs&#30340;&#27169;&#22411;&#23384;&#20648;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#38500;&#20102;SCM&#23398;&#20064;&#22120;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;&#20316;&#20026;&#26412;&#25991;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#25552;&#20379;&#20102;SCMs&#30340;&#23398;&#20064;&#33021;&#21147;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#23454;&#39564;&#30740;&#31350;&#20063;&#36827;&#34892;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time predictive modelling with desired accuracy is highly expected in industrial artificial intelligence (IAI), where neural networks play a key role. Neural networks in IAI require powerful, high-performance computing devices to operate a large number of floating point data. Based on stochastic configuration networks (SCNs), this paper proposes a new randomized learner model, termed stochastic configuration machines (SCMs), to stress effective modelling and data size saving that are useful and valuable for industrial applications. Compared to SCNs and random vector functional-link (RVFL) nets with binarized implementation, the model storage of SCMs can be significantly compressed while retaining favourable prediction performance. Besides the architecture of the SCM learner model and its learning algorithm, as an important part of this contribution, we also provide a theoretical basis on the learning capacity of SCMs by analysing the model's complexity. Experimental studies are ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.10792</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#35843;&#20248;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#65288;IT&#65289;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#36825;&#26159;&#19968;&#31181;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#25351;&#20196;&#35843;&#20248;&#26159;&#25351;&#20197;&#30417;&#30563;&#26041;&#24335;&#22312;&#21253;&#21547;&#8220;&#25351;&#20196;-&#36755;&#20986;&#8221;&#23545;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#19968;&#27493;&#35757;&#32451;LLM&#65292;&#36825;&#23558;LLM&#30340;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#30446;&#26631;&#19982;&#29992;&#25143;&#24076;&#26395;LLM&#36981;&#23432;&#20154;&#31867;&#25351;&#20196;&#30340;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#23545;IT&#30340;&#24120;&#35268;&#26041;&#27861;&#12289;IT&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#12289;IT&#27169;&#22411;&#30340;&#35757;&#32451;&#20197;&#21450;&#24212;&#29992;&#20110;&#19981;&#21516;&#27169;&#24577;&#12289;&#39046;&#22495;&#21644;&#24212;&#29992;&#30340;&#24773;&#20917;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#23545;&#24433;&#21709;IT&#32467;&#26524;&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#20998;&#26512;&#65288;&#20363;&#22914;&#65292;&#25351;&#20196;&#36755;&#20986;&#30340;&#29983;&#25104;&#12289;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#31561;&#65289;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;IT&#30340;&#28508;&#22312;&#38382;&#39064;&#20197;&#21450;&#38024;&#23545;&#20854;&#30340;&#25209;&#35780;&#65292;&#20197;&#21450;&#25351;&#20986;&#24403;&#21069;&#19981;&#36275;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26102;&#31354;&#33258;&#36866;&#24212;&#23884;&#20837;&#30340;&#26032;&#32452;&#20214;&#65292;&#22312;&#26222;&#36890;&#30340;Transformer&#20013;&#23454;&#29616;&#20102;&#39046;&#20808;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20132;&#36890;&#39044;&#27979;&#24615;&#33021;&#65292;&#36890;&#36807;&#25429;&#25417;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#31354;&#20851;&#31995;&#21644;&#26102;&#38388;&#20449;&#24687;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.10425</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#33258;&#36866;&#24212;&#23884;&#20837;&#65292;&#26222;&#36890;Transformer&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#39046;&#20808;&#20110;&#20854;&#20182;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Adaptive Embedding Makes Vanilla Transformer SOTA for Traffic Forecasting. (arXiv:2308.10425v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26102;&#31354;&#33258;&#36866;&#24212;&#23884;&#20837;&#30340;&#26032;&#32452;&#20214;&#65292;&#22312;&#26222;&#36890;&#30340;Transformer&#20013;&#23454;&#29616;&#20102;&#39046;&#20808;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20132;&#36890;&#39044;&#27979;&#24615;&#33021;&#65292;&#36890;&#36807;&#25429;&#25417;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#31354;&#20851;&#31995;&#21644;&#26102;&#38388;&#20449;&#24687;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20934;&#30830;&#30340;&#20132;&#36890;&#39044;&#27979;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20854;&#26680;&#24515;&#29942;&#39048;&#22312;&#20110;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#20132;&#36890;&#27169;&#24335;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#20855;&#26377;&#22797;&#26434;&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#26550;&#26500;&#30340;&#36827;&#27493;&#36935;&#21040;&#20102;&#24615;&#33021;&#25910;&#30410;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26102;&#31354;&#33258;&#36866;&#24212;&#23884;&#20837;&#30340;&#26032;&#32452;&#20214;&#65292;&#21487;&#20197;&#22312;&#26222;&#36890;&#30340;Transformer&#20013;&#33719;&#24471;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;Spatio-Temporal Adaptive Embedding transformer&#65288;STAEformer&#65289;&#22312;&#20116;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20132;&#36890;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26102;&#31354;&#33258;&#36866;&#24212;&#23884;&#20837;&#36890;&#36807;&#26377;&#25928;&#25429;&#25417;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#20869;&#22312;&#26102;&#31354;&#20851;&#31995;&#21644;&#26102;&#38388;&#20449;&#24687;&#65292;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of the Intelligent Transportation System (ITS), accurate traffic forecasting has emerged as a critical challenge. The key bottleneck lies in capturing the intricate spatio-temporal traffic patterns. In recent years, numerous neural networks with complicated architectures have been proposed to address this issue. However, the advancements in network architectures have encountered diminishing performance gains. In this study, we present a novel component called spatio-temporal adaptive embedding that can yield outstanding results with vanilla transformers. Our proposed Spatio-Temporal Adaptive Embedding transformer (STAEformer) achieves state-of-the-art performance on five real-world traffic forecasting datasets. Further experiments demonstrate that spatio-temporal adaptive embedding plays a crucial role in traffic forecasting by effectively capturing intrinsic spatio-temporal relations and chronological information in traffic time series.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;U&#22411;&#20998;&#23618;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#26041;&#26696;&#20197;&#25552;&#39640;&#36793;&#32536;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08896</link><description>&lt;p&gt;
&#20026;U&#22411;&#24182;&#34892;&#20998;&#23618;&#23398;&#20064;&#36827;&#34892;&#26368;&#20248;&#36164;&#28304;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Optimal Resource Allocation for U-Shaped Parallel Split Learning. (arXiv:2308.08896v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;U&#22411;&#20998;&#23618;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#26041;&#26696;&#20197;&#25552;&#39640;&#36793;&#32536;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#23398;&#20064;&#65288;SL&#65289;&#20316;&#20026;&#19968;&#31181;&#22312;&#19981;&#20844;&#24320;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#21407;&#22987;&#25968;&#25454;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#30340;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#32780;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;SL&#19981;&#21487;&#36991;&#20813;&#22320;&#27844;&#28431;&#20102;&#26631;&#31614;&#38544;&#31169;&#65292;&#22240;&#20026;&#23614;&#37096;&#27169;&#22411;&#65288;&#20855;&#26377;&#26368;&#21518;&#20960;&#23618;&#65289;&#24212;&#35813;&#25918;&#22312;&#26381;&#21153;&#22120;&#19978;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#21033;&#29992;U&#22411;&#26550;&#26500;&#23558;&#26089;&#26399;&#23618;&#21644;&#26368;&#21518;&#23618;&#37117;&#25918;&#22312;&#29992;&#25143;&#31471;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;U&#22411;&#20998;&#23618;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#26041;&#26696;&#20197;&#25552;&#39640;&#36793;&#32536;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#22810;&#20010;&#29992;&#25143;&#19982;&#36793;&#32536;&#26381;&#21153;&#22120;&#36827;&#34892;SL&#36890;&#20449;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36164;&#28304;&#20998;&#37197;&#31639;&#27861;&#65292;&#31216;&#20026;LSCRA&#65292;&#23427;&#21487;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#21644;&#20998;&#23618;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;LSCRA&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;U&#22411;PSL&#21487;&#20197;&#36798;&#21040;&#19982;&#20854;&#20182;S&#26041;&#27861;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split learning (SL) has emerged as a promising approach for model training without revealing the raw data samples from the data owners. However, traditional SL inevitably leaks label privacy as the tail model (with the last layers) should be placed on the server. To overcome this limitation, one promising solution is to utilize U-shaped architecture to leave both early layers and last layers on the user side. In this paper, we develop a novel parallel U-shaped split learning and devise the optimal resource optimization scheme to improve the performance of edge networks. In the proposed framework, multiple users communicate with an edge server for SL. We analyze the end-to-end delay of each client during the training process and design an efficient resource allocation algorithm, called LSCRA, which finds the optimal computing resource allocation and split layers. Our experimental results show the effectiveness of LSCRA and that U-shaped PSL can achieve a similar performance with other S
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#39640;&#20809;&#35889;&#20016;&#24230;&#39044;&#27979;&#30340;&#37325;&#26500;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#36127;&#26368;&#23567;&#20108;&#20056;&#20248;&#21270;&#26500;&#24314;&#31232;&#30095;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#26368;&#22823;&#20284;&#28982;&#21387;&#32553;&#21521;&#37327;&#20943;&#23569;&#20449;&#24687;&#25439;&#22833;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20809;&#35889;&#37325;&#24314;&#35823;&#24046;&#21644;&#21387;&#32553;&#29575;&#26041;&#38754;&#30456;&#27604;&#26631;&#20934;&#20462;&#21098;&#12289;&#26368;&#23567;&#20108;&#20056;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08653</link><description>&lt;p&gt;
&#29992;&#20110;&#31232;&#30095;&#39640;&#20809;&#35889;&#20016;&#24230;&#39044;&#27979;&#30340;&#37325;&#26500;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Reproducing Kernel Hilbert Space Pruning for Sparse Hyperspectral Abundance Prediction. (arXiv:2308.08653v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#39640;&#20809;&#35889;&#20016;&#24230;&#39044;&#27979;&#30340;&#37325;&#26500;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#36127;&#26368;&#23567;&#20108;&#20056;&#20248;&#21270;&#26500;&#24314;&#31232;&#30095;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#26368;&#22823;&#20284;&#28982;&#21387;&#32553;&#21521;&#37327;&#20943;&#23569;&#20449;&#24687;&#25439;&#22833;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20809;&#35889;&#37325;&#24314;&#35823;&#24046;&#21644;&#21387;&#32553;&#29575;&#26041;&#38754;&#30456;&#27604;&#26631;&#20934;&#20462;&#21098;&#12289;&#26368;&#23567;&#20108;&#20056;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#36317;&#31163;&#20256;&#24863;&#22120;&#33719;&#21462;&#30340;&#39640;&#20809;&#35889;&#27979;&#37327;&#25968;&#25454;&#21487;&#20197;&#32473;&#20986;&#22330;&#26223;&#20013;&#29289;&#21697;&#12289;&#26448;&#26009;&#21644;&#21270;&#23398;&#21697;&#30340;&#35814;&#32454;&#24773;&#20917;&#65292;&#20294;&#30001;&#20110;&#20808;&#36827;&#20256;&#24863;&#22120;&#30340;&#39640;&#31354;&#38388;&#21644;&#20809;&#35889;&#20998;&#36776;&#29575;&#65292;&#20998;&#26512;&#21464;&#24471;&#22256;&#38590;&#12289;&#32531;&#24930;&#21644;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;&#31232;&#30095;&#24615;&#23545;&#20110;&#23454;&#29616;&#20809;&#35889;&#21387;&#32553;&#21644;&#20998;&#26512;&#30340;&#26410;&#26469;&#38750;&#24120;&#37325;&#35201;&#12290;&#24050;&#32463;&#35266;&#23519;&#21040;&#29615;&#22659;&#21644;&#22823;&#27668;&#25928;&#24212;&#65292;&#21253;&#25324;&#25955;&#23556;&#65292;&#20250;&#20135;&#29983;&#38750;&#32447;&#24615;&#25928;&#24212;&#65292;&#23545;&#20110;&#29616;&#26377;&#30340;&#28304;&#20998;&#31163;&#21644;&#21387;&#32553;&#26041;&#27861;&#26500;&#25104;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#36127;&#26368;&#23567;&#20108;&#20056;&#20248;&#21270;&#22312;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#36827;&#34892;&#20462;&#21098;&#21644;&#26500;&#24314;&#31232;&#30095;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26368;&#22823;&#20284;&#28982;&#21387;&#32553;&#21521;&#37327;&#26469;&#20943;&#23569;&#20449;&#24687;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26631;&#20934;&#20462;&#21098;&#21644;&#26368;&#23567;&#20108;&#20056;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32771;&#23519;&#20102;&#24635;&#20307;&#20809;&#35889;&#37325;&#24314;&#35823;&#24046;&#21644;&#21387;&#32553;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperspectral measurements from long range sensors can give a detailed picture of the items, materials, and chemicals in a scene but analysis can be difficult, slow, and expensive due to high spatial and spectral resolutions of state-of-the-art sensors. As such, sparsity is important to enable the future of spectral compression and analytics. It has been observed that environmental and atmospheric effects, including scattering, can produce nonlinear effects posing challenges for existing source separation and compression methods. We present a novel transformation into Hilbert spaces for pruning and constructing sparse representations via non-negative least squares minimization. Then we introduce max likelihood compression vectors to decrease information loss. Our approach is benchmarked against standard pruning and least squares as well as deep learning methods. Our methods are evaluated in terms of overall spectral reconstruction error and compression rate using real and synthetic dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#27604;&#20998;&#26512;&#20102;12&#31181;&#33258;&#28982;&#21551;&#21457;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#22312;&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#21033;&#29992;&#36825;&#20123;&#31639;&#27861;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#24182;&#32467;&#21512;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#29305;&#24449;&#38598;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2308.08574</link><description>&lt;p&gt;
&#33258;&#28982;&#21551;&#21457;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#22312;&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#20013;&#30340;&#33021;&#21147;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Analysis of the Capabilities of Nature-inspired Feature Selection Algorithms in Predicting Student Performance. (arXiv:2308.08574v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#27604;&#20998;&#26512;&#20102;12&#31181;&#33258;&#28982;&#21551;&#21457;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#22312;&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#21033;&#29992;&#36825;&#20123;&#31639;&#27861;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#24182;&#32467;&#21512;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#29305;&#24449;&#38598;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#23545;&#20110;&#26377;&#25928;&#38450;&#27490;&#39118;&#38505;&#23398;&#29983;&#22833;&#36133;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#22871;12&#20010;&#33258;&#28982;&#21551;&#21457;&#31639;&#27861;&#22312;&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#20013;&#30340;&#30456;&#23545;&#24615;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#23454;&#20363;&#30340;&#28857;&#20987;&#27969;&#25968;&#25454;&#12289;&#35838;&#20869;&#21333;&#19968;&#35838;&#31243;&#34920;&#29616;&#20197;&#21450;&#21516;&#26102;&#21442;&#21152;&#22810;&#20010;&#35838;&#31243;&#26102;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#21033;&#29992;&#33258;&#28982;&#21551;&#21457;&#30340;&#31639;&#27861;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#24182;&#32467;&#21512;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#29305;&#24449;&#38598;&#22823;&#23567;&#30340;2/3&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting student performance is key in leveraging effective pre-failure interventions for at-risk students. In this paper, I have analyzed the relative performance of a suite of 12 nature-inspired algorithms when used to predict student performance across 3 datasets consisting of instance-based clickstream data, intra-course single-course performance, and performance when taking multiple courses simultaneously. I found that, for all datasets, leveraging an ensemble approach using NIAs for feature selection and traditional ML algorithms for classification increased predictive accuracy while also reducing feature set size by 2/3.
&lt;/p&gt;</description></item><item><title>NeFL&#26159;&#19968;&#20010;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#21644;&#23485;&#24230;&#32553;&#25918;&#23558;&#27169;&#22411;&#26377;&#25928;&#22320;&#21010;&#20998;&#20026;&#23376;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#24930;&#25110;&#33021;&#21147;&#26377;&#38480;&#30340;&#23458;&#25143;&#31471;&#23548;&#33268;&#30340;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07761</link><description>&lt;p&gt;
NeFL: &#38024;&#23545;&#24322;&#26500;&#23458;&#25143;&#31471;&#30340;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
NeFL: Nested Federated Learning for Heterogeneous Clients. (arXiv:2308.07761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07761
&lt;/p&gt;
&lt;p&gt;
NeFL&#26159;&#19968;&#20010;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#21644;&#23485;&#24230;&#32553;&#25918;&#23558;&#27169;&#22411;&#26377;&#25928;&#22320;&#21010;&#20998;&#20026;&#23376;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#24930;&#25110;&#33021;&#21147;&#26377;&#38480;&#30340;&#23458;&#25143;&#31471;&#23548;&#33268;&#30340;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#25345;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#24930;&#25110;&#33021;&#21147;&#26377;&#38480;&#30340;&#23458;&#25143;&#31471;&#65288;&#21363;&#38459;&#22622;&#32773;&#65289;&#20250;&#20943;&#24930;&#24635;&#20307;&#35757;&#32451;&#26102;&#38388;&#24182;&#38477;&#20302;&#24615;&#33021;&#12290;&#31995;&#32479;&#30340;&#24322;&#26500;&#24615;&#65292;&#21253;&#25324;&#24322;&#26500;&#35745;&#31639;&#21644;&#32593;&#32476;&#24102;&#23485;&#65292;&#24050;&#32463;&#34987;&#29992;&#26469;&#20943;&#36731;&#38459;&#22622;&#32773;&#30340;&#24433;&#21709;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#23558;&#27169;&#22411;&#20998;&#21106;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#22312;&#27169;&#22411;&#26550;&#26500;&#26041;&#38754;&#30340;&#33258;&#30001;&#24230;&#36739;&#23567;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;&#65288;NeFL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#29992;&#28145;&#24230;&#21644;&#23485;&#24230;&#32553;&#25918;&#23558;&#27169;&#22411;&#26377;&#25928;&#22320;&#20998;&#25104;&#23376;&#27169;&#22411;&#12290;NeFL&#36890;&#36807;&#23558;&#27169;&#22411;&#35299;&#37322;&#20026;&#35299;&#20915;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#27493;&#38271;&#26469;&#23454;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#35757;&#32451;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#30340;&#22810;&#20010;&#23376;&#27169;&#22411;&#26102;&#20986;&#29616;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#35299;&#32806;&#20102;&#19968;&#20123;&#21442;&#25968;&#12290;NeFL&#20351;&#36164;&#28304;&#21463;&#38480;&#30340;&#23458;&#25143;&#31471;&#33021;&#22815;&#26377;&#25928;&#22320;&#21152;&#20837;&#32852;&#37030;&#23398;&#20064;&#27969;&#31243;&#65292;&#24182;&#20351;&#27169;&#22411;&#33021;&#22815;&#34987;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a promising approach in distributed learning keeping privacy. However, during the training pipeline of FL, slow or incapable clients (i.e., stragglers) slow down the total training time and degrade performance. System heterogeneity, including heterogeneous computing and network bandwidth, has been addressed to mitigate the impact of stragglers. Previous studies split models to tackle the issue, but with less degree-of-freedom in terms of model architecture. We propose nested federated learning (NeFL), a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling. NeFL is implemented by interpreting models as solving ordinary differential equations (ODEs) with adaptive step sizes. To address the inconsistency that arises when training multiple submodels with different architecture, we decouple a few parameters. NeFL enables resource-constrained clients to effectively join the FL pipeline and the model to be 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#38750;&#21307;&#23398;&#22270;&#20687;&#19978;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#39044;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20110;ImageNet&#30340;&#39044;&#35757;&#32451;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#22686;&#24378;&#32593;&#32476;&#21021;&#22987;&#21270;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2308.07688</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#30340;&#33258;&#28982;&#22270;&#20687;&#22686;&#24378;&#21307;&#30103;AI&#27169;&#22411;&#30340;&#32593;&#32476;&#21021;&#22987;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhancing Network Initialization for Medical AI Models Using Large-Scale, Unlabeled Natural Images. (arXiv:2308.07688v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07688
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#38750;&#21307;&#23398;&#22270;&#20687;&#19978;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#39044;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20110;ImageNet&#30340;&#39044;&#35757;&#32451;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#22686;&#24378;&#32593;&#32476;&#21021;&#22987;&#21270;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet&#65289;&#24050;&#25104;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#40644;&#37329;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30340;&#20986;&#29616;&#25552;&#20379;&#20102;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#23398;&#20064;&#24378;&#22823;&#29305;&#24449;&#30340;&#26426;&#20250;&#65292;&#20174;&#32780;&#21487;&#20197;&#32469;&#36807;&#32321;&#37325;&#30340;&#26631;&#27880;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;SSL&#39044;&#35757;&#32451;&#22312;&#38750;&#21307;&#23398;&#22270;&#20687;&#19978;&#26159;&#21542;&#21487;&#20197;&#24212;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#65292;&#24182;&#19982;&#38750;&#21307;&#23398;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#24182;&#26681;&#25454;&#20197;&#19979;&#26041;&#24335;&#21021;&#22987;&#21270;&#20854;&#26435;&#37325;&#65306;&#65288;i&#65289;&#22522;&#20110;&#33258;&#28982;&#22270;&#20687;&#30340;SSL&#39044;&#35757;&#32451;&#65288;DINOv2&#65289;&#12289;&#65288;ii&#65289;&#22522;&#20110;&#33258;&#28982;&#22270;&#20687;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#65288;ImageNet&#25968;&#25454;&#38598;&#65289;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#22522;&#20110;MIMIC-CXR&#25968;&#25454;&#24211;&#20013;&#30340;&#33016;&#37096;X&#23556;&#32447;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#20845;&#20010;&#20840;&#29699;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;800,000&#22810;&#24352;&#33016;&#37096;X&#23556;&#32447;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35786;&#26029;&#20102;20&#22810;&#31181;&#19981;&#21516;&#30340;&#24433;&#20687;&#25152;&#35265;&#12290;&#25105;&#20204;&#30340;SSL&#39044;&#35757;&#32451;&#22312;&#32463;&#36807;&#31579;&#36873;&#30340;&#22270;&#20687;&#19978;&#19981;&#20165;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#22522;&#20110;ImageNet&#30340;&#39044;&#35757;&#32451;&#65288;&#23545;&#25152;&#26377;&#25968;&#25454;&#38598;&#65292;P&lt;0.001&#65289;&#65292;&#32780;&#19988;&#22312;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#36824;&#36229;&#36807;&#20102;&#22522;&#20110;&#21307;&#23398;&#22270;&#20687;&#30340;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training datasets, like ImageNet, have become the gold standard in medical image analysis. However, the emergence of self-supervised learning (SSL), which leverages unlabeled data to learn robust features, presents an opportunity to bypass the intensive labeling process. In this study, we explored if SSL for pre-training on non-medical images can be applied to chest radiographs and how it compares to supervised pre-training on non-medical images and on medical images. We utilized a vision transformer and initialized its weights based on (i) SSL pre-training on natural images (DINOv2), (ii) SL pre-training on natural images (ImageNet dataset), and (iii) SL pre-training on chest radiographs from the MIMIC-CXR database. We tested our approach on over 800,000 chest radiographs from six large global datasets, diagnosing more than 20 different imaging findings. Our SSL pre-training on curated images not only outperformed ImageNet-based pre-training (P&lt;0.001 for all datasets) but, in cert
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31070;&#32463;&#20998;&#31867;&#20808;&#39564;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25511;&#21046;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31163;&#25955;&#20449;&#24687;&#29942;&#39048;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2308.07200</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#20998;&#31867;&#20808;&#39564;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#25511;&#21046;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neural Categorical Priors for Physics-Based Character Control. (arXiv:2308.07200v2 [cs.GR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31070;&#32463;&#20998;&#31867;&#20808;&#39564;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25511;&#21046;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31163;&#25955;&#20449;&#24687;&#29942;&#39048;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#23398;&#20064;&#21487;&#37325;&#29992;&#36816;&#21160;&#20808;&#39564;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#29983;&#25104;&#33258;&#28982;&#34892;&#20026;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25511;&#21046;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#65292;&#30456;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#36816;&#21160;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#36861;&#36394;&#21644;&#27169;&#20223;&#26469;&#33258;&#38750;&#32467;&#26500;&#21270;&#36816;&#21160;&#21098;&#36753;&#30340;&#36924;&#30495;&#21160;&#20316;&#65292;&#20351;&#29992;&#31163;&#25955;&#20449;&#24687;&#29942;&#39048;&#65292;&#22914;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#20013;&#25152;&#37319;&#29992;&#30340;&#37027;&#26679;&#12290;&#35813;&#32467;&#26500;&#23558;&#26469;&#33258;&#36816;&#21160;&#21098;&#36753;&#30340;&#26368;&#30456;&#20851;&#20449;&#24687;&#21387;&#32553;&#25104;&#19968;&#20010;&#32039;&#20945;&#32780;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#21363;&#19968;&#20010;&#31163;&#25955;&#30340;&#21521;&#37327;&#37327;&#21270;&#30721;&#31354;&#38388;&#12290;&#36890;&#36807;&#20174;&#32463;&#36807;&#35757;&#32451;&#30340;&#20998;&#31867;&#20808;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#31354;&#38388;&#20013;&#30340;&#30721;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#30340;&#34892;&#20026;&#65292;&#31867;&#20284;&#20110;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#20351;&#29992;VQ-VAE&#12290;&#34429;&#28982;&#36825;&#20010;&#20808;&#39564;&#20998;&#24067;&#21487;&#20197;&#36890;&#36807;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent advances in learning reusable motion priors have demonstrated their effectiveness in generating naturalistic behaviors. In this paper, we propose a new learning framework in this paradigm for controlling physics-based characters with significantly improved motion quality and diversity over existing state-of-the-art methods. The proposed method uses reinforcement learning (RL) to initially track and imitate life-like movements from unstructured motion clips using the discrete information bottleneck, as adopted in the Vector Quantized Variational AutoEncoder (VQ-VAE). This structure compresses the most relevant information from the motion clips into a compact yet informative latent space, i.e., a discrete space over vector quantized codes. By sampling codes in the space from a trained categorical prior distribution, high-quality life-like behaviors can be generated, similar to the usage of VQ-VAE in computer vision. Although this prior distribution can be trained with the supervis
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#35843;&#22270;&#20013;&#30340;&#36335;&#24452;&#65292;&#31361;&#30772;&#20102;1-Weisfeiler-Lehmann&#27979;&#35797;&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#24182;&#22312;&#26080;&#38656;&#23545;&#22270;&#30340;&#23376;&#32467;&#26500;&#36827;&#34892;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06838</link><description>&lt;p&gt;
&#20351;&#29992;&#36335;&#24452;&#36827;&#34892;&#19968;&#33324;&#21270;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476; (arXiv:2308.06838v2 [cs.LG] &#24050;&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Generalizing Topological Graph Neural Networks with Paths. (arXiv:2308.06838v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#35843;&#22270;&#20013;&#30340;&#36335;&#24452;&#65292;&#31361;&#30772;&#20102;1-Weisfeiler-Lehmann&#27979;&#35797;&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#24182;&#22312;&#26080;&#38656;&#23545;&#22270;&#30340;&#23376;&#32467;&#26500;&#36827;&#34892;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;1-Weisfeiler-Lehmann&#27979;&#35797;&#30340;&#29702;&#35770;&#38480;&#21046;&#12290;&#23613;&#31649;&#39640;&#38454;GNNs&#30340;&#26368;&#26032;&#36827;&#23637;&#21487;&#20197;&#20811;&#26381;&#36825;&#20010;&#38556;&#30861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38598;&#20013;&#22312;&#29305;&#23450;&#30340;&#22270;&#32452;&#20214;&#65292;&#22914;&#22242;&#25110;&#29615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36208;&#20102;&#19981;&#21516;&#30340;&#36335;&#32447;&#12290;&#25105;&#20204;&#24378;&#35843;&#36335;&#24452;&#65292;&#36825;&#26159;&#27599;&#20010;&#22270;&#20013;&#22266;&#26377;&#30340;&#12290;&#25105;&#20204;&#33021;&#22815;&#26500;&#24314;&#19968;&#20010;&#26356;&#36890;&#29992;&#30340;&#25299;&#25169;&#35270;&#35282;&#65292;&#24182;&#19982;&#20854;&#20182;&#25299;&#25169;&#39046;&#22495;&#30340;&#19968;&#20123;&#25104;&#29087;&#29702;&#35770;&#24418;&#25104;&#26725;&#26753;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;&#27809;&#26377;&#23545;&#22270;&#30340;&#23376;&#32467;&#26500;&#36827;&#34892;&#20219;&#20309;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#35813;&#39046;&#22495;&#26089;&#26399;&#30340;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Graph Neural Networks (GNNs) have made significant strides in diverse areas, they are hindered by a theoretical constraint known as the 1-Weisfeiler-Lehmann test. Even though latest advancements in higher-order GNNs can overcome this boundary, they typically center around certain graph components like cliques or cycles. However, our investigation goes a different route. We put emphasis on paths, which are inherent in every graph. We are able to construct a more general topological perspective and form a bridge to certain established theories about other topological domains. Interestingly, without any assumptions on graph sub-structures, our approach surpasses earlier techniques in this field, achieving state-of-the-art performance on several benchmarks.
&lt;/p&gt;</description></item><item><title>PETformer&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#21344;&#20301;&#31526;&#22686;&#24378;&#25216;&#26415;&#65292;&#38271;&#23376;&#24207;&#21015;&#21010;&#20998;&#21644;&#22810;&#36890;&#36947;&#20998;&#31163;&#19982;&#20132;&#20114;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#23558;Transformer&#24212;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26102;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;PETformer&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04791</link><description>&lt;p&gt;
PETformer: &#36890;&#36807;&#22686;&#24378;&#21344;&#20301;&#31526;&#30340;Transformer&#23454;&#29616;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PETformer: Long-term Time Series Forecasting via Placeholder-enhanced Transformer. (arXiv:2308.04791v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04791
&lt;/p&gt;
&lt;p&gt;
PETformer&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#21344;&#20301;&#31526;&#22686;&#24378;&#25216;&#26415;&#65292;&#38271;&#23376;&#24207;&#21015;&#21010;&#20998;&#21644;&#22810;&#36890;&#36947;&#20998;&#31163;&#19982;&#20132;&#20114;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#23558;Transformer&#24212;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26102;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;PETformer&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;LTSF&#65289;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#36825;&#24402;&#22240;&#20110;&#23427;&#20204;&#33021;&#22815;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#23558;Transformer&#24212;&#29992;&#20110;LTSF&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#20173;&#28982;&#23384;&#22312;&#20105;&#35758;&#65292;&#23588;&#20854;&#26159;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#31616;&#21333;&#30340;&#32447;&#24615;&#27169;&#22411;&#21487;&#20197;&#32988;&#36807;&#35768;&#22810;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#12290;&#36825;&#34920;&#26126;&#65292;&#22312;LTSF&#20013;&#24212;&#29992;Transformer&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;Transformer&#24212;&#29992;&#20110;LTSF&#26102;&#30340;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#26102;&#38388;&#36830;&#32493;&#24615;&#65292;&#20449;&#24687;&#23494;&#24230;&#21644;&#22810;&#36890;&#36947;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#21019;&#26032;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#21344;&#20301;&#31526;&#22686;&#24378;&#25216;&#26415;&#65288;PET&#65289;&#65292;&#38271;&#23376;&#24207;&#21015;&#21010;&#20998;&#65288;LSD&#65289;&#21644;&#22810;&#36890;&#36947;&#20998;&#31163;&#19982;&#20132;&#20114;&#65288;MSI&#65289;&#65292;&#20849;&#21516;&#26500;&#25104;&#20102;&#19968;&#20010;&#21517;&#20026;PETformer&#30340;&#26032;&#27169;&#22411;&#12290;&#36825;&#19977;&#20010;&#20851;&#38190;&#35774;&#35745;&#24341;&#20837;&#20102;&#36866;&#21512;LTSF&#20219;&#21153;&#30340;&#20808;&#39564;&#20559;&#24046;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;PETformer&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Transformer-based models have shown remarkable performance in long-term time series forecasting (LTSF) tasks due to their ability to model long-term dependencies. However, the validity of Transformers for LTSF tasks remains debatable, particularly since recent work has shown that simple linear models can outperform numerous Transformer-based approaches. This suggests that there are limitations to the application of Transformer in LTSF. Therefore, this paper investigates three key issues when applying Transformer to LTSF: temporal continuity, information density, and multi-channel relationships. Accordingly, we propose three innovative solutions, including Placeholder Enhancement Technique (PET), Long Sub-sequence Division (LSD), and Multi-channel Separation and Interaction (MSI), which together form a novel model called PETformer. These three key designs introduce prior biases suitable for LTSF tasks. Extensive experiments have demonstrated that PETformer achieves state-of-th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24378;&#21270;&#23398;&#20064;&#30340;&#31574;&#30053;&#20248;&#21270;&#19982;&#22806;&#37096;&#24615;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#21644;&#22996;&#25176;-&#20195;&#29702;&#26694;&#26550;&#65292;&#19978;&#23618;&#23398;&#20064;&#36866;&#24403;&#30340;&#22870;&#21169;&#21442;&#25968;&#21270;&#65292;&#19979;&#23618;&#23398;&#20064;&#20195;&#29702;&#20154;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2308.02585</link><description>&lt;p&gt;
&#23558;&#20195;&#29702;&#31574;&#30053;&#19982;&#22806;&#37096;&#24615;&#23545;&#40784;&#65306;&#36890;&#36807;&#21452;&#23618;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Aligning Agent Policy with Externalities: Reward Design via Bilevel RL. (arXiv:2308.02585v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24378;&#21270;&#23398;&#20064;&#30340;&#31574;&#30053;&#20248;&#21270;&#19982;&#22806;&#37096;&#24615;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#21644;&#22996;&#25176;-&#20195;&#29702;&#26694;&#26550;&#65292;&#19978;&#23618;&#23398;&#20064;&#36866;&#24403;&#30340;&#22870;&#21169;&#21442;&#25968;&#21270;&#65292;&#19979;&#23618;&#23398;&#20064;&#20195;&#29702;&#20154;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#24120;&#22312;&#31574;&#30053;&#20248;&#21270;&#36807;&#31243;&#30340;&#24320;&#22987;&#22788;&#20551;&#35774;&#19968;&#20010;&#22870;&#21169;&#20989;&#25968;&#12290;&#22312;&#36825;&#31181;&#22266;&#23450;&#22870;&#21169;&#33539;&#24335;&#19979;&#30340;&#23398;&#20064;&#20013;&#65292;&#21487;&#33021;&#20250;&#24573;&#30053;&#37325;&#35201;&#30340;&#31574;&#30053;&#20248;&#21270;&#32771;&#34385;&#22240;&#32032;&#65292;&#27604;&#22914;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#21644;&#23433;&#20840;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#33021;&#26080;&#27861;&#28085;&#30422;&#31038;&#20250;&#31119;&#21033;&#12289;&#21487;&#25345;&#32493;&#24615;&#25110;&#24066;&#22330;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#26356;&#24191;&#27867;&#24433;&#21709;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#21462;&#30340; emergent &#34892;&#20026;&#21644;&#21487;&#33021;&#19981;&#23545;&#40784;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#25968;&#23398;&#21270;&#22320;&#27010;&#25324;&#23558;&#24378;&#21270;&#23398;&#20064;&#30340;&#31574;&#30053;&#20248;&#21270;&#19982;&#36825;&#31181;&#22806;&#22312;&#24615;&#23545;&#40784;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#19982;&#22996;&#25176;-&#20195;&#29702;&#26694;&#26550;&#30456;&#32852;&#31995;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#22996;&#25176;&#20154;&#22312;&#19978;&#23618;&#30830;&#23450;&#31995;&#32479;&#30340;&#26356;&#24191;&#27867;&#30446;&#26631;&#21644;&#32422;&#26463;&#65292;&#20195;&#29702;&#20154;&#22312;&#19979;&#23618;&#35299;&#20915;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#19978;&#23618;&#20219;&#21153;&#26159;&#23398;&#20064;&#19968;&#20010;&#19982;&#26356;&#24191;&#27867;&#30446;&#26631;&#30456;&#23545;&#24212;&#30340;&#36866;&#24403;&#22870;&#21169;&#21442;&#25968;&#21270;&#65292;&#19979;&#23618;&#20219;&#21153;&#26159;&#23398;&#20064;&#20195;&#29702;&#20154;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning (RL), a reward function is often assumed at the outset of a policy optimization procedure. Learning in such a fixed reward paradigm in RL can neglect important policy optimization considerations, such as state space coverage and safety. Moreover, it can fail to encompass broader impacts in terms of social welfare, sustainability, or market stability, potentially leading to undesirable emergent behavior and potentially misaligned policy. To mathematically encapsulate the problem of aligning RL policy optimization with such externalities, we consider a bilevel optimization problem and connect it to a principal-agent framework, where the principal specifies the broader goals and constraints of the system at the upper level and the agent solves a Markov Decision Process (MDP) at the lower level. The upper-level deals with learning a suitable reward parametrization corresponding to the broader goals and the lower-level deals with learning the policy for the agent. 
&lt;/p&gt;</description></item><item><title>PDS-Net is a novel framework for QoS prediction that effectively reduces errors resulting from noise data by utilizing a probabilistic space and a condition-based multitasking loss function.</title><link>http://arxiv.org/abs/2308.02580</link><description>&lt;p&gt;
Probabilistic Deep Supervision Network: &#19968;&#31181;&#25239;&#22122;&#22768;&#30340;QoS&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Deep Supervision Network: A Noise-Resilient Approach for QoS Prediction. (arXiv:2308.02580v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02580
&lt;/p&gt;
&lt;p&gt;
PDS-Net is a novel framework for QoS prediction that effectively reduces errors resulting from noise data by utilizing a probabilistic space and a condition-based multitasking loss function.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;QoS&#65288;&#26381;&#21153;&#36136;&#37327;&#65289;&#30340;&#39044;&#27979;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#20934;&#30830;&#39044;&#27979;&#26410;&#30693;&#30340;QoS&#20540;&#21487;&#20197;&#25552;&#39640;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;QoS&#39044;&#27979;&#25216;&#26415;&#22312;&#23384;&#22312;&#22122;&#22768;&#25968;&#25454;&#65288;&#22914;&#34394;&#20551;&#20301;&#32622;&#20449;&#24687;&#25110;&#34394;&#25311;&#32593;&#20851;&#65289;&#26102;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;QoS&#39044;&#27979;&#26694;&#26550;&#8212;&#8212;&#27010;&#29575;&#28145;&#24230;&#30417;&#30563;&#32593;&#32476;&#65288;PDS-Net&#65289;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;PDS-Net&#21033;&#29992;&#22522;&#20110;&#39640;&#26031;&#30340;&#27010;&#29575;&#31354;&#38388;&#30417;&#30563;&#20013;&#38388;&#23618;&#65292;&#24182;&#23398;&#20064;&#24050;&#30693;&#29305;&#24449;&#21644;&#30495;&#23454;&#26631;&#31614;&#30340;&#27010;&#29575;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;PDS-Net&#37319;&#29992;&#22522;&#20110;&#26465;&#20214;&#30340;&#22810;&#20219;&#21153;&#25439;&#22833;&#20989;&#25968;&#26469;&#35782;&#21035;&#20855;&#26377;&#22122;&#22768;&#25968;&#25454;&#30340;&#23545;&#35937;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36825;&#20123;&#23545;&#35937;&#30340;&#27010;&#29575;&#31354;&#38388;&#19982;&#30495;&#23454;&#26631;&#31614;&#27010;&#29575;&#31354;&#38388;&#20043;&#38388;&#30340;Kullback-Leibler&#36317;&#31163;&#65292;&#30452;&#25509;&#23545;&#20174;&#27010;&#29575;&#31354;&#38388;&#20013;&#37319;&#26679;&#30340;&#28145;&#24230;&#29305;&#24449;&#36827;&#34892;&#30417;&#30563;&#12290;&#22240;&#27492;&#65292;PDS-Net&#26377;&#25928;&#20943;&#23569;&#20102;&#22240;&#20256;&#25773;&#24341;&#36215;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quality of Service (QoS) prediction is an essential task in recommendation systems, where accurately predicting unknown QoS values can improve user satisfaction. However, existing QoS prediction techniques may perform poorly in the presence of noise data, such as fake location information or virtual gateways. In this paper, we propose the Probabilistic Deep Supervision Network (PDS-Net), a novel framework for QoS prediction that addresses this issue. PDS-Net utilizes a Gaussian-based probabilistic space to supervise intermediate layers and learns probability spaces for both known features and true labels. Moreover, PDS-Net employs a condition-based multitasking loss function to identify objects with noise data and applies supervision directly to deep features sampled from the probability space by optimizing the Kullback-Leibler distance between the probability space of these objects and the real-label probability space. Thus, PDS-Net effectively reduces errors resulting from the propag
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34394;&#25311;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#25216;&#26415;&#65288;DuRM&#65289;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#32463;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DuRM&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#26799;&#24230;&#30340;&#26041;&#24046;&#26469;&#20419;&#36827;&#27169;&#22411;&#30340;&#27867;&#21270;&#25928;&#26524;&#65292;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DuRM&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02287</link><description>&lt;p&gt;
&#36890;&#36807;&#34394;&#25311;&#39118;&#38505;&#26368;&#23567;&#21270;&#23454;&#29616;&#20196;&#20154;&#27822;&#20007;&#30340;&#27169;&#22411;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Frustratingly Easy Model Generalization by Dummy Risk Minimization. (arXiv:2308.02287v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02287
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34394;&#25311;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#25216;&#26415;&#65288;DuRM&#65289;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#32463;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DuRM&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#26799;&#24230;&#30340;&#26041;&#24046;&#26469;&#20419;&#36827;&#27169;&#22411;&#30340;&#27867;&#21270;&#25928;&#26524;&#65292;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DuRM&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#65292;&#23427;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#34394;&#25311;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;DuRM&#65289;&#65292;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#21644;&#36890;&#29992;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;ERM&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;DuRM&#38750;&#24120;&#31616;&#21333;&#23454;&#29616;&#65306;&#21482;&#38656;&#25193;&#22823;&#36755;&#20986;logits&#30340;&#32500;&#24230;&#65292;&#28982;&#21518;&#20351;&#29992;&#26631;&#20934;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#32463;&#39564;&#39564;&#35777;DuRM&#30340;&#26377;&#25928;&#24615;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DuRM&#23548;&#33268;&#26356;&#22823;&#30340;&#26799;&#24230;&#26041;&#24046;&#65292;&#36890;&#36807;&#35266;&#23519;&#26356;&#22909;&#30340;&#24179;&#22374;&#23616;&#37096;&#26368;&#23567;&#20540;&#20419;&#36827;&#27169;&#22411;&#27867;&#21270;&#12290;&#20174;&#32463;&#39564;&#19978;&#35762;&#65292;&#25105;&#20204;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#27169;&#24577;&#21644;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;DuRM&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#20256;&#32479;&#20998;&#31867;&#65292;&#35821;&#20041;&#20998;&#21106;&#65292;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#65292;&#23545;&#25239;&#35757;&#32451;&#21644;&#38271;&#23614;&#35782;&#21035;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;DuRM&#33021;&#22815;&#25345;&#32493;&#25913;&#36827;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical risk minimization (ERM) is a fundamental machine learning paradigm. However, its generalization ability is limited in various tasks. In this paper, we devise Dummy Risk Minimization (DuRM), a frustratingly easy and general technique to improve the generalization of ERM. DuRM is extremely simple to implement: just enlarging the dimension of the output logits and then optimizing using standard gradient descent. Moreover, we validate the efficacy of DuRM on both theoretical and empirical analysis. Theoretically, we show that DuRM derives greater variance of the gradient, which facilitates model generalization by observing better flat local minima. Empirically, we conduct evaluations of DuRM across different datasets, modalities, and network architectures on diverse tasks, including conventional classification, semantic segmentation, out-of-distribution generalization, adverserial training, and long-tailed recognition. Results demonstrate that DuRM could consistently improve the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32469;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#27010;&#24565;&#21024;&#38500;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#23436;&#20840;&#21024;&#38500;&#30446;&#26631;&#27010;&#24565;&#65292;&#24182;&#23545;&#20854;&#20351;&#29992;&#30340;&#33030;&#24369;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;</title><link>http://arxiv.org/abs/2308.01508</link><description>&lt;p&gt;
&#32469;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#27010;&#24565;&#21024;&#38500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Circumventing Concept Erasure Methods For Text-to-Image Generative Models. (arXiv:2308.01508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32469;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#27010;&#24565;&#21024;&#38500;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#23436;&#20840;&#21024;&#38500;&#30446;&#26631;&#27010;&#24565;&#65292;&#24182;&#23545;&#20854;&#20351;&#29992;&#30340;&#33030;&#24369;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#20026;&#26497;&#20854;&#24191;&#27867;&#30340;&#27010;&#24565;&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#20351;&#29992;&#22312;&#26222;&#36890;&#20844;&#20247;&#20013;&#24191;&#27867;&#26222;&#21450;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#35768;&#22810;&#32570;&#28857;&#65292;&#21253;&#25324;&#21487;&#33021;&#29983;&#25104;&#20855;&#26377;&#24615;&#21035;&#26292;&#38706;&#20869;&#23481;&#30340;&#22270;&#20687;&#65292;&#26410;&#32463;&#35768;&#21487;&#22320;&#27169;&#20223;&#33402;&#26415;&#39118;&#26684;&#65292;&#29978;&#33267;&#26159;&#28145;&#24230;&#20266;&#36896;&#21517;&#20154;&#30340;&#26679;&#35980;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#8220;&#25830;&#38500;&#8221;&#25935;&#24863;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20116;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#27010;&#24565;&#21024;&#38500;&#26041;&#27861;&#65292;&#24182;&#26174;&#31034;&#36825;&#20123;&#26041;&#27861;&#37117;&#26080;&#27861;&#23436;&#20840;&#21024;&#38500;&#30446;&#26631;&#27010;&#24565;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#29305;&#27530;&#30340;&#23398;&#20064;&#35789;&#23884;&#20837;&#30340;&#23384;&#22312;&#65292;&#21487;&#20197;&#22312;&#26080;&#38656;&#23545;&#20854;&#26435;&#37325;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#32463;&#36807;&#22788;&#29702;&#30340;&#27169;&#22411;&#20013;&#26816;&#32034;&#8220;&#21024;&#38500;&#8221;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#20107;&#21518;&#27010;&#24565;&#21024;&#38500;&#26041;&#27861;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#23545;&#23427;&#20204;&#22312;&#31639;&#27861;&#24037;&#20855;&#21253;&#20013;&#30340;&#20351;&#29992;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative models can produce photo-realistic images for an extremely broad range of concepts, and their usage has proliferated widely among the general public. On the flip side, these models have numerous drawbacks, including their potential to generate images featuring sexually explicit content, mirror artistic styles without permission, or even hallucinate (or deepfake) the likenesses of celebrities. Consequently, various methods have been proposed in order to "erase" sensitive concepts from text-to-image models. In this work, we examine five recently proposed concept erasure methods, and show that targeted concepts are not fully excised from any of these methods. Specifically, we leverage the existence of special learned word embeddings that can retrieve "erased" concepts from the sanitized models with no alterations to their weights. Our results highlight the brittleness of post hoc concept erasure methods, and call into question their use in the algorithmic toolkit 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32473;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#36827;&#32780;&#25552;&#39640;&#38646;&#26679;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01313</link><description>&lt;p&gt;
&#26356;&#22810;&#19978;&#19979;&#25991;&#65292;&#26356;&#23569;&#24178;&#25200;&#65306;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#36827;&#34892;&#35270;&#35273;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes. (arXiv:2308.01313v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32473;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#36827;&#32780;&#25552;&#39640;&#38646;&#26679;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CLIP&#20316;&#20026;&#19968;&#31181;&#22522;&#30784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#29702;&#35299;&#21508;&#31181;&#35270;&#35273;&#27010;&#24565;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#33021;&#21147;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#20805;&#20998;&#21033;&#29992;CLIP&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#20154;&#31867;&#33324;&#29702;&#35299;&#33021;&#21147;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#20154;&#31867;&#30340;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#20013;&#24471;&#21040;&#21551;&#21457;&#65306;&#29616;&#20195;&#31070;&#32463;&#31185;&#23398;&#35266;&#28857;&#35748;&#20026;&#65292;&#22312;&#23545;&#29289;&#20307;&#36827;&#34892;&#20998;&#31867;&#26102;&#65292;&#20154;&#31867;&#39318;&#20808;&#25512;&#26029;&#20854;&#19982;&#31867;&#21035;&#26080;&#20851;&#30340;&#23646;&#24615;&#65288;&#22914;&#32972;&#26223;&#21644;&#26041;&#21521;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#23558;&#21069;&#26223;&#23545;&#35937;&#19982;&#32972;&#26223;&#21306;&#20998;&#24320;&#26469;&#65292;&#28982;&#21518;&#20197;&#27492;&#20449;&#24687;&#20026;&#22522;&#30784;&#36827;&#34892;&#20915;&#31574;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20026;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#21487;&#20197;&#25913;&#21892;&#38646;&#26679;&#26412;&#20998;&#31867;&#24182;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;CLIP&#26412;&#36523;&#21487;&#20197;&#21512;&#29702;&#22320;&#20174;&#22270;&#20687;&#20013;&#25512;&#26029;&#20986;&#36825;&#20123;&#23646;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#35757;&#32451;&#12289;&#20004;&#27493;&#39588;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
CLIP, as a foundational vision language model, is widely used in zero-shot image classification due to its ability to understand various visual concepts and natural language descriptions. However, how to fully leverage CLIP's unprecedented human-like understanding capabilities to achieve better zero-shot classification is still an open question. This paper draws inspiration from the human visual perception process: a modern neuroscience view suggests that in classifying an object, humans first infer its class-independent attributes (e.g., background and orientation) which help separate the foreground object from the background, and then make decisions based on this information. Inspired by this, we observe that providing CLIP with contextual attributes improves zero-shot classification and mitigates reliance on spurious features. We also observe that CLIP itself can reasonably infer the attributes from an image. With these observations, we propose a training-free, two-step zero-shot cl
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMMs&#65289;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#38663;&#33633;&#25429;&#25417;&#31639;&#27861;&#12290;&#36825;&#31181;&#31639;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#21644;&#27969;&#21160;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2308.00086</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#38663;&#33633;&#25429;&#25417;&#39640;&#38454;CFD&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Unsupervised machine learning shock capturing for High-Order CFD solvers. (arXiv:2308.00086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00086
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMMs&#65289;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#38663;&#33633;&#25429;&#25417;&#31639;&#27861;&#12290;&#36825;&#31181;&#31639;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#21644;&#27969;&#21160;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMMs&#65289;&#30340;&#26032;&#22411;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#38663;&#33633;&#25429;&#25417;&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;GMM&#20256;&#24863;&#22120;&#22312;&#26816;&#27979;&#38663;&#33633;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#19981;&#38656;&#35201;&#21442;&#25968;&#35843;&#20248;&#30340;&#24773;&#20917;&#19979;&#22312;&#22810;&#26679;&#30340;&#27979;&#35797;&#26696;&#20363;&#19978;&#37117;&#34920;&#29616;&#20986;&#20102;&#36739;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;GMM&#30340;&#20256;&#24863;&#22120;&#19982;&#26368;&#20808;&#36827;&#30340;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25152;&#26377;&#26041;&#27861;&#37117;&#38598;&#25104;&#21040;&#39640;&#38454;&#21487;&#21387;&#24615;&#19981;&#36830;&#32493;Galerkin&#27714;&#35299;&#22120;&#20013;&#65292;&#20154;&#24037;&#40655;&#24615;&#21487;&#20197;&#35843;&#33410;&#20197;&#25429;&#25417;&#38663;&#33633;&#12290;&#36229;&#38899;&#36895;&#27979;&#35797;&#26696;&#20363;&#65292;&#21253;&#25324;&#39640;&#38647;&#35834;&#25968;&#65292;&#23637;&#31034;&#20102;&#20256;&#24863;&#22120;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#20854;&#25928;&#26524;&#19982;&#31934;&#35843;&#30340;&#26368;&#20808;&#36827;&#20256;&#24863;&#22120;&#30456;&#24403;&#12290;%&#33410;&#28857;DG&#26041;&#27861;&#20801;&#35768;&#22312;&#20122;&#21333;&#20803;&#36890;&#37327;&#26377;&#24046;&#24322;&#30340;&#20844;&#24335;&#20013;&#36827;&#34892;&#28508;&#22312;&#24212;&#29992;&#65292;&#36229;&#38899;&#36895;&#29305;&#24449;&#26816;&#27979;&#21644;&#32593;&#26684;&#32454;&#21270;&#12290;&#36825;&#31181;&#22522;&#20110;GMM&#30340;&#20256;&#24863;&#22120;&#36866;&#29992;&#20110;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#21644;&#21508;&#31181;&#27969;&#21160;&#37197;&#32622;&#65292;&#20854;&#33258;&#36866;&#24212;&#24615;&#21644;&#26080;&#38656;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#21151;&#33021;&#20351;&#20854;&#20855;&#22791;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel unsupervised machine learning shock capturing algorithm based on Gaussian Mixture Models (GMMs). The proposed GMM sensor demonstrates remarkable accuracy in detecting shocks and is robust across diverse test cases without the need for parameter tuning. We compare the GMM-based sensor with state-of-the-art alternatives. All methods are integrated into a high-order compressible discontinuous Galerkin solver where artificial viscosity can be modulated to capture shocks. Supersonic test cases, including high Reynolds numbers, showcase the sensor's performance, demonstrating the same effectiveness as fine-tuned state-of-the-art sensors. %The nodal DG aproach allows for potential applications in sub-cell flux-differencing formulations, supersonic feature detection, and mesh refinement. The adaptive nature and ability to function without extensive training datasets make this GMM-based sensor suitable for complex geometries and varied flow configurations. Our study reveals t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#20998;&#21106;&#19982;&#32465;&#23450;"&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#21253;&#25324;&#20851;&#27880;&#20002;&#22833;&#21644;&#32465;&#23450;&#20002;&#22833;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#25552;&#31034;&#21644;&#19981;&#36866;&#24403;&#23646;&#24615;&#32465;&#23450;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10864</link><description>&lt;p&gt;
&#23558;&#27880;&#24847;&#21147;&#20998;&#21106;&#19982;&#32465;&#23450;&#29992;&#20110;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
Divide &amp; Bind Your Attention for Improved Generative Semantic Nursing. (arXiv:2307.10864v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#20998;&#21106;&#19982;&#32465;&#23450;"&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#21253;&#25324;&#20851;&#27880;&#20002;&#22833;&#21644;&#32465;&#23450;&#20002;&#22833;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#25552;&#31034;&#21644;&#19981;&#36866;&#24403;&#23646;&#24615;&#32465;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#65292;&#23637;&#31034;&#20102;&#39640;&#24230;&#36924;&#30495;&#30340;&#21387;&#20498;&#24615;&#32467;&#26524;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#23436;&#20840;&#20381;&#29031;&#36755;&#20837;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#8212;&#8212;&#20851;&#27880;&#19982;&#28608;&#21457;&#65292;&#24341;&#20837;&#20102;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#65288;GSN&#65289;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#22312;&#25512;&#26029;&#26102;&#20248;&#21270;&#36328;&#27880;&#24847;&#21147;&#20197;&#26356;&#22909;&#22320;&#34701;&#20837;&#35821;&#20041;&#12290;&#23427;&#22312;&#29983;&#25104;&#31616;&#21333;&#25552;&#31034;&#65292;&#22914;&#8220;&#19968;&#21482;&#29483;&#21644;&#19968;&#21482;&#29399;&#8221;&#65292;&#26041;&#38754;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#25552;&#31034;&#20197;&#21450;&#35299;&#20915;&#19981;&#36866;&#24403;&#30340;&#23646;&#24615;&#32465;&#23450;&#38382;&#39064;&#26041;&#38754;&#30340;&#21151;&#25928;&#26377;&#25152;&#19979;&#38477;&#12290;&#20026;&#20102;&#24212;&#23545;&#22797;&#26434;&#25552;&#31034;&#25110;&#28041;&#21450;&#22810;&#20010;&#23454;&#20307;&#30340;&#22330;&#26223;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#25913;&#36827;&#30340;&#23646;&#24615;&#32465;&#23450;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#21106;&#19982;&#32465;&#23450;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;GSN&#25439;&#22833;&#30446;&#26631;&#65306;&#19968;&#31181;&#26032;&#30340;&#20851;&#27880;&#20002;&#22833;&#21644;&#19968;&#31181;&#32465;&#23450;&#20002;&#22833;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#23558;&#35821;&#20041;&#32435;&#20837;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#29305;&#28857;&#19978;&#33073;&#39062;&#32780;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging large-scale text-to-image generative models, e.g., Stable Diffusion (SD), have exhibited overwhelming results with high fidelity. Despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt. Prior work, Attend &amp; Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate the semantics. It demonstrates promising results in generating simple prompts, e.g., ``a cat and a dog''. However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding. To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide &amp; Bind. We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss. Our approach stands out in its ability to fa
&lt;/p&gt;</description></item><item><title>&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#65292;&#20256;&#32479;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#20302;&#31209;&#29305;&#24615;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;U&#22411;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#36339;&#36291;&#23618;&#36830;&#25509;&#21644;&#34917;&#19969;&#21512;&#24182;&#19982;&#20998;&#21106;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#20445;&#30041;&#39640;&#39057;&#19978;&#19979;&#25991;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.09019</link><description>&lt;p&gt;
U&#22411;&#21464;&#21387;&#22120;&#65306;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#20445;&#25345;&#39640;&#39057;&#19978;&#19979;&#25991;
&lt;/p&gt;
&lt;p&gt;
U-shaped Transformer: Retain High Frequency Context in Time Series Analysis. (arXiv:2307.09019v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09019
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#65292;&#20256;&#32479;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#20302;&#31209;&#29305;&#24615;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;U&#22411;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#36339;&#36291;&#23618;&#36830;&#25509;&#21644;&#34917;&#19969;&#21512;&#24182;&#19982;&#20998;&#21106;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#20445;&#30041;&#39640;&#39057;&#19978;&#19979;&#25991;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#21508;&#20010;&#24037;&#19994;&#39046;&#22495;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#65292;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#26368;&#31616;&#21333;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#19978;&#20063;&#32988;&#36807;&#39640;&#32423;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#26102;&#38388;&#24207;&#21015;&#24207;&#21015;&#23384;&#22312;&#20302;&#31209;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#21464;&#21387;&#22120;&#30340;&#20302;&#36890;&#29305;&#24615;&#65292;&#24182;&#23581;&#35797;&#32467;&#21512;MLP&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#23558;&#21463;Unet&#21551;&#21457;&#30340;&#36339;&#36291;&#23618;&#36830;&#25509;&#24212;&#29992;&#21040;&#20256;&#32479;&#30340;&#21464;&#21387;&#22120;&#32972;&#39592;&#32467;&#26500;&#20013;&#65292;&#20174;&#32780;&#23558;&#36755;&#20837;&#21040;&#36755;&#20986;&#30340;&#39640;&#39057;&#19978;&#19979;&#25991;&#20445;&#30041;&#19979;&#26469;&#65292;&#21363;U&#22411;&#21464;&#21387;&#22120;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#34917;&#19969;&#21512;&#24182;&#21644;&#20998;&#21106;&#25805;&#20316;&#26469;&#25552;&#21462;&#19981;&#21516;&#23610;&#24230;&#30340;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#36739;&#22823;&#30340;&#25968;&#25454;&#38598;&#20805;&#20998;&#21033;&#29992;&#21464;&#21387;&#22120;&#30340;&#32972;&#39592;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series prediction plays a crucial role in various industrial fields. In recent years, neural networks with a transformer backbone have achieved remarkable success in many domains, including computer vision and NLP. In time series analysis domain, some studies have suggested that even the simplest MLP networks outperform advanced transformer-based networks on time series forecast tasks. However, we believe these findings indicate there to be low-rank properties in time series sequences. In this paper, we consider the low-pass characteristics of transformers and try to incorporate the advantages of MLP. We adopt skip-layer connections inspired by Unet into traditional transformer backbone, thus preserving high-frequency context from input to output, namely U-shaped Transformer. We introduce patch merge and split operation to extract features with different scales and use larger datasets to fully make use of the transformer backbone. Our experiments demonstrate that the model perform
&lt;/p&gt;</description></item><item><title>Safe DreamerV3&#26159;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#35745;&#21010;&#30340;&#26041;&#27861;&#21040;&#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#32500;&#24230;&#21644;&#20165;&#37319;&#29992;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#20960;&#20046;&#38646;&#25104;&#26412;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.07176</link><description>&lt;p&gt;
Safe DreamerV3&#65306;&#24102;&#26377;&#19990;&#30028;&#27169;&#22411;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe DreamerV3: Safe Reinforcement Learning with World Models. (arXiv:2307.07176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07176
&lt;/p&gt;
&lt;p&gt;
Safe DreamerV3&#26159;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#35745;&#21010;&#30340;&#26041;&#27861;&#21040;&#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#32500;&#24230;&#21644;&#20165;&#37319;&#29992;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#20960;&#20046;&#38646;&#25104;&#26412;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#36824;&#27809;&#26377;&#23454;&#29616;, &#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#20854;&#26410;&#33021;&#28385;&#36275;&#36825;&#20123;&#31995;&#32479;&#30340;&#22522;&#26412;&#23433;&#20840;&#38656;&#27714;&#12290;&#29616;&#26377;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#25104;&#26412;&#20989;&#25968;&#26469;&#22686;&#24378;&#23433;&#20840;&#24615;&#65292;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#65292;&#21253;&#25324;&#20165;&#37319;&#29992;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#65292;&#21363;&#20351;&#36827;&#34892;&#20840;&#38754;&#30340;&#25968;&#25454;&#37319;&#26679;&#21644;&#35757;&#32451;&#65292;&#20063;&#26080;&#27861;&#23454;&#29616;&#38646;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Safe DreamerV3&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#35745;&#21010;&#30340;&#26041;&#27861;&#38598;&#25104;&#21040;&#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#26032;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#22312;SafeRL&#20013;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#36827;&#27493;&#65292;&#26159;&#31532;&#19968;&#20010;&#22312;Safety-Gymnasium&#22522;&#20934;&#20013;&#23454;&#29616;&#36817;&#20046;&#38646;&#25104;&#26412;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#32593;&#31449;&#21487;&#20197;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;&#65306;https://sites.google.com/view/safedreamerv3&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread application of Reinforcement Learning (RL) in real-world situations is yet to come to fruition, largely as a result of its failure to satisfy the essential safety demands of such systems. Existing safe reinforcement learning (SafeRL) methods, employing cost functions to enhance safety, fail to achieve zero-cost in complex scenarios, including vision-only tasks, even with comprehensive data sampling and training. To address this, we introduce Safe DreamerV3, a novel algorithm that integrates both Lagrangian-based and planning-based methods within a world model. Our methodology represents a significant advancement in SafeRL as the first algorithm to achieve nearly zero-cost in both low-dimensional and vision-only tasks within the Safety-Gymnasium benchmark. Our project website can be found in: https://sites.google.com/view/safedreamerv3.
&lt;/p&gt;</description></item><item><title>MoleBLEND&#26159;&#19968;&#31181;&#36890;&#36807;&#23545;2D&#21644;3D&#20998;&#23376;&#32467;&#26500;&#36827;&#34892;&#32479;&#19968;&#32534;&#30721;&#21644;&#34701;&#21512;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#26368;&#26032;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.06235</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#24577;&#34701;&#21512;&#23454;&#29616;&#32479;&#19968;&#30340;&#20998;&#23376;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Unified Molecular Modeling via Modality Blending. (arXiv:2307.06235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06235
&lt;/p&gt;
&lt;p&gt;
MoleBLEND&#26159;&#19968;&#31181;&#36890;&#36807;&#23545;2D&#21644;3D&#20998;&#23376;&#32467;&#26500;&#36827;&#34892;&#32479;&#19968;&#32534;&#30721;&#21644;&#34701;&#21512;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#26368;&#26032;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#23545;&#20110;&#22522;&#20110;&#20998;&#23376;&#30340;&#20219;&#21153;&#22914;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#33647;&#29289;&#21457;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#32771;&#34385;&#21033;&#29992;2D&#21644;3D&#20449;&#24687;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#37319;&#29992;&#23558;&#27599;&#31181;&#27169;&#24577;&#20998;&#24320;&#22788;&#29702;&#30340;&#30452;&#25509;&#23545;&#40784;&#31574;&#30053;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;"&#28151;&#21512;-&#39044;&#27979;"&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65288;MoleBLEND&#65289;&#65292;&#23558;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#21407;&#23376;&#38388;&#20851;&#31995;&#34701;&#21512;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#20851;&#31995;&#30697;&#38453;&#36827;&#34892;&#32534;&#30721;&#65292;&#28982;&#21518;&#24674;&#22797;2D&#21644;3D&#32467;&#26500;&#30340;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#12290;&#36890;&#36807;&#23558;&#21407;&#23376;&#20851;&#31995;&#35270;&#20026;&#38170;&#28857;&#65292;&#30475;&#20284;&#19981;&#30456;&#20284;&#30340;2D&#21644;3D&#27969;&#24418;&#22312;&#32454;&#31890;&#24230;&#30340;&#20851;&#31995;&#32423;&#21035;&#19978;&#26377;&#26426;&#22320;&#23545;&#40784;&#21644;&#25972;&#21512;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;MoleBLEND&#22312;&#20027;&#35201;&#30340;2D/3D&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20174;&#30456;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#23519;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#32479;&#19968;&#20102;&#23545;&#27604;&#12289;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Self-supervised molecular representation learning is critical for molecule-based tasks such as AI-assisted drug discovery. Recent studies consider leveraging both 2D and 3D information for representation learning, with straightforward alignment strategies that treat each modality separately. In this work, we introduce a novel "blend-then-predict" self-supervised learning method (MoleBLEND), which blends atom relations from different modalities into one unified relation matrix for encoding, then recovers modality-specific information for both 2D and 3D structures. By treating atom relationships as anchors, seemingly dissimilar 2D and 3D manifolds are aligned and integrated at fine-grained relation-level organically. Extensive experiments show that MoleBLEND achieves state-of-the-art performance across major 2D/3D benchmarks. We further provide theoretical insights from the perspective of mutual-information maximization, demonstrating that our method unifies contrastive, generative (inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22411;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#29366;&#24577;&#36873;&#25321;&#26368;&#20248;&#30340;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23545;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#36827;&#34892;&#36817;&#20284;&#21644;&#20272;&#35745;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#37325;&#26032;&#24179;&#34913;&#25104;&#26412;&#19979;&#20999;&#25442;&#25237;&#36164;&#32452;&#21512;&#27169;&#22411;&#26102;&#65292;&#20351;&#29992;&#23439;&#35266;&#32463;&#27982;&#20449;&#24687;&#30340;&#24615;&#33021;&#20248;&#20110;&#20107;&#21518;&#36873;&#25321;&#26368;&#20339;&#25237;&#36164;&#32452;&#21512;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.04754</link><description>&lt;p&gt;
&#21160;&#20316;&#29366;&#24577;&#30456;&#20851;&#30340;&#21160;&#24577;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Action-State Dependent Dynamic Model Selection. (arXiv:2307.04754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22411;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#29366;&#24577;&#36873;&#25321;&#26368;&#20248;&#30340;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23545;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#36827;&#34892;&#36817;&#20284;&#21644;&#20272;&#35745;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#37325;&#26032;&#24179;&#34913;&#25104;&#26412;&#19979;&#20999;&#25442;&#25237;&#36164;&#32452;&#21512;&#27169;&#22411;&#26102;&#65292;&#20351;&#29992;&#23439;&#35266;&#32463;&#27982;&#20449;&#24687;&#30340;&#24615;&#33021;&#20248;&#20110;&#20107;&#21518;&#36873;&#25321;&#26368;&#20339;&#25237;&#36164;&#32452;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19990;&#30028;&#30340;&#26576;&#20123;&#29366;&#24577;&#19979;&#65292;&#22810;&#20010;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#21487;&#33021;&#21482;&#22312;&#20854;&#20013;&#26576;&#20123;&#29366;&#24577;&#19979;&#34920;&#29616;&#26368;&#20339;&#12290;&#32780;&#22312;&#27169;&#22411;&#20043;&#38388;&#30340;&#20999;&#25442;&#20063;&#21487;&#33021;&#20195;&#20215;&#39640;&#26114;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23547;&#25214;&#19968;&#31181;&#33021;&#22815;&#21160;&#24577;&#36873;&#25321;&#27169;&#22411;&#30340;&#36807;&#31243;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#22797;&#26434;&#30340;&#20272;&#35745;&#38382;&#39064;&#21644;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#12290;&#26412;&#25991;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#20174;&#25968;&#25454;&#20013;&#36817;&#20284;&#21644;&#20272;&#35745;&#36825;&#20010;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#19968;&#33268;&#22320;&#20272;&#35745;&#20986;&#26681;&#25454;&#19968;&#32452;&#21327;&#21464;&#37327;&#36873;&#25321;&#19981;&#21516;&#27169;&#22411;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#20855;&#20307;&#24212;&#29992;&#26041;&#38754;&#65292;&#20363;&#22914;&#22312;&#37325;&#26032;&#24179;&#34913;&#25104;&#26412;&#19979;&#20999;&#25442;&#19981;&#21516;&#25237;&#36164;&#32452;&#21512;&#27169;&#22411;&#65292;&#20351;&#29992;&#23439;&#35266;&#32463;&#27982;&#20449;&#24687;&#36827;&#34892;&#20915;&#31574;&#12290;&#36890;&#36807;&#19968;&#32452;&#23439;&#35266;&#32463;&#27982;&#21464;&#37327;&#21644;&#20215;&#26684;&#25968;&#25454;&#65292;&#32463;&#39564;&#24212;&#29992;&#20110;&#19978;&#36848;&#25237;&#36164;&#32452;&#21512;&#38382;&#39064;&#34920;&#29616;&#20986;&#27604;&#20107;&#21518;&#36873;&#25321;&#26368;&#20339;&#25237;&#36164;&#32452;&#21512;&#27169;&#22411;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A model among many may only be best under certain states of the world. Switching from a model to another can also be costly. Finding a procedure to dynamically choose a model in these circumstances requires to solve a complex estimation procedure and a dynamic programming problem. A Reinforcement learning algorithm is used to approximate and estimate from the data the optimal solution to this dynamic programming problem. The algorithm is shown to consistently estimate the optimal policy that may choose different models based on a set of covariates. A typical example is the one of switching between different portfolio models under rebalancing costs, using macroeconomic information. Using a set of macroeconomic variables and price data, an empirical application to the aforementioned portfolio problem shows superior performance to choosing the best portfolio model with hindsight.
&lt;/p&gt;</description></item><item><title>Transformer&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#26159;&#22686;&#24378;&#35760;&#24518;&#33021;&#21147;&#32780;&#19981;&#26159;&#25913;&#36827;&#20449;&#29992;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2307.03864</link><description>&lt;p&gt;
&#20309;&#26102;&#20351;&#29992;Transformer&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21457;&#20809;&#65311;&#20174;&#35760;&#24518;&#21644;&#20449;&#29992;&#20998;&#37197;&#20013;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment. (arXiv:2307.03864v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03864
&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#26159;&#22686;&#24378;&#35760;&#24518;&#33021;&#21147;&#32780;&#19981;&#26159;&#25913;&#36827;&#20449;&#29992;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38754;&#20020;&#20004;&#20010;&#19981;&#21516;&#30340;&#25361;&#25112;&#65306;&#23398;&#20064;&#26377;&#25928;&#30340;&#36807;&#21435;&#21644;&#24403;&#21069;&#35266;&#27979;&#30340;&#34920;&#31034;&#65292;&#24182;&#30830;&#23450;&#34892;&#21160;&#22914;&#20309;&#24433;&#21709;&#26410;&#26469;&#30340;&#25910;&#30410;&#12290;&#36825;&#20004;&#20010;&#25361;&#25112;&#37117;&#28041;&#21450;&#21040;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;Transformer&#26550;&#26500;&#22312;&#35299;&#20915;&#28041;&#21450;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#26041;&#38754;&#38750;&#24120;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;Transformer&#22522;&#20110;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#24378;&#21170;&#30340;&#21407;&#22240;&#23578;&#19981;&#28165;&#26970;&#65306;&#26159;&#22240;&#20026;&#23427;&#20204;&#23398;&#20064;&#20102;&#26377;&#25928;&#30340;&#35760;&#24518;&#65292;&#36824;&#26159;&#22240;&#20026;&#23427;&#20204;&#25191;&#34892;&#20102;&#26377;&#25928;&#30340;&#20449;&#29992;&#20998;&#37197;&#65311;&#22312;&#24341;&#20837;&#35760;&#24518;&#38271;&#24230;&#21644;&#20449;&#29992;&#20998;&#37197;&#38271;&#24230;&#30340;&#24418;&#24335;&#23450;&#20041;&#20043;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31616;&#21333;&#30340;&#21487;&#37197;&#32622;&#20219;&#21153;&#26469;&#27979;&#37327;&#36825;&#20123;&#19981;&#21516;&#30340;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer&#21487;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#25193;&#23637;&#21040;&#38656;&#35201;&#35760;&#20303;1500&#27493;&#21069;&#35266;&#23519;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;Transformer&#26080;&#27861;&#25913;&#36827;&#38271;&#26399;&#30340;&#20449;&#29992;&#20998;&#37197;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#35760;&#24518;&#21644;&#20449;&#29992;&#20998;&#37197;&#26041;&#38754;&#30340;&#19981;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) algorithms face two distinct challenges: learning effective representations of past and present observations, and determining how actions influence future returns. Both challenges involve modeling long-term dependencies. The transformer architecture has been very successful to solve problems that involve long-term dependencies, including in the RL domain. However, the underlying reason for the strong performance of Transformer-based RL methods remains unclear: is it because they learn effective memory, or because they perform effective credit assignment? After introducing formal definitions of memory length and credit assignment length, we design simple configurable tasks to measure these distinct quantities. Our empirical results reveal that Transformers can enhance the memory capacity of RL algorithms, scaling up to tasks that require memorizing observations $1500$ steps ago. However, Transformers do not improve long-term credit assignment. In summary, our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#65292;&#21487;&#20197;&#20165;&#36890;&#36807;&#31995;&#32479;&#22312;&#24050;&#30693;&#24378;&#36843;&#20989;&#25968;&#24433;&#21709;&#19979;&#30340;&#35266;&#27979;&#65292;&#35782;&#21035;&#21644;&#25233;&#21046;&#26410;&#30693;&#31995;&#32479;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;&#36825;&#39033;&#26041;&#27861;&#23545;&#35757;&#32451;&#20989;&#25968;&#26377;&#38750;&#24120;&#28201;&#21644;&#30340;&#38480;&#21046;&#65292;&#33021;&#22815;&#31283;&#20581;&#22320;&#35782;&#21035;&#21644;&#25233;&#21046;&#22823;&#31867;&#21035;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2307.03690</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25233;&#21046;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#26410;&#30693;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Suppressing unknown disturbances to dynamical systems using machine learning. (arXiv:2307.03690v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#65292;&#21487;&#20197;&#20165;&#36890;&#36807;&#31995;&#32479;&#22312;&#24050;&#30693;&#24378;&#36843;&#20989;&#25968;&#24433;&#21709;&#19979;&#30340;&#35266;&#27979;&#65292;&#35782;&#21035;&#21644;&#25233;&#21046;&#26410;&#30693;&#31995;&#32479;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;&#36825;&#39033;&#26041;&#27861;&#23545;&#35757;&#32451;&#20989;&#25968;&#26377;&#38750;&#24120;&#28201;&#21644;&#30340;&#38480;&#21046;&#65292;&#33021;&#22815;&#31283;&#20581;&#22320;&#35782;&#21035;&#21644;&#25233;&#21046;&#22823;&#31867;&#21035;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#21644;&#25233;&#21046;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#26410;&#30693;&#24178;&#25200;&#26159;&#19968;&#20010;&#22312;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20165;&#22522;&#20110;&#31995;&#32479;&#22312;&#24050;&#30693;&#24378;&#36843;&#20989;&#25968;&#24433;&#21709;&#19979;&#30340;&#20808;&#21069;&#35266;&#27979;&#26469;&#35782;&#21035;&#21644;&#25233;&#21046;&#26410;&#30693;&#31995;&#32479;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23545;&#35757;&#32451;&#20989;&#25968;&#26377;&#38750;&#24120;&#28201;&#21644;&#30340;&#38480;&#21046;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#31283;&#20581;&#22320;&#35782;&#21035;&#21644;&#25233;&#21046;&#22823;&#31867;&#21035;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#31034;&#20363;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#26696;&#65292;&#20854;&#20013;&#35782;&#21035;&#21644;&#25233;&#21046;&#20102; Lorenz &#31995;&#32479;&#30340;&#28151;&#27788;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying and suppressing unknown disturbances to dynamical systems is a problem with applications in many different fields. In this Letter, we present a model-free method to identify and suppress an unknown disturbance to an unknown system based only on previous observations of the system under the influence of a known forcing function. We find that, under very mild restrictions on the training function, our method is able to robustly identify and suppress a large class of unknown disturbances. We illustrate our scheme with an example where a chaotic disturbance to the Lorenz system is identified and suppressed.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#29983;&#25104;&#30340;&#20851;&#38190;&#35789;&#19981;&#24179;&#31561;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#37325;&#35201;&#30340;&#20196;&#29260;&#21644;&#21547;&#26377;&#26377;&#38480;&#35821;&#20041;&#30340;&#21477;&#23376;&#34987;&#21516;&#31561;&#25110;&#26356;&#21152;&#37325;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20849;&#21516;&#36716;&#31227;&#20851;&#27880;&#28857;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01379</link><description>&lt;p&gt;
&#23558;&#20851;&#27880;&#28857;&#36716;&#31227;&#21040;&#30456;&#20851;&#24615;&#19978;: &#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models. (arXiv:2307.01379v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#29983;&#25104;&#30340;&#20851;&#38190;&#35789;&#19981;&#24179;&#31561;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#37325;&#35201;&#30340;&#20196;&#29260;&#21644;&#21547;&#26377;&#26377;&#38480;&#35821;&#20041;&#30340;&#21477;&#23376;&#34987;&#21516;&#31561;&#25110;&#26356;&#21152;&#37325;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20849;&#21516;&#36716;&#31227;&#20851;&#27880;&#28857;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26159;&#23545;&#20110;&#27169;&#22411;&#29983;&#25104;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#29305;&#24449;&#21270;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#29992;&#25143;&#20309;&#26102;&#21487;&#20197;&#20449;&#20219;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22522;&#20110;&#19968;&#20123;&#21551;&#21457;&#24615;&#30340;&#20107;&#23454;&#65292;&#21363;&#22312;&#33258;&#22238;&#24402;&#30340;LLMs&#20013;&#65292;&#20196;&#29260;&#22312;&#21453;&#26144;&#29983;&#25104;&#30340;&#21547;&#20041;&#26041;&#38754;&#26159;&#19981;&#24179;&#31561;&#30340;&#65292;&#21363;&#19968;&#20123;&#20196;&#29260;&#27604;&#20854;&#20182;&#20196;&#29260;&#26356;&#30456;&#20851;&#65288;&#25110;&#26356;&#20855;&#20195;&#34920;&#24615;&#65289;&#65292;&#28982;&#32780;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#25152;&#26377;&#30340;&#20196;&#29260;&#34987;&#31561;&#20540;&#23545;&#24453;&#12290;&#36825;&#26159;&#30001;&#20110;&#35821;&#35328;&#20887;&#20313;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#24773;&#20917;&#19979;&#65292;&#21482;&#38656;&#35201;&#20960;&#20010;&#20851;&#38190;&#35789;&#23601;&#36275;&#20197;&#20256;&#36798;&#19968;&#20010;&#38271;&#21477;&#30340;&#21547;&#20041;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#19981;&#24179;&#31561;&#31216;&#20026;&#29983;&#25104;&#30340;&#19981;&#24179;&#31561;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#22914;&#20309;&#24433;&#21709;&#19981;&#30830;&#23450;&#24615;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#65292;&#30456;&#24403;&#25968;&#37327;&#30340;&#20196;&#29260;&#21644;&#21253;&#21547;&#26377;&#38480;&#35821;&#20041;&#30340;&#21477;&#23376;&#65292;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#34987;&#21516;&#31561;&#25110;&#29978;&#33267;&#26356;&#21152;&#37325;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#29983;&#25104;&#30340;&#19981;&#24179;&#31561;&#24341;&#36215;&#30340;&#36825;&#20123;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20849;&#21516;&#36716;&#31227;&#20851;&#27880;&#28857;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Large Language Models (LLMs) have shown great potential in Natural Language Generation, it is still challenging to characterize the uncertainty of model generations, i.e., when users could trust model outputs. Our research is derived from the heuristic facts that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#22810;&#39033;&#24335;&#23485;&#24230;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#25237;&#24433;&#26799;&#24230;&#27969;&#30340;&#22343;&#22330;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#21407;&#22987;&#26799;&#24230;&#19979;&#38477;&#19982;NTK&#20043;&#38388;&#30340;&#26126;&#26174;&#24046;&#24322;&#65292;&#21363;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#21407;&#22987;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#27604;&#26680;&#26041;&#27861;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.16361</link><description>&lt;p&gt;
&#36229;&#36234;NTK&#19982;&#33539;&#24335;&#26799;&#24230;&#19979;&#38477;&#65306;&#23545;&#20855;&#26377;&#22810;&#39033;&#24335;&#23485;&#24230;&#12289;&#26679;&#26412;&#21644;&#26102;&#38388;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#22330;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Beyond NTK with Vanilla Gradient Descent: A Mean-Field Analysis of Neural Networks with Polynomial Width, Samples, and Time. (arXiv:2306.16361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#22810;&#39033;&#24335;&#23485;&#24230;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#25237;&#24433;&#26799;&#24230;&#27969;&#30340;&#22343;&#22330;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#21407;&#22987;&#26799;&#24230;&#19979;&#38477;&#19982;NTK&#20043;&#38388;&#30340;&#26126;&#26174;&#24046;&#24322;&#65292;&#21363;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#21407;&#22987;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#27604;&#26680;&#26041;&#27861;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;&#38750;&#20984;&#20248;&#21270;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#21462;&#24471;&#20102;&#29702;&#35770;&#19978;&#30340;&#36827;&#23637;&#65292;&#20294;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#26159;&#21542;&#21487;&#20197;&#27604;&#26680;&#26041;&#27861;&#23454;&#29616;&#26356;&#39640;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#22810;&#39033;&#24335;&#23485;&#24230;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#25237;&#24433;&#26799;&#24230;&#27969;&#30340;&#28165;&#26224;&#22343;&#22330;&#20998;&#26512;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#19981;&#38656;&#35201;&#23545;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#19981;&#33258;&#28982;&#30340;&#20462;&#25913;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#24403;&#26679;&#26412;&#22823;&#23567;$n = O(d^{3.1})$&#65292;&#20854;&#20013;$d$&#26159;&#36755;&#20837;&#30340;&#32500;&#24230;&#65292;&#32593;&#32476;&#22312;&#22810;&#39033;&#24335;&#27425;&#36845;&#20195;&#20013;&#25910;&#25947;&#21040;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#38169;&#35823;&#65292;&#36825;&#20010;&#38169;&#35823;&#26080;&#27861;&#36890;&#36807;&#20351;&#29992;$n \ll d^4$&#20010;&#26679;&#26412;&#30340;&#26680;&#26041;&#27861;&#23454;&#29616;&#65292;&#22240;&#27492;&#28165;&#26970;&#22320;&#35777;&#26126;&#20102;&#21407;&#22987;&#26799;&#24230;&#19979;&#38477;&#21644;NTK&#20043;&#38388;&#30340;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent theoretical progress on the non-convex optimization of two-layer neural networks, it is still an open question whether gradient descent on neural networks without unnatural modifications can achieve better sample complexity than kernel methods. This paper provides a clean mean-field analysis of projected gradient flow on polynomial-width two-layer neural networks. Different from prior works, our analysis does not require unnatural modifications of the optimization algorithm. We prove that with sample size $n = O(d^{3.1})$ where $d$ is the dimension of the inputs, the network converges in polynomially many iterations to a non-trivial error that is not achievable by kernel methods using $n \ll d^4$ samples, hence demonstrating a clear separation between unmodified gradient descent and NTK.
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26088;&#22312;&#36890;&#36807;&#20223;&#30495;&#33041;&#37096;&#25805;&#20316;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20294;&#26159;&#22312;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#19978;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.15749</link><description>&lt;p&gt;
&#20309;&#21435;&#20309;&#20174;&#65306;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#30340;&#25968;&#23383;&#30828;&#20214;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration. (arXiv:2306.15749v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15749
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26088;&#22312;&#36890;&#36807;&#20223;&#30495;&#33041;&#37096;&#25805;&#20316;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20294;&#26159;&#22312;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#19978;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#22312;&#28085;&#30422;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#31454;&#20105;&#21147;&#65307;&#28982;&#32780;&#65292;&#36825;&#26159;&#20197;&#25928;&#29575;&#20026;&#20195;&#20215;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#36234;&#26469;&#36234;&#22810;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#33021;&#21147;&#12290;&#29983;&#29289;&#33041;&#30340;&#21151;&#32791;&#25928;&#29575;&#36229;&#36807;&#20219;&#20309;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65307;&#22240;&#27492;&#65292;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#35797;&#22270;&#27169;&#20223;&#33041;&#37096;&#25805;&#20316;&#65292;&#20363;&#22914;&#22522;&#20110;&#33033;&#20914;&#30340;&#20449;&#24687;&#22788;&#29702;&#65292;&#20197;&#25552;&#39640;DL&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#23613;&#31649;&#33041;&#37096;&#26377;&#35832;&#22914;&#39640;&#25928;&#30340;&#20449;&#24687;&#20256;&#36755;&#12289;&#23494;&#38598;&#30340;&#31070;&#32463;&#20803;&#36830;&#25509;&#21644;&#35745;&#31639;&#19982;&#23384;&#20648;&#30340;&#20849;&#21516;&#20301;&#32622;&#31561;&#20248;&#21183;&#65292;&#20294;&#21487;&#29992;&#30340;&#29983;&#29289;&#22522;&#24213;&#20005;&#37325;&#38480;&#21046;&#20102;&#29983;&#29289;&#22823;&#33041;&#30340;&#36827;&#21270;&#12290;&#30005;&#23376;&#30828;&#20214;&#27809;&#26377;&#30456;&#21516;&#30340;&#32422;&#26463;&#65307;&#22240;&#27492;&#65292;&#34429;&#28982;&#24314;&#27169;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#21487;&#33021;&#25581;&#31034;&#20102;&#19968;&#20010;&#35868;&#39064;&#30340;&#19968;&#37096;&#20998;&#65292;&#20294;&#23545;&#20110;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning models scale, they become increasingly competitive from domains spanning computer vision to natural language processing; however, this happens at the expense of efficiency since they require increasingly more memory and computing power. The power efficiency of the biological brain outperforms the one of any large-scale deep learning (DL) model; thus, neuromorphic computing tries to mimic the brain operations, such as spike-based information processing, to improve the efficiency of DL models. Despite the benefits of the brain, such as efficient information transmission, dense neuronal interconnects, and the co-location of computation and memory, the available biological substrate has severely constrained the evolution of biological brains. Electronic hardware does not have the same constraints; therefore, while modeling spiking neural networks (SNNs) might uncover one piece of the puzzle, the design of efficient hardware backends for SNNs needs further investigation, po
&lt;/p&gt;</description></item><item><title>MIMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15128</link><description>&lt;p&gt;
MIMIC: &#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MIMIC: Masked Image Modeling with Image Correspondences. (arXiv:2306.15128v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15128
&lt;/p&gt;
&lt;p&gt;
MIMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20687;&#32032;&#32423;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#8212;&#8212;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#28145;&#24230;&#20272;&#35745;&#21644;&#35821;&#20041;&#20998;&#21106;&#8212;&#8212;&#22914;&#20170;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#31579;&#36873;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20165;&#36890;&#36807;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#24102;&#26377;&#27880;&#37322;&#30340;3D&#32593;&#26684;&#12289;&#28857;&#20113;&#21644;&#30456;&#26426;&#21442;&#25968;&#31579;&#36873;&#32780;&#26469;&#65292;&#24182;&#19981;&#20855;&#22791;&#22810;&#35270;&#35282;&#22330;&#26223;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#31579;&#36873;&#26426;&#21046;&#12290;&#25105;&#20204;&#20174;&#24320;&#28304;&#35270;&#39057;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#30340;3D&#29615;&#22659;&#20013;&#25366;&#25496;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;MIMIC-1M(&#21253;&#21547;1.3M&#20010;&#22810;&#35270;&#35282;&#22270;&#20687;&#23545;)&#21644;MIMIC-3M(&#21253;&#21547;3.1M&#20010;&#22810;&#35270;&#35282;&#22270;&#20687;&#23545;)&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#30446;&#26631;&#65292;&#23637;&#31034;&#20102;&#20197;&#19979;&#21457;&#29616;&#65306;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;MIMIC-3M&#35757;&#32451;&#30340;&#34920;&#31034;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#28145;&#24230;&#20272;&#35745;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#34920;&#38754;&#27861;&#32447;&#21644;&#23039;&#24577;&#20272;&#35745;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many pixelwise dense prediction tasks-depth estimation and semantic segmentation in computer vision today rely on pretrained image representations. Therefore, curating effective pretraining datasets is vital. Unfortunately, the effective pretraining datasets are those with multi-view scenes and have only been curated using annotated 3D meshes, point clouds, and camera parameters from simulated environments. We propose a dataset-curation mechanism that does not require any annotations. We mine two datasets: MIMIC-1M with 1.3M and MIMIC-3M with 3.1M multi-view image pairs from open-sourced video datasets and from synthetic 3D environments. We train multiple self-supervised models with different masked image modeling objectives to showcase the following findings: Representations trained on MIMIC-3M outperform those mined using annotations on multiple downstream tasks, including depth estimation, semantic segmentation, surface normals, and pose estimation. They also outperform representati
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#20248;&#21270;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#23884;&#20837;&#30340;&#32452;&#21512;&#20250;&#30053;&#24494;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#24182;&#19988;&#32452;&#21512;&#26041;&#24335;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2306.14939</link><description>&lt;p&gt;
&#23884;&#20837;&#34701;&#21512;&#30340;&#33402;&#26415;&#65306;&#20248;&#21270;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
The Art of Embedding Fusion: Optimizing Hate Speech Detection. (arXiv:2306.14939v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14939
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#20248;&#21270;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#23884;&#20837;&#30340;&#32452;&#21512;&#20250;&#30053;&#24494;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#24182;&#19988;&#32452;&#21512;&#26041;&#24335;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#38656;&#35201;&#25429;&#25417;&#35821;&#35328;&#21644;&#35821;&#22659;&#32454;&#24494;&#24046;&#21035;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#25991;&#26412;&#35821;&#20041;&#34920;&#31034;&#65292;&#21487;&#20197;&#25913;&#36827;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26377;&#25928;&#22320;&#32452;&#21512;PLMs&#30340;&#34920;&#31034;&#21644;&#21033;&#29992;&#23427;&#20204;&#30340;&#20114;&#34917;&#20248;&#21183;&#30340;&#26041;&#27861;&#36824;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20960;&#31181;PLMs&#32452;&#21512;&#25216;&#26415;&#30340;&#26041;&#24335;&#65292;&#24182;&#20840;&#38754;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#32452;&#21512;&#23884;&#20837;&#21487;&#20197;&#30053;&#24494;&#25913;&#21892;&#24615;&#33021;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#65292;&#32452;&#21512;&#26041;&#24335;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;&#25105;&#20204;&#36824;&#22312;https://github.com/aflah02/The-Art-of-Embedding-Fusion-Optimizing-Hate-Speech-Detection&#19978;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hate speech detection is a challenging natural language processing task that requires capturing linguistic and contextual nuances. Pre-trained language models (PLMs) offer rich semantic representations of text that can improve this task. However there is still limited knowledge about ways to effectively combine representations across PLMs and leverage their complementary strengths. In this work, we shed light on various combination techniques for several PLMs and comprehensively analyze their effectiveness. Our findings show that combining embeddings leads to slight improvements but at a high computational cost and the choice of combination has marginal effect on the final outcome. We also make our codebase public at https://github.com/aflah02/The-Art-of-Embedding-Fusion-Optimizing-Hate-Speech-Detection .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#35268;&#27169;&#19978;&#29983;&#25104;&#38598;&#21512;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#20135;&#29983;&#20102;&#19982;&#23436;&#25972;&#30340;GEFS 31&#25104;&#21592;&#38598;&#21512;&#30456;&#20284;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#19988;&#24456;&#22909;&#22320;&#27169;&#25311;&#20102;&#22823;&#35268;&#27169;&#38598;&#21512;&#30340;&#32479;&#35745;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.14066</link><description>&lt;p&gt;
SEEDS&#65306;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20223;&#30495;&#22825;&#27668;&#39044;&#27979;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
SEEDS: Emulation of Weather Forecast Ensembles with Diffusion Models. (arXiv:2306.14066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#35268;&#27169;&#19978;&#29983;&#25104;&#38598;&#21512;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#20135;&#29983;&#20102;&#19982;&#23436;&#25972;&#30340;GEFS 31&#25104;&#21592;&#38598;&#21512;&#30456;&#20284;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#19988;&#24456;&#22909;&#22320;&#27169;&#25311;&#20102;&#22823;&#35268;&#27169;&#38598;&#21512;&#30340;&#32479;&#35745;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#26410;&#26469;&#22825;&#27668;&#26102;&#65292;&#27010;&#29575;&#39044;&#27979;&#23545;&#20915;&#31574;&#38750;&#24120;&#37325;&#35201;&#12290;&#20027;&#35201;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#39044;&#27979;&#38598;&#21512;&#26469;&#34920;&#31034;&#21644;&#37327;&#21270;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#20135;&#29983;&#38598;&#21512;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#26368;&#36817;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#35268;&#27169;&#19978;&#29983;&#25104;&#38598;&#21512;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;5&#25104;&#21592;&#38598;&#21512;GEFS&#37325;&#26032;&#39044;&#25253;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#65292;&#20197;&#20135;&#29983;&#32852;&#21512;&#24773;&#20917;&#19979;&#30495;&#23454;&#30340;&#22825;&#27668;&#39044;&#27979;&#65292;&#36825;&#20123;&#24773;&#20917;&#21487;&#20197;&#22522;&#20110;&#25805;&#20316;GEFS&#39044;&#27979;&#31995;&#32479;&#30340;&#23569;&#25968;&#25104;&#21592;&#26465;&#20214;&#21270;&#12290;&#26681;&#25454;ERA5&#20998;&#26512;&#35780;&#20272;&#65292;&#29983;&#25104;&#30340;&#38598;&#21512;&#19982;&#23436;&#25972;&#30340;GEFS 31&#25104;&#21592;&#38598;&#21512;&#20855;&#26377;&#30456;&#20284;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#19988;&#24456;&#22909;&#22320;&#27169;&#25311;&#20102;&#22823;&#35268;&#27169;&#38598;&#21512;&#30340;&#32479;&#35745;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#23558;&#30456;&#21516;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#24320;&#21457;&#25193;&#25955;&#27169;&#22411;&#65292;&#36827;&#34892;&#29983;&#25104;&#21518;&#22788;&#29702;&#12290;&#27169;&#22411;&#21487;&#20197;&#22522;&#20110;&#23569;&#25968;&#39044;&#27979;&#25104;&#21592;&#26465;&#20214;&#21270;&#22320;&#29983;&#25104;&#31867;&#20284;&#20110;&#29289;&#29702;&#22823;&#27169;&#22411;&#38598;&#21512;&#30340;&#39044;&#27979;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic forecasting is crucial to decision-making under uncertainty about future weather. The dominant approach is to use an ensemble of forecasts to represent and quantify uncertainty in operational numerical weather prediction. However, generating ensembles is computationally costly. In this paper, we propose to generate ensemble forecasts at scale by leveraging recent advances in generative artificial intelligence. Our approach learns a data-driven probabilistic diffusion model from the 5-member ensemble GEFS reforecast dataset. The model can then be sampled efficiently to produce realistic weather forecasts, conditioned on a few members of the operational GEFS forecasting system. The generated ensembles have similar predictive skill as the full GEFS 31-member ensemble, evaluated against ERA5 reanalysis, and emulate well the statistics of large physics-based ensembles. We also apply the same methodology to developing a diffusion model for generative post-processing: the model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#37327;&#23376;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#21457;&#29616;&#19968;&#20010;&#31934;&#24515;&#21046;&#20316;&#30340;&#36890;&#29992;&#25200;&#21160;&#21487;&#20197;&#25104;&#21151;&#22320;&#27450;&#39575;&#20004;&#20010;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#65292;&#36825;&#20026;&#26500;&#24314;&#23433;&#20840;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24102;&#26469;&#28508;&#22312;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2306.11974</link><description>&lt;p&gt;
&#29992;&#20110;&#22810;&#20998;&#31867;&#20219;&#21153;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#30340;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
Universal adversarial perturbations for multiple classification tasks with quantum classifiers. (arXiv:2306.11974v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#37327;&#23376;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#21457;&#29616;&#19968;&#20010;&#31934;&#24515;&#21046;&#20316;&#30340;&#36890;&#29992;&#25200;&#21160;&#21487;&#20197;&#25104;&#21151;&#22320;&#27450;&#39575;&#20004;&#20010;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#65292;&#36825;&#20026;&#26500;&#24314;&#23433;&#20840;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24102;&#26469;&#28508;&#22312;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#38376;&#26032;&#20852;&#30340;&#39046;&#22495;&#65292;&#23427;&#30740;&#31350;&#20102;&#37327;&#23376;&#23398;&#20064;&#31995;&#32479;&#23545;&#25239;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#24182;&#24320;&#21457;&#20102;&#21487;&#33021;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#37327;&#23376;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#26159;&#19968;&#31181;&#23567;&#30340;&#25200;&#21160;&#65292;&#21487;&#20197;&#20351;&#19981;&#21516;&#30340;&#36755;&#20837;&#26679;&#26412;&#25104;&#20026;&#35823;&#23548;&#32473;&#23450;&#37327;&#23376;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#31034;&#20363;&#12290;&#23613;&#31649;&#27492;&#39046;&#22495;&#20043;&#21069;&#40092;&#26377;&#25506;&#31350;&#65292;&#20294;&#26159;&#36890;&#29992;&#25200;&#21160;&#21487;&#33021;&#26497;&#22823;&#22320;&#31616;&#21270;&#24694;&#24847;&#25915;&#20987;&#65292;&#23545;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36896;&#25104;&#24847;&#24819;&#19981;&#21040;&#30340;&#30772;&#22351;&#12290;&#26412;&#25991;&#22312;&#24322;&#26500;&#20998;&#31867;&#20219;&#21153;&#30340;&#32972;&#26223;&#19979;&#65292;&#36827;&#19968;&#27493;&#25506;&#35752;&#37327;&#23376;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#20960;&#20046;&#22312;&#20004;&#20010;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#65292;&#37117;&#21487;&#20197;&#34987;&#19968;&#20010;&#31934;&#24515;&#21046;&#20316;&#30340;&#36890;&#29992;&#25200;&#21160;&#25152;&#35825;&#23548;&#25104;&#21151;&#22320;&#27450;&#39575;&#12290;&#36825;&#19968;&#32467;&#26524;&#24050;&#32463;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;CIFAR-10&#21644;Iris&#20013;&#24471;&#21040;&#26126;&#30830;&#30340;&#35777;&#26126;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#26159;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#28508;&#22312;&#23041;&#32961;&#65292;&#24182;&#21487;&#33021;&#32473;&#26500;&#24314;&#23433;&#20840;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24102;&#26469;&#24040;&#22823;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum adversarial machine learning is an emerging field that studies the vulnerability of quantum learning systems against adversarial perturbations and develops possible defense strategies. Quantum universal adversarial perturbations are small perturbations, which can make different input samples into adversarial examples that may deceive a given quantum classifier. This is a field that was rarely looked into but worthwhile investigating because universal perturbations might simplify malicious attacks to a large extent, causing unexpected devastation to quantum machine learning models. In this paper, we take a step forward and explore the quantum universal perturbations in the context of heterogeneous classification tasks. In particular, we find that quantum classifiers that achieve almost state-of-the-art accuracy on two different classification tasks can be both conclusively deceived by one carefully-crafted universal perturbation. This result is explicitly demonstrated with well-
&lt;/p&gt;</description></item><item><title>&#26368;&#22823;&#29109;&#24322;&#36136;&#20195;&#29702;&#38236;&#20687;&#23398;&#20064;(MEHAML)&#26159;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#29109;&#21407;&#29702;&#35774;&#35745;&#20102;&#26368;&#22823;&#29109;MARL&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#20855;&#26377;&#32852;&#21512;&#26368;&#22823;&#29109;&#30446;&#26631;&#30340;&#21333;&#35843;&#25913;&#36827;&#21644;&#25910;&#25947;&#33267;&#20013;&#20301;&#21709;&#24212;&#22343;&#34913;(QRE)&#30340;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#24120;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;HASAC&#26469;&#39564;&#35777;&#20854;&#23454;&#29992;&#24615;&#21644;&#22312;&#25506;&#32034;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.10715</link><description>&lt;p&gt;
&#26368;&#22823;&#29109;&#24322;&#36136;&#20195;&#29702;&#38236;&#20687;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Maximum Entropy Heterogeneous-Agent Mirror Learning. (arXiv:2306.10715v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10715
&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#29109;&#24322;&#36136;&#20195;&#29702;&#38236;&#20687;&#23398;&#20064;(MEHAML)&#26159;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#29109;&#21407;&#29702;&#35774;&#35745;&#20102;&#26368;&#22823;&#29109;MARL&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#20855;&#26377;&#32852;&#21512;&#26368;&#22823;&#29109;&#30446;&#26631;&#30340;&#21333;&#35843;&#25913;&#36827;&#21644;&#25910;&#25947;&#33267;&#20013;&#20301;&#21709;&#24212;&#22343;&#34913;(QRE)&#30340;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#24120;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;HASAC&#26469;&#39564;&#35777;&#20854;&#23454;&#29992;&#24615;&#21644;&#22312;&#25506;&#32034;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#22312;&#21512;&#20316;&#21338;&#24328;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#38754;&#20020;&#26679;&#26412;&#25928;&#29575;&#20302;&#12289;&#36229;&#21442;&#25968;&#33030;&#24369;&#24615;&#21644;&#25910;&#25947;&#20110;&#27425;&#20248;&#32435;&#20160;&#22343;&#34913;&#30340;&#39118;&#38505;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#26368;&#22823;&#29109;&#24322;&#36136;&#20195;&#29702;&#38236;&#20687;&#23398;&#20064;(MEHAML)&#65292;&#21033;&#29992;&#26368;&#22823;&#29109;&#21407;&#29702;&#35774;&#35745;&#20102;&#26368;&#22823;&#29109;MARL&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20174;MEHAML&#26694;&#26550;&#23548;&#20986;&#30340;&#31639;&#27861;&#20855;&#26377;&#32852;&#21512;&#26368;&#22823;&#29109;&#30446;&#26631;&#30340;&#21333;&#35843;&#25913;&#36827;&#21644;&#25910;&#25947;&#33267;&#20013;&#20301;&#21709;&#24212;&#22343;&#34913;(QRE)&#30340;&#26399;&#26395;&#29305;&#24615;&#12290;MEHAML&#30340;&#23454;&#29992;&#24615;&#36890;&#36807;&#24320;&#21457;&#24191;&#27867;&#20351;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;HASAC&#30340;MEHAML&#25193;&#23637;&#26469;&#23637;&#31034;&#65292;&#22312;&#19977;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#23637;&#31034;&#20986;&#20102;&#25506;&#32034;&#21644;&#31283;&#20581;&#24615;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent reinforcement learning (MARL) has been shown effective for cooperative games in recent years. However, existing state-of-the-art methods face challenges related to sample inefficiency, brittleness regarding hyperparameters, and the risk of converging to a suboptimal Nash Equilibrium. To resolve these issues, in this paper, we propose a novel theoretical framework, named Maximum Entropy Heterogeneous-Agent Mirror Learning (MEHAML), that leverages the maximum entropy principle to design maximum entropy MARL actor-critic algorithms. We prove that algorithms derived from the MEHAML framework enjoy the desired properties of the monotonic improvement of the joint maximum entropy objective and the convergence to quantal response equilibrium (QRE). The practicality of MEHAML is demonstrated by developing a MEHAML extension of the widely used RL algorithm, HASAC (for soft actor-critic), which shows significant improvements in exploration and robustness on three challenging benchmark
&lt;/p&gt;</description></item><item><title>PEACE&#26159;&#19968;&#20010;&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#22240;&#26524;&#25351;&#23548;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#28304;&#24179;&#21488;&#25968;&#25454;&#24182;&#25512;&#24191;&#21040;&#30446;&#26631;&#24179;&#21488;&#65292;&#25506;&#32034;&#22914;&#20309;&#24314;&#31435;&#36866;&#29992;&#20110;&#19981;&#21516;&#24179;&#21488;&#30340;&#36890;&#29992;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.08804</link><description>&lt;p&gt;
PEACE: &#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;- &#19968;&#20010;&#22240;&#26524;&#25351;&#23548;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PEACE: Cross-Platform Hate Speech Detection- A Causality-guided Framework. (arXiv:2306.08804v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08804
&lt;/p&gt;
&lt;p&gt;
PEACE&#26159;&#19968;&#20010;&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#22240;&#26524;&#25351;&#23548;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#28304;&#24179;&#21488;&#25968;&#25454;&#24182;&#25512;&#24191;&#21040;&#30446;&#26631;&#24179;&#21488;&#65292;&#25506;&#32034;&#22914;&#20309;&#24314;&#31435;&#36866;&#29992;&#20110;&#19981;&#21516;&#24179;&#21488;&#30340;&#36890;&#29992;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26159;&#25351;&#26816;&#27979;&#22522;&#20110;&#23447;&#25945;&#12289;&#24615;&#21035;&#12289;&#24615;&#21462;&#21521;&#25110;&#20854;&#20182;&#29305;&#24449;&#32780;&#38024;&#23545;&#20010;&#20154;&#25110;&#32676;&#20307;&#30340;&#24694;&#24847;&#20869;&#23481;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#19981;&#21516;&#24179;&#21488;&#30340;&#25919;&#31574;&#19981;&#21516;&#65292;&#19981;&#21516;&#32676;&#20307;&#20197;&#19981;&#21516;&#26041;&#24335;&#34920;&#36798;&#20167;&#24680;&#35328;&#35770;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#26576;&#20123;&#24179;&#21488;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#65292;&#26500;&#24314;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#26159;&#21542;&#21487;&#20197;&#23398;&#20064;&#19968;&#20010;&#36866;&#29992;&#20110;&#36328;&#24179;&#21488;&#35774;&#32622;&#30340;&#21487;&#36801;&#31227;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#65292;&#21363;&#25105;&#20204;&#22312;&#19968;&#20010;&#65288;&#28304;&#65289;&#24179;&#21488;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23558;&#27169;&#22411;&#25512;&#24191;&#21040;&#22810;&#20010;&#65288;&#30446;&#26631;&#65289;&#24179;&#21488;&#12290;&#29616;&#26377;&#30340;&#25512;&#24191;&#27169;&#22411;&#20381;&#36182;&#20110;&#35821;&#35328;&#32447;&#32034;&#25110;&#36741;&#21161;&#20449;&#24687;&#65292;&#20351;&#20854;&#20559;&#21521;&#20110;&#28304;&#24179;&#21488;&#19978;&#30340;&#26576;&#20123;&#26631;&#31614;&#25110;&#26576;&#20123;&#31867;&#22411;&#30340;&#35789;&#65288;&#22914;&#36785;&#39554;&#24615;&#35789;&#35821;&#65289;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;&#30446;&#26631;&#24179;&#21488;&#12290;&#21463;&#21040;&#31038;&#20250;&#21644;&#24515;&#29702;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21162;&#21147;&#25506;&#32034;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#22240;&#26524;&#20851;&#31995;&#65292;&#21487;&#20197;&#20174;&#28304;&#24179;&#21488;&#23398;&#20064;&#29305;&#24449;&#24182;&#25512;&#24191;&#21040;&#30446;&#26631;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hate speech detection refers to the task of detecting hateful content that aims at denigrating an individual or a group based on their religion, gender, sexual orientation, or other characteristics. Due to the different policies of the platforms, different groups of people express hate in different ways. Furthermore, due to the lack of labeled data in some platforms it becomes challenging to build hate speech detection models. To this end, we revisit if we can learn a generalizable hate speech detection model for the cross platform setting, where we train the model on the data from one (source) platform and generalize the model across multiple (target) platforms. Existing generalization models rely on linguistic cues or auxiliary information, making them biased towards certain tags or certain kinds of words (e.g., abusive words) on the source platform and thus not applicable to the target platforms. Inspired by social and psychological theories, we endeavor to explore if there exist in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#25506;&#27979;&#20998;&#31867;&#22120;&#26469;&#35780;&#20272;&#32452;&#20214;&#26159;&#21542;&#34920;&#31034;&#23646;&#24615;&#65292;&#22635;&#34917;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20851;&#20110;&#8220;&#34920;&#31034;&#8221;&#30340;&#21746;&#23398;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2306.08193</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#31034;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Operationalising Representation in Natural Language Processing. (arXiv:2306.08193v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#25506;&#27979;&#20998;&#31867;&#22120;&#26469;&#35780;&#20272;&#32452;&#20214;&#26159;&#21542;&#34920;&#31034;&#23646;&#24615;&#65292;&#22635;&#34917;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20851;&#20110;&#8220;&#34920;&#31034;&#8221;&#30340;&#21746;&#23398;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#8220;&#34920;&#31034;&#8221;&#22312;&#35748;&#30693;&#31185;&#23398;&#21746;&#23398;&#20013;&#20855;&#26377;&#26680;&#24515;&#22320;&#20301;&#65292;&#20294;&#22312;&#24403;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23454;&#36341;&#20013;&#65292;&#20960;&#20046;&#27809;&#26377;&#21746;&#23398;&#39046;&#22495;&#30340;&#20808;&#21069;&#30740;&#31350;&#19982;&#20043;&#28041;&#21450;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65306;&#32467;&#21512;&#35748;&#30693;&#31185;&#23398;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35780;&#20272;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#32452;&#20214;&#25152;&#20316;&#20986;&#30340;&#34920;&#31034;&#24615;&#22768;&#26126;&#65292;&#24182;&#25552;&#20986;&#19977;&#20010;&#35780;&#20272;&#32452;&#20214;&#26159;&#21542;&#34920;&#31034;&#23646;&#24615;&#30340;&#26631;&#20934;&#65292;&#24182;&#20351;&#29992;&#25506;&#27979;&#20998;&#31867;&#22120;&#26469;&#23454;&#29616;&#36825;&#20123;&#26631;&#20934;&#30340;&#25805;&#20316;&#21270;&#65292;&#25506;&#27979;&#20998;&#31867;&#22120;&#26159;NLP&#65288;&#21644;&#26356;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#65289;&#20013;&#27969;&#34892;&#30340;&#20998;&#26512;&#25216;&#26415;&#12290;&#25805;&#20316;&#21270;&#19968;&#20010;&#22312;&#21746;&#23398;&#19978;&#21463;&#21040;&#21551;&#21457;&#30340;&#8220;&#34920;&#31034;&#8221;&#27010;&#24565;&#30340;&#39033;&#30446;&#24212;&#35813;&#24341;&#36215;&#31185;&#23398;&#21746;&#23398;&#23478;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23454;&#36341;&#32773;&#30340;&#20852;&#36259;&#12290;&#23545;&#20110;&#21746;&#23398;&#23478;&#26469;&#35828;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#26377;&#20851;&#34920;&#31034;&#30340;&#26412;&#36136;&#30340;&#35770;&#25454;&#30340;&#26032;&#39062;&#22330;&#22320;&#65292;&#24182;&#24110;&#21161;NLPers&#32452;&#32455;&#26377;&#20851;&#25506;&#27979;&#23454;&#39564;&#30340;&#22823;&#37327;&#25991;&#29486;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#32463;&#39564;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its centrality in the philosophy of cognitive science, there has been little prior philosophical work engaging with the notion of representation in contemporary NLP practice. This paper attempts to fill that lacuna: drawing on ideas from cognitive science, I introduce a framework for evaluating the representational claims made about components of neural NLP models, proposing three criteria with which to evaluate whether a component of a model represents a property and operationalising these criteria using probing classifiers, a popular analysis technique in NLP (and deep learning more broadly).  The project of operationalising a philosophically-informed notion of representation should be of interest to both philosophers of science and NLP practitioners. It affords philosophers a novel testing-ground for claims about the nature of representation, and helps NLPers organise the large literature on probing experiments, suggesting novel avenues for empirical research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;Probabilistic Reweighting&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#33258;&#28982;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.06599</link><description>&lt;p&gt;
&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;(Variational Imbalanced Regression)
&lt;/p&gt;
&lt;p&gt;
Variational Imbalanced Regression. (arXiv:2306.06599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;Probabilistic Reweighting&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#33258;&#28982;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#26102;&#65292;&#29616;&#26377;&#30340;&#22238;&#24402;&#27169;&#22411;&#24448;&#24448;&#22312;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#65292;&#23427;&#19981;&#20165;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#33258;&#28982;&#22320;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#19982;&#20856;&#22411;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20551;&#35774;I.I.D.&#34920;&#31034;&#65288;&#25968;&#25454;&#28857;&#30340;&#34920;&#31034;&#19981;&#30452;&#25509;&#21463;&#20854;&#20182;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;VIR&#20511;&#29992;&#20855;&#26377;&#31867;&#20284;&#22238;&#24402;&#26631;&#31614;&#30340;&#25968;&#25454;&#26469;&#35745;&#31639;&#28508;&#22312;&#34920;&#31034;&#30340;&#21464;&#20998;&#20998;&#24067;&#65307;&#27492;&#22806;&#65292;&#19981;&#21516;&#20110;&#20135;&#29983;&#28857;&#20272;&#35745;&#30340;&#30830;&#23450;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292; VIR&#39044;&#27979;&#25972;&#20010;&#27491;&#24577;&#21453;-&#20285;&#29595;&#20998;&#24067;&#24182;&#35843;&#33410;&#30456;&#20851;&#32852;&#30340;&#20849;&#36717;&#20998;&#24067;&#65292;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#26045;&#21152;&#27010;&#29575;&#37325;&#26032;&#21152;&#26435;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#22312;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing regression models tend to fall short in both accuracy and uncertainty estimation when the label distribution is imbalanced. In this paper, we propose a probabilistic deep learning model, dubbed variational imbalanced regression (VIR), which not only performs well in imbalanced regression but naturally produces reasonable uncertainty estimation as a byproduct. Different from typical variational autoencoders assuming I.I.D. representations (a data point's representation is not directly affected by other data points), our VIR borrows data with similar regression labels to compute the latent representation's variational distribution; furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions to impose probabilistic reweighting on the imbalanced data, thereby providing better uncertainty estimation. Experiments in several real-world datasets sh
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#27969;&#31243;&#65292;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24378;&#22823;&#29305;&#24449;&#34920;&#31034;&#65292;&#22312;&#35268;&#27169;&#19978;&#26377;&#25928;&#22320;&#23545;&#22270;&#20687;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36895;&#29575;&#38477;&#20302;&#30446;&#26631;&#21644; CLIP &#30340;&#22270;&#20687;-&#25991;&#26412;&#32465;&#23450;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#33258;&#26631;&#35760;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05272</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36895;&#29575;&#38477;&#20302;&#21407;&#21017;&#36827;&#34892;&#22270;&#20687;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models. (arXiv:2306.05272v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#27969;&#31243;&#65292;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24378;&#22823;&#29305;&#24449;&#34920;&#31034;&#65292;&#22312;&#35268;&#27169;&#19978;&#26377;&#25928;&#22320;&#23545;&#22270;&#20687;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36895;&#29575;&#38477;&#20302;&#30446;&#26631;&#21644; CLIP &#30340;&#22270;&#20687;-&#25991;&#26412;&#32465;&#23450;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#33258;&#26631;&#35760;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#24050;&#32463;&#22312;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#24102;&#26469;&#20102;&#33539;&#24335;&#36716;&#21464;&#65292;&#20294;&#26159;&#32858;&#31867;&#26410;&#26631;&#35760;&#30340;&#22270;&#20687;&#20316;&#20026;&#19968;&#31181;&#22522;&#26412;&#21644;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#20173;&#28982;&#32570;&#20047;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#27969;&#31243;&#65292;&#21033;&#29992; CLIP &#31561;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24378;&#22823;&#29305;&#24449;&#34920;&#31034;&#65292;&#22312;&#35268;&#27169;&#19978;&#26377;&#25928;&#22320;&#23545;&#22270;&#20687;&#36827;&#34892;&#32858;&#31867;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;&#29305;&#24449;&#36890;&#36807;&#36827;&#19968;&#27493;&#20248;&#21270;&#36895;&#29575;&#38477;&#20302;&#30446;&#26631;&#65292;&#26356;&#20855;&#26377;&#32467;&#26500;&#24615;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#29305;&#24449;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#20363;&#22914;&#20174; ImageNet-1k &#30340; 57&#65285;&#25552;&#39640;&#21040; 66&#65285;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992; CLIP &#30340;&#22270;&#20687;-&#25991;&#26412;&#32465;&#23450;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26032;&#30340;&#32858;&#31867;&#26041;&#27861;&#22914;&#20309;&#23548;&#33268;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#26631;&#35760;&#31639;&#27861;&#65292;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#26410;&#26631;&#35760;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914; MS-COCO &#21644; LAION-Aesthetics&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large pre-trained models has brought about a paradigm shift in both visual representation learning and natural language processing. However, clustering unlabeled images, as a fundamental and classic machine learning problem, still lacks effective solution, particularly for large-scale datasets. In this paper, we propose a novel image clustering pipeline that leverages the powerful feature representation of large pre-trained models such as CLIP and cluster images effectively and efficiently at scale. We show that the pre-trained features are significantly more structured by further optimizing the rate reduction objective. The resulting features may significantly improve the clustering accuracy, e.g., from 57\% to 66\% on ImageNet-1k. Furthermore, by leveraging CLIP's image-text binding, we show how the new clustering method leads to a simple yet effective self-labeling algorithm that successfully works on unlabeled large datasets such as MS-COCO and LAION-Aesthetics. We wi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#19968;&#33268;&#24615;&#30340;&#22312;&#32447;&#24191;&#21578;&#39044;&#25490;&#21517;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#19968;&#20010;&#22522;&#20110;&#22359;&#30340;&#37319;&#26679;&#27169;&#22359;&#21644;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#25490;&#21517;&#23545;&#40784;&#27169;&#22359;&#65292;&#26469;&#26174;&#24335;&#20248;&#21270;ECPM&#25490;&#21517;&#32467;&#26524;&#30340;&#19968;&#33268;&#24615;&#12290;&#20182;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;Delta NDCG&#30340;&#21152;&#26435;&#26426;&#21046;&#65292;&#20197;&#26356;&#22909;&#22320;&#21306;&#20998;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03516</link><description>&lt;p&gt;
COPR&#65306;&#38754;&#21521;&#19968;&#33268;&#24615;&#30340;&#22312;&#32447;&#24191;&#21578;&#39044;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
COPR: Consistency-Oriented Pre-Ranking for Online Advertising. (arXiv:2306.03516v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03516
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#19968;&#33268;&#24615;&#30340;&#22312;&#32447;&#24191;&#21578;&#39044;&#25490;&#21517;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#19968;&#20010;&#22522;&#20110;&#22359;&#30340;&#37319;&#26679;&#27169;&#22359;&#21644;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#25490;&#21517;&#23545;&#40784;&#27169;&#22359;&#65292;&#26469;&#26174;&#24335;&#20248;&#21270;ECPM&#25490;&#21517;&#32467;&#26524;&#30340;&#19968;&#33268;&#24615;&#12290;&#20182;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;Delta NDCG&#30340;&#21152;&#26435;&#26426;&#21046;&#65292;&#20197;&#26356;&#22909;&#22320;&#21306;&#20998;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32423;&#32852;&#26550;&#26500;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#24191;&#21578;&#31995;&#32479;&#20013;&#20197;&#24179;&#34913;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;&#22312;&#36825;&#31181;&#26550;&#26500;&#20013;&#65292;&#39044;&#25490;&#21517;&#27169;&#22411;&#34987;&#26399;&#26395;&#25104;&#20026;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25490;&#21517;&#27169;&#22411;&#36817;&#20284;&#65292;&#20197;&#22788;&#29702;&#26356;&#22810;&#20855;&#26377;&#20005;&#26684;&#24310;&#36831;&#35201;&#27714;&#30340;&#20505;&#36873;&#32773;&#12290;&#30001;&#20110;&#27169;&#22411;&#23481;&#37327;&#30340;&#24046;&#36317;&#65292;&#39044;&#25490;&#21517;&#21644;&#25490;&#21517;&#27169;&#22411;&#36890;&#24120;&#20250;&#29983;&#25104;&#19981;&#19968;&#33268;&#30340;&#25490;&#21517;&#32467;&#26524;&#65292;&#20174;&#32780;&#25439;&#23475;&#25972;&#20010;&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;&#25552;&#20986;&#20102;&#24471;&#20998;&#23545;&#40784;&#30340;&#33539;&#24335;&#20197;&#35268;&#33539;&#23427;&#20204;&#30340;&#21407;&#22987;&#20998;&#25968;&#65292;&#20351;&#23427;&#20204;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#22312;&#22312;&#32447;&#24191;&#21578;&#20013;&#24212;&#29992;&#26102;&#65292;&#30001;&#20110;&#24517;&#28982;&#30340;&#23545;&#40784;&#35823;&#24046;&#21644;&#31454;&#26631;&#30340;&#35823;&#24046;&#25918;&#22823;&#65292;&#23427;&#20250;&#36973;&#21463;&#22256;&#25200;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38754;&#21521;&#19968;&#33268;&#24615;&#30340;&#22312;&#32447;&#24191;&#21578;&#39044;&#25490;&#21517;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#19968;&#20010;&#22522;&#20110;&#22359;&#30340;&#37319;&#26679;&#27169;&#22359;&#21644;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#25490;&#21517;&#23545;&#40784;&#27169;&#22359;&#65292;&#26469;&#26174;&#24335;&#20248;&#21270;ECPM&#25490;&#21517;&#32467;&#26524;&#30340;&#19968;&#33268;&#24615;&#12290;&#37319;&#29992;&#20102;&#22522;&#20110;$\Delta NDCG$&#30340;&#21152;&#26435;&#26426;&#21046;&#65292;&#20197;&#26356;&#22909;&#22320;&#21306;&#20998;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cascading architecture has been widely adopted in large-scale advertising systems to balance efficiency and effectiveness. In this architecture, the pre-ranking model is expected to be a lightweight approximation of the ranking model, which handles more candidates with strict latency requirements. Due to the gap in model capacity, the pre-ranking and ranking models usually generate inconsistent ranked results, thus hurting the overall system effectiveness. The paradigm of score alignment is proposed to regularize their raw scores to be consistent. However, it suffers from inevitable alignment errors and error amplification by bids when applied in online advertising. To this end, we introduce a consistency-oriented pre-ranking framework for online advertising, which employs a chunk-based sampling module and a plug-and-play rank alignment module to explicitly optimize consistency of ECPM-ranked results. A $\Delta NDCG$-based weighting mechanism is adopted to better distinguish the import
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#29305;&#24449;&#19982;&#21709;&#24212;&#21464;&#37327;&#30340;&#20114;&#20449;&#24687;&#36827;&#34892;&#20248;&#20808;&#32423;&#25490;&#24207;&#65292;&#24182;&#35774;&#35745;&#20102;&#20272;&#35745;&#20114;&#20449;&#24687;&#30340;&#21028;&#21035;&#24335;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#22810;&#39033;&#25913;&#36827;&#25514;&#26045;&#20197;&#24212;&#23545;&#26356;&#22810;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.03301</link><description>&lt;p&gt;
&#21160;&#24577;&#29305;&#24449;&#36873;&#25321;&#20013;&#26465;&#20214;&#20114;&#20449;&#24687;&#30340;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimating Conditional Mutual Information for Dynamic Feature Selection. (arXiv:2306.03301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#29305;&#24449;&#19982;&#21709;&#24212;&#21464;&#37327;&#30340;&#20114;&#20449;&#24687;&#36827;&#34892;&#20248;&#20808;&#32423;&#25490;&#24207;&#65292;&#24182;&#35774;&#35745;&#20102;&#20272;&#35745;&#20114;&#20449;&#24687;&#30340;&#21028;&#21035;&#24335;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#22810;&#39033;&#25913;&#36827;&#25514;&#26045;&#20197;&#24212;&#23545;&#26356;&#22810;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#29305;&#24449;&#36873;&#25321;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#65292;&#23427;&#36890;&#36807;&#39034;&#24207;&#26597;&#35810;&#29305;&#24449;&#20197;&#22312;&#26368;&#23567;&#30340;&#39044;&#31639;&#20869;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#20197;&#20943;&#23569;&#29305;&#24449;&#33719;&#21462;&#25104;&#26412;&#65292;&#24182;&#20026;&#39044;&#27979;&#36807;&#31243;&#25552;&#20379;&#36879;&#26126;&#24230;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36825;&#20010;&#38382;&#39064;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#20351;&#29992;&#20219;&#24847;&#29305;&#24449;&#38598;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#23398;&#20064;&#31574;&#30053;&#20197;&#30830;&#23450;&#26368;&#26377;&#20215;&#20540;&#30340;&#36873;&#25321;&#12290;&#26412;&#25991;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#26681;&#25454;&#29305;&#24449;&#19982;&#21709;&#24212;&#21464;&#37327;&#30340;&#20114;&#20449;&#24687;&#23545;&#29305;&#24449;&#36827;&#34892;&#20248;&#20808;&#32423;&#25490;&#24207;&#12290;&#20854;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#23398;&#20064;&#27492;&#36873;&#25321;&#31574;&#30053;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#30452;&#25509;&#26032;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#21028;&#21035;&#32780;&#38750;&#29983;&#25104;&#27169;&#24335;&#20272;&#35745;&#20114;&#20449;&#24687;&#12290;&#24314;&#31435;&#22312;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#20043;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20010;&#36827;&#19968;&#27493;&#30340;&#25913;&#36827;&#65306;&#20801;&#35768;&#22312;&#26679;&#26412;&#20043;&#38388;&#36827;&#34892;&#21487;&#21464;&#30340;&#29305;&#24449;&#39044;&#31639;&#12289;&#25903;&#25345;&#19981;&#21516;&#29305;&#24449;&#20043;&#38388;&#30340;&#38750;&#22343;&#21248;&#25104;&#26412;&#12289;&#32467;&#21512;&#20808;&#21069;&#30340;&#20449;&#24687;&#21644;&#25506;&#31350;&#29616;&#20195;&#26550;&#26500;&#20197;&#22788;&#29702;&#37096;&#20998;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic feature selection, where we sequentially query features to make accurate predictions with a minimal budget, is a promising paradigm to reduce feature acquisition costs and provide transparency into the prediction process. The problem is challenging, however, as it requires both making predictions with arbitrary feature sets and learning a policy to identify the most valuable selections. Here, we take an information-theoretic perspective and prioritize features based on their mutual information with the response variable. The main challenge is learning this selection policy, and we design a straightforward new modeling approach that estimates the mutual information in a discriminative rather than generative fashion. Building on our learning approach, we introduce several further improvements: allowing variable feature budgets across samples, enabling non-uniform costs between features, incorporating prior information, and exploring modern architectures to handle partial input in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#36741;&#21161;&#30340;&#32852;&#37030;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#26080;&#20154;&#26426;&#22312;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#30340;&#36712;&#36857;&#35268;&#21010;&#21644;&#21327;&#35843;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#24314;&#31435;&#29615;&#22659;&#27169;&#25311;&#27169;&#22411;&#21644;&#32852;&#37030;&#23398;&#20064;&#26469;&#24555;&#36895;&#35757;&#32451;&#26080;&#20154;&#26426;&#20195;&#29702;&#65292;&#20197;&#25910;&#38598;&#20998;&#24067;&#24335;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.02029</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#36741;&#21161;&#30340;&#32852;&#37030;&#22686;&#24378;&#23398;&#20064;&#65292;&#29992;&#20110;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#22810;&#20010;&#26080;&#20154;&#26426;&#36712;&#36857;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Model-aided Federated Reinforcement Learning for Multi-UAV Trajectory Planning in IoT Networks. (arXiv:2306.02029v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02029
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#36741;&#21161;&#30340;&#32852;&#37030;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#26080;&#20154;&#26426;&#22312;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#30340;&#36712;&#36857;&#35268;&#21010;&#21644;&#21327;&#35843;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#24314;&#31435;&#29615;&#22659;&#27169;&#25311;&#27169;&#22411;&#21644;&#32852;&#37030;&#23398;&#20064;&#26469;&#24555;&#36895;&#35757;&#32451;&#26080;&#20154;&#26426;&#20195;&#29702;&#65292;&#20197;&#25910;&#38598;&#20998;&#24067;&#24335;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#26080;&#20154;&#26426;&#22242;&#38431;&#20174;&#20998;&#24067;&#24335;&#29289;&#32852;&#32593;&#35774;&#22791;&#25910;&#38598;&#25968;&#25454;&#38656;&#35201;&#39640;&#25928;&#30340;&#36712;&#36857;&#35268;&#21010;&#21644;&#21327;&#35843;&#31639;&#27861;&#12290;&#22810;&#26234;&#33021;&#20307;&#22686;&#24378;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#26114;&#36149;&#30340;&#23454;&#38469;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36741;&#21161;&#32852;&#37030;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#21482;&#26377;&#23545;&#29615;&#22659;&#30340;&#26377;&#38480;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#21327;&#35843;&#22810;&#20010;&#26080;&#20154;&#26426;&#36827;&#34892;&#25968;&#25454;&#37319;&#38598;&#20219;&#21153;&#12290;&#35813;&#31639;&#27861;&#20132;&#26367;&#24314;&#31435;&#19968;&#20010;&#20174;&#23454;&#38469;&#27979;&#37327;&#20013;&#24471;&#21040;&#30340;&#29615;&#22659;&#27169;&#25311;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#23398;&#20064;&#26080;&#32447;&#30005;&#36890;&#36947;&#29305;&#24615;&#21644;&#20272;&#35745;&#26410;&#30693;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#20301;&#32622;&#65292;&#20197;&#21450;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#32852;&#37030;QMIX&#35757;&#32451;&#12290;&#27599;&#20010;&#26080;&#20154;&#26426;&#20195;&#29702;&#22312;&#20854;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#19968;&#20010;&#26412;&#22320;&#30340;QMIX&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#19982;&#20854;&#20182;&#20195;&#29702;&#30340;&#32852;&#37030;&#23398;&#20064;&#19981;&#26029;&#24041;&#22266;&#65292;&#21152;&#36895;&#23398;&#20064;&#36807;&#31243;&#12290;&#19982;&#26631;&#20934;&#30340;M&#31639;&#27861;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying teams of unmanned aerial vehicles (UAVs) to harvest data from distributed Internet of Things (IoT) devices requires efficient trajectory planning and coordination algorithms. Multi-agent reinforcement learning (MARL) has emerged as a solution, but requires extensive and costly real-world training data. To tackle this challenge, we propose a novel model-aided federated MARL algorithm to coordinate multiple UAVs on a data harvesting mission with only limited knowledge about the environment. The proposed algorithm alternates between building an environment simulation model from real-world measurements, specifically learning the radio channel characteristics and estimating unknown IoT device positions, and federated QMIX training in the simulated environment. Each UAV agent trains a local QMIX model in its simulated environment and continuously consolidates it through federated learning with other agents, accelerating the learning process. A performance comparison with standard M
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;GNN&#22312;&#21516;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#21644;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#26500;&#33410;&#28857;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#22312;&#21478;&#19968;&#32452;&#33410;&#28857;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21152;&#26435;&#32858;&#21512;&#25552;&#39640;GNN&#22312;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#33410;&#28857;&#19978;&#30340;&#34920;&#29616;&#65292;&#26377;&#25928;&#35299;&#20915;&#32467;&#26500;&#24046;&#24322;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01323</link><description>&lt;p&gt;
&#25581;&#31034;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#32467;&#26500;&#24046;&#24322;&#24615;&#65306;&#19968;&#20010;&#23610;&#30721;&#36866;&#29992;&#20110;&#25152;&#26377;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?. (arXiv:2306.01323v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;GNN&#22312;&#21516;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#21644;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#26500;&#33410;&#28857;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#22312;&#21478;&#19968;&#32452;&#33410;&#28857;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21152;&#26435;&#32858;&#21512;&#25552;&#39640;GNN&#22312;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#33410;&#28857;&#19978;&#30340;&#34920;&#29616;&#65292;&#26377;&#25928;&#35299;&#20915;&#32467;&#26500;&#24046;&#24322;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#35777;&#21644;&#29702;&#35770;&#35777;&#25454;&#65292;&#25903;&#25345;&#23427;&#20204;&#22312;&#25429;&#25417;&#21516;&#26500;&#21644;&#26576;&#20123;&#24322;&#26500;&#22270;&#19978;&#30340;&#32467;&#26500;&#27169;&#24335;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#23454;&#38469;&#20013;&#30340;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#37117;&#30001;&#21516;&#26500;&#21644;&#24322;&#26500;&#32467;&#26500;&#27169;&#24335;&#30340;&#28151;&#21512;&#33410;&#28857;&#32452;&#25104;&#65292;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#32467;&#26500;&#24046;&#24322;&#24615;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#19979;&#30340;&#33410;&#28857;&#65288;&#20363;&#22914;&#22312;&#24322;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#65289;&#22312;GNN&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20998;&#26512;&#20173;&#28982;&#24456;&#26377;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#65292;GNN&#22312;&#21516;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#21644;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#26500;&#33410;&#28857;&#19978;&#30340;&#34920;&#29616;&#36890;&#24120;&#26159;&#20986;&#33394;&#30340;&#65292;&#32780;&#22312;&#21478;&#19968;&#32452;&#33410;&#28857;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#34920;&#29616;&#20986;&#24615;&#33021;&#24046;&#24322;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35782;&#21035;&#20102;&#27979;&#35797;&#23637;&#31034;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#33410;&#28857;&#26102;GNN&#30340;&#25928;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;GNN&#30340;&#21152;&#26435;&#32858;&#21512;&#20197;&#36866;&#24212;&#24615;&#32467;&#26500;&#24046;&#24322;&#24615;&#30340;&#26032;&#26694;&#26550;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#32467;&#26500;&#24046;&#24322;&#24615;&#21644;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on Graph Neural Networks(GNNs) provide both empirical and theoretical evidence supporting their effectiveness in capturing structural patterns on both homophilic and certain heterophilic graphs. Notably, most real-world homophilic and heterophilic graphs are comprised of a mixture of nodes in both homophilic and heterophilic structural patterns, exhibiting a structural disparity. However, the analysis of GNN performance with respect to nodes exhibiting different structural patterns, e.g., homophilic nodes in heterophilic graphs, remains rather limited. In the present study, we provide evidence that Graph Neural Networks(GNNs) on node classification typically perform admirably on homophilic nodes within homophilic graphs and heterophilic nodes within heterophilic graphs while struggling on the opposite node set, exhibiting a performance disparity. We theoretically and empirically identify effects of GNNs on testing nodes exhibiting distinct structural patterns. We then pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20449;&#24230;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#40657;&#30418;&#27169;&#22411;&#20013;&#32622;&#20449;&#24230;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#36873;&#25321;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.19187</link><description>&lt;p&gt;
&#29983;&#25104;&#21487;&#20449;&#30340;&#25991;&#26412;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models. (arXiv:2305.19187v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20449;&#24230;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#40657;&#30418;&#27169;&#22411;&#20013;&#32622;&#20449;&#24230;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#36873;&#25321;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#19987;&#38376;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#35780;&#20272;LLMs&#29983;&#25104;&#30340;&#32467;&#26524;&#30340;&#21487;&#20449;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#30740;&#31350;&#20063;&#36739;&#23569;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#25991;&#29486;&#36890;&#24120;&#20551;&#23450;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#30333;&#30418;&#35775;&#38382;&#65292;&#36825;&#35201;&#20040;&#26159;&#30001;&#20110;&#26368;&#26032;&#30340;LLMs&#30340;&#23553;&#38381;&#28304;&#20195;&#30721;&#30340;&#24615;&#36136;&#65292;&#35201;&#20040;&#26159;&#30001;&#20110;&#35745;&#31639;&#38480;&#21046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#40657;&#30418;LLMs&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#21306;&#20998;&#20102;&#20004;&#31181;&#23494;&#20999;&#30456;&#20851;&#30340;&#27010;&#24565;: &#21482;&#19982;&#36755;&#20837;&#26377;&#20851;&#30340;&#8220;&#19981;&#30830;&#23450;&#24615;&#8221;&#21644;&#36824;&#19982;&#29983;&#25104;&#30340;&#22238;&#22797;&#26377;&#20851;&#30340;&#8220;&#32622;&#20449;&#24230;&#8221;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#20960;&#20010;&#32622;&#20449;&#24230;/&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#8220;&#36873;&#25321;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#8221;&#65292;&#20854;&#20013;&#19981;&#21487;&#38752;&#30340;&#32467;&#26524;&#21487;&#20197;&#34987;&#24573;&#30053;&#25110;&#32773;&#31227;&#20132;&#32473;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) specializing in natural language generation (NLG) have recently started exhibiting promising capabilities across a variety of domains. However, gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification for NLG. Furthermore, existing literature typically assumes white-box access to language models, which is becoming unrealistic either due to the closed-source nature of the latest LLMs or due to computational constraints. In this work, we investigate uncertainty quantification in NLG for $\textit{black-box}$ LLMs. We first differentiate two closely-related notions: $\textit{uncertainty}$, which depends only on the input, and $\textit{confidence}$, which additionally depends on the generated response. We then propose and compare several confidence/uncertainty metrics, applying them to $\textit{selective NLG}$, where unreliable results could either be ignored or yielded for further 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#24378;&#20581;Lipschitz&#36172;&#24466;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#22312;&#24378;&#25932;&#25163;&#24773;&#20917;&#19979;&#26368;&#20248;&#12290;</title><link>http://arxiv.org/abs/2305.18543</link><description>&lt;p&gt;
&#38024;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24378;&#20581;Lipschitz&#36172;&#24466;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust Lipschitz Bandits to Adversarial Corruptions. (arXiv:2305.18543v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#24378;&#20581;Lipschitz&#36172;&#24466;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#22312;&#24378;&#25932;&#25163;&#24773;&#20917;&#19979;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lipschitz&#36172;&#24466;&#31639;&#27861;&#26159;&#19968;&#31181;&#22788;&#29702;&#23450;&#20041;&#22312;&#24230;&#37327;&#31354;&#38388;&#19978;&#30340;&#36830;&#32493;&#33218;&#38598;&#30340;&#38543;&#26426;&#36172;&#24466;&#31639;&#27861;&#30340;&#21464;&#20307;&#65292;&#20854;&#20013;&#22870;&#21169;&#20989;&#25968;&#21463;&#21040;Lipschitz&#32422;&#26463;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;Lipschitz&#36172;&#24466;&#38382;&#39064;&#65292;&#21363;&#22312;&#23545;&#25239;&#24615;&#30772;&#22351;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#36866;&#24212;&#25932;&#25163;&#23558;&#38543;&#26426;&#22870;&#21169;&#25439;&#22351;&#21040;&#24635;&#39044;&#31639; $C$&#12290; &#39044;&#31639;&#36890;&#36807;&#26102;&#38388;&#36328;&#24230; $T$ &#20013;&#30340;&#30772;&#22351;&#27700;&#24179;&#20043;&#21644;&#26469;&#34913;&#37327;&#12290; &#25105;&#20204;&#32771;&#34385;&#24369;&#21644;&#24378;&#25932;&#25163;&#65292;&#20854;&#20013;&#24369;&#25932;&#25163;&#22312;&#25915;&#20987;&#20043;&#21069;&#19981;&#30693;&#36947;&#24403;&#21069;&#30340;&#34892;&#21160;&#65292;&#32780;&#24378;&#25932;&#25163;&#21487;&#20197;&#35266;&#23519;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#31532;&#19968;&#34892;&#24378;&#20581;Lipschitz&#36172;&#24466;&#31639;&#27861;&#65292;&#22312;&#20004;&#31181;&#31867;&#22411;&#30340;&#25932;&#25163;&#19979;&#65292;&#29978;&#33267;&#22312;&#25439;&#22351;&#24635;&#39044;&#31639; $C$ &#26410;&#21521;&#20195;&#29702;&#25259;&#38706;&#30340;&#24773;&#20917;&#19979;&#65292;&#22343;&#33021;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;&#25105;&#20204;&#22312;&#27599;&#31181;&#31867;&#22411;&#30340;&#25932;&#25163;&#19979;&#25552;&#20379;&#19979;&#38480;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#24378;&#31867;&#22411;&#19979;&#26159;&#26368;&#20248;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#20197;&#35828;&#26126;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lipschitz bandit is a variant of stochastic bandits that deals with a continuous arm set defined on a metric space, where the reward function is subject to a Lipschitz constraint. In this paper, we introduce a new problem of Lipschitz bandits in the presence of adversarial corruptions where an adaptive adversary corrupts the stochastic rewards up to a total budget $C$. The budget is measured by the sum of corruption levels across the time horizon $T$. We consider both weak and strong adversaries, where the weak adversary is unaware of the current action before the attack, while the strong one can observe it. Our work presents the first line of robust Lipschitz bandit algorithms that can achieve sub-linear regret under both types of adversary, even when the total budget of corruption $C$ is unrevealed to the agent. We provide a lower bound under each type of adversary, and show that our algorithm is optimal under the strong case. Finally, we conduct experiments to illustrate the effecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#27169;&#24577;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;VAST&#21450;&#20854;&#25968;&#25454;&#38598;VAST-27M&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#24863;&#30693;&#21644;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#27169;&#24577;&#65292;&#36890;&#36807;&#33258;&#21160;&#38598;&#25104;&#22810;&#27169;&#24577;&#23383;&#24149;&#21644;&#36164;&#28304;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#25991;&#26412;&#20851;&#32852;&#30340;&#22810;&#31181;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.18500</link><description>&lt;p&gt;
VAST&#65306;&#19968;&#31181;&#35270;&#21548;&#23383;&#24149;&#25991;&#26412;&#20840;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#19982;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset. (arXiv:2305.18500v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#27169;&#24577;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;VAST&#21450;&#20854;&#25968;&#25454;&#38598;VAST-27M&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#24863;&#30693;&#21644;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#27169;&#24577;&#65292;&#36890;&#36807;&#33258;&#21160;&#38598;&#25104;&#22810;&#27169;&#24577;&#23383;&#24149;&#21644;&#36164;&#28304;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#25991;&#26412;&#20851;&#32852;&#30340;&#22810;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#23436;&#20840;&#25506;&#32034;&#20102;&#35270;&#35273;&#21644;&#25991;&#26412;&#65292;&#32780;&#20854;&#20182;&#27169;&#24577;&#65292;&#22914;&#35270;&#39057;&#20013;&#30340;&#38899;&#39057;&#21644;&#23383;&#24149;&#65292;&#21364;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#33258;&#21160;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#20840;&#27169;&#24577;&#35270;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;VAST-27M&#65292;&#24314;&#31435;&#22810;&#27169;&#24577;&#35270;&#39057;&#36712;&#36857;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#65292;&#24182;&#19982;&#25991;&#26412;&#36827;&#34892;&#20851;&#32852;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;2700&#19975;&#20010;&#24320;&#25918;&#39046;&#22495;&#35270;&#39057;&#29255;&#27573;&#65292;&#24182;&#20998;&#21035;&#35757;&#32451;&#35270;&#35273;&#21644;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#22120;&#20197;&#29983;&#25104;&#35270;&#35273;&#21644;&#38899;&#39057;&#23383;&#24149;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#29616;&#26377;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;&#29983;&#25104;&#30340;&#23383;&#24149;&#12289;&#23383;&#24149;&#21644;&#25351;&#23548;&#25552;&#31034;&#38598;&#25104;&#21040;&#20840;&#27169;&#24577;&#23383;&#24149;&#20013;&#12290;&#22522;&#20110;&#25552;&#20986;&#30340;VAST-27M&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#20840;&#27169;&#24577;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;VAST&#65292;&#23427;&#21487;&#20197;&#24863;&#30693;&#21644;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#27169;&#24577;&#65292;&#24182;&#26356;&#22909;&#22320;&#25903;&#25345;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision and text have been fully explored in contemporary video-text foundational models, while other modalities such as audio and subtitles in videos have not received sufficient attention. In this paper, we resort to establish connections between multi-modality video tracks, including Vision, Audio, and Subtitle, and Text by exploring an automatically generated large-scale omni-modality video caption dataset called VAST-27M. Specifically, we first collect 27 million open-domain video clips and separately train a vision and an audio captioner to generate vision and audio captions. Then, we employ an off-the-shelf Large Language Model (LLM) to integrate the generated captions, together with subtitles and instructional prompts into omni-modality captions. Based on the proposed VAST-27M dataset, we train an omni-modality video-text foundational model named VAST, which can perceive and process vision, audio, and subtitle modalities from video, and better support various tasks including vis
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#20855;&#26377;&#20020;&#30028;&#23485;&#24230;&#30340;Leaky-ReLU&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#32039;&#33268;&#22495;K&#19978;&#23454;&#29616;$L^p(K,\mathbb{R}^{d_y})$&#30340;UAP&#65292;&#32780;&#26412;&#25991;&#32473;&#20986;&#30340;&#26368;&#23567;&#23485;&#24230;$w_{\min}=\max(d_x+1,d_y)+1_{d_y=d_x+1}$&#21017;&#36866;&#29992;&#20110;&#20989;&#25968;&#31867;$C(K,\mathbb{R}^{d_y})$&#65292;&#32771;&#34385;&#21040;&#36755;&#20986;&#32500;&#24230;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.18460</link><description>&lt;p&gt;
Leaky-ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#22343;&#21248;&#36890;&#29992;&#36924;&#36817;&#20013;&#30340;&#26368;&#23567;&#23485;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Minimum Width of Leaky-ReLU Neural Networks for Uniform Universal Approximation. (arXiv:2305.18460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18460
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#20855;&#26377;&#20020;&#30028;&#23485;&#24230;&#30340;Leaky-ReLU&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#32039;&#33268;&#22495;K&#19978;&#23454;&#29616;$L^p(K,\mathbb{R}^{d_y})$&#30340;UAP&#65292;&#32780;&#26412;&#25991;&#32473;&#20986;&#30340;&#26368;&#23567;&#23485;&#24230;$w_{\min}=\max(d_x+1,d_y)+1_{d_y=d_x+1}$&#21017;&#36866;&#29992;&#20110;&#20989;&#25968;&#31867;$C(K,\mathbb{R}^{d_y})$&#65292;&#32771;&#34385;&#21040;&#36755;&#20986;&#32500;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#65288;UAP&#65289;&#30340;&#30740;&#31350;&#21382;&#21490;&#24736;&#20037;&#12290;&#24403;&#32593;&#32476;&#23485;&#24230;&#19981;&#21463;&#38480;&#21046;&#26102;&#65292;&#21482;&#38656;&#35201;&#19968;&#20010;&#38544;&#34255;&#23618;&#21363;&#21487;&#36827;&#34892;UAP&#12290;&#30456;&#21453;&#65292;&#24403;&#28145;&#24230;&#19981;&#21463;&#38480;&#21046;&#26102;&#65292;UAP&#30340;&#23485;&#24230;&#38656;&#35201;&#19981;&#23567;&#20110;&#20020;&#30028;&#23485;&#24230;$w^*_{\min}=\max(d_x,d_y)$, &#20854;&#20013;$d_x$&#21644;$d_y$&#20998;&#21035;&#26159;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#32500;&#24230;&#12290;&#26368;&#36817;&#65292;\cite{cai2022achieve}&#34920;&#26126;&#65292;&#20855;&#26377;&#36825;&#31181;&#20020;&#30028;&#23485;&#24230;&#30340;Leaky-ReLU&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#32039;&#33268;&#22495;$K$&#19978;&#23454;&#29616;$L^p$&#20989;&#25968;&#30340;UAP&#65292;&#21363;$L^p(K,\mathbb{R}^{d_y})$&#30340;UAP&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20989;&#25968;&#31867;$C(K,\mathbb{R}^{d_y})$&#30340;&#22343;&#21248;UAP&#65292;&#24182;&#32473;&#20986;&#20102;Leaky-ReLU NN&#30340;&#30830;&#20999;&#26368;&#23567;&#23485;&#24230;&#65292;&#20026;$w_{\min}=\max(d_x+1,d_y)+1_{d_y=d_x+1}$&#65292;&#20854;&#20013;&#28041;&#21450;&#36755;&#20986;&#32500;&#24230;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#24471;&#21040;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;lift-flow-discretization&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#22343;&#21248;UAP&#19982;&#25299;&#25169;&#29702;&#35770;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of universal approximation properties (UAP) for neural networks (NN) has a long history. When the network width is unlimited, only a single hidden layer is sufficient for UAP. In contrast, when the depth is unlimited, the width for UAP needs to be not less than the critical width $w^*_{\min}=\max(d_x,d_y)$, where $d_x$ and $d_y$ are the dimensions of the input and output, respectively. Recently, \cite{cai2022achieve} shows that a leaky-ReLU NN with this critical width can achieve UAP for $L^p$ functions on a compact domain $K$, \emph{i.e.,} the UAP for $L^p(K,\mathbb{R}^{d_y})$. This paper examines a uniform UAP for the function class $C(K,\mathbb{R}^{d_y})$ and gives the exact minimum width of the leaky-ReLU NN as $w_{\min}=\max(d_x+1,d_y)+1_{d_y=d_x+1}$, which involves the effects of the output dimensions. To obtain this result, we propose a novel lift-flow-discretization approach that shows that the uniform UAP has a deep connection with topological theory.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;StEik&#26469;&#31283;&#23450;&#31070;&#32463;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24418;&#29366;&#32454;&#33410;&#34920;&#31034;&#21644;&#20248;&#21270;&#25928;&#26524;&#65292;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;INR&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18414</link><description>&lt;p&gt;
StEik: &#31283;&#23450;&#31070;&#32463;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#21644;&#26356;&#32454;&#33268;&#24418;&#29366;&#34920;&#31034;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
StEik: Stabilizing the Optimization of Neural Signed Distance Functions and Finer Shape Representation. (arXiv:2305.18414v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;StEik&#26469;&#31283;&#23450;&#31070;&#32463;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24418;&#29366;&#32454;&#33410;&#34920;&#31034;&#21644;&#20248;&#21270;&#25928;&#26524;&#65292;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;INR&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;(INR)&#24418;&#29366;&#30340;&#26032;&#35265;&#35299;&#21644;&#26032;&#33539;&#24335;&#65288;StEik&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#22312;INR&#20013;&#29992;&#20110;&#26045;&#21152;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#32422;&#26463;&#30340;&#27969;&#34892;&#30340;Eikonal Loss&#30340;&#24615;&#36136;&#12290;&#25105;&#20204;&#20174;&#35299;&#26512;&#19978;&#35777;&#26126;&#65292;&#38543;&#30528;&#32593;&#32476;&#34920;&#31034;&#33021;&#21147;&#30340;&#22686;&#24378;&#65292;&#20248;&#21270;&#26041;&#27861;&#22312;&#36830;&#32493;&#26497;&#38480;&#19979;&#36924;&#36817;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#65292;&#32780;PDE&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#22312;&#29616;&#26377;&#30340;&#32593;&#32476;&#20248;&#21270;&#20013;&#21487;&#33021;&#34920;&#29616;&#20026;&#37325;&#26500;&#34920;&#38754;&#30340;&#19981;&#35268;&#21017;&#24615;&#21644;/&#25110;&#25910;&#25947;&#21040;&#27425;&#20248;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#22240;&#27492;&#26080;&#27861;&#25429;&#25417;&#21040;&#31934;&#32454;&#30340;&#20960;&#20309;&#21644;&#25299;&#25169;&#32467;&#26500;&#12290;&#25105;&#20204;&#20174;&#35299;&#26512;&#19978;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20854;&#20182;&#28155;&#21152;&#21040;&#25439;&#22833;&#20013;&#30340;&#26415;&#35821;(&#24403;&#21069;&#22312;&#25991;&#29486;&#20013;&#29992;&#20110;&#20854;&#20182;&#30446;&#30340;)&#26469;&#28040;&#38500;&#36825;&#20123;&#19981;&#31283;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39033;&#21487;&#33021;&#36807;&#24230;&#35268;&#21017;&#21270;&#34920;&#38754;&#65292;&#38450;&#27490;&#32454;&#33410;&#30340;&#31934;&#32454;&#24418;&#29366;&#34920;&#31034;&#12290;&#22522;&#20110;&#36830;&#32493;&#26497;&#38480;&#30340;&#31867;&#20284;PDE&#29702;&#35770;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#21516;&#26102;&#31283;&#23450;&#20248;&#21270;&#21644;&#25429;&#25417;&#31934;&#32454;&#30340;&#24418;&#29366;&#32454;&#33410;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;3D&#24418;&#29366;&#25968;&#25454;&#38598;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;INR&#26041;&#27861;&#65292;&#22312;&#24418;&#29366;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present new insights and a novel paradigm (StEik) for learning implicit neural representations (INR) of shapes. In particular, we shed light on the popular eikonal loss used for imposing a signed distance function constraint in INR. We show analytically that as the representation power of the network increases, the optimization approaches a partial differential equation (PDE) in the continuum limit that is unstable. We show that this instability can manifest in existing network optimization, leading to irregularities in the reconstructed surface and/or convergence to sub-optimal local minima, and thus fails to capture fine geometric and topological structure. We show analytically how other terms added to the loss, currently used in the literature for other purposes, can actually eliminate these instabilities. However, such terms can over-regularize the surface, preventing the representation of fine shape detail. Based on a similar PDE theory for the continuum limit, we introduce a n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GFlowNets&#30340;&#26426;&#22120;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#35757;&#32451;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.17010</link><description>&lt;p&gt;
&#21033;&#29992;GFlowNets&#35299;&#20915;&#22270;&#24418;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Let the Flows Tell: Solving Graph Combinatorial Optimization Problems with GFlowNets. (arXiv:2305.17010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GFlowNets&#30340;&#26426;&#22120;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#35757;&#32451;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#36890;&#24120;&#26159;NP&#38590;&#39064;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;&#31934;&#30830;&#31639;&#27861;&#65292;&#36825;&#20351;&#23427;&#20204;&#25104;&#20026;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#29702;&#24819;&#39046;&#22495;&#12290;&#36825;&#20123;&#38382;&#39064;&#20013;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#38480;&#21046;&#21487;&#33021;&#20250;&#30452;&#25509;&#38459;&#30861;&#20248;&#21270;&#25110;&#37319;&#26679;&#35299;&#20915;&#26041;&#26696;&#30340;&#31354;&#38388;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;GFlowNets&#26368;&#36817;&#34987;&#21457;&#29616;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26426;&#22120;&#65292;&#21487;&#20197;&#39034;&#24207;&#22320;&#20174;&#22797;&#21512;&#38750;&#35268;&#33539;&#21270;&#23494;&#24230;&#20013;&#26377;&#25928;&#22320;&#37319;&#26679;&#65292;&#24182;&#20855;&#26377;&#22312;CO&#20013;&#20998;&#25674;&#27492;&#31867;&#35299;&#20915;&#26041;&#26696;&#25628;&#32034;&#36807;&#31243;&#20197;&#21450;&#29983;&#25104;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#20505;&#36873;&#39033;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#32452;&#21512;&#38382;&#39064;&#30340;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#24182;&#25552;&#20986;&#35757;&#32451;&#26377;&#26465;&#20214;&#30340;GFlowNets&#20174;&#35299;&#31354;&#38388;&#20013;&#37319;&#26679;&#30340;&#31574;&#30053;&#12290;&#36824;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#35757;&#32451;&#25216;&#26415;&#26469;&#21463;&#30410;&#20110;&#36828;&#31243;&#20449;&#29992;&#20998;&#37197;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#20351;&#29992;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#19981;&#21516;CO&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;GFlowNet&#31574;&#30053;&#21487;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combinatorial optimization (CO) problems are often NP-hard and thus out of reach for exact algorithms, making them a tempting domain to apply machine learning methods. The highly structured constraints in these problems can hinder either optimization or sampling directly in the solution space. On the other hand, GFlowNets have recently emerged as a powerful machinery to efficiently sample from composite unnormalized densities sequentially and have the potential to amortize such solution-searching processes in CO, as well as generate diverse solution candidates. In this paper, we design Markov decision processes (MDPs) for different combinatorial problems and propose to train conditional GFlowNets to sample from the solution space. Efficient training techniques are also developed to benefit long-range credit assignment. Through extensive experiments on a variety of different CO tasks with synthetic and realistic data, we demonstrate that GFlowNet policies can efficiently find high-quali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#21644;&#39044;&#27979;&#26694;&#26550;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#32477;&#23545;&#25512;&#29702;&#33021;&#21147;&#26469;&#25552;&#39640;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.16646</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23569;&#26679;&#26412;&#30340;&#32477;&#23545;&#25512;&#29702;&#26469;&#25552;&#39640;&#20107;&#20214;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning. (arXiv:2305.16646v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#21644;&#39044;&#27979;&#26694;&#26550;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#32477;&#23545;&#25512;&#29702;&#33021;&#21147;&#26469;&#25552;&#39640;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#25512;&#29702;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20107;&#20214;&#65292;&#24110;&#21161;&#25552;&#39640;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24314;&#27169;&#21644;&#39044;&#27979;&#26694;&#26550;&#65292;&#20854;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#32477;&#23545;&#25512;&#29702;&#20197;&#36741;&#21161;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#65306;&#20107;&#20214;&#27169;&#22411;&#22312;&#32473;&#23450;&#36807;&#21435;&#30340;&#24773;&#20917;&#19979;&#25552;&#20986;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;; &#22312;&#20960;&#20010;&#19987;&#23478;&#27880;&#37322;&#31034;&#33539;&#30340;&#25351;&#23548;&#19979;&#65292;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#20102;&#20026;&#27599;&#20010;&#25552;&#35758;&#25552;&#20379;&#21487;&#33021;&#30340;&#21407;&#22240;; &#19968;&#20010;&#25628;&#32034;&#27169;&#22359;&#25214;&#21040;&#19982;&#21407;&#22240;&#21305;&#37197;&#30340;&#20808;&#21069;&#20107;&#20214;; &#19968;&#20010;&#35780;&#20998;&#20989;&#25968;&#23398;&#20250;&#26816;&#26597;&#26816;&#32034;&#21040;&#30340;&#20107;&#20214;&#26159;&#21542;&#23454;&#38469;&#19978;&#21487;&#20197;&#23548;&#33268;&#25552;&#35758;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65288;&#20122;&#39532;&#36874;&#35780;&#35770;&#21644;GDELT&#65289;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550; - &#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147; - &#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have shown astonishing performance on a wide range of reasoning tasks. In this paper, we investigate whether they could reason about real-world events and help improve the prediction accuracy of event sequence models. We design a modeling and prediction framework where a large language model performs abductive reasoning to assist an event sequence model: the event model proposes predictions on future events given the past; instructed by a few expert-annotated demonstrations, the language model learns to suggest possible causes for each proposal; a search module finds out the previous events that match the causes; a scoring function learns to examine whether the retrieved events could actually cause the proposal. Through extensive experiments on two challenging real-world datasets (Amazon Review and GDELT), we demonstrate that our framework -- thanks to the reasoning ability of language models -- could significantly outperform the state-of-the-art event sequence mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#40657;&#30418;DRL&#25152;&#20316;&#30340;&#20915;&#31574;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#35299;&#37322;&#26694;&#26550;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16532</link><description>&lt;p&gt;
&#20351;&#29992;&#31574;&#30053;&#33976;&#39311;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#22120;&#26694;&#26550;&#35299;&#37322;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explainer Framework for Deep Reinforcement Learning Models Using Policy Distillation. (arXiv:2305.16532v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#40657;&#30418;DRL&#25152;&#20316;&#30340;&#20915;&#31574;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#35299;&#37322;&#26694;&#26550;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#35299;&#20915;&#22797;&#26434;&#25511;&#21046;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#65292;DRL&#24212;&#29992;&#21463;&#21040;&#32570;&#20047;&#24378;&#22823;&#30340;&#39564;&#35777;&#25216;&#26415;&#26469;&#20445;&#35777;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#12290;&#39564;&#35777;&#36807;&#31243;&#30340;&#20851;&#38190;&#35201;&#27714;&#20043;&#19968;&#26159;&#24320;&#21457;&#26377;&#25928;&#30340;&#25216;&#26415;&#26469;&#35299;&#37322;&#31995;&#32479;&#30340;&#21151;&#33021;&#65292;&#21363;&#20026;&#20160;&#20040;&#31995;&#32479;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#20135;&#29983;&#29305;&#23450;&#32467;&#26524;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#21453;&#20107;&#23454;&#65288;Counterfactual&#65292;CF&#65289;&#35299;&#37322;&#26041;&#27861;&#30340;&#35299;&#37322;&#26041;&#27861;&#26469;&#35299;&#20915;DRL&#20013;&#30340;&#35299;&#37322;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CF&#35299;&#37322;&#26694;&#26550;&#65292;&#26469;&#35299;&#37322;&#40657;&#30418;DRL&#25152;&#20316;&#30340;&#20915;&#31574;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#35299;&#37322;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#21644;Atari Pong&#28216;&#25103;&#39046;&#22495;&#36827;&#34892;&#20102;&#20960;&#39033;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#29983;&#25104;&#20102;&#21512;&#29702;&#21644;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#19982;&#21407;&#22987;DRL&#27169;&#22411;&#30456;&#27604;&#30340;&#39640;&#24230;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (DRL) has demonstrated promising capability in solving complex control problems. However, DRL applications in safety-critical systems are hindered by the inherent lack of robust verification techniques to assure their performance in such applications. One of the key requirements of the verification process is the development of effective techniques to explain the system functionality, i.e., why the system produces specific results in given circumstances. Recently, interpretation methods based on the Counterfactual (CF) explanation approach have been proposed to address the problem of explanation in DRLs. This paper proposes a novel CF explanation framework to explain the decisions made by a black-box DRL. To evaluate the efficacy of the proposed explanation framework, we carried out several experiments in the domains of automated driving systems and Atari Pong game. Our analysis demonstrates that the proposed framework generates plausible and meaningful expl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#21517;&#20026;"Commotions"&#30340;&#26032;&#22411;&#35748;&#30693;&#21512;&#29702;&#27169;&#22411;&#65292;&#22312;&#39044;&#27979;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#23637;&#31034;&#20102;&#20854;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.15187</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#35748;&#30693;&#29702;&#35770;&#30340;&#27169;&#22411;&#39044;&#27979;&#20132;&#36890;&#20013;&#30340;&#20154;&#31867;&#34892;&#20026;&#65306;&#20197;&#26696;&#20363;&#30740;&#31350;&#20026;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Using Models Based on Cognitive Theory to Predict Human Behavior in Traffic: A Case Study. (arXiv:2305.15187v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#21517;&#20026;"Commotions"&#30340;&#26032;&#22411;&#35748;&#30693;&#21512;&#29702;&#27169;&#22411;&#65292;&#22312;&#39044;&#27979;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#23637;&#31034;&#20102;&#20854;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#21457;&#23637;&#26377;&#28508;&#21147;&#24443;&#24213;&#25913;&#21464;&#20132;&#36890;&#65292;&#20294;&#30446;&#21069;&#26080;&#27861;&#30830;&#20445;&#23433;&#20840;&#21644;&#39640;&#25928;&#30340;&#39550;&#39542;&#39118;&#26684;&#12290;&#21487;&#38752;&#30340;&#39044;&#27979;&#20154;&#31867;&#34892;&#20026;&#30340;&#27169;&#22411;&#23545;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#25968;&#25454;&#39537;&#21160;&#30340;&#27169;&#22411;&#36890;&#24120;&#29992;&#20110;&#27492;&#30446;&#30340;&#65292;&#20294;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#36793;&#30028;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#23384;&#22312;&#39118;&#38505;&#12290;&#36825;&#24341;&#21457;&#20102;&#23545;&#25972;&#21512;&#35748;&#30693;&#29702;&#35770;&#30340;&#27169;&#22411;&#30340;&#20852;&#36259;&#65292;&#20294;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#26159;&#20026;&#35299;&#37322;&#24615;&#30446;&#30340;&#32780;&#24320;&#21457;&#30340;&#65292;&#22240;&#27492;&#36825;&#31181;&#26041;&#27861;&#22312;&#34892;&#20026;&#39044;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23578;&#26410;&#32463;&#36807;&#27979;&#35797;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;"Commotions"&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#24341;&#20837;&#20102;&#26368;&#26032;&#30340;&#20154;&#31867;&#24863;&#30693;&#12289;&#20915;&#31574;&#21644;&#36816;&#21160;&#25511;&#21046;&#29702;&#35770;&#30340;&#35748;&#30693;&#21512;&#29702;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20132;&#36890;&#20013;&#30340;&#20154;&#31867;&#34892;&#20026;&#65292;&#21253;&#25324;&#36710;&#36947;&#21464;&#26356;&#21644;&#20132;&#21449;&#36335;&#21475;&#31561;&#35768;&#22810;&#37325;&#35201;&#30340;&#20132;&#36890;&#20114;&#21160;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#33021;&#22815;&#19982;&#29978;&#33267;&#36229;&#36234;&#24050;&#26377;&#27169;&#22411;&#31454;&#20105;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of automated vehicles has the potential to revolutionize transportation, but they are currently unable to ensure a safe and time-efficient driving style. Reliable models predicting human behavior are essential for overcoming this issue. While data-driven models are commonly used to this end, they can be vulnerable in safety-critical edge cases. This has led to an interest in models incorporating cognitive theory, but as such models are commonly developed for explanatory purposes, this approach's effectiveness in behavior prediction has remained largely untested so far. In this article, we investigate the usefulness of the \emph{Commotions} model -- a novel cognitively plausible model incorporating the latest theories of human perception, decision-making, and motor control -- for predicting human behavior in gap acceptance scenarios, which entail many important traffic interactions such as lane changes and intersections. We show that this model can compete with or even o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.14259</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#25991;&#29486;&#30340;&#35821;&#22659;&#21270;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#65288;LBD&#65289;&#26088;&#22312;&#36890;&#36807;&#25366;&#25496;&#35770;&#25991;&#24182;&#29983;&#25104;&#20551;&#35774;&#26469;&#21457;&#29616;&#26032;&#30340;&#31185;&#23398;&#30693;&#35782;&#12290;&#26631;&#20934;&#30340;LBD&#20165;&#38480;&#20110;&#39044;&#27979;&#31163;&#25955;&#27010;&#24565;&#20043;&#38388;&#30340;&#20004;&#20004;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#21644;&#30142;&#30149;&#30340;&#20851;&#32852;&#65289;&#12290;LBD&#36824;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#23454;&#39564;&#35774;&#32622;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#35780;&#20272;&#30340;&#29305;&#23450;&#24739;&#32773;&#32676;&#20307;&#65289;&#21644;&#20154;&#31867;&#31185;&#23398;&#23478;&#32771;&#34385;&#30340;&#32972;&#26223;&#30693;&#35782;&#21644;&#21160;&#26426;&#65288;&#20363;&#22914;&#65292;&#25214;&#21040;&#27809;&#26377;&#29305;&#23450;&#21103;&#20316;&#29992;&#30340;&#33647;&#29289;&#20505;&#36873;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#21270;LBD&#65288;C-LBD&#65289;&#34920;&#36848;&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65306;&#20197;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31185;&#23398;&#20551;&#35774;&#65292;&#21516;&#26102;&#23558;&#23427;&#20204;&#32852;&#31995;&#21040;&#25511;&#21046;&#20551;&#35774;&#25628;&#32034;&#31354;&#38388;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#26694;&#26550;&#65292;&#20351;&#29992;&#33719;&#24471;&#30340;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#24322;&#26500;&#32593;&#32476;&#20013;&#30340;&#8220;&#28789;&#24863;&#8221;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#20174;&#35770;&#25991;&#20013;&#27966;&#29983;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#20542;&#21521;&#20110;&#29983;&#25104;&#20855;&#26377;&#21019;&#26032;&#24615;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Literature-Based Discovery (LBD) aims to discover new scientific knowledge by mining papers and generating hypotheses. Standard LBD is limited to predicting pairwise relations between discrete concepts (e.g., drug-disease links). LBD also ignores critical contexts like experimental settings (e.g., a specific patient population where a drug is evaluated) and background knowledge and motivations that human scientists consider (e.g., to find a drug candidate without specific side effects). We address these limitations with a novel formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in natural language, while grounding them in a context that controls the hypothesis search space. We present a modeling framework using retrieval of ``inspirations'' from a heterogeneous network of citations and knowledge graph relations, and create a new dataset derived from papers. Our evaluations with powerful large language models (LLMs) reveal that GPT4 tends to generate ideas with 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#35843;&#26597;&#25506;&#35752;&#20102;&#22312;&#20196;&#29260;&#21361;&#26426;&#19979;&#25193;&#23637;LLM&#30340;&#37325;&#22797;&#39044;&#35757;&#32451;&#25968;&#25454;&#26041;&#27861;&#65292;&#21457;&#29616;&#27169;&#22411;&#23481;&#26131;&#36807;&#25311;&#21512;&#24182;&#23548;&#33268;&#22810;&#36718;&#27425;&#24615;&#33021;&#19979;&#38477;&#65292;&#20851;&#38190;&#22240;&#32032;&#21253;&#25324;&#25968;&#25454;&#38598;&#35268;&#27169;&#12289;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#30446;&#26631;&#65292;&#32780;&#27491;&#21017;&#21270;&#25216;&#26415;&#24182;&#19981;&#33021;&#26126;&#26174;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13230</link><description>&lt;p&gt;
&#26159;&#21542;&#37325;&#22797;&#30340;&#30097;&#38382;: &#22312;&#20196;&#29260;&#21361;&#26426;&#19979;&#25193;&#23637;LLM&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis. (arXiv:2305.13230v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13230
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#35843;&#26597;&#25506;&#35752;&#20102;&#22312;&#20196;&#29260;&#21361;&#26426;&#19979;&#25193;&#23637;LLM&#30340;&#37325;&#22797;&#39044;&#35757;&#32451;&#25968;&#25454;&#26041;&#27861;&#65292;&#21457;&#29616;&#27169;&#22411;&#23481;&#26131;&#36807;&#25311;&#21512;&#24182;&#23548;&#33268;&#22810;&#36718;&#27425;&#24615;&#33021;&#19979;&#38477;&#65292;&#20851;&#38190;&#22240;&#32032;&#21253;&#25324;&#25968;&#25454;&#38598;&#35268;&#27169;&#12289;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#30446;&#26631;&#65292;&#32780;&#27491;&#21017;&#21270;&#25216;&#26415;&#24182;&#19981;&#33021;&#26126;&#26174;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#25968;&#25454;&#38598;&#35268;&#27169;&#23545;&#20110;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#38750;&#24120;&#20381;&#36182;&#20110;&#20196;&#29260;&#65292;&#24182;&#19988;&#32593;&#32476;&#19978;&#30340;&#39640;&#36136;&#37327;&#25991;&#26412;&#25968;&#25454;&#24050;&#25509;&#36817;LLMs&#30340;&#25193;&#23637;&#38480;&#21046;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;LLMs&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26159;&#37325;&#22797;&#39044;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#36718;&#27425;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#23454;&#35777;&#35282;&#24230;&#25506;&#35752;&#20102;&#36825;&#31181;&#26041;&#27861;&#19979;&#30340;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#37325;&#22797;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#21518;&#26524;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#23481;&#26131;&#36807;&#25311;&#21512;&#65292;&#23548;&#33268;&#22810;&#36718;&#27425;&#24615;&#33021;&#19979;&#38477;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23548;&#33268;&#22810;&#36718;&#27425;&#24615;&#33021;&#19979;&#38477;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21457;&#29616;&#25968;&#25454;&#38598;&#35268;&#27169;&#12289;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#30446;&#26631;&#26159;&#26174;&#33879;&#22240;&#32032;&#65292;&#32780;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#27169;&#22411;FLOP&#21017;&#24433;&#21709;&#36739;&#23567;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#26159;&#21542;&#21487;&#20197;&#32531;&#35299;&#22810;&#36718;&#27425;&#24615;&#33021;&#19979;&#38477;&#12290;&#22823;&#22810;&#25968;&#27491;&#21017;&#21270;&#25216;&#26415;&#24182;&#19981;&#33021;&#26126;&#26174;&#32531;&#35299;&#36825;&#31181;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has highlighted the importance of dataset size in scaling language models. However, large language models (LLMs) are notoriously token-hungry during pre-training, and high-quality text data on the web is approaching its scaling limit for LLMs. To further enhance LLMs, a straightforward approach is to repeat the pre-training data for additional epochs. In this study, we empirically investigate three key aspects under this approach. First, we explore the consequences of repeating pre-training data, revealing that the model is susceptible to overfitting, leading to multi-epoch degradation. Second, we examine the key factors contributing to multi-epoch degradation, finding that significant factors include dataset size, model parameters, and training objectives, while less influential factors consist of dataset quality and model FLOPs. Finally, we explore whether widely used regularization can alleviate multi-epoch degradation. Most regularization techniques do not yield sig
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20844;&#24179;&#28151;&#28102;&#23450;&#21521;&#26799;&#24230;&#25628;&#32034;&#30340;&#35856;&#27874;&#35780;&#20272;&#26041;&#27861;RobustFair&#65292;&#21487;&#20197;&#35782;&#21035;&#19982;&#34394;&#20551;&#20844;&#24179;&#30456;&#32467;&#21512;&#30340;&#40065;&#26834;&#24615;&#32570;&#38519;&#65292;&#25552;&#39640;DNN&#30340;&#40065;&#26834;&#24615;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10906</link><description>&lt;p&gt;
RobustFair: &#36890;&#36807;&#20844;&#24179;&#28151;&#28102;&#23450;&#21521;&#26799;&#24230;&#25628;&#32034;&#30340;&#25932;&#23545;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RobustFair: Adversarial Evaluation through Fairness Confusion Directed Gradient Search. (arXiv:2305.10906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20844;&#24179;&#28151;&#28102;&#23450;&#21521;&#26799;&#24230;&#25628;&#32034;&#30340;&#35856;&#27874;&#35780;&#20272;&#26041;&#27861;RobustFair&#65292;&#21487;&#20197;&#35782;&#21035;&#19982;&#34394;&#20551;&#20844;&#24179;&#30456;&#32467;&#21512;&#30340;&#40065;&#26834;&#24615;&#32570;&#38519;&#65292;&#25552;&#39640;DNN&#30340;&#40065;&#26834;&#24615;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNN&#30340;&#21487;&#20449;&#24230;&#32463;&#24120;&#21463;&#21040;&#36731;&#24494;&#25932;&#23545;&#25200;&#21160;&#30340;&#25361;&#25112;&#65292;&#36825;&#19981;&#20165;&#20250;&#30772;&#22351;&#39044;&#27979;&#20934;&#30830;&#24615;&#65288;&#40065;&#26834;&#24615;&#65289;&#32780;&#19988;&#21487;&#33021;&#20026;&#31867;&#20284;&#30340;&#36755;&#20837;&#23548;&#33268;&#26377;&#20559;&#39044;&#27979;&#65288;&#20010;&#20307;&#20844;&#24179;&#24615;&#65289;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#20934;&#30830;&#20844;&#27491;&#24230;&#26469;&#24378;&#21046;&#23454;&#26045;&#20934;&#30830;&#24615;&#21644;&#20010;&#20307;&#20844;&#24179;&#20043;&#38388;&#30340;&#35856;&#21644;&#24179;&#34913;&#12290;&#23427;&#24341;&#20837;&#20102;&#20844;&#24179;&#28151;&#28102;&#30697;&#38453;&#30340;&#27010;&#24565;&#26469;&#23558;&#39044;&#27979;&#20998;&#31867;&#20026;&#30495;&#27491;&#20844;&#24179;&#12289;&#30495;&#27491;&#26377;&#20559;&#12289;&#20551;&#27491;&#20844;&#24179;&#21644;&#20551;&#26377;&#20559;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35856;&#27874;&#35780;&#20272;&#26041;&#27861;RobustFair&#65292;&#20351;&#29992;&#36890;&#36807;&#20844;&#24179;&#28151;&#28102;&#23450;&#21521;&#26799;&#24230;&#25628;&#32034;&#21046;&#20316;&#30340;&#25932;&#23545;&#25200;&#21160;&#65292;&#23545;DNN&#30340;&#20934;&#30830;&#20844;&#27491;&#24615;&#36827;&#34892;&#35780;&#20272;&#12290;&#36890;&#36807;&#20351;&#29992;Taylor&#23637;&#24320;&#26469;&#36817;&#20284;&#25932;&#23545;&#23454;&#20363;&#30340;&#22522;&#26412;&#30495;&#23454;&#24615;&#65292;RobustFair&#21487;&#20197;&#29305;&#21035;&#35782;&#21035;&#19982;&#34394;&#20551;&#20844;&#24179;&#32416;&#32544;&#22312;&#19968;&#36215;&#30340;&#40065;&#26834;&#24615;&#32570;&#38519;&#65292;&#36825;&#36890;&#24120;&#22312;&#40065;&#26834;&#24615;&#35780;&#20272;&#20013;&#38590;&#20197;&#25417;&#25720;&#65292;&#22312;&#20010;&#20307;&#20844;&#24179;&#35780;&#20272;&#20013;&#32570;&#22833;&#12290;RobustFair&#21487;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The trustworthiness of DNNs is often challenged by their vulnerability to minor adversarial perturbations, which may not only undermine prediction accuracy (robustness) but also cause biased predictions for similar inputs (individual fairness). Accurate fairness has been recently proposed to enforce a harmonic balance between accuracy and individual fairness. It induces the notion of fairness confusion matrix to categorize predictions as true fair, true biased, false fair, and false biased. This paper proposes a harmonic evaluation approach, RobustFair, for the accurate fairness of DNNs, using adversarial perturbations crafted through fairness confusion directed gradient search. By using Taylor expansions to approximate the ground truths of adversarial instances, RobustFair can particularly identify the robustness defects entangled for spurious fairness, which are often elusive in robustness evaluation, and missing in individual fairness evaluation. RobustFair can boost robustness and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#21453;&#24605;&#30340;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#24110;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#24050;&#32463;&#32534;&#30721;&#22312;&#20854;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#30340;&#30456;&#20851;&#28508;&#22312;&#30693;&#35782;&#65292;&#32780;&#19981;&#38656;&#35201;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#28155;&#21152;&#25552;&#31034;&#65292;&#24182;&#23558;&#30456;&#20851;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#36827;&#34892;&#25972;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#21644;GLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.08732</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
Knowledge Rumination for Pre-trained Language Models. (arXiv:2305.08732v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#21453;&#24605;&#30340;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#24110;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#24050;&#32463;&#32534;&#30721;&#22312;&#20854;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#30340;&#30456;&#20851;&#28508;&#22312;&#30693;&#35782;&#65292;&#32780;&#19981;&#38656;&#35201;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#28155;&#21152;&#25552;&#31034;&#65292;&#24182;&#23558;&#30456;&#20851;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#36827;&#34892;&#25972;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#21644;GLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#26222;&#36890;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21333;&#29420;&#22788;&#29702;&#30693;&#35782;&#23494;&#38598;&#22411;NLP&#20219;&#21153;&#30340;&#33021;&#21147;&#19981;&#36275;&#65292;&#22240;&#27492;&#65292;&#19968;&#20123;&#24037;&#20316;&#23581;&#35797;&#23558;&#22806;&#37096;&#30693;&#35782;&#38598;&#25104;&#21040;PLMs&#20013;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#30528;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35266;&#23519;&#21040;&#65292;PLM&#21487;&#33021;&#24050;&#32463;&#22312;&#20854;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#32534;&#30721;&#20102;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#20294;&#22312;&#24212;&#29992;&#21040;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#26102;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#21453;&#24605;&#30340;&#26032;&#33539;&#24335;&#65292;&#20197;&#24110;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#30456;&#20851;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#32780;&#19981;&#38656;&#35201;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#23427;&#20204;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#22312;PLMs&#20013;&#28155;&#21152;&#19968;&#20010;&#22914;&#8220;&#25454;&#25105;&#25152;&#30693;&#8221;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#35797;&#22270;&#22238;&#39038;&#30456;&#20851;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#27880;&#20837;&#27169;&#22411;&#20197;&#36827;&#34892;&#30693;&#35782;&#25972;&#21512;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#30693;&#35782;&#21453;&#24605;&#24212;&#29992;&#20110;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;RoBERTa&#12289;DeBERTa&#21644;GPT-3&#12290;&#22312;&#20845;&#20010;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#21644;GLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;.....
&lt;/p&gt;
&lt;p&gt;
Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus, several works have attempted to integrate external knowledge into PLMs. However, despite the promising outcome, we empirically observe that PLMs may have already encoded rich knowledge in their pre-trained parameters but fail to fully utilize them when applying them to knowledge-intensive tasks. In this paper, we propose a new paradigm dubbed Knowledge Rumination to help the pre-trained language model utilize that related latent knowledge without retrieving it from the external corpus. By simply adding a prompt like "As far as I know" to the PLMs, we try to review related latent knowledge and inject them back into the model for knowledge consolidation. We apply the proposed knowledge rumination to various language models, including RoBERTa, DeBERTa, and GPT-3. Experimental results on six commonsense reasoning tasks and GLUE benchmarks dem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#36890;&#29992;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#65292;&#20171;&#32461;&#20102;&#21160;&#21147;&#23398;&#24179;&#22343;&#22330;&#29702;&#35770;&#21450;&#20854;&#25968;&#20540;&#23454;&#29616;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#29289;&#29702;&#19978;&#36879;&#26126;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#25506;&#32034;&#32593;&#32476;&#30340;&#38598;&#20307;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2305.08459</link><description>&lt;p&gt;
&#36890;&#29992;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#23398;&#24179;&#22343;&#22330;&#29702;&#35770;&#20837;&#38376;
&lt;/p&gt;
&lt;p&gt;
Introduction to dynamical mean-field theory of generic random neural networks. (arXiv:2305.08459v2 [cond-mat.dis-nn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#36890;&#29992;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#65292;&#20171;&#32461;&#20102;&#21160;&#21147;&#23398;&#24179;&#22343;&#22330;&#29702;&#35770;&#21450;&#20854;&#25968;&#20540;&#23454;&#29616;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#29289;&#29702;&#19978;&#36879;&#26126;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#25506;&#32034;&#32593;&#32476;&#30340;&#38598;&#20307;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#21147;&#23398;&#24179;&#22343;&#22330;&#29702;&#35770;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#29289;&#29702;&#24037;&#20855;&#65292;&#29992;&#20110;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#30340;&#20856;&#22411;&#34892;&#20026;&#65292;&#20854;&#20013;&#31070;&#32463;&#20803;&#21487;&#20197;&#24490;&#29615;&#36830;&#25509;&#65292;&#25110;&#32773;&#21487;&#20197;&#22534;&#21472;&#22810;&#23618;&#31070;&#32463;&#20803;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21021;&#23398;&#32773;&#26469;&#35828;&#65292;&#24456;&#38590;&#25509;&#35302;&#21040;&#27492;&#24037;&#20855;&#21644;&#22522;&#30784;&#29289;&#29702;&#30340;&#31934;&#39635;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20197;&#36890;&#29992;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#20026;&#20363;&#65292;&#32473;&#20986;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#25945;&#32946;&#24615;&#20171;&#32461;&#65292;&#22312;&#27492;&#31867;&#32593;&#32476;&#20013;&#65292;&#31070;&#32463;&#20803;&#36890;&#36807;&#30456;&#20851;&#31361;&#35302;&#38543;&#26426;&#32780;&#23436;&#20840;&#36830;&#25509;&#65292;&#22240;&#27492;&#32593;&#32476;&#34920;&#29616;&#20986;&#20016;&#23500;&#30340;&#38598;&#20307;&#21160;&#24577;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#24212;&#29992;&#27492;&#24037;&#20855;&#30340;&#30456;&#20851;&#36807;&#21435;&#21644;&#26368;&#36817;&#37325;&#35201;&#20316;&#21697;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#29289;&#29702;&#19978;&#36879;&#26126;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21363;&#21160;&#24577;&#33108;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23548;&#20986;&#20102;&#23436;&#20840;&#30456;&#21516;&#30340;&#32467;&#26524;&#12290;&#35814;&#32454;&#20171;&#32461;&#20102;&#27714;&#35299;&#31215;&#20998;&#24494;&#20998;&#24179;&#22343;&#22330;&#26041;&#31243;&#30340;&#25968;&#20540;&#23454;&#29616;&#65292;&#20197;&#21450;&#25506;&#32034;&#27874;&#21160;&#32791;&#25955;&#23450;&#29702;&#30340;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamical mean-field theory is a powerful physics tool used to analyze the typical behavior of neural networks, where neurons can be recurrently connected, or multiple layers of neurons can be stacked. However, it is not easy for beginners to access the essence of this tool and the underlying physics. Here, we give a pedagogical introduction of this method in a particular example of generic random neural networks, where neurons are randomly and fully connected by correlated synapses and therefore the network exhibits rich emergent collective dynamics. We also review related past and recent important works applying this tool. In addition, a physically transparent and alternative method, namely the dynamical cavity method, is also introduced to derive exactly the same results. The numerical implementation of solving the integro-differential mean-field equations is also detailed, with an illustration of exploring the fluctuation dissipation theorem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#36890;&#29992;&#39640;&#25928;&#35757;&#32451;&#33539;&#24335;&#65292;&#29992;&#20110;&#22810;&#20219;&#21153;&#31070;&#32463;&#27714;&#35299;&#22120;&#30340;&#35757;&#32451;&#65292;&#36890;&#36807;&#20219;&#21153;&#24433;&#21709;&#30697;&#38453;&#36827;&#34892;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#35745;&#21010;&#65292;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#39044;&#31639;&#25110;&#30456;&#21516;&#30340;&#35757;&#32451;&#26102;&#38271;&#20869;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06361</link><description>&lt;p&gt;
&#22810;&#33218;&#36172;&#21338;&#26426;&#29992;&#20110;&#22810;&#20219;&#21153;&#31070;&#32463;&#27714;&#35299;&#22120;&#30340;&#39640;&#25928;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Efficient Training of Multi-task Neural Solver with Multi-armed Bandits. (arXiv:2305.06361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#36890;&#29992;&#39640;&#25928;&#35757;&#32451;&#33539;&#24335;&#65292;&#29992;&#20110;&#22810;&#20219;&#21153;&#31070;&#32463;&#27714;&#35299;&#22120;&#30340;&#35757;&#32451;&#65292;&#36890;&#36807;&#20219;&#21153;&#24433;&#21709;&#30697;&#38453;&#36827;&#34892;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#35745;&#21010;&#65292;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#39044;&#31639;&#25110;&#30456;&#21516;&#30340;&#35757;&#32451;&#26102;&#38271;&#20869;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22914;&#20309;&#39640;&#25928;&#22320;&#20026;&#21508;&#31181;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064; (COP) &#35757;&#32451;&#22810;&#20219;&#21153;&#31070;&#32463;&#27714;&#35299;&#22120;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#36890;&#29992;&#39640;&#25928;&#35757;&#32451;&#33539;&#24335;&#65292;&#20197;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#20219;&#21153;&#31070;&#32463;&#27714;&#35299;&#22120;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#19979;&#30340;&#22810;&#20219;&#21153;&#29702;&#35770;&#25439;&#22833;&#20998;&#35299;&#65292;&#36890;&#36807;&#19968;&#20010;&#20219;&#21153;&#24433;&#21709;&#30697;&#38453;&#36890;&#36807;&#27491;&#30830;&#30340;&#36172;&#21338;&#31639;&#27861;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#30456;&#27604;&#26631;&#20934;&#30340;&#35757;&#32451;&#35745;&#21010;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#39044;&#31639;&#25110;&#30456;&#21516;&#30340;&#35757;&#32451;&#26102;&#27573;&#20869;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#36825;&#21487;&#20197;&#20026;&#20854;&#20182;&#22810;&#20219;&#21153;&#22823;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#25552;&#20379;&#25351;&#23548;&#65292;&#27492;&#22806;&#65292;&#24433;&#21709;&#30697;&#38453;&#21487;&#20197;&#25552;&#20379;&#23398;&#20064;&#20248;&#21270;&#39046;&#22495;&#20013;&#24120;&#35265;&#23454;&#36341;&#30340;&#32463;&#39564;&#35777;&#25454;&#65292;&#20174;&#32780;&#25903;&#25345;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently training a multi-task neural solver for various combinatorial optimization problems (COPs) has been less studied so far. In this paper, we propose a general and efficient training paradigm based on multi-armed bandits to deliver a unified multi-task neural solver. To this end, we resort to the theoretical loss decomposition for multiple tasks under an encoder-decoder framework, which enables more efficient training via proper bandit task-sampling algorithms through an intra-task influence matrix. Our method achieves much higher overall performance with either limited training budgets or the same training epochs, compared to standard training schedules, which can be promising for advising efficient training of other multi-task large models. Additionally, the influence matrix can provide empirical evidence of some common practices in the area of learning to optimize, which in turn supports the validity of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21462;&#20195;&#20165;&#21463;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#65292;&#20174;&#32780;&#28040;&#38500;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#38480;&#21046;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06176</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Language Models with Generative Adversarial Feedback. (arXiv:2305.06176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21462;&#20195;&#20165;&#21463;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#65292;&#20174;&#32780;&#28040;&#38500;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#38480;&#21046;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#36755;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#30340;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;RLHF&#21463;&#21040;&#20154;&#31867;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#29983;&#20135;&#21147;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;: &#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#20195;&#26367;RLHF&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#21457;&#29616;&#34920;&#26126;&#65292;RLGAF&#21487;&#20197;&#24110;&#21161;&#23545;&#40784;LLM&#30340;&#36755;&#20986;&#65292;&#21516;&#26102;&#19981;&#20250;&#21463;&#21040;RLHF&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#20026;&#36827;&#19968;&#27493;&#33258;&#21160;&#21270;AI&#23545;&#40784;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to significantly enhance the performance of large language models (LLMs) by aligning their outputs with desired human values. However, RLHF is constrained by the expertise and productivity limitations of human evaluators. In this study, we investigate an alternative approach: Reinforcement Learning with Generative Adversarial Feedback (RLGAF) to RLHF. Our preliminary findings indicate that RLGAF can help align LLMs outputs while not suffering from the inherent restrictions of RLHF, suggesting promising avenues for further research on automating AI alignment.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861; HEER&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.05640</link><description>&lt;p&gt;
&#38754;&#21521;&#20010;&#20154;&#25110;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65306;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24212;&#29992;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Representation Learning for Person or Entity-centric Knowledge Graphs: an application in Healthcare. (arXiv:2305.05640v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861; HEER&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26159;&#19968;&#31181;&#25353;&#26412;&#20307;&#25110;&#27169;&#24335;&#32452;&#32455;&#20449;&#24687;&#30340;&#27969;&#34892;&#26041;&#24335;&#65292;&#24050;&#32463;&#22312;&#20174;&#25628;&#32034;&#21040;&#25512;&#33616;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;&#30693;&#35782;&#22270;&#35889;&#26041;&#38754;&#26377;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#30693;&#35782;&#34920;&#31034;&#20173;&#28982;&#26159;&#36328;&#34892;&#19994;&#30340;&#19968;&#20010;&#38750;&#24120;&#26840;&#25163;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#30001;&#20110;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20851;&#31995;&#12289;&#24322;&#36136;&#24615;&#12289;&#32570;&#20047;&#26631;&#20934;&#21270;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#31561;&#22240;&#32032;&#65292;&#36825;&#19968;&#20219;&#21153;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#25429;&#25417;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21517;&#20026;HEER&#65288;Healthcare Entity-Entity Representation learning&#65289;&#65292;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#12290;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;HEER&#22312;&#25913;&#21892;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) are a popular way to organise information based on ontologies or schemas and have been used across a variety of scenarios from search to recommendation. Despite advances in KGs, representing knowledge remains a non-trivial task across industries and it is especially challenging in the biomedical and healthcare domains due to complex interdependent relations between entities, heterogeneity, lack of standardization, and sparseness of data. KGs are used to discover diagnoses or prioritize genes relevant to disease, but they often rely on schemas that are not centred around a node or entity of interest, such as a person. Entity-centric KGs are relatively unexplored but hold promise in representing important facets connected to a central node and unlocking downstream tasks beyond graph traversal and reasoning, such as generating graph embeddings and training graph neural networks for a wide range of predictive tasks. This paper presents an end-to-end representation le
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861; ACDC&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#21333;&#20803;&#65292;&#20174;&#32780;&#23454;&#29616;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#33258;&#21160;&#30005;&#36335;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.14997</link><description>&lt;p&gt;
&#23454;&#29616;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#33258;&#21160;&#30005;&#36335;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Towards Automated Circuit Discovery for Mechanistic Interpretability. (arXiv:2304.14997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14997
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861; ACDC&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#21333;&#20803;&#65292;&#20174;&#32780;&#23454;&#29616;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#33258;&#21160;&#30005;&#36335;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#20498;&#25512;&#20102;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#30340;&#38750;&#24179;&#20961;&#34892;&#20026;&#12290;&#36825;&#20123;&#21457;&#29616;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#21644;&#30740;&#31350;&#32773;&#30340;&#30452;&#35273;&#65292;&#36825;&#20351;&#24471;&#24212;&#29992;&#30456;&#21516;&#30340;&#26041;&#27861;&#26469;&#20102;&#35299;&#24403;&#21069;&#27169;&#22411;&#25152;&#23637;&#31034;&#30340;&#22797;&#26434;&#34892;&#20026;&#21464;&#24471;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21457;&#29616;&#30340;&#26680;&#24515;&#24037;&#20316;&#27969;&#31243;&#38750;&#24120;&#30456;&#20284;&#12290;&#30740;&#31350;&#20154;&#21592;&#21019;&#24314;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#24230;&#37327;&#65292;&#35825;&#21457;&#25152;&#38656;&#30340;&#27169;&#22411;&#34892;&#20026;&#65292;&#23558;&#32593;&#32476;&#20998;&#20026;&#36866;&#24403;&#30340;&#25277;&#35937;&#21333;&#20803;&#65292;&#26367;&#25442;&#36825;&#20123;&#21333;&#20803;&#30340;&#28608;&#27963;&#20197;&#30830;&#23450;&#21738;&#20123;&#21442;&#19982;&#20102;&#34892;&#20026;&#65292;&#28982;&#21518;&#35299;&#37322;&#36825;&#20123;&#21333;&#20803;&#23454;&#26045;&#30340;&#21151;&#33021;&#12290;&#36890;&#36807;&#25913;&#21464;&#25968;&#25454;&#38598;&#12289;&#24230;&#37327;&#21644;&#24453;&#30740;&#31350;&#30340;&#21333;&#20803;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#29702;&#35299;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#21306;&#22495;&#30340;&#21151;&#33021;&#21644;&#23427;&#20204;&#32452;&#25104;&#30340;&#30005;&#36335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#33258;&#21160;&#30005;&#36335;&#21457;&#29616;&#65288;ACDC&#65289;&#65292;&#20197;&#33258;&#21160;&#35782;&#21035;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#21333;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in mechanistic interpretability has reverse-engineered nontrivial behaviors of transformer models. These contributions required considerable effort and researcher intuition, which makes it difficult to apply the same methods to understand the complex behavior that current models display. At their core however, the workflow for these discoveries is surprisingly similar. Researchers create a data set and metric that elicit the desired model behavior, subdivide the network into appropriate abstract units, replace activations of those units to identify which are involved in the behavior, and then interpret the functions that these units implement. By varying the data set, metric, and units under investigation, researchers can understand the functionality of each neural network region and the circuits they compose. This work proposes a novel algorithm, Automatic Circuit DisCovery (ACDC), to automate the identification of the important units in the network. Given a model's comput
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#35757;&#32451;&#25968;&#25454;&#19981;&#24179;&#34913;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65306;&#22312;&#22122;&#22768;&#31354;&#38388;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#29983;&#25104;&#31181;&#23376;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#27491;&#30830;&#29983;&#25104;&#32597;&#35265;&#30340;&#27010;&#24565;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.14530</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#31181;&#23376;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
It is all about where you start: Text-to-image generation with seed selection. (arXiv:2304.14530v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14530
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#35757;&#32451;&#25968;&#25454;&#19981;&#24179;&#34913;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65306;&#22312;&#22122;&#22768;&#31354;&#38388;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#29983;&#25104;&#31181;&#23376;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#27491;&#30830;&#29983;&#25104;&#32597;&#35265;&#30340;&#27010;&#24565;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#22312;&#26032;&#30340;&#32452;&#21512;&#21644;&#22330;&#26223;&#20013;&#21512;&#25104;&#22823;&#37327;&#30340;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#22312;&#29983;&#25104;&#19981;&#24120;&#35265;&#30340;&#27010;&#24565;&#12289;&#32597;&#35265;&#30340;&#19981;&#23547;&#24120;&#32452;&#21512;&#25110;&#32467;&#26500;&#21270;&#27010;&#24565;&#65288;&#22914;&#25163;&#25484;&#65289;&#26041;&#38754;&#26377;&#22256;&#38590;&#12290;&#23427;&#20204;&#30340;&#38480;&#21046;&#37096;&#20998;&#26159;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#38271;&#23614;&#24615;&#65306;&#32593;&#32476;&#29228;&#21462;&#30340;&#25968;&#25454;&#38598;&#20005;&#37325;&#19981;&#24179;&#34913;&#65292;&#23548;&#33268;&#27169;&#22411;&#22312;&#20998;&#24067;&#23614;&#37096;&#30340;&#27010;&#24565;&#19978;&#34920;&#29616;&#19981;&#36275;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#19981;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#22312;&#22122;&#22768;&#31354;&#38388;&#20013;&#31934;&#24515;&#36873;&#25321;&#36866;&#24403;&#30340;&#29983;&#25104;&#31181;&#23376;&#65292;&#21487;&#20197;&#27491;&#30830;&#29983;&#25104;&#32597;&#35265;&#30340;&#27010;&#24565;&#65292;&#36825;&#19968;&#25216;&#26415;&#34987;&#31216;&#20026;SeedSelect&#12290;SeedSelect&#26159;&#39640;&#25928;&#30340;&#65292;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#38382;&#39064;&#19978;&#35780;&#20272;&#20102;SeedSelect&#30340;&#25928;&#30410;&#12290;&#39318;&#20808;&#65292;&#22312;&#23569;&#26679;&#26412;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#20013;&#65292;&#25105;&#20204;&#20026;&#23569;&#26679;&#26412;&#21644;&#38271;&#23614;&#22522;&#20934;&#29983;&#25104;&#20102;&#35821;&#20041;&#27491;&#30830;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models can synthesize a large variety of concepts in new compositions and scenarios. However, they still struggle with generating uncommon concepts, rare unusual combinations, or structured concepts like hand palms. Their limitation is partly due to the long-tail nature of their training data: web-crawled data sets are strongly unbalanced, causing models to under-represent concepts from the tail of the distribution. Here we characterize the effect of unbalanced training data on text-to-image models and offer a remedy. We show that rare concepts can be correctly generated by carefully selecting suitable generation seeds in the noise space, a technique that we call SeedSelect. SeedSelect is efficient and does not require retraining the diffusion model. We evaluate the benefit of SeedSelect on a series of problems. First, in few-shot semantic data augmentation, where we generate semantically correct images for few-shot and long-tail benchmarks. We show classificati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26680;&#26041;&#27861;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#23545;&#22810;&#32452;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#21518;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#37117;&#26159;&#19968;&#31181;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.13202</link><description>&lt;p&gt;
&#26680;&#26041;&#27861;&#22312;&#31639;&#23376;&#23398;&#20064;&#20013;&#34920;&#29616;&#31454;&#20105;&#21147;
&lt;/p&gt;
&lt;p&gt;
Kernel Methods are Competitive for Operator Learning. (arXiv:2304.13202v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26680;&#26041;&#27861;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#23545;&#22810;&#32452;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#21518;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#37117;&#26159;&#19968;&#31181;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26680;&#30340;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#20808;&#39564;&#35823;&#24046;&#20998;&#26512;&#21644;&#19982;&#27969;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65288;&#22914;Deep Operator Net&#65288;DeepONet&#65289;[Lu et al.]&#21644;Fourier&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;[Li et al.]&#65289;&#30340;&#20840;&#38754;&#25968;&#23383;&#27604;&#36739;&#12290;&#25105;&#20204;&#32771;&#34385;&#30446;&#26631;&#31639;&#23376;$\mathcal{G}^\dagger:\mathcal{U}\to\mathcal{V}$&#30340;&#36755;&#20837;/&#36755;&#20986;&#31354;&#38388;&#26159;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#30340;&#24773;&#20917;&#65292;&#25968;&#25454;&#20197;&#36755;&#20837;/&#36755;&#20986;&#20989;&#25968;&#30340;&#37096;&#20998;&#35266;&#27979;$\varphi(v_i),\phi(u_i)$&#30340;&#24418;&#24335;&#20986;&#29616;&#65292;&#20854;&#20013;$v_i=\mathcal{G}^\dagger(u_i)$&#65288;$i=1,\ldots,N$&#65289;&#65292;&#27979;&#37327;&#31639;&#23376;$\varphi:\mathcal{V}\to\mathbb{R}^m$&#21644;$\phi:\mathcal{U}\to\mathbb{R}^n$&#26159;&#32447;&#24615;&#30340;&#12290;&#22312;&#20889;$\psi:\mathbb{R}^n\to\mathcal{U}$&#21644;$\chi:\mathbb{R}^m\to\mathcal{V}$&#20316;&#20026;&#19982;$\phi$&#21644;$\varphi$&#30456;&#20851;&#30340;&#26368;&#20339;&#24674;&#22797;&#26144;&#23556;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;$\bar{f}$ &#26680;&#26144;&#23556; $L^2(\mathcal{U},\mathbb{R}^n)$ &#23450;&#20041;&#19968;&#20010;$k$ &#31867;&#22411;&#30340;&#26368;&#23567;&#20108;&#20056;&#27169;&#22411;&#65292; &#28982;&#21518;&#29992; $\bar{\mathcal{G}}=\chi\circ\bar{f}\circ\psi$ &#26469;&#36817;&#20284;$\mathcal{G}^\dagger$&#12290; &#25105;&#20204;&#30340;&#20998;&#26512;&#28041;&#21450;&#22810;&#20010;&#20363;&#23376;&#65292;&#21253;&#25324;&#24120;&#35265;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#31639;&#23376;&#36817;&#20284;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#26680;&#26041;&#27861;&#37117;&#26159;&#19968;&#31181;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a general kernel-based framework for learning operators between Banach spaces along with a priori error analysis and comprehensive numerical comparisons with popular neural net (NN) approaches such as Deep Operator Net (DeepONet) [Lu et al.] and Fourier Neural Operator (FNO) [Li et al.]. We consider the setting where the input/output spaces of target operator $\mathcal{G}^\dagger\,:\, \mathcal{U}\to \mathcal{V}$ are reproducing kernel Hilbert spaces (RKHS), the data comes in the form of partial observations $\phi(u_i), \varphi(v_i)$ of input/output functions $v_i=\mathcal{G}^\dagger(u_i)$ ($i=1,\ldots,N$), and the measurement operators $\phi\,:\, \mathcal{U}\to \mathbb{R}^n$ and $\varphi\,:\, \mathcal{V} \to \mathbb{R}^m$ are linear. Writing $\psi\,:\, \mathbb{R}^n \to \mathcal{U}$ and $\chi\,:\, \mathbb{R}^m \to \mathcal{V}$ for the optimal recovery maps associated with $\phi$ and $\varphi$, we approximate $\mathcal{G}^\dagger$ with $\bar{\mathcal{G}}=\chi \circ \bar{f} \ci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;K&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#21464;&#20307;&#65292;&#21487;&#20197;&#30830;&#20445;&#26368;&#36817;&#37051;&#23621;&#30830;&#23454;&#25509;&#36817;&#26410;&#26631;&#35760;&#26679;&#26412;&#65292;&#24182;&#22312;&#36807;&#31243;&#20013;&#25214;&#21040;K&#20540;&#12290;&#19982;&#26631;&#20934;KNN&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#22312;&#23460;&#20869;&#25351;&#32441;&#23450;&#20301;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.10151</link><description>&lt;p&gt;
&#28789;&#27963;&#30340;K&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#65306;&#25512;&#23548;&#21644;&#22312;&#22522;&#20110;&#31163;&#23376;&#36801;&#31227;&#35889;&#30340;&#23460;&#20869;&#23450;&#20301;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flexible K Nearest Neighbors Classifier: Derivation and Application for Ion-mobility Spectrometry-based Indoor Localization. (arXiv:2304.10151v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;K&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#21464;&#20307;&#65292;&#21487;&#20197;&#30830;&#20445;&#26368;&#36817;&#37051;&#23621;&#30830;&#23454;&#25509;&#36817;&#26410;&#26631;&#35760;&#26679;&#26412;&#65292;&#24182;&#22312;&#36807;&#31243;&#20013;&#25214;&#21040;K&#20540;&#12290;&#19982;&#26631;&#20934;KNN&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#22312;&#23460;&#20869;&#25351;&#32441;&#23450;&#20301;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
K&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#24191;&#27867;&#24212;&#29992;&#20110;&#25351;&#32441;&#23450;&#20301;&#25110;&#21307;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#23427;&#22522;&#20110;K&#20010;&#26631;&#35760;&#26679;&#26412;&#65292;&#21363;&#26368;&#36817;&#37051;&#23621;&#30340;&#31867;&#25104;&#21592;&#36523;&#20221;&#65292;&#20915;&#23450;&#26410;&#26631;&#35760;&#26679;&#26412;&#30340;&#31867;&#25104;&#21592;&#36523;&#20221;&#12290;K&#30340;&#36873;&#25321;&#19968;&#30452;&#26159;&#21508;&#31181;&#30740;&#31350;&#21644;&#25552;&#20986;KNN&#21464;&#20307;&#30340;&#20027;&#39064;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#21464;&#20307;&#34987;&#35777;&#26126;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#21464;&#20307;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KNN&#21464;&#20307;&#65292;&#30830;&#20445;K&#20010;&#26368;&#36817;&#37051;&#23621;&#30830;&#23454;&#25509;&#36817;&#26410;&#26631;&#35760;&#26679;&#26412;&#65292;&#24182;&#22312;&#36807;&#31243;&#20013;&#25214;&#21040;K&#20540;&#12290;&#35813;&#31639;&#27861;&#22312;&#29702;&#35770;&#24773;&#26223;&#21644;&#22522;&#20110;&#31163;&#23376;&#36801;&#31227;&#35889;&#25351;&#32441;&#30340;&#23460;&#20869;&#23450;&#20301;&#26041;&#38754;&#36827;&#34892;&#20102;&#27979;&#35797;&#21644;&#27604;&#36739;&#12290;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#31639;&#27861;&#22312;&#19982;KNN&#21516;&#26679;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#27604;KNN&#26356;&#39640;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The K Nearest Neighbors (KNN) classifier is widely used in many fields such as fingerprint-based localization or medicine. It determines the class membership of unlabelled sample based on the class memberships of the K labelled samples, the so-called nearest neighbors, that are closest to the unlabelled sample. The choice of K has been the topic of various studies and proposed KNN-variants. Yet no variant has been proven to outperform all other variants. In this paper a new KNN-variant is proposed which ensures that the K nearest neighbors are indeed close to the unlabelled sample and finds K along the way. The proposed algorithm is tested and compared to the standard KNN in theoretical scenarios and for indoor localization based on ion-mobility spectrometry fingerprints. It achieves a higher classification accuracy than the KNN in the tests, while requiring having the same computational demand.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#31579;&#36873;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#28369;&#22369;&#26131;&#21457;&#24615;&#39044;&#27979;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#31579;&#36873;&#32593;&#32476;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#25552;&#21462;&#29615;&#22659;&#22240;&#32032;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06054</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#31579;&#36873;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#28369;&#22369;&#26131;&#21457;&#24615;&#39044;&#27979;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Landslide Susceptibility Prediction Modeling Based on Self-Screening Deep Learning Model. (arXiv:2304.06054v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#31579;&#36873;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#28369;&#22369;&#26131;&#21457;&#24615;&#39044;&#27979;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#31579;&#36873;&#32593;&#32476;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#25552;&#21462;&#29615;&#22659;&#22240;&#32032;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28369;&#22369;&#26131;&#21457;&#24615;&#39044;&#27979;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#26131;&#21457;&#24615;&#24314;&#27169;&#23384;&#22312;&#19968;&#20123;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#22914;&#28369;&#22369;&#26679;&#26412;&#35823;&#24046;&#21644;&#29615;&#22659;&#22240;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#31579;&#36873;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;SGCN-LSTM&#65289;&#27169;&#22411;&#65292;&#20197;&#20811;&#26381;&#28369;&#22369;&#26131;&#21457;&#24615;&#39044;&#27979;&#20013;&#30340;&#19978;&#36848;&#38382;&#39064;&#12290;SGCN-LSTM&#27169;&#22411;&#20855;&#26377;&#24191;&#27867;&#24615;&#21644;&#33391;&#22909;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#33258;&#31579;&#36873;&#32593;&#32476;&#21487;&#20197;&#28040;&#38500;&#19968;&#23450;&#38408;&#20540;&#21306;&#38388;&#22806;&#20855;&#26377;&#22823;&#35823;&#24046;&#30340;&#28369;&#22369;&#26679;&#26412;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#31354;&#38388;&#33410;&#28857;&#21644;&#26102;&#38388;&#24207;&#21015;&#20013;&#25552;&#21462;&#29615;&#22659;&#22240;&#32032;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#27169;&#25311;&#29615;&#22659;&#22240;&#32032;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;SGCN-LSTM&#27169;&#22411;&#24212;&#29992;&#20110;&#27743;&#35199;&#30465;&#23433;&#36828;&#21439;&#30340;&#28369;&#22369;&#26131;&#21457;&#24615;&#39044;&#27979;&#65292;&#24182;&#19982;&#22810;&#20803;&#36923;&#36753;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#26497;&#31471;&#23398;&#20064;&#26426;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#31561;&#22235;&#31181;&#24120;&#29992;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;SGCN-LSTM&#27169;&#22411;&#22312;&#28369;&#22369;&#26131;&#21457;&#24615;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#24230;&#21644;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#20540;&#65292;&#34920;&#26126;&#20854;&#22312;&#28369;&#22369;&#26131;&#21457;&#24615;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Landslide susceptibility prediction has always been an important and challenging content. However, there are some uncertain problems to be solved in susceptibility modeling, such as the error of landslide samples and the complex nonlinear relationship between environmental factors. A self-screening graph convolutional network and long short-term memory network (SGCN-LSTM) is proposed int this paper to overcome the above problems in landslide susceptibility prediction. The SGCN-LSTM model has the advantages of wide width and good learning ability. The landslide samples with large errors outside the set threshold interval are eliminated by self-screening network, and the nonlinear relationship between environmental factors can be extracted from both spatial nodes and time series, so as to better simulate the nonlinear relationship between environmental factors. The SGCN-LSTM model was applied to landslide susceptibility prediction in Anyuan County, Jiangxi Province, China, and compared w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#31361;&#30772;&#23433;&#20840;&#31995;&#32479;&#21644;&#31363;&#21462;&#25935;&#24863;&#25968;&#25454;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#22312;&#39044;&#27979;&#21644;&#38450;&#27490;&#32593;&#32476;&#29359;&#32618;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#21040;&#20102;&#26368;&#36817;&#30740;&#31350;&#20013;&#25928;&#26524;&#26368;&#22909;&#30340;&#36882;&#24402;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2304.04819</link><description>&lt;p&gt;
&#32593;&#32476;&#29359;&#32618;&#39044;&#27979;&#30340;&#36827;&#23637;&#65306;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Advances in Cybercrime Prediction: A Survey of Machine, Deep, Transfer, and Adaptive Learning Techniques. (arXiv:2304.04819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#31361;&#30772;&#23433;&#20840;&#31995;&#32479;&#21644;&#31363;&#21462;&#25935;&#24863;&#25968;&#25454;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#22312;&#39044;&#27979;&#21644;&#38450;&#27490;&#32593;&#32476;&#29359;&#32618;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#21040;&#20102;&#26368;&#36817;&#30740;&#31350;&#20013;&#25928;&#26524;&#26368;&#22909;&#30340;&#36882;&#24402;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#29359;&#32618;&#26159;&#19968;&#31181;&#19981;&#26029;&#22686;&#38271;&#30340;&#23041;&#32961;&#65292;&#32618;&#29359;&#20351;&#29992;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#25216;&#26415;&#26469;&#31361;&#30772;&#23433;&#20840;&#31995;&#32479;&#24182;&#31363;&#21462;&#25935;&#24863;&#25968;&#25454;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#39044;&#27979;&#32593;&#32476;&#29359;&#32618;&#21644;&#22312;&#20854;&#21457;&#29983;&#20043;&#21069;&#38450;&#27490;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#20351;&#29992;&#19978;&#36848;&#25216;&#26415;&#39044;&#27979;&#32593;&#32476;&#29359;&#32618;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#20840;&#38754;&#35843;&#26597;&#65292;&#37325;&#28857;&#20171;&#32461;&#27599;&#31181;&#26041;&#27861;&#30456;&#20851;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;150&#22810;&#31687;&#30740;&#31350;&#25991;&#31456;&#65292;&#24182;&#35752;&#35770;&#20102;&#22823;&#32422;50&#31687;&#26368;&#36817;&#21644;&#26368;&#30456;&#20851;&#30340;&#30740;&#31350;&#25991;&#31456;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#19968;&#20123;&#32593;&#32476;&#29359;&#32618;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#28982;&#21518;&#37325;&#28857;&#20171;&#32461;&#20102;&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20363;&#22914;&#36882;&#24402;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#26816;&#27979;&#24322;&#24120;&#34892;&#20026;&#21644;&#35782;&#21035;&#28508;&#22312;&#23041;&#32961;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cybercrime is a growing threat to organizations and individuals worldwide, with criminals using increasingly sophisticated techniques to breach security systems and steal sensitive data. In recent years, machine learning, deep learning, and transfer learning techniques have emerged as promising tools for predicting cybercrime and preventing it before it occurs. This paper aims to provide a comprehensive survey of the latest advancements in cybercrime prediction using above mentioned techniques, highlighting the latest research related to each approach. For this purpose, we reviewed more than 150 research articles and discussed around 50 most recent and relevant research articles. We start the review by discussing some common methods used by cyber criminals and then focus on the latest machine learning techniques and deep learning techniques, such as recurrent and convolutional neural networks, which were effective in detecting anomalous behavior and identifying potential threats. We al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#21644;&#24230;&#37327;&#38544;&#31169;&#23398;&#20064;&#22120;&#22312;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24471;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;&#65292;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#65292;&#19988;&#25193;&#23637;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#38544;&#31169;&#20998;&#26512;</title><link>http://arxiv.org/abs/2303.16372</link><description>&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#30340;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Non-Asymptotic Lower Bounds For Training Data Reconstruction. (arXiv:2303.16372v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#21644;&#24230;&#37327;&#38544;&#31169;&#23398;&#20064;&#22120;&#22312;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24471;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;&#65292;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#65292;&#19988;&#25193;&#23637;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#38544;&#31169;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19987;&#19994;&#23545;&#25163;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#26102;&#31169;&#26377;&#23398;&#20064;&#31639;&#27861;&#30340;&#35821;&#20041;&#20445;&#35777;&#24378;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23548;&#20986;&#38750;&#28176;&#36827;&#37327;&#32423;&#19979;&#30028;&#26469;&#30740;&#31350;&#20102;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21644;&#24230;&#37327;&#38544;&#31169;&#65288;mDP&#65289;&#30340;&#23398;&#20064;&#22120;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#23545;mDP&#30340;&#20998;&#26512;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#23545;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;DP-SGD&#21644;Projected Noisy SGD&#36827;&#34892;&#20102;&#24230;&#37327;&#24046;&#20998;&#38544;&#31169;&#30340;&#25193;&#23637;&#38544;&#31169;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate semantic guarantees of private learning algorithms for their resilience to training Data Reconstruction Attacks (DRAs) by informed adversaries. To this end, we derive non-asymptotic minimax lower bounds on the adversary's reconstruction error against learners that satisfy differential privacy (DP) and metric differential privacy (mDP). Furthermore, we demonstrate that our lower bound analysis for the latter also covers the high dimensional regime, wherein, the input data dimensionality may be larger than the adversary's query budget. Motivated by the theoretical improvements conferred by metric DP, we extend the privacy analysis of popular deep learning algorithms such as DP-SGD and Projected Noisy SGD to cover the broader notion of metric differential privacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.15714</link><description>&lt;p&gt;
&#26174;&#24335;&#35268;&#21010;&#26377;&#21161;&#20110;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23558;&#26174;&#24335;&#35268;&#21010;&#32435;&#20837;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#23637;&#26395;&#26410;&#26469;&#30340;&#25928;&#26524;&#26469;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#20840;&#22871;&#31995;&#32479;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#31572;&#39064;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#65292;&#23613;&#31649;&#21482;&#26377;&#32422;15&#20159;&#20010;&#21442;&#25968;&#65292;&#20294;&#19982;GPT-3-davinci&#34920;&#29616;&#30456;&#24403;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#28040;&#34701;&#30740;&#31350;&#20197;&#35777;&#26126;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose a novel system that uses language models to perform multi-step logical reasoning. Our system incorporates explicit planning into its inference procedure, thus able to make more informed reasoning decisions at each step by looking ahead into their future effects. In our experiments, our full system significantly outperforms other competing systems. On a multiple-choice question answering task, our system performs competitively compared to GPT-3-davinci despite having only around 1.5B parameters. We conduct several ablation studies to demonstrate that explicit planning plays a crucial role in the system's performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21435;&#20559;&#32622;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#36890;&#36807;&#22686;&#21152;2D&#25193;&#25955;&#27169;&#22411;&#24471;&#20986;&#30340;&#20998;&#25968;&#30340;&#25130;&#26029;&#20540;&#65292;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#35270;&#35282;&#25552;&#31034;&#21644;&#29289;&#20307;&#31354;&#38388;&#25668;&#20687;&#26426;&#23039;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20266;&#24433;&#65292;&#25552;&#39640;&#30495;&#23454;&#24863;&#12290;</title><link>http://arxiv.org/abs/2303.15413</link><description>&lt;p&gt;
2D&#25193;&#25955;&#31639;&#27861;&#30340;&#21435;&#20559;&#32622;&#26041;&#27861;&#29992;&#20110;&#25991;&#26412;&#21040;3D&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation. (arXiv:2303.15413v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21435;&#20559;&#32622;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#36890;&#36807;&#22686;&#21152;2D&#25193;&#25955;&#27169;&#22411;&#24471;&#20986;&#30340;&#20998;&#25968;&#30340;&#25130;&#26029;&#20540;&#65292;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#35270;&#35282;&#25552;&#31034;&#21644;&#29289;&#20307;&#31354;&#38388;&#25668;&#20687;&#26426;&#23039;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20266;&#24433;&#65292;&#25552;&#39640;&#30495;&#23454;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#20013;&#20986;&#29616;&#30340;&#35270;&#35282;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#20063;&#31216;&#20026;Janus&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#26469;&#33258;&#20110;2D&#25193;&#25955;&#27169;&#22411;&#30340;&#22266;&#26377;&#20559;&#32622;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;3D&#23545;&#35937;&#19981;&#30495;&#23454;&#12290;&#36890;&#36807;&#23545;&#20854;&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21435;&#38500;&#20559;&#32622;&#20197;&#23454;&#29616;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#30340;&#40065;&#26834;&#24615;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#21483;&#20570;score debiasing&#65292;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;2D&#25193;&#25955;&#27169;&#22411;&#24471;&#20986;&#30340;&#20998;&#25968;&#30340;&#25130;&#26029;&#20540;&#65292;&#26469;&#36798;&#21040;&#21435;&#38500;&#20559;&#32622;&#30340;&#25928;&#26524;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#21483;&#20570;prompt debiasing&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30830;&#23450;&#29992;&#25143;&#25552;&#31034;&#21644;&#35270;&#35282;&#25552;&#31034;&#20043;&#38388;&#30340;&#30683;&#30462;&#35789;&#35821;&#65292;&#24182;&#35843;&#25972;&#35270;&#35282;&#25552;&#31034;&#21644;&#29289;&#20307;&#31354;&#38388;&#25668;&#20687;&#26426;&#23039;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26174;&#33879;&#20943;&#23569;&#20266;&#24433;&#65292;&#25552;&#39640;&#20102;&#30495;&#23454;&#24863;&#65292;&#24182;&#22312;&#36136;&#37327;&#19982;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The view inconsistency problem in score-distilling text-to-3D generation, also known as the Janus problem, arises from the intrinsic bias of 2D diffusion models, which leads to the unrealistic generation of 3D objects. In this work, we explore score-distilling text-to-3D generation and identify the main causes of the Janus problem. Based on these findings, we propose two approaches to debias the score-distillation frameworks for robust text-to-3D generation. Our first approach, called score debiasing, involves gradually increasing the truncation value for the score estimated by 2D diffusion models throughout the optimization process. Our second approach, called prompt debiasing, identifies conflicting words between user prompts and view prompts utilizing a language model and adjusts the discrepancy between view prompts and object-space camera poses. Our experimental results show that our methods improve realism by significantly reducing artifacts and achieve a good trade-off between fa
&lt;/p&gt;</description></item><item><title>&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#20998;&#23376;&#32467;&#26500;&#30340;&#25972;&#20307;&#24615;&#21644;&#25163;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.11593</link><description>&lt;p&gt;
&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#25163;&#24615;&#26102;&#23384;&#22312;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Difficulty in learning chirality for Transformer fed with SMILES. (arXiv:2303.11593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11593
&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#20998;&#23376;&#32467;&#26500;&#30340;&#25972;&#20307;&#24615;&#21644;&#25163;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#23545;&#26497;&#20854;&#22810;&#26679;&#30340;&#20998;&#23376;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#25551;&#36848;&#31526;&#29983;&#25104;&#24050;&#32463;&#24471;&#21040;&#20102;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#24212;&#29992;&#20110;SMILES&#65292;&#21363;&#20998;&#23376;&#32467;&#26500;&#30340;&#25991;&#23383;&#34920;&#31034;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#29702;&#35299;&#21270;&#23398;&#32467;&#26500;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#31181;&#20195;&#34920;&#24615;&#30340;NLP&#27169;&#22411;&#8212;&#8212;Transformer&#65292;&#22312;&#23398;&#20064;SMILES&#21644;&#21270;&#23398;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;Transformer&#24555;&#36895;&#23398;&#20064;&#20998;&#23376;&#30340;&#37096;&#20998;&#32467;&#26500;&#65292;&#20294;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#25165;&#33021;&#29702;&#35299;&#25972;&#20307;&#32467;&#26500;&#12290;&#19982;&#20043;&#19968;&#33268;&#30340;&#26159;&#65292;&#22312;&#19981;&#21516;&#30340;&#23398;&#20064;&#27493;&#39588;&#20013;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;Transformer&#38656;&#35201;&#29305;&#21035;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#25165;&#33021;&#23398;&#20064;&#25163;&#24615;&#65292;&#24182;&#19988;&#26377;&#26102;&#20250;&#20986;&#29616;&#20302;&#32763;&#35793;&#20934;&#30830;&#29575;&#30340;&#20572;&#28382;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen development of descriptor generation based on representation learning of extremely diverse molecules, especially those that apply natural language processing (NLP) models to SMILES, a literal representation of molecular structure. However, little research has been done on how these models understand chemical structure. To address this, we investigated the relationship between the learning progress of SMILES and chemical structure using a representative NLP model, the Transformer. The results suggest that while the Transformer learns partial structures of molecules quickly, it requires extended training to understand overall structures. Consistently, the accuracy of molecular property predictions using descriptors generated from models at different learning steps was similar from the beginning to the end of training. Furthermore, we found that the Transformer requires particularly long training to learn chirality and sometimes stagnates with low translation accura
&lt;/p&gt;</description></item><item><title>cito&#26159;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;R&#21253;&#65292;&#20351;&#29992;torch&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#21253;&#25324;&#35768;&#22810;&#23545;&#39044;&#27979;&#21644;&#35780;&#20272;&#27169;&#22411;&#26377;&#29992;&#30340;&#29992;&#25143;&#21451;&#22909;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09599</link><description>&lt;p&gt;
cito: &#20351;&#29992;torch&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;R&#21253;
&lt;/p&gt;
&lt;p&gt;
cito: An R package for training neural networks using torch. (arXiv:2303.09599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09599
&lt;/p&gt;
&lt;p&gt;
cito&#26159;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;R&#21253;&#65292;&#20351;&#29992;torch&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#21253;&#25324;&#35768;&#22810;&#23545;&#39044;&#27979;&#21644;&#35780;&#20272;&#27169;&#22411;&#26377;&#29992;&#30340;&#29992;&#25143;&#21451;&#22909;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#24050;&#25104;&#20026;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#20013;&#24515;&#31639;&#27861;&#31867;&#21035;&#12290;&#34429;&#28982;&#19968;&#20123;&#21253;&#20801;&#35768;&#29992;&#25143;&#22312;R&#20013;&#25351;&#23450;DNN&#65292;&#20294;&#23427;&#20204;&#22312;&#21151;&#33021;&#19978;&#30456;&#24403;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#20381;&#36182;&#20110;&#20027;&#35201;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;PyTorch&#25110;TensorFlow&#26469;&#26500;&#24314;&#21644;&#35757;&#32451;DNN&#12290;&#28982;&#32780;&#65292;&#19982;R&#29615;&#22659;&#20013;&#21487;&#27604;&#30340;&#22238;&#24402;&#25110;&#26426;&#22120;&#23398;&#20064;&#21253;&#30456;&#27604;&#65292;&#20351;&#29992;&#36825;&#20123;&#26694;&#26550;&#38656;&#35201;&#26356;&#22810;&#30340;&#22521;&#35757;&#21644;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#20171;&#32461;&#20102;cito&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;R&#21253;&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#12290;cito&#20801;&#35768;R&#29992;&#25143;&#20351;&#29992;&#22823;&#22810;&#25968;R&#24314;&#27169;&#20989;&#25968;&#20013;&#20351;&#29992;&#30340;&#29087;&#24713;&#30340;&#20844;&#24335;&#35821;&#27861;&#26469;&#25351;&#23450;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#21518;&#21488;&#65292;cito&#20351;&#29992;torch&#26469;&#25311;&#21512;&#27169;&#22411;&#65292;&#21033;&#29992;torch&#24211;&#30340;&#25152;&#26377;&#25968;&#20540;&#20248;&#21270;&#65292;&#21253;&#25324;&#22312;CPU&#25110;GPU&#19978;&#20999;&#25442;&#35757;&#32451;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;cito&#21253;&#25324;&#35768;&#22810;&#29992;&#20110;&#39044;&#27979;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#29992;&#25143;&#21451;&#22909;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
1. Deep neural networks (DNN) have become a central class of algorithms for regression and classification tasks. Although some packages exist that allow users to specify DNN in R, those are rather limited in their functionality. Most current deep learning applications therefore rely on one of the major deep learning frameworks, PyTorch or TensorFlow, to build and train DNN. However, using these frameworks requires substantially more training and time than comparable regression or machine learning packages in the R environment.  2. Here, we present cito, an user-friendly R package for deep learning. cito allows R users to specify deep neural networks in the familiar formula syntax used by most modeling functions in R. In the background, cito uses torch to fit the models, taking advantage of all the numerical optimizations of the torch library, including the ability to switch between training models on CPUs or GPUs. Moreover, cito includes many user-friendly functions for predictions and
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26408;&#39532;&#25915;&#20987;&#26816;&#27979;&#21644;&#32531;&#35299;&#38382;&#39064;&#12290;&#30001;&#20110;&#36825;&#31181;&#25915;&#20987;&#21361;&#38505;&#38544;&#21311;&#65292;&#19988;&#22312;&#19979;&#28216;&#20998;&#31867;&#22120;&#20013;&#24456;&#38590;&#26816;&#27979;&#20986;&#26469;&#12290;&#30446;&#21069;&#22312;&#36229;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26408;&#39532;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#28508;&#22312;&#22320;&#20445;&#25252;SSL&#19979;&#28216;&#20998;&#31867;&#22120;&#65292;&#20294;&#22312;&#20854;&#24191;&#27867;&#20256;&#25773;&#20043;&#21069;&#35782;&#21035;&#21644;&#22788;&#29702;SSL&#32534;&#30721;&#22120;&#20013;&#30340;&#35302;&#21457;&#22120;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.09079</link><description>&lt;p&gt;
SSL&#28165;&#29702;&#65306;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26408;&#39532;&#26816;&#27979;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
SSL-Cleanse: Trojan Detection and Mitigation in Self-Supervised Learning. (arXiv:2303.09079v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26408;&#39532;&#25915;&#20987;&#26816;&#27979;&#21644;&#32531;&#35299;&#38382;&#39064;&#12290;&#30001;&#20110;&#36825;&#31181;&#25915;&#20987;&#21361;&#38505;&#38544;&#21311;&#65292;&#19988;&#22312;&#19979;&#28216;&#20998;&#31867;&#22120;&#20013;&#24456;&#38590;&#26816;&#27979;&#20986;&#26469;&#12290;&#30446;&#21069;&#22312;&#36229;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26408;&#39532;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#28508;&#22312;&#22320;&#20445;&#25252;SSL&#19979;&#28216;&#20998;&#31867;&#22120;&#65292;&#20294;&#22312;&#20854;&#24191;&#27867;&#20256;&#25773;&#20043;&#21069;&#35782;&#21035;&#21644;&#22788;&#29702;SSL&#32534;&#30721;&#22120;&#20013;&#30340;&#35302;&#21457;&#22120;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#23398;&#20064;&#21644;&#32534;&#30721;&#25968;&#25454;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;SSL&#22270;&#20687;&#32534;&#30721;&#22120;&#24182;&#22312;&#20854;&#39030;&#37096;&#35757;&#32451;&#19979;&#28216;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23454;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#32780;&#21482;&#38656;&#24456;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;SSL&#30340;&#22686;&#21152;&#20351;&#29992;&#23548;&#33268;&#20102;&#19982;SSL&#32534;&#30721;&#22120;&#30456;&#20851;&#30340;&#23433;&#20840;&#30740;&#31350;&#21644;&#21508;&#31181;&#26408;&#39532;&#25915;&#20987;&#30340;&#21457;&#23637;&#12290;&#22312;SSL&#32534;&#30721;&#22120;&#20013;&#25554;&#20837;&#26408;&#39532;&#25915;&#20987;&#30340;&#21361;&#38505;&#22312;&#20110;&#23427;&#20204;&#33021;&#22815;&#38544;&#34109;&#22320;&#25805;&#20316;&#24182;&#22312;&#21508;&#31181;&#29992;&#25143;&#21644;&#35774;&#22791;&#20043;&#38388;&#24191;&#27867;&#20256;&#25773;&#12290;Trojaned&#32534;&#30721;&#22120;&#20013;&#30340;&#21518;&#38376;&#34892;&#20026;&#30340;&#23384;&#22312;&#21487;&#33021;&#20250;&#34987;&#19979;&#28216;&#20998;&#31867;&#22120;&#24847;&#22806;&#32487;&#25215;&#65292;&#20351;&#26816;&#27979;&#21644;&#32531;&#35299;&#23041;&#32961;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#34429;&#28982;&#36229;&#30417;&#30563;&#23398;&#20064;&#20013;&#24403;&#21069;&#30340;&#26408;&#39532;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#28508;&#22312;&#22320;&#20445;&#25252;SSL&#19979;&#28216;&#20998;&#31867;&#22120;&#65292;&#20294;&#22312;&#20854;&#24191;&#27867;&#20256;&#25773;&#20043;&#21069;&#35782;&#21035;&#21644;&#22788;&#29702;SSL&#32534;&#30721;&#22120;&#20013;&#30340;&#35302;&#21457;&#22120;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) is a commonly used approach to learning and encoding data representations. By using a pre-trained SSL image encoder and training a downstream classifier on top of it, impressive performance can be achieved on various tasks with very little labeled data. The increasing usage of SSL has led to an uptick in security research related to SSL encoders and the development of various Trojan attacks. The danger posed by Trojan attacks inserted in SSL encoders lies in their ability to operate covertly and spread widely among various users and devices. The presence of backdoor behavior in Trojaned encoders can inadvertently be inherited by downstream classifiers, making it even more difficult to detect and mitigate the threat. Although current Trojan detection methods in supervised learning can potentially safeguard SSL downstream classifiers, identifying and addressing triggers in the SSL encoder before its widespread dissemination is a challenging task. This is be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38750;&#33258;&#21161;&#23398;&#20064;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#38750;&#33258;&#21161;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#20559;&#35265;&#21487;&#33021;&#23384;&#22312;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#20915;&#31574;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2303.08516</link><description>&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23454;&#29616;&#20844;&#24179;&#30340;&#38750;&#33258;&#21161;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fair Off-Policy Learning from Observational Data. (arXiv:2303.08516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38750;&#33258;&#21161;&#23398;&#20064;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#38750;&#33258;&#21161;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#20559;&#35265;&#21487;&#33021;&#23384;&#22312;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#20915;&#31574;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#21644;&#32452;&#32455;&#24517;&#39035;&#30830;&#20445;&#20854;&#31639;&#27861;&#20915;&#31574;&#30340;&#20844;&#24179;&#24615;&#65292;&#20197;&#28385;&#36275;&#31435;&#27861;&#12289;&#36947;&#24503;&#21644;&#31038;&#20250;&#35201;&#27714;&#12290;&#26412;&#25991;&#38024;&#23545;&#38750;&#33258;&#21161;&#23398;&#20064;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#38750;&#33258;&#21161;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#20915;&#31574;&#35268;&#21017;&#65292;&#20197;&#19981;&#21516;&#30340;&#20844;&#24179;&#27010;&#24565;&#26126;&#30830;&#20551;&#23450;&#35266;&#27979;&#25968;&#25454;&#26159;&#22312;&#19981;&#21516;&#65288;&#28508;&#22312;&#20559;&#35265;&#30340;&#65289;&#34892;&#20026;&#31574;&#30053;&#19979;&#25910;&#38598;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Businesses and organizations must ensure that their algorithmic decision-making is fair in order to meet legislative, ethical, and societal demands. For example, decision-making in automated hiring must not discriminate with respect to gender or race. To achieve this, prior research has contributed approaches that ensure algorithmic fairness in machine learning predictions, while comparatively little effort has focused on algorithmic fairness in decision models, specifically off-policy learning. In this paper, we propose a novel framework for fair off-policy learning: we learn decision rules from observational data under different notions of fairness, where we explicitly assume that observational data were collected under a different -- potentially biased -- behavioral policy. For this, we first formalize different fairness notions for off-policy learning. We then propose a machine learning approach to learn optimal policies under these fairness notions. Specifically, we reformulate th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#38598;&#25104;&#26041;&#27861;&#65292;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20219;&#21153;&#30340;&#39640;&#25928;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.08010</link><description>&lt;p&gt;
&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#32423;&#32852;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65306;&#24403;&#28145;&#24230;&#38598;&#25104;&#27604;&#21333;&#19968;&#27169;&#22411;&#26356;&#26377;&#25928;&#26102;
&lt;/p&gt;
&lt;p&gt;
Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep Ensembles are More Efficient than Single Models. (arXiv:2303.08010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#38598;&#25104;&#26041;&#27861;&#65292;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20219;&#21153;&#30340;&#39640;&#25928;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#26159;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#24615;&#33021;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#31616;&#21333;&#12289;&#21487;&#38752;&#21644;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#37096;&#32626;&#22810;&#20010;&#29420;&#31435;&#27169;&#22411;&#65292;&#23427;&#20204;&#34987;&#24191;&#27867;&#25209;&#35780;&#20026;&#35745;&#31639;&#24320;&#38144;&#22823;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25361;&#25112;&#20102;&#36825;&#31181;&#35266;&#28857;&#65292;&#34920;&#26126;&#23545;&#20110;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#38598;&#25104;&#21487;&#20197;&#27604;&#22312;&#21516;&#19968;&#26550;&#26500;&#26063;&#20013;&#32553;&#25918;&#21333;&#19968;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#26356;&#20855;&#35745;&#31639;&#25928;&#29575;&#12290;&#36890;&#36807;&#36890;&#36807;&#26089;&#26399;&#36864;&#20986;&#26041;&#27861;&#32423;&#32852;&#38598;&#25104;&#25104;&#21592;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#23558;&#36825;&#20123;&#25928;&#29575;&#25552;&#39640;&#25193;&#23637;&#21040;&#19982;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#35768;&#22810;&#36825;&#26679;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#36873;&#25321;&#24615;&#20998;&#31867;&#65292;&#37117;&#26159;&#20108;&#20998;&#31867;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#26032;&#39062;&#35265;&#35299;&#26159;&#20165;&#23558;&#25509;&#36817;&#20108;&#20998;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#20256;&#36882;&#21040;&#21518;&#32493;&#32423;&#32852;&#38454;&#27573;&#12290;&#22312;ImageNet&#35268;&#27169;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#38598;&#25104;&#22312;&#20351;&#29992;&#27604;&#22522;&#32447;&#26356;&#23569;&#30340;&#27169;&#22411;&#35780;&#20272;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#19982;&#23436;&#25972;&#38598;&#25104;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Ensembles are a simple, reliable, and effective method of improving both the predictive performance and uncertainty estimates of deep learning approaches. However, they are widely criticised as being computationally expensive, due to the need to deploy multiple independent models. Recent work has challenged this view, showing that for predictive accuracy, ensembles can be more computationally efficient (at inference) than scaling single models within an architecture family. This is achieved by cascading ensemble members via an early-exit approach. In this work, we investigate extending these efficiency gains to tasks related to uncertainty estimation. As many such tasks, e.g. selective classification, are binary classification, our key novel insight is to only pass samples within a window close to the binary decision boundary to later cascade stages. Experiments on ImageNet-scale data across a number of network architectures and uncertainty tasks show that the proposed window-base
&lt;/p&gt;</description></item><item><title>SLCA&#26159;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24930;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#26469;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#35299;&#20915;&#28176;&#36827;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.05118</link><description>&lt;p&gt;
SLCA: &#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#24930;&#23398;&#20064;&#32773;&#19982;&#20998;&#31867;&#22120;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model. (arXiv:2303.05118v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05118
&lt;/p&gt;
&lt;p&gt;
SLCA&#26159;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24930;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#26469;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#35299;&#20915;&#28176;&#36827;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#22312;&#23398;&#20064;&#39034;&#24207;&#21040;&#36798;&#30340;&#25968;&#25454;&#20013;&#25552;&#39640;&#35782;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22823;&#37096;&#20998;&#29616;&#26377;&#24037;&#20316;&#37117;&#24314;&#31435;&#22312;&#20174;&#22836;&#23398;&#20064;&#30340;&#21069;&#25552;&#19979;&#65292;&#20294;&#36234;&#26469;&#36234;&#22810;&#30340;&#21162;&#21147;&#24050;&#32463;&#33268;&#21147;&#20110;&#34701;&#20837;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#22312;&#27599;&#20010;&#22686;&#37327;&#20219;&#21153;&#20013;&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#30340;&#36830;&#32493;&#23398;&#20064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#24182;&#23558;&#20851;&#38190;&#25361;&#25112;&#24402;&#22240;&#20110;&#28176;&#36827;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#35266;&#23519;&#21040;&#22312;&#34920;&#24449;&#23618;&#27425;&#19978;&#36873;&#25321;&#24615;&#38477;&#20302;&#23398;&#20064;&#29575;&#20960;&#20046;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#24930;&#23398;&#20064;&#32773;&#19982;&#20998;&#31867;&#22120;&#23545;&#40784;&#65288;SLCA&#65289;&#65292;&#36890;&#36807;&#24314;&#27169;&#31867;&#21035;&#20998;&#24067;&#24182;&#22312;&#20107;&#21518;&#23545;&#40784;&#20998;&#31867;&#23618;&#27425;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#20998;&#31867;&#23618;&#27425;&#12290;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SLCA&#22312;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of continual learning is to improve the performance of recognition models in learning sequentially arrived data. Although most existing works are established on the premise of learning from scratch, growing efforts have been devoted to incorporating the benefits of pre-training. However, how to adaptively exploit the pre-trained knowledge for each incremental task while maintaining its generalizability remains an open question. In this work, we present an extensive analysis for continual learning on a pre-trained model (CLPM), and attribute the key challenge to a progressive overfitting problem. Observing that selectively reducing the learning rate can almost resolve this issue in the representation layer, we propose a simple but extremely effective approach named Slow Learner with Classifier Alignment (SLCA), which further improves the classification layer by modeling the class-wise distributions and aligning the classification layers in a post-hoc fashion. Across a variety o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#37325;&#26500;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LSTM&#32534;&#30721;&#22120;&#8212;&#35299;&#30721;&#22120;&#20849;&#21516;&#23398;&#20064;&#35266;&#27979;&#21644;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#20174;&#27491;&#24120;&#26679;&#26412;&#20013;&#20272;&#35745;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21463;&#21040;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#21487;&#20197;&#29992;&#39532;&#27663;&#36317;&#31163;&#35780;&#20272;&#24322;&#24120;&#32423;&#21035;&#12290;</title><link>http://arxiv.org/abs/2303.03324</link><description>&lt;p&gt;
&#22522;&#20110;&#37325;&#26500;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Time series anomaly detection with reconstruction-based state-space models. (arXiv:2303.03324v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#37325;&#26500;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LSTM&#32534;&#30721;&#22120;&#8212;&#35299;&#30721;&#22120;&#20849;&#21516;&#23398;&#20064;&#35266;&#27979;&#21644;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#20174;&#27491;&#24120;&#26679;&#26412;&#20013;&#20272;&#35745;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21463;&#21040;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#21487;&#20197;&#29992;&#39532;&#27663;&#36317;&#31163;&#35780;&#20272;&#24322;&#24120;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21270;&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#23548;&#33268;&#21508;&#31181;&#39046;&#22495;&#20013;&#20986;&#29616;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20351;&#24471;&#23454;&#26102;&#30417;&#27979;&#36816;&#33829;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#35782;&#21035;&#24322;&#24120;&#25968;&#25454;&#27169;&#24335;&#21644;&#26816;&#27979;&#28508;&#22312;&#25925;&#38556;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#20294;&#20063;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20849;&#21516;&#23398;&#20064;&#35266;&#27979;&#27169;&#22411;&#21644;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#20174;&#27491;&#24120;&#26679;&#26412;&#20013;&#20272;&#35745;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#30340;&#65292;&#37319;&#29992;&#22522;&#20110;&#38271;&#30701;&#26102;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#30340;&#32534;&#30721;&#22120;&#8212;&#35299;&#30721;&#22120;&#34920;&#31034;&#35266;&#27979;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;, &#34701;&#21512;&#20102;&#21521;&#21518;&#21644;&#21521;&#21069;&#30340;&#26102;&#38388;&#20449;&#24687;&#20197;&#21516;&#26102;&#24314;&#27169;&#29366;&#24577;&#30340;&#21452;&#21521;&#36716;&#25442;&#12290;&#28508;&#22312;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#32422;&#26463;&#20102;&#27491;&#24120;&#26679;&#26412;&#30340;&#29366;&#24577;&#65292;&#24182;&#20351;&#29992;&#39532;&#27663;&#36317;&#31163;&#35780;&#20272;&#24322;&#24120;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in digitization have led to the availability of multivariate time series data in various domains, enabling real-time monitoring of operations. Identifying abnormal data patterns and detecting potential failures in these scenarios are important yet rather challenging. In this work, we propose a novel unsupervised anomaly detection method for time series data. The proposed framework jointly learns the observation model and the dynamic model, and model uncertainty is estimated from normal samples. Specifically, a long short-term memory (LSTM)-based encoder-decoder is adopted to represent the mapping between the observation space and the latent space. Bidirectional transitions of states are simultaneously modeled by leveraging backward and forward temporal information. Regularization of the latent space places constraints on the states of normal samples, and Mahalanobis distance is used to evaluate the abnormality level. Empirical studies on synthetic and real-world dataset
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#21333;&#20301;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#29289;&#29702;&#31526;&#21495;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#29289;&#29702;&#25968;&#25454;&#20013;&#24674;&#22797;&#35299;&#26512;&#31526;&#21495;&#34920;&#36798;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#28040;&#38500;&#29289;&#29702;&#19978;&#19981;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36824;&#36890;&#36807;&#32500;&#24230;&#20998;&#26512;&#30340;&#35268;&#21017;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.03192</link><description>&lt;p&gt;
&#22522;&#20110;&#21333;&#20301;&#32422;&#26463;&#30340;&#28145;&#24230;&#31526;&#21495;&#22238;&#24402;&#65306;&#33258;&#21160;&#21457;&#29616;&#29289;&#29702;&#23450;&#24459;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Deep symbolic regression for physics guided by units constraints: toward the automated discovery of physical laws. (arXiv:2303.03192v2 [astro-ph.IM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03192
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#21333;&#20301;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#29289;&#29702;&#31526;&#21495;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#29289;&#29702;&#25968;&#25454;&#20013;&#24674;&#22797;&#35299;&#26512;&#31526;&#21495;&#34920;&#36798;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#28040;&#38500;&#29289;&#29702;&#19978;&#19981;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36824;&#36890;&#36807;&#32500;&#24230;&#20998;&#26512;&#30340;&#35268;&#21017;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#26159;&#30740;&#31350;&#33258;&#21160;&#25628;&#32034;&#19982;&#25968;&#25454;&#25311;&#21512;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#30340;&#31639;&#27861;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24341;&#36215;&#20102;&#23545;&#36825;&#31181;&#26041;&#27861;&#30340;&#26032;&#20852;&#20852;&#36259;&#65292;&#20294;&#26159;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#30340;&#21457;&#23637;&#24182;&#27809;&#26377;&#19987;&#27880;&#20110;&#29289;&#29702;&#23398;&#65292;&#32780;&#29289;&#29702;&#23398;&#26377;&#37325;&#35201;&#30340;&#21333;&#20301;&#32422;&#26463;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\Phi$-SO&#65292;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#25968;&#25454;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#23398;&#20064;&#21333;&#20301;&#32422;&#26463;&#26469;&#24674;&#22797;&#35299;&#26512;&#31526;&#21495;&#34920;&#36798;&#24335;&#30340;&#29289;&#29702;&#31526;&#21495;&#20248;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#20174;&#24213;&#23618;&#24320;&#22987;&#26500;&#24314;&#65292;&#36890;&#36807;&#35774;&#35745;&#20351;&#29289;&#29702;&#21333;&#20301;&#22312;&#26500;&#36896;&#36807;&#31243;&#20013;&#20445;&#25345;&#19968;&#33268;&#26469;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#19981;&#20165;&#26377;&#21161;&#20110;&#28040;&#38500;&#29289;&#29702;&#19978;&#19981;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20063;&#22240;&#20026;&#32500;&#24230;&#20998;&#26512;&#30340;&#8220;&#35821;&#27861;&#8221;&#35268;&#21017;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#26041;&#31243;&#29983;&#25104;&#22120;&#30340;&#33258;&#30001;&#24230;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#29992;&#20110;&#25311;&#21512;&#26080;&#22122;&#22768;&#25968;&#25454;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#29289;&#29702;&#36807;&#31243;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic Regression is the study of algorithms that automate the search for analytic expressions that fit data. While recent advances in deep learning have generated renewed interest in such approaches, the development of symbolic regression methods has not been focused on physics, where we have important additional constraints due to the units associated with our data. Here we present $\Phi$-SO, a Physical Symbolic Optimization framework for recovering analytical symbolic expressions from physics data using deep reinforcement learning techniques by learning units constraints. Our system is built, from the ground up, to propose solutions where the physical units are consistent by construction. This is useful not only in eliminating physically impossible solutions, but because the "grammatical" rules of dimensional analysis restrict enormously the freedom of the equation generator, thus vastly improving performance. The algorithm can be used to fit noiseless data, which can be useful fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FaceRNET&#30340;&#38754;&#37096;&#34920;&#24773;&#24378;&#24230;&#20272;&#35745;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#37319;&#29992;&#20102;&#34920;&#31034;&#25552;&#21462;&#22120;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#32452;&#21512;&#65292;&#33021;&#22815;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#21508;&#31181;&#24773;&#24863;&#25551;&#36848;&#31526;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#36335;&#30001;&#22788;&#29702;&#19981;&#21516;&#38271;&#24230;&#30340;&#36755;&#20837;&#35270;&#39057;&#12290;&#22312;Hume-Reaction&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.00180</link><description>&lt;p&gt;
FaceRNET: &#19968;&#31181;&#38754;&#37096;&#34920;&#24773;&#24378;&#24230;&#20272;&#35745;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
FaceRNET: a Facial Expression Intensity Estimation Network. (arXiv:2303.00180v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FaceRNET&#30340;&#38754;&#37096;&#34920;&#24773;&#24378;&#24230;&#20272;&#35745;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#37319;&#29992;&#20102;&#34920;&#31034;&#25552;&#21462;&#22120;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#32452;&#21512;&#65292;&#33021;&#22815;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#21508;&#31181;&#24773;&#24863;&#25551;&#36848;&#31526;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#36335;&#30001;&#22788;&#29702;&#19981;&#21516;&#38271;&#24230;&#30340;&#36755;&#20837;&#35270;&#39057;&#12290;&#22312;Hume-Reaction&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;&#35270;&#39057;&#20013;&#36827;&#34892;&#38754;&#37096;&#34920;&#24773;&#24378;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#20004;&#20010;&#32452;&#20214;&#65306;i) &#19968;&#20010;&#34920;&#31034;&#25552;&#21462;&#22120;&#32593;&#32476;&#65292;&#20174;&#27599;&#20010;&#35270;&#39057;&#24103;&#20013;&#25552;&#21462;&#21508;&#31181;&#24773;&#24863;&#25551;&#36848;&#31526;&#65288;&#20215;&#20540;-&#21796;&#37266;&#12289;&#21160;&#20316;&#21333;&#20803;&#21644;&#22522;&#26412;&#34920;&#24773;&#65289;&#65307;ii) &#19968;&#20010;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65292;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#65292;&#28982;&#21518;&#26159;&#19968;&#20010;&#25513;&#30721;&#23618;&#65292;&#36890;&#36807;&#21160;&#24577;&#36335;&#30001;&#23454;&#29616;&#23545;&#19981;&#21516;&#36755;&#20837;&#35270;&#39057;&#38271;&#24230;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#22312;Hume-Reaction&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents our approach for Facial Expression Intensity Estimation from videos. It includes two components: i) a representation extractor network that extracts various emotion descriptors (valence-arousal, action units and basic expressions) from each videoframe; ii) a RNN that captures temporal information in the data, followed by a mask layer which enables handling varying input video lengths through dynamic routing. This approach has been tested on the Hume-Reaction dataset yielding excellent results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#36136;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#30740;&#31350;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#22312;&#19981;&#21516;&#31867;&#21035;&#30340;&#22797;&#26434;&#24615;&#19979;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#31867;&#21035;$F$&#30456;&#27604;&#31867;&#21035;$G$&#26356;&#8220;&#31616;&#21333;&#8221;&#26102;&#65292;&#25105;&#20204;&#30340;&#39044;&#27979;&#22120;&#23545;&#20110;&#21327;&#21464;&#37327;&#20559;&#31227;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#23588;&#20854;&#22312;$\textbf{y}$&#30340;&#20559;&#31227;&#36828;&#23567;&#20110;$\textbf{x}$&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;ERM&#30340;&#34892;&#20026;&#19982;&#27491;&#20132;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#31867;&#20284;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.13934</link><description>&lt;p&gt;
&#24322;&#36136;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#32479;&#35745;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Statistical Learning under Heterogenous Distribution Shift. (arXiv:2302.13934v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#36136;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#30740;&#31350;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#22312;&#19981;&#21516;&#31867;&#21035;&#30340;&#22797;&#26434;&#24615;&#19979;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#31867;&#21035;$F$&#30456;&#27604;&#31867;&#21035;$G$&#26356;&#8220;&#31616;&#21333;&#8221;&#26102;&#65292;&#25105;&#20204;&#30340;&#39044;&#27979;&#22120;&#23545;&#20110;&#21327;&#21464;&#37327;&#20559;&#31227;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#23588;&#20854;&#22312;$\textbf{y}$&#30340;&#20559;&#31227;&#36828;&#23567;&#20110;$\textbf{x}$&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;ERM&#30340;&#34892;&#20026;&#19982;&#27491;&#20132;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#31867;&#20284;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#38543;&#26426;&#21464;&#37327;&#23545;$(\mathbf{x},\mathbf{y})$&#20013;&#39044;&#27979;&#30446;&#26631;$\mathbf{z}$, &#20854;&#20013;&#30495;&#23454;&#30340;&#39044;&#27979;&#22120;&#26159;&#21152;&#27861;&#30340;$\mathbb{E}[\mathbf{z} \mid \mathbf{x},\mathbf{y}] = f_\star(\mathbf{x}) +g_{\star}(\mathbf{y})$&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32473;&#23450;&#35757;&#32451;&#20998;&#24067;&#19978;&#25311;&#21512;&#30340;&#20989;&#25968;$f+g$, $f \in F$&#21644;$g \in G$&#19978;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#22312;&#34920;&#29616;&#19978;&#30340;&#24046;&#24322;&#65292;&#20294;&#22312;&#27979;&#35797;&#20998;&#24067;&#19978;&#24471;&#21040;&#35780;&#20272;&#26102;&#20250;&#26174;&#31034;&#20986;&#21327;&#21464;&#37327;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#31867;&#21035;$F$&#27604;$G$&#26356;&#8220;&#31616;&#21333;&#8221;&#65288;&#20363;&#22914;&#65292;&#20197;&#24230;&#37327;&#29109;&#20026;&#34913;&#37327;&#26631;&#20934;&#65289;&#26102;&#65292;&#25105;&#20204;&#30340;&#39044;&#27979;&#22120;&#23545;&#20110;&#21327;&#21464;&#37327;&#20559;&#31227;&#30340;&#25239;&#24178;&#25200;&#33021;&#21147;&#26356;&#24378;&#65292;&#20854;&#20013;$\textbf{y}$&#30340;&#20559;&#31227;&#35201;&#36828;&#23567;&#20110;$\textbf{x}$&#30340;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;ERM&#30340;&#34892;&#20026;&#19982;&#27491;&#20132;&#26426;&#22120;&#23398;&#20064;$\textbf{ qualitatively similarly}$&#65306;ERM&#24674;&#22797;&#39044;&#27979;&#22120;&#20013;&#30340;$f$&#25104;&#20998;&#30340;&#36895;&#29575;&#20165;&#23545;&#20110;&#31867;&#21035;$G$&#30340;&#22797;&#26434;&#24615;&#20855;&#26377;&#36739;&#20302;&#38454;&#30340;&#20381;&#36182;&#24615;&#65292;&#35843;&#25972;&#21518;...
&lt;/p&gt;
&lt;p&gt;
This paper studies the prediction of a target $\mathbf{z}$ from a pair of random variables $(\mathbf{x},\mathbf{y})$, where the ground-truth predictor is additive $\mathbb{E}[\mathbf{z} \mid \mathbf{x},\mathbf{y}] = f_\star(\mathbf{x}) +g_{\star}(\mathbf{y})$. We study the performance of empirical risk minimization (ERM) over functions $f+g$, $f \in F$ and $g \in G$, fit on a given training distribution, but evaluated on a test distribution which exhibits covariate shift. We show that, when the class $F$ is "simpler" than $G$ (measured, e.g., in terms of its metric entropy), our predictor is more resilient to $\textbf{heterogenous covariate shifts}$ in which the shift in $\mathbf{x}$ is much greater than that in $\mathbf{y}$. Our analysis proceeds by demonstrating that ERM behaves $\textbf{qualitatively similarly to orthogonal machine learning}$: the rate at which ERM recovers the $f$-component of the predictor has only a lower-order dependence on the complexity of the class $G$, adjus
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#28155;&#21152;&#32467;&#26500;&#21270;&#22122;&#22768;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#26679;&#31574;&#30053;&#26469;&#20248;&#20110;&#20256;&#32479;&#30340;&#35757;&#32451;&#21644;&#36203;&#24067;&#29983;&#35268;&#21017;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.13417</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#21270;&#22122;&#22768;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#20998;&#31867;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training neural networks with structured noise improves classification and generalization. (arXiv:2302.13417v3 [cond-mat.dis-nn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13417
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#28155;&#21152;&#32467;&#26500;&#21270;&#22122;&#22768;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#26679;&#31574;&#30053;&#26469;&#20248;&#20110;&#20256;&#32479;&#30340;&#35757;&#32451;&#21644;&#36203;&#24067;&#29983;&#35268;&#21017;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#22312;&#23398;&#20064;&#20013;&#30340;&#31215;&#26497;&#20316;&#29992;&#26159;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#20013;&#19968;&#20010;&#24050;&#32463;&#34987;&#30830;&#35748;&#30340;&#27010;&#24565;&#65292;&#36825;&#34920;&#26126;&#29978;&#33267;&#29983;&#29289;&#31995;&#32479;&#21487;&#33021;&#21033;&#29992;&#31867;&#20284;&#30340;&#26426;&#21046;&#26469;&#26368;&#22823;&#21270;&#24615;&#33021;&#12290;Gardner&#21644;&#21512;&#20316;&#32773;&#25552;&#20986;&#30340;&#22122;&#22768;&#35757;&#32451;&#31639;&#27861;&#26159;&#22312;&#24490;&#29615;&#32593;&#32476;&#20013;&#27880;&#20837;&#22122;&#22768;&#30340;&#20856;&#22411;&#31034;&#20363;&#65292;&#24490;&#29615;&#32593;&#32476;&#36890;&#24120;&#29992;&#20110;&#24314;&#27169;&#30495;&#23454;&#31070;&#32463;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22122;&#22768;&#35757;&#32451;&#25968;&#25454;&#20013;&#28155;&#21152;&#32467;&#26500;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#31639;&#27861;&#24615;&#33021;&#65292;&#20351;&#24471;&#21487;&#20197;&#25509;&#36817;&#23436;&#32654;&#20998;&#31867;&#21644;&#26368;&#22823;&#21560;&#24341;&#22495;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25152;&#35859;&#30340;&#36203;&#24067;&#29983;&#35268;&#21017;&#22312;&#22122;&#22768;&#36798;&#21040;&#26368;&#22823;&#19988;&#25968;&#25454;&#26159;&#32593;&#32476;&#21160;&#21147;&#23398;&#30340;&#22266;&#23450;&#28857;&#26102;&#19982;&#22122;&#22768;&#35757;&#32451;&#31639;&#27861;&#19968;&#33268;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#26045;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#20339;&#22122;&#22768;&#25968;&#25454;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#26469;&#36229;&#36234;&#22122;&#22768;&#35757;&#32451;&#21644;&#36203;&#24067;&#29983;&#35268;&#21017;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The beneficial role of noise in learning is nowadays a consolidated concept in the field of artificial neural networks, suggesting that even biological systems might take advantage of similar mechanisms to maximize their performance. The training-with-noise algorithm proposed by Gardner and collaborators is an emblematic example of a noise injection procedure in recurrent networks, which are usually employed to model real neural systems. We show how adding structure into noisy training data can substantially improve the algorithm performance, allowing to approach perfect classification and maximal basins of attraction. We also prove that the so-called Hebbian unlearning rule coincides with the training-with-noise algorithm when noise is maximal and data are fixed points of the network dynamics. A sampling scheme for optimal noisy data is eventually proposed and implemented to outperform both the training-with-noise and the Hebbian unlearning procedures.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CDG&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#23398;&#20064;&#22240;&#26524;&#35299;&#32544;&#34920;&#31034;&#21644;&#29983;&#25104;&#22240;&#26524;&#35299;&#32544;&#32467;&#26524;&#12290;&#36890;&#36807;&#25506;&#32034;&#29305;&#23450;&#27169;&#22411;&#19979;&#23454;&#29616;CDG&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#22312;&#32534;&#30721;&#22120;&#20013;&#21152;&#20837;&#30417;&#30563;&#27491;&#21017;&#21270;&#26159;&#19981;&#22815;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#24230;&#37327;&#26469;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#30340;&#22240;&#26524;&#35299;&#32544;&#31243;&#24230;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.11737</link><description>&lt;p&gt;
&#22240;&#26524;&#35299;&#32544;&#30340;&#29983;&#25104;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Causally Disentangled Generative Variational AutoEncoder. (arXiv:2302.11737v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CDG&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#23398;&#20064;&#22240;&#26524;&#35299;&#32544;&#34920;&#31034;&#21644;&#29983;&#25104;&#22240;&#26524;&#35299;&#32544;&#32467;&#26524;&#12290;&#36890;&#36807;&#25506;&#32034;&#29305;&#23450;&#27169;&#22411;&#19979;&#23454;&#29616;CDG&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#22312;&#32534;&#30721;&#22120;&#20013;&#21152;&#20837;&#30417;&#30563;&#27491;&#21017;&#21270;&#26159;&#19981;&#22815;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#24230;&#37327;&#26469;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#30340;&#22240;&#26524;&#35299;&#32544;&#31243;&#24230;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#65292;&#20351;&#20854;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#22240;&#26524;&#35299;&#32544;&#34920;&#31034;&#21644;&#29983;&#25104;&#22240;&#26524;&#35299;&#32544;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#22240;&#26524;&#35299;&#32544;&#29983;&#25104;&#65288;CDG&#65289;&#12290;CDG&#26159;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#22240;&#26524;&#35299;&#32544;&#34920;&#31034;&#20934;&#30830;&#22320;&#35299;&#30721;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20165;&#20165;&#22312;&#32534;&#30721;&#22120;&#20013;&#21152;&#20837;&#30417;&#30563;&#27491;&#21017;&#21270;&#26159;&#26080;&#27861;&#23454;&#29616;&#20855;&#26377;CDG&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#65292;&#21363;&#20351;&#23545;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#20219;&#21153;&#20063;&#26159;&#22914;&#27492;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#29305;&#23450;&#27169;&#22411;&#20013;&#23454;&#29616;CDG&#25152;&#38656;&#30340;&#24517;&#35201;&#26465;&#20214;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#22240;&#26524;&#35299;&#32544;&#31243;&#24230;&#30340;&#36890;&#29992;&#24230;&#37327;&#12290;&#26469;&#33258;&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#32467;&#26524;&#25903;&#25345;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new supervised learning technique for the Variational AutoEncoder (VAE) that allows it to learn a causally disentangled representation and generate causally disentangled outcomes simultaneously. We call this approach Causally Disentangled Generation (CDG). CDG is a generative model that accurately decodes an output based on a causally disentangled representation. Our research demonstrates that adding supervised regularization to the encoder alone is insufficient for achieving a generative model with CDG, even for a simple task. Therefore, we explore the necessary and sufficient conditions for achieving CDG within a specific model. Additionally, we introduce a universal metric for evaluating the causal disentanglement of a generative model. Empirical results from both image and tabular datasets support our findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#21270;&#30340;&#20302;&#31209;&#22810;&#20803;&#22238;&#24402;&#65292;&#36890;&#36807;&#37319;&#29992;&#22343;&#21248;&#37327;&#21270;&#19982;&#38543;&#26426;&#25238;&#21160;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#32422;&#26463;Lasso&#21644;&#27491;&#21017;&#21270;Lasso&#20272;&#35745;&#22120;&#65292;&#23454;&#29616;&#20102;&#26368;&#23567;&#26368;&#20248;&#29575;&#30340;&#20272;&#35745;&#65292;&#21516;&#26102;&#37327;&#21270;&#20165;&#23545;&#20056;&#27861;&#22240;&#23376;&#30053;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.11197</link><description>&lt;p&gt;
&#37327;&#23376;&#21270;&#30340;&#20302;&#31209;&#22810;&#20803;&#22238;&#24402;&#19982;&#38543;&#26426;&#25238;&#21160;
&lt;/p&gt;
&lt;p&gt;
Quantized Low-Rank Multivariate Regression with Random Dithering. (arXiv:2302.11197v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#21270;&#30340;&#20302;&#31209;&#22810;&#20803;&#22238;&#24402;&#65292;&#36890;&#36807;&#37319;&#29992;&#22343;&#21248;&#37327;&#21270;&#19982;&#38543;&#26426;&#25238;&#21160;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#32422;&#26463;Lasso&#21644;&#27491;&#21017;&#21270;Lasso&#20272;&#35745;&#22120;&#65292;&#23454;&#29616;&#20102;&#26368;&#23567;&#26368;&#20248;&#29575;&#30340;&#20272;&#35745;&#65292;&#21516;&#26102;&#37327;&#21270;&#20165;&#23545;&#20056;&#27861;&#22240;&#23376;&#30053;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#22810;&#20803;&#22238;&#24402;&#65288;LRMR&#65289;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#32479;&#35745;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#39640;&#24230;&#30456;&#20851;&#30340;&#20219;&#21153;&#20316;&#20026;&#20855;&#26377;&#20302;&#31209;&#20808;&#39564;&#30340;&#22810;&#21709;&#24212;&#22238;&#24402;&#38382;&#39064;&#36827;&#34892;&#32452;&#21512;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#21270;&#30340;LRMR&#65292;&#36825;&#26159;&#19968;&#31181;&#23454;&#38469;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#21709;&#24212;&#21644;/&#25110;&#21327;&#21464;&#37327;&#34987;&#31163;&#25955;&#21270;&#20026;&#26377;&#38480;&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20272;&#35745;&#22522;&#30784;&#31995;&#25968;&#30697;&#38453;&#12290;&#20026;&#20102;&#20351;&#33021;&#22815;&#23454;&#29616;&#20219;&#24847;&#23567;&#35823;&#24046;&#30340;&#19968;&#33268;&#20272;&#35745;&#22120;&#25104;&#20026;&#21487;&#33021;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22343;&#21248;&#37327;&#21270;&#19982;&#38543;&#26426;&#25238;&#21160;&#65292;&#21363;&#22312;&#37327;&#21270;&#20043;&#21069;&#21521;&#25968;&#25454;&#28155;&#21152;&#36866;&#24403;&#30340;&#38543;&#26426;&#22122;&#22768;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21709;&#24212;&#20351;&#29992;&#22343;&#21248;&#25238;&#21160;&#65292;&#21327;&#21464;&#37327;&#20351;&#29992;&#19977;&#35282;&#25238;&#21160;&#12290;&#22522;&#20110;&#37327;&#21270;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32422;&#26463;Lasso&#21644;&#27491;&#21017;&#21270;Lasso&#20272;&#35745;&#22120;&#65292;&#24182;&#25512;&#23548;&#20102;&#38750;&#28176;&#36817;&#24615;&#35823;&#24046;&#30028;&#12290;&#36890;&#36807;&#25238;&#21160;&#30340;&#24110;&#21161;&#65292;&#20272;&#35745;&#22120;&#23454;&#29616;&#20102;&#26368;&#23567;&#26368;&#20248;&#29575;&#65292;&#32780;&#37327;&#21270;&#20165;&#30053;&#24494;&#24694;&#21270;&#20102;&#20056;&#27861;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-rank multivariate regression (LRMR) is an important statistical learning model that combines highly correlated tasks as a multiresponse regression problem with low-rank priori on the coefficient matrix. In this paper, we study quantized LRMR, a practical setting where the responses and/or the covariates are discretized to finite precision. We focus on the estimation of the underlying coefficient matrix. To make consistent estimator that could achieve arbitrarily small error possible, we employ uniform quantization with random dithering, i.e., we add appropriate random noise to the data before quantization. Specifically, uniform dither and triangular dither are used for responses and covariates, respectively. Based on the quantized data, we propose the constrained Lasso and regularized Lasso estimators, and derive the non-asymptotic error bounds. With the aid of dithering, the estimators achieve minimax optimal rate, while quantization only slightly worsens the multiplicative factor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2302.04823</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#29983;&#25104;&#23545;&#25239;&#27169;&#25311;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Generative Adversarial Imitation Learning with Mid-level Input Generation for Autonomous Driving on Urban Environments. (arXiv:2302.04823v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29616;&#23454;&#20013;&#30340;&#22478;&#24066;&#23548;&#33322;&#22330;&#26223;&#65292;&#35774;&#35745;&#20581;&#22766;&#30340;&#25511;&#21046;&#31574;&#30053;&#24182;&#19981;&#26159;&#19968;&#39033;&#31616;&#21333;&#30340;&#20219;&#21153;&#12290;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#20013;&#65292;&#36825;&#20123;&#31574;&#30053;&#24517;&#39035;&#23558;&#36710;&#36742;&#25668;&#20687;&#22836;&#33719;&#24471;&#30340;&#39640;&#32500;&#22270;&#20687;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#65292;&#22914;&#36716;&#21521;&#21644;&#27833;&#38376;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deriving robust control policies for realistic urban navigation scenarios is not a trivial task. In an end-to-end approach, these policies must map high-dimensional images from the vehicle's cameras to low-level actions such as steering and throttle. While pure Reinforcement Learning (RL) approaches are based exclusively on rewards,Generative Adversarial Imitation Learning (GAIL) agents learn from expert demonstrations while interacting with the environment, which favors GAIL on tasks for which a reward signal is difficult to derive. In this work, the hGAIL architecture was proposed to solve the autonomous navigation of a vehicle in an end-to-end approach, mapping sensory perceptions directly to low-level actions, while simultaneously learning mid-level input representations of the agent's environment. The proposed hGAIL consists of an hierarchical Adversarial Imitation Learning architecture composed of two main modules: the GAN (Generative Adversarial Nets) which generates the Bird's-
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#25968;&#25454;&#23548;&#21521;&#26550;&#26500;&#65288;DOA&#65289;&#30340;&#37319;&#29992;&#24773;&#20917;&#65292;&#21457;&#29616;&#23613;&#31649;&#27809;&#26377;&#26126;&#30830;&#25552;&#21450;DOA&#65292;&#20294;&#35768;&#22810;&#35770;&#25991;&#20013;&#30340;&#35774;&#35745;&#20915;&#31574;&#40664;&#40664;&#22320;&#36981;&#24490;&#20102;DOA&#30340;&#21407;&#21017;&#12290;</title><link>http://arxiv.org/abs/2302.04810</link><description>&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65306;&#22522;&#20110;&#25968;&#25454;&#23548;&#21521;&#26550;&#26500;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Real-world Machine Learning Systems: A survey from a Data-Oriented Architecture Perspective. (arXiv:2302.04810v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04810
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#25968;&#25454;&#23548;&#21521;&#26550;&#26500;&#65288;DOA&#65289;&#30340;&#37319;&#29992;&#24773;&#20917;&#65292;&#21457;&#29616;&#23613;&#31649;&#27809;&#26377;&#26126;&#30830;&#25552;&#21450;DOA&#65292;&#20294;&#35768;&#22810;&#35770;&#25991;&#20013;&#30340;&#35774;&#35745;&#20915;&#31574;&#40664;&#40664;&#22320;&#36981;&#24490;&#20102;DOA&#30340;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#38271;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27491;&#22312;&#20316;&#20026;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#37096;&#32626;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#35774;&#35745;&#12289;&#23454;&#29616;&#21644;&#32500;&#25252;&#21463;&#21040;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#29615;&#22659;&#20135;&#29983;&#20102;&#26356;&#22810;&#30340;&#24322;&#26500;&#25968;&#25454;&#65292;&#29992;&#25143;&#38656;&#35201;&#26356;&#24555;&#30340;&#21709;&#24212;&#36895;&#24230;&#21644;&#39640;&#25928;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36825;&#20123;&#35201;&#27714;&#23558;&#26222;&#36941;&#23384;&#22312;&#30340;&#36719;&#20214;&#26550;&#26500;&#25512;&#21521;&#20102;&#26497;&#38480;&#65292;&#24403;&#37096;&#32626;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#26102;&#12290;&#25968;&#25454;&#23548;&#21521;&#26550;&#26500;&#65288;DOA&#65289;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#27010;&#24565;&#65292;&#23427;&#33021;&#26356;&#22909;&#22320;&#20026;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31995;&#32479;&#25552;&#20379;&#25903;&#25345;&#12290;DOA&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#26550;&#26500;&#65292;&#21019;&#24314;&#20102;&#25968;&#25454;&#39537;&#21160;&#12289;&#26494;&#32806;&#21512;&#12289;&#21435;&#20013;&#24515;&#21270;&#21644;&#24320;&#25918;&#30340;&#31995;&#32479;&#12290;&#23613;&#31649;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#35770;&#25991;&#20013;&#27809;&#26377;&#25552;&#21040;DOA&#65292;&#20294;&#23427;&#20204;&#30340;&#20316;&#32773;&#22312;&#35774;&#35745;&#19978;&#38544;&#21547;&#22320;&#36981;&#24490;&#20102;DOA&#12290;&#20026;&#20160;&#20040;&#12289;&#22914;&#20309;&#20197;&#21450;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#37319;&#29992;DOA&#22312;&#36825;&#20123;&#31995;&#32479;&#20013;&#23578;&#19981;&#28165;&#26970;&#12290;&#38544;&#21547;&#30340;&#35774;&#35745;&#20915;&#31574;&#38480;&#21046;&#20102;&#20174;&#19994;&#32773;&#23545;&#20110;&#35774;&#35745;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#26102;DOA&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning models are being deployed as parts of real-world systems with the upsurge of interest in artificial intelligence. The design, implementation, and maintenance of such systems are challenged by real-world environments that produce larger amounts of heterogeneous data and users requiring increasingly faster responses with efficient resource consumption. These requirements push prevalent software architectures to the limit when deploying ML-based systems. Data-oriented Architecture (DOA) is an emerging concept that equips systems better for integrating ML models. DOA extends current architectures to create data-driven, loosely coupled, decentralised, open systems. Even though papers on deployed ML-based systems do not mention DOA, their authors made design decisions that implicitly follow DOA. The reasons why, how, and the extent to which DOA is adopted in these systems are unclear. Implicit design decisions limit the practitioners' knowledge of DOA to design ML-based syst
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#32771;&#34385;&#22810;&#20010;&#26041;&#24046;&#26469;&#28304;&#21450;&#20854;&#19982;&#25968;&#25454;&#29305;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#35780;&#20272;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#65292;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.04054</link><description>&lt;p&gt;
&#36861;&#27714;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#25512;&#29702;&#22797;&#29616;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Inferential Reproducibility of Machine Learning Research. (arXiv:2302.04054v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#32771;&#34385;&#22810;&#20010;&#26041;&#24046;&#26469;&#28304;&#21450;&#20854;&#19982;&#25968;&#25454;&#29305;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#35780;&#20272;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#65292;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#8212;&#8212;&#21363;&#22312;&#22797;&#21046;&#30340;&#27169;&#22411;&#35757;&#32451;&#36816;&#34892;&#20013;&#35266;&#23519;&#21040;&#30340;&#35780;&#20272;&#20998;&#25968;&#30340;&#19968;&#33268;&#24615;&#8212;&#8212;&#21463;&#21040;&#20960;&#31181;&#38750;&#30830;&#23450;&#24615;&#26469;&#28304;&#30340;&#24433;&#21709;&#65292;&#21487;&#20197;&#34987;&#35270;&#20026;&#27979;&#37327;&#22122;&#22768;&#12290;&#30446;&#21069;&#30340;&#36235;&#21183;&#26159;&#21435;&#38500;&#22122;&#22768;&#65292;&#20197;&#24378;&#21046;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#24573;&#30053;&#20102;&#23454;&#29616;&#23618;&#38754;&#22266;&#26377;&#30340;&#38750;&#30830;&#23450;&#24615;&#20197;&#21450;&#31639;&#27861;&#22122;&#22768;&#22240;&#32032;&#21644;&#25968;&#25454;&#29305;&#24615;&#20043;&#38388;&#30340;&#20851;&#38190;&#30456;&#20114;&#20316;&#29992;&#25928;&#24212;&#12290;&#36825;&#38480;&#21046;&#20102;&#20174;&#36825;&#20123;&#23454;&#39564;&#20013;&#21487;&#20197;&#24471;&#20986;&#30340;&#32467;&#35770;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#23558;&#20960;&#20010;&#26041;&#24046;&#26469;&#28304;&#65292;&#21253;&#25324;&#23427;&#20204;&#19982;&#25968;&#25454;&#29305;&#24615;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#30340;&#26174;&#33879;&#24615;&#21644;&#21487;&#38752;&#24615;&#20998;&#26512;&#20013;&#65292;&#20197;&#26399;&#20174;&#35757;&#32451;&#27169;&#22411;&#30340;&#29305;&#23450;&#23454;&#20363;&#24471;&#20986;&#25512;&#29702;&#32467;&#35770;, &#32780;&#38750;&#21435;&#38500;&#22122;&#22768;&#12290;&#25105;&#20204;&#23637;&#31034;&#22914;&#20309;&#20351;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#29992;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#24335;&#26469;&#32771;&#34385;&#31639;&#27861;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;&#22122;&#22768;&#26469;&#28304;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#21508;&#20010;&#26041;&#24046;&#26469;&#28304;&#23545;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliability of machine learning evaluation -- the consistency of observed evaluation scores across replicated model training runs -- is affected by several sources of nondeterminism which can be regarded as measurement noise. Current tendencies to remove noise in order to enforce reproducibility of research results neglect inherent nondeterminism at the implementation level and disregard crucial interaction effects between algorithmic noise factors and data properties. This limits the scope of conclusions that can be drawn from such experiments. Instead of removing noise, we propose to incorporate several sources of variance, including their interaction with data properties, into an analysis of significance and reliability of machine learning evaluation, with the aim to draw inferences beyond particular instances of trained models. We show how to use linear mixed effects models (LMEMs) to analyze performance evaluation scores, and to conduct statistical inference with a generalized lik
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#24335;&#20960;&#20309;&#21644;&#20132;&#20114;&#23884;&#20837;&#26469;&#25552;&#39640;&#23569;&#26679;&#26412;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24320;&#21457;&#20998;&#23376;&#23884;&#20837;&#26469;&#32534;&#30721;&#22797;&#26434;&#30340;&#20998;&#23376;&#29305;&#24449;&#65292;&#32467;&#21512;&#22823;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#22522;&#20934;&#19978;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.02055</link><description>&lt;p&gt;
&#38544;&#24335;&#20960;&#20309;&#19982;&#20132;&#20114;&#23884;&#20837;&#25552;&#39640;&#23569;&#26679;&#26412;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Implicit Geometry and Interaction Embeddings Improve Few-Shot Molecular Property Prediction. (arXiv:2302.02055v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#24335;&#20960;&#20309;&#21644;&#20132;&#20114;&#23884;&#20837;&#26469;&#25552;&#39640;&#23569;&#26679;&#26412;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24320;&#21457;&#20998;&#23376;&#23884;&#20837;&#26469;&#32534;&#30721;&#22797;&#26434;&#30340;&#20998;&#23376;&#29305;&#24449;&#65292;&#32467;&#21512;&#22823;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#22522;&#20934;&#19978;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26041;&#27861;&#65292;&#22240;&#20026;&#30417;&#30563;&#25968;&#25454;&#36890;&#24120;&#38750;&#24120;&#26377;&#38480;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#37325;&#35201;&#30340;&#20998;&#23376;&#24615;&#36136;&#21462;&#20915;&#20110;&#22797;&#26434;&#30340;&#20998;&#23376;&#29305;&#24449;&#65292;&#22914;&#20998;&#23376;&#21487;&#33021;&#20855;&#26377;&#30340;&#21508;&#31181;&#19977;&#32500;&#20960;&#20309;&#24418;&#29366;&#25110;&#20854;&#21487;&#33021;&#24418;&#25104;&#30340;&#21270;&#23398;&#30456;&#20114;&#20316;&#29992;&#31867;&#22411;&#65292;&#36825;&#20123;&#29305;&#24449;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#24182;&#26410;&#26126;&#30830;&#32534;&#30721;&#65292;&#24517;&#39035;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#36817;&#20284;&#12290;&#23398;&#20064;&#36825;&#20123;&#29305;&#24449;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#35774;&#35745;&#29992;&#20110;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#31639;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33021;&#22815;&#32534;&#30721;&#22797;&#26434;&#20998;&#23376;&#29305;&#24449;&#30340;&#20998;&#23376;&#23884;&#20837;&#65292;&#20197;&#25552;&#39640;&#23569;&#26679;&#26412;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22823;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#21363;&#20998;&#23376;&#23545;&#25509;&#35745;&#31639;&#30340;&#32467;&#26524;&#65292;&#20197;&#21450;&#22810;&#20219;&#21153;&#23398;&#20064;&#33539;&#24335;&#26469;&#26500;&#24314;&#23884;&#20837;&#31354;&#38388;&#12290;&#22312;&#22810;&#20010;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#22522;&#20934;&#19978;&#65292;&#20174;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#35757;&#32451;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning is a promising approach to molecular property prediction as supervised data is often very limited. However, many important molecular properties depend on complex molecular characteristics -- such as the various 3D geometries a molecule may adopt or the types of chemical interactions it can form -- that are not explicitly encoded in the feature space and must be approximated from low amounts of data. Learning these characteristics can be difficult, especially for few-shot learning algorithms that are designed for fast adaptation to new tasks. In this work, we develop molecular embeddings that encode complex molecular characteristics to improve the performance of few-shot molecular property prediction. Our approach leverages large amounts of synthetic data, namely the results of molecular docking calculations, and a multi-task learning paradigm to structure the embedding space. On multiple molecular property prediction benchmarks, training from the embedding space subst
&lt;/p&gt;</description></item><item><title>Simplex&#38543;&#26426;&#29305;&#24449;&#65288;SimRFs&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#29305;&#24449;&#26426;&#21046;&#65292;&#36890;&#36807;&#20960;&#20309;&#30456;&#20851;&#24615;&#26469;&#26080;&#20559;&#20272;&#35745;softmax&#21644;&#39640;&#26031;&#26680;&#12290;&#22312;&#26435;&#37325;&#26080;&#20851;&#30340;&#20960;&#20309;&#30456;&#20851;&#27491;&#38543;&#26426;&#29305;&#24449;&#26426;&#21046;&#31867;&#20013;&#65292;SimRFs&#25552;&#20379;&#20102;&#26368;&#23567;&#21487;&#33021;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#39069;&#22806;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#26126;&#26174;&#20248;&#20110;&#20808;&#21069;&#26368;&#20934;&#30830;&#30340;&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;&#12290;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;SimRFs&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2301.13856</link><description>&lt;p&gt;
Simplex&#38543;&#26426;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Simplex Random Features. (arXiv:2301.13856v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13856
&lt;/p&gt;
&lt;p&gt;
Simplex&#38543;&#26426;&#29305;&#24449;&#65288;SimRFs&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#29305;&#24449;&#26426;&#21046;&#65292;&#36890;&#36807;&#20960;&#20309;&#30456;&#20851;&#24615;&#26469;&#26080;&#20559;&#20272;&#35745;softmax&#21644;&#39640;&#26031;&#26680;&#12290;&#22312;&#26435;&#37325;&#26080;&#20851;&#30340;&#20960;&#20309;&#30456;&#20851;&#27491;&#38543;&#26426;&#29305;&#24449;&#26426;&#21046;&#31867;&#20013;&#65292;SimRFs&#25552;&#20379;&#20102;&#26368;&#23567;&#21487;&#33021;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#39069;&#22806;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#26126;&#26174;&#20248;&#20110;&#20808;&#21069;&#26368;&#20934;&#30830;&#30340;&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;&#12290;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;SimRFs&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Simplex&#38543;&#26426;&#29305;&#24449;&#65288;SimRFs&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#25237;&#24433;&#21521;&#37327;&#30340;&#20960;&#20309;&#30456;&#20851;&#24615;&#26469;&#26080;&#20559;&#20272;&#35745;softmax&#21644;&#39640;&#26031;&#26680;&#30340;&#26032;&#38543;&#26426;&#29305;&#24449;&#65288;RF&#65289;&#26426;&#21046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26080;&#20559;&#20272;&#35745;&#36825;&#20123;&#26680;&#30340;&#26435;&#37325;&#26080;&#20851;&#30340;&#20960;&#20309;&#30456;&#20851;&#27491;&#38543;&#26426;&#29305;&#24449;&#65288;PRF&#65289;&#26426;&#21046;&#31867;&#20013;&#65292;SimRFs&#25552;&#20379;&#20102;&#26368;&#23567;&#21487;&#33021;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#65292;&#22312;&#27809;&#26377;&#35266;&#23519;&#21040;&#39069;&#22806;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#26126;&#26174;&#20248;&#20110;&#20808;&#21069;&#26368;&#20934;&#30830;&#30340;&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#25104;&#26412;&#26356;&#39640;&#30340;SimRFs+&#21464;&#31181;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#26356;&#24191;&#27867;&#30340;&#26435;&#37325;&#30456;&#20851;&#20960;&#20309;&#32806;&#21512;&#26041;&#26696;&#26063;&#20013;&#65288;&#20801;&#35768;&#38543;&#26426;&#21521;&#37327;&#26041;&#21521;&#21644;&#33539;&#25968;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65289;&#65292;&#23427;&#26159;&#28176;&#36817;&#20248;&#21270;&#30340;&#12290;&#22312;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SimRFs&#22312;&#21253;&#25324;&#36880;&#28857;&#26680;&#20272;&#35745;&#12289;&#38750;&#21442;&#25968;&#20998;&#31867;&#21644;&#21487;&#25193;&#23637;&#30340;Transformer&#20013;&#25552;&#20379;&#30340;&#19968;&#33268;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Simplex Random Features (SimRFs), a new random feature (RF) mechanism for unbiased approximation of the softmax and Gaussian kernels by geometrical correlation of random projection vectors. We prove that SimRFs provide the smallest possible mean square error (MSE) on unbiased estimates of these kernels among the class of weight-independent geometrically-coupled positive random feature (PRF) mechanisms, substantially outperforming the previously most accurate Orthogonal Random Features at no observable extra cost. We present a more computationally expensive SimRFs+ variant, which we prove is asymptotically optimal in the broader family of weight-dependent geometrical coupling schemes (which permit correlations between random vector directions and norms). In extensive empirical studies, we show consistent gains provided by SimRFs in settings including pointwise kernel estimation, nonparametric classification and scalable Transformers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#30340;&#30452;&#25509;&#22522;&#20110;&#20559;&#22909;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#21644;&#35774;&#35745;&#26032;&#30340;&#31574;&#30053;&#35780;&#20998;&#25351;&#26631;&#65292;&#33021;&#22815;&#20174;&#32473;&#23450;&#30340;&#20559;&#22909;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12842</link><description>&lt;p&gt;
&#19981;&#20381;&#36182;&#22870;&#21169;&#27169;&#22411;&#30340;&#30452;&#25509;&#22522;&#20110;&#20559;&#22909;&#30340;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Direct Preference-based Policy Optimization without Reward Modeling. (arXiv:2301.12842v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#30340;&#30452;&#25509;&#22522;&#20110;&#20559;&#22909;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#21644;&#35774;&#35745;&#26032;&#30340;&#31574;&#30053;&#35780;&#20998;&#25351;&#26631;&#65292;&#33021;&#22815;&#20174;&#32473;&#23450;&#30340;&#20559;&#22909;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;(PbRL)&#26159;&#19968;&#31181;&#20351;RL&#20195;&#29702;&#33021;&#22815;&#20174;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22312;&#21046;&#23450;&#22870;&#21169;&#20989;&#25968;&#26102;&#23384;&#22312;&#25361;&#25112;&#30340;&#24773;&#20917;&#12290;&#29616;&#26377;&#30340;PbRL&#26041;&#27861;&#19968;&#33324;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#39318;&#20808;&#26681;&#25454;&#32473;&#23450;&#30340;&#20559;&#22909;&#25968;&#25454;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#27169;&#22411;&#37319;&#29992;&#29616;&#25104;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20165;&#36890;&#36807;&#20559;&#22909;&#20449;&#24687;&#33719;&#21462;&#20934;&#30830;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22312;&#20559;&#22909;&#26469;&#33258;&#20154;&#31867;&#25945;&#24072;&#26102;&#65292;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20219;&#20309;&#22870;&#21169;&#27169;&#22411;&#30340;&#30452;&#25509;&#20174;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;PbRL&#31639;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#35780;&#20998;&#25351;&#26631;&#65292;&#20026;&#19982;&#32473;&#23450;&#20559;&#22909;&#19968;&#33268;&#30340;&#31574;&#30053;&#20998;&#37197;&#39640;&#20998;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#24212;&#29992;&#20110;&#24102;&#26377;&#23454;&#38469;&#20154;&#31867;&#20559;&#22909;&#26631;&#31614;&#30340;&#31163;&#32447;RL&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#25110;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preference-based reinforcement learning (PbRL) is an approach that enables RL agents to learn from preference, which is particularly useful when formulating a reward function is challenging. Existing PbRL methods generally involve a two-step procedure: they first learn a reward model based on given preference data and then employ off-the-shelf reinforcement learning algorithms using the learned reward model. However, obtaining an accurate reward model solely from preference information, especially when the preference is from human teachers, can be difficult. Instead, we propose a PbRL algorithm that directly learns from preference without requiring any reward modeling. To achieve this, we adopt a contrastive learning framework to design a novel policy scoring metric that assigns a high score to policies that align with the given preferences. We apply our algorithm to offline RL tasks with actual human preference labels and show that our algorithm outperforms or is on par with the exist
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857; &#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26435;&#37325;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#35206;&#30422;&#19981;&#36275;&#30340;&#22797;&#26434;&#29615;&#22659;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#36895;&#29575;&#21644;&#31574;&#30053;&#35206;&#30422;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2301.12714</link><description>&lt;p&gt;
&#26435;&#37325;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#29992;&#20110;&#20248;&#21270;&#20445;&#23432;&#22411;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Importance Weighted Actor-Critic for Optimal Conservative Offline Reinforcement Learning. (arXiv:2301.12714v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12714
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857; &#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26435;&#37325;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#35206;&#30422;&#19981;&#36275;&#30340;&#22797;&#26434;&#29615;&#22659;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#36895;&#29575;&#21644;&#31574;&#30053;&#35206;&#30422;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;A-Crab&#65288;&#24179;&#22343;&#36125;&#23572;&#26364;&#35823;&#24046;&#27491;&#21017;&#21270;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#35206;&#30422;&#19981;&#36275;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#23454;&#29992;&#26032;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;&#36793;&#38469;&#21270;&#37325;&#35201;&#24615;&#37319;&#26679;&#26694;&#26550;&#19982;&#28436;&#21592;-&#35780;&#35770;&#23478;&#33539;&#24335;&#30456;&#32467;&#21512;&#65292;&#20854;&#20013;&#35780;&#35770;&#23478;&#36820;&#22238;&#30456;&#23545;&#20110;&#31163;&#32447;&#25968;&#25454;&#24754;&#35266;&#30340;&#28436;&#21592;&#65288;&#31574;&#30053;&#65289;&#35780;&#20272;&#65292;&#24182;&#20855;&#26377;&#23567;&#30340;&#24179;&#22343;&#65288;&#26435;&#37325;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#65289;&#36125;&#23572;&#26364;&#35823;&#24046;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21516;&#26102;&#20855;&#26377;&#20197;&#19979;&#20248;&#21183;&#65306;&#65288;1&#65289;&#22312;&#25910;&#25947;&#21040;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#26368;&#20339;&#31574;&#30053;&#26102;&#65292;&#21363;&#20351;&#19982;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#32467;&#21512;&#20351;&#29992;&#65292;&#20063;&#33021;&#36798;&#21040;&#26368;&#20339;&#32479;&#35745;&#36895;&#29575;$1/\sqrt{N}$&#65292;&#20854;&#20013;$N$&#26159;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#12290;&#65288;2&#65289;&#23427;&#20381;&#36182;&#20110;&#36739;&#24369;&#30340;&#31574;&#30053;&#35206;&#30422;&#24179;&#22343;&#27010;&#24565;&#65288;&#19982;$l_\infty$&#21333;&#31574;&#30053;&#38598;&#20013;&#24615;&#30456;&#27604;&#65289;&#65292;&#21033;&#29992;&#31574;&#30053;&#35775;&#38382;&#32467;&#26500;&#12290;&#65288;3&#65289;&#23427;&#20248;&#20110;&#25968;&#25454;&#25910;&#38598;
&lt;/p&gt;
&lt;p&gt;
We propose A-Crab (Actor-Critic Regularized by Average Bellman error), a new practical algorithm for offline reinforcement learning (RL) in complex environments with insufficient data coverage. Our algorithm combines the marginalized importance sampling framework with the actor-critic paradigm, where the critic returns evaluations of the actor (policy) that are pessimistic relative to the offline data and have a small average (importance-weighted) Bellman error. Compared to existing methods, our algorithm simultaneously offers a number of advantages: (1) It achieves the optimal statistical rate of $1/\sqrt{N}$ -- where $N$ is the size of offline dataset -- in converging to the best policy covered in the offline dataset, even when combined with general function approximators. (2) It relies on a weaker average notion of policy coverage (compared to the $\ell_\infty$ single-policy concentrability) that exploits the structure of policy visitations. (3) It outperforms the data-collection be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedRC&#30340;&#26032;&#22411;&#32858;&#31867;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#22810;&#26679;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#40065;&#26834;&#24615;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#36827;&#29616;&#26377;&#32858;&#31867;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.12379</link><description>&lt;p&gt;
FedRC&#65306;&#36890;&#36807;&#40065;&#26834;&#32858;&#31867;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#22810;&#26679;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
FedRC: Tackling Diverse Distribution Shifts Challenge in Federated Learning by Robust Clustering. (arXiv:2301.12379v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedRC&#30340;&#26032;&#22411;&#32858;&#31867;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#22810;&#26679;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#40065;&#26834;&#24615;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#36827;&#29616;&#26377;&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#20445;&#30041;&#23458;&#25143;&#31471;&#25968;&#25454;&#26469;&#20445;&#25252;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23398;&#20064;&#31995;&#32479;&#30340;&#22810;&#26679;&#24615;&#21644;&#24322;&#36136;&#24615;&#65292;&#20248;&#21270;&#32852;&#37030;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#20250;&#38754;&#20020;&#25361;&#25112;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#24403;&#23458;&#25143;&#31471;&#20043;&#38388;&#20986;&#29616;&#20998;&#24067;&#36716;&#31227;&#26102;&#25913;&#21892;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21270;&#65292;&#20294;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#21516;&#26102;&#21457;&#29983;&#22810;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#36716;&#31227;&#65292;&#20363;&#22914;&#29305;&#24449;&#20998;&#24067;&#36716;&#31227;&#12289;&#26631;&#31614;&#20998;&#24067;&#36716;&#31227;&#21644;&#27010;&#24565;&#36716;&#31227;&#26102;&#65292;&#22914;&#20309;&#30830;&#20445;&#20840;&#23616;&#24615;&#33021;&#20173;&#28982;&#26159;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22810;&#26679;&#20998;&#24067;&#36716;&#31227;&#21516;&#26102;&#21457;&#29983;&#26102;&#25152;&#24102;&#26469;&#30340;&#23398;&#20064;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#31867;&#21407;&#21017;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#35299;&#20915;&#32858;&#31867;&#21407;&#21017;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#31639;&#27861;&#26694;&#26550;&#8212;&#8212;FedRC&#65292;&#23427;&#36981;&#24490;&#25105;&#20204;&#25552;&#20986;&#30340;&#32858;&#31867;&#21407;&#21017;&#65292;&#36890;&#36807;&#21253;&#21547;&#40065;&#26834;&#24615;&#25439;&#22833;&#20989;&#25968;&#25913;&#36827;&#29616;&#26377;&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning paradigm that safeguards privacy by retaining client data on edge devices. However, optimizing FL in practice can be challenging due to the diverse and heterogeneous nature of the learning system. Though recent research has focused on improving the optimization of FL when distribution shifts occur among clients, ensuring global performance when multiple types of distribution shifts occur simultaneously among clients -- such as feature distribution shift, label distribution shift, and concept shift -- remain under-explored.  In this paper, we identify the learning challenges posed by the simultaneous occurrence of diverse distribution shifts and propose a clustering principle to overcome these challenges. Through our research, we find that existing methods failed to address the clustering principle. Therefore, we propose a novel clustering algorithm framework, dubbed as FedRC, which adheres to our proposed clustering principle by incorporati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#31526;&#21495;&#38899;&#20048;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#23383;&#33410;&#23545;&#32534;&#30721;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#24207;&#21015;&#38271;&#24230;&#24182;&#22686;&#21152;&#35789;&#27719;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11975</link><description>&lt;p&gt;
&#31526;&#21495;&#38899;&#20048;&#30340;&#23383;&#33410;&#23545;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Byte Pair Encoding for Symbolic Music. (arXiv:2301.11975v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#31526;&#21495;&#38899;&#20048;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#23383;&#33410;&#23545;&#32534;&#30721;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#24207;&#21015;&#38271;&#24230;&#24182;&#22686;&#21152;&#35789;&#27719;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#19982;&#28145;&#24230;&#23398;&#20064;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#31526;&#21495;&#38899;&#20048;&#36890;&#24120;&#19982;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#30456;&#32467;&#21512;&#12290;&#20026;&#27492;&#65292;&#38899;&#20048;&#38656;&#35201;&#36827;&#34892;&#26631;&#35760;&#21270;&#65292;&#21363;&#36716;&#21270;&#20026;&#19968;&#31995;&#21015;&#31163;&#25955;&#30340;&#26631;&#35760;&#12290;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#26041;&#27861;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#22240;&#20026;&#38899;&#20048;&#21487;&#20197;&#30001;&#21516;&#26102;&#23384;&#22312;&#30340;&#36712;&#36947;&#65292;&#20855;&#26377;&#22810;&#20010;&#23646;&#24615;&#30340;&#21516;&#26102;&#38899;&#31526;&#32452;&#25104;&#12290;&#30446;&#21069;&#65292;&#25152;&#25552;&#20986;&#30340;&#26631;&#35760;&#21270;&#20381;&#36182;&#20110;&#25551;&#36848;&#38899;&#31526;&#23646;&#24615;&#21644;&#26102;&#38388;&#20107;&#20214;&#30340;&#23567;&#22411;&#26631;&#35760;&#23383;&#20856;&#65292;&#23548;&#33268;&#26631;&#35760;&#24207;&#21015;&#30456;&#24403;&#38271;&#65292;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#30340;&#20351;&#29992;&#19981;&#22815;&#20248;&#21270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#21512;&#24182;&#23884;&#20837;&#25110;&#32452;&#21512;&#26631;&#35760;&#26469;&#20943;&#23569;&#25972;&#20307;&#24207;&#21015;&#38271;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23383;&#33410;&#23545;&#32534;&#30721;&#65292;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;&#20854;&#26174;&#33879;&#20943;&#23567;&#20102;&#24207;&#21015;&#38271;&#24230;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#35789;&#27719;&#37327;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#23884;&#20837;&#33021;&#21147;&#19982;&#26356;&#26377;&#34920;&#29616;&#21147;&#30340;&#26631;&#35760;&#32467;&#21512;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
When used with deep learning, the symbolic music modality is often coupled with language model architectures. To do so, the music needs to be tokenized, i.e. converted into a sequence of discrete tokens. This can be achieved by different approaches, as music can be composed of simultaneous tracks, of simultaneous notes with several attributes. Until now, the proposed tokenizations rely on small vocabularies of tokens describing the note attributes and time events, resulting in fairly long token sequences, and a sub-optimal use of the embedding space of language models. Recent research has put efforts on reducing the overall sequence length by merging embeddings or combining tokens. In this paper, we show that Byte Pair Encoding, a compression technique widely used for natural language, significantly decreases the sequence length while increasing the vocabulary size. By doing so, we leverage the embedding capabilities of such models with more expressive tokens, resulting in both better 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25490;&#21015;&#19981;&#21464;&#21464;&#25442;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21160;&#24577;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#32534;&#35793;&#22120;&#65292;&#23454;&#29616;&#20102;&#39640;GPU&#21033;&#29992;&#29575;&#21644;&#20302;&#35206;&#30422;&#28010;&#36153;&#12290;</title><link>http://arxiv.org/abs/2301.10936</link><description>&lt;p&gt;
PIT: &#36890;&#36807;&#25490;&#21015;&#19981;&#21464;&#21464;&#25442;&#20248;&#21270;&#21160;&#24577;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PIT: Optimization of Dynamic Sparse Deep Learning Models via Permutation Invariant Transformation. (arXiv:2301.10936v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10936
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25490;&#21015;&#19981;&#21464;&#21464;&#25442;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21160;&#24577;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#32534;&#35793;&#22120;&#65292;&#23454;&#29616;&#20102;&#39640;GPU&#21033;&#29992;&#29575;&#21644;&#20302;&#35206;&#30422;&#28010;&#36153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31232;&#30095;&#24615;&#65292;&#22312;&#36816;&#34892;&#26102;&#25165;&#30830;&#23450;&#31232;&#30095;&#27169;&#24335;&#65292;&#23545;&#28145;&#24230;&#23398;&#20064;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#31232;&#30095;&#24863;&#30693;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#30001;&#20110;&#19982;&#39044;&#22788;&#29702;&#30456;&#20851;&#30340;&#37325;&#22823;&#24320;&#38144;&#65292;&#20165;&#38480;&#20110;&#39044;&#23450;&#20041;&#30340;&#38745;&#24577;&#31232;&#30095;&#27169;&#24335;&#12290;&#21160;&#24577;&#31232;&#30095;&#35745;&#31639;&#30340;&#39640;&#25928;&#25191;&#34892;&#24120;&#24120;&#38754;&#20020;GPU&#21451;&#22909;&#29943;&#30742;&#37197;&#32622;&#19982;&#26368;&#23567;&#21270;&#35206;&#30422;&#28010;&#36153;&#65288;&#24352;&#37327;&#20013;&#30340;&#38750;&#38646;&#20540;&#65289;&#30340;&#31232;&#30095;&#24863;&#30693;&#29943;&#30742;&#24418;&#29366;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PIT&#65292;&#19968;&#31181;&#38024;&#23545;&#21160;&#24577;&#31232;&#30095;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#32534;&#35793;&#22120;&#12290;PIT&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24179;&#38138;&#26426;&#21046;&#65292;&#21033;&#29992;&#25968;&#23398;&#35777;&#26126;&#30340;&#25490;&#21015;&#19981;&#21464;&#21464;&#25442;(PIT)&#23558;&#22810;&#20010;&#31232;&#30095;&#20301;&#32622;&#30340;&#24494;&#29943;&#30742;&#36716;&#21464;&#20026;GPU&#39640;&#25928;&#30340;&#31264;&#23494;&#29943;&#30742;&#65292;&#32780;&#19981;&#25913;&#21464;&#35745;&#31639;&#32467;&#26524;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;GPU&#21033;&#29992;&#29575;&#21644;&#20302;&#35206;&#30422;&#28010;&#36153;&#12290;&#32473;&#23450;&#19968;&#20010;&#27169;&#22411;&#65292;PIT&#39318;&#20808;&#25214;&#21040;&#20854;&#25152;&#26377;&#21487;&#34892;&#30340;PIT&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Dynamic sparsity, where the sparsity patterns are unknown until runtime, poses a significant challenge to deep learning. The state-of-the-art sparsity-aware deep learning solutions are restricted to pre-defined, static sparsity patterns due to significant overheads associated with preprocessing. Efficient execution of dynamic sparse computation often faces the misalignment between the GPU-friendly tile configuration for efficient execution and the sparsity-aware tile shape that minimizes coverage wastes (non-zero values in tensor).  In this paper, we propose PIT, a deep-learning compiler for dynamic sparsity. PIT proposes a novel tiling mechanism that leverages Permutation Invariant Transformation (PIT), a mathematically proven property, to transform multiple sparsely located micro-tiles into a GPU-efficient dense tile without changing the computation results, thus achieving both high GPU utilization and low coverage waste. Given a model, PIT first finds feasible PIT rules for all its 
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#38598;&#31934;&#31616;&#65288;DD&#65289;&#26159;&#19968;&#31181;&#20174;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20986;&#21253;&#21547;&#21512;&#25104;&#26679;&#26412;&#30340;&#36739;&#23567;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#22312;&#20943;&#36731;&#25968;&#25454;&#23384;&#20648;&#21644;&#20256;&#36755;&#36127;&#25285;&#30340;&#21516;&#26102;&#20445;&#35777;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#24615;&#33021;&#19982;&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;DD&#21450;&#20854;&#24212;&#29992;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2301.07014</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#31934;&#31616;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Dataset Distillation: A Comprehensive Review. (arXiv:2301.07014v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07014
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#31934;&#31616;&#65288;DD&#65289;&#26159;&#19968;&#31181;&#20174;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20986;&#21253;&#21547;&#21512;&#25104;&#26679;&#26412;&#30340;&#36739;&#23567;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#22312;&#20943;&#36731;&#25968;&#25454;&#23384;&#20648;&#21644;&#20256;&#36755;&#36127;&#25285;&#30340;&#21516;&#26102;&#20445;&#35777;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#24615;&#33021;&#19982;&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;DD&#21450;&#20854;&#24212;&#29992;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#20027;&#35201;&#24402;&#21151;&#20110;&#22823;&#37327;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#22686;&#21152;&#20102;&#23384;&#20648;&#21644;&#20256;&#36755;&#30340;&#36127;&#25285;&#65292;&#24182;&#36827;&#19968;&#27493;&#23548;&#33268;&#20102;&#32321;&#29712;&#30340;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#12290;&#21478;&#22806;&#65292;&#20381;&#38752;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#23384;&#22312;&#38544;&#31169;&#21644;&#29256;&#26435;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25968;&#25454;&#38598;&#31934;&#31616;&#65288;DD&#65289;&#34987;&#24341;&#20837;&#65292;&#24182;&#22312;&#23398;&#26415;&#30028;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;DD&#26088;&#22312;&#20174;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20986;&#21253;&#21547;&#21512;&#25104;&#26679;&#26412;&#30340;&#36739;&#23567;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#24615;&#33021;&#21487;&#19982;&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#26412;&#25991;&#23545;DD&#21450;&#20854;&#24212;&#29992;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#21644;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent success of deep learning is largely attributed to the sheer amount of data used for training deep neural networks.Despite the unprecedented success, the massive data, unfortunately, significantly increases the burden on storage and transmission and further gives rise to a cumbersome model training process. Besides, relying on the raw data for training \emph{per se} yields concerns about privacy and copyright. To alleviate these shortcomings, dataset distillation~(DD), also known as dataset condensation (DC), was introduced and has recently attracted much research attention in the community. Given an original dataset, DD aims to derive a much smaller dataset containing synthetic samples, based on which the trained models yield performance comparable with those trained on the original dataset. In this paper, we give a comprehensive review and summary of recent advances in DD and its application. We first introduce the task formally and propose an overall algorithmic framework foll
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#23569;&#37327;&#26597;&#35810;&#35775;&#38382;&#65292;&#21487;&#20197;&#31363;&#21462;&#30446;&#26631;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#26410;&#26631;&#27880;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#20027;&#21160;&#25277;&#26679;&#21644;&#26597;&#35810;&#30340;&#26041;&#24335;&#25552;&#21462;&#30446;&#26631;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20102;&#35299;&#20854;&#27169;&#22411;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2211.16044</link><description>&lt;p&gt;
&#38024;&#23545;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Model Extraction Attack against Self-supervised Speech Models. (arXiv:2211.16044v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#23569;&#37327;&#26597;&#35810;&#35775;&#38382;&#65292;&#21487;&#20197;&#31363;&#21462;&#30446;&#26631;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#26410;&#26631;&#27880;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#20027;&#21160;&#25277;&#26679;&#21644;&#26597;&#35810;&#30340;&#26041;&#24335;&#25552;&#21462;&#30446;&#26631;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20102;&#35299;&#20854;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#35821;&#38899;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#32473;&#23450;&#29255;&#27573;&#30340;&#26377;&#24847;&#20041;&#34920;&#31034;&#65292;&#24182;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20196;&#20154;&#38590;&#20197;&#32622;&#20449;&#30340;&#24615;&#33021;&#12290;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#65288;MEA&#65289;&#36890;&#24120;&#25351;&#30340;&#26159;&#25915;&#20987;&#32773;&#20165;&#36890;&#36807;&#26597;&#35810;&#35775;&#38382;&#23601;&#33021;&#31363;&#21462;&#21463;&#23475;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20165;&#26377;&#23569;&#37327;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#38024;&#23545;SSL&#35821;&#38899;&#27169;&#22411;&#30340;MEA&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26694;&#26550;&#26469;&#25552;&#21462;&#27169;&#22411;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#23545;&#22823;&#35268;&#27169;&#26410;&#26631;&#27880;&#35821;&#26009;&#24211;&#36827;&#34892;SSL&#39044;&#35757;&#32451;&#65292;&#24471;&#21040;&#19968;&#20010;&#23567;&#30340;&#35821;&#38899;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20027;&#21160;&#20174;&#26410;&#26631;&#27880;&#35821;&#26009;&#24211;&#20013;&#25277;&#26679;&#19968;&#23567;&#37096;&#20998;&#29255;&#27573;&#65292;&#24182;&#29992;&#36825;&#20123;&#29255;&#27573;&#21521;&#30446;&#26631;&#27169;&#22411;&#36827;&#34892;&#26597;&#35810;&#65292;&#20197;&#33719;&#24471;&#23427;&#20204;&#30340;&#34920;&#31034;&#20316;&#20026;&#23567;&#27169;&#22411;&#31532;&#20108;&#38454;&#27573;&#35757;&#32451;&#30340;&#26631;&#31614;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25277;&#26679;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#21462;&#30446;&#26631;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#30693;&#36947;&#20854;&#20219;&#20309;&#26377;&#20851;&#27169;&#22411;&#26550;&#26500;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) speech models generate meaningful representations of given clips and achieve incredible performance across various downstream tasks. Model extraction attack (MEA) often refers to an adversary stealing the functionality of the victim model with only query access. In this work, we study the MEA problem against SSL speech model with a small number of queries. We propose a two-stage framework to extract the model. In the first stage, SSL is conducted on the large-scale unlabeled corpus to pre-train a small speech model. Secondly, we actively sample a small portion of clips from the unlabeled corpus and query the target model with these clips to acquire their representations as labels for the small model's second-stage training. Experiment results show that our sampling methods can effectively extract the target model without knowing any information about its model architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FaiREE&#31639;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#21487;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#20844;&#24179;&#20998;&#31867;&#31639;&#27861;&#65292;&#24182;&#19988;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#21644;&#26080;&#20998;&#24067;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2211.15072</link><description>&lt;p&gt;
FaiREE&#65306;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#21644;&#26080;&#20998;&#24067;&#20445;&#35777;&#30340;&#20844;&#24179;&#20998;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FaiREE: Fair Classification with Finite-Sample and Distribution-Free Guarantee. (arXiv:2211.15072v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FaiREE&#31639;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#21487;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#20844;&#24179;&#20998;&#31867;&#31639;&#27861;&#65292;&#24182;&#19988;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#21644;&#26080;&#20998;&#24067;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20844;&#24179;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#21457;&#25381;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#32676;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#21644;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20844;&#24179;&#20998;&#31867;&#26041;&#27861;&#30340;&#20844;&#24179;&#20445;&#35777;&#20027;&#35201;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#25968;&#25454;&#20998;&#24067;&#20551;&#35774;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#26679;&#26412;&#37327;&#65292;&#24182;&#19988;&#22312;&#26679;&#26412;&#37327;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#36829;&#21453;&#20844;&#24179;&#24615;&#65292;&#32780;&#36825;&#22312;&#23454;&#36341;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FaiREE&#31639;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#20844;&#24179;&#20998;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#26679;&#26412;&#21644;&#26080;&#20998;&#24067;&#29702;&#35770;&#20445;&#35777;&#19979;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#12290;FaiREE&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#32676;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#26426;&#20250;&#24179;&#31561;&#65292;&#24179;&#34913;&#20960;&#29575;&#65292;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#34913;&#31561;&#65289;&#24182;&#23454;&#29616;&#26368;&#20339;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#29702;&#35770;&#20445;&#35777;&#36827;&#19968;&#27493;&#24471;&#21040;&#20102;&#23545;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#23454;&#39564;&#25903;&#25345;&#12290;FaiREE&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic fairness plays an increasingly critical role in machine learning research. Several group fairness notions and algorithms have been proposed. However, the fairness guarantee of existing fair classification methods mainly depends on specific data distributional assumptions, often requiring large sample sizes, and fairness could be violated when there is a modest number of samples, which is often the case in practice. In this paper, we propose FaiREE, a fair classification algorithm that can satisfy group fairness constraints with finite-sample and distribution-free theoretical guarantees. FaiREE can be adapted to satisfy various group fairness notions (e.g., Equality of Opportunity, Equalized Odds, Demographic Parity, etc.) and achieve the optimal accuracy. These theoretical guarantees are further supported by experiments on both synthetic and real data. FaiREE is shown to have favorable performance over state-of-the-art algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22122;&#22768;&#22810;&#22336;&#20449;&#36947;&#19978;&#36827;&#34892;&#20998;&#24067;&#24335;&#22270;&#20687;&#20256;&#36755;&#30340;&#28145;&#24230;&#32852;&#21512;&#28304;&#8212;&#20449;&#36947;&#32534;&#30721;&#26041;&#26696;&#12290;&#36890;&#36807;&#22312;&#38750;&#27491;&#20132;&#22810;&#22336;&#25509;&#20837;&#26041;&#24335;&#19979;&#36827;&#34892;&#32852;&#21512;&#22270;&#20687;&#21387;&#32553;&#21644;&#20256;&#36755;&#65292;&#21487;&#20197;&#23454;&#29616;&#37325;&#26500;&#22270;&#20687;&#36136;&#37327;&#30340;&#26174;&#33879;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2211.09920</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#28145;&#24230;&#32852;&#21512;&#28304;&#8212;&#20449;&#36947;&#32534;&#30721;&#22312;&#22810;&#22336;&#20449;&#36947;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Distributed Deep Joint Source-Channel Coding over a Multiple Access Channel. (arXiv:2211.09920v2 [eess.IV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22122;&#22768;&#22810;&#22336;&#20449;&#36947;&#19978;&#36827;&#34892;&#20998;&#24067;&#24335;&#22270;&#20687;&#20256;&#36755;&#30340;&#28145;&#24230;&#32852;&#21512;&#28304;&#8212;&#20449;&#36947;&#32534;&#30721;&#26041;&#26696;&#12290;&#36890;&#36807;&#22312;&#38750;&#27491;&#20132;&#22810;&#22336;&#25509;&#20837;&#26041;&#24335;&#19979;&#36827;&#34892;&#32852;&#21512;&#22270;&#20687;&#21387;&#32553;&#21644;&#20256;&#36755;&#65292;&#21487;&#20197;&#23454;&#29616;&#37325;&#26500;&#22270;&#20687;&#36136;&#37327;&#30340;&#26174;&#33879;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#28145;&#24230;&#32852;&#21512;&#28304;&#8212;&#20449;&#36947;&#32534;&#30721;&#65288;DeepJSCC&#65289;&#22312;&#22122;&#22768;&#22810;&#22336;&#20449;&#36947;&#65288;MAC&#65289;&#19978;&#36827;&#34892;&#20998;&#24067;&#24335;&#22270;&#20687;&#20256;&#36755;&#12290;&#24050;&#30693;&#22312;&#28176;&#36817;&#26080;&#38480;&#38271;&#24230;&#22359;&#30340;&#24773;&#20917;&#19979;&#65292;&#39321;&#20892;&#30340;&#20998;&#31163;&#23450;&#29702;&#25104;&#31435;&#65292;&#29992;&#20110;&#20256;&#36755;&#29420;&#31435;&#30340;&#28304;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#23454;&#38469;&#26377;&#38480;&#38271;&#24230;&#22359;&#30340;&#24773;&#20917;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21333;&#29420;&#30340;&#28304;&#32534;&#30721;&#21644;&#20449;&#36947;&#32534;&#30721;&#34987;&#35748;&#20026;&#26159;&#27425;&#20248;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#21512;&#22270;&#20687;&#21387;&#32553;&#21644;&#20256;&#36755;&#26041;&#26696;&#65292;&#20854;&#20013;&#35774;&#22791;&#20197;&#38750;&#27491;&#20132;&#30340;&#26041;&#24335;&#21457;&#36865;&#23427;&#20204;&#30340;&#21387;&#32553;&#22270;&#20687;&#34920;&#31034;&#12290;&#34429;&#28982;&#38750;&#27491;&#20132;&#22810;&#22336;&#25509;&#20837;&#65288;NOMA&#65289;&#24050;&#30693;&#21487;&#20197;&#36798;&#21040;&#23481;&#37327;&#21306;&#22495;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#38750;&#27491;&#20132;&#32852;&#21512;&#28304;&#20449;&#36947;&#32534;&#30721;&#65288;JSCC&#65289;&#26041;&#26696;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#23578;&#26410;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#37319;&#29992;&#24403;&#21069;&#30340;DeepJSCC&#26041;&#27861;&#36827;&#34892;&#27491;&#20132;&#20256;&#36755;&#30456;&#27604;&#65292;&#37325;&#24314;&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider distributed image transmission over a noisy multiple access channel (MAC) using deep joint source-channel coding (DeepJSCC). It is known that Shannon's separation theorem holds when transmitting independent sources over a MAC in the asymptotic infinite block length regime. However, we are interested in the practical finite block length regime, in which case separate source and channel coding is known to be suboptimal. We introduce a novel joint image compression and transmission scheme, where the devices send their compressed image representations in a non-orthogonal manner. While non-orthogonal multiple access (NOMA) is known to achieve the capacity region, to the best of our knowledge, non-orthogonal joint source channel coding (JSCC) scheme for practical systems has not been studied before. Through extensive experiments, we show significant improvements in terms of the quality of the reconstructed images compared to orthogonal transmission employing current DeepJSCC appr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;Softmax&#32452;&#20214;&#20013;&#24341;&#20837;&#26799;&#24230;&#34928;&#20943;&#36229;&#21442;&#25968;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#27867;&#21270;&#24615;&#33021;&#19982;&#26799;&#24230;&#34928;&#20943;&#29575;&#26174;&#33879;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#36739;&#23567;&#30340;&#26799;&#24230;&#34928;&#20943;&#30340;&#20248;&#21270;&#26041;&#27861;&#31867;&#20284;&#20110;&#35838;&#31243;&#23398;&#20064;&#24207;&#21015;&#65292;&#20351;&#24471;&#22256;&#38590;&#26679;&#26412;&#22312;&#26131;&#26679;&#26412;&#30830;&#20449;&#20043;&#21518;&#24471;&#21040;&#20851;&#27880;&#12290;&#22823;&#36793;&#38469;Softmax&#20250;&#24433;&#21709;&#23616;&#37096;Lipschitz&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2210.17145</link><description>&lt;p&gt;
&#22823;&#36793;&#38469;Softmax&#20013;&#30340;&#27010;&#29575;&#30456;&#20851;&#26799;&#24230;&#34928;&#20943;
&lt;/p&gt;
&lt;p&gt;
Probability-Dependent Gradient Decay in Large Margin Softmax. (arXiv:2210.17145v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;Softmax&#32452;&#20214;&#20013;&#24341;&#20837;&#26799;&#24230;&#34928;&#20943;&#36229;&#21442;&#25968;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#27867;&#21270;&#24615;&#33021;&#19982;&#26799;&#24230;&#34928;&#20943;&#29575;&#26174;&#33879;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#36739;&#23567;&#30340;&#26799;&#24230;&#34928;&#20943;&#30340;&#20248;&#21270;&#26041;&#27861;&#31867;&#20284;&#20110;&#35838;&#31243;&#23398;&#20064;&#24207;&#21015;&#65292;&#20351;&#24471;&#22256;&#38590;&#26679;&#26412;&#22312;&#26131;&#26679;&#26412;&#30830;&#20449;&#20043;&#21518;&#24471;&#21040;&#20851;&#27880;&#12290;&#22823;&#36793;&#38469;Softmax&#20250;&#24433;&#21709;&#23616;&#37096;Lipschitz&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;Softmax&#24050;&#32463;&#25104;&#20026;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#24120;&#35265;&#30340;&#32452;&#20214;&#12290;&#26412;&#25991;&#22312;Softmax&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26799;&#24230;&#34928;&#20943;&#36229;&#21442;&#25968;&#65292;&#20197;&#25511;&#21046;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#27010;&#29575;&#30456;&#20851;&#26799;&#24230;&#34928;&#20943;&#29575;&#12290;&#36890;&#36807;&#23545;&#22522;&#20110;MNIST&#12289;CIFAR-10/100&#21644;SVHN&#30340;&#21508;&#31181;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#32467;&#26524;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#27867;&#21270;&#24615;&#33021;&#19982;&#26799;&#24230;&#34928;&#20943;&#29575;&#26174;&#33879;&#30456;&#20851;&#65292;&#21363;&#38543;&#30528;&#32622;&#20449;&#27010;&#29575;&#30340;&#19978;&#21319;&#65292;&#26799;&#24230;&#20250;&#21576;&#20984;&#20989;&#25968;&#25110;&#20985;&#20989;&#25968;&#36882;&#20943;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#36739;&#23567;&#30340;&#26799;&#24230;&#34928;&#20943;&#30340;&#20248;&#21270;&#26041;&#27861;&#31867;&#20284;&#20110;&#35838;&#31243;&#23398;&#20064;&#24207;&#21015;&#65292;&#21363;&#22312;&#26131;&#26679;&#26412;&#36275;&#22815;&#30830;&#20449;&#20043;&#21518;&#65292;&#25165;&#20250;&#20851;&#27880;&#22256;&#38590;&#26679;&#26412;&#65292;&#24182;&#19988;&#23545;&#20110;&#26679;&#26412;&#20043;&#38388;&#30340;&#31867;&#20869;&#36317;&#31163;&#36739;&#22823;&#30340;&#24773;&#20917;&#20250;&#33719;&#24471;&#26356;&#39640;&#30340;&#26799;&#24230;&#20197;&#20943;&#23567;&#36317;&#31163;&#12290;&#26681;&#25454;&#20998;&#26512;&#32467;&#26524;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#20379;&#35777;&#25454;&#35777;&#26126;&#22823;&#36793;&#38469;Softmax&#23558;&#24433;&#21709;&#23616;&#37096;Lipschitz&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few years, Softmax has become a common component in neural network frameworks. In this paper, a gradient decay hyperparameter is introduced in Softmax to control the probability-dependent gradient decay rate during training. By following the theoretical analysis and empirical results of a variety of model architectures trained on MNIST, CIFAR-10/100 and SVHN, we find that the generalization performance depends significantly on the gradient decay rate as the confidence probability rises, i.e., the gradient decreases convexly or concavely as the sample probability increases. Moreover, optimization with the small gradient decay shows a similar curriculum learning sequence where hard samples are in the spotlight only after easy samples are convinced sufficiently, and well-separated samples gain a higher gradient to reduce intra-class distance. Based on the analysis results, we can provide evidence that the large margin Softmax will affect the local Lipschitz constraint of the l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MARGOT&#30340;&#36793;&#38469;&#26368;&#20248;&#20998;&#31867;&#26641;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#20108;&#21449;&#26641;&#32467;&#26500;&#20013;&#23884;&#22871;&#20102;&#26368;&#22823;&#36793;&#38469;&#22810;&#20803;&#36229;&#24179;&#38754;&#12290;&#19982;&#20256;&#32479;&#20915;&#31574;&#26641;&#30456;&#27604;&#65292;MARGOT&#20855;&#26377;&#26356;&#22909;&#30340;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.10567</link><description>&lt;p&gt;
&#36793;&#38469;&#26368;&#20248;&#20998;&#31867;&#26641;
&lt;/p&gt;
&lt;p&gt;
Margin Optimal Classification Trees. (arXiv:2210.10567v5 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MARGOT&#30340;&#36793;&#38469;&#26368;&#20248;&#20998;&#31867;&#26641;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#20108;&#21449;&#26641;&#32467;&#26500;&#20013;&#23884;&#22871;&#20102;&#26368;&#22823;&#36793;&#38469;&#22810;&#20803;&#36229;&#24179;&#38754;&#12290;&#19982;&#20256;&#32479;&#20915;&#31574;&#26641;&#30456;&#27604;&#65292;MARGOT&#20855;&#26377;&#26356;&#22909;&#30340;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#33021;&#22815;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#33021;&#22815;&#25552;&#20379;&#34892;&#20026;&#35299;&#37322;&#35265;&#35299;&#30340;&#20851;&#27880;&#36234;&#26469;&#36234;&#22810;&#12290;&#30001;&#20110;&#20915;&#31574;&#26641;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#23427;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#19988;&#30001;&#20110;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;(MIP)&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#24050;&#25552;&#20986;&#20102;&#22810;&#31181;&#26041;&#27861;&#23558;&#26368;&#20248;&#20998;&#31867;&#26641;(OCT)&#30340;&#35757;&#32451;&#38382;&#39064;&#24314;&#27169;&#20026;MIP&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#30340;OCT&#38382;&#39064;&#20844;&#24335;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#22312;&#20108;&#20998;&#31867;&#38382;&#39064;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#31216;&#20026;&#36793;&#38469;&#26368;&#20248;&#20998;&#31867;&#26641;(MARGOT)&#65292;&#21253;&#25324;&#23884;&#22871;&#22312;&#20108;&#21449;&#26641;&#32467;&#26500;&#20013;&#30340;&#26368;&#22823;&#36793;&#38469;&#22810;&#20803;&#36229;&#24179;&#38754;&#12290;&#20026;&#20102;&#22686;&#24378;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;MARGOT&#30340;&#20004;&#20010;&#26367;&#20195;&#29256;&#26412;&#65292;&#20854;&#20013;&#21253;&#25324;&#24341;&#20837;&#31232;&#30095;&#36229;&#24179;&#38754;&#31995;&#25968;&#30340;&#29305;&#24449;&#36873;&#25321;&#32422;&#26463;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;MARGOT&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been growing attention to interpretable machine learning models which can give explanatory insights on their behaviour. Thanks to their interpretability, decision trees have been intensively studied for classification tasks and, due to the remarkable advances in mixed integer programming (MIP), various approaches have been proposed to formulate the problem of training an Optimal Classification Tree (OCT) as a MIP model. We present a novel mixed integer quadratic formulation for the OCT problem, which exploits the generalization capabilities of Support Vector Machines for binary classification. Our model, denoted as Margin Optimal Classification Tree (MARGOT), encompasses maximum margin multivariate hyperplanes nested in a binary tree structure. To enhance the interpretability of our approach, we analyse two alternative versions of MARGOT, which include feature selection constraints inducing sparsity of the hyperplanes' coefficients. First, MARGOT has been tes
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#36719;&#33293;&#20837;&#30340;k-modes&#31639;&#27861;&#65288;SoftModes&#65289;&#65292;&#35299;&#20915;&#20102;&#32463;&#20856;k-modes&#31639;&#27861;&#22312;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#32570;&#28857;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2210.09640</link><description>&lt;p&gt;
&#20998;&#31867;&#25968;&#25454;&#32858;&#31867;&#65306;&#22522;&#20110;&#36719;&#33293;&#20837;&#30340;k-modes&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Clustering Categorical Data: Soft Rounding k-modes. (arXiv:2210.09640v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#36719;&#33293;&#20837;&#30340;k-modes&#31639;&#27861;&#65288;SoftModes&#65289;&#65292;&#35299;&#20915;&#20102;&#32463;&#20856;k-modes&#31639;&#27861;&#22312;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#32570;&#28857;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#19977;&#21313;&#24180;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;&#20110;&#20998;&#31867;&#25968;&#25454;&#20998;&#26512;&#25552;&#20986;&#20102;&#21508;&#31181;&#32858;&#31867;&#24037;&#20855;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#35768;&#22810;&#32858;&#31867;&#31639;&#27861;&#65292;&#32463;&#20856;&#30340;k-modes&#31639;&#27861;&#20173;&#28982;&#26159;&#26080;&#30417;&#30563;&#23398;&#20064;&#20998;&#31867;&#25968;&#25454;&#30340;&#28909;&#38376;&#36873;&#25321;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#21457;&#29616;&#26159;&#65292;&#22312;&#19968;&#20010;&#33258;&#28982;&#29983;&#25104;&#30340;&#22359;&#27169;&#22411;&#20013;&#65292;&#23545;&#20110;&#22823;&#33539;&#22260;&#30340;&#21442;&#25968;&#65292;k-modes&#31639;&#27861;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;k-modes&#31639;&#27861;&#30340;&#36719;&#33293;&#20837;&#21464;&#31181;&#65288;SoftModes&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#21464;&#31181;&#35299;&#20915;&#20102;k-modes&#31639;&#27861;&#22312;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;SoftModes&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last three decades, researchers have intensively explored various clustering tools for categorical data analysis. Despite the proposal of various clustering algorithms, the classical k-modes algorithm remains a popular choice for unsupervised learning of categorical data. Surprisingly, our first insight is that in a natural generative block model, the k-modes algorithm performs poorly for a large range of parameters. We remedy this issue by proposing a soft rounding variant of the k-modes algorithm (SoftModes) and theoretically prove that our variant addresses the drawbacks of the k-modes algorithm in the generative model. Finally, we empirically verify that SoftModes performs well on both synthetic and real-world datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#22810;&#23494;&#24230;DBSCAN&#31639;&#27861;&#65288;AMD-DBSCAN&#65289;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#25913;&#36827;&#30340;&#21442;&#25968;&#36866;&#24212;&#26041;&#27861;&#21644;&#37051;&#23621;&#25968;&#37327;&#26041;&#24046;&#65288;VNN&#65289;&#30340;&#24341;&#20837;&#65292;&#33021;&#22815;&#22312;&#23494;&#24230;&#21464;&#21270;&#26497;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26356;&#22909;&#30340;&#32858;&#31867;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AMD-DBSCAN&#33021;&#22815;&#24179;&#22343;&#38477;&#20302;75%&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#19968;&#20010;&#36229;&#21442;&#25968;&#65292;&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#37325;&#22797;&#21021;&#22987;&#21270;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2210.08162</link><description>&lt;p&gt;
AMD-DBSCAN&#65306;&#36866;&#24212;&#24615;&#22810;&#23494;&#24230;DBSCAN&#31639;&#27861;&#29992;&#20110;&#23494;&#24230;&#21464;&#21270;&#26497;&#22823;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
AMD-DBSCAN: An Adaptive Multi-density DBSCAN for datasets of extremely variable density. (arXiv:2210.08162v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#22810;&#23494;&#24230;DBSCAN&#31639;&#27861;&#65288;AMD-DBSCAN&#65289;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#25913;&#36827;&#30340;&#21442;&#25968;&#36866;&#24212;&#26041;&#27861;&#21644;&#37051;&#23621;&#25968;&#37327;&#26041;&#24046;&#65288;VNN&#65289;&#30340;&#24341;&#20837;&#65292;&#33021;&#22815;&#22312;&#23494;&#24230;&#21464;&#21270;&#26497;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26356;&#22909;&#30340;&#32858;&#31867;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AMD-DBSCAN&#33021;&#22815;&#24179;&#22343;&#38477;&#20302;75%&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#19968;&#20010;&#36229;&#21442;&#25968;&#65292;&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#37325;&#22797;&#21021;&#22987;&#21270;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DBSCAN&#24191;&#27867;&#24212;&#29992;&#20110;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23545;&#22810;&#23494;&#24230;&#32858;&#31867;&#38656;&#27714;&#30340;&#22686;&#21152;&#65292;&#20256;&#32479;DBSCAN&#26080;&#27861;&#22312;&#22810;&#23494;&#24230;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#33391;&#22909;&#30340;&#32858;&#31867;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#23494;&#24230;DBSCAN&#31639;&#27861;&#65288;AMD-DBSCAN&#65289;&#12290;&#22312;AMD-DBSCAN&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#21442;&#25968;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#25628;&#32034;&#22810;&#20010;&#21442;&#25968;&#23545;&#65288;&#21363;Eps&#21644;MinPts&#65289;&#65292;&#36825;&#20123;&#21442;&#25968;&#23545;&#30830;&#23450;&#20102;&#32858;&#31867;&#32467;&#26524;&#21644;&#24615;&#33021;&#65292;&#22240;&#27492;&#20801;&#35768;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#23494;&#24230;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;AMD-DBSCAN&#21482;&#38656;&#35201;&#19968;&#20010;&#36229;&#21442;&#25968;&#65292;&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#37325;&#22797;&#21021;&#22987;&#21270;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#37051;&#23621;&#25968;&#37327;&#26041;&#24046;&#65288;VNN&#65289;&#26469;&#34913;&#37327;&#27599;&#20010;&#31751;&#20043;&#38388;&#30340;&#23494;&#24230;&#24046;&#24322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;AMD-DBSCAN&#30001;&#20110;&#36739;&#20302;&#30340;&#31639;&#27861;&#22797;&#26434;&#24615;&#23558;&#25191;&#34892;&#26102;&#38388;&#24179;&#22343;&#38477;&#20302;&#20102;75&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
DBSCAN has been widely used in density-based clustering algorithms. However, with the increasing demand for Multi-density clustering, previous traditional DSBCAN can not have good clustering results on Multi-density datasets. In order to address this problem, an adaptive Multi-density DBSCAN algorithm (AMD-DBSCAN) is proposed in this paper. An improved parameter adaptation method is proposed in AMD-DBSCAN to search for multiple parameter pairs (i.e., Eps and MinPts), which are the key parameters to determine the clustering results and performance, therefore allowing the model to be applied to Multi-density datasets. Moreover, only one hyperparameter is required for AMD-DBSCAN to avoid the complicated repetitive initialization operations. Furthermore, the variance of the number of neighbors (VNN) is proposed to measure the difference in density between each cluster. The experimental results show that our AMD-DBSCAN reduces execution time by an average of 75% due to lower algorithm compl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25903;&#20184;&#36890;&#36947;&#32593;&#32476;&#20013;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20013;&#32487;&#33410;&#28857;&#30340;&#21033;&#28070;&#26368;&#22823;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#28508;&#27700;&#33351;&#20132;&#25442;&#30340;&#20877;&#24179;&#34913;&#26041;&#27861;&#65292;&#20013;&#32487;&#33410;&#28857;&#33021;&#22815;&#26368;&#22823;&#21270;&#20854;&#36153;&#29992;&#25910;&#30410;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#36827;&#34892;&#38382;&#39064;&#24314;&#27169;&#21644;&#27714;&#35299;&#12290;</title><link>http://arxiv.org/abs/2210.07302</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25903;&#20184;&#36890;&#36947;&#32593;&#32476;&#20013;&#20013;&#32487;&#33410;&#28857;&#21033;&#28070;&#26368;&#22823;&#21270;&#30340;&#20877;&#24179;&#34913;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning-based Rebalancing Policies for Profit Maximization of Relay Nodes in Payment Channel Networks. (arXiv:2210.07302v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25903;&#20184;&#36890;&#36947;&#32593;&#32476;&#20013;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20013;&#32487;&#33410;&#28857;&#30340;&#21033;&#28070;&#26368;&#22823;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#28508;&#27700;&#33351;&#20132;&#25442;&#30340;&#20877;&#24179;&#34913;&#26041;&#27861;&#65292;&#20013;&#32487;&#33410;&#28857;&#33021;&#22815;&#26368;&#22823;&#21270;&#20854;&#36153;&#29992;&#25910;&#30410;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#36827;&#34892;&#38382;&#39064;&#24314;&#27169;&#21644;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#20184;&#36890;&#36947;&#32593;&#32476;&#65288;PCNs&#65289;&#26159;&#19968;&#31181;&#23618;2&#30340;&#21306;&#22359;&#38142;&#21487;&#25193;&#23637;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20027;&#35201;&#23454;&#20307;&#26159;&#25903;&#20184;&#36890;&#36947;&#65292;&#36890;&#36807;&#22312;&#38142;&#22806;&#36827;&#34892;&#20132;&#26131;&#65292;&#20943;&#36731;&#20102;&#38142;&#19978;&#32593;&#32476;&#30340;&#36127;&#25285;&#12290;&#20855;&#26377;&#22810;&#20010;&#36890;&#36947;&#30340;&#33410;&#28857;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#27969;&#21160;&#24615;&#21644;&#30041;&#23384;&#37096;&#20998;&#25903;&#20184;&#37329;&#39069;&#20316;&#20026;&#36153;&#29992;&#26469;&#20316;&#20026;&#22810;&#36339;&#25903;&#20184;&#30340;&#20013;&#32487;&#12290;&#20013;&#32487;&#33410;&#28857;&#21487;&#33021;&#20250;&#20986;&#29616;&#19968;&#20010;&#25110;&#22810;&#20010;&#19981;&#24179;&#34913;&#30340;&#36890;&#36947;&#65292;&#22240;&#27492;&#38656;&#35201;&#35302;&#21457;&#20877;&#24179;&#34913;&#25805;&#20316;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20013;&#32487;&#33410;&#28857;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#28508;&#27700;&#33351;&#20132;&#25442;&#30340;&#20877;&#24179;&#34913;&#26041;&#27861;&#26469;&#26368;&#22823;&#21270;&#20854;&#36153;&#29992;&#25910;&#30410;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38543;&#26426;&#27169;&#22411;&#26469;&#25429;&#25417;&#20013;&#32487;&#33410;&#28857;&#35266;&#23519;&#21040;&#30340;&#38543;&#26426;&#20132;&#26131;&#21040;&#36798;&#21644;&#23450;&#26399;&#36827;&#34892;&#20877;&#24179;&#34913;&#25805;&#20316;&#30340;&#21160;&#24577;&#65292;&#24182;&#23558;&#31995;&#32479;&#28436;&#21270;&#34920;&#31034;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#23545;&#25152;&#26377;&#20877;&#24179;&#34913;&#31574;&#30053;&#30340;&#33410;&#28857;&#36130;&#23500;&#38543;&#26102;&#38388;&#30340;&#26368;&#22823;&#21270;&#38382;&#39064;&#36827;&#34892;&#20102;&#25968;&#23398;&#24314;&#27169;&#65292;&#24182;&#36817;&#20284;&#27714;&#35299;&#20102;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Payment channel networks (PCNs) are a layer-2 blockchain scalability solution, with its main entity, the payment channel, enabling transactions between pairs of nodes "off-chain," thus reducing the burden on the layer-1 network. Nodes with multiple channels can serve as relays for multihop payments by providing their liquidity and withholding part of the payment amount as a fee. Relay nodes might after a while end up with one or more unbalanced channels, and thus need to trigger a rebalancing operation. In this paper, we study how a relay node can maximize its profits from fees by using the rebalancing method of submarine swaps. We introduce a stochastic model to capture the dynamics of a relay node observing random transaction arrivals and performing occasional rebalancing operations, and express the system evolution as a Markov Decision Process. We formulate the problem of the maximization of the node's fortune over time over all rebalancing policies, and approximate the optimal solu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPUIR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33609;&#22270;&#25216;&#26415;&#23436;&#25104;&#26410;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#34917;&#20805;&#65292;&#20174;&#32780;&#36817;&#20284;&#20102;&#20840;&#20449;&#24687;&#21453;&#39304;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25511;&#21046;&#20559;&#24046;&#21644;&#27604;&#27809;&#26377;&#22870;&#21169;&#34917;&#20805;&#26041;&#27861;&#26356;&#23567;&#30340;&#26041;&#24046;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#30636;&#26102;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2210.06719</link><description>&lt;p&gt;
&#20351;&#29992;&#33609;&#22270;&#25216;&#26415;&#36827;&#34892;&#22870;&#21169;&#34917;&#20805;&#30340;&#19978;&#19979;&#25991;&#25209;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reward Imputation with Sketching for Contextual Batched Bandits. (arXiv:2210.06719v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPUIR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33609;&#22270;&#25216;&#26415;&#23436;&#25104;&#26410;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#34917;&#20805;&#65292;&#20174;&#32780;&#36817;&#20284;&#20102;&#20840;&#20449;&#24687;&#21453;&#39304;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25511;&#21046;&#20559;&#24046;&#21644;&#27604;&#27809;&#26377;&#22870;&#21169;&#34917;&#20805;&#26041;&#27861;&#26356;&#23567;&#30340;&#26041;&#24046;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#30636;&#26102;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#25209;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#35774;&#32622;&#65292;&#20854;&#20013;&#22312;&#27599;&#20010;episode&#32467;&#26463;&#26102;&#35266;&#23519;&#21040;&#29615;&#22659;&#20013;&#30340;&#19968;&#25209;&#22870;&#21169;&#65292;&#20294;&#26159;&#26410;&#25191;&#34892;&#25805;&#20316;&#30340;&#22870;&#21169;&#26159;&#26410;&#30693;&#30340;&#65292;&#23548;&#33268;&#20102;&#37096;&#20998;&#20449;&#24687;&#21453;&#39304;&#12290;&#29616;&#26377;&#30340;&#19978;&#19979;&#25991;&#25209;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#26410;&#25191;&#34892;&#25805;&#20316;&#30340;&#22870;&#21169;&#65292;&#23548;&#33268;&#21453;&#39304;&#20449;&#24687;&#30340;&#28010;&#36153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;Sketched Policy Updating with Imputed Rewards (SPUIR)&#65292;&#36890;&#36807;&#20351;&#29992;&#33609;&#22270;&#25216;&#26415;&#23436;&#25104;&#26410;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#34917;&#20805;&#65292;&#20174;&#32780;&#36817;&#20284;&#20102;&#20840;&#20449;&#24687;&#21453;&#39304;&#12290;&#25105;&#20204;&#23558;&#22870;&#21169;&#34917;&#20805;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#27714;&#35299;&#25191;&#34892;&#21644;&#26410;&#25191;&#34892;&#25805;&#20316;&#30340;&#21453;&#39304;&#26426;&#21046;&#30340;&#27491;&#21017;&#21270;&#23725;&#22238;&#24402;&#38382;&#39064;&#12290;&#20026;&#20102;&#38477;&#20302;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#33609;&#22270;&#25216;&#26415;&#26469;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25511;&#21046;&#20559;&#24046;&#21644;&#27604;&#27809;&#26377;&#22870;&#21169;&#34917;&#20805;&#26041;&#27861;&#26356;&#23567;&#30340;&#26041;&#24046;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#30636;&#26102;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual batched bandit (CBB) is a setting where a batch of rewards is observed from the environment at the end of each episode, but the rewards of the non-executed actions are unobserved, resulting in partial-information feedback. Existing approaches for CBB often ignore the rewards of the non-executed actions, leading to underutilization of feedback information. In this paper, we propose an efficient approach called Sketched Policy Updating with Imputed Rewards (SPUIR) that completes the unobserved rewards using sketching, which approximates the full-information feedbacks. We formulate reward imputation as an imputation regularized ridge regression problem that captures the feedback mechanisms of both executed and non-executed actions. To reduce time complexity, we solve the regression problem using randomized sketching. We prove that our approach achieves an instantaneous regret with controllable bias and smaller variance than approaches without reward imputation. Furthermore, our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#28508;&#22312;&#34920;&#31034;&#30340;&#35889;&#20998;&#26512;&#21457;&#29616;&#65292;&#34394;&#20551;&#30456;&#20851;&#23646;&#24615;&#20250;&#23548;&#33268;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20559;&#21521;&#32534;&#30721;&#36739;&#20302;&#26377;&#25928;&#31209;&#30340;&#34920;&#31034;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#21435;&#20559;&#26694;&#26550;&#65292;&#36890;&#36807;&#31209;&#27491;&#21017;&#21270;&#39044;&#35757;&#32451;&#26377;&#20559;&#32534;&#30721;&#22120;&#26469;&#23398;&#20064;&#34394;&#20551;&#30456;&#20851;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.05248</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#20302;&#31209;&#27491;&#21017;&#21270;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-supervised debiasing using low rank regularization. (arXiv:2210.05248v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#28508;&#22312;&#34920;&#31034;&#30340;&#35889;&#20998;&#26512;&#21457;&#29616;&#65292;&#34394;&#20551;&#30456;&#20851;&#23646;&#24615;&#20250;&#23548;&#33268;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20559;&#21521;&#32534;&#30721;&#36739;&#20302;&#26377;&#25928;&#31209;&#30340;&#34920;&#31034;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#21435;&#20559;&#26694;&#26550;&#65292;&#36890;&#36807;&#31209;&#27491;&#21017;&#21270;&#39044;&#35757;&#32451;&#26377;&#20559;&#32534;&#30721;&#22120;&#26469;&#23398;&#20064;&#34394;&#20551;&#30456;&#20851;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#30456;&#20851;&#24615;&#21487;&#33021;&#23548;&#33268;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24378;&#20559;&#35265;&#65292;&#24433;&#21709;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21435;&#20559;&#26041;&#27861;&#35201;&#27714;&#23545;&#34394;&#20551;&#23646;&#24615;&#25110;&#30446;&#26631;&#26631;&#31614;&#36827;&#34892;&#23436;&#20840;&#30417;&#30563;&#65292;&#20294;&#22914;&#20309;&#20165;&#36890;&#36807;&#26377;&#38480;&#30340;&#27880;&#37322;&#25968;&#25454;&#35757;&#32451;&#19968;&#20010;&#21435;&#20559;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#28508;&#22312;&#34920;&#31034;&#36827;&#34892;&#35889;&#20998;&#26512;&#30740;&#31350;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65306;&#34394;&#20551;&#30456;&#20851;&#23646;&#24615;&#20351;&#31070;&#32463;&#32593;&#32476;&#24402;&#32435;&#22320;&#20559;&#21521;&#32534;&#30721;&#36739;&#20302;&#26377;&#25928;&#31209;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31209;&#27491;&#21017;&#21270;&#21487;&#20197;&#25918;&#22823;&#36825;&#31181;&#20559;&#24046;&#65292;&#20197;&#40723;&#21169;&#39640;&#24230;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#30340;&#21435;&#20559;&#26694;&#26550;&#65292;&#21487;&#33021;&#19982;&#26080;&#26631;&#31614;&#26679;&#26412;&#20860;&#23481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#31209;&#27491;&#21017;&#21270;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#39044;&#35757;&#32451;&#19968;&#20010;&#26377;&#20559;&#32534;&#30721;&#22120;&#65292;&#20316;&#20026;&#35821;&#20041;&#29942;&#39048;&#26469;&#24378;&#21046;&#32534;&#30721;&#22120;&#23398;&#20064;&#34394;&#20551;&#30456;&#20851;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spurious correlations can cause strong biases in deep neural networks, impairing generalization ability. While most existing debiasing methods require full supervision on either spurious attributes or target labels, training a debiased model from a limited amount of both annotations is still an open question. To address this issue, we investigate an interesting phenomenon using the spectral analysis of latent representations: spuriously correlated attributes make neural networks inductively biased towards encoding lower effective rank representations. We also show that a rank regularization can amplify this bias in a way that encourages highly correlated features. Leveraging these findings, we propose a self-supervised debiasing framework potentially compatible with unlabeled samples. Specifically, we first pretrain a biased encoder in a self-supervised manner with the rank regularization, serving as a semantic bottleneck to enforce the encoder to learn the spuriously correlated attrib
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21327;&#35843;&#19982;&#29615;&#22659;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;HECOGrid&#29615;&#22659;&#22871;&#20214;&#65292;&#36890;&#36807;&#23545;&#21327;&#35843;&#21644;&#24322;&#36136;&#24615;&#27700;&#24179;&#30340;&#23450;&#37327;&#25511;&#21046;&#65292;&#20415;&#20110;&#23545;&#19981;&#21516;MARL&#26041;&#27861;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2210.03022</link><description>&lt;p&gt;
&#26377;&#29366;&#24577;&#30340;&#20027;&#21160;&#21327;&#35843;&#22120;&#65306;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21327;&#35843;&#19982;&#29615;&#22659;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stateful active facilitator: Coordination and Environmental Heterogeneity in Cooperative Multi-Agent Reinforcement Learning. (arXiv:2210.03022v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21327;&#35843;&#19982;&#29615;&#22659;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;HECOGrid&#29615;&#22659;&#22871;&#20214;&#65292;&#36890;&#36807;&#23545;&#21327;&#35843;&#21644;&#24322;&#36136;&#24615;&#27700;&#24179;&#30340;&#23450;&#37327;&#25511;&#21046;&#65292;&#20415;&#20110;&#23545;&#19981;&#21516;MARL&#26041;&#27861;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#19968;&#32452;&#26234;&#33021;&#20307;&#20849;&#21516;&#21162;&#21147;&#23454;&#29616;&#19968;&#20010;&#20849;&#21516;&#30340;&#30446;&#26631;&#12290;&#19981;&#21516;&#30340;&#29615;&#22659;&#25110;&#20219;&#21153;&#21487;&#33021;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#21327;&#35843;&#26469;&#20197;&#26368;&#20248;&#30340;&#26041;&#24335;&#23454;&#29616;&#30446;&#26631;&#12290;&#21327;&#35843;&#30340;&#24615;&#36136;&#21462;&#20915;&#20110;&#29615;&#22659;&#30340;&#23646;&#24615;&#65292;&#20363;&#22914;&#31354;&#38388;&#24067;&#23616;&#12289;&#38556;&#30861;&#29289;&#20998;&#24067;&#12289;&#21160;&#24577;&#31561;&#12290;&#25105;&#20204;&#23558;&#29615;&#22659;&#20869;&#23646;&#24615;&#30340;&#36825;&#31181;&#21464;&#21270;&#31216;&#20026;&#24322;&#36136;&#24615;&#12290;&#29616;&#26377;&#25991;&#29486;&#23578;&#26410;&#20805;&#20998;&#35299;&#20915;&#19981;&#21516;&#29615;&#22659;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#27700;&#24179;&#30340;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#29615;&#22659;&#30340;&#21327;&#35843;&#27700;&#24179;&#21644;&#24322;&#36136;&#24615;&#27700;&#24179;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;HECOGrid&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;RL&#29615;&#22659;&#22871;&#20214;&#65292;&#36890;&#36807;&#25552;&#20379;&#23545;&#29615;&#22659;&#30340;&#21327;&#35843;&#21644;&#24322;&#36136;&#24615;&#27700;&#24179;&#36827;&#34892;&#23450;&#37327;&#25511;&#21046;&#65292;&#20415;&#20110;&#23545;&#19981;&#21516;&#21327;&#20316;&#21644;&#29615;&#22659;&#24322;&#36136;&#24615;&#30340;MARL&#26041;&#27861;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In cooperative multi-agent reinforcement learning, a team of agents works together to achieve a common goal. Different environments or tasks may require varying degrees of coordination among agents in order to achieve the goal in an optimal way. The nature of coordination will depend on the properties of the environment -- its spatial layout, distribution of obstacles, dynamics, etc. We term this variation of properties within an environment as heterogeneity. Existing literature has not sufficiently addressed the fact that different environments may have different levels of heterogeneity. We formalize the notions of coordination level and heterogeneity level of an environment and present HECOGrid, a suite of multi-agent RL environments that facilitates empirical evaluation of different MARL approaches across different levels of coordination and environmental heterogeneity by providing a quantitative control over coordination and heterogeneity levels of the environment. Further, we prop
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;EvoGen&#65292;&#19968;&#20010;&#20803;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26657;&#20934;&#25110;&#34394;&#25311;&#29983;&#25104;&#30340;&#21516;&#28304;&#24207;&#21015;&#26469;&#24341;&#23548;AlphaFold2&#27169;&#22411;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;&#34507;&#30333;&#36136;&#25240;&#21472;&#21644;&#32467;&#26500;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2208.09652</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#24341;&#23548;AlphaFold2&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20934;&#30830;&#25240;&#21472;&#36335;&#24452;&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervisedly Prompting AlphaFold2 for Few-Shot Learning of Accurate Folding Landscape and Protein Structure Prediction. (arXiv:2208.09652v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;EvoGen&#65292;&#19968;&#20010;&#20803;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26657;&#20934;&#25110;&#34394;&#25311;&#29983;&#25104;&#30340;&#21516;&#28304;&#24207;&#21015;&#26469;&#24341;&#23548;AlphaFold2&#27169;&#22411;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;&#34507;&#30333;&#36136;&#25240;&#21472;&#21644;&#32467;&#26500;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#39044;&#27979;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#36716;&#21270;&#20026;&#29983;&#29289;&#27963;&#24615;&#32467;&#26500;&#65292;&#23545;&#31185;&#23398;&#30740;&#31350;&#21644;&#21307;&#23398;&#21457;&#23637;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#20351;&#29992;&#20849;&#36827;&#21270;&#20449;&#24687;&#30830;&#23450;&#20934;&#30830;&#30340;&#25240;&#21472;&#36335;&#24452;&#26159;&#29616;&#20195;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#25104;&#21151;&#30340;&#22522;&#30784;&#12290;&#20316;&#20026;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;AlphaFold2&#22312;&#19981;&#36827;&#34892;&#26174;&#24335;&#20849;&#36827;&#21270;&#20998;&#26512;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20854;&#24615;&#33021;&#20173;&#28982;&#24378;&#28872;&#20381;&#36182;&#20110;&#21487;&#29992;&#30340;&#24207;&#21015;&#21516;&#28304;&#20307;&#12290;&#22522;&#20110;&#23545;&#36825;&#31181;&#20381;&#36182;&#21407;&#22240;&#30340;&#25506;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EvoGen&#65292;&#19968;&#20010;&#20803;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;AlphaFold2&#22312;&#36139;&#20047;MSA&#30446;&#26631;&#19978;&#30340;&#24615;&#33021;&#19981;&#36275;&#12290;&#36890;&#36807;&#20351;&#29992;&#26657;&#20934;&#25110;&#34394;&#25311;&#29983;&#25104;&#30340;&#21516;&#28304;&#24207;&#21015;&#26469;&#24341;&#23548;&#27169;&#22411;&#65292;EvoGen&#24110;&#21161;AlphaFold2&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#20934;&#30830;&#25240;&#21472;&#65292;&#29978;&#33267;&#22312;&#21333;&#24207;&#21015;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#24615;&#33021;&#12290;&#33021;&#22815;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Data-driven predictive methods which can efficiently and accurately transform protein sequences into biologically active structures are highly valuable for scientific research and medical development. Determining accurate folding landscape using co-evolutionary information is fundamental to the success of modern protein structure prediction methods. As the state of the art, AlphaFold2 has dramatically raised the accuracy without performing explicit co-evolutionary analysis. Nevertheless, its performance still shows strong dependence on available sequence homologs. Based on the interrogation on the cause of such dependence, we presented EvoGen, a meta generative model, to remedy the underperformance of AlphaFold2 for poor MSA targets. By prompting the model with calibrated or virtually generated homologue sequences, EvoGen helps AlphaFold2 fold accurately in low-data regime and even achieve encouraging performance with single-sequence predictions. Being able to make accurate predictions
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#30340;&#26032;&#31639;&#27861;ADDS&#65292;&#36890;&#36807;&#21452;&#27169;&#24577;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340;&#23545;&#40784;&#12290;&#37319;&#29992;&#20102;Pyramid-Forwarding&#26041;&#27861;&#22686;&#24378;&#20102;&#23545;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#24615;&#35821;&#35328;&#30417;&#30563;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102; state-of-the-art &#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.09562</link><description>&lt;p&gt;
&#24320;&#25918;&#35789;&#27719;&#30340;&#21452;&#27169;&#24577;&#35299;&#30721;&#22120;&#23545;&#40784;&#35270;&#35273;-&#25991;&#26412;&#29305;&#24449;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Open Vocabulary Multi-Label Classification with Dual-Modal Decoder on Aligned Visual-Textual Features. (arXiv:2208.09562v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#30340;&#26032;&#31639;&#27861;ADDS&#65292;&#36890;&#36807;&#21452;&#27169;&#24577;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340;&#23545;&#40784;&#12290;&#37319;&#29992;&#20102;Pyramid-Forwarding&#26041;&#27861;&#22686;&#24378;&#20102;&#23545;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#24615;&#35821;&#35328;&#30417;&#30563;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102; state-of-the-art &#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#22810;&#26631;&#31614;&#35782;&#21035;&#26159;&#20855;&#26377;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#20294;&#26159;&#23545;&#20808;&#21069;&#26410;&#35265;&#26631;&#31614;&#36827;&#34892;&#20998;&#31867;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;Aligned Dual moDality ClaSsifier (ADDS)&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19968;&#20010;&#21452;&#27169;&#24577;&#35299;&#30721;&#22120;(DM-decoder)&#65292;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#24182;&#23545;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#31216;&#20026;Pyramid-Forwarding&#65292;&#20197;&#25552;&#39640;&#23545;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36873;&#25321;&#24615;&#35821;&#35328;&#30417;&#30563;&#26469;&#36827;&#19968;&#27493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#20960;&#20010;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;NUS-WIDE&#12289;ImageNet-1k&#12289;ImageNet-21k&#21644;MS-COCO&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#20026;&#24320;&#25918;&#35789;&#27719;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#12289;&#20256;&#32479;&#22810;&#26631;&#31614;&#20998;&#31867;&#21644;&#19968;&#31181;&#31216;&#20026;&#21333;&#26631;&#31614;&#20998;&#31867;&#30340;&#26497;&#31471;&#24773;&#20917;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In computer vision, multi-label recognition are important tasks with many real-world applications, but classifying previously unseen labels remains a significant challenge. In this paper, we propose a novel algorithm, Aligned Dual moDality ClaSsifier (ADDS), which includes a Dual-Modal decoder (DM-decoder) with alignment between visual and textual features, for open-vocabulary multi-label classification tasks. Then we design a simple and yet effective method called Pyramid-Forwarding to enhance the performance for inputs with high resolutions. Moreover, the Selective Language Supervision is applied to further enhance the model performance. Extensive experiments conducted on several standard benchmarks, NUS-WIDE, ImageNet-1k, ImageNet-21k, and MS-COCO, demonstrate that our approach significantly outperforms previous methods and provides state-of-the-art performance for open-vocabulary multi-label classification, conventional multi-label classification and an extreme case called single-t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#26080;&#35770;&#39044;&#35757;&#32451;&#37319;&#29992;&#20309;&#31181;&#21327;&#35758;&#65292;&#32447;&#24615;&#39044;&#27979;&#22120;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#21463;&#20854;&#22522;&#30784;&#34920;&#31034;&#40065;&#26834;&#24615;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25439;&#22833;&#19978;&#30028;&#21644;&#40065;&#26834;&#20998;&#31867;&#20934;&#21017;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#39564;&#35777;&#20102;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.03835</link><description>&lt;p&gt;
&#20851;&#20110;&#20174;&#39044;&#35757;&#32451;&#21040;&#19979;&#28216;&#20219;&#21153;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
On Transfer of Adversarial Robustness from Pretraining to Downstream Tasks. (arXiv:2208.03835v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#26080;&#35770;&#39044;&#35757;&#32451;&#37319;&#29992;&#20309;&#31181;&#21327;&#35758;&#65292;&#32447;&#24615;&#39044;&#27979;&#22120;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#21463;&#20854;&#22522;&#30784;&#34920;&#31034;&#40065;&#26834;&#24615;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25439;&#22833;&#19978;&#30028;&#21644;&#40065;&#26834;&#20998;&#31867;&#20934;&#21017;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#39564;&#35777;&#20102;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35757;&#32451;&#26041;&#26696;&#30340;&#27969;&#34892;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#34429;&#28982;&#23454;&#36341;&#20013;&#24050;&#32463;&#35777;&#26126;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#20174;&#39044;&#35757;&#32451;&#21040;&#19979;&#28216;&#20219;&#21153;&#30340;&#40065;&#26834;&#24615;&#23646;&#24615;&#30340;&#36716;&#31227;&#20173;&#28982;&#19981;&#22815;&#29702;&#35299;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32447;&#24615;&#39044;&#27979;&#22120;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#21487;&#20197;&#30001;&#20854;&#22522;&#30784;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#38480;&#21046;&#65292;&#32780;&#19981;&#31649;&#39044;&#35757;&#32451;&#20351;&#29992;&#30340;&#21327;&#35758;&#22914;&#20309;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;(i)&#19968;&#20010;&#22312;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#20013;&#37117;&#25104;&#31435;&#30340;&#25439;&#22833;&#19978;&#30028;&#65292;&#20197;&#21450;(ii)&#29305;&#23450;&#20110;&#40065;&#26834;&#20998;&#31867;&#30340;&#20934;&#21017;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#22914;&#20309;&#29992;&#20110;&#26657;&#20934;&#19979;&#28216;&#40065;&#26834;&#24615;&#30340;&#26399;&#26395;&#65292;&#20197;&#21450;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#26368;&#20248;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#29992;&#36884;&#12290;&#32508;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#34920;&#24449;&#35201;&#27714;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large-scale training regimes have gained popularity, the use of pretrained models for downstream tasks has become common practice in machine learning. While pretraining has been shown to enhance the performance of models in practice, the transfer of robustness properties from pretraining to downstream tasks remains poorly understood. In this study, we demonstrate that the robustness of a linear predictor on downstream tasks can be constrained by the robustness of its underlying representation, regardless of the protocol used for pretraining. We prove (i) a bound on the loss that holds independent of any downstream task, as well as (ii) a criterion for robust classification in particular. We validate our theoretical results in practical applications, show how our results can be used for calibrating expectations of downstream robustness, and when our results are useful for optimal transfer learning. Taken together, our results offer an initial step towards characterizing the requireme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AEnbMIMOCQR&#30340;&#26032;&#39062;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#38598;&#25104;&#30340;&#26041;&#24335;&#65292;&#22312;&#19981;&#38656;&#35201;&#25968;&#25454;&#25286;&#20998;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#20998;&#24067;&#26080;&#20851;&#30340;&#26041;&#24335;&#29983;&#25104;&#22810;&#27493;&#40077;&#22411;&#39044;&#27979;&#21306;&#38388;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#24322;&#26041;&#24046;&#24615;&#65292;&#24182;&#23545;&#20998;&#24067;&#36716;&#21464;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.14219</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#27493;&#40077;&#22411;&#33258;&#36866;&#24212;&#24322;&#26041;&#24046;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A general framework for multi-step ahead adaptive conformal heteroscedastic time series forecasting. (arXiv:2207.14219v7 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AEnbMIMOCQR&#30340;&#26032;&#39062;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#38598;&#25104;&#30340;&#26041;&#24335;&#65292;&#22312;&#19981;&#38656;&#35201;&#25968;&#25454;&#25286;&#20998;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#20998;&#24067;&#26080;&#20851;&#30340;&#26041;&#24335;&#29983;&#25104;&#22810;&#27493;&#40077;&#22411;&#39044;&#27979;&#21306;&#38388;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#24322;&#26041;&#24046;&#24615;&#65292;&#24182;&#23545;&#20998;&#24067;&#36716;&#21464;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#26080;&#20851;&#31639;&#27861;&#65292;&#21517;&#20026;&#33258;&#36866;&#24212;&#38598;&#25104;&#25209;&#37327;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#40077;&#22411;&#20998;&#20301;&#25968;&#22238;&#24402;&#65288;AEnbMIMOCQR&#65289;&#65292;&#20351;&#24471;&#39044;&#27979;&#32773;&#33021;&#22815;&#20197;&#20998;&#24067;&#26080;&#20851;&#30340;&#26041;&#24335;&#29983;&#25104;&#22266;&#23450;&#39044;&#35774;&#22833;&#37197;&#29575;&#30340;&#22810;&#27493;&#40077;&#22411;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#40077;&#22411;&#39044;&#27979;&#21407;&#29702;&#65292;&#20294;&#19981;&#38656;&#35201;&#25968;&#25454;&#25286;&#20998;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#25968;&#25454;&#19981;&#21487;&#20114;&#25442;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#25552;&#20379;&#25509;&#36817;&#31934;&#30830;&#30340;&#35206;&#30422;&#29575;&#12290;&#27492;&#22806;&#65292;&#25152;&#24471;&#21040;&#30340;&#39044;&#27979;&#21306;&#38388;&#22312;&#39044;&#27979;&#26102;&#38388;&#33539;&#22260;&#20869;&#32463;&#39564;&#35777;&#26126;&#26377;&#25928;&#65292;&#24182;&#19988;&#32771;&#34385;&#20102;&#24322;&#26041;&#24046;&#24615;&#12290;AEnbMIMOCQR&#34987;&#35774;&#35745;&#25104;&#23545;&#20998;&#24067;&#36716;&#21464;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#20854;&#39044;&#27979;&#21306;&#38388;&#22312;&#26080;&#38480;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#20445;&#25345;&#21487;&#38752;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#19981;&#20999;&#23454;&#38469;&#30340;&#20005;&#26684;&#20551;&#35774;&#12290;&#36890;&#36807;&#31995;&#32479;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#40077;&#22411;&#39044;&#27979;&#20013;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel model-agnostic algorithm called adaptive ensemble batch multi-input multi-output conformalized quantile regression (AEnbMIMOCQR} that enables forecasters to generate multi-step ahead prediction intervals for a fixed pre-specified miscoverage rate in a distribution-free manner. Our method is grounded on conformal prediction principles, however, it does not require data splitting and provides close to exact coverage even when the data is not exchangeable. Moreover, the resulting prediction intervals, besides being empirically valid along the forecast horizon, do not neglect heteroscedasticity. AEnbMIMOCQR is designed to be robust to distribution shifts, which means that its prediction intervals remain reliable over an unlimited period of time, without entailing retraining or imposing unrealistic strict assumptions on the data-generating process. Through methodically experimentation, we demonstrate that our approach outperforms other competitive methods on bo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#24863;&#24615;&#65292;&#21457;&#29616;&#23398;&#20064;&#20219;&#21153;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#23548;&#33268;&#30340;&#30446;&#26631;&#31867;&#21035;&#38169;&#35823;&#20998;&#31867;&#12290;&#36825;&#23545;&#25968;&#25454;&#23436;&#25972;&#24615;&#21644;&#38544;&#31169;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2207.05225</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#23545;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26131;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Susceptibility of Continual Learning Against Adversarial Attacks. (arXiv:2207.05225v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#24863;&#24615;&#65292;&#21457;&#29616;&#23398;&#20064;&#20219;&#21153;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#23548;&#33268;&#30340;&#30446;&#26631;&#31867;&#21035;&#38169;&#35823;&#20998;&#31867;&#12290;&#36825;&#23545;&#25968;&#25454;&#23436;&#25972;&#24615;&#21644;&#38544;&#31169;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#26377;&#20004;&#20010;&#20851;&#38190;&#39046;&#22495;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#65306;1&#65289;&#35780;&#20272;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;2&#65289;&#30830;&#20445;&#23398;&#20064;&#20219;&#21153;&#30340;&#23433;&#20840;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#65292;&#21253;&#25324;&#24403;&#21069;&#21644;&#20808;&#21069;&#33719;&#21462;&#30340;&#20219;&#21153;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#24863;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20219;&#20309;&#23646;&#20110;&#20219;&#20309;&#20219;&#21153;&#30340;&#31867;&#21035;&#37117;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#25104;&#20026;&#20219;&#20309;&#20854;&#20182;&#20219;&#21153;&#25152;&#38656;&#30446;&#26631;&#31867;&#21035;&#30340;&#30446;&#26631;&#65292;&#24182;&#19988;&#34987;&#38169;&#35823;&#20998;&#31867;&#12290;&#36825;&#31181;&#23398;&#20064;&#20219;&#21153;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#24863;&#24615;&#24341;&#21457;&#20102;&#26377;&#20851;&#25968;&#25454;&#23436;&#25972;&#24615;&#21644;&#38544;&#31169;&#30340;&#28145;&#21051;&#20851;&#20999;&#12290;&#20026;&#20102;&#35780;&#20272;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#22330;&#26223;&#19979;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#20219;&#21153;&#36882;&#22686;&#23398;&#20064;&#12289;&#39046;&#22495;&#36882;&#22686;&#23398;&#20064;&#21644;&#31867;&#36882;&#22686;&#23398;&#20064;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent continual learning approaches have primarily focused on mitigating catastrophic forgetting. Nevertheless, two critical areas have remained relatively unexplored: 1) evaluating the robustness of proposed methods and 2) ensuring the security of learned tasks. This paper investigates the susceptibility of continually learned tasks, including current and previously acquired tasks, to adversarial attacks. Specifically, we have observed that any class belonging to any task can be easily targeted and misclassified as the desired target class of any other task. Such susceptibility or vulnerability of learned tasks to adversarial attacks raises profound concerns regarding data integrity and privacy. To assess the robustness of continual learning approaches, we consider continual learning approaches in all three scenarios, i.e., task-incremental learning, domain-incremental learning, and class-incremental learning. In this regard, we explore the robustness of three regularization-based me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#37325;&#26500;&#35823;&#24046;&#30340;&#26041;&#24335;&#38598;&#20013;&#22312;&#25968;&#25454;&#27969;&#24418;&#19978;&#65292;&#20174;&#32780;&#35299;&#20915;&#28145;&#24230;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#22312;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#20013;&#23481;&#26131;&#20986;&#29616;&#30340;&#37325;&#26500;&#24322;&#24120;&#20449;&#21495;&#32780;&#23548;&#33268;&#26816;&#27979;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.11723</link><description>&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#29992;&#20110;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Training with Autoencoders for Visual Anomaly Detection. (arXiv:2206.11723v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#37325;&#26500;&#35823;&#24046;&#30340;&#26041;&#24335;&#38598;&#20013;&#22312;&#25968;&#25454;&#27969;&#24418;&#19978;&#65292;&#20174;&#32780;&#35299;&#20915;&#28145;&#24230;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#22312;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#20013;&#23481;&#26131;&#20986;&#29616;&#30340;&#37325;&#26500;&#24322;&#24120;&#20449;&#21495;&#32780;&#23548;&#33268;&#26816;&#27979;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#38750;&#32447;&#24615;&#38477;&#32500;&#12290;&#26368;&#36817;&#65292;&#23427;&#20204;&#24050;&#34987;&#29992;&#20110;&#35270;&#35273;&#39046;&#22495;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26080;&#24322;&#24120;&#30340;&#26679;&#26412;&#20248;&#21270;&#37325;&#26500;&#35823;&#24046;&#65292;&#26222;&#36941;&#35748;&#20026;&#30456;&#24212;&#30340;&#32593;&#32476;&#24212;&#22312;&#24212;&#29992;&#38454;&#27573;&#26410;&#33021;&#20934;&#30830;&#37325;&#26500;&#24322;&#24120;&#21306;&#22495;&#12290;&#36825;&#20010;&#30446;&#26631;&#36890;&#24120;&#36890;&#36807;&#25511;&#21046;&#32593;&#32476;&#30340;&#23481;&#37327;&#26469;&#35299;&#20915;&#65292;&#35201;&#20040;&#36890;&#36807;&#20943;&#23569;&#29942;&#39048;&#23618;&#30340;&#22823;&#23567;&#65292;&#35201;&#20040;&#36890;&#36807;&#23545;&#20854;&#28608;&#27963;&#26045;&#21152;&#31232;&#30095;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#25216;&#26415;&#37117;&#27809;&#26377;&#26126;&#30830;&#24809;&#32602;&#24322;&#24120;&#20449;&#21495;&#30340;&#37325;&#26500;&#65292;&#36890;&#24120;&#23548;&#33268;&#26816;&#27979;&#25928;&#26524;&#19981;&#20339;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#36866;&#24212;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#21028;&#21035;&#20449;&#24687;&#65292;&#36890;&#36807;&#20462;&#25913;&#30340;&#37325;&#26500;&#35823;&#24046;&#38598;&#20013;&#22312;&#25968;&#25454;&#27969;&#24418;&#19978;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#23616;&#37096;&#19968;&#33268;&#30340;&#37325;&#26500;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep convolutional autoencoders provide an effective tool for learning non-linear dimensionality reduction in an unsupervised way. Recently, they have been used for the task of anomaly detection in the visual domain. By optimising for the reconstruction error using anomaly-free examples, the common belief is that a corresponding network should fail to accurately reconstruct anomalous regions in the application phase. This goal is typically addressed by controlling the capacity of the network by either reducing the size of the bottleneck layer or enforcing sparsity constraints on its activations. However, neither of these techniques does explicitly penalize reconstruction of anomalous signals often resulting in poor detection. We tackle this problem by adapting a self-supervised learning regime, which allows to use discriminative information during training focusing on the data manifold by means of a modified reconstruction error. This regularizes the model to produce locally consistent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#26377;&#38480;&#34920;&#36798;&#26041;&#27861;&#65288;FEX&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20855;&#26377;&#26377;&#38480;&#20010;&#35299;&#26512;&#34920;&#36798;&#24335;&#30340;&#20989;&#25968;&#31354;&#38388;&#20013;&#23547;&#25214;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#36817;&#20284;&#35299;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23558;FEX&#24212;&#29992;&#20110;&#21508;&#31181;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#24230;&#20934;&#30830;&#30340;&#27714;&#35299;&#65292;&#24182;&#19988;&#36991;&#20813;&#20102;&#32500;&#24230;&#28798;&#38590;&#12290;&#36825;&#31181;&#26377;&#38480;&#35299;&#26512;&#34920;&#36798;&#24335;&#30340;&#36817;&#20284;&#35299;&#36824;&#21487;&#20197;&#25552;&#20379;&#23545;&#30495;&#23454;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#30340;&#21487;&#35299;&#37322;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2206.10121</link><description>&lt;p&gt;
&#29992;&#20110;&#27714;&#35299;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26377;&#38480;&#34920;&#36798;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Finite Expression Method for Solving High-Dimensional Partial Differential Equations. (arXiv:2206.10121v3 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#26377;&#38480;&#34920;&#36798;&#26041;&#27861;&#65288;FEX&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20855;&#26377;&#26377;&#38480;&#20010;&#35299;&#26512;&#34920;&#36798;&#24335;&#30340;&#20989;&#25968;&#31354;&#38388;&#20013;&#23547;&#25214;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#36817;&#20284;&#35299;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23558;FEX&#24212;&#29992;&#20110;&#21508;&#31181;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#24230;&#20934;&#30830;&#30340;&#27714;&#35299;&#65292;&#24182;&#19988;&#36991;&#20813;&#20102;&#32500;&#24230;&#28798;&#38590;&#12290;&#36825;&#31181;&#26377;&#38480;&#35299;&#26512;&#34920;&#36798;&#24335;&#30340;&#36817;&#20284;&#35299;&#36824;&#21487;&#20197;&#25552;&#20379;&#23545;&#30495;&#23454;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#30340;&#21487;&#35299;&#37322;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#39640;&#25928;&#20934;&#30830;&#30340;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#25968;&#20540;&#27714;&#35299;&#22120;&#20173;&#28982;&#26159;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#37325;&#35201;&#24615;&#30340;&#35838;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22312;&#35774;&#35745;&#33021;&#22815;&#25353;&#32500;&#25968;&#36827;&#34892;&#25193;&#23637;&#30340;&#25968;&#20540;&#26041;&#26696;&#20013;&#23384;&#22312;&#8220;&#32500;&#24230;&#28798;&#38590;&#8221;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#26377;&#38480;&#20010;&#35299;&#26512;&#34920;&#36798;&#24335;&#30340;&#20989;&#25968;&#31354;&#38388;&#20013;&#23547;&#25214;&#36817;&#20284;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#65292;&#22240;&#27492;&#23558;&#35813;&#26041;&#27861;&#21629;&#21517;&#20026;&#26377;&#38480;&#34920;&#36798;&#26041;&#27861;&#65288;FEX&#65289;&#12290;&#22312;&#36817;&#20284;&#29702;&#35770;&#20013;&#35777;&#26126;&#20102;FEX&#21487;&#20197;&#36991;&#20813;&#32500;&#24230;&#28798;&#38590;&#12290;&#20316;&#20026;&#27010;&#24565;&#35777;&#26126;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#32500;&#24230;&#19978;&#23454;&#29616;FEX&#27714;&#35299;&#21508;&#31181;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#23454;&#29616;&#20102;&#39640;&#29978;&#33267;&#26426;&#22120;&#31934;&#24230;&#65292;&#24182;&#20855;&#26377;&#22810;&#39033;&#24335;&#32500;&#24230;&#30340;&#35760;&#24518;&#22797;&#26434;&#24230;&#21644;&#21487;&#25805;&#20316;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#20855;&#26377;&#26377;&#38480;&#35299;&#26512;&#34920;&#36798;&#24335;&#30340;&#36817;&#20284;&#35299;&#36824;&#20026;&#22320;&#38754;&#30495;&#23454;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing efficient and accurate numerical solvers for high-dimensional partial differential equations (PDEs) remains a challenging and important topic in computational science and engineering, mainly due to the "curse of dimensionality" in designing numerical schemes that scale in dimension. This paper introduces a new methodology that seeks an approximate PDE solution in the space of functions with finitely many analytic expressions and, hence, this methodology is named the finite expression method (FEX). It is proved in approximation theory that FEX can avoid the curse of dimensionality. As a proof of concept, a deep reinforcement learning method is proposed to implement FEX for various high-dimensional PDEs in different dimensions, achieving high and even machine accuracy with a memory complexity polynomial in dimension and an amenable time complexity. An approximate solution with finite analytic expressions also provides interpretable insights into the ground truth PDE solution, w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#29615;&#22659;&#27169;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#23398;&#20064;&#23545;&#21453;&#20107;&#23454;&#25968;&#25454;&#36827;&#34892;&#27867;&#21270;&#22788;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#20915;&#31574;&#31574;&#30053;&#23398;&#20064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#24615;&#31574;&#30053;&#26597;&#35810;&#21453;&#20107;&#23454;&#25968;&#25454;&#65292;&#26368;&#32456;&#24471;&#21040;&#19968;&#20010;&#31283;&#20581;&#19988;&#21487;&#38752;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2206.04890</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#29615;&#22659;&#27169;&#22411;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adversarial Counterfactual Environment Model Learning. (arXiv:2206.04890v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#29615;&#22659;&#27169;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#23398;&#20064;&#23545;&#21453;&#20107;&#23454;&#25968;&#25454;&#36827;&#34892;&#27867;&#21270;&#22788;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#20915;&#31574;&#31574;&#30053;&#23398;&#20064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#24615;&#31574;&#30053;&#26597;&#35810;&#21453;&#20107;&#23454;&#25968;&#25454;&#65292;&#26368;&#32456;&#24471;&#21040;&#19968;&#20010;&#31283;&#20581;&#19988;&#21487;&#38752;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#24739;&#32773;&#27835;&#30103;&#36873;&#25321;&#31561;&#39046;&#22495;&#65292;&#19968;&#20010;&#22909;&#30340;&#34892;&#20026;-&#25928;&#26524;&#39044;&#27979;&#27169;&#22411;&#65292;&#21363;&#29615;&#22659;&#27169;&#22411;&#65292;&#23545;&#20110;&#23454;&#29616;&#39640;&#25928;&#30340;&#20915;&#31574;&#31574;&#30053;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#36825;&#26679;&#30340;&#27169;&#22411;&#36827;&#34892;&#26080;&#38480;&#27425;&#35797;&#39564;&#65292;&#20197;&#30830;&#23450;&#36866;&#24403;&#30340;&#34892;&#21160;&#65292;&#20174;&#32780;&#33410;&#30465;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#26597;&#35810;&#25104;&#26412;&#12290;&#36825;&#35201;&#27714;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#65292;&#21363;&#21453;&#20107;&#23454;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;&#25968;&#25454;&#25311;&#21512;&#25216;&#26415;&#19981;&#33021;&#33258;&#21160;&#23454;&#29616;&#36825;&#31181;&#27867;&#21270;&#33021;&#21147;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#19981;&#21487;&#38752;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38024;&#23545;&#29305;&#23450;&#30446;&#26631;&#31574;&#30053;&#26597;&#35810;&#30340;&#21453;&#20107;&#23454;&#26597;&#35810;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;CQRM&#65289;&#26041;&#27861;&#65292;&#22312;&#27169;&#22411;&#23398;&#20064;&#20013;&#23454;&#29616;&#23545;&#21453;&#20107;&#23454;&#25968;&#25454;&#30340;&#27867;&#21270;&#12290;&#30001;&#20110;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#30446;&#26631;&#31574;&#30053;&#21487;&#20197;&#26159;&#21508;&#31181;&#21508;&#26679;&#30340;&#19988;&#26410;&#30693;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;CQRM&#30446;&#26631;&#65292;&#27169;&#22411;&#36890;&#36807;&#23545;&#25239;&#24615;&#31574;&#30053;&#26597;&#35810;&#21453;&#20107;&#23454;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#65292;&#26368;&#32456;&#24471;&#21040;&#19968;&#20010;&#31283;&#20581;&#19988;&#21487;&#38752;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A good model for action-effect prediction, named environment model, is important to achieve sample-efficient decision-making policy learning in many domains like robot control, recommender systems, and patients' treatment selection. We can take unlimited trials with such a model to identify the appropriate actions so that the costs of queries in the real world can be saved. It requires the model to handle unseen data correctly, also called counterfactual data. However, standard data fitting techniques do not automatically achieve such generalization ability and commonly result in unreliable models. In this work, we introduce counterfactual-query risk minimization (CQRM) in model learning for generalizing to a counterfactual dataset queried by a specific target policy. Since the target policies can be various and unknown in policy learning, we propose an adversarial CQRM objective in which the model learns on counterfactual data queried by adversarial policies, and finally derive a trac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#38543;&#26426;&#36817;&#20284;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#21033;&#29992;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#32467;&#26500;&#21560;&#25910;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#36924;&#36817;&#35823;&#24046;&#24182;&#33719;&#24471;&#20102;&#25913;&#36827;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;SGLD&#21644;RBM&#65292;&#25105;&#20204;&#20998;&#21035;&#35777;&#26126;&#20102;&#19981;&#21516;&#30340;&#20551;&#35774;&#26465;&#20214;&#19979;&#36739;&#20248;&#30340;&#25910;&#25947;&#29575;&#21644;&#21442;&#25968;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2206.03792</link><description>&lt;p&gt;
&#21033;&#29992;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#32467;&#26500;&#30340;&#38543;&#26426;&#26799;&#24230;&#37319;&#26679;&#26041;&#27861;&#65306;&#25913;&#36827;&#30340;&#20998;&#26512;&#21644;&#26356;&#24555;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Utilising the CLT Structure in Stochastic Gradient based Sampling : Improved Analysis and Faster Algorithms. (arXiv:2206.03792v3 [math.PR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#38543;&#26426;&#36817;&#20284;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#21033;&#29992;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#32467;&#26500;&#21560;&#25910;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#36924;&#36817;&#35823;&#24046;&#24182;&#33719;&#24471;&#20102;&#25913;&#36827;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;SGLD&#21644;RBM&#65292;&#25105;&#20204;&#20998;&#21035;&#35777;&#26126;&#20102;&#19981;&#21516;&#30340;&#20551;&#35774;&#26465;&#20214;&#19979;&#36739;&#20248;&#30340;&#25910;&#25947;&#29575;&#21644;&#21442;&#25968;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#38543;&#26426;&#36817;&#20284;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#22914;&#38543;&#26426;&#26799;&#24230; langevin &#21160;&#21147;&#23398;&#65288;SGLD&#65289;&#21644;&#38543;&#26426;&#25209;&#22788;&#29702;&#26041;&#27861;&#65288;RBM&#65289;&#29992;&#20110;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#21160;&#21147;&#23398;&#65288;IPD&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30001;&#20110;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65288;CLT&#65289;&#65292;&#38543;&#26426;&#36924;&#36817;&#24341;&#20837;&#30340;&#22122;&#22768;&#20960;&#20046;&#26159;&#39640;&#26031;&#20998;&#24067;&#65292;&#32780;&#39537;&#21160;&#24067;&#26391;&#36816;&#21160;&#21017;&#26159;&#30830;&#20999;&#30340;&#39640;&#26031;&#20998;&#24067;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#26469;&#21560;&#25910;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#36924;&#36817;&#35823;&#24046;&#65292;&#24182;&#33719;&#24471;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#25913;&#36827;&#25910;&#25947;&#20445;&#35777;&#12290;&#23545;&#20110; SGLD&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19981;&#38656;&#35201;&#32479;&#19968;&#28201;&#26262;&#21551;&#21160;&#30340;&#24773;&#20917;&#19979;KL&#25955;&#24230;&#30340;&#31532;&#19968;&#20010;&#31283;&#23450;&#25910;&#25947;&#29575;&#65292;&#20551;&#35774;&#30446;&#26631;&#23494;&#24230;&#28385;&#36275;&#19968;&#20010;&#23545;&#25968; Sobolev &#19981;&#31561;&#24335;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24847;&#21619;&#30528;&#22312;&#26174;&#33879;&#36739;&#36731;&#30340;&#20551;&#35774;&#26465;&#20214;&#19979;&#65292;&#30456;&#23545;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#19968;&#38454; oracle &#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102; SGLD &#30340;&#31532;&#19968;&#20010;&#20445;&#35777;&#65292;&#23545;&#20110;&#26356;&#24369;&#30340;&#26465;&#20214;&#65292;&#22914; H\''{o}lder &#24179;&#28369;&#24615;&#21644; Poincare&#19981;&#31561;&#24335;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#29616;&#26377;&#25216;&#26415;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23545;&#20110; RBM&#65292;&#25105;&#20204;&#22312; IPD &#30340;&#24369;&#28151;&#21512;&#26465;&#20214;&#19979;&#33719;&#24471;&#20102;&#31532;&#19968;&#27425;&#25910;&#25947;&#20998;&#26512;&#21644;&#26368;&#20339;&#21442;&#25968;&#33539;&#22260;&#65292;&#36825;&#22312;&#32479;&#35745;&#29289;&#29702;&#21644;&#23398;&#20064;&#29702;&#35770;&#20013;&#20855;&#26377;&#20960;&#20010;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider stochastic approximations of sampling algorithms, such as Stochastic Gradient Langevin Dynamics (SGLD) and the Random Batch Method (RBM) for Interacting Particle Dynamcs (IPD). We observe that the noise introduced by the stochastic approximation is nearly Gaussian due to the Central Limit Theorem (CLT) while the driving Brownian motion is exactly Gaussian. We harness this structure to absorb the stochastic approximation error inside the diffusion process, and obtain improved convergence guarantees for these algorithms. For SGLD, we prove the first stable convergence rate in KL divergence without requiring uniform warm start, assuming the target density satisfies a Log-Sobolev Inequality. Our result implies superior first-order oracle complexity compared to prior works, under significantly milder assumptions. We also prove the first guarantees for SGLD under even weaker conditions such as H\"{o}lder smoothness and Poincare Inequality, thus bridging the gap between the state-
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#31070;&#32463;&#25913;&#36827;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#20811;&#26381;&#29616;&#26377;&#27169;&#22411;&#22312;&#22788;&#29702;&#36793;&#32536;&#20449;&#24687;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#30340;&#26032;&#27169;&#22411;&#33021;&#22815;&#22312;&#20559;&#22909;&#25490;&#24207;&#38382;&#39064;&#20013;&#25552;&#20379;&#26377;&#25928;&#30340;&#37051;&#22495;&#25805;&#20316;&#65292;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2206.00383</link><description>&lt;p&gt;
&#22270;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#31070;&#32463;&#25913;&#36827;&#21551;&#21457;&#24335;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neural Improvement Heuristics for Graph Combinatorial Optimization Problems. (arXiv:2206.00383v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00383
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#31070;&#32463;&#25913;&#36827;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#20811;&#26381;&#29616;&#26377;&#27169;&#22411;&#22312;&#22788;&#29702;&#36793;&#32536;&#20449;&#24687;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#30340;&#26032;&#27169;&#22411;&#33021;&#22815;&#22312;&#20559;&#22909;&#25490;&#24207;&#38382;&#39064;&#20013;&#25552;&#20379;&#26377;&#25928;&#30340;&#37051;&#22495;&#25805;&#20316;&#65292;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#36827;&#23637;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#25552;&#39640;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#32452;&#21512;&#20248;&#21270;&#39046;&#22495;&#12290;&#22312;&#24050;&#26377;&#30340;&#32452;&#21512;&#20248;&#21270;&#27169;&#22411;&#20013;&#65292;&#31070;&#32463;&#25913;&#36827;&#65288;NI&#65289;&#27169;&#22411;&#23588;&#20854;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NI&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#23616;&#38480;&#20110;&#23558;&#20851;&#38190;&#20449;&#24687;&#32534;&#30721;&#22312;&#36793;&#19978;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#32771;&#34385;&#33410;&#28857;&#29305;&#24449;&#21644;&#33410;&#28857;&#20301;&#32622;&#32534;&#30721;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NI&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#23558;&#20449;&#24687;&#32534;&#30721;&#22312;&#33410;&#28857;&#12289;&#36793;&#25110;&#20004;&#32773;&#20013;&#30340;&#22522;&#20110;&#22270;&#30340;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20316;&#20026;&#22522;&#20110;&#29228;&#23665;&#31639;&#27861;&#30340;&#31639;&#27861;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#25351;&#23548;&#27599;&#27425;&#36845;&#20195;&#30340;&#37051;&#22495;&#25805;&#20316;&#30340;&#36873;&#25321;&#12290;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#25512;&#33616;&#20248;&#20110;&#20256;&#32479;&#29256;&#26412;&#30340;&#37051;&#22495;&#25805;&#20316;&#65292;&#23545;&#20110;&#20559;&#22909;&#25490;&#24207;&#38382;&#39064;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;99&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in graph neural network architectures and increased computation power have revolutionized the field of combinatorial optimization (CO). Among the proposed models for CO problems, Neural Improvement (NI) models have been particularly successful. However, existing NI approaches are limited in their applicability to problems where crucial information is encoded in the edges, as they only consider node features and node-wise positional encodings. To overcome this limitation, we introduce a novel NI model capable of handling graph-based problems where information is encoded in the nodes, edges, or both. The presented model serves as a fundamental component for hill-climbing-based algorithms that guide the selection of neighborhood operations for each iteration. Conducted experiments demonstrate that the proposed model can recommend neighborhood operations that outperform conventional versions for the Preference Ranking Problem with a performance in the 99th percentile. We al
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23558;&#20004;&#20010;&#38598;&#21512;&#36716;&#21270;&#20026;&#32447;&#24615;&#21487;&#20998;&#30340;&#38598;&#21512;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#65292;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2205.11716</link><description>&lt;p&gt;
&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20351;&#25968;&#25454;&#32447;&#24615;&#21487;&#20998;
&lt;/p&gt;
&lt;p&gt;
Randomly Initialized One-Layer Neural Networks Make Data Linearly Separable. (arXiv:2205.11716v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11716
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23558;&#20004;&#20010;&#38598;&#21512;&#36716;&#21270;&#20026;&#32447;&#24615;&#21487;&#20998;&#30340;&#38598;&#21512;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#65292;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#23558;&#20004;&#20010;&#20219;&#24847;&#38598;&#21512;&#26144;&#23556;&#20026;&#20004;&#20010;&#32447;&#24615;&#21487;&#20998;&#38598;&#21512;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#30456;&#27604;&#23436;&#20840;&#35757;&#32451;&#30340;&#32593;&#32476;&#65292;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#19978;&#30340;&#21560;&#24341;&#21147;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#22312;&#20110;&#24314;&#31435;&#20102;&#22312;&#36275;&#22815;&#23485;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#33021;&#22815;&#23558;&#20004;&#20010;&#38598;&#21512;&#36716;&#21270;&#20026;&#32447;&#24615;&#21487;&#20998;&#30340;&#38598;&#21512;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#24517;&#35201;&#23485;&#24230;&#30340;&#31934;&#30830;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#21021;&#22987;&#30028;&#38480;&#22312;&#36755;&#20837;&#32500;&#24230;&#19978;&#21576;&#25351;&#25968;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#21442;&#25968;&#19978;&#21576;&#22810;&#39033;&#24335;&#20381;&#36182;&#20851;&#31995;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#30028;&#38480;&#19982;&#36755;&#20837;&#32500;&#24230;&#26080;&#20851;&#65292;&#26377;&#25928;&#22320;&#20811;&#26381;&#20102;&#32500;&#24230;&#28798;&#38590;&#12290;&#25105;&#20204;&#35777;&#26126;&#20013;&#20351;&#29992;&#30340;&#20027;&#35201;&#24037;&#20855;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20960;&#20309;&#21407;&#29702;&#21644;&#38543;&#26426;&#38598;&#20013;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, neural networks have demonstrated remarkable capabilities in mapping two arbitrary sets to two linearly separable sets. The prospect of achieving this with randomly initialized neural networks is particularly appealing due to the computational efficiency compared to fully trained networks. This paper contributes by establishing that, given sufficient width, a randomly initialized one-layer neural network can, with high probability, transform two sets into two linearly separable sets without any training. Moreover, we furnish precise bounds on the necessary width of the neural network for this phenomenon to occur. Our initial bound exhibits exponential dependence on the input dimension while maintaining polynomial dependence on all other parameters. In contrast, our second bound is independent of input dimension, effectively surmounting the curse of dimensionality. The main tools used in our proof heavily relies on a fusion of geometric principles and concentration of random m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;&#23646;&#24615;&#36951;&#24536;&#26469;&#23545;&#25239;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#65292;&#20294;&#21457;&#29616;&#35813;&#26041;&#27861;&#34429;&#28982;&#23545;&#20110;&#29305;&#23450;&#23545;&#25163;&#30340;&#30446;&#26631;&#27169;&#22411;&#38450;&#24481;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26080;&#27861;&#23545;&#25239;&#25972;&#20010;PIA&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2205.08821</link><description>&lt;p&gt;
&#12298;&#32463;&#39564;&#25945;&#35757;&#65306;&#25269;&#24481;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#12299;
&lt;/p&gt;
&lt;p&gt;
Lessons Learned: Defending Against Property Inference Attacks. (arXiv:2205.08821v4 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.08821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;&#23646;&#24615;&#36951;&#24536;&#26469;&#23545;&#25239;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#65292;&#20294;&#21457;&#29616;&#35813;&#26041;&#27861;&#34429;&#28982;&#23545;&#20110;&#29305;&#23450;&#23545;&#25163;&#30340;&#30446;&#26631;&#27169;&#22411;&#38450;&#24481;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26080;&#27861;&#23545;&#25239;&#25972;&#20010;PIA&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#21644;&#35780;&#20272;&#22810;&#31181;&#38450;&#24481;&#31574;&#30053;&#26469;&#23545;&#25239;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#65288;PIA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#31169;&#25915;&#20987;&#12290;&#22312;&#32473;&#23450;&#19968;&#20010;&#35757;&#32451;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;PIA&#26088;&#22312;&#25552;&#21462;&#20854;&#24213;&#23618;&#35757;&#32451;&#25968;&#25454;&#30340;&#32479;&#35745;&#23646;&#24615;&#65292;&#20363;&#22914;&#25581;&#31034;&#21307;&#30103;&#25968;&#25454;&#38598;&#20013;&#30007;&#24615;&#21644;&#22899;&#24615;&#30340;&#27604;&#20363;&#12290;&#34429;&#28982;&#38024;&#23545;&#20854;&#20182;&#38544;&#31169;&#25915;&#20987;&#65292;&#20363;&#22914;&#25104;&#21592;&#25512;&#26029;&#65292;&#24050;&#32463;&#26377;&#24456;&#22810;&#20851;&#20110;&#38450;&#24481;&#26426;&#21046;&#30340;&#30740;&#31350;&#21457;&#34920;&#65292;&#20294;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#27880;&#20110;&#38450;&#24481;PIA&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#36890;&#29992;&#30340;&#25269;&#24481;&#30333;&#30418;PIA&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;-&#23646;&#24615;&#36951;&#24536;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23646;&#24615;&#36951;&#24536;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#34429;&#28982;&#23646;&#24615;&#36951;&#24536;&#23545;&#20110;&#38024;&#23545;&#29305;&#23450;&#23545;&#25163;&#30340;&#30446;&#26631;&#27169;&#22411;&#30340;&#38450;&#24481;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26080;&#27861;&#27010;&#25324;&#65292;&#21363;&#26080;&#27861;&#20445;&#25252;&#25972;&#20010;PIA&#31867;&#21035;&#12290;&#20026;&#20102;&#25506;&#31350;&#36825;&#31181;&#38480;&#21046;&#30340;&#21407;&#22240;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23454;&#39564;&#20013;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates and evaluates multiple defense strategies against property inference attacks (PIAs), a privacy attack against machine learning models. Given a trained machine learning model, PIAs aim to extract statistical properties of its underlying training data, e.g., reveal the ratio of men and women in a medical training data set. While for other privacy attacks like membership inference, a lot of research on defense mechanisms has been published, this is the first work focusing on defending against PIAs. With the primary goal of developing a generic mitigation strategy against white-box PIAs, we propose the novel approach property unlearning. Extensive experiments with property unlearning show that while it is very effective when defending target models against specific adversaries, property unlearning is not able to generalize, i.e., protect against a whole class of PIAs. To investigate the reasons behind this limitation, we present the results of experiments with the ex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#21152;&#23494;&#32593;&#32476;&#27969;&#37327;&#24212;&#29992;&#26631;&#27880;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#26469;&#36827;&#34892;&#35757;&#32451;&#21644;&#26631;&#27880;&#12290;</title><link>http://arxiv.org/abs/2205.05628</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#21152;&#23494;&#32593;&#32476;&#27969;&#37327;&#24212;&#29992;&#26631;&#27880;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Extensible Machine Learning for Encrypted Network Traffic Application Labeling via Uncertainty Quantification. (arXiv:2205.05628v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.05628
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#21152;&#23494;&#32593;&#32476;&#27969;&#37327;&#24212;&#29992;&#26631;&#27880;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#26469;&#36827;&#34892;&#35757;&#32451;&#21644;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21152;&#23494;&#32593;&#32476;&#27969;&#37327;&#30340;&#26222;&#21450;&#65292;&#32593;&#32476;&#23433;&#20840;&#20998;&#26512;&#20154;&#21592;&#24320;&#22987;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#35835;&#20854;&#32593;&#32476;&#19978;&#30340;&#27969;&#37327;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#26032;&#30340;&#27969;&#37327;&#20986;&#29616;&#65292;&#36229;&#20986;&#20102;&#35757;&#32451;&#38598;&#30340;&#20998;&#24067;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#21464;&#24471;&#38472;&#26087;&#12290;&#20026;&#20102;&#22312;&#36825;&#20010;&#21160;&#24577;&#29615;&#22659;&#20013;&#21487;&#38752;&#22320;&#36866;&#24212;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24517;&#39035;&#25552;&#20379;&#19978;&#19979;&#25991;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#28982;&#32780;&#36825;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#40092;&#26377;&#20851;&#27880;&#12290;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22312;&#20004;&#26041;&#38754;&#26159;&#24517;&#35201;&#30340;&#65292;&#19968;&#26159;&#22312;&#26631;&#31614;&#20998;&#37197;&#26102;&#25351;&#31034;&#27169;&#22411;&#23545;&#20110;&#36873;&#25321;&#21738;&#20010;&#31867;&#21035;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#65292;&#20108;&#26159;&#24403;&#27969;&#37327;&#19981;&#22826;&#21487;&#33021;&#23646;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#31867;&#21035;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#21152;&#23494;&#34394;&#25311;&#31169;&#20154;&#32593;&#32476;(VPN)&#30340;&#32593;&#32476;&#27969;&#37327;&#30340;&#26032;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;10&#20010;&#24212;&#29992;&#31243;&#24207;&#29983;&#25104;&#30340;&#24102;&#26377;&#26631;&#31614;&#30340;&#32593;&#32476;&#27969;&#37327;&#65292;&#24182;&#23545;&#24212;5&#20010;&#24212;&#29992;&#31243;&#24207;&#31867;&#21035;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#24555;&#36895;&#35757;&#32451;&#27169;&#22411;&#24182;&#36827;&#34892;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing prevalence of encrypted network traffic, cyber security analysts have been turning to machine learning (ML) techniques to elucidate the traffic on their networks. However, ML models can become stale as new traffic emerges that is outside of the distribution of the training set. In order to reliably adapt in this dynamic environment, ML models must additionally provide contextualized uncertainty quantification to their predictions, which has received little attention in the cyber security domain. Uncertainty quantification is necessary both to signal when the model is uncertain about which class to choose in its label assignment and when the traffic is not likely to belong to any pre-trained classes.  We present a new, public dataset of network traffic that includes labeled, Virtual Private Network (VPN)-encrypted network traffic generated by 10 applications and corresponding to 5 application categories. We also present an ML framework that is designed to rapidly tra
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#22686;&#37327;&#26799;&#24230;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#20248;&#21270;&#20219;&#21153;&#20013;&#25163;&#21160;&#35843;&#25972;&#27493;&#38271;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#38024;&#23545;&#22686;&#37327;&#26799;&#24230;&#27861;&#30340;Barzilai-Borwein&#27493;&#38271;&#21464;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#25903;&#25345;&#38750;&#27431;&#20960;&#37324;&#24503;&#33539;&#25968;&#22635;&#34917;&#20102;&#29616;&#26377;SAGA&#31639;&#27861;&#20998;&#26512;&#30340;&#31354;&#30333;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2205.02273</link><description>&lt;p&gt;
&#25903;&#25345;&#38750;&#27431;&#20960;&#37324;&#24503;&#33539;&#25968;&#30340;&#33258;&#36866;&#24212;&#22686;&#37327;&#26799;&#24230;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Adaptive Incremental Gradient Method With Support for Non-Euclidean Norms. (arXiv:2205.02273v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02273
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#22686;&#37327;&#26799;&#24230;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#20248;&#21270;&#20219;&#21153;&#20013;&#25163;&#21160;&#35843;&#25972;&#27493;&#38271;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#38024;&#23545;&#22686;&#37327;&#26799;&#24230;&#27861;&#30340;Barzilai-Borwein&#27493;&#38271;&#21464;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#25903;&#25345;&#38750;&#27431;&#20960;&#37324;&#24503;&#33539;&#25968;&#22635;&#34917;&#20102;&#29616;&#26377;SAGA&#31639;&#27861;&#20998;&#26512;&#30340;&#31354;&#30333;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26041;&#24046;&#38477;&#20302;&#26041;&#27861;&#22312;&#35299;&#20915;&#26377;&#38480;&#21644;&#38382;&#39064;&#26102;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#29992;&#25143;&#25163;&#21160;&#35843;&#25972;&#27493;&#38271;&#65292;&#36825;&#23545;&#20110;&#19968;&#20123;&#22823;&#35268;&#27169;&#20248;&#21270;&#20219;&#21153;&#26469;&#35828;&#26159;&#32791;&#26102;&#29978;&#33267;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#20960;&#31181;&#26032;&#22411;&#30340;&#33258;&#36866;&#24212;&#21464;&#31181;SAGA&#31639;&#27861;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#38024;&#23545;&#22686;&#37327;&#26799;&#24230;&#27861;&#30340;Barzilai-Borwein&#27493;&#38271;&#21464;&#20307;&#65292;&#20197;&#30830;&#20445;&#20869;&#23384;&#25928;&#29575;&#21644;&#24555;&#36895;&#25910;&#25947;&#12290;&#25105;&#20204;&#22312;&#20801;&#35768;&#22312;&#24179;&#28369;&#24230;&#21644;&#22797;&#21512;&#30446;&#26631;&#30340;&#23450;&#20041;&#20013;&#20351;&#29992;&#38750;&#27431;&#20960;&#37324;&#24503;&#33539;&#25968;&#30340;&#19968;&#33324;&#35774;&#32622;&#19979;&#24314;&#31435;&#20854;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#36825;&#28085;&#30422;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#24191;&#27867;&#33539;&#22260;&#24212;&#29992;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;SAGA&#30340;&#20998;&#26512;&#65292;&#20197;&#25903;&#25345;&#38750;&#27431;&#20960;&#37324;&#24503;&#33539;&#25968;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#24037;&#20316;&#30340;&#31354;&#30333;&#12290;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#24046;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic variance reduced methods have shown strong performance in solving finite-sum problems. However, these methods usually require the users to manually tune the step-size, which is time-consuming or even infeasible for some large-scale optimization tasks. To overcome the problem, we propose and analyze several novel adaptive variants of the popular SAGA algorithm. Eventually, we design a variant of Barzilai-Borwein step-size which is tailored for the incremental gradient method to ensure memory efficiency and fast convergence. We establish its convergence guarantees under general settings that allow non-Euclidean norms in the definition of smoothness and the composite objectives, which cover a broad range of applications in machine learning. We improve the analysis of SAGA to support non-Euclidean norms, which fills the void of existing work. Numerical experiments on standard datasets demonstrate a competitive performance of the proposed algorithm compared with existing variance
&lt;/p&gt;</description></item><item><title>CANShield&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20449;&#21495;&#32423;&#21035;&#20837;&#20405;&#26816;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#20195;&#36710;&#36742;&#20013;&#23545;&#20110;CAN&#24635;&#32447;&#30340;&#26234;&#33021;&#25915;&#20987;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.01306</link><description>&lt;p&gt;
CANShield: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20449;&#21495;&#32423;&#25511;&#21046;&#22120;&#23616;&#22495;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CANShield: Deep Learning-Based Intrusion Detection Framework for Controller Area Networks at the Signal-Level. (arXiv:2205.01306v4 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.01306
&lt;/p&gt;
&lt;p&gt;
CANShield&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20449;&#21495;&#32423;&#21035;&#20837;&#20405;&#26816;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#20195;&#36710;&#36742;&#20013;&#23545;&#20110;CAN&#24635;&#32447;&#30340;&#26234;&#33021;&#25915;&#20987;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#36710;&#36742;&#20381;&#36182;&#20110;&#36890;&#36807;&#25511;&#21046;&#22120;&#23616;&#22495;&#32593;&#32476;(CAN)&#24635;&#32447;&#36830;&#25509;&#30340;&#19968;&#31995;&#21015;&#30005;&#23376;&#25511;&#21046;&#21333;&#20803;(ECU)&#36827;&#34892;&#20851;&#38190;&#24615;&#36710;&#36742;&#25511;&#21046;&#12290;&#38543;&#30528;&#27773;&#36710;&#20013;&#20808;&#36827;&#36830;&#25509;&#21151;&#33021;&#30340;&#25193;&#23637;&#20197;&#21450;&#20869;&#37096;&#31995;&#32479;&#26333;&#20809;&#39118;&#38505;&#30340;&#25552;&#39640;&#65292;CAN&#24635;&#32447;&#36234;&#26469;&#36234;&#23481;&#26131;&#21463;&#21040;&#20837;&#20405;&#21644;&#27880;&#20837;&#25915;&#20987;&#12290;&#26222;&#36890;&#30340;&#27880;&#20837;&#25915;&#20987;&#20250;&#30772;&#22351;CAN&#25968;&#25454;&#27969;&#30340;&#20856;&#22411;&#26102;&#24207;&#29305;&#24615;&#65292;&#22522;&#20110;&#35268;&#21017;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(IDS)&#21487;&#20197;&#36731;&#26494;&#26816;&#27979;&#21040;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#39640;&#32423;&#25915;&#20987;&#32773;&#21487;&#20197;&#21521;&#20449;&#21495;/&#35821;&#20041;&#32423;&#21035;&#27880;&#20837;&#34394;&#20551;&#25968;&#25454;&#65292;&#21516;&#26102;&#36890;&#36807;CAN&#28040;&#24687;&#30340;&#27169;&#24335;/&#39057;&#29575;&#21576;&#29616;&#20986;&#30475;&#20284;&#26080;&#23475;&#30340;&#24418;&#24335;&#12290;&#22522;&#20110;&#35268;&#21017;&#21644;&#22522;&#20110;&#24322;&#24120;&#30340;IDS&#20165;&#20165;&#24314;&#31435;&#22312;CAN&#28040;&#24687;ID&#24207;&#21015;&#25110;&#20165;&#20108;&#36827;&#21046;&#36127;&#36733;&#25968;&#25454;&#19978;&#65292;&#22312;&#26816;&#27979;&#27492;&#31867;&#25915;&#20987;&#26041;&#38754;&#25928;&#26524;&#36739;&#24046;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26816;&#27979;&#27492;&#31867;&#26234;&#33021;&#25915;&#20987;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CANShield&#65292;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20449;&#21495;&#32423;&#21035;&#20837;&#20405;&#26816;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;CAN&#24635;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern vehicles rely on a fleet of electronic control units (ECUs) connected through controller area network (CAN) buses for critical vehicular control. With the expansion of advanced connectivity features in automobiles and the elevated risks of internal system exposure, the CAN bus is increasingly prone to intrusions and injection attacks. As ordinary injection attacks disrupt the typical timing properties of the CAN data stream, rule-based intrusion detection systems (IDS) can easily detect them. However, advanced attackers can inject false data to the signal/semantic level, while looking innocuous by the pattern/frequency of the CAN messages. The rule-based IDS, as well as the anomaly-based IDS, are built merely on the sequence of CAN messages IDs or just the binary payload data and are less effective in detecting such attacks. Therefore, to detect such intelligent attacks, we propose CANShield, a deep learning-based signal-level intrusion detection framework for the CAN bus. CANSh
&lt;/p&gt;</description></item><item><title>ULF&#26159;&#19968;&#31181;&#29992;&#20110;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#26631;&#31614;&#20989;&#25968;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#21407;&#29702;&#30340;&#22122;&#22768;&#38477;&#20302;&#25216;&#26415;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#35760;&#12290;</title><link>http://arxiv.org/abs/2204.06863</link><description>&lt;p&gt;
ULF: &#26080;&#30417;&#30563;&#26631;&#31614;&#20989;&#25968;&#26657;&#27491;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ULF: Unsupervised Labeling Function Correction using Cross-Validation for Weak Supervision. (arXiv:2204.06863v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.06863
&lt;/p&gt;
&lt;p&gt;
ULF&#26159;&#19968;&#31181;&#29992;&#20110;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#26631;&#31614;&#20989;&#25968;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#21407;&#29702;&#30340;&#22122;&#22768;&#38477;&#20302;&#25216;&#26415;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#26159;&#19968;&#31181;&#20195;&#26367;&#25163;&#21160;&#26631;&#35760;&#25968;&#25454;&#30340;&#32463;&#27982;&#26377;&#25928;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#19968;&#32452;&#26631;&#31614;&#20989;&#25968;&#65288;LFs&#65289;&#23545;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#33258;&#21160;&#26631;&#27880;&#12290;&#26631;&#31614;&#20989;&#25968;&#26159;&#22522;&#20110;&#35268;&#21017;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#20026;&#30456;&#20851;&#31867;&#21035;&#29983;&#25104;&#20154;&#24037;&#26631;&#31614;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;k&#25240;&#20132;&#21449;&#39564;&#35777;&#21407;&#29702;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#22122;&#22768;&#38477;&#20302;&#25216;&#26415;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ULF&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#26631;&#31614;&#20989;&#25968;&#26657;&#27491;&#65292;&#36890;&#36807;&#21033;&#29992;&#22312;&#25152;&#26377;&#26631;&#31614;&#20989;&#25968;&#20043;&#22806;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#35782;&#21035;&#21644;&#32416;&#27491;&#29305;&#23450;&#20110;&#20445;&#30041;&#26631;&#31614;&#20989;&#25968;&#30340;&#20559;&#24046;&#65292;&#20174;&#32780;&#23545;&#24369;&#30417;&#30563;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ULF&#36890;&#36807;&#37325;&#26032;&#20272;&#35745;&#39640;&#21487;&#38752;&#24615;&#20132;&#21449;&#39564;&#35777;&#26679;&#26412;&#19978;&#30340;&#26631;&#31614;&#20989;&#25968;&#20998;&#37197;&#65292;&#26469;&#25913;&#36827;&#26631;&#31614;&#20989;&#25968;&#23545;&#31867;&#21035;&#30340;&#20998;&#37197;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#35777;&#23454;&#20102;ULF&#22312;&#22686;&#24378;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#26080;&#38656;&#25163;&#21160;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
A cost-effective alternative to manual data labeling is weak supervision (WS), where data samples are automatically annotated using a predefined set of labeling functions (LFs), rule-based mechanisms that generate artificial labels for the associated classes. In this work, we investigate noise reduction techniques for WS based on the principle of k-fold cross-validation. We introduce a new algorithm ULF for Unsupervised Labeling Function correction, which denoises WS data by leveraging models trained on all but some LFs to identify and correct biases specific to the held-out LFs. Specifically, ULF refines the allocation of LFs to classes by re-estimating this assignment on highly reliable cross-validated samples. Evaluation on multiple datasets confirms ULF's effectiveness in enhancing WS learning without the need for manual labeling
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#37325;&#35201;&#24615;&#25490;&#24207;&#23398;&#20064;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;LambdaMART&#12289;YetiRank&#21644;StochasticRank&#31561;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.01500</link><description>&lt;p&gt;
&#37325;&#35201;&#24615;&#25490;&#24207;&#23398;&#20064;&#20013;&#21738;&#20123;&#25216;&#24039;&#26159;&#37325;&#35201;&#30340;&#65311;
&lt;/p&gt;
&lt;p&gt;
Which Tricks Are Important for Learning to Rank?. (arXiv:2204.01500v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.01500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#37325;&#35201;&#24615;&#25490;&#24207;&#23398;&#20064;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;LambdaMART&#12289;YetiRank&#21644;StochasticRank&#31561;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#26368;&#20808;&#36827;&#30340;&#37325;&#35201;&#24615;&#25490;&#24207;&#23398;&#20064;&#26041;&#27861;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#12290;&#26368;&#33879;&#21517;&#30340;&#31639;&#27861;&#26159;LambdaMART&#65292;&#25552;&#20986;&#24050;&#26377;&#21313;&#22810;&#24180;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#20854;&#20182;&#22522;&#20110;GBDT&#30340;&#25490;&#24207;&#31639;&#27861;&#12290;&#26412;&#25991;&#22312;&#32479;&#19968;&#30340;&#35774;&#32622;&#19979;&#20840;&#38754;&#20998;&#26512;&#36825;&#20123;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35299;&#20915;&#20197;&#19979;&#38382;&#39064;&#12290;&#30452;&#25509;&#20248;&#21270;&#24179;&#28369;&#25490;&#24207;&#25439;&#22833;&#26159;&#21542;&#20248;&#20110;&#20248;&#21270;&#20984;&#20284;&#20195;&#29702;&#65311;&#22914;&#20309;&#27491;&#30830;&#26500;&#24314;&#21644;&#24179;&#28369;&#20195;&#29702;&#25490;&#24207;&#25439;&#22833;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;LambdaMART&#19982;YetiRank&#21644;StochasticRank&#26041;&#27861;&#21450;&#20854;&#20462;&#25913;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;YetiRank&#26041;&#27861;&#30340;&#31616;&#21333;&#25913;&#36827;&#65292;&#20801;&#35768;&#20248;&#21270;&#29305;&#23450;&#30340;&#25490;&#24207;&#25439;&#22833;&#20989;&#25968;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#23545;&#37325;&#35201;&#24615;&#25490;&#24207;&#25216;&#26415;&#26377;&#20102;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#24182;&#33719;&#24471;&#20102;&#19968;&#31181;&#26032;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, state-of-the-art learning-to-rank methods are based on gradient-boosted decision trees (GBDT). The most well-known algorithm is LambdaMART which was proposed more than a decade ago. Recently, several other GBDT-based ranking algorithms were proposed. In this paper, we thoroughly analyze these methods in a unified setup. In particular, we address the following questions. Is direct optimization of a smoothed ranking loss preferable over optimizing a convex surrogate? How to properly construct and smooth surrogate ranking losses? To address these questions, we compare LambdaMART with YetiRank and StochasticRank methods and their modifications. We also propose a simple improvement of the YetiRank approach that allows for optimizing specific ranking loss functions. As a result, we gain insights into learning-to-rank techniques and obtain a new state-of-the-art algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20132;&#20114;&#24335;&#21407;&#22411;&#20462;&#35746;&#36827;&#34892;&#25512;&#29702;&#35843;&#25972;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29992;&#25143;&#25552;&#31034;&#21644;&#32416;&#27491;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#21487;&#20197;&#28040;&#38500;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#21512;&#29702;&#22240;&#32032;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#26159;&#27491;&#30830;&#30340;&#20998;&#31867;&#20063;&#21487;&#33021;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#20013;&#20135;&#29983;&#30340;&#28151;&#28102;&#21464;&#37327;&#23548;&#33268;&#30340;&#19981;&#21512;&#29702;&#21407;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20132;&#20114;&#26041;&#26696;&#65292;&#21487;&#20197;&#20132;&#20114;&#22320;&#35782;&#21035;&#24182;&#21024;&#38500;&#38169;&#35823;&#30340;&#21407;&#22411;&#12290;</title><link>http://arxiv.org/abs/2203.10087</link><description>&lt;p&gt;
&#20294;&#36825;&#24182;&#19981;&#26159;&#21407;&#22240;&#65306;&#36890;&#36807;&#20132;&#20114;&#24335;&#21407;&#22411;&#20462;&#35746;&#36827;&#34892;&#25512;&#29702;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
But that's not why: Inference adjustment by interactive prototype revision. (arXiv:2203.10087v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.10087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20132;&#20114;&#24335;&#21407;&#22411;&#20462;&#35746;&#36827;&#34892;&#25512;&#29702;&#35843;&#25972;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29992;&#25143;&#25552;&#31034;&#21644;&#32416;&#27491;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#21487;&#20197;&#28040;&#38500;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#21512;&#29702;&#22240;&#32032;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#26159;&#27491;&#30830;&#30340;&#20998;&#31867;&#20063;&#21487;&#33021;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#20013;&#20135;&#29983;&#30340;&#28151;&#28102;&#21464;&#37327;&#23548;&#33268;&#30340;&#19981;&#21512;&#29702;&#21407;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20132;&#20114;&#26041;&#26696;&#65292;&#21487;&#20197;&#20132;&#20114;&#22320;&#35782;&#21035;&#24182;&#21024;&#38500;&#38169;&#35823;&#30340;&#21407;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#20173;&#28982;&#19981;&#23436;&#21892;&#65292;&#36890;&#24120;&#38656;&#35201;&#20154;&#31867;&#21518;&#26399;&#24178;&#39044;&#12290;&#22914;&#26524;&#27169;&#22411;&#30340;&#39044;&#27979;&#20381;&#36182;&#20110;&#19981;&#21512;&#29702;&#30340;&#22240;&#32032;&#65292;&#24076;&#26395;&#33021;&#22815;&#28040;&#38500;&#23427;&#20204;&#30340;&#24433;&#21709;&#12290;&#28145;&#24230;&#20132;&#20114;&#24335;&#21407;&#22411;&#35843;&#25972;&#20351;&#29992;&#25143;&#33021;&#22815;&#25552;&#20379;&#25552;&#31034;&#24182;&#32416;&#27491;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#26412;&#25991;&#28436;&#31034;&#20102;&#21407;&#22411;&#37096;&#20998;&#27169;&#22411;&#38750;&#24120;&#36866;&#21512;&#36825;&#39033;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#39044;&#27979;&#22522;&#20110;&#21487;&#20197;&#30001;&#29992;&#25143;&#36827;&#34892;&#35821;&#20041;&#35299;&#37322;&#30340;&#21407;&#22411;&#22270;&#20687;&#22359;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#27491;&#30830;&#30340;&#20998;&#31867;&#20063;&#21487;&#33021;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#28151;&#28102;&#21464;&#37327;&#23548;&#33268;&#30340;&#19981;&#21512;&#29702;&#21407;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25512;&#29702;&#35843;&#25972;&#20132;&#20114;&#26041;&#26696;&#65306;&#29992;&#25143;&#21487;&#20197;&#20132;&#20114;&#22320;&#35782;&#21035;&#38169;&#35823;&#30340;&#21407;&#22411;&#12290;&#38750;&#30446;&#26631;&#21407;&#22411;&#21487;&#20197;&#36890;&#36807;&#21407;&#22411;&#36974;&#32617;&#25110;&#33258;&#23450;&#20041;&#30340;&#21435;&#36873;&#25321;&#35757;&#32451;&#27169;&#24335;&#36827;&#34892;&#21024;&#38500;&#12290;&#20132;&#20114;&#24335;&#21407;&#22411;&#25298;&#32477;&#20351;&#26426;&#22120;&#23398;&#20064;&#33021;&#22815;&#35843;&#25972;&#20854;&#25512;&#29702;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant advances in machine learning, decision-making of artificial agents is still not perfect and often requires post-hoc human interventions. If the prediction of a model relies on unreasonable factors it is desirable to remove their effect. Deep interactive prototype adjustment enables the user to give hints and correct the model's reasoning. In this paper, we demonstrate that prototypical-part models are well suited for this task as their prediction is based on prototypical image patches that can be interpreted semantically by the user. It shows that even correct classifications can rely on unreasonable prototypes that result from confounding variables in a dataset. Hence, we propose simple yet effective interaction schemes for inference adjustment: The user is consulted interactively to identify faulty prototypes. Non-object prototypes can be removed by prototype masking or a custom mode of deselection training. Interactive prototype rejection allows machine learning 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23547;&#25214;&#31574;&#30053;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#23433;&#20840;&#21306;&#22495;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#20934;&#21017;&#36924;&#36817;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#35745;&#31639;&#20986;&#36867;&#36920;&#27010;&#29575;&#21644;&#23433;&#20840;&#21306;&#22495;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2202.11593</link><description>&lt;p&gt;
&#23547;&#25214;&#31574;&#30053;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#23433;&#20840;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Finding Safe Zones of policies Markov Decision Processes. (arXiv:2202.11593v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.11593
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23547;&#25214;&#31574;&#30053;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#23433;&#20840;&#21306;&#22495;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#20934;&#21017;&#36924;&#36817;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#35745;&#31639;&#20986;&#36867;&#36920;&#27010;&#29575;&#21644;&#23433;&#20840;&#21306;&#22495;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#23433;&#20840;&#21306;&#22495;&#65292;&#21363;&#29366;&#24577;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#22823;&#22810;&#25968;&#31574;&#30053;&#30340;&#36712;&#36857;&#37117;&#34987;&#38480;&#21046;&#22312;&#35813;&#23376;&#38598;&#20869;&#12290;&#23433;&#20840;&#21306;&#22495;&#30340;&#36136;&#37327;&#30001;&#29366;&#24577;&#25968;&#21644;&#36867;&#36920;&#27010;&#29575;&#21442;&#25968;&#21270;&#65292;&#21363;&#38543;&#26426;&#36712;&#36857;&#31163;&#24320;&#23376;&#38598;&#30340;&#27010;&#29575;&#12290;&#24403;&#23433;&#20840;&#21306;&#22495;&#20855;&#26377;&#23569;&#37327;&#30340;&#29366;&#24577;&#21644;&#36739;&#20302;&#30340;&#36867;&#36920;&#27010;&#29575;&#26102;&#65292;&#23588;&#20854;&#26377;&#36259;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23547;&#25214;&#26368;&#20248;&#23433;&#20840;&#21306;&#22495;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#33324;&#24773;&#20917;&#19979;&#35813;&#38382;&#39064;&#35745;&#31639;&#19978;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#19968;&#20010;&#21452;&#20934;&#21017;&#36924;&#36817;&#23398;&#20064;&#31639;&#27861;&#65292;&#20934;&#30830;&#24230;&#36817;&#20284;&#20026;$2$&#20493;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#36867;&#36920;&#27010;&#29575;&#21644;&#23433;&#20840;&#21306;&#22495;&#22823;&#23567;&#65292;&#24182;&#19988;&#20351;&#29992;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a policy of a Markov Decision Process, we define a SafeZone as a subset of states, such that most of the policy's trajectories are confined to this subset. The quality of a SafeZone is parameterized by the number of states and the escape probability, i.e., the probability that a random trajectory will leave the subset. SafeZones are especially interesting when they have a small number of states and low escape probability. We study the complexity of finding optimal SafeZones, and show that in general, the problem is computationally hard. Our main result is a bi-criteria approximation learning algorithm with a factor of almost $2$ approximation for both the escape probability and SafeZone size, using a polynomial size sample complexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24471;&#20998;&#24341;&#23548;&#32593;&#32476;&#65292;&#36890;&#36807;&#24471;&#20998;&#24341;&#23548;&#31574;&#30053;&#22686;&#24378;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#36807;&#28193;&#39046;&#22495;&#20013;&#27491;&#24120;&#21644;&#24322;&#24120;&#25968;&#25454;&#28151;&#21512;&#12289;&#23450;&#20041;&#26377;&#25928;&#24230;&#37327;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2109.04684</link><description>&lt;p&gt;
&#29992;&#24471;&#20998;&#24341;&#23548;&#32593;&#32476;&#22686;&#24378;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Unsupervised Anomaly Detection with Score-Guided Network. (arXiv:2109.04684v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.04684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24471;&#20998;&#24341;&#23548;&#32593;&#32476;&#65292;&#36890;&#36807;&#24471;&#20998;&#24341;&#23548;&#31574;&#30053;&#22686;&#24378;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#36807;&#28193;&#39046;&#22495;&#20013;&#27491;&#24120;&#21644;&#24322;&#24120;&#25968;&#25454;&#28151;&#21512;&#12289;&#23450;&#20041;&#26377;&#25928;&#24230;&#37327;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#21253;&#25324;&#21307;&#30103;&#21644;&#37329;&#34701;&#31995;&#32479;&#22312;&#20869;&#30340;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#30001;&#20110;&#36825;&#20123;&#22797;&#26434;&#31995;&#32479;&#20013;&#24322;&#24120;&#26631;&#31614;&#25968;&#37327;&#26377;&#38480;&#65292;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#29616;&#26377;&#26080;&#30417;&#30563;&#26041;&#27861;&#38754;&#20020;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#65306;&#65288;&#19968;&#65289;&#22312;&#36807;&#28193;&#39046;&#22495;&#20013;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#25968;&#25454;&#65292;&#20854;&#20013;&#27491;&#24120;&#21644;&#24322;&#24120;&#25968;&#25454;&#39640;&#24230;&#28151;&#21512;&#65307;&#65288;&#20108;&#65289;&#23450;&#20041;&#19968;&#31181;&#26377;&#25928;&#30340;&#24230;&#37327;&#26469;&#26368;&#22823;&#21270;&#22312;&#20551;&#35774;&#31354;&#38388;&#20013;&#27491;&#24120;&#21644;&#24322;&#24120;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#35813;&#31354;&#38388;&#26159;&#30001;&#34920;&#31034;&#23398;&#20064;&#22120;&#26500;&#24314;&#30340;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24471;&#20998;&#24341;&#23548;&#32593;&#32476;&#24182;&#37319;&#29992;&#24471;&#20998;&#24341;&#23548;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#21644;&#25193;&#22823;&#27491;&#24120;&#21644;&#24322;&#24120;&#25968;&#25454;&#20043;&#38388;&#30340;&#24322;&#24120;&#20998;&#25968;&#24046;&#24322;&#12290;&#36890;&#36807;&#36825;&#31181;&#24471;&#20998;&#24341;&#23548;&#31574;&#30053;&#65292;&#34920;&#31034;&#23398;&#20064;&#22120;&#21487;&#20197;&#22312;&#27169;&#22411;&#35757;&#32451;&#38454;&#27573;&#36880;&#28176;&#23398;&#20064;&#21040;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#36807;&#28193;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection plays a crucial role in various real-world applications, including healthcare and finance systems. Owing to the limited number of anomaly labels in these complex systems, unsupervised anomaly detection methods have attracted great attention in recent years. Two major challenges faced by the existing unsupervised methods are: (i) distinguishing between normal and abnormal data in the transition field, where normal and abnormal data are highly mixed together; (ii) defining an effective metric to maximize the gap between normal and abnormal data in a hypothesis space, which is built by a representation learner. To that end, this work proposes a novel scoring network with a score-guided regularization to learn and enlarge the anomaly score disparities between normal and abnormal data. With such score-guided strategy, the representation learner can gradually learn more informative representation during the model training stage, especially for the samples in the transition 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21151;&#33021;&#25968;&#25454;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#36830;&#32493;&#38544;&#34255;&#23618;&#23454;&#29616;&#23545;&#21151;&#33021;&#21709;&#24212;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#31181;&#27169;&#22411;&#25311;&#21512;&#31574;&#30053;&#65288;FDNN&#21644;FBNN&#65289;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#25216;&#26415;&#24471;&#21040;&#26356;&#21152;&#31616;&#26126;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2107.14151</link><description>&lt;p&gt;
&#29616;&#20195;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#65306;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21151;&#33021;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Modern Non-Linear Function-on-Function Regression. (arXiv:2107.14151v1 [stat.ME] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.14151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21151;&#33021;&#25968;&#25454;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#36830;&#32493;&#38544;&#34255;&#23618;&#23454;&#29616;&#23545;&#21151;&#33021;&#21709;&#24212;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#31181;&#27169;&#22411;&#25311;&#21512;&#31574;&#30053;&#65288;FDNN&#21644;FBNN&#65289;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#25216;&#26415;&#24471;&#21040;&#26356;&#21152;&#31616;&#26126;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#31867;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21151;&#33021;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#29992;&#30001;&#36830;&#32493;&#31070;&#32463;&#20803;&#32452;&#25104;&#30340;&#38544;&#34255;&#23618;&#65292;&#31216;&#20026;&#36830;&#32493;&#38544;&#34255;&#23618;&#65292;&#29992;&#20110;&#21151;&#33021;&#21709;&#24212;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#31181;&#27169;&#22411;&#25311;&#21512;&#31574;&#30053;&#65306;&#21151;&#33021;&#30452;&#25509;&#31070;&#32463;&#32593;&#32476;&#65288;FDNN&#65289;&#21644;&#21151;&#33021;&#22522;&#30784;&#31070;&#32463;&#32593;&#32476;&#65288;FBNN&#65289;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#26159;&#19987;&#38376;&#35774;&#35745;&#26469;&#21033;&#29992;&#21151;&#33021;&#25968;&#25454;&#22266;&#26377;&#30340;&#32467;&#26500;&#65292;&#24182;&#25429;&#25417;&#21151;&#33021;&#39044;&#27979;&#21464;&#37327;&#21644;&#21151;&#33021;&#21709;&#24212;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#27714;&#35299;&#20989;&#25968;&#26799;&#24230;&#24182;&#23454;&#26045;&#27491;&#21017;&#21270;&#25216;&#26415;&#36827;&#34892;&#27169;&#22411;&#25311;&#21512;&#65292;&#24471;&#21040;&#26356;&#31616;&#26126;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#31034;&#20363;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#21151;&#33021;&#27169;&#22411;&#26041;&#38754;&#30340;&#24378;&#22823;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new class of non-linear function-on-function regression models for functional data using neural networks. We propose a framework using a hidden layer consisting of continuous neurons, called a continuous hidden layer, for functional response modeling and give two model fitting strategies, Functional Direct Neural Network (FDNN) and Functional Basis Neural Network (FBNN). Both are designed explicitly to exploit the structure inherent in functional data and capture the complex relations existing between the functional predictors and the functional response. We fit these models by deriving functional gradients and implement regularization techniques for more parsimonious results. We demonstrate the power and flexibility of our proposed method in handling complex functional models through extensive simulation studies as well as real data examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#25163;&#20876;&#65292;&#24182;&#35299;&#38145;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20854;&#20182;&#31867;&#22411;&#25968;&#25454;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2107.12809</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#24207;&#36143;&#23454;&#39564;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#22686;&#26448;&#21046;&#36896;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimisation for Sequential Experimental Design with Applications in Additive Manufacturing. (arXiv:2107.12809v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.12809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#25163;&#20876;&#65292;&#24182;&#35299;&#38145;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20854;&#20182;&#31867;&#22411;&#25968;&#25454;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#29992;&#20110;&#20840;&#23616;&#20248;&#21270;&#26114;&#36149;&#35780;&#20272;&#30340;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#22312;&#26448;&#26009;&#31185;&#23398;&#12289;&#21270;&#23398;&#12289;&#23454;&#39564;&#29289;&#29702;&#23398;&#12289;&#33647;&#29289;&#24320;&#21457;&#31561;&#39046;&#22495;&#65292;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#23454;&#39564;&#35774;&#35745;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24341;&#36215;&#20154;&#20204;&#23545;&#24212;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#36827;&#34892;&#23454;&#39564;&#35774;&#35745;&#30340;&#30410;&#22788;&#30340;&#20851;&#27880;&#65292;&#24182;&#25552;&#20379;&#19968;&#20221;&#36125;&#21494;&#26031;&#20248;&#21270;&#25163;&#20876;&#65292;&#28085;&#30422;&#26041;&#27861;&#21644;&#36719;&#20214;&#65292;&#26041;&#20415;&#20219;&#20309;&#24076;&#26395;&#24212;&#29992;&#25110;&#23398;&#20064;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#20154;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31616;&#35201;&#35299;&#37322;&#36125;&#21494;&#26031;&#20248;&#21270;&#25216;&#26415;&#65292;&#22312;&#22686;&#26448;&#21046;&#36896;&#20013;&#22238;&#39038;&#25152;&#26377;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#24212;&#29992;&#65292;&#27604;&#36739;&#21644;&#20030;&#20363;&#19981;&#21516;&#24320;&#25918;&#36125;&#21494;&#26031;&#20248;&#21270;&#24211;&#30340;&#29305;&#28857;&#65292;&#35299;&#38145;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20854;&#20182;&#31867;&#22411;&#25968;&#25454;&#65288;&#20363;&#22914;&#65292;&#20559;&#22909;&#36755;&#20986;&#65289;&#30340;&#28508;&#22312;&#26032;&#24212;&#29992;&#12290;&#26412;&#25991;&#38754;&#21521;&#23545;&#36125;&#21494;&#26031;&#26041;&#27861;&#26377;&#19968;&#23450;&#20102;&#35299;&#20294;&#19981;&#19968;&#23450;&#20102;&#35299;&#22686;&#26448;&#21046;&#36896;&#30340;&#35835;&#32773;&#65292;&#36719;&#20214;&#24615;&#33021;&#27010;&#36848;&#21644;&#23454;&#29616;&#25351;&#21335;&#23545;&#20219;&#20309;&#23454;&#39564;&#35774;&#35745;&#21644;&#25968;&#25454;&#20998;&#26512;&#30340;&#30740;&#31350;&#37117;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is an approach to globally optimizing black-box objective functions that are expensive to evaluate. BO-powered experimental design has found wide application in materials science, chemistry, experimental physics, drug development, etc. This work aims to bring attention to the benefits of applying BO in designing experiments and to provide a BO manual, covering both methodology and software, for the convenience of anyone who wants to apply or learn BO. In particular, we briefly explain the BO technique, review all the applications of BO in additive manufacturing, compare and exemplify the features of different open BO libraries, unlock new potential applications of BO to other types of data (e.g., preferential output). This article is aimed at readers with some understanding of Bayesian methods, but not necessarily with knowledge of additive manufacturing; the software performance overview and implementation instructions are instrumental for any experimental-d
&lt;/p&gt;</description></item><item><title>Small-Text&#26159;&#19968;&#20010;Python&#20013;&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#20027;&#21160;&#23398;&#20064;&#24211;&#65292;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;&#20808;&#36827;&#30340;&#26597;&#35810;&#31574;&#30053;&#21644;&#33879;&#21517;&#30340;&#26426;&#22120;&#23398;&#20064;&#24211;&#65292;&#25903;&#25345;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#30740;&#31350;&#32773;&#20351;&#29992;&#35813;&#24211;&#35843;&#30740;&#20102;&#26368;&#26032;&#30340;SetFit&#35757;&#32451;&#33539;&#24335;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2107.10314</link><description>&lt;p&gt;
Small-Text: Python&#20013;&#30340;&#25991;&#26412;&#20998;&#31867;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Small-Text: Active Learning for Text Classification in Python. (arXiv:2107.10314v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.10314
&lt;/p&gt;
&lt;p&gt;
Small-Text&#26159;&#19968;&#20010;Python&#20013;&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#20027;&#21160;&#23398;&#20064;&#24211;&#65292;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;&#20808;&#36827;&#30340;&#26597;&#35810;&#31574;&#30053;&#21644;&#33879;&#21517;&#30340;&#26426;&#22120;&#23398;&#20064;&#24211;&#65292;&#25903;&#25345;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#30740;&#31350;&#32773;&#20351;&#29992;&#35813;&#24211;&#35843;&#30740;&#20102;&#26368;&#26032;&#30340;SetFit&#35757;&#32451;&#33539;&#24335;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#20027;&#21160;&#23398;&#20064;&#24211;small-text&#65292;&#25552;&#20379;Python&#20013;&#30340;&#27744;&#24335;&#20027;&#21160;&#23398;&#20064;&#65292;&#29992;&#20110;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#23427;&#25552;&#20379;&#20102;&#35768;&#22810;&#39044;&#20808;&#23454;&#29616;&#30340;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#31574;&#30053;&#65292;&#21253;&#25324;&#19968;&#20123;&#21033;&#29992;GPU&#30340;&#31574;&#30053;&#12290;&#26631;&#20934;&#21270;&#30340;&#25509;&#21475;&#20801;&#35768;&#32452;&#21512;&#21508;&#31181;&#20998;&#31867;&#22120;&#12289;&#26597;&#35810;&#31574;&#30053;&#21644;&#20572;&#27490;&#20934;&#21017;&#65292;&#20415;&#20110;&#24555;&#36895;&#28151;&#25645;&#65292;&#26041;&#20415;&#24555;&#36895;&#24320;&#21457;&#20027;&#21160;&#23398;&#20064;&#23454;&#39564;&#21644;&#24212;&#29992;&#12290;&#20026;&#20102;&#20351;&#21508;&#31181;&#20998;&#31867;&#22120;&#21644;&#26597;&#35810;&#31574;&#30053;&#23545;&#20027;&#21160;&#23398;&#20064;&#21487;&#35775;&#38382;&#65292;small-text&#38598;&#25104;&#20102;&#20960;&#20010;&#33879;&#21517;&#30340;&#26426;&#22120;&#23398;&#20064;&#24211;&#65292;&#21253;&#25324;scikit-learn&#12289;PyTorch&#21644;Hugging Face transformers&#12290;&#21518;&#20004;&#20010;&#38598;&#25104;&#26159;&#21487;&#36873;&#23433;&#35013;&#30340;&#25193;&#23637;&#65292;&#22240;&#27492;&#21487;&#20197;&#20351;&#29992;GPU&#65292;&#20294;&#19981;&#26159;&#24517;&#38656;&#30340;&#12290;&#20351;&#29992;&#36825;&#20010;&#26032;&#24211;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#36817;&#21457;&#24067;&#30340;SetFit&#35757;&#32451;&#33539;&#24335;&#30340;&#24615;&#33021;&#65292;&#23558;&#20854;&#19982;&#26222;&#36890;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce small-text, an easy-to-use active learning library, which offers pool-based active learning for single- and multi-label text classification in Python. It features numerous pre-implemented state-of-the-art query strategies, including some that leverage the GPU. Standardized interfaces allow the combination of a variety of classifiers, query strategies, and stopping criteria, facilitating a quick mix and match, and enabling a rapid and convenient development of both active learning experiments and applications. With the objective of making various classifiers and query strategies accessible for active learning, small-text integrates several well-known machine learning libraries, namely scikit-learn, PyTorch, and Hugging Face transformers. The latter integrations are optionally installable extensions, so GPUs can be used but are not required. Using this new library, we investigate the performance of the recently published SetFit training paradigm, which we compare to vanilla 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26410;&#30693;&#27969;&#24418;&#19978;&#35299;&#26925;&#22278;&#22411;PDE&#38382;&#39064;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#26144;&#23556;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26080;&#32593;&#26684;&#35745;&#31639;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;PDE&#27714;&#35299;&#36716;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#37319;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#26469;&#36817;&#20284;&#27714;&#35299;&#20195;&#25968;&#26041;&#31243;&#65292;&#24471;&#21040;&#20102;&#19968;&#33268;&#20272;&#35745;&#37327;&#65292;&#26368;&#32456;&#24471;&#21040;&#30340;&#25968;&#20540;&#26041;&#27861;&#22312;&#26497;&#38480;&#24773;&#20917;&#19979;&#26159;&#19968;&#33268;&#35299;&#12290;</title><link>http://arxiv.org/abs/2106.06682</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#26410;&#30693;&#27969;&#24418;&#19978;&#35299;PDE&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving PDEs on Unknown Manifolds with Machine Learning. (arXiv:2106.06682v3 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.06682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26410;&#30693;&#27969;&#24418;&#19978;&#35299;&#26925;&#22278;&#22411;PDE&#38382;&#39064;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#26144;&#23556;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26080;&#32593;&#26684;&#35745;&#31639;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;PDE&#27714;&#35299;&#36716;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#37319;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#26469;&#36817;&#20284;&#27714;&#35299;&#20195;&#25968;&#26041;&#31243;&#65292;&#24471;&#21040;&#20102;&#19968;&#33268;&#20272;&#35745;&#37327;&#65292;&#26368;&#32456;&#24471;&#21040;&#30340;&#25968;&#20540;&#26041;&#27861;&#22312;&#26497;&#38480;&#24773;&#20917;&#19979;&#26159;&#19968;&#33268;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#26144;&#23556;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#32593;&#26684;&#35745;&#31639;&#26694;&#26550;&#21644;&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#65292;&#29992;&#20110;&#22312;&#26410;&#30693;&#27969;&#24418;&#19978;&#35299;&#26925;&#22278;&#22411;PDE&#38382;&#39064;&#65292;&#20854;&#20013;&#27969;&#24418;&#29992;&#28857;&#20113;&#34920;&#31034;&#12290;PDE&#27714;&#35299;&#22120;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#20197;&#35299;&#20915;&#19968;&#20010;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#36890;&#36807;&#36817;&#20284;&#34920;&#31034;&#19968;&#20010;PDE&#65288;&#20197;&#21450;&#36793;&#30028;&#26465;&#20214;&#65292;&#22914;&#26524;&#36866;&#29992;&#65289;&#30340;&#20195;&#25968;&#26041;&#31243;&#12290;&#36825;&#20010;&#20195;&#25968;&#26041;&#31243;&#28041;&#21450;&#21040;&#36890;&#36807;&#25193;&#25955;&#26144;&#23556;&#28176;&#36827;&#23637;&#24320;&#24471;&#21040;&#30340;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#65292;&#23427;&#26159;&#19968;&#20010;&#20851;&#20110;&#20108;&#38454;&#26925;&#22278;&#24494;&#20998;&#31639;&#23376;&#30340;&#19968;&#33268;&#20272;&#35745;&#37327;&#12290;&#26368;&#32456;&#24471;&#21040;&#30340;&#25968;&#20540;&#26041;&#27861;&#26159;&#35299;&#20915;&#19968;&#20010;&#39640;&#24230;&#38750;&#20984;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#38480;&#21046;&#22312;&#31070;&#32463;&#32593;&#32476;&#20551;&#35774;&#31354;&#38388;&#30340;&#35299;&#31354;&#38388;&#20869;&#12290;&#22312;&#33391;&#23450;&#20041;&#30340;&#26925;&#22278;&#22411;PDE&#35774;&#32622;&#20013;&#65292;&#24403;&#20551;&#35774;&#31354;&#38388;&#21253;&#25324;&#26080;&#38480;&#23485;&#24230;&#25110;&#28145;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#32463;&#39564;&#25439;&#22833;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#26159;&#26497;&#38480;&#24773;&#20917;&#19979;&#30340;&#19968;&#33268;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a mesh-free computational framework and machine learning theory for solving elliptic PDEs on unknown manifolds, identified with point clouds, based on diffusion maps (DM) and deep learning. The PDE solver is formulated as a supervised learning task to solve a least-squares regression problem that imposes an algebraic equation approximating a PDE (and boundary conditions if applicable). This algebraic equation involves a graph-Laplacian type matrix obtained via DM asymptotic expansion, which is a consistent estimator of second-order elliptic differential operators. The resulting numerical method is to solve a highly non-convex empirical risk minimization problem subjected to a solution from a hypothesis space of neural networks (NNs). In a well-posed elliptic PDE setting, when the hypothesis space consists of neural networks with either infinite width or depth, we show that the global minimizer of the empirical loss function is a consistent solution in the limit of l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;k-Mixup&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#21487;&#20197;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#24182;&#22686;&#21152;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23545;k&#20010;&#25209;&#27425;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#25200;&#21160;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20445;&#25345;&#25968;&#25454;&#30340;&#32858;&#31867;&#21644;&#27969;&#24418;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#31181;&#32593;&#32476;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2106.02933</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#30340;k-Mixup&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
k-Mixup Regularization for Deep Learning via Optimal Transport. (arXiv:2106.02933v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;k-Mixup&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#21487;&#20197;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#24182;&#22686;&#21152;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23545;k&#20010;&#25209;&#27425;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#25200;&#21160;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20445;&#25345;&#25968;&#25454;&#30340;&#32858;&#31867;&#21644;&#27969;&#24418;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#31181;&#32593;&#32476;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#24182;&#22686;&#21152;&#23545;&#29305;&#23450;&#20998;&#24067;&#20559;&#31227;&#30340;&#40065;&#26834;&#24615;&#12290;&#23427;&#36890;&#36807;&#23558;&#36755;&#20837;&#30340;&#35757;&#32451;&#25968;&#25454;&#27839;&#30528;&#35757;&#32451;&#38598;&#20013;&#20854;&#20182;&#38543;&#26426;&#36873;&#25321;&#30340;&#23454;&#20363;&#30340;&#26041;&#21521;&#36827;&#34892;&#25200;&#21160;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;&#25968;&#25454;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#20197;&#19968;&#31181;&#31616;&#21333;&#19988;&#24191;&#27867;&#36866;&#29992;&#30340;&#26041;&#24335;&#23558;mixup&#25193;&#23637;&#20026;k-mixup&#65292;&#23427;&#23558;k&#20010;&#25209;&#27425;&#30340;&#35757;&#32451;&#25968;&#25454;&#22312;&#20854;&#20182;k&#20010;&#25209;&#27425;&#30340;&#26041;&#21521;&#19978;&#36827;&#34892;&#25200;&#21160;&#12290;&#25200;&#21160;&#26159;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#19979;&#30340;&#20301;&#31227;&#25554;&#20540;&#23454;&#29616;&#30340;&#65292;&#21363;&#22312;Wasserstein&#24230;&#37327;&#19979;&#30340;&#25554;&#20540;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#27169;&#25311;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;k-mixup&#21487;&#20197;&#20445;&#25345;&#32858;&#31867;&#21644;&#27969;&#24418;&#32467;&#26500;&#65292;&#36824;&#25193;&#23637;&#20102;&#23545;&#26631;&#20934;mixup&#22312;k-mixup&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;k-mixup&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20960;&#31181;&#32593;&#32476;&#26550;&#26500;&#21644;&#19981;&#21516;&#27169;&#24577;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup is a popular regularization technique for training deep neural networks that improves generalization and increases robustness to certain distribution shifts. It perturbs input training data in the direction of other randomly-chosen instances in the training set. To better leverage the structure of the data, we extend mixup in a simple, broadly applicable way to \emph{$k$-mixup}, which perturbs $k$-batches of training points in the direction of other $k$-batches. The perturbation is done with displacement interpolation, i.e. interpolation under the Wasserstein metric. We demonstrate theoretically and in simulations that $k$-mixup preserves cluster and manifold structures, and we extend theory studying the efficacy of standard mixup to the $k$-mixup case. Our empirical results show that training with $k$-mixup further improves generalization and robustness across several network architectures and benchmark datasets of differing modalities. For the wide variety of real datasets cons
&lt;/p&gt;</description></item><item><title>DS^3M&#26159;&#19968;&#31181;&#28145;&#24230;&#20999;&#25442;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#20855;&#26377;&#22797;&#26434;&#38750;&#32447;&#24615;&#20381;&#36182;&#21644;&#19981;&#35268;&#21017;&#20999;&#25442;&#26426;&#21046;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#20197;&#23454;&#29616;&#32463;&#27982;&#25928;&#30410;&#21644;&#26356;&#28145;&#20837;&#30340;&#29616;&#35937;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2106.02329</link><description>&lt;p&gt;
&#28145;&#24230;&#20999;&#25442;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;DS^3M&#65289;&#29992;&#20110;&#20855;&#26377;&#20999;&#25442;&#26426;&#21046;&#30340;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Switching State Space Model (DS$^3$M) for Nonlinear Time Series Forecasting with Regime Switching. (arXiv:2106.02329v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02329
&lt;/p&gt;
&lt;p&gt;
DS^3M&#26159;&#19968;&#31181;&#28145;&#24230;&#20999;&#25442;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#20855;&#26377;&#22797;&#26434;&#38750;&#32447;&#24615;&#20381;&#36182;&#21644;&#19981;&#35268;&#21017;&#20999;&#25442;&#26426;&#21046;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#20197;&#23454;&#29616;&#32463;&#27982;&#25928;&#30410;&#21644;&#26356;&#28145;&#20837;&#30340;&#29616;&#35937;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#32463;&#24120;&#23637;&#31034;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#19981;&#35268;&#21017;&#30340;&#20999;&#25442;&#26426;&#21046;&#34892;&#20026;&#12290;&#36825;&#20123;&#29305;&#24449;&#22312;&#24314;&#27169;&#12289;&#25512;&#29702;&#21644;&#23545;&#28508;&#22312;&#38543;&#26426;&#29616;&#35937;&#25552;&#20379;&#28145;&#20837;&#29702;&#35299;&#26041;&#38754;&#37117;&#23384;&#22312;&#25216;&#26415;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#21363;&#28145;&#24230;&#20999;&#25442;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;DS^3M&#65289;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#20934;&#30830;&#39044;&#27979;&#27492;&#31867;&#26102;&#38388;&#24207;&#21015;&#65292;&#21516;&#26102;&#29087;&#32451;&#22320;&#35782;&#21035;&#21160;&#24577;&#20013;&#38544;&#34255;&#30340;&#19981;&#35268;&#21017;&#20999;&#25442;&#26426;&#21046;&#12290;&#36825;&#31181;&#35782;&#21035;&#19981;&#20165;&#22312;&#32463;&#27982;&#19978;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#36824;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#28508;&#22312;&#29616;&#35937;&#12290;&#22312;DS^3M&#20013;&#65292;&#35813;&#26550;&#26500;&#20351;&#29992;&#31163;&#25955;&#28508;&#22312;&#21464;&#37327;&#26469;&#34920;&#31034;&#20999;&#25442;&#26426;&#21046;&#65292;&#20351;&#29992;&#36830;&#32493;&#28508;&#22312;&#21464;&#37327;&#26469;&#32771;&#34385;&#38543;&#26426;&#39537;&#21160;&#22240;&#32032;&#12290;&#36890;&#36807;&#23558;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#19982;&#38750;&#32447;&#24615;&#20999;&#25442;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSSM&#65289;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#25104;&#21151;&#25429;&#25417;&#20102;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#21644;&#19981;&#35268;&#21017;&#20999;&#25442;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern time series data often display complex nonlinear dependencies along with irregular regime-switching behaviors. These features present technical challenges in modeling, inference, and in offering insightful understanding into the underlying stochastic phenomena. To tackle these challenges, we introduce a novel modeling framework known as the Deep Switching State Space Model (DS$^3$M). This framework is engineered to make accurate forecasts for such time series while adeptly identifying the irregular regimes hidden within the dynamics. These identifications not only have significant economic ramifications but also contribute to a deeper understanding of the underlying phenomena. In DS$^3$M, the architecture employs discrete latent variables to represent regimes and continuous latent variables to account for random driving factors. By melding a Recurrent Neural Network (RNN) with a nonlinear Switching State Space Model (SSSM), we manage to capture the nonlinear dependencies and irr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#31639;&#27861;&#65292;&#36825;&#31181;&#31639;&#27861;&#33021;&#22815;&#22788;&#29702;&#26469;&#33258;&#23457;&#26597;&#21644;&#30456;&#20851;&#25968;&#25454;&#30340;&#35266;&#27979;&#12290;&#31639;&#27861;&#24314;&#31435;&#22312;&#22312;&#32447;&#29275;&#39039;&#27493;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#20351;&#29992;&#20999;&#25442;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2104.05087</link><description>&lt;p&gt;
&#23398;&#20064;&#26469;&#33258;&#34987;&#23457;&#26597;&#21644;&#30456;&#20851;&#25968;&#25454;&#65306;&#32447;&#24615;&#21160;&#24577;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from Censored and Dependent Data: The case of Linear Dynamics. (arXiv:2104.05087v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.05087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#31639;&#27861;&#65292;&#36825;&#31181;&#31639;&#27861;&#33021;&#22815;&#22788;&#29702;&#26469;&#33258;&#23457;&#26597;&#21644;&#30456;&#20851;&#25968;&#25454;&#30340;&#35266;&#27979;&#12290;&#31639;&#27861;&#24314;&#31435;&#22312;&#22312;&#32447;&#29275;&#39039;&#27493;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#20351;&#29992;&#20999;&#25442;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26469;&#33258;&#21160;&#24577;&#31995;&#32479;&#30340;&#35266;&#27979;&#32467;&#26524;&#32463;&#24120;&#20986;&#29616;&#19981;&#35268;&#21017;&#24615;&#65292;&#20363;&#22914;&#65292;&#23457;&#26597;&#65292;&#21482;&#26377;&#22312;&#20540;&#33853;&#22312;&#26576;&#20010;&#33539;&#22260;&#20869;&#26102;&#25165;&#35760;&#24405;&#12290;&#23457;&#26597;&#22312;&#23454;&#36341;&#20013;&#24456;&#26222;&#36941;&#65292;&#30001;&#20110;&#39281;&#21644;&#20256;&#24863;&#22120;&#12289;&#26816;&#27979;&#38480;&#21046;&#21644;&#22270;&#20687;&#24103;&#25928;&#24212;&#12290;&#37492;&#20110;&#26368;&#36817;&#20851;&#20110;&#23398;&#20064;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65288;LDS&#65289;&#21644;&#29420;&#31435;&#25968;&#25454;&#30340;&#34987;&#23457;&#26597;&#32479;&#35745;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#37325;&#26032;&#35752;&#35770;&#20102;&#20960;&#21313;&#24180;&#26469;&#20851;&#20110;&#20174;&#34987;&#23457;&#26597;&#35266;&#27979;&#20013;&#23398;&#20064;LDS&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#23398;&#20064;&#32773;&#21482;&#26377;&#22312;&#29366;&#24577;$x_t \in \mathbb{R}^d$&#23646;&#20110;&#26576;&#20010;&#38598;&#21512;$S_t \subseteq \mathbb{R}^d$&#26102;&#35266;&#23519;&#21040;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#22312;&#21482;&#26377;&#23545;$S_t$&#30340;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#31995;&#32479;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#24102;&#26377;&#20999;&#25442;&#26799;&#24230;&#30340;&#38543;&#26426;&#22312;&#32447;&#29275;&#39039;&#26041;&#27861;&#65292;&#26159;&#19968;&#20010;&#24314;&#31435;&#22312;Hazan&#31561;&#20154;&#30340;&#22312;&#32447;&#29275;&#39039;&#27493;&#65288;ONS&#65289;&#22522;&#30784;&#19978;&#30340;&#26032;&#39062;&#30340;&#20108;&#38454;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Observations from dynamical systems often exhibit irregularities, such as censoring, where values are recorded only if they fall within a certain range. Censoring is ubiquitous in practice, due to saturating sensors, limit-of-detection effects, and image-frame effects. In light of recent developments on learning linear dynamical systems (LDSs), and on censored statistics with independent data, we revisit the decades-old problem of learning an LDS, from censored observations (Lee and Maddala (1985); Zeger and Brookmeyer (1986)). Here, the learner observes the state $x_t \in \mathbb{R}^d$ if and only if $x_t$ belongs to some set $S_t \subseteq \mathbb{R}^d$. We develop the first computationally and statistically efficient algorithm for learning the system, assuming only oracle access to the sets $S_t$. Our algorithm, Stochastic Online Newton with Switching Gradients, is a novel second-order method that builds on the Online Newton Step (ONS) of Hazan et al. (2007). Our Switching-Gradient 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#26410;&#26631;&#35760;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;UPCA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#25968;&#20960;&#20309;&#35777;&#26126;&#20102;&#20854;&#26159;&#19968;&#20010;&#33391;&#23450;&#20041;&#30340;&#20195;&#25968;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#31639;&#27861;&#27969;&#31243;&#26469;&#24212;&#23545;&#34987;&#32622;&#25442;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#26080;&#26631;&#35760;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2101.09446</link><description>&lt;p&gt;
&#26410;&#26631;&#35760;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#30697;&#38453;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Unlabeled Principal Component Analysis and Matrix Completion. (arXiv:2101.09446v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.09446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#26410;&#26631;&#35760;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;UPCA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#25968;&#20960;&#20309;&#35777;&#26126;&#20102;&#20854;&#26159;&#19968;&#20010;&#33391;&#23450;&#20041;&#30340;&#20195;&#25968;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#31639;&#27861;&#27969;&#31243;&#26469;&#24212;&#23545;&#34987;&#32622;&#25442;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#26080;&#26631;&#35760;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#26410;&#26631;&#35760;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;UPCA&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#34987;&#32622;&#25442;&#25439;&#22351;&#30340;&#25968;&#25454;&#30697;&#38453;&#20013;&#25552;&#21462;&#40065;&#26834;&#30340;&#20027;&#25104;&#20998;&#12290;&#21033;&#29992;&#20195;&#25968;&#20960;&#20309;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;UPCA&#26159;&#19968;&#20010;&#33391;&#23450;&#20041;&#30340;&#20195;&#25968;&#38382;&#39064;&#65292;&#21363;&#19982;&#32473;&#23450;&#25968;&#25454;&#30456;&#19968;&#33268;&#30340;&#26368;&#23567;&#31209;&#30697;&#38453;&#21482;&#26377;&#20316;&#20026;&#22810;&#39033;&#24335;&#26041;&#31243;&#32452;&#30340;&#21807;&#19968;&#35299;&#30340;&#23454;&#38469;&#30697;&#38453;&#30340;&#34892;&#32622;&#25442;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#23454;&#38469;&#30456;&#20851;&#24773;&#20917;&#30340;&#39640;&#25928;&#20004;&#38454;&#27573;&#31639;&#27861;&#27969;&#31243;&#29992;&#20110;UPCA&#65292;&#20854;&#20013;&#21482;&#26377;&#19968;&#37096;&#20998;&#25968;&#25454;&#34987;&#32622;&#25442;&#12290;&#38454;&#27573;I&#21033;&#29992;&#40065;&#26834;&#30340;&#24322;&#24120;&#20540;&#20027;&#25104;&#20998;&#20998;&#26512;&#26041;&#27861;&#26469;&#20272;&#35745;&#23454;&#38469;&#30697;&#38453;&#30340;&#21015;&#31354;&#38388;&#12290;&#22312;&#20855;&#22791;&#21015;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#38454;&#27573;II&#24212;&#29992;&#26368;&#36817;&#30340;&#26080;&#26631;&#35760;&#24863;&#30693;&#26041;&#27861;&#26469;&#24674;&#22797;&#34987;&#32622;&#25442;&#30340;&#25968;&#25454;&#12290;&#22312;UPCA&#30340;&#22522;&#30784;&#19978;&#20801;&#35768;&#20986;&#29616;&#32570;&#22833;&#26465;&#30446;&#21644;&#32622;&#25442;&#23548;&#33268;&#20102;&#26080;&#26631;&#35760;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#27492;&#25512;&#23548;&#20102;&#29702;&#35770;&#21644;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce robust principal component analysis from a data matrix in which the entries of its columns have been corrupted by permutations, termed Unlabeled Principal Component Analysis (UPCA). Using algebraic geometry, we establish that UPCA is a well-defined algebraic problem in the sense that the only matrices of minimal rank that agree with the given data are row-permutations of the ground-truth matrix, arising as the unique solutions of a polynomial system of equations. Further, we propose an efficient two-stage algorithmic pipeline for UPCA suitable for the practically relevant case where only a fraction of the data have been permuted. Stage-I employs outlier-robust PCA methods to estimate the ground-truth column-space. Equipped with the column-space, Stage-II applies recent methods for unlabeled sensing to restore the permuted data. Allowing for missing entries on top of permutations in UPCA leads to the problem of unlabeled matrix completion, for which we derive theory and alg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#20915;&#31574;&#25361;&#25112;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#36890;&#36807;&#24212;&#29992;&#22810;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25511;&#21046;&#20248;&#21270;&#38382;&#39064;&#65292;&#20026;&#33258;&#20027;&#23398;&#20064;&#21644;&#33258;&#25105;&#25913;&#36827;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2008.01302</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39640;&#36895;&#20844;&#36335;&#20915;&#31574;&#20013;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Analysis of Deep Reinforcement Learning-enabled Freeway Decision-making for Automated Vehicles. (arXiv:2008.01302v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.01302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#20915;&#31574;&#25361;&#25112;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#36890;&#36807;&#24212;&#29992;&#22810;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25511;&#21046;&#20248;&#21270;&#38382;&#39064;&#65292;&#20026;&#33258;&#20027;&#23398;&#20064;&#21644;&#33258;&#25105;&#25913;&#36827;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#25361;&#25112;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#30001;&#20110;&#20854;&#22312;&#33258;&#20027;&#23398;&#20064;&#21644;&#33258;&#25105;&#25913;&#36827;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;DRL&#22312;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#37117;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#38754;&#20020;&#30340;&#20915;&#31574;&#25361;&#25112;&#36827;&#34892;&#20102;&#20960;&#31181;DRL&#26041;&#27861;&#30340;&#32508;&#21512;&#27604;&#36739;&#12290;&#36825;&#20123;&#25216;&#26415;&#21253;&#25324;&#24120;&#35265;&#30340;&#28145;&#24230;Q&#23398;&#20064;(DQL)&#12289;&#21452;&#28145;&#24230;Q&#23398;&#20064;(DDQL)&#12289;&#23545;&#20915;&#28145;&#24230;Q&#23398;&#20064;&#21644;&#20248;&#20808;&#37325;&#25918;&#28145;&#24230;Q&#23398;&#20064;&#12290;&#39318;&#20808;&#20171;&#32461;&#20102;&#24378;&#21270;&#23398;&#20064;(RL)&#26694;&#26550;&#65292;&#28982;&#21518;&#23545;&#19978;&#36848;DRL&#26041;&#27861;&#30340;&#23454;&#29616;&#36827;&#34892;&#20102;&#25968;&#23398;&#24314;&#27169;&#12290;&#38543;&#21518;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39640;&#36895;&#20844;&#36335;&#39550;&#39542;&#22330;&#26223;&#65292;&#23558;&#20915;&#31574;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#25511;&#21046;&#20248;&#21270;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#27169;&#25311;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) has emerged as a pervasive and potent methodology for addressing artificial intelligence challenges. Due to its substantial potential for autonomous self-learning and self-improvement, DRL finds broad applications across various research domains. This article undertakes a comprehensive comparison of several DRL approaches con-cerning the decision-making challenges encountered by autono-mous vehicles on freeways. These techniques encompass common deep Q-learning (DQL), double deep Q-learning (DDQL), dueling deep Q-learning, and prioritized replay deep Q-learning. Initially, the reinforcement learning (RL) framework is introduced, fol-lowed by a mathematical establishment of the implementations of the aforementioned DRL methods. Subsequently, a freeway driving scenario for automated vehicles is constructed, wherein the decision-making problem is reformulated as a control opti-mization challenge. Finally, a series of simulation experiments are conducted t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#30697;&#38453;&#25200;&#21160;&#29702;&#35770;&#23545;&#20005;&#26684;&#38797;&#28857;&#37051;&#22495;&#21608;&#22260;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#36827;&#34892;&#20005;&#26684;&#30340;&#20960;&#20309;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#38190;&#32467;&#26524;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#20219;&#24847;&#32473;&#23450;&#21021;&#22987;&#26465;&#20214;&#19979;&#30340;&#36817;&#20284;&#26799;&#24230;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2006.01106</link><description>&lt;p&gt;
&#36817;&#20284;&#26799;&#24230;&#19979;&#38477;&#36712;&#36857;&#22312;&#38797;&#28857;&#21608;&#22260;&#30340;&#36864;&#20986;&#26102;&#38388;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exit Time Analysis for Approximations of Gradient Descent Trajectories Around Saddle Points. (arXiv:2006.01106v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.01106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#30697;&#38453;&#25200;&#21160;&#29702;&#35770;&#23545;&#20005;&#26684;&#38797;&#28857;&#37051;&#22495;&#21608;&#22260;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#36827;&#34892;&#20005;&#26684;&#30340;&#20960;&#20309;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#38190;&#32467;&#26524;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#20219;&#24847;&#32473;&#23450;&#21021;&#22987;&#26465;&#20214;&#19979;&#30340;&#36817;&#20284;&#26799;&#24230;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#19968;&#20123;&#21021;&#22987;&#36793;&#30028;&#26465;&#20214;&#19979;&#65292;&#26799;&#24230;&#30456;&#20851;&#30340;&#19968;&#38454;&#26041;&#27861;&#20174;&#38797;&#28857;&#37051;&#22495;&#20013;&#30340;&#36712;&#36857;&#30340;&#36864;&#20986;&#26102;&#38388;&#38382;&#39064;&#12290;&#30001;&#20110;&#38797;&#28857;&#21608;&#22260;&#30340;&#20960;&#20309;&#32467;&#26500;&#8220;&#24179;&#22374;&#8221;&#65292;&#19968;&#38454;&#26041;&#27861;&#22312;&#36935;&#21040;&#26799;&#24230;&#20540;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#24555;&#36895;&#31163;&#24320;&#36825;&#20123;&#21306;&#22495;&#12290;&#29305;&#21035;&#22320;&#65292;&#34429;&#28982;&#24050;&#30693;&#26799;&#24230;&#30456;&#20851;&#30340;&#19968;&#38454;&#26041;&#27861;&#20250;&#31163;&#24320;&#20005;&#26684;&#30340;&#38797;&#28857;&#37051;&#22495;&#65292;&#20294;&#29616;&#26377;&#30340;&#20998;&#26512;&#25216;&#26415;&#24182;&#26410;&#26126;&#30830;&#21033;&#29992;&#38797;&#28857;&#21608;&#22260;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#20197;&#25511;&#21046;&#26799;&#24230;&#36712;&#36857;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#20351;&#29992;&#30697;&#38453;&#25200;&#21160;&#29702;&#35770;&#23545;&#20005;&#26684;&#38797;&#28857;&#37051;&#22495;&#21608;&#22260;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#20960;&#20309;&#20998;&#26512;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#38190;&#32467;&#26524;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#20219;&#24847;&#32473;&#23450;&#21021;&#22987;&#26465;&#20214;&#19979;&#30340;&#36817;&#20284;&#26799;&#24230;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#35813;&#20998;&#26512;&#36824;&#23548;&#33268;&#20102;&#19968;&#20010;&#32447;&#24615;&#36864;&#20986;&#26102;&#38388;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the problem of understanding the exit time for trajectories of gradient-related first-order methods from saddle neighborhoods under some initial boundary conditions. Given the 'flat' geometry around saddle points, first-order methods can struggle to escape these regions in a fast manner due to the small magnitudes of gradients encountered. In particular, while it is known that gradient-related first-order methods escape strict-saddle neighborhoods, existing analytic techniques do not explicitly leverage the local geometry around saddle points in order to control behavior of gradient trajectories. It is in this context that this paper puts forth a rigorous geometric analysis of the gradient-descent method around strict-saddle neighborhoods using matrix perturbation theory. In doing so, it provides a key result that can be used to generate an approximate gradient trajectory for any given initial conditions. In addition, the analysis leads to a linear exit-time soluti
&lt;/p&gt;</description></item><item><title>CrossQ&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#31639;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#36816;&#29992;&#25209;&#24402;&#19968;&#21270;&#21644;&#21024;&#38500;&#30446;&#26631;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#23454;&#26045;&#31616;&#21333;&#12290;</title><link>http://arxiv.org/abs/1902.05605</link><description>&lt;p&gt;
CrossQ: &#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26679;&#26412;&#25928;&#29575;&#21644;&#31616;&#27905;&#24615;&#30340;&#25209;&#24402;&#19968;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity. (arXiv:1902.05605v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1902.05605
&lt;/p&gt;
&lt;p&gt;
CrossQ&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#31639;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#36816;&#29992;&#25209;&#24402;&#19968;&#21270;&#21644;&#21024;&#38500;&#30446;&#26631;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#23454;&#26045;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26679;&#26412;&#25928;&#29575;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#31639;&#27861;&#65292;&#22914;REDQ&#21644;DroQ&#65292;&#36890;&#36807;&#23558;&#25209;&#27425;&#26631;&#20934;&#21270;&#30340;&#26356;&#26032;&#25968;&#25454;&#65288;UTD&#65289;&#27604;&#29575;&#22686;&#21152;&#21040;&#27599;&#20010;&#29615;&#22659;&#26679;&#26412;&#19978;&#30340;20&#20010;&#26799;&#24230;&#26356;&#26032;&#27493;&#39588;&#65292;&#25913;&#21892;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#20570;&#20250;&#24102;&#26469;&#22823;&#24133;&#22686;&#21152;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#31181;&#35745;&#31639;&#36127;&#25285;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CrossQ&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#31639;&#27861;&#65292;&#23427;&#24039;&#22937;&#22320;&#36816;&#29992;&#25209;&#24402;&#19968;&#21270;&#65292;&#24182;&#21435;&#38500;&#20102;&#30446;&#26631;&#32593;&#32476;&#65292;&#20197;&#22312;&#20445;&#25345;&#20302;UTD&#27604;&#29575;&#20026;1&#30340;&#21516;&#26102;&#36229;&#36234;&#30446;&#21069;&#30340;&#26368;&#26032;&#26679;&#26412;&#25928;&#29575;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CrossQ&#19981;&#20381;&#36182;&#20110;&#24403;&#21069;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#39640;&#32423;&#20559;&#24046;&#32553;&#20943;&#26041;&#26696;&#12290;CrossQ&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#26368;&#20808;&#36827;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#65288;2&#65289;&#19982;REDQ&#21644;DroQ&#30456;&#27604;&#22823;&#24133;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#65288;3&#65289;&#23454;&#26045;&#31616;&#21333;&#65292;&#20165;&#38656;&#35201;&#22312;SAC&#20043;&#19978;&#28155;&#21152;&#20960;&#34892;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sample efficiency is a crucial problem in deep reinforcement learning. Recent algorithms, such as REDQ and DroQ, found a way to improve the sample efficiency by increasing the update-to-data (UTD) ratio to 20 gradient update steps on the critic per environment sample. However, this comes at the expense of a greatly increased computational cost. To reduce this computational burden, we introduce Cross$Q$: a lightweight algorithm that makes careful use of Batch Normalization and removes target networks to surpass the state-of-the-art in sample efficiency while maintaining a low UTD ratio of $1$. Notably, Cross$Q$ does not rely on advanced bias-reduction schemes used in current methods. Cross$Q$'s contributions are thus threefold: (1) state-of-the-art sample efficiency, (2) substantial reduction in computational cost compared to REDQ and DroQ, and (3) ease of implementation, requiring just a few lines of code on top of SAC.
&lt;/p&gt;</description></item></channel></rss>