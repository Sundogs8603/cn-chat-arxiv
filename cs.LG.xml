<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#35774;&#35745;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#22411;&#21435;&#22122;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#20855;&#26377;&#26356;&#39640;&#30340;&#21442;&#25968;&#25928;&#29575;&#12289;&#24555;&#36895;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#22122;&#22768;&#31283;&#20581;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#26631;&#20934;U-Net&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#26174;&#33879;&#20943;&#23569;&#65292;&#24182;&#19988;&#22312;&#25512;&#29702;&#36895;&#24230;&#21644;&#36136;&#37327;&#35299;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.20092</link><description>&lt;p&gt;
&#36229;&#36234;U&#65306;&#20351;&#25193;&#25955;&#27169;&#22411;&#26356;&#24555;&#26356;&#36731;
&lt;/p&gt;
&lt;p&gt;
Beyond U: Making Diffusion Models Faster &amp; Lighter. (arXiv:2310.20092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#35774;&#35745;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#22411;&#21435;&#22122;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#20855;&#26377;&#26356;&#39640;&#30340;&#21442;&#25968;&#25928;&#29575;&#12289;&#24555;&#36895;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#22122;&#22768;&#31283;&#20581;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#26631;&#20934;U-Net&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#26174;&#33879;&#20943;&#23569;&#65292;&#24182;&#19988;&#22312;&#25512;&#29702;&#36895;&#24230;&#21644;&#36136;&#37327;&#35299;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#22270;&#20687;&#21512;&#25104;&#12289;&#35270;&#39057;&#29983;&#25104;&#21644;&#20998;&#23376;&#35774;&#35745;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21019;&#32426;&#24405;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#20855;&#22791;&#36825;&#20123;&#33021;&#21147;&#65292;&#20294;&#20854;&#25928;&#29575;&#65292;&#29305;&#21035;&#26159;&#22312;&#36870;&#21521;&#21435;&#22122;&#36807;&#31243;&#20013;&#65292;&#20173;&#28982;&#38754;&#20020;&#30528;&#24930;&#25910;&#25947;&#36895;&#24230;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#26469;&#35774;&#35745;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#22411;&#21435;&#22122;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26356;&#20855;&#21442;&#25968;&#25928;&#29575;&#65292;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#22122;&#22768;&#31283;&#20581;&#24615;&#12290;&#36890;&#36807;&#23545;&#21435;&#22122;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#30340;&#21442;&#25968;&#32422;&#20026;&#26631;&#20934;&#21435;&#22122;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65288;DDPM&#65289;&#20013;&#26631;&#20934;U-Net&#30340;&#22235;&#20998;&#20043;&#19968;&#65292;&#28014;&#28857;&#36816;&#31639;&#65288;FLOPs&#65289;&#32422;&#20026;&#26631;&#20934;U-Net&#30340;30%&#12290;&#27492;&#22806;&#65292;&#22312;&#30456;&#31561;&#26465;&#20214;&#19979;&#27979;&#37327;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#27604;&#22522;&#20934;&#27169;&#22411;&#24555;70&#65285;&#65292;&#21516;&#26102;&#25910;&#25947;&#21040;&#26356;&#22909;&#30340;&#36136;&#37327;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a family of generative models that yield record-breaking performance in tasks such as image synthesis, video generation, and molecule design. Despite their capabilities, their efficiency, especially in the reverse denoising process, remains a challenge due to slow convergence rates and high computational costs. In this work, we introduce an approach that leverages continuous dynamical systems to design a novel denoising network for diffusion models that is more parameter-efficient, exhibits faster convergence, and demonstrates increased noise robustness. Experimenting with denoising probabilistic diffusion models, our framework operates with approximately a quarter of the parameters and 30% of the Floating Point Operations (FLOPs) compared to standard U-Nets in Denoising Diffusion Probabilistic Models (DDPMs). Furthermore, our model is up to 70% faster in inference than the baseline models when measured in equal conditions while converging to better quality solutio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28431;&#27934;&#26816;&#27979;&#30340;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;CausalVul&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24615;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#25200;&#21160;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#26816;&#27979;&#20013;&#27169;&#22411;&#19981;&#31283;&#23450;&#21644;&#27867;&#21270;&#24615;&#33021;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07958</link><description>&lt;p&gt;
&#36808;&#21521;&#38024;&#23545;&#28431;&#27934;&#26816;&#27979;&#30340;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Causal Deep Learning for Vulnerability Detection. (arXiv:2310.07958v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28431;&#27934;&#26816;&#27979;&#30340;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;CausalVul&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24615;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#25200;&#21160;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#26816;&#27979;&#20013;&#27169;&#22411;&#19981;&#31283;&#23450;&#21644;&#27867;&#21270;&#24615;&#33021;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#28431;&#27934;&#26816;&#27979;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#38459;&#30861;&#20854;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#26377;&#29992;&#30340;&#37325;&#35201;&#25361;&#25112;&#26159;&#27169;&#22411;&#22312;&#25200;&#21160;&#19979;&#19981;&#31283;&#23450;&#65292;&#24182;&#19988;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#25968;&#25454;&#65292;&#20363;&#22914;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#23558;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#24212;&#29992;&#21040;&#26410;&#35265;&#36807;&#30340;&#39033;&#30446;&#19978;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#26159;&#22240;&#20026;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#38750;&#31283;&#23450;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#21464;&#37327;&#21517;&#65292;&#19982;&#26631;&#31614;&#20855;&#26377;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#24403;&#25200;&#21160;&#21644;OOD&#25968;&#25454;&#38598;&#19981;&#20877;&#20855;&#26377;&#30456;&#21516;&#30340;&#34394;&#20551;&#29305;&#24449;&#26102;&#65292;&#27169;&#22411;&#39044;&#27979;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22240;&#26524;&#24615;&#24341;&#20837;&#20102;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#26816;&#27979;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;CausalVul&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#30340;&#25200;&#21160;&#26469;&#21457;&#29616;&#27169;&#22411;&#21487;&#33021;&#29992;&#20110;&#36827;&#34892;&#39044;&#27979;&#30340;&#34394;&#20551;&#29305;&#24449;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#19978;&#24212;&#29992;&#20102;&#22240;&#26524;&#23398;&#20064;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;do-&#35745;&#31639;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning vulnerability detection has shown promising results in recent years. However, an important challenge that still blocks it from being very useful in practice is that the model is not robust under perturbation and it cannot generalize well over the out-of-distribution (OOD) data, e.g., applying a trained model to unseen projects in real world. We hypothesize that this is because the model learned non-robust features, e.g., variable names, that have spurious correlations with labels. When the perturbed and OOD datasets no longer have the same spurious features, the model prediction fails. To address the challenge, in this paper, we introduced causality into deep learning vulnerability detection. Our approach CausalVul consists of two phases. First, we designed novel perturbations to discover spurious features that the model may use to make predictions. Second, we applied the causal learning algorithms, specifically, do-calculus, on top of existing deep learning models to sys
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#26222;&#36890;&#35805;&#35328;&#35821;&#30196;&#21574;&#35780;&#20272;&#31995;&#32479;&#12290;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#24182;&#25552;&#21462;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#21644;&#20020;&#24202;&#30196;&#21574;&#35780;&#20998;&#39044;&#27979;&#26041;&#38754;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.03985</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35821;&#38899;&#35782;&#21035;&#32534;&#30721;&#22120;&#36827;&#34892;&#26222;&#36890;&#35805;&#35328;&#35821;&#30340;&#30196;&#21574;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dementia Assessment Using Mandarin Speech with an Attention-based Speech Recognition Encoder. (arXiv:2310.03985v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#26222;&#36890;&#35805;&#35328;&#35821;&#30196;&#21574;&#35780;&#20272;&#31995;&#32479;&#12290;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#24182;&#25552;&#21462;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#21644;&#20020;&#24202;&#30196;&#21574;&#35780;&#20998;&#39044;&#27979;&#26041;&#38754;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30196;&#21574;&#35786;&#26029;&#38656;&#35201;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#36825;&#26159;&#22797;&#26434;&#19988;&#32791;&#26102;&#30340;&#12290;&#30196;&#21574;&#30340;&#26089;&#26399;&#26816;&#27979;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#38450;&#27490;&#30149;&#24773;&#36827;&#19968;&#27493;&#24694;&#21270;&#12290;&#26412;&#25991;&#21033;&#29992;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#20010;&#38024;&#23545;&#26222;&#36890;&#35805;&#20351;&#29992;&#32773;&#22312;&#22270;&#29255;&#25551;&#36848;&#20219;&#21153;&#20013;&#30340;&#30196;&#21574;&#35780;&#20272;&#31995;&#32479;&#12290;&#36890;&#36807;&#22312;&#19982;&#30495;&#23454;&#19990;&#30028;&#24773;&#22659;&#38750;&#24120;&#30456;&#20284;&#30340;&#35821;&#38899;&#25968;&#25454;&#19978;&#35757;&#32451;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#25105;&#20204;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20174;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#25552;&#21462;&#32534;&#30721;&#22120;&#65292;&#24182;&#28155;&#21152;&#20102;&#19968;&#20010;&#32447;&#24615;&#23618;&#29992;&#20110;&#30196;&#21574;&#35780;&#20272;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;99&#21517;&#34987;&#35797;&#30340;&#26222;&#36890;&#35805;&#35821;&#38899;&#25968;&#25454;&#65292;&#24182;&#20174;&#24403;&#22320;&#21307;&#38498;&#33719;&#21462;&#20102;&#20182;&#20204;&#30340;&#20020;&#24202;&#35780;&#20272;&#25968;&#25454;&#12290;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;92.04%&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#20020;&#24202;&#30196;&#21574;&#35780;&#20998;&#39044;&#27979;&#20013;&#36798;&#21040;&#20102;9%&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dementia diagnosis requires a series of different testing methods, which is complex and time-consuming. Early detection of dementia is crucial as it can prevent further deterioration of the condition. This paper utilizes a speech recognition model to construct a dementia assessment system tailored for Mandarin speakers during the picture description task. By training an attention-based speech recognition model on voice data closely resembling real-world scenarios, we have significantly enhanced the model's recognition capabilities. Subsequently, we extracted the encoder from the speech recognition model and added a linear layer for dementia assessment. We collected Mandarin speech data from 99 subjects and acquired their clinical assessments from a local hospital. We achieved an accuracy of 92.04% in Alzheimer's disease detection and a mean absolute error of 9% in clinical dementia rating score prediction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25968;&#25454;&#20381;&#36182;&#32806;&#21512;&#26469;&#26500;&#24314;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#36229;&#20998;&#36776;&#29575;&#21644;&#20462;&#22797;&#20219;&#21153;&#20013;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03725</link><description>&lt;p&gt;
&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#32806;&#21512;&#30340;&#38543;&#26426;&#25554;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic interpolants with data-dependent couplings. (arXiv:2310.03725v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25968;&#25454;&#20381;&#36182;&#32806;&#21512;&#26469;&#26500;&#24314;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#36229;&#20998;&#36776;&#29575;&#21644;&#20462;&#22797;&#20219;&#21153;&#20013;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21160;&#24577;&#27979;&#24230;&#20256;&#36755;&#21551;&#21457;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#27969;&#21644;&#25193;&#25955;&#65289;&#26500;&#24314;&#20102;&#20004;&#20010;&#27010;&#29575;&#23494;&#24230;&#20043;&#38388;&#30340;&#36830;&#32493;&#26102;&#38388;&#26144;&#23556;&#12290;&#25353;&#29031;&#20256;&#32479;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#26159;&#30446;&#26631;&#23494;&#24230;&#65292;&#21482;&#33021;&#36890;&#36807;&#26679;&#26412;&#35775;&#38382;&#65292;&#32780;&#21478;&#19968;&#20010;&#26159;&#31616;&#21333;&#30340;&#22522;&#30784;&#23494;&#24230;&#65292;&#19982;&#25968;&#25454;&#26080;&#20851;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#25554;&#20540;&#30340;&#26694;&#26550;&#65292;&#35268;&#33539;&#21270;&#20102;&#22914;&#20309;&#8220;&#32806;&#21512;&#8221;&#22522;&#26412;&#23494;&#24230;&#21644;&#30446;&#26631;&#23494;&#24230;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#31867;&#21035;&#26631;&#31614;&#25110;&#36830;&#32493;&#23884;&#20837;&#30340;&#20449;&#24687;&#32435;&#20837;&#21040;&#26500;&#24314;&#21160;&#24577;&#20256;&#36755;&#26144;&#23556;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#35299;&#20915;&#31867;&#20284;&#20110;&#26631;&#20934;&#29420;&#31435;&#35774;&#32622;&#30340;&#31616;&#21333;&#24179;&#26041;&#25439;&#22833;&#22238;&#24402;&#38382;&#39064;&#26469;&#23398;&#20064;&#36825;&#20123;&#20256;&#36755;&#26144;&#23556;&#12290;&#36890;&#36807;&#36229;&#20998;&#36776;&#29575;&#21644;&#20462;&#22797;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26500;&#24314;&#20381;&#36182;&#32806;&#21512;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models inspired by dynamical transport of measure -- such as flows and diffusions -- construct a continuous-time map between two probability densities. Conventionally, one of these is the target density, only accessible through samples, while the other is taken as a simple base density that is data-agnostic. In this work, using the framework of stochastic interpolants, we formalize how to \textit{couple} the base and the target densities. This enables us to incorporate information about class labels or continuous embeddings to construct dynamical transport maps that serve as conditional generative models. We show that these transport maps can be learned by solving a simple square loss regression problem analogous to the standard independent setting. We demonstrate the usefulness of constructing dependent couplings in practice through experiments in super-resolution and in-painting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#20559;&#22909;&#35757;&#32451;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.03708</link><description>&lt;p&gt;
&#36229;&#36234;&#19968;&#35270;&#21516;&#20161;&#65306;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization. (arXiv:2310.03708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#20559;&#22909;&#35757;&#32451;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#19982;&#26222;&#36890;&#26631;&#35760;&#32773;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#21487;&#33021;&#19981;&#36866;&#24212;&#21508;&#31181;&#21508;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#26041;&#27861;&#36873;&#25321;&#36890;&#36807;&#25910;&#38598;&#22810;&#32500;&#24230;&#21453;&#39304;&#24182;&#20026;&#27599;&#20010;&#32500;&#24230;&#21019;&#24314;&#19981;&#21516;&#30340;&#22870;&#21169;&#65288;&#20363;&#22914;&#65292;&#26377;&#30410;&#24615;&#65292;&#26080;&#23475;&#24615;&#65292;&#35802;&#23454;&#24615;&#65289;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#22870;&#21169;&#26435;&#37325;&#65292;&#21487;&#20197;&#36890;&#36807;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#23558;LM&#35843;&#25972;&#21040;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#30340;&#24494;&#35843;&#22312;MORLHF&#20013;&#19981;&#31283;&#23450;&#19988;&#32791;&#36153;&#36164;&#28304;&#65292;&#29305;&#21035;&#26159;&#22240;&#20026;&#21508;&#31181;&#24120;&#24120;&#30683;&#30462;&#30340;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#23427;&#23558;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#25193;&#23637;&#21040;&#22810;&#20010;&#23545;&#40784;&#30446;&#26631;&#12290;&#22522;&#26412;&#19978;&#65292;MODPO&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#30340;LM&#26469;&#20195;&#34920;&#19981;&#21516;&#30340;&#38598;&#20307;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#23558;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#36827;&#34892;&#32452;&#21512;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;LM&#26681;&#25454;MOD&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs), despite aligning well with an average labeler through reinforcement learning from human feedback (RLHF), may not universally suit diverse human preferences. Recent approaches therefore opt for customization by collecting multi-dimensional feedback and creating distinct rewards for each dimension (e.g., helpfulness, harmlessness, honesty). LMs can then be tailored to different preferences using multi-objective RL (MORL) with different reward weightings. Yet, RL fine-tuning is unstable and resource-heavy, especially for MORLHF with diverse and usually conflicting objectives. In this paper, we present Multi-Objective Direct Preference Optimization (MODPO), an RL-free algorithm that extends Direct Preference Optimization (DPO) for multiple alignment objectives. Essentially, MODPO trains different LMs to represent different collective reward models that combine all objectives with specific weightings. With a simple cross-entropy loss, the LMs optimized against the MOD
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#19982;&#21464;&#20998;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#29256;&#26412;&#30340;&#26032;&#22411;&#21464;&#20998;CTC&#65292;&#29992;&#20110;&#35757;&#32451;&#26356;&#20855;&#26222;&#36866;&#24615;&#30340;&#20445;&#24207;&#24207;&#21015;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#20248;&#21270;&#27169;&#22411;&#23545;&#25968;&#20284;&#28982;&#30340;&#21464;&#20998;&#19979;&#30028;&#65292;&#24182;&#35299;&#20915;&#20102;&#35745;&#31639;&#19978;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.11983</link><description>&lt;p&gt;
&#21464;&#20998;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#29992;&#20110;&#20445;&#24207;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Variational Connectionist Temporal Classification for Order-Preserving Sequence Modeling. (arXiv:2309.11983v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#19982;&#21464;&#20998;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#29256;&#26412;&#30340;&#26032;&#22411;&#21464;&#20998;CTC&#65292;&#29992;&#20110;&#35757;&#32451;&#26356;&#20855;&#26222;&#36866;&#24615;&#30340;&#20445;&#24207;&#24207;&#21015;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#20248;&#21270;&#27169;&#22411;&#23545;&#25968;&#20284;&#28982;&#30340;&#21464;&#20998;&#19979;&#30028;&#65292;&#24182;&#35299;&#20915;&#20102;&#35745;&#31639;&#19978;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#24120;&#34987;&#29992;&#20110;&#20445;&#24207;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#65292;&#27604;&#22914;&#35821;&#38899;&#35782;&#21035;&#65292;&#20854;&#20013;&#20445;&#25345;&#36755;&#20837;&#21644;&#30446;&#26631;&#24207;&#21015;&#30340;&#39034;&#24207;&#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;CTC&#20165;&#24212;&#29992;&#20110;&#30830;&#23450;&#24615;&#24207;&#21015;&#27169;&#22411;&#65292;&#20854;&#20013;&#28508;&#22312;&#31354;&#38388;&#26159;&#19981;&#36830;&#32493;&#19988;&#31232;&#30095;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#22788;&#29702;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#26041;&#38754;&#27604;&#21464;&#20998;&#27169;&#22411;&#33021;&#21147;&#26356;&#24369;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;CTC&#19982;&#21464;&#20998;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#24182;&#23548;&#20986;&#20102;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#26356;&#20855;&#26222;&#36866;&#24615;&#30340;&#24207;&#21015;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#20445;&#25345;&#39034;&#24207;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;&#20004;&#20010;&#21512;&#29702;&#30340;&#20551;&#35774;&#23548;&#20986;&#20102;&#20004;&#20010;&#29256;&#26412;&#30340;&#26032;&#22411;&#21464;&#20998;CTC&#65292;&#31532;&#19968;&#20010;&#20551;&#35774;&#26159;&#27599;&#20010;&#26102;&#38388;&#27493;&#30340;&#21464;&#20998;&#28508;&#22312;&#21464;&#37327;&#22312;&#26465;&#20214;&#19979;&#26159;&#29420;&#31435;&#30340;&#65307;&#31532;&#20108;&#20010;&#20551;&#35774;&#26159;&#36825;&#20123;&#28508;&#22312;&#21464;&#37327;&#26159;&#39532;&#23572;&#21487;&#22827;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#37117;&#20801;&#35768;&#30452;&#25509;&#20248;&#21270;&#27169;&#22411;&#23545;&#25968;&#20284;&#28982;&#30340;&#21464;&#20998;&#19979;&#30028;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#35745;&#31639;&#19978;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Connectionist temporal classification (CTC) is commonly adopted for sequence modeling tasks like speech recognition, where it is necessary to preserve order between the input and target sequences. However, CTC is only applied to deterministic sequence models, where the latent space is discontinuous and sparse, which in turn makes them less capable of handling data variability when compared to variational models. In this paper, we integrate CTC with a variational model and derive loss functions that can be used to train more generalizable sequence models that preserve order. Specifically, we derive two versions of the novel variational CTC based on two reasonable assumptions, the first being that the variational latent variables at each time step are conditionally independent; and the second being that these latent variables are Markovian. We show that both loss functions allow direct optimization of the variational lower bound for the model log-likelihood, and present computationally t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#36825;&#31181;&#24402;&#32422;&#26041;&#24335;&#21487;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.04339</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#20984;&#20248;&#21270;&#23454;&#29616;&#22312;&#32447;&#23376;&#27169;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Online Submodular Maximization via Online Convex Optimization. (arXiv:2309.04339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#36825;&#31181;&#24402;&#32422;&#26041;&#24335;&#21487;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#22312;&#19968;&#33324;&#24615;&#27169;&#24615;&#32422;&#26463;&#19979;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#32447;&#20248;&#21270;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#65292;&#21363;&#21152;&#26435;&#38408;&#20540;&#21183;&#20989;&#25968;&#65292;&#21487;&#20197;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;(OCO)&#38382;&#39064;&#12290;&#36825;&#26159;&#22240;&#20026;&#36825;&#20010;&#31867;&#21035;&#30340;&#20989;&#25968;&#21487;&#20197;&#36827;&#34892;&#20985;&#26494;&#24347;;&#22240;&#27492;&#65292;&#32467;&#21512;&#36866;&#24403;&#30340;&#33293;&#20837;&#26041;&#26696;&#65292;OCO&#31574;&#30053;&#21487;&#20197;&#22312;&#32452;&#21512;&#35774;&#32622;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31616;&#21270;&#26041;&#24335;&#21487;&#20197;&#24212;&#29992;&#22312;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#21253;&#25324;&#21160;&#24577;&#36951;&#25022;&#12289;&#24378;&#30423;&#21644;&#20048;&#35266;&#23398;&#20064;&#31561;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study monotone submodular maximization under general matroid constraints in the online setting. We prove that online optimization of a large class of submodular functions, namely, weighted threshold potential functions, reduces to online convex optimization (OCO). This is precisely because functions in this class admit a concave relaxation; as a result, OCO policies, coupled with an appropriate rounding scheme, can be used to achieve sublinear regret in the combinatorial setting. We show that our reduction extends to many different versions of the online learning problem, including the dynamic regret, bandit, and optimistic-learning settings.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#20204;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#30340;&#20986;&#34892;&#25968;&#25454;&#65292;&#21457;&#29616;&#22478;&#24066;&#24418;&#24577;&#21464;&#37327;&#23545;&#20110;&#22478;&#24066;&#20869;&#20986;&#34892;&#26041;&#24335;&#20135;&#29983;&#20102;&#38388;&#25509;&#24433;&#21709;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#30740;&#31350;&#30340;&#19981;&#36275;&#65292;&#20026;&#23454;&#29616;&#20840;&#29699;&#21487;&#25345;&#32493;&#20132;&#36890;&#31995;&#32479;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.16599</link><description>&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#26041;&#27861;&#23398;&#20064;&#22478;&#24066;&#24418;&#24577;&#22914;&#20309;&#24433;&#21709;&#19981;&#21516;&#22823;&#27954;&#30340;&#21487;&#25345;&#32493;&#20986;&#34892;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
A Causal Discovery Approach To Learn How Urban Form Shapes Sustainable Mobility Across Continents. (arXiv:2308.16599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16599
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#20204;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#30340;&#20986;&#34892;&#25968;&#25454;&#65292;&#21457;&#29616;&#22478;&#24066;&#24418;&#24577;&#21464;&#37327;&#23545;&#20110;&#22478;&#24066;&#20869;&#20986;&#34892;&#26041;&#24335;&#20135;&#29983;&#20102;&#38388;&#25509;&#24433;&#21709;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#30740;&#31350;&#30340;&#19981;&#36275;&#65292;&#20026;&#23454;&#29616;&#20840;&#29699;&#21487;&#25345;&#32493;&#20132;&#36890;&#31995;&#32479;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#21487;&#25345;&#32493;&#21457;&#23637;&#38656;&#35201;&#20302;&#30899;&#22478;&#24066;&#20132;&#36890;&#31995;&#32479;&#65292;&#36825;&#38656;&#35201;&#36866;&#24403;&#30340;&#22522;&#30784;&#35774;&#26045;&#65292;&#20302;&#30899;&#20132;&#36890;&#26041;&#24335;&#30340;&#25512;&#24191;&#21644;&#20986;&#34892;&#34892;&#20026;&#30340;&#25913;&#21464;&#12290;&#20026;&#20102;&#27491;&#30830;&#23454;&#26045;&#22522;&#30784;&#35774;&#26045;&#21464;&#38761;&#65292;&#20102;&#35299;&#24314;&#31569;&#29615;&#22659;&#23545;&#20986;&#34892;&#30340;&#24433;&#21709;&#30340;&#22320;&#29702;&#29305;&#23450;&#30340;&#22240;&#26524;&#20851;&#31995;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#22312;&#34920;&#31034;6D&#22478;&#24066;&#24418;&#24577;&#21464;&#37327;&#21644;&#20986;&#34892;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12289;&#22312;&#19981;&#21516;&#22320;&#21306;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#20197;&#21450;&#22312;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#19979;&#24314;&#27169;&#22478;&#24066;&#24418;&#24577;&#25928;&#24212;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22240;&#26524;&#21457;&#29616;&#21644;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#26681;&#25454;&#19977;&#20010;&#22823;&#27954;&#20845;&#20010;&#22478;&#24066;&#30340;&#39640;&#20998;&#36776;&#29575;&#20986;&#34892;&#25968;&#25454;&#65292;&#26816;&#27979;&#22478;&#24066;&#24418;&#24577;&#23545;&#22478;&#24066;&#20869;&#20986;&#34892;&#30340;&#24433;&#21709;&#65292;&#24182;&#22635;&#34917;&#20102;&#36825;&#19977;&#20010;&#24046;&#36317;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36317;&#31163;&#24066;&#20013;&#24515;&#12289;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#21644;&#23494;&#24230;&#38388;&#25509;&#24433;&#21709;&#20854;&#20182;&#22478;&#24066;&#24418;&#24577;&#29305;&#24449;&#12290;&#36890;&#36807;&#32771;&#34385;&#36825;&#20123;&#22240;&#26524;&#20851;&#31995;&#65292;&#25105;&#20204;&#21457;&#29616;&#22320;&#29702;&#20301;&#32622;&#23545;&#19981;&#21516;&#22478;&#24066;&#20043;&#38388;&#30340;&#20132;&#36890;&#26041;&#24335;&#36873;&#25321;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global sustainability requires low-carbon urban transport systems, shaped by adequate infrastructure, deployment of low-carbon transport modes and shifts in travel behavior. To adequately implement alterations in infrastructure, it's essential to grasp the location-specific cause-and-effect mechanisms that the constructed environment has on travel. Yet, current research falls short in representing causal relationships between the 6D urban form variables and travel, generalizing across different regions, and modeling urban form effects at high spatial resolution. Here, we address all three gaps by utilizing a causal discovery and an explainable machine learning framework to detect urban form effects on intra-city travel based on high-resolution mobility data of six cities across three continents. We show that both distance to city center, demographics and density indirectly affect other urban form features. By considering the causal relationships, we find that location-specific influenc
&lt;/p&gt;</description></item><item><title>3D-MuPPET&#26159;&#19968;&#20010;&#29992;&#20110;&#20272;&#35745;&#21644;&#36319;&#36394;&#22810;&#21482;&#40509;&#23376;&#19977;&#32500;&#23039;&#21183;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#35270;&#35282;&#23454;&#26102;&#25512;&#27979;2D&#20851;&#38190;&#28857;&#24182;&#23558;&#20854;&#19977;&#35282;&#21270;&#21040;3D&#31354;&#38388;&#65292;&#21516;&#26102;&#20351;&#29992;&#21160;&#24577;&#21305;&#37197;&#21644;2D&#36319;&#36394;&#22120;&#32500;&#25345;&#23545;&#24212;&#20851;&#31995;&#12290;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;3D&#23039;&#21183;&#20272;&#35745;&#22120;&#65292;&#20855;&#26377;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26694;&#26550;&#36824;&#33021;&#22312;&#20351;&#29992;&#21333;&#21482;&#40509;&#23376;&#25968;&#25454;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#22810;&#21482;&#40509;&#23376;&#25968;&#25454;&#65292;&#31616;&#21270;&#39046;&#22495;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2308.15316</link><description>&lt;p&gt;
3D-MuPPET: 3D&#22810;&#40509;&#23039;&#24577;&#20272;&#35745;&#19982;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
3D-MuPPET: 3D Multi-Pigeon Pose Estimation and Tracking. (arXiv:2308.15316v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15316
&lt;/p&gt;
&lt;p&gt;
3D-MuPPET&#26159;&#19968;&#20010;&#29992;&#20110;&#20272;&#35745;&#21644;&#36319;&#36394;&#22810;&#21482;&#40509;&#23376;&#19977;&#32500;&#23039;&#21183;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#35270;&#35282;&#23454;&#26102;&#25512;&#27979;2D&#20851;&#38190;&#28857;&#24182;&#23558;&#20854;&#19977;&#35282;&#21270;&#21040;3D&#31354;&#38388;&#65292;&#21516;&#26102;&#20351;&#29992;&#21160;&#24577;&#21305;&#37197;&#21644;2D&#36319;&#36394;&#22120;&#32500;&#25345;&#23545;&#24212;&#20851;&#31995;&#12290;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;3D&#23039;&#21183;&#20272;&#35745;&#22120;&#65292;&#20855;&#26377;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26694;&#26550;&#36824;&#33021;&#22312;&#20351;&#29992;&#21333;&#21482;&#40509;&#23376;&#25968;&#25454;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#22810;&#21482;&#40509;&#23376;&#25968;&#25454;&#65292;&#31616;&#21270;&#39046;&#22495;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#21160;&#29289;&#23039;&#21183;&#36319;&#36394;&#30340;&#26080;&#26631;&#35760;&#26041;&#27861;&#24050;&#26377;&#25152;&#21457;&#23637;&#65292;&#20294;&#20173;&#32570;&#20047;&#29992;&#20110;&#36861;&#36394;&#22823;&#35268;&#27169;&#21160;&#29289;&#32676;&#20307;&#30340;&#19977;&#32500;&#26694;&#26550;&#21644;&#22522;&#20934;&#12290;&#20026;&#20102;&#24357;&#34917;&#25991;&#29486;&#20013;&#30340;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;3D-MuPPET&#65292;&#19968;&#20010;&#20351;&#29992;&#22810;&#35270;&#35282;&#23454;&#26102;&#20272;&#35745;&#21644;&#36319;&#36394;&#22810;&#36798;10&#21482;&#40509;&#23376;&#30340;&#19977;&#32500;&#23039;&#21183;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#23039;&#21183;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#25512;&#27979;&#22810;&#21482;&#40509;&#23376;&#30340;2D&#20851;&#38190;&#28857;&#21644;&#36793;&#30028;&#26694;&#65292;&#28982;&#21518;&#23558;&#20851;&#38190;&#28857;&#19977;&#35282;&#21270;&#21040;3D&#31354;&#38388;&#12290;&#23545;&#20110;&#21305;&#37197;&#23545;&#24212;&#20851;&#31995;&#65292;&#25105;&#20204;&#39318;&#20808;&#21160;&#24577;&#22320;&#23558;2D&#26816;&#27979;&#32467;&#26524;&#19982;&#31532;&#19968;&#24103;&#20013;&#30340;&#20840;&#23616;&#36523;&#20221;&#36827;&#34892;&#21305;&#37197;&#65292;&#28982;&#21518;&#20351;&#29992;2D&#36319;&#36394;&#22120;&#22312;&#21518;&#32493;&#24103;&#20013;&#32500;&#25345;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;3D&#23039;&#21183;&#20272;&#35745;&#22120;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#21363;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#21644;&#27491;&#30830;&#20851;&#38190;&#28857;&#30334;&#20998;&#27604;&#65288;PCK&#65289;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29992;&#20363;&#65292;&#21363;&#25105;&#20204;&#20351;&#29992;&#21333;&#21482;&#40509;&#23376;&#30340;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#24182;&#22312;&#21253;&#21547;&#22810;&#21482;&#40509;&#23376;&#30340;&#25968;&#25454;&#19978;&#24471;&#21040;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#65292;&#36825;&#21487;&#20197;&#31616;&#21270;&#23545;&#26032;&#22330;&#26223;&#30340;&#39046;&#22495;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Markerless methods for animal posture tracking have been developing recently, but frameworks and benchmarks for tracking large animal groups in 3D are still lacking. To overcome this gap in the literature, we present 3D-MuPPET, a framework to estimate and track 3D poses of up to 10 pigeons at interactive speed using multiple-views. We train a pose estimator to infer 2D keypoints and bounding boxes of multiple pigeons, then triangulate the keypoints to 3D. For correspondence matching, we first dynamically match 2D detections to global identities in the first frame, then use a 2D tracker to maintain correspondences accross views in subsequent frames. We achieve comparable accuracy to a state of the art 3D pose estimator for Root Mean Square Error (RMSE) and Percentage of Correct Keypoints (PCK). We also showcase a novel use case where our model trained with data of single pigeons provides comparable results on data containing multiple pigeons. This can simplify the domain shift to new sp
&lt;/p&gt;</description></item><item><title>OEBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20851;&#31995;&#25968;&#25454;&#27969;&#20013;&#24320;&#25918;&#29615;&#22659;&#25361;&#25112;&#30340;&#24320;&#25918;&#29615;&#22659;&#22522;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;&#36825;&#31181;&#25361;&#25112;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2308.15059</link><description>&lt;p&gt;
OEBench: &#30740;&#31350;&#29616;&#23454;&#19990;&#30028;&#20013;&#20851;&#31995;&#25968;&#25454;&#27969;&#20013;&#30340;&#24320;&#25918;&#29615;&#22659;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
OEBench: Investigating Open Environment Challenges in Real-World Relational Data Streams. (arXiv:2308.15059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15059
&lt;/p&gt;
&lt;p&gt;
OEBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20851;&#31995;&#25968;&#25454;&#27969;&#20013;&#24320;&#25918;&#29615;&#22659;&#25361;&#25112;&#30340;&#24320;&#25918;&#29615;&#22659;&#22522;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;&#36825;&#31181;&#25361;&#25112;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25968;&#25454;&#38598;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#38750;&#24120;&#26222;&#36941;&#65292;&#24182;&#19988;&#36890;&#24120;&#20197;&#25968;&#25454;&#27969;&#30340;&#26041;&#24335;&#20256;&#36882;&#12290;&#36825;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#27969;&#21487;&#33021;&#23384;&#22312;&#19968;&#20123;&#29305;&#27530;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#20998;&#24067;&#28418;&#31227;&#12289;&#24322;&#24120;&#20540;&#12289;&#26032;&#20852;&#31867;&#21035;&#21644;&#29305;&#24449;&#21464;&#21270;&#65292;&#26368;&#36817;&#23558;&#36825;&#20123;&#25361;&#25112;&#25551;&#36848;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24320;&#25918;&#29615;&#22659;&#25361;&#25112;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#19968;&#20123;&#20851;&#20110;&#25968;&#25454;&#27969;&#30340;&#22686;&#37327;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#20294;&#20854;&#35780;&#20272;&#20027;&#35201;&#26159;&#22522;&#20110;&#25163;&#21160;&#20998;&#21106;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#26377;&#20960;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#27969;&#25968;&#25454;&#38598;&#21487;&#29992;&#65292;&#20294;&#36825;&#20123;&#24320;&#25918;&#29615;&#22659;&#25361;&#25112;&#26159;&#21542;&#26222;&#36941;&#23384;&#22312;&#20197;&#21450;&#29616;&#26377;&#30340;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;&#23578;&#19981;&#30830;&#23450;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OEBench&#30340;&#24320;&#25918;&#29615;&#22659;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#20851;&#31995;&#25968;&#25454;&#27969;&#20013;&#30340;&#24320;&#25918;&#29615;&#22659;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;55&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#27969;&#25968;&#25454;&#38598;&#65292;&#24182;&#30830;&#31435;&#20102;&#24320;&#25918;&#29615;&#22659;&#22330;&#26223;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#30340;&#26222;&#36941;&#23384;&#22312;&#24615;&#65292;&#36825;&#22312;&#24403;&#21069;&#30340;&#30740;&#31350;&#20013;&#36824;&#27809;&#26377;&#34987;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relational datasets are widespread in real-world scenarios and are usually delivered in a streaming fashion. This type of data stream can present unique challenges, such as distribution drifts, outliers, emerging classes, and changing features, which have recently been described as open environment challenges for machine learning. While some work has been done on incremental learning for data streams, their evaluations are mostly conducted with manually partitioned datasets. Moreover, while several real-world streaming datasets are available, it is uncertain whether these open environment challenges are prevalent and how existing incremental learning algorithms perform on real datasets. To fill this gap, we develop an Open Environment Benchmark named OEBench to evaluate open environment challenges in relational data streams. Specifically, we investigate 55 real-world streaming datasets and establish that open environment scenarios are indeed widespread in real-world datasets, which pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#25937;&#26041;&#27861;&#65288;SafeAR&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#39118;&#38505;&#22240;&#32032;&#22312;&#35745;&#31639;&#21644;&#35780;&#20272;&#34917;&#25937;&#25514;&#26045;&#26102;&#65292;&#20026;&#37027;&#20123;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#19981;&#21033;&#24433;&#21709;&#30340;&#20010;&#20307;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.12367</link><description>&lt;p&gt;
SafeAR: &#36890;&#36807;&#39118;&#38505;&#24863;&#30693;&#31574;&#30053;&#23454;&#29616;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#20607;
&lt;/p&gt;
&lt;p&gt;
SafeAR: Towards Safer Algorithmic Recourse by Risk-Aware Policies. (arXiv:2308.12367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#25937;&#26041;&#27861;&#65288;SafeAR&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#39118;&#38505;&#22240;&#32032;&#22312;&#35745;&#31639;&#21644;&#35780;&#20272;&#34917;&#25937;&#25514;&#26045;&#26102;&#65292;&#20026;&#37027;&#20123;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#19981;&#21033;&#24433;&#21709;&#30340;&#20010;&#20307;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#37329;&#34701;&#21644;&#21307;&#30103;&#31561;&#20851;&#38190;&#39046;&#22495;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#20026;&#37027;&#20123;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#19981;&#21033;&#24433;&#21709;&#30340;&#20010;&#20307;&#25552;&#20379;&#34917;&#25937;&#25514;&#26045;&#30340;&#38656;&#27714;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#65307;&#20010;&#20307;&#24212;&#35813;&#33719;&#24471;&#25913;&#21892;&#33258;&#36523;&#24773;&#20917;&#21644;&#33719;&#24471;&#26377;&#21033;&#20915;&#31574;&#30340;&#24314;&#35758;&#12290;&#20043;&#21069;&#20851;&#20110;&#39034;&#24207;&#31639;&#27861;&#34917;&#25937;&#30340;&#24037;&#20316;&#8212;&#8212;&#25512;&#33616;&#19968;&#31995;&#21015;&#21464;&#21270;&#8212;&#8212;&#20027;&#35201;&#20851;&#27880;&#34892;&#21160;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20351;&#29992;&#29305;&#24449;&#21464;&#21270;&#30340;&#25509;&#36817;&#31243;&#24230;&#30830;&#23450;&#34892;&#21160;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#26410;&#32771;&#34385;&#29305;&#24449;&#21464;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#34917;&#25937;&#20013;&#39640;&#20110;&#24179;&#22343;&#25104;&#26412;&#30340;&#39118;&#38505;&#12290;&#22914;&#26524;&#34917;&#25937;&#25514;&#26045;&#21487;&#33021;&#65288;&#20197;&#19968;&#23450;&#27010;&#29575;&#65289;&#23548;&#33268;&#26356;&#31967;&#31957;&#30340;&#24773;&#20917;&#65292;&#32780;&#24674;&#22797;&#38656;&#35201;&#20184;&#20986;&#38750;&#24120;&#39640;&#30340;&#20195;&#20215;&#65292;&#37027;&#23558;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#22312;&#35745;&#31639;&#21644;&#35780;&#20272;&#34917;&#25937;&#25514;&#26045;&#26102;&#65292;&#24517;&#39035;&#32771;&#34385;&#39118;&#38505;&#12290;&#25105;&#20204;&#23558;&#32771;&#34385;&#20102;&#36825;&#31181;&#39118;&#38505;&#22240;&#32032;&#35745;&#31639;&#20986;&#30340;&#34917;&#25937;&#25514;&#26045;&#31216;&#20026;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#25937;&#65288;SafeAR&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing use of machine learning (ML) models in critical domains such as finance and healthcare, the need to offer recourse for those adversely affected by the decisions of ML models has become more important; individuals ought to be provided with recommendations on actions to take for improving their situation and thus receive a favorable decision. Prior work on sequential algorithmic recourse -- which recommends a series of changes -- focuses on action feasibility and uses the proximity of feature changes to determine action costs. However, the uncertainties of feature changes and the risk of higher than average costs in recourse have not been considered. It is undesirable if a recourse could (with some probability) result in a worse situation from which recovery requires an extremely high cost. It is essential to incorporate risks when computing and evaluating recourse. We call the recourse computed with such risk considerations as Safer Algorithmic Recourse (SafeAR). The ob
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#22686;&#24191;&#26500;&#24314;&#20102;&#23431;&#23449;&#23398;&#25968;&#25454;&#30340;&#20195;&#34920;&#24615;&#27719;&#24635;&#65292;&#21487;&#20197;&#26377;&#25928;&#21387;&#32553;&#25968;&#25454;&#24182;&#29992;&#20110;&#31934;&#30830;&#21442;&#25968;&#25512;&#26029;&#65292;&#20026;&#23431;&#23449;&#23398;&#25968;&#25454;&#30340;&#21387;&#32553;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26032;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2308.09751</link><description>&lt;p&gt;
&#23431;&#23449;&#23398;&#20013;&#30340;&#25968;&#25454;&#21387;&#32553;&#19982;&#25512;&#26029;&#65306;&#33258;&#25105;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Data Compression and Inference in Cosmology with Self-Supervised Machine Learning. (arXiv:2308.09751v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#22686;&#24191;&#26500;&#24314;&#20102;&#23431;&#23449;&#23398;&#25968;&#25454;&#30340;&#20195;&#34920;&#24615;&#27719;&#24635;&#65292;&#21487;&#20197;&#26377;&#25928;&#21387;&#32553;&#25968;&#25454;&#24182;&#29992;&#20110;&#31934;&#30830;&#21442;&#25968;&#25512;&#26029;&#65292;&#20026;&#23431;&#23449;&#23398;&#25968;&#25454;&#30340;&#21387;&#32553;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#21644;&#21363;&#23558;&#21040;&#26469;&#30340;&#23431;&#23449;&#23398;&#35843;&#26597;&#25152;&#20135;&#29983;&#30340;&#28023;&#37327;&#25968;&#25454;&#65292;&#38656;&#35201;&#33021;&#22815;&#20197;&#26368;&#23567;&#30340;&#20449;&#24687;&#25439;&#22833;&#26377;&#25928;&#22320;&#27719;&#24635;&#25968;&#25454;&#30340;&#21387;&#32553;&#26041;&#26696;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#20197;&#26032;&#39062;&#26041;&#24335;&#26500;&#24314;&#20195;&#34920;&#24615;&#25968;&#25454;&#27719;&#24635;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#27169;&#25311;&#30340;&#22686;&#24191;&#25216;&#26415;&#12290;&#22312;&#27969;&#20307;&#21147;&#23398;&#23431;&#23449;&#23398;&#27169;&#25311;&#25968;&#25454;&#19978;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#33021;&#22815;&#25552;&#20379;&#39640;&#24230;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#27719;&#24635;&#65292;&#21487;&#20197;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#21253;&#25324;&#31934;&#30830;&#21644;&#20934;&#30830;&#30340;&#21442;&#25968;&#25512;&#26029;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#33539;&#20363;&#22914;&#20309;&#26500;&#24314;&#23545;&#39044;&#23450;&#31995;&#32479;&#25928;&#24212;&#19981;&#25935;&#24863;&#30340;&#27719;&#24635;&#34920;&#31034;&#65292;&#20363;&#22914;&#24357;&#25955;&#29289;&#29702;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#25105;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20026;&#23431;&#23449;&#23398;&#25968;&#25454;&#30340;&#21387;&#32553;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
The influx of massive amounts of data from current and upcoming cosmological surveys necessitates compression schemes that can efficiently summarize the data with minimal loss of information. We introduce a method that leverages the paradigm of self-supervised machine learning in a novel manner to construct representative summaries of massive datasets using simulation-based augmentations. Deploying the method on hydrodynamical cosmological simulations, we show that it can deliver highly informative summaries, which can be used for a variety of downstream tasks, including precise and accurate parameter inference. We demonstrate how this paradigm can be used to construct summary representations that are insensitive to prescribed systematic effects, such as the influence of baryonic physics. Our results indicate that self-supervised machine learning techniques offer a promising new approach for compression of cosmological data as well its analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#31526;&#21512;&#39044;&#27979;&#30340;&#26080;&#32447;&#36890;&#36947;&#32852;&#21512;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#22791;&#21040;&#26381;&#21153;&#22120;&#30340;&#36890;&#20449;&#25552;&#39640;&#20102;&#26381;&#21153;&#22120;&#30340;&#25512;&#29702;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04237</link><description>&lt;p&gt;
&#22522;&#20110;&#31526;&#21512;&#39044;&#27979;&#30340;&#21487;&#38752;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26080;&#32447;&#36890;&#36947;&#32852;&#21512;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Federated Inference with Reliable Uncertainty Quantification over Wireless Channels via Conformal Prediction. (arXiv:2308.04237v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#31526;&#21512;&#39044;&#27979;&#30340;&#26080;&#32447;&#36890;&#36947;&#32852;&#21512;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#22791;&#21040;&#26381;&#21153;&#22120;&#30340;&#36890;&#20449;&#25552;&#39640;&#20102;&#26381;&#21153;&#22120;&#30340;&#25512;&#29702;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#20849;&#20139;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35774;&#32622;&#20013;&#65292;&#26381;&#21153;&#22120;&#24076;&#26395;&#26681;&#25454;&#27169;&#22411;&#23545;&#26032;&#36755;&#20837;&#36827;&#34892;&#25512;&#29702;&#12290;&#35774;&#22791;&#21487;&#20197;&#36890;&#36807;&#20849;&#21516;&#30340;&#26080;&#32447;&#20449;&#36947;&#19982;&#26381;&#21153;&#22120;&#36890;&#20449;&#65292;&#24182;&#19988;&#21487;&#20197;&#35775;&#38382;&#20197;&#21069;&#26410;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#12290;&#22914;&#26524;&#35774;&#22791;&#26080;&#27861;&#35775;&#38382;&#26032;&#36755;&#20837;&#65292;&#35774;&#22791;&#21040;&#26381;&#21153;&#22120;&#30340;&#36890;&#20449;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#26381;&#21153;&#22120;&#30340;&#25512;&#29702;&#20915;&#31574;&#36136;&#37327;&#65311;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#32852;&#21512;&#31526;&#21512;&#39044;&#27979;&#65288;CP&#65289;&#65292;&#21033;&#29992;&#35774;&#22791;&#21040;&#26381;&#21153;&#22120;&#30340;&#36890;&#20449;&#26469;&#25552;&#39640;&#26381;&#21153;&#22120;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#32852;&#21512;CP&#20013;&#65292;&#35774;&#22791;&#21521;&#26381;&#21153;&#22120;&#20256;&#36882;&#20851;&#20110;&#26412;&#22320;&#25968;&#25454;&#19978;&#20849;&#20139;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#25439;&#22833;&#30340;&#20449;&#24687;&#65292;&#26381;&#21153;&#22120;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#26657;&#20934;&#19968;&#20010;&#20915;&#31574;&#21306;&#38388;&#65292;&#20197;&#20415;&#20854;&#22312;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#21487;&#38752;&#24615;&#27700;&#24179;&#19979;&#20445;&#35777;&#21253;&#21547;&#27491;&#30830;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider a setting in which devices and a server share a pre-trained model. The server wishes to make an inference on a new input given the model. Devices have access to data, previously not used for training, and can communicate to the server over a common wireless channel. If the devices have no access to the new input, can communication from devices to the server enhance the quality of the inference decision at the server? Recent work has introduced federated conformal prediction (CP), which leverages devices-to-server communication to improve the reliability of the server's decision. With federated CP, devices communicate to the server information about the loss accrued by the shared pre-trained model on the local data, and the server leverages this information to calibrate a decision interval, or set, so that it is guaranteed to contain the correct answer with a pre-defined target reliability level. Previous work assumed noise-free communication, whereby devices can communicate a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#31163;&#31574;&#30053;&#20540;&#20989;&#25968;&#20272;&#35745;&#20013;&#30340;&#36924;&#36817;&#22240;&#23376;&#65292;&#24182;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#26368;&#20248;&#30340;&#28176;&#36817;&#36924;&#36817;&#22240;&#23376;&#65292;&#36825;&#20123;&#22240;&#23376;&#20915;&#23450;&#20102;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#22256;&#38590;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.13332</link><description>&lt;p&gt;
&#22312;&#38169;&#35823;&#25351;&#23450;&#30340;&#31163;&#31574;&#30053;&#20540;&#20989;&#25968;&#20272;&#35745;&#20013;&#30340;&#26368;&#20339;&#36924;&#36817;&#22240;&#23376;
&lt;/p&gt;
&lt;p&gt;
The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation. (arXiv:2307.13332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#31163;&#31574;&#30053;&#20540;&#20989;&#25968;&#20272;&#35745;&#20013;&#30340;&#36924;&#36817;&#22240;&#23376;&#65292;&#24182;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#26368;&#20248;&#30340;&#28176;&#36817;&#36924;&#36817;&#22240;&#23376;&#65292;&#36825;&#20123;&#22240;&#23376;&#20915;&#23450;&#20102;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#22256;&#38590;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#30693;&#36947;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#29702;&#35770;&#20445;&#35777;&#22312;&#20989;&#25968;&#36924;&#36817;&#30340;&#38169;&#35823;&#25351;&#23450;&#20013;&#20250;&#20986;&#29616;&#20056;&#27861;&#25918;&#22823;&#22240;&#23376;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;\emph{&#36924;&#36817;&#22240;&#23376;}&#30340;&#24615;&#36136;&#65292;&#29305;&#21035;&#26159;&#22312;&#32473;&#23450;&#30340;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#26368;&#20339;&#24418;&#24335;&#65292;&#20173;&#28982;&#19981;&#20026;&#20154;&#25152;&#20102;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#22312;&#32447;&#24615;&#31163;&#31574;&#30053;&#20540;&#20989;&#25968;&#20272;&#35745;&#20013;&#30340;&#24191;&#27867;&#35774;&#32622;&#20013;&#30340;&#36924;&#36817;&#22240;&#23376;&#65292;&#20854;&#20013;&#20173;&#26377;&#35768;&#22810;&#24320;&#25918;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#30340;&#36924;&#36817;&#22240;&#23376;&#65292;&#20363;&#22914;&#21152;&#26435;$L_2$&#33539;&#25968;&#65288;&#20854;&#20013;&#21152;&#26435;&#26159;&#31163;&#32447;&#29366;&#24577;&#20998;&#24067;&#65289;&#65292;$L_\infty$&#33539;&#25968;&#65292;&#29366;&#24577;&#21035;&#21517;&#30340;&#23384;&#22312;&#19982;&#21542;&#20197;&#21450;&#23545;&#29366;&#24577;&#31354;&#38388;&#30340;&#20840;&#38754;&#19982;&#37096;&#20998;&#35206;&#30422;&#12290;&#23545;&#20110;&#25152;&#26377;&#36825;&#20123;&#35774;&#32622;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#26368;&#20248;&#30340;&#28176;&#36817;&#36924;&#36817;&#22240;&#23376;&#65288;&#33267;&#22810;&#24120;&#25968;&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#30830;&#23450;&#20102;$L_2(\mu)$&#33539;&#25968;&#30340;&#20004;&#20010;&#20381;&#36182;&#20110;&#23454;&#20363;&#30340;&#22240;&#23376;&#21644;$L_\infty$&#33539;&#25968;&#30340;&#19968;&#20010;&#22240;&#23376;&#65292;&#23427;&#20204;&#34987;&#35777;&#26126;&#20915;&#23450;&#20102;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#22256;&#38590;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theoretical guarantees in reinforcement learning (RL) are known to suffer multiplicative blow-up factors with respect to the misspecification error of function approximation. Yet, the nature of such \emph{approximation factors} -especially their optimal form in a given learning problem -- is poorly understood. In this paper we study this question in linear off-policy value function estimation, where many open questions remain. We study the approximation factor in a broad spectrum of settings, such as with the weighted $L_2$-norm (where the weighting is the offline state distribution), the $L_\infty$ norm, the presence vs. absence of state aliasing, and full vs. partial coverage of the state space. We establish the optimal asymptotic approximation factors (up to constants) for all of these settings. In particular, our bounds identify two instance-dependent factors for the $L_2(\mu)$ norm and only one for the $L_\infty$ norm, which are shown to dictate the hardness of off-policy evalua
&lt;/p&gt;</description></item><item><title>&#38750;&#32447;&#24615;&#20803;&#23398;&#20064;&#21487;&#20197;&#20445;&#35777;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.10870</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#20803;&#23398;&#20064;&#21487;&#20197;&#20445;&#35777;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Nonlinear Meta-Learning Can Guarantee Faster Rates. (arXiv:2307.10870v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10870
&lt;/p&gt;
&lt;p&gt;
&#38750;&#32447;&#24615;&#20803;&#23398;&#20064;&#21487;&#20197;&#20445;&#35777;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35768;&#22810;&#20851;&#20110;&#20803;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#30456;&#20284;&#34920;&#31034;&#32467;&#26500;&#26469;&#31616;&#21270;&#30446;&#26631;&#20219;&#21153;&#65292;&#24182;&#23454;&#29616;&#25910;&#25947;&#36895;&#29575;&#30340;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#34920;&#31034;&#24448;&#24448;&#26159;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#65292;&#24341;&#20837;&#20102;&#27599;&#20010;&#20219;&#21153;&#20013;&#19981;&#21487;&#31616;&#21333;&#24179;&#22343;&#30340;&#38750;&#24179;&#20961;&#20559;&#24046;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#38750;&#32447;&#24615;&#34920;&#31034;&#25512;&#23548;&#20986;&#20803;&#23398;&#20064;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent theoretical works on \emph{meta-learning} aim to achieve guarantees in leveraging similar representational structures from related tasks towards simplifying a target task. Importantly, the main aim in theory works on the subject is to understand the extent to which convergence rates -- in learning a common representation -- \emph{may scale with the number $N$ of tasks} (as well as the number of samples per task). First steps in this setting demonstrate this property when both the shared representation amongst tasks, and task-specific regression functions, are linear. This linear setting readily reveals the benefits of aggregating tasks, e.g., via averaging arguments. In practice, however, the representation is often highly nonlinear, introducing nontrivial biases in each task that cannot easily be averaged out as in the linear case. In the present work, we derive theoretical guarantees for meta-learning with nonlinear representations. In particular, assuming the shared nonl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;$\nu^2$-&#27969;&#26041;&#27861;&#65292;&#23427;&#26159;$\nu$-&#27969;&#26041;&#27861;&#22312;&#21253;&#21547;&#22810;&#20010;&#20013;&#24494;&#23376;&#30340;&#26411;&#24577;&#20013;&#30340;&#25193;&#23637;&#12290;&#19982;&#26631;&#20934;&#35299;&#26512;&#25216;&#26415;&#30456;&#27604;&#65292;$\nu^2$-&#27969;&#22312;&#37325;&#24314;&#20013;&#24494;&#23376;&#21160;&#37327;&#21644;&#30456;&#20851;&#24615;&#26041;&#38754;&#26356;&#20934;&#30830;&#65292;&#25512;&#26029;&#26102;&#38388;&#26356;&#24555;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#20379;&#25913;&#36827;&#30340;&#32479;&#35745;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.02405</link><description>&lt;p&gt;
$\nu^2$-&#27969;&#65306;&#22312;&#22810;&#20013;&#24494;&#23376;&#26411;&#24577;&#20013;&#20351;&#29992;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#24555;&#36895;&#21644;&#25913;&#36827;&#30340;&#20013;&#24494;&#23376;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
$\nu^2$-Flows: Fast and improved neutrino reconstruction in multi-neutrino final states with conditional normalizing flows. (arXiv:2307.02405v2 [hep-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;$\nu^2$-&#27969;&#26041;&#27861;&#65292;&#23427;&#26159;$\nu$-&#27969;&#26041;&#27861;&#22312;&#21253;&#21547;&#22810;&#20010;&#20013;&#24494;&#23376;&#30340;&#26411;&#24577;&#20013;&#30340;&#25193;&#23637;&#12290;&#19982;&#26631;&#20934;&#35299;&#26512;&#25216;&#26415;&#30456;&#27604;&#65292;$\nu^2$-&#27969;&#22312;&#37325;&#24314;&#20013;&#24494;&#23376;&#21160;&#37327;&#21644;&#30456;&#20851;&#24615;&#26041;&#38754;&#26356;&#20934;&#30830;&#65292;&#25512;&#26029;&#26102;&#38388;&#26356;&#24555;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#20379;&#25913;&#36827;&#30340;&#32479;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;$\nu^2$-&#27969;&#26041;&#27861;&#65292;&#36825;&#26159;$\nu$-&#27969;&#26041;&#27861;&#23545;&#20110;&#21253;&#21547;&#22810;&#20010;&#20013;&#24494;&#23376;&#30340;&#26411;&#24577;&#30340;&#19968;&#31181;&#25193;&#23637;&#12290;&#35813;&#26550;&#26500;&#21487;&#20197;&#21407;&#29983;&#22320;&#20026;&#26411;&#24577;&#20013;&#30340;&#25152;&#26377;&#23545;&#35937;&#31867;&#22411;&#21644;&#22810;&#37325;&#24615;&#32452;&#21512;&#36827;&#34892;&#32553;&#25918;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#25152;&#38656;&#30340;&#20013;&#24494;&#23376;&#22810;&#37325;&#24615;&#12290;&#22312;$t\bar{t}$&#20108;&#36731;&#23376;&#20107;&#20214;&#20013;&#65292;&#19982;&#20351;&#29992;&#26368;&#27969;&#34892;&#30340;&#26631;&#20934;&#35299;&#26512;&#25216;&#26415;&#30456;&#27604;&#65292;&#20013;&#24494;&#23376;&#30340;&#21160;&#37327;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#37325;&#24314;&#65292;&#24182;&#19988;&#21487;&#20197;&#25214;&#21040;&#25152;&#26377;&#20107;&#20214;&#30340;&#35299;&#12290;&#25512;&#26029;&#26102;&#38388;&#27604;&#31454;&#20105;&#26041;&#27861;&#26174;&#33879;&#26356;&#24555;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22312;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#19978;&#24182;&#34892;&#35780;&#20272;&#26469;&#36827;&#19968;&#27493;&#20943;&#23569;&#12290;&#25105;&#20204;&#23558;$\nu^2$-&#27969;&#24212;&#29992;&#20110;$t\bar{t}$&#20108;&#36731;&#23376;&#20107;&#20214;&#65292;&#24182;&#23637;&#31034;&#20102;&#23637;&#24320;&#20998;&#24067;&#20013;&#27599;&#20010;&#31665;&#23376;&#30340;&#19981;&#30830;&#23450;&#24615;&#27604;&#26631;&#20934;&#25216;&#26415;&#26356;&#25509;&#36817;&#23436;&#32654;&#20013;&#24494;&#23376;&#37325;&#24314;&#30340;&#24615;&#33021;&#30028;&#38480;&#12290;&#23545;&#20110;&#36873;&#25321;&#30340;&#21452;&#24494;&#20998;&#35266;&#27979;&#37327;&#65292;$\nu^2$-&#27969;&#21487;&#20197;&#22312;&#27599;&#20010;&#31665;&#23376;&#20013;&#25552;&#20379;&#25913;&#36827;&#30340;&#32479;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we introduce $\nu^2$-Flows, an extension of the $\nu$-Flows method to final states containing multiple neutrinos. The architecture can natively scale for all combinations of object types and multiplicities in the final state for any desired neutrino multiplicities. In $t\bar{t}$ dilepton events, the momenta of both neutrinos and correlations between them are reconstructed more accurately than when using the most popular standard analytical techniques, and solutions are found for all events. Inference time is significantly faster than competing methods, and can be reduced further by evaluating in parallel on graphics processing units. We apply $\nu^2$-Flows to $t\bar{t}$ dilepton events and show that the per-bin uncertainties in unfolded distributions is much closer to the limit of performance set by perfect neutrino reconstruction than standard techniques. For the chosen double differential observables $\nu^2$-Flows results in improved statistical precision for each bin by
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22312;&#20302;&#28201;&#31561;&#31163;&#23376;&#20307;&#24314;&#27169;&#21644;&#27169;&#25311;&#26041;&#38754;&#30340;&#24212;&#29992;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23427;&#33021;&#22815;&#25552;&#20379;&#26032;&#30340;&#26041;&#27861;&#21644;&#36884;&#24452;&#65292;&#20419;&#36827;&#31561;&#31163;&#23376;&#20307;&#31185;&#23398;&#21644;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2307.00131</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#25512;&#21160;&#20302;&#28201;&#31561;&#31163;&#23376;&#20307;&#24314;&#27169;&#21644;&#27169;&#25311;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine learning for advancing low-temperature plasma modeling and simulation. (arXiv:2307.00131v1 [physics.plasm-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00131
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20302;&#28201;&#31561;&#31163;&#23376;&#20307;&#24314;&#27169;&#21644;&#27169;&#25311;&#26041;&#38754;&#30340;&#24212;&#29992;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23427;&#33021;&#22815;&#25552;&#20379;&#26032;&#30340;&#26041;&#27861;&#21644;&#36884;&#24452;&#65292;&#20419;&#36827;&#31561;&#31163;&#23376;&#20307;&#31185;&#23398;&#21644;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#37117;&#20135;&#29983;&#20102;&#24040;&#22823;&#24433;&#21709;&#65292;&#36817;&#24180;&#26469;&#65292;&#22312;&#20302;&#28201;&#31561;&#31163;&#23376;&#20307;&#24314;&#27169;&#21644;&#27169;&#25311;&#39046;&#22495;&#20063;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#23427;&#30340;&#24212;&#29992;&#24212;&#35813;&#34987;&#35880;&#24910;&#35780;&#20272;&#65292;&#20294;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#31561;&#31163;&#23376;&#20307;&#24314;&#27169;&#21644;&#27169;&#25311;&#30340;&#35768;&#22810;&#26041;&#38754;&#37117;&#21463;&#30410;&#21290;&#27973;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20027;&#35201;&#36861;&#27714;&#20004;&#20010;&#30446;&#26631;&#65306;(a)&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#20302;&#28201;&#31561;&#31163;&#23376;&#20307;&#24314;&#27169;&#21644;&#27169;&#25311;&#30340;&#26041;&#27861;&#30340;&#29616;&#29366;&#36827;&#34892;&#32508;&#36848;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#32508;&#36848;&#21010;&#20998;&#20026;&#31561;&#31163;&#23376;&#20307;&#29289;&#29702;&#23398;&#12289;&#31561;&#31163;&#23376;&#20307;&#21270;&#23398;&#12289;&#31561;&#31163;&#23376;&#20307;&#34920;&#38754;&#30456;&#20114;&#20316;&#29992;&#21644;&#31561;&#31163;&#23376;&#20307;&#36807;&#31243;&#25511;&#21046;&#22235;&#20010;&#26041;&#38754;&#65292;&#25105;&#20204;&#26088;&#22312;&#24191;&#27867;&#35752;&#35770;&#25991;&#29486;&#20013;&#30340;&#30456;&#20851;&#23454;&#20363;&#12290;(b)&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#31561;&#31163;&#23376;&#20307;&#31185;&#23398;&#21644;&#25216;&#26415;&#28508;&#22312;&#36827;&#23637;&#30340;&#23637;&#26395;&#12290;&#25105;&#20204;&#29305;&#21035;&#38416;&#36848;&#20102;&#21487;&#33021;&#36890;&#36807;&#20174;&#20854;&#20182;&#31185;&#23398;&#23398;&#31185;&#20013;&#30340;&#36866;&#24212;&#25512;&#21160;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24050;&#30693;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#23558;&#20351;&#20302;&#28201;&#31561;&#31163;&#23376;&#20307;&#24314;&#27169;&#21644;&#27169;&#25311;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has had an enormous impact in many scientific disciplines. Also in the field of low-temperature plasma modeling and simulation it has attracted significant interest within the past years. Whereas its application should be carefully assessed in general, many aspects of plasma modeling and simulation have benefited substantially from recent developments within the field of machine learning and data-driven modeling. In this survey, we approach two main objectives: (a) We review the state-of-the-art focusing on approaches to low-temperature plasma modeling and simulation. By dividing our survey into plasma physics, plasma chemistry, plasma-surface interactions, and plasma process control, we aim to extensively discuss relevant examples from literature. (b) We provide a perspective of potential advances to plasma science and technology. We specifically elaborate on advances possibly enabled by adaptation from other scientific disciplines. We argue that not only the known un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#21322;&#30417;&#30563;&#31232;&#30095;&#32479;&#35745;&#25512;&#26029;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#34701;&#21512;&#20102;&#26377;/&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#20026;M&#20272;&#35745;&#21644;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#25552;&#20379;&#20102;&#23450;&#21046;&#21435;&#20559;&#26041;&#27861;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#32467;&#21512;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.10395</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#21322;&#30417;&#30563;&#31232;&#30095;&#32479;&#35745;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Distributed Semi-Supervised Sparse Statistical Inference. (arXiv:2306.10395v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#21322;&#30417;&#30563;&#31232;&#30095;&#32479;&#35745;&#25512;&#26029;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#34701;&#21512;&#20102;&#26377;/&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#20026;M&#20272;&#35745;&#21644;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#25552;&#20379;&#20102;&#23450;&#21046;&#21435;&#20559;&#26041;&#27861;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#32467;&#21512;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#21322;&#30417;&#30563;&#31232;&#30095;&#32479;&#35745;&#25512;&#26029;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#36718;&#20998;&#24067;&#24335;&#21435;&#20559;&#20272;&#35745;&#22120;&#65292;&#23427;&#34701;&#21512;&#20102;&#26377;&#26631;&#35760;&#21644;&#26080;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#19988;&#28436;&#31034;&#20102;&#39069;&#22806;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#22914;&#20309;&#24110;&#21161;&#25552;&#39640;&#27599;&#36718;&#36845;&#20195;&#30340;&#32479;&#35745;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;$M$- &#20272;&#35745;&#21644;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#25552;&#20379;&#20102;&#37327;&#36523;&#23450;&#21046;&#30340;&#21435;&#20559;&#26041;&#27861;&#65292;&#20855;&#20307;&#26681;&#25454;&#25439;&#22833;&#20989;&#25968;&#30340;&#29305;&#23450;&#24418;&#24335;&#32780;&#23450;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#38750;&#20809;&#28369;&#25439;&#22833;&#65292;&#20363;&#22914;&#32477;&#23545;&#20559;&#24046;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#39640;&#32500;&#36870;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20272;&#35745;&#12290;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#21644;&#30495;&#23454;&#25968;&#25454;&#24212;&#29992;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#32467;&#21512;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is devoted to studying the semi-supervised sparse statistical inference in a distributed setup. An efficient multi-round distributed debiased estimator, which integrates both labeled and unlabelled data, is developed. We will show that the additional unlabeled data helps to improve the statistical rate of each round of iteration. Our approach offers tailored debiasing methods for $M$-estimation and generalized linear model according to the specific form of the loss function. Our method also applies to a non-smooth loss like absolute deviation loss. Furthermore, our algorithm is computationally efficient since it requires only one estimation of a high-dimensional inverse covariance matrix. We demonstrate the effectiveness of our method by presenting simulation studies and real data applications that highlight the benefits of incorporating unlabeled data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20351;&#29992;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#36817;&#20284;&#26041;&#27861;&#26367;&#25442;transformer&#26550;&#26500;&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#23494;&#38598;&#30340;&#36816;&#31639;&#31526;&#65292;&#23454;&#29616;&#20102;&#22823;&#24133;&#38477;&#20302;&#31169;&#26377;&#25512;&#26029;&#25104;&#26412;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#35745;&#31639;&#21152;&#36895;&#21644;&#36890;&#20449;&#24320;&#38144;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2305.18396</link><description>&lt;p&gt;
LLM&#21487;&#20197;&#29702;&#35299;&#21152;&#23494;&#25552;&#31034;&#65306;&#38754;&#21521;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;Transformers
&lt;/p&gt;
&lt;p&gt;
LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers. (arXiv:2305.18396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20351;&#29992;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#36817;&#20284;&#26041;&#27861;&#26367;&#25442;transformer&#26550;&#26500;&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#23494;&#38598;&#30340;&#36816;&#31639;&#31526;&#65292;&#23454;&#29616;&#20102;&#22823;&#24133;&#38477;&#20302;&#31169;&#26377;&#25512;&#26029;&#25104;&#26412;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#35745;&#31639;&#21152;&#36895;&#21644;&#36890;&#20449;&#24320;&#38144;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#22312;&#26381;&#21153;&#22120;&#23458;&#25143;&#31471;&#29615;&#22659;&#20013;&#20026;&#22522;&#20110;transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#26500;&#24314;&#31169;&#26377;&#25512;&#26029;&#26694;&#26550;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#25345;&#26377;&#27169;&#22411;&#21442;&#25968;&#65292;&#23458;&#25143;&#31471;&#36755;&#20837;&#31169;&#26377;&#25968;&#25454;&#36827;&#34892;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#24403;&#31169;&#26377;&#36755;&#20837;&#36890;&#36807;&#21407;&#22987;LLMs&#36827;&#34892;&#21069;&#21521;&#20256;&#25773;&#26102;&#65292;&#36825;&#20123;&#26694;&#26550;&#20250;&#20135;&#29983;&#26174;&#30528;&#30340;&#24320;&#38144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#29992;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#36817;&#20284;&#26367;&#25442;transformer&#26550;&#26500;&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#23494;&#38598;&#30340;&#36816;&#31639;&#31526;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#31169;&#26377;&#25512;&#26029;&#25104;&#26412;&#65292;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;&#19982;&#26368;&#26032;&#30340;Iron&#65288;NeurIPS 2022&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#27169;&#22411;&#25512;&#26029;&#31649;&#36947;&#22312;&#35745;&#31639;&#19978;&#23454;&#29616;&#20102;$5 \times$&#30340;&#21152;&#36895;&#65292;&#22312;&#36890;&#20449;&#24320;&#38144;&#19978;&#23454;&#29616;&#20102;80\%&#30340;&#38477;&#20302;&#65292;&#21516;&#26102;&#20960;&#20046;&#20445;&#25345;&#20102;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior works have attempted to build private inference frameworks for transformer-based large language models (LLMs) in a server-client setting, where the server holds the model parameters and the client inputs the private data for inference. However, these frameworks impose significant overhead when the private inputs are forward propagated through the original LLMs. In this paper, we show that substituting the computation- and communication-heavy operators in the transformer architecture with privacy-computing friendly approximations can greatly reduce the private inference costs with minor impact on model performance. Compared to the state-of-the-art Iron (NeurIPS 2022), our privacy-computing friendly model inference pipeline achieves a $5\times$ acceleration in computation and an 80\% reduction in communication overhead, while retaining nearly identical accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PDE+&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#20998;&#24067;&#25193;&#25955;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#19981;&#20165;&#33021;&#21462;&#24471;&#26368;&#20808;&#36827;&#25110;&#26377;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#36824;&#26377;&#26356;&#23569;&#30340;&#35757;&#32451;&#36718;&#27425;&#21644;&#32593;&#32476;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.15835</link><description>&lt;p&gt;
PDE+&#65306;&#36890;&#36807;&#33258;&#36866;&#24212;&#20998;&#24067;&#25193;&#25955;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PDE+: Enhancing Generalization via PDE with Adaptive Distributional Diffusion. (arXiv:2305.15835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PDE+&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#20998;&#24067;&#25193;&#25955;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#19981;&#20165;&#33021;&#21462;&#24471;&#26368;&#20808;&#36827;&#25110;&#26377;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#36824;&#26377;&#26356;&#23569;&#30340;&#35757;&#32451;&#36718;&#27425;&#21644;&#32593;&#32476;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#30340;&#20998;&#24067;&#26102;&#30340;&#24615;&#33021;&#12290;&#30446;&#21069;&#20027;&#35201;&#37319;&#29992;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#33539;&#24335;&#30340;&#26041;&#24335;&#65292;&#22914;&#25968;&#25454;&#22686;&#24378;&#65292;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#22122;&#22768;&#27880;&#20837;&#31561;&#65292;&#36825;&#20123;&#26041;&#27861;&#30001;&#20110;&#27169;&#22411;&#30340;&#38750;&#24179;&#28369;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#21463;&#38480;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;&#27867;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#30452;&#25509;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#30784;&#20989;&#25968;&#26469;&#22686;&#24378;&#23427;&#65292;&#32780;&#19981;&#26159;&#32858;&#28966;&#20110;&#35843;&#25972;&#36755;&#20837;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#19982;&#29305;&#23450;PDE&#35299;&#30340;&#24179;&#28369;&#24230;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#21363;&#8220;&#36755;&#36816;&#26041;&#31243;&#8221;&#12290;&#36825;&#26679;&#24314;&#31435;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24635;&#20307;&#26694;&#26550;&#65292;&#23558;&#33258;&#36866;&#24212;&#20998;&#24067;&#25193;&#25955;&#24341;&#20837;&#36755;&#36816;&#26041;&#31243;&#20013;&#65292;&#20197;&#22686;&#24378;&#20854;&#35299;&#30340;&#24179;&#28369;&#24230;&#65292;&#20174;&#32780;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#25554;&#20214;&#27169;&#22359;&#20351;&#29992;&#65292;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#35757;&#32451;&#31639;&#27861;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#23569;&#30340;&#32593;&#32476;&#21442;&#25968;&#21644;&#35757;&#32451;&#36718;&#27425;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#25110;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization of neural networks is a central challenge in machine learning, especially concerning the performance under distributions that differ from training ones. Current methods, mainly based on the data-driven paradigm such as data augmentation, adversarial training, and noise injection, may encounter limited generalization due to model non-smoothness. In this paper, we propose to investigate generalization from a Partial Differential Equation (PDE) perspective, aiming to enhance it directly through the underlying function of neural networks, rather than focusing on adjusting input data. Specifically, we first establish the connection between neural network generalization and the smoothness of the solution to a specific PDE, namely ``transport equation''. Building upon this, we propose a general framework that introduces adaptive distributional diffusion into transport equation to enhance the smoothness of its solution, thereby improving generalization. In the context of neu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#21046;&#21644;&#25512;&#24191;&#20043;&#38388;&#24179;&#34913;&#30340; RL &#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13030</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
Adaptive action supervision in reinforcement learning from real-world multi-agent demonstrations. (arXiv:2305.13030v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#21046;&#21644;&#25512;&#24191;&#20043;&#38388;&#24179;&#34913;&#30340; RL &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#65292;&#23545;&#30495;&#23454;&#19990;&#30028;&#29983;&#29289;&#22810;&#26234;&#33021;&#20307;&#36827;&#34892;&#24314;&#27169;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#22312;&#32593;&#32476;&#31354;&#38388;&#20013;&#29983;&#25104;&#28789;&#27963;&#21644;&#22810;&#26679;&#21270;&#34892;&#20026;&#30340;&#24378;&#22823;&#26694;&#26550;&#65307;&#28982;&#32780;&#65292;&#22312;&#24314;&#27169;&#30495;&#23454;&#19990;&#30028;&#29983;&#29289;&#22810;&#26234;&#33021;&#20307;&#26102;&#65292;&#22312;&#28304;&#65288;&#21363;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65289;&#21644;&#30446;&#26631;&#65288;&#21363; RL &#30340;&#32593;&#32476;&#31354;&#38388;&#65289;&#20043;&#38388;&#23384;&#22312;&#22495;&#24046;&#24322;&#65292;&#24182;&#19988;&#28304;&#29615;&#22659;&#21442;&#25968;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#30340;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#36827;&#34892; RL &#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#32467;&#21512; RL &#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#22522;&#20110;&#21160;&#24577;&#26102;&#38388;&#25197;&#26354;&#30340;&#28436;&#31034;&#21160;&#20316;&#26469;&#22312; RL &#20013;&#21033;&#29992;&#26410;&#30693;&#28304;&#21160;&#24577;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#35768;&#22810;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#20026;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#22312;&#22797;&#21046;&#21644;&#25512;&#24191;&#20043;&#38388;&#24179;&#34913;&#30340; RL &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling of real-world biological multi-agents is a fundamental problem in various scientific and engineering fields. Reinforcement learning (RL) is a powerful framework to generate flexible and diverse behaviors in cyberspace; however, when modeling real-world biological multi-agents, there is a domain gap between behaviors in the source (i.e., real-world data) and the target (i.e., cyberspace for RL), and the source environment parameters are usually unknown. In this paper, we propose a method for adaptive action supervision in RL from real-world demonstrations in multi-agent scenarios. We adopt an approach that combines RL and supervised learning by selecting actions of demonstrations in RL based on the minimum distance of dynamic time warping for utilizing the information of the unknown source dynamics. This approach can be easily applied to many existing neural network architectures and provide us with an RL model balanced between reproducibility as imitation and generalization ab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#26799;&#24230;&#24179;&#34913;&#25915;&#20987;&#65288;DGBA&#65289;&#26694;&#26550;&#26469;&#25915;&#20987;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#22238;&#31572;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#12289;&#22810;&#20219;&#21153;&#25915;&#20987;&#21644;&#23545;&#25239;&#35757;&#32451;&#26159;&#21542;&#22686;&#24378;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#31561;&#23433;&#20840;&#30740;&#31350;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.12066</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#21160;&#24577;&#26799;&#24230;&#24179;&#34913;&#22686;&#24378;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Dynamic Gradient Balancing for Enhanced Adversarial Attacks on Multi-Task Models. (arXiv:2305.12066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#26799;&#24230;&#24179;&#34913;&#25915;&#20987;&#65288;DGBA&#65289;&#26694;&#26550;&#26469;&#25915;&#20987;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#22238;&#31572;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#12289;&#22810;&#20219;&#21153;&#25915;&#20987;&#21644;&#23545;&#25239;&#35757;&#32451;&#26159;&#21542;&#22686;&#24378;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#31561;&#23433;&#20840;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064; (MTL) &#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#21333;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#22810;&#20010;&#20219;&#21153;&#12290;&#34429;&#28982;&#21333;&#20219;&#21153;&#20998;&#31867;&#22120;&#30340;&#23433;&#20840;&#24615;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#23384;&#22312;&#30528;&#20960;&#20010;&#20851;&#38190;&#30340;&#23433;&#20840;&#24615;&#30740;&#31350;&#38382;&#39064;&#65292;&#21253;&#25324;: 1&#65289;&#22810;&#20219;&#21153;&#27169;&#22411;&#23545;&#21333;&#20219;&#21153;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#22914;&#20309;&#65311;2&#65289;&#33021;&#21542;&#35774;&#35745;&#23545;&#25239;&#24615;&#25915;&#20987;&#26469;&#21516;&#26102;&#25915;&#20987;&#22810;&#20010;&#20219;&#21153;&#65311; 3&#65289;&#20219;&#21153;&#20849;&#20139;&#21644;&#23545;&#25239;&#35757;&#32451;&#26159;&#21542;&#22686;&#21152;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65311;&#26412;&#25991;&#36890;&#36807;&#20180;&#32454;&#20998;&#26512;&#21644;&#20005;&#26684;&#30340;&#23454;&#39564;&#22238;&#31572;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#21333;&#20219;&#21153;&#30333;&#30418;&#25915;&#20987;&#30340;&#21021;&#32423;&#36716;&#21270;&#24182;&#20998;&#26512;&#20102;&#20854;&#22266;&#26377;&#32570;&#38519;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26694;&#26550;&#65292;&#21160;&#24577;&#26799;&#24230;&#24179;&#34913;&#25915;&#20987;&#65288;DGBA&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25226;&#25915;&#20987;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#38382;&#39064;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#24179;&#22343;&#30456;&#23545;&#25439;&#22833;&#21464;&#21270;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) creates a single machine learning model called multi-task model to simultaneously perform multiple tasks. Although the security of single task classifiers has been extensively studied, there are several critical security research questions for multi-task models including 1) How secure are multi-task models to single task adversarial machine learning attacks, 2) Can adversarial attacks be designed to attack multiple tasks simultaneously, and 3) Does task sharing and adversarial training increase multi-task model robustness to adversarial attacks? In this paper, we answer these questions through careful analysis and rigorous experimentation. First, we develop na\"ive adaptation of single-task white-box attacks and analyze their inherent drawbacks. We then propose a novel attack framework, Dynamic Gradient Balancing Attack (DGBA). Our framework poses the problem of attacking a multi-task model as an optimization problem based on averaged relative loss change, whi
&lt;/p&gt;</description></item><item><title>RPPO&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#31243;&#24207;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#20855;&#22791;&#22810;&#26679;&#30340;&#39118;&#38505;&#20559;&#22909;&#65292;&#20174;&#32780;&#22686;&#21152;&#33258;&#25105;&#23545;&#25239;&#31639;&#27861;&#20013;&#30340;&#31574;&#30053;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#39640;&#20195;&#29702;&#31243;&#24207;&#38754;&#23545;&#19981;&#21516;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11476</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#32676;&#33258;&#25105;&#23545;&#25239;&#23398;&#20064;&#22810;&#26679;&#39118;&#38505;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Learning Diverse Risk Preferences in Population-based Self-play. (arXiv:2305.11476v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11476
&lt;/p&gt;
&lt;p&gt;
RPPO&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#31243;&#24207;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#20855;&#22791;&#22810;&#26679;&#30340;&#39118;&#38505;&#20559;&#22909;&#65292;&#20174;&#32780;&#22686;&#21152;&#33258;&#25105;&#23545;&#25239;&#31639;&#27861;&#20013;&#30340;&#31574;&#30053;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#39640;&#20195;&#29702;&#31243;&#24207;&#38754;&#23545;&#19981;&#21516;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#25104;&#21151;&#26696;&#20363;&#20013;&#65292;&#33258;&#25105;&#23545;&#25239;&#31639;&#27861;&#22312;&#35299;&#20915;&#31454;&#20105;&#24615;&#28216;&#25103;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#24403;&#21069;&#30340;&#33258;&#25105;&#23545;&#25239;&#31639;&#27861;&#22312;&#20248;&#21270;&#20195;&#29702;&#31243;&#24207;&#20197;&#26368;&#22823;&#21270;&#39044;&#26399;&#32988;&#29575;&#26102;&#65292;&#24448;&#24448;&#20250;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#24182;&#20135;&#29983;&#21333;&#19968;&#21516;&#36136;&#21270;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#25171;&#30772;&#20725;&#23616;&#24182;&#22686;&#24378;&#20195;&#29702;&#31243;&#24207;&#38754;&#23545;&#19981;&#21516;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#65292;&#35299;&#20915;&#26041;&#27861;&#21487;&#33021;&#22312;&#20110;&#22686;&#21152;&#31574;&#30053;&#30340;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#25105;&#23545;&#25239;&#31639;&#27861;&#20013;&#22686;&#21152;&#22810;&#26679;&#24615;&#24182;&#19981;&#26159;&#26131;&#22914;&#21453;&#25484;&#30340;&#12290;&#26412;&#25991;&#35797;&#22270;&#20174;&#20195;&#29702;&#31243;&#24207;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#21487;&#20197;&#20855;&#22791;&#22810;&#26679;&#30340;&#39118;&#38505;&#20559;&#22909;&#36825;&#19968;&#35270;&#35282;&#20986;&#21457;&#22686;&#21152;&#31574;&#30053;&#22810;&#26679;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#39118;&#38505;&#25935;&#24863;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;(RPPO)&#65292;&#23427;&#22312;&#26368;&#22351;&#21644;&#26368;&#22909;&#30340;&#31574;&#30053;&#23398;&#20064;&#20043;&#38388;&#24179;&#28369;&#22320;&#25554;&#20540;&#65292;&#20801;&#35768;&#20855;&#26377;&#25152;&#38656;&#39118;&#38505;&#20559;&#22909;&#30340;&#31574;&#30053;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among the great successes of Reinforcement Learning (RL), self-play algorithms play an essential role in solving competitive games. Current self-play algorithms optimize the agent to maximize expected win-rates against its current or historical copies, making it often stuck in the local optimum and its strategy style simple and homogeneous. A possible solution is to improve the diversity of policies, which helps the agent break the stalemate and enhances its robustness when facing different opponents. However, enhancing diversity in the self-play algorithms is not trivial. In this paper, we aim to introduce diversity from the perspective that agents could have diverse risk preferences in the face of uncertainty. Specifically, we design a novel reinforcement learning algorithm called Risk-sensitive Proximal Policy Optimization (RPPO), which smoothly interpolates between worst-case and best-case policy learning and allows for policy learning with desired risk preferences. Seamlessly inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; W-MAE &#30340;&#39044;&#35757;&#32451;&#22825;&#27668;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#37325;&#24314;&#27668;&#35937;&#21464;&#37327;&#20043;&#38388;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#39044;&#27979;&#27668;&#35937;&#21464;&#37327;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#20174;&#32780;&#23545;&#22825;&#27668;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2304.08754</link><description>&lt;p&gt;
W-MAE&#65306;&#20855;&#26377;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#22825;&#27668;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#22825;&#27668;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
W-MAE: Pre-trained weather model with masked autoencoder for multi-variable weather forecasting. (arXiv:2304.08754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; W-MAE &#30340;&#39044;&#35757;&#32451;&#22825;&#27668;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#37325;&#24314;&#27668;&#35937;&#21464;&#37327;&#20043;&#38388;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#39044;&#27979;&#27668;&#35937;&#21464;&#37327;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#20174;&#32780;&#23545;&#22825;&#27668;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#39044;&#27979;&#26159;&#20855;&#26377;&#30452;&#25509;&#31038;&#20250;&#21644;&#32463;&#27982;&#24433;&#21709;&#30340;&#38271;&#26399;&#35745;&#31639;&#25361;&#25112;&#12290;&#35813;&#20219;&#21153;&#28041;&#21450;&#22823;&#37327;&#30340;&#36830;&#32493;&#25968;&#25454;&#25910;&#38598;&#65292;&#24182;&#22312;&#38271;&#26102;&#38388;&#20869;&#34920;&#29616;&#20986;&#20016;&#23500;&#30340;&#26102;&#31354;&#20381;&#36182;&#24615;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#23558;&#39044;&#35757;&#32451;&#25216;&#26415;&#24212;&#29992;&#20110;&#22825;&#27668;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#21464;&#37327;&#22825;&#27668;&#39044;&#27979;&#30340;&#20855;&#26377;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#30340;&#22825;&#27668;&#27169;&#22411;W-MAE&#12290;W-MAE&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#37325;&#24314;&#27668;&#35937;&#21464;&#37327;&#20043;&#38388;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#22312;&#26102;&#38388;&#23610;&#24230;&#19978;&#65292;&#25105;&#20204;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;W-MAE&#20197;&#39044;&#27979;&#27668;&#35937;&#21464;&#37327;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#20174;&#32780;&#23545;&#22825;&#27668;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#20351;&#29992;&#27599;&#20845;&#23567;&#26102;&#36873;&#25321;&#19968;&#27425;&#26679;&#26412;&#65292;&#20165;&#20351;&#29992;&#20004;&#24180;&#30340;ERA5&#25968;&#25454;&#65292;&#23545;W-MAE&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#25968;&#25454;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#23558;W-MAE&#19982;FourCastNet&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weather forecasting is a long-standing computational challenge with direct societal and economic impacts. This task involves a large amount of continuous data collection and exhibits rich spatiotemporal dependencies over long periods, making it highly suitable for deep learning models. In this paper, we apply pre-training techniques to weather forecasting and propose W-MAE, a Weather model with Masked AutoEncoder pre-training for multi-variable weather forecasting. W-MAE is pre-trained in a self-supervised manner to reconstruct spatial correlations within meteorological variables. On the temporal scale, we fine-tune the pre-trained W-MAE to predict the future states of meteorological variables, thereby modeling the temporal dependencies present in weather data. We pre-train W-MAE using the fifth-generation ECMWF Reanalysis (ERA5) data, with samples selected every six hours and using only two years of data. Under the same training data conditions, we compare W-MAE with FourCastNet, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#26041;&#27861;&#65292;&#21046;&#20316;&#20102;&#20122;&#31859;&#32423;&#20896;&#23618;&#39640;&#24230;&#22270;&#65292;&#21487;&#29992;&#20110;&#32454;&#31890;&#24230;&#30340;&#26893;&#34987;&#32467;&#26500;&#30417;&#27979;&#65292;&#20026;&#30899;&#36890;&#37327;&#35780;&#20272;&#21644;&#22303;&#22320;&#21033;&#29992;&#31649;&#29702;&#25552;&#20379;&#23453;&#36149;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.07213</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#21046;&#20316;&#20122;&#31859;&#32423;&#20896;&#23618;&#39640;&#24230;&#22270;
&lt;/p&gt;
&lt;p&gt;
Sub-meter resolution canopy height maps using self-supervised learning and a vision transformer trained on Aerial and GEDI Lidar. (arXiv:2304.07213v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#26041;&#27861;&#65292;&#21046;&#20316;&#20102;&#20122;&#31859;&#32423;&#20896;&#23618;&#39640;&#24230;&#22270;&#65292;&#21487;&#29992;&#20110;&#32454;&#31890;&#24230;&#30340;&#26893;&#34987;&#32467;&#26500;&#30417;&#27979;&#65292;&#20026;&#30899;&#36890;&#37327;&#35780;&#20272;&#21644;&#22303;&#22320;&#21033;&#29992;&#31649;&#29702;&#25552;&#20379;&#23453;&#36149;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26893;&#34987;&#32467;&#26500;&#30340;&#26144;&#23556;&#23545;&#20110;&#29702;&#35299;&#20840;&#29699;&#30899;&#24490;&#29615;&#21644;&#30417;&#27979;&#22522;&#20110;&#33258;&#28982;&#30340;&#27668;&#20505;&#36866;&#24212;&#21644;&#20943;&#32531;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#26041;&#27861;&#65292;&#20351;&#29992;&#33322;&#31354;&#21644;GEDI&#28608;&#20809;&#36965;&#24863;&#25968;&#25454;&#21046;&#20316;&#20102;&#20122;&#31859;&#32423;&#20896;&#23618;&#39640;&#24230;&#22270;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#12290;&#21046;&#20316;&#30340;&#20896;&#23618;&#39640;&#24230;&#22270;&#21487;&#20197;&#23454;&#29616;&#32454;&#31890;&#24230;&#30340;&#26893;&#34987;&#32467;&#26500;&#30417;&#27979;&#65292;&#20026;&#30899;&#36890;&#37327;&#35780;&#20272;&#21644;&#22303;&#22320;&#21033;&#29992;&#31649;&#29702;&#25552;&#20379;&#23453;&#36149;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vegetation structure mapping is critical for understanding the global carbon cycle and monitoring nature-based approaches to climate adaptation and mitigation. Repeat measurements of these data allow for the observation of deforestation or degradation of existing forests, natural forest regeneration, and the implementation of sustainable agricultural practices like agroforestry. Assessments of tree canopy height and crown projected area at a high spatial resolution are also important for monitoring carbon fluxes and assessing tree-based land uses, since forest structures can be highly spatially heterogeneous, especially in agroforestry systems. Very high resolution satellite imagery (less than one meter (1m) ground sample distance) makes it possible to extract information at the tree level while allowing monitoring at a very large scale. This paper presents the first high-resolution canopy height map concurrently produced for multiple sub-national jurisdictions. Specifically, we produc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#32852;&#37030;&#23398;&#20064;&#23433;&#20840;&#21338;&#24328;&#65288;FLSG&#65289;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21516;&#26102;&#32771;&#34385;&#21040;&#32852;&#37030;&#23398;&#20064;&#30340;&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#30340;&#25910;&#30410;&#65292;&#21253;&#25324;&#35745;&#31639;&#25104;&#26412;&#12289;FL&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#27844;&#28431;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#31639;&#27861;&#26469;&#36817;&#20284;oracle&#24182;&#20445;&#25345;&#38544;&#31169;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#31639;&#27861;&#23545;&#20110;&#39044;&#38450;&#21644;&#26816;&#27979;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#25915;&#20987;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.05836</link><description>&lt;p&gt;
&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Game-theoretic Framework for Federated Learning. (arXiv:2304.05836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#32852;&#37030;&#23398;&#20064;&#23433;&#20840;&#21338;&#24328;&#65288;FLSG&#65289;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21516;&#26102;&#32771;&#34385;&#21040;&#32852;&#37030;&#23398;&#20064;&#30340;&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#30340;&#25910;&#30410;&#65292;&#21253;&#25324;&#35745;&#31639;&#25104;&#26412;&#12289;FL&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#27844;&#28431;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#31639;&#27861;&#26469;&#36817;&#20284;oracle&#24182;&#20445;&#25345;&#38544;&#31169;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#31639;&#27861;&#23545;&#20110;&#39044;&#38450;&#21644;&#26816;&#27979;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#25915;&#20987;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#33391;&#24615;&#21442;&#19982;&#32773;&#26088;&#22312;&#21327;&#21516;&#20248;&#21270;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#21322;&#35802;&#23454;&#30340;&#23545;&#25163;&#26102;&#65292;\textit{&#38544;&#31169;&#27844;&#28431;}&#30340;&#39118;&#38505;&#26159;&#19981;&#21487;&#24573;&#35270;&#30340;&#12290;&#29616;&#26377;&#30740;&#31350;&#35201;&#20040;&#19987;&#27880;&#20110;&#35774;&#35745;&#20445;&#25252;&#26426;&#21046;&#65292;&#35201;&#20040;&#19987;&#27880;&#20110;&#21457;&#26126;&#25915;&#20987;&#26426;&#21046;&#12290;&#34429;&#28982;&#20445;&#25252;&#32773;&#19982;&#25915;&#20987;&#32773;&#20043;&#38388;&#30340;&#26007;&#20105;&#20284;&#20046;&#27704;&#26080;&#27490;&#22659;&#65292;&#20294;&#25105;&#20204;&#20851;&#24515;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#33021;&#20107;&#20808;&#39044;&#38450;&#28508;&#22312;&#30340;&#25915;&#20987;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#21516;&#26102;&#32771;&#34385;FL&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#30340;&#30456;&#24212;&#25910;&#30410;&#65292;&#20854;&#20013;&#21253;&#25324;&#35745;&#31639;&#25104;&#26412;&#12289;FL&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#27844;&#28431;&#39118;&#38505;&#12290;&#25105;&#20204;&#23558;&#27492;&#28216;&#25103;&#31216;&#20026;&#32852;&#37030;&#23398;&#20064;&#23433;&#20840;&#21338;&#24328;&#65288;FLSG&#65289;&#65292;&#22312;&#20854;&#20013;&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#37117;&#19981;&#30693;&#36947;&#25152;&#26377;&#21442;&#19982;&#32773;&#30340;&#25910;&#30410;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#22266;&#26377;&#30340;\textit{&#19981;&#23436;&#20840;&#20449;&#24687;}&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;FLSG&#19982;&#19968;&#20010;\textit{oracle}&#30456;&#20851;&#32852;&#65292;&#35813;oracle&#20855;&#26377;&#25152;&#26377;&#21442;&#19982;&#32773;&#30340;&#25910;&#30410;&#30693;&#35782;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#21508;&#31181;&#25928;&#29992;&#20989;&#25968;&#21644;&#25915;&#20987;&#27169;&#22411;&#32452;&#21512;&#19979;FLSG&#30340;&#32435;&#20160;&#22343;&#34913;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#31639;&#27861;&#26469;&#36817;&#20284;oracle&#24182;&#20445;&#25345;&#38544;&#31169;&#12290;&#23454;&#39564;&#32467;&#26524;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#39044;&#38450;&#21644;&#26816;&#27979;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;FL&#22330;&#26223;&#20013;&#30340;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated learning, benign participants aim to optimize a global model collaboratively. However, the risk of \textit{privacy leakage} cannot be ignored in the presence of \textit{semi-honest} adversaries. Existing research has focused either on designing protection mechanisms or on inventing attacking mechanisms. While the battle between defenders and attackers seems never-ending, we are concerned with one critical question: is it possible to prevent potential attacks in advance? To address this, we propose the first game-theoretic framework that considers both FL defenders and attackers in terms of their respective payoffs, which include computational costs, FL model utilities, and privacy leakage risks. We name this game the Federated Learning Security Game (FLSG), in which neither defenders nor attackers are aware of all participants' payoffs.  To handle the \textit{incomplete information} inherent in this situation, we propose associating the FLSG with an \textit{oracle} that ha
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20301;&#32622;&#21435;&#22122;&#39044;&#27979;&#26131;&#24471;&#20960;&#20309;&#32467;&#26500;&#30340;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;&#65292;&#21487;&#20197;&#29992;&#30456;&#23545;&#23481;&#26131;&#33719;&#24471;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#31934;&#30830;&#39044;&#27979;&#24615;&#36136;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#20197;&#21450;&#21270;&#23398;&#21453;&#24212;&#24615;&#36136;&#30340;&#39044;&#27979;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2304.03724</link><description>&lt;p&gt;
&#21033;&#29992;&#20301;&#32622;&#21435;&#22122;&#39044;&#27979;&#26131;&#24471;&#20960;&#20309;&#32467;&#26500;&#30340;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Predicting quantum chemical property with easy-to-obtain geometry via positional denoising. (arXiv:2304.03724v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03724
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20301;&#32622;&#21435;&#22122;&#39044;&#27979;&#26131;&#24471;&#20960;&#20309;&#32467;&#26500;&#30340;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;&#65292;&#21487;&#20197;&#29992;&#30456;&#23545;&#23481;&#26131;&#33719;&#24471;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#31934;&#30830;&#39044;&#27979;&#24615;&#36136;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#20197;&#21450;&#21270;&#23398;&#21453;&#24212;&#24615;&#36136;&#30340;&#39044;&#27979;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;&#19982;&#20854;&#20960;&#20309;&#32467;&#26500;&#26377;&#37325;&#35201;&#20851;&#32852;&#65292;&#20351;&#29992;3D&#20960;&#20309;&#20449;&#24687;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#39640;&#32423;&#37327;&#23376;&#21147;&#23398;&#35745;&#31639;&#24471;&#20986;&#30340;3D&#20960;&#20309;&#32467;&#26500;&#65292;&#36825;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#29616;&#23454;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#23481;&#26131;&#33719;&#24471;&#30340;&#20960;&#20309;&#32467;&#26500;&#65288;&#20363;&#22914;&#26469;&#33258;&#20998;&#23376;&#21147;&#22330;&#30340;&#20248;&#21270;&#20960;&#20309;&#32467;&#26500;&#65289;&#31934;&#30830;&#39044;&#27979;&#24615;&#36136;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#36755;&#20837;&#20960;&#20309;&#32467;&#26500;&#36880;&#28176;&#25509;&#36817;&#27491;&#30830;&#20960;&#20309;&#32467;&#26500;&#65292;&#36890;&#36807;&#22534;&#21472;&#21435;&#22122;&#23618;&#12290;&#25105;&#20204;&#20351;&#29992;3D&#28040;&#24687;&#20256;&#36882;&#20307;&#31995;&#32467;&#26500;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#39044;&#27979;&#20219;&#21153;&#65288;&#20998;&#23376;&#24615;&#36136;&#21644;&#21270;&#23398;&#21453;&#24212;&#24615;&#36136;&#65289;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#21435;&#22122;&#36807;&#31243;&#20943;&#23569;&#20301;&#32622;&#35823;&#24046;&#26377;&#21161;&#20110;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
As quantum chemical properties have a significant dependence on their geometries, graph neural networks (GNNs) using 3D geometric information have achieved high prediction accuracy in many tasks. However, they often require 3D geometries obtained from high-level quantum mechanical calculations, which are practically infeasible, limiting their applicability in real-world problems. To tackle this, we propose a method to accurately predict the properties with relatively easy-to-obtain geometries (e.g., optimized geometries from the molecular force field). In this method, the input geometry, regarded as the corrupted geometry of the correct one, gradually approaches the correct one as it passes through the stacked denoising layers. We investigated the performance of the proposed method using 3D message-passing architectures for two prediction tasks: molecular properties and chemical reaction property. The reduction of positional errors through the denoising process contributed to performan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26377;&#36873;&#25321;&#30340;&#32852;&#37030;&#33976;&#39311;&#26426;&#21046;Selective-FD&#65292;&#21487;&#20197;&#31934;&#30830;&#22320;&#35782;&#21035;&#26469;&#33258;&#26412;&#22320;&#21644;&#38598;&#21512;&#39044;&#27979;&#30340;&#30693;&#35782;&#65292;&#20197;&#35299;&#20915;&#23616;&#37096;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#24322;&#21644;&#32570;&#20047;&#22909;&#30340;&#25945;&#24072;&#27169;&#22411;&#32780;&#23548;&#33268;&#30340;&#35823;&#23548;&#21644;&#27169;&#31946;&#30340;&#30693;&#35782;&#20849;&#20139;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.01731</link><description>&lt;p&gt;
&#38754;&#21521;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#33976;&#39311;&#30340;&#26377;&#36873;&#25321;&#30693;&#35782;&#20849;&#20139;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Selective Knowledge Sharing for Privacy-Preserving Federated Distillation without A Good Teacher. (arXiv:2304.01731v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26377;&#36873;&#25321;&#30340;&#32852;&#37030;&#33976;&#39311;&#26426;&#21046;Selective-FD&#65292;&#21487;&#20197;&#31934;&#30830;&#22320;&#35782;&#21035;&#26469;&#33258;&#26412;&#22320;&#21644;&#38598;&#21512;&#39044;&#27979;&#30340;&#30693;&#35782;&#65292;&#20197;&#35299;&#20915;&#23616;&#37096;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#24322;&#21644;&#32570;&#20047;&#22909;&#30340;&#25945;&#24072;&#27169;&#22411;&#32780;&#23548;&#33268;&#30340;&#35823;&#23548;&#21644;&#27169;&#31946;&#30340;&#30693;&#35782;&#20849;&#20139;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#26159;&#23481;&#26131;&#21463;&#21040;&#30333;&#30418;&#25915;&#20987;&#65292;&#24182;&#19988;&#38590;&#20197;&#36866;&#24212;&#24322;&#26500;&#23458;&#25143;&#31471;&#12290;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#32852;&#37030;&#33976;&#39311;&#26159;&#19968;&#31181;&#25552;&#20379;&#22686;&#24378;&#38544;&#31169;&#20445;&#35777;&#24182;&#35299;&#20915;&#27169;&#22411;&#24322;&#26500;&#24615;&#30340;&#26367;&#20195;&#33539;&#20363;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#36873;&#25321;&#30340;&#32852;&#37030;&#33976;&#39311;&#26426;&#21046;Selective-FD&#26469;&#24212;&#23545;&#23616;&#37096;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#24322;&#21644;&#32570;&#20047;&#22909;&#30340;&#25945;&#24072;&#27169;&#22411;&#32780;&#23548;&#33268;&#30340;&#35823;&#23548;&#21644;&#27169;&#31946;&#30340;&#30693;&#35782;&#20849;&#20139;&#38382;&#39064;&#12290;&#23427;&#21253;&#25324;&#23458;&#25143;&#31471;&#36873;&#25321;&#22120;&#21644;&#26381;&#21153;&#22120;&#36873;&#25321;&#22120;&#65292;&#20197;&#31934;&#30830;&#22320;&#35782;&#21035;&#26469;&#33258;&#26412;&#22320;&#21644;&#38598;&#21512;&#39044;&#27979;&#30340;&#30693;&#35782;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;Selective-FD&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
While federated learning is promising for privacy-preserving collaborative learning without revealing local data, it remains vulnerable to white-box attacks and struggles to adapt to heterogeneous clients. Federated distillation (FD), built upon knowledge distillation--an effective technique for transferring knowledge from a teacher model to student models--emerges as an alternative paradigm, which provides enhanced privacy guarantees and addresses model heterogeneity. Nevertheless, challenges arise due to variations in local data distributions and the absence of a well-trained teacher model, which leads to misleading and ambiguous knowledge sharing that significantly degrades model performance. To address these issues, this paper proposes a selective knowledge sharing mechanism for FD, termed Selective-FD. It includes client-side selectors and a server-side selector to accurately and precisely identify knowledge from local and ensemble predictions, respectively. Empirical studies, bac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#23618;&#22270;&#21367;&#31215;&#26680;&#26426;&#22120;(GCKM)&#65292;&#36890;&#36807;&#22534;&#21472;&#22810;&#20010;&#27973;&#26680;&#26426;&#22120;&#65292;&#35813;&#26426;&#22120;&#33021;&#22815;&#22312;&#22270;&#20013;&#36827;&#34892;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#65292;&#24182;&#19988;&#22312;&#21487;&#29992;&#26631;&#31614;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2301.13764</link><description>&lt;p&gt;
&#24102;&#26377;&#22270;&#21367;&#31215;&#26680;&#26426;&#22120;&#30340;&#21322;&#30417;&#30563;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Classification with Graph Convolutional Kernel Machines. (arXiv:2301.13764v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13764
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#23618;&#22270;&#21367;&#31215;&#26680;&#26426;&#22120;(GCKM)&#65292;&#36890;&#36807;&#22534;&#21472;&#22810;&#20010;&#27973;&#26680;&#26426;&#22120;&#65292;&#35813;&#26426;&#22120;&#33021;&#22815;&#22312;&#22270;&#20013;&#36827;&#34892;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#65292;&#24182;&#19988;&#22312;&#21487;&#29992;&#26631;&#31614;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20013;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#30340;&#28145;&#23618;&#22270;&#21367;&#31215;&#26680;&#26426;&#22120;(GCKM)&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#26680;&#26426;&#22120;&#26469;&#22312;&#19968;&#20010;&#19968;&#36339;&#37051;&#22495;&#20869;&#20256;&#25773;&#33410;&#28857;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;Fenchel-Young&#19981;&#31561;&#24335;&#30340;&#35270;&#35282;&#26469;&#25351;&#23450;&#21322;&#30417;&#30563;&#20998;&#31867;&#26680;&#26426;&#22120;&#12290;&#36890;&#36807;&#22534;&#21472;&#22810;&#20010;&#27973;&#26680;&#26426;&#22120;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#28145;&#24230;&#22270;&#21367;&#31215;&#26680;&#26426;&#22120;&#12290;&#22312;&#23637;&#31034;&#20102;&#26080;&#30417;&#30563;&#23618;&#21644;&#21322;&#30417;&#30563;&#23618;&#20998;&#21035;&#23545;&#24212;&#20110;&#32858;&#21512;&#33410;&#28857;&#29305;&#24449;&#30340;&#29305;&#24449;&#20540;&#38382;&#39064;&#21644;&#32447;&#24615;&#31995;&#32479;&#20043;&#21518;&#65292;&#25105;&#20204;&#22312;&#23545;&#20598;&#21464;&#37327;&#20013;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#31639;&#27861;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21516;&#36136;&#21644;&#24322;&#36136;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24403;&#21487;&#29992;&#26631;&#31614;&#24456;&#23569;&#26102;&#65292;GCKM&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a deep Graph Convolutional Kernel Machine (GCKM) for semi-supervised node classification in graphs. First, we introduce an unsupervised kernel machine propagating the node features in a one-hop neighbourhood. Then, we specify a semi-supervised classification kernel machine through the lens of the Fenchel-Young inequality. The deep graph convolutional kernel machine is obtained by stacking multiple shallow kernel machines. After showing that unsupervised and semi-supervised layer corresponds to an eigenvalue problem and a linear system on the aggregated node features, respectively, we derive an efficient end-to-end training algorithm in the dual variables. Numerical experiments demonstrate that our approach is competitive with state-of-the-art graph neural networks for homophilious and heterophilious benchmark datasets. Notably, GCKM achieves superior performance when very few labels are available.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21442;&#25968;&#21270;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21463;&#30410;&#20110;&#29109;&#27491;&#21017;&#21270;&#65292;&#22312;&#22810;&#20010;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#26410;&#30693;&#31867;&#21035;&#25968;&#37327;&#20855;&#26377;&#24378;&#22823;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.11727</link><description>&lt;p&gt;
&#22522;&#20110;&#21442;&#25968;&#21270;&#20998;&#31867;&#30340;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;:&#19968;&#20010;&#22522;&#32447;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Parametric Classification for Generalized Category Discovery: A Baseline Study. (arXiv:2211.11727v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11727
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21442;&#25968;&#21270;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21463;&#30410;&#20110;&#29109;&#27491;&#21017;&#21270;&#65292;&#22312;&#22810;&#20010;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#26410;&#30693;&#31867;&#21035;&#25968;&#37327;&#20855;&#26377;&#24378;&#22823;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#26088;&#22312;&#21033;&#29992;&#20174;&#26631;&#27880;&#26679;&#26412;&#20013;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#65292;&#22312;&#26410;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#26032;&#30340;&#31867;&#21035;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35748;&#20026;&#65292;&#21442;&#25968;&#21270;&#20998;&#31867;&#22120;&#23481;&#26131;&#23545;&#24050;&#30693;&#31867;&#21035;&#36807;&#24230;&#25311;&#21512;&#65292;&#24182;&#25903;&#25345;&#20351;&#29992;&#21322;&#30417;&#30563;k&#22343;&#20540;&#24418;&#25104;&#30340;&#38750;&#21442;&#25968;&#21270;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#21442;&#25968;&#21270;&#20998;&#31867;&#22120;&#30340;&#22833;&#36133;&#24773;&#20917;&#65292;&#39564;&#35777;&#20102;&#24403;&#26377;&#39640;&#36136;&#37327;&#30340;&#30417;&#30563;&#21487;&#29992;&#26102;&#20808;&#21069;&#30340;&#35774;&#35745;&#36873;&#25321;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#30830;&#23450;&#19981;&#21487;&#38752;&#30340;&#20266;&#26631;&#31614;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23384;&#22312;&#20004;&#31181;&#39044;&#27979;&#20559;&#24046;&#65306;&#20998;&#31867;&#22120;&#26356;&#20542;&#21521;&#20110;&#26356;&#39057;&#32321;&#22320;&#39044;&#27979;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#22312;&#24050;&#30693;&#21644;&#26032;&#39062;&#31867;&#21035;&#20043;&#38388;&#20135;&#29983;&#19968;&#20010;&#19981;&#24179;&#34913;&#30340;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21442;&#25968;&#21270;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#21463;&#30410;&#20110;&#29109;&#27491;&#21017;&#21270;&#65292;&#22312;&#22810;&#20010;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#31034;&#23545;&#26410;&#30693;&#31867;&#21035;&#25968;&#37327;&#20855;&#26377;&#24378;&#22823;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#30740;&#31350;&#25104;&#26524;&#33021;&#20026;&#26410;&#26469;&#26356;&#26377;&#25928;&#30340;GCD&#26041;&#27861;&#30340;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized Category Discovery (GCD) aims to discover novel categories in unlabelled datasets using knowledge learned from labelled samples. Previous studies argued that parametric classifiers are prone to overfitting to seen categories, and endorsed using a non-parametric classifier formed with semi-supervised k-means. However, in this study, we investigate the failure of parametric classifiers, verify the effectiveness of previous design choices when high-quality supervision is available, and identify unreliable pseudo-labels as a key problem. We demonstrate that two prediction biases exist: the classifier tends to predict seen classes more often, and produces an imbalanced distribution across seen and novel categories. Based on these findings, we propose a simple yet effective parametric classification method that benefits from entropy regularisation, achieves state-of-the-art performance on multiple GCD benchmarks and shows strong robustness to unknown class numbers. We hope the in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36335;&#24452;&#30456;&#20851;&#30340;&#31070;&#32463;&#36339;&#36291;ODE&#23545;&#36890;&#29992;&#21160;&#21147;&#23398;&#36827;&#34892;&#26368;&#20248;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25903;&#25345;&#20102;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#38750;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#21644;&#38480;&#20215;&#35746;&#21333;&#31807;&#25968;&#25454;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2206.14284</link><description>&lt;p&gt;
&#20351;&#29992;&#36335;&#24452;&#30456;&#20851;&#30340;&#31070;&#32463;&#36339;&#36291;ODE&#23545;&#36890;&#29992;&#21160;&#21147;&#23398;&#36827;&#34892;&#26368;&#20248;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Optimal Estimation of Generic Dynamics by Path-Dependent Neural Jump ODEs. (arXiv:2206.14284v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36335;&#24452;&#30456;&#20851;&#30340;&#31070;&#32463;&#36339;&#36291;ODE&#23545;&#36890;&#29992;&#21160;&#21147;&#23398;&#36827;&#34892;&#26368;&#20248;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25903;&#25345;&#20102;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#38750;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#21644;&#38480;&#20215;&#35746;&#21333;&#31807;&#25968;&#25454;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#36339;&#36291;ODE&#65288;NJ-ODE&#65289;&#26694;&#26550;&#30340;&#36335;&#24452;&#30456;&#20851;&#25193;&#23637;&#26469;&#39044;&#27979;&#19968;&#33324;&#38543;&#26426;&#36807;&#31243;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;NJ-ODE&#26159;&#31532;&#19968;&#20010;&#24314;&#31435;&#36215;&#38024;&#23545;&#19981;&#35268;&#21017;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;&#26694;&#26550;&#65292;&#20294;&#36825;&#20123;&#32467;&#26524;&#20165;&#36866;&#29992;&#20110;&#26469;&#33258;&#20855;&#26377;&#23436;&#25972;&#35266;&#27979;&#30340;It\^o&#25193;&#25955;&#30340;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#25152;&#26377;&#22352;&#26631;&#21516;&#26102;&#35266;&#27979;&#21040;&#30340;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#31614;&#21517;&#21464;&#25442;&#30340;&#37325;&#26500;&#24615;&#36136;&#23558;&#36825;&#20123;&#32467;&#26524;&#25512;&#24191;&#21040;&#36890;&#29992;&#30340;&#12289;&#21487;&#33021;&#26159;&#38750;&#39532;&#23572;&#21487;&#22827;&#25110;&#19981;&#36830;&#32493;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25903;&#25345;&#20102;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#65292;&#22312;&#38750;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36335;&#24452;&#30456;&#20851;&#30340;NJ-ODE&#20248;&#20110;&#21407;&#22987;NJ-ODE&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;PD-NJ-ODE&#21487;&#20197;&#25104;&#21151;&#24212;&#29992;&#20110;&#32463;&#20856;&#30340;&#38543;&#26426;&#28388;&#27874;&#38382;&#39064;&#21644;&#38480;&#20215;&#35746;&#21333;&#31807;&#65288;LOB&#65289;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of forecasting general stochastic processes using a path-dependent extension of the Neural Jump ODE (NJ-ODE) framework. While NJ-ODE was the first framework to establish convergence guarantees for the prediction of irregularly observed time series, these results were limited to data stemming from It\^o-diffusions with complete observations, in particular Markov processes where all coordinates are observed simultaneously. In this work, we generalise these results to generic, possibly non-Markovian or discontinuous, stochastic processes with incomplete observations, by utilising the reconstruction properties of the signature transform. These theoretical results are supported by empirical studies, where it is shown that the path-dependent NJ-ODE outperforms the original NJ-ODE framework in the case of non-Markovian data. Moreover, we show that PD-NJ-ODE can be applied successfully to classical stochastic filtering problems and to limit order book (LOB) data.
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#29289;&#29702;&#22686;&#24378;&#30340;&#28145;&#24230;&#20195;&#29702;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#20302;&#20445;&#30495;&#24230;&#30340;&#29289;&#29702;&#27169;&#25311;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#22120;&#26469;&#24320;&#21457;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#24555;&#36895;&#20195;&#29702;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#31934;&#30830;&#24615;&#21644;&#25104;&#26412;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2111.05841</link><description>&lt;p&gt;
&#29289;&#29702;&#22686;&#24378;&#30340;&#28145;&#24230;&#20195;&#29702;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Physics-enhanced deep surrogates for PDEs. (arXiv:2111.05841v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.05841
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#29289;&#29702;&#22686;&#24378;&#30340;&#28145;&#24230;&#20195;&#29702;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#20302;&#20445;&#30495;&#24230;&#30340;&#29289;&#29702;&#27169;&#25311;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#22120;&#26469;&#24320;&#21457;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#24555;&#36895;&#20195;&#29702;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#31934;&#30830;&#24615;&#21644;&#25104;&#26412;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29289;&#29702;&#21644;&#24037;&#31243;&#24212;&#29992;&#38656;&#35201;&#20256;&#32479;&#19978;&#29992;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#39640;&#20445;&#30495;&#25968;&#20540;&#27714;&#35299;&#22120;&#35745;&#31639;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#23646;&#24615;&#35780;&#20272;&#12290;&#25968;&#25454;&#39537;&#21160;&#30340;&#20195;&#29702;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#35757;&#32451;&#25104;&#26412;&#24456;&#39640;&#12290;&#26032;&#20852;&#24212;&#29992;&#23558;&#33719;&#30410;&#20110;&#20855;&#26377;&#25913;&#36827;&#30340;&#20934;&#30830;&#24615;-&#25104;&#26412;&#24179;&#34913;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#22823;&#35268;&#27169;&#19978;&#36827;&#34892;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;"&#29289;&#29702;&#22686;&#24378;&#30340;&#28145;&#24230;&#20195;&#29702;"&#65288;"PEDS"&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#24320;&#21457;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#24555;&#36895;&#20195;&#29702;&#27169;&#22411;&#65292;&#35813;&#31995;&#32479;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#25551;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25552;&#20986;&#20102;&#20302;&#20445;&#30495;&#24230;&#21487;&#35299;&#37322;&#30340;&#29289;&#29702;&#27169;&#25311;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#22120;&#30340;&#32452;&#21512;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#20840;&#23616;&#21305;&#37197;&#26114;&#36149;&#39640;&#20445;&#30495;&#25968;&#20540;&#27714;&#35299;&#22120;&#30340;&#36755;&#20986;&#12290;&#22312;&#25193;&#25955;&#12289;&#21453;&#24212;&#25193;&#25955;&#21644;&#30005;&#30913;&#25955;&#23556;&#27169;&#22411;&#30340;&#19977;&#20010;&#31034;&#20363;&#27979;&#35797;&#29992;&#20363;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PEDS&#20195;&#29702;&#27604;&#19968;&#20010;&#20363;&#23376;&#21152;&#19978;&#30340;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many physics and engineering applications demand Partial Differential Equations (PDE) property evaluations that are traditionally computed with resource-intensive high-fidelity numerical solvers. Data-driven surrogate models provide an efficient alternative but come with a significant cost of training. Emerging applications would benefit from surrogates with an improved accuracy-cost tradeoff, while studied at scale. Here we present a "physics-enhanced deep-surrogate" ("PEDS") approach towards developing fast surrogate models for complex physical systems, which is described by PDEs. Specifically, a combination of a low-fidelity, explainable physics simulator and a neural network generator is proposed, which is trained end-to-end to globally match the output of an expensive high-fidelity numerical solver. Experiments on three exemplar testcases, diffusion, reaction-diffusion, and electromagnetic scattering models, show that a PEDS surrogate can be up to 3$\times$ more accurate than an e
&lt;/p&gt;</description></item></channel></rss>