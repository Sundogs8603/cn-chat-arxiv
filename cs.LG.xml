<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#25913;&#36827;&#20102;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#27861;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.01223</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#25193;&#25955;&#27169;&#22411;: &#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Reinforcement Learning: A Survey. (arXiv:2311.01223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01223
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#25913;&#36827;&#20102;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#27861;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#24050;&#32463;&#20986;&#29616;&#65292;&#36229;&#36234;&#20102;&#20197;&#24448;&#26041;&#27861;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#21253;&#25324;&#20316;&#20026;&#36712;&#36857;&#35268;&#21010;&#22120;&#12289;&#34920;&#36798;&#33021;&#21147;&#20016;&#23500;&#30340;&#31574;&#30053;&#31867;&#21035;&#12289;&#25968;&#25454;&#21512;&#25104;&#22120;&#31561;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#35813;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#24076;&#26395;&#33021;&#21551;&#21457;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#24403;&#21069;RL&#31639;&#27861;&#36935;&#21040;&#30340;&#19968;&#20123;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#25193;&#25955;&#27169;&#22411;&#22312;RL&#20013;&#25152;&#25198;&#28436;&#30340;&#35282;&#33394;&#65292;&#25552;&#20986;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#35299;&#20915;&#29616;&#26377;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#27010;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#31181;&#19982;RL&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#39033;&#32508;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35265;&#35299;&#65292;&#37325;&#28857;&#26159;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as a prominent class of generative models, surpassing previous methods regarding sample quality and training stability. Recent works have shown the advantages of diffusion models in improving reinforcement learning (RL) solutions, including as trajectory planners, expressive policy classes, data synthesizers, etc. This survey aims to provide an overview of the advancements in this emerging field and hopes to inspire new avenues of research. First, we examine several challenges encountered by current RL algorithms. Then, we present a taxonomy of existing methods based on the roles played by diffusion models in RL and explore how the existing challenges are addressed. We further outline successful applications of diffusion models in various RL-related tasks while discussing the limitations of current approaches. Finally, we conclude the survey and offer insights into future research directions, focusing on enhancing model performance and applying diffusion m
&lt;/p&gt;</description></item><item><title>JADE&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#26512;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;&#65292;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#20013;&#25991;&#21644;&#33521;&#25991;LLM&#65292;&#24182;&#29983;&#25104;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.00286</link><description>&lt;p&gt;
JADE&#65306;&#22522;&#20110;&#35821;&#35328;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
JADE: A Linguistic-based Safety Evaluation Platform for LLM. (arXiv:2311.00286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00286
&lt;/p&gt;
&lt;p&gt;
JADE&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#26512;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;&#65292;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#20013;&#25991;&#21644;&#33521;&#25991;LLM&#65292;&#24182;&#29983;&#25104;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;JADE&#65292;&#19968;&#31181;&#38024;&#23545;&#35821;&#35328;&#20998;&#26512;&#30340;&#27169;&#31946;&#27979;&#35797;&#24179;&#21488;&#65292;&#36890;&#36807;&#22686;&#24378;&#31181;&#23376;&#38382;&#39064;&#30340;&#35821;&#35328;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#24182;&#22987;&#32456;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#19977;&#31867;LLM&#65306;&#20843;&#20010;&#24320;&#28304;&#20013;&#25991;LLM&#65292;&#20845;&#20010;&#21830;&#19994;&#20013;&#25991;LLM&#21644;&#22235;&#20010;&#21830;&#19994;&#33521;&#25991;LLM&#12290;JADE&#20026;&#36825;&#19977;&#31867;LLM&#29983;&#25104;&#20102;&#19977;&#20010;&#23433;&#20840;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#65306;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#21516;&#26102;&#35302;&#21457;&#22810;&#20010;LLM&#30340;&#26377;&#23475;&#29983;&#25104;&#65292;&#24179;&#22343;&#19981;&#23433;&#20840;&#29983;&#25104;&#27604;&#20363;&#20026;70%&#65288;&#35831;&#21442;&#35265;&#19979;&#34920;&#65289;&#65292;&#21516;&#26102;&#36825;&#20123;&#38382;&#39064;&#20173;&#28982;&#26159;&#33258;&#28982;&#12289;&#27969;&#30021;&#19988;&#20445;&#30041;&#20102;&#26680;&#24515;&#30340;&#19981;&#23433;&#20840;&#35821;&#20041;&#12290;&#25105;&#20204;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#21457;&#24067;&#20102;&#23545;&#21830;&#19994;&#33521;&#25991;LLM&#21644;&#24320;&#28304;&#33521;&#25991;LLM&#29983;&#25104;&#30340;&#22522;&#20934;&#28436;&#31034;&#65306;https://github.com/whitzard-ai/jade-db&#12290;&#23545;&#20110;&#23545;JADE&#29983;&#25104;&#30340;&#26356;&#22810;&#38382;&#39064;&#24863;&#20852;&#36259;&#30340;&#35835;&#32773;&#65292;&#35831;&#19982;&#25105;&#20204;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present \textit{JADE}, a targeted linguistic fuzzing platform which strengthens the linguistic complexity of seed questions to simultaneously and consistently break a wide range of widely-used LLMs categorized in three groups: eight open-sourced Chinese, six commercial Chinese and four commercial English LLMs. JADE generates three safety benchmarks for the three groups of LLMs, which contain unsafe questions that are highly threatening: the questions simultaneously trigger harmful generation of multiple LLMs, with an average unsafe generation ratio of \textbf{$70\%$} (please see the table below), while are still natural questions, fluent and preserving the core unsafe semantics. We release the benchmark demos generated for commercial English LLMs and open-sourced English LLMs in the following link: https://github.com/whitzard-ai/jade-db. For readers who are interested in evaluating on more questions generated by JADE, please contact us.  \textit{JADE} is based on Noam
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38477;&#27700;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21306;&#22495;&#38477;&#27700;&#26657;&#27491;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20187</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#29992;&#20110;&#38477;&#27700;&#21518;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Pre-training for Precipitation Post-processor. (arXiv:2310.20187v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38477;&#27700;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21306;&#22495;&#38477;&#27700;&#26657;&#27491;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#39044;&#38450;&#21361;&#38505;&#22825;&#27668;&#20107;&#20214;&#65292;&#30830;&#20445;&#20805;&#36275;&#30340;&#23616;&#22320;&#38477;&#27700;&#39044;&#25253;&#25552;&#21069;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20840;&#29699;&#21464;&#26262;&#24341;&#36215;&#30340;&#27668;&#20505;&#21464;&#21270;&#22686;&#21152;&#20102;&#20934;&#30830;&#39044;&#27979;&#20005;&#37325;&#38477;&#27700;&#20107;&#20214;&#65288;&#22914;&#26292;&#38632;&#65289;&#30340;&#25361;&#25112;&#12290;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38477;&#27700;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#27169;&#22411;&#12290;&#38477;&#27700;&#21518;&#22788;&#29702;&#21253;&#25324;&#65288;i&#65289;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#30340;&#21442;&#25968;&#22312;&#22823;&#27668;&#29289;&#29702;&#39046;&#22495;&#30340;&#36974;&#34109;&#21464;&#37327;&#37325;&#26500;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#20174;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#20013;&#36716;&#31227;&#23398;&#20064;&#21040;&#38477;&#27700;&#20998;&#21106;&#20219;&#21153;&#65288;&#30446;&#26631;&#39046;&#22495;&#65289;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26631;&#35760;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#21306;&#22495;NWP&#20013;&#30340;&#38477;&#27700;&#26657;&#27491;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Securing sufficient forecast lead time for local precipitation is essential for preventing hazardous weather events. Nonetheless, global warming-induced climate change is adding to the challenge of accurately predicting severe precipitation events, such as heavy rainfall. In this work, we propose a deep learning-based precipitation post-processor approach to numerical weather prediction (NWP) models. The precipitation post-processor consists of (i) self-supervised pre-training, where parameters of encoder are pre-trained on the reconstruction of masked variables of the atmospheric physics domain, and (ii) transfer learning on precipitation segmentation tasks (target domain) from the pre-trained encoder. We also introduce a heuristic labeling approach for effectively training class-imbalanced datasets. Our experiment results in precipitation correction for regional NWP show that the proposed method outperforms other approaches.
&lt;/p&gt;</description></item><item><title>MGAS&#26159;&#19968;&#20010;&#22810;&#31890;&#24230;&#26550;&#26500;&#25628;&#32034;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#29305;&#23450;&#31890;&#24230;&#32423;&#21035;&#30340;&#31163;&#25955;&#21270;&#20989;&#25968;&#65292;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#21097;&#20313;&#27604;&#20363;&#65292;&#20174;&#32780;&#23454;&#29616;&#21516;&#26102;&#20248;&#21270;&#27169;&#22411;&#22823;&#23567;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15074</link><description>&lt;p&gt;
MGAS: &#22810;&#31890;&#24230;&#26550;&#26500;&#25628;&#32034;&#20197;&#23454;&#29616;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MGAS: Multi-Granularity Architecture Search for Effective and Efficient Neural Networks. (arXiv:2310.15074v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15074
&lt;/p&gt;
&lt;p&gt;
MGAS&#26159;&#19968;&#20010;&#22810;&#31890;&#24230;&#26550;&#26500;&#25628;&#32034;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#29305;&#23450;&#31890;&#24230;&#32423;&#21035;&#30340;&#31163;&#25955;&#21270;&#20989;&#25968;&#65292;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#21097;&#20313;&#27604;&#20363;&#65292;&#20174;&#32780;&#23454;&#29616;&#21516;&#26102;&#20248;&#21270;&#27169;&#22411;&#22823;&#23567;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#26550;&#26500;&#25628;&#32034;(DAS)&#36890;&#36807;&#26102;&#38388;&#39640;&#25928;&#30340;&#33258;&#21160;&#21270;&#25913;&#21464;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;(NAS)&#30340;&#26041;&#24335;&#65292;&#20174;&#31163;&#25955;&#20505;&#36873;&#37319;&#26679;&#21644;&#35780;&#20272;&#36716;&#21464;&#20026;&#21487;&#24494;&#20998;&#36229;&#32593;&#32476;&#20248;&#21270;&#21644;&#31163;&#25955;&#21270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DAS&#26041;&#27861;&#35201;&#20040;&#21482;&#36827;&#34892;&#31895;&#31890;&#24230;&#30340;&#25805;&#20316;&#32423;&#25628;&#32034;&#65292;&#35201;&#20040;&#25163;&#21160;&#23450;&#20041;&#21097;&#20313;&#30340;&#32454;&#31890;&#24230;&#30340;&#26680;&#32423;&#21644;&#26435;&#37325;&#32423;&#21333;&#20301;&#30340;&#27604;&#20363;&#65292;&#20174;&#32780;&#26080;&#27861;&#21516;&#26102;&#20248;&#21270;&#27169;&#22411;&#22823;&#23567;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20026;&#20102;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#32780;&#29306;&#29298;&#20102;&#25628;&#32034;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#31890;&#24230;&#26550;&#26500;&#25628;&#32034;(MGAS)&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20840;&#38754;&#32780;&#20869;&#23384;&#39640;&#25928;&#22320;&#25506;&#32034;&#22810;&#31890;&#24230;&#25628;&#32034;&#31354;&#38388;&#65292;&#21457;&#29616;&#26082;&#26377;&#25928;&#21448;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#38024;&#23545;&#27599;&#20010;&#31890;&#24230;&#32423;&#21035;&#30340;&#31163;&#25955;&#21270;&#20989;&#25968;&#65292;&#26681;&#25454;&#19981;&#26029;&#28436;&#21270;&#30340;&#26550;&#26500;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#21097;&#20313;&#30340;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable architecture search (DAS) revolutionizes neural architecture search (NAS) with time-efficient automation, transitioning from discrete candidate sampling and evaluation to differentiable super-net optimization and discretization. However, existing DAS methods either only conduct coarse-grained operation-level search or manually define the remaining ratios for fine-grained kernel-level and weight-level units, which fail to simultaneously optimize model size and model performance. Furthermore, these methods compromise search quality to reduce memory consumption. To tackle these issues, we introduce multi-granularity architecture search (MGAS), a unified framework which aims to comprehensively and memory-efficiently explore the multi-granularity search space to discover both effective and efficient neural networks. Specifically, we learn discretization functions specific to each granularity level to adaptively determine the remaining ratios according to the evolving architec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#20027;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#19981;&#21516;&#20110;&#29616;&#26377;&#23450;&#20041;&#30340;&#20960;&#20046;&#31561;&#21464;&#24615;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#26446;&#32676;&#30340;&#26446;&#20195;&#25968;&#32473;&#20986;&#20102;&#22312;&#27169;&#22411;&#20013;&#32534;&#30721;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.13164</link><description>&lt;p&gt;
&#20960;&#20046;&#31561;&#21464;&#24615;&#36890;&#36807;&#26446;&#20195;&#25968;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Almost Equivariance via Lie Algebra Convolutions. (arXiv:2310.13164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#20027;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#19981;&#21516;&#20110;&#29616;&#26377;&#23450;&#20041;&#30340;&#20960;&#20046;&#31561;&#21464;&#24615;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#26446;&#32676;&#30340;&#26446;&#20195;&#25968;&#32473;&#20986;&#20102;&#22312;&#27169;&#22411;&#20013;&#32534;&#30721;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#30456;&#23545;&#20110;&#32676;&#20316;&#29992;&#30340;&#31561;&#21464;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#36171;&#20104;&#19968;&#20010;&#26550;&#26500;&#20855;&#20307;&#30340;&#32676;&#31561;&#21464;&#24615;&#23545;&#27169;&#22411;&#25152;&#26399;&#26395;&#30475;&#21040;&#30340;&#25968;&#25454;&#21464;&#25442;&#31867;&#22411;&#26045;&#21152;&#20102;&#24378;&#22823;&#30340;&#20808;&#39564;&#12290;&#20005;&#26684;&#31561;&#21464;&#27169;&#22411;&#24378;&#21046;&#25191;&#34892;&#23545;&#31216;&#24615;&#65292;&#20294;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#24182;&#19981;&#24635;&#26159;&#31526;&#21512;&#36825;&#26679;&#30340;&#20005;&#26684;&#31561;&#21464;&#24615;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#25110;&#20165;&#32534;&#30721;&#20102;&#36817;&#20284;&#25110;&#37096;&#20998;&#23545;&#31216;&#24615;&#30340;&#28508;&#22312;&#29289;&#29702;&#23450;&#24459;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20005;&#26684;&#31561;&#21464;&#24615;&#30340;&#20808;&#39564;&#23454;&#38469;&#19978;&#21487;&#33021;&#36807;&#20110;&#24378;&#22823;&#65292;&#23548;&#33268;&#27169;&#22411;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#30456;&#20851;&#30340;&#20027;&#39064;&#65292;&#21363;&#20960;&#20046;&#31561;&#21464;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19982;&#24403;&#21069;&#25991;&#29486;&#20013;&#29616;&#26377;&#23450;&#20041;&#19981;&#21516;&#30340;&#20960;&#20046;&#31561;&#21464;&#24615;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#26446;&#32676;&#30340;&#26446;&#20195;&#25968;&#32473;&#20986;&#20102;&#22312;&#27169;&#22411;&#20013;&#32534;&#30721;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the equivariance of models with respect to a group action has become an important topic of research in machine learning. However, imbuing an architecture with a specific group equivariance imposes a strong prior on the types of data transformations that the model expects to see. While strictly-equivariant models enforce symmetries, real-world data does not always conform to such strict equivariances, be it due to noise in the data or underlying physical laws that encode only approximate or partial symmetries. In such cases, the prior of strict equivariance can actually prove too strong and cause models to underperform on real-world data. Therefore, in this work we study a closely related topic, that of almost equivariance. We provide a definition of almost equivariance that differs from those extant in the current literature and give a practical method for encoding almost equivariance in models by appealing to the Lie algebra of a Lie group. Specifically, we define Lie algebr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#65292;&#20855;&#26377;&#31283;&#20581;&#24615;&#21644;&#22810;&#27169;&#24577;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12609</link><description>&lt;p&gt;
&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Denoising Heat-inspired Diffusion with Insulators for Collision Free Motion Planning. (arXiv:2310.12609v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#65292;&#20855;&#26377;&#31283;&#20581;&#24615;&#21644;&#22810;&#27169;&#24577;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#22810;&#27169;&#24577;&#24615;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;&#23613;&#31649;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22797;&#26434;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20005;&#37325;&#20381;&#36182;&#20110;&#25512;&#29702;&#26102;&#30340;&#38556;&#30861;&#29289;&#26816;&#27979;&#24182;&#38656;&#35201;&#39069;&#22806;&#30340;&#35774;&#22791;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#20174;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#20013;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26032;&#39062;&#30340;&#30896;&#25758;&#36991;&#20813;&#25193;&#25955;&#26680;&#36827;&#34892;&#20351;&#29992;&#12290;&#36890;&#36807;&#19982;&#34892;&#20026;&#20811;&#38534;&#21644;&#32463;&#20856;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#35777;&#26126;&#20102;&#20854;&#31283;&#20581;&#24615;&#12290;&#29305;&#21035;&#26159;&#22312;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#65292;&#23427;&#33021;&#22815;&#23548;&#33322;&#21040;&#30446;&#26631;&#24182;&#36991;&#24320;&#34987;&#38556;&#30861;&#29289;&#38459;&#25377;&#30340;&#19981;&#21487;&#36798;&#30446;&#26631;&#65292;&#21516;&#26102;&#30830;&#20445;&#36991;&#20813;&#30896;&#25758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have risen as a powerful tool in robotics due to their flexibility and multi-modality. While some of these methods effectively address complex problems, they often depend heavily on inference-time obstacle detection and require additional equipment. Addressing these challenges, we present a method that, during inference time, simultaneously generates only reachable goals and plans motions that avoid obstacles, all from a single visual input. Central to our approach is the novel use of a collision-avoiding diffusion kernel for training. Through evaluations against behavior-cloning and classical diffusion models, our framework has proven its robustness. It is particularly effective in multi-modal environments, navigating toward goals and avoiding unreachable ones blocked by obstacles, while ensuring collision avoidance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Quantum Gramian Angular Field (QGAF)&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25104;&#21151;&#23558;&#32929;&#31080;&#22238;&#25253;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36716;&#21270;&#20026;&#36866;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#35757;&#32451;&#30340;&#20108;&#32500;&#22270;&#20687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;QGAF&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.07427</link><description>&lt;p&gt;
&#22686;&#24378;&#37327;&#23376;&#39044;&#27979;&#33021;&#21147;&#65306;&#21033;&#29992;&#37327;&#23376;Gramian&#35282;&#24230;&#22330;&#21644;CNN&#36827;&#34892;&#32929;&#31080;&#22238;&#25253;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field and CNNs for Stock Return Predictions. (arXiv:2310.07427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07427
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Quantum Gramian Angular Field (QGAF)&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25104;&#21151;&#23558;&#32929;&#31080;&#22238;&#25253;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36716;&#21270;&#20026;&#36866;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#35757;&#32451;&#30340;&#20108;&#32500;&#22270;&#20687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;QGAF&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Quantum Gramian Angular Field (QGAF)&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;&#36890;&#36807;&#35774;&#35745;&#29305;&#23450;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#32929;&#31080;&#22238;&#25253;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36716;&#21270;&#20026;&#36866;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#35757;&#32451;&#30340;&#20108;&#32500;&#22270;&#20687;&#12290;&#19982;&#32463;&#20856;&#30340;Gramian Angular Field (GAF)&#26041;&#27861;&#19981;&#21516;&#65292;QGAF&#30340;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#28040;&#38500;&#20102;&#25968;&#25454;&#24402;&#19968;&#21270;&#21644;&#21453;&#20313;&#24358;&#35745;&#31639;&#30340;&#38656;&#27714;&#65292;&#31616;&#21270;&#20102;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21040;&#20108;&#32500;&#22270;&#20687;&#30340;&#36716;&#25442;&#36807;&#31243;&#12290;&#20026;&#20102;&#39564;&#35777;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#20013;&#22269;A&#32929;&#24066;&#22330;&#12289;&#39321;&#28207;&#32929;&#24066;&#21644;&#32654;&#22269;&#32929;&#24066;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#32463;&#20856;&#30340;GAF&#26041;&#27861;&#30456;&#27604;&#65292;QGAF&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a time series forecasting method named Quantum Gramian Angular Field (QGAF). This approach merges the advantages of quantum computing technology with deep learning, aiming to enhance the precision of time series classification and forecasting. We successfully transformed stock return time series data into two-dimensional images suitable for Convolutional Neural Network (CNN) training by designing specific quantum circuits. Distinct from the classical Gramian Angular Field (GAF) approach, QGAF's uniqueness lies in eliminating the need for data normalization and inverse cosine calculations, simplifying the transformation process from time series data to two-dimensional images. To validate the effectiveness of this method, we conducted experiments on datasets from three major stock markets: the China A-share market, the Hong Kong stock market, and the US stock market. Experimental results revealed that compared to the classical GAF method, the QGAF approach significantly improv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#26377;&#38480;&#24773;&#20917;&#19979;&#65292;&#23545;&#39640;&#32500;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#24418;&#29366;&#36317;&#31163;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25512;&#23548;&#20986;&#23545;&#24418;&#29366;&#36317;&#31163;&#26631;&#20934;&#20272;&#35745;&#22120;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#19978;&#19979;&#30028;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#25361;&#25112;&#24615;&#36136;&#12290;&#20026;&#20102;&#20811;&#26381;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#30697;&#27861;&#20272;&#35745;&#22120;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#39640;&#32500;&#35774;&#32622;&#19979;&#30456;&#23545;&#20110;&#26631;&#20934;&#20272;&#35745;&#22120;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05742</link><description>&lt;p&gt;
&#26377;&#38480;&#37319;&#26679;&#19979;&#31070;&#32463;&#34920;&#31034;&#30340;&#24418;&#29366;&#36317;&#31163;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimating Shape Distances on Neural Representations with Limited Samples. (arXiv:2310.05742v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#26377;&#38480;&#24773;&#20917;&#19979;&#65292;&#23545;&#39640;&#32500;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#24418;&#29366;&#36317;&#31163;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25512;&#23548;&#20986;&#23545;&#24418;&#29366;&#36317;&#31163;&#26631;&#20934;&#20272;&#35745;&#22120;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#19978;&#19979;&#30028;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#25361;&#25112;&#24615;&#36136;&#12290;&#20026;&#20102;&#20811;&#26381;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#30697;&#27861;&#20272;&#35745;&#22120;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#39640;&#32500;&#35774;&#32622;&#19979;&#30456;&#23545;&#20110;&#26631;&#20934;&#20272;&#35745;&#22120;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#31185;&#23398;&#21644;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#65292;&#34913;&#37327;&#39640;&#32500;&#32593;&#32476;&#34920;&#31034;&#20043;&#38388;&#30340;&#20960;&#20309;&#30456;&#20284;&#24615;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#24037;&#20316;&#23545;&#23427;&#20204;&#30340;&#32479;&#35745;&#25928;&#29575;&#36827;&#34892;&#20102;&#20005;&#26684;&#20998;&#26512;&#65292;&#25110;&#32773;&#23545;&#25968;&#25454;&#26377;&#38480;&#24773;&#20917;&#19979;&#30340;&#20272;&#35745;&#22120;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20102;&#37327;&#21270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#26631;&#20934;&#24418;&#29366;&#36317;&#31163;&#20272;&#35745;&#22120;&#65288;&#30001;Williams et al. (2021)&#25552;&#20986;&#65289;&#30340;&#26368;&#22351;&#24773;&#20917;&#25910;&#25947;&#19978;&#19979;&#30028;&#12290;&#36825;&#20123;&#30028;&#38480;&#25581;&#31034;&#20102;&#22312;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#20013;&#36825;&#20010;&#38382;&#39064;&#30340;&#25361;&#25112;&#24615;&#36136;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#30697;&#27861;&#20272;&#35745;&#22120;&#65292;&#20855;&#26377;&#21487;&#35843;&#30340;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#20272;&#35745;&#22120;&#22312;&#27169;&#25311;&#21644;&#31070;&#32463;&#25968;&#25454;&#19978;&#30456;&#23545;&#20110;&#26631;&#20934;&#20272;&#35745;&#22120;&#22312;&#39640;&#32500;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20026;&#39640;&#32500;&#24418;&#29366;&#20998;&#26512;&#22880;&#23450;&#20102;&#20005;&#26684;&#30340;&#32479;&#35745;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring geometric similarity between high-dimensional network representations is a topic of longstanding interest to neuroscience and deep learning. Although many methods have been proposed, only a few works have rigorously analyzed their statistical efficiency or quantified estimator uncertainty in data-limited regimes. Here, we derive upper and lower bounds on the worst-case convergence of standard estimators of shape distance$\unicode{x2014}$a measure of representational dissimilarity proposed by Williams et al. (2021). These bounds reveal the challenging nature of the problem in high-dimensional feature spaces. To overcome these challenges, we introduce a new method-of-moments estimator with a tunable bias-variance tradeoff. We show that this estimator achieves superior performance to standard estimators in simulation and on neural data, particularly in high-dimensional settings. Thus, we lay the foundation for a rigorous statistical theory for high-dimensional shape analysis, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#20013;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#65292;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05365</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Molecular De Novo Design through Transformer-based Reinforcement Learning. (arXiv:2310.05365v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#20013;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#65292;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#21033;&#29992;Transformer&#30456;&#23545;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#20248;&#36234;&#24207;&#21015;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;RNN&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#39044;&#27979;&#23545;&#22810;&#31181;&#29983;&#29289;&#38774;&#28857;&#20855;&#26377;&#27963;&#24615;&#30340;&#21270;&#21512;&#29289;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#25429;&#25417;&#20102;&#20998;&#23376;&#32467;&#26500;&#24207;&#21015;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#21253;&#25324;&#29983;&#25104;&#19982;&#26597;&#35810;&#32467;&#26500;&#31867;&#20284;&#30340;&#20998;&#23376;&#21644;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#21270;&#21512;&#29289;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#22522;&#32447;&#30340;&#22522;&#20110;RNN&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#26725;&#25509;&#21270;&#23398;&#12289;&#20174;&#21333;&#20010;&#20998;&#23376;&#24320;&#22987;&#25193;&#23637;&#24211;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#39640;&#39044;&#27979;&#27963;&#24615;&#30340;&#21270;&#21512;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a method to fine-tune a Transformer-based generative model for molecular de novo design. Leveraging the superior sequence learning capacity of Transformers over Recurrent Neural Networks (RNNs), our model can generate molecular structures with desired properties effectively. In contrast to the traditional RNN-based models, our proposed method exhibits superior performance in generating compounds predicted to be active against various biological targets, capturing long-term dependencies in the molecular structure sequence. The model's efficacy is demonstrated across numerous tasks, including generating analogues to a query structure and producing compounds with particular attributes, outperforming the baseline RNN-based methods. Our approach can be used for scaffold hopping, library expansion starting from a single molecule, and generating compounds with high predicted activity against biological targets.
&lt;/p&gt;</description></item><item><title>COPRA&#26159;&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#36873;&#25321;&#31574;&#30053;&#21644;&#26816;&#32034;&#23450;&#20041;&#21644;&#24341;&#29702;&#36827;&#34892;&#35777;&#26126;&#65292;&#22312;MiniF2F&#22522;&#20934;&#21644;Coq&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04353</link><description>&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Language-Agent Approach to Formal Theorem-Proving. (arXiv:2310.04353v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04353
&lt;/p&gt;
&lt;p&gt;
COPRA&#26159;&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#36873;&#25321;&#31574;&#30053;&#21644;&#26816;&#32034;&#23450;&#20041;&#21644;&#24341;&#29702;&#36827;&#34892;&#35777;&#26126;&#65292;&#22312;MiniF2F&#22522;&#20934;&#21644;Coq&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#19982;&#22806;&#37096;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25511;&#21046;&#20219;&#21153;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language agents, which use a large language model (LLM) capable of in-context learning to interact with an external environment, have recently emerged as a promising approach to control tasks. We present the first language-agent approach to formal theorem-proving. Our method, COPRA, uses a high-capacity, black-box LLM (GPT-4) as part of a policy for a stateful backtracking search. During the search, the policy can select proof tactics and retrieve lemmas and definitions from an external database. Each selected tactic is executed in the underlying proof framework, and the execution feedback is used to build the prompt for the next policy invocation. The search also tracks selected information from its history and uses it to reduce hallucinations and unnecessary LLM queries.  We evaluate COPRA on the miniF2F benchmark for Lean and a set of Coq tasks from the Compcert project. On these benchmarks, COPRA is significantly better than one-shot invocations of GPT-4, as well as state-of-the-ar
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#38750;&#22343;&#21248;&#37319;&#26679;&#31574;&#30053;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#27979;&#37327;&#20195;&#29702;&#30340;&#20869;&#37096;&#34920;&#31034;&#19982;&#35757;&#32451;&#23618;&#32423;&#38598;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#65292;&#21457;&#29616;&#22522;&#20110;&#20540;&#25439;&#22833;&#20248;&#20808;&#32423;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#33021;&#26356;&#22909;&#22320;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.03494</link><description>&lt;p&gt;
&#22914;&#20309;&#27700;&#24179;&#37319;&#26679;&#36807;&#31243;&#24433;&#21709;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
How the level sampling process impacts zero-shot generalisation in deep reinforcement learning. (arXiv:2310.03494v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03494
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#38750;&#22343;&#21248;&#37319;&#26679;&#31574;&#30053;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#27979;&#37327;&#20195;&#29702;&#30340;&#20869;&#37096;&#34920;&#31034;&#19982;&#35757;&#32451;&#23618;&#32423;&#38598;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#65292;&#21457;&#29616;&#22522;&#20110;&#20540;&#25439;&#22833;&#20248;&#20808;&#32423;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#33021;&#26356;&#22909;&#22320;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38459;&#27490;&#24191;&#27867;&#37319;&#29992;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35757;&#32451;&#30340;&#33258;&#20027;&#20195;&#29702;&#20195;&#29702;&#30340;&#20851;&#38190;&#23616;&#38480;&#26159;&#23427;&#20204;&#26377;&#38480;&#30340;&#36866;&#24212;&#26032;&#29615;&#22659;&#33021;&#21147;&#65292;&#21363;&#20351;&#36825;&#20123;&#29615;&#22659;&#19982;&#35757;&#32451;&#20013;&#36935;&#21040;&#30340;&#29615;&#22659;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#20307;&#29615;&#22659;&#23454;&#20363;&#25110;&#23618;&#32423;&#30340;&#38750;&#22343;&#21248;&#37319;&#26679;&#31574;&#30053;&#22914;&#20309;&#24433;&#21709;RL&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#65288;ZSG&#65289;&#33021;&#21147;&#65292;&#24182;&#32771;&#34385;&#20102;&#20004;&#31181;&#22833;&#25928;&#27169;&#24335;&#65306;&#36807;&#25311;&#21512;&#21644;&#36807;&#24230;&#27867;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#27979;&#37327;&#20102;&#20195;&#29702;&#30340;&#20869;&#37096;&#34920;&#31034;&#19982;&#35757;&#32451;&#23618;&#32423;&#38598;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#65292;&#21457;&#29616;MI&#19982;&#23454;&#20363;&#30340;&#36807;&#25311;&#21512;&#30456;&#20851;&#24615;&#24456;&#24378;&#12290;&#19982;&#22343;&#21248;&#37319;&#26679;&#30456;&#27604;&#65292;&#22522;&#20110;&#20540;&#25439;&#22833;&#20248;&#20808;&#32423;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#26356;&#33021;&#26377;&#25928;&#22320;&#20445;&#25345;&#36739;&#20302;&#30340;MI&#65292;&#36825;&#20026;&#36825;&#31867;&#25216;&#26415;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#27880;&#24847;&#21147;&#36716;&#21521;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;U&#65289;
&lt;/p&gt;
&lt;p&gt;
A key limitation preventing the wider adoption of autonomous agents trained via deep reinforcement learning (RL) is their limited ability to generalise to new environments, even when these share similar characteristics with environments encountered during training. In this work, we investigate how a non-uniform sampling strategy of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents, considering two failure modes: overfitting and over-generalisation. As a first step, we measure the mutual information (MI) between the agent's internal representation and the set of training levels, which we find to be well-correlated to instance overfitting. In contrast to uniform sampling, adaptive sampling strategies prioritising levels based on their value loss are more effective at maintaining lower MI, which provides a novel theoretical justification for this class of techniques. We then turn our attention to unsupervised environment design (U
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24369;&#20998;&#24067;&#19981;&#21464;&#24615;&#36827;&#34892;&#22810;&#39046;&#22495;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#34701;&#20837;&#36825;&#31181;&#19981;&#21464;&#24615;&#30340;&#33258;&#32534;&#30721;&#22120;&#33021;&#22815;&#21487;&#38752;&#22320;&#35782;&#21035;&#20986;&#31283;&#23450;&#30340;&#21464;&#37327;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.02854</link><description>&lt;p&gt;
&#36890;&#36807;&#24369;&#20998;&#24067;&#19981;&#21464;&#24615;&#23454;&#29616;&#22810;&#39046;&#22495;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Domain Causal Representation Learning via Weak Distributional Invariances. (arXiv:2310.02854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24369;&#20998;&#24067;&#19981;&#21464;&#24615;&#36827;&#34892;&#22810;&#39046;&#22495;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#34701;&#20837;&#36825;&#31181;&#19981;&#21464;&#24615;&#30340;&#33258;&#32534;&#30721;&#22120;&#33021;&#22815;&#21487;&#38752;&#22320;&#35782;&#21035;&#20986;&#31283;&#23450;&#30340;&#21464;&#37327;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#24050;&#25104;&#20026;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#26680;&#24515;&#12290;&#29305;&#21035;&#26159;&#65292;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#20026;&#23637;&#31034;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30456;&#23545;&#20110;&#26631;&#20934;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#20248;&#21183;&#25552;&#20379;&#20102;&#33258;&#28982;&#26426;&#20250;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#36807;&#20110;&#31616;&#21270;&#25968;&#25454;&#30340;&#20551;&#35774;&#65292;&#23427;&#20204;&#24448;&#24448;&#19981;&#33021;&#36866;&#29992;&#20110;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#65307;&#20363;&#22914;&#65292;&#27599;&#20010;&#39046;&#22495;&#37117;&#26469;&#33258;&#19981;&#21516;&#30340;&#21333;&#33410;&#28857;&#23436;&#32654;&#24178;&#39044;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25918;&#23485;&#20102;&#36825;&#20123;&#20551;&#35774;&#65292;&#24182;&#21033;&#29992;&#20197;&#19979;&#35266;&#23519;&#32467;&#26524;&#65306;&#22312;&#22810;&#39046;&#22495;&#25968;&#25454;&#20013;&#65292;&#24448;&#24448;&#23384;&#22312;&#19968;&#37096;&#20998;&#28508;&#21464;&#37327;&#30340;&#26576;&#20123;&#20998;&#24067;&#23646;&#24615;&#65288;&#20363;&#22914;&#25903;&#25345;&#24230;&#12289;&#26041;&#24046;&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#20445;&#25345;&#31283;&#23450;&#65307;&#24403;&#27599;&#20010;&#39046;&#22495;&#26469;&#33258;&#22810;&#33410;&#28857;&#19981;&#23436;&#32654;&#24178;&#39044;&#26102;&#65292;&#36825;&#20010;&#23646;&#24615;&#25104;&#31435;&#12290;&#21033;&#29992;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#34701;&#20837;&#36825;&#31181;&#19981;&#21464;&#24615;&#30340;&#33258;&#32534;&#30721;&#22120;&#33021;&#22815;&#21487;&#38752;&#22320;&#35782;&#21035;&#20986;&#31283;&#23450;&#30340;&#21464;&#37327;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal representation learning has emerged as the center of action in causal machine learning research. In particular, multi-domain datasets present a natural opportunity for showcasing the advantages of causal representation learning over standard unsupervised representation learning. While recent works have taken crucial steps towards learning causal representations, they often lack applicability to multi-domain datasets due to over-simplifying assumptions about the data; e.g. each domain comes from a different single-node perfect intervention. In this work, we relax these assumptions and capitalize on the following observation: there often exists a subset of latents whose certain distributional properties (e.g., support, variance) remain stable across domains; this property holds when, for example, each domain comes from a multi-node imperfect intervention. Leveraging this observation, we show that autoencoders that incorporate such invariances can provably identify the stable set o
&lt;/p&gt;</description></item><item><title>L2MAC&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2310.02003</link><description>&lt;p&gt;
L2MAC&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35745;&#31639;&#26426;&#29992;&#20110;&#26080;&#38480;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation. (arXiv:2310.02003v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02003
&lt;/p&gt;
&lt;p&gt;
L2MAC&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21463;&#21040;&#24213;&#23618;Transformer&#26550;&#26500;&#22266;&#23450;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#22686;&#24378;&#35760;&#24518;&#30340;LLM&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#20040;&#21482;&#20851;&#27880;&#20110;&#35835;&#21462;&#20869;&#23384;&#24182;&#23558;&#20854;&#28436;&#21464;&#20026;&#26032;&#20869;&#23384;&#30340;&#36830;&#25509;&#65292;&#35201;&#20040;&#20351;&#29992;&#38750;&#24120;&#19987;&#38376;&#30340;&#20869;&#23384;&#65292;&#26080;&#27861;&#36866;&#24212;&#20854;&#20182;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;L2MAC&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#38271;&#19988;&#19968;&#33268;&#20195;&#30721;&#29983;&#25104;&#30340;&#23454;&#29992;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#12290;&#23427;&#30340;&#20869;&#23384;&#26377;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#25351;&#20196;&#27880;&#20876;&#34920;&#65292;&#20854;&#20013;&#22635;&#20805;&#20102;&#19968;&#20010;&#35299;&#20915;&#29992;&#25143;&#32473;&#23450;&#20219;&#21153;&#30340;&#25552;&#31034;&#31243;&#24207;&#65292;&#20197;&#21450;&#25991;&#20214;&#23384;&#20648;&#65292;&#20854;&#20013;&#21253;&#21547;&#26368;&#32456;&#21644;&#20013;&#38388;&#36755;&#20986;&#12290;&#27599;&#20010;&#25351;&#20196;&#30001;&#21333;&#29420;&#30340;LLM&#23454;&#20363;&#25191;&#34892;&#65292;&#20854;&#19978;&#19979;&#25991;&#30001;&#25511;&#21046;&#21333;&#20803;&#31649;&#29702;&#65292;&#33021;&#22815;&#31934;&#30830;&#35835;&#21462;&#21644;&#20889;&#20837;&#20869;&#23384;&#65292;&#20197;&#30830;&#20445;&#26377;&#25928;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and logically consistent code. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long code generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains. This paper presents L2MAC, the first practical LLM-based stored-program automatic computer for long and consistent code generation. Its memory has two components: the instruction registry, which is populated with a prompt program to solve the user-given task, and a file store, which will contain the final and intermediate outputs. Each instruction is executed by a separate LLM instance, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective inte
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25439;&#22833;&#24179;&#22374;&#24615;&#21644;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2310.01770</link><description>&lt;p&gt;
&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#32593;&#32476;&#20013;&#21387;&#32553;&#34920;&#31034;&#30340;&#31616;&#21333;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
A simple connection from loss flatness to compressed representations in neural networks. (arXiv:2310.01770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25439;&#22833;&#24179;&#22374;&#24615;&#21644;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#30740;&#31350;&#30340;&#26041;&#27861;&#26377;&#24456;&#22810;&#31181;&#65292;&#21253;&#25324;&#33267;&#23569;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#31354;&#38388;&#20013;&#25439;&#22833;&#26223;&#35266;&#30340;&#24418;&#29366;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#20013;&#34920;&#31034;&#27969;&#24418;&#30340;&#32467;&#26500;&#65288;&#21363;&#21333;&#20301;&#27963;&#21160;&#30340;&#31354;&#38388;&#65289;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#20851;&#20294;&#24456;&#23569;&#21516;&#26102;&#36827;&#34892;&#30740;&#31350;&#21644;&#26126;&#30830;&#20851;&#32852;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20998;&#26512;&#26041;&#27861;&#26469;&#24314;&#31435;&#36825;&#31181;&#32852;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#26368;&#21518;&#38454;&#27573;&#65292;&#31070;&#32463;&#34920;&#31034;&#27969;&#24418;&#30340;&#20307;&#31215;&#21387;&#32553;&#19982;&#27491;&#22312;&#36827;&#34892;&#30340;&#21442;&#25968;&#20248;&#21270;&#25152;&#25506;&#32034;&#30340;&#26368;&#23567;&#20540;&#21608;&#22260;&#30340;&#25439;&#22833;&#24179;&#22374;&#24615;&#30456;&#20851;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#21487;&#20197;&#30001;&#19968;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#26469;&#39044;&#27979;&#65306;&#25439;&#22833;&#24179;&#22374;&#24615;&#24847;&#21619;&#30528;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;\citet{ma_linear_2021}&#30340;&#20808;&#21069;&#30740;&#31350;&#23494;&#20999;&#30456;&#20851;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#24179;&#22374;&#24615;&#65288;&#21363;&#23567;&#29305;&#24449;&#20540;&#65289;&#19982;&#34920;&#31034;&#27969;&#24418;&#30340;&#20307;&#31215;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks' generalization capacity has been studied in a variety of ways, including at least two distinct categories of approach: one based on the shape of the loss landscape in parameter space, and the other based on the structure of the representation manifold in feature space (that is, in the space of unit activities). These two approaches are related, but they are rarely studied together and explicitly connected. Here, we present a simple analysis that makes such a connection. We show that, in the last phase of learning of deep neural networks, compression of the volume of the manifold of neural representations correlates with the flatness of the loss around the minima explored by ongoing parameter optimization. We show that this is predicted by a relatively simple mathematical relationship: loss flatness implies compression of neural representations. Our results build closely on prior work of \citet{ma_linear_2021}, which shows how flatness (i.e., small eigenvalues of t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21307;&#23398;&#39627;&#20851;&#33410;X&#23556;&#32447;&#22270;&#20687;&#20013;&#30340;&#26631;&#35760;&#28857;&#26816;&#27979;&#20219;&#21153;&#30340;&#26631;&#31614;&#22686;&#24378;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20165;&#26631;&#31614;&#22686;&#24378;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;&#26679;&#26412;&#21033;&#29992;&#25928;&#29575;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#25552;&#39640;&#26631;&#35760;&#28857;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16066</link><description>&lt;p&gt;
&#21307;&#23398;&#39627;&#20851;&#33410;X&#23556;&#32447;&#22270;&#20687;&#20013;&#30340;&#26631;&#35760;&#28857;&#26816;&#27979;&#30340;&#26631;&#31614;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Label Augmentation Method for Medical Landmark Detection in Hip Radiograph Images. (arXiv:2309.16066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16066
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21307;&#23398;&#39627;&#20851;&#33410;X&#23556;&#32447;&#22270;&#20687;&#20013;&#30340;&#26631;&#35760;&#28857;&#26816;&#27979;&#20219;&#21153;&#30340;&#26631;&#31614;&#22686;&#24378;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20165;&#26631;&#31614;&#22686;&#24378;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;&#26679;&#26412;&#21033;&#29992;&#25928;&#29575;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#25552;&#39640;&#26631;&#35760;&#28857;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#36947;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#39627;&#20851;&#33410;X&#23556;&#32447;&#22270;&#20687;&#20013;&#20020;&#24202;&#26631;&#35760;&#28857;&#30340;&#33258;&#21160;&#21270;&#21307;&#23398;&#26631;&#35760;&#28857;&#26816;&#27979;&#26041;&#27861;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35813;&#26816;&#27979;&#26041;&#27861;&#26159;&#20351;&#29992;&#20165;&#26631;&#31614;&#22686;&#24378;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#30340;&#65307;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#22686;&#24378;&#24418;&#24335;&#20248;&#20110;&#20256;&#32479;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#24182;&#19988;&#20135;&#29983;&#39640;&#25928;&#30340;&#26679;&#26412;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#36890;&#29992;U-Net&#26550;&#26500;&#30340;&#35838;&#31243;&#35757;&#32451;&#65292;&#35813;&#35757;&#32451;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#39318;&#20808;&#36890;&#36807;&#23558;&#26631;&#35760;&#28857;&#25193;&#22823;&#21040;&#21306;&#22495;&#26469;&#25918;&#26494;&#26631;&#35760;&#20219;&#21153;&#65292;&#28982;&#21518;&#36880;&#28176;&#23558;&#36825;&#20123;&#26631;&#31614;&#21306;&#22495;&#22238;&#24402;&#21040;&#22522;&#26412;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#21547;&#26377;&#40644;&#37329;&#26631;&#20934;&#19987;&#23478;&#27880;&#37322;&#30340;&#20845;&#20010;&#25918;&#23556;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work reports the empirical performance of an automated medical landmark detection method for predict clinical markers in hip radiograph images. Notably, the detection method was trained using a label-only augmentation scheme; our results indicate that this form of augmentation outperforms traditional data augmentation and produces highly sample efficient estimators. We train a generic U-Net-based architecture under a curriculum consisting of two phases: initially relaxing the landmarking task by enlarging the label points to regions, then gradually eroding these label regions back to the base task. We measure the benefits of this approach on six datasets of radiographs with gold-standard expert annotations.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#30446;&#26631;&#35757;&#32451;&#27169;&#22411;&#26469;&#23454;&#29616;&#21516;&#26102;&#29983;&#25104;&#21644;&#20998;&#21106;&#22270;&#20687;&#12290;&#36825;&#31181;&#26550;&#26500;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#21010;&#20998;&#21306;&#22495;&#24182;&#24182;&#34892;&#21435;&#22122;&#20197;&#21450;&#21512;&#24182;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#26080;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;&#21644;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.15726</link><description>&lt;p&gt;
&#22240;&#24335;&#20998;&#35299;&#25193;&#25955;&#26550;&#26500;&#29992;&#20110;&#26080;&#30417;&#30563;&#22270;&#20687;&#29983;&#25104;&#21644;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Factorized Diffusion Architectures for Unsupervised Image Generation and Segmentation. (arXiv:2309.15726v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15726
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#30446;&#26631;&#35757;&#32451;&#27169;&#22411;&#26469;&#23454;&#29616;&#21516;&#26102;&#29983;&#25104;&#21644;&#20998;&#21106;&#22270;&#20687;&#12290;&#36825;&#31181;&#26550;&#26500;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#21010;&#20998;&#21306;&#22495;&#24182;&#24182;&#34892;&#21435;&#22122;&#20197;&#21450;&#21512;&#24182;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#26080;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;&#21644;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#38750;&#30417;&#30563;&#26041;&#24335;&#20316;&#20026;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#23398;&#20064;&#29983;&#25104;&#21644;&#20998;&#21106;&#22270;&#20687;&#12290;&#23398;&#20064;&#23436;&#20840;&#26159;&#30001;&#21435;&#22122;&#25193;&#25955;&#30446;&#26631;&#39537;&#21160;&#30340;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#27809;&#26377;&#20219;&#20309;&#27880;&#37322;&#25110;&#20851;&#20110;&#21306;&#22495;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#30340;&#35745;&#31639;&#29942;&#39048;&#40723;&#21169;&#21435;&#22122;&#32593;&#32476;&#23558;&#36755;&#20837;&#21010;&#20998;&#20026;&#21306;&#22495;&#65292;&#21516;&#26102;&#23545;&#23427;&#20204;&#36827;&#34892;&#21435;&#22122;&#65292;&#24182;&#23558;&#32467;&#26524;&#21512;&#24182;&#12290;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#36890;&#36807;&#31616;&#21333;&#26816;&#26597;&#20854;&#20869;&#37096;&#39044;&#27979;&#30340;&#20998;&#21306;&#65292;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#21644;&#36825;&#20123;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#25105;&#20204;&#30452;&#25509;&#23558;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#24212;&#29992;&#20110;&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#28982;&#21518;&#21435;&#22122;&#26469;&#20998;&#21106;&#30495;&#23454;&#22270;&#20687;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#26080;&#38656;&#20219;&#20309;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#26080;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;&#21644;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a neural network architecture which, trained in an unsupervised manner as a denoising diffusion model, simultaneously learns to both generate and segment images. Learning is driven entirely by the denoising diffusion objective, without any annotation or prior knowledge about regions during training. A computational bottleneck, built into the neural architecture, encourages the denoising network to partition an input into regions, denoise them in parallel, and combine the results. Our trained model generates both synthetic images and, by simple examination of its internal predicted partitions, a semantic segmentation of those images. Without any finetuning, we directly apply our unsupervised model to the downstream task of segmenting real images via noising and subsequently denoising them. Experiments demonstrate that our model achieves accurate unsupervised image segmentation and high-quality synthetic image generation across multiple datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23558;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#26041;&#27861;&#24212;&#29992;&#20110;&#28608;&#23376;&#26497;&#21270;&#23376;&#24211;&#20262;&#20957;&#32858;&#31995;&#32479;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#23545;Gross-Pitaevskii&#26041;&#31243;&#30340;&#27714;&#35299;&#65292;&#21487;&#20197;&#20197;&#25509;&#36817;1000&#20493;&#30340;&#36895;&#24230;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#29366;&#24577;&#30340;&#35299;&#65292;&#20026;&#28608;&#23376;&#26497;&#21270;&#23376;&#24211;&#20262;&#20957;&#32858;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.15593</link><description>&lt;p&gt;
&#28608;&#23376;&#26497;&#21270;&#23376;&#24211;&#20262;&#20957;&#32858;&#65306;&#19968;&#31181;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exciton-Polariton Condensates: A Fourier Neural Operator Approach. (arXiv:2309.15593v1 [cond-mat.quant-gas])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23558;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#26041;&#27861;&#24212;&#29992;&#20110;&#28608;&#23376;&#26497;&#21270;&#23376;&#24211;&#20262;&#20957;&#32858;&#31995;&#32479;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#23545;Gross-Pitaevskii&#26041;&#31243;&#30340;&#27714;&#35299;&#65292;&#21487;&#20197;&#20197;&#25509;&#36817;1000&#20493;&#30340;&#36895;&#24230;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#29366;&#24577;&#30340;&#35299;&#65292;&#20026;&#28608;&#23376;&#26497;&#21270;&#23376;&#24211;&#20262;&#20957;&#32858;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#21322;&#23548;&#20307;&#21046;&#36896;&#25216;&#26415;&#30340;&#36827;&#23637;&#20652;&#29983;&#20102;&#23545;&#30001;&#28608;&#23376;&#26497;&#21270;&#23376;&#24211;&#20262;&#20957;&#32858;&#39537;&#21160;&#30340;&#20840;&#20809;&#23398;&#22120;&#20214;&#30340;&#24191;&#27867;&#30740;&#31350;&#12290;&#21253;&#25324;&#26230;&#20307;&#31649;&#22312;&#20869;&#30340;&#36825;&#31867;&#22120;&#20214;&#30340;&#21021;&#27493;&#39564;&#35777;&#24050;&#32463;&#22312;&#29615;&#22659;&#26465;&#20214;&#19979;&#21462;&#24471;&#20102;&#40723;&#33310;&#20154;&#24515;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#39046;&#22495;&#65306;&#32570;&#20047;&#19968;&#20010;&#20581;&#22766;&#30340;&#27714;&#35299;&#22120;&#65292;&#21487;&#20197;&#29992;&#20110;&#27169;&#25311;&#38656;&#35201;&#36739;&#38271;&#26102;&#38388;&#36798;&#21040;&#31283;&#23450;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#26041;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#19982;&#39069;&#22806;&#28608;&#23376;&#36895;&#29575;&#26041;&#31243;&#32806;&#21512;&#30340;Gross-Pitaevskii&#26041;&#31243;&#12290;&#36825;&#39033;&#24037;&#20316;&#26631;&#24535;&#30528;&#31070;&#32463;&#31639;&#23376;&#39318;&#27425;&#30452;&#25509;&#24212;&#29992;&#20110;&#28608;&#23376;&#26497;&#21270;&#23376;&#24211;&#20262;&#20957;&#32858;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20110;CUDA&#30340;GPU&#27714;&#35299;&#22120;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20197;&#25509;&#36817;1000&#20493;&#30340;&#36895;&#24230;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#29366;&#24577;&#30340;&#35299;&#12290;&#27492;&#22806;&#65292;&#36825;&#20026;&#20170;&#21518;&#30340;&#28145;&#20837;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in semiconductor fabrication over the past decade have catalyzed extensive research into all-optical devices driven by exciton-polariton condensates. Preliminary validations of such devices, including transistors, have shown encouraging results even under ambient conditions. A significant challenge still remains for large scale application however: the lack of a robust solver that can be used to simulate complex nonlinear systems which require an extended period of time to stabilize. Addressing this need, we propose the application of a machine-learning-based Fourier Neural Operator approach to find the solution to the Gross-Pitaevskii equations coupled with extra exciton rate equations. This work marks the first direct application of Neural Operators to an exciton-polariton condensate system. Our findings show that the proposed method can predict final-state solutions to a high degree of accuracy almost 1000 times faster than CUDA-based GPU solvers. Moreover, this paves t
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#29992;&#20110;&#21315;&#31859;&#23610;&#24230;&#22823;&#27668;&#38477;&#23610;&#24230;&#30340;&#29983;&#25104;&#27531;&#24046;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22825;&#27668;&#21644;&#27668;&#20505;&#30340;&#29289;&#29702;&#28798;&#23475;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15214</link><description>&lt;p&gt;
&#29992;&#20110;&#21315;&#31859;&#23610;&#24230;&#22823;&#27668;&#38477;&#23610;&#24230;&#30340;&#29983;&#25104;&#27531;&#24046;&#25193;&#25955;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Residual Diffusion Modeling for Km-scale Atmospheric Downscaling. (arXiv:2309.15214v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15214
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21315;&#31859;&#23610;&#24230;&#22823;&#27668;&#38477;&#23610;&#24230;&#30340;&#29983;&#25104;&#27531;&#24046;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22825;&#27668;&#21644;&#27668;&#20505;&#30340;&#29289;&#29702;&#28798;&#23475;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#20174;&#22825;&#27668;&#21644;&#27668;&#20505;&#20013;&#36827;&#34892;&#29289;&#29702;&#28798;&#23475;&#39044;&#27979;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#26114;&#36149;&#30340;&#21315;&#31859;&#23610;&#24230;&#25968;&#20540;&#27169;&#25311;&#65292;&#24182;&#39537;&#21160;&#36739;&#31895;&#20998;&#36776;&#29575;&#30340;&#20840;&#29699;&#36755;&#20837;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21315;&#31859;&#23610;&#24230;&#38477;&#23610;&#24230;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#26159;&#20174;&#21488;&#28286;&#30340;&#21306;&#22495;&#39640;&#20998;&#36776;&#29575;&#22825;&#27668;&#27169;&#22411;&#35757;&#32451;&#24471;&#21040;&#30340;&#65292;&#24182;&#22312;ERA5&#20877;&#20998;&#26512;&#25968;&#25454;&#30340;&#22522;&#30784;&#19979;&#36827;&#34892;&#20102;&#26465;&#20214;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#38477;&#23610;&#24230;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22823;&#20998;&#36776;&#29575;&#27604;&#29575;&#65288;25km&#33267;2km&#65289;&#65292;&#19981;&#21516;&#23610;&#24230;&#19978;&#28041;&#21450;&#30340;&#19981;&#21516;&#29289;&#29702;&#36807;&#31243;&#20197;&#21450;&#22312;&#36755;&#20837;&#25968;&#25454;&#20013;&#19981;&#23384;&#22312;&#30340;&#39044;&#27979;&#36890;&#36947;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#27493;&#30340;&#26041;&#27861;&#65288;ResDiff&#65289;&#65292;&#20854;&#20013;&#19968;&#20010;&#65288;UNet&#65289;&#22238;&#24402;&#22312;&#31532;&#19968;&#27493;&#39044;&#27979;&#24179;&#22343;&#20540;&#65292;&#32780;&#25193;&#25955;&#27169;&#22411;&#22312;&#31532;&#20108;&#27493;&#39044;&#27979;&#27531;&#24046;&#12290;\textit{ResDiff}&#22312;&#22359;&#22343;&#26041;&#26681;&#35823;&#24046;&#21644;CRPS&#24471;&#20998;&#19978;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#25216;&#33021;&#12290;ResDiff&#39044;&#27979;&#30340;&#20809;&#35889;&#21644;&#20998;&#24067;&#24544;&#23454;&#22320;&#24674;&#22797;&#20102;&#35843;&#33410;&#26377;&#23475;&#39118;&#21644;&#38632;&#30340;&#37325;&#35201;&#24130;&#24459;&#20851;&#31995;&#12290;&#32479;&#19968;&#30340;&#22825;&#27668;&#29616;&#35937;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The state of the art for physical hazard prediction from weather and climate requires expensive km-scale numerical simulations driven by coarser resolution global inputs. Here, a km-scale downscaling diffusion model is presented as a cost effective alternative. The model is trained from a regional high-resolution weather model over Taiwan, and conditioned on ERA5 reanalysis data. To address the downscaling uncertainties, large resolution ratios (25km to 2km), different physics involved at different scales and predict channels that are not in the input data, we employ a two-step approach (\textit{ResDiff}) where a (UNet) regression predicts the mean in the first step and a diffusion model predicts the residual in the second step. \textit{ResDiff} exhibits encouraging skill in bulk RMSE and CRPS scores. The predicted spectra and distributions from ResDiff faithfully recover important power law relationships regulating damaging wind and rain extremes. Case studies of coherent weather phen
&lt;/p&gt;</description></item><item><title>Grad DFT&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#22686;&#24378;&#30340;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#36719;&#20214;&#24211;&#65292;&#36890;&#36807;&#20351;&#29992;&#26435;&#37325;&#21644;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20132;&#25442;&#20851;&#32852;&#33021;&#37327;&#27867;&#20989;&#65292;&#23545;DFT&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.15127</link><description>&lt;p&gt;
Grad DFT&#65306;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#22686;&#24378;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#30340;&#36719;&#20214;&#24211;
&lt;/p&gt;
&lt;p&gt;
Grad DFT: a software library for machine learning enhanced density functional theory. (arXiv:2309.15127v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15127
&lt;/p&gt;
&lt;p&gt;
Grad DFT&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#22686;&#24378;&#30340;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#36719;&#20214;&#24211;&#65292;&#36890;&#36807;&#20351;&#29992;&#26435;&#37325;&#21644;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20132;&#25442;&#20851;&#32852;&#33021;&#37327;&#27867;&#20989;&#65292;&#23545;DFT&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#20316;&#20026;&#35745;&#31639;&#37327;&#23376;&#21270;&#23398;&#21644;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#22522;&#30707;&#26041;&#27861;&#65292;&#22240;&#20854;&#20986;&#33394;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#32780;&#38395;&#21517;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#24378;&#20851;&#32852;&#31995;&#32479;&#26102;&#65292;DFT&#23384;&#22312;&#31934;&#24230;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#22987;&#25506;&#32034;&#22914;&#20309;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25193;&#23637;DFT&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#20805;&#28385;&#35768;&#22810;&#24320;&#25918;&#38382;&#39064;&#21644;&#25216;&#26415;&#25361;&#25112;&#30340;&#21162;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Grad DFT&#65306;&#19968;&#20010;&#23436;&#20840;&#21487;&#24494;&#30340;&#22522;&#20110;JAX&#30340;DFT&#24211;&#65292;&#33021;&#22815;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#21644;&#23454;&#39564;&#26426;&#22120;&#23398;&#20064;&#22686;&#24378;&#30340;&#20132;&#25442;&#20851;&#32852;&#33021;&#37327;&#27867;&#20989;&#12290;Grad DFT&#37319;&#29992;&#20102;&#19968;&#31181;&#20808;&#39537;&#24615;&#30340;&#20132;&#25442;&#20851;&#32852;&#27867;&#20989;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#33021;&#37327;&#23494;&#24230;&#30340;&#21152;&#26435;&#21644;&#26469;&#30830;&#23450;&#26435;&#37325;&#65292;&#26435;&#37325;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30830;&#23450;&#12290;&#27492;&#22806;&#65292;Grad DFT&#21253;&#21547;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#36741;&#21161;&#20989;&#25968;&#65292;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#29305;&#28857;&#26159;&#21487;&#21363;&#26102;&#32534;&#35793;&#21644;&#23436;&#20840;&#21487;&#24494;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Density functional theory (DFT) stands as a cornerstone method in computational quantum chemistry and materials science due to its remarkable versatility and scalability. Yet, it suffers from limitations in accuracy, particularly when dealing with strongly correlated systems. To address these shortcomings, recent work has begun to explore how machine learning can expand the capabilities of DFT; an endeavor with many open questions and technical challenges. In this work, we present Grad DFT: a fully differentiable JAX-based DFT library, enabling quick prototyping and experimentation with machine learning-enhanced exchange-correlation energy functionals. Grad DFT employs a pioneering parametrization of exchange-correlation functionals constructed using a weighted sum of energy densities, where the weights are determined using neural networks. Moreover, Grad DFT encompasses a comprehensive suite of auxiliary functions, notably featuring a just-in-time compilable and fully differentiable s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LogGPT&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;GPT&#36827;&#34892;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#21644;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;LogGPT&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#31995;&#32479;&#26085;&#24535;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.14482</link><description>&lt;p&gt;
LogGPT&#65306;&#36890;&#36807;GPT&#36827;&#34892;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
LogGPT: Log Anomaly Detection via GPT. (arXiv:2309.14482v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LogGPT&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;GPT&#36827;&#34892;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#21644;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;LogGPT&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#31995;&#32479;&#26085;&#24535;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26085;&#24535;&#25968;&#25454;&#30340;&#31995;&#32479;&#24322;&#24120;&#26816;&#27979;&#23545;&#20110;&#30830;&#20445;&#35745;&#31639;&#26426;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#12290;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#26085;&#24535;&#24207;&#21015;&#24314;&#27169;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230;&#26102;&#24207;&#27169;&#22411;&#65288;&#22914;LSTM&#25110;Transformer&#65289;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#26469;&#32534;&#30721;&#26085;&#24535;&#24207;&#21015;&#20013;&#30340;&#27491;&#24120;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#35821;&#35328;&#24314;&#27169;&#19982;&#24322;&#24120;&#26816;&#27979;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#65292;&#22240;&#20026;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#35757;&#32451;&#26102;&#24207;&#27169;&#22411;&#30340;&#30446;&#26631;&#19982;&#24322;&#24120;&#26816;&#27979;&#19981;&#30452;&#25509;&#30456;&#20851;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LogGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;GPT&#36827;&#34892;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35757;&#32451;LogGPT&#26681;&#25454;&#21069;&#24207;&#24207;&#21015;&#39044;&#27979;&#19979;&#19968;&#20010;&#26085;&#24535;&#26465;&#30446;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;LogGPT&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#19987;&#38376;&#20026;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#24494;&#35843;&#27169;&#22411;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting system anomalies based on log data is important for ensuring the security and reliability of computer systems. Recently, deep learning models have been widely used for log anomaly detection. The core idea is to model the log sequences as natural language and adopt deep sequential models, such as LSTM or Transformer, to encode the normal patterns in log sequences via language modeling. However, there is a gap between language modeling and anomaly detection as the objective of training a sequential model via a language modeling loss is not directly related to anomaly detection. To fill up the gap, we propose LogGPT, a novel framework that employs GPT for log anomaly detection. LogGPT is first trained to predict the next log entry based on the preceding sequence. To further enhance the performance of LogGPT, we propose a novel reinforcement learning strategy to finetune the model specifically for the log anomaly detection task. The experimental results on three datasets show tha
&lt;/p&gt;</description></item><item><title>NAS-NeRF&#26159;&#19968;&#31181;&#29983;&#25104;&#24335;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#31574;&#30053;&#65292;&#36890;&#36807;&#24179;&#34913;&#26550;&#26500;&#22797;&#26434;&#24230;&#21644;&#30446;&#26631;&#21512;&#25104;&#36136;&#37327;&#25351;&#26631;&#29983;&#25104;&#32039;&#20945;&#12289;&#38024;&#23545;&#22330;&#26223;&#30340;NeRF&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.14293</link><description>&lt;p&gt;
NAS-NeRF: &#29992;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#29983;&#25104;&#24335;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
NAS-NeRF: Generative Neural Architecture Search for Neural Radiance Fields. (arXiv:2309.14293v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14293
&lt;/p&gt;
&lt;p&gt;
NAS-NeRF&#26159;&#19968;&#31181;&#29983;&#25104;&#24335;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#31574;&#30053;&#65292;&#36890;&#36807;&#24179;&#34913;&#26550;&#26500;&#22797;&#26434;&#24230;&#21644;&#30446;&#26631;&#21512;&#25104;&#36136;&#37327;&#25351;&#26631;&#29983;&#25104;&#32039;&#20945;&#12289;&#38024;&#23545;&#22330;&#26223;&#30340;NeRF&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#26032;&#35270;&#22270;&#21512;&#25104;&#65292;&#20294;&#20854;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#38480;&#21046;&#20102;&#20854;&#21487;&#37096;&#32626;&#24615;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#21162;&#21147;&#25552;&#39640;&#25928;&#29575;&#65292;&#20294;&#19981;&#32771;&#34385;&#22330;&#26223;&#22797;&#26434;&#24615;&#65292;&#20351;&#29992;&#36890;&#29992;&#26550;&#26500;&#12290;&#21516;&#19968;&#20010;&#26550;&#26500;&#21487;&#33021;&#23545;&#31616;&#21333;&#22330;&#26223;&#26469;&#35828;&#36807;&#20110;&#24222;&#22823;&#65292;&#23545;&#22797;&#26434;&#22330;&#26223;&#21017;&#19981;&#36275;&#22815;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#21160;&#24577;&#20248;&#21270;NeRF&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#20197;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#21512;&#25104;&#36136;&#37327;&#20043;&#38388;&#23454;&#29616;&#24179;&#34913;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;NAS-NeRF&#65292;&#19968;&#31181;&#29983;&#25104;&#24335;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#31574;&#30053;&#65292;&#36890;&#36807;&#24179;&#34913;&#26550;&#26500;&#22797;&#26434;&#24230;&#21644;&#30446;&#26631;&#21512;&#25104;&#36136;&#37327;&#25351;&#26631;&#29983;&#25104;&#32039;&#20945;&#12289;&#38024;&#23545;&#22330;&#26223;&#30340;NeRF&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#30446;&#26631;&#24230;&#37327;&#21644;&#39044;&#31639;&#32422;&#26463;&#65292;&#25351;&#23548;&#25628;&#32034;&#20197;&#33719;&#24471;&#36866;&#21512;&#27599;&#20010;&#22330;&#26223;&#30340;&#26550;&#26500;&#12290;&#22312;Blender&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25552;&#20986;&#30340;NAS-NeRF&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#22810;&#36798;5&#20010;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural radiance fields (NeRFs) enable high-quality novel view synthesis, but their high computational complexity limits deployability. While existing neural-based solutions strive for efficiency, they use one-size-fits-all architectures regardless of scene complexity. The same architecture may be unnecessarily large for simple scenes but insufficient for complex ones. Thus, there is a need to dynamically optimize the neural network component of NeRFs to achieve a balance between computational complexity and specific targets for synthesis quality. We introduce NAS-NeRF, a generative neural architecture search strategy that generates compact, scene-specialized NeRF architectures by balancing architecture complexity and target synthesis quality metrics. Our method incorporates constraints on target metrics and budgets to guide the search towards architectures tailored for each scene. Experiments on the Blender synthetic dataset show the proposed NAS-NeRF can generate architectures up to 5
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#25968;&#25454;&#20998;&#24067;&#19978;&#26159;&#31283;&#23450;&#30340;&#65292;&#24182;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#27169;&#22411;&#31867;&#21644;&#20840;&#23616;&#21464;&#37327;&#37325;&#35201;&#24615;&#25351;&#26631;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.13775</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;The Rashomon Importance Distribution: &#25670;&#33073;&#19981;&#31283;&#23450;&#30340;&#22522;&#20110;&#21333;&#19968;&#27169;&#22411;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Rashomon Importance Distribution: Getting RID of Unstable, Single Model-based Variable Importance. (arXiv:2309.13775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13775
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#25968;&#25454;&#20998;&#24067;&#19978;&#26159;&#31283;&#23450;&#30340;&#65292;&#24182;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#27169;&#22411;&#31867;&#21644;&#20840;&#23616;&#21464;&#37327;&#37325;&#35201;&#24615;&#25351;&#26631;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#21464;&#37327;&#37325;&#35201;&#24615;&#23545;&#20110;&#22238;&#31572;&#36951;&#20256;&#23398;&#12289;&#20844;&#20849;&#25919;&#31574;&#21644;&#21307;&#23398;&#31561;&#39046;&#22495;&#30340;&#37325;&#22823;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#35745;&#31639;&#32473;&#23450;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32473;&#23450;&#27169;&#22411;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#32473;&#23450;&#25968;&#25454;&#38598;&#65292;&#21487;&#33021;&#26377;&#35768;&#22810;&#27169;&#22411;&#21516;&#26679;&#33021;&#35299;&#37322;&#30446;&#26631;&#32467;&#26524;;&#22914;&#26524;&#19981;&#32771;&#34385;&#25152;&#26377;&#21487;&#33021;&#30340;&#35299;&#37322;&#65292;&#19981;&#21516;&#30340;&#30740;&#31350;&#32773;&#21487;&#33021;&#20250;&#24471;&#20986;&#35768;&#22810;&#20914;&#31361;&#20294;&#21516;&#26679;&#26377;&#25928;&#30340;&#32467;&#35770;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#32771;&#34385;&#20102;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#25152;&#26377;&#21487;&#33021;&#35299;&#37322;&#65292;&#36825;&#20123;&#27934;&#23519;&#21147;&#21487;&#33021;&#19981;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#22240;&#20026;&#24182;&#38750;&#25152;&#26377;&#22909;&#30340;&#35299;&#37322;&#22312;&#21512;&#29702;&#30340;&#25968;&#25454;&#25200;&#21160;&#19979;&#37117;&#26159;&#31283;&#23450;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37327;&#21270;&#20102;&#22312;&#25152;&#26377;&#22909;&#30340;&#27169;&#22411;&#38598;&#21512;&#20013;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#20998;&#24067;&#19978;&#26159;&#31283;&#23450;&#30340;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38750;&#24120;&#28789;&#27963;&#65292;&#21487;&#20197;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#27169;&#22411;&#31867;&#21644;&#20840;&#23616;&#21464;&#37327;&#37325;&#35201;&#24615;&#25351;&#26631;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantifying variable importance is essential for answering high-stakes questions in fields like genetics, public policy, and medicine. Current methods generally calculate variable importance for a given model trained on a given dataset. However, for a given dataset, there may be many models that explain the target outcome equally well; without accounting for all possible explanations, different researchers may arrive at many conflicting yet equally valid conclusions given the same data. Additionally, even when accounting for all possible explanations for a given dataset, these insights may not generalize because not all good explanations are stable across reasonable data perturbations. We propose a new variable importance framework that quantifies the importance of a variable across the set of all good models and is stable across the data distribution. Our framework is extremely flexible and can be integrated with most existing model classes and global variable importance metrics. We d
&lt;/p&gt;</description></item><item><title>&#19968;&#20010;&#21517;&#20026;BioinspiredLLM&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#22823;&#37327;&#30340;&#25991;&#29486;&#36827;&#34892;&#24494;&#35843;&#65292;&#33021;&#22815;&#20027;&#21160;&#20132;&#20114;&#22320;&#22238;&#24518;&#21644;&#35780;&#20272;&#29983;&#29289;&#26448;&#26009;&#30340;&#20449;&#24687;&#65292;&#25552;&#20986;&#26032;&#30340;&#38382;&#39064;&#21644;&#22238;&#31572;&#65292;&#24182;&#20026;&#29983;&#29289;&#26448;&#26009;&#35774;&#35745;&#25552;&#20379;&#21512;&#29702;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2309.08788</link><description>&lt;p&gt;
BioinspiredLLM: &#29983;&#29289;&#21644;&#29983;&#29289;&#21463;&#21551;&#21457;&#26448;&#26009;&#21147;&#23398;&#30340;&#23545;&#35805;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BioinspiredLLM: Conversational Large Language Model for the Mechanics of Biological and Bio-inspired Materials. (arXiv:2309.08788v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08788
&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#21517;&#20026;BioinspiredLLM&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#22823;&#37327;&#30340;&#25991;&#29486;&#36827;&#34892;&#24494;&#35843;&#65292;&#33021;&#22815;&#20027;&#21160;&#20132;&#20114;&#22320;&#22238;&#24518;&#21644;&#35780;&#20272;&#29983;&#29289;&#26448;&#26009;&#30340;&#20449;&#24687;&#65292;&#25552;&#20986;&#26032;&#30340;&#38382;&#39064;&#21644;&#22238;&#31572;&#65292;&#24182;&#20026;&#29983;&#29289;&#26448;&#26009;&#35774;&#35745;&#25552;&#20379;&#21512;&#29702;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#26448;&#26009;&#21644;&#29983;&#29289;&#21463;&#21551;&#21457;&#26448;&#26009;&#31185;&#23398;&#30340;&#30740;&#31350;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#21457;&#23637;&#65307;&#28982;&#32780;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#24456;&#23569;&#26377;&#31995;&#32479;&#22320;&#23558;&#36825;&#20123;&#30693;&#35782;&#36716;&#21270;&#20026;&#24037;&#31243;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#21152;&#24555;&#21457;&#29616;&#24182;&#24341;&#23548;&#27934;&#23519;&#65292;&#25253;&#36947;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#33258;&#22238;&#24402;&#36716;&#25442;&#22120;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;BioinspiredLLM&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;&#19968;&#21315;&#22810;&#31687;&#32463;&#36807;&#21516;&#34892;&#35780;&#23457;&#30340;&#32467;&#26500;&#29983;&#29289;&#23398;&#21644;&#29983;&#29289;&#21463;&#21551;&#21457;&#26448;&#26009;&#39046;&#22495;&#30340;&#25991;&#31456;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#21487;&#20197;&#34987;&#25552;&#31034;&#20027;&#21160;&#21644;&#20132;&#20114;&#22320;&#22238;&#24518;&#20449;&#24687;&#65292;&#21327;&#21161;&#30740;&#31350;&#20219;&#21153;&#65292;&#24182;&#20316;&#20026;&#21019;&#36896;&#21147;&#30340;&#24341;&#25806;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#31034;&#20363;&#35777;&#26126;&#20102;&#23427;&#19981;&#20165;&#33021;&#22815;&#22312;&#26597;&#35810;&#26102;&#20934;&#30830;&#22238;&#24518;&#26377;&#20851;&#29983;&#29289;&#26448;&#26009;&#30340;&#20449;&#24687;&#65292;&#36824;&#33021;&#22815;&#25552;&#20986;&#29983;&#29289;&#26448;&#26009;&#38382;&#39064;&#21644;&#31572;&#26696;&#26469;&#35780;&#20272;&#33258;&#24049;&#30340;&#24615;&#33021;&#12290;BioinspiredLLM&#36824;&#34987;&#35777;&#26126;&#33021;&#22815;&#23545;&#29983;&#29289;&#26448;&#26009;&#35774;&#35745;&#25552;&#20986;&#21512;&#29702;&#30340;&#20551;&#35774;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#37027;&#20123;&#20174;&#26410;&#26126;&#30830;&#30740;&#31350;&#36807;&#30340;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of biological materials and bio-inspired materials science is well established; however, surprisingly little knowledge has been systematically translated to engineering solutions. To accelerate discovery and guide insights, an open-source autoregressive transformer large language model, BioinspiredLLM, is reported. The model was finetuned with a corpus of over a thousand peer-reviewed articles in the field of structural biological and bio-inspired materials and can be prompted to actively and interactively recall information, assist with research tasks, and function as an engine for creativity. The model has proven by example that it is not only able to accurately recall information about biological materials when queried but also formulate biomaterials questions and answers that can evaluate its own performance. BioinspiredLLM also has been shown to develop sound hypotheses regarding biological materials design and remarkably so for materials that have never been explicitly 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;Verilog&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#24615;&#33021;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;156&#20010;&#38382;&#39064;&#30340;&#32508;&#21512;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#19982;&#40644;&#37329;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27604;&#36739;&#65292;&#21487;&#20197;&#33258;&#21160;&#27979;&#35797;Verilog&#20195;&#30721;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#38382;&#39064;-&#20195;&#30721;&#23545;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#65292;&#21487;&#20197;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;Verilog&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.07544</link><description>&lt;p&gt;
VerilogEval&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;Verilog&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
VerilogEval: Evaluating Large Language Models for Verilog Code Generation. (arXiv:2309.07544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;Verilog&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#24615;&#33021;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;156&#20010;&#38382;&#39064;&#30340;&#32508;&#21512;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#19982;&#40644;&#37329;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27604;&#36739;&#65292;&#21487;&#20197;&#33258;&#21160;&#27979;&#35797;Verilog&#20195;&#30721;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#38382;&#39064;-&#20195;&#30721;&#23545;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#65292;&#21487;&#20197;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;Verilog&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26085;&#30410;&#26222;&#21450;&#20026;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29305;&#21035;&#38024;&#23545;Verilog&#20195;&#30721;&#29983;&#25104;&#24615;&#33021;&#35780;&#20272;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#29992;&#20110;&#30828;&#20214;&#35774;&#35745;&#21644;&#39564;&#35777;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;Verilog&#25945;&#23398;&#32593;&#31449;HDLBits&#30340;156&#20010;&#38382;&#39064;&#30340;&#32508;&#21512;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#35813;&#35780;&#20272;&#38598;&#21253;&#21547;&#20102;&#21508;&#31181;Verilog&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#20174;&#31616;&#21333;&#30340;&#32452;&#21512;&#30005;&#36335;&#21040;&#22797;&#26434;&#30340;&#26377;&#38480;&#29366;&#24577;&#26426;&#12290;&#21487;&#20197;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#35774;&#35745;&#30340;&#30636;&#24577;&#20223;&#30495;&#36755;&#20986;&#19982;&#40644;&#37329;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27604;&#36739;&#65292;&#33258;&#21160;&#27979;&#35797;Verilog&#20195;&#30721;&#23436;&#25104;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;Verilog&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#38382;&#39064;-&#20195;&#30721;&#23545;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#26469;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing popularity of large language models (LLMs) has paved the way for their application in diverse domains. This paper proposes a benchmarking framework tailored specifically for evaluating LLM performance in the context of Verilog code generation for hardware design and verification. We present a comprehensive evaluation dataset consisting of 156 problems from the Verilog instructional website HDLBits. The evaluation set consists of a diverse set of Verilog code generation tasks, ranging from simple combinational circuits to complex finite state machines. The Verilog code completions can be automatically tested for functional correctness by comparing the transient simulation outputs of the generated design with a golden solution. We also demonstrate that the Verilog code generation capability of pretrained language models could be improved with supervised fine-tuning by bootstrapping with LLM generated synthetic problem-code pairs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;</title><link>http://arxiv.org/abs/2309.05961</link><description>&lt;p&gt;
&#35780;&#20272;&#28526;&#36215;&#28526;&#33853;&#65306;&#23545;&#19981;&#21516;&#24179;&#21488;&#38388;&#38382;&#31572;&#36235;&#21183;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#22240;&#20854;&#24555;&#36895;&#22238;&#31572;&#29992;&#25143;&#26597;&#35810;&#30340;&#33021;&#21147;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#22238;&#31572;&#36895;&#24230;&#30340;&#24555;&#24930;&#21462;&#20915;&#20110;&#26597;&#35810;&#29305;&#23450;&#21644;&#29992;&#25143;&#30456;&#20851;&#30340;&#22240;&#32032;&#30340;&#32508;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20845;&#20010;&#39640;&#24230;&#27969;&#34892;&#30340;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#65292;&#20998;&#26512;&#20102;&#36825;&#20123;&#22240;&#32032;&#22312;&#20854;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#22238;&#31572;&#25152;&#33457;&#36153;&#30340;&#26102;&#38388;&#19982;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#30340;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#20114;&#21160;&#27700;&#24179;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#36825;&#20123;&#20803;&#25968;&#25454;&#21644;&#29992;&#25143;&#20114;&#21160;&#27169;&#24335;&#65292;&#25105;&#20204;&#35797;&#22270;&#39044;&#27979;&#21738;&#20123;&#26597;&#35810;&#23558;&#36805;&#36895;&#33719;&#24471;&#21021;&#22987;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;GPU&#21644;CPU&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPU&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#20855;&#26377;&#26356;&#20302;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#23545;&#20110;&#36739;&#31616;&#21333;&#30340;&#32593;&#32476;&#65292;GPU&#24182;&#27809;&#26377;&#22826;&#22810;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.02521</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;CPU&#21644;GPU&#24615;&#33021;&#20998;&#26512;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of CPU and GPU Profiling for Deep Learning Models. (arXiv:2309.02521v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;GPU&#21644;CPU&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPU&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#20855;&#26377;&#26356;&#20302;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#23545;&#20110;&#36739;&#31616;&#21333;&#30340;&#32593;&#32476;&#65292;GPU&#24182;&#27809;&#26377;&#22826;&#22810;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#22825;&#65292;&#28145;&#24230;&#23398;&#20064;(DL)&#21644;&#26426;&#22120;&#23398;&#20064;(ML)&#24212;&#29992;&#27491;&#22312;&#24555;&#36895;&#22686;&#21152;&#12290;&#22823;&#37327;&#30340;&#25968;&#25454;&#36890;&#36807;&#20114;&#32852;&#32593;&#29983;&#25104;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;ML&#21644;DL&#31639;&#27861;&#26469;&#24471;&#20986;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#30828;&#20214;&#36164;&#28304;&#21644;&#24320;&#28304;&#24211;&#20351;&#24471;&#23454;&#29616;&#36825;&#20123;&#31639;&#27861;&#21464;&#24471;&#23481;&#26131;&#12290;Tensorflow&#21644;PyTorch&#26159;&#23454;&#29616;ML&#39033;&#30446;&#30340;&#39046;&#20808;&#26694;&#26550;&#20043;&#19968;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#36319;&#36394;&#22312;GPU&#21644;CPU&#19978;&#25191;&#34892;&#30340;&#25805;&#20316;&#65292;&#20197;&#20998;&#26512;&#36164;&#28304;&#20998;&#37197;&#21644;&#28040;&#32791;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;PyTorch&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;CPU&#21644;GPU&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#20998;&#37197;&#24773;&#20917;&#12290;&#35813;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;CPU&#30456;&#27604;&#65292;GPU&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#20855;&#26377;&#26356;&#20302;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#23545;&#20110;&#19968;&#20010;&#36739;&#31616;&#21333;&#30340;&#32593;&#32476;&#65292;GPU&#22312;CPU&#19978;&#24182;&#27809;&#26377;&#22826;&#22810;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning(DL) and Machine Learning(ML) applications are rapidly increasing in recent days. Massive amounts of data are being generated over the internet which can derive meaningful results by the use of ML and DL algorithms. Hardware resources and open-source libraries have made it easy to implement these algorithms. Tensorflow and Pytorch are one of the leading frameworks for implementing ML projects. By using those frameworks, we can trace the operations executed on both GPU and CPU to analyze the resource allocations and consumption. This paper presents the time and memory allocation of CPU and GPU while training deep neural networks using Pytorch. This paper analysis shows that GPU has a lower running time as compared to CPU for deep neural networks. For a simpler network, there are not many significant improvements in GPU over the CPU.
&lt;/p&gt;</description></item><item><title>MedShapeNet&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#19977;&#32500;&#21307;&#23398;&#24418;&#29366;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#19968;&#31181;&#23545;&#20110;&#24120;&#29992;&#24418;&#29366;&#22522;&#20934;&#30340;&#26367;&#20195;&#21697;&#65292;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2308.16139</link><description>&lt;p&gt;
MedShapeNet - &#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#22823;&#35268;&#27169;&#19977;&#32500;&#21307;&#23398;&#24418;&#29366;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer Vision. (arXiv:2308.16139v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16139
&lt;/p&gt;
&lt;p&gt;
MedShapeNet&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#19977;&#32500;&#21307;&#23398;&#24418;&#29366;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#19968;&#31181;&#23545;&#20110;&#24120;&#29992;&#24418;&#29366;&#22522;&#20934;&#30340;&#26367;&#20195;&#21697;&#65292;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MedShapeNet&#65292;&#19968;&#20010;&#21253;&#21547;&#20102;&#35299;&#21078;&#24418;&#29366;&#65288;&#22914;&#39592;&#39612;&#12289;&#22120;&#23448;&#12289;&#34880;&#31649;&#65289;&#21644;&#19977;&#32500;&#25163;&#26415;&#22120;&#26800;&#27169;&#22411;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#20043;&#21069;&#65292;&#32479;&#35745;&#24418;&#29366;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#35777;&#26126;&#20102;&#24418;&#29366;&#24120;&#34987;&#29992;&#26469;&#25551;&#36848;&#21307;&#23398;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#30340;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20027;&#35201;&#26159;&#22522;&#20110;&#20307;&#32032;&#30340;&#12290;&#30456;&#21453;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#24418;&#29366;&#65288;&#21253;&#25324;&#20307;&#32032;&#21344;&#25454;&#32593;&#26684;&#12289;&#32593;&#26684;&#12289;&#28857;&#20113;&#21644;&#38544;&#24335;&#34920;&#38754;&#27169;&#22411;&#65289;&#26159;&#19977;&#32500;&#25968;&#25454;&#30340;&#39318;&#36873;&#34920;&#31034;&#26041;&#27861;&#65292;&#36825;&#19968;&#28857;&#21487;&#20197;&#20174;&#22823;&#37327;&#20851;&#20110;&#24418;&#29366;&#30340;&#25991;&#31456;&#21450;&#22312;&#39030;&#32423;&#35745;&#31639;&#26426;&#35270;&#35273;&#20250;&#35758;&#65288;&#22914;IEEE/CVF&#35745;&#31639;&#26426;&#35270;&#35273;&#19982;&#27169;&#24335;&#35782;&#21035;&#20250;&#35758;&#65288;CVPR&#65289;&#65289;&#20013;&#35265;&#21040;&#65292;&#21516;&#26102;ShapeNet&#65288;&#32422;51300&#20010;&#27169;&#22411;&#65289;&#21644;&#26222;&#26519;&#26031;&#39039;ModelNet&#65288;127,915&#20010;&#27169;&#22411;&#65289;&#30340;&#27969;&#34892;&#24230;&#20063;&#22312;&#19981;&#26029;&#22686;&#21152;&#12290;MedShapeNet&#30340;&#21019;&#24314;&#26159;&#20026;&#20102;&#20316;&#20026;&#36825;&#20123;&#24120;&#29992;&#24418;&#29366;&#22522;&#20934;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MedShapeNet, a large collection of anatomical shapes (e.g., bones, organs, vessels) and 3D surgical instrument models. Prior to the deep learning era, the broad application of statistical shape models (SSMs) in medical image analysis is evidence that shapes have been commonly used to describe medical data. Nowadays, however, state-of-the-art (SOTA) deep learning algorithms in medical imaging are predominantly voxel-based. In computer vision, on the contrary, shapes (including, voxel occupancy grids, meshes, point clouds and implicit surface models) are preferred data representations in 3D, as seen from the numerous shape-related publications in premier vision conferences, such as the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), as well as the increasing popularity of ShapeNet (about 51,300 models) and Princeton ModelNet (127,915 models) in computer vision research. MedShapeNet is created as an alternative to these commonly used shape benchmarks to f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#26041;&#27861;SAT&#65292;&#36890;&#36807;&#22312;&#32447;&#21160;&#24577;&#23884;&#20837;&#39044;&#27979;&#20943;&#36731;&#20102;&#36807;&#26102;&#24615;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#20013;&#30340;&#24182;&#21457;&#24615;&#21644;&#36890;&#20449;&#24320;&#38144;&#30340;&#22256;&#25200;&#12290;</title><link>http://arxiv.org/abs/2308.13466</link><description>&lt;p&gt;
&#21487;&#36890;&#36807;&#22312;&#32447;&#21160;&#24577;&#23884;&#20837;&#39044;&#27979;&#20943;&#36731;&#36807;&#26102;&#24615;&#30340;&#20998;&#24067;&#24335;GNN&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Staleness-Alleviated Distributed GNN Training via Online Dynamic-Embedding Prediction. (arXiv:2308.13466v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#26041;&#27861;SAT&#65292;&#36890;&#36807;&#22312;&#32447;&#21160;&#24577;&#23884;&#20837;&#39044;&#27979;&#20943;&#36731;&#20102;&#36807;&#26102;&#24615;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#20013;&#30340;&#24182;&#21457;&#24615;&#21644;&#36890;&#20449;&#24320;&#38144;&#30340;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#37051;&#23621;&#25193;&#25955;&#65292;&#20173;&#28982;&#38590;&#20197;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#20998;&#24067;&#24335;&#35745;&#31639;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#20016;&#23500;&#30340;&#35745;&#31639;&#36164;&#28304;&#65288;&#22914;GPU&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#20013;&#65292;&#30001;&#20110;&#22270;&#25968;&#25454;&#30340;&#33410;&#28857;&#20381;&#36182;&#24615;&#22686;&#21152;&#65292;&#23454;&#29616;&#39640;&#24182;&#21457;&#24615;&#21464;&#24471;&#22256;&#38590;&#65292;&#36825;&#20250;&#23548;&#33268;&#24040;&#22823;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21382;&#21490;&#20540;&#36817;&#20284;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#25216;&#26415;&#31867;&#21035;&#12290;&#23427;&#21033;&#29992;&#31163;&#32447;&#20869;&#23384;&#32531;&#23384;&#21382;&#21490;&#20449;&#24687;&#65288;&#22914;&#33410;&#28857;&#23884;&#20837;&#65289;&#65292;&#20316;&#20026;&#31934;&#30830;&#20540;&#30340;&#21487;&#25215;&#21463;&#30340;&#36817;&#20284;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#24182;&#21457;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22909;&#22788;&#26159;&#20197;&#20351;&#29992;&#36807;&#26102;&#30340;&#35757;&#32451;&#20449;&#24687;&#20026;&#20195;&#20215;&#30340;&#65292;&#23548;&#33268;&#36807;&#26102;&#24615;&#12289;&#19981;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SAT&#65288;&#20943;&#36731;&#36807;&#26102;&#24615;&#35757;&#32451;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#19988;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent success of Graph Neural Networks (GNNs), it remains challenging to train GNNs on large-scale graphs due to neighbor explosions. As a remedy, distributed computing becomes a promising solution by leveraging abundant computing resources (e.g., GPU). However, the node dependency of graph data increases the difficulty of achieving high concurrency in distributed GNN training, which suffers from the massive communication overhead. To address it, Historical value approximation is deemed a promising class of distributed training techniques. It utilizes an offline memory to cache historical information (e.g., node embedding) as an affordable approximation of the exact value and achieves high concurrency. However, such benefits come at the cost of involving dated training information, leading to staleness, imprecision, and convergence issues. To overcome these challenges, this paper proposes SAT (Staleness-Alleviated Training), a novel and scalable distributed GNN training fr
&lt;/p&gt;</description></item><item><title>BLIVA&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#23884;&#20837;&#20102;&#25991;&#26412;&#30340;&#22270;&#20687;&#19978;&#19979;&#25991;&#22330;&#26223;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#21644;LLaVA&#30340;&#32534;&#30721;&#34917;&#19969;&#23884;&#20837;&#25216;&#26415;&#26469;&#25913;&#36827;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09936</link><description>&lt;p&gt;
BLIVA: &#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions. (arXiv:2308.09936v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09936
&lt;/p&gt;
&lt;p&gt;
BLIVA&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#23884;&#20837;&#20102;&#25991;&#26412;&#30340;&#22270;&#20687;&#19978;&#19979;&#25991;&#22330;&#26223;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#21644;LLaVA&#30340;&#32534;&#30721;&#34917;&#19969;&#23884;&#20837;&#25216;&#26415;&#26469;&#25913;&#36827;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#36890;&#36807;&#25972;&#21512;&#35270;&#35273;&#29702;&#35299;&#33021;&#21147;&#25193;&#23637;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#35299;&#20915;&#24320;&#25918;&#24335;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#35299;&#37322;&#23884;&#20837;&#25991;&#26412;&#30340;&#22270;&#20687;&#65292;&#36825;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#26631;&#20934;&#27969;&#31243;&#36890;&#24120;&#28041;&#21450;&#23398;&#20064;&#19968;&#32452;&#22266;&#23450;&#30340;&#26597;&#35810;&#23884;&#20837;&#12290;&#36825;&#20123;&#23884;&#20837;&#34987;&#35774;&#35745;&#20026;&#23553;&#35013;&#22270;&#20687;&#19978;&#19979;&#25991;&#65292;&#24182;&#38543;&#21518;&#29992;&#20316;LLM&#20013;&#30340;&#36719;&#25552;&#31034;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#21463;&#20196;&#29260;&#25968;&#37327;&#30340;&#38480;&#21046;&#65292;&#21487;&#33021;&#38480;&#21046;&#23545;&#25991;&#26412;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#22330;&#26223;&#30340;&#35782;&#21035;&#12290;&#20026;&#20102;&#25913;&#36827;&#36825;&#19968;&#28857;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;BLIVA&#65306;InstructBLIP with Visual Assistant&#30340;&#22686;&#24378;&#29256;&#26412;&#12290;BLIVA&#38598;&#25104;&#20102;&#26469;&#33258;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#65292;&#24182;&#23558;&#32534;&#30721;&#30340;&#34917;&#19969;&#23884;&#20837;&#30452;&#25509;&#25237;&#24433;&#21040;LLM&#20013;&#65292;&#36825;&#26159;&#21463;&#21040;LLaVA&#30340;&#21551;&#21457;&#30340;&#19968;&#31181;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#22788;&#29702;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21322;&#21512;&#25104;&#25968;&#25454;&#26469;&#25552;&#21319;&#20195;&#30721;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#12290;&#26041;&#27861;&#21517;&#20026;MultiPL-T&#65292;&#36890;&#36807;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#36716;&#21270;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.09895</link><description>&lt;p&gt;
&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;&#30340;&#30693;&#35782;&#36716;&#31227;&#29992;&#20110;&#20195;&#30721;LLMs
&lt;/p&gt;
&lt;p&gt;
Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs. (arXiv:2308.09895v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21322;&#21512;&#25104;&#25968;&#25454;&#26469;&#25552;&#21319;&#20195;&#30721;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#12290;&#26041;&#27861;&#21517;&#20026;MultiPL-T&#65292;&#36890;&#36807;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#36716;&#21270;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20195;&#30721;LLMs&#65288;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65289;&#24320;&#22987;&#23545;&#32534;&#31243;&#23454;&#36341;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#20195;&#30721;LLMs&#36824;&#25104;&#20026;&#32534;&#31243;&#35821;&#35328;&#21644;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20195;&#30721;LLMs&#29983;&#25104;&#30340;&#20195;&#30721;&#36136;&#37327;&#22312;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#20195;&#30721;LLMs&#23545;&#35757;&#32451;&#25968;&#25454;&#20805;&#20998;&#30340;&#32534;&#31243;&#35821;&#35328;&#65288;&#22914;Java&#12289;Python&#25110;JavaScript&#65289;&#20135;&#29983;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#20687;OCaml&#21644;Racket&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#34920;&#29616;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21322;&#21512;&#25104;&#25968;&#25454;&#25552;&#39640;&#20195;&#30721;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#20102;&#39640;&#36136;&#37327;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#24182;&#21487;&#29992;&#20110;&#24494;&#35843;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;LLMs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;MultiPL-T&#65292;&#23427;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#36716;&#21270;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#25104;&#21313;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as a building block for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming languages. Code LLMs produce impressive results on programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages, like OCaml and Racket.  This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. Our approach, called MultiPL-T, translates training data from high-resource languages into training data for low-resource languages. We apply our approach to generate ten
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#35843;&#22270;&#20013;&#30340;&#36335;&#24452;&#65292;&#31361;&#30772;&#20102;1-Weisfeiler-Lehmann&#27979;&#35797;&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#24182;&#22312;&#26080;&#38656;&#23545;&#22270;&#30340;&#23376;&#32467;&#26500;&#36827;&#34892;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06838</link><description>&lt;p&gt;
&#20351;&#29992;&#36335;&#24452;&#36827;&#34892;&#19968;&#33324;&#21270;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476; (arXiv:2308.06838v2 [cs.LG] &#24050;&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Generalizing Topological Graph Neural Networks with Paths. (arXiv:2308.06838v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#35843;&#22270;&#20013;&#30340;&#36335;&#24452;&#65292;&#31361;&#30772;&#20102;1-Weisfeiler-Lehmann&#27979;&#35797;&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#24182;&#22312;&#26080;&#38656;&#23545;&#22270;&#30340;&#23376;&#32467;&#26500;&#36827;&#34892;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;1-Weisfeiler-Lehmann&#27979;&#35797;&#30340;&#29702;&#35770;&#38480;&#21046;&#12290;&#23613;&#31649;&#39640;&#38454;GNNs&#30340;&#26368;&#26032;&#36827;&#23637;&#21487;&#20197;&#20811;&#26381;&#36825;&#20010;&#38556;&#30861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38598;&#20013;&#22312;&#29305;&#23450;&#30340;&#22270;&#32452;&#20214;&#65292;&#22914;&#22242;&#25110;&#29615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36208;&#20102;&#19981;&#21516;&#30340;&#36335;&#32447;&#12290;&#25105;&#20204;&#24378;&#35843;&#36335;&#24452;&#65292;&#36825;&#26159;&#27599;&#20010;&#22270;&#20013;&#22266;&#26377;&#30340;&#12290;&#25105;&#20204;&#33021;&#22815;&#26500;&#24314;&#19968;&#20010;&#26356;&#36890;&#29992;&#30340;&#25299;&#25169;&#35270;&#35282;&#65292;&#24182;&#19982;&#20854;&#20182;&#25299;&#25169;&#39046;&#22495;&#30340;&#19968;&#20123;&#25104;&#29087;&#29702;&#35770;&#24418;&#25104;&#26725;&#26753;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;&#27809;&#26377;&#23545;&#22270;&#30340;&#23376;&#32467;&#26500;&#36827;&#34892;&#20219;&#20309;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#35813;&#39046;&#22495;&#26089;&#26399;&#30340;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Graph Neural Networks (GNNs) have made significant strides in diverse areas, they are hindered by a theoretical constraint known as the 1-Weisfeiler-Lehmann test. Even though latest advancements in higher-order GNNs can overcome this boundary, they typically center around certain graph components like cliques or cycles. However, our investigation goes a different route. We put emphasis on paths, which are inherent in every graph. We are able to construct a more general topological perspective and form a bridge to certain established theories about other topological domains. Interestingly, without any assumptions on graph sub-structures, our approach surpasses earlier techniques in this field, achieving state-of-the-art performance on several benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#21644;&#23618;&#27425;&#32858;&#31867;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20892;&#23398;&#30740;&#31350;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#36890;&#36807;&#25972;&#21512;&#38543;&#26426;&#25928;&#24212;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#32467;&#26500;&#23398;&#20064;&#33021;&#21147;&#65292;&#23454;&#29616;&#22240;&#26524;&#20851;&#31995;&#32593;&#32476;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.06399</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#21644;&#23618;&#27425;&#32858;&#31867;&#23398;&#20064;&#20855;&#26377;&#24322;&#26500;&#20892;&#19994;&#25968;&#25454;&#38598;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering. (arXiv:2308.06399v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#21644;&#23618;&#27425;&#32858;&#31867;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20892;&#23398;&#30740;&#31350;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#36890;&#36807;&#25972;&#21512;&#38543;&#26426;&#25928;&#24212;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#32467;&#26500;&#23398;&#20064;&#33021;&#21147;&#65292;&#23454;&#29616;&#22240;&#26524;&#20851;&#31995;&#32593;&#32476;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28041;&#21450;&#22810;&#26679;&#20294;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#20013;&#65292;&#20854;&#20013;&#21327;&#21464;&#37327;&#19982;&#32467;&#26524;&#20043;&#38388;&#30340;&#20851;&#32852;&#21487;&#33021;&#20250;&#26377;&#25152;&#19981;&#21516;&#65292;&#22312;&#21253;&#25324;&#20892;&#23398;&#30740;&#31350;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#37117;&#24456;&#26222;&#36941;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24120;&#24120;&#20351;&#29992;&#23618;&#27425;&#27169;&#22411;&#65292;&#20063;&#34987;&#31216;&#20026;&#22810;&#23618;&#27169;&#22411;&#65292;&#26469;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#65292;&#24182;&#36866;&#24212;&#23427;&#20204;&#30340;&#19981;&#21516;&#29305;&#28857;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#32467;&#26500;&#36229;&#20986;&#20102;&#31616;&#21333;&#30340;&#24322;&#36136;&#24615;&#65292;&#22240;&#20026;&#21464;&#37327;&#36890;&#24120;&#24418;&#25104;&#22797;&#26434;&#30340;&#22240;&#26524;&#20851;&#31995;&#32593;&#32476;&#12290;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BNs&#65289;&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#26469;&#27169;&#25311;&#36825;&#31181;&#20851;&#31995;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#38543;&#26426;&#25928;&#24212;&#25972;&#21512;&#21040;BN&#23398;&#20064;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#23618;&#27425;&#25968;&#25454;&#12290;&#26469;&#33258;&#30495;&#23454;&#20892;&#23398;&#35797;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#32467;&#26500;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Research involving diverse but related data sets, where associations between covariates and outcomes may vary, is prevalent in various fields including agronomic studies. In these scenarios, hierarchical models, also known as multilevel models, are frequently employed to assimilate information from different data sets while accommodating their distinct characteristics. However, their structure extend beyond simple heterogeneity, as variables often form complex networks of causal relationships.  Bayesian networks (BNs) provide a powerful framework for modelling such relationships using directed acyclic graphs to illustrate the connections between variables. This study introduces a novel approach that integrates random effects into BN learning. Rooted in linear mixed-effects models, this approach is particularly well-suited for handling hierarchical data. Results from a real-world agronomic trial suggest that employing this approach enhances structural learning, leading to the discovery 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#38024;&#23545;&#38754;&#21521;&#20799;&#31461;&#30340;&#32593;&#31449;&#36827;&#34892;&#36861;&#36394;&#21644;&#24191;&#21578;&#30340;&#35814;&#32454;&#27979;&#37327;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22810;&#35821;&#35328;&#20998;&#31867;&#22120;&#65292;&#24182;&#23545;&#36807;&#20004;&#30334;&#19975;&#20010;&#32593;&#39029;&#36827;&#34892;&#20998;&#31867;&#65292;&#30740;&#31350;&#32773;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#20799;&#31461;&#32593;&#31449;&#21015;&#34920;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#32593;&#31449;&#36827;&#34892;&#29228;&#21462;&#21644;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#36861;&#36394;&#22120;&#12289;&#25351;&#32441;&#33050;&#26412;&#21644;&#24191;&#21578;&#37117;&#23384;&#22312;&#20110;&#36825;&#20123;&#32593;&#31449;&#19978;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#26816;&#27979;&#21040;&#36825;&#20123;&#24191;&#21578;&#26159;&#21542;&#21551;&#29992;&#20102;&#23450;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.04887</link><description>&lt;p&gt;
&#30596;&#20934;&#19988;&#40635;&#28902;&#65306;&#36861;&#36394;&#21644;&#24191;&#21578;&#22312;&#20799;&#31461;&#32593;&#31449;&#19978;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Targeted and Troublesome: Tracking and Advertising on Children's Websites. (arXiv:2308.04887v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04887
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#38024;&#23545;&#38754;&#21521;&#20799;&#31461;&#30340;&#32593;&#31449;&#36827;&#34892;&#36861;&#36394;&#21644;&#24191;&#21578;&#30340;&#35814;&#32454;&#27979;&#37327;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22810;&#35821;&#35328;&#20998;&#31867;&#22120;&#65292;&#24182;&#23545;&#36807;&#20004;&#30334;&#19975;&#20010;&#32593;&#39029;&#36827;&#34892;&#20998;&#31867;&#65292;&#30740;&#31350;&#32773;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#20799;&#31461;&#32593;&#31449;&#21015;&#34920;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#32593;&#31449;&#36827;&#34892;&#29228;&#21462;&#21644;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#36861;&#36394;&#22120;&#12289;&#25351;&#32441;&#33050;&#26412;&#21644;&#24191;&#21578;&#37117;&#23384;&#22312;&#20110;&#36825;&#20123;&#32593;&#31449;&#19978;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#26816;&#27979;&#21040;&#36825;&#20123;&#24191;&#21578;&#26159;&#21542;&#21551;&#29992;&#20102;&#23450;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#32593;&#32476;&#19978;&#65292;&#36861;&#36394;&#22120;&#21644;&#24191;&#21578;&#21830;&#32463;&#24120;&#22312;&#26410;&#32463;&#21516;&#24847;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#21644;&#21033;&#29992;&#29992;&#25143;&#30340;&#35814;&#32454;&#34892;&#20026;&#37197;&#32622;&#25991;&#20214;&#12290;&#23613;&#31649;&#23545;&#32593;&#32476;&#36861;&#36394;&#26426;&#21046;&#21644;&#24191;&#21578;&#36827;&#34892;&#20102;&#21508;&#31181;&#30740;&#31350;&#65292;&#20294;&#36824;&#27809;&#26377;&#23545;&#38754;&#21521;&#20799;&#31461;&#30340;&#32593;&#31449;&#36827;&#34892;&#20005;&#26684;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38754;&#21521;&#20799;&#31461;&#30340;&#32593;&#31449;&#36827;&#34892;&#36861;&#36394;&#21644;&#65288;&#23450;&#21521;&#65289;&#24191;&#21578;&#30340;&#27979;&#37327;&#26041;&#27861;&#12290;&#22312;&#32570;&#20047;&#20840;&#38754;&#30340;&#38024;&#23545;&#20799;&#31461;&#30340;&#32593;&#31449;&#65288;&#21363;&#38754;&#21521;&#20799;&#31461;&#30340;&#32593;&#31449;&#65289;&#21015;&#34920;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#32593;&#39029;&#26631;&#39064;&#21644;&#25551;&#36848;&#30340;&#22810;&#35821;&#35328;&#20998;&#31867;&#22120;&#12290;&#23558;&#35813;&#20998;&#31867;&#22120;&#24212;&#29992;&#20110;&#36229;&#36807;&#20004;&#30334;&#19975;&#20010;&#32593;&#39029;&#20013;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;&#20004;&#21315;&#20010;&#20799;&#31461;&#32593;&#31449;&#30340;&#21015;&#34920;&#12290;&#36890;&#36807;&#20174;&#20116;&#20010;&#35266;&#27979;&#28857;&#29228;&#21462;&#36825;&#20123;&#32593;&#31449;&#65292;&#25105;&#20204;&#27979;&#37327;&#20102;&#36861;&#36394;&#22120;&#12289;&#25351;&#32441;&#33050;&#26412;&#21644;&#24191;&#21578;&#30340;&#26222;&#36941;&#23384;&#22312;&#12290;&#25105;&#20204;&#30340;&#29228;&#34411;&#26816;&#27979;&#21040;&#22312;&#38754;&#21521;&#20799;&#31461;&#30340;&#32593;&#31449;&#19978;&#26174;&#31034;&#30340;&#24191;&#21578;&#65292;&#24182;&#36890;&#36807;&#25235;&#21462;&#24191;&#21578;&#25259;&#38706;&#39029;&#38754;&#26469;&#30830;&#23450;&#26159;&#21542;&#21551;&#29992;&#20102;&#24191;&#21578;&#23450;&#21521;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;ar
&lt;/p&gt;
&lt;p&gt;
On the modern web, trackers and advertisers frequently construct and monetize users' detailed behavioral profiles without consent. Despite various studies on web tracking mechanisms and advertisements, there has been no rigorous study focusing on websites targeted at children. To address this gap, we present a measurement of tracking and (targeted) advertising on websites directed at children. Motivated by lacking a comprehensive list of child-directed (i.e., targeted at children) websites, we first build a multilingual classifier based on web page titles and descriptions. Applying this classifier to over two million pages, we compile a list of two thousand child-directed websites. Crawling these sites from five vantage points, we measure the prevalence of trackers, fingerprinting scripts, and advertisements. Our crawler detects ads displayed on child-directed websites and determines if ad targeting is enabled by scraping ad disclosure pages whenever available. Our results show that ar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#35299;&#32544;&#36755;&#20837;&#34920;&#31034;&#20026;&#19981;&#21464;&#29305;&#24449;&#21644;&#24179;&#21488;&#30456;&#20851;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#26410;&#35265;&#24179;&#21488;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02080</link><description>&lt;p&gt;
&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#22240;&#26524;&#24341;&#23548;&#35299;&#32544;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Causality Guided Disentanglement for Cross-Platform Hate Speech Detection. (arXiv:2308.02080v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#35299;&#32544;&#36755;&#20837;&#34920;&#31034;&#20026;&#19981;&#21464;&#29305;&#24449;&#21644;&#24179;&#21488;&#30456;&#20851;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#26410;&#35265;&#24179;&#21488;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22312;&#20419;&#36827;&#20844;&#24320;&#23545;&#35805;&#26041;&#38754;&#20855;&#26377;&#20215;&#20540;&#65292;&#20294;&#20182;&#20204;&#32463;&#24120;&#34987;&#21033;&#29992;&#26469;&#20256;&#25773;&#26377;&#23475;&#20869;&#23481;&#12290;&#30446;&#21069;&#29992;&#20110;&#26816;&#27979;&#36825;&#31181;&#26377;&#23475;&#20869;&#23481;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#20110;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#65292;&#24433;&#21709;&#21040;&#20102;&#23427;&#20204;&#36866;&#24212;&#27867;&#21270;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;&#36825;&#26159;&#22240;&#20026;&#23427;&#20204;&#20542;&#21521;&#20110;&#36807;&#20110;&#29421;&#38552;&#22320;&#20851;&#27880;&#29305;&#23450;&#30340;&#35821;&#35328;&#20449;&#21495;&#25110;&#26576;&#20123;&#35789;&#35821;&#31867;&#21035;&#30340;&#20351;&#29992;&#12290;&#24403;&#24179;&#21488;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#26102;&#65292;&#21478;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#20986;&#29616;&#20102;&#65292;&#38656;&#35201;&#36328;&#24179;&#21488;&#27169;&#22411;&#26469;&#36866;&#24212;&#19981;&#21516;&#30340;&#20998;&#24067;&#36716;&#21270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#19968;&#20010;&#24179;&#21488;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#24182;&#25512;&#24191;&#21040;&#22810;&#20010;&#26410;&#35265;&#24179;&#21488;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#19981;&#21516;&#24179;&#21488;&#30340;&#33391;&#22909;&#27867;&#21270;&#24615;&#33021;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#23558;&#36755;&#20837;&#34920;&#31034;&#35299;&#32544;&#20026;&#19981;&#21464;&#29305;&#24449;&#21644;&#24179;&#21488;&#30456;&#20851;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#35748;&#20026;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#26159;&#25552;&#20379;&#26356;&#22909;&#35299;&#32544;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media platforms, despite their value in promoting open discourse, are often exploited to spread harmful content. Current deep learning and natural language processing models used for detecting this harmful content overly rely on domain-specific terms affecting their capabilities to adapt to generalizable hate speech detection. This is because they tend to focus too narrowly on particular linguistic signals or the use of certain categories of words. Another significant challenge arises when platforms lack high-quality annotated data for training, leading to a need for cross-platform models that can adapt to different distribution shifts. Our research introduces a cross-platform hate speech detection model capable of being trained on one platform's data and generalizing to multiple unseen platforms. To achieve good generalizability across platforms, one way is to disentangle the input representations into invariant and platform-dependent features. We also argue that learning causa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26144;&#23556;&#24863;&#30693;&#30340;&#22270;&#24418;&#21387;&#32553;&#26041;&#27861;&#65288;MCond&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#33410;&#28857;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#22312;&#21512;&#25104;&#22270;&#20013;&#39640;&#25928;&#22320;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.15967</link><description>&lt;p&gt;
&#22270;&#24418;&#21387;&#32553;&#26041;&#27861;&#29992;&#20110;&#24402;&#32435;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Condensation for Inductive Node Representation Learning. (arXiv:2307.15967v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26144;&#23556;&#24863;&#30693;&#30340;&#22270;&#24418;&#21387;&#32553;&#26041;&#27861;&#65288;MCond&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#33410;&#28857;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#22312;&#21512;&#25104;&#22270;&#20013;&#39640;&#25928;&#22320;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22270;&#24341;&#23548;&#32593;&#32476;&#38754;&#20020;&#30528;&#35745;&#31639;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22270;&#24418;&#21387;&#32553;&#20316;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#25216;&#26415;&#20986;&#29616;&#20102;&#65292;&#23427;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23567;&#30340;&#21512;&#25104;&#22270;&#26469;&#39640;&#25928;&#22320;&#35757;&#32451;&#22270;&#24341;&#23548;&#32593;&#32476;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33410;&#28857;&#20043;&#38388;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#22270;&#24418;&#21387;&#32553;&#20165;&#38480;&#20110;&#21387;&#32553;&#35266;&#23519;&#21040;&#30340;&#35757;&#32451;&#33410;&#28857;&#21450;&#20854;&#23545;&#24212;&#30340;&#32467;&#26500;&#65292;&#22240;&#27492;&#32570;&#20047;&#26377;&#25928;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#22312;&#25512;&#29702;&#38454;&#27573;&#20173;&#38656;&#35201;&#21407;&#22987;&#22823;&#22270;&#26469;&#23545;&#24402;&#32435;&#33410;&#28857;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#65292;&#23548;&#33268;&#35745;&#31639;&#38656;&#27714;&#24040;&#22823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26144;&#23556;&#24863;&#30693;&#30340;&#22270;&#24418;&#21387;&#32553;&#65288;MCond&#65289;&#26041;&#27861;&#65292;&#26126;&#30830;&#23398;&#20064;&#20174;&#21407;&#22987;&#33410;&#28857;&#21040;&#21512;&#25104;&#33410;&#28857;&#30340;&#19968;&#23545;&#22810;&#33410;&#28857;&#26144;&#23556;&#65292;&#20197;&#26080;&#32541;&#22320;&#23558;&#26032;&#33410;&#28857;&#25972;&#21512;&#21040;&#21512;&#25104;&#22270;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) encounter significant computational challenges when handling large-scale graphs, which severely restricts their efficacy across diverse applications. To address this limitation, graph condensation has emerged as a promising technique, which constructs a small synthetic graph for efficiently training GNNs while retaining performance. However, due to the topology structure among nodes, graph condensation is limited to condensing only the observed training nodes and their corresponding structure, thus lacking the ability to effectively handle the unseen data. Consequently, the original large graph is still required in the inference stage to perform message passing to inductive nodes, resulting in substantial computational demands. To overcome this issue, we propose mapping-aware graph condensation (MCond), explicitly learning the one-to-many node mapping from original nodes to synthetic nodes to seamlessly integrate new nodes into the synthetic graph for induc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#23433;&#20840;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#23433;&#20840;&#27169;&#22359;&#65292;&#36890;&#36807;&#32467;&#21512;&#21152;&#23494;&#25216;&#26415;&#21644;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;&#25216;&#26415;&#26469;&#23545;&#25239;&#36890;&#20449;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2307.11730</link><description>&lt;p&gt;
&#36890;&#36807;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;&#20943;&#36731;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense. (arXiv:2307.11730v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#23433;&#20840;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#23433;&#20840;&#27169;&#22359;&#65292;&#36890;&#36807;&#32467;&#21512;&#21152;&#23494;&#25216;&#26415;&#21644;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;&#25216;&#26415;&#26469;&#23545;&#25239;&#36890;&#20449;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#30340;&#20852;&#36215;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#22312;&#32852;&#37030;&#21442;&#19982;&#26041;&#20043;&#38388;&#36827;&#34892;&#35757;&#32451;&#65292;&#20419;&#36827;&#20102;&#20998;&#25955;&#24335;&#27169;&#22411;&#32858;&#21512;&#24182;&#20943;&#23569;&#23545;&#26381;&#21153;&#22120;&#30340;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24341;&#20837;&#20102;&#29420;&#29305;&#30340;&#36890;&#20449;&#23433;&#20840;&#25361;&#25112;&#65292;&#23578;&#26410;&#22312;&#25991;&#29486;&#20013;&#24471;&#21040;&#20805;&#20998;&#35299;&#20915;&#12290;&#36825;&#20123;&#25361;&#25112;&#20027;&#35201;&#28304;&#20110;&#32858;&#21512;&#36807;&#31243;&#30340;&#20998;&#25955;&#24615;&#36136;&#12289;&#21442;&#19982;&#32773;&#30340;&#22810;&#26679;&#21270;&#35282;&#33394;&#21644;&#36131;&#20219;&#20197;&#21450;&#32570;&#20047;&#30417;&#31649;&#21644;&#32531;&#35299;&#23041;&#32961;&#30340;&#20013;&#22830;&#26426;&#26500;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#39318;&#20808;&#30028;&#23450;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#31361;&#20986;&#20102;DFL&#36890;&#20449;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#38024;&#23545;&#36825;&#20123;&#30830;&#23450;&#30340;&#39118;&#38505;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#20026;DFL&#24179;&#21488;&#35774;&#35745;&#30340;&#23433;&#20840;&#27169;&#22359;&#26469;&#23545;&#25239;&#22522;&#20110;&#36890;&#20449;&#30340;&#25915;&#20987;&#12290;&#35813;&#27169;&#22359;&#32467;&#21512;&#20102;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#21152;&#23494;&#31561;&#23433;&#20840;&#25216;&#26415;&#19982;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;&#65288;MTD&#65289;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of Decentralized Federated Learning (DFL) has enabled the training of machine learning models across federated participants, fostering decentralized model aggregation and reducing dependence on a server. However, this approach introduces unique communication security challenges that have yet to be thoroughly addressed in the literature. These challenges primarily originate from the decentralized nature of the aggregation process, the varied roles and responsibilities of the participants, and the absence of a central authority to oversee and mitigate threats. Addressing these challenges, this paper first delineates a comprehensive threat model, highlighting the potential risks of DFL communications. In response to these identified risks, this work introduces a security module designed for DFL platforms to counter communication-based attacks. The module combines security techniques such as symmetric and asymmetric encryption with Moving Target Defense (MTD) techniques, including
&lt;/p&gt;</description></item><item><title>DiTTO&#26159;&#19968;&#31181;&#25193;&#25955;&#21551;&#21457;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;Transformer&#26550;&#26500;&#30340;&#20803;&#32032;&#65292;&#26080;&#38656;&#26102;&#38388;&#31163;&#25955;&#21270;&#36830;&#32493;&#35299;&#20915;&#26102;&#38388;&#30456;&#20851;PDEs&#65292;&#24182;&#22312;&#22810;&#32500;&#24230;&#30340;&#21508;&#31181;PDE&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09072</link><description>&lt;p&gt;
DiTTO&#65306;&#21463;&#25193;&#25955;&#21551;&#21457;&#30340;&#26102;&#31354;&#36716;&#25442;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
DiTTO: Diffusion-inspired Temporal Transformer Operator. (arXiv:2307.09072v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09072
&lt;/p&gt;
&lt;p&gt;
DiTTO&#26159;&#19968;&#31181;&#25193;&#25955;&#21551;&#21457;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;Transformer&#26550;&#26500;&#30340;&#20803;&#32032;&#65292;&#26080;&#38656;&#26102;&#38388;&#31163;&#25955;&#21270;&#36830;&#32493;&#35299;&#20915;&#26102;&#38388;&#30456;&#20851;PDEs&#65292;&#24182;&#22312;&#22810;&#32500;&#24230;&#30340;&#21508;&#31181;PDE&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#24050;&#32463;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#26368;&#36817;&#30340;&#31639;&#23376;&#23398;&#20064;&#33539;&#24335;&#30340;&#21457;&#23637;&#20351;&#24471;&#35299;&#20915;&#26356;&#24191;&#27867;PDE&#30456;&#20851;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#36830;&#32493;&#22320;&#35299;&#20915;&#26102;&#38388;&#30456;&#20851;&#30340;PDEs&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#26102;&#38388;&#31163;&#25955;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;DiTTO&#65292;&#21463;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#21551;&#21457;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#36890;&#24120;&#29992;&#20110;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#65292;&#20294;&#20854;&#26102;&#38388;&#26465;&#20214;&#26426;&#21046;&#23545;PDEs&#38750;&#24120;&#26377;&#29992;&#12290;&#25193;&#25955;&#21551;&#21457;&#30340;&#26694;&#26550;&#19982;Transformer&#26550;&#26500;&#30340;&#20803;&#32032;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20854;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26032;&#26041;&#27861;&#22312;&#22810;&#32500;&#24230;&#30340;&#24191;&#27867;PDE&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;1&#32500;Burgers&#26041;&#31243;&#65292;2&#32500;Navier-Stokes&#26041;&#31243;&#21644;2&#32500;&#21644;3&#32500;&#22768;&#27874;&#26041;&#31243;&#12290;DiTTO&#22312;&#36825;&#20123;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving partial differential equations (PDEs) using a data-driven approach has become increasingly common. The recent development of the operator learning paradigm has enabled the solution of a broader range of PDE-related problems. We propose an operator learning method to solve time-dependent PDEs continuously in time without needing any temporal discretization. The proposed approach, named DiTTO, is inspired by latent diffusion models. While diffusion models are usually used in generative artificial intelligence tasks, their time-conditioning mechanism is extremely useful for PDEs. The diffusion-inspired framework is combined with elements from the Transformer architecture to improve its capabilities.  We demonstrate the effectiveness of the new approach on a wide variety of PDEs in multiple dimensions, namely the 1-D Burgers' equation, 2-D Navier-Stokes equations, and the acoustic wave equation in 2-D and 3-D. DiTTO achieves state-of-the-art results in terms of accuracy for these p
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#31995;&#32479;&#22312;&#38544;&#34109;&#24863;&#30693;&#25915;&#20987;&#19979;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#31574;&#30053;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#22270;&#20687;&#25200;&#21160;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.08939</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#22312;&#19978;&#19979;&#25991;&#24863;&#30693;&#25915;&#20987;&#19979;&#30340;&#23433;&#20840;&#24615;&#23454;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Experimental Security Analysis of DNN-based Adaptive Cruise Control under Context-Aware Perception Attacks. (arXiv:2307.08939v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08939
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#31995;&#32479;&#22312;&#38544;&#34109;&#24863;&#30693;&#25915;&#20987;&#19979;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#31574;&#30053;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#22270;&#20687;&#25200;&#21160;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;ACC&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#39550;&#39542;&#21592;&#36741;&#21161;&#21151;&#33021;&#65292;&#29992;&#20110;&#20445;&#25345;&#26399;&#26395;&#36895;&#24230;&#21644;&#19982;&#21069;&#26041;&#36710;&#36742;&#30340;&#23433;&#20840;&#36317;&#31163;&#12290;&#26412;&#25991;&#35780;&#20272;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;ACC&#31995;&#32479;&#22312;&#38544;&#34109;&#24863;&#30693;&#25915;&#20987;&#19979;&#30340;&#23433;&#20840;&#24615;&#65292;&#35813;&#25915;&#20987;&#20250;&#23545;&#25668;&#20687;&#26426;&#25968;&#25454;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#25200;&#21160;&#65292;&#20197;&#23548;&#33268;&#21069;&#26041;&#30896;&#25758;&#20107;&#25925;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#31574;&#30053;&#65292;&#29992;&#20110;&#36873;&#25321;&#35302;&#21457;&#25915;&#20987;&#26368;&#20851;&#38190;&#30340;&#26102;&#38388;&#28857;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#36816;&#34892;&#26102;&#29983;&#25104;&#36866;&#24212;&#24615;&#22270;&#20687;&#25200;&#21160;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#38469;&#39550;&#39542;&#25968;&#25454;&#38598;&#21644;&#36924;&#30495;&#30340;&#20223;&#30495;&#24179;&#21488;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#20223;&#30495;&#24179;&#21488;&#20351;&#29992;&#20102;&#26469;&#33258;&#29983;&#20135;ACC&#31995;&#32479;&#30340;&#25511;&#21046;&#36719;&#20214;&#21644;&#29289;&#29702;&#19990;&#30028;&#39550;&#39542;&#27169;&#25311;&#22120;&#65292;&#24182;&#32771;&#34385;&#20102;&#39550;&#39542;&#21592;&#30340;&#24178;&#39044;&#20197;&#21450;&#33258;&#21160;&#32039;&#24613;&#21046;&#21160;&#65288;AEB&#65289;&#21644;&#21069;&#21521;&#30896;&#25758;&#35686;&#31034;&#65288;FCW&#65289;&#31561;&#23433;&#20840;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive Cruise Control (ACC) is a widely used driver assistance feature for maintaining desired speed and safe distance to the leading vehicles. This paper evaluates the security of the deep neural network (DNN) based ACC systems under stealthy perception attacks that strategically inject perturbations into camera data to cause forward collisions. We present a combined knowledge-and-data-driven approach to design a context-aware strategy for the selection of the most critical times for triggering the attacks and a novel optimization-based method for the adaptive generation of image perturbations at run-time. We evaluate the effectiveness of the proposed attack using an actual driving dataset and a realistic simulation platform with the control software from a production ACC system and a physical-world driving simulator while considering interventions by the driver and safety features such as Automatic Emergency Braking (AEB) and Forward Collision Warning (FCW). Experimental results sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#24418;&#24335;&#65292;&#20351;&#24471;&#22823;&#35268;&#27169;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#21464;&#24471;&#21487;&#34892;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#33258;&#28982;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#20989;&#25968;&#36924;&#36817;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#25910;&#25947;&#21040;&#26368;&#20339;&#40065;&#26834;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.08875</link><description>&lt;p&gt;
&#33258;&#28982;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#29992;&#20110;&#24102;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation. (arXiv:2307.08875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#24418;&#24335;&#65292;&#20351;&#24471;&#22823;&#35268;&#27169;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#21464;&#24471;&#21487;&#34892;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#33258;&#28982;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#20989;&#25968;&#36924;&#36817;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#25910;&#25947;&#21040;&#26368;&#20339;&#40065;&#26834;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#30830;&#23450;&#19968;&#20010;&#23545;&#35757;&#32451;&#27169;&#25311;&#22120;&#21644;&#27979;&#35797;&#29615;&#22659;&#20043;&#38388;&#30340;&#27169;&#22411;&#19981;&#21305;&#37197;&#20855;&#26377;&#33391;&#22909;&#24615;&#33021;&#30340;&#31574;&#30053;&#30340;&#30446;&#26631;&#19979;&#12290;&#20197;&#21069;&#22522;&#20110;&#31574;&#30053;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20027;&#35201;&#20851;&#27880;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#19979;&#30340;&#34920;&#26684;&#35774;&#32622;&#65292;&#35813;&#38598;&#21512;&#20415;&#20110;&#40065;&#26834;&#31574;&#30053;&#35780;&#20272;&#65292;&#20294;&#22312;&#29366;&#24577;&#25968;&#37327;&#22686;&#21152;&#26102;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#24418;&#24335;&#65292;&#19968;&#31181;&#22522;&#20110;&#21452;&#37325;&#25277;&#26679;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;&#12290;&#20004;&#32773;&#37117;&#20351;&#24471;&#21363;&#20351;&#21482;&#33021;&#35775;&#38382;&#27169;&#25311;&#22120;&#65292;&#20063;&#33021;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#33258;&#28982;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65288;RNAC&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#24182;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20110;&#36825;&#20010;RNAC&#31639;&#27861;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#25910;&#25947;&#21040;&#26368;&#20339;&#40065;&#26834;&#31574;&#30053;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#32771;&#34385;&#20989;&#25968;&#36924;&#36817;&#35823;&#24046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study robust reinforcement learning (RL) with the goal of determining a well-performing policy that is robust against model mismatch between the training simulator and the testing environment. Previous policy-based robust RL algorithms mainly focus on the tabular setting under uncertainty sets that facilitate robust policy evaluation, but are no longer tractable when the number of states scales up. To this end, we propose two novel uncertainty set formulations, one based on double sampling and the other on an integral probability metric. Both make large-scale robust RL tractable even when one only has access to a simulator. We propose a robust natural actor-critic (RNAC) approach that incorporates the new uncertainty sets and employs function approximation. We provide finite-time convergence guarantees for the proposed RNAC algorithm to the optimal robust policy within the function approximation error. Finally, we demonstrate the robust performance of the policy learned by our propo
&lt;/p&gt;</description></item><item><title>&#20803;&#20215;&#20540;&#23398;&#20064;&#26159;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#24847;&#35782;&#30340;&#23398;&#20064;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#26234;&#33021;&#20307;&#23398;&#20064;&#36807;&#31243;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20351;&#29992;&#20803;&#20215;&#20540;&#20989;&#25968;&#26469;&#25351;&#23548;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36924;&#36817;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.08863</link><description>&lt;p&gt;
&#20803;&#20215;&#20540;&#23398;&#20064;&#65306;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#24847;&#35782;&#30340;&#23398;&#20064;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Meta-Value Learning: a General Framework for Learning with Learning Awareness. (arXiv:2307.08863v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08863
&lt;/p&gt;
&lt;p&gt;
&#20803;&#20215;&#20540;&#23398;&#20064;&#26159;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#24847;&#35782;&#30340;&#23398;&#20064;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#26234;&#33021;&#20307;&#23398;&#20064;&#36807;&#31243;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20351;&#29992;&#20803;&#20215;&#20540;&#20989;&#25968;&#26469;&#25351;&#23548;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36924;&#36817;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#26799;&#24230;&#23398;&#20064;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#26799;&#24230;&#26469;&#33258;&#20110;&#19968;&#20010;&#19968;&#38454;&#27169;&#22411;&#65292;&#19981;&#32771;&#34385;&#26234;&#33021;&#20307;&#23398;&#20064;&#36807;&#31243;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;LOLA&#30340;&#24605;&#24819;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#23436;&#20840;&#36890;&#29992;&#30340;&#22522;&#20110;&#20215;&#20540;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;&#26680;&#24515;&#24605;&#24819;&#26159;&#19968;&#20010;&#31216;&#20026;&#20803;&#20215;&#20540;&#30340;&#20989;&#25968;&#65292;&#23427;&#22312;&#32852;&#21512;&#31574;&#30053;&#31354;&#38388;&#30340;&#27599;&#20010;&#28857;&#19978;&#65292;&#20026;&#27599;&#20010;&#26234;&#33021;&#20307;&#32473;&#20986;&#20854;&#26410;&#26469;&#20248;&#21270;&#27493;&#39588;&#20013;&#30446;&#26631;&#30340;&#25240;&#25187;&#24635;&#21644;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20803;&#20215;&#20540;&#30340;&#26799;&#24230;&#27604;&#21407;&#22987;&#30446;&#26631;&#30340;&#26799;&#24230;&#26356;&#21487;&#38752;&#30340;&#25913;&#36827;&#26041;&#21521;&#65292;&#22240;&#20026;&#20803;&#20215;&#20540;&#26469;&#33258;&#23545;&#20248;&#21270;&#25928;&#26524;&#30340;&#32463;&#39564;&#35266;&#23519;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#20803;&#20215;&#20540;&#65292;&#20197;&#27839;&#30528;&#26234;&#33021;&#20307;&#27839;&#30528;&#20803;&#20215;&#20540;&#26799;&#24230;&#30340;&#20248;&#21270;&#36712;&#36857;&#36827;&#34892;TD&#35823;&#24046;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-based learning in multi-agent systems is difficult because the gradient derives from a first-order model which does not account for the interaction between agents' learning processes. LOLA (arXiv:1709.04326) accounts for this by differentiating through one step of optimization. We extend the ideas of LOLA and develop a fully-general value-based approach to optimization. At the core is a function we call the meta-value, which at each point in joint-policy space gives for each agent a discounted sum of its objective over future optimization steps. We argue that the gradient of the meta-value gives a more reliable improvement direction than the gradient of the original objective, because the meta-value derives from empirical observations of the effects of optimization. We show how the meta-value can be approximated by training a neural network to minimize TD error along optimization trajectories in which agents follow the gradient of the meta-value. We analyze the behavior of our
&lt;/p&gt;</description></item><item><title>&#20256;&#25773;&#23398;&#39046;&#22495;&#20013;&#30340;&#33258;&#21160;&#21270;&#20869;&#23481;&#20998;&#26512;&#24120;&#24573;&#35270;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#20171;&#32461;&#24182;&#27979;&#35797;&#20102;&#32479;&#35745;&#26041;&#27861;&#26469;&#32416;&#27491;&#36825;&#31181;&#20559;&#24046;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20462;&#22797;&#20043;&#12290;</title><link>http://arxiv.org/abs/2307.06483</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#20869;&#23481;&#20998;&#26512;&#20013;&#30340;&#38169;&#35823;&#20998;&#31867;&#23548;&#33268;&#22238;&#24402;&#20998;&#26512;&#20013;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#33021;&#20462;&#22797;&#21527;&#65311;&#26159;&#30340;&#65292;&#25105;&#20204;&#33021;&#65281;
&lt;/p&gt;
&lt;p&gt;
Misclassification in Automated Content Analysis Causes Bias in Regression. Can We Fix It? Yes We Can!. (arXiv:2307.06483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06483
&lt;/p&gt;
&lt;p&gt;
&#20256;&#25773;&#23398;&#39046;&#22495;&#20013;&#30340;&#33258;&#21160;&#21270;&#20869;&#23481;&#20998;&#26512;&#24120;&#24573;&#35270;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#20171;&#32461;&#24182;&#27979;&#35797;&#20102;&#32479;&#35745;&#26041;&#27861;&#26469;&#32416;&#27491;&#36825;&#31181;&#20559;&#24046;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20462;&#22797;&#20043;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20998;&#31867;&#22120;&#65288;ACs&#65289;&#36890;&#24120;&#36890;&#36807;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;SML&#65289;&#26500;&#24314;&#65292;&#21487;&#20197;&#23545;&#20174;&#25991;&#26412;&#21040;&#22270;&#29255;&#21644;&#35270;&#39057;&#30340;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#24050;&#32463;&#25104;&#20026;&#20256;&#25773;&#31185;&#23398;&#21644;&#30456;&#20851;&#39046;&#22495;&#20013;&#24191;&#27867;&#27969;&#34892;&#30340;&#27979;&#37327;&#35774;&#22791;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#21363;&#20351;&#26159;&#39640;&#24230;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#20063;&#20250;&#20135;&#29983;&#38169;&#35823;&#65292;&#36825;&#23548;&#33268;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#21644;&#19979;&#28216;&#20998;&#26512;&#20013;&#35823;&#23548;&#24615;&#30340;&#32467;&#26524;&#65292;&#38500;&#38750;&#36825;&#20123;&#20998;&#26512;&#32771;&#34385;&#21040;&#36825;&#20123;&#38169;&#35823;&#12290;&#36890;&#36807;&#23545;SML&#24212;&#29992;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#25773;&#23398;&#32773;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#12290;&#21407;&#21017;&#19978;&#65292;&#29616;&#26377;&#30340;&#32479;&#35745;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#8220;&#40644;&#37329;&#26631;&#20934;&#8221;&#39564;&#35777;&#25968;&#25454;&#65288;&#22914;&#30001;&#20154;&#31867;&#27880;&#37322;&#32773;&#21019;&#24314;&#30340;&#25968;&#25454;&#65289;&#26469;&#32416;&#27491;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#65292;&#24182;&#20135;&#29983;&#19968;&#33268;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#20171;&#32461;&#24182;&#27979;&#35797;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#21253;&#25324;&#25105;&#20204;&#22312;R&#21253;misclassificationmodels&#20013;&#35774;&#35745;&#21644;&#23454;&#29616;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#26469;&#25581;&#31034;&#27599;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated classifiers (ACs), often built via supervised machine learning (SML), can categorize large, statistically powerful samples of data ranging from text to images and video, and have become widely popular measurement devices in communication science and related fields. Despite this popularity, even highly accurate classifiers make errors that cause misclassification bias and misleading results in downstream analyses-unless such analyses account for these errors. As we show in a systematic literature review of SML applications, communication scholars largely ignore misclassification bias. In principle, existing statistical methods can use "gold standard" validation data, such as that created by human annotators, to correct misclassification bias and produce consistent estimates. We introduce and test such methods, including a new method we design and implement in the R package misclassificationmodels, via Monte Carlo simulations designed to reveal each method's limitations, which 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#22312;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#27604;&#30340;&#24615;&#33021;&#34920;&#29616;&#19978;&#30456;&#24403;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#27169;&#22411;&#36234;&#22823;&#30340;&#24773;&#20917;&#19979;&#25928;&#29575;&#36234;&#39640;&#65292;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05695</link><description>&lt;p&gt;
&#20197;&#19981;&#21516;&#26041;&#24335;&#22534;&#21472;&#26356;&#22810;&#23618;&#65306;&#36890;&#36807;&#20302;&#31209;&#26356;&#26032;&#36827;&#34892;&#39640;&#31209;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Stack More Layers Differently: High-Rank Training Through Low-Rank Updates. (arXiv:2307.05695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#22312;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#27604;&#30340;&#24615;&#33021;&#34920;&#29616;&#19978;&#30456;&#24403;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#27169;&#22411;&#36234;&#22823;&#30340;&#24773;&#20917;&#19979;&#25928;&#29575;&#36234;&#39640;&#65292;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#32593;&#32476;&#25317;&#26377;&#25968;&#30334;&#20159;&#20010;&#21442;&#25968;&#30340;&#35268;&#27169;&#24050;&#32463;&#21344;&#20027;&#23548;&#22320;&#20301;&#24182;&#19988;&#25928;&#26524;&#26174;&#33879;&#65292;&#20294;&#23545;&#20110;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#35757;&#32451;&#24517;&#35201;&#24615;&#20173;&#28982;&#32570;&#20047;&#28165;&#26224;&#30340;&#29702;&#35299;&#65292;&#32780;&#26367;&#20195;&#26041;&#27861;&#19981;&#19968;&#23450;&#33021;&#22815;&#38477;&#20302;&#35757;&#32451;&#39640;&#24615;&#33021;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#39640;&#31209;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;ReLoRA&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#21442;&#25968;&#37327;&#39640;&#36798;350M&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#30340;&#25928;&#29575;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#25552;&#39640;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#30340;&#28508;&#21147;&#21450;&#20854;&#23545;&#20110;&#32553;&#25918;&#23450;&#24459;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters, the necessity to train overparametrized models remains poorly understood, and alternative approaches do not necessarily make it cheaper to train high-performance models. In this paper, we explore low-rank training techniques as an alternative approach to training large neural networks. We introduce a novel method called ReLoRA, which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to pre-training transformer language models with up to 350M parameters and demonstrate comparable performance to regular neural network training. Furthermore, we observe that the efficiency of ReLoRA increases with model size, making it a promising approach for training multi-billion-parameter networks efficiently. Our findings shed light on the potential of low-rank training techniques and their implications for scaling laws.
&lt;/p&gt;</description></item><item><title>&#21452;&#21521;&#27880;&#24847;&#21147;&#27169;&#22411;&#20855;&#26377;&#28151;&#21512;&#19987;&#23478;&#26435;&#37325;&#65292;&#31867;&#20284;&#20110;&#36830;&#32493;&#35789;&#34955;&#27169;&#22411;&#65288;CBOW&#65289;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#23427;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.04057</link><description>&lt;p&gt;
&#21452;&#21521;&#27880;&#24847;&#21147;&#20316;&#20026;&#36830;&#32493;&#35789;&#19987;&#23478;&#30340;&#28151;&#21512;&#29289;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Attention as a Mixture of Continuous Word Experts. (arXiv:2307.04057v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04057
&lt;/p&gt;
&lt;p&gt;
&#21452;&#21521;&#27880;&#24847;&#21147;&#27169;&#22411;&#20855;&#26377;&#28151;&#21512;&#19987;&#23478;&#26435;&#37325;&#65292;&#31867;&#20284;&#20110;&#36830;&#32493;&#35789;&#34955;&#27169;&#22411;&#65288;CBOW&#65289;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#23427;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#21521;&#27880;&#24847;&#21147;&#30001;&#20301;&#32622;&#32534;&#30721;&#21644;&#23631;&#34109;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#30446;&#26631;&#32452;&#25104;&#30340;&#33258;&#27880;&#24847;&#21147;&#26500;&#25104;&#65292;&#24050;&#25104;&#20026;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;&#23613;&#31649;&#23427;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#23427;&#30340;&#32479;&#35745;&#22522;&#30784;&#65306;&#21452;&#21521;&#27880;&#24847;&#21147;&#38544;&#21547;&#22320;&#25311;&#21512;&#20102;&#20160;&#20040;&#32479;&#35745;&#27169;&#22411;&#65311;&#23427;&#19982;&#38750;&#27880;&#24847;&#26426;&#21046;&#30340;&#20808;&#39537;&#26377;&#20309;&#19981;&#21516;&#65311;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#37325;&#26032;&#21442;&#25968;&#21270;&#21518;&#65292;&#25311;&#21512;&#21333;&#23618;&#21333;&#22836;&#21452;&#21521;&#27880;&#24847;&#21147;&#31561;&#20110;&#25311;&#21512;&#20855;&#26377;&#28151;&#21512;&#19987;&#23478;&#26435;&#37325;&#30340;&#36830;&#32493;&#35789;&#34955;&#65288;CBOW&#65289;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20855;&#26377;&#22810;&#20010;&#22836;&#21644;&#22810;&#20010;&#23618;&#30340;&#21452;&#21521;&#27880;&#24847;&#21147;&#31561;&#20215;&#20110;&#22534;&#21472;&#30340;MoEs&#21644;MoEs&#30340;&#28151;&#21512;&#12290;&#36825;&#20010;&#32479;&#35745;&#35266;&#28857;&#25581;&#31034;&#20102;MoE&#22312;&#21452;&#21521;&#27880;&#24847;&#21147;&#20013;&#30340;&#29420;&#29305;&#29992;&#36884;&#65292;&#36825;&#19982;&#20854;&#22312;&#22788;&#29702;&#24322;&#26500;&#24615;&#26041;&#38754;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bidirectional attention $\unicode{x2013}$ composed of self-attention with positional encodings and the masked language model (MLM) objective $\unicode{x2013}$ has emerged as a key component of modern large language models (LLMs). Despite its empirical success, few studies have examined its statistical underpinnings: What statistical model is bidirectional attention implicitly fitting? What sets it apart from its non-attention predecessors? We explore these questions in this paper. The key observation is that fitting a single-layer single-head bidirectional attention, upon reparameterization, is equivalent to fitting a continuous bag of words (CBOW) model with mixture-of-experts (MoE) weights. Further, bidirectional attention with multiple heads and multiple layers is equivalent to stacked MoEs and a mixture of MoEs, respectively. This statistical viewpoint reveals the distinct use of MoE in bidirectional attention, which aligns with its practical effectiveness in handling heterogeneous
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;EHRSHOT&#65292;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#22522;&#20934;&#12290;&#35813;&#35770;&#25991;&#21033;&#29992;EHRSHOT&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;CLMBR-T-base&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;ML&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.02028</link><description>&lt;p&gt;
EHRSHOT:&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models. (arXiv:2307.02028v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;EHRSHOT&#65292;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#22522;&#20934;&#12290;&#35813;&#35770;&#25991;&#21033;&#29992;EHRSHOT&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;CLMBR-T-base&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;ML&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#19968;&#33324;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#31038;&#21306;&#24050;&#32463;&#21463;&#30410;&#20110;&#20844;&#24320;&#30340;&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#27169;&#22411;&#65292;&#20294;&#26159;ML&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#20849;&#20139;&#36164;&#20135;&#30340;&#32570;&#20047;&#30340;&#38459;&#30861;&#12290;&#22522;&#30784;&#27169;&#22411;&#30340;&#25104;&#21151;&#20026;&#21307;&#30103;&#20445;&#20581;ML&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#35775;&#38382;&#20849;&#20139;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#39564;&#35777;&#24615;&#33021;&#20248;&#21183;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#36129;&#29486;&#26469;&#24110;&#21161;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;EHRSHOT&#65292;&#20854;&#20013;&#21253;&#21547;6,739&#21517;&#26469;&#33258;&#26031;&#22374;&#31119;&#21307;&#23398;&#30340;&#24739;&#32773;&#30340;&#21435;&#35782;&#21035;&#32467;&#26500;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHR)&#25968;&#25454;&#12290;&#19982;MIMIC-III/IV&#21644;&#20854;&#20182;&#27969;&#34892;&#30340;EHR&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;EHRSHOT&#26159;&#32437;&#21521;&#30340;&#65292;&#19981;&#20165;&#23616;&#38480;&#20110;ICU/ED&#24739;&#32773;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;CLMBR-T-base&#30340;&#26435;&#37325;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32467;&#26500;&#21270;EHR&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#30340;141M&#21442;&#25968;&#20020;&#24202;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#25968;&#25454;&#21253;&#25324;2.57M&#21517;&#24739;&#32773;&#12290;&#25105;&#20204;&#26159;&#26368;&#26089;&#23436;&#20840;&#21457;&#24067;&#36825;&#26679;&#19968;&#20010;&#29992;&#20110;&#32534;&#30721;EHR&#25968;&#25454;&#30340;&#27169;&#22411;&#20043;&#19968;&#65307;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#21457;&#24067;&#30340;&#20020;&#24202;&#25968;&#25454;&#27169;&#22411;&#65288;&#22914;GatorTron&#12289;ClinicalBER&#65289;&#24182;&#27809;&#26377;&#23436;&#20840;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the general machine learning (ML) community has benefited from public datasets, tasks, and models, the progress of ML in healthcare has been hampered by a lack of such shared assets. The success of foundation models creates new challenges for healthcare ML by requiring access to shared pretrained models to validate performance benefits. We help address these challenges through three contributions. First, we publish a new dataset, EHRSHOT, which contains deidentified structured data from the electronic health records (EHRs) of 6,739 patients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients. Second, we publish the weights of CLMBR-T-base, a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients. We are one of the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data (e.g. GatorTron, ClinicalBER
&lt;/p&gt;</description></item><item><title>&#22312;&#26080;&#38480;&#28145;&#24230;&#21644;&#23485;&#24230;&#30340;&#27604;&#20363;&#26497;&#38480;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#20462;&#25913;Softmax-based&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;Transformer&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#26102;&#65292;&#26497;&#38480;&#20998;&#24067;&#21487;&#20197;&#29992;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#25551;&#36848;&#12290;&#36890;&#36807;&#20462;&#25913;&#27880;&#24847;&#21147;&#26426;&#21046;&#24182;&#20351;&#29992;&#27531;&#24046;&#36830;&#25509;&#65292;&#25105;&#20204;&#21487;&#20197;&#25511;&#21046;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#21644;&#21327;&#26041;&#24046;&#32467;&#26500;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.17759</link><description>&lt;p&gt;
&#21463;&#24418;&#29366;&#25913;&#21464;&#30340;Transformer&#65306;&#22312;&#26080;&#38480;&#28145;&#24230;&#21644;&#23485;&#24230;&#26497;&#38480;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit. (arXiv:2306.17759v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17759
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#38480;&#28145;&#24230;&#21644;&#23485;&#24230;&#30340;&#27604;&#20363;&#26497;&#38480;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#20462;&#25913;Softmax-based&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;Transformer&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#26102;&#65292;&#26497;&#38480;&#20998;&#24067;&#21487;&#20197;&#29992;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#25551;&#36848;&#12290;&#36890;&#36807;&#20462;&#25913;&#27880;&#24847;&#21147;&#26426;&#21046;&#24182;&#20351;&#29992;&#27531;&#24046;&#36830;&#25509;&#65292;&#25105;&#20204;&#21487;&#20197;&#25511;&#21046;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#21644;&#21327;&#26041;&#24046;&#32467;&#26500;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#65292;&#34920;&#31034;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#29992;&#20316;&#26816;&#26597;&#32593;&#32476;&#21487;&#35757;&#32451;&#24615;&#30340;&#20195;&#29702;&#12290;&#21463;Transformer&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26080;&#38480;&#28145;&#24230;&#21644;&#23485;&#24230;&#30340;&#27604;&#20363;&#26497;&#38480;&#19979;&#65292;&#24102;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#20462;&#25913;Softmax-based&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#21021;&#22987;&#21270;&#26102;&#65292;&#26497;&#38480;&#20998;&#24067;&#21487;&#20197;&#29992;&#28145;&#24230;&#19982;&#23485;&#24230;&#27604;&#29575;&#20026;&#32034;&#24341;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#26469;&#25551;&#36848;&#12290;&#20026;&#20102;&#23454;&#29616;&#33391;&#23450;&#20041;&#30340;&#38543;&#26426;&#26497;&#38480;&#65292;Transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#36890;&#36807;&#23558;Softmax&#36755;&#20986;&#23621;&#20013;&#22312;&#21333;&#20301;&#30697;&#38453;&#19978;&#65292;&#24182;&#36890;&#36807;&#23485;&#24230;&#30456;&#20851;&#30340;&#28201;&#24230;&#21442;&#25968;&#23545;Softmax logits&#36827;&#34892;&#32553;&#25918;&#26469;&#36827;&#34892;&#20462;&#25913;&#12290;&#25105;&#20204;&#36890;&#36807;&#30456;&#24212;&#30340;SDE&#30740;&#31350;&#20102;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#27531;&#24046;&#36830;&#25509;&#20248;&#38597;&#22320;&#25511;&#21046;&#28418;&#31227;&#21644;&#25193;&#25955;&#30340;&#23610;&#24230;&#12290;&#31283;&#23450;SDE&#30340;&#23384;&#22312;&#24847;&#21619;&#30528;&#21327;&#26041;&#24046;&#32467;&#26500;&#26159;&#33391; behaved &#30340;&#65292;&#21363;&#20351;&#23545;&#20110;&#38750;&#24120;&#22823;&#30340;&#28145;&#24230;&#21644;&#23485;&#24230;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
In deep learning theory, the covariance matrix of the representations serves as a proxy to examine the network's trainability. Motivated by the success of Transformers, we study the covariance matrix of a modified Softmax-based attention model with skip connections in the proportional limit of infinite-depth-and-width. We show that at initialization the limiting distribution can be described by a stochastic differential equation (SDE) indexed by the depth-to-width ratio. To achieve a well-defined stochastic limit, the Transformer's attention mechanism is modified by centering the Softmax output at identity, and scaling the Softmax logits by a width-dependent temperature parameter. We examine the stability of the network through the corresponding SDE, showing how the scale of both the drift and diffusion can be elegantly controlled with the aid of residual connections. The existence of a stable SDE implies that the covariance structure is well-behaved, even for very large depth and widt
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32508;&#21512;&#24615;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;LabelBench&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.09910</link><description>&lt;p&gt;
LabelBench&#65306;&#22522;&#20110;&#32508;&#21512;&#26694;&#26550;&#30340;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient Learning. (arXiv:2306.09910v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32508;&#21512;&#24615;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;LabelBench&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;&#25968;&#25454;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#30340;&#20851;&#38190;&#65292;&#20294;&#33719;&#21462;&#26631;&#35760;&#21487;&#33021;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#20943;&#32531;&#36825;&#19968;&#25104;&#26412;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;&#36801;&#31227;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#65289;&#26088;&#22312;&#23454;&#29616;&#26631;&#31614;&#39640;&#25928;&#24615;&#65306;&#20174;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#20013;&#23454;&#29616;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#34429;&#28982;&#22312;&#23454;&#36341;&#20013;&#33719;&#24471;&#26368;&#20339;&#30340;&#26631;&#31614;&#25928;&#29575;&#36890;&#24120;&#38656;&#35201;&#36825;&#20123;&#25216;&#26415;&#30340;&#32452;&#21512;&#65292;&#20294;&#29616;&#26377;&#30340;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;&#24182;&#27809;&#26377;&#25429;&#25417;&#21040;&#25152;&#26377;&#36825;&#20123;&#25216;&#26415;&#30340;&#21327;&#21516;&#32452;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;LabelBench&#35299;&#20915;&#20102;&#36825;&#20010;&#32570;&#38519;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#32508;&#21512;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#35780;&#20272;&#22810;&#20010;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#25216;&#26415;&#12290;&#20316;&#20026;LabelBench&#30340;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#19968;&#36215;&#20351;&#29992;&#30340;&#26368;&#26032;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#26032;&#22522;&#20934;&#65292;&#29992;&#20110;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#35777;&#26126;&#20102;&#27604;&#20808;&#21069;&#25253;&#21578;&#30340;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeled data are critical to modern machine learning applications, but obtaining labels can be expensive. To mitigate this cost, machine learning methods, such as transfer learning, semi-supervised learning and active learning, aim to be label-efficient: achieving high predictive performance from relatively few labeled examples. While obtaining the best label-efficiency in practice often requires combinations of these techniques, existing benchmark and evaluation frameworks do not capture a concerted combination of all such techniques. This paper addresses this deficiency by introducing LabelBench, a new computationally-efficient framework for joint evaluation of multiple label-efficient learning techniques. As an application of LabelBench, we introduce a novel benchmark of state-of-the-art active learning methods in combination with semi-supervised learning for fine-tuning pretrained vision transformers. Our benchmark demonstrates better label-efficiencies than previously reported in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#21644;&#22522;&#20934;&#65292;&#29992;&#20110;&#32452;&#21512;&#21644;&#28151;&#21512;&#21464;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#24182;&#25552;&#20379;&#22810;&#26679;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#27492;&#26694;&#26550;&#65292;&#20316;&#32773;&#23637;&#31034;&#20102;4&#31181;&#24120;&#35265;&#30340;MCBO&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2306.09803</link><description>&lt;p&gt;
&#32452;&#21512;&#21644;&#28151;&#21512;&#21464;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26694;&#26550;&#21644;&#22522;&#20934;&#12290; (arXiv:2306.09803v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Framework and Benchmarks for Combinatorial and Mixed-variable Bayesian Optimization. (arXiv:2306.09803v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#21644;&#22522;&#20934;&#65292;&#29992;&#20110;&#32452;&#21512;&#21644;&#28151;&#21512;&#21464;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#24182;&#25552;&#20379;&#22810;&#26679;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#27492;&#26694;&#26550;&#65292;&#20316;&#32773;&#23637;&#31034;&#20102;4&#31181;&#24120;&#35265;&#30340;MCBO&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#28151;&#21512;&#21464;&#37327;&#21644;&#32452;&#21512;&#36125;&#21494;&#26031;&#20248;&#21270;(MCBO)&#26469;&#35299;&#20915;&#39046;&#22495;&#20013;&#32570;&#20047;&#31995;&#32479;&#21270;&#22522;&#20934;&#21644;&#26631;&#20934;&#21270;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;MCBO&#35770;&#25991;&#36890;&#24120;&#24341;&#20837;&#38750;&#22810;&#26679;&#24615;&#25110;&#38750;&#26631;&#20934;&#22522;&#20934;&#26469;&#35780;&#20272;&#20854;&#26041;&#27861;&#65292;&#38459;&#30861;&#20102;&#19981;&#21516;MCBO&#21407;&#35821;&#21450;&#20854;&#32452;&#21512;&#30340;&#27491;&#30830;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#20171;&#32461;&#21333;&#20010;MCBO&#21407;&#35821;&#30340;&#35770;&#25991;&#36890;&#24120;&#30465;&#30053;&#20102;&#38024;&#23545;&#20351;&#29992;&#30456;&#21516;&#26041;&#27861;&#36827;&#34892;&#21097;&#20313;&#21407;&#35821;&#30340;&#22522;&#32447;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#36825;&#31181;&#30465;&#30053;&#20027;&#35201;&#26159;&#30001;&#20110;&#28041;&#21450;&#30340;&#23454;&#29616;&#24037;&#20316;&#37327;&#38750;&#24120;&#22823;&#65292;&#23548;&#33268;&#32570;&#20047;&#25511;&#21046;&#35780;&#20272;&#24182;&#26080;&#27861;&#26377;&#25928;&#23637;&#31034;&#36129;&#29486;&#30340;&#20248;&#28857;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20351;&#36125;&#21494;&#26031;&#20248;&#21270;&#32452;&#20214;&#30340;&#32452;&#21512;&#36731;&#26494;&#26131;&#34892;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#26679;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#27979;&#35797;&#20219;&#21153;&#12290;&#21033;&#29992;&#36825;&#31181;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;4&#31181;&#24120;&#35265;&#30340;MCBO&#25216;&#26415;&#65292;&#24182;&#22312;&#21508;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a modular framework for Mixed-variable and Combinatorial Bayesian Optimization (MCBO) to address the lack of systematic benchmarking and standardized evaluation in the field. Current MCBO papers often introduce non-diverse or non-standard benchmarks to evaluate their methods, impeding the proper assessment of different MCBO primitives and their combinations. Additionally, papers introducing a solution for a single MCBO primitive often omit benchmarking against baselines that utilize the same methods for the remaining primitives. This omission is primarily due to the significant implementation overhead involved, resulting in a lack of controlled assessments and an inability to showcase the merits of a contribution effectively. To overcome these challenges, our proposed framework enables an effortless combination of Bayesian Optimization components, and provides a diverse set of synthetic and real-world benchmarking tasks. Leveraging this flexibility, we implement 4
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#29992;&#20110;&#22823;&#35268;&#27169;&#22330;&#26223;&#19979;&#37327;&#23376;&#21487;&#20998;&#24615;&#30340;&#36817;&#20284;&#35299;&#65292;&#36890;&#36807;&#26377;&#25928;&#31639;&#27861;&#36817;&#20284;&#26597;&#25214;&#26368;&#36817;&#30340;&#21487;&#20998;&#31163;&#23494;&#24230;&#30697;&#38453;&#65292;&#24182;&#23558;&#37327;&#23376;&#21487;&#20998;&#24615;&#35270;&#20026;&#20998;&#31867;&#38382;&#39064;&#65292;&#23545;&#20219;&#20309;&#20108;&#32500;&#28151;&#21512;&#29366;&#24577;&#37117;&#36866;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.09444</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#22797;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#22823;&#35268;&#27169;&#37327;&#23376;&#21487;&#20998;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Quantum Separability Through a Reproducible Machine Learning Lens. (arXiv:2306.09444v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#29992;&#20110;&#22823;&#35268;&#27169;&#22330;&#26223;&#19979;&#37327;&#23376;&#21487;&#20998;&#24615;&#30340;&#36817;&#20284;&#35299;&#65292;&#36890;&#36807;&#26377;&#25928;&#31639;&#27861;&#36817;&#20284;&#26597;&#25214;&#26368;&#36817;&#30340;&#21487;&#20998;&#31163;&#23494;&#24230;&#30697;&#38453;&#65292;&#24182;&#23558;&#37327;&#23376;&#21487;&#20998;&#24615;&#35270;&#20026;&#20998;&#31867;&#38382;&#39064;&#65292;&#23545;&#20219;&#20309;&#20108;&#32500;&#28151;&#21512;&#29366;&#24577;&#37117;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#21487;&#20998;&#24615;&#38382;&#39064;&#26159;&#25351;&#22914;&#20309;&#21028;&#26029;&#19968;&#20010;&#20108;&#20998;&#20307;&#23494;&#24230;&#30697;&#38453;&#26159;&#32416;&#32544;&#30340;&#36824;&#26159;&#21487;&#20998;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#65292;&#29992;&#20110;&#22312;&#22823;&#35268;&#27169;&#22330;&#26223;&#19979;&#25214;&#21040;&#27492;NP-&#38590;&#38382;&#39064;&#30340;&#36817;&#20284;&#35299;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Frank-Wolfe&#30340;&#26377;&#25928;&#31639;&#27861;&#26469;&#36817;&#20284;&#26597;&#25214;&#26368;&#36817;&#30340;&#21487;&#20998;&#31163;&#23494;&#24230;&#30697;&#38453;&#65292;&#24182;&#25512;&#23548;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#23558;&#23494;&#24230;&#30697;&#38453;&#26631;&#35760;&#20026;&#21487;&#20998;&#31163;&#30340;&#25110;&#32416;&#32544;&#30340;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#37327;&#23376;&#21487;&#20998;&#24615;&#35270;&#20026;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#20108;&#32500;&#28151;&#21512;&#29366;&#24577;&#12290;&#23545;3-&#21644;7&#32500;&#24230;&#20013;&#30340;&#37327;&#23376;&#24577;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#31243;&#24207;&#30340;&#25928;&#29575;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#25193;&#23637;&#21040;&#19978;&#21315;&#20010;&#23494;&#24230;&#30697;&#38453;&#65292;&#24182;&#20855;&#26377;&#39640;&#37327;&#23376;&#32416;&#32544;&#26816;&#27979;&#31934;&#24230;&#12290;&#36825;&#19968;&#36827;&#23637;&#26377;&#21161;&#20110;&#22522;&#20934;&#27979;&#35797;&#37327;&#23376;&#21487;&#20998;&#24615;&#65292;&#24182;&#25903;&#25345;&#26356;&#24378;&#22823;&#30340;&#32416;&#32544;&#26816;&#27979;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quantum separability problem consists in deciding whether a bipartite density matrix is entangled or separable. In this work, we propose a machine learning pipeline for finding approximate solutions for this NP-hard problem in large-scale scenarios. We provide an efficient Frank-Wolfe-based algorithm to approximately seek the nearest separable density matrix and derive a systematic way for labeling density matrices as separable or entangled, allowing us to treat quantum separability as a classification problem. Our method is applicable to any two-qudit mixed states. Numerical experiments with quantum states of 3- and 7-dimensional qudits validate the efficiency of the proposed procedure, and demonstrate that it scales up to thousands of density matrices with a high quantum entanglement detection accuracy. This takes a step towards benchmarking quantum separability to support the development of more powerful entanglement detection techniques.
&lt;/p&gt;</description></item><item><title>TSMixer&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#23646;&#24615;&#24182;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;Transformers&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09364</link><description>&lt;p&gt;
TSMixer: &#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting. (arXiv:2306.09364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09364
&lt;/p&gt;
&lt;p&gt;
TSMixer&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#23646;&#24615;&#24182;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;Transformers&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22240;&#20854;&#33021;&#22815;&#25429;&#25417;&#38271;&#24207;&#21015;&#20132;&#20114;&#32780;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#20854;&#20869;&#23384;&#21644;&#35745;&#31639;&#35201;&#27714;&#39640;&#30340;&#38382;&#39064;&#23545;&#38271;&#26399;&#39044;&#27979;&#26500;&#25104;&#20102;&#20005;&#37325;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TSMixer&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#31070;&#32463;&#26550;&#26500;&#65292;&#19987;&#20026;&#22810;&#20803;&#39044;&#27979;&#21644;&#34917;&#19969;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#32780;&#35774;&#35745;&#65292;&#26159;Transformers&#30340;&#26377;&#25928;&#26367;&#20195;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20511;&#37492;&#20102;MLP-Mixer&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#25104;&#21151;&#32463;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#35270;&#35273;MLP-Mixer&#36866;&#24212;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#25361;&#25112;&#65292;&#24182;&#24341;&#20837;&#20102;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#30340;&#32452;&#20214;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#36825;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#35774;&#35745;&#33539;&#24335;&#65292;&#21363;&#23558;&#22312;&#32447;&#21327;&#35843;&#22836;&#38468;&#21152;&#21040;MLP-Mixer&#39592;&#24178;&#19978;&#65292;&#20197;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#30340;&#23646;&#24615;&#65292;&#22914;&#23618;&#27425;&#32467;&#26500;&#21644;&#36890;&#36947;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36890;&#36947;&#24314;&#27169;&#26041;&#27861;&#65292;&#24179;&#34913;&#20102;&#32534;&#30721;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#36890;&#36947;&#21644;&#20445;&#30041;&#21333;&#20010;&#36890;&#36947;&#20449;&#24687;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TSMixer&#22312;&#19968;&#20803;&#21644;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#27604;&#22522;&#20110;Transformers&#30340;&#26041;&#27861;&#23569;&#24471;&#22810;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have gained popularity in time series forecasting for their ability to capture long-sequence interactions. However, their high memory and computing requirements pose a critical bottleneck for long-term forecasting. To address this, we propose TSMixer, a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules. TSMixer is designed for multivariate forecasting and representation learning on patched time series, providing an efficient alternative to Transformers. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#35780;&#20272;&#22270;&#20687;&#30340;&#24863;&#30693;&#24230;&#37327;DreamSim&#65292;&#35813;&#24230;&#37327;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#23398;&#20064;&#20154;&#31867;&#35270;&#35273;&#30456;&#20284;&#24615;&#30340;&#26032;&#32500;&#24230;&#65292;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09344</link><description>&lt;p&gt;
DreamSim&#65306;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#23398;&#20064;&#20154;&#31867;&#35270;&#35273;&#30456;&#20284;&#24615;&#30340;&#26032;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data. (arXiv:2306.09344v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#35780;&#20272;&#22270;&#20687;&#30340;&#24863;&#30693;&#24230;&#37327;DreamSim&#65292;&#35813;&#24230;&#37327;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#23398;&#20064;&#20154;&#31867;&#35270;&#35273;&#30456;&#20284;&#24615;&#30340;&#26032;&#32500;&#24230;&#65292;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#24863;&#30693;&#30456;&#20284;&#24615;&#24230;&#37327;&#26159;&#22312;&#20687;&#32032;&#21644;&#22270;&#20687;&#22359;&#30340;&#23618;&#38754;&#25805;&#20316;&#30340;&#12290;&#36825;&#20123;&#24230;&#37327;&#20351;&#29992;&#20302;&#23618;&#27425;&#30340;&#39068;&#33394;&#21644;&#32441;&#29702;&#26469;&#27604;&#36739;&#22270;&#20687;&#65292;&#20294;&#26410;&#33021;&#25429;&#25417;&#22270;&#20687;&#24067;&#23616;&#12289;&#23545;&#35937;&#23039;&#24577;&#21644;&#35821;&#20041;&#20869;&#23481;&#30340;&#20013;&#23618;&#27425;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#20840;&#38754;&#35780;&#20272;&#22270;&#20687;&#30340;&#24863;&#30693;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#27493;&#26159;&#25910;&#38598;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#30456;&#20284;&#32500;&#24230;&#22270;&#20687;&#23545;&#30340;&#20154;&#31867;&#30456;&#20284;&#24615;&#21028;&#26029;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#26159;&#35780;&#21028;&#26159;&#20960;&#20046;&#33258;&#21160;&#30340;&#65292;&#24182;&#19988;&#30001;&#25152;&#26377;&#35266;&#23519;&#32773;&#20849;&#20139;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21019;&#24314;&#20102;&#19968;&#20123;&#27839;&#19981;&#21516;&#32500;&#24230;&#25200;&#21160;&#30340;&#21512;&#25104;&#22270;&#20687;&#23545;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#29616;&#26377;&#30340;&#27969;&#34892;&#30340;&#24863;&#30693;&#24230;&#37327;&#26080;&#27861;&#35299;&#37322;&#25105;&#20204;&#30340;&#26032;&#25968;&#25454;&#65292;&#22240;&#27492;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;DreamSim&#65292;&#20197;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#24863;&#30693;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#35270;&#35273;&#23646;&#24615;&#22914;&#20309;&#24433;&#21709;&#24230;&#37327;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;&#23427;&#20005;&#37325;&#20851;&#27880;&#21069;&#26223;&#29289;&#20307;&#21644;&#35821;&#20041;&#20869;&#23481;&#12290;DreamSim&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#20013;&#27604;&#29616;&#26377;&#30340;&#24230;&#37327;&#26356;&#20248;&#65292;&#21253;&#25324;&#39044;&#27979;&#34892;&#20026;&#23454;&#39564;&#32467;&#26524;&#12289;&#39044;&#27979;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#19982;&#20154;&#31867;&#30456;&#20284;&#24615;&#21028;&#26029;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current perceptual similarity metrics operate at the level of pixels and patches. These metrics compare images in terms of their low-level colors and textures, but fail to capture mid-level similarities and differences in image layout, object pose, and semantic content. In this paper, we develop a perceptual metric that assesses images holistically. Our first step is to collect a new dataset of human similarity judgments over image pairs that are alike in diverse ways. Critical to this dataset is that judgments are nearly automatic and shared by all observers. To achieve this we use recent text-to-image models to create synthetic pairs that are perturbed along various dimensions. We observe that popular perceptual metrics fall short of explaining our new data, and we introduce a new metric, DreamSim, tuned to better align with human perception. We analyze how our metric is affected by different visual attributes, and find that it focuses heavily on foreground objects and semantic conte
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#37327;&#23376;&#23398;&#20064;&#20013;&#37327;&#23376;&#32416;&#32544;&#21644;&#32479;&#35745;&#23398;&#30340;&#20316;&#29992;&#65292;&#30740;&#31350;&#20102;&#32416;&#32544;&#27979;&#37327;&#19982;&#21487;&#20998;&#31163;&#27979;&#37327;&#20197;&#21450;&#32416;&#32544;&#27979;&#37327;&#19982;&#32479;&#35745;&#27979;&#37327;&#22312;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#21306;&#21035;&#65292;&#35777;&#26126;&#20102;QSQ&#23398;&#20064;&#19982;&#21033;&#29992;&#32416;&#32544;&#27979;&#37327;&#30340;&#37327;&#23376;&#23398;&#20064;&#20043;&#38388;&#30340;&#25351;&#25968;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.03161</link><description>&lt;p&gt;
&#35770;&#37327;&#23376;&#23398;&#20064;&#20013;&#37327;&#23376;&#32416;&#32544;&#21644;&#32479;&#35745;&#23398;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Role of Entanglement and Statistics in Learning. (arXiv:2306.03161v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#37327;&#23376;&#23398;&#20064;&#20013;&#37327;&#23376;&#32416;&#32544;&#21644;&#32479;&#35745;&#23398;&#30340;&#20316;&#29992;&#65292;&#30740;&#31350;&#20102;&#32416;&#32544;&#27979;&#37327;&#19982;&#21487;&#20998;&#31163;&#27979;&#37327;&#20197;&#21450;&#32416;&#32544;&#27979;&#37327;&#19982;&#32479;&#35745;&#27979;&#37327;&#22312;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#21306;&#21035;&#65292;&#35777;&#26126;&#20102;QSQ&#23398;&#20064;&#19982;&#21033;&#29992;&#32416;&#32544;&#27979;&#37327;&#30340;&#37327;&#23376;&#23398;&#20064;&#20043;&#38388;&#30340;&#25351;&#25968;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;(QSQ)&#27169;&#22411;&#20013;&#21033;&#29992;&#37327;&#23376;&#32416;&#32544;&#12289;&#21487;&#20998;&#31163;&#20197;&#21450;&#32479;&#35745;&#27979;&#37327;&#26041;&#27861;&#36827;&#34892;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#24471;&#20986;&#19979;&#21015;&#32467;&#35770;&#12290;$\textbf{&#32416;&#32544;&#27979;&#37327;&#19982;&#21487;&#20998;&#31163;&#27979;&#37327;}$&#65306;&#22312;&#19968;&#20010;&#32473;&#23450;&#30340;&#27010;&#24565;&#31867;$C\subseteq \{f:\{0,1\}^n\rightarrow [k]\}$&#20013;&#65292;&#21033;&#29992;$\frac{1}{\sqrt{2^n}}\sum_x\vert x,f(x)\rangle$&#30340;&#21103;&#26412;&#26469;&#23398;&#20064;&#19968;&#20010;&#26410;&#30693;&#20989;&#25968;$f$&#65292;&#22914;&#26524;&#21033;&#29992;&#32416;&#32544;&#27979;&#37327;&#65292;&#21017;&#38656;&#35201;$T$&#20010;&#21103;&#26412;&#21363;&#21487;&#23436;&#25104;&#23398;&#20064;&#65292;&#21017;&#21482;&#38656;&#21033;&#29992;&#21487;&#20998;&#31163;&#27979;&#37327;&#65292;&#23601;&#38656;&#35201;$O(nT^2)$&#20010;&#21103;&#26412;&#12290;$\textbf{&#32416;&#32544;&#27979;&#37327;&#19982;&#32479;&#35745;&#27979;&#37327;}$&#65306;&#22312;&#21487;&#20998;&#31163;&#27979;&#37327;&#21644;&#32479;&#35745;&#27979;&#37327;&#30340;&#22522;&#30784;&#19978;&#23398;&#20064;&#19968;&#20010;&#20989;&#25968;$f\in C$&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31867;$C$&#65292;&#35777;&#26126;&#20102;QSQ&#23398;&#20064;&#19982;&#21033;&#29992;&#32416;&#32544;&#27979;&#37327;&#30340;&#37327;&#23376;&#23398;&#20064;&#20043;&#38388;&#30340;&#25351;&#25968;&#24046;&#24322;&#65288;&#21363;&#20351;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#65289;&#65292;&#36825;&#35777;&#26126;&#20102;&#37327;&#23376;&#23398;&#20064;&#30340;&#37327;&#23376;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we make progress in understanding the relationship between learning models with access to entangled, separable and statistical measurements in the quantum statistical query (QSQ) model. To this end, we show the following results.  $\textbf{Entangled versus separable measurements.}$ The goal here is to learn an unknown $f$ from the concept class $C\subseteq \{f:\{0,1\}^n\rightarrow [k]\}$ given copies of $\frac{1}{\sqrt{2^n}}\sum_x \vert x,f(x)\rangle$. We show that, if $T$ copies suffice to learn $f$ using entangled measurements, then $O(nT^2)$ copies suffice to learn $f$ using just separable measurements.  $\textbf{Entangled versus statistical measurements}$ The goal here is to learn a function $f \in C$ given access to separable measurements and statistical measurements. We exhibit a class $C$ that gives an exponential separation between QSQ learning and quantum learning with entangled measurements (even in the presence of noise). This proves the "quantum analogue" of th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20004;&#20010;Diffusion&#27169;&#22411;&#20013;&#25552;&#21462;&#22870;&#21169;&#20989;&#25968;&#30340;&#23454;&#29992;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#23548;&#33322;&#29615;&#22659;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#20197;&#27492;&#25511;&#21046;Diffusion&#27169;&#22411;&#23398;&#20064;&#36275;&#22815;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.01804</link><description>&lt;p&gt;
&#20174;Diffusion&#27169;&#22411;&#20013;&#25552;&#21462;&#22870;&#21169;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Extracting Reward Functions from Diffusion Models. (arXiv:2306.01804v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20004;&#20010;Diffusion&#27169;&#22411;&#20013;&#25552;&#21462;&#22870;&#21169;&#20989;&#25968;&#30340;&#23454;&#29992;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#23548;&#33322;&#29615;&#22659;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#20197;&#27492;&#25511;&#21046;Diffusion&#27169;&#22411;&#23398;&#20064;&#36275;&#22815;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Diffusion&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#20063;&#34987;&#29992;&#20110;&#23398;&#20064;&#24207;&#21015;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#39640;&#24615;&#33021;&#31574;&#30053;&#12290;&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;&#27604;&#36739;&#24314;&#27169;&#20302;&#22870;&#21169;&#34892;&#20026;&#21644;&#24314;&#27169;&#39640;&#22870;&#21169;&#34892;&#20026;&#30340;&#20915;&#31574;&#20256;&#25773;&#27169;&#22411;&#26469;&#25552;&#21462;&#22870;&#21169;&#20989;&#25968;&#30340;&#38382;&#39064;&#65307;&#36825;&#19982;&#36870;&#24378;&#21270;&#23398;&#20064;&#30456;&#20851;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26799;&#24230;&#19982;&#20004;&#20010;Diffusion&#27169;&#22411;&#30340;&#36755;&#20986;&#24046;&#24322;&#23545;&#40784;&#26469;&#25552;&#21462;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#23548;&#33322;&#29615;&#22659;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#34920;&#26126;&#21487;&#20197;&#36890;&#36807;&#25511;&#21046;Diffusion&#27169;&#22411;&#26469;&#23398;&#20064;&#36275;&#22815;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have achieved remarkable results in image generation, and have similarly been used to learn high-performing policies in sequential decision-making tasks. Decision-making diffusion models can be trained on lower-quality data, and then be steered with a reward function to generate near-optimal trajectories. We consider the problem of extracting a reward function by comparing a decision-making diffusion model that models low-reward behavior and one that models high-reward behavior; a setting related to inverse reinforcement learning. We first define the notion of a relative reward function of two diffusion models and show conditions under which it exists and is unique. We then devise a practical learning algorithm for extracting it by aligning the gradients of a reward function -- parametrized by a neural network -- to the difference in outputs of both diffusion models. Our method finds correct reward functions in navigation environments, and we demonstrate that steering 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23558;&#20219;&#20309;&#21333;&#33218;&#31574;&#30053;&#36716;&#21270;&#20026;&#21407;&#22987;&#30340;$N$&#33218;&#38382;&#39064;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#20381;&#36182;&#20110;&#22797;&#26434;UGAP&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#20855;&#26377;$O(1/\sqrt{N})$&#26368;&#20248;&#24615;&#24046;&#36317;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.00196</link><description>&lt;p&gt;
&#20855;&#26377;&#24179;&#22343;&#22870;&#21169;&#30340;&#19981;&#23433;&#23450;&#36172;&#24466;&#38382;&#39064;&#65306;&#25171;&#30772;&#32479;&#19968;&#20840;&#23616;&#24341;&#23376;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption. (arXiv:2306.00196v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23558;&#20219;&#20309;&#21333;&#33218;&#31574;&#30053;&#36716;&#21270;&#20026;&#21407;&#22987;&#30340;$N$&#33218;&#38382;&#39064;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#20381;&#36182;&#20110;&#22797;&#26434;UGAP&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#20855;&#26377;$O(1/\sqrt{N})$&#26368;&#20248;&#24615;&#24046;&#36317;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#24179;&#22343;&#22870;&#21169;&#26631;&#20934;&#19979;&#30340;&#26080;&#38480;&#26102;&#19981;&#23433;&#23450;&#36172;&#24466;&#38382;&#39064;&#65292;&#21253;&#25324;&#31163;&#25955;&#26102;&#38388;&#21644;&#36830;&#32493;&#26102;&#38388;&#35774;&#32622;&#12290;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#22914;&#20309;&#35774;&#35745;&#35745;&#31639;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#20351;&#24471;&#20248;&#21270;&#24046;&#36317;&#38543;&#30528;&#33218;&#30340;&#25968;&#37327;$N$&#30340;&#22686;&#21152;&#32780;&#20943;&#23567;&#12290;&#29616;&#26377;&#30340;&#28176;&#36817;&#26368;&#20248;&#24615;&#32467;&#26524;&#37117;&#20381;&#36182;&#20110;&#32479;&#19968;&#20840;&#23616;&#24341;&#23376;&#24615;&#36136;(UGAP)&#65292;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#19988;&#38590;&#20197;&#39564;&#35777;&#30340;&#20551;&#35774;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#22522;&#20110;&#27169;&#25311;&#30340;&#26694;&#26550;&#65292;&#23558;&#20219;&#20309;&#21333;&#33218;&#31574;&#30053;&#36716;&#21270;&#20026;&#21407;&#22987;&#30340;$N$&#33218;&#38382;&#39064;&#30340;&#31574;&#30053;&#12290;&#36825;&#26159;&#36890;&#36807;&#22312;&#27599;&#20010;&#33218;&#19978;&#27169;&#25311;&#21333;&#33218;&#31574;&#30053;&#65292;&#24182;&#20180;&#32454;&#22320;&#23558;&#30495;&#23454;&#29366;&#24577;&#24341;&#23548;&#21521;&#27169;&#25311;&#29366;&#24577;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#23454;&#20363;&#21270;&#65292;&#20135;&#29983;&#19968;&#20010;&#20855;&#26377;$O(1/\sqrt{N})$&#30340;&#26368;&#20248;&#35299;&#24046;&#36317;&#30340;&#31574;&#30053;&#12290;&#22312;&#31163;&#25955;&#26102;&#38388;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#26356;&#31616;&#21333;&#30340;&#21516;&#27493;&#20551;&#35774;&#19979;&#25104;&#31435;&#65292;&#28085;&#30422;&#20102;&#19968;&#20123;&#19981;&#28385;&#36275;UGAP&#30340;&#38382;&#39064;&#23454;&#20363;&#12290;&#26356;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#22788;&#29702;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22823;&#30340;&#38382;&#39064;&#31867;&#65292;&#32780;&#19981;&#38656;&#23545;&#38382;&#39064;&#23454;&#20363;&#20570;&#20219;&#20309;&#29305;&#23450;&#30340;&#32467;&#26500;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the infinite-horizon restless bandit problem with the average reward criterion, under both discrete-time and continuous-time settings. A fundamental question is how to design computationally efficient policies that achieve a diminishing optimality gap as the number of arms, $N$, grows large. Existing results on asymptotical optimality all rely on the uniform global attractor property (UGAP), a complex and challenging-to-verify assumption. In this paper, we propose a general, simulation-based framework that converts any single-armed policy into a policy for the original $N$-armed problem. This is accomplished by simulating the single-armed policy on each arm and carefully steering the real state towards the simulated state. Our framework can be instantiated to produce a policy with an $O(1/\sqrt{N})$ optimality gap. In the discrete-time setting, our result holds under a simpler synchronization assumption, which covers some problem instances that do not satisfy UGAP. More notabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27602;&#21270;&#25915;&#20987;&#30340;&#40065;&#26834;&#38750;&#21442;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;Huber&#25439;&#22833;&#30340;M-&#35780;&#20272;&#22120;&#21644;&#36890;&#36807;&#23558;&#21021;&#22987;&#20272;&#35745;&#25237;&#24433;&#21040;Lipschitz&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#26657;&#27491;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27491;&#30830;&#36873;&#25321;&#24102;&#23485;&#26102;$\ell_\infty $&#35823;&#24046;&#26159;&#26497;&#23567;&#21270;&#26368;&#20248;&#30340;&#65292;&#32780;$\ell_2 $&#35823;&#24046;&#22312;$q\lesssim \sqrt{N/\ln^2 N}$&#26102;&#26368;&#20248;&#12290;</title><link>http://arxiv.org/abs/2305.16771</link><description>&lt;p&gt;
&#27602;&#21270;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#38750;&#21442;&#25968;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Robust Nonparametric Regression under Poisoning Attack. (arXiv:2305.16771v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27602;&#21270;&#25915;&#20987;&#30340;&#40065;&#26834;&#38750;&#21442;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;Huber&#25439;&#22833;&#30340;M-&#35780;&#20272;&#22120;&#21644;&#36890;&#36807;&#23558;&#21021;&#22987;&#20272;&#35745;&#25237;&#24433;&#21040;Lipschitz&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#26657;&#27491;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27491;&#30830;&#36873;&#25321;&#24102;&#23485;&#26102;$\ell_\infty $&#35823;&#24046;&#26159;&#26497;&#23567;&#21270;&#26368;&#20248;&#30340;&#65292;&#32780;$\ell_2 $&#35823;&#24046;&#22312;$q\lesssim \sqrt{N/\ln^2 N}$&#26102;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#40065;&#26834;&#38750;&#21442;&#25968;&#22238;&#24402;&#65292;&#22312;&#36825;&#31181;&#22238;&#24402;&#20013;&#65292;&#23545;&#25239;&#24615;&#25915;&#20987;&#32773;&#21487;&#20197;&#20462;&#25913;&#26469;&#33258;&#22823;&#23567;&#20026;N&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26368;&#22810;q&#20010;&#26679;&#26412;&#30340;&#20540;&#12290;&#25105;&#20204;&#30340;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#26159;&#22522;&#20110;Huber&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;M-&#35780;&#20272;&#22120;&#12290;&#19982;&#31616;&#21333;&#30340;&#26680;&#22238;&#24402;&#65288;&#21363;Nadaraya-Watson&#20272;&#35745;&#65289;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#20943;&#24369;&#24694;&#24847;&#26679;&#26412;&#23545;&#22238;&#24402;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25910;&#25947;&#36895;&#29575;&#20197;&#21450;&#30456;&#24212;&#30340;&#26497;&#23567;&#21270;&#19979;&#30028;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#27491;&#30830;&#36873;&#25321;&#24102;&#23485;&#65292;$\ell_\infty $&#35823;&#24046;&#26159;&#26497;&#23567;&#21270;&#26368;&#20248;&#30340;&#12290;&#22914;&#26524;$q\lesssim \sqrt{N/\ln^2 N}$&#65292;&#21017;$\ell_2 $&#35823;&#24046;&#26159;&#26368;&#20248;&#30340;&#65292;&#20294;&#26159;&#22914;&#26524;$q$&#26356;&#22823;&#65292;&#21017;&#26159;&#27425;&#20248;&#30340;&#12290;&#21407;&#22240;&#26159;&#22914;&#26524;&#26377;&#35768;&#22810;&#34987;&#25915;&#20987;&#30340;&#26679;&#26412;&#38598;&#20013;&#22312;&#19968;&#20010;&#23567;&#21306;&#22495;&#20013;&#65292;&#36825;&#20010;&#20272;&#35745;&#37327;&#23601;&#20250;&#24456;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26657;&#27491;&#26041;&#27861;&#65292;&#23558;&#21021;&#22987;&#20272;&#35745;&#25237;&#24433;&#21040;Lipschitz&#20989;&#25968;&#31354;&#38388;&#20013;&#12290;&#26368;&#32456;&#30340;&#20272;&#35745;&#20540;&#20960;&#20046;&#26159;&#20219;&#24847;$q$&#30340;&#26497;&#23567;&#21270;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies robust nonparametric regression, in which an adversarial attacker can modify the values of up to $q$ samples from a training dataset of size $N$. Our initial solution is an M-estimator based on Huber loss minimization. Compared with simple kernel regression, i.e. the Nadaraya-Watson estimator, this method can significantly weaken the impact of malicious samples on the regression performance. We provide the convergence rate as well as the corresponding minimax lower bound. The result shows that, with proper bandwidth selection, $\ell_\infty$ error is minimax optimal. The $\ell_2$ error is optimal if $q\lesssim \sqrt{N/\ln^2 N}$, but is suboptimal with larger $q$. The reason is that this estimator is vulnerable if there are many attacked samples concentrating in a small region. To address this issue, we propose a correction method by projecting the initial estimate to the space of Lipschitz functions. The final estimate is nearly minimax optimal for arbitrary $q$, up t
&lt;/p&gt;</description></item><item><title>BertRLFuzzer&#26159;&#19968;&#31181;&#22522;&#20110;BERT&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;Fuzzer&#24037;&#20855;&#65292;&#26088;&#22312;&#21457;&#29616;Web&#24212;&#29992;&#31243;&#24207;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;&#36890;&#36807;&#20351;&#29992;BERT&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#26469;&#25351;&#23548;Fuzzer&#36827;&#34892;&#39640;&#25928;&#23398;&#20064;&#65292;BertRLFuzzer&#30456;&#23545;&#20110;&#20854;&#20182;&#40657;&#30418;&#21644;&#30333;&#30418;Fuzzer&#22312;&#26102;&#38388;&#21040;&#39318;&#27425;&#25915;&#20987;&#12289;&#26032;&#28431;&#27934;&#21457;&#29616;&#21644;&#25915;&#20987;&#29575;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.12534</link><description>&lt;p&gt;
BertRLFuzzer: &#19968;&#31181;&#22522;&#20110;BERT&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;Fuzzer
&lt;/p&gt;
&lt;p&gt;
BertRLFuzzer: A BERT and Reinforcement Learning based Fuzzer. (arXiv:2305.12534v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12534
&lt;/p&gt;
&lt;p&gt;
BertRLFuzzer&#26159;&#19968;&#31181;&#22522;&#20110;BERT&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;Fuzzer&#24037;&#20855;&#65292;&#26088;&#22312;&#21457;&#29616;Web&#24212;&#29992;&#31243;&#24207;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;&#36890;&#36807;&#20351;&#29992;BERT&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#26469;&#25351;&#23548;Fuzzer&#36827;&#34892;&#39640;&#25928;&#23398;&#20064;&#65292;BertRLFuzzer&#30456;&#23545;&#20110;&#20854;&#20182;&#40657;&#30418;&#21644;&#30333;&#30418;Fuzzer&#22312;&#26102;&#38388;&#21040;&#39318;&#27425;&#25915;&#20987;&#12289;&#26032;&#28431;&#27934;&#21457;&#29616;&#21644;&#25915;&#20987;&#29575;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24037;&#20855;BertRLFuzzer&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;BERT&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;Fuzzer&#65292;&#26088;&#22312;&#21457;&#29616;Web&#24212;&#29992;&#31243;&#24207;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;BertRLFuzzer&#30340;&#24037;&#20316;&#21407;&#29702;&#22914;&#19979;&#65306;&#32473;&#23450;&#19968;&#32452;&#31181;&#23376;&#36755;&#20837;&#65292;Fuzzer&#23545;&#23427;&#20204;&#25191;&#34892;&#36981;&#24490;&#35821;&#27861;&#24182;&#24341;&#21457;&#25915;&#20987;&#30340;&#21464;&#24322;&#25805;&#20316;&#65292;&#20197;&#29983;&#25104;&#20505;&#36873;&#25915;&#20987;&#21521;&#37327;&#12290;BertRLFuzzer&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#20351;&#29992;BERT&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#26469;&#25351;&#23548;Fuzzer&#39640;&#25928;&#23398;&#20064;&#36981;&#24490;&#35821;&#27861;&#21644;&#24341;&#21457;&#25915;&#20987;&#30340;&#21464;&#24322;&#25805;&#20316;&#31526;&#12290;&#20026;&#20102;&#39564;&#35777;BertRLFuzzer&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#20849;&#35745;13&#20010;&#40657;&#30418;&#21644;&#30333;&#30418;Fuzzer&#22312;9&#20010;&#21463;&#23475;&#32773;&#32593;&#31449;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#28041;&#21450;&#36229;&#36807;16K&#34892;&#30340;&#28304;&#20195;&#30721;&#12290;&#30456;&#23545;&#20110;&#26368;&#25509;&#36817;&#30340;&#31454;&#20105;&#24037;&#20855;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#26102;&#38388;&#21040;&#39318;&#27425;&#25915;&#20987;&#30340;&#26174;&#33879;&#25913;&#36827;&#65288;&#20943;&#23569;54&#65285;&#65289;&#65292;&#21457;&#29616;&#30340;&#26032;&#28431;&#27934;&#65288;17&#20010;&#26032;&#28431;&#27934;&#65289;&#21644;&#25915;&#20987;&#29575;&#65288;&#29983;&#25104;&#30340;&#25915;&#20987;&#21521;&#37327;&#22686;&#21152;&#20102;4.4&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel tool BertRLFuzzer, a BERT and Reinforcement Learning (RL) based fuzzer aimed at finding security vulnerabilities for Web applications. BertRLFuzzer works as follows: given a set of seed inputs, the fuzzer performs grammar-adhering and attack-provoking mutation operations on them to generate candidate attack vectors. The key insight of BertRLFuzzer is the use of RL with a BERT model as an agent to guide the fuzzer to efficiently learn grammar-adhering and attack-provoking mutation operators. In order to establish the efficacy of BertRLFuzzer we compare it against a total of 13 black box and white box fuzzers over a benchmark of 9 victim websites with over 16K LOC. We observed a significant improvement, relative to the nearest competing tool, in terms of time to first attack (54% less), new vulnerabilities found (17 new vulnerabilities), and attack rate (4.4% more attack vectors generated).
&lt;/p&gt;</description></item><item><title>Waymo&#24320;&#25918;&#27169;&#25311;&#20195;&#29702;&#25361;&#25112;&#36187;&#25552;&#20986;&#20351;&#29992;&#30495;&#23454;&#12289;&#20114;&#21160;&#30340;&#26234;&#33021;&#20307;&#20223;&#30495;&#20197;&#20419;&#36827;&#33258;&#21160;&#39550;&#39542;&#34892;&#20026;&#27169;&#22411;&#30340;&#35780;&#20272;&#21644;&#35757;&#32451;&#65292;&#26159;&#35813;&#39046;&#22495;&#30340;&#39318;&#20010;&#20844;&#24320;&#25361;&#25112;&#36187;&#65292;&#26088;&#22312;&#25512;&#21160;&#36924;&#30495;&#27169;&#25311;&#22120;&#30340;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.12032</link><description>&lt;p&gt;
Waymo&#24320;&#25918;&#27169;&#25311;&#20195;&#29702;&#25361;&#25112;&#36187;
&lt;/p&gt;
&lt;p&gt;
The Waymo Open Sim Agents Challenge. (arXiv:2305.12032v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12032
&lt;/p&gt;
&lt;p&gt;
Waymo&#24320;&#25918;&#27169;&#25311;&#20195;&#29702;&#25361;&#25112;&#36187;&#25552;&#20986;&#20351;&#29992;&#30495;&#23454;&#12289;&#20114;&#21160;&#30340;&#26234;&#33021;&#20307;&#20223;&#30495;&#20197;&#20419;&#36827;&#33258;&#21160;&#39550;&#39542;&#34892;&#20026;&#27169;&#22411;&#30340;&#35780;&#20272;&#21644;&#35757;&#32451;&#65292;&#26159;&#35813;&#39046;&#22495;&#30340;&#39318;&#20010;&#20844;&#24320;&#25361;&#25112;&#36187;&#65292;&#26088;&#22312;&#25512;&#21160;&#36924;&#30495;&#27169;&#25311;&#22120;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#20041;&#20102;Waymo&#24320;&#25918;&#27169;&#25311;&#20195;&#29702;&#25361;&#25112;&#36187;(WOSAC)&#12290;&#36890;&#36807;&#19982;&#30495;&#23454;&#12289;&#20114;&#21160;&#30340;&#26234;&#33021;&#20307;&#36827;&#34892;&#20223;&#30495;&#26159;&#33258;&#21160;&#39550;&#39542;&#36719;&#20214;&#24320;&#21457;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;WOSAC&#26159;&#31532;&#19968;&#20010;&#20844;&#24320;&#30340;&#25361;&#25112;&#36187;&#65292;&#26088;&#22312;&#35299;&#20915;&#35813;&#20219;&#21153;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#35813;&#25361;&#25112;&#30340;&#30446;&#26631;&#26159;&#28608;&#21457;&#35774;&#35745;&#36924;&#30495;&#27169;&#25311;&#22120;&#30340;&#20852;&#36259;&#65292;&#20197;&#29992;&#20110;&#35780;&#20272;&#21644;&#35757;&#32451;&#33258;&#21160;&#39550;&#39542;&#30340;&#34892;&#20026;&#27169;&#22411;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20960;&#31181;&#22522;&#20934;&#20223;&#30495;&#20195;&#29702;&#26041;&#27861;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we define the Waymo Open Sim Agents Challenge (WOSAC). Simulation with realistic, interactive agents represents a key task for autonomous vehicle software development. WOSAC is the first public challenge to tackle this task and propose corresponding metrics. The goal of the challenge is to stimulate the design of realistic simulators that can be used to evaluate and train a behavior model for autonomous driving. We outline our evaluation methodology and present preliminary results for a number of different baseline simulation agent methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25932;&#23545;&#29615;&#22659;&#19979;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#30340;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#12290;&#22312;&#21608;&#26399;&#24615;&#27880;&#20837;&#25915;&#20987;&#26102;&#65292;&#31995;&#32479;&#21160;&#24577;&#21487;&#31934;&#30830;&#24674;&#22797;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;O(n)&#65307;&#24403;&#25915;&#20987;&#20197;&#27010;&#29575;p&#36827;&#34892;&#26102;&#65292;&#31934;&#30830;&#24674;&#22797;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;O(log(n)p/(1-p)^2) &#12290;&#21363;&#20351;&#26377;&#36229;&#36807;&#19968;&#21322;&#30340;&#25968;&#25454;&#21463;&#25439;&#65292;&#20272;&#35745;&#22120;&#20173;&#21487;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.10506</link><description>&lt;p&gt;
&#26356;&#22810;&#33039;&#25968;&#25454;&#19979;&#30340;&#31995;&#32479;&#35782;&#21035;&#31934;&#30830;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Exact Recovery for System Identification with More Corrupt Data than Clean Data. (arXiv:2305.10506v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25932;&#23545;&#29615;&#22659;&#19979;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#30340;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#12290;&#22312;&#21608;&#26399;&#24615;&#27880;&#20837;&#25915;&#20987;&#26102;&#65292;&#31995;&#32479;&#21160;&#24577;&#21487;&#31934;&#30830;&#24674;&#22797;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;O(n)&#65307;&#24403;&#25915;&#20987;&#20197;&#27010;&#29575;p&#36827;&#34892;&#26102;&#65292;&#31934;&#30830;&#24674;&#22797;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;O(log(n)p/(1-p)^2) &#12290;&#21363;&#20351;&#26377;&#36229;&#36807;&#19968;&#21322;&#30340;&#25968;&#25454;&#21463;&#25439;&#65292;&#20272;&#35745;&#22120;&#20173;&#21487;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25932;&#23545;&#29615;&#22659;&#19979;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#30340;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#20004;&#31181;Lasso&#20272;&#35745;&#22120;&#30340;&#28176;&#36817;&#21644;&#38750;&#28176;&#36817;&#29305;&#24615;&#65292;&#28041;&#21450;&#21040;&#23545;&#20110;&#25915;&#20987;&#26102;&#21051;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#20004;&#31181;&#19981;&#21516;&#22330;&#26223;&#12290;&#30001;&#20110;&#25910;&#38598;&#30340;&#26679;&#26412;&#30456;&#20851;&#65292;&#29616;&#26377;&#30340;Lasso&#32467;&#26524;&#19981;&#36866;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#31995;&#32479;&#31283;&#23450;&#19988;&#25915;&#20987;&#20197;&#21608;&#26399;&#24615;&#27880;&#20837;&#26102;&#65292;&#31995;&#32479;&#21160;&#24577;&#30340;&#31934;&#30830;&#24674;&#22797;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;O(n)&#65292;&#20854;&#20013;n&#26159;&#29366;&#24577;&#30340;&#32500;&#24230;&#12290;&#24403;&#25915;&#20987;&#22312;&#27599;&#20010;&#26102;&#38388;&#23454;&#20363;&#20013;&#20197;&#27010;&#29575;p&#36827;&#34892;&#26102;&#65292;&#31934;&#30830;&#24674;&#22797;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#23558;&#25353;O(log (n)p / (1-p)^2)&#36827;&#34892;&#32553;&#25918;&#12290;&#35813;&#32467;&#26524;&#22312;&#28176;&#36817;&#29366;&#24577;&#19979;&#24847;&#21619;&#30528;&#20960;&#20046;&#32943;&#23450;&#25910;&#25947;&#20110;&#30495;&#23454;&#31995;&#32479;&#21160;&#24577;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#21363;&#20351;&#36229;&#36807;&#19968;&#21322;&#30340;&#25968;&#25454;&#21463;&#25439;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#20173;&#28982;&#33021;&#22815;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the system identification problem for linear discrete-time systems under adversaries and analyze two lasso-type estimators. We study both asymptotic and non-asymptotic properties of these estimators in two separate scenarios, corresponding to deterministic and stochastic models for the attack times. Since the samples collected from the system are correlated, the existing results on lasso are not applicable. We show that when the system is stable and the attacks are injected periodically, the sample complexity for the exact recovery of the system dynamics is O(n), where n is the dimension of the states. When the adversarial attacks occur at each time instance with probability p, the required sample complexity for the exact recovery scales as O(\log(n)p/(1-p)^2). This result implies the almost sure convergence to the true system dynamics under the asymptotic regime. As a by-product, even when more than half of the data is compromised, our estimators still learn th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21475;&#34955;&#30340;&#19977;&#32500;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#25200;&#21160;&#21644;&#24674;&#22797;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#34920;&#31034;&#24418;&#24335;&#12290; &#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#21644;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#20248;&#28857;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#19977;&#32500;&#20998;&#23376;&#12290;</title><link>http://arxiv.org/abs/2305.10133</link><description>&lt;p&gt;
Lingo3DMol:&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22522;&#20110;&#21475;&#34955;&#30340;&#19977;&#32500;&#20998;&#23376;
&lt;/p&gt;
&lt;p&gt;
Lingo3DMol: Generation of a Pocket-based 3D Molecule using a Language Model. (arXiv:2305.10133v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21475;&#34955;&#30340;&#19977;&#32500;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#25200;&#21160;&#21644;&#24674;&#22797;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#34920;&#31034;&#24418;&#24335;&#12290; &#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#21644;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#20248;&#28857;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#19977;&#32500;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#25512;&#21160;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#22791;&#21463;&#30633;&#30446;&#12290; &#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#20108;&#32500;&#32467;&#26500;&#20013;&#29983;&#25104;&#26377;&#25928;&#20998;&#23376;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#32780;&#22522;&#20110;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#21017;&#21487;&#20197;&#30452;&#25509;&#20135;&#29983;&#20855;&#26377;&#20934;&#30830;&#19977;&#32500;&#22352;&#26631;&#30340;&#20998;&#23376;&#12290;&#21463;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21475;&#34955;&#30340;&#19977;&#32500;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#29983;&#25104;&#19977;&#32500;&#22352;&#26631;&#30340;&#33021;&#21147;&#12290; &#30001;&#20110;&#39640;&#36136;&#37327;&#30340;&#34507;&#30333;&#36136;-&#37197;&#20307;&#22797;&#21512;&#29289;&#25968;&#25454;&#19981;&#36275;&#65292;&#22240;&#27492;&#35774;&#35745;&#20102;&#19968;&#31181;&#25200;&#21160;&#21644;&#24674;&#22797;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#21487;&#20197;&#21033;&#29992;&#22823;&#37327;&#30340;&#23567;&#20998;&#23376;&#25968;&#25454;&#12290; &#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#34920;&#31034;&#24418;&#24335;&#65292;&#21363;&#24102;&#26377;&#23616;&#37096;&#21644;&#20840;&#23616;&#22352;&#26631;&#30340;&#22522;&#20110;&#29255;&#27573;&#30340;SMILES&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#20998;&#23376;&#25299;&#25169;&#32467;&#26500;&#21644;&#31354;&#38388;&#20301;&#32622;&#20449;&#24687;&#12290;&#26368;&#32456;&#65292;CrossDocked&#21644;DUD-E&#25968;&#25454;&#38598;&#34987;&#29992;&#20110;&#35780;&#20272;&#21644;&#34913;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure-based drug design powered by deep generative models have attracted increasing research interest in recent years. Language models have demonstrated a robust capacity for generating valid molecules in 2D structures, while methods based on geometric deep learning can directly produce molecules with accurate 3D coordinates. Inspired by both methods, this article proposes a pocket-based 3D molecule generation method that leverages the language model with the ability to generate 3D coordinates. High quality protein-ligand complex data are insufficient; hence, a perturbation and restoration pre-training task is designed that can utilize vast amounts of small-molecule data. A new molecular representation, a fragment-based SMILES with local and global coordinates, is also presented, enabling the language model to learn molecular topological structures and spatial position information effectively. Ultimately, CrossDocked and DUD-E dataset is employed for evaluation and additional metri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#31639;&#23376;Newton&#26041;&#27861;&#65292;&#21487;&#20197;&#36845;&#20195;&#26500;&#36896;&#19968;&#20010;&#26131;&#22788;&#29702;&#30340;&#21407;&#27010;&#29575;&#27979;&#24230;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#28385;&#36275;&#30446;&#26631;&#20998;&#25968;&#20809;&#28369;&#24615;&#20551;&#35774;&#19979;&#65292;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09792</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#31639;&#23376; Newton &#26041;&#27861;&#29992;&#20110;&#27979;&#37327;&#36816;&#36755;
&lt;/p&gt;
&lt;p&gt;
A score-based operator Newton method for measure transport. (arXiv:2305.09792v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#31639;&#23376;Newton&#26041;&#27861;&#65292;&#21487;&#20197;&#36845;&#20195;&#26500;&#36896;&#19968;&#20010;&#26131;&#22788;&#29702;&#30340;&#21407;&#27010;&#29575;&#27979;&#24230;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#28385;&#36275;&#30446;&#26631;&#20998;&#25968;&#20809;&#28369;&#24615;&#20551;&#35774;&#19979;&#65292;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#27979;&#24230;&#30340;&#36816;&#36755;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#35768;&#22810;&#26680;&#24515;&#20219;&#21153;&#30340;&#22522;&#30784;&#65292;&#20174;&#21464;&#20998;&#25512;&#29702;&#21040;&#29983;&#25104;&#24314;&#27169;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#30446;&#26631;&#26159;&#23558;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#27010;&#29575;&#27979;&#24230;&#34920;&#31034;&#20026;&#36890;&#36807;&#23398;&#20064;&#30340;&#26144;&#23556;&#23558;&#19968;&#20010;&#26131;&#22788;&#29702;&#30340;&#21407;&#27010;&#29575;&#27979;&#24230;&#25512;&#21521;&#21069;&#38754;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26500;&#24314;&#36825;&#26679;&#19968;&#20010;&#36816;&#36755;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#32473;&#20986;&#20102;&#35780;&#20272;&#30446;&#26631;&#20998;&#24067;&#20998;&#25968;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#35813;&#26144;&#23556;&#29305;&#24449;&#21270;&#20026;&#19968;&#20010;&#26080;&#31351;&#32500;&#30340;&#20998;&#25968;&#27531;&#24046;&#31639;&#23376;&#30340;&#38646;&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#31181;&#36845;&#20195;&#26500;&#36896;&#36825;&#26679;&#19968;&#20010;&#38646;&#30340;&#29275;&#39039;&#31867;&#22411;&#26041;&#27861;&#12290;&#36890;&#36807;&#35843;&#29992;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#32463;&#20856;&#26925;&#22278;&#27491;&#21017;&#24615;&#29702;&#35770;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#36845;&#20195;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#34920;&#26126;&#22312;&#30446;&#26631;&#20998;&#25968;&#20809;&#28369;&#24615;&#20551;&#35774;&#19979;&#65292;&#36825;&#31181;&#26500;&#36896;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#20803;&#32032;&#26159;&#23558;&#22522;&#26412;&#30340;&#29275;&#39039;&#26041;&#27861;&#25512;&#24191;&#21040;&#26080;&#31351;&#32500;&#31639;&#23376;&#65292;&#20854;&#20182;&#24418;&#24335;&#30340;&#26080;&#31351;&#32500;&#31639;&#23376;&#24050;&#32463;&#20986;&#29616;&#22312;&#38750;&#32447;&#24615; PDE &#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transportation of probability measures underlies many core tasks in statistics and machine learning, from variational inference to generative modeling. A typical goal is to represent a target probability measure of interest as the push-forward of a tractable source measure through a learned map. We present a new construction of such a transport map, given the ability to evaluate the score of the target distribution. Specifically, we characterize the map as a zero of an infinite-dimensional score-residual operator and derive a Newton-type method for iteratively constructing such a zero. We prove convergence of these iterations by invoking classical elliptic regularity theory for partial differential equations (PDE) and show that this construction enjoys rapid convergence, under smoothness assumptions on the target score. A key element of our approach is a generalization of the elementary Newton method to infinite-dimensional operators, other forms of which have appeared in nonlinear PDE
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38598;&#25104;&#22810;&#27169;&#24577;&#24863;&#30693;&#65288;IMP&#65289;&#26041;&#27861;&#65292;&#23558;&#22810;&#27169;&#24577;&#36755;&#20837;&#38598;&#25104;&#21040;&#21333;&#20010;&#32534;&#30721;&#22120;&#20013;&#65292;&#37319;&#29992;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;AGD&#65289;&#21644;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#23454;&#29616;&#39640;&#25928;&#30340;&#27169;&#22411;&#21644;&#20219;&#21153;&#25193;&#23637;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.06324</link><description>&lt;p&gt;
AGD&#21644;MoE&#29992;&#20110;&#38598;&#25104;&#22810;&#27169;&#24577;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception. (arXiv:2305.06324v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38598;&#25104;&#22810;&#27169;&#24577;&#24863;&#30693;&#65288;IMP&#65289;&#26041;&#27861;&#65292;&#23558;&#22810;&#27169;&#24577;&#36755;&#20837;&#38598;&#25104;&#21040;&#21333;&#20010;&#32534;&#30721;&#22120;&#20013;&#65292;&#37319;&#29992;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;AGD&#65289;&#21644;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#23454;&#29616;&#39640;&#25928;&#30340;&#27169;&#22411;&#21644;&#20219;&#21153;&#25193;&#23637;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#24314;&#27169;&#26041;&#27861;&#8212;&#8212;&#38598;&#25104;&#22810;&#27169;&#24577;&#24863;&#30693;&#65288;IMP&#65289;&#12290;IMP&#23558;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#25991;&#26412;&#21644;&#38899;&#39057;&#31561;&#22810;&#27169;&#24577;&#36755;&#20837;&#38598;&#25104;&#21040;&#21333;&#20010;Transformer&#32534;&#30721;&#22120;&#20013;&#65292;&#24182;&#20855;&#26377;&#26368;&#23567;&#30340;&#27169;&#24577;&#29305;&#23450;&#32452;&#20214;&#12290;IMP&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35774;&#35745;&#65292;&#23558;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;AGD&#65289;&#21644;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#27169;&#22411;&#21644;&#20219;&#21153;&#25193;&#23637;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20197;&#19979;&#20851;&#38190;&#35265;&#35299;&#65306;1&#65289;&#22312;&#22810;&#26679;&#21270;&#30340;&#24322;&#26500;&#27169;&#24577;&#12289;&#25439;&#22833;&#20989;&#25968;&#21644;&#20219;&#21153;&#19978;&#20132;&#26367;&#25191;&#34892;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#65292;&#24182;&#21516;&#26102;&#25913;&#21464;&#36755;&#20837;&#20998;&#36776;&#29575;&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#22810;&#27169;&#24577;&#29702;&#35299;&#12290;2&#65289;&#22312;&#21333;&#19968;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#32534;&#30721;&#22120;&#19978;&#20351;&#29992;MoE&#36827;&#34892;&#27169;&#22411;&#31232;&#30095;&#21270;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#32988;&#36807;&#20351;&#29992;&#27169;&#24577;&#29305;&#23450;&#32534;&#30721;&#22120;&#25110;&#39069;&#22806;&#34701;&#21512;&#23618;&#30340;&#31264;&#23494;&#27169;&#22411;&#65292;&#24182;&#22823;&#22823;&#32531;&#35299;&#27169;&#24577;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;IMP&#22312;&#19977;&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#32988;&#36807;&#20102;&#22823;&#37096;&#20998;&#24050;&#21457;&#34920;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Integrated Multimodal Perception (IMP), a simple and scalable multimodal multi-task training and modeling approach. IMP integrates multimodal inputs including image, video, text, and audio into a single Transformer encoder with minimal modality-specific components. IMP makes use of a novel design that combines Alternating Gradient Descent (AGD) and Mixture-of-Experts (MoE) for efficient model \&amp; task scaling. We conduct extensive empirical studies about IMP and reveal the following key insights: 1) performing gradient descent updates by alternating on diverse heterogeneous modalities, loss functions, and tasks, while also varying input resolutions, efficiently improves multimodal understanding. 2) model sparsification with MoE on a single modality-agnostic encoder substantially improves the performance, outperforming dense models that use modality-specific encoders or additional fusion layers and greatly mitigating the conflicts between modalities. IMP achieves competitive p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;Deep Evidence Regression&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#20449;&#29992;&#39118;&#38505;&#20013;&#30340;&#36829;&#32422;&#25439;&#22833;&#65307;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.04967</link><description>&lt;p&gt;
&#20449;&#29992;&#39118;&#38505;&#31649;&#29702;&#20013;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65306;&#19968;&#31181;&#28145;&#24230;&#35777;&#25454;&#22238;&#24402;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
UQ for Credit Risk Management: A deep evidence regression approach. (arXiv:2305.04967v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;Deep Evidence Regression&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#20449;&#29992;&#39118;&#38505;&#20013;&#30340;&#36829;&#32422;&#25439;&#22833;&#65307;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#20449;&#29992;&#39118;&#38505;&#24212;&#29992;&#31243;&#24207;&#20013;&#12290;&#30001;&#20110;&#20449;&#29992;&#39118;&#38505;&#30340;&#22266;&#26377;&#24615;&#36136;&#65292;&#37327;&#21270;&#39044;&#27979;&#39118;&#38505;&#25351;&#26631;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#24517;&#35201;&#30340;&#65292;&#23558;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#20449;&#29992;&#39118;&#38505;&#35774;&#32622;&#20013;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;&#22312;&#26412;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;UQ&#24863;&#30693;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;Deep Evidence Regression&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#36829;&#32422;&#25439;&#22833;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;Deep Evidence Regression&#26041;&#27861;&#25193;&#23637;&#21040;&#36890;&#36807;Weibull&#36807;&#31243;&#29983;&#25104;&#30340;&#30446;&#26631;&#21464;&#37327;&#30340;&#23398;&#20064;&#26469;&#20026;&#25991;&#29486;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning has invariantly found its way into various Credit Risk applications. Due to the intrinsic nature of Credit Risk, quantifying the uncertainty of the predicted risk metrics is essential, and applying uncertainty-aware deep learning models to credit risk settings can be very helpful. In this work, we have explored the application of a scalable UQ-aware deep learning technique, Deep Evidence Regression and applied it to predicting Loss Given Default. We contribute to the literature by extending the Deep Evidence Regression methodology to learning target variables generated by a Weibull process and provide the relevant learning framework. We demonstrate the application of our approach to both simulated and real-world data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedAVO&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#38750;&#27954;&#31171;&#40555;&#20248;&#21270;&#22120;&#36873;&#25321;&#26368;&#20339;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#25928;&#29575;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#19982;FL&#25805;&#20316;&#30456;&#20851;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.01154</link><description>&lt;p&gt;
FedAVO&#65306;&#21033;&#29992;&#38750;&#27954;&#31171;&#40555;&#20248;&#21270;&#22120;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
FedAVO: Improving Communication Efficiency in Federated Learning with African Vultures Optimizer. (arXiv:2305.01154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedAVO&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#38750;&#27954;&#31171;&#40555;&#20248;&#21270;&#22120;&#36873;&#25321;&#26368;&#20339;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#25928;&#29575;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#19982;FL&#25805;&#20316;&#30456;&#20851;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30001;&#20110;&#24378;&#35843;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;FL&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#21487;&#33021;&#23548;&#33268;&#36890;&#20449;&#21463;&#38480;&#24182;&#19988;&#23398;&#20064;&#36807;&#31243;&#21464;&#24471;&#25302;&#24310;&#65292;&#38656;&#35201;&#23545;&#23458;&#25143;-&#26381;&#21153;&#22120;&#36890;&#20449;&#25104;&#26412;&#36827;&#34892;&#20248;&#21270;&#12290;&#36873;&#25321;&#30340;&#23458;&#25143;&#27604;&#20363;&#21644;&#26412;&#22320;&#35757;&#32451;&#24490;&#29615;&#27425;&#25968;&#26159;&#23545;FL&#24615;&#33021;&#26377;&#37325;&#22823;&#24433;&#21709;&#30340;&#20004;&#20010;&#36229;&#21442;&#25968;&#12290;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#23384;&#22312;&#19981;&#21516;&#30340;&#35757;&#32451;&#20559;&#22909;&#65292;&#22240;&#27492;FL&#20174;&#19994;&#32773;&#24456;&#38590;&#25163;&#21160;&#36873;&#25321;&#36825;&#20123;&#36229;&#21442;&#25968;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FedAVO&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;FL&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#27954;&#31171;&#40555;&#20248;&#21270;&#22120;&#65288;AVO&#65289;&#36873;&#25321;&#26368;&#20339;&#36229;&#21442;&#25968;&#26469;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#37319;&#29992;AVO&#36827;&#34892;FL&#36229;&#21442;&#25968;&#35843;&#25972;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#19982;FL&#25805;&#20316;&#30456;&#20851;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL), a distributed machine learning technique has recently experienced tremendous growth in popularity due to its emphasis on user data privacy. However, the distributed computations of FL can result in constrained communication and drawn-out learning processes, necessitating the client-server communication cost optimization. The ratio of chosen clients and the quantity of local training passes are two hyperparameters that have a significant impact on FL performance. Due to different training preferences across various applications, it can be difficult for FL practitioners to manually select such hyperparameters. In our research paper, we introduce FedAVO, a novel FL algorithm that enhances communication effectiveness by selecting the best hyperparameters leveraging the African Vulture Optimizer (AVO). Our research demonstrates that the communication costs associated with FL operations can be substantially reduced by adopting AVO for FL hyperparameter adjustment. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26377;&#25928;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#36866;&#24212;&#23398;&#20064;&#26368;&#20248;&#23450;&#20215;&#21644;&#24191;&#21578;&#31574;&#30053;&#65292;&#36798;&#21040;&#27425;&#32447;&#24615;&#21518;&#24724;&#12290;</title><link>http://arxiv.org/abs/2304.14385</link><description>&lt;p&gt;
&#24102;&#26377;&#36125;&#21494;&#26031;&#35828;&#26381;&#30340;&#21160;&#24577;&#23450;&#20215;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamic Pricing and Learning with Bayesian Persuasion. (arXiv:2304.14385v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26377;&#25928;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#36866;&#24212;&#23398;&#20064;&#26368;&#20248;&#23450;&#20215;&#21644;&#24191;&#21578;&#31574;&#30053;&#65292;&#36798;&#21040;&#27425;&#32447;&#24615;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21160;&#24577;&#23450;&#20215;&#21644;&#23398;&#20064;&#35774;&#32622;&#65292;&#22312;&#25353;&#39034;&#24207;&#35774;&#32622;&#20135;&#21697;&#20215;&#26684;&#30340;&#21516;&#26102;&#65292;&#21334;&#23478;&#36824;&#39044;&#20808;&#25215;&#35834;&#8220;&#24191;&#21578;&#26041;&#26696;&#8221;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22312;&#27599;&#36718;&#24320;&#22987;&#26102;&#65292;&#21334;&#23478;&#21487;&#20197;&#20915;&#23450;&#25552;&#20379;&#20160;&#20040;&#26679;&#30340;&#20449;&#21495;&#26469;&#21578;&#30693;&#20080;&#23478;&#20135;&#21697;&#23454;&#38469;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#27969;&#34892;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#26694;&#26550;&#26469;&#27169;&#25311;&#36825;&#20123;&#20449;&#21495;&#23545;&#20080;&#23478;&#30340;&#35780;&#20272;&#21644;&#36141;&#20080;&#21453;&#24212;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#22312;&#26368;&#22823;&#21270;&#21334;&#26041;&#39044;&#26399;&#25910;&#20837;&#30340;&#21516;&#26102;&#25214;&#21040;&#24191;&#21578;&#26041;&#26696;&#21644;&#23450;&#20215;&#26041;&#26696;&#30340;&#26368;&#20248;&#35774;&#35745;&#38382;&#39064;&#12290;&#22312;&#27809;&#26377;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#22312;&#32447;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20351;&#29992;&#36807;&#21435;&#30340;&#36141;&#20080;&#21453;&#24212;&#26469;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#26368;&#20248;&#23450;&#20215;&#21644;&#24191;&#21578;&#31574;&#30053;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#31639;&#27861;&#30340;&#21518;&#24724;&#65292;&#19982;&#26368;&#20248;&#30340;&#21315;&#37324;&#20043;&#22564;&#20215;&#26684;&#21644;&#24191;&#21578;&#35745;&#21010;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#19968;&#31181;&#35745;&#31639;&#26377;&#25928;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#21363;&#20351;&#21334;&#23478;&#27809;&#26377;&#20080;&#23478;&#38656;&#27714;&#20989;&#25968;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20339;&#22266;&#23450;&#20215;&#26684;&#21644;&#24191;&#21578;&#26041;&#26696;&#30456;&#20851;&#30340;&#27425;&#32447;&#24615;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a novel dynamic pricing and learning setting where in addition to setting prices of products in sequential rounds, the seller also ex-ante commits to 'advertising schemes'. That is, in the beginning of each round the seller can decide what kind of signal they will provide to the buyer about the product's quality upon realization. Using the popular Bayesian persuasion framework to model the effect of these signals on the buyers' valuation and purchase responses, we formulate the problem of finding an optimal design of the advertising scheme along with a pricing scheme that maximizes the seller's expected revenue. Without any apriori knowledge of the buyers' demand function, our goal is to design an online algorithm that can use past purchase responses to adaptively learn the optimal pricing and advertising strategy. We study the regret of the algorithm when compared to the optimal clairvoyant price and advertising scheme.  Our main result is a computationally efficient onlin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11171</link><description>&lt;p&gt;
&#39063;&#31890;&#29699;&#35745;&#31639;&#65306;&#19968;&#31181;&#39640;&#25928;&#12289;&#40065;&#26834;&#21644;&#21487;&#35299;&#37322;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method. (arXiv:2304.11171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35748;&#30693;&#20855;&#26377;&#8220;&#20808;&#22823;&#21518;&#23567;&#8221;&#30340;&#35748;&#30693;&#26426;&#21046;&#65292;&#22240;&#27492;&#20855;&#26377;&#33258;&#36866;&#24212;&#30340;&#22810;&#31890;&#24230;&#25551;&#36848;&#33021;&#21147;&#12290;&#36825;&#23548;&#33268;&#20102;&#26377;&#25928;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#35745;&#31639;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#12290;&#20182;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20960;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human cognition has a ``large-scale first'' cognitive mechanism, therefore possesses adaptive multi-granularity description capabilities. This results in computational characteristics such as efficiency, robustness, and interpretability. Although most existing artificial intelligence learning methods have certain multi-granularity features, they do not fully align with the ``large-scale first'' cognitive mechanism. Multi-granularity granular-ball computing is an important model method developed in recent years. This method can use granular-balls of different sizes to adaptively represent and cover the sample space, and perform learning based on granular-balls. Since the number of coarse-grained "granular-ball" is smaller than the number of sample points, granular-ball computing is more efficient; the coarse-grained characteristics of granular-balls are less likely to be affected by fine-grained sample points, making them more robust; the multi-granularity structure of granular-balls ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#948;&#35843;&#33410;&#26041;&#27861;&#8212;&#8212;AdapterGNN&#65292;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#21033;&#29992;&#39640;&#24230;&#34920;&#36798;&#30340;&#36866;&#37197;&#22120;&#33021;&#22815;&#22312;&#20165;&#26377;&#23569;&#37327;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20248;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.09595</link><description>&lt;p&gt;
AdapterGNN&#65306;&#39640;&#25928;&#30340;&#948;&#35843;&#33410;&#25552;&#39640;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
AdapterGNN: Efficient Delta Tuning Improves Generalization Ability in Graph Neural Networks. (arXiv:2304.09595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#948;&#35843;&#33410;&#26041;&#27861;&#8212;&#8212;AdapterGNN&#65292;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#21033;&#29992;&#39640;&#24230;&#34920;&#36798;&#30340;&#36866;&#37197;&#22120;&#33021;&#22815;&#22312;&#20165;&#26377;&#23569;&#37327;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#38500;&#20102;&#39044;&#35757;&#32451;&#25216;&#26415;&#22806;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#39046;&#22495;&#30340;&#26368;&#26032;&#24037;&#20316;&#30340;&#21551;&#31034;&#65292;&#26356;&#36817;&#26399;&#30340;&#30740;&#31350;&#36716;&#21521;&#24212;&#29992;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20363;&#22914;&#21442;&#25968;&#26377;&#25928;&#30340;&#35843;&#33410;&#65288;&#948;&#35843;&#33410;&#65289;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;GNNs&#21644;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#24046;&#24322;&#65292;&#23558;&#36825;&#20123;&#26041;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;GNNs&#35777;&#26126;&#25928;&#26524;&#36739;&#24369;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;GNNs&#30340;&#948;&#35843;&#33410;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#20026;GNNs&#35774;&#35745;&#30340;&#26032;&#22411;&#948;&#35843;&#33410;&#26041;&#27861;&#8212;&#8212;AdapterGNN&#12290;AdapterGNN&#20445;&#30041;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#24182;&#21033;&#29992;&#39640;&#24230;&#34920;&#36798;&#30340;GNN&#36866;&#37197;&#22120;&#65292;&#22312;&#20165;&#26377;&#23569;&#37327;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;AdapterGNN&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#39640;&#25928;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pre-trained models has recently yielded remarkable performance gains in graph neural networks (GNNs). In addition to pre-training techniques, inspired by the latest work in the natural language fields, more recent work has shifted towards applying effective fine-tuning approaches, such as parameter-efficient tuning (delta tuning). However, given the substantial differences between GNNs and transformer-based models, applying such approaches directly to GNNs proved to be less effective. In this paper, we present a comprehensive comparison of delta tuning techniques for GNNs and propose a novel delta tuning method specifically designed for GNNs, called AdapterGNN. AdapterGNN preserves the knowledge of the large pre-trained model and leverages highly expressive adapters for GNNs, which can adapt to downstream tasks effectively with only a few parameters, while also improving the model's generalization ability on the downstream tasks. Extensive experiments show that AdapterGNN a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PFELS&#30340;&#26080;&#32447;FL&#26041;&#26696;&#65292;&#36890;&#36807;&#20808;&#21387;&#32553;&#27169;&#22411;&#26356;&#26032;&#20877;&#33258;&#36866;&#24212;&#22320;&#35774;&#35745;&#21457;&#36865;&#21151;&#29575;&#26469;&#25552;&#20379;&#23458;&#25143;&#31471;&#32423;&#21035;DP&#20445;&#35777;&#65292;&#24182;&#38477;&#20302;&#36890;&#20449;&#21644;&#33021;&#37327;&#24320;&#38144;&#24182;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.07460</link><description>&lt;p&gt;
&#20855;&#26377;&#20869;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#39640;&#25928;&#36890;&#20449;&#21644;&#33410;&#33021;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication and Energy Efficient Wireless Federated Learning with Intrinsic Privacy. (arXiv:2304.07460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PFELS&#30340;&#26080;&#32447;FL&#26041;&#26696;&#65292;&#36890;&#36807;&#20808;&#21387;&#32553;&#27169;&#22411;&#26356;&#26032;&#20877;&#33258;&#36866;&#24212;&#22320;&#35774;&#35745;&#21457;&#36865;&#21151;&#29575;&#26469;&#25552;&#20379;&#23458;&#25143;&#31471;&#32423;&#21035;DP&#20445;&#35777;&#65292;&#24182;&#38477;&#20302;&#36890;&#20449;&#21644;&#33021;&#37327;&#24320;&#38144;&#24182;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#36793;&#32536;&#35774;&#22791;&#22312;&#20445;&#30041;&#21407;&#22987;&#25968;&#25454;&#30340;&#21516;&#26102;&#21327;&#21516;&#23398;&#20064;&#20840;&#23616;&#27169;&#22411;&#12290;&#34429;&#28982;FL&#36991;&#20813;&#20102;&#20174;&#26412;&#22320;&#25968;&#25454;&#38598;&#27844;&#28431;&#30452;&#25509;&#20449;&#24687;&#65292;&#20294;&#20173;&#21487;&#33021;&#20174;&#20849;&#20139;&#27169;&#22411;&#25512;&#26029;&#20986;&#25935;&#24863;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;FL&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26426;&#21046;&#25552;&#20379;&#27491;&#24335;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26080;&#32447;&#36793;&#32536;&#37096;&#32626;FL&#26102;&#65292;&#30830;&#20445;&#23458;&#25143;&#31471;&#32423;&#21035;&#30340;DP&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24102;&#31232;&#30095;&#21270;&#30340;&#31169;&#26377;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#65288;PFELS&#65289;&#30340;&#26032;&#22411;&#26080;&#32447;FL&#26041;&#26696;&#65292;&#20197;&#25552;&#20379;&#20855;&#26377;&#20869;&#22312;&#20449;&#36947;&#22122;&#22768;&#30340;&#23458;&#25143;&#31471;&#32423;&#21035;DP&#20445;&#35777;&#65292;&#21516;&#26102;&#38477;&#20302;&#36890;&#20449;&#21644;&#33021;&#37327;&#24320;&#38144;&#24182;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;PFELS&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#27599;&#20010;&#35774;&#22791;&#20808;&#21387;&#32553;&#20854;&#27169;&#22411;&#26356;&#26032;&#65292;&#28982;&#21518;&#26681;&#25454;&#26080;&#32447;&#20449;&#36947;&#33258;&#36866;&#24212;&#35774;&#35745;&#21387;&#32553;&#27169;&#22411;&#26356;&#26032;&#30340;&#21457;&#36865;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a collaborative learning framework that enables edge devices to collaboratively learn a global model while keeping raw data locally. Although FL avoids leaking direct information from local datasets, sensitive information can still be inferred from the shared models. To address the privacy issue in FL, differential privacy (DP) mechanisms are leveraged to provide formal privacy guarantee. However, when deploying FL at the wireless edge with over-the-air computation, ensuring client-level DP faces significant challenges. In this paper, we propose a novel wireless FL scheme called private federated edge learning with sparsification (PFELS) to provide client-level DP guarantee with intrinsic channel noise while reducing communication and energy overhead and improving model accuracy. The key idea of PFELS is for each device to first compress its model update and then adaptively design the transmit power of the compressed model update according to the wireless cha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#27491;&#21017;&#21270;&#21644;&#22810;&#35270;&#35282;&#25903;&#25345;&#21521;&#37327;&#26426;&#23398;&#20064;&#38382;&#39064;&#30340;&#26412;&#22320;&#21270;&#29256;&#26412;&#65292;&#35777;&#26126;&#20102;&#19968;&#20123;&#34920;&#31034;&#23450;&#29702;&#65292;&#30740;&#31350;&#20102;&#19982;&#25439;&#22833;&#20989;&#25968;&#21644;&#36755;&#20837;&#31354;&#38388;&#32500;&#24230;&#30456;&#20851;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#29305;&#21035;&#26159;&#25439;&#22833;&#20989;&#25968;&#20026; G&#226;teaux &#21487;&#24494;&#20989;&#25968;&#26102;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2304.05655</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#21644;&#22810;&#35270;&#35282;&#25903;&#25345;&#21521;&#37327;&#26426;&#23398;&#20064;&#30340;&#26412;&#22320;&#21270;
&lt;/p&gt;
&lt;p&gt;
Localisation of Regularised and Multiview Support Vector Machine Learning. (arXiv:2304.05655v1 [math.FA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#27491;&#21017;&#21270;&#21644;&#22810;&#35270;&#35282;&#25903;&#25345;&#21521;&#37327;&#26426;&#23398;&#20064;&#38382;&#39064;&#30340;&#26412;&#22320;&#21270;&#29256;&#26412;&#65292;&#35777;&#26126;&#20102;&#19968;&#20123;&#34920;&#31034;&#23450;&#29702;&#65292;&#30740;&#31350;&#20102;&#19982;&#25439;&#22833;&#20989;&#25968;&#21644;&#36755;&#20837;&#31354;&#38388;&#32500;&#24230;&#30456;&#20851;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#29305;&#21035;&#26159;&#25439;&#22833;&#20989;&#25968;&#20026; G&#226;teaux &#21487;&#24494;&#20989;&#25968;&#26102;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102; H.Q. Minh&#12289;L. Bazzani &#21644; V. Murino &#22312;&#12298;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#12299;&#65288;Journal of Machine Learning Research&#65289;&#20013;&#20171;&#32461;&#30340;&#19968;&#31181;&#28041;&#21450;&#31639;&#23376;&#20540;&#27491;&#23450;&#26680;&#21450;&#20854;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#21644;&#22810;&#35270;&#35282;&#25903;&#25345;&#21521;&#37327;&#26426;&#23398;&#20064;&#38382;&#39064;&#30340;&#26412;&#22320;&#21270;&#29256;&#26412;&#30340;&#19968;&#20123;&#34920;&#31034;&#23450;&#29702;&#12290;&#32467;&#26524;&#28041;&#21450;&#21040;&#32771;&#34385;&#20984;&#25110;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#26377;&#38480;&#25110;&#26080;&#38480;&#32500;&#36755;&#20837;&#31354;&#38388;&#30340;&#19968;&#33324;&#24773;&#20917;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#19968;&#33324;&#26694;&#26550;&#20801;&#35768;&#19968;&#20123;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#26080;&#38480;&#32500;&#36755;&#20837;&#31354;&#38388;&#21644;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#29305;&#21035;&#26159;&#24403;&#25439;&#22833;&#20989;&#25968;&#20026; G&#226;teaux &#21487;&#24494;&#20989;&#25968;&#26102;&#12290;&#23545;&#23548;&#33268;&#37096;&#20998;&#38750;&#32447;&#24615;&#38382;&#39064;&#30340;&#25351;&#25968;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20102;&#35814;&#32454;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove a few representer theorems for a localised version of the regularised and multiview support vector machine learning problem introduced by H.Q.~Minh, L.~Bazzani, and V.~Murino, \textit{Journal of Machine Learning Research}, \textbf{17}(2016) 1--72, that involves operator valued positive semidefinite kernels and their reproducing kernel Hilbert spaces. The results concern general cases when convex or nonconvex loss functions and finite or infinite dimensional input spaces are considered. We show that the general framework allows infinite dimensional input spaces and nonconvex loss functions for some special cases, in particular in case the loss functions are G\^ateaux differentiable. Detailed calculations are provided for the exponential least squares loss functions that leads to partially nonlinear problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25910;&#32553;&#24341;&#23548;&#30340;&#33258;&#36866;&#24212;&#20998;&#21306;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#24102;&#26377;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#21644;&#24178;&#25200;&#30340;&#38750;&#32447;&#24615;&#21453;&#39304;&#22238;&#36335;&#20013;&#21306;&#38388;&#20540;&#40065;&#26834;&#21487;&#36798;&#38598;&#20272;&#35745;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27493;&#39588;&#21644;&#21487;&#36798;&#24615;&#20998;&#21306;&#23618;&#30340;&#35299;&#32806;&#65292;&#21487;&#20197;&#22312;&#24456;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25552;&#20379;&#31934;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2304.03671</link><description>&lt;p&gt;
&#22522;&#20110;&#25910;&#32553;&#24341;&#23548;&#30340;&#33258;&#36866;&#24212;&#20998;&#21306;&#27861;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#21487;&#36798;&#38598;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Contraction-Guided Adaptive Partitioning for Reachability Analysis of Neural Network Controlled Systems. (arXiv:2304.03671v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25910;&#32553;&#24341;&#23548;&#30340;&#33258;&#36866;&#24212;&#20998;&#21306;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#24102;&#26377;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#21644;&#24178;&#25200;&#30340;&#38750;&#32447;&#24615;&#21453;&#39304;&#22238;&#36335;&#20013;&#21306;&#38388;&#20540;&#40065;&#26834;&#21487;&#36798;&#38598;&#20272;&#35745;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27493;&#39588;&#21644;&#21487;&#36798;&#24615;&#20998;&#21306;&#23618;&#30340;&#35299;&#32806;&#65292;&#21487;&#20197;&#22312;&#24456;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25552;&#20379;&#31934;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25910;&#32553;&#24341;&#23548;&#30340;&#33258;&#36866;&#24212;&#20998;&#21306;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#24102;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#21644;&#25200;&#21160;&#30340;&#38750;&#32447;&#24615;&#21453;&#39304;&#22238;&#36335;&#20013;&#21306;&#38388;&#20540;&#40065;&#26834;&#21487;&#36798;&#38598;&#20272;&#35745;&#12290;&#31639;&#27861;&#26681;&#25454;&#36229;&#36924;&#36817;&#21306;&#38388;&#30340;&#25910;&#32553;&#36895;&#29575;&#20272;&#35745;&#26469;&#36873;&#25321;&#20309;&#26102;&#20309;&#22320;&#36827;&#34892;&#20998;&#21306;&#12290;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27493;&#39588;&#21644;&#21487;&#36798;&#24615;&#20998;&#21306;&#23618;&#30340;&#35299;&#32806;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#24456;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25552;&#20379;&#31934;&#24230;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#20855;&#26377;&#36275;&#22815;&#31934;&#24230;&#30340;&#24320;&#29615;&#21306;&#38388;&#20540;&#21487;&#36798;&#24615;&#20272;&#35745;&#25216;&#26415;&#21644;&#20219;&#20309;&#29992;&#20110;&#30028;&#23450;&#31070;&#32463;&#32593;&#32476;&#36755;&#20837;&#36755;&#20986;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#22522;&#20110;&#25910;&#32553;&#30340;&#40065;&#26834;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#20026;&#28151;&#21512;&#21333;&#35843;&#21487;&#36798;&#24615;&#31639;&#27861;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#25968;&#20540;&#27169;&#25311;&#26469;&#23637;&#31034;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a contraction-guided adaptive partitioning algorithm for improving interval-valued robust reachable set estimates in a nonlinear feedback loop with a neural network controller and disturbances. Based on an estimate of the contraction rate of over-approximated intervals, the algorithm chooses when and where to partition. Then, by leveraging a decoupling of the neural network verification step and reachability partitioning layers, the algorithm can provide accuracy improvements for little computational cost. This approach is applicable with any sufficiently accurate open-loop interval-valued reachability estimation technique and any method for bounding the input-output behavior of a neural network. Using contraction-based robustness analysis, we provide guarantees of the algorithm's performance with mixed monotone reachability. Finally, we demonstrate the algorithm's performance through several numerical simulations and compare it with existing methods in the li
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;Neo4j&#22270;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#20013;&#30340;&#20132;&#36890;&#25317;&#22581;&#38382;&#39064;&#65292;&#33021;&#22815;&#23454;&#29616;&#25317;&#22581;&#36335;&#27573;&#30340;&#36127;&#36733;&#24179;&#34913;&#21644;&#20248;&#21270;&#65292;&#21516;&#26102;&#21487;&#20197;&#39044;&#27979;&#20132;&#36890;&#20107;&#25925;&#23545;&#25972;&#20307;&#20132;&#36890;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.00192</link><description>&lt;p&gt;
&#22522;&#20110;Neo4j&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20132;&#36890;&#25317;&#22581;&#27169;&#25311;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Leveraging Neo4j and deep learning for traffic congestion simulation &amp; optimization. (arXiv:2304.00192v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;Neo4j&#22270;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#20013;&#30340;&#20132;&#36890;&#25317;&#22581;&#38382;&#39064;&#65292;&#33021;&#22815;&#23454;&#29616;&#25317;&#22581;&#36335;&#27573;&#30340;&#36127;&#36733;&#24179;&#34913;&#21644;&#20248;&#21270;&#65292;&#21516;&#26102;&#21487;&#20197;&#39044;&#27979;&#20132;&#36890;&#20107;&#25925;&#23545;&#25972;&#20307;&#20132;&#36890;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#25317;&#22581;&#19968;&#30452;&#26159;&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#36807;&#21435;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20197;&#20984;&#26174;&#19982;&#20132;&#36890;&#25317;&#22581;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#20132;&#36890;&#25317;&#22581;&#20998;&#26512;&#37117;&#26159;&#20351;&#29992;&#27169;&#25311;&#36719;&#20214;&#36827;&#34892;&#30340;&#65292;&#36825;&#20123;&#36719;&#20214;&#30001;&#20110;&#20351;&#29992;&#30340;&#24037;&#20855;&#21644;&#23454;&#29992;&#31243;&#24207;&#30340;&#38480;&#21046;&#32780;&#25552;&#20379;&#20102;&#26377;&#38480;&#30340;&#27934;&#35265;&#12290;&#25152;&#26377;&#36825;&#20123;&#37117;&#24433;&#21709;&#21040;&#23450;&#21046;&#19994;&#21153;&#38382;&#39064;&#30340;&#21046;&#23450;&#65292;&#36825;&#20123;&#38382;&#39064;&#22240;&#22320;&#21306;&#21644;&#22269;&#23478;&#32780;&#24322;&#12290;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#20132;&#36890;&#25317;&#22581;&#38382;&#39064;&#24314;&#27169;&#20026;Neo4j&#22270;&#65292;&#28982;&#21518;&#20351;&#29992;&#36127;&#36733;&#24179;&#34913;&#12289;&#20248;&#21270;&#31639;&#27861;&#26469;&#35782;&#21035;&#26080;&#25317;&#22581;&#30340;&#36947;&#36335;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#25317;&#22581;&#25110;&#20107;&#25925;&#24773;&#20917;&#19979;&#20132;&#36890;&#22914;&#20309;&#21521;&#21518;&#20256;&#25773;&#20197;&#21450;&#20854;&#23545;&#20854;&#20182;&#36947;&#36335;&#27573;&#30340;&#24635;&#20307;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#22312;&#23454;&#26102;&#20132;&#36890;&#25968;&#25454;&#19978;&#35757;&#32451;&#20102;&#39034;&#24207;RNN-LSTM(&#38271;&#30701;&#26102;&#35760;&#24518;)&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic congestion has been a major challenge in many urban road networks. Extensive research studies have been conducted to highlight traffic-related congestion and address the issue using data-driven approaches. Currently, most traffic congestion analyses are done using simulation software that offers limited insight due to the limitations in the tools and utilities being used to render various traffic congestion scenarios. All that impacts the formulation of custom business problems which vary from place to place and country to country. By exploiting the power of the knowledge graph, we model a traffic congestion problem into the Neo4j graph and then use the load balancing, optimization algorithm to identify congestion-free road networks. We also show how traffic propagates backward in case of congestion or accident scenarios and its overall impact on other segments of the roads. We also train a sequential RNN-LSTM (Long Short-Term Memory) deep learning model on the real-time traffi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#23558;&#29305;&#23450;&#20248;&#21270;&#38382;&#39064;&#19982;&#22810;&#26102;&#38388;Hamilton-Jacobi PDEs&#32852;&#31995;&#36215;&#26469;&#65292;&#34920;&#26126;&#24403;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26102;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#23545;&#24212;&#30340;&#22810;&#26102;&#38388;HJ PDEs&#21644;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;&#21033;&#29992;&#36825;&#31181;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.12928</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#26102;&#38388; Hamilton-Jacobi PDE &#35299;&#20915;&#19968;&#20123;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Multi-time Hamilton-Jacobi PDEs for Certain Scientific Machine Learning Problems. (arXiv:2303.12928v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#23558;&#29305;&#23450;&#20248;&#21270;&#38382;&#39064;&#19982;&#22810;&#26102;&#38388;Hamilton-Jacobi PDEs&#32852;&#31995;&#36215;&#26469;&#65292;&#34920;&#26126;&#24403;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26102;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#23545;&#24212;&#30340;&#22810;&#26102;&#38388;HJ PDEs&#21644;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;&#21033;&#29992;&#36825;&#31181;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Hamilton-Jacobi &#20559;&#24494;&#20998;&#26041;&#31243;(HJ PDEs)&#19982;&#24191;&#27867;&#39046;&#22495;&#65292;&#22914;&#26368;&#20248;&#25511;&#21046;&#12289;&#24494;&#20998;&#28216;&#25103;&#21644;&#25104;&#20687;&#31185;&#23398;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#12290;&#36890;&#36807;&#23558;&#26102;&#38388;&#21464;&#37327;&#35270;&#20026;&#26356;&#39640;&#32500;&#30340;&#37327;&#65292;HJ PDEs &#21487;&#20197;&#25193;&#23637;&#21040;&#22810;&#26102;&#38388;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#22312;&#29305;&#23450;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#38382;&#39064;&#19982;&#22810;&#26102;&#38388;Hopf&#20844;&#24335;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#32852;&#31995;&#65292;&#35813;&#20844;&#24335;&#23545;&#24212;&#20110;&#26576;&#20123;&#22810;&#26102;&#38388; HJ PDEs &#30340;&#35299;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#36825;&#31181;&#32852;&#31995;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#24403;&#25105;&#20204;&#35299;&#20915;&#36825;&#20123;&#23398;&#20064;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#20063;&#35299;&#20915;&#20102;&#19968;&#20010;&#22810;&#26102;&#38388; HJ PDE &#21644;&#30456;&#24212;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#26576;&#20123;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#30340;&#35757;&#32451;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20316;&#20026;&#36825;&#31181;&#32852;&#31995;&#30340;&#31532;&#19968;&#20010;&#25506;&#32034;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;&#27491;&#21017;&#21270;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#19982;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120; (LQR) &#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#29702;&#35770;&#26694;&#26550;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hamilton-Jacobi partial differential equations (HJ PDEs) have deep connections with a wide range of fields, including optimal control, differential games, and imaging sciences. By considering the time variable to be a higher dimensional quantity, HJ PDEs can be extended to the multi-time case. In this paper, we establish a novel theoretical connection between specific optimization problems arising in machine learning and the multi-time Hopf formula, which corresponds to a representation of the solution to certain multi-time HJ PDEs. Through this connection, we increase the interpretability of the training process of certain machine learning applications by showing that when we solve these learning problems, we also solve a multi-time HJ PDE and, by extension, its corresponding optimal control problem. As a first exploration of this connection, we develop the relation between the regularized linear regression problem and the Linear Quadratic Regulator (LQR). We then leverage our theoret
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.12247</link><description>&lt;p&gt;
&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#65306;&#19968;&#31181;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Quantifying &amp; Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12247
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#25152;&#38656;&#30340;&#20132;&#20114;&#22914;&#20309;&#36827;&#34892;&#37327;&#21270;&#65311;&#26368;&#36866;&#21512;&#25429;&#25417;&#36825;&#20123;&#20132;&#20114;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#20160;&#20040;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#26041;&#27861;&#26469;&#37327;&#21270;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#19977;&#20010;&#34913;&#37327;&#26631;&#20934;&#31216;&#20026;&#22810;&#27169;&#24577;&#20998;&#24067;&#65288;&#25110;&#31616;&#31216;PID&#65289;&#30340;PID&#32479;&#35745;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#20998;&#24067;&#12290;&#20026;&#20102;&#39564;&#35777;PID&#20272;&#35745;&#65292;&#25105;&#20204;&#22312;&#24050;&#30693;PID&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#38024;&#23545;&#20154;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36127;&#36131;&#20219;&#25968;&#25454;&#31649;&#29702;&#24314;&#35758;&#65292;&#37319;&#29992;&#39044;&#38450;&#24615;&#21453;&#24605;&#30340;&#35266;&#28857;&#65292;&#36981;&#24490;&#21407;&#21017;&#20027;&#20041;&#30340;&#20262;&#29702;&#26694;&#26550;&#65292;&#35299;&#20915;&#38544;&#31169;&#21644;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.03629</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#21017;&#20027;&#20041;&#30340;&#36127;&#36131;&#20219;&#25968;&#25454;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Principlism Guided Responsible Data Curation. (arXiv:2302.03629v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03629
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#38024;&#23545;&#20154;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36127;&#36131;&#20219;&#25968;&#25454;&#31649;&#29702;&#24314;&#35758;&#65292;&#37319;&#29992;&#39044;&#38450;&#24615;&#21453;&#24605;&#30340;&#35266;&#28857;&#65292;&#36981;&#24490;&#21407;&#21017;&#20027;&#20041;&#30340;&#20262;&#29702;&#26694;&#26550;&#65292;&#35299;&#20915;&#38544;&#31169;&#21644;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#25972;&#29702;&#23454;&#36341;&#32463;&#24120;&#24573;&#30053;&#38544;&#31169;&#21644;&#20559;&#24046;&#38382;&#39064;&#65292;&#23548;&#33268;&#25968;&#25454;&#38598;&#25764;&#22238;&#21644;&#19981;&#20844;&#24179;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#38750;&#21516;&#24847;&#32593;&#32476;&#29228;&#21462;&#26500;&#24314;&#30340;&#20154;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#32570;&#20047;&#20840;&#38754;&#30340;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#35780;&#20272;&#25152;&#24517;&#38656;&#30340;&#20803;&#25968;&#25454;&#12290;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#27861;&#21518;&#26399;&#35299;&#20915;&#38382;&#39064;&#65292;&#32570;&#20047;&#35828;&#26381;&#21147;&#30340;&#37319;&#29992;&#29702;&#30001;&#25110;&#26410;&#33021;&#25552;&#20379;&#36866;&#24403;&#24212;&#29992;&#30340;&#21512;&#36866;&#32972;&#26223;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30528;&#37325;&#20110;&#38024;&#23545;&#20154;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#20027;&#21160;&#39046;&#22495;&#29305;&#23450;&#24314;&#35758;&#65292;&#35299;&#20915;&#38544;&#31169;&#21644;&#20559;&#24046;&#38382;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#21453;&#24605;&#30340;&#35266;&#28857;&#65292;&#24182;&#20511;&#37492;&#20102;&#29616;&#26377;&#30340;&#23454;&#36341;&#21644;&#25351;&#21335;&#65292;&#36981;&#24490;&#21407;&#21017;&#20027;&#20041;&#30340;&#20262;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-centric computer vision (HCCV) data curation practices often neglect privacy and bias concerns, leading to dataset retractions and unfair models. Further, HCCV datasets constructed through nonconsensual web scraping lack the necessary metadata for comprehensive fairness and robustness evaluations. Current remedies address issues post hoc, lack persuasive justification for adoption, or fail to provide proper contextualization for appropriate application. Our research focuses on proactive, domain-specific recommendations for curating HCCV datasets, addressing privacy and bias. We adopt an ante hoc reflective perspective and draw from current practices and guidelines, guided by the ethical framework of principlism.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#27010;&#29575;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;MTP-GO&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22330;&#26223;&#65292;&#37319;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#23454;&#29616;&#36816;&#21160;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#21644;&#21345;&#23572;&#26364;&#28388;&#27874;&#23454;&#29616;&#22810;&#27169;&#24577;&#27010;&#29575;&#39044;&#27979;&#65292;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.00735</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#27010;&#29575;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;MTP-GO&#19982;&#31070;&#32463;ODE
&lt;/p&gt;
&lt;p&gt;
MTP-GO: Graph-Based Probabilistic Multi-Agent Trajectory Prediction with Neural ODEs. (arXiv:2302.00735v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#27010;&#29575;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;MTP-GO&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22330;&#26223;&#65292;&#37319;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#23454;&#29616;&#36816;&#21160;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#21644;&#21345;&#23572;&#26364;&#28388;&#27874;&#23454;&#29616;&#22810;&#27169;&#24577;&#27010;&#29575;&#39044;&#27979;&#65292;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#24377;&#24615;&#30340;&#33258;&#20027;&#36335;&#24452;&#35268;&#21010;&#38656;&#35201;&#23545;&#21608;&#22260;&#36947;&#36335;&#29992;&#25143;&#26410;&#26469;&#34892;&#20026;&#20570;&#20986;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;&#20026;&#21709;&#24212;&#27492;&#38656;&#27714;&#21450;&#30456;&#20851;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#21517;&#20026;MTP-GO&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#22330;&#26223;&#36827;&#34892;&#32534;&#30721;&#65292;&#29983;&#25104;&#24213;&#23618;&#36816;&#21160;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#36816;&#21160;&#27169;&#22411;&#37319;&#29992;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#20854;&#20013;&#30340;&#29366;&#24577;&#36716;&#31227;&#20989;&#25968;&#23558;&#21644;&#20854;&#20182;&#37096;&#20998;&#19968;&#36215;&#36827;&#34892;&#23398;&#20064;&#12290;&#32467;&#21512;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#21644;&#21345;&#23572;&#26364;&#28388;&#27874;&#30340;&#27010;&#24565;&#65292;&#21487;&#20197;&#33719;&#24471;&#22810;&#27169;&#24577;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#27979;&#33021;&#21147;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enabling resilient autonomous motion planning requires robust predictions of surrounding road users' future behavior. In response to this need and the associated challenges, we introduce our model titled MTP-GO. The model encodes the scene using temporal graph neural networks to produce the inputs to an underlying motion model. The motion model is implemented using neural ordinary differential equations where the state-transition functions are learned with the rest of the model. Multimodal probabilistic predictions are obtained by combining the concept of mixture density networks and Kalman filtering. The results illustrate the predictive capabilities of the proposed model across various data sets, outperforming several state-of-the-art methods on a number of metrics.
&lt;/p&gt;</description></item><item><title>ClimaX&#26159;&#19968;&#31181;&#28789;&#27963;&#19988;&#21487;&#25512;&#24191;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#31185;&#23398;&#65292;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2301.10343</link><description>&lt;p&gt;
ClimaX:&#19968;&#31181;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ClimaX: A foundation model for weather and climate. (arXiv:2301.10343v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10343
&lt;/p&gt;
&lt;p&gt;
ClimaX&#26159;&#19968;&#31181;&#28789;&#27963;&#19988;&#21487;&#25512;&#24191;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#31185;&#23398;&#65292;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#20808;&#36827;&#30340;&#22825;&#27668;&#21644;&#27668;&#20505;&#27169;&#22411;&#37117;&#26159;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#25968;&#20540;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#27169;&#25311;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#21644;&#22810;&#21464;&#37327;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#24456;&#38590;&#36817;&#20284;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#36825;&#26679;&#30340;&#25968;&#20540;&#27169;&#22411;&#22312;&#27169;&#25311;&#32454;&#31890;&#24230;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#22823;&#27668;&#29616;&#35937;&#26102;&#35745;&#31639;&#37327;&#24456;&#22823;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#21151;&#33021;&#26144;&#23556;&#26469;&#30452;&#25509;&#35299;&#20915;&#19979;&#28216;&#39044;&#27979;&#25110;&#25237;&#23556;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32593;&#32476;&#26159;&#20351;&#29992;&#20026;&#29305;&#23450;&#26102;&#31354;&#20219;&#21153;&#31574;&#21010;&#21644;&#21516;&#36136;&#21270;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#32570;&#20047;&#25968;&#20540;&#27169;&#22411;&#30340;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#28436;&#31034;&#20102;ClimaX&#65292;&#36825;&#26159;&#19968;&#20010;&#28789;&#27963;&#19988;&#21487;&#25512;&#24191;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#31185;&#23398;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#36328;&#36234;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#24322;&#26500;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most state-of-the-art approaches for weather and climate modeling are based on physics-informed numerical models of the atmosphere. These approaches aim to model the non-linear dynamics and complex interactions between multiple variables, which are challenging to approximate. Additionally, many such numerical models are computationally intensive, especially when modeling the atmospheric phenomenon at a fine-grained spatial and temporal resolution. Recent data-driven approaches based on machine learning instead aim to directly solve a downstream forecasting or projection task by learning a data-driven functional mapping using deep neural networks. However, these networks are trained using curated and homogeneous climate datasets for specific spatiotemporal tasks, and thus lack the generality of numerical models. We develop and demonstrate ClimaX, a flexible and generalizable deep learning model for weather and climate science that can be trained using heterogeneous datasets spanning dif
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#21452;&#23618;&#32972;&#21253;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#27604;&#31934;&#30830;&#31639;&#27861;&#24555;500&#20493;&#65292;&#21487;&#25214;&#21040;&#21487;&#34892;&#24615;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2211.13436</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#21452;&#23618;&#32972;&#21253;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Bilevel Knapsack Problem using Graph Neural Networks. (arXiv:2211.13436v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#21452;&#23618;&#32972;&#21253;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#27604;&#31934;&#30830;&#31639;&#27861;&#24555;500&#20493;&#65292;&#21487;&#25214;&#21040;&#21487;&#34892;&#24615;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#26159;&#19968;&#31181;&#20855;&#26377;&#20004;&#20010;&#20195;&#29702;&#20154;&#65288;&#39046;&#23548;&#32773;&#21644;&#36861;&#38543;&#32773;&#65289;&#30340;&#23618;&#27425;&#20248;&#21270;&#38382;&#39064;&#12290;&#39046;&#23548;&#32773;&#39318;&#20808;&#20570;&#20986;&#33258;&#24049;&#30340;&#20915;&#31574;&#65292;&#36861;&#38543;&#32773;&#38543;&#21518;&#20570;&#20986;&#26368;&#20339;&#36873;&#25321;&#12290;&#39046;&#23548;&#32773;&#20102;&#35299;&#36861;&#38543;&#32773;&#30340;&#20449;&#24687;&#65292;&#38382;&#39064;&#30340;&#30446;&#26631;&#26159;&#20174;&#39046;&#23548;&#32773;&#30340;&#35282;&#24230;&#32771;&#34385;&#36861;&#38543;&#32773;&#30340;&#21453;&#24212;&#65292;&#25214;&#21040;&#26368;&#20248;&#35299;&#12290;&#23545;&#20110;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#26469;&#35828;&#65292;&#27809;&#26377;&#36890;&#29992;&#30340;&#39640;&#25928;&#31639;&#27861;&#25110;&#21830;&#29992;&#27714;&#35299;&#22120;&#21487;&#20197;&#24471;&#21040;&#26368;&#20248;&#35299;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#38382;&#39064;&#20063;&#24456;&#38590;&#24471;&#21040;&#33391;&#22909;&#30340;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#21452;&#23618;&#32972;&#21253;&#38382;&#39064;&#12290;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#26469;&#39044;&#27979;&#39046;&#23548;&#32773;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#23558;&#23618;&#27425;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#21333;&#23618;&#20248;&#21270;&#38382;&#39064;&#20197;&#33719;&#21462;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21457;&#29616;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36895;&#24230;&#27604;&#31934;&#30830;&#31639;&#27861;&#24555;500&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Bilevel Optimization Problem is a hierarchical optimization problem with two agents, a leader and a follower. The leader make their own decisions first, and the followers make the best choices accordingly. The leader knows the information of the followers, and the goal of the problem is to find the optimal solution by considering the reactions of the followers from the leader's point of view. For the Bilevel Optimization Problem, there are no general and efficient algorithms or commercial solvers to get an optimal solution, and it is very difficult to get a good solution even for a simple problem. In this paper, we propose a deep learning approach using Graph Neural Networks to solve the bilevel knapsack problem. We train the model to predict the leader's solution and use it to transform the hierarchical optimization problem into a single-level optimization problem to get the solution. Our model found the feasible solution that was about 500 times faster than the exact algorithm wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36882;&#24402;&#35745;&#31639;&#36817;&#20284;&#20805;&#20998;&#32479;&#35745;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#20195;&#29702;&#35774;&#35745;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2211.01595</link><description>&lt;p&gt;
&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning in Non-Markovian Environments. (arXiv:2211.01595v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36882;&#24402;&#35745;&#31639;&#36817;&#20284;&#20805;&#20998;&#32479;&#35745;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#20195;&#29702;&#35774;&#35745;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;Van Roy&#21450;&#20854;&#21512;&#20316;&#32773;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#22312;&#20219;&#24847;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#33539;&#24335;&#65292;&#24182;&#26126;&#30830;&#20102;&#24403;&#22312;&#35813;&#33539;&#24335;&#19978;&#24212;&#29992;Q&#23398;&#20064;&#31639;&#27861;&#26102;&#65292;&#30001;&#20110;&#38750;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#24341;&#36215;&#30340;&#38169;&#35823;&#12290;&#22522;&#20110;&#27492;&#35266;&#23519;&#65292;&#25105;&#20204;&#24314;&#35758;&#20195;&#29702;&#35774;&#35745;&#30340;&#26631;&#20934;&#24212;&#26159;&#23547;&#25214;&#26576;&#20123;&#26465;&#20214;&#35268;&#24459;&#30340;&#33391;&#22909;&#36817;&#20284;&#12290;&#21463;&#32463;&#20856;&#38543;&#26426;&#25511;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#38382;&#39064;&#24402;&#32467;&#20026;&#36882;&#24402;&#35745;&#31639;&#36817;&#20284;&#20805;&#20998;&#32479;&#35745;&#37327;&#30340;&#38382;&#39064;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#20195;&#29702;&#35774;&#35745;&#26041;&#26696;&#65292;&#25105;&#20204;&#22312;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#25968;&#20540;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the novel paradigm developed by Van Roy and coauthors for reinforcement learning in arbitrary non-Markovian environments, we propose a related formulation and explicitly pin down the error caused by non-Markovianity of observations when the Q-learning algorithm is applied on this formulation. Based on this observation, we propose that the criterion for agent design should be to seek good approximations for certain conditional laws. Inspired by classical stochastic control, we show that our problem reduces to that of recursive computation of approximate sufficient statistics. This leads to an autoencoder-based scheme for agent design which is then numerically tested on partially observed reinforcement learning environments.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#65292;&#23558;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.08964</link><description>&lt;p&gt;
PromptCast&#65306;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting. (arXiv:2210.08964v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#65292;&#23558;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#12290;&#22312;&#36825;&#31181;&#26032;&#30340;&#20219;&#21153;&#20013;&#65292;&#23558;&#21407;&#26469;&#30340;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#39044;&#27979;&#30340;&#30446;&#30340;&#12290;&#20026;&#20102;&#25903;&#25345;&#21644;&#20419;&#36827;&#36825;&#20010;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65288;PISA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new perspective on time series forecasting. In existing time series forecasting methods, the models take a sequence of numerical values as input and yield numerical values as output. The existing SOTA models are largely based on the Transformer architecture, modified with multiple encoding mechanisms to incorporate the context and semantics around the historical data. Inspired by the successes of pre-trained language foundation models, we pose a question about whether these models can also be adapted to solve time-series forecasting. Thus, we propose a new forecasting paradigm: prompt-based time series forecasting (PromptCast). In this novel task, the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-to-sentence manner, making it possible to directly apply language models for forecasting purposes. To support and facilitate the research of this task, we also present a large-scale dataset (PISA) that includes th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#21521;&#25968;&#25454;&#20013;&#28155;&#21152;&#25200;&#21160;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#21160;&#20316;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;BAFFLE&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.04688</link><description>&lt;p&gt;
BAFFLE: &#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
BAFFLE: Backdoor Attack in Offline Reinforcement Learning. (arXiv:2210.04688v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#21521;&#25968;&#25454;&#20013;&#28155;&#21152;&#25200;&#21160;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#21160;&#20316;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;BAFFLE&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#20801;&#35768;&#26234;&#33021;&#20307;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#20013;&#25910;&#38598;&#30340;&#35797;&#38169;&#32463;&#39564;&#36827;&#34892;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#31163;&#32447;RL&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;RL&#33539;&#20363;&#65292;&#22240;&#20026;&#23427;&#33410;&#30465;&#20102;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#12290;&#22312;&#31163;&#32447;RL&#20013;&#65292;&#25968;&#25454;&#25552;&#20379;&#32773;&#20849;&#20139;&#22823;&#35268;&#27169;&#30340;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20182;&#20154;&#21487;&#20197;&#22312;&#19981;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#33539;&#20363;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#12289;&#33258;&#21160;&#39550;&#39542;&#31561;&#20851;&#38190;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36739;&#23569;&#20851;&#27880;&#30740;&#31350;&#31163;&#32447;RL&#31995;&#32479;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#26412;&#25991;&#20851;&#27880;&#21518;&#38376;&#25915;&#20987;&#65292;&#20854;&#20013;&#19968;&#20123;&#25200;&#21160;&#34987;&#28155;&#21152;&#21040;&#25968;&#25454;&#65288;&#35266;&#27979;&#20540;&#65289;&#20013;&#65292;&#20351;&#24471;&#22312;&#32473;&#23450;&#27491;&#24120;&#35266;&#27979;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#37319;&#21462;&#39640;&#22870;&#21169;&#30340;&#21160;&#20316;&#65292;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BAFFLE&#65288;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of research has focused on the Reinforcement Learning (RL) methods which allow the agent to learn from trial-and-error experiences gathered during the interaction with the environment. Recently, offline RL becomes a popular RL paradigm because it saves the interactions with environments. In offline RL, data providers share large pre-collected datasets, and others can train high-quality agents without interacting with the environments. This paradigm has demonstrated effectiveness in critical tasks like robot control, autonomous driving, etc. However, less attention is paid to investigating the security threats to the offline RL system. This paper focuses on backdoor attacks, where some perturbations are added to the data (observations) such that given normal observations, the agent takes high-rewards actions, and low-reward actions on observations injected with triggers. In this paper, we propose Baffle (Backdoor Attack for Offline Reinforcement Learning), an approach tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UniCon&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23458;&#25143;VQA&#20219;&#21153;&#30340;&#20445;&#23494;&#24615;&#32422;&#26463;&#21644;&#23458;&#25143;&#26377;&#38480;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27169;&#22411;&#20849;&#20139;&#23398;&#20064;&#36328;&#27169;&#24577;&#34920;&#31034;&#65292;&#37319;&#29992;&#20998;&#35010;&#23398;&#20064;&#26550;&#26500;&#30830;&#20445;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2208.11435</link><description>&lt;p&gt;
UniCon: &#24102;&#26377;&#23545;&#27604;&#25439;&#22833;&#30340;&#21333;&#21521;&#20998;&#27495;&#23398;&#20064;&#29992;&#20110;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
UniCon: Unidirectional Split Learning with Contrastive Loss for Visual Question Answering. (arXiv:2208.11435v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UniCon&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23458;&#25143;VQA&#20219;&#21153;&#30340;&#20445;&#23494;&#24615;&#32422;&#26463;&#21644;&#23458;&#25143;&#26377;&#38480;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27169;&#22411;&#20849;&#20139;&#23398;&#20064;&#36328;&#27169;&#24577;&#34920;&#31034;&#65292;&#37319;&#29992;&#20998;&#35010;&#23398;&#20064;&#26550;&#26500;&#30830;&#20445;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#25968;&#25454;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26377;&#21161;&#20110;&#29616;&#23454;&#24212;&#29992;&#65292;&#22914;&#23478;&#24237;&#26426;&#22120;&#20154;&#21644;&#21307;&#23398;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#20026;&#21508;&#31181;&#23458;&#25143;&#20219;&#21153;&#35774;&#35745;&#24378;&#22823;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#30830;&#20445;&#38544;&#31169;&#65292;&#22240;&#20026;&#30001;&#20110;&#20445;&#23494;&#38382;&#39064;&#65292;&#23458;&#25143;&#25968;&#25454;&#20849;&#20139;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#33268;&#21147;&#20110;&#35299;&#20915;&#22810;&#23458;&#25143;VQA&#20219;&#21153;&#30340;&#20445;&#23494;&#24615;&#32422;&#26463;&#21644;&#23458;&#25143;&#26377;&#38480;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26377;&#23545;&#27604;&#25439;&#22833;&#30340;&#21333;&#21521;&#20998;&#27495;&#23398;&#20064;&#65288;UniCon&#65289;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#23458;&#25143;&#30340;&#25972;&#20010;&#25968;&#25454;&#20998;&#24067;&#19978;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#65292;&#36890;&#36807;&#27169;&#22411;&#20849;&#20139;&#23398;&#20064;&#31934;&#32454;&#30340;&#36328;&#27169;&#24577;&#34920;&#31034;&#26469;&#23454;&#29616;&#38544;&#31169;&#20445;&#35777;&#65292;&#21033;&#29992;&#20998;&#35010;&#23398;&#20064;&#26550;&#26500;&#30830;&#20445;&#38544;&#31169;&#65292;&#20854;&#20013;&#23436;&#25972;&#27169;&#22411;&#20998;&#20026;&#20004;&#20010;&#32452;&#20214;&#36827;&#34892;&#29420;&#31435;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#21457;&#29616;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#19982;&#25105;&#20204;&#30340;&#26041;&#27861;&#39640;&#24230;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Question Answering (VQA) using multi-modal data facilitates real-life applications, such as home robots and medical diagnoses. However, one significant challenge is to design a robust learning method for various client tasks. One critical aspect is to ensure privacy, as client data sharing is limited due to confidentiality concerns. This work focuses on addressing the issue of confidentiality constraints in multi-client VQA tasks and limited labeled training data of clients. We propose the Unidirectional Split Learning with Contrastive Loss (UniCon) method to overcome these limitations. The proposed method trains a global model on the entire data distribution of different clients, learning refined cross-modal representations through model sharing. Privacy is ensured by utilizing a split learning architecture in which a complete model is partitioned into two components for independent training. Moreover, recent self-supervised learning techniques were found to be highly compatibl
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#22810;&#23610;&#24230;&#24314;&#27169;&#20013;&#37319;&#26679;&#21644;&#38598;&#21512;&#24179;&#22343;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#22686;&#24378;&#37319;&#26679;&#25216;&#26415;&#19982;&#20998;&#23376;&#27169;&#25311;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2208.10715</link><description>&lt;p&gt;
GAN&#21644;&#38381;&#21512;&#24615;: &#22810;&#23610;&#24230;&#24314;&#27169;&#20013;&#30340;&#24494;&#35266;-&#23439;&#35266;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
GANs and Closures: Micro-Macro Consistency in Multiscale Modeling. (arXiv:2208.10715v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10715
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#22810;&#23610;&#24230;&#24314;&#27169;&#20013;&#37319;&#26679;&#21644;&#38598;&#21512;&#24179;&#22343;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#22686;&#24378;&#37319;&#26679;&#25216;&#26415;&#19982;&#20998;&#23376;&#27169;&#25311;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37319;&#26679;&#20998;&#23376;&#31995;&#32479;&#30340;&#30456;&#31354;&#38388;&#65288;&#26356;&#24191;&#20041;&#30340;&#26159;&#65292;&#29992;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#25551;&#36848;&#30340;&#22797;&#26434;&#31995;&#32479;&#30340;&#30456;&#31354;&#38388;&#65289;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#26159;&#37325;&#35201;&#30340;&#24314;&#27169;&#27493;&#39588;&#65292;&#20174;&#34507;&#30333;&#36136;&#25240;&#21472;&#21040;&#26448;&#26009;&#21457;&#29616;&#12290;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#20855;&#26377;&#22810;&#23610;&#24230;&#24615;&#36136;&#65306;&#23427;&#20204;&#21487;&#20197;&#29992;&#23569;&#37327;&#8220;&#32531;&#24930;&#8221;&#21453;&#24212;&#22352;&#26631;&#21442;&#25968;&#21270;&#30340;&#20302;&#32500;&#26377;&#25928;&#33258;&#30001;&#33021;&#38754;&#26469;&#25551;&#36848;&#65307;&#21097;&#20313;&#30340;&#8220;&#24555;&#36895;&#8221;&#33258;&#30001;&#24230;&#22312;&#21453;&#24212;&#22352;&#26631;&#20540;&#19978;&#22635;&#20805;&#24179;&#34913;&#27979;&#24230;&#12290;&#36825;&#20123;&#38382;&#39064;&#30340;&#37319;&#26679;&#36807;&#31243;&#29992;&#20110;&#20272;&#35745;&#26377;&#25928;&#30340;&#33258;&#30001;&#33021;&#24046;&#24322;&#21644;&#19982;&#26465;&#20214;&#24179;&#34913;&#20998;&#24067;&#30456;&#20851;&#30340;&#38598;&#21512;&#24179;&#22343;&#25968;&#65307;&#36825;&#20123;&#21518;&#32773;&#30340;&#24179;&#22343;&#25968;&#23548;&#33268;&#26377;&#25928;&#30340;&#20943;&#23569;&#21160;&#24577;&#27169;&#22411;&#30340;&#38381;&#21512;&#12290;&#22810;&#24180;&#26469;&#65292;&#22686;&#24378;&#37319;&#26679;&#25216;&#26415;&#19982;&#20998;&#23376;&#27169;&#25311;&#30456;&#32467;&#21512;&#12290;&#19982;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#23384;&#22312;&#19968;&#20010;&#26377;&#36259;&#30340;&#31867;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling the phase space of molecular systems -- and, more generally, of complex systems effectively modeled by stochastic differential equations -- is a crucial modeling step in many fields, from protein folding to materials discovery. These problems are often multiscale in nature: they can be described in terms of low-dimensional effective free energy surfaces parametrized by a small number of "slow" reaction coordinates; the remaining "fast" degrees of freedom populate an equilibrium measure on the reaction coordinate values. Sampling procedures for such problems are used to estimate effective free energy differences as well as ensemble averages with respect to the conditional equilibrium distributions; these latter averages lead to closures for effective reduced dynamic models. Over the years, enhanced sampling techniques coupled with molecular simulation have been developed. An intriguing analogy arises with the field of Machine Learning (ML), where Generative Adversarial Networks
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#20010;&#24615;&#21270;&#20869;&#23481;&#30340;&#20379;&#32473;&#20391;&#22343;&#34913;&#38382;&#39064;&#65292;&#20854;&#29305;&#28857;&#26159;&#29983;&#20135;&#32773;&#20915;&#31574;&#31354;&#38388;&#26159;&#22810;&#32500;&#30340;&#21644;&#29992;&#25143;&#32676;&#20307;&#26159;&#24322;&#26500;&#30340;&#65292;&#39640;&#32500;&#24230;&#21644;&#24322;&#36136;&#24615;&#30340;&#27169;&#22411;&#21019;&#36896;&#20102;&#19987;&#19994;&#21270;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.13489</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20379;&#32473;&#20391;&#22343;&#34913;
&lt;/p&gt;
&lt;p&gt;
Supply-Side Equilibria in Recommender Systems. (arXiv:2206.13489v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#20010;&#24615;&#21270;&#20869;&#23481;&#30340;&#20379;&#32473;&#20391;&#22343;&#34913;&#38382;&#39064;&#65292;&#20854;&#29305;&#28857;&#26159;&#29983;&#20135;&#32773;&#20915;&#31574;&#31354;&#38388;&#26159;&#22810;&#32500;&#30340;&#21644;&#29992;&#25143;&#32676;&#20307;&#26159;&#24322;&#26500;&#30340;&#65292;&#39640;&#32500;&#24230;&#21644;&#24322;&#36136;&#24615;&#30340;&#27169;&#22411;&#21019;&#36896;&#20102;&#19987;&#19994;&#21270;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#25512;&#33616;&#31995;&#32479;&#65288;&#22914;Spotify&#21644;Netflix&#65289;&#19981;&#20165;&#24433;&#21709;&#28040;&#36153;&#32773;&#34892;&#20026;&#65292;&#32780;&#19988;&#24433;&#21709;&#29983;&#20135;&#32773;&#30340;&#28608;&#21169;&#12290;&#29983;&#20135;&#32773;&#35797;&#22270;&#21019;&#24314;&#23558;&#34987;&#25512;&#33616;&#31639;&#27861;&#26174;&#31034;&#30340;&#20869;&#23481;&#65292;&#36825;&#21487;&#33021;&#24433;&#21709;&#20182;&#20204;&#20869;&#23481;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#12290;&#26412;&#25991;&#30740;&#31350;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20379;&#32473;&#20391;&#22343;&#34913;&#12290;&#25105;&#20204;&#23558;&#29992;&#25143;&#21644;&#20869;&#23481;&#24314;&#27169;&#20026; $D$ &#32500;&#21521;&#37327;&#65292;&#25512;&#33616;&#31639;&#27861;&#26174;&#31034;&#27599;&#20010;&#29992;&#25143;&#19982;&#20043;&#26368;&#39640;&#28857;&#31215;&#30340;&#20869;&#23481;&#65292;&#29983;&#20135;&#32773;&#26368;&#22823;&#21270;&#34987;&#25512;&#33616;&#20854;&#20869;&#23481;&#30340;&#29992;&#25143;&#25968;&#20943;&#21435;&#29983;&#20135;&#25104;&#26412;&#12290;&#25105;&#20204;&#27169;&#22411;&#30340;&#20004;&#20010;&#20851;&#38190;&#29305;&#24449;&#26159;&#29983;&#20135;&#32773;&#20915;&#31574;&#31354;&#38388;&#26159;&#22810;&#32500;&#30340;&#65292;&#29992;&#25143;&#32676;&#20307;&#26159;&#24322;&#26500;&#30340;&#65292;&#36825;&#19982;&#32463;&#20856;&#20302;&#32500;&#27169;&#22411;&#19981;&#21516;&#12290;&#22810;&#32500;&#24615;&#21644;&#24322;&#36136;&#24615;&#21019;&#36896;&#20102;&#19987;&#19994;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#19981;&#21516;&#30340;&#29983;&#20135;&#32773;&#22312;&#22343;&#34913;&#29366;&#24577;&#19979;&#21019;&#24314;&#19981;&#21516;&#31867;&#22411;&#30340;&#20869;&#23481;&#12290;&#20351;&#29992;&#23545;&#20598;&#35770;&#35777;&#27861;
&lt;/p&gt;
&lt;p&gt;
Algorithmic recommender systems such as Spotify and Netflix affect not only consumer behavior but also producer incentives. Producers seek to create content that will be shown by the recommendation algorithm, which can impact both the diversity and quality of their content. In this work, we investigate the resulting supply-side equilibria in personalized content recommender systems. We model users and content as $D$-dimensional vectors, the recommendation algorithm as showing each user the content with highest dot product, and producers as maximizing the number of users who are recommended their content minus the cost of production. Two key features of our model are that the producer decision space is multi-dimensional and the user base is heterogeneous, which contrasts with classical low-dimensional models.  Multi-dimensionality and heterogeneity create the potential for specialization, where different producers create different types of content at equilibrium. Using a duality argumen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25239;&#24615;&#20195;&#29702;&#39118;&#38505;&#30340;&#23384;&#22312;&#24615;&#12289;&#27491;&#21017;&#24615;&#21644;&#26497;&#23567;&#21270;&#23450;&#29702;&#65292;&#36825;&#19968;&#32467;&#26524;&#20026;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#29702;&#35770;&#25552;&#20379;&#20102;&#25903;&#25345;&#65292;&#24182;&#19988;&#21487;&#20197;&#25351;&#23548;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2206.09098</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#20195;&#29702;&#39118;&#38505;&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#30340;&#23384;&#22312;&#24615;&#21644;&#26497;&#23567;&#21270;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Existence and Minimax Theorems for Adversarial Surrogate Risks in Binary Classification. (arXiv:2206.09098v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25239;&#24615;&#20195;&#29702;&#39118;&#38505;&#30340;&#23384;&#22312;&#24615;&#12289;&#27491;&#21017;&#24615;&#21644;&#26497;&#23567;&#21270;&#23450;&#29702;&#65292;&#36825;&#19968;&#32467;&#26524;&#20026;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#29702;&#35770;&#25552;&#20379;&#20102;&#25903;&#25345;&#65292;&#24182;&#19988;&#21487;&#20197;&#25351;&#23548;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#35757;&#32451;&#26159;&#35757;&#32451;&#40065;&#26834;&#24615;&#24378;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#28982;&#32780;&#65292;&#23427;&#20174;&#29702;&#35770;&#35282;&#24230;&#24182;&#19981;&#20026;&#20154;&#20204;&#25152;&#29087;&#30693;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25239;&#24615;&#20195;&#29702;&#39118;&#38505;&#30340;&#23384;&#22312;&#24615;&#12289;&#27491;&#21017;&#24615;&#21644;&#26497;&#23567;&#21270;&#23450;&#29702;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35299;&#37322;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#26377;&#20851;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#19968;&#20123;&#32463;&#39564;&#35266;&#23519;&#65292;&#24182;&#25552;&#20986;&#20102;&#31639;&#27861;&#21457;&#23637;&#30340;&#26032;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#23558;&#20043;&#21069;&#24050;&#30693;&#30340;&#23545;&#25239;&#20998;&#31867;&#39118;&#38505;&#30340;&#23384;&#22312;&#21644;&#26497;&#23567;&#21270;&#23450;&#29702;&#25193;&#23637;&#21040;&#20102;&#20195;&#29702;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training is one of the most popular methods for training methods robust to adversarial attacks, however, it is not well-understood from a theoretical perspective. We prove and existence, regularity, and minimax theorems for adversarial surrogate risks. Our results explain some empirical observations on adversarial robustness from prior work and suggest new directions in algorithm development. Furthermore, our results extend previously known existence and minimax theorems for the adversarial classification risk to surrogate risks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#8212;&#8212;Selectively Adaptive Lasso&#65288;SAL&#65289;&#65292;&#23427;&#22522;&#20110;HAL&#30340;&#29702;&#35770;&#26500;&#24314;&#65292;&#20445;&#30041;&#20102;&#26080;&#32500;&#24230;&#12289;&#38750;&#21442;&#25968;&#25910;&#25947;&#36895;&#29575;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#20063;&#20855;&#26377;&#21487;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#31639;&#27861;&#23558;&#35768;&#22810;&#22238;&#24402;&#31995;&#25968;&#33258;&#21160;&#35774;&#32622;&#20026;&#38646;&#12290;</title><link>http://arxiv.org/abs/2205.10697</link><description>&lt;p&gt;
Selectively Adaptive Lasso&#36873;&#36866;&#24212;Lasso
&lt;/p&gt;
&lt;p&gt;
The Selectively Adaptive Lasso. (arXiv:2205.10697v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#8212;&#8212;Selectively Adaptive Lasso&#65288;SAL&#65289;&#65292;&#23427;&#22522;&#20110;HAL&#30340;&#29702;&#35770;&#26500;&#24314;&#65292;&#20445;&#30041;&#20102;&#26080;&#32500;&#24230;&#12289;&#38750;&#21442;&#25968;&#25910;&#25947;&#36895;&#29575;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#20063;&#20855;&#26377;&#21487;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#31639;&#27861;&#23558;&#35768;&#22810;&#22238;&#24402;&#31995;&#25968;&#33258;&#21160;&#35774;&#32622;&#20026;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#26041;&#27861;&#33021;&#22815;&#36827;&#34892;&#26080;&#38656;&#36807;&#22810;&#30340;&#21442;&#25968;&#20551;&#35774;&#30340;&#20989;&#25968;&#20272;&#35745;&#12290;&#34429;&#28982;&#23427;&#20204;&#21487;&#20197;&#22312;&#39044;&#27979;&#35823;&#24046;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22823;&#22810;&#25968;&#32570;&#20047;&#31867;&#21322;&#21442;&#25968;&#26377;&#25928;&#20272;&#35745;&#65288;&#20363;&#22914;&#65292;TMLE&#65292;AIPW&#65289;&#25152;&#38656;&#30340;&#29702;&#35770;&#25910;&#25947;&#36895;&#24230;&#12290;&#39640;&#24230;&#33258;&#36866;&#24212;Lasso&#65288;HAL&#65289;&#26159;&#21807;&#19968;&#32463;&#35777;&#26126;&#33021;&#22815;&#24555;&#36895;&#25910;&#25947;&#21040;&#24847;&#20041;&#19978;&#30340;&#22823;&#31867;&#20989;&#25968;&#30340;&#22238;&#24402;&#26041;&#27861;&#65292;&#19982;&#39044;&#27979;&#21464;&#37327;&#30340;&#32500;&#24230;&#26080;&#20851;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;HAL&#26080;&#27861;&#25193;&#23637;&#35745;&#31639;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;HAL&#29702;&#35770;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#36873;&#25321;&#33258;&#36866;&#24212;Lasso&#65288;SAL&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20445;&#30041;HAL&#30340;&#26080;&#32500;&#24230;&#12289;&#38750;&#21442;&#25968;&#25910;&#25947;&#29575;&#65292;&#20294;&#20063;&#33021;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#30340;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20123;&#19982;&#23884;&#22871;Donsker&#31867;&#20013;&#30340;&#32463;&#39564;&#25439;&#22833;&#26368;&#23567;&#21270;&#26377;&#20851;&#30340;&#19968;&#33324;&#29702;&#35770;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#19968;&#31181;&#26799;&#24230;&#19979;&#38477;&#24418;&#24335;&#65292;&#20855;&#26377;&#31616;&#21333;&#30340;&#20998;&#32452;&#35268;&#21017;&#65292;&#33258;&#21160;&#23558;&#35768;&#22810;&#22238;&#24402;&#31995;&#25968;&#35774;&#20026;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning regression methods allow estimation of functions without unrealistic parametric assumptions. Although they can perform exceptionally in prediction error, most lack theoretical convergence rates necessary for semi-parametric efficient estimation (e.g. TMLE, AIPW) of parameters like average treatment effects. The Highly Adaptive Lasso (HAL) is the only regression method proven to converge quickly enough for a meaningfully large class of functions, independent of the dimensionality of the predictors. Unfortunately, HAL is not computationally scalable. In this paper we build upon the theory of HAL to construct the Selectively Adaptive Lasso (SAL), a new algorithm which retains HAL's dimension-free, nonparametric convergence rate but which also scales computationally to large high-dimensional datasets. To accomplish this, we prove some general theoretical results pertaining to empirical loss minimization in nested Donsker classes. Our resulting algorithm is a form of gradie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;HiFi++&#65292;&#29992;&#20110;&#24102;&#23485;&#25193;&#23637;&#21644;&#35821;&#38899;&#22686;&#24378;&#20219;&#21153;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#29983;&#25104;&#22120;&#26550;&#26500;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#28040;&#32791;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2203.13086</link><description>&lt;p&gt;
HiFi++&#65306;&#29992;&#20110;&#24102;&#23485;&#25193;&#23637;&#21644;&#35821;&#38899;&#22686;&#24378;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HiFi++: a Unified Framework for Bandwidth Extension and Speech Enhancement. (arXiv:2203.13086v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.13086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;HiFi++&#65292;&#29992;&#20110;&#24102;&#23485;&#25193;&#23637;&#21644;&#35821;&#38899;&#22686;&#24378;&#20219;&#21153;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#29983;&#25104;&#22120;&#26550;&#26500;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#28040;&#32791;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#22312;&#31070;&#32463;&#22768;&#30721;&#22120;&#20013;&#23637;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#26368;&#20339;&#30340;&#33258;&#22238;&#24402;&#21644;&#27969;&#27169;&#22411;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#25104;&#21151;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#26377;&#26465;&#20214;&#38899;&#39057;&#29983;&#25104;&#20219;&#21153;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312; HiFi &#22768;&#30721;&#22120;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; HiFi++ &#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#24102;&#23485;&#25193;&#23637;&#21644;&#35821;&#38899;&#22686;&#24378;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#25913;&#36827;&#30340;&#29983;&#25104;&#22120;&#26550;&#26500;&#65292;HiFi++ &#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#28040;&#32791;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks have recently demonstrated outstanding performance in neural vocoding outperforming best autoregressive and flow-based models. In this paper, we show that this success can be extended to other tasks of conditional audio generation. In particular, building upon HiFi vocoders, we propose a novel HiFi++ general framework for bandwidth extension and speech enhancement. We show that with the improved generator architecture, HiFi++ performs better or comparably with the state-of-the-art in these tasks while spending significantly less computational resources. The effectiveness of our approach is validated through a series of extensive experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20943;&#23569;&#27599;&#20010;&#32452;&#20998;&#25968;&#20998;&#24067;&#20043;&#38388;&#30340;&#32479;&#35745;&#36317;&#31163;&#65292;&#22312;&#20219;&#20309;&#20915;&#31574;&#38408;&#20540;&#19979;&#21516;&#26102;&#25552;&#39640;&#20998;&#31867;&#30340;&#20844;&#24179;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20339;&#36816;&#36755;&#30340;&#21518;&#22788;&#29702;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2203.07490</link><description>&lt;p&gt;
&#20219;&#20309;&#20915;&#31574;&#38408;&#20540;&#19979;&#20844;&#24179;&#20998;&#31867;&#30340;&#20960;&#20309;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Geometric Repair for Fair Classification at Any Decision Threshold. (arXiv:2203.07490v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20943;&#23569;&#27599;&#20010;&#32452;&#20998;&#25968;&#20998;&#24067;&#20043;&#38388;&#30340;&#32479;&#35745;&#36317;&#31163;&#65292;&#22312;&#20219;&#20309;&#20915;&#31574;&#38408;&#20540;&#19979;&#21516;&#26102;&#25552;&#39640;&#20998;&#31867;&#30340;&#20844;&#24179;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20339;&#36816;&#36755;&#30340;&#21518;&#22788;&#29702;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#30340;&#38382;&#39064;&#26159;&#21518;&#22788;&#29702;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#22120;&#20197;&#22312;&#25152;&#26377;&#20915;&#31574;&#38408;&#20540;&#19979;&#26368;&#22823;&#21270;&#20844;&#24179;&#30340;&#20108;&#20998;&#31867;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;&#20943;&#23569;&#27599;&#20010;&#32452;&#20998;&#25968;&#20998;&#24067;&#20043;&#38388;&#30340;&#32479;&#35745;&#36317;&#31163;&#65292;&#25105;&#20204;&#21487;&#20197;&#21516;&#26102;&#22686;&#21152;&#25152;&#26377;&#38408;&#20540;&#19978;&#30340;&#20844;&#24179;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#20570;&#21040;&#22312;&#19981;&#26174;&#30528;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#24418;&#24335;&#21270;&#30340;&#20998;&#24067;&#24179;&#31561;&#24230;&#37327;&#65292;&#23427;&#25429;&#33719;&#20102;&#19981;&#21516;&#20445;&#25252;&#32452;&#30340;&#20998;&#31867;&#20998;&#24067;&#30456;&#20284;&#31243;&#24230;&#12290;&#19982;&#20808;&#21069;&#21482;&#38024;&#23545;&#25152;&#26377;&#38408;&#20540;&#30340;&#20154;&#21475;&#32479;&#35745;&#24179;&#31561;&#24230;&#30740;&#31350;&#30340;&#30740;&#31350;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#36866;&#29992;&#20110;&#22823;&#31867;&#20844;&#24179;&#24615;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25104;&#26524;&#26159;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26368;&#20339;&#36816;&#36755;&#30340;&#21518;&#22788;&#29702;&#31639;&#27861;&#65292;&#21487;&#20197;&#35777;&#26126;&#23427;&#26368;&#22823;&#21270;&#20102;&#20998;&#24067;&#24179;&#31561;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#20844;&#24179;&#24615;&#22522;&#20934;&#27979;&#35797;&#19978;&#25903;&#25345;&#27492;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of post-processing a supervised machine-learned regressor to maximize fair binary classification at all decision thresholds. Specifically, we show that by decreasing the statistical distance between each group's score distributions, we can increase fair performance across all thresholds at once, and that we can do so without a significant decrease in accuracy. To this end, we introduce a formal measure of distributional parity, which captures the degree of similarity in the distributions of classifications for different protected groups. In contrast to prior work, which has been limited to studies of demographic parity across all thresholds, our measure applies to a large class of fairness metrics. Our main result is to put forward a novel post-processing algorithm based on optimal transport, which provably maximizes distributional parity. We support this result with experiments on several fairness benchmarks.
&lt;/p&gt;</description></item><item><title>Han&#23618;&#26159;&#19968;&#31181;&#26799;&#24230;&#31283;&#23450;&#12289;&#21442;&#25968;&#26356;&#23569;&#30340;&#31070;&#32463;&#23618;&#32467;&#26500;&#65292;&#21487;&#20197;&#26367;&#25442;&#20840;&#36830;&#25509;&#23618;&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2106.04088</link><description>&lt;p&gt;
&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#26799;&#24230;&#31283;&#23450;&#30340;&#31070;&#32463;&#23618;
&lt;/p&gt;
&lt;p&gt;
A Lightweight and Gradient-Stable Nerual Layer. (arXiv:2106.04088v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.04088
&lt;/p&gt;
&lt;p&gt;
Han&#23618;&#26159;&#19968;&#31181;&#26799;&#24230;&#31283;&#23450;&#12289;&#21442;&#25968;&#26356;&#23569;&#30340;&#31070;&#32463;&#23618;&#32467;&#26500;&#65292;&#21487;&#20197;&#26367;&#25442;&#20840;&#36830;&#25509;&#23618;&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Householder&#26435;&#37325;&#21644;&#32477;&#23545;&#20540;&#28608;&#27963;&#30340;&#31070;&#32463;&#23618;&#32467;&#26500;&#65292;&#22240;&#27492;&#34987;&#31216;&#20026;Householder-absolute&#31070;&#32463;&#23618;&#25110;&#31616;&#31216;Han&#23618;&#12290;&#19982;&#20855;&#26377;$d$&#20010;&#31070;&#32463;&#20803;&#21644;$d$&#20010;&#36755;&#20986;&#30340;&#20840;&#36830;&#25509;&#23618;&#30456;&#27604;&#65292;Han&#23618;&#23558;&#21442;&#25968;&#25968;&#37327;&#21644;&#30456;&#24212;&#30340;&#22797;&#26434;&#24230;&#20174;$O&#65288;d ^ 2&#65289;$&#38477;&#20302;&#21040;$O&#65288;d&#65289;$&#12290;Han&#23618;&#32467;&#26500;&#20445;&#35777;&#20102;&#20004;&#20010;&#29702;&#24819;&#23646;&#24615;&#65306;&#65288;1&#65289;&#26799;&#24230;&#31283;&#23450;&#24615;&#65288;&#19981;&#20250;&#20986;&#29616;&#26799;&#24230;&#28040;&#22833;&#25110;&#26799;&#24230;&#29190;&#28856;&#65289;&#65292;&#20197;&#21450;&#65288;2&#65289;1-Lipschitz&#36830;&#32493;&#24615;&#12290;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#21487;&#20197;&#26377;&#31574;&#30053;&#22320;&#20351;&#29992;Han&#23618;&#26367;&#25442;&#20840;&#36830;&#25509;&#65288;FC&#65289;&#23618;&#65292;&#20174;&#32780;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#29978;&#33267;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#23637;&#31034;Han&#23618;&#32467;&#26500;&#22312;&#19968;&#20123;&#23567;&#22411;&#21270;&#30340;&#27169;&#22411;&#19978;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#35752;&#35770;&#20854;&#24403;&#21069;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a neural-layer architecture based on Householder weighting and absolute-value activating, hence called Householder-absolute neural layer or simply Han-layer. Compared to a fully-connected layer with $d$-neurons and $d$ outputs, a Han-layer reduces the number of parameters and the corresponding complexity from $O(d^2)$ to $O(d)$. The Han-layer structure guarantees two desirable properties: (1) gradient stability (free of vanishing or exploding gradient), and (2) 1-Lipschitz continuity. Extensive numerical experiments show that one can strategically use Han-layers to replace fully-connected (FC) layers, reducing the number of model parameters while maintaining or even improving the generalization performance. We will showcase the capabilities of the Han-layer architecture on a few small stylized models, and also discuss its current limitations.
&lt;/p&gt;</description></item></channel></rss>