<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24067;&#26009;&#27169;&#25311;&#26041;&#27861; NeuralClothSim&#65292;&#20351;&#29992;&#34180;&#22771;&#29702;&#35770;&#21644;&#31070;&#32463;&#21464;&#24418;&#22330;&#36827;&#34892;&#34920;&#38754;&#28436;&#21270;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#24067;&#26009;&#27169;&#25311;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#25361;&#25112;&#65292;&#20026;&#29289;&#29702;&#21512;&#29702;&#30340;&#24067;&#26009;&#27169;&#25311;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2308.12970</link><description>&lt;p&gt;
NeuralClothSim: &#31070;&#32463;&#21464;&#24418;&#22330;&#19982;Kirchhoff-Love&#34180;&#22771;&#29702;&#35770;&#30456;&#36935;
&lt;/p&gt;
&lt;p&gt;
NeuralClothSim: Neural Deformation Fields Meet the Kirchhoff-Love Thin Shell Theory. (arXiv:2308.12970v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24067;&#26009;&#27169;&#25311;&#26041;&#27861; NeuralClothSim&#65292;&#20351;&#29992;&#34180;&#22771;&#29702;&#35770;&#21644;&#31070;&#32463;&#21464;&#24418;&#22330;&#36827;&#34892;&#34920;&#38754;&#28436;&#21270;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#24067;&#26009;&#27169;&#25311;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#25361;&#25112;&#65292;&#20026;&#29289;&#29702;&#21512;&#29702;&#30340;&#24067;&#26009;&#27169;&#25311;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#26009;&#27169;&#25311;&#26159;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#25991;&#29486;&#20013;&#26377;&#22823;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#24067;&#26009;&#27169;&#25311;&#22120;&#20135;&#29983;&#31526;&#21512;&#19981;&#21516;&#31867;&#22411;&#36793;&#30028;&#26465;&#20214;&#30340;&#36924;&#30495;&#24067;&#26009;&#21464;&#24418;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25805;&#20316;&#21407;&#29702;&#22312;&#20960;&#20010;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#65306;&#23427;&#20204;&#22312;&#20855;&#26377;&#22266;&#23450;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#26174;&#24335;&#34920;&#38754;&#34920;&#31034;&#19978;&#36827;&#34892;&#25805;&#20316;&#65292;&#25191;&#34892;&#19968;&#31995;&#21015;&#31163;&#25955;&#21270;&#30340;&#26356;&#26032;&#65288;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#65289;&#65292;&#24182;&#19988;&#38656;&#35201;&#30456;&#23545;&#36739;&#22823;&#30340;&#23384;&#20648;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#29616;&#26377;&#30340;&#27714;&#35299;&#22120;&#36827;&#34892;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#36890;&#24120;&#24182;&#19981;&#30452;&#35266;&#65292;&#36825;&#22312;&#23558;&#20854;&#38598;&#25104;&#21040;&#29616;&#20195;&#31070;&#32463;&#26550;&#26500;&#20013;&#26102;&#36896;&#25104;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#38024;&#23545;&#19978;&#36848;&#38480;&#21046;&#65292;&#26412;&#25991;&#20174;&#26681;&#26412;&#19978;&#20197;&#19968;&#31181;&#26681;&#26412;&#19981;&#21516;&#30340;&#35270;&#35282;&#26469;&#32771;&#34385;&#29289;&#29702;&#21512;&#29702;&#30340;&#24067;&#26009;&#27169;&#25311;&#65292;&#24182;&#37325;&#26032;&#24605;&#32771;&#36825;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;NeuralClothSim&#65292;&#21363;&#19968;&#31181;&#20351;&#29992;&#34180;&#22771;&#30340;&#26032;&#24067;&#26009;&#27169;&#25311;&#26041;&#27861;&#65292;&#20854;&#20013;&#34920;&#38754;&#28436;&#21270;&#36890;&#36807;&#31070;&#32463;&#21464;&#24418;&#22330;&#31561;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloth simulation is an extensively studied problem, with a plethora of solutions available in computer graphics literature. Existing cloth simulators produce realistic cloth deformations that obey different types of boundary conditions. Nevertheless, their operational principle remains limited in several ways: They operate on explicit surface representations with a fixed spatial resolution, perform a series of discretised updates (which bounds their temporal resolution), and require comparably large amounts of storage. Moreover, back-propagating gradients through the existing solvers is often not straightforward, which poses additional challenges when integrating them into modern neural architectures. In response to the limitations mentioned above, this paper takes a fundamentally different perspective on physically-plausible cloth simulation and re-thinks this long-standing problem: We propose NeuralClothSim, i.e., a new cloth simulation approach using thin shells, in which surface ev
&lt;/p&gt;</description></item><item><title>Scenimefy&#26159;&#19968;&#20010;&#36890;&#36807;&#21322;&#30417;&#30563;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#20174;&#29616;&#23454;&#19990;&#30028;&#22270;&#20687;&#20013;&#28210;&#26579;&#39640;&#36136;&#37327;&#30340;&#21160;&#28459;&#22330;&#26223;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#19968;&#33268;&#30340;&#35821;&#20041;&#12289;&#26126;&#26174;&#30340;&#39118;&#26684;&#21270;&#21644;&#31934;&#32454;&#30340;&#32454;&#33410;&#12290;</title><link>http://arxiv.org/abs/2308.12968</link><description>&lt;p&gt;
Scenimefy: &#36890;&#36807;&#21322;&#30417;&#30563;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#23398;&#20064;&#21046;&#20316;&#21160;&#28459;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation. (arXiv:2308.12968v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12968
&lt;/p&gt;
&lt;p&gt;
Scenimefy&#26159;&#19968;&#20010;&#36890;&#36807;&#21322;&#30417;&#30563;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#20174;&#29616;&#23454;&#19990;&#30028;&#22270;&#20687;&#20013;&#28210;&#26579;&#39640;&#36136;&#37327;&#30340;&#21160;&#28459;&#22330;&#26223;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#19968;&#33268;&#30340;&#35821;&#20041;&#12289;&#26126;&#26174;&#30340;&#39118;&#26684;&#21270;&#21644;&#31934;&#32454;&#30340;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#22270;&#20687;&#33258;&#21160;&#39640;&#36136;&#37327;&#22320;&#28210;&#26579;&#21160;&#28459;&#22330;&#26223;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#20215;&#20540;&#12290;&#36825;&#19968;&#20219;&#21153;&#30340;&#25361;&#25112;&#22312;&#20110;&#22330;&#26223;&#30340;&#22797;&#26434;&#24615;&#12289;&#21160;&#28459;&#39118;&#26684;&#30340;&#29420;&#29305;&#29305;&#28857;&#20197;&#21450;&#32570;&#20047;&#29992;&#20110;&#22635;&#34917;&#39046;&#22495;&#24046;&#36317;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#21162;&#21147;&#26377;&#25152;&#36827;&#23637;&#65292;&#20294;&#26159;&#20173;&#28982;&#19981;&#33021;&#28385;&#24847;&#22320;&#20445;&#25345;&#19968;&#33268;&#30340;&#35821;&#20041;&#20445;&#23384;&#12289;&#26126;&#26174;&#30340;&#39118;&#26684;&#21270;&#21644;&#31934;&#32454;&#30340;&#32454;&#33410;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Scenimefy&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20855;&#26377;&#32467;&#26500;&#19968;&#33268;&#24615;&#30340;&#20266;&#37197;&#23545;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#65292;&#31616;&#21270;&#20102;&#32431;&#26080;&#30417;&#30563;&#35774;&#32622;&#12290;&#20266;&#25968;&#25454;&#26159;&#36890;&#36807;&#35821;&#20041;&#32422;&#26463;&#30340;StyleGAN&#21807;&#19968;&#22320;&#23548;&#20986;&#30340;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#20687;CLIP&#36825;&#26679;&#30340;&#20016;&#23500;&#27169;&#22411;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24212;&#29992;&#20998;&#21106;&#24341;&#23548;&#30340;&#25968;&#25454;&#36873;&#25321;&#26469;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#20266;&#30417;&#30563;&#12290;&#24341;&#20837;&#20102;&#22522;&#20110;&#34917;&#19969;&#30340;&#23545;&#27604;&#39118;&#26684;&#25439;&#22833;&#26469;&#25913;&#21892;&#39118;&#26684;&#21270;&#21644;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic high-quality rendering of anime scenes from complex real-world images is of significant practical value. The challenges of this task lie in the complexity of the scenes, the unique features of anime style, and the lack of high-quality datasets to bridge the domain gap. Despite promising attempts, previous efforts are still incompetent in achieving satisfactory results with consistent semantic preservation, evident stylization, and fine details. In this study, we propose Scenimefy, a novel semi-supervised image-to-image translation framework that addresses these challenges. Our approach guides the learning with structure-consistent pseudo paired data, simplifying the pure unsupervised setting. The pseudo data are derived uniquely from a semantic-constrained StyleGAN leveraging rich model priors like CLIP. We further apply segmentation-guided data selection to obtain high-quality pseudo supervision. A patch-wise contrastive style loss is introduced to improve stylization and fi
&lt;/p&gt;</description></item><item><title>NeO 360&#26159;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#35270;&#35282;&#21512;&#25104;&#25143;&#22806;&#22330;&#26223;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20998;&#24067;&#65292;&#20351;&#29992;&#28151;&#21512;&#30340;&#22270;&#20687;&#26465;&#20214;&#24335;&#19977;&#24179;&#38754;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#20174;&#21333;&#20010;&#25110;&#23569;&#25968;&#20960;&#20010;&#22270;&#20687;&#37325;&#24314;360&#24230;&#22330;&#26223;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12967</link><description>&lt;p&gt;
NeO 360: &#29992;&#20110;&#31232;&#30095;&#35270;&#35282;&#21512;&#25104;&#25143;&#22806;&#22330;&#26223;&#30340;&#31070;&#32463;&#22330;
&lt;/p&gt;
&lt;p&gt;
NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes. (arXiv:2308.12967v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12967
&lt;/p&gt;
&lt;p&gt;
NeO 360&#26159;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#35270;&#35282;&#21512;&#25104;&#25143;&#22806;&#22330;&#26223;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20998;&#24067;&#65292;&#20351;&#29992;&#28151;&#21512;&#30340;&#22270;&#20687;&#26465;&#20214;&#24335;&#19977;&#24179;&#38754;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#20174;&#21333;&#20010;&#25110;&#23569;&#25968;&#20960;&#20010;&#22270;&#20687;&#37325;&#24314;360&#24230;&#22330;&#26223;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#26041;&#27861;&#22312;&#26032;&#35270;&#35282;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#22312;&#35768;&#22810;&#35270;&#35282;&#19979;&#36827;&#34892;&#26114;&#36149;&#30340;&#22330;&#26223;&#20248;&#21270;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#26080;&#38480;&#21046;&#30340;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#25110;&#32972;&#26223;&#21482;&#20174;&#24456;&#23569;&#30340;&#35270;&#35282;&#35266;&#23519;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;NeO 360&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#31232;&#30095;&#35270;&#35282;&#21512;&#25104;&#25143;&#22806;&#22330;&#26223;&#12290;NeO 360&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#25110;&#23569;&#25968;&#20960;&#20010;&#20301;&#32622;&#30340;RGB&#22270;&#20687;&#37325;&#24314;360&#24230;&#22330;&#26223;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#22312;&#20110;&#25429;&#33719;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#25143;&#22806;&#19977;&#32500;&#22330;&#26223;&#30340;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#28151;&#21512;&#30340;&#22270;&#20687;&#26465;&#20214;&#24335;&#19977;&#24179;&#38754;&#34920;&#31034;&#65292;&#21487;&#20197;&#20174;&#20219;&#20309;&#19990;&#30028;&#28857;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#34920;&#31034;&#26041;&#27861;&#32467;&#21512;&#20102;&#20307;&#32032;&#21644;&#40479;&#30640;&#22270;&#34920;&#31034;&#30340;&#20248;&#28857;&#65292;&#27604;&#27599;&#31181;&#34920;&#31034;&#26041;&#27861;&#26356;&#26377;&#25928;&#21644;&#34920;&#36798;&#33021;&#21147;&#26356;&#24378;&#12290;NeO 360&#30340;&#34920;&#31034;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#20174;&#19968;&#20010;&#24222;&#22823;&#30340;&#26080;&#38480;&#21046;&#30340;&#19977;&#32500;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent implicit neural representations have shown great results for novel view synthesis. However, existing methods require expensive per-scene optimization from many views hence limiting their application to real-world unbounded urban settings where the objects of interest or backgrounds are observed from very few views. To mitigate this challenge, we introduce a new approach called NeO 360, Neural fields for sparse view synthesis of outdoor scenes. NeO 360 is a generalizable method that reconstructs 360{\deg} scenes from a single or a few posed RGB images. The essence of our approach is in capturing the distribution of complex real-world outdoor 3D scenes and using a hybrid image-conditional triplanar representation that can be queried from any world point. Our representation combines the best of both voxel-based and bird's-eye-view (BEV) representations and is more effective and expressive than each. NeO 360's representation allows us to learn from a large collection of unbounded 3D
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DenseDiffusion&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20351;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22788;&#29702;&#31264;&#23494;&#26631;&#39064;&#24182;&#20855;&#26377;&#22330;&#26223;&#24067;&#23616;&#25511;&#21046;&#65292;&#26080;&#38656;&#39069;&#22806;&#24494;&#35843;&#25110;&#25968;&#25454;&#38598;&#65292;&#25552;&#21319;&#20102;&#22270;&#20687;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12964</link><description>&lt;p&gt;
&#24102;&#26377;&#27880;&#24847;&#21147;&#35843;&#33410;&#30340;&#31264;&#23494;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Dense Text-to-Image Generation with Attention Modulation. (arXiv:2308.12964v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DenseDiffusion&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20351;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22788;&#29702;&#31264;&#23494;&#26631;&#39064;&#24182;&#20855;&#26377;&#22330;&#26223;&#24067;&#23616;&#25511;&#21046;&#65292;&#26080;&#38656;&#39069;&#22806;&#24494;&#35843;&#25110;&#25968;&#25454;&#38598;&#65292;&#25552;&#21319;&#20102;&#22270;&#20687;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#32473;&#23450;&#31264;&#23494;&#26631;&#39064;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#21512;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#20854;&#20013;&#27599;&#20010;&#25991;&#26412;&#25552;&#31034;&#20026;&#29305;&#23450;&#22270;&#20687;&#21306;&#22495;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#25551;&#36848;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DenseDiffusion&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#20351;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#36825;&#20123;&#31264;&#23494;&#26631;&#39064;&#65292;&#24182;&#21516;&#26102;&#25552;&#20379;&#23545;&#22330;&#26223;&#24067;&#23616;&#30340;&#25511;&#21046;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#29983;&#25104;&#22270;&#20687;&#24067;&#23616;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20013;&#38388;&#27880;&#24847;&#21147;&#22270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#35843;&#33410;&#26041;&#27861;&#65292;&#26681;&#25454;&#24067;&#23616;&#25351;&#23548;&#23558;&#23545;&#35937;&#24341;&#23548;&#21040;&#29305;&#23450;&#21306;&#22495;&#26174;&#31034;&#12290;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#24494;&#35843;&#25110;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#20998;&#25968;&#26041;&#38754;&#25913;&#36827;&#20102;&#32473;&#23450;&#31264;&#23494;&#26631;&#39064;&#30340;&#22270;&#20687;&#29983;&#25104;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;&#19987;&#38376;&#20351;&#29992;&#24067;&#23616;&#26465;&#20214;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#20284;&#36136;&#37327;&#30340;&#35270;&#35273;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing text-to-image diffusion models struggle to synthesize realistic images given dense captions, where each text prompt provides a detailed description for a specific image region. To address this, we propose DenseDiffusion, a training-free method that adapts a pre-trained text-to-image model to handle such dense captions while offering control over the scene layout. We first analyze the relationship between generated images' layouts and the pre-trained model's intermediate attention maps. Next, we develop an attention modulation method that guides objects to appear in specific regions according to layout guidance. Without requiring additional fine-tuning or datasets, we improve image generation performance given dense captions regarding both automatic and human evaluation scores. In addition, we achieve similar-quality visual results with models specifically trained with layout conditions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DLIP&#65292;&#25552;&#21462;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#27169;&#22359;&#30340;&#26550;&#26500;&#29305;&#24615;&#21644;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#20256;&#36882;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#22914;&#20309;&#25552;&#21462;&#36731;&#37327;&#32423;&#20294;&#24615;&#33021;&#20248;&#36234;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;DLIP&#33021;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.12956</link><description>&lt;p&gt;
DLIP: &#25552;&#21462;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DLIP: Distilling Language-Image Pre-training. (arXiv:2308.12956v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DLIP&#65292;&#25552;&#21462;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#27169;&#22359;&#30340;&#26550;&#26500;&#29305;&#24615;&#21644;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#20256;&#36882;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#22914;&#20309;&#25552;&#21462;&#36731;&#37327;&#32423;&#20294;&#24615;&#33021;&#20248;&#36234;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;DLIP&#33021;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451; (VLP) &#22312;&#26497;&#37325;&#30340;&#21442;&#25968;&#36741;&#21161;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36825;&#20063;&#32473;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#30693;&#35782;&#25552;&#21462;&#20316;&#20026;&#27169;&#22411;&#21387;&#32553;&#20013;&#30340;&#37325;&#35201;&#36807;&#31243;&#34987;&#24191;&#27867;&#35748;&#21487;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30693;&#35782;&#25552;&#21462;&#25216;&#26415;&#23545;&#20110;VLP&#30340;&#28145;&#20837;&#35843;&#26597;&#21644;&#20998;&#26512;&#36824;&#19981;&#22815;&#65292;&#24182;&#19988;VLP&#23548;&#21521;&#30340;&#25552;&#21462;&#23454;&#36341;&#25351;&#21335;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DLIP&#65292;&#19968;&#20010;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#25552;&#21462;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#23427;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#25552;&#21462;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;VLP&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20174;&#22810;&#20010;&#32500;&#24230;&#21078;&#26512;&#20102;&#27169;&#22411;&#25552;&#21462;&#65292;&#22914;&#19981;&#21516;&#27169;&#22359;&#30340;&#26550;&#26500;&#29305;&#24615;&#21644;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#20256;&#36882;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#24182;&#23545;&#22914;&#20309;&#25552;&#21462;&#36731;&#37327;&#32423;&#20294;&#24615;&#33021;&#20248;&#36234;&#30340;VLP&#27169;&#22411;&#25552;&#20379;&#20102;&#28145;&#21051;&#30340;&#35265;&#35299;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DLIP&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#26435;&#34913;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Pre-training (VLP) shows remarkable progress with the assistance of extremely heavy parameters, which challenges deployment in real applications. Knowledge distillation is well recognized as the essential procedure in model compression. However, existing knowledge distillation techniques lack an in-depth investigation and analysis of VLP, and practical guidelines for VLP-oriented distillation are still not yet explored. In this paper, we present DLIP, a simple yet efficient Distilling Language-Image Pre-training framework, through which we investigate how to distill a light VLP model. Specifically, we dissect the model distillation from multiple dimensions, such as the architecture characteristics of different modules and the information transfer of different modalities. We conduct comprehensive experiments and provide insights on distilling a light but performant VLP model. Experimental results reveal that DLIP can achieve a state-of-the-art accuracy/efficiency trade-o
&lt;/p&gt;</description></item><item><title>BridgeData V2&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#34892;&#20026;&#25968;&#25454;&#38598;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20064;&#30740;&#31350;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#22791;&#20219;&#21153;&#21644;&#29615;&#22659;&#21464;&#24322;&#24615;&#65292;&#24182;&#19988;&#20860;&#23481;&#22810;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#27492;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12952</link><description>&lt;p&gt;
BridgeData V2:&#19968;&#20010;&#29992;&#20110;&#35268;&#27169;&#21270;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BridgeData V2: A Dataset for Robot Learning at Scale. (arXiv:2308.12952v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12952
&lt;/p&gt;
&lt;p&gt;
BridgeData V2&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#34892;&#20026;&#25968;&#25454;&#38598;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20064;&#30740;&#31350;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#22791;&#20219;&#21153;&#21644;&#29615;&#22659;&#21464;&#24322;&#24615;&#65292;&#24182;&#19988;&#20860;&#23481;&#22810;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#27492;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;BridgeData V2&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#34892;&#20026;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20419;&#36827;&#35268;&#27169;&#21270;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#30740;&#31350;&#12290;BridgeData V2&#21253;&#21547;&#20102;&#22312;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#19988;&#25104;&#26412;&#36739;&#20302;&#30340;&#26426;&#22120;&#20154;&#19978;&#25910;&#38598;&#30340;60,096&#20010;&#36712;&#36857;&#65292;&#35206;&#30422;&#20102;24&#20010;&#29615;&#22659;&#12290;BridgeData V2&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#20219;&#21153;&#21644;&#29615;&#22659;&#21464;&#24322;&#24615;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#12289;&#39046;&#22495;&#21644;&#26426;&#26500;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#30340;&#25216;&#33021;&#65292;&#20351;&#24471;&#35813;&#25968;&#25454;&#38598;&#25104;&#20026;&#24191;&#22823;&#30740;&#31350;&#20154;&#21592;&#30340;&#26377;&#29992;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#35813;&#25968;&#25454;&#38598;&#19982;&#22810;&#31181;&#24320;&#25918;&#35789;&#27719;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#20197;&#30446;&#26631;&#22270;&#20687;&#25110;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20026;&#26465;&#20214;&#26159;&#20860;&#23481;&#30340;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;6&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#19968;&#31995;&#21015;&#38656;&#35201;&#19981;&#21516;&#27867;&#21270;&#31243;&#24230;&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#38543;&#30528;&#26356;&#22810;&#30340;&#25968;&#25454;&#21644;&#26356;&#39640;&#23481;&#37327;&#30340;&#27169;&#22411;&#32780;&#25913;&#21892;&#65292;&#24182;&#19988;&#36890;&#36807;&#35757;&#32451;&#38598;&#22823;&#23567;&#30340;&#22686;&#21152;&#21644;&#27169;&#22411;&#23481;&#37327;&#30340;&#22686;&#21152;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce BridgeData V2, a large and diverse dataset of robotic manipulation behaviors designed to facilitate research on scalable robot learning. BridgeData V2 contains 60,096 trajectories collected across 24 environments on a publicly available low-cost robot. BridgeData V2 provides extensive task and environment variability, leading to skills that can generalize across environments, domains, and institutions, making the dataset a useful resource for a broad range of researchers. Additionally, the dataset is compatible with a wide variety of open-vocabulary, multi-task learning methods conditioned on goal images or natural language instructions. In our experiments, we train 6 state-of-the-art imitation learning and offline reinforcement learning methods on our dataset, and find that they succeed on a suite of tasks requiring varying amounts of generalization. We also demonstrate that the performance of these methods improves with more data and higher capacity models, and that trai
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#25552;&#20986;&#24182;&#23450;&#20041;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#39044;&#31639;&#20998;&#37197;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#33258;&#36866;&#24212;&#39044;&#31639;&#20998;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#21644;&#26368;&#22823;&#21270;&#26032;&#20449;&#24687;&#30340;&#31243;&#24230;&#26469;&#25552;&#39640;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12949</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#39044;&#31639;&#20998;&#37197;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Label Budget Allocation in Multi-Task Learning. (arXiv:2308.12949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12949
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#25552;&#20986;&#24182;&#23450;&#20041;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#39044;&#31639;&#20998;&#37197;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#33258;&#36866;&#24212;&#39044;&#31639;&#20998;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#21644;&#26368;&#22823;&#21270;&#26032;&#20449;&#24687;&#30340;&#31243;&#24230;&#26469;&#25552;&#39640;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26631;&#35760;&#25104;&#26412;&#24120;&#24120;&#38480;&#21046;&#20102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#65292;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#30456;&#20114;&#25552;&#20379;&#20449;&#24687;&#24182;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#65292;&#20294;&#26159;&#26631;&#31614;&#25104;&#26412;&#21487;&#33021;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#21464;&#21270;&#12290;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#22914;&#20309;&#20998;&#37197;&#26631;&#31614;&#39044;&#31639;&#65288;&#21363;&#22312;&#26631;&#35760;&#19978;&#25237;&#20837;&#30340;&#37329;&#39069;&#65289;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#22810;&#20219;&#21153;&#24615;&#33021;? &#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#24182;&#27491;&#24335;&#23450;&#20041;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#39044;&#31639;&#20998;&#37197;&#38382;&#39064;&#65292;&#24182;&#23454;&#35777;&#34920;&#26126;&#19981;&#21516;&#30340;&#39044;&#31639;&#20998;&#37197;&#31574;&#30053;&#23545;&#20854;&#24615;&#33021;&#26377;&#24456;&#22823;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#20219;&#21153;&#33258;&#36866;&#24212;&#39044;&#31639;&#20998;&#37197;&#31639;&#27861;&#65292;&#20197;&#40065;&#26834;&#22320;&#29983;&#25104;&#36866;&#24212;&#19981;&#21516;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#30340;&#26368;&#20339;&#39044;&#31639;&#20998;&#37197;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#20272;&#35745;&#21644;&#26368;&#22823;&#21270;&#20998;&#37197;&#39044;&#31639;&#25152;&#33719;&#24471;&#30340;&#26032;&#20449;&#24687;&#30340;&#31243;&#24230;&#20316;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#24615;&#33021;&#30340;&#20195;&#29702;&#12290;&#22312;PASCAL VOC&#21644;Taskonomy&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cost of labeling data often limits the performance of machine learning systems. In multi-task learning, related tasks provide information to each other and improve overall performance, but the label cost can vary among tasks. How should the label budget (i.e. the amount of money spent on labeling) be allocated among different tasks to achieve optimal multi-task performance? We are the first to propose and formally define the label budget allocation problem in multi-task learning and to empirically show that different budget allocation strategies make a big difference to its performance. We propose a Task-Adaptive Budget Allocation algorithm to robustly generate the optimal budget allocation adaptive to different multi-task learning settings. Specifically, we estimate and then maximize the extent of new information obtained from the allocated budget as a proxy for multi-task learning performance. Experiments on PASCAL VOC and Taskonomy demonstrate the efficacy of our approach over o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#29289;&#29702;&#30340;&#31070;&#32463;&#31639;&#23376;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#20960;&#20309;&#21442;&#25968;&#19979;&#35299;&#20915;&#26080;&#26631;&#35760;&#25968;&#25454;&#30340;&#36793;&#30028;&#20540;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;PDE&#37325;&#26032;&#34920;&#36848;&#20026;BIEs&#65292;&#24182;&#20165;&#22312;&#22495;&#30340;&#36793;&#30028;&#19978;&#35757;&#32451;&#31639;&#23376;&#32593;&#32476;&#65292;&#26174;&#33879;&#21152;&#36895;&#20102;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#26080;&#30028;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12939</link><description>&lt;p&gt;
&#22312;&#36793;&#30028;&#23398;&#20064;&#65306;&#19968;&#31181;&#38754;&#21521;&#29289;&#29702;&#30340;&#31070;&#32463;&#31639;&#23376;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#20960;&#20309;&#21442;&#25968;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning Only On Boundaries: a Physics-Informed Neural operator for Solving Parametric Partial Differential Equations in Complex Geometries. (arXiv:2308.12939v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12939
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#29289;&#29702;&#30340;&#31070;&#32463;&#31639;&#23376;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#20960;&#20309;&#21442;&#25968;&#19979;&#35299;&#20915;&#26080;&#26631;&#35760;&#25968;&#25454;&#30340;&#36793;&#30028;&#20540;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;PDE&#37325;&#26032;&#34920;&#36848;&#20026;BIEs&#65292;&#24182;&#20165;&#22312;&#22495;&#30340;&#36793;&#30028;&#19978;&#35757;&#32451;&#31639;&#23376;&#32593;&#32476;&#65292;&#26174;&#33879;&#21152;&#36895;&#20102;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#26080;&#30028;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#26367;&#20195;&#26041;&#26696;&#21644;&#31070;&#32463;&#31639;&#23376;&#22312;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#21482;&#38480;&#20110;&#26377;&#30028;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#29289;&#29702;&#30340;&#31070;&#32463;&#31639;&#23376;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#26631;&#35760;&#25968;&#25454;&#30340;&#21442;&#25968;&#21270;&#36793;&#30028;&#20540;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;PDE&#30340;&#37325;&#26032;&#34920;&#36848;&#20026;&#36793;&#30028;&#31215;&#20998;&#26041;&#31243;(BIEs)&#65292;&#25105;&#20204;&#21487;&#20197;&#20165;&#22312;&#22495;&#30340;&#36793;&#30028;&#19978;&#35757;&#32451;&#31639;&#23376;&#32593;&#32476;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#25152;&#38656;&#26679;&#26412;&#28857;&#30340;&#25968;&#37327;&#20174;$O(N^d)$&#20943;&#23569;&#21040;$O(N^{d-1})$&#65292;&#20854;&#20013;$d$&#20026;&#22495;&#30340;&#32500;&#25968;&#65292;&#20174;&#32780;&#26174;&#33879;&#21152;&#36895;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#26080;&#30028;&#38382;&#39064;&#65292;&#23545;&#20110;&#29616;&#26377;&#30340;&#38754;&#21521;&#29289;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#21644;&#31070;&#32463;&#31639;&#23376;&#26469;&#35828;&#26159;&#26080;&#27861;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#21442;&#25968;&#21270;&#22797;&#26434;&#20960;&#20309;&#20307;&#21644;&#26080;&#30028;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently deep learning surrogates and neural operators have shown promise in solving partial differential equations (PDEs). However, they often require a large amount of training data and are limited to bounded domains. In this work, we present a novel physics-informed neural operator method to solve parametrized boundary value problems without labeled data. By reformulating the PDEs into boundary integral equations (BIEs), we can train the operator network solely on the boundary of the domain. This approach reduces the number of required sample points from $O(N^d)$ to $O(N^{d-1})$, where $d$ is the domain's dimension, leading to a significant acceleration of the training process. Additionally, our method can handle unbounded problems, which are unattainable for existing physics-informed neural networks (PINNs) and neural operators. Our numerical experiments show the effectiveness of parametrized complex geometries and unbounded problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#20302;&#35745;&#25968;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#29983;&#25104;&#36807;&#31243;&#26469;&#21019;&#24314;&#21253;&#21547;&#24322;&#24120;&#29255;&#27573;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#35299;&#37322;&#20102;&#24120;&#29992;&#31639;&#27861;&#22312;&#27491;&#24120;&#21644;&#24322;&#24120;&#29255;&#27573;&#20043;&#38388;&#30340;&#20998;&#24067;&#37325;&#21472;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12925</link><description>&lt;p&gt;
&#20302;&#35745;&#25968;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Low-count Time Series Anomaly Detection. (arXiv:2308.12925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#20302;&#35745;&#25968;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#29983;&#25104;&#36807;&#31243;&#26469;&#21019;&#24314;&#21253;&#21547;&#24322;&#24120;&#29255;&#27573;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#35299;&#37322;&#20102;&#24120;&#29992;&#31639;&#27861;&#22312;&#27491;&#24120;&#21644;&#24322;&#24120;&#29255;&#27573;&#20043;&#38388;&#30340;&#20998;&#24067;&#37325;&#21472;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#35745;&#25968;&#26102;&#38388;&#24207;&#21015;&#25551;&#36848;&#31232;&#30095;&#25110;&#38388;&#26029;&#20107;&#20214;&#65292;&#36825;&#22312;&#25429;&#33719;&#21644;&#30417;&#25511;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#30340;&#22823;&#35268;&#27169;&#22312;&#32447;&#24179;&#21488;&#20013;&#24456;&#24120;&#35265;&#12290;&#24314;&#27169;&#20302;&#35745;&#25968;&#26102;&#38388;&#24207;&#21015;&#38754;&#20020;&#20960;&#20010;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#20302;&#20449;&#22122;&#27604;&#65288;&#24403;&#24322;&#24120;&#31614;&#21517;&#26080;&#27861;&#26816;&#27979;&#26102;&#65289;&#21644;&#38750;&#22343;&#21248;&#24615;&#33021;&#65288;&#24179;&#22343;&#24230;&#37327;&#25351;&#26631;&#19981;&#33021;&#20195;&#34920;&#23616;&#37096;&#34892;&#20026;&#65289;&#12290;&#24403;&#21069;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#32570;&#20047;&#26126;&#30830;&#30340;&#24037;&#20855;&#21644;&#27969;&#31243;&#26469;&#24314;&#27169;&#21644;&#21487;&#38752;&#22320;&#26816;&#27979;&#36825;&#20123;&#24773;&#20917;&#19979;&#30340;&#24322;&#24120;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#29992;&#20110;&#21019;&#24314;&#21253;&#21547;&#26377;&#24322;&#24120;&#29255;&#27573;&#30340;&#20302;&#35745;&#25968;&#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#30340;&#28151;&#21512;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#35299;&#37322;&#20102;&#24120;&#29992;&#31639;&#27861;&#22312;&#27491;&#24120;&#21644;&#24322;&#24120;&#29255;&#27573;&#20043;&#38388;&#30340;&#20998;&#24067;&#37325;&#21472;&#20013;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#21457;&#29616;&#26469;&#23637;&#31034;&#22914;&#20309;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-count time series describe sparse or intermittent events, which are prevalent in large-scale online platforms that capture and monitor diverse data types. Several distinct challenges surface when modelling low-count time series, particularly low signal-to-noise ratios (when anomaly signatures are provably undetectable), and non-uniform performance (when average metrics are not representative of local behaviour). The time series anomaly detection community currently lacks explicit tooling and processes to model and reliably detect anomalies in these settings. We address this gap by introducing a novel generative procedure for creating benchmark datasets comprising of low-count time series with anomalous segments. Via a mixture of theoretical and empirical analysis, our work explains how widely-used algorithms struggle with the distribution overlap between normal and anomalous segments. In order to mitigate this shortcoming, we then leverage our findings to demonstrate how anomaly sc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;OptiChat&#65292;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#35786;&#26029;&#19981;&#21487;&#34892;&#30340;&#20248;&#21270;&#27169;&#22411;&#12290;&#23427;&#21487;&#20197;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#29702;&#35299;&#21644;&#35299;&#37322;&#19981;&#21487;&#34892;&#30340;&#20248;&#21270;&#27169;&#22411;&#65292;&#26080;&#38656;&#20855;&#22791;&#28145;&#21402;&#30340;&#20248;&#21270;&#32972;&#26223;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2308.12923</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35786;&#26029;&#19981;&#21487;&#34892;&#30340;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Diagnosing Infeasible Optimization Problems Using Large Language Models. (arXiv:2308.12923v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;OptiChat&#65292;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#35786;&#26029;&#19981;&#21487;&#34892;&#30340;&#20248;&#21270;&#27169;&#22411;&#12290;&#23427;&#21487;&#20197;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#29702;&#35299;&#21644;&#35299;&#37322;&#19981;&#21487;&#34892;&#30340;&#20248;&#21270;&#27169;&#22411;&#65292;&#26080;&#38656;&#20855;&#22791;&#28145;&#21402;&#30340;&#20248;&#21270;&#32972;&#26223;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#38382;&#39064;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#25968;&#23398;&#20248;&#21270;&#27169;&#22411;&#65292;&#22312;&#32463;&#27982;&#23398;&#12289;&#24037;&#31243;&#23398;&#19982;&#21046;&#36896;&#19994;&#12289;&#20132;&#36890;&#36816;&#36755;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#20248;&#21270;&#27169;&#22411;&#26159;&#22312;&#28385;&#36275;&#19968;&#32452;&#35201;&#27714;&#25110;&#32422;&#26463;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#20570;&#20986;&#26368;&#20339;&#20915;&#31574;&#30340;&#25968;&#23398;&#25277;&#35937;&#12290;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#30340;&#20027;&#35201;&#38556;&#30861;&#20043;&#19968;&#26159;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#29702;&#35299;&#21644;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#20204;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#27809;&#26377;&#20915;&#31574;&#28385;&#36275;&#25152;&#26377;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#29616;&#26377;&#30340;&#35786;&#26029;&#19981;&#21487;&#34892;&#20248;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#19987;&#23478;&#31995;&#32479;&#65292;&#38656;&#35201;&#22312;&#20248;&#21270;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#30340;&#32972;&#26223;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;OptiChat&#65292;&#36825;&#26159;&#19968;&#31181;&#39318;&#21019;&#30340;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#31995;&#32479;&#65292;&#37197;&#22791;&#20102;&#19968;&#20010;&#19982;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#20132;&#20114;&#24335;&#23545;&#35805;&#30340;GUI&#65292;&#29992;&#20110;&#35752;&#35770;&#19981;&#21487;&#34892;&#30340;&#20248;&#21270;&#27169;&#22411;&#12290;OptiChat&#21487;&#20197;&#25552;&#20379;&#23545;&#20248;&#21270;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-making problems can be represented as mathematical optimization models, finding wide applications in fields such as economics, engineering and manufacturing, transportation, and health care. Optimization models are mathematical abstractions of the problem of making the best decision while satisfying a set of requirements or constraints. One of the primary barriers to deploying these models in practice is the challenge of helping practitioners understand and interpret such models, particularly when they are infeasible, meaning no decision satisfies all the constraints. Existing methods for diagnosing infeasible optimization models often rely on expert systems, necessitating significant background knowledge in optimization. In this paper, we introduce OptiChat, a first-of-its-kind natural language-based system equipped with a chatbot GUI for engaging in interactive conversations about infeasible optimization models. OptiChat can provide natural language descriptions of the optim
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;EV&#20805;&#30005;&#25511;&#21046;&#26694;&#26550;&#65292;&#36890;&#36807;&#20445;&#25252;&#38544;&#31169;&#21644;&#20943;&#23569;&#32593;&#32476;&#25104;&#26412;&#26469;&#25913;&#21892;&#20805;&#30005;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12921</link><description>&lt;p&gt;
&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#32593;&#32476;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
An Efficient Distributed Multi-Agent Reinforcement Learning for EV Charging Network Control. (arXiv:2308.12921v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12921
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;EV&#20805;&#30005;&#25511;&#21046;&#26694;&#26550;&#65292;&#36890;&#36807;&#20445;&#25252;&#38544;&#31169;&#21644;&#20943;&#23569;&#32593;&#32476;&#25104;&#26412;&#26469;&#25913;&#21892;&#20805;&#30005;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21160;&#27773;&#36710;&#65288;EV&#65289;&#30340;&#26222;&#21450;&#36235;&#21183;&#23558;&#26174;&#33879;&#24433;&#21709;&#20303;&#23429;&#30005;&#21147;&#38656;&#27714;&#65292;&#23548;&#33268;&#37197;&#30005;&#32593;&#21464;&#21387;&#22120;&#36229;&#36127;&#33655;&#39118;&#38505;&#22686;&#21152;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#39118;&#38505;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#26377;&#25928;&#30340;EV&#20805;&#30005;&#25511;&#21046;&#22120;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;EV&#20805;&#30005;&#25511;&#21046;&#22120;&#22522;&#20110;&#38598;&#20013;&#24335;&#26041;&#27861;&#31649;&#29702;&#21333;&#20010;EV&#25110;&#19968;&#32452;EV&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20805;&#30005;&#26694;&#26550;&#65292;&#20248;&#20808;&#20445;&#25252;EV&#36710;&#20027;&#30340;&#38544;&#31169;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#38598;&#20013;&#24335;&#35757;&#32451;&#20998;&#25955;&#24335;&#25191;&#34892;-&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;CTDE-DDPG&#65289;&#27169;&#24335;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21521;&#29992;&#25143;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#20445;&#25252;&#38544;&#31169;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;CTDE&#26694;&#26550;&#36890;&#36807;&#20943;&#23569;&#32593;&#32476;&#25104;&#26412;&#25552;&#39640;&#20102;&#20805;&#30005;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23792;&#20540;-&#24179;&#22343;&#27604;&#65288;PAR&#65289;&#30340;
&lt;/p&gt;
&lt;p&gt;
The increasing trend in adopting electric vehicles (EVs) will significantly impact the residential electricity demand, which results in an increased risk of transformer overload in the distribution grid. To mitigate such risks, there are urgent needs to develop effective EV charging controllers. Currently, the majority of the EV charge controllers are based on a centralized approach for managing individual EVs or a group of EVs. In this paper, we introduce a decentralized Multi-agent Reinforcement Learning (MARL) charging framework that prioritizes the preservation of privacy for EV owners. We employ the Centralized Training Decentralized Execution-Deep Deterministic Policy Gradient (CTDE-DDPG) scheme, which provides valuable information to users during training while maintaining privacy during execution. Our results demonstrate that the CTDE framework improves the performance of the charging network by reducing the network costs. Moreover, we show that the Peak-to-Average Ratio (PAR) 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#26080;&#30417;&#30563;&#24494;&#35843;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#26410;&#30693;&#31867;&#21035;&#21644;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;UEO&#30340;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#25552;&#39640;&#23545;&#36229;&#20986;&#20998;&#24067;&#26679;&#26412;&#30340;&#26816;&#27979;&#33021;&#21147;&#21644;&#39044;&#23450;&#20041;&#31867;&#21035;&#23454;&#20363;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12919</link><description>&lt;p&gt;
&#29992;CLIP&#23454;&#29616;&#30495;&#23454;&#30340;&#26080;&#30417;&#30563;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Towards Realistic Unsupervised Fine-tuning with CLIP. (arXiv:2308.12919v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#26080;&#30417;&#30563;&#24494;&#35843;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#26410;&#30693;&#31867;&#21035;&#21644;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;UEO&#30340;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#25552;&#39640;&#23545;&#36229;&#20986;&#20998;&#24067;&#26679;&#26412;&#30340;&#26816;&#27979;&#33021;&#21147;&#21644;&#39044;&#23450;&#20041;&#31867;&#21035;&#23454;&#20363;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22914;CLIP&#30340;&#20986;&#29616;&#25512;&#21160;&#20102;&#20154;&#20204;&#22312;&#19979;&#28216;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;&#12290;&#23613;&#31649;&#19968;&#20123;&#20043;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;CLIP&#30340;&#26080;&#30417;&#30563;&#24494;&#35843;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#20381;&#36182;&#20110;&#19982;&#30495;&#23454;&#26631;&#31614;&#30456;&#20851;&#30340;&#31867;&#21517;&#31561;&#20808;&#39564;&#30693;&#35782;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#30495;&#23454;&#30340;&#26080;&#30417;&#30563;&#24494;&#35843;&#24773;&#26223;&#65292;&#20551;&#35774;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#26469;&#33258;&#26410;&#30693;&#31867;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#39044;&#23450;&#20041;&#31867;&#26631;&#31614;&#30340;&#35782;&#21035;&#20043;&#22806;&#65292;&#21516;&#26102;&#25552;&#39640;&#23545;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;Universal Entropy Optimization (UEO)&#12290;UEO&#21033;&#29992;&#26679;&#26412;&#32423;&#32622;&#20449;&#24230;&#65292;&#20197;&#36817;&#20284;&#26041;&#24335;&#26368;&#23567;&#21270;&#32622;&#20449;&#23454;&#20363;&#30340;&#26465;&#20214;&#29109;&#24182;&#26368;&#22823;&#21270;&#36793;&#32536;&#29109;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of vision-language models (VLMs), such as CLIP, has spurred a significant research effort towards their application for downstream supervised learning tasks. Although some previous studies have explored the unsupervised fine-tuning of CLIP, they often rely on prior knowledge in the form of class names associated with ground truth labels. In this paper, we delve into a realistic unsupervised fine-tuning scenario by assuming that the unlabeled data might contain out-of-distribution samples from unknown classes. Furthermore, we emphasize the importance of simultaneously enhancing out-of-distribution detection capabilities alongside the recognition of instances associated with predefined class labels.  To tackle this problem, we present a simple, efficient, and effective fine-tuning approach called Universal Entropy Optimization (UEO). UEO leverages sample-level confidence to approximately minimize the conditional entropy of confident instances and maximize the marginal entro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#30340;&#28431;&#27934;&#65292;&#24182;&#35752;&#35770;&#20102;&#28431;&#27934;&#21487;&#33021;&#30340;&#21407;&#22240;&#12289;&#23545;&#25239;&#25915;&#20987;&#19982;&#38543;&#26426;&#21270;&#31034;&#20363;&#30340;&#24046;&#24322;&#20197;&#21450;&#30456;&#20851;&#30340;&#36947;&#24503;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12918</link><description>&lt;p&gt;
&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#23545;&#25239;&#25915;&#20987;&#26041;&#38754;&#30340;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Vulnerabilities in ML systems in terms of adversarial attacks. (arXiv:2308.12918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#30340;&#28431;&#27934;&#65292;&#24182;&#35752;&#35770;&#20102;&#28431;&#27934;&#21487;&#33021;&#30340;&#21407;&#22240;&#12289;&#23545;&#25239;&#25915;&#20987;&#19982;&#38543;&#26426;&#21270;&#31034;&#20363;&#30340;&#24046;&#24322;&#20197;&#21450;&#30456;&#20851;&#30340;&#36947;&#24503;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#20123;&#38590;&#20197;&#21457;&#29616;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#36825;&#20123;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#21487;&#33021;&#23545;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#38450;&#24481;&#31995;&#32479;&#26500;&#25104;&#25361;&#25112;&#65292;&#24182;&#21487;&#33021;&#24433;&#21709;&#26410;&#26469;&#32593;&#32476;&#25915;&#20987;&#30340;&#38450;&#24481;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#20316;&#32773;&#19987;&#27880;&#20110;&#36825;&#20010;&#39046;&#22495;&#12290;&#20182;&#20204;&#25506;&#35752;&#20102;AI&#31995;&#32479;&#30340;&#28431;&#27934;&#24102;&#26469;&#30340;&#21518;&#26524;&#12290;&#36825;&#21253;&#25324;&#35752;&#35770;&#28431;&#27934;&#21487;&#33021;&#21457;&#29983;&#30340;&#21407;&#22240;&#65292;&#38543;&#26426;&#21270;&#21644;&#23545;&#25239;&#24615;&#31034;&#20363;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#28431;&#27934;&#21487;&#33021;&#24341;&#21457;&#30340;&#36947;&#24503;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#24403;AI&#31995;&#32479;&#22788;&#20110;&#27979;&#35797;&#38454;&#27573;&#24182;&#20934;&#22791;&#36827;&#34892;&#26356;&#24191;&#27867;&#30340;&#20351;&#29992;&#26102;&#65292;&#36866;&#24403;&#36827;&#34892;&#38024;&#23545;&#24615;&#30340;&#35757;&#32451;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
There have been recent adversarial attacks that are difficult to find. These new adversarial attacks methods may pose challenges to current deep learning cyber defense systems and could influence the future defense of cyberattacks. The authors focus on this domain in this research paper. They explore the consequences of vulnerabilities in AI systems. This includes discussing how they might arise, differences between randomized and adversarial examples and also potential ethical implications of vulnerabilities. Moreover, it is important to train the AI systems appropriately when they are in testing phase and getting them ready for broader use.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#20013;&#30340;&#21151;&#29575;&#36229;&#39069;&#20351;&#29992;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#22810;&#31181;LLM&#21450;&#20854;&#19981;&#21516;&#37197;&#32622;&#30340;&#21151;&#32791;&#27169;&#24335;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;LLM&#38598;&#32676;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#21151;&#29575;&#36229;&#39069;&#20351;&#29992;&#26426;&#20250;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#20013;&#24515;&#30340;&#21151;&#29575;&#25928;&#29575;&#65292;&#24182;&#19988;&#20801;&#35768;&#26356;&#22810;&#30340;&#26381;&#21153;&#22120;&#37096;&#32626;&#65292;&#21516;&#26102;&#20943;&#23569;&#37096;&#32626;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.12908</link><description>&lt;p&gt;
POLCA&#65306;LLM&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#20013;&#30340;&#21151;&#29575;&#36229;&#39069;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
POLCA: Power Oversubscription in LLM Cloud Providers. (arXiv:2308.12908v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#20013;&#30340;&#21151;&#29575;&#36229;&#39069;&#20351;&#29992;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#22810;&#31181;LLM&#21450;&#20854;&#19981;&#21516;&#37197;&#32622;&#30340;&#21151;&#32791;&#27169;&#24335;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;LLM&#38598;&#32676;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#21151;&#29575;&#36229;&#39069;&#20351;&#29992;&#26426;&#20250;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#20013;&#24515;&#30340;&#21151;&#29575;&#25928;&#29575;&#65292;&#24182;&#19988;&#20801;&#35768;&#26356;&#22810;&#30340;&#26381;&#21153;&#22120;&#37096;&#32626;&#65292;&#21516;&#26102;&#20943;&#23569;&#37096;&#32626;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21019;&#26032;&#21450;&#20854;&#21508;&#31181;&#29992;&#20363;&#36805;&#36895;&#25512;&#39640;&#20102;&#25968;&#25454;&#20013;&#24515;GPU&#30340;&#35745;&#31639;&#33021;&#21147;&#38656;&#27714;&#12290;&#20960;&#23478;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#21644;&#20854;&#20182;&#20225;&#19994;&#24050;&#32463;&#21046;&#23450;&#20102;&#22823;&#35268;&#27169;&#25193;&#24352;&#35745;&#21010;&#65292;&#20197;&#25903;&#25345;&#36825;&#20123;&#26032;&#24037;&#20316;&#36127;&#36733;&#12290;&#25968;&#25454;&#20013;&#24515;&#30340;&#20851;&#38190;&#29942;&#39048;&#36164;&#28304;&#20043;&#19968;&#26159;&#30005;&#21147;&#65292;&#32780;&#38543;&#30528;LLM&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#22823;&#65292;&#23427;&#20204;&#30340;&#21151;&#32791;&#20063;&#36234;&#26469;&#36234;&#39640;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;LLM&#38598;&#32676;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#21151;&#29575;&#36229;&#39069;&#20351;&#29992;&#26426;&#20250;&#12290;&#21151;&#29575;&#36229;&#39069;&#20351;&#29992;&#25552;&#39640;&#20102;&#36825;&#20123;&#25968;&#25454;&#20013;&#24515;&#30340;&#21151;&#29575;&#25928;&#29575;&#65292;&#20801;&#35768;&#27599;&#20010;&#25968;&#25454;&#20013;&#24515;&#37096;&#32626;&#26356;&#22810;&#30340;&#26381;&#21153;&#22120;&#65292;&#24182;&#20943;&#23569;&#20102;&#37096;&#32626;&#26102;&#38388;&#65292;&#22240;&#20026;&#24314;&#35774;&#26032;&#30340;&#25968;&#25454;&#20013;&#24515;&#24456;&#24930;&#12290;&#25105;&#20204;&#24191;&#27867;&#22320;&#34920;&#24449;&#20102;&#21508;&#31181;LLM&#21450;&#20854;&#37197;&#32622;&#30340;&#21151;&#32791;&#27169;&#24335;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#25512;&#29702;&#21644;&#35757;&#32451;&#21151;&#32791;&#27169;&#24335;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#26681;&#25454;&#25105;&#20204;&#23545;&#36825;&#20123;LLM&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#22768;&#31216;&#24179;&#22343;&#24320;&#38144;&#31243;&#24230;&#30340;&#21151;&#32791;&#36229;&#39069;&#20351;&#29992;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#20013;&#24515;&#30340;&#33021;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent innovation in large language models (LLMs), and their myriad use-cases have rapidly driven up the compute capacity demand for datacenter GPUs. Several cloud providers and other enterprises have made substantial plans of growth in their datacenters to support these new workloads. One of the key bottleneck resources in datacenters is power, and given the increasing model sizes of LLMs, they are becoming increasingly power intensive. In this paper, we show that there is a significant opportunity to oversubscribe power in LLM clusters. Power oversubscription improves the power efficiency of these datacenters, allowing more deployable servers per datacenter, and reduces the deployment time, since building new datacenters is slow.  We extensively characterize the power consumption patterns of a variety of LLMs and their configurations. We identify the differences between the inference and training power consumption patterns. Based on our analysis of these LLMs, we claim that the avera
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CDAN&#30340;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;&#65292;&#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#12290;&#35813;&#32593;&#32476;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#12289;&#21367;&#31215;&#21644;&#31264;&#23494;&#22359;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36339;&#36291;&#36830;&#25509;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#21518;&#22788;&#29702;&#38454;&#27573;&#36827;&#19968;&#27493;&#25913;&#21892;&#33394;&#24425;&#24179;&#34913;&#21644;&#23545;&#27604;&#24230;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12902</link><description>&lt;p&gt;
CDAN: &#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement. (arXiv:2308.12902v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CDAN&#30340;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;&#65292;&#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#12290;&#35813;&#32593;&#32476;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#12289;&#21367;&#31215;&#21644;&#31264;&#23494;&#22359;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36339;&#36291;&#36830;&#25509;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#21518;&#22788;&#29702;&#38454;&#27573;&#36827;&#19968;&#27493;&#25913;&#21892;&#33394;&#24425;&#24179;&#34913;&#21644;&#23545;&#27604;&#24230;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20809;&#22270;&#20687;&#20197;&#19981;&#36275;&#30340;&#29031;&#26126;&#20026;&#29305;&#24449;&#65292;&#38754;&#20020;&#28165;&#26224;&#24230;&#20943;&#24369;&#12289;&#39068;&#33394;&#26263;&#28129;&#21644;&#32454;&#33410;&#20943;&#23569;&#30340;&#25361;&#25112;&#12290;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#25913;&#21892;&#20142;&#24230;&#12289;&#23545;&#27604;&#24230;&#21644;&#25972;&#20307;&#24863;&#30693;&#36136;&#37327;&#26469;&#32416;&#27491;&#36825;&#20123;&#38382;&#39064;&#65292;&#20174;&#32780;&#20419;&#36827;&#20934;&#30830;&#30340;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;&#65288;CDAN&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#20302;&#20809;&#22270;&#20687;&#12290;CDAN&#23558;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#19982;&#21367;&#31215;&#21644;&#31264;&#23494;&#22359;&#30456;&#32467;&#21512;&#65292;&#37197;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36339;&#36291;&#36830;&#25509;&#12290;&#35813;&#26550;&#26500;&#30830;&#20445;&#20102;&#26377;&#25928;&#30340;&#20449;&#24687;&#20256;&#36882;&#21644;&#29305;&#24449;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#19987;&#38376;&#30340;&#21518;&#22788;&#29702;&#38454;&#27573;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#33394;&#24425;&#24179;&#34913;&#21644;&#23545;&#27604;&#24230;&#12290;&#19982;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#39046;&#22495;&#30340;&#26368;&#26032;&#25104;&#26524;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-light images, characterized by inadequate illumination, pose challenges of diminished clarity, muted colors, and reduced details. Low-light image enhancement, an essential task in computer vision, aims to rectify these issues by improving brightness, contrast, and overall perceptual quality, thereby facilitating accurate analysis and interpretation. This paper introduces the Convolutional Dense Attention-guided Network (CDAN), a novel solution for enhancing low-light images. CDAN integrates an autoencoder-based architecture with convolutional and dense blocks, complemented by an attention mechanism and skip connections. This architecture ensures efficient information propagation and feature learning. Furthermore, a dedicated post-processing phase refines color balance and contrast. Our approach demonstrates notable progress compared to state-of-the-art results in low-light image enhancement, showcasing its robustness across a wide range of challenging scenarios. Our model performs 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#30340;&#32479;&#19968;&#25968;&#25454;&#31649;&#29702;&#21644;&#32508;&#21512;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#12290;&#20854;&#36129;&#29486;&#21253;&#25324;&#24341;&#20837;&#8220;&#21407;&#23376;&#25991;&#20214;&#8221;&#20316;&#20026;&#32479;&#19968;&#23384;&#20648;&#26684;&#24335;&#65292;&#25552;&#20379;&#20102;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#25216;&#26415;&#36827;&#23637;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#24314;&#31435;&#20102;&#24615;&#33021;&#25490;&#34892;&#27036;&#21644;&#37492;&#23450;&#20102;&#26377;&#28508;&#21147;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.12899</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#25968;&#25454;&#31649;&#29702;&#21644;&#32508;&#21512;&#24615;&#33021;&#35780;&#20272;&#29992;&#20110;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;[&#23454;&#39564;&#65292;&#20998;&#26512;&#21644;&#22522;&#20934;]&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Unified Data Management and Comprehensive Performance Evaluation for Urban Spatial-Temporal Prediction [Experiment, Analysis &amp; Benchmark]. (arXiv:2308.12899v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12899
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#30340;&#32479;&#19968;&#25968;&#25454;&#31649;&#29702;&#21644;&#32508;&#21512;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#12290;&#20854;&#36129;&#29486;&#21253;&#25324;&#24341;&#20837;&#8220;&#21407;&#23376;&#25991;&#20214;&#8221;&#20316;&#20026;&#32479;&#19968;&#23384;&#20648;&#26684;&#24335;&#65292;&#25552;&#20379;&#20102;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#25216;&#26415;&#36827;&#23637;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#24314;&#31435;&#20102;&#24615;&#33021;&#25490;&#34892;&#27036;&#21644;&#37492;&#23450;&#20102;&#26377;&#28508;&#21147;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#21457;&#23637;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#65292;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#20174;&#19981;&#21516;&#26469;&#28304;&#21644;&#20197;&#19981;&#21516;&#26684;&#24335;&#23384;&#20648;&#30340;&#22810;&#26679;&#21270;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#30340;&#35775;&#38382;&#21644;&#21033;&#29992;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#32780;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22823;&#37327;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#65292;&#30830;&#23450;&#26377;&#25928;&#30340;&#27169;&#22411;&#32467;&#26500;&#21644;&#32452;&#20214;&#20063;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#20010;&#37325;&#35201;&#30340;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#21407;&#23376;&#25991;&#20214;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#20026;&#22478;&#24066;&#26102;&#31354;&#22823;&#25968;&#25454;&#35774;&#35745;&#30340;&#32479;&#19968;&#23384;&#20648;&#26684;&#24335;&#65292;&#24182;&#22312;40&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#31616;&#21270;&#20102;&#25968;&#25454;&#31649;&#29702;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20840;&#38754;&#27010;&#36848;&#20102;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#30340;&#25216;&#26415;&#36827;&#23637;&#65292;&#25351;&#23548;&#20102;&#24378;&#22823;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24314;&#31435;&#20102;&#24615;&#33021;&#25490;&#34892;&#27036;&#24182;&#30830;&#23450;&#20102;&#26377;&#28508;&#21147;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of urban spatial-temporal prediction is advancing rapidly with the development of deep learning techniques and the availability of large-scale datasets. However, challenges persist in accessing and utilizing diverse urban spatial-temporal datasets from different sources and stored in different formats, as well as determining effective model structures and components with the proliferation of deep learning models. This work addresses these challenges and provides three significant contributions. Firstly, we introduce "atomic files", a unified storage format designed for urban spatial-temporal big data, and validate its effectiveness on 40 diverse datasets, simplifying data management. Secondly, we present a comprehensive overview of technological advances in urban spatial-temporal prediction models, guiding the development of robust models. Thirdly, we conduct extensive experiments using diverse models and datasets, establishing a performance leaderboard and identifying promis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24378;&#35843;&#20102;&#23558;&#25991;&#26723;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#26356;&#25509;&#36817;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#25552;&#20986;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#21450;&#39640;&#25928;&#30340;&#22810;&#39029;&#25991;&#26723;&#34920;&#31034;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#23436;&#25972;&#25991;&#26723;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12896</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#26723;&#39029;&#20998;&#31867;&#65306;&#35774;&#35745;&#12289;&#25968;&#25454;&#38598;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Beyond Document Page Classification: Design, Datasets, and Challenges. (arXiv:2308.12896v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24378;&#35843;&#20102;&#23558;&#25991;&#26723;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#26356;&#25509;&#36817;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#25552;&#20986;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#21450;&#39640;&#25928;&#30340;&#22810;&#39029;&#25991;&#26723;&#34920;&#31034;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#23436;&#25972;&#25991;&#26723;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24378;&#35843;&#20102;&#23558;&#25991;&#26723;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#26356;&#25509;&#36817;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#21363;&#22312;&#27979;&#35797;&#25968;&#25454;&#30340;&#24615;&#36136;&#19978;&#65288;$X$&#65306;&#22810;&#36890;&#36947;&#12289;&#22810;&#39029;&#12289;&#22810;&#34892;&#19994;&#65307;$Y$&#65306;&#31867;&#21035;&#20998;&#24067;&#21644;&#26631;&#31614;&#38598;&#30340;&#22810;&#26679;&#24615;&#65289;&#21644;&#32771;&#34385;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#65288;$f$&#65306;&#22810;&#39029;&#25991;&#26723;&#12289;&#39029;&#38754;&#27969;&#21644;&#25991;&#26723;&#25414;&#32465;&#20998;&#31867;&#65292;...&#65289;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20844;&#20849;&#30340;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#65292;&#24182;&#35268;&#33539;&#20102;&#24212;&#29992;&#22330;&#26223;&#20013;&#20135;&#29983;&#30340;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#28608;&#21457;&#20102;&#20197;&#39640;&#25928;&#30340;&#22810;&#39029;&#25991;&#26723;&#34920;&#31034;&#20026;&#30446;&#26631;&#30340;&#20215;&#20540;&#12290;&#23545;&#25552;&#20986;&#30340;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#24050;&#32463;&#21464;&#24471;&#26080;&#20851;&#32039;&#35201;&#65292;&#24182;&#38656;&#35201;&#26356;&#26032;&#20197;&#35780;&#20272;&#23454;&#38469;&#20013;&#33258;&#28982;&#21457;&#29983;&#30340;&#23436;&#25972;&#25991;&#26723;&#12290;&#36825;&#20010;&#29616;&#23454;&#24773;&#20917;&#26816;&#26597;&#20063;&#21628;&#21505;&#26356;&#25104;&#29087;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#28085;&#30422;&#26657;&#20934;&#35780;&#20272;&#12289;&#25512;&#29702;&#22797;&#26434;&#24615;&#65288;&#26102;&#38388;-&#20869;&#23384;&#65289;&#21644;&#19968;&#31995;&#21015;&#29616;&#23454;&#20998;&#25955;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper highlights the need to bring document classification benchmarking closer to real-world applications, both in the nature of data tested ($X$: multi-channel, multi-paged, multi-industry; $Y$: class distributions and label set variety) and in classification tasks considered ($f$: multi-page document, page stream, and document bundle classification, ...). We identify the lack of public multi-page document classification datasets, formalize different classification tasks arising in application scenarios, and motivate the value of targeting efficient multi-page document representations. An experimental study on proposed multi-page document classification datasets demonstrates that current benchmarks have become irrelevant and need to be updated to evaluate complete documents, as they naturally occur in practice. This reality check also calls for more mature evaluation methodologies, covering calibration evaluation, inference complexity (time-memory), and a range of realistic distr
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#36127;&#36131;&#20219;&#30340;AI&#25968;&#25454;&#25910;&#38598;&#65292;&#38656;&#35201;&#23545;&#25968;&#25454;&#30340;&#36136;&#37327;&#36827;&#34892;&#24443;&#24213;&#30340;&#23457;&#26597;&#65292;&#36991;&#20813;&#19981;&#20844;&#24179;&#12289;&#20559;&#35265;&#25110;&#19981;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.12885</link><description>&lt;p&gt;
&#25910;&#38598;&#65292;&#27979;&#37327;&#65292;&#37325;&#22797;&#65306;&#36127;&#36131;&#20219;&#30340;AI&#25968;&#25454;&#25910;&#38598;&#30340;&#21487;&#38752;&#24615;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Collect, Measure, Repeat: Reliability Factors for Responsible AI Data Collection. (arXiv:2308.12885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12885
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36127;&#36131;&#20219;&#30340;AI&#25968;&#25454;&#25910;&#38598;&#65292;&#38656;&#35201;&#23545;&#25968;&#25454;&#30340;&#36136;&#37327;&#36827;&#34892;&#24443;&#24213;&#30340;&#23457;&#26597;&#65292;&#36991;&#20813;&#19981;&#20844;&#24179;&#12289;&#20559;&#35265;&#25110;&#19981;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36805;&#36895;&#36827;&#20837;&#25105;&#20204;&#30340;&#26085;&#24120;&#27963;&#21160;&#21644;&#39640;&#39118;&#38505;&#39046;&#22495;&#65292;&#35201;&#27714;&#23545;&#20854;&#20844;&#24179;&#24615;&#21644;&#21487;&#38752;&#24615;&#36827;&#34892;&#36879;&#26126;&#21644;&#23457;&#26597;&#12290;&#20026;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#30740;&#31350;&#36890;&#24120;&#20250;&#38598;&#20013;&#22312;&#20854;&#37096;&#32626;&#25152;&#20351;&#29992;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#65292;&#20363;&#22914;&#21019;&#24314;&#21644;&#32500;&#25252;&#25991;&#20214;&#20197;&#20102;&#35299;&#20854;&#26469;&#28304;&#12289;&#24320;&#21457;&#36807;&#31243;&#21644;&#20262;&#29702;&#32771;&#34385;&#12290;&#28982;&#32780;&#65292;AI&#30340;&#25968;&#25454;&#25910;&#38598;&#36890;&#24120;&#20173;&#28982;&#26159;&#19968;&#27425;&#24615;&#30340;&#23454;&#36341;&#65292;&#32780;&#19988;&#32463;&#24120;&#20026;&#29305;&#23450;&#30446;&#30340;&#25110;&#24212;&#29992;&#31243;&#24207;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20250;&#34987;&#37325;&#22797;&#29992;&#20110;&#20854;&#20182;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#21487;&#33021;&#38543;&#26102;&#38388;&#19981;&#20855;&#26377;&#20195;&#34920;&#24615;&#65292;&#21253;&#21547;&#27169;&#31946;&#25110;&#38169;&#35823;&#30340;&#27880;&#37322;&#65292;&#25110;&#32773;&#26080;&#27861;&#36328;&#38382;&#39064;&#25110;&#39046;&#22495;&#36827;&#34892;&#27867;&#21270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#20570;&#27861;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#12289;&#20559;&#35265;&#25110;&#19981;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;AI&#30340;&#25968;&#25454;&#25910;&#38598;&#24212;&#35813;&#20197;&#36127;&#36131;&#20219;&#30340;&#26041;&#24335;&#36827;&#34892;&#65292;&#23545;&#25968;&#25454;&#30340;&#36136;&#37327;&#36827;&#34892;&#24443;&#24213;&#30340;&#23457;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid entry of machine learning approaches in our daily activities and high-stakes domains demands transparency and scrutiny of their fairness and reliability. To help gauge machine learning models' robustness, research typically focuses on the massive datasets used for their deployment, e.g., creating and maintaining documentation for understanding their origin, process of development, and ethical considerations. However, data collection for AI is still typically a one-off practice, and oftentimes datasets collected for a certain purpose or application are reused for a different problem. Additionally, dataset annotations may not be representative over time, contain ambiguous or erroneous annotations, or be unable to generalize across issues or domains. Recent research has shown these practices might lead to unfair, biased, or inaccurate outcomes. We argue that data collection for AI should be performed in a responsible manner where the quality of the data is thoroughly scrutinized
&lt;/p&gt;</description></item><item><title>LCANets++&#26159;&#19968;&#31181;&#20351;&#29992;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#21644;&#23618;&#38388;&#31454;&#20105;&#30340;&#40065;&#26834;&#24615;&#38899;&#39057;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#26469;&#25552;&#39640;&#23545;&#25200;&#21160;&#21644;&#23545;&#25239;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12882</link><description>&lt;p&gt;
LCANets++: &#20351;&#29992;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#21644;&#23618;&#38388;&#31454;&#20105;&#30340;&#40065;&#26834;&#24615;&#38899;&#39057;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
LCANets++: Robust Audio Classification using Multi-layer Neural Networks with Lateral Competition. (arXiv:2308.12882v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12882
&lt;/p&gt;
&lt;p&gt;
LCANets++&#26159;&#19968;&#31181;&#20351;&#29992;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#21644;&#23618;&#38388;&#31454;&#20105;&#30340;&#40065;&#26834;&#24615;&#38899;&#39057;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#26469;&#25552;&#39640;&#23545;&#25200;&#21160;&#21644;&#23545;&#25239;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#20998;&#31867;&#26088;&#22312;&#35782;&#21035;&#38899;&#39057;&#20449;&#21495;&#65292;&#21253;&#25324;&#35821;&#38899;&#21629;&#20196;&#25110;&#22768;&#38899;&#20107;&#20214;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#38899;&#39057;&#20998;&#31867;&#22120;&#23481;&#26131;&#21463;&#21040;&#25200;&#21160;&#21644;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#36890;&#24120;&#21463;&#21040;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#36890;&#36807;&#23616;&#37096;&#31454;&#20105;&#31639;&#27861;&#65288;LCA&#65289;&#22312;&#31532;&#19968;&#23618;&#20351;&#29992;&#20102;&#31070;&#32463;&#21551;&#21457;&#24335;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#36827;&#34892;&#31232;&#30095;&#32534;&#30721;&#65292;&#21363;LCANets&#12290;LCANets&#36890;&#36807;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#32452;&#21512;&#26469;&#23398;&#20064;&#65292;&#20943;&#23569;&#23545;&#26631;&#35760;&#26679;&#26412;&#30340;&#20381;&#36182;&#24615;&#12290;&#21463;&#21040;&#21548;&#35273;&#30382;&#23618;&#20063;&#26159;&#31232;&#30095;&#30340;&#20107;&#23454;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;LCANets&#25193;&#23637;&#21040;&#38899;&#39057;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;LCANets++&#65292;&#23427;&#20204;&#26159;CNNs&#65292;&#36890;&#36807;LCA&#22312;&#22810;&#23618;&#27425;&#19978;&#36827;&#34892;&#31232;&#30095;&#32534;&#30721;&#12290;&#25105;&#20204;&#35777;&#26126;LCANets++&#23545;&#20110;&#25200;&#21160;&#65288;&#22914;&#32972;&#26223;&#22122;&#22768;&#65289;&#20197;&#21450;&#40657;&#30418;&#21644;&#30333;&#30418;&#25915;&#20987;&#65288;&#22914;&#36867;&#36991;&#21644;&#30772;&#22351;&#65289;&#27604;&#26631;&#20934;CNN&#21644;LCANets&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio classification aims at recognizing audio signals, including speech commands or sound events. However, current audio classifiers are susceptible to perturbations and adversarial attacks. In addition, real-world audio classification tasks often suffer from limited labeled data. To help bridge these gaps, previous work developed neuro-inspired convolutional neural networks (CNNs) with sparse coding via the Locally Competitive Algorithm (LCA) in the first layer (i.e., LCANets) for computer vision. LCANets learn in a combination of supervised and unsupervised learning, reducing dependency on labeled samples. Motivated by the fact that auditory cortex is also sparse, we extend LCANets to audio recognition tasks and introduce LCANets++, which are CNNs that perform sparse coding in multiple layers via LCA. We demonstrate that LCANets++ are more robust than standard CNNs and LCANets against perturbations, e.g., background noise, as well as black-box and white-box attacks, e.g., evasion an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31616;&#26131;&#27880;&#24847;&#21147;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#25552;&#39640;Transformer&#31070;&#32463;&#32593;&#32476;&#22312;&#28151;&#27788;&#31995;&#32479;&#26102;&#38388;&#21160;&#24577;&#39044;&#27979;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#38190;&#12289;&#26597;&#35810;&#21644;softmax&#65292;&#30452;&#25509;&#23558;&#27880;&#24847;&#21147;&#24471;&#20998;&#20316;&#20026;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#37325;&#26500;&#21644;&#39044;&#27979;&#28151;&#27788;&#31995;&#32479;&#30340;&#26102;&#38388;&#21160;&#24577;&#26041;&#38754;&#27604;&#20256;&#32479;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#26041;&#27861;&#26356;&#20855;&#40065;&#26834;&#24615;&#21644;&#31616;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12874</link><description>&lt;p&gt;
&#31616;&#26131;&#27880;&#24847;&#21147;&#65306;&#19968;&#31181;&#29992;&#20110;Transformer&#30340;&#31616;&#21333;&#33258;&#27880;&#24847;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Easy attention: A simple self-attention mechanism for Transformers. (arXiv:2308.12874v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31616;&#26131;&#27880;&#24847;&#21147;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#25552;&#39640;Transformer&#31070;&#32463;&#32593;&#32476;&#22312;&#28151;&#27788;&#31995;&#32479;&#26102;&#38388;&#21160;&#24577;&#39044;&#27979;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#38190;&#12289;&#26597;&#35810;&#21644;softmax&#65292;&#30452;&#25509;&#23558;&#27880;&#24847;&#21147;&#24471;&#20998;&#20316;&#20026;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#37325;&#26500;&#21644;&#39044;&#27979;&#28151;&#27788;&#31995;&#32479;&#30340;&#26102;&#38388;&#21160;&#24577;&#26041;&#38754;&#27604;&#20256;&#32479;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#26041;&#27861;&#26356;&#20855;&#40065;&#26834;&#24615;&#21644;&#31616;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#29992;&#20110;&#28151;&#27788;&#31995;&#32479;&#26102;&#38388;&#21160;&#24577;&#39044;&#27979;&#30340;Transformer&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#31216;&#20026;&#31616;&#26131;&#27880;&#24847;&#21147;&#12290;&#30001;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#20165;&#20351;&#29992;&#26597;&#35810;&#21644;&#38190;&#30340;&#20869;&#31215;&#65292;&#22240;&#27492;&#35777;&#26126;&#20102;&#20026;&#20102;&#33719;&#21462;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#25152;&#38656;&#30340;&#27880;&#24847;&#21147;&#24471;&#20998;&#65292;&#24182;&#19981;&#38656;&#35201;&#38190;&#12289;&#26597;&#35810;&#21644;softmax&#12290;&#36890;&#36807;&#22312;softmax&#27880;&#24847;&#21147;&#24471;&#20998;&#19978;&#23454;&#26045;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35266;&#23519;&#21040;&#33258;&#27880;&#24847;&#21147;&#22312;&#27880;&#24847;&#21147;&#24471;&#20998;&#30340;&#24352;&#25104;&#31354;&#38388;&#20013;&#21387;&#32553;&#20102;&#26469;&#33258;&#26597;&#35810;&#21644;&#38190;&#30340;&#36129;&#29486;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31616;&#26131;&#27880;&#24847;&#21147;&#26041;&#27861;&#30452;&#25509;&#23558;&#27880;&#24847;&#21147;&#24471;&#20998;&#20316;&#20026;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#37325;&#26500;&#21644;&#39044;&#27979;&#23637;&#29616;&#26356;&#24378;&#40065;&#26834;&#24615;&#21644;&#26356;&#23569;&#22797;&#26434;&#24615;&#30340;&#28151;&#27788;&#31995;&#32479;&#30340;&#26102;&#38388;&#21160;&#24577;&#26102;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#65292;&#27604;&#33258;&#27880;&#24847;&#26426;&#21046;&#25110;&#24191;&#27867;&#20351;&#29992;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
To improve the robustness of transformer neural networks used for temporal-dynamics prediction of chaotic systems, we propose a novel attention mechanism called easy attention. Due to the fact that self attention only makes usage of the inner product of queries and keys, it is demonstrated that the keys, queries and softmax are not necessary for obtaining the attention score required to capture long-term dependencies in temporal sequences. Through implementing singular-value decomposition (SVD) on the softmax attention score, we further observe that the self attention compresses contribution from both queries and keys in the spanned space of the attention score. Therefore, our proposed easy-attention method directly treats the attention scores as learnable parameters. This approach produces excellent results when reconstructing and predicting the temporal dynamics of chaotic systems exhibiting more robustness and less complexity than the self attention or the widely-used long short-ter
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IPA&#30340;&#22312;&#32447;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#31649;&#36947;&#33258;&#36866;&#24212;&#31995;&#32479;&#65292;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25209;&#22788;&#29702;&#22823;&#23567;&#12289;&#22797;&#21046;&#21644;&#27169;&#22411;&#21464;&#20307;&#65292;&#20197;&#20248;&#21270;&#20934;&#30830;&#24615;&#12289;&#26368;&#23567;&#21270;&#25104;&#26412;&#24182;&#28385;&#36275;&#29992;&#25143;&#23450;&#20041;&#30340;&#24310;&#36831;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.12871</link><description>&lt;p&gt;
IPA&#65306;&#25512;&#29702;&#31649;&#36947;&#33258;&#36866;&#24212;&#20197;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;
&lt;/p&gt;
&lt;p&gt;
IPA: Inference Pipeline Adaptation to Achieve High Accuracy and Cost-Efficiency. (arXiv:2308.12871v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12871
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IPA&#30340;&#22312;&#32447;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#31649;&#36947;&#33258;&#36866;&#24212;&#31995;&#32479;&#65292;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25209;&#22788;&#29702;&#22823;&#23567;&#12289;&#22797;&#21046;&#21644;&#27169;&#22411;&#21464;&#20307;&#65292;&#20197;&#20248;&#21270;&#20934;&#30830;&#24615;&#12289;&#26368;&#23567;&#21270;&#25104;&#26412;&#24182;&#28385;&#36275;&#29992;&#25143;&#23450;&#20041;&#30340;&#24310;&#36831;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#20135;&#31995;&#32479;&#20013;&#65292;&#39640;&#25928;&#22320;&#20248;&#21270;&#22810;&#27169;&#22411;&#25512;&#29702;&#31649;&#36947;&#20197;&#23454;&#29616;&#24555;&#36895;&#12289;&#20934;&#30830;&#21644;&#25104;&#26412;&#25928;&#30410;&#30340;&#25512;&#29702;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#32771;&#34385;&#21040;&#23427;&#20204;&#23545;&#31471;&#21040;&#31471;&#24310;&#36831;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;&#20026;&#20102;&#31616;&#21270;&#20934;&#30830;&#24615;&#21644;&#25104;&#26412;&#20043;&#38388;&#24191;&#38420;&#32780;&#22797;&#26434;&#30340;&#26435;&#34913;&#31354;&#38388;&#30340;&#25506;&#32034;&#65292;&#25552;&#20379;&#32773;&#36890;&#24120;&#36873;&#25321;&#32771;&#34385;&#20854;&#20013;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#21327;&#35843;&#20934;&#30830;&#24615;&#21644;&#25104;&#26412;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#24182;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#31649;&#29702;&#25512;&#29702;&#31649;&#36947;&#20013;&#27169;&#22411;&#21464;&#20307;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IPA&#65292;&#19968;&#31181;&#22312;&#32447;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#31649;&#36947;&#33258;&#36866;&#24212;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#21033;&#29992;&#27599;&#20010;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#30340;&#27169;&#22411;&#21464;&#20307;&#12290;&#27169;&#22411;&#21464;&#20307;&#26159;&#21516;&#19968;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19981;&#21516;&#29256;&#26412;&#65292;&#20854;&#36164;&#28304;&#38656;&#27714;&#12289;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#26377;&#25152;&#19981;&#21516;&#12290;IPA&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25209;&#22788;&#29702;&#22823;&#23567;&#12289;&#22797;&#21046;&#21644;&#27169;&#22411;&#21464;&#20307;&#26469;&#20248;&#21270;&#20934;&#30830;&#24615;&#12289;&#26368;&#23567;&#21270;&#25104;&#26412;&#24182;&#28385;&#36275;&#29992;&#25143;&#23450;&#20041;&#30340;&#24310;&#36831;SLA&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently optimizing multi-model inference pipelines for fast, accurate, and cost-effective inference is a crucial challenge in ML production systems, given their tight end-to-end latency requirements. To simplify the exploration of the vast and intricate trade-off space of accuracy and cost in inference pipelines, providers frequently opt to consider one of them. However, the challenge lies in reconciling accuracy and cost trade-offs. To address this challenge and propose a solution to efficiently manage model variants in inference pipelines, we present IPA, an online deep-learning Inference Pipeline Adaptation system that efficiently leverages model variants for each deep learning task. Model variants are different versions of pre-trained models for the same deep learning task with variations in resource requirements, latency, and accuracy. IPA dynamically configures batch size, replication, and model variants to optimize accuracy, minimize costs, and meet user-defined latency SLAs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#21516;&#21270;&#31574;&#30053;&#65292;&#21487;&#21487;&#38752;&#22320;&#22788;&#29702;&#21253;&#21547;&#19981;&#30830;&#23450;&#24230;&#37327;&#21270;&#30340;&#23380;&#38553;&#23610;&#24230;&#21453;&#24212;&#21453;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#25968;&#25454;&#39537;&#21160;&#21644;&#29289;&#29702;&#24314;&#27169;&#65292;&#30830;&#20445;&#20102;&#23380;&#38553;&#23610;&#24230;&#27169;&#22411;&#30340;&#21487;&#38752;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2308.12864</link><description>&lt;p&gt;
&#33258;&#21160;&#21152;&#26435;&#30340;&#36125;&#21494;&#26031;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21644;&#40065;&#26834;&#20272;&#35745;&#22312;&#23380;&#38553;&#23610;&#24230;&#28342;&#35299;&#22270;&#20687;&#30340;&#22810;&#20219;&#21153;&#21453;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Auto-weighted Bayesian Physics-Informed Neural Networks and robust estimations for multitask inverse problems in pore-scale imaging of dissolution. (arXiv:2308.12864v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#21516;&#21270;&#31574;&#30053;&#65292;&#21487;&#21487;&#38752;&#22320;&#22788;&#29702;&#21253;&#21547;&#19981;&#30830;&#23450;&#24230;&#37327;&#21270;&#30340;&#23380;&#38553;&#23610;&#24230;&#21453;&#24212;&#21453;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#25968;&#25454;&#39537;&#21160;&#21644;&#29289;&#29702;&#24314;&#27169;&#65292;&#30830;&#20445;&#20102;&#23380;&#38553;&#23610;&#24230;&#27169;&#22411;&#30340;&#21487;&#38752;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#21516;&#21270;&#31574;&#30053;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20351;&#25105;&#20204;&#33021;&#22815;&#21487;&#38752;&#22320;&#22788;&#29702;&#21253;&#21547;&#19981;&#30830;&#23450;&#24230;&#37327;&#21270;&#30340;&#21453;&#24212;&#21453;&#38382;&#39064;&#12290;&#23380;&#38553;&#23610;&#24230;&#30340;&#21453;&#24212;&#27969;&#21160;&#24314;&#27169;&#20026;&#30740;&#31350;&#23439;&#35266;&#24615;&#36136;&#22312;&#21160;&#24577;&#36807;&#31243;&#20013;&#30340;&#28436;&#21464;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21463;&#21040;&#30456;&#20851;&#30340;X&#23556;&#32447;&#24494;&#35745;&#31639;&#39640;&#24230;&#20998;&#36776;&#29575;&#25104;&#20687; (X&#23556;&#32447;&#24494;CT) &#36807;&#31243;&#20013;&#30340;&#25104;&#20687;&#38480;&#21046;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#20102;&#24615;&#36136;&#20272;&#35745;&#20013;&#30340;&#24046;&#24322;&#12290;&#21160;&#21147;&#23398;&#21442;&#25968;&#30340;&#35780;&#20272;&#20063;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#21453;&#24212;&#31995;&#25968;&#26159;&#20851;&#38190;&#21442;&#25968;&#65292;&#20854;&#25968;&#20540;&#33539;&#22260;&#24456;&#24191;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23558;&#19981;&#30830;&#23450;&#24230;&#37327;&#21270;&#38598;&#25104;&#21040;&#24037;&#20316;&#27969;&#31243;&#20013;&#65292;&#30830;&#20445;&#20102;&#23380;&#38553;&#23610;&#24230;&#27169;&#22411;&#30340;&#21487;&#38752;&#26657;&#20934;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#22522;&#20110;&#21453;&#24212;&#21453;&#38382;&#39064;&#30340;&#22810;&#20219;&#21153;&#20844;&#24335;&#65292;&#23558;&#25968;&#25454;&#39537;&#21160;&#21644;&#29289;&#29702;&#24314;&#27169;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we present a novel data assimilation strategy in pore-scale imaging and demonstrate that this makes it possible to robustly address reactive inverse problems incorporating Uncertainty Quantification (UQ). Pore-scale modeling of reactive flow offers a valuable opportunity to investigate the evolution of macro-scale properties subject to dynamic processes. Yet, they suffer from imaging limitations arising from the associated X-ray microtomography (X-ray microCT) process, which induces discrepancies in the properties estimates. Assessment of the kinetic parameters also raises challenges, as reactive coefficients are critical parameters that can cover a wide range of values. We account for these two issues and ensure reliable calibration of pore-scale modeling, based on dynamical microCT images, by integrating uncertainty quantification in the workflow.  The present method is based on a multitasking formulation of reactive inverse problems combining data-driven and physics
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22768;&#23398;&#31354;&#38388;&#25429;&#33719;-&#20877;&#25429;&#33719;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20010;&#20307;&#32423;&#21035;&#30340;&#27979;&#37327;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21160;&#29289;&#23494;&#24230;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.12859</link><description>&lt;p&gt;
&#37319;&#29992;&#22768;&#23398;&#31354;&#38388;&#25429;&#33719;-&#20877;&#25429;&#33719;&#26041;&#27861;&#23454;&#29616;&#33258;&#21160;&#21160;&#29289;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Towards Automated Animal Density Estimation with Acoustic Spatial Capture-Recapture. (arXiv:2308.12859v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22768;&#23398;&#31354;&#38388;&#25429;&#33719;-&#20877;&#25429;&#33719;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20010;&#20307;&#32423;&#21035;&#30340;&#27979;&#37327;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21160;&#29289;&#23494;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#22768;&#21160;&#29289;&#30340;&#34987;&#21160;&#22768;&#23398;&#30417;&#27979;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#30417;&#27979;&#26041;&#27861;&#65292;&#20294;&#38590;&#20197;&#36827;&#34892;&#35270;&#35273;&#35843;&#26597;&#12290;&#25968;&#23383;&#24405;&#38899;&#20202;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#25910;&#38598;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#20294;&#22312;&#36825;&#20123;&#25968;&#25454;&#20013;&#35782;&#21035;&#30446;&#26631;&#29289;&#31181;&#30340;&#22768;&#38899;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#29992;&#20110;&#36827;&#34892;&#22768;&#38899;&#35782;&#21035;&#65292;&#23427;&#20204;&#21487;&#20197;&#24555;&#36895;&#22788;&#29702;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#20294;&#19981;&#33021;&#26816;&#27979;&#21040;&#25152;&#26377;&#30340;&#22768;&#38899;&#65292;&#24182;&#19988;&#20250;&#20135;&#29983;&#19968;&#20123;&#35823;&#25253;&#65288;&#19981;&#23646;&#20110;&#30446;&#26631;&#29289;&#31181;&#30340;&#22768;&#38899;&#65289;&#12290;&#29616;&#26377;&#30340;&#37326;&#29983;&#21160;&#29289;&#31181;&#32676;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#24050;&#32463;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#36825;&#20123;&#38169;&#35823;&#20013;&#30340;&#31532;&#19968;&#20010;&#65292;&#20294;&#30446;&#21069;&#22788;&#29702;&#35823;&#25253;&#30340;&#26041;&#27861;&#36824;&#19981;&#22815;&#25104;&#29087;&#12290;&#23427;&#20204;&#24182;&#27809;&#26377;&#32771;&#34385;&#21040;&#20010;&#20307;&#22768;&#38899;&#29305;&#24449;&#65292;&#20854;&#20013;&#19968;&#20123;&#29305;&#24449;&#26356;&#26377;&#21487;&#33021;&#26159;&#35823;&#25253;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#22768;&#23398;&#31354;&#38388;&#25429;&#33719;-&#20877;&#25429;&#33719;&#25512;&#26029;&#26041;&#27861;&#65292;&#23558;&#20010;&#20307;&#32423;&#27979;&#37327;&#29305;&#24449;&#32508;&#21512;&#36827;&#21435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Passive acoustic monitoring can be an effective way of monitoring wildlife populations that are acoustically active but difficult to survey visually. Digital recorders allow surveyors to gather large volumes of data at low cost, but identifying target species vocalisations in these data is non-trivial. Machine learning (ML) methods are often used to do the identification. They can process large volumes of data quickly, but they do not detect all vocalisations and they do generate some false positives (vocalisations that are not from the target species). Existing wildlife abundance survey methods have been designed specifically to deal with the first of these mistakes, but current methods of dealing with false positives are not well-developed. They do not take account of features of individual vocalisations, some of which are more likely to be false positives than others. We propose three methods for acoustic spatial capture-recapture inference that integrate individual-level measures o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65288;FAT&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#24179;&#28369;&#25910;&#25947;&#36807;&#31243;&#21644;&#25391;&#33633;&#32422;&#26463;&#26469;&#35299;&#20915;&#22312;&#22788;&#29702;&#22823;&#30340;&#25200;&#21160;&#39044;&#31639;&#26102;&#20986;&#29616;&#30340;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12857</link><description>&lt;p&gt;
&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#19982;&#24179;&#28369;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Fast Adversarial Training with Smooth Convergence. (arXiv:2308.12857v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65288;FAT&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#24179;&#28369;&#25910;&#25947;&#36807;&#31243;&#21644;&#25391;&#33633;&#32422;&#26463;&#26469;&#35299;&#20915;&#22312;&#22788;&#29702;&#22823;&#30340;&#25200;&#21160;&#39044;&#31639;&#26102;&#20986;&#29616;&#30340;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#65288;FAT&#65289;&#26377;&#21161;&#20110;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;FAT&#24037;&#20316;&#22312;&#22788;&#29702;&#22823;&#30340;&#25200;&#21160;&#39044;&#31639;&#26102;&#36935;&#21040;&#20102;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#21363;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#19979;&#38477;&#21040;&#25509;&#36817;&#38646;&#30340;&#31243;&#24230;&#65292;&#34987;&#31216;&#20026;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20043;&#21069;FAT&#24037;&#20316;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#35266;&#23519;&#21040;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#20276;&#38543;&#30528;&#25910;&#25947;&#25439;&#22833;&#31163;&#32676;&#20540;&#30340;&#20986;&#29616;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#19968;&#20010;&#36866;&#24230;&#24179;&#28369;&#30340;&#25910;&#25947;&#36807;&#31243;&#23558;&#26159;&#19968;&#20010;&#31283;&#23450;&#30340;FAT&#36807;&#31243;&#65292;&#21487;&#20197;&#35299;&#20915;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#12290;&#20026;&#20102;&#33719;&#24471;&#24179;&#28369;&#30340;&#25910;&#25947;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25391;&#33633;&#32422;&#26463;&#65288;&#31216;&#20026;ConvergeSmooth&#65289;&#65292;&#26469;&#38480;&#21046;&#30456;&#37051;&#21608;&#26399;&#20043;&#38388;&#30340;&#25439;&#22833;&#24046;&#24322;&#12290;ConvergeSmooth&#30340;&#25910;&#25947;&#27493;&#38271;&#34987;&#24341;&#20837;&#26469;&#24179;&#34913;&#25910;&#25947;&#21644;&#24179;&#28369;&#12290;&#21516;&#26679;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19981;&#24341;&#20837;&#39069;&#22806;&#36229;&#21442;&#25968;&#30340;&#26435;&#37325;&#38598;&#20013;&#65292;&#38500;&#20102;l.
&lt;/p&gt;
&lt;p&gt;
Fast adversarial training (FAT) is beneficial for improving the adversarial robustness of neural networks. However, previous FAT work has encountered a significant issue known as catastrophic overfitting when dealing with large perturbation budgets, \ie the adversarial robustness of models declines to near zero during training.  To address this, we analyze the training process of prior FAT work and observe that catastrophic overfitting is accompanied by the appearance of loss convergence outliers.  Therefore, we argue a moderately smooth loss convergence process will be a stable FAT process that solves catastrophic overfitting.  To obtain a smooth loss convergence process, we propose a novel oscillatory constraint (dubbed ConvergeSmooth) to limit the loss difference between adjacent epochs. The convergence stride of ConvergeSmooth is introduced to balance convergence and smoothing. Likewise, we design weight centralization without introducing additional hyperparameters other than the l
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#20648;&#23618;&#35745;&#31639;&#20316;&#20026;&#26680;&#24515;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#36125;&#21494;&#26031;&#21644;&#30830;&#23450;&#24615;&#26041;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#30340;&#20860;&#23481;&#24615;&#65292;&#24182;&#35780;&#20272;&#27604;&#36739;&#20102;&#23427;&#20204;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#35745;&#31639;&#36164;&#28304;&#25928;&#29575;&#21644;&#20272;&#35745;&#21487;&#38752;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.12844</link><description>&lt;p&gt;
&#24102;&#26377;&#20648;&#23618;&#35745;&#31639;&#30340;&#27010;&#29575;&#36127;&#36733;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic load forecasting with Reservoir Computing. (arXiv:2308.12844v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12844
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#20648;&#23618;&#35745;&#31639;&#20316;&#20026;&#26680;&#24515;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#36125;&#21494;&#26031;&#21644;&#30830;&#23450;&#24615;&#26041;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#30340;&#20860;&#23481;&#24615;&#65292;&#24182;&#35780;&#20272;&#27604;&#36739;&#20102;&#23427;&#20204;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#35745;&#31639;&#36164;&#28304;&#25928;&#29575;&#21644;&#20272;&#35745;&#21487;&#38752;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20123;&#24212;&#29992;&#19981;&#20165;&#38656;&#35201;&#25552;&#20379;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#36824;&#38656;&#35201;&#37327;&#21270;&#20854;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;&#30005;&#21147;&#32593;&#32476;&#31649;&#29702;&#23601;&#26159;&#20854;&#20013;&#20043;&#19968;&#65306;&#20026;&#20102;&#36991;&#20813;&#39118;&#38505;&#65292;&#20915;&#31574;&#32773;&#38656;&#35201;&#31934;&#30830;&#21487;&#38752;&#30340;&#36127;&#36733;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#20165;&#20165;&#25552;&#20379;&#28857;&#39044;&#27979;&#26159;&#19981;&#22815;&#30340;&#65292;&#22240;&#27492;&#38656;&#35201;&#37319;&#29992;&#33021;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#20197;&#20648;&#23618;&#35745;&#31639;&#20316;&#20026;&#26680;&#24515;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#22240;&#20854;&#35745;&#31639;&#25928;&#29575;&#21644;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25928;&#26524;&#32780;&#34987;&#36873;&#25321;&#12290;&#34429;&#28982;&#20648;&#23618;&#35745;&#31639;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#28857;&#39044;&#27979;&#19978;&#65292;&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#19968;&#20123;&#24120;&#29992;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#19982;&#20648;&#23618;&#35774;&#32622;&#30340;&#20860;&#23481;&#24615;&#12290;&#36125;&#21494;&#26031;&#21644;&#30830;&#23450;&#24615;&#26041;&#27861;&#37117;&#34987;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#28041;&#21450;&#30340;&#35780;&#20272;&#25351;&#26631;&#26377;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#35745;&#31639;&#36164;&#28304;&#25928;&#29575;&#21644;&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Some applications of deep learning require not only to provide accurate results but also to quantify the amount of confidence in their prediction. The management of an electric power grid is one of these cases: to avoid risky scenarios, decision-makers need both precise and reliable forecasts of, for example, power loads. For this reason, point forecasts are not enough hence it is necessary to adopt methods that provide an uncertainty quantification.  This work focuses on reservoir computing as the core time series forecasting method, due to its computational efficiency and effectiveness in predicting time series. While the RC literature mostly focused on point forecasting, this work explores the compatibility of some popular uncertainty quantification methods with the reservoir setting. Both Bayesian and deterministic approaches to uncertainty assessment are evaluated and compared in terms of their prediction accuracy, computational resource efficiency and reliability of the estimated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#26080;&#20154;&#26426;&#39030;&#37096;&#26426;&#26800;&#33218;&#25191;&#34892;&#22120;&#36712;&#36857;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26102;&#38388;&#21040;&#30896;&#25758;&#30340;&#36816;&#21160;&#35268;&#21010;&#27169;&#22411;&#20197;&#32469;&#36807;&#38556;&#30861;&#29289;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;Q-learning&#27169;&#22411;&#29420;&#31435;&#36861;&#36394;&#21644;&#25511;&#21046;&#26426;&#26800;&#33218;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;&#26399;&#26395;&#36712;&#36857;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#19968;&#31995;&#21015;&#22312;&#39640;&#38590;&#24230;&#21644;&#21361;&#38505;&#29615;&#22659;&#20013;&#30340;&#25191;&#34892;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.12843</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#20154;&#26426;&#39030;&#37096;&#26426;&#26800;&#33218;&#25191;&#34892;&#22120;&#36712;&#36857;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Actuator Trajectory Planning for UAVs with Overhead Manipulator using Reinforcement Learning. (arXiv:2308.12843v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#26080;&#20154;&#26426;&#39030;&#37096;&#26426;&#26800;&#33218;&#25191;&#34892;&#22120;&#36712;&#36857;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26102;&#38388;&#21040;&#30896;&#25758;&#30340;&#36816;&#21160;&#35268;&#21010;&#27169;&#22411;&#20197;&#32469;&#36807;&#38556;&#30861;&#29289;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;Q-learning&#27169;&#22411;&#29420;&#31435;&#36861;&#36394;&#21644;&#25511;&#21046;&#26426;&#26800;&#33218;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;&#26399;&#26395;&#36712;&#36857;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#19968;&#31995;&#21015;&#22312;&#39640;&#38590;&#24230;&#21644;&#21361;&#38505;&#29615;&#22659;&#20013;&#30340;&#25191;&#34892;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31354;&#20013;&#26426;&#26800;&#33218;&#31995;&#32479;&#65292;&#21363;&#35013;&#22791;&#26377;&#21487;&#25511;&#21046;&#30340;&#20108;&#33258;&#30001;&#24230;&#33218;&#30340;&#26080;&#20154;&#26426;&#65292;&#20197;&#23454;&#29616;&#21363;&#26102;&#25191;&#34892;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#20351;&#29992;Q-learning&#26041;&#27861;&#26469;&#25511;&#21046;&#33218;&#23574;&#31471;&#65288;&#21363;&#26411;&#31471;&#25191;&#34892;&#22120;&#65289;&#30340;&#36712;&#36857;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#21040;&#30896;&#25758;&#65288;TTC&#65289;&#30340;&#36816;&#21160;&#35268;&#21010;&#27169;&#22411;&#65292;&#20351;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#33021;&#22815;&#22312;&#20445;&#35777;&#26426;&#26800;&#33218;&#21487;&#36798;&#24615;&#30340;&#21516;&#26102;&#32469;&#36807;&#38556;&#30861;&#29289;&#33322;&#34892;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;Q-learning&#27169;&#22411;&#29420;&#31435;&#36861;&#36394;&#21644;&#25511;&#21046;&#26426;&#26800;&#33218;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;&#26399;&#26395;&#36712;&#36857;&#65292;&#32473;&#23450;&#26080;&#20154;&#26426;&#24179;&#21488;&#30340;&#20219;&#24847;&#22522;&#20934;&#36712;&#36857;&#12290;&#36825;&#31181;&#32452;&#21512;&#20351;&#24471;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#25191;&#34892;&#20219;&#21153;&#65292;&#22914;&#39640;&#31354;&#28938;&#25509;&#12289;&#32467;&#26500;&#30417;&#27979;&#21644;&#20462;&#22797;&#12289;&#30005;&#27744;&#26356;&#25442;&#12289;&#25490;&#27700;&#27807;&#28165;&#29702;&#12289;&#25705;&#22825;&#22823;&#27004;&#28165;&#27905;&#21644;&#30005;&#21147;&#32447;&#36335;&#32500;&#25252;&#22312;&#38590;&#20197;&#21040;&#36798;&#21644;&#21361;&#38505;&#30340;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the operation of an aerial manipulator system, namely an Unmanned Aerial Vehicle (UAV) equipped with a controllable arm with two degrees of freedom to carry out actuation tasks on the fly. Our solution is based on employing a Q-learning method to control the trajectory of the tip of the arm, also called \textit{end-effector}. More specifically, we develop a motion planning model based on Time To Collision (TTC), which enables a quadrotor UAV to navigate around obstacles while ensuring the manipulator's reachability. Additionally, we utilize a model-based Q-learning model to independently track and control the desired trajectory of the manipulator's end-effector, given an arbitrary baseline trajectory for the UAV platform. Such a combination enables a variety of actuation tasks such as high-altitude welding, structural monitoring and repair, battery replacement, gutter cleaning, sky scrapper cleaning, and power line maintenance in hard-to-reach and risky en
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30701;&#36884;&#20844;&#20132;&#32447;&#36335;&#35268;&#21010;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#36890;&#36807;&#35843;&#25972;&#36335;&#32447;&#30340;&#29305;&#23450;&#37096;&#20998;&#65292;&#20943;&#23569;&#26102;&#38388;&#24182;&#25552;&#21319;&#20844;&#20849;&#20132;&#36890;&#26381;&#21153;&#12290;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#28304;&#65292;&#36890;&#36807;&#39044;&#27979;&#36947;&#36335;&#27573;&#30340;&#24310;&#36831;&#20540;&#20316;&#20026;&#36793;&#26435;&#37325;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#36335;&#24452;&#35268;&#21010;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2308.12828</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21152;&#26435;&#22270;&#30340;&#30701;&#36884;&#20844;&#20132;&#32447;&#36335;&#35268;&#21010;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Short Run Transit Route Planning Decision Support System Using a Deep Learning-Based Weighted Graph. (arXiv:2308.12828v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12828
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30701;&#36884;&#20844;&#20132;&#32447;&#36335;&#35268;&#21010;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#36890;&#36807;&#35843;&#25972;&#36335;&#32447;&#30340;&#29305;&#23450;&#37096;&#20998;&#65292;&#20943;&#23569;&#26102;&#38388;&#24182;&#25552;&#21319;&#20844;&#20849;&#20132;&#36890;&#26381;&#21153;&#12290;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#28304;&#65292;&#36890;&#36807;&#39044;&#27979;&#36947;&#36335;&#27573;&#30340;&#24310;&#36831;&#20540;&#20316;&#20026;&#36793;&#26435;&#37325;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#36335;&#24452;&#35268;&#21010;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#20132;&#36890;&#36335;&#32447;&#35268;&#21010;&#22312;&#20132;&#36890;&#32593;&#32476;&#35774;&#35745;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#30830;&#20445;&#20056;&#23458;&#33719;&#24471;&#28385;&#24847;&#30340;&#26381;&#21153;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#36335;&#32447;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#36816;&#33829;&#30740;&#31350;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#23454;&#26045;&#36215;&#26469;&#32791;&#26102;&#65292;&#24182;&#19988;&#32570;&#20047;&#25552;&#20379;&#24555;&#36895;&#35299;&#20915;&#26041;&#26696;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#20351;&#20844;&#20849;&#20132;&#36890;&#35268;&#21010;&#32773;&#33021;&#22815;&#24555;&#36895;&#30830;&#23450;&#30701;&#26399;&#36335;&#32447;&#25913;&#36827;&#12290;&#36890;&#36807;&#22312;&#29305;&#23450;&#26102;&#38388;&#27573;&#20869;&#26080;&#32541;&#35843;&#25972;&#20004;&#20010;&#31449;&#28857;&#20043;&#38388;&#29305;&#23450;&#36335;&#27573;&#30340;&#36335;&#32447;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#20943;&#23569;&#26102;&#38388;&#24182;&#25552;&#21319;&#20844;&#20849;&#20132;&#36890;&#26381;&#21153;&#12290;&#21033;&#29992;GTFS&#21644;&#26234;&#33021;&#21345;&#25968;&#25454;&#31561;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#28304;&#65292;&#25105;&#20204;&#25552;&#21462;&#29305;&#24449;&#24182;&#23558;&#20132;&#36890;&#32593;&#32476;&#24314;&#27169;&#20026;&#26377;&#21521;&#22270;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#36947;&#36335;&#27573;&#30340;&#24310;&#36831;&#20540;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#24310;&#36831;&#20540;&#34987;&#29992;&#20316;&#20132;&#36890;&#22270;&#20013;&#30340;&#36793;&#26435;&#37325;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#36335;&#24452;&#35268;&#21010;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Public transport routing plays a crucial role in transit network design, ensuring a satisfactory level of service for passengers. However, current routing solutions rely on traditional operational research heuristics, which can be time-consuming to implement and lack the ability to provide quick solutions. Here, we propose a novel deep learning-based methodology for a decision support system that enables public transport (PT) planners to identify short-term route improvements rapidly. By seamlessly adjusting specific sections of routes between two stops during specific times of the day, our method effectively reduces times and enhances PT services. Leveraging diverse data sources such as GTFS and smart card data, we extract features and model the transportation network as a directed graph. Using self-supervision, we train a deep learning model for predicting lateness values for road segments.  These lateness values are then utilized as edge weights in the transportation graph, enabling
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#21518;&#32493;&#39564;&#35777;&#30340;&#27491;&#24335;&#27979;&#35797;&#31243;&#24207;&#65292;&#29992;&#20110;&#26816;&#27979;&#27169;&#22411;&#20998;&#37197;&#22266;&#23450;&#39044;&#27979;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#24320;&#21457;&#21487;&#38752;&#30340;&#26426;&#21046;&#65292;&#21487;&#20197;&#30830;&#23450;&#32473;&#23450;&#27169;&#22411;&#26159;&#21542;&#33021;&#20026;&#20915;&#31574;&#23545;&#35937;&#25552;&#20379;&#21518;&#32493;&#25514;&#26045;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#27169;&#22411;&#20998;&#37197;&#22266;&#23450;&#39044;&#27979;&#21487;&#33021;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20013;&#30830;&#20445;&#21518;&#32493;&#25514;&#26045;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#36151;&#27454;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#21518;&#32493;&#25514;&#26045;&#30340;&#19981;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12820</link><description>&lt;p&gt;
&#19981;&#25490;&#38500;&#39044;&#27979;&#65306;&#22522;&#20110;&#21487;&#36798;&#38598;&#30340;&#21518;&#32493;&#39564;&#35777;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prediction without Preclusion: Recourse Verification with Reachable Sets. (arXiv:2308.12820v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12820
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#21518;&#32493;&#39564;&#35777;&#30340;&#27491;&#24335;&#27979;&#35797;&#31243;&#24207;&#65292;&#29992;&#20110;&#26816;&#27979;&#27169;&#22411;&#20998;&#37197;&#22266;&#23450;&#39044;&#27979;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#24320;&#21457;&#21487;&#38752;&#30340;&#26426;&#21046;&#65292;&#21487;&#20197;&#30830;&#23450;&#32473;&#23450;&#27169;&#22411;&#26159;&#21542;&#33021;&#20026;&#20915;&#31574;&#23545;&#35937;&#25552;&#20379;&#21518;&#32493;&#25514;&#26045;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#27169;&#22411;&#20998;&#37197;&#22266;&#23450;&#39044;&#27979;&#21487;&#33021;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20013;&#30830;&#20445;&#21518;&#32493;&#25514;&#26045;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#36151;&#27454;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#21518;&#32493;&#25514;&#26045;&#30340;&#19981;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24120;&#34987;&#29992;&#20110;&#20915;&#23450;&#35841;&#26377;&#36164;&#26684;&#24471;&#21040;&#36151;&#27454;&#12289;&#38754;&#35797;&#25110;&#20844;&#20849;&#31119;&#21033;&#12290;&#26631;&#20934;&#25216;&#26415;&#29992;&#20110;&#26500;&#24314;&#36825;&#20123;&#27169;&#22411;&#26102;&#65292;&#20250;&#20351;&#29992;&#20851;&#20110;&#20154;&#30340;&#29305;&#24449;&#65292;&#20294;&#24573;&#35270;&#20182;&#20204;&#30340;&#21487;&#25805;&#20316;&#24615;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#20998;&#37197;&#22266;&#23450;&#30340;&#39044;&#27979;&#65292;&#36825;&#24847;&#21619;&#30528;&#34987;&#25298;&#32477;&#36151;&#27454;&#12289;&#38754;&#35797;&#25110;&#31119;&#21033;&#30340;&#28040;&#36153;&#32773;&#21487;&#33021;&#27704;&#20037;&#34987;&#25490;&#38500;&#22312;&#33719;&#24471;&#20449;&#36151;&#12289;&#23601;&#19994;&#25110;&#25588;&#21161;&#30340;&#26426;&#20250;&#20043;&#22806;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#27491;&#24335;&#30340;&#27979;&#35797;&#31243;&#24207;&#26469;&#26816;&#27979;&#20998;&#37197;&#22266;&#23450;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21518;&#32493;&#39564;&#35777;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#26426;&#21046;&#21487;&#38752;&#22320;&#30830;&#23450;&#32473;&#23450;&#27169;&#22411;&#26159;&#21542;&#33021;&#25552;&#20379;&#23545;&#20915;&#31574;&#23545;&#35937;&#30340;&#21518;&#32493;&#25163;&#27573;&#65292;&#36825;&#20123;&#25163;&#27573;&#30001;&#29992;&#25143;&#25351;&#23450;&#30340;&#21487;&#25805;&#20316;&#24615;&#32422;&#26463;&#30830;&#23450;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#24037;&#20855;&#22914;&#20309;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20013;&#30830;&#20445;&#21518;&#32493;&#25514;&#26045;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#30740;&#31350;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#36151;&#27454;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#21518;&#32493;&#25514;&#26045;&#30340;&#19981;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#27169;&#22411;&#22914;&#20309;&#26080;&#24847;&#20013;&#20998;&#37197;&#22266;&#23450;&#39044;&#27979;&#65292;&#20174;&#32780;&#27704;&#20037;&#31105;&#27490;&#20351;&#29992;&#32773;&#33719;&#24471;&#30456;&#20851;&#26435;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are often used to decide who will receive a loan, a job interview, or a public benefit. Standard techniques to build these models use features about people but overlook their actionability. In turn, models can assign predictions that are fixed, meaning that consumers who are denied loans, interviews, or benefits may be permanently locked out from access to credit, employment, or assistance. In this work, we introduce a formal testing procedure to flag models that assign fixed predictions that we call recourse verification. We develop machinery to reliably determine if a given model can provide recourse to its decision subjects from a set of user-specified actionability constraints. We demonstrate how our tools can ensure recourse and adversarial robustness in real-world datasets and use them to study the infeasibility of recourse in real-world lending datasets. Our results highlight how models can inadvertently assign fixed predictions that permanently bar acces
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#20998;&#26512;&#24202;&#36793;&#30417;&#27979;&#20135;&#29983;&#30340;&#26102;&#38388;&#25968;&#25454;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#29992;&#20110;&#39044;&#27979;ICU&#24739;&#32773;&#27515;&#20129;&#29575;&#21644;&#20303;&#38498;&#26102;&#38388;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.12800</link><description>&lt;p&gt;
ICU&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#36827;&#34892;&#27515;&#20129;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ICU Mortality Prediction Using Long Short-Term Memory Networks. (arXiv:2308.12800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12800
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#20998;&#26512;&#24202;&#36793;&#30417;&#27979;&#20135;&#29983;&#30340;&#26102;&#38388;&#25968;&#25454;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#29992;&#20110;&#39044;&#27979;ICU&#24739;&#32773;&#27515;&#20129;&#29575;&#21644;&#20303;&#38498;&#26102;&#38388;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151; (ICU) &#36827;&#34892;&#30340;&#22823;&#37327;&#24202;&#36793;&#30417;&#27979;&#20135;&#29983;&#20102;&#19982;&#24739;&#32773;&#29983;&#29702;&#30456;&#20851;&#30340;&#22797;&#26434;&#26102;&#38388;&#25968;&#25454;&#65292;&#36825;&#20026;&#20020;&#24202;&#25968;&#25454;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#32972;&#26223;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35782;&#21035;&#20986;&#36825;&#20123;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#21487;&#33021;&#20250;&#25552;&#20379;&#39640;&#24230;&#33021;&#21147;&#26469;&#39044;&#27979;&#20020;&#24202;&#20107;&#20214;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23454;&#26045;&#19968;&#20010;&#33258;&#21160;&#25968;&#25454;&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20998;&#26512;&#26469;&#33258;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405; (EHRs) &#30340;&#22823;&#37327;&#22810;&#21464;&#37327;&#26102;&#38388;&#25968;&#25454;&#65292;&#24182;&#25552;&#21462;&#39640;&#32423;&#20449;&#24687;&#20197;&#26089;&#26399;&#39044;&#27979;&#20303;&#38498;&#27515;&#20129;&#29575;&#21644;&#20303;&#38498;&#26102;&#38388;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#26102;&#38388;&#31383;&#21475;&#32553;&#30701;&#21040;6&#23567;&#26102;&#26469;&#30740;&#31350;LSTM&#32593;&#32476;&#30340;&#36866;&#29992;&#24615;&#65292;&#20197;&#22686;&#24378;&#20020;&#24202;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#31361;&#20986;&#20102;LSTM&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#27979;&#37327;&#26469;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#30340;&#39044;&#27979;&#24341;&#25806;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extensive bedside monitoring in Intensive Care Units (ICUs) has resulted in complex temporal data regarding patient physiology, which presents an upscale context for clinical data analysis. In the other hand, identifying the time-series patterns within these data may provide a high aptitude to predict clinical events. Hence, we investigate, during this work, the implementation of an automatic data-driven system, which analyzes large amounts of multivariate temporal data derived from Electronic Health Records (EHRs), and extracts high-level information so as to predict in-hospital mortality and Length of Stay (LOS) early. Practically, we investigate the applicability of LSTM network by reducing the time-frame to 6-hour so as to enhance clinical tasks. The experimental results highlight the efficiency of LSTM model with rigorous multivariate time-series measurements for building real-world prediction engines.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#24320;&#28304;&#30340;GitHub&#20179;&#24211;&#20026;&#26426;&#22120;&#35843;&#24230;&#38382;&#39064;&#25552;&#20379;&#20102;&#32508;&#21512;&#22522;&#20934;&#65292;&#21253;&#25324;&#22810;&#31181;&#29615;&#22659;&#21644;&#23454;&#20363;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#38598;&#20013;&#30340;&#20013;&#24515;&#12290;</title><link>http://arxiv.org/abs/2308.12794</link><description>&lt;p&gt;
&#24037;&#20316;&#36710;&#38388;&#35843;&#24230;&#22522;&#20934;&#65306;&#29992;&#20110;&#23398;&#20064;&#21644;&#38750;&#23398;&#20064;&#26041;&#27861;&#30340;&#29615;&#22659;&#21644;&#23454;&#20363;
&lt;/p&gt;
&lt;p&gt;
Job Shop Scheduling Benchmark: Environments and Instances for Learning and Non-learning Methods. (arXiv:2308.12794v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12794
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#24320;&#28304;&#30340;GitHub&#20179;&#24211;&#20026;&#26426;&#22120;&#35843;&#24230;&#38382;&#39064;&#25552;&#20379;&#20102;&#32508;&#21512;&#22522;&#20934;&#65292;&#21253;&#25324;&#22810;&#31181;&#29615;&#22659;&#21644;&#23454;&#20363;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#38598;&#20013;&#30340;&#20013;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;GitHub&#20179;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#24191;&#27867;&#30340;&#26426;&#22120;&#35843;&#24230;&#38382;&#39064;&#30340;&#32508;&#21512;&#22522;&#20934;&#65292;&#21253;&#25324;&#24037;&#20316;&#36710;&#38388;&#35843;&#24230;&#65288;JSP&#65289;&#65292;&#27969;&#27700;&#36710;&#38388;&#35843;&#24230;&#65288;FSP&#65289;&#65292;&#28789;&#27963;&#24037;&#20316;&#36710;&#38388;&#35843;&#24230;&#65288;FJSP&#65289;&#65292;&#20855;&#26377;&#35013;&#37197;&#32422;&#26463;&#30340;FJSP&#65288;FAJSP&#65289;&#65292;&#20855;&#26377;&#24207;&#21015;&#20381;&#36182;&#35774;&#32622;&#26102;&#38388;&#30340;FJSP&#65288;FJSP-SDST&#65289;&#21644;&#22312;&#32447;FJSP&#65288;&#22312;&#32447;&#20316;&#19994;&#21040;&#36798;&#65289;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20026;&#23545;&#26426;&#22120;&#35843;&#24230;&#25361;&#25112;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#20174;&#19994;&#32773;&#21644;&#29233;&#22909;&#32773;&#25552;&#20379;&#19968;&#20010;&#38598;&#20013;&#30340;&#20013;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an open-source GitHub repository containing comprehensive benchmarks for a wide range of machine scheduling problems, including Job Shop Scheduling (JSP), Flow Shop Scheduling (FSP), Flexible Job Shop Scheduling (FJSP), FJSP with Assembly constraints (FAJSP), FJSP with Sequence-Dependent Setup Times (FJSP-SDST), and the online FJSP (with online job arrivals). Our primary goal is to provide a centralized hub for researchers, practitioners, and enthusiasts interested in tackling machine scheduling challenges.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#27425;MC dropout&#36817;&#20284;&#26041;&#27861;&#65292;&#20197;&#23558;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#36125;&#21494;&#26031;&#21464;&#20307;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#19982;&#26222;&#36890;&#31070;&#32463;&#32593;&#32476;&#30456;&#21516;&#30340;&#35745;&#31639;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#36125;&#21494;&#26031;&#21464;&#20307;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#30340;&#19981;&#30830;&#23450;&#24230;&#27979;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.12785</link><description>&lt;p&gt;
&#21333;&#27425;&#36125;&#21494;&#26031;&#36817;&#20284;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Single-shot Bayesian approximation for neural networks. (arXiv:2308.12785v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12785
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#27425;MC dropout&#36817;&#20284;&#26041;&#27861;&#65292;&#20197;&#23558;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#36125;&#21494;&#26031;&#21464;&#20307;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#19982;&#26222;&#36890;&#31070;&#32463;&#32593;&#32476;&#30456;&#21516;&#30340;&#35745;&#31639;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#36125;&#21494;&#26031;&#21464;&#20307;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#30340;&#19981;&#30830;&#23450;&#24230;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20197;&#20854;&#39640;&#39044;&#27979;&#24615;&#33021;&#32780;&#38395;&#21517;&#12290;&#28982;&#32780;&#65292;&#24403;&#36935;&#21040;&#23436;&#20840;&#26032;&#30340;&#24773;&#20917;&#24182;&#19988;&#27809;&#26377;&#25351;&#31034;&#20854;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#31070;&#32463;&#32593;&#32476;&#24456;&#23481;&#26131;&#20135;&#29983;&#19981;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;&#36125;&#21494;&#26031;&#21464;&#20307;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#65292;&#22914;&#33945;&#29305;&#21345;&#27931;&#65288;MC&#65289;dropout BNNs&#65292;&#22312;&#25552;&#20379;&#19981;&#30830;&#23450;&#24230;&#27979;&#37327;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;BNNs&#21807;&#19968;&#30340;&#32570;&#28857;&#26159;&#23427;&#20204;&#22312;&#27979;&#35797;&#26102;&#35745;&#31639;&#26102;&#38388;&#36739;&#38271;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#19968;&#31181;&#37319;&#26679;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#27425;MC dropout&#36817;&#20284;&#65292;&#23427;&#20445;&#30041;&#20102;BNNs&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#19982;&#31070;&#32463;&#32593;&#32476;&#19968;&#26679;&#24555;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#30697;&#20256;&#25773;&#65288;MP&#65289;&#65292;&#21487;&#20197;&#22312;&#24120;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#65288;&#21367;&#31215;&#12289;&#26368;&#22823;&#27744;&#21270;&#12289;&#20840;&#36830;&#25509;&#12289;softmax&#21644;dropout&#23618;&#65289;&#20013;&#35299;&#26512;&#22320;&#36817;&#20284;MC dropout&#20449;&#21495;&#30340;&#26399;&#26395;&#20540;&#21644;&#26041;&#24046;&#12290;MP&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23558;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;BNN&#65292;&#21482;&#35201;NN&#24050;&#32463;&#20351;&#29992;&#26631;&#20934;&#30340;dropout&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (NNs) are known for their high-prediction performances. However, NNs are prone to yield unreliable predictions when encountering completely new situations without indicating their uncertainty. Bayesian variants of NNs (BNNs), such as Monte Carlo (MC) dropout BNNs, do provide uncertainty measures and simultaneously increase the prediction performance. The only disadvantage of BNNs is their higher computation time during test time because they rely on a sampling approach. Here we present a single-shot MC dropout approximation that preserves the advantages of BNNs while being as fast as NNs. Our approach is based on moment propagation (MP) and allows to analytically approximate the expected value and the variance of the MC dropout signal for commonly used layers in NNs, i.e. convolution, max pooling, dense, softmax, and dropout layers. The MP approach can convert an NN into a BNN without re-training given the NN has been trained with standard dropout. We evaluate our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#20013;&#32456;&#27490;&#29366;&#24577;&#19979;&#23384;&#22312;&#30340;&#22870;&#21169;&#35774;&#35745;&#38382;&#39064;&#65292;&#36890;&#36807;&#25925;&#24847;&#20302;&#20272;&#32456;&#27490;&#21518;&#30340;&#20540;&#26469;&#36991;&#20813;&#23398;&#20064;&#38169;&#35823;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2308.12772</link><description>&lt;p&gt;
&#36890;&#36807;&#25925;&#24847;&#20302;&#20272;&#32456;&#27490;&#29366;&#24577;&#30340;&#20540;&#20989;&#25968;&#26469;&#35299;&#20915;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#20013;&#30340;&#38169;&#35823;&#22870;&#21169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Intentionally-underestimated Value Function at Terminal State for Temporal-difference Learning with Mis-designed Reward. (arXiv:2308.12772v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#20013;&#32456;&#27490;&#29366;&#24577;&#19979;&#23384;&#22312;&#30340;&#22870;&#21169;&#35774;&#35745;&#38382;&#39064;&#65292;&#36890;&#36807;&#25925;&#24847;&#20302;&#20272;&#32456;&#27490;&#21518;&#30340;&#20540;&#26469;&#36991;&#20813;&#23398;&#20064;&#38169;&#35823;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#24050;&#32463;&#21464;&#24471;&#27969;&#34892;&#65292;&#20294;&#26159;&#30001;&#20110;&#23433;&#20840;&#21644;&#33410;&#32422;&#26102;&#38388;&#30340;&#21407;&#22240;&#65292;&#23398;&#20064;&#36807;&#31243;&#36890;&#24120;&#22312;&#19968;&#38598;&#30340;&#20013;&#36884;&#32456;&#27490;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#36825;&#31181;&#32456;&#27490;&#24773;&#20917;&#19979;&#26102;&#38388;&#24046;&#24322;&#65288;TD&#65289;&#23398;&#20064;&#25191;&#34892;&#30340;&#26368;&#24120;&#35265;&#24322;&#24120;&#22788;&#29702;&#38382;&#39064;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22312;&#32456;&#27490;&#21518;&#36890;&#36807;&#24378;&#21046;&#20551;&#35774;&#20540;&#20026;&#38646;&#65292;&#20250;&#24847;&#22806;&#22320;&#24341;&#36215;&#20302;&#20272;&#25110;&#39640;&#20272;&#65292;&#36825;&#21462;&#20915;&#20110;&#27491;&#24120;&#29366;&#24577;&#19979;&#30340;&#22870;&#21169;&#35774;&#35745;&#12290;&#24403;&#20219;&#21153;&#22833;&#36133;&#23548;&#33268;&#19968;&#38598;&#32456;&#27490;&#26102;&#65292;&#30001;&#20110;&#38750;&#39044;&#26399;&#30340;&#39640;&#20272;&#65292;&#22833;&#36133;&#21487;&#33021;&#34987;&#39640;&#24230;&#35780;&#20215;&#65292;&#24182;&#19988;&#21487;&#33021;&#33719;&#21462;&#38169;&#35823;&#30340;&#31574;&#30053;&#12290;&#34429;&#28982;&#36890;&#36807;&#27880;&#24847;&#22870;&#21169;&#35774;&#35745;&#21487;&#20197;&#36991;&#20813;&#27492;&#38382;&#39064;&#65292;&#20294;&#22312;&#23454;&#38469;&#20351;&#29992;&#20013;&#23457;&#26597;&#32456;&#27490;&#26102;&#30340;&#24322;&#24120;&#22788;&#29702;&#26159;TD&#23398;&#20064;&#30340;&#35201;&#28857;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25925;&#24847;&#20302;&#20272;&#32456;&#27490;&#21518;&#30340;&#20540;&#65292;&#20197;&#36991;&#20813;&#30001;&#20110;&#38750;&#39044;&#26399;&#30340;&#39640;&#20272;&#32780;&#23548;&#33268;&#30340;&#23398;&#20064;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot control using reinforcement learning has become popular, but its learning process generally terminates halfway through an episode for safety and time-saving reasons. This study addresses the problem of the most popular exception handling that temporal-difference (TD) learning performs at such termination. That is, by forcibly assuming zero value after termination, unintentionally implicit underestimation or overestimation occurs, depending on the reward design in the normal states. When the episode is terminated due to task failure, the failure may be highly valued with the unintentional overestimation, and the wrong policy may be acquired. Although this problem can be avoided by paying attention to the reward design, it is essential in practical use of TD learning to review the exception handling at termination. This paper therefore proposes a method to intentionally underestimate the value after termination to avoid learning failures due to the unintentional overestimation. In 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#24179;&#22343;&#23884;&#20837;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#24179;&#22343;&#23884;&#20837;&#22312;&#25512;&#33616;&#20013;&#19968;&#33268;&#24615;&#36739;&#20302;&#65292;&#20026;&#36827;&#19968;&#27493;&#25913;&#36827;&#29616;&#23454;&#19990;&#30028;&#23884;&#20837;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.12767</link><description>&lt;p&gt;
&#20851;&#20110;&#24179;&#22343;&#23884;&#20837;&#29992;&#20110;&#29289;&#21697;&#25512;&#33616;&#30340;&#19968;&#33268;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Consistency of Average Embeddings for Item Recommendation. (arXiv:2308.12767v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#24179;&#22343;&#23884;&#20837;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#24179;&#22343;&#23884;&#20837;&#22312;&#25512;&#33616;&#20013;&#19968;&#33268;&#24615;&#36739;&#20302;&#65292;&#20026;&#36827;&#19968;&#27493;&#25913;&#36827;&#29616;&#23454;&#19990;&#30028;&#23884;&#20837;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#20570;&#27861;&#26159;&#23558;&#29289;&#21697;&#23884;&#20837;&#36827;&#34892;&#24179;&#22343;&#20197;&#22312;&#21516;&#19968;&#23884;&#20837;&#31354;&#38388;&#20013;&#20195;&#34920;&#29992;&#25143;&#25110;&#26356;&#39640;&#32423;&#30340;&#27010;&#24565;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#31181;&#20570;&#27861;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26399;&#26395;&#31934;&#24230;&#20998;&#25968;&#65292;&#29992;&#20110;&#34913;&#37327;&#24179;&#22343;&#23884;&#20837;&#19982;&#20854;&#26500;&#24314;&#25152;&#20351;&#29992;&#30340;&#29289;&#21697;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#38543;&#21518;&#22312;&#20855;&#26377;&#29305;&#23450;&#20551;&#35774;&#30340;&#29702;&#35770;&#29615;&#22659;&#21644;&#26469;&#33258;&#38899;&#20048;&#27969;&#23186;&#20307;&#26381;&#21153;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#20998;&#26512;&#20102;&#35813;&#20998;&#25968;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#21450;&#20854;&#32463;&#39564;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#29616;&#23454;&#19990;&#30028;&#30340;&#24179;&#22343;&#20540;&#22312;&#25512;&#33616;&#20013;&#30340;&#19968;&#33268;&#24615;&#36739;&#20302;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#26356;&#22909;&#22320;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#23884;&#20837;&#19982;&#25105;&#20204;&#29702;&#35770;&#29615;&#22659;&#30340;&#20551;&#35774;&#30456;&#19968;&#33268;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
A prevalent practice in recommender systems consists of averaging item embeddings to represent users or higher-level concepts in the same embedding space. This paper investigates the relevance of such a practice. For this purpose, we propose an expected precision score, designed to measure the consistency of an average embedding relative to the items used for its construction. We subsequently analyze the mathematical expression of this score in a theoretical setting with specific assumptions, as well as its empirical behavior on real-world data from music streaming services. Our results emphasize that real-world averages are less consistent for recommendation, which paves the way for future research to better align real-world embeddings with assumptions from our theoretical setting.
&lt;/p&gt;</description></item><item><title>IP-UNet&#26159;&#19968;&#31181;&#29992;&#20110;3D&#21307;&#23398;&#20307;&#31215;&#20998;&#21106;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;3D&#20307;&#31215;&#25968;&#25454;&#30340;&#24378;&#24230;&#25237;&#24433;&#36827;&#34892;&#22810;&#31867;&#21035;&#20998;&#21106;&#65292;&#22312;&#20445;&#30041;&#21407;&#22987;&#20998;&#36776;&#29575;&#30340;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2308.12761</link><description>&lt;p&gt;
IP-UNet&#65306;&#29992;&#20110;3D&#21307;&#23398;&#20307;&#31215;&#20998;&#21106;&#30340;&#24378;&#24230;&#25237;&#24433;UNet&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
IP-UNet: Intensity Projection UNet Architecture for 3D Medical Volume Segmentation. (arXiv:2308.12761v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12761
&lt;/p&gt;
&lt;p&gt;
IP-UNet&#26159;&#19968;&#31181;&#29992;&#20110;3D&#21307;&#23398;&#20307;&#31215;&#20998;&#21106;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;3D&#20307;&#31215;&#25968;&#25454;&#30340;&#24378;&#24230;&#25237;&#24433;&#36827;&#34892;&#22810;&#31867;&#21035;&#20998;&#21106;&#65292;&#22312;&#20445;&#30041;&#21407;&#22987;&#20998;&#36776;&#29575;&#30340;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CNN&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#20869;&#23384;&#23481;&#37327;&#26159;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;3D&#20307;&#31215;&#25968;&#25454;&#30340;&#24120;&#35265;&#32570;&#28857;&#20043;&#19968;&#12290;&#36890;&#24120;&#22312;&#22788;&#29702;&#20043;&#21069;&#38656;&#35201;&#23545;3D&#20307;&#31215;&#36827;&#34892;&#35009;&#21098;&#25110;&#32553;&#23567;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20998;&#36776;&#29575;&#38477;&#20302;&#65292;&#22686;&#21152;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#24182;&#24433;&#21709;&#20998;&#21106;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;IP-UNet&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;IP-UNet&#26159;&#19968;&#31181;&#22522;&#20110;UNet&#30340;&#27169;&#22411;&#65292;&#23427;&#23545;3D&#20307;&#31215;&#25968;&#25454;&#30340;&#24378;&#24230;&#25237;&#24433;&#65288;IP&#65289;&#36827;&#34892;&#22810;&#31867;&#21035;&#20998;&#21106;&#65292;&#32780;&#19981;&#26159;&#21344;&#29992;&#22823;&#37327;&#20869;&#23384;&#30340;3D&#20307;&#31215;&#12290;IP-UNet&#21033;&#29992;&#26377;&#38480;&#30340;&#20869;&#23384;&#33021;&#21147;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21407;&#22987;&#30340;3D&#22270;&#20687;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#21106;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#27604;&#36739;&#20102;&#19977;&#31181;&#27169;&#22411;&#30340;&#24615;&#33021;&#65306;1&#65289;&#20351;&#29992;&#20256;&#32479;&#30340;2D UNet&#27169;&#22411;&#23545;CT&#25195;&#25551;&#22270;&#20687;&#36827;&#34892;&#36880;&#23618;2D&#20998;&#21106;&#12290;2&#65289;&#20351;&#29992;IP-UNet&#23545;&#25552;&#21462;&#30340;&#26368;&#22823;&#24378;&#24230;&#25237;&#24433;&#36827;&#34892;&#25968;&#25454;&#34701;&#21512;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
CNNs have been widely applied for medical image analysis. However, limited memory capacity is one of the most common drawbacks of processing high-resolution 3D volumetric data. 3D volumes are usually cropped or downsized first before processing, which can result in a loss of resolution, increase class imbalance, and affect the performance of the segmentation algorithms. In this paper, we propose an end-to-end deep learning approach called IP-UNet. IP-UNet is a UNet-based model that performs multi-class segmentation on Intensity Projection (IP) of 3D volumetric data instead of the memory-consuming 3D volumes. IP-UNet uses limited memory capability for training without losing the original 3D image resolution. We compare the performance of three models in terms of segmentation accuracy and computational cost: 1) Slice-by-slice 2D segmentation of the CT scan images using a conventional 2D UNet model. 2) IP-UNet that operates on data obtained by merging the extracted Maximum Intensity Proje
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#30456;&#20301;&#27969;&#24418;&#30340;&#21160;&#20316;&#25554;&#24103;&#31995;&#32479;&#65292;&#36890;&#36807;&#23398;&#20064;&#30456;&#20301;&#21464;&#37327;&#21644;&#28151;&#21512;&#19987;&#23478;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#29983;&#25104;&#30446;&#26631;&#23039;&#21183;&#20043;&#38388;&#30340;&#36830;&#32493;&#23039;&#21183;&#24207;&#21015;&#65292;&#21516;&#26102;&#21487;&#20197;&#28385;&#36275;&#21160;&#30011;&#24072;&#25163;&#21160;&#20462;&#25913;&#30340;&#23039;&#21183;&#21644;&#26411;&#31471;&#25928;&#24212;&#22120;&#20316;&#20026;&#32422;&#26463;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.12751</link><description>&lt;p&gt;
&#20351;&#29992;&#30456;&#20301;&#27969;&#24418;&#30340;&#21160;&#20316;&#25554;&#24103;
&lt;/p&gt;
&lt;p&gt;
Motion In-Betweening with Phase Manifolds. (arXiv:2308.12751v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#30456;&#20301;&#27969;&#24418;&#30340;&#21160;&#20316;&#25554;&#24103;&#31995;&#32479;&#65292;&#36890;&#36807;&#23398;&#20064;&#30456;&#20301;&#21464;&#37327;&#21644;&#28151;&#21512;&#19987;&#23478;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#29983;&#25104;&#30446;&#26631;&#23039;&#21183;&#20043;&#38388;&#30340;&#36830;&#32493;&#23039;&#21183;&#24207;&#21015;&#65292;&#21516;&#26102;&#21487;&#20197;&#28385;&#36275;&#21160;&#30011;&#24072;&#25163;&#21160;&#20462;&#25913;&#30340;&#23039;&#21183;&#21644;&#26411;&#31471;&#25928;&#24212;&#22120;&#20316;&#20026;&#32422;&#26463;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#21160;&#20316;&#25554;&#24103;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#21608;&#26399;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#30340;&#30456;&#20301;&#21464;&#37327;&#26469;&#36798;&#21040;&#35282;&#33394;&#30340;&#30446;&#26631;&#23039;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#28151;&#21512;&#19987;&#23478;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20854;&#20013;&#30456;&#20301;&#20197;&#19981;&#21516;&#30340;&#19987;&#23478;&#26435;&#37325;&#23558;&#21160;&#20316;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#36827;&#34892;&#32858;&#31867;&#12290;&#27599;&#32452;&#29983;&#25104;&#30340;&#26435;&#37325;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#22312;&#24403;&#21069;&#29366;&#24577;&#21644;&#30446;&#26631;&#29366;&#24577;&#20043;&#38388;&#20135;&#29983;&#19968;&#31995;&#21015;&#23039;&#21183;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#28385;&#36275;&#21160;&#30011;&#24072;&#25163;&#21160;&#20462;&#25913;&#30340;&#23039;&#21183;&#25110;&#26576;&#20123;&#26411;&#31471;&#25928;&#24212;&#22120;&#20316;&#20026;&#21160;&#30011;&#35201;&#36798;&#21040;&#30340;&#32422;&#26463;&#65292;&#23454;&#26045;&#20102;&#19968;&#31181;&#23398;&#20064;&#30340;&#21452;&#21521;&#25511;&#21046;&#26041;&#26696;&#26469;&#28385;&#36275;&#36825;&#20123;&#32422;&#26463;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#30456;&#20301;&#36827;&#34892;&#21160;&#20316;&#25554;&#24103;&#21487;&#20197;&#25552;&#39640;&#25554;&#20540;&#21160;&#20316;&#30340;&#38160;&#24230;&#65292;&#24182;&#36827;&#19968;&#27493;&#31283;&#23450;&#23398;&#20064;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#30456;&#20301;&#36827;&#34892;&#21160;&#20316;&#25554;&#24103;&#36824;&#21487;&#20197;&#21512;&#25104;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#36816;&#21160;&#65292;&#36229;&#36234;&#20102;&#34892;&#36208;&#31561;&#22522;&#26412;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel data-driven motion in-betweening system to reach target poses of characters by making use of phases variables learned by a Periodic Autoencoder. Our approach utilizes a mixture-of-experts neural network model, in which the phases cluster movements in both space and time with different expert weights. Each generated set of weights then produces a sequence of poses in an autoregressive manner between the current and target state of the character. In addition, to satisfy poses which are manually modified by the animators or where certain end effectors serve as constraints to be reached by the animation, a learned bi-directional control scheme is implemented to satisfy such constraints. The results demonstrate that using phases for motion in-betweening tasks sharpen the interpolated movements, and furthermore stabilizes the learning process. Moreover, using phases for motion in-betweening tasks can also synthesize more challenging movements beyond locomotion b
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#22522;&#22240;&#32452;&#35268;&#27169;&#20195;&#35874;&#32593;&#32476;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;(ILP)&#26694;&#26550;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#20174;&#23454;&#39564;&#20013;&#23398;&#20064;&#26032;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#20197;&#26377;&#25928;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#21644;&#25351;&#23548;&#23454;&#39564;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.12740</link><description>&lt;p&gt;
&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#22522;&#22240;&#32452;&#35268;&#27169;&#20195;&#35874;&#32593;&#32476;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Human Comprehensible Active Learning of Genome-Scale Metabolic Networks. (arXiv:2308.12740v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12740
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#22522;&#22240;&#32452;&#35268;&#27169;&#20195;&#35874;&#32593;&#32476;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;(ILP)&#26694;&#26550;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#20174;&#23454;&#39564;&#20013;&#23398;&#20064;&#26032;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#20197;&#26377;&#25928;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#21644;&#25351;&#23548;&#23454;&#39564;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#29983;&#29289;&#23398;&#30340;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#26159;&#23558;&#23487;&#20027;&#32454;&#32990;&#31995;&#32479;&#24037;&#31243;&#21270;&#20197;&#20135;&#29983;&#26377;&#29992;&#30340;&#20135;&#21697;&#12290;&#28982;&#32780;&#65292;&#23487;&#20027;&#31995;&#32479;&#35268;&#27169;&#30340;&#22686;&#21152;&#23548;&#33268;&#35774;&#35745;&#31354;&#38388;&#24040;&#22823;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#39640;&#26114;&#30340;&#39564;&#35777;&#35797;&#39564;&#12290;&#20026;&#20102;&#23487;&#20027;&#32454;&#32990;&#31995;&#32479;&#30340;&#35774;&#35745;-&#26500;&#24314;-&#27979;&#35797;-&#23398;&#20064;&#65288;Design-Build-Test-Learn&#65292;DBTL&#65289;&#21608;&#26399;&#65292;&#36843;&#20999;&#38656;&#35201;&#19968;&#31181;&#33021;&#26377;&#25928;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#24182;&#25351;&#23548;&#23454;&#39564;&#35774;&#35745;&#30340;&#21487;&#29702;&#35299;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#30340;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;ILP-iML1515&#65292;&#23427;&#36890;&#36807;&#35825;&#23548;&#36923;&#36753;&#25512;&#29702;&#21644;&#20174;&#35757;&#32451;&#23454;&#20363;&#20013;&#31215;&#26497;&#23398;&#20064;&#26469;&#25191;&#34892;&#35828;&#26126;&#24615;&#30340;&#36923;&#36753;&#25512;&#29702;&#12290;&#19982;&#25968;&#20540;&#27169;&#22411;&#19981;&#21516;&#65292;ILP-iML1515&#24314;&#31435;&#22312;&#23545;&#22522;&#22240;&#32452;&#35268;&#27169;&#20195;&#35874;&#27169;&#22411;&#30340;&#21487;&#29702;&#35299;&#30340;&#36923;&#36753;&#34920;&#31034;&#19978;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#20174;&#32570;&#20047;&#33829;&#20859;&#30340;&#31361;&#21464;&#20307;&#35797;&#39564;&#20013;&#23398;&#20064;&#26032;&#30340;&#36923;&#36753;&#32467;&#26500;&#26469;&#26356;&#26032;&#27169;&#22411;&#12290;ILP-iML1515&#26694;&#26550;&#20855;&#26377;&#39640;&#36890;&#37327;&#27169;&#25311;&#33021;&#21147;&#65292;&#24182;&#33021;&#20027;&#21160;&#36873;&#25321;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important application of Synthetic Biology is the engineering of the host cell system to yield useful products. However, an increase in the scale of the host system leads to huge design space and requires a large number of validation trials with high experimental costs. A comprehensible machine learning approach that efficiently explores the hypothesis space and guides experimental design is urgently needed for the Design-Build-Test-Learn (DBTL) cycle of the host cell system. We introduce a novel machine learning framework ILP-iML1515 based on Inductive Logic Programming (ILP) that performs abductive logical reasoning and actively learns from training examples. In contrast to numerical models, ILP-iML1515 is built on comprehensible logical representations of a genome-scale metabolic model and can update the model by learning new logical structures from auxotrophic mutant trials. The ILP-iML1515 framework 1) allows high-throughput simulations and 2) actively selects experiments that 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21019;&#24314;&#20102;DEEP-VOICE&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#20102;&#23454;&#26102;&#26816;&#27979;AI&#29983;&#25104;&#35821;&#38899;&#30340;&#30446;&#26631;&#65292;&#20197;&#24212;&#23545;DeepFake&#35821;&#38899;&#36716;&#25442;&#24102;&#26469;&#30340;&#36947;&#24503;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12734</link><description>&lt;p&gt;
&#23454;&#26102;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#35821;&#38899;&#29992;&#20110;DeepFake&#35821;&#38899;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion. (arXiv:2308.12734v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12734
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21019;&#24314;&#20102;DEEP-VOICE&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#20102;&#23454;&#26102;&#26816;&#27979;AI&#29983;&#25104;&#35821;&#38899;&#30340;&#30446;&#26631;&#65292;&#20197;&#24212;&#23545;DeepFake&#35821;&#38899;&#36716;&#25442;&#24102;&#26469;&#30340;&#36947;&#24503;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#38899;&#39046;&#22495;&#20013;&#65292;&#29983;&#25104;&#22411;AI&#25216;&#26415;&#20351;&#24471;&#35821;&#38899;&#20811;&#38534;&#21644;&#23454;&#26102;&#35821;&#38899;&#36716;&#25442;&#25104;&#20026;&#21487;&#33021;&#65292;&#36825;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#28508;&#22312;&#30340;&#36947;&#24503;&#38382;&#39064;&#65292;&#21253;&#25324;&#38544;&#31169;&#20405;&#29359;&#21644;&#34394;&#20551;&#38472;&#36848;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20127;&#38656;&#19968;&#31181;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;AI&#29983;&#25104;&#35821;&#38899;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;DeepFake&#35821;&#38899;&#36716;&#25442;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;DEEP-VOICE&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#20843;&#20301;&#30693;&#21517;&#20154;&#29289;&#30340;&#30495;&#23454;&#35821;&#38899;&#21644;&#20182;&#20204;&#20043;&#38388;&#30456;&#20114;&#36716;&#25442;&#21518;&#30340;&#35821;&#38899;&#12290;&#36890;&#36807;&#23545;&#35821;&#38899;&#30495;&#23454;&#24615;&#36827;&#34892;&#20108;&#20998;&#31867;&#65292;&#36890;&#36807;t&#26816;&#39564;&#23545;&#26102;&#38388;&#38899;&#39057;&#29305;&#24449;&#36827;&#34892;&#20102;&#32479;&#35745;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#20998;&#24067;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#20351;&#29992;&#36229;&#21442;&#25968;&#20248;&#21270;&#26469;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#35782;&#21035;&#35821;&#38899;&#30340;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are growing implications surrounding generative AI in the speech domain that enable voice cloning and real-time voice conversion from one individual to another. This technology poses a significant ethical threat and could lead to breaches of privacy and misrepresentation, thus there is an urgent need for real-time detection of AI-generated speech for DeepFake Voice Conversion. To address the above emerging issues, the DEEP-VOICE dataset is generated in this study, comprised of real human speech from eight well-known figures and their speech converted to one another using Retrieval-based Voice Conversion. Presenting as a binary classification problem of whether the speech is real or AI-generated, statistical analysis of temporal audio features through t-testing reveals that there are significantly different distributions. Hyperparameter optimisation is implemented for machine learning models to identify the source of speech. Following the training of 208 individual machine learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#26694;&#26550;ExpLTV&#65292;&#21033;&#29992;&#28216;&#25103;&#40120;&#40060;&#30340;&#26816;&#27979;&#26469;&#25913;&#36827;&#23458;&#25143;&#32456;&#36523;&#20215;&#20540;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12729</link><description>&lt;p&gt;
&#29420;&#29305;&#24605;&#32500;&#65306;&#36890;&#36807;&#19987;&#23478;&#36335;&#24452;&#36873;&#25321;&#21644;&#28216;&#25103;&#40120;&#40060;&#26816;&#27979;&#25913;&#36827;&#23458;&#25143;&#32456;&#36523;&#20215;&#20540;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Out of the Box Thinking: Improving Customer Lifetime Value Modelling via Expert Routing and Game Whale Detection. (arXiv:2308.12729v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#26694;&#26550;ExpLTV&#65292;&#21033;&#29992;&#28216;&#25103;&#40120;&#40060;&#30340;&#26816;&#27979;&#26469;&#25913;&#36827;&#23458;&#25143;&#32456;&#36523;&#20215;&#20540;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23458;&#25143;&#32456;&#36523;&#20215;&#20540;&#65288;LTV&#65289;&#39044;&#27979;&#23545;&#20110;&#35797;&#22270;&#26681;&#25454;&#20272;&#35745;&#20215;&#20540;&#20248;&#21270;&#24191;&#21578;&#25237;&#36164;&#30340;&#31227;&#21160;&#28216;&#25103;&#21457;&#34892;&#21830;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#31227;&#21160;&#28216;&#25103;&#20013;&#65292;&#37096;&#32626;&#24494;&#20132;&#26131;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36135;&#24065;&#21270;&#31574;&#30053;&#65292;&#21560;&#24341;&#20102;&#19968;&#23567;&#32676;&#22312;&#28216;&#25103;&#20869;&#36141;&#20080;&#19978;&#22823;&#37327;&#28040;&#36153;&#30340;&#28216;&#25103;&#40120;&#40060;&#12290;&#36825;&#31181;&#28216;&#25103;&#40120;&#40060;&#30340;&#23384;&#22312;&#21487;&#33021;&#38459;&#30861;&#29616;&#26377;LTV&#39044;&#27979;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#65292;&#22240;&#20026;&#28216;&#25103;&#40120;&#40060;&#30340;&#36141;&#20080;&#34892;&#20026;&#24635;&#26159;&#34920;&#29616;&#20986;&#19982;&#26222;&#36890;&#29992;&#25143;&#19981;&#21516;&#30340;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#28216;&#25103;&#40120;&#40060;&#21487;&#20197;&#20026;&#25913;&#36827;LTV&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25171;&#24320;&#26032;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;LTV&#39044;&#27979;&#20013;&#24212;&#29992;&#28216;&#25103;&#40120;&#40060;&#26816;&#27979;&#30340;&#30740;&#31350;&#24456;&#23569;&#65292;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#38024;&#23545;&#20551;&#35774;&#21487;&#33719;&#24471;&#39640;&#36136;&#37327;&#29992;&#25143;&#29305;&#24449;&#30340;&#38271;&#26399;LTV&#39044;&#27979;&#65292;&#36825;&#22312;&#29992;&#25143;&#33719;&#21462;&#38454;&#27573;&#19981;&#36866;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#26694;&#26550;ExpLTV&#12290;
&lt;/p&gt;
&lt;p&gt;
Customer lifetime value (LTV) prediction is essential for mobile game publishers trying to optimize the advertising investment for each user acquisition based on the estimated worth. In mobile games, deploying microtransactions is a simple yet effective monetization strategy, which attracts a tiny group of game whales who splurge on in-game purchases. The presence of such game whales may impede the practicality of existing LTV prediction models, since game whales' purchase behaviours always exhibit varied distribution from general users. Consequently, identifying game whales can open up new opportunities to improve the accuracy of LTV prediction models. However, little attention has been paid to applying game whale detection in LTV prediction, and existing works are mainly specialized for the long-term LTV prediction with the assumption that the high-quality user features are available, which is not applicable in the UA stage. In this paper, we propose ExpLTV, a novel multi-task framew
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;&#38590;&#24230;&#35843;&#25972;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#35270;&#35273;&#24037;&#20316;&#35760;&#24518;&#28216;&#25103;&#20013;&#30340;&#22797;&#26434;&#38590;&#24230;&#35760;&#24518;&#38382;&#39064;&#12290;&#36890;&#36807;&#26681;&#25454;&#29609;&#23478;&#30340;&#24471;&#20998;&#21644;&#19978;&#19968;&#36718;&#28216;&#25103;&#30340;&#38590;&#24230;&#37327;&#24230;&#26469;&#35843;&#25972;&#28216;&#25103;&#38590;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;52&#20301;&#21463;&#35797;&#32773;&#30340;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.12726</link><description>&lt;p&gt;
&#22522;&#20110;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#30340;&#35270;&#35273;&#24037;&#20316;&#35760;&#24518;&#28216;&#25103;&#21160;&#24577;&#38590;&#24230;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Continuous Reinforcement Learning-based Dynamic Difficulty Adjustment in a Visual Working Memory Game. (arXiv:2308.12726v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;&#38590;&#24230;&#35843;&#25972;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#35270;&#35273;&#24037;&#20316;&#35760;&#24518;&#28216;&#25103;&#20013;&#30340;&#22797;&#26434;&#38590;&#24230;&#35760;&#24518;&#38382;&#39064;&#12290;&#36890;&#36807;&#26681;&#25454;&#29609;&#23478;&#30340;&#24471;&#20998;&#21644;&#19978;&#19968;&#36718;&#28216;&#25103;&#30340;&#38590;&#24230;&#37327;&#24230;&#26469;&#35843;&#25972;&#28216;&#25103;&#38590;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;52&#20301;&#21463;&#35797;&#32773;&#30340;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#38590;&#24230;&#35843;&#25972;&#65288;DDA&#65289;&#26159;&#25552;&#21319;&#29609;&#23478;&#22312;&#35270;&#39057;&#28216;&#25103;&#20013;&#20307;&#39564;&#30340;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#24050;&#34987;&#24212;&#29992;&#20110;&#38750;&#31454;&#20105;&#24615;&#28216;&#25103;&#30340;DDA&#65307;&#23613;&#31649;&#22914;&#27492;&#65292;&#23427;&#20204;&#20165;&#20381;&#36182;&#20110;&#20855;&#26377;&#23567;&#30340;&#25628;&#32034;&#31354;&#38388;&#30340;&#31163;&#25955;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#30340;DDA&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#35270;&#35273;&#24037;&#20316;&#35760;&#24518;&#65288;VWM&#65289;&#28216;&#25103;&#20013;&#38590;&#24230;&#35760;&#24518;&#30340;&#22797;&#26434;&#25628;&#32034;&#31354;&#38388;&#12290;&#35813;&#25552;&#20986;&#30340;&#22522;&#20110;RL&#30340;DDA&#26681;&#25454;&#29609;&#23478;&#30340;&#24471;&#20998;&#21644;&#19978;&#19968;&#36718;&#28216;&#25103;&#30340;&#38590;&#24230;&#37327;&#24230;&#26469;&#35843;&#25972;&#28216;&#25103;&#38590;&#24230;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#36830;&#32493;&#30340;&#38590;&#24230;&#35760;&#24518;&#24230;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#38590;&#24230;&#21644;&#38590;&#24230;-&#24471;&#20998;&#21521;&#37327;&#20998;&#21035;&#20316;&#20026;RL&#30340;&#21160;&#20316;&#21644;&#29366;&#24577;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#28041;&#21450;52&#20301;&#21463;&#35797;&#32773;&#30340;&#34987;&#35797;&#20869;&#23454;&#39564;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#22312;&#29609;&#23478;&#30340;&#24471;&#20998;&#21644;&#28216;&#25103;&#20307;&#39564;&#24230;&#37327;&#26041;&#38754;&#65292;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20004;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#38590;&#24230;&#35843;&#25972;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic Difficulty Adjustment (DDA) is a viable approach to enhance a player's experience in video games. Recently, Reinforcement Learning (RL) methods have been employed for DDA in non-competitive games; nevertheless, they rely solely on discrete state-action space with a small search space. In this paper, we propose a continuous RL-based DDA methodology for a visual working memory (VWM) game to handle the complex search space for the difficulty of memorization. The proposed RL-based DDA tailors game difficulty based on the player's score and game difficulty in the last trial. We defined a continuous metric for the difficulty of memorization. Then, we consider the task difficulty and the vector of difficulty-score as the RL's action and state, respectively. We evaluated the proposed method through a within-subject experiment involving 52 subjects. The proposed approach was compared with two rule-based difficulty adjustment methods in terms of player's score and game experience measure
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#35299;&#20915;&#25509;&#35302;&#21147;&#23398;&#30340;&#21069;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#65292;&#24182;&#23558;&#19981;&#31561;&#24335;&#32422;&#26463;&#36716;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#20013;&#30340;&#36719;&#32422;&#26463;&#12290;&#23454;&#39564;&#35777;&#26126;PINNs&#21487;&#20197;&#20316;&#20026;PDE&#27714;&#35299;&#22120;&#12289;&#25968;&#25454;&#22686;&#24378;&#30340;&#21069;&#21521;&#27169;&#22411;&#21644;&#21453;&#21521;&#27714;&#35299;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.12716</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#25509;&#35302;&#21147;&#23398;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Forward and Inverse Problems of Contact Mechanics using Physics-Informed Neural Networks. (arXiv:2308.12716v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#35299;&#20915;&#25509;&#35302;&#21147;&#23398;&#30340;&#21069;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#65292;&#24182;&#23558;&#19981;&#31561;&#24335;&#32422;&#26463;&#36716;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#20013;&#30340;&#36719;&#32422;&#26463;&#12290;&#23454;&#39564;&#35777;&#26126;PINNs&#21487;&#20197;&#20316;&#20026;PDE&#27714;&#35299;&#22120;&#12289;&#25968;&#25454;&#22686;&#24378;&#30340;&#21069;&#21521;&#27169;&#22411;&#21644;&#21453;&#21521;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#23567;&#21464;&#24418;&#24377;&#24615;&#25509;&#35302;&#21147;&#23398;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#28151;&#21512;&#21464;&#37327;&#24418;&#24335;&#30340;PINNs&#65292;&#24182;&#36890;&#36807;&#36755;&#20986;&#21464;&#25442;&#22686;&#24378;&#20854;&#33021;&#22815;&#24378;&#21046;&#25191;&#34892;&#36842;&#37324;&#20999;&#29305;&#21644;&#35834;&#20381;&#26364;&#36793;&#30028;&#26465;&#20214;&#20316;&#20026;&#30828;&#32422;&#26463;&#12290;&#25509;&#35302;&#38382;&#39064;&#30340;&#19981;&#31561;&#24335;&#32422;&#26463;&#65288;&#21363; Karush-Kuhn-Tucker (KKT) &#31867;&#22411;&#26465;&#20214;&#65289;&#36890;&#36807;&#23558;&#20854;&#32435;&#20837;&#32593;&#32476;&#35757;&#32451;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#20316;&#20026;&#36719;&#32422;&#26463;&#26469;&#24378;&#21046;&#25191;&#34892;&#12290;&#20026;&#20102;&#24418;&#25104;KKT&#32422;&#26463;&#30340;&#25439;&#22833;&#20989;&#25968;&#36129;&#29486;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#26377;&#30340;&#29992;&#20110;&#24377;&#22609;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#20114;&#34917;&#38382;&#39064;&#65288;NCP&#65289;&#20989;&#25968;&#65292;&#21363; Fischer-Burmeister &#20989;&#25968;&#65292;&#23427;&#22312;&#20248;&#21270;&#26041;&#38754;&#20855;&#26377;&#26377;&#21033;&#30340;&#29305;&#24615;&#12290;&#22522;&#20110;&#36203;&#20857;&#25509;&#35302;&#38382;&#39064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PINNs&#21487;&#20197;&#20316;&#20026;&#32431;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#27714;&#35299;&#22120;&#12289;&#25968;&#25454;&#22686;&#24378;&#30340;&#21069;&#21521;&#27169;&#22411;&#21644;&#21453;&#21521;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the ability of physics-informed neural networks (PINNs) to solve forward and inverse problems of contact mechanics for small deformation elasticity. We deploy PINNs in a mixed-variable formulation enhanced by output transformation to enforce Dirichlet and Neumann boundary conditions as hard constraints. Inequality constraints of contact problems, namely Karush-Kuhn-Tucker (KKT) type conditions, are enforced as soft constraints by incorporating them into the loss function during network training. To formulate the loss function contribution of KKT constraints, existing approaches applied to elastoplasticity problems are investigated and we explore a nonlinear complementarity problem (NCP) function, namely Fischer-Burmeister, which possesses advantageous characteristics in terms of optimization. Based on the Hertzian contact problem, we show that PINNs can serve as pure partial differential equation (PDE) solver, as data-enhanced forward model, as inverse solver for pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25299;&#25169;&#25439;&#22833;&#23454;&#29616;&#35299;&#32544;&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#29992;&#20110;&#35299;&#32544;&#30340;&#21487;&#24494;&#25299;&#25169;&#25439;&#22833;&#30340;&#35770;&#25991;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#26032;&#32467;&#26524;&#25913;&#36827;&#20102;&#35299;&#32544;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.12696</link><description>&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#23398;&#20064;&#23454;&#29616;&#35299;&#32544;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Disentanglement Learning via Topology. (arXiv:2308.12696v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25299;&#25169;&#25439;&#22833;&#23454;&#29616;&#35299;&#32544;&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#29992;&#20110;&#35299;&#32544;&#30340;&#21487;&#24494;&#25299;&#25169;&#25439;&#22833;&#30340;&#35770;&#25991;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#26032;&#32467;&#26524;&#25913;&#36827;&#20102;&#35299;&#32544;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;TopDis&#65288;&#25299;&#25169;&#35299;&#32544;&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#22686;&#21152;&#22810;&#23610;&#24230;&#25299;&#25169;&#25439;&#22833;&#39033;&#23398;&#20064;&#35299;&#32544;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#35299;&#32544;&#26159;&#25968;&#25454;&#34920;&#31034;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#32423;&#35748;&#30693;&#30340;&#23454;&#29616;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;&#22522;&#20110;VAE&#30340;&#26368;&#26032;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#28508;&#21464;&#37327;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#24635;&#20307;&#30456;&#20851;&#24615;&#26469;&#23454;&#29616;&#35299;&#32544;&#12290;&#25105;&#20204;&#20174;&#20998;&#26512;&#25968;&#25454;&#27969;&#24418;&#30340;&#25299;&#25169;&#23646;&#24615;&#30340;&#35282;&#24230;&#26469;&#30475;&#24453;&#35299;&#32544;&#65292;&#29305;&#21035;&#26159;&#20248;&#21270;&#25968;&#25454;&#27969;&#24418;&#36941;&#21382;&#30340;&#25299;&#25169;&#30456;&#20284;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#29992;&#20110;&#35299;&#32544;&#30340;&#21487;&#24494;&#25299;&#25169;&#25439;&#22833;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25299;&#25169;&#25439;&#22833;&#30456;&#23545;&#20110;&#26368;&#26032;&#32467;&#26524;&#25913;&#36827;&#20102;&#35299;&#32544;&#24471;&#20998;&#65292;&#22914;MIG&#12289;FactorVAE&#24471;&#20998;&#12289;SAP&#24471;&#20998;&#21644;DCI&#35299;&#32544;&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose TopDis (Topological Disentanglement), a method for learning disentangled representations via adding multi-scale topological loss term. Disentanglement is a crucial property of data representations substantial for the explainability and robustness of deep learning models and a step towards high-level cognition. The state-of-the-art method based on VAE minimizes the total correlation of the joint distribution of latent variables. We take a different perspective on disentanglement by analyzing topological properties of data manifolds. In particular, we optimize the topological similarity for data manifolds traversals. To the best of our knowledge, our paper is the first one to propose a differentiable topological loss for disentanglement. Our experiments have shown that the proposed topological loss improves disentanglement scores such as MIG, FactorVAE score, SAP score and DCI disentanglement score with respect to state-of-the-art results. Our method works in an unsupervised m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#22411;&#32447;&#24615;&#22238;&#24402;&#65288;MMLR&#65289;&#30340;&#39640;&#25928;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#36755;&#20837;&#25968;&#25454;&#38598;&#20998;&#20026;&#23376;&#38598;&#24182;&#26500;&#24314;&#23616;&#37096;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#36817;&#20284;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26102;&#38388;&#22797;&#26434;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24230;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12691</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#22411;&#32447;&#24615;&#22238;&#24402;&#30340;&#39640;&#25928;&#22823;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Efficient Data Analysis Method for Big Data using Multiple-Model Linear Regression. (arXiv:2308.12691v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#22411;&#32447;&#24615;&#22238;&#24402;&#65288;MMLR&#65289;&#30340;&#39640;&#25928;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#36755;&#20837;&#25968;&#25454;&#38598;&#20998;&#20026;&#23376;&#38598;&#24182;&#26500;&#24314;&#23616;&#37096;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#36817;&#20284;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26102;&#38388;&#22797;&#26434;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24230;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#29992;&#26032;&#23450;&#20041;&#30340;&#22238;&#24402;&#27169;&#22411;&#22810;&#27169;&#22411;&#32447;&#24615;&#22238;&#24402;&#65288;MMLR&#65289;&#65292;&#35813;&#26041;&#27861;&#23558;&#36755;&#20837;&#25968;&#25454;&#38598;&#20998;&#20026;&#23376;&#38598;&#24182;&#26500;&#24314;&#23616;&#37096;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#27604;&#20854;&#20182;&#22522;&#20110;&#22238;&#24402;&#30340;&#26041;&#27861;&#26356;&#39640;&#25928;&#21644;&#28789;&#27963;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;$(\epsilon,\delta)$-&#20272;&#35745;&#37327;&#30340;MMLR&#27169;&#22411;&#26500;&#36896;&#30340;&#36817;&#20284;&#31639;&#27861;&#65292;&#24182;&#23545;MMLR&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#21644;&#25928;&#29575;&#32473;&#20986;&#20102;&#25968;&#23398;&#35777;&#26126;&#65292;&#20854;&#26102;&#38388;&#22797;&#26434;&#24615;&#19982;&#36755;&#20837;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#26412;&#25991;&#36824;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#23454;&#29616;&#65292;&#31639;&#27861;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#19982;&#29616;&#26377;&#22238;&#24402;&#26041;&#27861;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#39640;&#39044;&#27979;&#20934;&#30830;&#24230;&#25152;&#38656;&#30340;&#26102;&#38388;&#20960;&#20046;&#26368;&#30701;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new data analysis method for big data using a newly defined regression model named multiple model linear regression(MMLR), which separates input datasets into subsets and construct local linear regression models of them. The proposed data analysis method is shown to be more efficient and flexible than other regression based methods. This paper also proposes an approximate algorithm to construct MMLR models based on $(\epsilon,\delta)$-estimator, and gives mathematical proofs of the correctness and efficiency of MMLR algorithm, of which the time complexity is linear with respect to the size of input datasets. This paper also empirically implements the method on both synthetic and real-world datasets, the algorithm shows to have comparable performance to existing regression methods in many cases, while it takes almost the shortest time to provide a high prediction accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Match-And-Deform&#65288;MAD&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#21644;&#26102;&#38388;&#23545;&#40784;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#20013;&#25214;&#21040;&#23545;&#24212;&#20851;&#31995;&#24182;&#20801;&#35768;&#26102;&#38388;&#22833;&#30495;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MAD&#21487;&#20197;&#29983;&#25104;&#23545;&#40784;&#39046;&#22495;&#24182;&#26368;&#22823;&#21270;&#32593;&#32476;&#21028;&#21035;&#33021;&#21147;&#30340;&#26032;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#24418;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.12686</link><description>&lt;p&gt;
Match-And-Deform: &#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#21644;&#26102;&#38388;&#23545;&#40784;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Match-And-Deform: Time Series Domain Adaptation through Optimal Transport and Temporal Alignment. (arXiv:2308.12686v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Match-And-Deform&#65288;MAD&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#21644;&#26102;&#38388;&#23545;&#40784;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#20013;&#25214;&#21040;&#23545;&#24212;&#20851;&#31995;&#24182;&#20801;&#35768;&#26102;&#38388;&#22833;&#30495;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MAD&#21487;&#20197;&#29983;&#25104;&#23545;&#40784;&#39046;&#22495;&#24182;&#26368;&#22823;&#21270;&#32593;&#32476;&#21028;&#21035;&#33021;&#21147;&#30340;&#26032;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36890;&#24120;&#23384;&#22312;&#22823;&#37327;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#20294;&#30456;&#20851;&#32852;&#30340;&#26631;&#31614;&#24448;&#24448;&#24456;&#23569;&#12290;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#26088;&#22312;&#21033;&#29992;&#26469;&#33258;&#28304;&#39046;&#22495;&#30340;&#26631;&#31614;&#26469;&#23545;&#26469;&#33258;&#30456;&#20851;&#20294;&#19981;&#21516;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#24403;&#28041;&#21450;&#26102;&#38388;&#24207;&#21015;&#26102;&#65292;&#38500;&#20102;&#26631;&#20934;&#29305;&#24449;&#20998;&#24067;&#20559;&#31227;&#20043;&#22806;&#65292;&#36824;&#20250;&#20986;&#29616;&#26102;&#38388;&#20559;&#31227;&#30340;&#26032;&#38590;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Match-And-Deform&#65288;MAD&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#22312;&#20801;&#35768;&#26102;&#38388;&#22833;&#30495;&#30340;&#21516;&#26102;&#22312;&#28304;&#26102;&#38388;&#24207;&#21015;&#21644;&#30446;&#26631;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#25214;&#21040;&#23545;&#24212;&#20851;&#31995;&#12290;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#25439;&#22833;&#21644;&#21160;&#24577;&#26102;&#38388;&#25197;&#26354;&#21516;&#26102;&#23545;&#40784;&#20102;&#31995;&#21015;&#12290;&#24403;&#23884;&#20837;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26102;&#65292;MAD&#26377;&#21161;&#20110;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#34920;&#31034;&#65292;&#26082;&#21487;&#20197;&#23545;&#40784;&#39046;&#22495;&#21448;&#21487;&#20197;&#26368;&#22823;&#21270;&#32593;&#32476;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#36965;&#24863;&#25968;&#25454;&#36827;&#34892;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;MAD&#20855;&#26377;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large volumes of unlabeled data are usually available, associated labels are often scarce. The unsupervised domain adaptation problem aims at exploiting labels from a source domain to classify data from a related, yet different, target domain. When time series are at stake, new difficulties arise as temporal shifts may appear in addition to the standard feature distribution shift. In this paper, we introduce the Match-And-Deform (MAD) approach that aims at finding correspondences between the source and target time series while allowing temporal distortions. The associated optimization problem simultaneously aligns the series thanks to an optimal transport loss and the time stamps through dynamic time warping. When embedded into a deep neural network, MAD helps learning new representations of time series that both align the domains and maximize the discriminative power of the network. Empirical studies on benchmark datasets and remote sensing data demonstrate that MAD makes meanin
&lt;/p&gt;</description></item><item><title>LR-XFL&#26159;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36923;&#36753;&#35268;&#21017;&#21644;&#27169;&#22411;&#26356;&#26032;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#23545;FL&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#25552;&#21319;&#21644;&#21152;&#26435;&#32858;&#21512;&#65292;&#24182;&#22312;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.12681</link><description>&lt;p&gt;
LR-XFL: &#22522;&#20110;&#36923;&#36753;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LR-XFL: Logical Reasoning-based Explainable Federated Learning. (arXiv:2308.12681v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12681
&lt;/p&gt;
&lt;p&gt;
LR-XFL&#26159;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36923;&#36753;&#35268;&#21017;&#21644;&#27169;&#22411;&#26356;&#26032;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#23545;FL&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#25552;&#21319;&#21644;&#21152;&#26435;&#32858;&#21512;&#65292;&#24182;&#22312;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21327;&#20316;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#38544;&#31169;&#20445;&#25252;&#30340;&#38656;&#27714;&#20351;&#24471;FL&#27169;&#22411;&#24456;&#38590;&#23454;&#29616;&#20840;&#23616;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36923;&#36753;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#32852;&#37030;&#23398;&#20064; (LR-XFL) &#26041;&#27861;&#65292;&#23558;&#36923;&#36753;&#25512;&#29702;&#34701;&#20837;FL&#20013;&#12290;&#22312;LR-XFL&#20013;&#65292;FL&#23458;&#25143;&#31471;&#26681;&#25454;&#20854;&#26412;&#22320;&#25968;&#25454;&#21019;&#24314;&#26412;&#22320;&#36923;&#36753;&#35268;&#21017;&#65292;&#24182;&#23558;&#20854;&#19982;&#27169;&#22411;&#26356;&#26032;&#19968;&#36215;&#21457;&#36865;&#21040;FL&#26381;&#21153;&#22120;&#12290;FL&#26381;&#21153;&#22120;&#36890;&#36807;&#36866;&#24403;&#30340;&#36923;&#36753;&#36830;&#25509;&#31526;&#23558;&#26412;&#22320;&#36923;&#36753;&#35268;&#21017;&#36830;&#25509;&#36215;&#26469;&#65292;&#35813;&#36830;&#25509;&#31526;&#22522;&#20110;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#23646;&#24615;&#36827;&#34892;&#25512;&#23548;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#26381;&#21153;&#22120;&#36824;&#26681;&#25454;&#23458;&#25143;&#31471;&#19978;&#20256;&#30340;&#36923;&#36753;&#35268;&#21017;&#21453;&#26144;&#30340;&#26412;&#22320;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#20351;&#29992;&#26435;&#37325;&#20540;&#23545;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#36827;&#34892;&#32858;&#21512;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LR-XFL&#22312;&#26368;&#30456;&#20851;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;1.19&#65285;&#65292;5.81&#65285;&#21644;5.41&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is an emerging approach for training machine learning models collaboratively while preserving data privacy. The need for privacy protection makes it difficult for FL models to achieve global transparency and explainability. To address this limitation, we incorporate logic-based explanations into FL by proposing the Logical Reasoning-based eXplainable Federated Learning (LR-XFL) approach. Under LR-XFL, FL clients create local logic rules based on their local data and send them, along with model updates, to the FL server. The FL server connects the local logic rules through a proper logical connector that is derived based on properties of client data, without requiring access to the raw data. In addition, the server also aggregates the local model updates with weight values determined by the quality of the clients' local data as reflected by their uploaded logic rules. The results show that LR-XFL outperforms the most relevant baseline by 1.19%, 5.81% and 5.41% in
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#38750;&#32447;&#24615;&#36793;&#38469;&#21453;&#39304;&#21644;&#22810;&#26679;&#24615;&#32422;&#26463;&#30340;&#21069;K&#20010;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#26032;&#22411;&#20027;&#20174;&#26550;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#20845;&#20010;&#20174;&#27169;&#22411;&#21644;&#25945;&#24072;&#23398;&#20064;&#20248;&#21270;&#20197;&#21450;&#31574;&#30053;&#20849;&#35757;&#32451;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#20043;&#38388;&#30340;&#20915;&#31574;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.12680</link><description>&lt;p&gt;
&#20351;&#29992;&#20027;&#20174;&#28145;&#24230;&#26550;&#26500;&#35299;&#20915;&#20855;&#26377;&#38750;&#32447;&#24615;&#36793;&#38469;&#21453;&#39304;&#21644;&#22810;&#26679;&#24615;&#32422;&#26463;&#30340;&#21069;K&#20010;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Master-slave Deep Architecture for Top-K Multi-armed Bandits with Non-linear Bandit Feedback and Diversity Constraints. (arXiv:2308.12680v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12680
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#38750;&#32447;&#24615;&#36793;&#38469;&#21453;&#39304;&#21644;&#22810;&#26679;&#24615;&#32422;&#26463;&#30340;&#21069;K&#20010;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#26032;&#22411;&#20027;&#20174;&#26550;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#20845;&#20010;&#20174;&#27169;&#22411;&#21644;&#25945;&#24072;&#23398;&#20064;&#20248;&#21270;&#20197;&#21450;&#31574;&#30053;&#20849;&#35757;&#32451;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#20043;&#38388;&#30340;&#20915;&#31574;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#20174;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#38750;&#32447;&#24615;&#36793;&#38469;&#21453;&#39304;&#21644;&#22810;&#26679;&#24615;&#32422;&#26463;&#30340;&#21069;K&#20010;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#36172;&#21338;&#21453;&#39304;&#19979;&#32771;&#34385;&#22810;&#26679;&#24615;&#32422;&#26463;&#30340;&#32452;&#21512;&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#39640;&#25928;&#22320;&#25506;&#32034;&#32452;&#21512;&#21644;&#21463;&#32422;&#26463;&#30340;&#34892;&#21160;&#31354;&#38388;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20845;&#20010;&#20855;&#26377;&#26174;&#33879;&#20248;&#28857;&#30340;&#20174;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#24179;&#34913;&#22870;&#21169;&#21644;&#32422;&#26463;&#20197;&#21450;&#25928;&#29575;&#30340;&#22810;&#26679;&#21270;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25945;&#24072;&#23398;&#20064;&#30340;&#20248;&#21270;&#21644;&#31574;&#30053;&#20849;&#35757;&#32451;&#25216;&#26415;&#65292;&#20197;&#25552;&#21319;&#22810;&#20010;&#20174;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#20027;&#27169;&#22411;&#25910;&#38598;&#20174;&#27169;&#22411;&#25552;&#20379;&#30340;&#31934;&#33521;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#31070;&#32463;&#19978;&#19979;&#25991;UCB&#32593;&#32476;&#20272;&#35745;&#30340;&#26368;&#20339;&#26679;&#26412;&#26469;&#20570;&#20986;&#22312;&#25506;&#32034;&#21644;&#21033;&#29992;&#20043;&#38388;&#26435;&#34913;&#30340;&#20915;&#31574;&#12290;&#30001;&#20110;&#20174;&#27169;&#22411;&#30340;&#31934;&#24515;&#35774;&#35745;&#65292;&#20849;&#21516;&#35757;&#32451;&#26426;&#21046;&#25104;&#25928;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel master-slave architecture to solve the top-$K$ combinatorial multi-armed bandits problem with non-linear bandit feedback and diversity constraints, which, to the best of our knowledge, is the first combinatorial bandits setting considering diversity constraints under bandit feedback. Specifically, to efficiently explore the combinatorial and constrained action space, we introduce six slave models with distinguished merits to generate diversified samples well balancing rewards and constraints as well as efficiency. Moreover, we propose teacher learning based optimization and the policy co-training technique to boost the performance of the multiple slave models. The master model then collects the elite samples provided by the slave models and selects the best sample estimated by a neural contextual UCB-based network to make a decision with a trade-off between exploration and exploitation. Thanks to the elaborate design of slave models, the co-training mechanism among s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#21548;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#30333;&#32454;&#32990;&#20998;&#31867;&#65292;&#36890;&#36807;&#36873;&#25321;&#20195;&#34920;&#26679;&#26412;&#26469;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2308.12679</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#30333;&#32454;&#32990;&#20998;&#31867;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Continual Learning Approach for Cross-Domain White Blood Cell Classification. (arXiv:2308.12679v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12679
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#21548;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#30333;&#32454;&#32990;&#20998;&#31867;&#65292;&#36890;&#36807;&#36873;&#25321;&#20195;&#34920;&#26679;&#26412;&#26469;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#20998;&#31867;&#22806;&#21608;&#34880;&#28082;&#20013;&#30340;&#30333;&#32454;&#32990;&#23545;&#20110;&#35786;&#26029;&#34880;&#28082;&#30149;&#38750;&#24120;&#37325;&#35201;&#12290;&#30001;&#20110;&#20020;&#24202;&#29615;&#22659;&#12289;&#25968;&#25454;&#26469;&#28304;&#21644;&#30142;&#30149;&#20998;&#31867;&#19981;&#26029;&#21464;&#21270;&#65292;&#20026;&#20102;&#36866;&#24212;&#23454;&#38469;&#24212;&#29992;&#65292;&#38656;&#35201;&#23450;&#26399;&#26356;&#26032;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20174;&#36755;&#20837;&#25968;&#25454;&#27969;&#20013;&#39034;&#24207;&#23398;&#20064;&#26102;&#20250;&#21463;&#30410;&#21290;&#27973;&#65292;&#20294;&#26159;&#24403;&#22312;&#26032;&#25968;&#25454;&#19978;&#24494;&#35843;&#26102;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#23548;&#33268;&#22312;&#20043;&#21069;&#30340;&#20219;&#21153;&#19978;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#21548;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#30333;&#32454;&#32990;&#20998;&#31867;&#20013;&#30340;&#31867;&#22686;&#37327;&#21644;&#39046;&#22495;&#22686;&#37327;&#22330;&#26223;&#12290;&#20026;&#20102;&#36873;&#25321;&#21069;&#19968;&#20219;&#21153;&#20013;&#30340;&#20195;&#34920;&#26679;&#26412;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#30340;&#26679;&#26412;&#36873;&#25321;&#26041;&#27861;&#12290;&#36825;&#21253;&#25324;&#36890;&#36807;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#36873;&#25321;&#26368;&#26377;&#20449;&#24515;&#30340;&#26679;&#26412;&#21644;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate classification of white blood cells in peripheral blood is essential for diagnosing hematological diseases. Due to constantly evolving clinical settings, data sources, and disease classifications, it is necessary to update machine learning classification models regularly for practical real-world use. Such models significantly benefit from sequentially learning from incoming data streams without forgetting previously acquired knowledge. However, models can suffer from catastrophic forgetting, causing a drop in performance on previous tasks when fine-tuned on new data. Here, we propose a rehearsal-based continual learning approach for class incremental and domain incremental scenarios in white blood cell classification. To choose representative samples from previous tasks, we employ exemplar set selection based on the model's predictions. This involves selecting the most confident samples and the most challenging samples identified through uncertainty estimation of the model. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36974;&#32617;&#29305;&#24449;&#24314;&#27169;&#26041;&#27861;(MFM)&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#22359;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#20998;&#35789;&#22120;&#26469;&#37325;&#26500;&#35270;&#39057;&#20013;&#23545;&#35937;&#30340;&#36974;&#32617;&#29305;&#24449;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;GAT&#22359;&#34701;&#20837;&#21040;&#35270;&#39057;&#20107;&#20214;&#35782;&#21035;&#26550;&#26500;ViGAT&#20013;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12673</link><description>&lt;p&gt;
&#36974;&#32617;&#29305;&#24449;&#24314;&#27169;&#65306;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#22359;&#30340;&#29305;&#24449;&#36974;&#32617;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Masked Feature Modelling: Feature Masking for the Unsupervised Pre-training of a Graph Attention Network Block for Bottom-up Video Event Recognition. (arXiv:2308.12673v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36974;&#32617;&#29305;&#24449;&#24314;&#27169;&#26041;&#27861;(MFM)&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#22359;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#20998;&#35789;&#22120;&#26469;&#37325;&#26500;&#35270;&#39057;&#20013;&#23545;&#35937;&#30340;&#36974;&#32617;&#29305;&#24449;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;GAT&#22359;&#34701;&#20837;&#21040;&#35270;&#39057;&#20107;&#20214;&#35782;&#21035;&#26550;&#26500;ViGAT&#20013;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#36974;&#32617;&#29305;&#24449;&#24314;&#27169;(MFM)&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;(GAT)&#22359;&#12290;MFM&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#20998;&#35789;&#22120;&#26469;&#37325;&#26500;&#35270;&#39057;&#20013;&#23545;&#35937;&#30340;&#36974;&#32617;&#29305;&#24449;&#65292;&#21033;&#29992;MiniKinetics&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#23558;&#39044;&#35757;&#32451;&#30340;GAT&#22359;&#34701;&#20837;&#21040;&#26368;&#20808;&#36827;&#30340;&#33258;&#24213;&#21521;&#19978;&#26377;&#30417;&#30563;&#35270;&#39057;&#20107;&#20214;&#35782;&#21035;&#26550;&#26500;ViGAT&#20013;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#36215;&#28857;&#21644;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;&#22312;YLI-MED&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;MFM&#22312;&#25552;&#39640;&#20107;&#20214;&#35782;&#21035;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Masked Feature Modelling (MFM), a novel approach for the unsupervised pre-training of a Graph Attention Network (GAT) block. MFM utilizes a pretrained Visual Tokenizer to reconstruct masked features of objects within a video, leveraging the MiniKinetics dataset. We then incorporate the pre-trained GAT block into a state-of-the-art bottom-up supervised video-event recognition architecture, ViGAT, to improve the model's starting point and overall accuracy. Experimental evaluations on the YLI-MED dataset demonstrate the effectiveness of MFM in improving event recognition performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32500;&#25252;&#25805;&#20316;&#20013;&#65292;&#36890;&#36807;&#25968;&#25454;&#20849;&#20139;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#25104;&#26412;&#30340;&#26368;&#20248;&#25968;&#25454;&#27719;&#32858;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.12670</link><description>&lt;p&gt;
&#32500;&#25252;&#25805;&#20316;&#20013;&#25968;&#25454;&#20849;&#20139;&#23398;&#20064;&#30340;&#26368;&#20248;&#25968;&#25454;&#27719;&#32858;
&lt;/p&gt;
&lt;p&gt;
Optimal data pooling for shared learning in maintenance operations. (arXiv:2308.12670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32500;&#25252;&#25805;&#20316;&#20013;&#65292;&#36890;&#36807;&#25968;&#25454;&#20849;&#20139;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#25104;&#26412;&#30340;&#26368;&#20248;&#25968;&#25454;&#27719;&#32858;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#32500;&#25252;&#25805;&#20316;&#20013;&#27719;&#32858;&#25968;&#25454;&#36827;&#34892;&#20849;&#20139;&#23398;&#20064;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#32452;&#36890;&#36807;&#20808;&#39564;&#26410;&#30693;&#36895;&#29575;&#30456;&#20114;&#32806;&#21512;&#30340;&#27850;&#26494;&#36864;&#21270;&#31995;&#32479;&#12290;&#28041;&#21450;&#36825;&#20123;&#31995;&#32479;&#30340;&#20915;&#31574;&#38382;&#39064;&#26159;&#39640;&#32500;&#24230;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#36825;&#26679;&#30340;MDP&#20998;&#35299;&#20026;&#20108;&#32500;MDP&#30340;&#20998;&#35299;&#32467;&#26524;&#65292;&#20174;&#32780;&#23454;&#29616;&#32467;&#26500;&#20998;&#26512;&#21644;&#35745;&#31639;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#20998;&#35299;&#32467;&#26524;&#35777;&#26126;&#20102;&#25968;&#25454;&#20849;&#20139;&#21487;&#20197;&#30456;&#23545;&#20110;&#19981;&#20849;&#20139;&#26102;&#26174;&#33879;&#38477;&#20302;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the benefits of pooling data for shared learning in maintenance operations. We consider a set of systems subject to Poisson degradation that are coupled through an a-priori unknown rate. Decision problems involving these systems are high-dimensional Markov decision processes (MDPs). We present a decomposition result that reduces such an MDP to two-dimensional MDPs, enabling structural analyses and computations. We leverage this decomposition to demonstrate that pooling data can lead to significant cost reductions compared to not pooling.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#27979;&#22320;&#32447;&#27169;&#24335;&#36830;&#36890;&#24615;&#30340;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#36817;&#20284;&#27979;&#22320;&#32447;&#65292;&#23454;&#29616;&#20102;&#27169;&#24335;&#36830;&#36890;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12666</link><description>&lt;p&gt;
&#27979;&#22320;&#32447;&#27169;&#24335;&#36830;&#36890;&#24615;
&lt;/p&gt;
&lt;p&gt;
Geodesic Mode Connectivity. (arXiv:2308.12666v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#27979;&#22320;&#32447;&#27169;&#24335;&#36830;&#36890;&#24615;&#30340;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#36817;&#20284;&#27979;&#22320;&#32447;&#65292;&#23454;&#29616;&#20102;&#27169;&#24335;&#36830;&#36890;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#24335;&#36830;&#36890;&#24615;&#26159;&#25351;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#19968;&#26465;&#20302;&#25439;&#22833;&#36335;&#24452;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#29616;&#35937;&#37325;&#26032;&#35299;&#37322;&#20026;&#20449;&#24687;&#20960;&#20309;&#30340;&#19968;&#37096;&#20998;&#65292;&#20854;&#20013;&#31070;&#32463;&#32593;&#32476;&#34987;&#30740;&#31350;&#20026;&#20855;&#26377;&#26354;&#32447;&#20960;&#20309;&#30340;&#21442;&#25968;&#21270;&#20998;&#24067;&#31354;&#38388;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#20123;&#31354;&#38388;&#20013;&#30340;&#26368;&#30701;&#36335;&#24452;&#65292;&#21363;&#27979;&#22320;&#32447;&#65292;&#23545;&#24212;&#20110;&#25439;&#22833;&#26223;&#35266;&#20013;&#30340;&#27169;&#24335;&#36830;&#25509;&#36335;&#24452;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#27979;&#22320;&#32447;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#23454;&#29616;&#20102;&#27169;&#24335;&#36830;&#36890;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mode connectivity is a phenomenon where trained models are connected by a path of low loss. We reframe this in the context of Information Geometry, where neural networks are studied as spaces of parameterized distributions with curved geometry. We hypothesize that shortest paths in these spaces, known as geodesics, correspond to mode-connecting paths in the loss landscape. We propose an algorithm to approximate geodesics and demonstrate that they achieve mode connectivity.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#22270;&#20687;&#36827;&#34892;&#26333;&#20809;&#25915;&#20987;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#35813;&#25915;&#20987;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12661</link><description>&lt;p&gt;
&#19981;&#35201;&#26395;&#21521;&#22826;&#38451;&#65306;&#23545;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#26333;&#20809;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Don't Look into the Sun: Adversarial Solarization Attacks on Image Classifiers. (arXiv:2308.12661v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12661
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#22270;&#20687;&#36827;&#34892;&#26333;&#20809;&#25915;&#20987;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#35813;&#25915;&#20987;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#20687;&#33258;&#21160;&#39550;&#39542;&#36825;&#26679;&#30340;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#20294;&#20063;&#36866;&#29992;&#20110;&#24694;&#24847;&#25915;&#20987;&#32773;&#21487;&#20197;&#25968;&#23383;&#22320;&#20462;&#25913;&#36755;&#20837;&#20197;&#32469;&#36807;&#23433;&#20840;&#20445;&#25252;&#30340;&#23433;&#20840;&#31995;&#32479;&#20013;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#26377;&#25928;&#30340;&#36229;&#20986;&#20998;&#24067;&#27979;&#35797;&#65292;&#22312;&#20445;&#25345;&#20934;&#30830;&#30340;&#26631;&#31614;&#20449;&#24687;&#30340;&#21516;&#26102;&#28085;&#30422;&#25152;&#26377;&#21487;&#33021;&#30340;&#22330;&#26223;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#38656;&#35201;&#22312;&#25915;&#20987;&#30340;&#22810;&#26679;&#24615;&#21644;&#38480;&#21046;&#27700;&#24179;&#20043;&#38388;&#20570;&#20986;&#22949;&#21327;&#65292;&#26377;&#26102;&#29978;&#33267;&#20004;&#32773;&#20860;&#32780;&#26377;&#20043;&#12290;&#20316;&#20026;&#23545;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#26356;&#20840;&#38754;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#26333;&#20809;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27010;&#24565;&#19978;&#31616;&#21333;&#26126;&#20102;&#65292;&#20294;&#21448;&#33021;&#36991;&#20813;&#23616;&#37096;&#33539;&#22260;&#20869;&#30340;&#33258;&#28982;&#22270;&#20687;&#30340;&#20840;&#23616;&#32467;&#26500;&#21463;&#21040;&#25439;&#23475;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;ImageNet&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#35813;&#25915;&#20987;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#20934;&#30830;&#24615;&#65292;&#21069;&#25552;&#26159;&#23427;&#27809;&#26377;&#38598;&#25104;&#21040;
&lt;/p&gt;
&lt;p&gt;
Assessing the robustness of deep neural networks against out-of-distribution inputs is crucial, especially in safety-critical domains like autonomous driving, but also in safety systems where malicious actors can digitally alter inputs to circumvent safety guards. However, designing effective out-of-distribution tests that encompass all possible scenarios while preserving accurate label information is a challenging task. Existing methodologies often entail a compromise between variety and constraint levels for attacks and sometimes even both. In a first step towards a more holistic robustness evaluation of image classification models, we introduce an attack method based on image solarization that is conceptually straightforward yet avoids jeopardizing the global structure of natural images independent of the intensity. Through comprehensive evaluations of multiple ImageNet models, we demonstrate the attack's capacity to degrade accuracy significantly, provided it is not integrated into
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;APART&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20840;&#32452;&#23545;&#21028;&#21035;&#22120;&#12289;&#26032;&#39062;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#21644;&#20002;&#24323;&#25216;&#26415;&#65292;&#22312;&#26080;&#22870;&#21169;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#22810;&#26679;&#21270;&#25216;&#33021;&#30340;&#21457;&#29616;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#65292;&#24182;&#22312;&#31616;&#21333;&#30340;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#21457;&#29616;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12649</link><description>&lt;p&gt;
APART: &#20351;&#29992;&#26377;&#21319;&#24207;&#22870;&#21169;&#21644;&#20002;&#24323;&#25216;&#26415;&#30340;&#20840;&#32452;&#23545;&#22810;&#26679;&#21270;&#25216;&#33021;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
APART: Diverse Skill Discovery using All Pairs with Ascending Reward and DropouT. (arXiv:2308.12649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;APART&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20840;&#32452;&#23545;&#21028;&#21035;&#22120;&#12289;&#26032;&#39062;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#21644;&#20002;&#24323;&#25216;&#26415;&#65292;&#22312;&#26080;&#22870;&#21169;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#22810;&#26679;&#21270;&#25216;&#33021;&#30340;&#21457;&#29616;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#65292;&#24182;&#22312;&#31616;&#21333;&#30340;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#21457;&#29616;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26080;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#22810;&#26679;&#21270;&#25216;&#33021;&#21457;&#29616;&#65292;&#22312;&#31616;&#21333;&#30340;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#21457;&#29616;&#25152;&#26377;&#21487;&#33021;&#30340;&#25216;&#33021;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#24456;&#38590;&#25104;&#21151;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#21046;&#23450;&#20026;&#20351;&#29992;&#20869;&#22312;&#22870;&#21169;&#21644;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#21028;&#21035;&#22120;&#26469;&#30456;&#20114;&#35757;&#32451;&#25216;&#33021;&#20197;&#39044;&#27979;&#32473;&#23450;&#36712;&#36857;&#30340;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#29992;&#20840;&#32452;&#23545;&#65288;all pairs&#65289;&#21028;&#21035;&#22120;&#26367;&#25442;&#20102;&#26631;&#20934;&#30340;&#19968;&#23545;&#22810;&#65288;softmax&#65289;&#21028;&#21035;&#22120;&#65292;&#24182;&#32467;&#21512;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#21644;&#20002;&#24323;&#65288;dropout&#65289;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#36825;&#31181;&#32508;&#21512;&#26041;&#27861;&#34987;&#21629;&#21517;&#20026;APART: &#20351;&#29992;&#26377;&#21319;&#24207;&#22870;&#21169;&#21644;&#20002;&#24323;&#25216;&#26415;&#30340;&#20840;&#32452;&#23545;&#22810;&#26679;&#21270;&#25216;&#33021;&#21457;&#29616;&#12290;&#25105;&#20204;&#35777;&#26126;APART&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#26356;&#23569;&#26679;&#26412;&#23601;&#33021;&#21457;&#29616;&#32593;&#26684;&#19990;&#30028;&#20013;&#25152;&#26377;&#21487;&#33021;&#30340;&#25216;&#33021;&#12290;&#21463;&#21040;APART&#30340;&#23454;&#35777;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#25913;&#21464;VIC&#65292;&#37325;&#26032;&#35843;&#25972;&#20854;&#20869;&#22312;&#22870;&#21169;&#65292;&#24182;&#35843;&#33410;&#20854;softmax&#28201;&#24230;&#26469;&#23454;&#29616;&#26368;&#22823;&#21270;&#25216;&#33021;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study diverse skill discovery in reward-free environments, aiming to discover all possible skills in simple grid-world environments where prior methods have struggled to succeed. This problem is formulated as mutual training of skills using an intrinsic reward and a discriminator trained to predict a skill given its trajectory. Our initial solution replaces the standard one-vs-all (softmax) discriminator with a one-vs-one (all pairs) discriminator and combines it with a novel intrinsic reward function and a dropout regularization technique. The combined approach is named APART: Diverse Skill Discovery using All Pairs with Ascending Reward and Dropout. We demonstrate that APART discovers all the possible skills in grid worlds with remarkably fewer samples than previous works. Motivated by the empirical success of APART, we further investigate an even simpler algorithm that achieves maximum skills by altering VIC, rescaling its intrinsic reward, and tuning the temperature of its softm
&lt;/p&gt;</description></item><item><title>GENEA Challenge 2023&#26159;&#19968;&#39033;&#22823;&#35268;&#27169;&#35780;&#20272;&#25163;&#21183;&#29983;&#25104;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#21442;&#19982;&#22242;&#38431;&#20351;&#29992;&#30456;&#21516;&#35821;&#38899;&#21644;&#36816;&#21160;&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;&#22522;&#20110;&#35821;&#38899;&#39537;&#21160;&#30340;&#25163;&#21183;&#29983;&#25104;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#20960;&#39033;&#29992;&#25143;&#30740;&#31350;&#35780;&#20272;&#20102;&#36825;&#20123;&#31995;&#32479;&#30340;&#20154;&#31867;&#30456;&#20284;&#24230;&#20197;&#21450;&#23545;&#20110;&#21457;&#35328;&#32773;&#21644;&#20114;&#21160;&#32773;&#34892;&#20026;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12646</link><description>&lt;p&gt;
GENEA Challenge 2023&#65306;&#21333;&#22768;&#36947;&#21644;&#21452;&#22768;&#36947;&#29615;&#22659;&#20013;&#25163;&#21183;&#29983;&#25104;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
The GENEA Challenge 2023: A large scale evaluation of gesture generation models in monadic and dyadic settings. (arXiv:2308.12646v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12646
&lt;/p&gt;
&lt;p&gt;
GENEA Challenge 2023&#26159;&#19968;&#39033;&#22823;&#35268;&#27169;&#35780;&#20272;&#25163;&#21183;&#29983;&#25104;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#21442;&#19982;&#22242;&#38431;&#20351;&#29992;&#30456;&#21516;&#35821;&#38899;&#21644;&#36816;&#21160;&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;&#22522;&#20110;&#35821;&#38899;&#39537;&#21160;&#30340;&#25163;&#21183;&#29983;&#25104;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#20960;&#39033;&#29992;&#25143;&#30740;&#31350;&#35780;&#20272;&#20102;&#36825;&#20123;&#31995;&#32479;&#30340;&#20154;&#31867;&#30456;&#20284;&#24230;&#20197;&#21450;&#23545;&#20110;&#21457;&#35328;&#32773;&#21644;&#20114;&#21160;&#32773;&#34892;&#20026;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25253;&#21578;&#20102;GENEA Challenge 2023&#30340;&#24773;&#20917;&#65292;&#21442;&#19982;&#22242;&#38431;&#20351;&#29992;&#30456;&#21516;&#30340;&#35821;&#38899;&#21644;&#36816;&#21160;&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;&#22522;&#20110;&#35821;&#38899;&#39537;&#21160;&#30340;&#25163;&#21183;&#29983;&#25104;&#31995;&#32479;&#65292;&#24182;&#36827;&#34892;&#20102;&#20849;&#21516;&#35780;&#20272;&#12290;&#20170;&#24180;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#21452;&#20154;&#20114;&#21160;&#30340;&#25968;&#25454;&#65292;&#20351;&#22242;&#38431;&#21487;&#20197;&#26681;&#25454;&#21457;&#35328;&#32773;&#30340;&#35821;&#38899;&#65288;&#25991;&#26412;&#21644;&#38899;&#39057;&#65289;&#20197;&#21450;&#20114;&#21160;&#32773;&#30340;&#35821;&#38899;&#21644;&#21160;&#20316;&#29983;&#25104;&#23436;&#25972;&#30340;&#20840;&#36523;&#21160;&#20316;&#12290;&#25105;&#20204;&#23545;12&#20010;&#25552;&#20132;&#21644;2&#20010;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#20445;&#30041;&#30340;&#36816;&#21160;&#25429;&#25417;&#25968;&#25454;&#36827;&#34892;&#20102;&#20960;&#39033;&#22823;&#35268;&#27169;&#29992;&#25143;&#30740;&#31350;&#12290;&#36825;&#20123;&#30740;&#31350;&#38598;&#20013;&#22312;&#19977;&#20010;&#26041;&#38754;&#65306;1&#65289;&#21160;&#20316;&#30340;&#20154;&#31867;&#30456;&#20284;&#24230;&#65292;2&#65289;&#21160;&#20316;&#23545;&#20110;&#21457;&#35328;&#32773;&#33258;&#36523;&#35821;&#38899;&#30340;&#36866;&#24212;&#24615;&#65288;&#21516;&#26102;&#25511;&#21046;&#21160;&#20316;&#30340;&#20154;&#31867;&#30456;&#20284;&#24230;&#65289;&#65292;&#21644;3&#65289;&#21160;&#20316;&#23545;&#20110;&#20114;&#21160;&#20013;&#23545;&#35805;&#32773;&#34892;&#20026;&#30340;&#36866;&#24212;&#24615;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#25511;&#21046;&#20102;&#21160;&#20316;&#30340;&#20154;&#31867;&#30456;&#20284;&#24230;&#21644;&#21457;&#35328;&#32773;&#33258;&#36523;&#35821;&#38899;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#21457;&#29616;&#25361;&#25112;&#30340;&#25552;&#20132;&#20043;&#38388;&#30340;&#20154;&#31867;&#30456;&#20284;&#24230;&#24046;&#24322;&#24456;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reports on the GENEA Challenge 2023, in which participating teams built speech-driven gesture-generation systems using the same speech and motion dataset, followed by a joint evaluation. This year's challenge provided data on both sides of a dyadic interaction, allowing teams to generate full-body motion for an agent given its speech (text and audio) and the speech and motion of the interlocutor. We evaluated 12 submissions and 2 baselines together with held-out motion-capture data in several large-scale user studies. The studies focused on three aspects: 1) the human-likeness of the motion, 2) the appropriateness of the motion for the agent's own speech whilst controlling for the human-likeness of the motion, and 3) the appropriateness of the motion for the behaviour of the interlocutor in the interaction, using a setup that controls for both the human-likeness of the motion and the agent's own speech. We found a large span in human-likeness between challenge submissions, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21306;&#22495;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#34701;&#21512;&#21306;&#22495;&#34917;&#19969;&#20449;&#24687;&#20197;&#24471;&#20986;&#28369;&#29255;&#32423;&#21035;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#22534;&#21472;&#21306;&#22495;&#32858;&#21512;&#26469;&#20998;&#23618;&#22788;&#29702;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#32858;&#28966;&#20110;&#39640;&#20851;&#27880;&#21306;&#22495;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.12634</link><description>&lt;p&gt;
&#38754;&#21521;&#20998;&#23618;&#21306;&#22495;&#21464;&#21387;&#22120;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Hierarchical Regional Transformer-based Multiple Instance Learning. (arXiv:2308.12634v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21306;&#22495;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#34701;&#21512;&#21306;&#22495;&#34917;&#19969;&#20449;&#24687;&#20197;&#24471;&#20986;&#28369;&#29255;&#32423;&#21035;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#22534;&#21472;&#21306;&#22495;&#32858;&#21512;&#26469;&#20998;&#23618;&#22788;&#29702;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#32858;&#28966;&#20110;&#39640;&#20851;&#27880;&#21306;&#22495;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#21644;&#31934;&#30830;&#21307;&#23398;&#20013;&#65292;&#20351;&#29992;&#28145;&#24230;&#22810;&#23454;&#20363;&#23398;&#20064;&#27169;&#22411;&#23545;&#24040;&#20687;&#32032;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#24050;&#25104;&#20026;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#29992;&#21306;&#22495;&#24615;&#30340;&#12289;&#21463;&#21040;&#35270;&#35273;&#21464;&#21387;&#22120;&#21551;&#21457;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#26367;&#20195;&#20102;&#20256;&#32479;&#30340;&#23398;&#20064;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#21306;&#22495;&#34917;&#19969;&#20449;&#24687;&#20197;&#24471;&#20986;&#28369;&#29255;&#32423;&#21035;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#22534;&#21472;&#36825;&#31181;&#21306;&#22495;&#32858;&#21512;&#20197;&#20998;&#23618;&#22320;&#22788;&#29702;&#19981;&#21516;&#36317;&#31163;&#27700;&#24179;&#19978;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#23567;&#30340;&#23616;&#37096;&#24418;&#24577;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#26399;&#38388;&#23558;&#22270;&#20687;&#22788;&#29702;&#38598;&#20013;&#22312;&#39640;&#20851;&#27880;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#20004;&#20010;&#32452;&#32455;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24182;&#25351;&#21521;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The classification of gigapixel histopathology images with deep multiple instance learning models has become a critical task in digital pathology and precision medicine. In this work, we propose a Transformer-based multiple instance learning approach that replaces the traditional learned attention mechanism with a regional, Vision Transformer inspired self-attention mechanism. We present a method that fuses regional patch information to derive slide-level predictions and show how this regional aggregation can be stacked to hierarchically process features on different distance levels. To increase predictive accuracy, especially for datasets with small, local morphological features, we introduce a method to focus the image processing on high attention regions during inference. Our approach is able to significantly improve performance over the baseline on two histopathology datasets and points towards promising directions for further research.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37325;&#24314;&#22768;&#36895;&#27979;&#20117;&#26354;&#32447;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;NGBoost&#31639;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22815;&#39044;&#27979;&#32467;&#26524;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#30340;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;SHAP&#26041;&#27861;&#36827;&#34892;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;NGBoost&#27169;&#22411;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.12625</link><description>&lt;p&gt;
&#37325;&#24314;&#22768;&#36895;&#27979;&#20117;&#26354;&#32447;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Uncertainty and Explainable Analysis of Machine Learning Model for Reconstruction of Sonic Slowness Logs. (arXiv:2308.12625v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37325;&#24314;&#22768;&#36895;&#27979;&#20117;&#26354;&#32447;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;NGBoost&#31639;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22815;&#39044;&#27979;&#32467;&#26524;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#30340;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;SHAP&#26041;&#27861;&#36827;&#34892;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;NGBoost&#27169;&#22411;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#20117;&#26354;&#32447;&#23545;&#20110;&#27833;&#27668;&#30000;&#26469;&#35828;&#26159;&#23453;&#36149;&#30340;&#20449;&#24687;&#65292;&#23427;&#20204;&#24110;&#21161;&#30830;&#23450;&#20117;&#23380;&#21608;&#22260;&#22320;&#23618;&#30340;&#23721;&#24615;&#20197;&#21450;&#22320;&#19979;&#27833;&#27668;&#20648;&#23618;&#30340;&#20301;&#32622;&#21644;&#20648;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#27700;&#24179;&#25110;&#26087;&#20117;&#20013;&#32463;&#24120;&#32570;&#23569;&#37325;&#35201;&#30340;&#27979;&#20117;&#26354;&#32447;&#65292;&#36825;&#32473;&#29616;&#22330;&#24212;&#29992;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#21033;&#29992;SPWLA&#30340;2020&#24180;&#26426;&#22120;&#23398;&#20064;&#31454;&#36187;&#30340;&#25968;&#25454;&#65292;&#26088;&#22312;&#21033;&#29992;&#21516;&#19968;&#20117;&#30524;&#20013;&#30340;&#20854;&#20182;&#27979;&#20117;&#26354;&#32447;&#26469;&#39044;&#27979;&#32570;&#22833;&#30340;&#21387;&#32553;&#27874;&#24930;&#24230;&#21644;&#21098;&#20999;&#27874;&#24930;&#24230;&#27979;&#20117;&#26354;&#32447;&#12290;&#25105;&#20204;&#37319;&#29992;NGBoost&#31639;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#32467;&#26524;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;SHAP&#26041;&#27861;&#26469;&#30740;&#31350;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23558;NGBosst&#27169;&#22411;&#19982;&#20854;&#20182;&#22235;&#31181;&#24120;&#29992;&#30340;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#12289;GBDT&#12289;XGBoost&#21644;LightGBM&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;NGBoost&#27169;&#22411;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logs are valuable information for oil and gas fields as they help to determine the lithology of the formations surrounding the borehole and the location and reserves of subsurface oil and gas reservoirs. However, important logs are often missing in horizontal or old wells, which poses a challenge in field applications. In this paper, we utilize data from the 2020 machine learning competition of the SPWLA, which aims to predict the missing compressional wave slowness and shear wave slowness logs using other logs in the same borehole. We employ the NGBoost algorithm to construct an Ensemble Learning model that can predicate the results as well as their uncertainty. Furthermore, we combine the SHAP method to investigate the interpretability of the machine learning model. We compare the performance of the NGBosst model with four other commonly used Ensemble Learning methods, including Random Forest, GBDT, XGBoost, LightGBM. The results show that the NGBoost model performs well in the testi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#25913;&#36827;&#20256;&#32479;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#22522;&#20110;&#26085;&#24535;&#30340;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#20854;&#25928;&#26524;&#65292;&#20174;&#32780;&#20351;&#20854;&#19982;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2308.12612</link><description>&lt;p&gt;
&#23581;&#35797;&#26356;&#31616;&#21333;-&#25913;&#36827;&#20027;&#25104;&#20998;&#20998;&#26512;&#22312;&#22522;&#20110;&#26085;&#24535;&#30340;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Try with Simpler -- An Evaluation of Improved Principal Component Analysis in Log-based Anomaly Detection. (arXiv:2308.12612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#25913;&#36827;&#20256;&#32479;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#22522;&#20110;&#26085;&#24535;&#30340;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#20854;&#25928;&#26524;&#65292;&#20174;&#32780;&#20351;&#20854;&#19982;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#28608;&#21457;&#20102;&#23545;&#22686;&#24378;&#22522;&#20110;&#26085;&#24535;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#20852;&#36259;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20174;&#26085;&#24535;&#20107;&#20214;&#65288;&#26085;&#24535;&#28040;&#24687;&#27169;&#26495;&#65289;&#20013;&#25552;&#21462;&#24847;&#20041;&#65292;&#24182;&#24320;&#21457;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#38754;&#20020;&#30528;&#20381;&#36182;&#35757;&#32451;&#25968;&#25454;&#12289;&#26631;&#31614;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#27169;&#22411;&#22797;&#26434;&#24615;&#36739;&#39640;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#34429;&#28982;&#19981;&#22826;&#20381;&#36182;&#25968;&#25454;&#65292;&#26356;&#39640;&#25928;&#65292;&#20294;&#27604;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25928;&#26524;&#36739;&#24046;&#12290;&#20026;&#20102;&#20351;&#22522;&#20110;&#26085;&#24535;&#30340;&#24322;&#24120;&#26816;&#27979;&#26356;&#23454;&#29992;&#65292;&#30446;&#26631;&#26159;&#22686;&#24378;&#20256;&#32479;&#25216;&#26415;&#20197;&#36798;&#21040;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#20197;&#21069;&#22312;&#19981;&#21516;&#39046;&#22495;&#65288;&#38142;&#25509;Stack Overflow&#19978;&#30340;&#38382;&#39064;&#65289;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32463;&#36807;&#20248;&#21270;&#30340;&#20256;&#32479;&#25216;&#26415;&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;&#21463;&#21040;&#36825;&#19968;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#32463;&#39564;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#25972;&#21512;&#36731;&#37327;&#32423;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#20248;&#21270;&#26080;&#30417;&#30563;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#65292;&#19968;&#31181;&#20256;&#32479;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth of deep learning (DL) has spurred interest in enhancing log-based anomaly detection. This approach aims to extract meaning from log events (log message templates) and develop advanced DL models for anomaly detection. However, these DL methods face challenges like heavy reliance on training data, labels, and computational resources due to model complexity. In contrast, traditional machine learning and data mining techniques are less data-dependent and more efficient but less effective than DL. To make log-based anomaly detection more practical, the goal is to enhance traditional techniques to match DL's effectiveness. Previous research in a different domain (linking questions on Stack Overflow) suggests that optimized traditional techniques can rival state-of-the-art DL methods. Drawing inspiration from this concept, we conducted an empirical study. We optimized the unsupervised PCA (Principal Component Analysis), a traditional technique, by incorporating lightweight se
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#22871;&#39184;&#20248;&#21270;&#38382;&#39064;&#30340;&#26032;&#39062;&#32452;&#21512;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#38024;&#23545;&#30005;&#20449;&#36816;&#33829;&#21830;&#22312;&#36873;&#25321;&#28608;&#21169;&#22871;&#39184;&#21644;&#30446;&#26631;&#29992;&#25143;&#26102;&#38754;&#20020;&#30340;&#22256;&#38590;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2308.12606</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#21521;&#30005;&#20449;&#29992;&#25143;&#25552;&#20379;&#30340;&#36138;&#24515;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Greedy Approach for Offering to Telecom Subscribers. (arXiv:2308.12606v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#22871;&#39184;&#20248;&#21270;&#38382;&#39064;&#30340;&#26032;&#39062;&#32452;&#21512;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#38024;&#23545;&#30005;&#20449;&#36816;&#33829;&#21830;&#22312;&#36873;&#25321;&#28608;&#21169;&#22871;&#39184;&#21644;&#30446;&#26631;&#29992;&#25143;&#26102;&#38754;&#20020;&#30340;&#22256;&#38590;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23458;&#25143;&#20445;&#30041;&#25110;&#20943;&#23569;&#27969;&#22833;&#26159;&#30005;&#20449;&#36816;&#33829;&#21830;&#38754;&#20020;&#30340;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20854;&#20013;&#19968;&#20010;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#21521;&#29992;&#25143;&#25552;&#20379;&#19968;&#20123;&#26377;&#21560;&#24341;&#21147;&#30340;&#28608;&#21169;&#25514;&#26045;&#25110;&#38468;&#21152;&#26381;&#21153;&#25110;&#37329;&#38065;&#65292;&#20197;&#20445;&#25345;&#20182;&#20204;&#30340;&#21442;&#19982;&#24182;&#30830;&#20445;&#20182;&#20204;&#22312;&#36816;&#33829;&#21830;&#30340;&#32593;&#32476;&#20013;&#20572;&#30041;&#26356;&#38271;&#26102;&#38388;&#12290;&#36890;&#24120;&#65292;&#36816;&#33829;&#21830;&#20250;&#20998;&#37197;&#19968;&#23450;&#37329;&#39069;&#30340;&#39044;&#31639;&#26469;&#36827;&#34892;&#25512;&#24191;&#27963;&#21160;&#12290;&#36825;&#39033;&#27963;&#21160;&#30340;&#22256;&#38590;&#20043;&#22788;&#22312;&#20110;&#20174;&#24222;&#22823;&#30340;&#35746;&#25143;&#32676;&#20307;&#20013;&#36873;&#25321;&#19968;&#32452;&#23458;&#25143;&#65292;&#24182;&#20915;&#23450;&#24212;&#35813;&#21521;&#20010;&#20307;&#25552;&#20379;&#22810;&#23569;&#37329;&#39069;&#65292;&#20197;&#23454;&#29616;&#36816;&#33829;&#21830;&#30340;&#30446;&#26631;&#12290;&#36873;&#25321;&#35746;&#25143;&#21644;&#36873;&#25321;&#25552;&#20379;&#32473;&#34987;&#36873;&#23450;&#35746;&#25143;&#30340;&#22871;&#39184;&#21487;&#33021;&#26377;&#22810;&#20010;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#26368;&#22823;&#21270;&#25910;&#20837;&#65292;&#26368;&#23567;&#21270;&#27969;&#22833;&#25968;&#37327;&#65289;&#12290;&#38500;&#20102;&#37329;&#38065;&#21033;&#30410;&#65292;&#22871;&#39184;&#36824;&#21487;&#20197;&#21253;&#25324;&#39069;&#22806;&#30340;&#25968;&#25454;&#12289;&#30701;&#20449;&#12289;&#25163;&#26426;&#28909;&#28857;&#20849;&#20139;&#31561;&#31561;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#22871;&#39184;&#20248;&#21270;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32452;&#21512;&#31639;&#27861;&#26469;&#35299;&#20915;&#22871;&#39184;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Customer retention or churn prevention is a challenging task of a telecom operator. One of the effective approaches is to offer some attractive incentive or additional services or money to the subscribers for keeping them engaged and make sure they stay in the operator's network for longer time. Often, operators allocate certain amount of monetary budget to carry out the offer campaign. The difficult part of this campaign is the selection of a set of customers from a large subscriber-base and deciding the amount that should be offered to an individual so that operator's objective is achieved. There may be multiple objectives (e.g., maximizing revenue, minimizing number of churns) for selection of subscriber and selection of an offer to the selected subscriber. Apart from monetary benefit, offers may include additional data, SMS, hots-spot tethering, and many more. This problem is known as offer optimization. In this paper, we propose a novel combinatorial algorithm for solving offer op
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Conformer&#26550;&#26500;&#30340;&#38899;&#39057;&#22686;&#24378;&#31995;&#32479;&#65292;&#36890;&#36807;&#25506;&#32034;&#20854;&#27880;&#24847;&#26426;&#21046;&#24182;&#27979;&#35797;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#22312;&#21333;&#38899;&#20048;&#38899;&#39057;&#22686;&#24378;&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12599</link><description>&lt;p&gt;
&#21033;&#29992;&#26102;&#39057;&#36866;&#37197;&#22120;&#36827;&#34892;&#38899;&#20048;&#38899;&#39057;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Exploiting Time-Frequency Conformers for Music Audio Enhancement. (arXiv:2308.12599v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12599
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Conformer&#26550;&#26500;&#30340;&#38899;&#39057;&#22686;&#24378;&#31995;&#32479;&#65292;&#36890;&#36807;&#25506;&#32034;&#20854;&#27880;&#24847;&#26426;&#21046;&#24182;&#27979;&#35797;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#22312;&#21333;&#38899;&#20048;&#38899;&#39057;&#22686;&#24378;&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#19978;&#35270;&#39057;&#24179;&#21488;&#30340;&#26222;&#21450;&#65292;&#20351;&#29992;&#31227;&#21160;&#35774;&#22791;&#24405;&#21046;&#38899;&#20048;&#28436;&#20986;&#24050;&#32463;&#25104;&#20026;&#24120;&#35265;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24405;&#38899;&#24448;&#24448;&#21463;&#21040;&#22122;&#38899;&#21644;&#28151;&#21709;&#31561;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#23545;&#21548;&#20247;&#30340;&#20307;&#39564;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#38899;&#20048;&#38899;&#39057;&#22686;&#24378;&#65288;&#20197;&#19979;&#31616;&#31216;&#38899;&#39057;&#22686;&#24378;&#65289;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#65292;&#26088;&#22312;&#23558;&#21463;&#25439;&#30340;&#38899;&#39057;&#24405;&#38899;&#36716;&#21270;&#20026;&#39640;&#21697;&#36136;&#30340;&#38899;&#20048;&#65292;&#20197;&#25552;&#21319;&#21548;&#35273;&#20307;&#39564;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Conformer&#26550;&#26500;&#30340;&#38899;&#39057;&#22686;&#24378;&#31995;&#32479;&#65292;&#35813;&#26550;&#26500;&#22312;&#35821;&#38899;&#22686;&#24378;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25506;&#32034;&#20102;Conformer&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#27979;&#35797;&#20854;&#22312;&#38899;&#39057;&#22686;&#24378;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#21333;&#38899;&#20048;&#38899;&#39057;&#22686;&#24378;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the proliferation of video platforms on the internet, recording musical performances by mobile devices has become commonplace. However, these recordings often suffer from degradation such as noise and reverberation, which negatively impact the listening experience. Consequently, the necessity for music audio enhancement (referred to as music enhancement from this point onward), involving the transformation of degraded audio recordings into pristine high-quality music, has surged to augment the auditory experience. To address this issue, we propose a music enhancement system based on the Conformer architecture that has demonstrated outstanding performance in speech enhancement tasks. Our approach explores the attention mechanisms of the Conformer and examines their performance to discover the best approach for the music enhancement task. Our experimental results show that our proposed model achieves state-of-the-art performance on single-stem music enhancement. Furthermore, our sys
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#31070;&#32463;&#21160;&#21147;&#31995;&#32479;&#20013;&#65292;&#38500;&#20102;&#24191;&#27867;&#30740;&#31350;&#30340;&#36830;&#32493;&#21560;&#24341;&#23376;&#22806;&#65292;&#21608;&#26399;&#24615;&#21644;&#20934;&#21608;&#26399;&#24615;&#21560;&#24341;&#23376;&#20063;&#21487;&#20197;&#25903;&#25345;&#23398;&#20064;&#26102;&#38388;&#20851;&#31995;&#12290;&#30456;&#27604;&#20110;&#36830;&#32493;&#21560;&#24341;&#23376;&#65292;&#20934;&#21608;&#26399;&#24615;&#21560;&#24341;&#23376;&#26356;&#36866;&#21512;&#23398;&#20064;&#20135;&#29983;&#26102;&#38388;&#32467;&#26500;&#21270;&#34892;&#20026;&#65292;&#24182;&#19988;&#23545;&#20110;&#20154;&#24037;&#23398;&#20064;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#29983;&#29289;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#21487;&#35266;&#27979;&#29305;&#24449;&#37117;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.12585</link><description>&lt;p&gt;
&#27809;&#26377;&#36830;&#32493;&#21560;&#24341;&#23376;&#30340;&#25345;&#20037;&#23398;&#20064;&#20449;&#21495;&#21644;&#24037;&#20316;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Persistent learning signals and working memory without continuous attractors. (arXiv:2308.12585v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#31070;&#32463;&#21160;&#21147;&#31995;&#32479;&#20013;&#65292;&#38500;&#20102;&#24191;&#27867;&#30740;&#31350;&#30340;&#36830;&#32493;&#21560;&#24341;&#23376;&#22806;&#65292;&#21608;&#26399;&#24615;&#21644;&#20934;&#21608;&#26399;&#24615;&#21560;&#24341;&#23376;&#20063;&#21487;&#20197;&#25903;&#25345;&#23398;&#20064;&#26102;&#38388;&#20851;&#31995;&#12290;&#30456;&#27604;&#20110;&#36830;&#32493;&#21560;&#24341;&#23376;&#65292;&#20934;&#21608;&#26399;&#24615;&#21560;&#24341;&#23376;&#26356;&#36866;&#21512;&#23398;&#20064;&#20135;&#29983;&#26102;&#38388;&#32467;&#26500;&#21270;&#34892;&#20026;&#65292;&#24182;&#19988;&#23545;&#20110;&#20154;&#24037;&#23398;&#20064;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#29983;&#29289;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#21487;&#35266;&#27979;&#29305;&#24449;&#37117;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#21560;&#24341;&#23376;&#32467;&#26500;&#30340;&#31070;&#32463;&#21160;&#21147;&#31995;&#32479;&#65292;&#22914;&#28857;&#21560;&#24341;&#23376;&#21644;&#36830;&#32493;&#21560;&#24341;&#23376;&#65292;&#34987;&#20551;&#35774;&#20026;&#25903;&#25345;&#38656;&#35201;&#24037;&#20316;&#35760;&#24518;&#30340;&#26377;&#24847;&#20041;&#26102;&#38388;&#34892;&#20026;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#24037;&#20316;&#35760;&#24518;&#21487;&#33021;&#19981;&#25903;&#25345;&#36866;&#24212;&#29615;&#22659;&#26102;&#38388;&#32467;&#26500;&#21464;&#21270;&#25152;&#38656;&#30340;&#26377;&#29992;&#23398;&#20064;&#20449;&#21495;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38500;&#20102;&#24191;&#27867;&#28041;&#21450;&#30340;&#36830;&#32493;&#21560;&#24341;&#23376;&#22806;&#65292;&#21608;&#26399;&#24615;&#21644;&#20934;&#21608;&#26399;&#24615;&#21560;&#24341;&#23376;&#20063;&#21487;&#20197;&#25903;&#25345;&#23398;&#20064;&#20219;&#24847;&#38271;&#30340;&#26102;&#38388;&#20851;&#31995;&#12290;&#19982;&#21463;&#21040;&#24494;&#35843;&#38382;&#39064;&#22256;&#25200;&#30340;&#36830;&#32493;&#21560;&#24341;&#23376;&#19981;&#21516;&#65292;&#36739;&#23569;&#34987;&#25506;&#32034;&#30340;&#20934;&#21608;&#26399;&#24615;&#21560;&#24341;&#23376;&#26377;&#29420;&#29305;&#30340;&#23398;&#20064;&#20135;&#29983;&#26102;&#38388;&#32467;&#26500;&#21270;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#23545;&#20110;&#20154;&#24037;&#23398;&#20064;&#31995;&#32479;&#30340;&#35774;&#35745;&#20855;&#26377;&#24191;&#27867;&#30340;&#24433;&#21709;&#65292;&#24182;&#23545;&#33021;&#22815;&#25903;&#25345;&#26102;&#38388;&#20851;&#32852;&#23398;&#20064;&#21644;&#24037;&#20316;&#35760;&#24518;&#30340;&#29983;&#29289;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#21487;&#35266;&#27979;&#29305;&#24449;&#25552;&#20986;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural dynamical systems with stable attractor structures, such as point attractors and continuous attractors, are hypothesized to underlie meaningful temporal behavior that requires working memory. However, working memory may not support useful learning signals necessary to adapt to changes in the temporal structure of the environment. We show that in addition to the continuous attractors that are widely implicated, periodic and quasi-periodic attractors can also support learning arbitrarily long temporal relationships. Unlike the continuous attractors that suffer from the fine-tuning problem, the less explored quasi-periodic attractors are uniquely qualified for learning to produce temporally structured behavior. Our theory has broad implications for the design of artificial learning systems and makes predictions about observable signatures of biological neural dynamics that can support temporal dependence learning and working memory. Based on our theory, we developed a new initializ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LORD&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#30693;&#25968;&#25454;&#36827;&#34892;&#24320;&#25918;&#24335;&#35782;&#21035;&#12290;LORD&#22312;&#20998;&#31867;&#22120;&#35757;&#32451;&#36807;&#31243;&#20013;&#26126;&#30830;&#22320;&#24314;&#27169;&#24320;&#25918;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#35757;&#32451;&#31574;&#30053;&#23454;&#29616;&#20102;&#23545;&#26410;&#30693;&#25968;&#25454;&#35782;&#21035;&#33021;&#21147;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.12584</link><description>&lt;p&gt;
LORD: &#21033;&#29992;&#26410;&#30693;&#25968;&#25454;&#36827;&#34892;&#24320;&#25918;&#24335;&#35782;&#21035;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LORD: Leveraging Open-Set Recognition with Unknown Data. (arXiv:2308.12584v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LORD&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#30693;&#25968;&#25454;&#36827;&#34892;&#24320;&#25918;&#24335;&#35782;&#21035;&#12290;LORD&#22312;&#20998;&#31867;&#22120;&#35757;&#32451;&#36807;&#31243;&#20013;&#26126;&#30830;&#22320;&#24314;&#27169;&#24320;&#25918;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#35757;&#32451;&#31574;&#30053;&#23454;&#29616;&#20102;&#23545;&#26410;&#30693;&#25968;&#25454;&#35782;&#21035;&#33021;&#21147;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#23436;&#20840;&#26410;&#30693;&#25968;&#25454;&#23545;&#20110;&#20219;&#20309;&#37096;&#32626;&#30340;&#20998;&#31867;&#22120;&#37117;&#26159;&#19968;&#31181;&#25361;&#25112;&#12290;&#20998;&#31867;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#19968;&#20010;&#38745;&#24577;&#39044;&#23450;&#20041;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23545;&#20110;&#26410;&#20998;&#37197;&#30340;&#24320;&#25918;&#29305;&#24449;&#31354;&#38388;&#19968;&#26080;&#25152;&#30693;&#12290;&#22240;&#27492;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#23427;&#20204;&#24456;&#38590;&#22788;&#29702;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#25968;&#25454;&#12290;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#22312;&#31867;&#21035;&#32423;&#21035;&#19978;&#36827;&#34892;&#24320;&#25918;&#24335;&#35782;&#21035;&#65288;OSR&#65289;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;OSR&#26041;&#27861;&#26412;&#36136;&#19978;&#26159;&#26377;&#38480;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#35757;&#32451;&#30340;&#26159;&#23553;&#38381;&#38598;&#20998;&#31867;&#22120;&#65292;&#24182;&#19988;&#21482;&#35843;&#25972;&#20102;&#23545;OSR&#30340;&#19979;&#28216;&#39044;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LORD&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#30693;&#25968;&#25454;&#26469;&#23454;&#29616;&#24320;&#25918;&#24335;&#35782;&#21035;&#12290;LORD&#22312;&#20998;&#31867;&#22120;&#35757;&#32451;&#36807;&#31243;&#20013;&#26126;&#30830;&#22320;&#24314;&#27169;&#24320;&#25918;&#31354;&#38388;&#65292;&#24182;&#23545;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#20102;&#32972;&#26223;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#21040;&#24050;&#32463;&#24314;&#31435;&#30340;&#20998;&#31867;&#22120;&#19978;&#12290;&#30001;&#20110;LORD&#30340;&#24191;&#27867;&#35780;&#20272;&#21327;&#35758;&#65292;&#25105;&#20204;&#22987;&#32456;&#33021;&#22815;&#23637;&#31034;&#23545;&#26410;&#30693;&#25968;&#25454;&#30340;&#25913;&#36827;&#35782;&#21035;&#33021;&#21147;&#12290;&#36825;&#20123;&#22522;&#20934;&#26377;&#21161;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Handling entirely unknown data is a challenge for any deployed classifier. Classification models are typically trained on a static pre-defined dataset and are kept in the dark for the open unassigned feature space. As a result, they struggle to deal with out-of-distribution data during inference. Addressing this task on the class-level is termed open-set recognition (OSR). However, most OSR methods are inherently limited, as they train closed-set classifiers and only adapt the downstream predictions to OSR. This work presents LORD, a framework to Leverage Open-set Recognition by exploiting unknown Data. LORD explicitly models open space during classifier training and provides a systematic evaluation for such approaches. We identify three model-agnostic training strategies that exploit background data and applied them to well-established classifiers. Due to LORD's extensive evaluation protocol, we consistently demonstrate improved recognition of unknown data. The benchmarks facilitate i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Huber&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;&#26032;&#22411;&#32858;&#21512;&#22120;&#29992;&#20110;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#20551;&#35774;&#19979;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#21183;&#65292;&#24182;&#23545;&#38750;i.i.d&#25968;&#25454;&#36827;&#34892;&#20102;&#25193;&#23637;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.12581</link><description>&lt;p&gt;
&#19968;&#31181;Huber&#25439;&#22833;&#26368;&#23567;&#21270;&#26041;&#27861;&#29992;&#20110;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Huber Loss Minimization Approach to Byzantine Robust Federated Learning. (arXiv:2308.12581v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Huber&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;&#26032;&#22411;&#32858;&#21512;&#22120;&#29992;&#20110;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#20551;&#35774;&#19979;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#21183;&#65292;&#24182;&#23545;&#38750;i.i.d&#25968;&#25454;&#36827;&#34892;&#20102;&#25193;&#23637;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Huber&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;&#26032;&#22411;&#32858;&#21512;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;i.i.d&#65289;&#20551;&#35774;&#19979;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#23545;&#20110;&#34987;&#25915;&#20987;&#23458;&#25143;&#31471;&#27604;&#29575;$\epsilon$&#20855;&#26377;&#26368;&#20248;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;$\epsilon$&#26377;&#31934;&#30830;&#30340;&#30693;&#35782;&#12290;&#31532;&#19977;&#65292;&#23427;&#20801;&#35768;&#19981;&#21516;&#30340;&#23458;&#25143;&#31471;&#20855;&#26377;&#19981;&#22343;&#31561;&#30340;&#25968;&#25454;&#22823;&#23567;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;&#21253;&#25324;&#38750;i.i.d&#25968;&#25454;&#65292;&#36825;&#24847;&#21619;&#30528;&#23458;&#25143;&#31471;&#20855;&#26377;&#30053;&#26377;&#19981;&#21516;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning systems are susceptible to adversarial attacks. To combat this, we introduce a novel aggregator based on Huber loss minimization, and provide a comprehensive theoretical analysis. Under independent and identically distributed (i.i.d) assumption, our approach has several advantages compared to existing methods. Firstly, it has optimal dependence on $\epsilon$, which stands for the ratio of attacked clients. Secondly, our approach does not need precise knowledge of $\epsilon$. Thirdly, it allows different clients to have unequal data sizes. We then broaden our analysis to include non-i.i.d data, such that clients have slightly different distributions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36229;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;ICU&#24739;&#32773;&#30456;&#20284;&#24615;&#20998;&#26512;&#21644;&#39118;&#38505;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#25429;&#25417;&#38544;&#34255;&#30340;&#29305;&#24449;&#32467;&#26500;&#65292;&#24182;&#24212;&#29992;&#20110;&#20010;&#24615;&#21270;&#30340;&#27515;&#20129;&#39118;&#38505;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.12575</link><description>&lt;p&gt;
&#29992;&#20110;&#32454;&#31890;&#24230;ICU&#24739;&#32773;&#30456;&#20284;&#24615;&#20998;&#26512;&#21644;&#39118;&#38505;&#39044;&#27979;&#30340;&#36229;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Convolutional Networks for Fine-grained ICU Patient Similarity Analysis and Risk Prediction. (arXiv:2308.12575v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36229;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;ICU&#24739;&#32773;&#30456;&#20284;&#24615;&#20998;&#26512;&#21644;&#39118;&#38505;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#25429;&#25417;&#38544;&#34255;&#30340;&#29305;&#24449;&#32467;&#26500;&#65292;&#24182;&#24212;&#29992;&#20110;&#20010;&#24615;&#21270;&#30340;&#27515;&#20129;&#39118;&#38505;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#26159;&#21307;&#38498;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#20043;&#19968;&#65292;&#29992;&#20110;&#25910;&#27835;&#37325;&#30151;&#24739;&#32773;&#24182;&#25552;&#20379;&#36830;&#32493;&#30417;&#27979;&#21644;&#27835;&#30103;&#12290;&#24050;&#32463;&#23581;&#35797;&#20102;&#21508;&#31181;&#24739;&#32773;&#39044;&#27979;&#26041;&#27861;&#26469;&#36741;&#21161;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#12290;&#29616;&#26377;&#26041;&#27861;&#20391;&#37325;&#20110;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#34913;&#37327;&#24739;&#32773;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20197;&#25429;&#25417;&#38544;&#34255;&#30340;&#29305;&#24449;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#26356;&#39640;&#38454;&#30340;&#20851;&#31995;&#34987;&#24573;&#35270;&#20102;&#65292;&#20363;&#22914;&#24739;&#32773;&#29305;&#24449;&#65288;&#22914;&#35786;&#26029;&#20195;&#30721;&#65289;&#20197;&#21450;&#23427;&#20204;&#23545;&#19979;&#28216;&#20020;&#24202;&#39044;&#27979;&#30340;&#22240;&#26524;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36229;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#20801;&#35768;&#22312;&#36229;&#22270;&#20013;&#34920;&#31034;&#35786;&#26029;&#20195;&#30721;&#20043;&#38388;&#30340;&#38750;&#25104;&#23545;&#20851;&#31995;&#65292;&#20197;&#25429;&#25417;&#38544;&#34255;&#30340;&#29305;&#24449;&#32467;&#26500;&#65292;&#20174;&#32780;&#21487;&#20197;&#35745;&#31639;&#32454;&#31890;&#24230;&#24739;&#32773;&#30456;&#20284;&#24615;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#30340;&#27515;&#20129;&#39118;&#38505;&#39044;&#27979;&#12290;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;eICU&#21327;&#20316;&#30740;&#31350;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Intensive Care Unit (ICU) is one of the most important parts of a hospital, which admits critically ill patients and provides continuous monitoring and treatment. Various patient outcome prediction methods have been attempted to assist healthcare professionals in clinical decision-making. Existing methods focus on measuring the similarity between patients using deep neural networks to capture the hidden feature structures. However, the higher-order relationships are ignored, such as patient characteristics (e.g., diagnosis codes) and their causal effects on downstream clinical predictions.  In this paper, we propose a novel Hypergraph Convolutional Network that allows the representation of non-pairwise relationships among diagnosis codes in a hypergraph to capture the hidden feature structures so that fine-grained patient similarity can be calculated for personalized mortality risk prediction. Evaluation using a publicly available eICU Collaborative Research Database indicates that
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20197;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#29615;&#22659;&#20026;&#22522;&#30784;&#65292;&#20165;&#20973;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#36827;&#34892;&#20223;&#30495;&#23398;&#20064;&#65292;&#26080;&#38656;&#35775;&#38382;&#36716;&#31227;&#21160;&#21147;&#23398;&#20449;&#24687;&#12289;&#22870;&#21169;&#32467;&#26500;&#65292;&#25110;&#32773;&#20219;&#20309;&#39069;&#22806;&#30340;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2308.12573</link><description>&lt;p&gt;
&#26465;&#20214;&#26680;&#27169;&#20223;&#23398;&#20064;&#22312;&#36830;&#32493;&#29366;&#24577;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Conditional Kernel Imitation Learning for Continuous State Environments. (arXiv:2308.12573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20197;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#29615;&#22659;&#20026;&#22522;&#30784;&#65292;&#20165;&#20973;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#36827;&#34892;&#20223;&#30495;&#23398;&#20064;&#65292;&#26080;&#38656;&#35775;&#38382;&#36716;&#31227;&#21160;&#21147;&#23398;&#20449;&#24687;&#12289;&#22870;&#21169;&#32467;&#26500;&#65292;&#25110;&#32773;&#20219;&#20309;&#39069;&#22806;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#23398;&#20064;&#65288;IL&#65289;&#26159;&#22312;&#26356;&#24191;&#27867;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#20013;&#30340;&#37325;&#35201;&#33539;&#20363;&#12290;&#19982;&#22823;&#22810;&#25968;RL&#19981;&#21516;&#65292;&#23427;&#19981;&#20551;&#35774;&#22238;&#39304;&#22870;&#21169;&#30340;&#21487;&#29992;&#24615;&#12290;&#22870;&#21169;&#25512;&#26029;&#21644;&#22609;&#24418;&#24050;&#30693;&#26159;&#22256;&#38590;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#24403;&#28436;&#31034;&#25968;&#25454;&#26469;&#33258;&#20154;&#31867;&#19987;&#23478;&#26102;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#34892;&#20026;&#20811;&#38534;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#23545;&#20272;&#35745;&#35823;&#24046;&#38750;&#24120;&#25935;&#24863;&#65292;&#36825;&#22312;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#38382;&#39064;&#20013;&#23588;&#20026;&#20005;&#37325;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#20808;&#36827;&#30340;IL&#31639;&#27861;&#23558;&#34892;&#20026;&#31574;&#30053;&#23398;&#20064;&#38382;&#39064;&#36716;&#25442;&#20026;&#20998;&#24067;&#21305;&#37197;&#38382;&#39064;&#65292;&#36890;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#22312;&#32447;&#20132;&#20114;&#25968;&#25454;&#25165;&#33021;&#21457;&#25381;&#20316;&#29992;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#29615;&#22659;&#20013;&#20165;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#36827;&#34892;&#20223;&#30495;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#36716;&#31227;&#21160;&#21147;&#23398;&#20449;&#24687;&#12289;&#22870;&#21169;&#32467;&#26500;&#65292;&#25110;&#32773;&#26368;&#37325;&#35201;&#30340;&#26159;&#19981;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#20219;&#20309;&#39069;&#22806;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning (IL) is an important paradigm within the broader reinforcement learning (RL) methodology. Unlike most of RL, it does not assume availability of reward-feedback. Reward inference and shaping are known to be difficult and error-prone methods particularly when the demonstration data comes from human experts. Classical methods such as behavioral cloning and inverse reinforcement learning are highly sensitive to estimation errors, a problem that is particularly acute in continuous state space problems. Meanwhile, state-of-the-art IL algorithms convert behavioral policy learning problems into distribution-matching problems which often require additional online interaction data to be effective. In this paper, we consider the problem of imitation learning in continuous state space environments based solely on observed behavior, without access to transition dynamics information, reward structure, or, most importantly, any additional interactions with the environment. Our appr
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#26377;&#27745;&#26579;&#25968;&#25454;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21435;&#27745;&#21644;&#21464;&#37327;&#20381;&#36182;&#24314;&#27169;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#23545;&#20110;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.12563</link><description>&lt;p&gt;
&#38024;&#23545;&#24102;&#26377;&#27745;&#26579;&#25968;&#25454;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65306;&#23545;&#29983;&#29702;&#20449;&#21495;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multivariate Time-Series Anomaly Detection with Contaminated Data: Application to Physiological Signals. (arXiv:2308.12563v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12563
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#26377;&#27745;&#26579;&#25968;&#25454;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21435;&#27745;&#21644;&#21464;&#37327;&#20381;&#36182;&#24314;&#27169;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#23545;&#20110;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#27969;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#36890;&#24120;&#22312;&#23398;&#26415;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#21463;&#21040;&#25511;&#21046;&#23454;&#39564;&#26465;&#20214;&#19979;&#30340;&#28165;&#27905;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#21253;&#21547;&#22122;&#22768;&#30340;&#25361;&#25112;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#26412;&#30740;&#31350;&#22312;&#24863;&#30693;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65288;TSAD&#65289;&#20013;&#28145;&#20837;&#30740;&#31350;&#20102;&#26631;&#31614;&#32423;&#22122;&#22768;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#23454;&#29992;&#30340;&#31471;&#21040;&#31471;&#26080;&#30417;&#30563;TSAD&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#20013;&#21253;&#21547;&#24322;&#24120;&#30340;&#24773;&#20917;&#19979;&#12290;&#35813;&#26041;&#27861;&#31216;&#20026;TSAD-C&#65292;&#20854;&#22312;&#35757;&#32451;&#38454;&#27573;&#19981;&#38656;&#35201;&#35775;&#38382;&#24322;&#24120;&#26631;&#31614;&#12290;TSAD-C&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;&#19968;&#20010;&#21435;&#27745;&#22120;&#29992;&#20110;&#32416;&#27491;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#24322;&#24120;&#65288;&#20063;&#31216;&#20026;&#22122;&#22768;&#65289;&#65292;&#19968;&#20010;&#21464;&#37327;&#20381;&#36182;&#24314;&#27169;&#27169;&#22359;&#29992;&#20110;&#25429;&#25417;&#21435;&#27745;&#21518;&#25968;&#25454;&#20013;&#30340;&#38271;&#26399;&#20869;&#37096;&#21644;&#36328;&#21464;&#37327;&#20381;&#36182;&#20851;&#31995;&#65292;&#21487;&#20197;&#35270;&#20026;&#26367;&#20195;&#24615;&#30340;&#24322;&#24120;&#24615;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mainstream unsupervised anomaly detection algorithms often excel in academic datasets, yet their real-world performance is restricted due to the controlled experimental conditions involving clean training data. Addressing the challenge of training with noise, a prevalent issue in practical anomaly detection, is frequently overlooked. In a pioneering endeavor, this study delves into the realm of label-level noise within sensory time-series anomaly detection (TSAD). This paper presents a novel and practical end-to-end unsupervised TSAD when the training data are contaminated with anomalies. The introduced approach, called TSAD-C, is devoid of access to abnormality labels during the training phase. TSAD-C encompasses three modules: a Decontaminator to rectify the abnormalities (aka noise) present in the training data, a Variable Dependency Modeling module to capture both long-term intra- and inter-variable dependencies within the decontaminated data that can be considered as a surrogate o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#21464;&#20998;&#20449;&#24687;&#36861;&#27714;(V-IP)&#26694;&#26550;&#65292;&#36890;&#36807;&#39034;&#24207;&#36873;&#25321;&#20219;&#21153;&#30456;&#20851;&#30340;&#21487;&#35299;&#37322;&#26597;&#35810;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#26631;&#27880;&#30340;&#38480;&#21046;&#65292;&#24341;&#20837;&#20102;&#22522;&#30784;&#27169;&#22411;(FMs)&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;&#20505;&#36873;&#21487;&#35299;&#37322;&#27010;&#24565;&#38598;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#27880;&#37322;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#12290;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.12562</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#21487;&#35299;&#37322;&#39044;&#27979;&#30340;&#21464;&#20998;&#20449;&#24687;&#36861;&#27714;
&lt;/p&gt;
&lt;p&gt;
Variational Information Pursuit with Large Language and Multimodal Models for Interpretable Predictions. (arXiv:2308.12562v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#21464;&#20998;&#20449;&#24687;&#36861;&#27714;(V-IP)&#26694;&#26550;&#65292;&#36890;&#36807;&#39034;&#24207;&#36873;&#25321;&#20219;&#21153;&#30456;&#20851;&#30340;&#21487;&#35299;&#37322;&#26597;&#35810;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#26631;&#27880;&#30340;&#38480;&#21046;&#65292;&#24341;&#20837;&#20102;&#22522;&#30784;&#27169;&#22411;(FMs)&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;&#20505;&#36873;&#21487;&#35299;&#37322;&#27010;&#24565;&#38598;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#27880;&#37322;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#12290;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#20449;&#24687;&#36861;&#27714;(V-IP)&#26159;&#19968;&#20010;&#36890;&#36807;&#39034;&#24207;&#36873;&#25321;&#19982;&#20219;&#21153;&#30456;&#20851;&#12289;&#29992;&#25143;&#23450;&#20041;&#21644;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#26597;&#35810;&#26469;&#35774;&#35745;&#21487;&#35299;&#37322;&#39044;&#27979;&#30340;&#26694;&#26550;&#12290;&#34429;&#28982;&#36825;&#20351;&#24471;&#39044;&#27979;&#27169;&#22411;&#20855;&#26377;&#20869;&#32622;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#23558;V-IP&#24212;&#29992;&#20110;&#20219;&#20309;&#20219;&#21153;&#37117;&#38656;&#35201;&#20855;&#26377;&#30001;&#39046;&#22495;&#19987;&#23478;&#36827;&#34892;&#23494;&#38598;&#27010;&#24565;&#26631;&#27880;&#30340;&#25968;&#25454;&#26679;&#26412;&#65292;&#38480;&#21046;&#20102;V-IP&#22312;&#25163;&#21160;&#25968;&#25454;&#27880;&#37322;&#21487;&#34892;&#30340;&#23567;&#35268;&#27169;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22522;&#30784;&#27169;&#22411;(FMs)&#26469;&#25193;&#23637;V-IP&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20004;&#27493;&#27969;&#31243;&#65292;&#39318;&#20808;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;&#36275;&#22815;&#22823;&#30340;&#20505;&#36873;&#20219;&#21153;&#30456;&#20851;&#21487;&#35299;&#37322;&#27010;&#24565;&#38598;&#65292;&#28982;&#21518;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#36890;&#36807;&#19982;&#29983;&#25104;&#30340;&#27010;&#24565;&#38598;&#20013;&#30340;&#27599;&#20010;&#27010;&#24565;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#23545;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#27880;&#37322;&#12290;&#34429;&#28982;&#36824;&#26377;&#20854;&#20182;&#21487;&#35299;&#37322;&#35774;&#35745;&#26694;&#26550;&#65292;&#27604;&#22914;Concept Bot&#65292;&#20294;&#36825;&#20123;&#26694;&#26550;&#19981;&#36866;&#21512;&#22788;&#29702;&#22823;&#35268;&#27169;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational Information Pursuit (V-IP) is a framework for making interpretable predictions by design by sequentially selecting a short chain of task-relevant, user-defined and interpretable queries about the data that are most informative for the task. While this allows for built-in interpretability in predictive models, applying V-IP to any task requires data samples with dense concept-labeling by domain experts, limiting the application of V-IP to small-scale tasks where manual data annotation is feasible. In this work, we extend the V-IP framework with Foundational Models (FMs) to address this limitation. More specifically, we use a two-step process, by first leveraging Large Language Models (LLMs) to generate a sufficiently large candidate set of task-relevant interpretable concepts, then using Large Multimodal Models to annotate each data sample by semantic similarity with each concept in the generated concept set. While other interpretable-by-design frameworks such as Concept Bot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36328;&#31038;&#21306;&#33021;&#37327;&#20132;&#20114;&#20248;&#21270;&#35843;&#24230;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#31038;&#21306;&#30340;&#36127;&#36733;&#29305;&#24615;&#65292;&#24182;&#22522;&#20110;&#35813;&#30693;&#35782;&#20570;&#20986;&#20915;&#31574;&#65292;&#23454;&#29616;&#32508;&#21512;&#33021;&#37327;&#31995;&#32479;&#30340;&#25972;&#20307;&#20248;&#21270;&#21644;&#35843;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.12554</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36328;&#31038;&#21306;&#33021;&#37327;&#20132;&#20114;&#20248;&#21270;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning-driven Cross-Community Energy Interaction Optimal Scheduling. (arXiv:2308.12554v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36328;&#31038;&#21306;&#33021;&#37327;&#20132;&#20114;&#20248;&#21270;&#35843;&#24230;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#31038;&#21306;&#30340;&#36127;&#36733;&#29305;&#24615;&#65292;&#24182;&#22522;&#20110;&#35813;&#30693;&#35782;&#20570;&#20986;&#20915;&#31574;&#65292;&#23454;&#29616;&#32508;&#21512;&#33021;&#37327;&#31995;&#32479;&#30340;&#25972;&#20307;&#20248;&#21270;&#21644;&#35843;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#19981;&#30830;&#23450;&#30340;&#26465;&#20214;&#19979;&#21327;&#35843;&#21508;&#20010;&#31038;&#21306;&#20043;&#38388;&#30340;&#33021;&#37327;&#20132;&#20114;&#21644;&#22810;&#33021;&#28304;&#23376;&#31995;&#32479;&#20043;&#38388;&#30340;&#33021;&#37327;&#36716;&#25442;&#65292;&#24182;&#23454;&#29616;&#32508;&#21512;&#33021;&#37327;&#31995;&#32479;&#30340;&#25972;&#20307;&#20248;&#21270;&#21644;&#35843;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#35843;&#24230;&#27169;&#22411;&#65292;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23398;&#20064;&#19981;&#21516;&#31038;&#21306;&#30340;&#36127;&#36733;&#29305;&#24615;&#65292;&#24182;&#22522;&#20110;&#35813;&#30693;&#35782;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#32508;&#21512;&#33021;&#37327;&#31995;&#32479;&#30340;&#35843;&#24230;&#38382;&#39064;&#34987;&#36716;&#21270;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35299;&#20915;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#23545;&#22810;&#31038;&#21306;&#21644;&#22810;&#33021;&#28304;&#23376;&#31995;&#32479;&#20043;&#38388;&#22797;&#26434;&#33021;&#37327;&#32806;&#21512;&#20851;&#31995;&#30340;&#24314;&#27169;&#38656;&#27714;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#19981;&#21516;&#31038;&#21306;&#30340;&#36127;&#36733;&#29305;&#24615;&#65292;&#24182;&#21033;&#29992;&#20854;&#20114;&#34917;&#29305;&#24615;&#36827;&#34892;&#21327;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to coordinate energy interactions among various communities and energy conversions among multi-energy subsystems within the multi-community integrated energy system under uncertain conditions, and achieve overall optimization and scheduling of the comprehensive energy system, this paper proposes a comprehensive scheduling model that utilizes a multi-agent deep reinforcement learning algorithm to learn load characteristics of different communities and make decisions based on this knowledge. In this model, the scheduling problem of the integrated energy system is transformed into a Markov decision process and solved using a data-driven deep reinforcement learning algorithm, which avoids the need for modeling complex energy coupling relationships between multi-communities and multi-energy subsystems. The simulation results show that the proposed method effectively captures the load characteristics of different communities and utilizes their complementary features to coordinate re
&lt;/p&gt;</description></item><item><title>&#40664;&#35748;-ERM&#27169;&#22411;&#36890;&#36807;&#26368;&#22823;&#21270;&#38388;&#38548;&#26469;&#20248;&#21270;&#35757;&#32451;&#65292;&#23548;&#33268;&#27169;&#22411;&#26356;&#22810;&#20381;&#36182;&#20110;&#25463;&#24452;&#32780;&#38750;&#31283;&#23450;&#29305;&#24449;&#65292;&#36825;&#23545;&#24863;&#30693;&#20219;&#21153;&#26469;&#35828;&#26159;&#19981;&#21512;&#36866;&#30340;&#12290;</title><link>http://arxiv.org/abs/2308.12553</link><description>&lt;p&gt;
&#19981;&#35201;&#24618;&#25968;&#25454;&#38598;&#36716;&#31227;&#65281;&#26799;&#24230;&#21644;&#20132;&#21449;&#29109;&#23548;&#33268;&#20102;&#25463;&#24452;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Don't blame Dataset Shift! Shortcut Learning due to Gradients and Cross Entropy. (arXiv:2308.12553v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12553
&lt;/p&gt;
&lt;p&gt;
&#40664;&#35748;-ERM&#27169;&#22411;&#36890;&#36807;&#26368;&#22823;&#21270;&#38388;&#38548;&#26469;&#20248;&#21270;&#35757;&#32451;&#65292;&#23548;&#33268;&#27169;&#22411;&#26356;&#22810;&#20381;&#36182;&#20110;&#25463;&#24452;&#32780;&#38750;&#31283;&#23450;&#29305;&#24449;&#65292;&#36825;&#23545;&#24863;&#30693;&#20219;&#21153;&#26469;&#35828;&#26159;&#19981;&#21512;&#36866;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35265;&#23545;&#20110;&#25463;&#24452;&#23398;&#20064;&#30340;&#35299;&#37322;&#35748;&#20026;&#25463;&#24452;&#22312;&#35757;&#32451;&#20998;&#24067;&#19979;&#25913;&#21892;&#20102;&#39044;&#27979;&#32467;&#26524;&#65292;&#20294;&#22312;&#27979;&#35797;&#20998;&#24067;&#19979;&#21364;&#27809;&#26377;&#25913;&#21892;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20856;&#22411;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20132;&#21449;&#29109;&#20248;&#21270;&#35757;&#32451;&#30340;&#27169;&#22411;&#65288;&#25105;&#20204;&#31216;&#20854;&#20026;&#40664;&#35748;-ERM&#65289;&#21033;&#29992;&#20102;&#36825;&#20010;&#25463;&#24452;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#20998;&#24067;&#20013;&#31283;&#23450;&#29305;&#24449;&#20915;&#23450;&#20102;&#26631;&#31614;&#32780;&#25463;&#24452;&#24182;&#27809;&#26377;&#25552;&#20379;&#39069;&#22806;&#30340;&#20449;&#24687;&#65292;&#27604;&#22914;&#22312;&#24863;&#30693;&#20219;&#21153;&#20013;&#65292;&#40664;&#35748;-ERM&#20173;&#28982;&#34920;&#29616;&#20986;&#20102;&#25463;&#24452;&#23398;&#20064;&#12290;&#20026;&#20160;&#20040;&#36825;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#26356;&#21463;&#38738;&#30544;&#65292;&#24403;&#21487;&#20197;&#21333;&#29420;&#20351;&#29992;&#31283;&#23450;&#29305;&#24449;&#23558;&#40664;&#35748;-ERM&#30340;&#25439;&#22833;&#39537;&#21160;&#20026;&#38646;&#26102;&#65311;&#36890;&#36807;&#30740;&#31350;&#32447;&#24615;&#24863;&#30693;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#40664;&#35748;-ERM&#23545;&#20110;&#26368;&#22823;&#21270;&#38388;&#38548;&#30340;&#20559;&#22909;&#23548;&#33268;&#20102;&#26356;&#22810;&#20381;&#36182;&#20110;&#25463;&#24452;&#32780;&#38750;&#31283;&#23450;&#29305;&#24449;&#30340;&#27169;&#22411;&#65292;&#21363;&#20351;&#27809;&#26377;&#36807;&#24230;&#21442;&#25968;&#21270;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;&#40664;&#35748;-ERM&#30340;&#38544;&#24615;&#24402;&#32435;&#20559;&#22909;&#21363;&#26368;&#22823;&#38388;&#38548;&#23545;&#20110;&#24863;&#30693;&#20219;&#21153;&#26159;&#19981;&#21512;&#36866;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#21512;&#24863;&#30693;&#20219;&#21153;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Common explanations for shortcut learning assume that the shortcut improves prediction under the training distribution but not in the test distribution. Thus, models trained via the typical gradient-based optimization of cross-entropy, which we call default-ERM, utilize the shortcut. However, even when the stable feature determines the label in the training distribution and the shortcut does not provide any additional information, like in perception tasks, default-ERM still exhibits shortcut learning. Why are such solutions preferred when the loss for default-ERM can be driven to zero using the stable feature alone? By studying a linear perception task, we show that default-ERM's preference for maximizing the margin leads to models that depend more on the shortcut than the stable feature, even without overparameterization. This insight suggests that default-ERM's implicit inductive bias towards max-margin is unsuitable for perception tasks. Instead, we develop an inductive bias toward 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#23398;&#20064;&#30340;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#35270;&#22270;&#23398;&#20064;&#26469;&#20943;&#36731;&#25968;&#25454;&#22122;&#22768;&#21644;&#25439;&#22351;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#22522;&#20934;&#19978;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.12551</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#23398;&#20064;&#30340;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Co-training Approach for Noisy Time Series Learning. (arXiv:2308.12551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#23398;&#20064;&#30340;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#35270;&#22270;&#23398;&#20064;&#26469;&#20943;&#36731;&#25968;&#25454;&#22122;&#22768;&#21644;&#25439;&#22351;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#22522;&#20934;&#19978;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#40065;&#26834;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#29616;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#26159;&#22024;&#26434;&#30340;&#65292;&#24182;&#19988;&#26469;&#33258;&#21516;&#19968;&#26102;&#38388;&#24207;&#21015;&#30340;&#19981;&#21516;&#35266;&#28857;&#30340;&#20114;&#34917;&#20449;&#24687;&#22312;&#20998;&#26512;&#22024;&#26434;&#36755;&#20837;&#26102;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;&#32534;&#30721;&#22120;&#20026;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#21019;&#24314;&#20102;&#20004;&#20010;&#35270;&#22270;&#12290;&#25105;&#20204;&#36890;&#36807;&#21327;&#21516;&#35757;&#32451;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#24335;&#26469;&#36845;&#20195;&#23398;&#20064;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#35270;&#22270;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;TS-CoT&#26041;&#27861;&#21487;&#20197;&#20943;&#36731;&#25968;&#25454;&#22122;&#22768;&#21644;&#25439;&#22351;&#30340;&#24433;&#21709;&#12290;&#22312;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#23545;&#22235;&#20010;&#26102;&#38388;&#24207;&#21015;&#22522;&#20934;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;TS-CoT&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24494;&#35843;&#65292;TS-CoT&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#21487;&#20197;&#24456;&#22909;&#22320;&#36866;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we focus on robust time series representation learning. Our assumption is that real-world time series is noisy and complementary information from different views of the same time series plays an important role while analyzing noisy input. Based on this, we create two views for the input time series through two different encoders. We conduct co-training based contrastive learning iteratively to learn the encoders. Our experiments demonstrate that this co-training approach leads to a significant improvement in performance. Especially, by leveraging the complementary information from different views, our proposed TS-CoT method can mitigate the impact of data noise and corruption. Empirical evaluations on four time series benchmarks in unsupervised and semi-supervised settings reveal that TS-CoT outperforms existing methods. Furthermore, the representations learned by TS-CoT can transfer well to downstream tasks through fine-tuning.
&lt;/p&gt;</description></item><item><title>CALM&#26159;&#19968;&#20010;&#29992;&#20110;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#30456;&#27604;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.12539</link><description>&lt;p&gt;
CALM: &#19968;&#31181;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias. (arXiv:2308.12539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12539
&lt;/p&gt;
&lt;p&gt;
CALM&#26159;&#19968;&#20010;&#29992;&#20110;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#30456;&#27604;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#19981;&#26029;&#22686;&#24378;&#65292;&#37327;&#21270;&#21644;&#27604;&#36739;&#23427;&#20204;&#22312;&#31038;&#20250;&#21644;&#20154;&#21475;&#23398;&#20559;&#35265;&#26041;&#38754;&#30340;&#33021;&#21147;&#20197;&#21450;&#28508;&#22312;&#30340;&#21361;&#23475;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#20559;&#35265;&#27979;&#37327;&#25968;&#25454;&#38598;&#23545;&#20110;&#20154;&#24037;&#35774;&#35745;&#27169;&#26495;&#30340;&#25200;&#21160;&#25935;&#24863;&#65292;&#22240;&#27492;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#20445;&#35777;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20840;&#38754;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#65288;CALM&#65289;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#37327;&#21270;LMs&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#65288;&#22914;&#32500;&#22522;&#30334;&#31185;&#21644;&#26032;&#38395;&#25991;&#31456;&#65289;&#30340;16&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#36807;&#28388;&#20986;224&#20010;&#27169;&#26495;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;78,400&#20010;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#24179;&#22343;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#27169;&#26495;&#38271;&#24230;&#30340;&#21464;&#24322;&#31243;&#24230;&#31561;&#25351;&#26631;&#65292;&#27604;&#36739;CALM&#19982;&#20808;&#21069;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#27979;&#35797;&#20854;&#23545;&#32454;&#24494;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30456;&#23545;&#20110;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#22240;&#27492;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;20&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models (LMs) become increasingly powerful, it is important to quantify and compare them for sociodemographic bias with potential for harm. Prior bias measurement datasets are sensitive to perturbations in their manually designed templates, therefore unreliable. To achieve reliability, we introduce the Comprehensive Assessment of Language Model bias (CALM), a benchmark dataset to quantify bias in LMs across three tasks. We integrate 16 existing datasets across different domains, such as Wikipedia and news articles, to filter 224 templates from which we construct a dataset of 78,400 examples. We compare the diversity of CALM with prior datasets on metrics such as average semantic similarity, and variation in template length, and test the sensitivity to small perturbations. We show that our dataset is more diverse and reliable than previous datasets, thus better capture the breadth of linguistic variation required to reliably evaluate model bias. We evaluate 20 large language 
&lt;/p&gt;</description></item><item><title>FedSoL&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#24179;&#34913;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#26469;&#25913;&#21892;FL&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.12532</link><description>&lt;p&gt;
FedSoL: &#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FedSoL: Bridging Global Alignment and Local Generality in Federated Learning. (arXiv:2308.12532v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12532
&lt;/p&gt;
&lt;p&gt;
FedSoL&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#24179;&#34913;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#26469;&#25913;&#21892;FL&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Federated Learning, FL)&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;&#20010;&#20307;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#26469;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#34429;&#28982;FL&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#24403;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#26102;&#65292;&#24120;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;FL&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#21508;&#31181;&#36817;&#20284;&#32422;&#26463;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#32422;&#26463;&#26088;&#22312;&#36890;&#36807;&#38480;&#21046;&#23616;&#37096;&#23398;&#20064;&#19982;&#20840;&#23616;&#30446;&#26631;&#30340;&#20559;&#31163;&#26469;&#20419;&#36827;&#20840;&#23616;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26412;&#36136;&#19978;&#36890;&#36807;&#24178;&#25200;&#21407;&#22987;&#30340;&#23616;&#37096;&#30446;&#26631;&#32780;&#38480;&#21046;&#20102;&#23616;&#37096;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#25913;&#21892;&#26412;&#22320;&#23398;&#20064;&#30340;&#19968;&#33324;&#24615;&#12290;&#36890;&#36807;&#22312;&#24179;&#28369;&#30340;&#25439;&#22833;&#31354;&#38388;&#20013;&#33719;&#24471;&#26412;&#22320;&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#20943;&#36731;&#20102;&#23458;&#25143;&#31471;&#19981;&#21516;&#26412;&#22320;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#28982;&#32780;&#65292;&#23427;&#19981;&#33021;&#30830;&#20445;&#31283;&#23450;&#30340;&#20840;&#23616;&#23545;&#40784;&#65292;&#22240;&#20026;&#26412;&#22320;&#23398;&#20064;&#19981;&#32771;&#34385;&#20840;&#23616;&#30446;&#26631;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#31283;&#23450;&#24615;(FedSoL)&#26041;&#27861;&#26469;&#22312;FL&#20013;&#35299;&#20915;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) aggregates locally trained models from individual clients to construct a global model. While FL enables learning a model with data privacy, it often suffers from significant performance degradation when client data distributions are heterogeneous. Many previous FL algorithms have addressed this issue by introducing various proximal restrictions. These restrictions aim to encourage global alignment by constraining the deviation of local learning from the global objective. However, they inherently limit local learning by interfering with the original local objectives. Recently, an alternative approach has emerged to improve local learning generality. By obtaining local models within a smooth loss landscape, this approach mitigates conflicts among different local objectives of the clients. Yet, it does not ensure stable global alignment, as local learning does not take the global objective into account. In this study, we propose Federated Stability on Learning (Fed
&lt;/p&gt;</description></item><item><title>SieveNet&#26159;&#19968;&#31181;&#26032;&#30340;&#32593;&#29366;&#32593;&#32476;&#33539;&#24335;&#65292;&#32467;&#21512;&#20102;&#35268;&#21017;&#25299;&#25169;&#21644;&#20934;&#30830;&#20960;&#20309;&#20449;&#24687;&#65292;&#36890;&#36807;&#37325;&#32593;&#29366;&#21644;&#28857;&#37319;&#26679;&#30340;&#26041;&#27861;&#25552;&#21462;&#29305;&#24449;&#65292;&#28040;&#38500;&#20102;&#23545;&#25163;&#24037;&#29305;&#24449;&#24037;&#31243;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;3D&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#24418;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.12530</link><description>&lt;p&gt;
SieveNet&#65306;&#36873;&#25321;&#22522;&#20110;&#28857;&#30340;&#29305;&#24449;&#29992;&#20110;&#32593;&#29366;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SieveNet: Selecting Point-Based Features for Mesh Networks. (arXiv:2308.12530v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12530
&lt;/p&gt;
&lt;p&gt;
SieveNet&#26159;&#19968;&#31181;&#26032;&#30340;&#32593;&#29366;&#32593;&#32476;&#33539;&#24335;&#65292;&#32467;&#21512;&#20102;&#35268;&#21017;&#25299;&#25169;&#21644;&#20934;&#30830;&#20960;&#20309;&#20449;&#24687;&#65292;&#36890;&#36807;&#37325;&#32593;&#29366;&#21644;&#28857;&#37319;&#26679;&#30340;&#26041;&#27861;&#25552;&#21462;&#29305;&#24449;&#65292;&#28040;&#38500;&#20102;&#23545;&#25163;&#24037;&#29305;&#24449;&#24037;&#31243;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;3D&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#24418;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#29366;&#22312;&#19977;&#32500;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#24418;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23427;&#20204;&#30340;&#19981;&#35268;&#21017;&#25299;&#25169;&#32467;&#26500;&#32473;&#29616;&#26377;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#24212;&#29992;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26368;&#26032;&#30340;&#32593;&#29366;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#23637;&#36716;&#21521;&#20102;&#37325;&#32593;&#29366;&#21644;&#25512;&#21160;&#20102;&#20165;&#23558;&#21407;&#22987;&#32593;&#29366;&#20316;&#20026;&#36755;&#20837;&#30340;&#20808;&#39537;&#26041;&#27861;&#30340;&#36793;&#30028;&#12290;&#34429;&#28982;&#37325;&#32593;&#29366;&#25552;&#20379;&#20102;&#19968;&#31181;&#35268;&#21017;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#20174;&#32780;&#26174;&#33879;&#20419;&#36827;&#20102;&#32593;&#29366;&#32593;&#32476;&#26550;&#26500;&#30340;&#35774;&#35745;&#65292;&#20294;&#26159;&#20174;&#36825;&#31181;&#37325;&#32593;&#29366;&#20195;&#29702;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#21487;&#33021;&#20250;&#38590;&#20197;&#20934;&#30830;&#20445;&#30041;&#24213;&#23618;&#20960;&#20309;&#24615;&#36136;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#21518;&#32493;&#31070;&#32463;&#32593;&#32476;&#30340;&#23481;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SieveNet&#65292;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#35268;&#21017;&#25299;&#25169;&#21644;&#20934;&#30830;&#20960;&#20309;&#30340;&#26032;&#33539;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#20174;&#37325;&#32593;&#29366;&#20013;&#25552;&#21462;&#24471;&#21040;&#30340;&#32467;&#26500;&#21270;&#32593;&#29366;&#25299;&#25169;&#20197;&#21450;&#21407;&#22987;&#32593;&#29366;&#34920;&#38754;&#19978;&#22522;&#20110;&#30072;&#21464;&#24863;&#30693;&#30340;&#28857;&#37319;&#26679;&#24471;&#21040;&#30340;&#20934;&#30830;&#20960;&#20309;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#23545;&#25163;&#24037;&#29305;&#24449;&#24037;&#31243;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#21487;&#20197;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meshes are widely used in 3D computer vision and graphics, but their irregular topology poses challenges in applying them to existing neural network architectures. Recent advances in mesh neural networks turn to remeshing and push the boundary of pioneer methods that solely take the raw meshes as input. Although the remeshing offers a regular topology that significantly facilitates the design of mesh network architectures, features extracted from such remeshed proxies may struggle to retain the underlying geometry faithfully, limiting the subsequent neural network's capacity. To address this issue, we propose SieveNet, a novel paradigm that takes into account both the regular topology and the exact geometry. Specifically, this method utilizes structured mesh topology from remeshing and accurate geometric information from distortion-aware point sampling on the surface of the original mesh. Furthermore, our method eliminates the need for hand-crafted feature engineering and can leverage 
&lt;/p&gt;</description></item><item><title>UNISOUND&#22242;&#38431;&#22312;VoxCeleb Speaker Recognition Challenge 2023&#20013;&#25552;&#20132;&#30340;&#31995;&#32479;&#36890;&#36807;&#19968;&#33268;&#24615;&#24863;&#30693;&#30340;&#24471;&#20998;&#26657;&#20934;&#26041;&#27861;&#21644;&#22823;&#35268;&#27169;ResNet&#21644;RepVGG&#26550;&#26500;&#30340;&#20351;&#29992;&#65292;&#22312;Track1&#20013;&#33719;&#24471;&#31532;&#19968;&#21517;&#65292;&#22312;Track2&#20013;&#33719;&#24471;&#31532;&#20108;&#21517;&#65292;&#24182;&#23454;&#29616;&#20102;&#36739;&#20302;&#30340;minDCF&#21644;EER&#12290;</title><link>http://arxiv.org/abs/2308.12526</link><description>&lt;p&gt;
UNISOUND&#31995;&#32479;&#23545;VoxCeleb Speaker Recognition Challenge 2023&#30340;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
UNISOUND System for VoxCeleb Speaker Recognition Challenge 2023. (arXiv:2308.12526v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12526
&lt;/p&gt;
&lt;p&gt;
UNISOUND&#22242;&#38431;&#22312;VoxCeleb Speaker Recognition Challenge 2023&#20013;&#25552;&#20132;&#30340;&#31995;&#32479;&#36890;&#36807;&#19968;&#33268;&#24615;&#24863;&#30693;&#30340;&#24471;&#20998;&#26657;&#20934;&#26041;&#27861;&#21644;&#22823;&#35268;&#27169;ResNet&#21644;RepVGG&#26550;&#26500;&#30340;&#20351;&#29992;&#65292;&#22312;Track1&#20013;&#33719;&#24471;&#31532;&#19968;&#21517;&#65292;&#22312;Track2&#20013;&#33719;&#24471;&#31532;&#20108;&#21517;&#65292;&#24182;&#23454;&#29616;&#20102;&#36739;&#20302;&#30340;minDCF&#21644;EER&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#25551;&#36848;&#20102;UNISOUND&#22312;VoxCeleb Speaker Recognition Challenge 2023&#65288;VoxSRC 2023&#65289;&#30340;Track1&#21644;Track2&#20013;&#30340;&#25552;&#20132;&#12290;&#25105;&#20204;&#22312;Track 1&#21644;Track 2&#19978;&#25552;&#20132;&#20102;&#30456;&#21516;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20165;&#20351;&#29992;VoxCeleb2-dev&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#20026;&#25361;&#25112;&#20219;&#21153;&#24320;&#21457;&#20102;&#22823;&#35268;&#27169;ResNet&#21644;RepVGG&#26550;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#24863;&#30693;&#30340;&#24471;&#20998;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#33268;&#24615;&#27979;&#37327;&#22240;&#23376;&#65288;CMF&#65289;&#21033;&#29992;&#22768;&#38899;&#29305;&#24449;&#30340;&#31283;&#23450;&#24615;&#26469;&#25552;&#39640;&#30456;&#20284;&#24615;&#24471;&#20998;&#30340;&#31283;&#23450;&#24615;&#12290;CMF&#20351;&#25105;&#20204;&#22312;&#35813;&#25361;&#25112;&#20013;&#33719;&#24471;&#20102;&#24040;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#26368;&#32456;&#30340;&#31995;&#32479;&#26159;&#20845;&#20010;&#27169;&#22411;&#30340;&#34701;&#21512;&#65292;&#22312;VoxSRC 2023&#30340;Track1&#20013;&#33719;&#24471;&#31532;&#19968;&#21517;&#65292;&#22312;Track2&#20013;&#33719;&#24471;&#31532;&#20108;&#21517;&#12290;&#25105;&#20204;&#30340;&#25552;&#20132;&#30340;minDCF&#20026;0.0855&#65292;EER&#20026;1.5880%&#12290;
&lt;/p&gt;
&lt;p&gt;
This report describes the UNISOUND submission for Track1 and Track2 of VoxCeleb Speaker Recognition Challenge 2023 (VoxSRC 2023). We submit the same system on Track 1 and Track 2, which is trained with only VoxCeleb2-dev. Large-scale ResNet and RepVGG architectures are developed for the challenge. We propose a consistency-aware score calibration method, which leverages the stability of audio voiceprints in similarity score by a Consistency Measure Factor (CMF). CMF brings a huge performance boost in this challenge. Our final system is a fusion of six models and achieves the first place in Track 1 and second place in Track 2 of VoxSRC 2023. The minDCF of our submission is 0.0855 and the EER is 1.5880%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20026;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#26469;&#22788;&#29702;&#32422;&#26463;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#19981;&#21516;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#35757;&#32451;&#20013;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12517</link><description>&lt;p&gt;
&#19981;&#20165;&#20165;&#22870;&#21169;&#65292;&#36824;&#26377;&#32422;&#26463;&#65306;&#29992;&#20110;&#33151;&#24335;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion. (arXiv:2308.12517v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20026;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#26469;&#22788;&#29702;&#32422;&#26463;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#19981;&#21516;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#35757;&#32451;&#20013;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#30340;&#19968;&#20123;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#24182;&#20351;&#29992;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#65292;&#23637;&#31034;&#20102;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20855;&#26377;&#33258;&#28982;&#21160;&#20316;&#39118;&#26684;&#21644;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#20986;&#33394;&#25511;&#21046;&#22120;&#26159;&#36890;&#36807;&#36827;&#34892;&#22823;&#37327;&#22870;&#21169;&#24037;&#31243;&#32780;&#24320;&#21457;&#30340;&#65292;&#35813;&#36807;&#31243;&#38750;&#24120;&#36153;&#26102;&#36153;&#21147;&#65292;&#38656;&#35201;&#35774;&#35745;&#22823;&#37327;&#22870;&#21169;&#39033;&#24182;&#30830;&#23450;&#21512;&#36866;&#30340;&#22870;&#21169;&#31995;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#21516;&#26102;&#21253;&#21547;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#20026;&#20102;&#35753;&#24037;&#31243;&#24072;&#33021;&#22815;&#36866;&#24403;&#22320;&#21453;&#26144;&#20182;&#20204;&#23545;&#32422;&#26463;&#30340;&#24847;&#22270;&#24182;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#22788;&#29702;&#23427;&#20204;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32422;&#26463;&#31867;&#22411;&#21644;&#19968;&#31181;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#12290;&#35813;&#23398;&#20064;&#26694;&#26550;&#34987;&#24212;&#29992;&#20110;&#35757;&#32451;&#19981;&#21516;&#24418;&#24577;&#21644;&#29289;&#29702;&#23646;&#24615;&#30340;&#20960;&#20010;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several earlier studies have shown impressive control performance in complex robotic systems by designing the controller using a neural network and training it with model-free reinforcement learning. However, these outstanding controllers with natural motion style and high task performance are developed through extensive reward engineering, which is a highly laborious and time-consuming process of designing numerous reward terms and determining suitable reward coefficients. In this work, we propose a novel reinforcement learning framework for training neural network controllers for complex robotic systems consisting of both rewards and constraints. To let the engineers appropriately reflect their intent to constraints and handle them with minimal computation overhead, two constraint types and an efficient policy optimization algorithm are suggested. The learning framework is applied to train locomotion controllers for several legged robots with different morphology and physical attribu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;(MAEs)&#20316;&#20026;&#39640;&#25928;&#30340;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#22120;&#65292;&#36890;&#36807;&#37325;&#24314;&#21407;&#22987;&#36755;&#20837;&#22270;&#20687;&#21644;&#23398;&#20064;&#22270;&#20687;&#32423;&#21644;&#23884;&#20837;&#32423;&#34701;&#21512;&#26469;&#23384;&#20648;&#21644;&#23398;&#20064;&#36807;&#21435;&#20219;&#21153;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#22312;CIFAR-100&#65292;ImageNet-Subset&#21644;ImageNet-Full&#19978;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.12510</link><description>&lt;p&gt;
Masked Autoencoders&#26159;&#39640;&#25928;&#30340;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Masked Autoencoders are Efficient Class Incremental Learners. (arXiv:2308.12510v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;(MAEs)&#20316;&#20026;&#39640;&#25928;&#30340;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#22120;&#65292;&#36890;&#36807;&#37325;&#24314;&#21407;&#22987;&#36755;&#20837;&#22270;&#20687;&#21644;&#23398;&#20064;&#22270;&#20687;&#32423;&#21644;&#23884;&#20837;&#32423;&#34701;&#21512;&#26469;&#23384;&#20648;&#21644;&#23398;&#20064;&#36807;&#21435;&#20219;&#21153;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#22312;CIFAR-100&#65292;ImageNet-Subset&#21644;ImageNet-Full&#19978;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;(CIL)&#26088;&#22312;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#21516;&#26102;&#36991;&#20813;&#23545;&#20043;&#21069;&#30693;&#35782;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;(MAEs)&#20316;&#20026;CIL&#30340;&#39640;&#25928;&#23398;&#20064;&#22120;&#12290;MAEs&#26368;&#21021;&#26159;&#20026;&#20102;&#36890;&#36807;&#37325;&#24314;&#26080;&#30417;&#30563;&#23398;&#20064;&#26469;&#23398;&#20064;&#26377;&#29992;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#23427;&#20204;&#21487;&#20197;&#36731;&#26494;&#22320;&#19982;&#30417;&#30563;&#25439;&#22833;&#32467;&#21512;&#20197;&#29992;&#20110;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;MAEs&#21487;&#20197;&#21487;&#38752;&#22320;&#20174;&#38543;&#26426;&#36873;&#25321;&#30340;&#22270;&#20687;&#34917;&#19969;&#20013;&#37325;&#24314;&#21407;&#22987;&#36755;&#20837;&#22270;&#20687;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#26356;&#39640;&#25928;&#22320;&#23384;&#20648;&#36807;&#21435;&#20219;&#21153;&#30340;&#33539;&#20363;&#29992;&#20110;CIL&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21452;&#36793;MAE&#26694;&#26550;&#26469;&#23398;&#20064;&#22270;&#20687;&#32423;&#21644;&#23884;&#20837;&#32423;&#34701;&#21512;&#65292;&#23427;&#21487;&#20197;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#22270;&#20687;&#21644;&#26356;&#31283;&#23450;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;CIFAR-100&#65292;ImageNet-Subset&#21644;ImageNet-Full&#19978;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/scok30/MAE-CIL&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class Incremental Learning (CIL) aims to sequentially learn new classes while avoiding catastrophic forgetting of previous knowledge. We propose to use Masked Autoencoders (MAEs) as efficient learners for CIL. MAEs were originally designed to learn useful representations through reconstructive unsupervised learning, and they can be easily integrated with a supervised loss for classification. Moreover, MAEs can reliably reconstruct original input images from randomly selected patches, which we use to store exemplars from past tasks more efficiently for CIL. We also propose a bilateral MAE framework to learn from image-level and embedding-level fusion, which produces better-quality reconstructed images and more stable representations. Our experiments confirm that our approach performs better than the state-of-the-art on CIFAR-100, ImageNet-Subset, and ImageNet-Full. The code is available at https://github.com/scok30/MAE-CIL .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32508;&#21512;&#22810;&#23398;&#31185;&#30340;&#35265;&#35299;&#65292;&#23545;&#31038;&#20132;&#23186;&#20307;&#25805;&#32437;&#30340;&#35201;&#32032;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#65292;&#21253;&#25324;&#34394;&#20551;&#20449;&#24687;&#12289;&#26426;&#22120;&#20154;&#21644;&#24694;&#24847;&#23459;&#20256;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.12497</link><description>&lt;p&gt;
&#38169;&#35823;&#20449;&#24687;&#65292;&#26426;&#22120;&#20154;&#21644;&#24694;&#24847;&#23459;&#20256;&#65306;&#25581;&#31192;&#31038;&#20132;&#23186;&#20307;&#25805;&#32437;&#30340;&#35201;&#32032;
&lt;/p&gt;
&lt;p&gt;
False Information, Bots and Malicious Campaigns: Demystifying Elements of Social Media Manipulations. (arXiv:2308.12497v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32508;&#21512;&#22810;&#23398;&#31185;&#30340;&#35265;&#35299;&#65292;&#23545;&#31038;&#20132;&#23186;&#20307;&#25805;&#32437;&#30340;&#35201;&#32032;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#65292;&#21253;&#25324;&#34394;&#20551;&#20449;&#24687;&#12289;&#26426;&#22120;&#20154;&#21644;&#24694;&#24847;&#23459;&#20256;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#30340;&#24555;&#36895;&#20256;&#25773;&#21644;&#23545;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#30340;&#25345;&#32493;&#25805;&#32437;&#25915;&#20987;&#65292;&#24448;&#24448;&#26159;&#20986;&#20110;&#25919;&#27835;&#12289;&#24847;&#35782;&#24418;&#24577;&#25110;&#32463;&#27982;&#21033;&#30410;&#65292;&#24050;&#32463;&#24433;&#21709;&#20102;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#30340;&#24320;&#25918;&#24615;&#12290;&#34429;&#28982;&#26469;&#33258;&#19981;&#21516;&#23398;&#31185;&#30340;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#35843;&#26597;&#20102;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#19981;&#21516;&#25805;&#32437;&#24341;&#21457;&#35201;&#32032;&#65288;&#22914;&#20102;&#35299;&#20449;&#24687;&#22312;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#19978;&#30340;&#20256;&#25773;&#25110;&#26816;&#27979;&#36134;&#25143;&#30340;&#33258;&#21160;&#21270;&#34892;&#20026;&#65289;&#65292;&#20294;&#36825;&#20123;&#24037;&#20316;&#27809;&#26377;&#34987;&#25972;&#21512;&#36215;&#26469;&#20197;&#25552;&#20379;&#23545;&#36825;&#20123;&#35201;&#32032;&#20043;&#38388;&#30456;&#20114;&#20851;&#31995;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#24573;&#35270;&#20102;&#29992;&#25143;&#24515;&#29702;&#12289;&#26426;&#22120;&#20154;&#30340;&#26222;&#21450;&#31243;&#24230;&#20197;&#21450;&#23427;&#20204;&#19982;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#31574;&#30053;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#26412;&#25991;&#32508;&#21512;&#20102;&#26469;&#33258;&#21508;&#20010;&#23398;&#31185;&#30340;&#35265;&#35299;&#65292;&#25552;&#20379;&#20102;&#23545;&#25805;&#32437;&#39046;&#22495;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#36890;&#36807;&#25972;&#21512;&#31038;&#20132;&#23186;&#20307;&#25805;&#32437;&#30340;&#20027;&#35201;&#35201;&#32032;&#65292;&#21253;&#25324;&#34394;&#20551;&#20449;&#24687;&#12289;&#26426;&#22120;&#20154;&#21644;&#24694;&#24847;&#23459;&#20256;&#65292;&#25105;&#20204;&#24191;&#27867;&#22320;&#30740;&#31350;&#20102;&#27599;&#20010;&#31038;&#20132;&#23186;&#20307;&#25805;&#32437;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid spread of false information and persistent manipulation attacks on online social networks (OSNs), often for political, ideological, or financial gain, has affected the openness of OSNs. While researchers from various disciplines have investigated different manipulation-triggering elements of OSNs (such as understanding information diffusion on OSNs or detecting automated behavior of accounts), these works have not been consolidated to present a comprehensive overview of the interconnections among these elements. Notably, user psychology, the prevalence of bots, and their tactics in relation to false information detection have been overlooked in previous research. To address this research gap, this paper synthesizes insights from various disciplines to provide a comprehensive analysis of the manipulation landscape. By integrating the primary elements of social media manipulation (SMM), including false information, bots, and malicious campaigns, we extensively examine each SMM 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32553;&#25918;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#24515;&#30005;&#22270;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20248;&#21270;&#32593;&#32476;&#35268;&#27169;&#30340;&#20851;&#38190;&#21442;&#25968;&#65288;&#23618;&#28145;&#24230;&#12289;&#36890;&#36947;&#25968;&#21644;&#21367;&#31215;&#26680;&#22823;&#23567;&#65289;&#65292;&#24471;&#20986;&#20102;&#22312;ECG&#20998;&#31867;&#20013;&#36739;&#27973;&#30340;&#32593;&#32476;&#12289;&#36739;&#22823;&#30340;&#36890;&#36947;&#25968;&#21644;&#36739;&#23567;&#30340;&#21367;&#31215;&#26680;&#23610;&#23544;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2308.12492</link><description>&lt;p&gt;
&#20248;&#21270;&#24515;&#30005;&#22270;&#20998;&#31867;&#20013;&#31070;&#32463;&#32593;&#32476;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Optimizing Neural Network Scale for ECG Classification. (arXiv:2308.12492v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12492
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32553;&#25918;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#24515;&#30005;&#22270;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20248;&#21270;&#32593;&#32476;&#35268;&#27169;&#30340;&#20851;&#38190;&#21442;&#25968;&#65288;&#23618;&#28145;&#24230;&#12289;&#36890;&#36947;&#25968;&#21644;&#21367;&#31215;&#26680;&#22823;&#23567;&#65289;&#65292;&#24471;&#20986;&#20102;&#22312;ECG&#20998;&#31867;&#20013;&#36739;&#27973;&#30340;&#32593;&#32476;&#12289;&#36739;&#22823;&#30340;&#36890;&#36947;&#25968;&#21644;&#36739;&#23567;&#30340;&#21367;&#31215;&#26680;&#23610;&#23544;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32553;&#25918;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#20855;&#20307;&#38024;&#23545;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#65288;ResNet&#65289;&#65292;&#29992;&#20110;&#20998;&#26512;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#12290;&#34429;&#28982;ECG&#20449;&#21495;&#26159;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20294;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#22312;ECG&#20998;&#26512;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;&#20854;&#20182;&#19981;&#21516;&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;ECG&#20998;&#26512;&#30740;&#31350;&#24573;&#30053;&#20102;&#32593;&#32476;&#32553;&#25918;&#20248;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#36825;&#20250;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;&#20851;&#38190;&#21442;&#25968;&#65288;&#21253;&#25324;&#23618;&#28145;&#24230;&#12289;&#36890;&#36947;&#25968;&#21644;&#21367;&#31215;&#26680;&#22823;&#23567;&#65289;&#25506;&#32034;&#21644;&#35777;&#26126;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;ResNet&#32553;&#25918;&#26041;&#27861;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;ECG&#20998;&#31867;&#20013;&#65292;&#36739;&#27973;&#30340;&#32593;&#32476;&#12289;&#36739;&#22823;&#30340;&#36890;&#36947;&#25968;&#21644;&#36739;&#23567;&#30340;&#21367;&#31215;&#26680;&#23610;&#23544;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#26368;&#20339;&#32593;&#32476;&#35268;&#27169;&#21487;&#33021;&#22240;&#30446;&#26631;&#20219;&#21153;&#32780;&#24322;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20026;&#33719;&#24471;&#26356;&#39640;&#25928;&#20934;&#30830;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study scaling convolutional neural networks (CNNs), specifically targeting Residual neural networks (ResNet), for analyzing electrocardiograms (ECGs). Although ECG signals are time-series data, CNN-based models have been shown to outperform other neural networks with different architectures in ECG analysis. However, most previous studies in ECG analysis have overlooked the importance of network scaling optimization, which significantly improves performance. We explored and demonstrated an efficient approach to scale ResNet by examining the effects of crucial parameters, including layer depth, the number of channels, and the convolution kernel size. Through extensive experiments, we found that a shallower network, a larger number of channels, and smaller kernel sizes result in better performance for ECG classifications. The optimal network scale might differ depending on the target task, but our findings provide insight into obtaining more efficient and accurate models with fewer com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#20302;&#21151;&#32791;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#24847;&#22806;&#25684;&#20498;&#26816;&#27979;&#65292;&#24182;&#20351;&#29992;&#20102;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;LSTM&#27169;&#22411;&#26469;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#20998;&#26512;&#21508;&#31181;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23454;&#29616;&#23454;&#26102;&#26816;&#27979;&#33021;&#21147;&#65292;&#24182;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#31934;&#24230;&#21644;&#38477;&#20302;&#21151;&#32791;&#12290;</title><link>http://arxiv.org/abs/2308.12481</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#30340;&#31163;&#32447;&#23884;&#20837;&#24335;&#20302;&#21151;&#32791;&#35013;&#32622;&#30340;&#25684;&#20498;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fall Detection using Knowledge Distillation Based Long short-term memory for Offline Embedded and Low Power Devices. (arXiv:2308.12481v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#20302;&#21151;&#32791;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#24847;&#22806;&#25684;&#20498;&#26816;&#27979;&#65292;&#24182;&#20351;&#29992;&#20102;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;LSTM&#27169;&#22411;&#26469;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#20998;&#26512;&#21508;&#31181;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23454;&#29616;&#23454;&#26102;&#26816;&#27979;&#33021;&#21147;&#65292;&#24182;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#31934;&#24230;&#21644;&#38477;&#20302;&#21151;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#20302;&#21151;&#32791;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#24847;&#22806;&#25684;&#20498;&#26816;&#27979;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;LSTM&#27169;&#22411;&#65292;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#20027;&#35201;&#20851;&#27880;&#20174;&#21508;&#31181;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20998;&#26512;&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#23454;&#26102;&#26816;&#27979;&#33021;&#21147;&#65292;&#30830;&#20445;&#21450;&#26102;&#21487;&#38752;&#22320;&#35782;&#21035;&#25684;&#20498;&#20107;&#20214;&#12290;&#20316;&#32773;&#30740;&#31350;&#20102;&#22522;&#20110;&#19981;&#21516;&#20256;&#24863;&#22120;&#30340;&#25684;&#20498;&#26816;&#27979;&#27169;&#22411;&#65292;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#20934;&#30830;&#29575;&#21644;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#37319;&#29992;&#20102;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#31934;&#24230;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#28040;&#32791;&#26356;&#20302;&#21151;&#29575;&#30340;&#31934;&#30830;&#37197;&#32622;&#12290;&#22240;&#27492;&#65292;&#36825;&#20010;&#25552;&#35758;&#30340;&#35299;&#20915;&#26041;&#26696;&#20026;&#26410;&#26469;&#36825;&#19968;&#20851;&#38190;&#39046;&#22495;&#20013;&#33410;&#33021;&#25684;&#20498;&#26816;&#27979;&#31995;&#32479;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a cost-effective, low-power approach to unintentional fall detection using knowledge distillation-based LSTM (Long Short-Term Memory) models to significantly improve accuracy. With a primary focus on analyzing time-series data collected from various sensors, the solution offers real-time detection capabilities, ensuring prompt and reliable identification of falls. The authors investigate fall detection models that are based on different sensors, comparing their accuracy rates and performance. Furthermore, they employ the technique of knowledge distillation to enhance the models' precision, resulting in refined accurate configurations that consume lower power. As a result, this proposed solution presents a compelling avenue for the development of energy-efficient fall detection systems for future advancements in this critical domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#27969;&#24335;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#19968;&#33268;&#22320;&#37325;&#24314;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#38646;&#24310;&#36831;&#20449;&#21495;&#37325;&#24314;&#30340;&#31895;&#31961;&#24230;&#12290; (arXiv:2308.12459v1 [eess.SP])</title><link>http://arxiv.org/abs/2308.12459</link><description>&lt;p&gt;
&#20174;&#27969;&#24335;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#23454;&#29616;&#38646;&#24310;&#36831;&#19968;&#33268;&#20449;&#21495;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Zero-delay Consistent Signal Reconstruction from Streamed Multivariate Time Series. (arXiv:2308.12459v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#27969;&#24335;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#19968;&#33268;&#22320;&#37325;&#24314;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#38646;&#24310;&#36831;&#20449;&#21495;&#37325;&#24314;&#30340;&#31895;&#31961;&#24230;&#12290; (arXiv:2308.12459v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21270;&#29616;&#23454;&#19990;&#30028;&#30340;&#27169;&#25311;&#20449;&#21495;&#36890;&#24120;&#28041;&#21450;&#26102;&#38388;&#37319;&#26679;&#21644;&#24133;&#24230;&#31163;&#25955;&#21270;&#12290;&#21518;&#32493;&#30340;&#20449;&#21495;&#37325;&#24314;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#20135;&#29983;&#19968;&#20010;&#19982;&#24133;&#24230;&#20998;&#36776;&#29575;&#21644;&#33719;&#21462;&#26679;&#26412;&#30340;&#26102;&#38388;&#23494;&#24230;&#26377;&#20851;&#30340;&#35823;&#24046;&#12290;&#20174;&#23454;&#26045;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#19968;&#33268;&#30340;&#20449;&#21495;&#37325;&#24314;&#26041;&#27861;&#22312;&#37319;&#26679;&#29575;&#22686;&#21152;&#26102;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#26377;&#30410;&#30340;&#35823;&#24046;&#34928;&#20943;&#25928;&#26524;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36825;&#20123;&#32467;&#26524;&#26159;&#22312;&#31163;&#32447;&#35774;&#32622;&#19979;&#33719;&#24471;&#30340;&#12290;&#22240;&#27492;&#65292;&#20851;&#20110;&#20174;&#25968;&#25454;&#27969;&#20013;&#36827;&#34892;&#19968;&#33268;&#20449;&#21495;&#37325;&#24314;&#30340;&#26041;&#27861;&#23384;&#22312;&#30740;&#31350;&#31354;&#30333;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38646;&#24310;&#36831;&#21709;&#24212;&#35201;&#27714;&#19979;&#19968;&#33268;&#22320;&#37325;&#24314;&#37327;&#21270;&#38388;&#38548;&#30340;&#27969;&#24335;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#26041;&#27861;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#21033;&#29992;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#21487;&#20197;&#20943;&#23569;&#38646;&#24310;&#36831;&#20449;&#21495;&#37325;&#24314;&#30340;&#31895;&#31961;&#24230;&#12290;&#26412;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#23384;&#22312;&#30528;&#26102;&#31354;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digitalizing real-world analog signals typically involves sampling in time and discretizing in amplitude. Subsequent signal reconstructions inevitably incur an error that depends on the amplitude resolution and the temporal density of the acquired samples. From an implementation viewpoint, consistent signal reconstruction methods have proven a profitable error-rate decay as the sampling rate increases. Despite that, these results are obtained under offline settings. Therefore, a research gap exists regarding methods for consistent signal reconstruction from data streams. This paper presents a method that consistently reconstructs streamed multivariate time series of quantization intervals under a zero-delay response requirement. On the other hand, previous work has shown that the temporal dependencies within univariate time series can be exploited to reduce the roughness of zero-delay signal reconstructions. This work shows that the spatiotemporal dependencies within multivariate time 
&lt;/p&gt;</description></item><item><title>PFL-GAN&#26159;&#19968;&#31181;&#22312;&#20010;&#24615;&#21270;&#32852;&#21512;&#23398;&#20064;&#20013;&#35299;&#20915;&#23458;&#25143;&#24322;&#36136;&#24615;&#30340;&#26032;&#22411;GAN&#20849;&#20139;&#21644;&#32858;&#21512;&#31574;&#30053;&#65292;&#36890;&#36807;&#23398;&#20064;&#23458;&#25143;&#38388;&#30340;&#30456;&#20284;&#24230;&#24182;&#37319;&#29992;&#21152;&#26435;&#21327;&#21516;&#25968;&#25454;&#32858;&#21512;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.12454</link><description>&lt;p&gt;
PFL-GAN&#65306;&#24403;&#23458;&#25143;&#24322;&#36136;&#24615;&#19982;&#29983;&#25104;&#27169;&#22411;&#30456;&#36935;&#22312;&#20010;&#24615;&#21270;&#32852;&#21512;&#23398;&#20064;&#20013;
&lt;/p&gt;
&lt;p&gt;
PFL-GAN: When Client Heterogeneity Meets Generative Models in Personalized Federated Learning. (arXiv:2308.12454v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12454
&lt;/p&gt;
&lt;p&gt;
PFL-GAN&#26159;&#19968;&#31181;&#22312;&#20010;&#24615;&#21270;&#32852;&#21512;&#23398;&#20064;&#20013;&#35299;&#20915;&#23458;&#25143;&#24322;&#36136;&#24615;&#30340;&#26032;&#22411;GAN&#20849;&#20139;&#21644;&#32858;&#21512;&#31574;&#30053;&#65292;&#36890;&#36807;&#23398;&#20064;&#23458;&#25143;&#38388;&#30340;&#30456;&#20284;&#24230;&#24182;&#37319;&#29992;&#21152;&#26435;&#21327;&#21516;&#25968;&#25454;&#32858;&#21512;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#29983;&#25104;&#23398;&#20064;&#27169;&#22411;&#30340;&#36827;&#23637;&#20276;&#38543;&#30528;&#23545;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#27169;&#22411;&#30340;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#26085;&#30410;&#20851;&#27880;&#12290;&#22312;FL&#30340;&#32972;&#26223;&#19979;&#65292;GAN&#21487;&#20197;&#25429;&#25417;&#24213;&#23618;&#23458;&#25143;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#37325;&#26032;&#29983;&#25104;&#31867;&#20284;&#20110;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#32780;&#19981;&#25439;&#23475;&#31169;&#26377;&#21407;&#22987;&#25968;&#25454;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#22522;&#20110;GAN&#30340;FL&#24037;&#20316;&#38598;&#20013;&#22312;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#19978;&#65292;&#20294;&#22312;&#23458;&#25143;&#25968;&#25454;&#24322;&#36136;&#24615;&#26041;&#38754;&#65292;&#20010;&#24615;&#21270;FL&#65288;PFL&#65289;&#26377;&#26102;&#21487;&#33021;&#26356;&#21152;&#26377;&#25928;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#19981;&#21516;&#25968;&#25454;&#26679;&#26412;&#20998;&#24067;&#12289;&#29305;&#24449;&#31354;&#38388;&#21644;&#26631;&#31614;&#12290;&#20026;&#20102;&#24212;&#23545;GAN-based FL&#20013;&#30340;&#23458;&#25143;&#24322;&#36136;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;PFL&#30340;&#26032;&#22411;GAN&#20849;&#20139;&#21644;&#32858;&#21512;&#31574;&#30053;&#12290;&#25152;&#25552;&#20986;&#30340;PFL-GAN&#35299;&#20915;&#20102;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#23458;&#25143;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23398;&#20064;&#23458;&#25143;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#28982;&#21518;&#24320;&#21457;&#19968;&#20010;&#21152;&#26435;&#30340;&#21327;&#21516;&#25968;&#25454;&#32858;&#21512;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#20960;&#20010;&#30693;&#21517;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20005;&#26684;&#30340;&#23454;&#39564;&#26469;&#39564;&#35777;&#27492;&#26041;&#27861;&#30340;&#23454;&#38469;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances of generative learning models are accompanied by the growing interest in federated learning (FL) based on generative adversarial network (GAN) models. In the context of FL, GAN can capture the underlying client data structure, and regenerate samples resembling the original data distribution without compromising the private raw data. Although most existing GAN-based FL works focus on training a global model, Personalized FL (PFL) sometimes can be more effective in view of client data heterogeneity in terms of distinct data sample distributions, feature spaces, and labels. To cope with client heterogeneity in GAN-based FL, we propose a novel GAN sharing and aggregation strategy for PFL. The proposed PFL-GAN addresses the client heterogeneity in different scenarios. More specially, we first learn the similarity among clients and then develop an weighted collaborative data aggregation. The empirical results through the rigorous experimentation on several well-known datasets
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#22312;&#25968;&#25454;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#39281;&#21644;&#25928;&#26524;&#65292;&#27604;&#28155;&#21152;&#30495;&#23454;&#22270;&#20687;&#33719;&#24471;&#30340;&#24615;&#33021;&#25552;&#21319;&#35201;&#23567;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2308.12453</link><description>&lt;p&gt;
&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Augmenting medical image classifiers with synthetic data from latent diffusion models. (arXiv:2308.12453v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12453
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#22312;&#25968;&#25454;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#39281;&#21644;&#25928;&#26524;&#65292;&#27604;&#28155;&#21152;&#30495;&#23454;&#22270;&#20687;&#33719;&#24471;&#30340;&#24615;&#33021;&#25552;&#21319;&#35201;&#23567;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#29616;&#22312;&#26377;&#25968;&#30334;&#31181;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#24050;&#33719;&#24471;&#32654;&#22269;&#39135;&#21697;&#21644;&#33647;&#29289;&#31649;&#29702;&#23616;&#30340;&#25209;&#20934;&#25110;&#28165;&#38500;&#65292;&#20294;&#35768;&#22810;&#30740;&#31350;&#26174;&#31034;&#20854;&#19968;&#33268;&#24615;&#27867;&#21270;&#25110;&#28508;&#22312;&#20559;&#24046;&#23384;&#22312;&#19981;&#19968;&#33268;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23569;&#25968;&#32676;&#20307;&#12290;&#26377;&#20154;&#25552;&#20986;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#20943;&#23569;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#20294;&#20854;&#22312;&#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#25928;&#29992;&#23578;&#19981;&#28165;&#26970;&#12290;&#30382;&#32932;&#30142;&#30149;&#26159;&#21512;&#25104;&#22270;&#20687;&#29983;&#25104;&#30340;&#26377;&#29992;&#26696;&#20363;&#30740;&#31350;&#65292;&#22240;&#20026;&#30142;&#30149;&#22806;&#35266;&#20855;&#26377;&#22810;&#26679;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#30382;&#32932;&#33394;&#35843;&#36825;&#19968;&#20445;&#25252;&#23646;&#24615;&#19978;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#21487;&#25193;&#23637;&#22320;&#29983;&#25104;&#30382;&#32932;&#30142;&#30149;&#22270;&#20687;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#21463;&#38480;&#24773;&#20917;&#19979;&#65292;&#23558;&#36825;&#20123;&#25968;&#25454;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#36825;&#31181;&#24615;&#33021;&#25552;&#21319;&#22312;&#21512;&#25104;&#21040;&#30495;&#23454;&#22270;&#20687;&#27604;&#20363;&#36229;&#36807;10&#65306;1&#21518;&#36798;&#21040;&#39281;&#21644;&#65292;&#24182;&#19988;&#27604;&#28155;&#21152;&#30495;&#23454;&#22270;&#20687;&#25152;&#33719;&#24471;&#30340;&#25552;&#21319;&#35201;&#23567;&#24471;&#22810;&#12290;&#20316;&#20026;&#25105;&#20204;&#20998;&#26512;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#29983;&#25104;&#24182;&#20998;&#26512;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;458,920&#20010;&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
While hundreds of artificial intelligence (AI) algorithms are now approved or cleared by the US Food and Drugs Administration (FDA), many studies have shown inconsistent generalization or latent bias, particularly for underrepresented populations. Some have proposed that generative AI could reduce the need for real data, but its utility in model development remains unclear. Skin disease serves as a useful case study in synthetic image generation due to the diversity of disease appearance, particularly across the protected attribute of skin tone. Here we show that latent diffusion models can scalably generate images of skin disease and that augmenting model training with these data improves performance in data-limited settings. These performance gains saturate at synthetic-to-real image ratios above 10:1 and are substantially smaller than the gains obtained from adding real images. As part of our analysis, we generate and analyze a new dataset of 458,920 synthetic images produced using 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Dr. DRL &#30340;&#33258;&#24840;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#19968;&#20123;&#25928;&#29575;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#24341;&#20837;&#26377;&#24847;&#36951;&#24536;&#30340;&#26426;&#21046;&#26469;&#24212;&#23545;&#29615;&#22659;&#28418;&#31227;&#24341;&#36215;&#30340;&#22256;&#25200;&#12290;</title><link>http://arxiv.org/abs/2308.12445</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#26377;&#24847;&#36951;&#24536;&#39537;&#21160;&#30340;&#33258;&#24840;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems. (arXiv:2308.12445v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Dr. DRL &#30340;&#33258;&#24840;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#19968;&#20123;&#25928;&#29575;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#24341;&#20837;&#26377;&#24847;&#36951;&#24536;&#30340;&#26426;&#21046;&#26469;&#24212;&#23545;&#29615;&#22659;&#28418;&#31227;&#24341;&#36215;&#30340;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064; (DRL) &#22312;&#20687; Netflix &#21644; Facebook &#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#22810;&#12290;&#21644;&#22823;&#22810;&#25968;&#25968;&#25454;&#39537;&#21160;&#31995;&#32479;&#19968;&#26679;&#65292;DRL &#31995;&#32479;&#21487;&#33021;&#30001;&#20110;&#29615;&#22659;&#28418;&#31227;&#23548;&#33268;&#19981;&#33391;&#34892;&#20026;&#65292;&#32780;&#36825;&#31181;&#28418;&#31227;&#32463;&#24120;&#21457;&#29983;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29983;&#20135;&#29615;&#22659;&#20013;&#12290;&#36830;&#32493;&#23398;&#20064; (CL) &#26159;&#33258;&#24840;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#29615;&#22659;&#26465;&#20214;&#30340;&#21464;&#21270;&#35843;&#25972; DRL &#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#30340;&#36830;&#32493;&#21464;&#21270;&#21487;&#33021;&#23548;&#33268;&#29983;&#20135;&#29615;&#22659;&#20174;&#21407;&#22987;&#29366;&#24577;&#20559;&#31163;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#29615;&#22659;&#28418;&#31227;&#24448;&#24448;&#23548;&#33268;&#36830;&#32493;&#23398;&#20064;&#36827;&#20837;&#38271;&#26102;&#38388;&#30340;&#33258;&#24840;&#21608;&#26399;&#65292;&#29978;&#33267;&#26080;&#27861;&#25104;&#21151;&#65292;&#36825;&#26159;&#30001;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#12289;&#28201;&#21644;&#36215;&#22987;&#22833;&#36133;&#21644;&#25910;&#25947;&#32531;&#24930;&#31561;&#25928;&#29575;&#20302;&#19979;&#38382;&#39064;&#24341;&#36215;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; Dr. DRL&#65292;&#19968;&#31181;&#23545; DRL &#31995;&#32479;&#30340;&#26377;&#25928;&#33258;&#24840;&#26041;&#27861;&#65292;&#23427;&#23558;&#26377;&#24847;&#36951;&#24536;&#30340;&#26032;&#39062;&#26426;&#21046;&#25972;&#21512;&#21040;&#21407;&#22987;&#30340;&#36830;&#32493;&#23398;&#20064;&#20013;&#20197;&#35299;&#20915;&#20854;&#20027;&#35201;&#38382;&#39064;&#12290;Dr. DRL &#26377;&#24847;&#22320;&#25830;&#38500;...
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) is increasingly applied in large-scale productions like Netflix and Facebook. As with most data-driven systems, DRL systems can exhibit undesirable behaviors due to environmental drifts, which often occur in constantly-changing production settings. Continual Learning (CL) is the inherent self-healing approach for adapting the DRL agent in response to the environment's conditions shifts. However, successive shifts of considerable magnitude may cause the production environment to drift from its original state. Recent studies have shown that these environmental drifts tend to drive CL into long, or even unsuccessful, healing cycles, which arise from inefficiencies such as catastrophic forgetting, warm-starting failure, and slow convergence. In this paper, we propose Dr. DRL, an effective self-healing approach for DRL systems that integrates a novel mechanism of intentional forgetting into vanilla CL to overcome its main issues. Dr. DRL deliberately erases
&lt;/p&gt;</description></item><item><title>TAI-GAN&#26159;&#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#24515;&#33039;PET&#36816;&#21160;&#26657;&#27491;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#26102;&#38388;&#21644;&#35299;&#21078;&#20449;&#24687;&#24863;&#30693;&#23454;&#29616;&#26089;&#26399;&#21040;&#26202;&#26399;&#24103;&#30340;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2308.12443</link><description>&lt;p&gt;
TAI-GAN&#65306;&#26102;&#38388;&#21644;&#35299;&#21078;&#20449;&#24687;&#24863;&#30693;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#29992;&#20110;&#21160;&#24577;&#24515;&#33039;PET&#36816;&#21160;&#26657;&#27491;&#20013;&#30340;&#26089;&#26399;&#21040;&#26202;&#26399;&#24103;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
TAI-GAN: Temporally and Anatomically Informed GAN for early-to-late frame conversion in dynamic cardiac PET motion correction. (arXiv:2308.12443v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12443
&lt;/p&gt;
&lt;p&gt;
TAI-GAN&#26159;&#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#24515;&#33039;PET&#36816;&#21160;&#26657;&#27491;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#26102;&#38388;&#21644;&#35299;&#21078;&#20449;&#24687;&#24863;&#30693;&#23454;&#29616;&#26089;&#26399;&#21040;&#26202;&#26399;&#24103;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40077;&#27604;&#38135;&#24341;&#29289;($^{82}$Rb)&#30340;&#24555;&#36895;&#31034;&#36394;&#21058;&#21160;&#21147;&#23398;&#21644;&#21160;&#24577;&#24515;&#33039;&#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#26174;&#20687;(PET)&#20013;&#24103;&#38388;&#20998;&#24067;&#30340;&#39640;&#21464;&#24322;&#24615;&#32473;&#24103;&#38388;&#36816;&#21160;&#26657;&#27491;&#25552;&#20986;&#20102;&#26174;&#33879;&#30340;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#20256;&#32479;&#30340;&#22522;&#20110;&#24378;&#24230;&#30340;&#22270;&#20687;&#37197;&#20934;&#25216;&#26415;&#19981;&#36866;&#29992;&#30340;&#26089;&#26399;&#24103;&#19978;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#29983;&#25104;&#26041;&#27861;&#22788;&#29702;&#31034;&#36394;&#21058;&#20998;&#24067;&#21464;&#21270;&#65292;&#20197;&#36741;&#21161;&#29616;&#26377;&#30340;&#37197;&#20934;&#26041;&#27861;&#12290;&#20026;&#20102;&#25913;&#36827;&#36880;&#24103;&#37197;&#20934;&#21644;&#21442;&#25968;&#21270;&#37327;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#21644;&#35299;&#21078;&#20449;&#24687;&#24863;&#30693;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(TAI-GAN)&#65292;&#23558;&#26089;&#26399;&#24103;&#36716;&#25442;&#20026;&#26202;&#26399;&#21442;&#32771;&#24103;&#65292;&#20351;&#29992;&#19968;&#20010;&#23545;&#19968;&#30340;&#26144;&#23556;&#12290;&#20855;&#20307;&#22320;&#65292;&#19968;&#20010;&#29305;&#24449;&#36880;&#36890;&#36947;&#32447;&#24615;&#35843;&#21046;&#23618;&#32534;&#30721;&#26469;&#33258;&#26102;&#38388;&#31034;&#36394;&#21058;&#21160;&#21147;&#23398;&#20449;&#24687;&#29983;&#25104;&#30340;&#36890;&#36947;&#21442;&#25968;&#65292;&#31895;&#30053;&#30340;&#24515;&#33039;&#20998;&#21106;&#24102;&#26377;&#23616;&#37096;&#31227;&#20301;&#20316;&#20026;&#35299;&#21078;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#20020;&#24202;&#25968;&#25454;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid tracer kinetics of rubidium-82 ($^{82}$Rb) and high variation of cross-frame distribution in dynamic cardiac positron emission tomography (PET) raise significant challenges for inter-frame motion correction, particularly for the early frames where conventional intensity-based image registration techniques are not applicable. Alternatively, a promising approach utilizes generative methods to handle the tracer distribution changes to assist existing registration methods. To improve frame-wise registration and parametric quantification, we propose a Temporally and Anatomically Informed Generative Adversarial Network (TAI-GAN) to transform the early frames into the late reference frame using an all-to-one mapping. Specifically, a feature-wise linear modulation layer encodes channel-wise parameters generated from temporal tracer kinetics information, and rough cardiac segmentations with local shifts serve as the anatomical information. We validated our proposed method on a clinica
&lt;/p&gt;</description></item><item><title>BaDExpert&#26159;&#19968;&#31181;&#38450;&#24481;&#21518;&#38376;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#25552;&#21462;&#32473;&#23450;&#21518;&#38376;&#27169;&#22411;&#30340;&#21518;&#38376;&#21151;&#33021;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#19987;&#23478;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21482;&#33021;&#35782;&#21035;&#21518;&#38376;&#36755;&#20837;&#12290;&#21487;&#20197;&#36827;&#19968;&#27493;&#20351;&#29992;&#35813;&#27169;&#22411;&#35774;&#35745;&#39640;&#24230;&#20934;&#30830;&#30340;&#21518;&#38376;&#36755;&#20837;&#26816;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.12439</link><description>&lt;p&gt;
BaDExpert: &#25552;&#21462;&#21518;&#38376;&#21151;&#33021;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#21518;&#38376;&#36755;&#20837;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection. (arXiv:2308.12439v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12439
&lt;/p&gt;
&lt;p&gt;
BaDExpert&#26159;&#19968;&#31181;&#38450;&#24481;&#21518;&#38376;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#25552;&#21462;&#32473;&#23450;&#21518;&#38376;&#27169;&#22411;&#30340;&#21518;&#38376;&#21151;&#33021;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#19987;&#23478;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21482;&#33021;&#35782;&#21035;&#21518;&#38376;&#36755;&#20837;&#12290;&#21487;&#20197;&#36827;&#19968;&#27493;&#20351;&#29992;&#35813;&#27169;&#22411;&#35774;&#35745;&#39640;&#24230;&#20934;&#30830;&#30340;&#21518;&#38376;&#36755;&#20837;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#19978;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20854;&#20013;&#23545;&#25163;&#23558;&#24694;&#24847;&#34892;&#20026;&#65288;&#21518;&#38376;&#65289;&#31192;&#23494;&#22320;&#26893;&#20837;DNN&#20013;&#12290;&#25105;&#20204;&#30340;&#38450;&#24481;&#23646;&#20110;&#21518;&#26399;&#24320;&#21457;&#30340;&#38450;&#24481;&#33539;&#30068;&#65292;&#29420;&#31435;&#20110;&#27169;&#22411;&#29983;&#25104;&#30340;&#26041;&#24335;&#12290;&#25152;&#25552;&#20986;&#30340;&#38450;&#24481;&#26041;&#27861;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#36870;&#21521;&#24037;&#31243;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#25552;&#21462;&#32473;&#23450;&#21518;&#38376;&#27169;&#22411;&#30340;&#21518;&#38376;&#21151;&#33021;&#24182;&#29983;&#25104;&#19968;&#20010;&#19987;&#23478;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#24456;&#31616;&#21333; - &#22312;&#19968;&#23567;&#32452;&#26377;&#24847;&#20041;&#30340;&#38169;&#35823;&#26631;&#35760;&#30340;&#24178;&#20928;&#26679;&#26412;&#19978;&#24494;&#35843;&#21518;&#38376;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#20854;&#24536;&#35760;&#27491;&#24120;&#21151;&#33021;&#20294;&#20173;&#20445;&#30041;&#21518;&#38376;&#21151;&#33021;&#65292;&#20174;&#32780;&#29983;&#25104;&#19968;&#20010;&#21482;&#33021;&#35782;&#21035;&#21518;&#38376;&#36755;&#20837;&#30340;&#27169;&#22411;&#65288;&#31216;&#20026;&#21518;&#38376;&#19987;&#23478;&#27169;&#22411;&#65289;&#12290;&#22522;&#20110;&#25552;&#21462;&#30340;&#21518;&#38376;&#19987;&#23478;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35774;&#35745;&#39640;&#24230;&#20934;&#30830;&#30340;&#21518;&#38376;&#36755;&#20837;&#26816;&#27979;&#22120;&#30340;&#21487;&#34892;&#24615;&#65292;&#22312;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#20013;&#36807;&#28388;&#25481;&#21518;&#38376;&#36755;&#20837;&#12290;&#36827;&#19968;&#27493;&#36890;&#36807;...
&lt;/p&gt;
&lt;p&gt;
We present a novel defense, against backdoor attacks on Deep Neural Networks (DNNs), wherein adversaries covertly implant malicious behaviors (backdoors) into DNNs. Our defense falls within the category of post-development defenses that operate independently of how the model was generated. The proposed defense is built upon a novel reverse engineering approach that can directly extract backdoor functionality of a given backdoored model to a backdoor expert model. The approach is straightforward -- finetuning the backdoored model over a small set of intentionally mislabeled clean samples, such that it unlearns the normal functionality while still preserving the backdoor functionality, and thus resulting in a model (dubbed a backdoor expert model) that can only recognize backdoor inputs. Based on the extracted backdoor expert model, we show the feasibility of devising highly accurate backdoor input detectors that filter out the backdoor inputs during model inference. Further augmented by
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#24320;&#21457;&#32773;&#38382;&#31572;&#35770;&#22363;Stack Overflow&#19978;&#30340;&#24086;&#23376;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#24635;&#32467;&#20102;&#37096;&#32626;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#37096;&#32626;&#24179;&#21488;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2308.12438</link><description>&lt;p&gt;
&#37096;&#32626;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65306;&#25361;&#25112;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deploying Deep Reinforcement Learning Systems: A Taxonomy of Challenges. (arXiv:2308.12438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#24320;&#21457;&#32773;&#38382;&#31572;&#35770;&#22363;Stack Overflow&#19978;&#30340;&#24086;&#23376;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#24635;&#32467;&#20102;&#37096;&#32626;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#37096;&#32626;&#24179;&#21488;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24050;&#32463;&#22312;&#21253;&#25324;&#26426;&#22120;&#20154;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#30005;&#33041;&#28216;&#25103;&#31561;&#39046;&#22495;&#26174;&#31034;&#20986;&#26174;&#33879;&#28508;&#21147;&#65292;&#20197;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#33258;&#20027;&#33021;&#21147;&#12290;&#36825;&#31181;&#28508;&#21147;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#23545;DRL&#30340;&#28909;&#24773;&#21644;&#26085;&#30410;&#22686;&#38271;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#31038;&#21306;&#20027;&#35201;&#38598;&#20013;&#22312;DRL&#31995;&#32479;&#24320;&#21457;&#38454;&#27573;&#65292;&#23545;DRL&#37096;&#32626;&#20851;&#27880;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24320;&#21457;&#32773;&#26368;&#27969;&#34892;&#30340;&#38382;&#31572;&#35770;&#22363;Stack Overflow&#65288;SO&#65289;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#25581;&#31034;&#21644;&#20102;&#35299;&#20174;&#19994;&#20154;&#21592;&#22312;&#37096;&#32626;DRL&#31995;&#32479;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25353;&#37096;&#32626;&#24179;&#21488;&#23545;&#30456;&#20851;SO&#24086;&#23376;&#36827;&#34892;&#20102;&#20998;&#31867;&#65306;&#26381;&#21153;&#22120;/&#20113;&#12289;&#31227;&#21160;/&#23884;&#20837;&#24335;&#31995;&#32479;&#12289;&#27983;&#35272;&#22120;&#21644;&#28216;&#25103;&#24341;&#25806;&#12290;&#32463;&#36807;&#36807;&#28388;&#21644;&#25163;&#21160;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;357&#20010;&#26377;&#20851;DRL&#37096;&#32626;&#30340;SO&#24086;&#23376;&#65292;&#35843;&#26597;&#20102;&#24403;&#21069;&#29366;&#20917;&#65292;&#24182;&#30830;&#23450;&#20102;&#19982;&#37096;&#32626;DRL&#31995;&#32479;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL), leveraging Deep Learning (DL) in reinforcement learning, has shown significant potential in achieving human-level autonomy in a wide range of domains, including robotics, computer vision, and computer games. This potential justifies the enthusiasm and growing interest in DRL in both academia and industry. However, the community currently focuses mostly on the development phase of DRL systems, with little attention devoted to DRL deployment. In this paper, we propose an empirical study on Stack Overflow (SO), the most popular Q&amp;A forum for developers, to uncover and understand the challenges practitioners faced when deploying DRL systems. Specifically, we categorized relevant SO posts by deployment platforms: server/cloud, mobile/embedded system, browser, and game engine. After filtering and manual analysis, we examined 357 SO posts about DRL deployment, investigated the current state, and identified the challenges related to deploying DRL systems. The
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;NLP&#20998;&#26512;&#20102;ESG&#20027;&#23548;&#30340;DLT&#30740;&#31350;&#30340;&#28436;&#21270;&#65292;&#36890;&#36807;&#26500;&#24314;&#24341;&#29992;&#32593;&#32476;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#23545;DLT&#22312;ESG&#32972;&#26223;&#19979;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.12420</link><description>&lt;p&gt;
ESG&#20027;&#23548;&#30340;DLT&#30740;&#31350;&#30340;&#28436;&#21270;&#65306;&#23545;&#25991;&#29486;&#36827;&#34892;NLP&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature. (arXiv:2308.12420v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;NLP&#20998;&#26512;&#20102;ESG&#20027;&#23548;&#30340;DLT&#30740;&#31350;&#30340;&#28436;&#21270;&#65292;&#36890;&#36807;&#26500;&#24314;&#24341;&#29992;&#32593;&#32476;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#23545;DLT&#22312;ESG&#32972;&#26223;&#19979;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#36134;&#26412;&#25216;&#26415;(DLT)&#36805;&#36895;&#21457;&#23637;&#65292;&#38656;&#35201;&#20840;&#38754;&#20102;&#35299;&#20854;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;DLT&#30340;&#29615;&#22659;&#12289;&#21487;&#25345;&#32493;&#24615;&#21644;&#27835;&#29702;(ESG)&#32452;&#25104;&#37096;&#20998;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#36824;&#19981;&#36275;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;107&#31687;&#31181;&#23376;&#25991;&#29486;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;63,083&#20010;&#21442;&#32771;&#25991;&#29486;&#30340;&#24341;&#29992;&#32593;&#32476;&#65292;&#24182;&#23558;&#20854;&#31934;&#28860;&#20026;24,539&#31687;&#25991;&#29486;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#19968;&#20010;&#24050;&#24314;&#31435;&#30340;&#25216;&#26415;&#20998;&#31867;&#27861;&#20174;46&#31687;&#35770;&#25991;&#20013;&#26631;&#35760;&#20102;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#36890;&#36807;&#25214;&#20986;DLT&#30340;ESG&#35201;&#32032;&#26469;&#23436;&#21892;&#36825;&#20010;&#20998;&#31867;&#27861;&#12290;&#21033;&#29992;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#32454;&#21270;&#35843;&#25972;&#65292;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#20351;&#29992;&#25105;&#20204;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#35843;&#25972;&#21518;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#31934;&#31616;&#65292;&#24471;&#21040;&#20102;505&#31687;&#20851;&#38190;&#35770;&#25991;&#65292;&#36890;&#36807;&#21629;&#21517;&#23454;&#20307;&#21644;&#26102;&#38388;&#22270;&#20998;&#26512;&#65292;&#20419;&#36827;&#20102;&#23545;DLT&#22312;ESG&#32972;&#26223;&#19979;&#30340;&#28436;&#21270;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed Ledger Technologies (DLTs) have rapidly evolved, necessitating comprehensive insights into their diverse components. However, a systematic literature review that emphasizes the Environmental, Sustainability, and Governance (ESG) components of DLT remains lacking. To bridge this gap, we selected 107 seed papers to build a citation network of 63,083 references and refined it to a corpus of 24,539 publications for analysis. Then, we labeled the named entities in 46 papers according to twelve top-level categories derived from an established technology taxonomy and enhanced the taxonomy by pinpointing DLT's ESG elements. Leveraging transformer-based language models, we fine-tuned a pre-trained language model for a Named Entity Recognition (NER) task using our labeled dataset. We used our fine-tuned language model to distill the corpus to 505 key papers, facilitating a literature review via named entities and temporal graph analysis on DLT evolution in the context of ESG. Our con
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;Huber&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#38750;&#32447;&#24615;&#31995;&#32479;&#21442;&#25968;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#31934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12393</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#38750;&#32447;&#24615;&#31995;&#32479;&#21442;&#25968;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine learning in parameter estimation of nonlinear systems. (arXiv:2308.12393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12393
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;Huber&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#38750;&#32447;&#24615;&#31995;&#32479;&#21442;&#25968;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#31934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#65292;&#20934;&#30830;&#20272;&#35745;&#22797;&#26434;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#21442;&#25968;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;Huber&#25439;&#22833;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#33021;&#21147;&#26469;&#25581;&#31034;&#38750;&#32447;&#24615;&#26041;&#31243;&#20013;&#30340;&#22797;&#26434;&#34892;&#20026;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#39044;&#23450;&#20041;&#30340;&#27169;&#22411;&#31995;&#32479;&#21160;&#21147;&#23398;&#20989;&#25968;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#24102;&#26377;&#22122;&#22768;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#23545;&#21442;&#25968;&#36827;&#34892;&#31934;&#30830;&#35843;&#25972;&#65292;&#20351;&#20854;&#25910;&#25947;&#21040;&#20934;&#30830;&#20540;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#38459;&#23612;&#25391;&#33633;&#23376;&#12289;Van der Pol&#25391;&#33633;&#23376;&#12289;Lotka-Volterra&#31995;&#32479;&#21644;Lorenz&#31995;&#32479;&#22312;&#20056;&#27861;&#22122;&#22768;&#19979;&#30340;&#24773;&#20917;&#12290;&#35757;&#32451;&#21518;&#30340;&#31070;&#32463;&#32593;&#32476;&#20174;&#19982;&#28508;&#22312;&#21160;&#24577;&#23494;&#20999;&#21305;&#37197;&#30340;&#32467;&#26524;&#20013;&#20934;&#30830;&#20272;&#35745;&#21442;&#25968;&#12290;&#36890;&#36807;&#23545;&#27604;&#30495;&#23454;&#36712;&#36857;&#21644;&#20272;&#35745;&#36712;&#36857;&#30340;&#21487;&#35270;&#21270;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#31934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#20197;Huber&#25439;&#22833;&#20026;&#25351;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately estimating parameters in complex nonlinear systems is crucial across scientific and engineering fields. We present a novel approach for parameter estimation using a neural network with the Huber loss function. This method taps into deep learning's abilities to uncover parameters governing intricate behaviors in nonlinear equations. We validate our approach using synthetic data and predefined functions that model system dynamics. By training the neural network with noisy time series data, it fine-tunes the Huber loss function to converge to accurate parameters. We apply our method to damped oscillators, Van der Pol oscillators, Lotka-Volterra systems, and Lorenz systems under multiplicative noise. The trained neural network accurately estimates parameters, evident from closely matching latent dynamics. Comparing true and estimated trajectories visually reinforces our method's precision and robustness. Our study underscores the Huber loss-guided neural network as a versatile t
&lt;/p&gt;</description></item><item><title>FOSA&#26159;&#19968;&#31181;&#20840;&#20449;&#24687;&#26368;&#22823;&#20284;&#28982; (FIML) &#20248;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#32570;&#22833;&#25968;&#25454;&#34917;&#20840;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;FIML&#20272;&#35745;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#20934;&#30830;&#24615;&#12289;&#35745;&#31639;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.12388</link><description>&lt;p&gt;
FOSA: &#20840;&#20449;&#24687;&#26368;&#22823;&#20284;&#28982; (FIML) &#20248;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#32570;&#22833;&#25968;&#25454;&#34917;&#20840;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FOSA: Full Information Maximum Likelihood (FIML) Optimized Self-Attention Imputation for Missing Data. (arXiv:2308.12388v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12388
&lt;/p&gt;
&lt;p&gt;
FOSA&#26159;&#19968;&#31181;&#20840;&#20449;&#24687;&#26368;&#22823;&#20284;&#28982; (FIML) &#20248;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#32570;&#22833;&#25968;&#25454;&#34917;&#20840;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;FIML&#20272;&#35745;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#20934;&#30830;&#24615;&#12289;&#35745;&#31639;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#34917;&#20840;&#20013;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#32570;&#22833;&#20540;&#23588;&#20026;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#20013;&#12290;&#26412;&#35770;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;FIML&#20248;&#21270;&#33258;&#27880;&#24847;&#21147;&#65288;FOSA&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#34701;&#21512;&#20102;&#20840;&#20449;&#24687;&#26368;&#22823;&#20284;&#28982;&#65288;FIML&#65289;&#20272;&#35745;&#21644;&#33258;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#33021;&#21147;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;FIML&#23545;&#32570;&#22833;&#20540;&#36827;&#34892;&#21021;&#22987;&#20272;&#35745;&#65292;&#28982;&#21518;&#36890;&#36807;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#36827;&#19968;&#27493;&#25552;&#28860;&#36825;&#20123;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#20840;&#38754;&#23454;&#39564;&#35777;&#26126;&#20102;FOSA&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;FIML&#25216;&#26415;&#22312;&#20934;&#30830;&#24615;&#12289;&#35745;&#31639;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#65288;SEM&#65289;&#21487;&#33021;&#38169;&#35823;&#35268;&#23450;&#23548;&#33268;&#23376;&#20248;&#30340;FIML&#20272;&#35745;&#30340;&#24773;&#20917;&#19979;&#65292;FOSA&#33258;&#27880;&#24847;&#21147;&#32452;&#20214;&#30340;&#31283;&#20581;&#26550;&#26500;&#33021;&#22815;&#28789;&#27963;&#22320;&#32416;&#27491;&#21644;&#20248;&#21270;&#34917;&#20840;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In data imputation, effectively addressing missing values is pivotal, especially in intricate datasets. This paper delves into the FIML Optimized Self-attention (FOSA) framework, an innovative approach that amalgamates the strengths of Full Information Maximum Likelihood (FIML) estimation with the capabilities of self-attention neural networks. Our methodology commences with an initial estimation of missing values via FIML, subsequently refining these estimates by leveraging the self-attention mechanism. Our comprehensive experiments on both simulated and real-world datasets underscore FOSA's pronounced advantages over traditional FIML techniques, encapsulating facets of accuracy, computational efficiency, and adaptability to diverse data structures. Intriguingly, even in scenarios where the Structural Equation Model (SEM) might be mis-specified, leading to suboptimal FIML estimates, the robust architecture of FOSA's self-attention component adeptly rectifies and optimizes the imputati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20174;&#22995;&#21517;&#20013;&#25512;&#26029;&#24615;&#21035;&#30340;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#24615;&#21035;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#26159;&#19968;&#31181;&#21487;&#34892;&#19988;&#24191;&#27867;&#24212;&#29992;&#30340;&#26041;&#27861;&#12290;&#20854;&#37325;&#35201;&#24615;&#22312;&#20110;&#30740;&#31350;&#21508;&#31181;&#31185;&#23398;&#23398;&#31185;&#20013;&#23545;&#24615;&#21035;&#24046;&#24322;&#30340;&#27169;&#24335;&#21644;&#20915;&#23450;&#22240;&#32032;&#36827;&#34892;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.12381</link><description>&lt;p&gt;
&#20174;&#22995;&#21517;&#20013;&#25512;&#26029;&#24615;&#21035;&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24615;&#33021;&#35780;&#20272;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Inferring gender from name: a large scale performance evaluation study. (arXiv:2308.12381v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20174;&#22995;&#21517;&#20013;&#25512;&#26029;&#24615;&#21035;&#30340;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#24615;&#21035;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#26159;&#19968;&#31181;&#21487;&#34892;&#19988;&#24191;&#27867;&#24212;&#29992;&#30340;&#26041;&#27861;&#12290;&#20854;&#37325;&#35201;&#24615;&#22312;&#20110;&#30740;&#31350;&#21508;&#31181;&#31185;&#23398;&#23398;&#31185;&#20013;&#23545;&#24615;&#21035;&#24046;&#24322;&#30340;&#27169;&#24335;&#21644;&#20915;&#23450;&#22240;&#32032;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#20154;&#30340;&#24615;&#21035;&#26159;&#22312;&#36827;&#34892;&#21307;&#23398;&#12289;&#31038;&#20250;&#23398;&#12289;&#25919;&#27835;&#23398;&#21644;&#32463;&#27982;&#23398;&#31561;&#21508;&#31181;&#31185;&#23398;&#23398;&#31185;&#30340;&#30740;&#31350;&#26102;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#25968;&#25454;&#30340;&#28608;&#22686;&#65292;&#24615;&#21035;&#20449;&#24687;&#30340;&#33719;&#21462;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20154;&#21592;&#38656;&#35201;&#20174;&#21487;&#33719;&#24471;&#30340;&#20449;&#24687;&#20013;&#65292;&#20027;&#35201;&#26159;&#20174;&#20154;&#21517;&#20013;&#25512;&#26029;&#24615;&#21035;&#12290;&#23613;&#31649;&#36890;&#36807;&#22995;&#21517;&#26469;&#25512;&#26029;&#24615;&#21035;&#21487;&#33021;&#24341;&#21457;&#19968;&#20123;&#20262;&#29702;&#38382;&#39064;&#65292;&#20294;&#32570;&#20047;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#27861;&#24847;&#21619;&#30528;&#30740;&#31350;&#20154;&#21592;&#19981;&#24471;&#19981;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#24403;&#30446;&#26631;&#20351;&#25163;&#27573;&#21512;&#29702;&#26102;-&#22312;&#22823;&#22810;&#25968;&#36825;&#31867;&#30740;&#31350;&#20013;&#65292;&#30446;&#26631;&#26159;&#30740;&#31350;&#24615;&#21035;&#24046;&#24322;&#30340;&#27169;&#24335;&#21644;&#20915;&#23450;&#22240;&#32032;&#12290;&#22995;&#21517;&#21040;&#24615;&#21035;&#25512;&#26029;&#30340;&#24517;&#35201;&#24615;&#20135;&#29983;&#20102;&#19968;&#20010;&#36234;&#26469;&#36234;&#22810;&#30340;&#31639;&#27861;&#26041;&#27861;&#21644;&#36719;&#20214;&#20135;&#21697;&#30340;&#39046;&#22495;&#12290;&#36825;&#20123;&#26041;&#27861;&#24050;&#32463;&#22312;&#19990;&#30028;&#21508;&#22320;&#30340;&#23398;&#26415;&#30028;&#12289;&#24037;&#19994;&#30028;&#12289;&#25919;&#24220;&#21644;&#38750;&#25919;&#24220;&#32452;&#32455;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A person's gender is a crucial piece of information when performing research across a wide range of scientific disciplines, such as medicine, sociology, political science, and economics, to name a few. However, in increasing instances, especially given the proliferation of big data, gender information is not readily available. In such cases researchers need to infer gender from readily available information, primarily from persons' names. While inferring gender from name may raise some ethical questions, the lack of viable alternatives means that researchers have to resort to such approaches when the goal justifies the means - in the majority of such studies the goal is to examine patterns and determinants of gender disparities. The necessity of name-to-gender inference has generated an ever-growing domain of algorithmic approaches and software products. These approaches have been used throughout the world in academia, industry, governmental and non-governmental organizations. Neverthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#19968;&#32452;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#19982;&#22522;&#20110;&#36793;&#32536;&#30340;&#25104;&#26412;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#25506;&#32034;&#38468;&#21152;&#26679;&#26412;&#26469;&#25552;&#39640;&#24320;&#25918;&#38598;&#20154;&#33080;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22806;&#37096;&#25968;&#25454;&#24211;&#33719;&#21462;&#36741;&#21161;&#36127;&#26679;&#26412;&#25110;&#36890;&#36807;&#28151;&#21512;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21512;&#25104;&#24314;&#31435;&#36127;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#21319;&#23553;&#38381;&#38598;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12371</link><description>&lt;p&gt;
&#24102;&#26377;&#31070;&#32463;&#38598;&#21512;&#12289;&#26368;&#22823;&#29109;&#25439;&#22833;&#21644;&#29305;&#24449;&#22686;&#24378;&#30340;&#24320;&#25918;&#38598;&#20154;&#33080;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Open-set Face Recognition with Neural Ensemble, Maximal Entropy Loss and Feature Augmentation. (arXiv:2308.12371v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#19968;&#32452;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#19982;&#22522;&#20110;&#36793;&#32536;&#30340;&#25104;&#26412;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#25506;&#32034;&#38468;&#21152;&#26679;&#26412;&#26469;&#25552;&#39640;&#24320;&#25918;&#38598;&#20154;&#33080;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22806;&#37096;&#25968;&#25454;&#24211;&#33719;&#21462;&#36741;&#21161;&#36127;&#26679;&#26412;&#25110;&#36890;&#36807;&#28151;&#21512;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21512;&#25104;&#24314;&#31435;&#36127;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#21319;&#23553;&#38381;&#38598;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#38598;&#20154;&#33080;&#35782;&#21035;&#26159;&#25351;&#29983;&#29289;&#35782;&#21035;&#31995;&#32479;&#23545;&#25152;&#26377;&#24050;&#23384;&#22312;&#20027;&#20307;&#30340;&#30693;&#35782;&#19981;&#23436;&#25972;&#30340;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#26399;&#26395;&#23427;&#20204;&#33021;&#22815;&#38450;&#27490;&#23558;&#26410;&#27880;&#20876;&#20027;&#20307;&#30340;&#20154;&#33080;&#26679;&#26412;&#35782;&#21035;&#20026;&#20808;&#21069;&#27880;&#20876;&#30340;&#36523;&#20221;&#12290;&#36825;&#31181;&#30417;&#35270;&#21015;&#34920;&#30340;&#32972;&#26223;&#22686;&#21152;&#20102;&#19968;&#20010;&#33392;&#24040;&#30340;&#35201;&#27714;&#65292;&#35201;&#27714;&#20027;&#35201;&#20851;&#27880;&#24863;&#20852;&#36259;&#30340;&#20027;&#20307;&#65292;&#20174;&#32780;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#20154;&#33080;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#19968;&#32452;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#19982;&#22522;&#20110;&#36793;&#32536;&#30340;&#25104;&#26412;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#35813;&#20989;&#25968;&#25506;&#32034;&#38468;&#21152;&#26679;&#26412;&#12290;&#36741;&#21161;&#36127;&#26679;&#26412;&#21487;&#20197;&#20174;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#33719;&#24471;&#65292;&#25110;&#32773;&#22312;&#35757;&#32451;&#26102;&#36890;&#36807;&#26032;&#30340;&#28151;&#21512;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#22312;&#34920;&#31034;&#23618;&#27425;&#19978;&#36827;&#34892;&#21512;&#25104;&#26500;&#24314;&#12290;&#39044;&#20808;&#22312;&#22823;&#22411;&#20154;&#33080;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#21021;&#27493;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#12290;&#25105;&#20204;&#22312;&#30693;&#21517;&#30340;LFW&#21644;IJB-C&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#21319;&#23553;&#38381;&#38598;&#30340;
&lt;/p&gt;
&lt;p&gt;
Open-set face recognition refers to a scenario in which biometric systems have incomplete knowledge of all existing subjects. Therefore, they are expected to prevent face samples of unregistered subjects from being identified as previously enrolled identities. This watchlist context adds an arduous requirement that calls for the dismissal of irrelevant faces by focusing mainly on subjects of interest. As a response, this work introduces a novel method that associates an ensemble of compact neural networks with a margin-based cost function that explores additional samples. Supplementary negative samples can be obtained from external databases or synthetically built at the representation level in training time with a new mix-up feature augmentation approach. Deep neural networks pre-trained on large face datasets serve as the preliminary feature extraction module. We carry out experiments on well-known LFW and IJB-C datasets where results show that the approach is able to boost closed an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#25937;&#26041;&#27861;&#65288;SafeAR&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#39118;&#38505;&#22240;&#32032;&#22312;&#35745;&#31639;&#21644;&#35780;&#20272;&#34917;&#25937;&#25514;&#26045;&#26102;&#65292;&#20026;&#37027;&#20123;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#19981;&#21033;&#24433;&#21709;&#30340;&#20010;&#20307;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.12367</link><description>&lt;p&gt;
SafeAR: &#36890;&#36807;&#39118;&#38505;&#24863;&#30693;&#31574;&#30053;&#23454;&#29616;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#20607;
&lt;/p&gt;
&lt;p&gt;
SafeAR: Towards Safer Algorithmic Recourse by Risk-Aware Policies. (arXiv:2308.12367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#25937;&#26041;&#27861;&#65288;SafeAR&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#39118;&#38505;&#22240;&#32032;&#22312;&#35745;&#31639;&#21644;&#35780;&#20272;&#34917;&#25937;&#25514;&#26045;&#26102;&#65292;&#20026;&#37027;&#20123;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#19981;&#21033;&#24433;&#21709;&#30340;&#20010;&#20307;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#37329;&#34701;&#21644;&#21307;&#30103;&#31561;&#20851;&#38190;&#39046;&#22495;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#20026;&#37027;&#20123;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#19981;&#21033;&#24433;&#21709;&#30340;&#20010;&#20307;&#25552;&#20379;&#34917;&#25937;&#25514;&#26045;&#30340;&#38656;&#27714;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#65307;&#20010;&#20307;&#24212;&#35813;&#33719;&#24471;&#25913;&#21892;&#33258;&#36523;&#24773;&#20917;&#21644;&#33719;&#24471;&#26377;&#21033;&#20915;&#31574;&#30340;&#24314;&#35758;&#12290;&#20043;&#21069;&#20851;&#20110;&#39034;&#24207;&#31639;&#27861;&#34917;&#25937;&#30340;&#24037;&#20316;&#8212;&#8212;&#25512;&#33616;&#19968;&#31995;&#21015;&#21464;&#21270;&#8212;&#8212;&#20027;&#35201;&#20851;&#27880;&#34892;&#21160;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20351;&#29992;&#29305;&#24449;&#21464;&#21270;&#30340;&#25509;&#36817;&#31243;&#24230;&#30830;&#23450;&#34892;&#21160;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#26410;&#32771;&#34385;&#29305;&#24449;&#21464;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#34917;&#25937;&#20013;&#39640;&#20110;&#24179;&#22343;&#25104;&#26412;&#30340;&#39118;&#38505;&#12290;&#22914;&#26524;&#34917;&#25937;&#25514;&#26045;&#21487;&#33021;&#65288;&#20197;&#19968;&#23450;&#27010;&#29575;&#65289;&#23548;&#33268;&#26356;&#31967;&#31957;&#30340;&#24773;&#20917;&#65292;&#32780;&#24674;&#22797;&#38656;&#35201;&#20184;&#20986;&#38750;&#24120;&#39640;&#30340;&#20195;&#20215;&#65292;&#37027;&#23558;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#22312;&#35745;&#31639;&#21644;&#35780;&#20272;&#34917;&#25937;&#25514;&#26045;&#26102;&#65292;&#24517;&#39035;&#32771;&#34385;&#39118;&#38505;&#12290;&#25105;&#20204;&#23558;&#32771;&#34385;&#20102;&#36825;&#31181;&#39118;&#38505;&#22240;&#32032;&#35745;&#31639;&#20986;&#30340;&#34917;&#25937;&#25514;&#26045;&#31216;&#20026;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#25937;&#65288;SafeAR&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing use of machine learning (ML) models in critical domains such as finance and healthcare, the need to offer recourse for those adversely affected by the decisions of ML models has become more important; individuals ought to be provided with recommendations on actions to take for improving their situation and thus receive a favorable decision. Prior work on sequential algorithmic recourse -- which recommends a series of changes -- focuses on action feasibility and uses the proximity of feature changes to determine action costs. However, the uncertainties of feature changes and the risk of higher than average costs in recourse have not been considered. It is undesirable if a recourse could (with some probability) result in a worse situation from which recovery requires an extremely high cost. It is essential to incorporate risks when computing and evaluating recourse. We call the recourse computed with such risk considerations as Safer Algorithmic Recourse (SafeAR). The ob
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#32479;&#35745;&#23398;&#21644;&#37327;&#23376;&#22330;&#35770;&#30340;&#36870;&#35268;&#33539;&#21270;&#32676;&#27969;&#65292;&#20026;&#26500;&#24314;&#29992;&#20110;&#30740;&#31350;&#22330;&#35770;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#20855;&#20307;&#26694;&#26550;&#65292;&#24182;&#35814;&#32454;&#35828;&#26126;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#23450;&#20041;&#19968;&#31867;&#33258;&#36866;&#24212;&#26725;&#25509;&#21462;&#26679;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.12355</link><description>&lt;p&gt;
&#35268;&#33539;&#21270;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Renormalizing Diffusion Models. (arXiv:2308.12355v1 [hep-th])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12355
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#32479;&#35745;&#23398;&#21644;&#37327;&#23376;&#22330;&#35770;&#30340;&#36870;&#35268;&#33539;&#21270;&#32676;&#27969;&#65292;&#20026;&#26500;&#24314;&#29992;&#20110;&#30740;&#31350;&#22330;&#35770;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#20855;&#20307;&#26694;&#26550;&#65292;&#24182;&#35814;&#32454;&#35828;&#26126;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#23450;&#20041;&#19968;&#31867;&#33258;&#36866;&#24212;&#26725;&#25509;&#21462;&#26679;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#37322;&#20102;&#22914;&#20309;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#32479;&#35745;&#23398;&#21644;&#37327;&#23376;&#22330;&#35770;&#30340;&#36870;&#35268;&#33539;&#21270;&#32676;&#27969;&#12290;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#23558;&#25968;&#25454;&#28155;&#21152;&#22122;&#22768;&#30340;&#36870;&#36807;&#31243;&#65292;&#29983;&#25104;&#22797;&#26434;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#20363;&#22914;&#33258;&#28982;&#22270;&#20687;&#30340;&#20998;&#24067;&#12290;&#38750;&#24494;&#25200;&#35268;&#33539;&#21270;&#32676;&#26041;&#26696;&#21487;&#20197;&#33258;&#28982;&#22320;&#20889;&#25104;&#22330;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#32467;&#21512;&#21040;&#19968;&#20010;&#20855;&#20307;&#30340;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#26500;&#24314;&#29992;&#20110;&#30740;&#31350;&#22330;&#35770;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#27169;&#22411;&#23398;&#20064;&#21040;&#19968;&#20010;&#26126;&#30830;&#25351;&#23450;&#30340;&#35268;&#33539;&#21270;&#32676;&#26041;&#26696;&#30340;&#36870;&#36807;&#31243;&#12290;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#23450;&#20041;&#26684;&#28857;&#22330;&#35770;&#30340;&#33258;&#36866;&#24212;&#26725;&#25509;&#65288;&#25110;&#24179;&#34892;&#28140;&#28779;&#65289;&#21462;&#26679;&#22120;&#30340;&#19968;&#31867;&#12290;&#30001;&#20110;&#35268;&#33539;&#21270;&#32676;&#26041;&#26696;&#20855;&#26377;&#29289;&#29702;&#24847;&#20041;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26126;&#30830;&#27604;&#36739;&#19981;&#21516;&#26041;&#26696;&#30340;&#25351;&#23548;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explain how to use diffusion models to learn inverse renormalization group flows of statistical and quantum field theories. Diffusion models are a class of machine learning models which have been used to generate samples from complex distributions, such as the distribution of natural images, by learning the inverse process to a diffusion process which adds noise to the data until the distribution of the data is pure noise. Nonperturbative renormalization group schemes can naturally be written as diffusion processes in the space of fields. We combine these observations in a concrete framework for building ML-based models for studying field theories, in which the models learn the inverse process to an explicitly-specified renormalization group scheme. We detail how these models define a class of adaptive bridge (or parallel tempering) samplers for lattice field theory. Because renormalization group schemes have a physical meaning, we provide explicit prescriptions for how to compare r
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#39044;&#27979;&#23567;&#20998;&#23376;&#24615;&#36136;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#19981;&#21516;&#24615;&#36136;&#30340;&#39044;&#27979;&#21644;&#20248;&#21270;&#25361;&#25112;&#20197;&#21450;&#22810;&#30446;&#26631;&#20248;&#21270;&#25216;&#26415;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#35780;&#20272;&#20102;&#25552;&#20379;&#27169;&#22411;&#39044;&#27979;&#29702;&#35299;&#30340;&#25216;&#26415;&#23545;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#20915;&#31574;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12354</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#39044;&#27979;&#23567;&#20998;&#23376;&#24615;&#36136;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Small Molecule Properties in Drug Discovery. (arXiv:2308.12354v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12354
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#39044;&#27979;&#23567;&#20998;&#23376;&#24615;&#36136;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#19981;&#21516;&#24615;&#36136;&#30340;&#39044;&#27979;&#21644;&#20248;&#21270;&#25361;&#25112;&#20197;&#21450;&#22810;&#30446;&#26631;&#20248;&#21270;&#25216;&#26415;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#35780;&#20272;&#20102;&#25552;&#20379;&#27169;&#22411;&#39044;&#27979;&#29702;&#35299;&#30340;&#25216;&#26415;&#23545;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#20915;&#31574;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26159;&#19968;&#31181;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#39044;&#27979;&#23567;&#20998;&#23376;&#24615;&#36136;&#30340;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#20026;&#27492;&#30446;&#30340;&#24341;&#20837;&#30340;&#21508;&#31181;ML&#26041;&#27861;&#12290;&#25105;&#20204;&#23457;&#26597;&#20102;&#19968;&#31995;&#21015;&#24615;&#36136;&#65292;&#21253;&#25324;&#32467;&#21512;&#20146;&#21644;&#21147;&#12289;&#28342;&#35299;&#24230;&#21644;ADMET&#65288;&#21560;&#25910;&#12289;&#20998;&#24067;&#12289;&#20195;&#35874;&#12289;&#25490;&#27844;&#21644;&#27602;&#24615;&#65289;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30340;&#27969;&#34892;&#25968;&#25454;&#38598;&#21644;&#20998;&#23376;&#25551;&#36848;&#31526;&#21644;&#23884;&#20837;&#26041;&#27861;&#65292;&#22914;&#21270;&#23398;&#25351;&#32441;&#21644;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#22312;&#33647;&#29289;&#21457;&#29616;&#30340;&#21629;&#20013;&#20248;&#36873;&#21644;&#20248;&#21270;&#24341;&#23548;&#38454;&#27573;&#39044;&#27979;&#21644;&#20248;&#21270;&#22810;&#20010;&#24615;&#36136;&#30340;&#25361;&#25112;&#65292;&#24182;&#31616;&#35201;&#25506;&#35752;&#20102;&#21487;&#33021;&#29992;&#20110;&#24179;&#34913;&#19981;&#21516;&#24615;&#36136;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25552;&#20379;&#27169;&#22411;&#39044;&#27979;&#29702;&#35299;&#30340;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#20851;&#38190;&#20915;&#31574;&#30340;&#24773;&#20917;&#19979;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#23545;&#26426;&#22120;&#23398;&#20064;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#39044;&#27979;&#23567;&#20998;&#23376;&#24615;&#36136;&#26041;&#38754;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) is a promising approach for predicting small molecule properties in drug discovery. Here, we provide a comprehensive overview of various ML methods introduced for this purpose in recent years. We review a wide range of properties, including binding affinities, solubility, and ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity). We discuss existing popular datasets and molecular descriptors and embeddings, such as chemical fingerprints and graph-based neural networks. We highlight also challenges of predicting and optimizing multiple properties during hit-to-lead and lead optimization stages of drug discovery and explore briefly possible multi-objective optimization techniques that can be used to balance diverse properties while optimizing lead candidates. Finally, techniques to provide an understanding of model predictions, especially for critical decision-making in drug discovery are assessed. Overall, this review provides insights into the land
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Schr&#246;dinger&#26725;&#21644;&#25193;&#25955;&#27169;&#22411;&#21019;&#24314;&#30340;&#23637;&#24320;&#26041;&#27861;SBUnfold&#65292;&#23427;&#23558;&#21028;&#21035;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#12290;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#21512;&#25104;&#30340;Z+jets&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12351</link><description>&lt;p&gt;
&#29992;Schr&#246;dinger&#26725;&#25913;&#36827;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#23637;&#24320;
&lt;/p&gt;
&lt;p&gt;
Improving Generative Model-based Unfolding with Schr\"{o}dinger Bridges. (arXiv:2308.12351v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Schr&#246;dinger&#26725;&#21644;&#25193;&#25955;&#27169;&#22411;&#21019;&#24314;&#30340;&#23637;&#24320;&#26041;&#27861;SBUnfold&#65292;&#23427;&#23558;&#21028;&#21035;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#12290;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#21512;&#25104;&#30340;Z+jets&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23637;&#24320;&#24050;&#32463;&#23454;&#29616;&#20102;&#26080;bin&#21644;&#39640;&#32500;&#24494;&#20998;&#25130;&#38754;&#27979;&#37327;&#12290;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#20986;&#29616;&#20102;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#21028;&#21035;&#27169;&#22411;&#65292;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#12290;&#21028;&#21035;&#27169;&#22411;&#30340;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#65292;&#23427;&#20204;&#23398;&#20064;&#20102;&#23545;&#36215;&#22987;&#27169;&#25311;&#30340;&#23567;&#20462;&#27491;&#65292;&#32780;&#29983;&#25104;&#27169;&#22411;&#22312;&#25968;&#25454;&#31232;&#30095;&#30340;&#30456;&#31354;&#38388;&#21306;&#22495;&#20855;&#26377;&#26356;&#22909;&#30340;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;Schr&#246;dinger&#26725;&#21644;&#25193;&#25955;&#27169;&#22411;&#21019;&#24314;SBUnfold&#65292;&#19968;&#31181;&#23558;&#21028;&#21035;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#30340;&#23637;&#24320;&#26041;&#27861;&#12290;SBUnfold&#30340;&#20851;&#38190;&#29305;&#28857;&#26159;&#23427;&#30340;&#29983;&#25104;&#27169;&#22411;&#23558;&#19968;&#32452;&#20107;&#20214;&#26144;&#23556;&#21040;&#21478;&#19968;&#32452;&#20107;&#20214;&#65292;&#32780;&#26080;&#38656;&#36890;&#36807;&#24050;&#30693;&#30340;&#27010;&#29575;&#23494;&#24230;&#65288;&#19982;&#26631;&#20934;&#25193;&#25955;&#27169;&#22411;&#21644;&#26631;&#20934;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;SBUnfold&#22312;&#21512;&#25104;&#30340;Z+jets&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning-based unfolding has enabled unbinned and high-dimensional differential cross section measurements. Two main approaches have emerged in this research area: one based on discriminative models and one based on generative models. The main advantage of discriminative models is that they learn a small correction to a starting simulation while generative models scale better to regions of phase space with little data. We propose to use Schroedinger Bridges and diffusion models to create SBUnfold, an unfolding approach that combines the strengths of both discriminative and generative models. The key feature of SBUnfold is that its generative model maps one set of events into another without having to go through a known probability density as is the case for normalizing flows and standard diffusion models. We show that SBUnfold achieves excellent performance compared to state of the art methods on a synthetic Z+jets dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#21644;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#33647;&#29289;&#28342;&#35299;&#24230;&#65292;&#20854;&#20013;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;&#36890;&#36807;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#26512;&#65292;&#21487;&#20197;&#20102;&#35299;&#27599;&#20010;&#21151;&#33021;&#22242;&#23545;&#28342;&#35299;&#24230;&#30340;&#24433;&#21709;&#12290;&#23558;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#24615;&#33021;&#19982;&#32447;&#24615;&#22238;&#24402;&#30340;&#21487;&#35299;&#37322;&#24615;&#30456;&#32467;&#21512;&#26159;&#26410;&#26469;&#24037;&#20316;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.12325</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#39044;&#27979;&#33647;&#29289;&#28342;&#35299;&#24230;--&#25552;&#21462;&#21270;&#23398;&#29305;&#24449;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#19982;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Predicting Drug Solubility Using Different Machine Learning Methods -- Linear Regression Model with Extracted Chemical Features vs Graph Convolutional Neural Network. (arXiv:2308.12325v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#21644;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#33647;&#29289;&#28342;&#35299;&#24230;&#65292;&#20854;&#20013;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;&#36890;&#36807;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#26512;&#65292;&#21487;&#20197;&#20102;&#35299;&#27599;&#20010;&#21151;&#33021;&#22242;&#23545;&#28342;&#35299;&#24230;&#30340;&#24433;&#21709;&#12290;&#23558;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#24615;&#33021;&#19982;&#32447;&#24615;&#22238;&#24402;&#30340;&#21487;&#35299;&#37322;&#24615;&#30456;&#32467;&#21512;&#26159;&#26410;&#26469;&#24037;&#20316;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#32473;&#23450;&#20998;&#23376;&#30340;&#28342;&#35299;&#24230;&#26159;&#21046;&#33647;&#34892;&#19994;&#20013;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#22240;&#27492;&#26159;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#35838;&#39064;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#29616;&#20195;&#35745;&#31639;&#36164;&#28304;&#30340;&#20248;&#21183;&#37325;&#26032;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#20102;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#21644;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#20004;&#31181;&#26041;&#27861;&#37117;&#33021;&#20570;&#20986;&#21512;&#29702;&#30340;&#39044;&#27979;&#65292;&#32780;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26159;&#19968;&#20010;&#40657;&#30418;&#23376;&#65292;&#32780;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#26512;&#25552;&#20379;&#20102;&#26356;&#22810;&#26377;&#20851;&#24213;&#23618;&#21270;&#23398;&#24433;&#21709;&#30340;&#27934;&#23519;&#12290;&#21033;&#29992;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27599;&#20010;&#21151;&#33021;&#22242;&#23545;&#25972;&#20307;&#28342;&#35299;&#24230;&#30340;&#24433;&#21709;&#12290;&#26368;&#32456;&#65292;&#22312;&#35774;&#35745;&#26032;&#33647;&#26102;&#65292;&#20102;&#35299;&#21270;&#23398;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#21270;&#23398;&#24615;&#36136;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#24212;&#35813;&#33268;&#21147;&#20110;&#23558;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#24615;&#33021;&#19982;&#32447;&#24615;&#22238;&#24402;&#30340;&#21487;&#35299;&#37322;&#24615;&#30456;&#32467;&#21512;&#65292;&#37322;&#25918;&#20986;&#26032;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the solubility of given molecules is an important task in the pharmaceutical industry, and consequently this is a well-studied topic. In this research, we revisited this problem with the advantage of modern computing resources. We applied two machine learning models, a linear regression model and a graph convolutional neural network model, on multiple experimental datasets. Both methods can make reasonable predictions while the GCNN model had the best performance. However, the current GCNN model is a black box, while feature importance analysis from the linear regression model offers more insights into the underlying chemical influences. Using the linear regression model, we show how each functional group affects the overall solubility. Ultimately, knowing how chemical structure influences chemical properties is crucial when designing new drugs. Future work should aim to combine the high performance of GCNNs with the interpretability of linear regression, unlocking new advan
&lt;/p&gt;</description></item><item><title>&#22270;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Graph Neural SDEs&#65289;&#36890;&#36807;&#23558;&#38543;&#26426;&#24615;&#23884;&#20837;&#21040;&#22270;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20013;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#22312;&#38745;&#24577;&#21644;&#26102;&#31354;&#32972;&#26223;&#19979;&#30340;&#20998;&#24067;&#22806;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2308.12316</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Stochastic Differential Equations. (arXiv:2308.12316v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12316
&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Graph Neural SDEs&#65289;&#36890;&#36807;&#23558;&#38543;&#26426;&#24615;&#23884;&#20837;&#21040;&#22270;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20013;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#22312;&#38745;&#24577;&#21644;&#26102;&#31354;&#32972;&#26223;&#19979;&#30340;&#20998;&#24067;&#22806;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#22270;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Graph Neural Stochastic Differential Equations&#65292;Graph Neural SDEs&#65289;&#12290;&#36825;&#31181;&#25216;&#26415;&#36890;&#36807;&#23558;&#24067;&#26391;&#36816;&#21160;&#24341;&#20837;&#25968;&#25454;&#34920;&#31034;&#20013;&#65292;&#23558;&#38543;&#26426;&#24615;&#23884;&#20837;&#21040;&#22270;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Graph Neural Ordinary Differential Equations&#65292;Graph Neural ODEs&#65289;&#20013;&#12290;&#36825;&#31181;&#23884;&#20837;&#20351;&#24471;&#21487;&#20197;&#35780;&#20272;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#26159;&#30446;&#21069;&#30340;&#27169;&#22411;&#32463;&#24120;&#24573;&#35270;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#8220;&#28508;&#22312;&#22270;&#31070;&#32463;SDE&#8221;&#21464;&#20307;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#28508;&#22312;&#22270;&#31070;&#32463;SDE&#22312;&#32622;&#20449;&#24230;&#39044;&#27979;&#26041;&#38754;&#20248;&#20110;&#24120;&#35268;&#27169;&#22411;&#22914;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#22270;&#31070;&#32463;ODEs&#65292;&#23588;&#20854;&#22312;&#22788;&#29702;&#38745;&#24577;&#21644;&#26102;&#31354;&#32972;&#26223;&#19979;&#30340;&#20998;&#24067;&#22806;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel model Graph Neural Stochastic Differential Equations (Graph Neural SDEs). This technique enhances the Graph Neural Ordinary Differential Equations (Graph Neural ODEs) by embedding randomness into data representation using Brownian motion. This inclusion allows for the assessment of prediction uncertainty, a crucial aspect frequently missed in current models. In our framework, we spotlight the \textit{Latent Graph Neural SDE} variant, demonstrating its effectiveness. Through empirical studies, we find that Latent Graph Neural SDEs surpass conventional models like Graph Convolutional Networks and Graph Neural ODEs, especially in confidence prediction, making them superior in handling out-of-distribution detection across both static and spatio-temporal contexts.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#36328;&#39046;&#22495;&#30340;&#21487;&#20449;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21253;&#25324;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#27010;&#24565;&#65292;&#23545;&#35813;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.12315</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#30340;&#21487;&#20449;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Representation Learning Across Domains. (arXiv:2308.12315v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#36328;&#39046;&#22495;&#30340;&#21487;&#20449;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21253;&#25324;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#27010;&#24565;&#65292;&#23545;&#35813;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#21644;&#20154;&#31867;&#31038;&#20250;&#20013;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20154;&#20204;&#26082;&#20139;&#21463;&#21040;&#20102;&#36825;&#20123;&#25216;&#26415;&#24102;&#26469;&#30340;&#22909;&#22788;&#65292;&#20063;&#38754;&#20020;&#22240;&#36825;&#20123;&#31995;&#32479;&#32780;&#24341;&#21457;&#30340;&#35768;&#22810;&#31038;&#20250;&#38382;&#39064;&#12290;&#20026;&#20102;&#20351;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36275;&#22815;&#22909;&#24182;&#19988;&#21487;&#20449;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#24314;&#31435;&#20102;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#25351;&#21335;&#12290;&#26426;&#22120;&#23398;&#20064;&#26159;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#20043;&#19968;&#65292;&#32780;&#34920;&#31034;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#30784;&#25216;&#26415;&#12290;&#22914;&#20309;&#20351;&#34920;&#31034;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#21487;&#20449;&#24230;&#65292;&#20363;&#22914;&#36328;&#39046;&#22495;&#22330;&#26223;&#65292;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#39046;&#22495;&#37117;&#26159;&#38750;&#24120;&#26377;&#20215;&#20540;&#21644;&#24517;&#35201;&#30340;&#12290;&#22312;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36328;&#39046;&#22495;&#30340;&#21487;&#20449;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#25324;&#20102;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#36825;&#22235;&#20010;&#27010;&#24565;&#65292;&#23545;&#36825;&#20010;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI systems have obtained significant performance to be deployed widely in our daily live and human society, people both enjoy the benefits brought by these technologies and suffer many social issues induced by these systems. To make AI systems good enough and trustworthy, plenty of researches have been done to build guidelines for trustworthy AI systems. Machine learning is one of the most important parts for AI systems and representation learning is the fundamental technology in machine learning. How to make the representation learning trustworthy in real-world application, e.g., cross domain scenarios, is very valuable and necessary for both machine learning and AI system fields. Inspired by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework which includes four concepts, i.e, robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20511;&#21161;&#24067;&#23572;&#24433;&#21709;&#21147;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;NPN&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#21644;&#31639;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#24433;&#21709;&#21147;&#22312;&#20943;&#23567;&#36716;&#25442;&#20989;&#25968;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.12311</link><description>&lt;p&gt;
&#20511;&#21161;&#24433;&#21709;&#36741;&#21161;&#30340;&#35268;&#33539;&#24418;&#24335;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;NPN&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Fast Exact NPN Classification with Influence-aided Canonical Form. (arXiv:2308.12311v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20511;&#21161;&#24067;&#23572;&#24433;&#21709;&#21147;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;NPN&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#21644;&#31639;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#24433;&#21709;&#21147;&#22312;&#20943;&#23567;&#36716;&#25442;&#20989;&#25968;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NPN&#20998;&#31867;&#22312;&#25968;&#23383;&#30005;&#36335;&#30340;&#32508;&#21512;&#21644;&#39564;&#35777;&#20013;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#22522;&#20110;&#35268;&#33539;&#24418;&#24335;&#30340;&#26041;&#27861;&#26159;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#35774;&#35745;&#19968;&#20010;&#20316;&#20026;NPN&#31561;&#20215;&#31867;&#20195;&#34920;&#30340;&#35268;&#33539;&#24418;&#24335;&#65292;&#28982;&#21518;&#26681;&#25454;&#35268;&#33539;&#24418;&#24335;&#35745;&#31639;&#36716;&#25442;&#20989;&#25968;&#12290;&#22823;&#22810;&#25968;&#24037;&#20316;&#20351;&#29992;&#21464;&#37327;&#23545;&#31216;&#24615;&#21644;&#20960;&#20010;&#26631;&#35760;&#65292;&#20027;&#35201;&#22522;&#20110;&#20313;&#23376;&#24335;&#65292;&#26469;&#31616;&#21270;&#35268;&#33539;&#24418;&#24335;&#30340;&#26500;&#24314;&#21644;&#35745;&#31639;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#24067;&#23572;&#20989;&#25968;&#20998;&#26512;&#20013;&#30340;&#24067;&#23572;&#24433;&#21709;&#21147;&#65292;&#25551;&#36848;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35268;&#33539;&#24418;&#24335;&#21450;&#20854;&#35745;&#31639;&#31639;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24433;&#21709;&#21147;&#19982;NPN&#20998;&#31867;&#30340;&#36755;&#20837;&#21542;&#23450;&#26080;&#20851;&#12289;&#36755;&#20837;&#32622;&#25442;&#20381;&#36182;&#20197;&#21450;&#20855;&#26377;&#20854;&#20182;&#32467;&#26500;&#20449;&#24687;&#65292;&#22240;&#27492;&#23427;&#26159;&#21152;&#36895;NPN&#20998;&#31867;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;&#24433;&#21709;&#21147;&#22312;&#20943;&#23567;&#36716;&#25442;&#20989;&#25968;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
NPN classification has many applications in the synthesis and verification of digital circuits. The canonical-form-based method is the most common approach, designing a canonical form as representative for the NPN equivalence class first and then computing the transformation function according to the canonical form. Most works use variable symmetries and several signatures, mainly based on the cofactor, to simplify the canonical form construction and computation. This paper describes a novel canonical form and its computation algorithm by introducing Boolean influence to NPN classification, which is a basic concept in analysis of Boolean functions. We show that influence is input-negation-independent, input-permutation-dependent, and has other structural information than previous signatures for NPN classification. Therefore, it is a significant ingredient in speeding up NPN classification. Experimental results prove that influence plays an important role in reducing the transformation 
&lt;/p&gt;</description></item><item><title>FedDAT&#26159;&#19968;&#31181;&#22312;&#22810;&#27169;&#24577;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#36827;&#34892;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#26469;&#20943;&#36731;&#23458;&#25143;&#31471;&#35745;&#31639;&#36127;&#25285;&#21644;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12305</link><description>&lt;p&gt;
FedDAT: &#19968;&#31181;&#22810;&#27169;&#24577;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning. (arXiv:2308.12305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12305
&lt;/p&gt;
&lt;p&gt;
FedDAT&#26159;&#19968;&#31181;&#22312;&#22810;&#27169;&#24577;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#36827;&#34892;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#26469;&#20943;&#36731;&#23458;&#25143;&#31471;&#35745;&#31639;&#36127;&#25285;&#21644;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#65292;&#22522;&#30784;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#37197;&#22791;&#20102;&#25968;&#30334;&#19975;&#65288;&#25110;&#25968;&#21313;&#20159;&#65289;&#20010;&#21442;&#25968;&#65292;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#30340;&#38544;&#31169;&#27861;&#35268;&#65292;&#20174;&#19981;&#21516;&#39046;&#22495;&#25910;&#38598;&#21644;&#38598;&#20013;&#35757;&#32451;&#25968;&#25454;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#21487;&#20197;&#35753;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#19981;&#38598;&#20013;&#20854;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#20026;&#20102;&#20943;&#36731;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#36127;&#25285;&#21644;&#36890;&#20449;&#24320;&#38144;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#37319;&#29992;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#29992;&#20110;FL&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#23569;&#37327;&#27169;&#22411;&#21442;&#25968;&#22312;&#32852;&#37030;&#36890;&#20449;&#26399;&#38388;&#36827;&#34892;&#20248;&#21270;&#21644;&#20256;&#36755;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#24037;&#20316;&#37117;&#19987;&#27880;&#20110;&#21333;&#19968;&#24418;&#24577;&#65292;&#24182;&#24573;&#30053;&#20102;&#19968;&#31181;&#24120;&#35265;&#29616;&#35937;&#65292;&#21363;&#23458;&#25143;&#31471;&#20043;&#38388;&#23384;&#22312;&#25968;&#25454;&#24322;&#26500;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24494;&#35843;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, foundation models have exhibited remarkable advancements in multi-modal learning. These models, equipped with millions (or billions) of parameters, typically require a substantial amount of data for finetuning. However, collecting and centralizing training data from diverse sectors becomes challenging due to distinct privacy regulations. Federated Learning (FL) emerges as a promising solution, enabling multiple clients to collaboratively train neural networks without centralizing their local data. To alleviate client computation burdens and communication overheads, previous works have adapted Parameter-efficient Finetuning (PEFT) methods for FL. Hereby, only a small fraction of the model parameters are optimized and communicated during federated communications. Nevertheless, most previous works have focused on a single modality and neglected one common phenomenon, i.e., the presence of data heterogeneity across the clients. Therefore, in this work, we propose a finetuning fra
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#37327;&#23376;&#27979;&#37327;&#31867;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#23545;&#24212;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#19978;&#30028;&#12290;&#25105;&#20204;&#21457;&#29616;&#26631;&#20934;ERM&#26410;&#28385;&#36275;&#32479;&#19968;&#25910;&#25947;&#24615;&#30340;&#38382;&#39064;&#65292;&#20110;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#35268;&#21017;&#8212;&#8212;&#21435;&#22122;ERM&#65292;&#35813;&#35268;&#21017;&#22312;POVM&#21644;&#27010;&#29575;&#35266;&#27979;&#30340;&#27010;&#24565;&#31867;&#21035;&#20013;&#20855;&#26377;&#26222;&#36866;&#24615;&#24182;&#28385;&#36275;&#32479;&#19968;&#25910;&#25947;&#24615;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2308.12304</link><description>&lt;p&gt;
&#33026;&#32938;&#30862;&#21270;&#12289;&#32852;&#21512;&#21487;&#27979;&#24615;&#21644;POVM&#20551;&#35774;&#31867;&#30340;PAC&#21487;&#23398;&#20064;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fat Shattering, Joint Measurability, and PAC Learnability of POVM Hypothesis Classes. (arXiv:2308.12304v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12304
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#37327;&#23376;&#27979;&#37327;&#31867;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#23545;&#24212;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#19978;&#30028;&#12290;&#25105;&#20204;&#21457;&#29616;&#26631;&#20934;ERM&#26410;&#28385;&#36275;&#32479;&#19968;&#25910;&#25947;&#24615;&#30340;&#38382;&#39064;&#65292;&#20110;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#35268;&#21017;&#8212;&#8212;&#21435;&#22122;ERM&#65292;&#35813;&#35268;&#21017;&#22312;POVM&#21644;&#27010;&#29575;&#35266;&#27979;&#30340;&#27010;&#24565;&#31867;&#21035;&#20013;&#20855;&#26377;&#26222;&#36866;&#24615;&#24182;&#28385;&#36275;&#32479;&#19968;&#25910;&#25947;&#24615;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24314;&#31435;&#21305;&#37197;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#32473;&#20986;&#30456;&#24212;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#19978;&#30028;&#65292;&#25105;&#20204;&#23545;&#37327;&#23376;&#27979;&#37327;&#31867;&#30340;&#21487;&#23398;&#20064;&#24615;&#36827;&#34892;&#20102;&#21051;&#30011;&#65292;&#20854;&#20013;&#23398;&#20064;&#22120;&#20165;&#33021;&#25509;&#35302;&#21040;&#24050;&#20934;&#22791;&#22909;&#30340;&#37327;&#23376;&#24577;&#12290;&#25105;&#20204;&#39318;&#20808;&#25506;&#31350;&#20102;&#20808;&#21069;&#20316;&#21697;&#20013;&#20851;&#20110;&#35813;&#35774;&#32622;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19968;&#20123;&#21487;&#23398;&#20064;&#31867;&#21035;&#20013;&#65292;&#20808;&#21069;&#20316;&#21697;&#20013;&#23450;&#20041;&#30340;&#32463;&#39564;&#39118;&#38505;&#19982;&#32463;&#20856;&#29702;&#35770;&#20013;&#30340;&#23450;&#20041;&#30456;&#19968;&#33268;&#65292;&#20294;&#26410;&#33021;&#28385;&#36275;&#32479;&#19968;&#25910;&#25947;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#20808;&#21069;&#20316;&#21697;&#20013;&#23545;VC&#32500;&#24230;&#24191;&#20041;&#19978;&#30028;&#30340;&#25512;&#24191;&#24120;&#24120;&#26159;&#26080;&#31351;&#30340;&#65292;&#21363;&#20351;&#23545;&#20110;&#26377;&#38480;&#32500;&#30340;POVM&#31867;&#21035;&#20063;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#20811;&#26381;&#26631;&#20934;ERM&#26410;&#33021;&#28385;&#36275;&#32479;&#19968;&#25910;&#25947;&#24615;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#35268;&#21017;&#8212;&#8212;&#21435;&#22122;ERM&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;POVM&#21644;&#27010;&#29575;&#35266;&#27979;&#30340;&#27010;&#24565;&#31867;&#21035;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#23398;&#20064;&#35268;&#21017;&#65292;&#24182;&#32473;&#20986;&#20102;&#23427;&#28385;&#36275;&#32479;&#19968;&#25910;&#25947;&#24615;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterize learnability for quantum measurement classes by establishing matching necessary and sufficient conditions for their PAC learnability, along with corresponding sample complexity bounds, in the setting where the learner is given access only to prepared quantum states. We first probe the results from previous works on this setting. We show that the empirical risk defined in previous works and matching the definition in the classical theory fails to satisfy the uniform convergence property enjoyed in the classical setting for some learnable classes. Moreover, we show that VC dimension generalization upper bounds in previous work are frequently infinite, even for finite-dimensional POVM classes. To surmount the failure of the standard ERM to satisfy uniform convergence, we define a new learning rule -- denoised ERM. We show this to be a universal learning rule for POVM and probabilistically observed concept classes, and the condition for it to satisfy uniform convergence is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#21521;&#25104;&#20687;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#30340;&#28145;&#24230;&#31070;&#32463;&#32423;&#38598;&#65288;ILDLS&#65289;&#26041;&#27861;&#29992;&#20110;&#25513;&#33180;&#20248;&#21270;&#65292;&#24182;&#22312;&#36845;&#20195;&#36807;&#31243;&#20013;&#21033;&#29992;&#32423;&#38598;&#20026;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.12299</link><description>&lt;p&gt;
&#21453;&#21521;&#25104;&#20687;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#30340;&#28145;&#24230;&#31070;&#32463;&#32423;&#38598;&#29992;&#20110;&#25513;&#33180;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Inverse Lithography Physics-informed Deep Neural Level Set for Mask Optimization. (arXiv:2308.12299v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#21521;&#25104;&#20687;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#30340;&#28145;&#24230;&#31070;&#32463;&#32423;&#38598;&#65288;ILDLS&#65289;&#26041;&#27861;&#29992;&#20110;&#25513;&#33180;&#20248;&#21270;&#65292;&#24182;&#22312;&#36845;&#20195;&#36807;&#31243;&#20013;&#21033;&#29992;&#32423;&#38598;&#20026;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38598;&#25104;&#30005;&#36335;&#29305;&#24449;&#23610;&#23544;&#30340;&#19981;&#26029;&#20943;&#23567;&#65292;&#20809;&#23398;&#37051;&#36817;&#20462;&#27491;&#65288;OPC&#65289;&#24050;&#25104;&#20026;&#20445;&#35777;&#20809;&#21051;&#36807;&#31243;&#20013;&#39640;&#21360;&#21047;&#24615;&#30340;&#20851;&#38190;&#20998;&#36776;&#29575;&#22686;&#24378;&#25216;&#26415;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#32423;&#38598;&#30340;&#21453;&#21521;&#25104;&#20687;&#25216;&#26415;&#65288;ILT&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;OPC&#35299;&#20915;&#26041;&#26696;&#65292;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#30340;&#22270;&#26696;&#20445;&#30495;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#20808;&#36827;&#24037;&#33402;&#20013;&#12290;&#28982;&#32780;&#65292;ILT&#30340;&#24040;&#22823;&#35745;&#31639;&#26102;&#38388;&#28040;&#32791;&#38480;&#21046;&#20102;&#20854;&#20027;&#35201;&#29992;&#20110;&#20462;&#27491;&#37096;&#20998;&#23618;&#21644;&#28909;&#28857;&#21306;&#22495;&#12290;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#22312;&#21152;&#36895;ILT&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#21453;&#21521;&#25104;&#20687;&#39046;&#22495;&#30693;&#35782;&#38480;&#21046;&#20102;&#22522;&#20110;DL&#30340;&#31639;&#27861;&#22312;&#24037;&#33402;&#31383;&#21475;&#65288;PW&#65289;&#22686;&#24378;&#31561;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#21521;&#25104;&#20687;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#30340;&#28145;&#24230;&#31070;&#32463;&#32423;&#38598;&#65288;ILDLS&#65289;&#26041;&#27861;&#26469;&#36827;&#34892;&#25513;&#33180;&#20248;&#21270;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#32423;&#38598;&#20026;DL&#26694;&#26550;&#20013;&#30340;&#19968;&#23618;&#65292;&#24182;&#22312;&#36845;&#20195;&#20013;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the feature size of integrated circuits continues to decrease, optical proximity correction (OPC) has emerged as a crucial resolution enhancement technology for ensuring high printability in the lithography process. Recently, level set-based inverse lithography technology (ILT) has drawn considerable attention as a promising OPC solution, showcasing its powerful pattern fidelity, especially in advanced process. However, massive computational time consumption of ILT limits its applicability to mainly correcting partial layers and hotspot regions. Deep learning (DL) methods have shown great potential in accelerating ILT. However, lack of domain knowledge of inverse lithography limits the ability of DL-based algorithms in process window (PW) enhancement and etc. In this paper, we propose an inverse lithography physics-informed deep neural level set (ILDLS) approach for mask optimization. This approach utilizes level set based-ILT as a layer within the DL framework and iteratively condu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25972;&#25968;&#22240;&#24335;&#20998;&#35299;&#27010;&#29575;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#36890;&#36807;&#22823;&#37327;&#21512;&#25104;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21487;&#20197;&#22312;&#32463;&#20856;&#35745;&#31639;&#26426;&#19978;&#24212;&#29992;&#65292;&#20855;&#26377;&#23454;&#29992;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12290</link><description>&lt;p&gt;
&#25972;&#25968;&#22240;&#24335;&#20998;&#35299;&#12289;&#36153;&#39532;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#32463;&#20856;&#35745;&#31639;&#26426;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Integer Factorisation, Fermat &amp; Machine Learning on a Classical Computer. (arXiv:2308.12290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12290
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25972;&#25968;&#22240;&#24335;&#20998;&#35299;&#27010;&#29575;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#36890;&#36807;&#22823;&#37327;&#21512;&#25104;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21487;&#20197;&#22312;&#32463;&#20856;&#35745;&#31639;&#26426;&#19978;&#24212;&#29992;&#65292;&#20855;&#26377;&#23454;&#29992;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27010;&#29575;&#31639;&#27861;&#29992;&#20110;&#25972;&#25968;&#22240;&#24335;&#20998;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#21171;&#20262;&#26031;&#23545;&#36153;&#39532;&#22240;&#24335;&#20998;&#35299;&#31639;&#27861;&#30340;&#25193;&#23637;&#65292;&#23558;&#25972;&#25968;&#22240;&#24335;&#20998;&#35299;&#38382;&#39064;&#36716;&#21270;&#20026;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#20998;&#31867;&#38382;&#39064;&#65292;&#22312;&#33021;&#22815;&#29983;&#25104;&#22823;&#22411;&#20266;&#38543;&#26426;&#32032;&#25968;&#30340;&#20415;&#21033;&#24615;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21512;&#25104;&#29983;&#25104;&#20102;&#19968;&#32452;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#20171;&#32461;&#35813;&#31639;&#27861;&#65292;&#24635;&#32467;&#19968;&#20123;&#23454;&#39564;&#65292;&#20998;&#26512;&#36825;&#20123;&#23454;&#39564;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#24182;&#21628;&#21505;&#20854;&#20182;&#20154;&#37325;&#29616;&#12289;&#39564;&#35777;&#24182;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#25913;&#36827;&#36825;&#31181;&#26041;&#27861;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#23454;&#29992;&#30340;&#21487;&#25193;&#23637;&#30340;&#22240;&#24335;&#20998;&#35299;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we describe a deep learning--based probabilistic algorithm for integer factorisation. We use Lawrence's extension of Fermat's factorisation algorithm to reduce the integer factorisation problem to a binary classification problem. To address the classification problem, based on the ease of generating large pseudo--random primes, a corpus of training data, as large as needed, is synthetically generated. We will introduce the algorithm, summarise some experiments, analyse where these experiments fall short, and finally put out a call to others to reproduce, verify and see if this approach can be improved to a point where it becomes a practical, scalable factorisation algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#21160;&#37327;&#30340;&#21152;&#36895;&#20998;&#22359;&#36817;&#31471;&#26694;&#26550;(ABPL+)&#26469;&#35299;&#20915;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#27604;&#36739;&#36807;&#31243;&#35299;&#20915;&#22806;&#25512;&#27493;&#39588;&#22833;&#36133;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25193;&#23637;&#31639;&#27861;&#36866;&#29992;&#20110;&#26356;&#26032;&#22359;&#21464;&#37327;&#30340;&#20219;&#20309;&#24773;&#20917;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#23637;&#31034;&#24207;&#21015;&#30340;&#23548;&#25968;&#38598;&#20026;&#20851;&#38190;&#28857;&#30340;&#24615;&#36136;&#65292;&#26356;&#26126;&#26174;&#22320;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.12126</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#21160;&#37327;&#30340;&#21152;&#36895;&#20998;&#22359;&#36817;&#31471;&#26694;&#26550;&#29992;&#20110;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
An Accelerated Block Proximal Framework with Adaptive Momentum for Nonconvex and Nonsmooth Optimization. (arXiv:2308.12126v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#21160;&#37327;&#30340;&#21152;&#36895;&#20998;&#22359;&#36817;&#31471;&#26694;&#26550;(ABPL+)&#26469;&#35299;&#20915;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#27604;&#36739;&#36807;&#31243;&#35299;&#20915;&#22806;&#25512;&#27493;&#39588;&#22833;&#36133;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25193;&#23637;&#31639;&#27861;&#36866;&#29992;&#20110;&#26356;&#26032;&#22359;&#21464;&#37327;&#30340;&#20219;&#20309;&#24773;&#20917;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#23637;&#31034;&#24207;&#21015;&#30340;&#23548;&#25968;&#38598;&#20026;&#20851;&#38190;&#28857;&#30340;&#24615;&#36136;&#65292;&#26356;&#26126;&#26174;&#22320;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#21160;&#37327;&#30340;&#21152;&#36895;&#20998;&#22359;&#36817;&#31471;&#32447;&#24615;&#26694;&#26550;&#65288;ABPL+&#65289;&#29992;&#20110;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#20248;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20123;&#31639;&#27861;&#20013;&#30340;&#22806;&#25512;&#27493;&#39588;&#22833;&#36133;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#27604;&#36739;&#36807;&#31243;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#36807;&#31243;&#35780;&#20272;&#20102;&#25105;&#20204;&#31639;&#27861;&#20013;&#36817;&#31471;&#26799;&#24230;&#27493;&#39588;&#19982;&#32447;&#24615;&#22806;&#25512;&#27493;&#39588;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#31639;&#27861;&#25193;&#23637;&#21040;&#28041;&#21450;&#20351;&#29992;&#27491;&#25972;&#25968;&#26356;&#26032;&#22359;&#21464;&#37327;&#30340;&#20219;&#20309;&#24773;&#20917;&#65292;&#20801;&#35768;&#27599;&#20010;&#21608;&#26399;&#38543;&#26426;&#37325;&#26032;&#25490;&#21015;&#21464;&#37327;&#22359;&#30340;&#26356;&#26032;&#39034;&#24207;&#12290;&#27492;&#22806;&#65292;&#22312;&#19968;&#20123;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ABPL+&#21487;&#20197;&#22312;&#20005;&#26684;&#38480;&#23450;&#22806;&#25512;&#21442;&#25968;&#21644;&#27493;&#38271;&#30340;&#24773;&#20917;&#19979;&#21333;&#35843;&#22320;&#20943;&#23567;&#20989;&#25968;&#20540;&#65292;&#24182;&#23637;&#31034;&#20102;&#20197;&#38543;&#26426;&#39034;&#24207;&#26356;&#26032;&#36825;&#20123;&#22359;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#26356;&#26126;&#26174;&#30452;&#35266;&#22320;&#23637;&#31034;&#30001;&#25105;&#20204;&#31639;&#27861;&#29983;&#25104;&#30340;&#24207;&#21015;&#30340;&#23548;&#25968;&#38598;&#26159;&#20851;&#38190;&#28857;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an accelerated block proximal linear framework with adaptive momentum (ABPL$^+$) for nonconvex and nonsmooth optimization. We analyze the potential causes of the extrapolation step failing in some algorithms, and resolve this issue by enhancing the comparison process that evaluates the trade-off between the proximal gradient step and the linear extrapolation step in our algorithm. Furthermore, we extends our algorithm to any scenario involving updating block variables with positive integers, allowing each cycle to randomly shuffle the update order of the variable blocks. Additionally, under mild assumptions, we prove that ABPL$^+$ can monotonically decrease the function value without strictly restricting the extrapolation parameters and step size, demonstrates the viability and effectiveness of updating these blocks in a random order, and we also more obviously and intuitively demonstrate that the derivative set of the sequence generated by our algorithm is a critical point 
&lt;/p&gt;</description></item><item><title>CSVTO&#26159;&#19968;&#31181;&#21463;&#38480;&#26031;&#22374;&#21464;&#20998;&#36712;&#36857;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#29983;&#25104;&#22810;&#26679;&#30340;&#32422;&#26463;&#28385;&#36275;&#36712;&#36857;&#38598;&#21512;&#65292;&#25552;&#39640;&#20102;&#22312;&#20855;&#26377;&#20219;&#24847;&#32422;&#26463;&#30340;&#38382;&#39064;&#20013;&#30340;&#20248;&#21270;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12110</link><description>&lt;p&gt;
&#21463;&#38480;&#26031;&#22374;&#21464;&#20998;&#36712;&#36857;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Constrained Stein Variational Trajectory Optimization. (arXiv:2308.12110v1 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12110
&lt;/p&gt;
&lt;p&gt;
CSVTO&#26159;&#19968;&#31181;&#21463;&#38480;&#26031;&#22374;&#21464;&#20998;&#36712;&#36857;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#29983;&#25104;&#22810;&#26679;&#30340;&#32422;&#26463;&#28385;&#36275;&#36712;&#36857;&#38598;&#21512;&#65292;&#25552;&#39640;&#20102;&#22312;&#20855;&#26377;&#20219;&#24847;&#32422;&#26463;&#30340;&#38382;&#39064;&#20013;&#30340;&#20248;&#21270;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#38480;&#26031;&#22374;&#21464;&#20998;&#36712;&#36857;&#20248;&#21270;&#65288;CSVTO&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#19968;&#32452;&#36712;&#36857;&#19978;&#36827;&#34892;&#24102;&#32422;&#26463;&#30340;&#36712;&#36857;&#20248;&#21270;&#12290;&#25105;&#20204;&#23558;&#21463;&#38480;&#36712;&#36857;&#20248;&#21270;&#35270;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#36712;&#36857;&#20998;&#24067;&#32422;&#26463;&#30340;&#20989;&#25968;&#26368;&#23567;&#21270;&#24418;&#24335;&#65292;&#36991;&#20813;&#23558;&#32422;&#26463;&#35270;&#20026;&#30446;&#26631;&#20989;&#25968;&#30340;&#24809;&#32602;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#30340;&#28385;&#36275;&#32422;&#26463;&#30340;&#36712;&#36857;&#38598;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65288;SVGD&#65289;&#23547;&#25214;&#19968;&#32452;&#31890;&#23376;&#65292;&#36817;&#20284;&#34920;&#31034;&#19968;&#20010;&#20302;&#25104;&#26412;&#36712;&#36857;&#30340;&#20998;&#24067;&#65292;&#24182;&#36981;&#23432;&#32422;&#26463;&#12290;CSVTO&#36866;&#29992;&#20110;&#20855;&#26377;&#20219;&#24847;&#31561;&#24335;&#21644;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#38382;&#39064;&#65292;&#24182;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#31890;&#23376;&#37325;&#26032;&#37319;&#26679;&#27493;&#39588;&#26469;&#36991;&#20813;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#36890;&#36807;&#26126;&#30830;&#29983;&#25104;&#22810;&#26679;&#30340;&#36712;&#36857;&#38598;&#21512;&#65292;CSVTO&#33021;&#22815;&#26356;&#22909;&#22320;&#36991;&#20813;&#19981;&#33391;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#24182;&#19988;&#23545;&#21021;&#22987;&#21270;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;CSVTO&#22312;&#20855;&#26377;&#39640;&#24230;&#32422;&#26463;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Constrained Stein Variational Trajectory Optimization (CSVTO), an algorithm for performing trajectory optimization with constraints on a set of trajectories in parallel. We frame constrained trajectory optimization as a novel form of constrained functional minimization over trajectory distributions, which avoids treating the constraints as a penalty in the objective and allows us to generate diverse sets of constraint-satisfying trajectories. Our method uses Stein Variational Gradient Descent (SVGD) to find a set of particles that approximates a distribution over low-cost trajectories while obeying constraints. CSVTO is applicable to problems with arbitrary equality and inequality constraints and includes a novel particle resampling step to escape local minima. By explicitly generating diverse sets of trajectories, CSVTO is better able to avoid poor local minima and is more robust to initialization. We demonstrate that CSVTO outperforms baselines in challenging highly-constr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#20197;&#35299;&#20915;DNNs&#20013;&#31232;&#30095;&#24615;&#21644;&#25968;&#20540;&#25928;&#29575;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2308.12044</link><description>&lt;p&gt;
&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A multiobjective continuation method to compute the regularization path of deep neural networks. (arXiv:2308.12044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#20197;&#35299;&#20915;DNNs&#20013;&#31232;&#30095;&#24615;&#21644;&#25968;&#20540;&#25928;&#29575;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#24615;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#20013;&#38750;&#24120;&#29702;&#24819;&#30340;&#29305;&#24449;&#65292;&#22240;&#20026;&#23427;&#30830;&#20445;&#20102;&#25968;&#20540;&#25928;&#29575;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;(&#30001;&#20110;&#30456;&#20851;&#29305;&#24449;&#30340;&#25968;&#37327;&#36739;&#23569;)&#21644;&#40065;&#26834;&#24615;&#12290;&#22312;&#22522;&#20110;&#32447;&#24615;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#20247;&#25152;&#21608;&#30693;&#22312;$\ell^1$&#33539;&#25968;(&#21363;&#38646;&#26435;&#37325;)&#30340;&#26368;&#31232;&#30095;&#35299;&#21644;&#38750;&#27491;&#21017;&#21270;&#35299;&#20043;&#38388;&#23384;&#22312;&#19968;&#26465;&#36830;&#25509;&#36335;&#24452;&#65292;&#36825;&#26465;&#36335;&#24452;&#34987;&#31216;&#20026;&#27491;&#21017;&#21270;&#36335;&#24452;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#23558;&#32463;&#39564;&#25439;&#22833;&#21644;&#31232;&#30095;&#24615;($\ell^1$&#33539;&#25968;)&#20316;&#20026;&#20004;&#20010;&#20914;&#31361;&#30340;&#26631;&#20934;&#65292;&#24182;&#35299;&#20915;&#30001;&#27492;&#20135;&#29983;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#39318;&#27425;&#23581;&#35797;&#23558;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;DNNs&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;$\ell^1$&#33539;&#25968;&#30340;&#19981;&#20809;&#28369;&#24615;&#21644;&#21442;&#25968;&#25968;&#37327;&#30340;&#39640;&#24230;&#65292;&#20174;&#35745;&#31639;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#26159;&#24456;&#26377;&#25928;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#35745;&#31639;&#25972;&#20010;&#24085;&#32047;&#25176;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39044;&#31639;&#30340;&#38543;&#26426;&#20108;&#33218;&#36172;&#21338;&#26426;&#20013;&#36827;&#34892;&#26368;&#20339;&#33218;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#19981;&#23384;&#22312;&#27604;&#31561;&#27010;&#29575;&#37319;&#26679;&#31639;&#27861;&#26356;&#22909;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#33268;&#31283;&#23450;&#31639;&#27861;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#19982;&#31561;&#27010;&#29575;&#37319;&#26679;&#31639;&#27861;&#34920;&#29616;&#19968;&#26679;&#22909;&#30340;&#31639;&#27861;&#24517;&#39035;&#23646;&#20110;&#36825;&#20010;&#31867;&#21035;&#12290;&#36825;&#19968;&#32467;&#26524;&#35299;&#20915;&#20102;&#20043;&#21069;&#30340;&#20004;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;</title><link>http://arxiv.org/abs/2308.12000</link><description>&lt;p&gt;
&#26377;&#20851;&#22312;&#26377;&#38480;&#39044;&#31639;&#20108;&#33218;&#36172;&#21338;&#26426;&#20013;&#36827;&#34892;&#26368;&#20339;&#33218;&#36873;&#25321;&#30340;&#32479;&#19968;&#26368;&#20248;&#31639;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget. (arXiv:2308.12000v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39044;&#31639;&#30340;&#38543;&#26426;&#20108;&#33218;&#36172;&#21338;&#26426;&#20013;&#36827;&#34892;&#26368;&#20339;&#33218;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#19981;&#23384;&#22312;&#27604;&#31561;&#27010;&#29575;&#37319;&#26679;&#31639;&#27861;&#26356;&#22909;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#33268;&#31283;&#23450;&#31639;&#27861;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#19982;&#31561;&#27010;&#29575;&#37319;&#26679;&#31639;&#27861;&#34920;&#29616;&#19968;&#26679;&#22909;&#30340;&#31639;&#27861;&#24517;&#39035;&#23646;&#20110;&#36825;&#20010;&#31867;&#21035;&#12290;&#36825;&#19968;&#32467;&#26524;&#35299;&#20915;&#20102;&#20043;&#21069;&#30340;&#20004;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#20271;&#21162;&#21033;&#22870;&#21169;&#30340;&#38543;&#26426;&#20108;&#33218;&#36172;&#21338;&#26426;&#20013;&#65292;&#20351;&#29992;&#26377;&#38480;&#39044;&#31639;&#36827;&#34892;&#26368;&#20339;&#33218;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#19981;&#23384;&#22312;&#19968;&#20010;&#31639;&#27861;&#21487;&#20197;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#19982;&#31561;&#27010;&#29575;&#37319;&#26679;&#31639;&#27861;&#34920;&#29616;&#19968;&#26679;&#22909;&#65288;&#35813;&#31639;&#27861;&#34987;&#31216;&#20026;&#8220;&#22343;&#21248;&#37319;&#26679;&#8221;&#31639;&#27861;&#65289;&#65292;&#24182;&#19988;&#22312;&#33267;&#23569;&#19968;&#20010;&#24773;&#20917;&#19979;&#26126;&#26174;&#20248;&#20110;&#35813;&#31639;&#27861;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#19981;&#23384;&#22312;&#27604;&#22343;&#21248;&#37319;&#26679;&#31639;&#27861;&#26356;&#22909;&#30340;&#31639;&#27861;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#19968;&#33268;&#8221;&#21644;&#8220;&#31283;&#23450;&#8221;&#31639;&#27861;&#30340;&#33258;&#28982;&#31867;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#20219;&#20309;&#31639;&#27861;&#35201;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#19982;&#22343;&#21248;&#37319;&#26679;&#31639;&#27861;&#34920;&#29616;&#19968;&#26679;&#22909;&#65292;&#24517;&#39035;&#23646;&#20110;&#36825;&#20010;&#31867;&#21035;&#12290;&#36890;&#36807;&#23548;&#20986;&#28385;&#36275;&#20219;&#20309;&#19968;&#33268;&#19988;&#31283;&#23450;&#31639;&#27861;&#30340;&#38169;&#35823;&#29575;&#30340;&#19979;&#30028;&#65292;&#24182;&#35777;&#26126;&#22343;&#21248;&#37319;&#26679;&#31639;&#27861;&#19982;&#27492;&#19979;&#30028;&#30456;&#21305;&#37197;&#65292;&#25105;&#20204;&#23436;&#25104;&#20102;&#35777;&#26126;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35299;&#20915;&#20102;\cite{qin2022open}&#20013;&#25552;&#20986;&#30340;&#20004;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of best-arm identification with fixed budget in stochastic two-arm bandits with Bernoulli rewards. We prove that surprisingly, there is no algorithm that (i) performs as well as the algorithm sampling each arm equally (this algorithm is referred to as the {\it uniform sampling} algorithm) on all instances, and that (ii) strictly outperforms this algorithm on at least one instance. In short, there is no algorithm better than the uniform sampling algorithm. Towards this result, we introduce the natural class of {\it consistent} and {\it stable} algorithms, and show that any algorithm that performs as well as the uniform sampling algorithm on all instances belongs to this class. The proof is completed by deriving a lower bound on the error rate satisfied by any consistent and stable algorithm, and by showing that the uniform sampling algorithm matches this lower bound. Our results provide a solution to the two open problems presented in \cite{qin2022open}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#39304;&#25511;&#21046;&#30340;&#26032;&#22411;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21453;&#39304;&#25511;&#21046;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#22686;&#24378;DNN&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11881</link><description>&lt;p&gt;
&#20351;&#29992;&#21453;&#39304;&#24490;&#29615;&#30340;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Adversarial Training Using Feedback Loops. (arXiv:2308.11881v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#39304;&#25511;&#21046;&#30340;&#26032;&#22411;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21453;&#39304;&#25511;&#21046;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#22686;&#24378;DNN&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30001;&#20110;&#33021;&#22815;&#20934;&#30830;&#22320;&#23398;&#20064;&#38750;&#24120;&#22797;&#26434;&#30340;&#36755;&#20837;-&#36755;&#20986;&#20851;&#31995;&#32780;&#22312;&#35768;&#22810;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;DNN&#20934;&#30830;&#24615;&#39640;&#19988;&#20351;&#29992;&#24191;&#27867;&#65292;&#20294;&#30001;&#20110;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#23427;&#20204;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#38750;&#24120;&#25935;&#24863;&#12290;&#20026;&#20102;&#26410;&#26469;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#36827;&#23637;&#65292;&#26500;&#24314;&#23545;&#20219;&#20309;&#25968;&#25454;&#28857;&#30340;&#25200;&#21160;&#37117;&#20855;&#26377;&#25269;&#25239;&#33021;&#21147;&#30340;DNN&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36807;&#21435;&#25552;&#20986;&#20102;&#35768;&#22810;&#20351;&#29992;&#32593;&#32476;&#30340;&#19968;&#38454;&#23548;&#25968;&#20449;&#24687;&#26469;&#22686;&#24378;DNN&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#35770;&#30340;&#26032;&#22411;&#22686;&#24378;&#26041;&#27861;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21453;&#39304;&#25511;&#21046;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;&#21453;&#39304;&#31070;&#32463;&#32593;&#32476;&#12290;&#25511;&#21046;&#22120;&#26412;&#36523;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#24120;&#35268;&#21644;&#23545;&#25239;&#24615;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#31283;&#23450;&#31995;&#32479;&#30340;&#36755;&#20986;&#12290;&#22522;&#20110;&#21453;&#39304;&#25511;&#21046;&#26550;&#26500;&#30340;&#26032;&#22411;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#31216;&#20026;&#21453;&#39304;&#24490;&#29615;&#23545;&#25239;&#35757;&#32451;(FLAT)&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNN) have found wide applicability in numerous fields due to their ability to accurately learn very complex input-output relations. Despite their accuracy and extensive use, DNNs are highly susceptible to adversarial attacks due to limited generalizability. For future progress in the field, it is essential to build DNNs that are robust to any kind of perturbations to the data points. In the past, many techniques have been proposed to robustify DNNs using first-order derivative information of the network.  This paper proposes a new robustification approach based on control theory. A neural network architecture that incorporates feedback control, named Feedback Neural Networks, is proposed. The controller is itself a neural network, which is trained using regular and adversarial data such as to stabilize the system outputs. The novel adversarial training approach based on the feedback control architecture is called Feedback Looped Adversarial Training (FLAT). Numeri
&lt;/p&gt;</description></item><item><title>HypBO&#26159;&#19968;&#31181;&#21033;&#29992;&#19987;&#23478;&#20154;&#31867;&#30693;&#35782;&#24341;&#23548;&#36125;&#21494;&#26031;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#25913;&#36827;&#30340;&#26679;&#26412;&#31181;&#23376;&#26469;&#26356;&#24555;&#22320;&#25214;&#21040;&#26377;&#24076;&#26395;&#30340;&#21270;&#23398;&#31354;&#38388;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.11787</link><description>&lt;p&gt;
HypBO: &#19987;&#23478;&#24341;&#23548;&#19979;&#30340;&#21270;&#23398;&#23478;&#21442;&#19982;&#30340;&#36125;&#21494;&#26031;&#25628;&#32034;&#26032;&#26448;&#26009;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
HypBO: Expert-Guided Chemist-in-the-Loop Bayesian Search for New Materials. (arXiv:2308.11787v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11787
&lt;/p&gt;
&lt;p&gt;
HypBO&#26159;&#19968;&#31181;&#21033;&#29992;&#19987;&#23478;&#20154;&#31867;&#30693;&#35782;&#24341;&#23548;&#36125;&#21494;&#26031;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#25913;&#36827;&#30340;&#26679;&#26412;&#31181;&#23376;&#26469;&#26356;&#24555;&#22320;&#25214;&#21040;&#26377;&#24076;&#26395;&#30340;&#21270;&#23398;&#31354;&#38388;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#21270;&#21487;&#20197;&#22823;&#22823;&#21152;&#36895;&#35299;&#20915;&#26448;&#26009;&#21457;&#29616;&#31561;&#38590;&#20197;&#35299;&#20915;&#30340;&#22810;&#21464;&#37327;&#31185;&#23398;&#38382;&#39064;&#65292;&#20294;&#21487;&#29992;&#30340;&#25628;&#32034;&#31354;&#38388;&#21487;&#33021;&#38750;&#24120;&#24222;&#22823;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#21463;&#27426;&#36814;&#30340;&#26679;&#26412;&#39640;&#25928;&#20248;&#21270;&#24341;&#25806;&#65292;&#22312;&#27809;&#26377;&#30446;&#26631;&#20989;&#25968;&#25110;&#23646;&#24615;&#30340;&#35299;&#26512;&#24418;&#24335;&#34987;&#30693;&#36947;&#30340;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#25104;&#21151;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21033;&#29992;&#19987;&#23478;&#20154;&#31867;&#30693;&#35782;&#20197;&#20551;&#35774;&#30340;&#24418;&#24335;&#65292;&#26356;&#24555;&#22320;&#23558;&#36125;&#21494;&#26031;&#25628;&#32034;&#24341;&#23548;&#21040;&#26377;&#24076;&#26395;&#30340;&#21270;&#23398;&#31354;&#38388;&#21306;&#22495;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#20174;&#29616;&#26377;&#23454;&#39564;&#27979;&#37327;&#24471;&#21040;&#30340;&#28508;&#22312;&#20998;&#24067;&#65292;&#36825;&#23545;&#20110;&#26032;&#30340;&#26410;&#24320;&#21457;&#30340;&#31185;&#23398;&#20219;&#21153;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#36825;&#26679;&#30340;&#20998;&#24067;&#26080;&#27861;&#25429;&#25417;&#31934;&#32454;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;HypBO&#65292;&#21033;&#29992;&#19987;&#23478;&#20154;&#31867;&#20551;&#35774;&#29983;&#25104;&#25913;&#36827;&#30340;&#26679;&#26412;&#31181;&#23376;&#12290;&#19981;&#22826;&#26377;&#24076;&#26395;&#30340;&#31181;&#23376;&#33258;&#21160;&#25240;&#25187;&#65292;&#32780;&#26377;&#24076;&#26395;&#30340;&#31181;&#23376;&#29992;&#20110;&#22686;&#21152;&#20195;&#29702;&#27169;&#22411;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20855;&#20449;&#24687;&#30340;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotics and automation offer massive accelerations for solving intractable, multivariate scientific problems such as materials discovery, but the available search spaces can be dauntingly large. Bayesian optimization (BO) has emerged as a popular sample-efficient optimization engine, thriving in tasks where no analytic form of the target function/property is known. Here we exploit expert human knowledge in the form of hypotheses to direct Bayesian searches more quickly to promising regions of chemical space. Previous methods have used underlying distributions derived from existing experimental measurements, which is unfeasible for new, unexplored scientific tasks. Also, such distributions cannot capture intricate hypotheses. Our proposed method, which we call HypBO, uses expert human hypotheses to generate an improved seed of samples. Unpromising seeds are automatically discounted, while promising seeds are used to augment the surrogate model data, thus achieving better-informed sampl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#21327;&#21516;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#36328;&#22330;&#26223;&#30340;&#26234;&#33021;&#26381;&#21153;&#12290;&#22312;&#22823;&#27169;&#22411;&#26102;&#20195;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#24322;&#26500;&#25968;&#25454;&#12289;&#27169;&#22411;&#32858;&#21512;&#12289;&#24615;&#33021;&#21644;&#25104;&#26412;&#26435;&#34913;&#12289;&#25968;&#25454;&#38544;&#31169;&#20197;&#21450;&#28608;&#21169;&#26426;&#21046;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.11217</link><description>&lt;p&gt;
&#22823;&#27169;&#22411;&#26102;&#20195;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#22823;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models. (arXiv:2308.11217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#21327;&#21516;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#36328;&#22330;&#26223;&#30340;&#26234;&#33021;&#26381;&#21153;&#12290;&#22312;&#22823;&#27169;&#22411;&#26102;&#20195;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#24322;&#26500;&#25968;&#25454;&#12289;&#27169;&#22411;&#32858;&#21512;&#12289;&#24615;&#33021;&#21644;&#25104;&#26412;&#26435;&#34913;&#12289;&#25968;&#25454;&#38544;&#31169;&#20197;&#21450;&#28608;&#21169;&#26426;&#21046;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25968;&#25454;&#33021;&#22815;&#20840;&#38754;&#24863;&#30693;&#21644;&#35782;&#21035;&#29289;&#29702;&#19990;&#30028;&#65292;&#24050;&#25104;&#20026;&#36890;&#24448;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#36335;&#24452;&#12290;&#28982;&#32780;&#65292;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#22823;&#27169;&#22411;&#22312;&#29305;&#23450;&#24037;&#19994;&#39046;&#22495;&#30340;&#24615;&#33021;&#24448;&#24448;&#19981;&#29702;&#24819;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#22810;&#20010;&#20225;&#19994;&#21033;&#29992;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#21327;&#21516;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#23454;&#29616;&#36328;&#22330;&#26223;&#30340;&#26234;&#33021;&#26381;&#21153;&#12290;&#20316;&#32773;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#27169;&#22411;&#26102;&#20195;&#32852;&#37030;&#23398;&#20064;&#30340;&#26234;&#33021;&#22522;&#30784;&#21644;&#30446;&#26631;&#30340;&#25112;&#30053;&#36716;&#21464;&#65292;&#20197;&#21450;&#22312;&#24322;&#26500;&#25968;&#25454;&#12289;&#27169;&#22411;&#32858;&#21512;&#12289;&#24615;&#33021;&#21644;&#25104;&#26412;&#26435;&#34913;&#12289;&#25968;&#25454;&#38544;&#31169;&#21644;&#28608;&#21169;&#26426;&#21046;&#26041;&#38754;&#38754;&#20020;&#30340;&#26032;&#25361;&#25112;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#39046;&#20808;&#20225;&#19994;&#22312;&#22478;&#24066;&#23433;&#20840;&#36816;&#33829;&#31649;&#29702;&#26041;&#38754;&#36129;&#29486;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#19987;&#23478;&#30693;&#35782;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21253;&#25324;&#20998;&#24067;&#24335;&#37096;&#32626;&#21644;&#39640;&#25928;&#24615;&#33021;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal data, which can comprehensively perceive and recognize the physical world, has become an essential path towards general artificial intelligence. However, multimodal large models trained on public datasets often underperform in specific industrial domains. This paper proposes a multimodal federated learning framework that enables multiple enterprises to utilize private domain data to collaboratively train large models for vertical domains, achieving intelligent services across scenarios. The authors discuss in-depth the strategic transformation of federated learning in terms of intelligence foundation and objectives in the era of big model, as well as the new challenges faced in heterogeneous data, model aggregation, performance and cost trade-off, data privacy, and incentive mechanism. The paper elaborates a case study of leading enterprises contributing multimodal data and expert knowledge to city safety operation management , including distributed deployment and efficient 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Wasserstein&#20960;&#20309;&#29983;&#25104;&#22120;&#23398;&#20064;&#26465;&#20214;&#20998;&#24067;&#65292;&#29983;&#25104;&#32473;&#23450;&#29305;&#23450;&#26631;&#31614;&#30340;&#26679;&#26412;&#12290;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#23398;&#20064;&#35266;&#23519;&#22495;&#30340;&#26465;&#20214;&#20998;&#24067;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#26144;&#23556;&#12290;&#22312;&#20154;&#33080;&#22270;&#20687;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.10145</link><description>&lt;p&gt;
Wasserstein&#20960;&#20309;&#29983;&#25104;&#22120;&#29992;&#20110;&#26465;&#20214;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Geodesic Generator for Conditional Distributions. (arXiv:2308.10145v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10145
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Wasserstein&#20960;&#20309;&#29983;&#25104;&#22120;&#23398;&#20064;&#26465;&#20214;&#20998;&#24067;&#65292;&#29983;&#25104;&#32473;&#23450;&#29305;&#23450;&#26631;&#31614;&#30340;&#26679;&#26412;&#12290;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#23398;&#20064;&#35266;&#23519;&#22495;&#30340;&#26465;&#20214;&#20998;&#24067;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#26144;&#23556;&#12290;&#22312;&#20154;&#33080;&#22270;&#20687;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#32473;&#23450;&#29305;&#23450;&#26631;&#31614;&#30340;&#26679;&#26412;&#38656;&#35201;&#20272;&#35745;&#26465;&#20214;&#20998;&#24067;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#26465;&#20214;&#20998;&#24067;&#20043;&#38388;Wasserstein&#36317;&#31163;&#30340;&#21487;&#22788;&#29702;&#30340;&#19978;&#30028;&#65292;&#20197;&#24314;&#31435;&#23398;&#20064;&#26465;&#20214;&#20998;&#24067;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#22522;&#20110;&#36825;&#19968;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#29983;&#25104;&#31639;&#27861;&#65292;&#20854;&#20013;&#26465;&#20214;&#20998;&#24067;&#23436;&#20840;&#30001;&#30001;&#32479;&#35745;&#36317;&#31163;&#23450;&#20041;&#30340;&#24230;&#37327;&#31354;&#38388;&#26469;&#34920;&#24449;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#26469;&#25552;&#20986;&#20102;Wasserstein&#20960;&#20309;&#29983;&#25104;&#22120;&#65292;&#19968;&#31181;&#23398;&#20064;Wasserstein&#20960;&#20309;&#30340;&#26032;&#30340;&#26465;&#20214;&#29983;&#25104;&#22120;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#20064;&#35266;&#23519;&#22495;&#30340;&#26465;&#20214;&#20998;&#24067;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#26144;&#23556;&#12290;&#32473;&#23450;&#20004;&#20010;&#35266;&#23519;&#22495;&#26631;&#31614;&#65292;&#26410;&#35266;&#23519;&#21040;&#30340;&#20013;&#38388;&#22495;&#30340;&#26465;&#20214;&#20998;&#24067;&#20301;&#20110;&#32473;&#23450;&#30340;&#26465;&#20214;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein&#20960;&#20309;&#20013;&#12290;&#22312;&#20197;&#20809;&#29031;&#26465;&#20214;&#20026;&#22495;&#26631;&#31614;&#30340;&#20154;&#33080;&#22270;&#20687;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating samples given a specific label requires estimating conditional distributions. We derive a tractable upper bound of the Wasserstein distance between conditional distributions to lay the theoretical groundwork to learn conditional distributions. Based on this result, we propose a novel conditional generation algorithm where conditional distributions are fully characterized by a metric space defined by a statistical distance. We employ optimal transport theory to propose the \textit{Wasserstein geodesic generator}, a new conditional generator that learns the Wasserstein geodesic. The proposed method learns both conditional distributions for observed domains and optimal transport maps between them. The conditional distributions given unobserved intermediate domains are on the Wasserstein geodesic between conditional distributions given two observed domain labels. Experiments on face images with light conditions as domain labels demonstrate the efficacy of the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MoCLIM&#30340;&#22810;&#32452;&#23398;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#30284;&#30151;&#20122;&#22411;&#21010;&#20998;&#20013;&#21033;&#29992;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#25311;&#21512;&#24230;&#21644;&#20122;&#22411;&#21010;&#20998;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09725</link><description>&lt;p&gt;
MoCLIM: &#29992;&#22810;&#32452;&#23398;&#23545;&#27604;&#23398;&#20064;&#21644;&#32452;&#23398;&#25512;&#29702;&#24314;&#27169;&#23454;&#29616;&#20934;&#30830;&#30340;&#30284;&#30151;&#20122;&#22411;&#21010;&#20998;
&lt;/p&gt;
&lt;p&gt;
MoCLIM: Towards Accurate Cancer Subtyping via Multi-Omics Contrastive Learning with Omics-Inference Modeling. (arXiv:2308.09725v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MoCLIM&#30340;&#22810;&#32452;&#23398;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#30284;&#30151;&#20122;&#22411;&#21010;&#20998;&#20013;&#21033;&#29992;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#25311;&#21512;&#24230;&#21644;&#20122;&#22411;&#21010;&#20998;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#20934;&#21307;&#23398;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#30284;&#30151;&#20122;&#22411;&#30340;&#29983;&#21270;&#26426;&#21046;&#19982;&#30142;&#30149;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#22522;&#20110;&#32452;&#23398;&#30340;&#30284;&#30151;&#20122;&#22411;&#21010;&#20998;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#19981;&#21516;&#32423;&#21035;&#30340;&#32452;&#23398;&#35760;&#24405;&#20102;&#30284;&#30151;&#20013;&#22810;&#27493;&#39588;&#36807;&#31243;&#30340;&#29983;&#21270;&#20135;&#29289;&#12290;&#26412;&#25991;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#28508;&#21147;&#26469;&#25913;&#21892;&#30284;&#30151;&#20122;&#22411;&#21010;&#20998;&#32467;&#26524;&#65292;&#22240;&#27492;&#24320;&#21457;&#20102;MoCLIM&#65292;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;MoCLIM&#29420;&#31435;&#22320;&#20174;&#19981;&#21516;&#30340;&#32452;&#23398;&#27169;&#24335;&#20013;&#25552;&#21462;&#26377;&#20449;&#24687;&#37327;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#32452;&#23398;&#27169;&#24335;&#20043;&#38388;&#30340;&#23545;&#27604;&#23398;&#20064;&#25152;&#24471;&#21040;&#30340;&#32479;&#19968;&#34920;&#31034;&#65292;&#22312;&#32473;&#23450;&#30284;&#30151;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#20122;&#22411;&#24456;&#22909;&#22320;&#32858;&#31867;&#21040;&#36739;&#20302;&#30340;&#28508;&#31354;&#38388;&#20013;&#12290;&#36825;&#31181;&#23545;&#27604;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#22312;&#29983;&#29289;&#32593;&#32476;&#20013;&#35266;&#23519;&#21040;&#30340;&#32452;&#38469;&#25512;&#29702;&#30340;&#25237;&#24433;&#12290;&#22312;&#20845;&#20010;&#30284;&#30151;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36739;&#23569;&#30340;&#39640;&#32500;&#30284;&#30151;&#25968;&#25454;&#25311;&#21512;&#21644;&#20122;&#22411;&#21010;&#20998;&#24615;&#33021;&#26041;&#38754;&#26174;&#33879;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precision medicine fundamentally aims to establish causality between dysregulated biochemical mechanisms and cancer subtypes. Omics-based cancer subtyping has emerged as a revolutionary approach, as different level of omics records the biochemical products of multistep processes in cancers. This paper focuses on fully exploiting the potential of multi-omics data to improve cancer subtyping outcomes, and hence developed MoCLIM, a representation learning framework. MoCLIM independently extracts the informative features from distinct omics modalities. Using a unified representation informed by contrastive learning of different omics modalities, we can well-cluster the subtypes, given cancer, into a lower latent space. This contrast can be interpreted as a projection of inter-omics inference observed in biological networks. Experimental results on six cancer datasets demonstrate that our approach significantly improves data fit and subtyping performance in fewer high-dimensional cancer ins
&lt;/p&gt;</description></item><item><title>&#22810;&#20445;&#30495;&#24230;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#22320;&#36136;&#30899;&#20648;&#23384;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#32463;&#27982;&#24615;&#26356;&#39640;&#30340;&#22810;&#20445;&#30495;&#24230;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#20197;&#19982;&#39640;&#20445;&#30495;&#24230;&#27169;&#22411;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.09113</link><description>&lt;p&gt;
&#22810;&#20445;&#30495;&#24230;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#29992;&#20110;&#24555;&#36895;&#24314;&#27169;&#22823;&#35268;&#27169;&#22320;&#36136;&#30899;&#20648;&#23384;
&lt;/p&gt;
&lt;p&gt;
Multi-fidelity Fourier Neural Operator for Fast Modeling of Large-Scale Geological Carbon Storage. (arXiv:2308.09113v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09113
&lt;/p&gt;
&lt;p&gt;
&#22810;&#20445;&#30495;&#24230;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#22320;&#36136;&#30899;&#20648;&#23384;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#32463;&#27982;&#24615;&#26356;&#39640;&#30340;&#22810;&#20445;&#30495;&#24230;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#20197;&#19982;&#39640;&#20445;&#30495;&#24230;&#27169;&#22411;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#20195;&#29702;&#27169;&#22411;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#22320;&#36136;&#30899;&#20648;&#23384;&#65288;GCS&#65289;&#38382;&#39064;&#65292;&#20197;&#21152;&#24555;&#39044;&#27979;&#20648;&#21387;&#21644;&#20108;&#27687;&#21270;&#30899;&#20113;&#23618;&#31227;&#21160;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#22823;&#35268;&#27169;&#19977;&#32500;&#38382;&#39064;&#30340;&#21487;&#29992;&#35757;&#32451;&#25968;&#25454;&#22987;&#32456;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22810;&#20445;&#30495;&#24230;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;GCS&#38382;&#39064;&#65292;&#21033;&#29992;&#26356;&#20855;&#32463;&#27982;&#24615;&#30340;&#22810;&#20445;&#30495;&#24230;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#20855;&#26377;&#33391;&#22909;&#30340;&#32593;&#26684;&#19981;&#21464;&#24615;&#65292;&#31616;&#21270;&#20102;&#19981;&#21516;&#31163;&#25955;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#36801;&#31227;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19968;&#20010;GCS&#20648;&#23618;&#27169;&#22411;&#19978;&#36827;&#34892;&#27169;&#22411;&#26377;&#25928;&#24615;&#27979;&#35797;&#65292;&#35813;&#27169;&#22411;&#34987;&#21010;&#20998;&#20026;110,000&#20010;&#32593;&#26684;&#21333;&#20803;&#12290;&#22810;&#20445;&#30495;&#24230;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#21487;&#19982;&#39640;&#20445;&#30495;&#24230;&#27169;&#22411;&#30340;&#35757;&#32451;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based surrogate models have been widely applied in geological carbon storage (GCS) problems to accelerate the prediction of reservoir pressure and CO2 plume migration. Large amounts of data from physics-based numerical simulators are required to train a model to accurately predict the complex physical behaviors associated with this process. In practice, the available training data are always limited in large-scale 3D problems due to the high computational cost. Therefore, we propose to use a multi-fidelity Fourier Neural Operator to solve large-scale GCS problems with more affordable multi-fidelity training datasets. The Fourier Neural Operator has a desirable grid-invariant property, which simplifies the transfer learning procedure between datasets with different discretization. We first test the model efficacy on a GCS reservoir model being discretized into 110k grid cells. The multi-fidelity model can predict with accuracy comparable to a high-fidelity model trained wi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22312;&#32858;&#31867;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#38598;&#21512;&#20154;&#33080;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#24863;&#20852;&#36259;&#30340;&#20010;&#20307;&#65292;&#21516;&#26102;&#26377;&#25928;&#22788;&#29702;&#38476;&#29983;&#30340;&#38754;&#23380;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21363;&#20351;&#22312;&#22823;&#35268;&#27169;&#22270;&#24211;&#20013;&#20063;&#33021;&#21462;&#24471;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07445</link><description>&lt;p&gt;
&#20351;&#29992;&#22312;&#32858;&#31867;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#38598;&#21512;&#20154;&#33080;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Open-set Face Recognition using Ensembles trained on Clustered Data. (arXiv:2308.07445v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22312;&#32858;&#31867;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#38598;&#21512;&#20154;&#33080;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#24863;&#20852;&#36259;&#30340;&#20010;&#20307;&#65292;&#21516;&#26102;&#26377;&#25928;&#22788;&#29702;&#38476;&#29983;&#30340;&#38754;&#23380;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21363;&#20351;&#22312;&#22823;&#35268;&#27169;&#22270;&#24211;&#20013;&#20063;&#33021;&#21462;&#24471;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#38598;&#21512;&#20154;&#33080;&#35782;&#21035;&#25551;&#36848;&#20102;&#22312;&#27979;&#35797;&#26102;&#20986;&#29616;&#26410;&#30693;&#30340;&#20027;&#39064;&#65292;&#32780;&#22312;&#35757;&#32451;&#38454;&#27573;&#26410;&#35265;&#36807;&#12290;&#23427;&#19981;&#20165;&#38656;&#35201;&#20934;&#30830;&#35782;&#21035;&#24863;&#20852;&#36259;&#30340;&#20010;&#20307;&#30340;&#26041;&#27861;&#65292;&#36824;&#38656;&#35201;&#26377;&#25928;&#22788;&#29702;&#38476;&#29983;&#30340;&#38754;&#23380;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#24320;&#25918;&#38598;&#21512;&#20154;&#33080;&#35782;&#21035;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#21253;&#21547;&#25968;&#30334;&#21644;&#25968;&#21315;&#20010;&#20027;&#39064;&#30340;&#22270;&#24211;&#12290;&#23427;&#30001;&#32858;&#31867;&#21644;&#19968;&#32452;&#20108;&#36827;&#21046;&#23398;&#20064;&#31639;&#27861;&#32452;&#25104;&#65292;&#29992;&#20110;&#20272;&#35745;&#26597;&#35810;&#20154;&#33080;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#20154;&#33080;&#22270;&#24211;&#65292;&#24182;&#26816;&#32034;&#20854;&#27491;&#30830;&#30340;&#36523;&#20221;&#12290;&#35813;&#26041;&#27861;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#22270;&#24211;&#20027;&#39064;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#27169;&#22411;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#30693;&#21517;&#30340;LFW&#21644;YTF&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#38024;&#23545;&#21487;&#25193;&#23637;&#24615;&#65292;&#20063;&#21487;&#20197;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-set face recognition describes a scenario where unknown subjects, unseen during the training stage, appear on test time. Not only it requires methods that accurately identify individuals of interest, but also demands approaches that effectively deal with unfamiliar faces. This work details a scalable open-set face identification approach to galleries composed of hundreds and thousands of subjects. It is composed of clustering and an ensemble of binary learning algorithms that estimates when query face samples belong to the face gallery and then retrieves their correct identity. The approach selects the most suitable gallery subjects and uses the ensemble to improve prediction performance. We carry out experiments on well-known LFW and YTF benchmarks. Results show that competitive performance can be achieved even when targeting scalability.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.07134</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#26159;&#22270;&#34920;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22914;ChatGPT&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36880;&#28176;&#21462;&#20195;&#20102;CNN&#21644;RNN&#65292;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#32479;&#19968;&#36215;&#26469;&#12290;&#19982;&#30456;&#23545;&#29420;&#31435;&#23384;&#22312;&#30340;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#25991;&#26412;&#65289;&#30456;&#27604;&#65292;&#22270;&#34920;&#26159;&#19968;&#31181;&#21253;&#21547;&#20016;&#23500;&#32467;&#26500;&#21644;&#20851;&#31995;&#20449;&#24687;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#21516;&#26102;&#65292;&#20316;&#20026;&#26368;&#20855;&#34920;&#29616;&#21147;&#30340;&#23186;&#20171;&#20043;&#19968;&#65292;&#33258;&#28982;&#35821;&#35328;&#22312;&#25551;&#36848;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23558;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#32435;&#20837;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#30340;&#29616;&#26377;&#24037;&#20316;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#38271;&#65292;&#25506;&#32034;LLMs&#26159;&#21542;&#20063;&#21487;&#20197;&#26367;&#20195;GNNs&#25104;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructGLM&#65288;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#65289;&#31639;&#27861;&#65292;&#31995;&#32479;&#22320;&#35774;&#35745;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#27604;&#36739;&#20102;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06534</link><description>&lt;p&gt;
&#35299;&#20915;&#21307;&#23398;&#24433;&#20687;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#23567;&#22411;&#27880;&#37322;&#25968;&#25454;&#38598;&#38382;&#39064;&#65306;&#23545;&#27604;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dealing with Small Annotated Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models. (arXiv:2308.06534v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#27604;&#36739;&#20102;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#26377;&#28508;&#21147;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#30340;&#39118;&#38505;&#12289;&#20943;&#36731;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#36127;&#25285;&#24182;&#21152;&#36895;&#30830;&#35786;&#12290;&#35757;&#32451;&#36825;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#22411;&#19988;&#20934;&#30830;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#38656;&#35201;&#20026;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#25552;&#20379;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#65292;&#30001;&#20110;&#27880;&#37322;&#30340;&#39640;&#22797;&#26434;&#24615;&#12289;&#21463;&#38480;&#30340;&#33719;&#21462;&#26041;&#24335;&#25110;&#30142;&#30149;&#30340;&#32597;&#35265;&#24615;&#65292;&#29305;&#23450;&#20219;&#21153;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#36890;&#24120;&#24456;&#23567;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#27880;&#37322;&#30340;&#22823;&#22411;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;&#23567;&#22411;&#30340;&#24050;&#27880;&#37322;&#25968;&#25454;&#38598;&#23601;&#36275;&#20197;&#23545;&#27169;&#22411;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#21363;&#25152;&#35859;&#30340;&#8220;&#19979;&#28216;&#20219;&#21153;&#8221;&#12290;&#21307;&#23398;&#24433;&#20687;&#20013;&#26368;&#27969;&#34892;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#22522;&#20110;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#33258;&#28982;&#22270;&#20687;&#22788;&#29702;&#30740;&#31350;&#34920;&#26126;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20108;&#32773;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning in medical imaging has the potential to minimize the risk of diagnostic errors, reduce radiologist workload, and accelerate diagnosis. Training such deep learning models requires large and accurate datasets, with annotations for all training samples. However, in the medical imaging domain, annotated datasets for specific tasks are often small due to the high complexity of annotations, limited access, or the rarity of diseases. To address this challenge, deep learning models can be pre-trained on large image datasets without annotations using methods from the field of self-supervised learning. After pre-training, small annotated datasets are sufficient to fine-tune the models for a specific task, the so-called ``downstream task". The most popular self-supervised pre-training approaches in medical imaging are based on contrastive learning. However, recent studies in natural image processing indicate a strong potential for masked autoencoder approaches. Our work compares sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Pareto Invariant Representation Learning&#65288;PaInvRL&#65289;&#30340;&#26694;&#26550;&#65292;&#24212;&#29992;&#20110;&#22810;&#23186;&#20307;&#25512;&#33616;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#21644;&#21464;&#20307;&#34920;&#31034;&#30340;&#21516;&#26102;&#26469;&#32531;&#35299;&#36890;&#29992;&#34920;&#31034;&#24341;&#20837;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#38382;&#39064;&#12290;&#20174;IID-OOD&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#35282;&#24230;&#65292;PaInvRL&#20943;&#23569;&#20102;&#38169;&#35823;&#30456;&#20851;&#24615;&#23545;&#29992;&#25143;&#20559;&#22909;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.04706</link><description>&lt;p&gt;
Pareto&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#22312;&#22810;&#23186;&#20307;&#25512;&#33616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Pareto Invariant Representation Learning for Multimedia Recommendation. (arXiv:2308.04706v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Pareto Invariant Representation Learning&#65288;PaInvRL&#65289;&#30340;&#26694;&#26550;&#65292;&#24212;&#29992;&#20110;&#22810;&#23186;&#20307;&#25512;&#33616;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#21644;&#21464;&#20307;&#34920;&#31034;&#30340;&#21516;&#26102;&#26469;&#32531;&#35299;&#36890;&#29992;&#34920;&#31034;&#24341;&#20837;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#38382;&#39064;&#12290;&#20174;IID-OOD&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#35282;&#24230;&#65292;PaInvRL&#20943;&#23569;&#20102;&#38169;&#35823;&#30456;&#20851;&#24615;&#23545;&#29992;&#25143;&#20559;&#22909;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23186;&#20307;&#25512;&#33616;&#28041;&#21450;&#20010;&#24615;&#21270;&#25490;&#24207;&#20219;&#21153;&#65292;&#36890;&#24120;&#20351;&#29992;&#36890;&#29992;&#32534;&#30721;&#22120;&#34920;&#31034;&#22810;&#23186;&#20307;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36890;&#29992;&#34920;&#31034;&#24341;&#20837;&#20102;&#38169;&#35823;&#30340;&#30456;&#20851;&#24615;&#65292;&#26080;&#27861;&#25581;&#31034;&#29992;&#25143;&#30340;&#30495;&#23454;&#20559;&#22909;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#23581;&#35797;&#36890;&#36807;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#24573;&#35270;&#20102;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;IID&#65289;&#21644;&#38750;&#20998;&#24067;&#65288;OOD&#65289;&#24191;&#20041;&#21270;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Pareto Invariant Representation Learning&#65288;PaInvRL&#65289;&#30340;&#26694;&#26550;&#65292;&#20174;IID-OOD&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#35282;&#24230;&#20943;&#23569;&#20102;&#38169;&#35823;&#30456;&#20851;&#24615;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#65288;&#21560;&#24341;&#29992;&#25143;&#27880;&#24847;&#30340;&#20869;&#22312;&#22240;&#32032;&#65289;&#21644;&#21464;&#20307;&#34920;&#31034;&#65288;&#20854;&#20182;&#22240;&#32032;&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PaInvRL&#21253;&#25324;&#19977;&#20010;&#36845;&#20195;&#25191;&#34892;&#30340;&#27169;&#22359;&#65306;&#65288;i&#65289;&#38750;&#21516;&#36136;&#35782;&#21035;&#27169;&#22359;&#65292;&#29992;&#20110;&#35782;&#21035;&#21453;&#26144;&#20998;&#24067;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Multimedia recommendation involves personalized ranking tasks, where multimedia content is usually represented using a generic encoder. However, these generic representations introduce spurious correlations that fail to reveal users' true preferences. Existing works attempt to alleviate this problem by learning invariant representations, but overlook the balance between independent and identically distributed (IID) and out-of-distribution (OOD) generalization. In this paper, we propose a framework called Pareto Invariant Representation Learning (PaInvRL) to mitigate the impact of spurious correlations from an IID-OOD multi-objective optimization perspective, by learning invariant representations (intrinsic factors that attract user attention) and variant representations (other factors) simultaneously. Specifically, PaInvRL includes three iteratively executed modules: (i) heterogeneous identification module, which identifies the heterogeneous environments to reflect distributional shift
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#20102;min-max&#20248;&#21270;&#22312;&#24310;&#36831;&#19979;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#31616;&#21333;&#23454;&#20363;&#65292;&#21363;&#20351;&#26159;&#23567;&#30340;&#24310;&#36831;&#20063;&#21487;&#33021;&#23548;&#33268;Extra-gradient&#31639;&#27861;&#21457;&#25955;&#65292;&#22240;&#27492;&#38656;&#35201;&#23545;&#24310;&#36831;&#29256;&#26412;&#30340;min-max&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20180;&#32454;&#20998;&#26512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36866;&#24403;&#30340;&#25216;&#26415;&#20551;&#35774;&#19979;&#65292;&#26799;&#24230;&#19979;&#38477;-&#19978;&#21319;&#31639;&#27861;&#22312;&#24310;&#36831;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.06886</link><description>&lt;p&gt;
Min-Max&#20248;&#21270;&#22312;&#24310;&#36831;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Min-Max Optimization under Delays. (arXiv:2307.06886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06886
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#20102;min-max&#20248;&#21270;&#22312;&#24310;&#36831;&#19979;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#31616;&#21333;&#23454;&#20363;&#65292;&#21363;&#20351;&#26159;&#23567;&#30340;&#24310;&#36831;&#20063;&#21487;&#33021;&#23548;&#33268;Extra-gradient&#31639;&#27861;&#21457;&#25955;&#65292;&#22240;&#27492;&#38656;&#35201;&#23545;&#24310;&#36831;&#29256;&#26412;&#30340;min-max&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20180;&#32454;&#20998;&#26512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36866;&#24403;&#30340;&#25216;&#26415;&#20551;&#35774;&#19979;&#65292;&#26799;&#24230;&#19979;&#38477;-&#19978;&#21319;&#31639;&#27861;&#22312;&#24310;&#36831;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36890;&#20449;&#36215;&#37325;&#35201;&#20316;&#29992;&#30340;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#24310;&#36831;&#21644;&#24322;&#27493;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#22240;&#27492;&#65292;&#19968;&#20123;&#30740;&#31350;&#22242;&#38431;&#24191;&#27867;&#20998;&#26512;&#20102;&#20855;&#26377;&#24310;&#36831;&#26799;&#24230;&#30340;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#12290;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23578;&#26080;&#31867;&#20284;&#30340;&#29702;&#35770;&#21487;&#29992;&#20110;min-max&#20248;&#21270;&#65292;&#36825;&#20010;&#35805;&#39064;&#30001;&#20110;&#22312;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#21338;&#24328;&#35770;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#32780;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#38024;&#23545;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23545;&#24102;&#26377;&#24310;&#36831;&#26799;&#24230;&#26356;&#26032;&#30340;&#26631;&#20934;min-max&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#65288;&#32463;&#39564;&#24615;&#22320;&#65289;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#23567;&#30340;&#24310;&#36831;&#20063;&#21487;&#33021;&#23548;&#33268;&#20687;Extra-gradient (EG) &#36825;&#26679;&#30340;&#26480;&#20986;&#31639;&#27861;&#22312;&#31616;&#21333;&#23454;&#20363;&#19978;&#21457;&#25955;&#65292;&#32780;&#22312;&#27809;&#26377;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;EG&#21487;&#20197;&#20445;&#35777;&#25910;&#25947;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#26377;&#24517;&#35201;&#23545;&#24310;&#36831;&#29256;&#26412;&#30340;min-max&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20180;&#32454;&#20998;&#26512;&#12290;&#30456;&#24212;&#22320;&#65292;&#22312;&#36866;&#24403;&#30340;&#25216;&#26415;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477; - &#19978;&#21319; (GDA)&#31639;&#27861;&#22312;&#24310;&#36831;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Delays and asynchrony are inevitable in large-scale machine-learning problems where communication plays a key role. As such, several works have extensively analyzed stochastic optimization with delayed gradients. However, as far as we are aware, no analogous theory is available for min-max optimization, a topic that has gained recent popularity due to applications in adversarial robustness, game theory, and reinforcement learning. Motivated by this gap, we examine the performance of standard min-max optimization algorithms with delayed gradient updates. First, we show (empirically) that even small delays can cause prominent algorithms like Extra-gradient (\texttt{EG}) to diverge on simple instances for which \texttt{EG} guarantees convergence in the absence of delays. Our empirical study thus suggests the need for a careful analysis of delayed versions of min-max optimization algorithms. Accordingly, under suitable technical assumptions, we prove that Gradient Descent-Ascent (\texttt{G
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32039;&#26680;&#30340;&#31639;&#23376;&#29702;&#35770;&#26041;&#27861;&#26469;&#35299;&#20915;&#26465;&#20214;&#26399;&#26395;&#20272;&#35745;&#38382;&#39064;&#65292;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#23454;&#29616;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#19988;&#25104;&#21151;&#24212;&#29992;&#20110;&#23454;&#38469;&#38382;&#39064;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.10592</link><description>&lt;p&gt;
&#22522;&#20110;&#32039;&#26680;&#30340;&#26465;&#20214;&#26399;&#26395;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Conditional expectation via compact kernels. (arXiv:2306.10592v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32039;&#26680;&#30340;&#31639;&#23376;&#29702;&#35770;&#26041;&#27861;&#26469;&#35299;&#20915;&#26465;&#20214;&#26399;&#26395;&#20272;&#35745;&#38382;&#39064;&#65292;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#23454;&#29616;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#19988;&#25104;&#21151;&#24212;&#29992;&#20110;&#23454;&#38469;&#38382;&#39064;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#12289;&#26465;&#20214;&#26399;&#26395;&#21644;&#27969;&#24418;&#23398;&#20064;&#20219;&#21153;&#36890;&#24120;&#21487;&#20197;&#22312;&#23547;&#25214;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#31215;&#30340;&#26465;&#20214;&#26399;&#26395;&#30340;&#20844;&#20849;&#29615;&#22659;&#19979;&#34920;&#36848;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#20010;&#26356;&#19968;&#33324;&#30340;&#38382;&#39064;&#65292;&#25551;&#36848;&#20102;&#19968;&#31181;&#31639;&#23376;&#29702;&#35770;&#26041;&#27861;&#26469;&#20272;&#35745;&#26465;&#20214;&#26399;&#26395;&#12290;&#26680;&#31215;&#20998;&#31639;&#23376;&#34987;&#29992;&#20316;&#32039;&#33268;&#21270;&#24037;&#20855;&#65292;&#23558;&#20272;&#35745;&#38382;&#39064;&#35774;&#32622;&#20026;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#36870;&#38382;&#39064;&#12290;&#35813;&#26041;&#31243;&#30340;&#35299;&#34987;&#35777;&#26126;&#23545;&#25968;&#20540;&#36924;&#36817;&#26159;&#31283;&#23450;&#30340;&#65292;&#20174;&#32780;&#30830;&#20445;&#20102;&#25968;&#25454;&#39537;&#21160;&#23454;&#29616;&#30340;&#25910;&#25947;&#24615;&#12290;&#24635;&#20307;&#25216;&#26415;&#26131;&#20110;&#23454;&#29616;&#65292;&#36824;&#23637;&#31034;&#20102;&#20854;&#22312;&#19968;&#20123;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#25104;&#21151;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The separate tasks of denoising, conditional expectation and manifold learning can often be posed in a common setting of finding the conditional expectations arising from a product of two random variables. This paper focuses on this more general problem and describes an operator theoretic approach to estimating the conditional expectation. Kernel integral operators are used as a compactification tool, to set up the estimation problem as a linear inverse problem in a reproducing kernel Hilbert space. This equation is shown to have solutions that are stable to numerical approximation, thus guaranteeing the convergence of data-driven implementations. The overall technique is easy to implement, and their successful application to some real-world problems are also shown.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20013;&#38388;&#36890;&#20449;&#30340;&#31616;&#21270;&#24182;&#34892;GNN&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#27748;&#30340;&#21407;&#29702;&#20943;&#36731;&#20102;GNN&#25193;&#23637;&#36807;&#31243;&#20013;&#30340;&#20869;&#23384;&#29942;&#39048;&#21644;&#21487;&#35757;&#32451;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.10466</link><description>&lt;p&gt;
&#22270;&#21152;&#36733;&#65306;&#20196;&#20154;&#38663;&#24778;&#30340;&#31616;&#21270;&#24182;&#34892;GNN&#35757;&#32451;&#65292;&#26080;&#38656;&#20013;&#38388;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication. (arXiv:2306.10466v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10466
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20013;&#38388;&#36890;&#20449;&#30340;&#31616;&#21270;&#24182;&#34892;GNN&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#27748;&#30340;&#21407;&#29702;&#20943;&#36731;&#20102;GNN&#25193;&#23637;&#36807;&#31243;&#20013;&#30340;&#20869;&#23384;&#29942;&#39048;&#21644;&#21487;&#35757;&#32451;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#22312;&#21508;&#22788;&#37117;&#26159;&#23384;&#22312;&#30340;&#65292;&#32780;GNN&#26159;&#19968;&#31867;&#29992;&#20110;&#22270;&#23398;&#20064;&#30340;&#24378;&#22823;&#31070;&#32463;&#32593;&#32476;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#36890;&#36807;&#21152;&#28145;&#25110;&#21152;&#23485;&#26469;&#25193;&#23637;GNN&#36824;&#23384;&#22312;&#30528;&#26799;&#24230;&#19981;&#20581;&#24247;&#12289;&#36807;&#24230;&#24179;&#28369;&#12289;&#20449;&#24687;&#21387;&#32553;&#31561;&#26222;&#36941;&#38382;&#39064;&#65292;&#24120;&#24120;&#23548;&#33268;&#27425;&#26631;&#20934;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26377;&#20852;&#36259;&#25506;&#32034;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#20197;&#19981;&#21152;&#28145;&#25110;&#21152;&#23485;&#30340;&#26041;&#24335;&#25193;&#23637;GNN&#30340;&#23481;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#22312;&#22810;&#20010;&#23567;&#22411;&#21644;&#22823;&#22411;&#22270;&#19978;&#30340;&#24615;&#33021;&#12290;&#21463;&#26368;&#36817;&#24341;&#20154;&#27880;&#30446;&#30340;&#27169;&#22411;&#27748;&#29616;&#35937;&#30340;&#21551;&#21457;&#65292;&#35813;&#29616;&#35937;&#34920;&#26126;&#22810;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24494;&#35843;&#26435;&#37325;&#21487;&#20197;&#21512;&#24182;&#25104;&#26356;&#22909;&#30340;&#26368;&#23567;&#20540;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#27169;&#22411;&#27748;&#30340;&#22522;&#26412;&#21407;&#29702;&#26469;&#20943;&#36731;GNN&#25193;&#23637;&#36807;&#31243;&#20013;&#30340;&#20869;&#23384;&#29942;&#39048;&#21644;&#21487;&#35757;&#32451;&#24615;&#38382;&#39064;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#24314;&#35758;&#19981;&#21152;&#28145;&#25110;&#21152;&#23485;&#24403;&#21069;&#30340;GNN&#65292;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;GNN&#23450;&#21046;&#30340;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#27748;&#35270;&#35282;&#65292;&#21363;&#26500;&#24314;&#21151;&#33021;&#24378;&#22823;&#30340;
&lt;/p&gt;
&lt;p&gt;
Graphs are omnipresent and GNNs are a powerful family of neural networks for learning over graphs. Despite their popularity, scaling GNNs either by deepening or widening suffers from prevalent issues of unhealthy gradients, over-smoothening, information squashing, which often lead to sub-standard performance. In this work, we are interested in exploring a principled way to scale GNNs capacity without deepening or widening, which can improve its performance across multiple small and large graphs. Motivated by the recent intriguing phenomenon of model soups, which suggest that fine-tuned weights of multiple large-language pre-trained models can be merged to a better minima, we argue to exploit the fundamentals of model soups to mitigate the aforementioned issues of memory bottleneck and trainability during GNNs scaling. More specifically, we propose not to deepen or widen current GNNs, but instead present a data-centric perspective of model soups tailored for GNNs, i.e., to build powerfu
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#32858;&#28966;&#20110;&#24102;&#24335;&#34880;&#21387;&#30417;&#27979;&#25216;&#26415;&#65292;&#24378;&#35843;&#20102;&#30001;&#20110;&#27979;&#37327;&#21644;&#35774;&#22791;&#35823;&#24046;&#12289;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#20307;&#22411;&#24046;&#24322;&#31561;&#22240;&#32032;&#23548;&#33268;&#30340;&#34880;&#21387;&#27979;&#37327;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;&#30740;&#21457;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#26469;&#32416;&#27491;&#35823;&#24046;&#30340;&#26032;&#19968;&#20195;&#24102;&#24335;&#34880;&#21387;&#35774;&#22791;&#26159;&#37325;&#28857;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.08451</link><description>&lt;p&gt;
&#34880;&#21387;&#27979;&#37327;&#25216;&#26415;&#32508;&#36848;&#65306;&#35299;&#20915;&#28508;&#22312;&#30340;&#20559;&#24046;&#26469;&#28304;
&lt;/p&gt;
&lt;p&gt;
A Survey on Blood Pressure Measurement Technologies: Addressing Potential Sources of Bias. (arXiv:2306.08451v2 [physics.med-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08451
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#32858;&#28966;&#20110;&#24102;&#24335;&#34880;&#21387;&#30417;&#27979;&#25216;&#26415;&#65292;&#24378;&#35843;&#20102;&#30001;&#20110;&#27979;&#37327;&#21644;&#35774;&#22791;&#35823;&#24046;&#12289;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#20307;&#22411;&#24046;&#24322;&#31561;&#22240;&#32032;&#23548;&#33268;&#30340;&#34880;&#21387;&#27979;&#37327;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;&#30740;&#21457;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#26469;&#32416;&#27491;&#35823;&#24046;&#30340;&#26032;&#19968;&#20195;&#24102;&#24335;&#34880;&#21387;&#35774;&#22791;&#26159;&#37325;&#28857;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#21644;&#27969;&#21160;&#22330;&#26223;&#20013;&#30340;&#23450;&#26399;&#34880;&#21387;&#65288;BP&#65289;&#30417;&#27979;&#22312;&#39044;&#38450;&#12289;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#31649;&#29702;&#24515;&#34880;&#31649;&#30142;&#30149;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#27969;&#21160;&#24335;BP&#27979;&#37327;&#35774;&#22791;&#30340;&#24191;&#27867;&#37319;&#29992;&#20027;&#35201;&#26159;&#30001;&#20110;&#39640;&#34880;&#21387;&#30340;&#26222;&#36941;&#22686;&#21152;&#20197;&#21450;&#20854;&#30456;&#20851;&#39118;&#38505;&#21644;&#20020;&#24202;&#29366;&#20917;&#12290;&#26368;&#36817;&#30340;&#25351;&#21335;&#24314;&#35758;&#23558;&#23450;&#26399;BP&#30417;&#27979;&#20316;&#20026;&#24120;&#35268;&#20020;&#24202;&#35775;&#35270;&#29978;&#33267;&#22312;&#23478;&#37324;&#36827;&#34892;&#12290; &#36825;&#31181;&#22686;&#21152;&#30340;BP&#27979;&#37327;&#25216;&#26415;&#21033;&#29992;&#24102;&#24335;&#27979;&#21387;&#26041;&#27861;&#20063;&#24102;&#26469;&#20102;&#37325;&#35201;&#30340;&#20851;&#27880;&#28857;&#65292;&#28041;&#21450;&#21508;&#31181;&#35774;&#32622;&#19979;&#25253;&#21578;&#30340;BP&#20540;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#22522;&#20110;&#24102;&#24335;&#27979;&#21387;&#25216;&#26415;&#30340;BP&#27979;&#37327;&#26041;&#27861;&#22914;&#20309;&#30001;&#20110;&#27979;&#37327;&#21644;&#35774;&#22791;&#35823;&#24046;&#12289;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#20307;&#22411;&#24046;&#24322;&#31561;&#22240;&#32032;&#23548;&#33268;&#26174;&#33879;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;&#22312;&#36825;&#20123;&#22266;&#26377;&#30340;&#20559;&#24046;&#24773;&#20917;&#19979;&#65292;&#21457;&#23637;&#19968;&#31181;&#26032;&#19968;&#20195;&#30340;&#22522;&#20110;&#24102;&#24335;&#27979;&#21387;&#35774;&#22791;&#65292;&#20854;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#32416;&#27491;&#35823;&#24046;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regular blood pressure (BP) monitoring in clinical and ambulatory settings plays a crucial role in the prevention, diagnosis, treatment, and management of cardiovascular diseases. Recently, the widespread adoption of ambulatory BP measurement devices has been driven predominantly by the increased prevalence of hypertension and its associated risks and clinical conditions. Recent guidelines advocate for regular BP monitoring as part of regular clinical visits or even at home. This increased utilization of BP measurement technologies has brought up significant concerns, regarding the accuracy of reported BP values across settings.  In this survey, focusing mainly on cuff-based BP monitoring technologies, we highlight how BP measurements can demonstrate substantial biases and variances due to factors such as measurement and device errors, demographics, and body habitus. With these inherent biases, the development of a new generation of cuff-based BP devices which use artificial-intelligen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21516;&#24577;&#26144;&#23556;&#30340;&#26032;&#22411;&#38543;&#26426;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;&#26399;&#26395;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35745;&#31639;&#65292;&#33021;&#22312;&#26399;&#26395;&#20013;&#21306;&#20998;&#25152;&#26377;&#38750;&#21516;&#26500;&#22270;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#26367;&#20195;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05838</link><description>&lt;p&gt;
&#22522;&#20110;&#21516;&#24577;&#26144;&#23556;&#30340;&#26399;&#26395;&#23436;&#20840;&#22270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Expectation-Complete Graph Representations with Homomorphisms. (arXiv:2306.05838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05838
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21516;&#24577;&#26144;&#23556;&#30340;&#26032;&#22411;&#38543;&#26426;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;&#26399;&#26395;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35745;&#31639;&#65292;&#33021;&#22312;&#26399;&#26395;&#20013;&#21306;&#20998;&#25152;&#26377;&#38750;&#21516;&#26500;&#22270;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#26367;&#20195;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#22270;&#23884;&#20837;&#65292;&#21487;&#20197;&#22312;&#26399;&#26395;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35745;&#31639;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#26399;&#26395;&#20013;&#21306;&#20998;&#25152;&#26377;&#38750;&#21516;&#26500;&#22270;&#12290;&#20197;&#21069;&#30340;&#22270;&#23884;&#20837;&#20855;&#26377;&#26377;&#38480;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#35201;&#20040;&#19981;&#33021;&#21306;&#20998;&#25152;&#26377;&#22270;&#65292;&#35201;&#20040;&#19981;&#33021;&#26377;&#25928;&#22320;&#35745;&#31639;&#27599;&#20010;&#22270;&#12290;&#20026;&#20102;&#33021;&#22815;&#22312;&#22270;&#19978;&#36817;&#20284;&#20219;&#24847;&#20989;&#25968;&#65292;&#25105;&#20204;&#23545;&#20855;&#26377;&#36882;&#22686;&#36164;&#28304;&#30340;&#39640;&#25928;&#26367;&#20195;&#26041;&#26696;&#24863;&#20852;&#36259;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110; Lov\'asz &#23545;&#36890;&#36807;&#21516;&#24577;&#35745;&#25968;&#30340;&#26080;&#38480;&#32500;&#21521;&#37327;&#30340;&#22270;&#21516;&#26500;&#30340;&#34920;&#24449;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#20102;&#22312;&#20960;&#20010;&#22522;&#20934;&#22270;&#23398;&#20064;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate novel random graph embeddings that can be computed in expected polynomial time and that are able to distinguish all non-isomorphic graphs in expectation. Previous graph embeddings have limited expressiveness and either cannot distinguish all graphs or cannot be computed efficiently for every graph. To be able to approximate arbitrary functions on graphs, we are interested in efficient alternatives that become arbitrarily expressive with increasing resources. Our approach is based on Lov\'asz' characterisation of graph isomorphism through an infinite dimensional vector of homomorphism counts. Our empirical evaluation shows competitive results on several benchmark graph learning tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#29983;&#29289;&#25968;&#25454;&#38598;&#35757;&#32451;&#26679;&#26412;&#36739;&#23567;&#26102;&#65292;&#38646;&#26679;&#20363;ChatGPT&#29978;&#33267;&#20248;&#20110;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#30001;&#27492;&#34920;&#26126;ChatGPT&#20855;&#26377;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#25104;&#20026;&#26377;&#20215;&#20540;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04504</link><description>&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#35780;&#20272;ChatGPT&#65306;&#19982;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#30340;&#38646;&#26679;&#20363;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers. (arXiv:2306.04504v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#29983;&#29289;&#25968;&#25454;&#38598;&#35757;&#32451;&#26679;&#26412;&#36739;&#23567;&#26102;&#65292;&#38646;&#26679;&#20363;ChatGPT&#29978;&#33267;&#20248;&#20110;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#30001;&#27492;&#34920;&#26126;ChatGPT&#20855;&#26377;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#25104;&#20026;&#26377;&#20215;&#20540;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;OpenAI&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23613;&#31649;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20808;&#21069;&#30340;&#24037;&#20316;&#23578;&#26410;&#30740;&#31350;&#20854;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;ChatGPT&#22312;&#21508;&#31181;&#22522;&#20934;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#22914;&#20851;&#31995;&#25552;&#21462;&#12289;&#25991;&#26723;&#20998;&#31867;&#12289;&#38382;&#31572;&#21644;&#25688;&#35201;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#23545;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#24037;&#20316;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;&#35757;&#32451;&#38598;&#36739;&#23567;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#38646;&#26679;&#20363;ChatGPT&#29978;&#33267;&#20248;&#20110;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#22914;BioGPT&#21644;BioBART&#12290;&#36825;&#34920;&#26126;ChatGPT&#22312;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#30340;&#39044;&#35757;&#32451;&#20351;&#20854;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#26377;&#30456;&#24403;&#30340;&#19987;&#19994;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#26377;&#25104;&#20026;&#21508;&#31181;&#20219;&#21153;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a large language model developed by OpenAI. Despite its impressive performance across various tasks, no prior work has investigated its capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of ChatGPT on various benchmark biomedical tasks, such as relation extraction, document classification, question answering, and summarization. To the best of our knowledge, this is the first work that conducts an extensive evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative transformer models, such as BioGPT and BioBART. This suggests that ChatGPT's pre-training on large text corpora makes it quite specialized even in the biomedical domain. Our findings demonstrate that ChatGPT has the potential to be a valuable tool for various tasks in the biomedical domain that la
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#26368;&#23567;&#20869;&#22312;&#32500;&#24230;&#32553;&#25918;&#29616;&#35937;&#65292;&#22312;&#19981;&#20570;&#20986;&#25968;&#25454;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#29109;&#20248;&#21270;&#36755;&#36816;&#38382;&#39064;&#20013;&#65292;&#20197;&#36798;&#21040;&#26356;&#20248;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03398</link><description>&lt;p&gt;
&#29109;&#20248;&#21270;&#36755;&#36816;&#30340;&#26368;&#23567;&#20869;&#22312;&#32500;&#24230;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
Minimum intrinsic dimension scaling for entropic optimal transport. (arXiv:2306.03398v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03398
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#26368;&#23567;&#20869;&#22312;&#32500;&#24230;&#32553;&#25918;&#29616;&#35937;&#65292;&#22312;&#19981;&#20570;&#20986;&#25968;&#25454;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#29109;&#20248;&#21270;&#36755;&#36816;&#38382;&#39064;&#20013;&#65292;&#20197;&#36798;&#21040;&#26356;&#20248;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#27969;&#24418;&#20551;&#35828;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23545;&#29109;&#20248;&#21270;&#36755;&#36816;&#36827;&#34892;&#31934;&#32454;&#30340;&#32479;&#35745;&#30028;&#38480;&#35774;&#35745;&#65292;&#35813;&#30028;&#38480;&#23545;&#25968;&#25454;&#30340;&#20869;&#22312;&#32500;&#24230;&#25935;&#24863;&#65292;&#24182;&#20165;&#22312;&#21333;&#19968;&#36317;&#31163;&#23610;&#24230;&#19979;&#34913;&#37327;&#30450;&#21306;&#20869;&#22312;&#32500;&#24230;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#26368;&#23567;&#20869;&#22312;&#32500;&#24230;&#32553;&#25918;&#65288;MID scaling&#65289;&#29616;&#35937;&#65292;&#23427;&#34920;&#26126;&#20165;&#26377;&#36825;&#20123;&#21333;&#23610;&#24230;&#20869;&#22312;&#32500;&#24230;&#30340;&#26368;&#23567;&#20540;&#25165;&#25511;&#21046;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;MID scaling&#26159;&#19968;&#20010;&#36890;&#29992;&#29616;&#35937;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#29109;&#20248;&#21270;&#36755;&#36816;&#38382;&#39064;&#65292;&#19981;&#38656;&#35201;&#23545;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#20551;&#35774;&#12290;&#24403;&#19968;&#20010;&#20998;&#24067;&#38598;&#20013;&#22312;&#27969;&#24418;&#19978;&#26102;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26377;&#26356;&#24378;&#30340;&#23545;&#24212;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the manifold hypothesis, which states that data with a high extrinsic dimension may yet have a low intrinsic dimension, we develop refined statistical bounds for entropic optimal transport that are sensitive to the intrinsic dimension of the data. Our bounds involve a robust notion of intrinsic dimension, measured at only a single distance scale depending on the regularization parameter, and show that it is only the minimum of these single-scale intrinsic dimensions which governs the rate of convergence. We call this the Minimum Intrinsic Dimension scaling (MID scaling) phenomenon, and establish MID scaling with no assumptions on the data distributions so long as the cost is bounded and Lipschitz, and for various entropic optimal transport quantities beyond just values, with stronger analogs when one distribution is supported on a manifold. Our results significantly advance the theoretical state of the art by showing that MID scaling is a generic phenomenon, and provide th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;YNN&#30340;&#26041;&#27861;&#65292;&#23558;&#21516;&#19968;&#32423;&#21035;&#30340;ANN&#33410;&#28857;&#36830;&#25509;&#22312;&#19968;&#36215;&#24418;&#25104;&#31070;&#32463;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#26222;&#36890;ANN&#26080;&#27861;&#20849;&#20139;&#20449;&#24687;&#30340;&#32570;&#38519;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20449;&#24687;&#20256;&#36755;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02157</link><description>&lt;p&gt;
&#23558;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;Yoked&#31070;&#32463;&#32593;&#32476;&#20197;&#25913;&#36827;ANN&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Transforming to Yoked Neural Networks to Improve ANN Structure. (arXiv:2306.02157v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;YNN&#30340;&#26041;&#27861;&#65292;&#23558;&#21516;&#19968;&#32423;&#21035;&#30340;ANN&#33410;&#28857;&#36830;&#25509;&#22312;&#19968;&#36215;&#24418;&#25104;&#31070;&#32463;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#26222;&#36890;ANN&#26080;&#27861;&#20849;&#20139;&#20449;&#24687;&#30340;&#32570;&#38519;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20449;&#24687;&#20256;&#36755;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#24050;&#32463;&#23384;&#22312;&#30340;&#32463;&#20856;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#37117;&#34987;&#35774;&#35745;&#25104;&#26641;&#24418;&#32467;&#26500;&#20197;&#27169;&#25311;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#26641;&#24418;&#32467;&#26500;&#30340;&#36830;&#25509;&#19981;&#36275;&#20197;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#12290;&#21516;&#19968;&#32423;&#21035;&#30340;&#33410;&#28857;&#19981;&#33021;&#36830;&#25509;&#22312;&#19968;&#36215;&#65292;&#21363;&#36825;&#20123;&#31070;&#32463;&#20803;&#19981;&#33021;&#30456;&#20114;&#20849;&#20139;&#20449;&#24687;&#65292;&#36825;&#26159;ANN&#30340;&#19968;&#20010;&#37325;&#22823;&#32570;&#38519;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#20026;&#21516;&#19968;&#32423;&#21035;&#30340;&#33410;&#28857;&#24314;&#31435;&#21452;&#21521;&#23436;&#20840;&#22270;&#65292;&#23558;&#21516;&#19968;&#32423;&#21035;&#30340;&#33410;&#28857;&#36830;&#25509;&#21040;&#19968;&#36215;&#24418;&#25104;&#31070;&#32463;&#27169;&#22359;&#12290;&#25105;&#20204;&#25226;&#25105;&#20204;&#30340;&#27169;&#22411;&#31216;&#20026;YNN&#12290;YNN&#26174;&#33879;&#20419;&#36827;&#20102;&#20449;&#24687;&#20256;&#36755;&#65292;&#26126;&#26174;&#26377;&#21161;&#20110;&#25552;&#39640;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;YNN&#21487;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#31070;&#32463;&#32593;&#32476;&#65292;&#30456;&#27604;&#20854;&#20182;ANN&#26041;&#27861;&#26377;&#30528;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing classical artificial neural networks (ANN) are designed as a tree structure to imitate neural networks. In this paper, we argue that the connectivity of a tree is not sufficient to characterize a neural network. The nodes of the same level of a tree cannot be connected with each other, i.e., these neural unit cannot share information with each other, which is a major drawback of ANN. Although ANN has been significantly improved in recent years to more complex structures, such as the directed acyclic graph (DAG), these methods also have unidirectional and acyclic bias for ANN. In this paper, we propose a method to build a bidirectional complete graph for the nodes in the same level of an ANN, which yokes the nodes of the same level to formulate a neural module. We call our model as YNN in short. YNN promotes the information transfer significantly which obviously helps in improving the performance of the method. Our YNN can imitate neural networks much better compared with 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#31934;&#30830;&#25512;&#29702;&#31163;&#25955;&#32479;&#35745;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#25903;&#25345;&#31163;&#25955;&#37319;&#26679;&#12289;&#36830;&#32493;&#37319;&#26679;&#12289;&#31163;&#25955;&#35266;&#27979;&#12289;&#20223;&#23556;&#20989;&#25968;&#12289;&#65288;&#38543;&#26426;&#65289;&#20998;&#25903;&#21644;&#20107;&#20214;&#26465;&#20214;&#12290;&#36890;&#36807;&#27010;&#29575;&#29983;&#25104;&#20989;&#25968;&#23454;&#29616;&#21518;&#39564;&#27010;&#29575;&#12289;&#26399;&#26395;&#12289;&#26041;&#24046;&#21644;&#39640;&#38454;&#30697;&#30340;&#31934;&#30830;&#35745;&#31639;&#12290;&#35813;&#26041;&#27861;&#24615;&#33021;&#20248;&#20110;&#36817;&#20284;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#24182;&#36991;&#20813;&#20102;&#36817;&#20284;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.17058</link><description>&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;&#29983;&#25104;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#31163;&#25955;&#27169;&#22411;&#31934;&#30830;&#25512;&#29702;&#65306;&#27010;&#29575;&#32534;&#31243;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exact Bayesian Inference on Discrete Models via Probability Generating Functions: A Probabilistic Programming Approach. (arXiv:2305.17058v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17058
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#31934;&#30830;&#25512;&#29702;&#31163;&#25955;&#32479;&#35745;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#25903;&#25345;&#31163;&#25955;&#37319;&#26679;&#12289;&#36830;&#32493;&#37319;&#26679;&#12289;&#31163;&#25955;&#35266;&#27979;&#12289;&#20223;&#23556;&#20989;&#25968;&#12289;&#65288;&#38543;&#26426;&#65289;&#20998;&#25903;&#21644;&#20107;&#20214;&#26465;&#20214;&#12290;&#36890;&#36807;&#27010;&#29575;&#29983;&#25104;&#20989;&#25968;&#23454;&#29616;&#21518;&#39564;&#27010;&#29575;&#12289;&#26399;&#26395;&#12289;&#26041;&#24046;&#21644;&#39640;&#38454;&#30697;&#30340;&#31934;&#30830;&#35745;&#31639;&#12290;&#35813;&#26041;&#27861;&#24615;&#33021;&#20248;&#20110;&#36817;&#20284;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#24182;&#36991;&#20813;&#20102;&#36817;&#20284;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#25955;&#32479;&#35745;&#27169;&#22411;&#30340;&#31934;&#30830;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#21363;&#20351;&#26159;&#23545;&#20110;&#26080;&#38480;&#25903;&#25345;&#21644;&#36830;&#32493;&#20808;&#39564;&#20063;&#21487;&#20197;&#25214;&#21040;&#20934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#34920;&#36798;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25903;&#25345;&#31163;&#25955;&#21644;&#36830;&#32493;&#37319;&#26679;&#12289;&#31163;&#25955;&#35266;&#27979;&#12289;&#20223;&#23556;&#20989;&#25968;&#12289;&#65288;&#38543;&#26426;&#65289;&#20998;&#25903;&#21644;&#20107;&#20214;&#26465;&#20214;&#30340;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24037;&#20855;&#26159;&#27010;&#29575;&#29983;&#25104;&#20989;&#25968;&#65306;&#23427;&#20204;&#25552;&#20379;&#20102;&#23450;&#20041;&#31243;&#24207;&#30340;&#20998;&#24067;&#30340;&#32039;&#20945;&#38381;&#21512;&#24418;&#24335;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21518;&#39564;&#27010;&#29575;&#12289;&#26399;&#26395;&#12289;&#26041;&#24046;&#21644;&#39640;&#38454;&#30697;&#30340;&#31934;&#30830;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#25512;&#29702;&#26041;&#27861;&#26159;&#21487;&#35777;&#26126;&#27491;&#30830;&#30340;&#12289;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#65292;&#20351;&#29992;&#33258;&#21160;&#24494;&#20998;&#65288;&#29305;&#21035;&#26159;&#27888;&#21202;&#22810;&#39033;&#24335;&#65289;&#65292;&#20294;&#19981;&#38656;&#35201;&#35745;&#31639;&#26426;&#20195;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#19990;&#30028;&#30340;&#20363;&#23376;&#20013;&#30340;&#24615;&#33021;&#19982;&#36817;&#20284;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#31454;&#20105;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#36817;&#20284;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an exact Bayesian inference method for discrete statistical models, which can find exact solutions to many discrete inference problems, even with infinite support and continuous priors. To express such models, we introduce a probabilistic programming language that supports discrete and continuous sampling, discrete observations, affine functions, (stochastic) branching, and conditioning on events. Our key tool is probability generating functions: they provide a compact closed-form representation of distributions that are definable by programs, thus enabling the exact computation of posterior probabilities, expectation, variance, and higher moments. Our inference method is provably correct, fully automated and uses automatic differentiation (specifically, Taylor polynomials), but does not require computer algebra. Our experiments show that its performance on a range of real-world examples is competitive with approximate Monte Carlo methods, while avoiding approximation errors
&lt;/p&gt;</description></item><item><title>LANISTR&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26694;&#26550;&#65292;&#21487;&#20174;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#22312;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.16556</link><description>&lt;p&gt;
LANISTR&#65306;&#20174;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LANISTR: Multimodal Learning from Structured and Unstructured Data. (arXiv:2305.16556v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16556
&lt;/p&gt;
&lt;p&gt;
LANISTR&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26694;&#26550;&#65292;&#21487;&#20174;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#22312;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#24050;&#32463;&#22312;&#22788;&#29702;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#65289;&#26041;&#38754;&#23637;&#29616;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20294;&#26159;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#26368;&#24120;&#35265;&#30340;&#24773;&#20917;&#26159;&#32467;&#26500;&#21270;&#65288;&#21253;&#25324;&#34920;&#26684;&#21644;&#26102;&#38388;&#24207;&#21015;&#65289;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#32467;&#21512;&#65292;&#20294;&#36825;&#19968;&#39046;&#22495;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LANISTR&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#35821;&#35328;&#12289;&#22270;&#20687;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#22810;&#27169;&#24577;&#36974;&#32617;&#25439;&#22833;&#65292;&#20351;&#24471;LANISTR&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#36328;&#27169;&#24577;&#20851;&#31995;&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#22788;&#29702;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;MIMIC-IV&#21644;Amazon Product Review&#19978;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30456;&#27604;&#65292;LANISTR&#20998;&#21035;&#36798;&#21040;&#20102;6.47%&#65288;AUROC&#65289;&#21644;&#39640;&#36798;17.69%&#65288;&#20934;&#30830;&#24230;&#65289;&#30340;&#32477;&#23545;&#25552;&#21319;&#65292;&#24182;&#26174;&#31034;&#20986;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal large-scale pretraining has shown impressive performance gains for unstructured data including language, image, audio, and video. Yet, the scenario most prominent in real-world applications is the existence of combination of structured (including tabular and time-series) and unstructured data, and this has so far been understudied. Towards this end, we propose LANISTR, a novel attention-based framework to learn from LANguage, Image, and STRuctured data. We introduce a new multimodal fusion module with a similarity-based multimodal masking loss that enables LANISTR to learn cross-modal relations from large-scale multimodal data with missing modalities during training and test time. On two publicly available challenging datasets, MIMIC-IV and Amazon Product Review, LANISTR achieves absolute improvements of 6.47% (AUROC) and up to 17.69% (accuracy), respectively, compared to the state-of-the-art multimodal models while showing superior generalization capabilities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#20108;&#27425;&#27969;&#24418;&#23545;&#39640;&#32500;&#21704;&#23494;&#39039;&#31995;&#32479;&#36827;&#34892;&#27491;&#21017;&#27169;&#22411;&#31616;&#21270;&#65292;&#20174;&#32780;&#20351;&#24471;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#38382;&#39064;&#20013;&#22266;&#26377;&#30340;&#20302;&#32500;&#24615;&#65292;&#24182;&#22312;&#36229;&#20986;&#20854;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#30340;&#35774;&#32622;&#20013;&#25552;&#20379;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15490</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#20108;&#27425;&#27969;&#24418;&#36827;&#34892;&#21704;&#23494;&#39039;&#31995;&#32479;&#27491;&#21017;&#27169;&#22411;&#31616;&#21270;
&lt;/p&gt;
&lt;p&gt;
Symplectic model reduction of Hamiltonian systems using data-driven quadratic manifolds. (arXiv:2305.15490v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#20108;&#27425;&#27969;&#24418;&#23545;&#39640;&#32500;&#21704;&#23494;&#39039;&#31995;&#32479;&#36827;&#34892;&#27491;&#21017;&#27169;&#22411;&#31616;&#21270;&#65292;&#20174;&#32780;&#20351;&#24471;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#38382;&#39064;&#20013;&#22266;&#26377;&#30340;&#20302;&#32500;&#24615;&#65292;&#24182;&#22312;&#36229;&#20986;&#20854;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#30340;&#35774;&#32622;&#20013;&#25552;&#20379;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#20108;&#27425;&#27969;&#24418;&#23545;&#39640;&#32500;&#21704;&#23494;&#39039;&#31995;&#32479;&#36827;&#34892;&#27491;&#21017;&#27169;&#22411;&#31616;&#21270;&#12290;&#20256;&#32479;&#30340;&#27491;&#21017;&#27169;&#22411;&#31616;&#21270;&#26041;&#27861;&#37319;&#29992;&#32447;&#24615;&#27491;&#21017;&#23376;&#31354;&#38388;&#34920;&#31034;&#39640;&#32500;&#31995;&#32479;&#29366;&#24577;&#30340;&#20943;&#23569;&#32500;&#24230;&#22352;&#26631;&#31995;&#12290;&#34429;&#28982;&#36825;&#20123;&#36817;&#20284;&#32771;&#34385;&#20102;&#21704;&#23494;&#39039;&#31995;&#32479;&#27491;&#21017;&#24615;&#36136;&#65292;&#20294;&#26159;&#36817;&#20284;&#30340;&#32447;&#24615;&#24615;&#20351;&#24471;&#26080;&#27861;&#36798;&#21040;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22522;&#20110;&#26368;&#36817;&#24320;&#21457;&#30340;&#20108;&#27425;&#27969;&#24418;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#31616;&#21270;&#26041;&#27861;&#65292;&#27599;&#19968;&#31181;&#26041;&#27861;&#37117;&#20855;&#26377;&#29420;&#29305;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#22312;&#29366;&#24577;&#36817;&#20284;&#20013;&#21152;&#20837;&#20108;&#27425;&#39033;&#65292;&#36825;&#26159;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#34920;&#31034;&#38382;&#39064;&#20013;&#22266;&#26377;&#30340;&#20302;&#32500;&#24615;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#36229;&#20986;&#20854;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#30340;&#35774;&#32622;&#20013;&#21457;&#20986;&#39044;&#27979;&#26102;&#37117;&#26159;&#26377;&#25928;&#30340;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents two novel approaches for the symplectic model reduction of high-dimensional Hamiltonian systems using data-driven quadratic manifolds. Classical symplectic model reduction approaches employ linear symplectic subspaces for representing the high-dimensional system states in a reduced-dimensional coordinate system. While these approximations respect the symplectic nature of Hamiltonian systems, the linearity of the approximation imposes a fundamental limitation to the accuracy that can be achieved. We propose two different model reduction methods based on recently developed quadratic manifolds, each presenting its own advantages and limitations. The addition of quadratic terms in the state approximation, which sits at the heart of the proposed methodologies, enables us to better represent intrinsic low-dimensionality in the problem at hand. Both approaches are effective for issuing predictions in settings well outside the range of their training data while providing mor
&lt;/p&gt;</description></item><item><title>PruMUX&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#32467;&#26500;&#21270;&#21098;&#26525;&#21644;&#25968;&#25454;&#22797;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;BERT-base&#27169;&#22411;&#30340;&#21534;&#21520;&#37327;&#12290;Auto-PruMUX&#21487;&#20197;&#39044;&#27979;&#20462;&#21098;&#21644;&#22797;&#29992;&#30340;&#39640;&#24615;&#33021;&#21442;&#25968;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.14706</link><description>&lt;p&gt;
PruMUX&#65306;&#21033;&#29992;&#27169;&#22411;&#21387;&#32553;&#22686;&#24378;&#25968;&#25454;&#22797;&#29992;
&lt;/p&gt;
&lt;p&gt;
PruMUX: Augmenting Data Multiplexing with Model Compression. (arXiv:2305.14706v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14706
&lt;/p&gt;
&lt;p&gt;
PruMUX&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#32467;&#26500;&#21270;&#21098;&#26525;&#21644;&#25968;&#25454;&#22797;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;BERT-base&#27169;&#22411;&#30340;&#21534;&#21520;&#37327;&#12290;Auto-PruMUX&#21487;&#20197;&#39044;&#27979;&#20462;&#21098;&#21644;&#22797;&#29992;&#30340;&#39640;&#24615;&#33021;&#21442;&#25968;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#26029;&#25193;&#22823;&#65292;&#25552;&#39640;&#20854;&#25512;&#29702;&#25928;&#29575;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290; &#20808;&#21069;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#27169;&#22411;&#20462;&#21098;&#65292;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22797;&#29992;&#31561;&#25216;&#26415;&#65292;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#21534;&#21520;&#37327;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#32467;&#26500;&#21270;&#21098;&#26525;&#21644;&#25968;&#25454;&#22797;&#29992;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#22686;&#24378;&#20004;&#31181;&#26041;&#27861;&#33719;&#24471;&#30340;&#21152;&#36895;&#20248;&#21183;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;PruMUX&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#38408;&#20540;&#20026;80&#65285;&#21040;74&#65285;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;BERT-base&#27169;&#22411;&#30456;&#27604;&#65292;&#21487;&#33719;&#24471;7.5-29.5&#20493;&#30340;&#21534;&#21520;&#37327;&#25552;&#39640;&#12290; &#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#20004;&#31181;&#25216;&#26415;&#20013;&#19981;&#21516;&#21442;&#25968;&#65288;&#20363;&#22914;&#31232;&#30095;&#24615;&#21644;&#22797;&#29992;&#22240;&#23376;&#65289;&#30340;&#21508;&#31181;&#32452;&#21512;&#65292;&#20197;&#25552;&#20379;&#26377;&#20851;&#20934;&#30830;&#24615;&#21644;&#21534;&#21520;&#37327;&#20043;&#38388;&#26435;&#34913;&#30340;&#32508;&#21512;&#20998;&#26512;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Auto-PruMUX&#65292;&#36825;&#26159;&#19968;&#31181;&#20803;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#26399;&#26395;&#30340;&#31934;&#24230;&#25439;&#22833;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#20462;&#21098;&#21644;&#22797;&#29992;&#30340;&#39640;&#24615;&#33021;&#21442;&#25968;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models increase in size by the day, methods for efficient inference are critical to leveraging their capabilities for various applications. Prior work has investigated techniques like model pruning, knowledge distillation, and data multiplexing to increase model throughput without sacrificing accuracy. In this paper, we combine two such methods -structured pruning and data multiplexing -- to compound the speedup gains obtained by either method. Our approach, PruMUX, obtains up to 7.5-29.5X throughput improvement over BERT-base model with accuracy threshold from 80% to 74%. We further study various combinations of parameters (such as sparsity and multiplexing factor) in the two techniques to provide a comprehensive analysis of the tradeoff between accuracy and throughput in the resulting models. We then propose Auto-PruMUX, a meta-level model that can predict the high-performance parameters for pruning and multiplexing given a desired accuracy loss budget, providing a prac
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#38598;&#33976;&#39311;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21512;&#25104;&#39640;&#20449;&#24687;&#23494;&#24230;&#30340;&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#25345;&#32493;&#23398;&#20064;&#12289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#36825;&#31687;&#32508;&#36848;&#24615;&#35843;&#26597;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#26041;&#27861;&#65292;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#65292;&#24182;&#31995;&#32479;&#22238;&#39038;&#20102;&#25968;&#25454;&#27169;&#24577;&#21644;&#30456;&#20851;&#24212;&#29992;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#25361;&#25112;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.01975</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#30740;&#31350;&#32508;&#36848;: &#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Survey on Dataset Distillation: Approaches, Applications and Future Directions. (arXiv:2305.01975v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01975
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21512;&#25104;&#39640;&#20449;&#24687;&#23494;&#24230;&#30340;&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#25345;&#32493;&#23398;&#20064;&#12289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#36825;&#31687;&#32508;&#36848;&#24615;&#35843;&#26597;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#26041;&#27861;&#65292;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#65292;&#24182;&#31995;&#32479;&#22238;&#39038;&#20102;&#25968;&#25454;&#27169;&#24577;&#21644;&#30456;&#20851;&#24212;&#29992;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#25361;&#25112;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35757;&#32451;&#38598;&#30340;&#19981;&#26029;&#22686;&#38271;&#21644;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#25104;&#26412;&#36234;&#26469;&#36234;&#39640;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36890;&#36807;&#21512;&#25104;&#39640;&#20449;&#24687;&#23494;&#24230;&#30340;&#25968;&#25454;&#38598;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#28508;&#22312;&#24212;&#29992;&#65292;&#21253;&#25324;&#25903;&#25345;&#25345;&#32493;&#23398;&#20064;&#12289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#26377;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#23545;&#26041;&#27861;&#21644;&#24212;&#29992;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#39318;&#20808;&#25552;&#20986;&#19968;&#31181;&#25968;&#25454;&#38598;&#33976;&#39311;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#29305;&#24449;&#21270;&#65292;&#28982;&#21518;&#31995;&#32479;&#22320;&#22238;&#39038;&#25968;&#25454;&#27169;&#24577;&#21644;&#30456;&#20851;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#25361;&#25112;&#24182;&#35752;&#35770;&#20102;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset distillation is attracting more attention in machine learning as training sets continue to grow and the cost of training state-of-the-art models becomes increasingly high. By synthesizing datasets with high information density, dataset distillation offers a range of potential applications, including support for continual learning, neural architecture search, and privacy protection. Despite recent advances, we lack a holistic understanding of the approaches and applications. Our survey aims to bridge this gap by first proposing a taxonomy of dataset distillation, characterizing existing approaches, and then systematically reviewing the data modalities, and related applications. In addition, we summarize the challenges and discuss future directions for this field of research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21407;&#23376;&#25991;&#20214;&#30340;&#32479;&#19968;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#23384;&#20648;&#26684;&#24335;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;LibCity&#30340;&#24320;&#28304;&#24211;&#65292;&#37325;&#26032;&#26500;&#24314;&#20102;65&#20010;&#31354;&#38388;&#26102;&#38388;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#25910;&#38598;&#20102;55&#20010;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#22522;&#20934;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2304.14343</link><description>&lt;p&gt;
&#23454;&#29616;&#39640;&#25928;&#21644;&#20840;&#38754;&#30340;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#24211;&#21644;&#24615;&#33021;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark. (arXiv:2304.14343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21407;&#23376;&#25991;&#20214;&#30340;&#32479;&#19968;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#23384;&#20648;&#26684;&#24335;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;LibCity&#30340;&#24320;&#28304;&#24211;&#65292;&#37325;&#26032;&#26500;&#24314;&#20102;65&#20010;&#31354;&#38388;&#26102;&#38388;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#25910;&#38598;&#20102;55&#20010;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#22522;&#20934;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#19981;&#26029;&#25512;&#36827;&#21644;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#30340;&#31215;&#32047;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#39046;&#22495;&#23384;&#22312;&#35768;&#22810;&#38480;&#21046;&#65292;&#21253;&#25324;&#24320;&#25918;&#25968;&#25454;&#20197;&#21508;&#31181;&#26684;&#24335;&#23384;&#22312;&#65292;&#20351;&#29992;&#22256;&#38590;&#65292;&#26497;&#23569;&#25968;&#35770;&#25991;&#20844;&#24320;&#20854;&#20195;&#30721;&#21644;&#25968;&#25454;&#65292;&#20197;&#21450;&#24320;&#28304;&#27169;&#22411;&#32463;&#24120;&#20351;&#29992;&#19981;&#21516;&#30340;&#26694;&#26550;&#21644;&#24179;&#21488;&#65292;&#20351;&#24471;&#27604;&#36739;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36843;&#20999;&#38656;&#35201;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#23454;&#26045;&#21644;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#30340;&#32508;&#21512;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21407;&#23376;&#25991;&#20214;&#30340;&#32479;&#19968;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#23384;&#20648;&#26684;&#24335;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LibCity&#30340;&#24320;&#28304;&#24211;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#23454;&#39564;&#24037;&#20855;&#21644;&#19968;&#20010;&#26041;&#20415;&#30340;&#24320;&#21457;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#24211;&#20013;&#65292;&#25105;&#20204;&#24050;&#32463;&#37325;&#26032;&#26500;&#24314;&#20102;65&#20010;&#31354;&#38388;&#26102;&#38388;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#25910;&#38598;&#20102;55&#20010;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#22522;&#20934;&#65292;&#21253;&#25324;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#24230;&#37327;&#65292;&#20197;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#32479;&#19968;&#24211;&#21644;&#22522;&#20934;&#30340;&#26377;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning technology advances and more urban spatial-temporal data accumulates, an increasing number of deep learning models are being proposed to solve urban spatial-temporal prediction problems. However, there are limitations in the existing field, including open-source data being in various formats and difficult to use, few papers making their code and data openly available, and open-source models often using different frameworks and platforms, making comparisons challenging. A standardized framework is urgently needed to implement and evaluate these methods. To address these issues, we provide a comprehensive review of urban spatial-temporal prediction and propose a unified storage format for spatial-temporal data called atomic files. We also propose LibCity, an open-source library that offers researchers a credible experimental tool and a convenient development framework. In this library, we have reproduced 65 spatial-temporal prediction models and collected 55 spatial-temp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#22238;&#39038;&#20102;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#8220;&#33258;&#30417;&#30563;&#20449;&#24687;&#29702;&#35770;&#23398;&#20064;&#38382;&#39064;&#8221;&#32479;&#19968;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#35752;&#35770;&#20102;&#21387;&#32553;&#24615;&#21644;&#21387;&#32553;&#31639;&#27861;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#20984;&#26174;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.09355</link><description>&lt;p&gt;
&#21387;&#32553;&#19982;&#21542;&#8212;&#8212;&#33258;&#30417;&#30563;&#23398;&#20064;&#19982;&#20449;&#24687;&#35770;:&#19968;&#31687;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
To Compress or Not to Compress -- Self-Supervised Learning and Information Theory: A Review. (arXiv:2304.09355v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#22238;&#39038;&#20102;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#8220;&#33258;&#30417;&#30563;&#20449;&#24687;&#29702;&#35770;&#23398;&#20064;&#38382;&#39064;&#8221;&#32479;&#19968;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#35752;&#35770;&#20102;&#21387;&#32553;&#24615;&#21644;&#21387;&#32553;&#31639;&#27861;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#20984;&#26174;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#33539;&#20363;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#26126;&#30830;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#12290;&#20449;&#24687;&#35770;&#22312;&#29702;&#35299;&#21644;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#34987;&#24212;&#29992;&#20110;&#22312;&#30417;&#30563;&#35774;&#32622;&#20013;&#20248;&#21270;&#21387;&#32553;&#21644;&#30456;&#20851;&#20449;&#24687;&#20445;&#23384;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#28982;&#32780;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26368;&#20339;&#20449;&#24687;&#30446;&#26631;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#22238;&#39038;&#20102;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#8220;&#33258;&#30417;&#30563;&#20449;&#24687;&#29702;&#35770;&#23398;&#20064;&#38382;&#39064;&#8221;&#32479;&#19968;&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30740;&#31350;&#34701;&#21512;&#25104;&#19968;&#20010;&#19968;&#33268;&#30340;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#26368;&#36817;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#30740;&#31350;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#21387;&#32553;&#24615;&#21644;&#21387;&#32553;&#31639;&#27861;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#20984;&#26174;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have demonstrated remarkable performance in supervised learning tasks but require large amounts of labeled data. Self-supervised learning offers an alternative paradigm, enabling the model to learn from data without explicit labels. Information theory has been instrumental in understanding and optimizing deep neural networks. Specifically, the information bottleneck principle has been applied to optimize the trade-off between compression and relevant information preservation in supervised settings. However, the optimal information objective in self-supervised learning remains unclear. In this paper, we review various approaches to self-supervised learning from an information-theoretic standpoint and present a unified framework that formalizes the \textit{self-supervised information-theoretic learning problem}. We integrate existing research into a coherent framework, examine recent self-supervised methods, and identify research opportunities and challenges. Moreove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32858;&#28966;&#20110;&#31446;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;BadVFL&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#21518;&#38376;&#27880;&#20837;VFL&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25104;&#21151;&#29575;&#39640;&#24182;&#19988;&#35823;&#20998;&#31867;&#29575;&#24456;&#20302;&#12290;</title><link>http://arxiv.org/abs/2304.08847</link><description>&lt;p&gt;
BadVFL: &#31446;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
BadVFL: Backdoor Attacks in Vertical Federated Learning. (arXiv:2304.08847v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32858;&#28966;&#20110;&#31446;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;BadVFL&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#21518;&#38376;&#27880;&#20837;VFL&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25104;&#21151;&#29575;&#39640;&#24182;&#19988;&#35823;&#20998;&#31867;&#29575;&#24456;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#22810;&#20010;&#21442;&#19982;&#32773;&#21487;&#20197;&#22312;&#19981;&#20849;&#20139;&#20854;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#20316;&#22320;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65307;&#30456;&#21453;&#65292;&#20182;&#20204;&#22312;&#26412;&#22320;&#35757;&#32451;&#33258;&#24049;&#30340;&#27169;&#22411;&#65292;&#24182;&#23558;&#26356;&#26032;&#21457;&#36865;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#36827;&#34892;&#32858;&#21512;&#12290;&#26681;&#25454;&#25968;&#25454;&#22312;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20998;&#24067;&#26041;&#24335;&#65292;FL&#21487;&#20197;&#20998;&#20026;&#27700;&#24179;&#65288;HFL&#65289;&#21644;&#31446;&#30452;&#65288;VFL&#65289;&#12290;&#22312;VFL&#20013;&#65292;&#21442;&#19982;&#32773;&#20849;&#20139;&#30456;&#21516;&#30340;&#35757;&#32451;&#23454;&#20363;&#38598;&#65292;&#20294;&#20165;&#25176;&#31649;&#25972;&#20010;&#29305;&#24449;&#31354;&#38388;&#30340;&#19981;&#21516;&#21644;&#38750;&#37325;&#21472;&#23376;&#38598;&#12290;&#32780;&#22312;HFL&#20013;&#65292;&#27599;&#20010;&#21442;&#19982;&#32773;&#20849;&#20139;&#30456;&#21516;&#30340;&#29305;&#24449;&#38598;&#65292;&#32780;&#35757;&#32451;&#38598;&#34987;&#20998;&#20026;&#26412;&#22320;&#25317;&#26377;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#12290;&#23613;&#31649;VFL&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#31561;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#20294;&#24456;&#23569;&#26377;&#24037;&#20316;&#20998;&#26512;&#20854;&#23433;&#20840;&#24615;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;VFL&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#21518;&#38376;&#25915;&#20987;&#65292;&#20854;&#20013;&#23545;&#25163;&#35797;&#22270;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25805;&#32437;&#32858;&#21512;&#27169;&#22411;&#20197;&#35302;&#21457;&#38169;&#35823;&#20998;&#31867;&#12290;&#22312;VFL&#19978;&#25191;&#34892;&#21518;&#38376;&#25915;&#20987;&#21487;&#20197;&#21019;&#24314;&#20005;&#37325;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#20801;&#35768;&#25915;&#20987;&#32773;&#26377;&#38024;&#23545;&#24615;&#22320;&#25511;&#21046;&#27169;&#22411;&#39044;&#27979;&#30340;&#32467;&#26524;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;VFL&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;&#65292;&#31216;&#20026;BadVFL&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#21518;&#38376;&#27880;&#20837;VFL&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#32771;&#34385;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#36824;&#36866;&#24212;&#20110;VFL&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#20013;&#24515;&#24230;&#37327;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#24178;&#20928;&#25968;&#25454;&#30340;&#20302;&#22833;&#30495;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables multiple parties to collaboratively train a machine learning model without sharing their data; rather, they train their own model locally and send updates to a central server for aggregation. Depending on how the data is distributed among the participants, FL can be classified into Horizontal (HFL) and Vertical (VFL). In VFL, the participants share the same set of training instances but only host a different and non-overlapping subset of the whole feature space. Whereas in HFL, each participant shares the same set of features while the training set is split into locally owned training data subsets.  VFL is increasingly used in applications like financial fraud detection; nonetheless, very little work has analyzed its security. In this paper, we focus on robustness in VFL, in particular, on backdoor attacks, whereby an adversary attempts to manipulate the aggregate model during the training process to trigger misclassifications. Performing backdoor attack
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20020;&#36817;&#36793;&#32536;&#26696;&#20363;&#30340;&#38754;&#37096;&#39564;&#35777;&#38382;&#39064;&#65292;&#21457;&#29616;&#32467;&#21512;&#20154;&#26426;&#20915;&#31574;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;&#38754;&#37096;&#39564;&#35777;&#31995;&#32479;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.08134</link><description>&lt;p&gt;
&#35299;&#20915;&#38754;&#37096;&#39564;&#35777;&#36793;&#32536;&#26696;&#20363;&#65306;&#28145;&#24230;&#20998;&#26512;&#21644;&#20154;&#26426;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tackling Face Verification Edge Cases: In-Depth Analysis and Human-Machine Fusion Approach. (arXiv:2304.08134v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20020;&#36817;&#36793;&#32536;&#26696;&#20363;&#30340;&#38754;&#37096;&#39564;&#35777;&#38382;&#39064;&#65292;&#21457;&#29616;&#32467;&#21512;&#20154;&#26426;&#20915;&#31574;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;&#38754;&#37096;&#39564;&#35777;&#31995;&#32479;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#38754;&#37096;&#35782;&#21035;&#31995;&#32479;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#24050;&#32463;&#36229;&#36807;&#20102;&#20154;&#31867;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#26426;&#22120;&#26080;&#27861;&#27491;&#30830;&#20998;&#31867;&#30340;&#36793;&#32536;&#26696;&#20363;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#21644;&#20154;&#25805;&#20316;&#21592;&#22312;&#38754;&#37096;&#39564;&#35777;&#20219;&#21153;&#20013;&#30340;&#32452;&#21512;&#25928;&#24212;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20180;&#32454;&#30740;&#31350;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#36793;&#32536;&#26696;&#20363;&#65292;&#20197;&#21457;&#29616;&#24120;&#35265;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#24615;&#35774;&#32622;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#36873;&#23450;&#20219;&#21153;&#20013;&#30340;60&#20010;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#19968;&#39033;&#20154;&#31867;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#26426;&#22120;&#21644;&#20154;&#31867;&#20915;&#31574;&#32467;&#21512;&#36215;&#26469;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;&#38754;&#37096;&#39564;&#35777;&#31995;&#32479;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;GitHub&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, face recognition systems surpass human performance on several datasets. However, there are still edge cases that the machine can't correctly classify. This paper investigates the effect of a combination of machine and human operators in the face verification task. First, we look closer at the edge cases for several state-of-the-art models to discover common datasets' challenging settings. Then, we conduct a study with 60 participants on these selected tasks with humans and provide an extensive analysis. Finally, we demonstrate that combining machine and human decisions can further improve the performance of state-of-the-art face verification systems on various benchmark datasets. Code and data are publicly available on GitHub.
&lt;/p&gt;</description></item><item><title>HyperTab&#26159;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#32467;&#21512;&#20102;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#20248;&#28857;&#30340;&#23567;&#22411;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#27599;&#20010;&#29305;&#23450;&#20302;&#32500;&#35270;&#22270;&#22788;&#29702;&#25968;&#25454;&#65292;&#34394;&#25311;&#22686;&#21152;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#65292;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2304.03543</link><description>&lt;p&gt;
HyperTab: &#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#23567;&#22411;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HyperTab: Hypernetwork Approach for Deep Learning on Small Tabular Datasets. (arXiv:2304.03543v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03543
&lt;/p&gt;
&lt;p&gt;
HyperTab&#26159;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#32467;&#21512;&#20102;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#20248;&#28857;&#30340;&#23567;&#22411;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#27599;&#20010;&#29305;&#23450;&#20302;&#32500;&#35270;&#22270;&#22788;&#29702;&#25968;&#25454;&#65292;&#34394;&#25311;&#22686;&#21152;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#65292;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#34920;&#29616;&#65292;&#20363;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20294;&#23427;&#22312;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20256;&#32479;&#27973;&#23618;&#26041;&#27861;&#30340;&#20248;&#21183;&#20173;&#28982;&#20540;&#24471;&#21830;&#27063;&#12290;&#22312;&#23567;&#22411;&#25968;&#25454;&#38598;&#65288;&#23567;&#20110;1k&#20010;&#26679;&#26412;&#65289;&#19978;&#36229;&#36807;&#26641;&#29366;&#38598;&#25104;&#65288;&#22914;XGBoost&#25110;&#38543;&#26426;&#26862;&#26519;&#65289;&#30340;&#34920;&#29616;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;HyperTab&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#35299;&#20915;&#34920;&#26684;&#25968;&#25454;&#38598;&#23567;&#26679;&#26412;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;HyperTab&#29983;&#25104;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#65292;&#20854;&#20013;&#27599;&#20010;&#30446;&#26631;&#27169;&#22411;&#19987;&#38376;&#22788;&#29702;&#25968;&#25454;&#30340;&#29305;&#23450;&#20302;&#32500;&#35270;&#22270;&#12290;&#30001;&#20110;&#27599;&#20010;&#35270;&#22270;&#25198;&#28436;&#25968;&#25454;&#22686;&#24378;&#30340;&#35282;&#33394;&#65292;&#25105;&#20204;&#22312;&#20445;&#25345;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#65292;&#34394;&#25311;&#22686;&#21152;&#20102;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36807;&#24230;&#25311;&#21512;&#12290;&#25105;&#20204;&#23545;40&#22810;&#20010;&#22823;&#23567;&#19981;&#21516;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#23545;HyperTab&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has achieved impressive performance in many domains, such as computer vision and natural language processing, but its advantage over classical shallow methods on tabular datasets remains questionable. It is especially challenging to surpass the performance of tree-like ensembles, such as XGBoost or Random Forests, on small-sized datasets (less than 1k samples). To tackle this challenge, we introduce HyperTab, a hypernetwork-based approach to solving small sample problems on tabular datasets. By combining the advantages of Random Forests and neural networks, HyperTab generates an ensemble of neural networks, where each target model is specialized to process a specific lower-dimensional view of the data. Since each view plays the role of data augmentation, we virtually increase the number of training samples while keeping the number of trainable parameters unchanged, which prevents model overfitting. We evaluated HyperTab on more than 40 tabular datasets of a varying number
&lt;/p&gt;</description></item><item><title>&#27492;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HALO&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#20010;&#23618;&#32423;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#12289;&#32454;&#31890;&#24230;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#65292;&#19988;&#26080;&#38656;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02169</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#32423;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#26497;&#39640;&#32500;&#38271;&#26399;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
Synthesize Extremely High-dimensional Longitudinal Electronic Health Records via Hierarchical Autoregressive Language Model. (arXiv:2304.02169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02169
&lt;/p&gt;
&lt;p&gt;
&#27492;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HALO&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#20010;&#23618;&#32423;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#12289;&#32454;&#31890;&#24230;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#65292;&#19988;&#26080;&#38656;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHRs)&#33021;&#22815;&#22312;&#26426;&#22120;&#23398;&#20064;(ML)&#21644;&#32479;&#35745;&#20998;&#26512;&#20013;&#20316;&#20026;&#30495;&#23454;EHRs&#30340;&#26367;&#20195;&#21697;&#65292;&#26082;&#30495;&#23454;&#21448;&#20445;&#25252;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#65292;&#20197;&#20854;&#21407;&#22987;&#39640;&#24230;&#32500;&#24418;&#24335;&#29983;&#25104;&#39640;&#20445;&#30495;&#12289;&#32454;&#31890;&#24230;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHR)&#25968;&#25454;&#23545;&#29616;&#26377;&#26041;&#27861;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Hierarchical Autoregressive Language mOdel (HALO)&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#32437;&#21521;&#39640;&#32500;EHR&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#30495;&#23454;EHR&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#32780;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;HALO&#26041;&#27861;&#34987;&#35774;&#35745;&#20026;&#19968;&#20010;&#23618;&#32423;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#29983;&#25104;&#19968;&#32452;&#38024;&#23545;&#21307;&#23398;&#20195;&#30721;&#12289;&#20020;&#24202;&#23601;&#35786;&#21644;&#30149;&#20154;&#35760;&#24405;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#20854;&#21407;&#22987;&#26410;&#32858;&#21512;&#24418;&#24335;&#19979;&#29983;&#25104;&#30495;&#23454;&#30340;EHR&#25968;&#25454;&#65292;&#26080;&#38656;&#36827;&#34892;&#21464;&#37327;&#36873;&#25321;&#25110;&#32858;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#20135;&#29983;&#22823;&#37327;&#30340;&#38543;&#26426;&#26679;&#26412;&#65292;&#20197;&#25552;&#20379;&#22797;&#26434;&#24230;&#36739;&#20302;&#20294;&#20173;&#26377;&#24847;&#20041;&#30340;EHR&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic electronic health records (EHRs) that are both realistic and preserve privacy can serve as an alternative to real EHRs for machine learning (ML) modeling and statistical analysis. However, generating high-fidelity and granular electronic health record (EHR) data in its original, highly-dimensional form poses challenges for existing methods due to the complexities inherent in high-dimensional data. In this paper, we propose Hierarchical Autoregressive Language mOdel (HALO) for generating longitudinal high-dimensional EHR, which preserve the statistical properties of real EHR and can be used to train accurate ML models without privacy concerns. Our HALO method, designed as a hierarchical autoregressive model, generates a probability density function of medical codes, clinical visits, and patient records, allowing for the generation of realistic EHR data in its original, unaggregated form without the need for variable selection or aggregation. Additionally, our model also produc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39044;&#27979;&#35823;&#24046;&#21442;&#25968;&#21270;&#20026;&#22810;&#20010;&#26102;&#38388;&#27493;&#38271;&#65292;&#20197;&#25214;&#21040;&#19981;&#20445;&#23432;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#23454;&#29616;&#22312;&#20351;&#29992;&#23398;&#20064;&#21551;&#29992;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#36827;&#34892;&#38271;&#26399;&#35268;&#21010;&#21644;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.01075</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#20114;&#34917;&#32534;&#31243;&#30340;&#26102;&#24207;&#31526;&#21512;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction Regions for Time Series using Linear Complementarity Programming. (arXiv:2304.01075v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39044;&#27979;&#35823;&#24046;&#21442;&#25968;&#21270;&#20026;&#22810;&#20010;&#26102;&#38388;&#27493;&#38271;&#65292;&#20197;&#25214;&#21040;&#19981;&#20445;&#23432;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#23454;&#29616;&#22312;&#20351;&#29992;&#23398;&#20064;&#21551;&#29992;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#36827;&#34892;&#38271;&#26399;&#35268;&#21010;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21512;&#39044;&#27979;&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#21306;&#38388;&#30340;&#32479;&#35745;&#24037;&#20855;&#65292;&#20854;&#20855;&#26377;&#39640;&#27010;&#29575;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23558;&#31526;&#21512;&#39044;&#27979;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20250;&#23548;&#33268;&#20445;&#23432;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#31181;&#20445;&#23432;&#24615;&#65292;&#20197;&#20415;&#22312;&#20351;&#29992;&#23398;&#20064;&#21551;&#29992;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#36827;&#34892;&#38271;&#26399;&#35268;&#21010;&#21644;&#39564;&#35777;&#12290;&#25105;&#20204;&#23558;&#39044;&#27979;&#35823;&#24046;&#21442;&#25968;&#21270;&#20026;&#22810;&#20010;&#26102;&#38388;&#27493;&#38271;&#65292;&#36890;&#36807;&#23545;&#39069;&#22806;&#25968;&#25454;&#38598;&#19978;&#30340;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#25214;&#21040;&#20102;&#19981;&#20445;&#23432;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#35813;&#38382;&#39064;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#20114;&#34917;&#35268;&#21010;&#65288;MILCP&#65289;&#65292;&#25105;&#20204;&#23558;&#20854;&#25918;&#23485;&#20026;&#19968;&#20010;&#32447;&#24615;&#20114;&#34917;&#35268;&#21010;&#65288;LCP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction is a statistical tool for producing prediction regions of machine learning models that are valid with high probability. However, applying conformal prediction to time series data leads to conservative prediction regions. In fact, to obtain prediction regions over $T$ time steps with confidence $1-\delta$, {previous works require that each individual prediction region is valid} with confidence $1-\delta/T$. We propose an optimization-based method for reducing this conservatism to enable long horizon planning and verification when using learning-enabled time series predictors. Instead of considering prediction errors individually at each time step, we consider a parameterized prediction error over multiple time steps. By optimizing the parameters over an additional dataset, we find prediction regions that are not conservative. We show that this problem can be cast as a mixed integer linear complementarity program (MILCP), which we then relax into a linear complementa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#37322;&#31354;&#38388;&#30340;&#31639;&#27861;&#26041;&#27861;&#65292;&#27979;&#37327;&#20998;&#32452;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#30340;&#36829;&#35268;&#24773;&#20917;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#26816;&#26597;&#32452;&#38388;&#27495;&#35270;&#30340;&#21407;&#22240;&#65292;&#25552;&#39640;&#20102;&#23457;&#35745;&#20844;&#24179;&#24615;&#30340;&#25935;&#24863;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.08040</link><description>&lt;p&gt;
&#12298;&#20154;&#21475;&#32479;&#35745;&#24179;&#31561;&#26816;&#26597;&#21592;&#65306;&#36890;&#36807;&#35299;&#37322;&#31354;&#38388;&#36827;&#34892;&#20844;&#24179;&#23457;&#26680;&#12299;
&lt;/p&gt;
&lt;p&gt;
Demographic Parity Inspector: Fairness Audits via the Explanation Space. (arXiv:2303.08040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08040
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#37322;&#31354;&#38388;&#30340;&#31639;&#27861;&#26041;&#27861;&#65292;&#27979;&#37327;&#20998;&#32452;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#30340;&#36829;&#35268;&#24773;&#20917;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#26816;&#26597;&#32452;&#38388;&#27495;&#35270;&#30340;&#21407;&#22240;&#65292;&#25552;&#39640;&#20102;&#23457;&#35745;&#20844;&#24179;&#24615;&#30340;&#25935;&#24863;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#20855;&#26377;&#26368;&#22909;&#30340;&#24847;&#22270;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20063;&#21487;&#33021;&#24310;&#32493;&#12289;&#25918;&#22823;&#29978;&#33267;&#21019;&#36896;&#31038;&#20250;&#20559;&#35265;&#12290;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27495;&#35270;&#24615;&#65288;&#38750;&#27495;&#35270;&#24615;&#65289;&#30340;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#23548;&#33268;&#27495;&#35270;&#25928;&#26524;&#30340;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20195;&#29702;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#27979;&#37327;&#20998;&#32452;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#30340;&#36829;&#35268;&#24773;&#20917;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#26816;&#26597;&#32452;&#38388;&#27495;&#35270;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#24605;&#24819;&#65292;&#21363;&#22522;&#20110;&#35299;&#37322;&#31354;&#38388;&#23545;&#27169;&#22411;&#23545;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20381;&#36182;&#24230;&#36827;&#34892;&#27979;&#37327;&#65292;&#35299;&#37322;&#31354;&#38388;&#26159;&#19968;&#31181;&#25552;&#20379;&#27604;&#36755;&#20837;&#25968;&#25454;&#25110;&#39044;&#27979;&#20998;&#24067;&#30340;&#21407;&#22987;&#31354;&#38388;&#26356;&#25935;&#24863;&#23457;&#35745;&#30340;&#20449;&#24687;&#31354;&#38388;&#65292;&#20174;&#32780;&#20801;&#35768;&#26029;&#35328;&#29702;&#35770;&#19978;&#30340;&#20154;&#21475;&#32479;&#35745;&#23457;&#26680;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#23398;&#20998;&#26512;&#12289;&#21512;&#25104;&#26679;&#20363;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;Pytorch&#23454;&#29616;&#21644;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;Web&#24212;&#29992;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even if deployed with the best intentions, machine learning methods can perpetuate, amplify or even create social biases. Measures of (un-)fairness have been proposed as a way to gauge the (non-)discriminatory nature of machine learning models. However, proxies of protected attributes causing discriminatory effects remain challenging to address. In this work, we propose a new algorithmic approach that measures group-wise demographic parity violations and allows us to inspect the causes of inter-group discrimination. Our method relies on the novel idea of measuring the dependence of a model on the protected attribute based on the explanation space, an informative space that allows for more sensitive audits than the primary space of input data or prediction distributions, and allowing for the assertion of theoretical demographic parity auditing guarantees. We provide a mathematical analysis, synthetic examples, and experimental evaluation of real-world data. We release an open-source Pyt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Uni-RXN&#26694;&#26550;&#65292;&#22312;&#21270;&#23398;&#21453;&#24212;Pretraining&#21644;&#20998;&#23376;&#29983;&#25104;&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#20855;&#22791;&#21270;&#23398;&#30693;&#35782;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#20165;&#20381;&#36182;&#23569;&#37327;&#21453;&#24212;&#27169;&#26495;&#30340;&#38480;&#21046;&#65292;&#29983;&#25104;&#36136;&#37327;&#39640;&#12289;&#21487;&#21512;&#25104;&#30340;&#33647;&#29289;&#31867;&#20998;&#23376;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2303.06965</link><description>&lt;p&gt;
Uni-RXN: &#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#24357;&#21512;&#21270;&#23398;&#21453;&#24212;Pretraining&#21644;&#26465;&#20214;&#20998;&#23376;&#29983;&#25104;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Uni-RXN: A Unified Framework Bridging the Gap between Chemical Reaction Pretraining and Conditional Molecule Generation. (arXiv:2303.06965v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Uni-RXN&#26694;&#26550;&#65292;&#22312;&#21270;&#23398;&#21453;&#24212;Pretraining&#21644;&#20998;&#23376;&#29983;&#25104;&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#20855;&#22791;&#21270;&#23398;&#30693;&#35782;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#20165;&#20381;&#36182;&#23569;&#37327;&#21453;&#24212;&#27169;&#26495;&#30340;&#38480;&#21046;&#65292;&#29983;&#25104;&#36136;&#37327;&#39640;&#12289;&#21487;&#21512;&#25104;&#30340;&#33647;&#29289;&#31867;&#20998;&#23376;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#21453;&#24212;&#26159;&#33647;&#29289;&#35774;&#35745;&#21644;&#26377;&#26426;&#21270;&#23398;&#30740;&#31350;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#19968;&#20010;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#21270;&#23398;&#21453;&#24212;&#22522;&#26412;&#35268;&#21017;&#30340;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#21453;&#24212;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#23376;&#29983;&#25104;&#20219;&#21153;&#65292;&#20801;&#35768;&#26356;&#25972;&#20307;&#30340;&#26041;&#27861;&#12290;&#21463;&#26377;&#26426;&#21270;&#23398;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#24402;&#32435;&#20559;&#35265;&#32435;&#20837;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#20855;&#22791;&#21270;&#23398;&#30693;&#35782;&#65292;&#35813;&#26694;&#26550;&#21487;&#24212;&#29992;&#20110;&#22522;&#20110;&#21453;&#24212;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#20165;&#20381;&#36182;&#23569;&#37327;&#21453;&#24212;&#27169;&#26495;&#30340;&#38480;&#21046;&#12290;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#20102;&#39640;&#36136;&#37327;&#30340;&#21487;&#21512;&#25104;&#33647;&#29289;&#31867;&#20998;&#23376;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chemical reactions are the fundamental building blocks of drug design and organic chemistry research. In recent years, there has been a growing need for a large-scale deep-learning framework that can efficiently capture the basic rules of chemical reactions. In this paper, we have proposed a unified framework that addresses both the reaction representation learning and molecule generation tasks, which allows for a more holistic approach. Inspired by the organic chemistry mechanism, we develop a novel pretraining framework that enables us to incorporate inductive biases into the model. Our framework achieves state-of-the-art results on challenging downstream tasks. By possessing chemical knowledge, this framework can be applied to reaction-based generative models, overcoming the limitations of current molecule generation models that rely on a small number of reaction templates. In the extensive experiments, our model generates synthesizable drug-like structures of high quality. Overall,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#39044;&#35757;&#32451;&#30340;GAN&#21644;VAE&#27169;&#22411;&#20013;&#28040;&#38500;&#29305;&#23450;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05699</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;GAN&#21644;VAE&#30340;&#29305;&#24449;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
Feature Unlearning for Pre-trained GANs and VAEs. (arXiv:2303.05699v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#39044;&#35757;&#32451;&#30340;GAN&#21644;VAE&#27169;&#22411;&#20013;&#28040;&#38500;&#29305;&#23450;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20174;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;GAN&#21644;VAE&#65289;&#20013;&#28040;&#38500;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;&#19982;&#24120;&#35265;&#30340;&#28040;&#38500;&#20219;&#21153;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20174;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#28040;&#38500;&#29305;&#23450;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#38754;&#37096;&#22270;&#20687;&#20013;&#30340;&#21457;&#22411;&#12290;&#30001;&#20110;&#30446;&#26631;&#29305;&#24449;&#20165;&#20986;&#29616;&#22312;&#22270;&#20687;&#30340;&#23616;&#37096;&#21306;&#22495;&#20013;&#65292;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#28040;&#38500;&#25972;&#20010;&#22270;&#20687;&#21487;&#33021;&#23548;&#33268;&#22833;&#21435;&#22270;&#20687;&#21097;&#20313;&#21306;&#22495;&#20013;&#30340;&#20854;&#20182;&#32454;&#33410;&#12290;&#20026;&#20102;&#25351;&#23450;&#35201;&#28040;&#38500;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#25910;&#38598;&#21253;&#21547;&#30446;&#26631;&#29305;&#24449;&#30340;&#38543;&#26426;&#29983;&#25104;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35782;&#21035;&#19982;&#30446;&#26631;&#29305;&#24449;&#23545;&#24212;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#34920;&#31034;&#26469;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;MNIST&#21644;CelebA&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25104;&#21151;&#21024;&#38500;&#30446;&#26631;&#29305;&#24449;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;&#36827;&#19968;&#27493;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#23454;&#39564;&#35777;&#26126;&#20102;&#28040;&#38500;&#21518;&#30340;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the problem of feature unlearning from a pre-trained image generative model: GANs and VAEs. Unlike a common unlearning task where an unlearning target is a subset of the training set, we aim to unlearn a specific feature, such as hairstyle from facial images, from the pre-trained generative models. As the target feature is only presented in a local region of an image, unlearning the entire image from the pre-trained model may result in losing other details in the remaining region of the image. To specify which features to unlearn, we collect randomly generated images that contain the target features. We then identify a latent representation corresponding to the target feature and then use the representation to fine-tune the pre-trained model. Through experiments on MNIST and CelebA datasets, we show that target features are successfully removed while keeping the fidelity of the original models. Further experiments with an adversarial attack show that the unlearned model is mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#21270;&#26080;&#32447;&#30005;&#22270;&#20272;&#35745;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24352;&#37327;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#26080;&#32447;&#30005;&#22270;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#26694;&#26550;&#26469;&#23454;&#29616;&#20174;&#39640;&#24230;&#37327;&#21270;&#30340;&#20256;&#24863;&#22120;&#27979;&#37327;&#20540;&#20013;&#24674;&#22797;&#26080;&#32447;&#30005;&#22320;&#22270;&#12290;</title><link>http://arxiv.org/abs/2303.01770</link><description>&lt;p&gt;
&#20351;&#29992;&#24352;&#37327;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#37327;&#21270;&#26080;&#32447;&#30005;&#22270;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Quantized Radio Map Estimation Using Tensor and Deep Generative Models. (arXiv:2303.01770v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#21270;&#26080;&#32447;&#30005;&#22270;&#20272;&#35745;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24352;&#37327;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#26080;&#32447;&#30005;&#22270;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#26694;&#26550;&#26469;&#23454;&#29616;&#20174;&#39640;&#24230;&#37327;&#21270;&#30340;&#20256;&#24863;&#22120;&#27979;&#37327;&#20540;&#20013;&#24674;&#22797;&#26080;&#32447;&#30005;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39057;&#35889;&#21046;&#22270;&#65288;SC&#65289;&#65292;&#20063;&#31216;&#20026;&#26080;&#32447;&#30005;&#22270;&#20272;&#35745;&#65288;RME&#65289;&#65292;&#26088;&#22312;&#20174;&#26377;&#38480;&#30340;&#20256;&#24863;&#22120;&#27979;&#37327;&#25968;&#25454;&#20013;&#21046;&#20316;&#22810;&#39046;&#22495;&#65288;&#20363;&#22914;&#39057;&#29575;&#21644;&#31354;&#38388;&#65289;&#26080;&#32447;&#30005;&#21151;&#29575;&#20256;&#25773;&#22270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#20302;&#32500;&#27169;&#22411;&#65288;&#22914;&#22359;&#29366;&#24352;&#37327;&#20998;&#35299;&#65288;BTD&#65289;&#27169;&#22411;&#21644;&#26576;&#20123;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;DGM&#65289;&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;&#29702;&#35770;&#20445;&#35777;&#24674;&#22797;&#26080;&#32447;&#30005;&#22320;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#30340;&#21487;&#35777;&#26126;SC&#26041;&#27861;&#20551;&#35774;&#20256;&#24863;&#22120;&#23558;&#23454;&#20540;&#65288;&#20840;&#20998;&#36776;&#29575;&#65289;&#27979;&#37327;&#20540;&#21457;&#36865;&#33267;&#34701;&#21512;&#20013;&#24515;&#65292;&#36825;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#21270;SC&#26694;&#26550;&#65292;&#23558;&#22522;&#20110;BTD&#21644;&#22522;&#20110;DGM&#30340;SC&#25512;&#24191;&#21040;&#20351;&#29992;&#39640;&#24230;&#37327;&#21270;&#30340;&#20256;&#24863;&#22120;&#27979;&#37327;&#20540;&#30340;&#24773;&#20917;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#30340;SC&#26694;&#26550;&#65292;&#22312;&#39640;&#26031;&#37327;&#21270;&#22120;&#30340;&#20316;&#29992;&#19979;&#23545;&#26080;&#32447;&#30005;&#22320;&#22270;&#30340;&#21487;&#24674;&#22797;&#24615;&#36827;&#34892;&#20102;&#21051;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectrum cartography (SC), also known as radio map estimation (RME), aims at crafting multi-domain (e.g., frequency and space) radio power propagation maps from limited sensor measurements. While early methods often lacked theoretical support, recent works have demonstrated that radio maps can be provably recovered using low-dimensional models -- such as the block-term tensor decomposition (BTD) model and certain deep generative models (DGMs) -- of the high-dimensional multi-domain radio signals. However, these existing provable SC approaches assume that sensors send real-valued (full-resolution) measurements to the fusion center, which is unrealistic. This work puts forth a quantized SC framework that generalizes the BTD and DGM-based SC to scenarios where heavily quantized sensor measurements are used. A maximum likelihood estimation (MLE)-based SC framework under a Gaussian quantizer is proposed. Recoverability of the radio map using the MLE criterion are characterized under realist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#19979;&#20256;&#24863;&#22120;&#25918;&#32622;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#31163;&#25955;&#21270;&#29615;&#22659;&#24182;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21487;&#36890;&#36807;&#36138;&#23146;&#31639;&#27861;&#25214;&#21040;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.00028</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#30340;&#36830;&#32493;&#21644;&#31163;&#25955;&#31354;&#38388;&#30340;&#22238;&#24402;&#20256;&#24863;&#22120;&#25918;&#32622;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Sensor Placement from Regression with Sparse Gaussian Processes in Continuous and Discrete Spaces. (arXiv:2303.00028v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#19979;&#20256;&#24863;&#22120;&#25918;&#32622;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#31163;&#25955;&#21270;&#29615;&#22659;&#24182;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21487;&#36890;&#36807;&#36138;&#23146;&#31639;&#27861;&#25214;&#21040;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#26041;&#27861;&#30340;&#20256;&#24863;&#22120;&#25918;&#32622;&#26041;&#26696;&#65292;&#29992;&#20110;&#30417;&#27979;&#28201;&#24230;&#12289;&#38477;&#27700;&#31561;&#31354;&#38388;&#65288;&#25110;&#26102;&#31354;&#65289;&#30456;&#20851;&#29616;&#35937;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#20256;&#24863;&#22120;&#25918;&#32622;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#24050;&#30693;&#20869;&#26680;&#20989;&#25968;&#21442;&#25968;&#30340;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#25311;&#21512;&#21040;&#29615;&#22659;&#20013;&#38543;&#26426;&#37319;&#26679;&#30340;&#26410;&#26631;&#35760;&#20301;&#32622;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#24471;&#21040;&#30340;&#35825;&#23548;&#28857;&#26469;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#30340;&#20256;&#24863;&#22120;&#25918;&#32622;&#38382;&#39064;&#12290;&#20351;&#29992;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#36991;&#20813;&#20102;&#23545;&#29615;&#22659;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#24182;&#23558;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;&#31435;&#26041;&#32423;&#21035;&#38477;&#20302;&#21040;&#32447;&#24615;&#32423;&#21035;&#12290;&#22312;&#20505;&#36873;&#20256;&#24863;&#22120;&#25918;&#32622;&#28857;&#38598;&#21512;&#30340;&#38480;&#21046;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#36138;&#23146;&#39034;&#24207;&#36873;&#25321;&#31639;&#27861;&#26469;&#25214;&#21040;&#36739;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach based on sparse Gaussian processes (SGPs) to address the sensor placement problem for monitoring spatially (or spatiotemporally) correlated phenomena such as temperature and precipitation. Existing Gaussian process (GP) based sensor placement approaches use GPs with known kernel function parameters to model a phenomenon and subsequently optimize the sensor locations in a discretized representation of the environment. In our approach, we fit an SGP with known kernel function parameters to randomly sampled unlabeled locations in the environment and show that the learned inducing points of the SGP inherently solve the sensor placement problem in continuous spaces. Using SGPs avoids discretizing the environment and reduces the computation cost from cubic to linear complexity. When restricted to a candidate set of sensor placement locations, we can use greedy sequential selection algorithms on the SGP's optimization bound to find good solutions. We also present a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38544;&#31169;&#38382;&#39064;&#21644;&#26377;&#38480;&#36890;&#20449;&#33021;&#21147;&#30340;&#22810;&#20010;&#29992;&#25143;&#30340;&#21327;&#20316;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#65292;&#22312;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#35270;&#35282;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#31163;&#25955;&#20540;&#26426;&#21046;&#30340;&#32039;&#23494;$f$-&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#65292;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#38544;&#31169;&#25918;&#22823;&#30340;&#31232;&#30095;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.09624</link><description>&lt;p&gt;
&#36890;&#36807;$f$-&#24046;&#20998;&#38544;&#31169;&#25171;&#30772;&#36890;&#20449;-&#38544;&#31169;-&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Breaking the Communication-Privacy-Accuracy Tradeoff with $f$-Differential Privacy. (arXiv:2302.09624v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38544;&#31169;&#38382;&#39064;&#21644;&#26377;&#38480;&#36890;&#20449;&#33021;&#21147;&#30340;&#22810;&#20010;&#29992;&#25143;&#30340;&#21327;&#20316;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#65292;&#22312;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#35270;&#35282;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#31163;&#25955;&#20540;&#26426;&#21046;&#30340;&#32039;&#23494;$f$-&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#65292;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#38544;&#31169;&#25918;&#22823;&#30340;&#31232;&#30095;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#32852;&#37030;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#21327;&#35843;&#20855;&#26377;&#38544;&#31169;&#38382;&#39064;&#21644;&#26377;&#38480;&#36890;&#20449;&#33021;&#21147;&#30340;&#22810;&#20010;&#29992;&#25143;&#30340;&#21327;&#20316;&#25968;&#25454;&#20998;&#26512;&#12290;&#36890;&#24120;&#37319;&#29992;&#30340;&#21387;&#32553;&#26041;&#26696;&#22312;&#25913;&#21892;&#36890;&#20449;&#25928;&#29575;&#30340;&#21516;&#26102;&#24341;&#20837;&#20102;&#23616;&#37096;&#25968;&#25454;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#20294;&#36825;&#20123;&#31163;&#25955;&#20540;&#26426;&#21046;&#26159;&#21542;&#25552;&#20379;&#20102;&#20219;&#20309;&#38544;&#31169;&#20445;&#25252;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;$f$-&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#36755;&#20986;&#31354;&#38388;&#30340;&#31163;&#25955;&#20540;&#26426;&#21046;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#20986;&#21508;&#31181;&#31163;&#25955;&#20540;&#26426;&#21046;&#30340;&#32039;&#23494;$f$-&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#65292;&#21253;&#25324;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;&#20108;&#39033;&#22122;&#22768;&#21644;&#20108;&#39033;&#26426;&#21046;&#20197;&#21450;&#29992;&#20110;&#25968;&#25454;&#21387;&#32553;&#30340;&#22522;&#20110;&#31526;&#21495;&#30340;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#31232;&#30095;&#21270;&#23545;&#38544;&#31169;&#30340;&#25918;&#22823;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a federated data analytics problem in which a server coordinates the collaborative data analysis of multiple users with privacy concerns and limited communication capability. The commonly adopted compression schemes introduce information loss into local data while improving communication efficiency, and it remains an open problem whether such discrete-valued mechanisms provide any privacy protection. In this paper, we study the local differential privacy guarantees of discrete-valued mechanisms with finite output space through the lens of $f$-differential privacy (DP). More specifically, we advance the existing literature by deriving tight $f$-DP guarantees for a variety of discrete-valued mechanisms, including the binomial noise and the binomial mechanisms that are proposed for privacy preservation, and the sign-based methods that are proposed for data compression, in closed-form expressions. We further investigate the amplification in privacy by sparsification and propose
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;PINN&#22312;&#35757;&#32451;&#22495;&#20043;&#22806;&#30340;&#39044;&#27979;&#34892;&#20026;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#35780;&#20272;&#20102;&#31639;&#27861;&#35774;&#32622;&#23545;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#25552;&#20379;&#20102;&#26377;&#35265;&#22320;&#19988;&#26377;&#26102;&#30452;&#35266;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2302.07557</link><description>&lt;p&gt;
&#20851;&#20110;PINNs&#22312;&#35757;&#32451;&#22495;&#20043;&#22806;&#30340;&#27867;&#21270;&#21450;&#20854;&#24433;&#21709;&#30340;&#36229;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
On the Generalization of PINNs outside the training domain and the Hyperparameters influencing it. (arXiv:2302.07557v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;PINN&#22312;&#35757;&#32451;&#22495;&#20043;&#22806;&#30340;&#39044;&#27979;&#34892;&#20026;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#35780;&#20272;&#20102;&#31639;&#27861;&#35774;&#32622;&#23545;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#25552;&#20379;&#20102;&#26377;&#35265;&#22320;&#19988;&#26377;&#26102;&#30452;&#35266;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#35757;&#32451;&#26469;&#27169;&#25311;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65292;&#32780;&#26080;&#38656;&#35299;&#20915;&#25968;&#25454;&#12290;&#30001;&#20110;&#20854;&#28789;&#27963;&#21644;&#26377;&#21069;&#26223;&#30340;&#35774;&#32622;&#65292;PINNs&#30446;&#21069;&#22312;&#31185;&#23398;&#25991;&#29486;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21487;&#29992;&#30340;&#30740;&#31350;&#24456;&#23569;&#25552;&#20379;&#23454;&#38469;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#23450;&#37327;&#29702;&#35299;&#36825;&#31181;&#26550;&#26500;&#21450;&#20854;&#21151;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;PINN&#22312;&#20854;&#35757;&#32451;&#22495;&#20043;&#22806;&#30340;&#39044;&#27979;&#34892;&#20026;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#30740;&#31350;PINN&#22312;&#25552;&#20379;&#19968;&#33268;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#30340;&#22330;&#26223;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;PINNs&#30340;&#31639;&#27861;&#35774;&#32622;&#26159;&#21542;&#20250;&#24433;&#21709;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#39044;&#27979;&#30340;&#30456;&#24212;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#26377;&#35265;&#22320;&#19988;&#26377;&#26102;&#30452;&#35266;&#30340;&#35266;&#28857;&#65292;&#23545;&#20110;&#36825;&#31181;&#26550;&#26500;&#30340;&#30456;&#20851;&#24615;&#21487;&#33021;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks (PINNs) are Neural Network architectures trained to emulate solutions of differential equations without the necessity of solution data. They are currently ubiquitous in the scientific literature due to their flexible and promising settings. However, very little of the available research provides practical studies that aim for a better quantitative understanding of such architecture and its functioning. In this paper, we perform an empirical analysis of the behavior of PINN predictions outside their training domain. The primary goal is to investigate the scenarios in which a PINN can provide consistent predictions outside the training area. Thereinafter, we assess whether the algorithmic setup of PINNs can influence their potential for generalization and showcase the respective effect on the prediction. The results obtained in this study returns insightful and at times counterintuitive perspectives which can be highly relevant for architectures which com
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#65288;UAP&#65289;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35266;&#23519;UAPs&#29983;&#25104;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#21457;&#29616;&#21518;&#38376;&#27169;&#22411;&#21482;&#38656;&#35201;&#36739;&#23569;&#30340;&#25200;&#21160;&#21363;&#21487;&#27450;&#39575;&#27169;&#22411;&#65292;&#32780;&#24178;&#20928;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#25200;&#21160;&#12290;&#36825;&#19968;&#21457;&#29616;&#21487;&#20197;&#29992;&#26469;&#21306;&#20998;&#24178;&#20928;&#27169;&#22411;&#21644;&#21518;&#38376;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.00747</link><description>&lt;p&gt;
&#36890;&#29992;&#22763;&#20853;&#65306;&#21033;&#29992;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#26469;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Universal Soldier: Using Universal Adversarial Perturbations for Detecting Backdoor Attacks. (arXiv:2302.00747v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#65288;UAP&#65289;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35266;&#23519;UAPs&#29983;&#25104;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#21457;&#29616;&#21518;&#38376;&#27169;&#22411;&#21482;&#38656;&#35201;&#36739;&#23569;&#30340;&#25200;&#21160;&#21363;&#21487;&#27450;&#39575;&#27169;&#22411;&#65292;&#32780;&#24178;&#20928;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#25200;&#21160;&#12290;&#36825;&#19968;&#21457;&#29616;&#21487;&#20197;&#29992;&#26469;&#21306;&#20998;&#24178;&#20928;&#27169;&#22411;&#21644;&#21518;&#38376;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26159;&#23427;&#20204;&#38754;&#20020;&#30528;&#35832;&#22914;&#23545;&#25239;&#31034;&#20363;&#21644;&#20013;&#27602;&#65288;&#21518;&#38376;&#65289;&#25915;&#20987;&#31561;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#36890;&#36807;&#20351;&#29992;&#24102;&#26377;&#21518;&#38376;&#25968;&#25454;&#30340;&#35757;&#32451;&#25110;&#20462;&#25913;&#20869;&#37096;&#32593;&#32476;&#21442;&#25968;&#26469;&#20013;&#27602;&#12290;&#28982;&#21518;&#65292;&#24403;&#25509;&#25910;&#21040;&#24178;&#20928;&#36755;&#20837;&#26102;&#65292;&#21518;&#38376;&#27169;&#22411;&#34920;&#29616;&#22914;&#39044;&#26399;&#65292;&#20294;&#26159;&#24403;&#25509;&#25910;&#21040;&#24102;&#26377;&#39044;&#20808;&#35774;&#35745;&#30340;"&#35302;&#21457;&#22120;"&#30340;&#21518;&#38376;&#36755;&#20837;&#26102;&#65292;&#23427;&#20250;&#38169;&#35823;&#20998;&#31867;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22914;&#26524;&#27809;&#26377;&#20107;&#20808;&#20102;&#35299;&#35302;&#21457;&#22120;&#65292;&#24456;&#38590;&#21306;&#20998;&#24178;&#20928;&#27169;&#22411;&#21644;&#21518;&#38376;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#38376;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#8212;&#8212;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#65288;UAP&#65289;&#21450;&#20854;&#19982;&#21518;&#38376;&#35302;&#21457;&#22120;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#20010;&#30452;&#35266;&#30340;&#29616;&#35937;&#65306;&#20174;&#21518;&#38376;&#27169;&#22411;&#29983;&#25104;&#30340;UAP&#27604;&#20174;&#24178;&#20928;&#27169;&#22411;&#29983;&#25104;&#30340;UAP&#38656;&#35201;&#26356;&#23569;&#30340;&#25200;&#21160;&#26469;&#24341;&#23548;&#27169;&#22411;&#35823;&#23548;&#12290;&#21518;&#38376;&#27169;&#22411;&#30340;UAP&#20542;&#21521;&#20110;&#21033;&#29992;&#32593;&#32476;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models achieve excellent performance in numerous machine learning tasks. Yet, they suffer from security-related issues such as adversarial examples and poisoning (backdoor) attacks. A deep learning model may be poisoned by training with backdoored data or by modifying inner network parameters. Then, a backdoored model performs as expected when receiving a clean input, but it misclassifies when receiving a backdoored input stamped with a pre-designed pattern called "trigger". Unfortunately, it is difficult to distinguish between clean and backdoored models without prior knowledge of the trigger. This paper proposes a backdoor detection method by utilizing a special type of adversarial attack, universal adversarial perturbation (UAP), and its similarities with a backdoor trigger. We observe an intuitive phenomenon: UAPs generated from backdoored models need fewer perturbations to mislead the model than UAPs from clean models. UAPs of backdoored models tend to exploit the sh
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22312;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#20013;&#25506;&#32034;&#20102;Anderson&#21152;&#36895;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;AA&#65292;&#36798;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#21644;&#20943;&#23567;&#35757;&#32451;&#35823;&#24046;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.00347</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#29289;&#20449;&#24687;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;Anderson&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Anderson Acceleration For Bioinformatics-Based Machine Learning. (arXiv:2302.00347v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00347
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22312;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#20013;&#25506;&#32034;&#20102;Anderson&#21152;&#36895;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;AA&#65292;&#36798;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#21644;&#20943;&#23567;&#35757;&#32451;&#35823;&#24046;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Anderson&#21152;&#36895;&#65288;AA&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#21152;&#24555;&#36845;&#20195;&#31639;&#27861;&#25910;&#25947;&#36895;&#24230;&#30340;&#33879;&#21517;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#20248;&#21270;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#23613;&#31649;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;AA&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#22312;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#20013;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#30740;&#31350;&#12290;&#23588;&#20854;&#26159;&#23545;&#20110;&#34920;&#26684;&#25968;&#25454;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#32780;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#21463;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#20998;&#31867;&#22120;&#21464;&#31181;&#65292;&#32467;&#21512;&#20102;AA&#20197;&#21152;&#36895;&#25910;&#25947;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#22810;&#20010;&#29983;&#29289;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;Anderson&#21152;&#36895;&#30340;SVM&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#22312;&#36845;&#20195;&#27425;&#25968;&#22686;&#21152;&#26102;&#65292;&#20351;&#29992;AA&#26174;&#33879;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20943;&#23567;&#20102;&#35757;&#32451;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#19968;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anderson acceleration (AA) is a well-known method for accelerating the convergence of iterative algorithms, with applications in various fields including deep learning and optimization. Despite its popularity in these areas, the effectiveness of AA in classical machine learning classifiers has not been thoroughly studied. Tabular data, in particular, presents a unique challenge for deep learning models, and classical machine learning models are known to perform better in these scenarios. However, the convergence analysis of these models has received limited attention. To address this gap in research, we implement a support vector machine (SVM) classifier variant that incorporates AA to speed up convergence. We evaluate the performance of our SVM with and without Anderson acceleration on several datasets from the biology domain and demonstrate that the use of AA significantly improves convergence and reduces the training loss as the number of iterations increases. Our findings provide a
&lt;/p&gt;</description></item><item><title>BallGAN&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;3D&#24863;&#30693;GAN&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#32972;&#26223;&#36817;&#20284;&#20026;&#29699;&#24418;&#34920;&#38754;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#32422;&#26463;&#26469;&#20943;&#23569;&#32972;&#26223;&#33258;&#30001;&#24230;&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#21512;&#29702;&#30340;3D&#20960;&#20309;&#12290;</title><link>http://arxiv.org/abs/2301.09091</link><description>&lt;p&gt;
BallGAN: &#24102;&#26377;&#29699;&#24418;&#32972;&#26223;&#30340;3D&#24863;&#30693;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
BallGAN: 3D-aware Image Synthesis with a Spherical Background. (arXiv:2301.09091v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09091
&lt;/p&gt;
&lt;p&gt;
BallGAN&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;3D&#24863;&#30693;GAN&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#32972;&#26223;&#36817;&#20284;&#20026;&#29699;&#24418;&#34920;&#38754;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#32422;&#26463;&#26469;&#20943;&#23569;&#32972;&#26223;&#33258;&#30001;&#24230;&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#21512;&#29702;&#30340;3D&#20960;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#24863;&#30693;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26088;&#22312;&#21512;&#25104;&#36924;&#30495;&#30340;3D&#22330;&#26223;&#65292;&#20197;&#20415;&#21487;&#20197;&#20197;&#20219;&#24847;&#35282;&#24230;&#36827;&#34892;&#28210;&#26579;&#20197;&#20135;&#29983;&#22270;&#20687;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#20294;&#23427;&#20204;&#22312;&#35757;&#32451;&#19981;&#31283;&#23450;&#25110;&#23384;&#22312;&#19981;&#33258;&#28982;&#30340;3D&#20960;&#20309;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;3D&#20960;&#20309;&#22312;&#32422;&#26463;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#26159;&#19981;&#30830;&#23450;&#30340;&#65292;&#21363;&#20165;&#23558;&#20854;&#20998;&#31867;&#20026;&#30495;&#23454;&#22270;&#20687;&#23545;&#20110;&#37492;&#21035;&#22120;&#26469;&#35828;&#26159;&#19981;&#22815;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#32972;&#26223;&#36817;&#20284;&#20026;&#29699;&#24418;&#34920;&#38754;&#65292;&#24182;&#23558;&#22330;&#26223;&#34920;&#31034;&#20026;&#25918;&#32622;&#22312;&#29699;&#20307;&#20013;&#30340;&#21069;&#26223;&#21644;&#34180;&#29699;&#24418;&#32972;&#26223;&#30340;&#32852;&#21512;&#12290;&#36825;&#26679;&#21487;&#20197;&#20943;&#23569;&#32972;&#26223;&#22330;&#30340;&#33258;&#30001;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#20307;&#28210;&#26579;&#26041;&#31243;&#65292;&#24182;&#21152;&#20837;&#20102;&#19987;&#29992;&#30340;&#32422;&#26463;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;BallGAN&#30340;&#26032;&#22411;3D&#24863;&#30693;GAN&#26694;&#26550;&#12290; BallGAN&#20855;&#26377;&#20197;&#19979;&#22810;&#20010;&#20248;&#28857;&#12290;1&#65289;&#23427;&#20135;&#29983;&#20102;&#26356;&#21512;&#29702;&#30340;3D&#20960;&#20309;&#65307;&#22330;&#26223;&#22312;&#19981;&#21516;&#35270;&#35282;&#19979;&#30340;&#22270;&#20687;&#20855;&#26377;&#26356;&#22909;&#30340;&#20809;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D-aware GANs aim to synthesize realistic 3D scenes such that they can be rendered in arbitrary perspectives to produce images. Although previous methods produce realistic images, they suffer from unstable training or degenerate solutions where the 3D geometry is unnatural. We hypothesize that the 3D geometry is underdetermined due to the insufficient constraint, i.e., being classified as real image to the discriminator is not enough. To solve this problem, we propose to approximate the background as a spherical surface and represent a scene as a union of the foreground placed in the sphere and the thin spherical background. It reduces the degree of freedom in the background field. Accordingly, we modify the volume rendering equation and incorporate dedicated constraints to design a novel 3D-aware GAN framework named BallGAN. BallGAN has multiple advantages as follows. 1) It produces more reasonable 3D geometry; the images of a scene across different viewpoints have better photometric 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;&#25509;&#36817;&#20302;&#32500;&#27969;&#24418;&#24182;&#20855;&#26377;&#32447;&#24615;&#34920;&#31034;&#30340;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#21644;&#23398;&#20064;&#12290;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#38750;&#32447;&#24615;&#27969;&#24418;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#26679;&#26412;&#30340;&#32858;&#31867;&#25104;&#21592;&#36164;&#26684;&#20570;&#20986;&#20551;&#35774;&#25110;&#23545;&#37319;&#26679;&#23494;&#24230;&#35201;&#27714;&#36807;&#39640;&#12290;</title><link>http://arxiv.org/abs/2301.01805</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#27969;&#24418;&#32447;&#24615;&#21270;&#21644;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Manifold Linearizing and Clustering. (arXiv:2301.01805v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;&#25509;&#36817;&#20302;&#32500;&#27969;&#24418;&#24182;&#20855;&#26377;&#32447;&#24615;&#34920;&#31034;&#30340;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#21644;&#23398;&#20064;&#12290;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#38750;&#32447;&#24615;&#27969;&#24418;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#26679;&#26412;&#30340;&#32858;&#31867;&#25104;&#21592;&#36164;&#26684;&#20570;&#20986;&#20551;&#35774;&#25110;&#23545;&#37319;&#26679;&#23494;&#24230;&#35201;&#27714;&#36807;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#21516;&#26102;&#23545;&#25509;&#36817;&#20302;&#32500;&#27969;&#24418;&#24182;&#20855;&#26377;&#32447;&#24615;&#34920;&#31034;&#30340;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#21644;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#12290;&#24403;&#20551;&#35774;&#27969;&#24418;&#26159;&#32447;&#24615;&#23376;&#31354;&#38388;&#26102;&#65292;&#36825;&#24402;&#32467;&#20026;&#32463;&#20856;&#30340;&#23376;&#31354;&#38388;&#32858;&#31867;&#38382;&#39064;&#65292;&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#65288;&#22914;&#33258;&#28982;&#22270;&#20687;&#65289;&#19981;&#33021;&#24456;&#22909;&#22320;&#29992;&#32447;&#24615;&#23376;&#31354;&#38388;&#36817;&#20284;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35768;&#22810;&#30740;&#31350;&#23581;&#35797;&#23398;&#20064;&#25968;&#25454;&#30340;&#36866;&#24403;&#21464;&#25442;&#65292;&#20351;&#24471;&#25968;&#25454;&#20174;&#19968;&#32452;&#38750;&#32447;&#24615;&#27969;&#24418;&#26144;&#23556;&#21040;&#19968;&#32452;&#32447;&#24615;&#23376;&#31354;&#38388;&#65288;&#23558;&#21516;&#19968;&#27969;&#24418;&#19978;&#30340;&#28857;&#26144;&#23556;&#21040;&#21516;&#19968;&#23376;&#31354;&#38388;&#65289;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#35832;&#22914;&#20551;&#35774;&#24050;&#30693;&#26679;&#26412;&#23545;&#32858;&#31867;&#30340;&#25104;&#21592;&#36164;&#26684;&#12289;&#35201;&#27714;&#39640;&#37319;&#26679;&#23494;&#24230;&#25110;&#22312;&#29702;&#35770;&#19978;&#34987;&#35777;&#26126;&#23398;&#20064;&#30340;&#26159;&#24179;&#20961;&#34920;&#31034;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of simultaneously clustering and learning a linear representation of data lying close to a union of low-dimensional manifolds, a fundamental task in machine learning and computer vision. When the manifolds are assumed to be linear subspaces, this reduces to the classical problem of subspace clustering, which has been studied extensively over the past two decades. Unfortunately, many real-world datasets such as natural images can not be well approximated by linear subspaces. On the other hand, numerous works have attempted to learn an appropriate transformation of the data, such that data is mapped from a union of general non-linear manifolds to a union of linear subspaces (with points from the same manifold being mapped to the same subspace). However, many existing works have limitations such as assuming knowledge of the membership of samples to clusters, requiring high sampling density, or being shown theoretically to learn trivial representations. In this pape
&lt;/p&gt;</description></item><item><title>&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#31639;&#27861;&#36827;&#23637;&#23545;&#36827;&#27493;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22686;&#24378;&#35745;&#31639;&#30340;&#31639;&#27861;&#21019;&#26032;&#20351;&#35745;&#31639;&#38656;&#27714;&#20943;&#21322;&#65292;&#36895;&#24230;&#27604;&#25705;&#23572;&#23450;&#24459;&#30456;&#20851;&#36895;&#24230;&#24555;&#20004;&#20493;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2212.05153</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#31639;&#27861;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Algorithmic progress in computer vision. (arXiv:2212.05153v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05153
&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#31639;&#27861;&#36827;&#23637;&#23545;&#36827;&#27493;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22686;&#24378;&#35745;&#31639;&#30340;&#31639;&#27861;&#21019;&#26032;&#20351;&#35745;&#31639;&#38656;&#27714;&#20943;&#21322;&#65292;&#36895;&#24230;&#27604;&#25705;&#23572;&#23450;&#24459;&#30456;&#20851;&#36895;&#24230;&#24555;&#20004;&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;ImageNet&#19978;&#30340;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#31639;&#27861;&#36827;&#23637;&#65292;&#36825;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#26368;&#33879;&#21517;&#30340;&#27979;&#35797;&#24179;&#21488;&#20043;&#19968;&#12290;&#25105;&#20204;&#20272;&#35745;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#30340;&#24037;&#20316;&#25512;&#26029;&#20102;&#36827;&#23637;&#30340;&#20998;&#35299;&#65292;&#21253;&#25324;&#35745;&#31639;&#12289;&#25968;&#25454;&#21644;&#31639;&#27861;&#30340;&#32553;&#25918;&#12290;&#20351;&#29992;Shapley&#20540;&#26469;&#24402;&#22240;&#24615;&#33021;&#25913;&#36827;&#65292;&#25105;&#20204;&#21457;&#29616;&#31639;&#27861;&#25913;&#36827;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#36827;&#23637;&#20013;&#30340;&#20316;&#29992;&#19982;&#35745;&#31639;&#30340;&#32553;&#25918;&#24046;&#19981;&#22810;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#34920;&#26126;&#65292;&#31639;&#27861;&#21019;&#26032;&#20027;&#35201;&#20197;&#22686;&#24378;&#35745;&#31639;&#30340;&#31639;&#27861;&#36827;&#27493;&#30340;&#24418;&#24335;&#20986;&#29616;&#65288;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#22312;&#36739;&#23569;&#35745;&#31639;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#65289;&#65292;&#32780;&#19981;&#26159;&#22686;&#21152;&#25968;&#25454;&#30340;&#31639;&#27861;&#36827;&#27493;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22686;&#24378;&#35745;&#31639;&#30340;&#31639;&#27861;&#21019;&#26032;&#30340;&#36895;&#24230;&#27604;&#36890;&#24120;&#19982;&#25705;&#23572;&#23450;&#24459;&#30456;&#20851;&#30340;&#36895;&#24230;&#26356;&#24555;&#20004;&#20493;&#20197;&#19978;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20272;&#35745;&#27599;&#20061;&#20010;&#26376;&#35745;&#31639;&#22686;&#24378;&#30340;&#21019;&#26032;&#23558;&#20351;&#35745;&#31639;&#38656;&#27714;&#20943;&#21322;&#65288;95&#65285;&#30340;&#32622;&#20449;&#21306;&#38388;&#20026;4&#21040;25&#20010;&#26376;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate algorithmic progress in image classification on ImageNet, perhaps the most well-known test bed for computer vision. We estimate a model, informed by work on neural scaling laws, and infer a decomposition of progress into the scaling of compute, data, and algorithms. Using Shapley values to attribute performance improvements, we find that algorithmic improvements have been roughly as important as the scaling of compute for progress computer vision. Our estimates indicate that algorithmic innovations mostly take the form of compute-augmenting algorithmic advances (which enable researchers to get better performance from less compute), not data-augmenting algorithmic advances. We find that compute-augmenting algorithmic advances are made at a pace more than twice as fast as the rate usually associated with Moore's law. In particular, we estimate that compute-augmenting innovations halve compute requirements every nine months (95\% confidence interval: 4 to 25 months).
&lt;/p&gt;</description></item><item><title>FIESTA&#26159;&#19968;&#20010;&#21487;&#38752;&#12289;&#31283;&#20581;&#12289;&#23436;&#20840;&#33258;&#21160;&#21270;&#19988;&#26131;&#20110;&#21322;&#33258;&#21160;&#26657;&#20934;&#30340;&#27969;&#31243;&#65292;&#22522;&#20110;&#28145;&#24230;&#33258;&#21160;&#32534;&#30721;&#22120;&#21487;&#20197;&#35299;&#21078;&#21644;&#23436;&#20840;&#22635;&#20805;&#30333;&#36136;&#26463;&#65292;&#36890;&#36807;&#29983;&#25104;&#37319;&#26679;&#30340;&#26041;&#27861;&#25913;&#21892;&#38590;&#20197;&#36861;&#36394;&#30340;&#26463;&#30340;&#20998;&#21106;&#35206;&#30422;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.00143</link><description>&lt;p&gt;
FIESTA&#65306;&#29992;&#20110;&#31934;&#30830;&#32420;&#32500;&#20998;&#21106;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#36947;&#36335;&#22270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
FIESTA: Autoencoders for accurate fiber segmentation in tractography. (arXiv:2212.00143v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00143
&lt;/p&gt;
&lt;p&gt;
FIESTA&#26159;&#19968;&#20010;&#21487;&#38752;&#12289;&#31283;&#20581;&#12289;&#23436;&#20840;&#33258;&#21160;&#21270;&#19988;&#26131;&#20110;&#21322;&#33258;&#21160;&#26657;&#20934;&#30340;&#27969;&#31243;&#65292;&#22522;&#20110;&#28145;&#24230;&#33258;&#21160;&#32534;&#30721;&#22120;&#21487;&#20197;&#35299;&#21078;&#21644;&#23436;&#20840;&#22635;&#20805;&#30333;&#36136;&#26463;&#65292;&#36890;&#36807;&#29983;&#25104;&#37319;&#26679;&#30340;&#26041;&#27861;&#25913;&#21892;&#38590;&#20197;&#36861;&#36394;&#30340;&#26463;&#30340;&#20998;&#21106;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30333;&#36136;&#26463;&#20998;&#21106;&#26159;&#29616;&#20195;&#36947;&#36335;&#22270;&#30740;&#31350;&#33041;&#32467;&#26500;&#36830;&#25509;&#24615;&#30340;&#22522;&#30707;&#65292;&#36866;&#29992;&#20110;&#31070;&#32463;&#30142;&#30149;&#12289;&#31070;&#32463;&#22806;&#31185;&#21644;&#34928;&#32769;&#31561;&#39046;&#22495;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FIESTA&#65288;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#32420;&#32500;&#20998;&#21106;&#30340;FIbEr Segmentation in Tractography&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#38752;&#12289;&#31283;&#20581;&#12289;&#23436;&#20840;&#33258;&#21160;&#21270;&#19988;&#26131;&#20110;&#21322;&#33258;&#21160;&#26657;&#20934;&#30340;&#27969;&#31243;&#65292;&#22522;&#20110;&#28145;&#24230;&#33258;&#21160;&#32534;&#30721;&#22120;&#21487;&#20197;&#35299;&#21078;&#21644;&#23436;&#20840;&#22635;&#20805;&#30333;&#36136;&#26463;&#12290;&#35813;&#27969;&#31243;&#24314;&#31435;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#65292;&#36825;&#20123;&#24037;&#20316;&#35777;&#26126;&#20102;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#36947;&#36335;&#22270;&#20013;&#30340;&#27969;&#32447;&#36807;&#28388;&#12289;&#26463;&#20998;&#21106;&#21644;&#27969;&#32447;&#29983;&#25104;&#26041;&#38754;&#30340;&#25104;&#21151;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#20027;&#20307;&#26463;&#21644;&#20934;&#21017;&#26463;&#30340;&#28508;&#31354;&#38388;&#26679;&#26412;&#29983;&#25104;&#26469;&#24674;&#22797;&#38590;&#20197;&#36861;&#36394;&#30340;&#26463;&#65292;&#20174;&#32780;&#25913;&#21892;&#26463;&#20998;&#21106;&#30340;&#35206;&#30422;&#29575;&#12290;&#20351;&#29992;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#27969;&#32447;&#30340;&#28508;&#31354;&#38388;&#12290;&#20351;&#29992;&#26631;&#20934;&#31354;&#38388;&#65288;MNI&#65289;&#20013;&#30340;&#26463;&#30340;&#20998;&#21106;&#22270;&#20316;&#20026;&#20934;&#21017;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
White matter bundle segmentation is a cornerstone of modern tractography to study the brain's structural connectivity in domains such as neurological disorders, neurosurgery, and aging. In this study, we present FIESTA (FIbEr Segmentation in Tractography using Autoencoders), a reliable and robust, fully automated, and easily semi-automatically calibrated pipeline based on deep autoencoders that can dissect and fully populate white matter bundles. This pipeline is built upon previous works that demonstrated how autoencoders can be used successfully for streamline filtering, bundle segmentation, and streamline generation in tractography. Our proposed method improves bundle segmentation coverage by recovering hard-to-track bundles with generative sampling through the latent space seeding of the subject bundle and the atlas bundle. A latent space of streamlines is learned using autoencoder-based modeling combined with contrastive learning. Using an atlas of bundles in standard space (MNI),
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#22330;&#34394;&#25311;&#36127;&#36733;&#30417;&#27979;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#31163;&#23736;&#39118;&#30005;&#32467;&#26500;&#30417;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#23454;&#38469;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.00642</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#31163;&#23736;&#39118;&#30005;&#32467;&#26500;&#20840;&#22330;&#34394;&#25311;&#36127;&#36733;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Farm-wide virtual load monitoring for offshore wind structures via Bayesian neural networks. (arXiv:2211.00642v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00642
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#22330;&#34394;&#25311;&#36127;&#36733;&#30417;&#27979;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#31163;&#23736;&#39118;&#30005;&#32467;&#26500;&#30417;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#23454;&#38469;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#23736;&#39118;&#30005;&#32467;&#26500;&#22312;&#36816;&#34892;&#23551;&#21629;&#20869;&#20250;&#21463;&#21040;&#36864;&#21270;&#26426;&#21046;&#30340;&#24433;&#21709;&#12290;&#21363;&#20351;&#36890;&#36807;&#22522;&#20110;&#29289;&#29702;&#30340;&#36864;&#21270;&#27169;&#22411;&#21487;&#20197;&#20272;&#35745;&#32467;&#26500;&#20803;&#20214;&#30340;&#36864;&#21270;&#28436;&#21464;&#65292;&#20294;&#36807;&#31243;&#20013;&#28041;&#21450;&#30340;&#19981;&#30830;&#23450;&#24615;&#38459;&#30861;&#20102;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#20915;&#31574;&#30340;&#36873;&#25321;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#30417;&#27979;&#31995;&#32479;&#25910;&#38598;&#30456;&#20851;&#20449;&#24687;&#65292;&#21487;&#20197;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#65292;&#26368;&#32456;&#25512;&#21160;&#26356;&#20248;&#21270;&#30340;&#29983;&#21629;&#21608;&#26399;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#22312;&#25972;&#20010;&#39118;&#30005;&#22330;&#30340;&#25152;&#26377;&#39118;&#21147;&#28065;&#36718;&#19978;&#23454;&#26045;&#23436;&#25972;&#30340;&#30417;&#27979;&#20202;&#22120;&#21487;&#33021;&#30001;&#20110;&#23454;&#38469;&#21644;&#32463;&#27982;&#38480;&#21046;&#32780;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#27492;&#22806;&#65292;&#26576;&#20123;&#36127;&#36733;&#30417;&#27979;&#31995;&#32479;&#22312;&#28023;&#27915;&#29615;&#22659;&#26292;&#38706;&#20960;&#24180;&#21518;&#32463;&#24120;&#21457;&#29983;&#25925;&#38556;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#30001;&#19968;&#20010;&#39046;&#23548;&#22411;&#39118;&#21147;&#28065;&#36718;&#25351;&#23548;&#30340;&#20840;&#22330;&#34394;&#25311;&#36127;&#36733;&#30417;&#27979;&#26041;&#26696;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offshore wind structures are subject to deterioration mechanisms throughout their operational lifetime. Even if the deterioration evolution of structural elements can be estimated through physics-based deterioration models, the uncertainties involved in the process hurdle the selection of lifecycle management decisions. In this scenario, the collection of relevant information through an efficient monitoring system enables the reduction of uncertainties, ultimately driving more optimal lifecycle decisions. However, a full monitoring instrumentation implemented on all wind turbines in a farm might become unfeasible due to practical and economical constraints. Besides, certain load monitoring systems often become defective after a few years of marine environment exposure. Addressing the aforementioned concerns, a farm-wide virtual load monitoring scheme directed by a fleet-leader wind turbine offers an attractive solution. Fetched with data retrieved from a fully-instrumented wind turbine
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22797;&#26434;&#27169;&#22411;&#20013;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#26356;&#26032;&#21644;&#40654;&#26364;&#27969;&#24418;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#39640;&#26031;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.14598</link><description>&lt;p&gt;
&#30830;&#20999;&#30340;&#27969;&#24418;&#39640;&#26031;&#21464;&#20998;&#36125;&#21494;&#26031;
&lt;/p&gt;
&lt;p&gt;
Exact Manifold Gaussian Variational Bayes. (arXiv:2210.14598v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14598
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22797;&#26434;&#27169;&#22411;&#20013;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#26356;&#26032;&#21644;&#40654;&#26364;&#27969;&#24418;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#39640;&#26031;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22797;&#26434;&#27169;&#22411;&#20013;&#21464;&#20998;&#25512;&#26029;&#65288;VI&#65289;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#33258;&#28982;&#26799;&#24230;&#26356;&#26032;&#65292;&#20854;&#20013;&#21464;&#20998;&#31354;&#38388;&#26159;&#19968;&#20010;&#40654;&#26364;&#27969;&#24418;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#39640;&#26031;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#20197;&#38544;&#24335;&#28385;&#36275;&#21464;&#20998;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#27491;&#23450;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#30830;&#20999;&#27969;&#24418;&#39640;&#26031;&#21464;&#20998;&#36125;&#21494;&#26031;&#65288;EMGVB&#65289;&#25552;&#20379;&#20102;&#31934;&#30830;&#20294;&#31616;&#21333;&#30340;&#26356;&#26032;&#35268;&#21017;&#65292;&#24182;&#19988;&#26131;&#20110;&#23454;&#29616;&#12290;&#30001;&#20110;&#20854;&#40657;&#30418;&#24615;&#36136;&#65292;EMGVB&#25104;&#20026;&#22797;&#26434;&#27169;&#22411;&#20013;&#21363;&#25554;&#21363;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#32479;&#35745;&#12289;&#35745;&#37327;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#20351;&#29992;&#20116;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#21487;&#34892;&#24615;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#65292;&#24182;&#19982;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#24615;&#33021;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an optimization algorithm for Variational Inference (VI) in complex models. Our approach relies on natural gradient updates where the variational space is a Riemann manifold. We develop an efficient algorithm for Gaussian Variational Inference that implicitly satisfies the positive definite constraint on the variational covariance matrix. Our Exact manifold Gaussian Variational Bayes (EMGVB) provides exact but simple update rules and is straightforward to implement. Due to its black-box nature, EMGVB stands as a ready-to-use solution for VI in complex models. Over five datasets, we empirically validate our feasible approach on different statistical, econometric, and deep learning models, discussing its performance with respect to baseline methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20498;&#21521;&#28145;&#24230;BSDE&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#26368;&#20248;&#20572;&#27490;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#35823;&#24046;&#20272;&#35745;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.04118</link><description>&lt;p&gt;
&#20498;&#21521;&#28145;&#24230;BSDE&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#21450;&#20854;&#22312;&#26368;&#20248;&#20572;&#27490;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Convergence of the Backward Deep BSDE Method with Applications to Optimal Stopping Problems. (arXiv:2210.04118v3 [math.PR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20498;&#21521;&#28145;&#24230;BSDE&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#26368;&#20248;&#20572;&#27490;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#35823;&#24046;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20572;&#27490;&#38382;&#39064;&#26159;&#37329;&#34701;&#24066;&#22330;&#20013;&#30340;&#26680;&#24515;&#38382;&#39064;&#20043;&#19968;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#22914;&#23450;&#20215;&#32654;&#24335;&#21644;&#30334;&#24917;&#36798;&#26399;&#26435;&#12290;&#28145;&#24230;BSDE&#26041;&#27861;&#22312;&#35299;&#20915;&#39640;&#32500;&#21069;&#21518;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;FBSDEs&#65289;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#28608;&#21457;&#20102;&#35768;&#22810;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#20197;&#27491;&#21521;&#26041;&#24335;&#35299;&#20915;&#20498;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;BSDEs&#65289;&#65292;&#19981;&#33021;&#29992;&#20110;&#36890;&#24120;&#38656;&#35201;&#21521;&#21518;&#36816;&#34892;BSDE&#30340;&#26368;&#20248;&#20572;&#27490;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#22256;&#38590;&#65292;&#19968;&#31687;&#26368;&#36817;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#20498;&#21521;&#28145;&#24230;BSDE&#26041;&#27861;&#26469;&#35299;&#20915;&#26368;&#20248;&#20572;&#27490;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20498;&#21521;&#28145;&#24230;BSDE&#26041;&#27861;&#30340;&#20005;&#26684;&#29702;&#35770;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#21518;&#39564;&#35823;&#24046;&#20272;&#35745;&#65292;&#21363;&#36890;&#36807;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#38480;&#21046;&#25968;&#20540;&#35299;&#30340;&#35823;&#24046;&#65307;&#24182;&#19988;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;-
&lt;/p&gt;
&lt;p&gt;
The optimal stopping problem is one of the core problems in financial markets, with broad applications such as pricing American and Bermudan options. The deep BSDE method [Han, Jentzen and E, PNAS, 115(34):8505-8510, 2018] has shown great power in solving high-dimensional forward-backward stochastic differential equations (FBSDEs), and inspired many applications. However, the method solves backward stochastic differential equations (BSDEs) in a forward manner, which can not be used for optimal stopping problems that in general require running BSDE backwardly. To overcome this difficulty, a recent paper [Wang, Chen, Sudjianto, Liu and Shen, arXiv:1807.06622, 2018] proposed the backward deep BSDE method to solve the optimal stopping problem. In this paper, we provide the rigorous theory for the backward deep BSDE method. Specifically, 1. We derive the a posteriori error estimation, i.e., the error of the numerical solution can be bounded by the training loss function; and; 2. We give an 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#25351;&#23548;&#30340;&#31574;&#30053;&#26469;&#25552;&#21319;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#30340;&#31283;&#23450;&#24615;&#21644;&#36136;&#37327;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2210.00939</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#25351;&#23548;&#25552;&#39640;&#25193;&#25955;&#27169;&#22411;&#30340;&#26679;&#26412;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Improving Sample Quality of Diffusion Models Using Self-Attention Guidance. (arXiv:2210.00939v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00939
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#25351;&#23548;&#30340;&#31574;&#30053;&#26469;&#25552;&#21319;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#30340;&#31283;&#23450;&#24615;&#21644;&#36136;&#37327;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#20197;&#20854;&#20986;&#33394;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#31181;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#20351;&#29992;&#20998;&#31867;&#25110;&#25991;&#26412;&#26465;&#20214;&#30340;&#25193;&#25955;&#25351;&#23548;&#26041;&#27861;&#65292;&#22914;&#20998;&#31867;&#22120;&#21644;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#20840;&#38754;&#30340;&#35270;&#35282;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#25351;&#23548;&#26041;&#27861;&#12290;&#20174;&#36825;&#20010;&#24191;&#20041;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#26080;&#26465;&#20214;&#21644;&#26080;&#30417;&#30563;&#30340;&#31574;&#30053;&#26469;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#27169;&#31946;&#25351;&#23548;&#25913;&#21892;&#20102;&#20013;&#38388;&#26679;&#26412;&#30340;&#36866;&#29992;&#24615;&#65292;&#20351;&#24471;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#20197;&#36866;&#24230;&#30340;&#25351;&#23548;&#23610;&#24230;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#33258;&#27880;&#24847;&#21147;&#25351;&#23548;&#65288;SAG&#65289;&#21033;&#29992;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20013;&#38388;&#33258;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#22686;&#24378;&#23427;&#20204;&#30340;&#31283;&#23450;&#24615;&#21644;&#25928;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SAG&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#20165;&#23545;&#25193;&#25955;&#27169;&#22411;&#20851;&#27880;&#30340;&#21306;&#22495;&#36827;&#34892;&#23545;&#25239;&#24615;&#27169;&#31946;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models (DDMs) have attracted attention for their exceptional generation quality and diversity. This success is largely attributed to the use of class- or text-conditional diffusion guidance methods, such as classifier and classifier-free guidance. In this paper, we present a more comprehensive perspective that goes beyond the traditional guidance methods. From this generalized perspective, we introduce novel condition- and training-free strategies to enhance the quality of generated images. As a simple solution, blur guidance improves the suitability of intermediate samples for their fine-scale information and structures, enabling diffusion models to generate higher quality samples with a moderate guidance scale. Improving upon this, Self-Attention Guidance (SAG) uses the intermediate self-attention maps of diffusion models to enhance their stability and efficacy. Specifically, SAG adversarially blurs only the regions that diffusion models attend to at each iteratio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#20010;&#20307;&#38544;&#31169;&#26680;&#31639;&#30340;&#39640;&#26031;&#24046;&#20998;&#38544;&#31169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#33258;&#36866;&#24212;&#32452;&#21512;&#38543;&#26426;&#26426;&#21046;&#36827;&#34892;&#20180;&#32454;&#20998;&#26512;&#65292;&#20026;&#39640;&#26031;&#26426;&#21046;&#25552;&#20379;&#20102;&#26368;&#20248;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2209.15596</link><description>&lt;p&gt;
&#29992;&#39640;&#26031;&#26426;&#22120;&#38544;&#31169;&#23454;&#29616;&#20010;&#20307;&#38544;&#31169;&#26680;&#31639;
&lt;/p&gt;
&lt;p&gt;
Individual Privacy Accounting with Gaussian Differential Privacy. (arXiv:2209.15596v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#20010;&#20307;&#38544;&#31169;&#26680;&#31639;&#30340;&#39640;&#26031;&#24046;&#20998;&#38544;&#31169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#33258;&#36866;&#24212;&#32452;&#21512;&#38543;&#26426;&#26426;&#21046;&#36827;&#34892;&#20180;&#32454;&#20998;&#26512;&#65292;&#20026;&#39640;&#26031;&#26426;&#21046;&#25552;&#20379;&#20102;&#26368;&#20248;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20307;&#38544;&#31169;&#26680;&#31639;&#33021;&#22815;&#20026;&#21442;&#19982;&#20998;&#26512;&#30340;&#27599;&#20010;&#21442;&#19982;&#32773;&#20010;&#21035;&#22320;&#38480;&#21046;&#24046;&#20998;&#38544;&#31169;&#25439;&#22833;&#12290;&#36825;&#36890;&#24120;&#26159;&#26377;&#24847;&#20041;&#30340;&#65292;&#22240;&#20026;&#20010;&#20307;&#38544;&#31169;&#25439;&#22833;&#24448;&#24448;&#27604;&#32771;&#34385;&#27599;&#27425;&#25968;&#25454;&#35775;&#38382;&#30340;&#26368;&#22351;&#24773;&#20917;&#36793;&#30028;&#25152;&#31034;&#30340;&#24046;&#20998;&#38544;&#31169;&#36793;&#30028;&#35201;&#23567;&#24471;&#22810;&#12290;&#20026;&#20102;&#20197;&#26377;&#21407;&#21017;&#30340;&#26041;&#24335;&#26680;&#31639;&#20010;&#20307;&#38544;&#31169;&#25439;&#22833;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#38024;&#23545;&#33258;&#36866;&#24212;&#32452;&#21512;&#38543;&#26426;&#26426;&#21046;&#30340;&#38544;&#31169;&#26680;&#31639;&#26041;&#27861;&#65292;&#20854;&#20013;&#22312;&#32473;&#23450;&#30340;&#25968;&#25454;&#35775;&#38382;&#20013;&#25152;&#20135;&#29983;&#30340;&#25439;&#22833;&#20801;&#35768;&#27604;&#26368;&#22351;&#24773;&#20917;&#25439;&#22833;&#35201;&#23567;&#12290;&#36153;&#23572;&#24503;&#26364;&#21644;&#20857;&#23572;&#23612;&#20811;&#65288;2021&#65289;&#24050;&#23545;R&#233;nyi&#24046;&#20998;&#38544;&#31169;&#65288;RDP&#65289;&#36827;&#34892;&#20102;&#36825;&#31181;&#20998;&#26512;&#65292;&#20294;&#23578;&#26410;&#24212;&#29992;&#20110;&#25152;&#35859;&#30340;&#26368;&#20248;&#38544;&#31169;&#26680;&#31639;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#24046;&#20998;&#38544;&#31169;&#36827;&#34892;&#20180;&#32454;&#20998;&#26512;&#65292;&#20026;&#27492;&#26041;&#21521;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#65292;&#39640;&#26031;&#24046;&#20998;&#38544;&#31169;&#20026;&#26368;&#22810;&#21151;&#33021;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#20043;&#19968;&#25552;&#20379;&#20102;&#26368;&#20248;&#36793;&#30028;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Individual privacy accounting enables bounding differential privacy (DP) loss individually for each participant involved in the analysis. This can be informative as often the individual privacy losses are considerably smaller than those indicated by the DP bounds that are based on considering worst-case bounds at each data access. In order to account for the individual privacy losses in a principled manner, we need a privacy accountant for adaptive compositions of randomised mechanisms, where the loss incurred at a given data access is allowed to be smaller than the worst-case loss. This kind of analysis has been carried out for the R\'enyi differential privacy (RDP) by Feldman and Zrnic (2021), however not yet for the so-called optimal privacy accountants. We make first steps in this direction by providing a careful analysis using the Gaussian differential privacy which gives optimal bounds for the Gaussian mechanism, one of the most versatile DP mechanisms. This approach is based on 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#36890;&#36807;&#20869;&#37096;&#31070;&#32463;&#20803;&#20171;&#23548;&#36882;&#24402;&#36890;&#20449;&#19982;&#30452;&#25509;&#36882;&#24402;&#36830;&#25509;&#30456;&#27604;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#36890;&#36807;&#20998;&#26512;&#36830;&#32493;&#31361;&#35302;&#21160;&#24577;&#21644;&#25968;&#20540;&#27169;&#25311;&#65292;&#34920;&#26126;&#20855;&#26377;&#20869;&#37096;&#31070;&#32463;&#20803;&#30340;&#32593;&#32476;&#27604;&#20855;&#26377;&#30452;&#25509;&#36882;&#24402;&#36830;&#25509;&#30340;&#32593;&#32476;&#26356;&#33021;&#25269;&#25239;&#21021;&#22987;&#21270;&#30340;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2209.10634</link><description>&lt;p&gt;
&#22312;&#32479;&#35745;&#33258;&#36866;&#24212;&#20013;&#65292;&#20869;&#37096;&#31070;&#32463;&#20803;&#21152;&#36895;&#20102;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Interneurons accelerate learning dynamics in recurrent neural networks for statistical adaptation. (arXiv:2209.10634v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10634
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#36890;&#36807;&#20869;&#37096;&#31070;&#32463;&#20803;&#20171;&#23548;&#36882;&#24402;&#36890;&#20449;&#19982;&#30452;&#25509;&#36882;&#24402;&#36830;&#25509;&#30456;&#27604;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#36890;&#36807;&#20998;&#26512;&#36830;&#32493;&#31361;&#35302;&#21160;&#24577;&#21644;&#25968;&#20540;&#27169;&#25311;&#65292;&#34920;&#26126;&#20855;&#26377;&#20869;&#37096;&#31070;&#32463;&#20803;&#30340;&#32593;&#32476;&#27604;&#20855;&#26377;&#30452;&#25509;&#36882;&#24402;&#36830;&#25509;&#30340;&#32593;&#32476;&#26356;&#33021;&#25269;&#25239;&#21021;&#22987;&#21270;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#20013;&#30340;&#26089;&#26399;&#24863;&#30693;&#31995;&#32479;&#24555;&#36895;&#36866;&#24212;&#27874;&#21160;&#30340;&#36755;&#20837;&#32479;&#35745;&#65292;&#36825;&#38656;&#35201;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#36882;&#24402;&#36890;&#20449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36890;&#36807;&#20869;&#37096;&#31070;&#32463;&#20803;&#20171;&#23548;&#36882;&#24402;&#36890;&#20449;&#19982;&#30452;&#25509;&#36882;&#24402;&#36830;&#25509;&#30456;&#27604;&#30340;&#35745;&#31639;&#20248;&#21183;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#25968;&#23398;&#21487;&#36861;&#36394;&#30340;&#36882;&#24402;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#20204;&#23545;&#36755;&#20837;&#36827;&#34892;&#32479;&#35745;&#30333;&#21270;&#8212;&#8212;&#19968;&#31181;&#20855;&#26377;&#30452;&#25509;&#36882;&#24402;&#36830;&#25509;&#65292;&#21478;&#19968;&#31181;&#20855;&#26377;&#20171;&#23548;&#36882;&#24402;&#36890;&#20449;&#30340;&#20869;&#37096;&#31070;&#32463;&#20803;&#12290;&#36890;&#36807;&#20998;&#26512;&#30456;&#24212;&#30340;&#36830;&#32493;&#31361;&#35302;&#21160;&#24577;&#24182;&#23545;&#32593;&#32476;&#36827;&#34892;&#25968;&#20540;&#27169;&#25311;&#65292;&#25105;&#20204;&#34920;&#26126;&#20855;&#26377;&#20869;&#37096;&#31070;&#32463;&#20803;&#30340;&#32593;&#32476;&#27604;&#20855;&#26377;&#30452;&#25509;&#36882;&#24402;&#36830;&#25509;&#30340;&#32593;&#32476;&#26356;&#33021;&#25269;&#25239;&#21021;&#22987;&#21270;&#30340;&#24178;&#25200;&#65292;&#21363;&#20869;&#37096;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#31361;&#35302;&#21160;&#24577;&#30340;&#25910;&#25947;&#26102;&#38388;&#65288;&#25110;&#32773;&#30452;&#25509;&#36882;&#24402;&#36830;&#25509;&#30340;&#32593;&#32476;&#65289;&#21576;&#23545;&#25968;&#23610;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early sensory systems in the brain rapidly adapt to fluctuating input statistics, which requires recurrent communication between neurons. Mechanistically, such recurrent communication is often indirect and mediated by local interneurons. In this work, we explore the computational benefits of mediating recurrent communication via interneurons compared with direct recurrent connections. To this end, we consider two mathematically tractable recurrent linear neural networks that statistically whiten their inputs -- one with direct recurrent connections and the other with interneurons that mediate recurrent communication. By analyzing the corresponding continuous synaptic dynamics and numerically simulating the networks, we show that the network with interneurons is more robust to initialization than the network with direct recurrent connections in the sense that the convergence time for the synaptic dynamics in the network with interneurons (resp. direct recurrent connections) scales logar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#20174;&#21487;&#34920;&#36798;&#24615;&#21040;&#21487;&#25191;&#34892;&#24615;&#23454;&#29616;&#20102;&#33258;&#39030;&#21521;&#19979;&#30340;&#33258;&#21160;&#21270;&#24320;&#21457;&#12290;&#36890;&#36807;&#24314;&#31435;&#20195;&#30721;&#20998;&#31867;&#27861;&#21644;&#20351;&#29992;&#35821;&#20041;&#37329;&#23383;&#22612;&#26469;&#20851;&#32852;&#25991;&#26412;&#25968;&#25454;&#21644;&#20195;&#30721;&#25968;&#25454;&#65292;&#25105;&#20204;&#21487;&#20197;&#25913;&#36827;&#28145;&#24230;&#20195;&#30721;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.01566</link><description>&lt;p&gt;
&#20174;&#21487;&#34920;&#36798;&#24615;&#21040;&#21487;&#25191;&#34892;&#24615;&#65306;&#22312;&#26377;&#38480;&#33539;&#22260;&#20869;&#23454;&#29616;&#33258;&#39030;&#21521;&#19979;&#30340;&#33258;&#21160;&#21270;&#24320;&#21457;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Towards Top-Down Automated Development in Limited Scopes: A Neuro-Symbolic Framework from Expressibles to Executables. (arXiv:2209.01566v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#20174;&#21487;&#34920;&#36798;&#24615;&#21040;&#21487;&#25191;&#34892;&#24615;&#23454;&#29616;&#20102;&#33258;&#39030;&#21521;&#19979;&#30340;&#33258;&#21160;&#21270;&#24320;&#21457;&#12290;&#36890;&#36807;&#24314;&#31435;&#20195;&#30721;&#20998;&#31867;&#27861;&#21644;&#20351;&#29992;&#35821;&#20041;&#37329;&#23383;&#22612;&#26469;&#20851;&#32852;&#25991;&#26412;&#25968;&#25454;&#21644;&#20195;&#30721;&#25968;&#25454;&#65292;&#25105;&#20204;&#21487;&#20197;&#25913;&#36827;&#28145;&#24230;&#20195;&#30721;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20195;&#30721;&#29983;&#25104;&#26159;&#36719;&#20214;&#24037;&#31243;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#39064;&#65292;&#37319;&#29992;&#31070;&#32463;&#27169;&#22411;&#20026;&#39044;&#26399;&#21151;&#33021;&#29983;&#25104;&#20195;&#30721;&#12290;&#30001;&#20110;&#31471;&#21040;&#31471;&#31070;&#32463;&#26041;&#27861;&#32570;&#20047;&#39046;&#22495;&#30693;&#35782;&#21644;&#36719;&#20214;&#23618;&#27425;&#24847;&#35782;&#65292;&#23427;&#20204;&#22312;&#39033;&#30446;&#32423;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#25506;&#32034;&#20195;&#30721;&#29983;&#25104;&#30340;&#28508;&#22312;&#25913;&#36827;&#65292;&#25105;&#20204;&#35753;&#20854;&#21442;&#19982;&#20174;&#8220;&#21487;&#34920;&#36798;&#24615;&#8221;&#21040;&#8220;&#21487;&#25191;&#34892;&#24615;&#8221;&#30340;&#33258;&#39030;&#21521;&#19979;&#24320;&#21457;&#65292;&#36825;&#22312;&#26377;&#38480;&#30340;&#33539;&#22260;&#20869;&#26159;&#21487;&#33021;&#30340;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#23427;&#20174;&#22823;&#37327;&#30340;&#26679;&#26412;&#12289;&#29305;&#24449;&#21644;&#30693;&#35782;&#20013;&#21463;&#30410;&#12290;&#20316;&#20026;&#22522;&#30784;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#20195;&#30721;&#25968;&#25454;&#19978;&#24314;&#31435;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#21363;&#20195;&#30721;&#20998;&#31867;&#27861;&#65292;&#21033;&#29992;&#20195;&#30721;&#20449;&#24687;&#30340;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#19977;&#23618;&#35821;&#20041;&#37329;&#23383;&#22612;(SP)&#26469;&#20851;&#32852;&#25991;&#26412;&#25968;&#25454;&#21644;&#20195;&#30721;&#25968;&#25454;&#12290;&#23427;&#35782;&#21035;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#20851;&#20110;&#24320;&#21457;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#25581;&#31034;&#20102;&#36719;&#20214;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep code generation is a topic of deep learning for software engineering (DL4SE), which adopts neural models to generate code for the intended functions. Since end-to-end neural methods lack domain knowledge and software hierarchy awareness, they tend to perform poorly w.r.t project-level tasks. To systematically explore the potential improvements of code generation, we let it participate in the whole top-down development from \emph{expressibles} to \emph{executables}, which is possible in limited scopes. In the process, it benefits from massive samples, features, and knowledge. As the foundation, we suggest building a taxonomy on code data, namely code taxonomy, leveraging the categorization of code information. Moreover, we introduce a three-layer semantic pyramid (SP) to associate text data and code data. It identifies the information of different abstraction levels, and thus introduces the domain knowledge on development and reveals the hierarchy of software. Furthermore, we propo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Scene-Rep Transformer&#26469;&#25552;&#21319;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#33021;&#21147;&#65292;&#36890;&#36807;&#25913;&#36827;&#22330;&#26223;&#34920;&#31034;&#32534;&#30721;&#21644;&#39034;&#24207;&#39044;&#27979;&#28508;&#22312;&#33976;&#39311;&#12290;&#37319;&#29992;&#22810;&#38454;&#27573;Transformer&#32534;&#30721;&#22120;&#24314;&#27169;&#20132;&#20114;&#24847;&#35782;&#21644;&#24847;&#22270;&#24847;&#35782;&#65292;&#24182;&#20351;&#29992;&#39034;&#24207;&#28508;&#22312;Transformer&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#21152;&#36895;&#35757;&#32451;&#21644;&#20943;&#23569;&#25506;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2208.12263</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Augmenting Reinforcement Learning with Transformer-based Scene Representation Learning for Decision-making of Autonomous Driving. (arXiv:2208.12263v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Scene-Rep Transformer&#26469;&#25552;&#21319;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#33021;&#21147;&#65292;&#36890;&#36807;&#25913;&#36827;&#22330;&#26223;&#34920;&#31034;&#32534;&#30721;&#21644;&#39034;&#24207;&#39044;&#27979;&#28508;&#22312;&#33976;&#39311;&#12290;&#37319;&#29992;&#22810;&#38454;&#27573;Transformer&#32534;&#30721;&#22120;&#24314;&#27169;&#20132;&#20114;&#24847;&#35782;&#21644;&#24847;&#22270;&#24847;&#35782;&#65292;&#24182;&#20351;&#29992;&#39034;&#24207;&#28508;&#22312;Transformer&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#21152;&#36895;&#35757;&#32451;&#21644;&#20943;&#23569;&#25506;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#33258;&#21160;&#39550;&#39542;&#30340;&#20915;&#31574;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#30001;&#20110;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#38543;&#26426;&#24615;&#21644;&#36947;&#36335;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#23613;&#31649;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#20915;&#31574;&#26041;&#26696;&#22312;&#22788;&#29702;&#22478;&#24066;&#39550;&#39542;&#22330;&#26223;&#26041;&#38754;&#24456;&#26377;&#21069;&#26223;&#65292;&#20294;&#23427;&#30340;&#37319;&#26679;&#25928;&#29575;&#20302;&#19988;&#36866;&#24212;&#24615;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Scene-Rep Transformer&#26469;&#25913;&#21892;RL&#20915;&#31574;&#33021;&#21147;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#22330;&#26223;&#34920;&#31034;&#32534;&#30721;&#21644;&#39034;&#24207;&#39044;&#27979;&#28508;&#22312;&#33976;&#39311;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;Transformer&#65288;MST&#65289;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#24314;&#27169;&#33258;&#36710;&#19982;&#20854;&#37051;&#23621;&#20043;&#38388;&#30340;&#20132;&#20114;&#24847;&#35782;&#20197;&#21450;&#20195;&#29702;&#32773;&#19982;&#20505;&#36873;&#36335;&#24452;&#20043;&#38388;&#30340;&#24847;&#22270;&#24847;&#35782;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#30340;&#39034;&#24207;&#28508;&#22312;Transformer&#65288;SLT&#65289;&#65292;&#23558;&#26410;&#26469;&#30340;&#39044;&#27979;&#20449;&#24687;&#33976;&#39311;&#21040;&#28508;&#22312;&#30340;&#22330;&#26223;&#34920;&#31034;&#20013;&#65292;&#20197;&#20943;&#23569;&#25506;&#32034;&#31354;&#38388;&#24182;&#21152;&#24555;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-making for urban autonomous driving is challenging due to the stochastic nature of interactive traffic participants and the complexity of road structures. Although reinforcement learning (RL)-based decision-making scheme is promising to handle urban driving scenarios, it suffers from low sample efficiency and poor adaptability. In this paper, we propose Scene-Rep Transformer to improve the RL decision-making capabilities with better scene representation encoding and sequential predictive latent distillation. Specifically, a multi-stage Transformer (MST) encoder is constructed to model not only the interaction awareness between the ego vehicle and its neighbors but also intention awareness between the agents and their candidate routes. A sequential latent Transformer (SLT) with self-supervised learning objectives is employed to distill the future predictive information into the latent scene representation, in order to reduce the exploration space and speed up training. The fina
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#33258;&#36866;&#24212;&#28608;&#27963;&#33293;&#20837;&#30340;&#35757;&#32451;&#21518;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#28608;&#27963;&#30340;&#33293;&#20837;&#26041;&#26696;&#26469;&#38477;&#20302;&#36755;&#20986;&#35823;&#24046;&#65292;&#24182;&#35299;&#20915;&#20102;&#21160;&#24577;&#28608;&#27963;&#21644;&#36816;&#34892;&#26102;&#24320;&#38144;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2208.11945</link><description>&lt;p&gt;
&#39640;&#25928;&#33258;&#36866;&#24212;&#28608;&#27963;&#33293;&#20837;&#29992;&#20110;&#35757;&#32451;&#21518;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Adaptive Activation Rounding for Post-Training Quantization. (arXiv:2208.11945v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#33258;&#36866;&#24212;&#28608;&#27963;&#33293;&#20837;&#30340;&#35757;&#32451;&#21518;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#28608;&#27963;&#30340;&#33293;&#20837;&#26041;&#26696;&#26469;&#38477;&#20302;&#36755;&#20986;&#35823;&#24046;&#65292;&#24182;&#35299;&#20915;&#20102;&#21160;&#24577;&#28608;&#27963;&#21644;&#36816;&#34892;&#26102;&#24320;&#38144;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#37096;&#32626;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#30340;&#20415;&#21033;&#24615;&#65292;&#35757;&#32451;&#21518;&#37327;&#21270;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#26368;&#36817;&#33293;&#20837;&#20173;&#28982;&#26159;DNN&#37327;&#21270;&#30340;&#20027;&#27969;&#26041;&#27861;&#65292;&#20294;&#23427;&#22312;&#26435;&#37325;&#37327;&#21270;&#20013;&#34920;&#29616;&#20986;&#20102;&#27425;&#20248;&#30340;&#24615;&#36136;&#12290;&#20182;&#20204;&#25552;&#20986;&#36890;&#36807;&#21033;&#29992;&#36755;&#20986;&#35823;&#24046;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#26435;&#37325;&#37327;&#21270;&#35823;&#24046;&#26469;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#30456;&#20284;&#30340;&#33293;&#20837;&#25361;&#25112;&#20063;&#36866;&#29992;&#20110;&#28608;&#27963;&#37327;&#21270;&#12290;&#23613;&#31649;&#24456;&#23481;&#26131;&#25512;&#24191;&#65292;&#20294;&#25361;&#25112;&#22312;&#20110;&#28608;&#27963;&#30340;&#21160;&#24577;&#24615;&#12290;&#38656;&#35201;&#38024;&#23545;&#19981;&#21516;&#30340;&#28608;&#27963;&#36827;&#34892;&#33258;&#36866;&#24212;&#33293;&#20837;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#20250;&#23548;&#33268;&#36816;&#34892;&#26102;&#24320;&#38144;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;AQuant&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#35843;&#25972;&#28608;&#27963;&#30340;&#33293;&#20837;&#26041;&#26696;&#26469;&#38477;&#20302;&#36755;&#20986;&#35823;&#24046;&#12290;&#19982;&#20351;&#29992;&#24120;&#25968;&#33293;&#20837;&#36793;&#30028;0.5&#30340;&#26368;&#36817;&#33293;&#20837;&#25805;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#36793;&#30028;&#25104;&#20026;&#28608;&#27963;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization attracts increasing attention due to its convenience in deploying quantized neural networks. Although rounding-to-nearest remains the prevailing method for DNN quantization, prior research has demonstrated its suboptimal nature when applied to weight quantization. They propose optimizing weight rounding schemes by leveraging output error rather than the traditional weight quantization error. Our study reveals that similar rounding challenges also extend to activation quantization. Despite the easy generalization, the challenges lie in the dynamic nature of activation. Adaptive rounding is expected for varying activations and the method is subjected to runtime overhead. To tackle this, we propose the AQuant quantization framework with a novel perspective to reduce output error by adjusting rounding schemes of activations. Instead of using the constant rounding border 0.5 of the rounding-to-nearest operation, we make the border become a function w.r.t. the acti
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32479;&#19968;&#19981;&#21516;&#25968;&#25454;&#30340;&#26799;&#24230;&#26469;&#38450;&#24481;&#22522;&#20110;&#35780;&#20998;&#30340;&#26597;&#35810;&#25915;&#20987;&#65288;SQAs&#65289;&#65292;&#36825;&#26679;SQAs&#21482;&#33021;&#25506;&#27979;&#21040;&#19968;&#20010;&#26356;&#24369;&#30340;&#25915;&#20987;&#26041;&#21521;&#65292;&#20445;&#25252;&#30495;&#23454;&#19990;&#30028;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2208.06228</link><description>&lt;p&gt;
&#23558;&#26799;&#24230;&#32479;&#19968;&#21270;&#20197;&#25552;&#39640;&#28145;&#24230;&#32593;&#32476;&#30340;&#30495;&#23454;&#19990;&#30028;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unifying Gradients to Improve Real-world Robustness for Deep Networks. (arXiv:2208.06228v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06228
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32479;&#19968;&#19981;&#21516;&#25968;&#25454;&#30340;&#26799;&#24230;&#26469;&#38450;&#24481;&#22522;&#20110;&#35780;&#20998;&#30340;&#26597;&#35810;&#25915;&#20987;&#65288;SQAs&#65289;&#65292;&#36825;&#26679;SQAs&#21482;&#33021;&#25506;&#27979;&#21040;&#19968;&#20010;&#26356;&#24369;&#30340;&#25915;&#20987;&#26041;&#21521;&#65292;&#20445;&#25252;&#30495;&#23454;&#19990;&#30028;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#23545;&#23427;&#20204;&#30340;&#30495;&#23454;&#19990;&#30028;&#40065;&#26834;&#24615;&#25552;&#20986;&#20102;&#26356;&#22810;&#20851;&#27880;&#65292;&#21363;DNN&#26159;&#21542;&#33021;&#22815;&#25269;&#25239;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#65292;&#20854;&#20013;&#22522;&#20110;&#35780;&#20998;&#30340;&#26597;&#35810;&#25915;&#20987;&#65288;SQAs&#65289;&#26368;&#20855;&#23041;&#32961;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#33021;&#36890;&#36807;&#35775;&#38382;&#27169;&#22411;&#36755;&#20986;&#26377;&#25928;&#22320;&#25915;&#20987;&#21463;&#23475;&#32593;&#32476;&#12290;&#25269;&#24481;SQAs&#38656;&#35201;&#23545;&#36755;&#20986;&#36827;&#34892;&#36731;&#24494;&#20294;&#24039;&#22937;&#30340;&#21464;&#21270;&#65292;&#22240;&#20026;&#29992;&#25143;&#19982;SQAs&#20849;&#20139;&#30456;&#21516;&#30340;&#36755;&#20986;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32479;&#19968;&#19981;&#21516;&#25968;&#25454;&#30340;&#26799;&#24230;&#26469;&#36827;&#34892;&#30495;&#23454;&#19990;&#30028;&#38450;&#24481;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;SQAs&#21482;&#33021;&#25506;&#27979;&#21040;&#19968;&#20010;&#26356;&#24369;&#30340;&#25915;&#20987;&#26041;&#21521;&#65292;&#36825;&#20010;&#25915;&#20987;&#26041;&#21521;&#23545;&#20110;&#19981;&#21516;&#26679;&#26412;&#26159;&#30456;&#20284;&#30340;&#12290;&#30001;&#20110;&#36825;&#31181;&#32479;&#19968;&#30340;&#25915;&#20987;&#25200;&#21160;&#34987;&#39564;&#35777;&#20026;&#27604;&#36755;&#20837;&#29305;&#23450;&#30340;&#25200;&#21160;&#26356;&#19981;&#20855;&#20405;&#30053;&#24615;&#65292;UniG&#36890;&#36807;&#25351;&#31034;&#25915;&#20987;&#32773;&#19968;&#20010;&#25197;&#26354;&#19988;&#20449;&#24687;&#36739;&#23569;&#30340;&#25915;&#20987;&#26041;&#21521;&#26469;&#20445;&#25252;&#30495;&#23454;&#19990;&#30028;&#30340;DNN&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#21487;&#25554;&#25300;&#30340;Hadamard&#20056;&#31215;&#27169;&#22359;&#39640;&#25928;&#23454;&#29616;&#20102;UniG&#12290;
&lt;/p&gt;
&lt;p&gt;
The wide application of deep neural networks (DNNs) demands an increasing amount of attention to their real-world robustness, i.e., whether a DNN resists black-box adversarial attacks, among which score-based query attacks (SQAs) are most threatening since they can effectively hurt a victim network with the only access to model outputs. Defending against SQAs requires a slight but artful variation of outputs due to the service purpose for users, who share the same output information with SQAs. In this paper, we propose a real-world defense by Unifying Gradients (UniG) of different data so that SQAs could only probe a much weaker attack direction that is similar for different samples. Since such universal attack perturbations have been validated as less aggressive than the input-specific perturbations, UniG protects real-world DNNs by indicating attackers a twisted and less informative attack direction. We implement UniG efficiently by a Hadamard product module which is plug-and-play. A
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#30828;&#20214;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#33033;&#20914;&#32534;&#30721;&#35757;&#32451;&#26426;&#21046;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#22806;&#37096;&#20869;&#23384;&#21644;&#35745;&#31639;&#35775;&#38382;&#12290;</title><link>http://arxiv.org/abs/2207.09755</link><description>&lt;p&gt;
&#19968;&#31181;&#26102;&#38388;&#21644;&#31354;&#38388;&#23616;&#37096;&#30340;&#22522;&#20110;&#33033;&#20914;&#30340;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#30828;&#20214;&#20013;&#30340;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
A temporally and spatially local spike-based backpropagation algorithm to enable training in hardware. (arXiv:2207.09755v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09755
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#30828;&#20214;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#33033;&#20914;&#32534;&#30721;&#35757;&#32451;&#26426;&#21046;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#22806;&#37096;&#20869;&#23384;&#21644;&#35745;&#31639;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#30828;&#20214;&#39640;&#25928;&#26550;&#26500;&#12290;&#33033;&#20914;&#32534;&#30721;&#30340;&#25361;&#25112;&#22312;&#20110;&#32570;&#20047;&#23436;&#20840;&#20351;&#29992;&#33033;&#20914;&#36827;&#34892;&#35757;&#32451;&#30340;&#36890;&#29992;&#26426;&#21046;&#12290;&#24050;&#32463;&#26377;&#20960;&#27425;&#23581;&#35797;&#37319;&#29992;&#38750;&#33033;&#20914;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#20013;&#20351;&#29992;&#30340;&#24378;&#22823;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#25216;&#26415;&#65306;&#65288;1&#65289;&#21487;&#20197;&#36890;&#36807;&#22806;&#37096;&#35745;&#31639;&#30340;&#25968;&#20540;&#26799;&#24230;&#26469;&#35757;&#32451;SNN&#65307;&#65288;2&#65289;&#26397;&#21521;&#21069;/&#21518;&#20256;&#36882;&#30340;&#30456;&#20301;&#32467;&#21512;&#20102;&#36817;&#20284;&#30340;&#22522;&#20110;&#33033;&#20914;&#30340;&#21453;&#21521;&#20256;&#25773;&#65292;&#21033;&#29992;&#20102;&#33033;&#20914;&#26102;&#24207;&#20381;&#36182;&#21487;&#22609;&#24615;&#65288;STDP&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30456;&#20301;&#20043;&#38388;&#30340;&#20449;&#24687;&#20256;&#36882;&#65292;&#29992;&#20110;&#26799;&#24230;&#21644;&#26435;&#37325;&#26356;&#26032;&#35745;&#31639;&#65292;&#38656;&#35201;&#22806;&#37096;&#20869;&#23384;&#21644;&#35745;&#31639;&#35775;&#38382;&#12290;&#36825;&#23545;&#20110;&#26631;&#20934;&#30340;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#23454;&#29616;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;SNN&#30340;&#21453;&#21521;&#20256;&#25773;&#65288;SSNN-BP&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#22797;&#21512;&#31070;&#32463;&#20803;&#21516;&#26102;&#35745;&#31639;...
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) have emerged as a hardware efficient architecture for classification tasks. The challenge of spike-based encoding has been the lack of a universal training mechanism performed entirely using spikes. There have been several attempts to adopt the powerful backpropagation (BP) technique used in non-spiking artificial neural networks (ANN): (1) SNNs can be trained by externally computed numerical gradients. (2) A major advancement towards native spike-based learning has been the use of approximate Backpropagation using spike-time dependent plasticity (STDP) with phased forward/backward passes. However, the transfer of information between such phases for gradient and weight update calculation necessitates external memory and computational access. This is a challenge for standard neuromorphic hardware implementations. In this paper, we propose a stochastic SNN based Back-Prop (SSNN-BP) algorithm that utilizes a composite neuron to simultaneously compute the for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#37325;&#26500;&#35823;&#24046;&#30340;&#26041;&#24335;&#38598;&#20013;&#22312;&#25968;&#25454;&#27969;&#24418;&#19978;&#65292;&#20174;&#32780;&#35299;&#20915;&#28145;&#24230;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#22312;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#20013;&#23481;&#26131;&#20986;&#29616;&#30340;&#37325;&#26500;&#24322;&#24120;&#20449;&#21495;&#32780;&#23548;&#33268;&#26816;&#27979;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.11723</link><description>&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#29992;&#20110;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Training with Autoencoders for Visual Anomaly Detection. (arXiv:2206.11723v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#37325;&#26500;&#35823;&#24046;&#30340;&#26041;&#24335;&#38598;&#20013;&#22312;&#25968;&#25454;&#27969;&#24418;&#19978;&#65292;&#20174;&#32780;&#35299;&#20915;&#28145;&#24230;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#22312;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#20013;&#23481;&#26131;&#20986;&#29616;&#30340;&#37325;&#26500;&#24322;&#24120;&#20449;&#21495;&#32780;&#23548;&#33268;&#26816;&#27979;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#38750;&#32447;&#24615;&#38477;&#32500;&#12290;&#26368;&#36817;&#65292;&#23427;&#20204;&#24050;&#34987;&#29992;&#20110;&#35270;&#35273;&#39046;&#22495;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26080;&#24322;&#24120;&#30340;&#26679;&#26412;&#20248;&#21270;&#37325;&#26500;&#35823;&#24046;&#65292;&#26222;&#36941;&#35748;&#20026;&#30456;&#24212;&#30340;&#32593;&#32476;&#24212;&#22312;&#24212;&#29992;&#38454;&#27573;&#26410;&#33021;&#20934;&#30830;&#37325;&#26500;&#24322;&#24120;&#21306;&#22495;&#12290;&#36825;&#20010;&#30446;&#26631;&#36890;&#24120;&#36890;&#36807;&#25511;&#21046;&#32593;&#32476;&#30340;&#23481;&#37327;&#26469;&#35299;&#20915;&#65292;&#35201;&#20040;&#36890;&#36807;&#20943;&#23569;&#29942;&#39048;&#23618;&#30340;&#22823;&#23567;&#65292;&#35201;&#20040;&#36890;&#36807;&#23545;&#20854;&#28608;&#27963;&#26045;&#21152;&#31232;&#30095;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#25216;&#26415;&#37117;&#27809;&#26377;&#26126;&#30830;&#24809;&#32602;&#24322;&#24120;&#20449;&#21495;&#30340;&#37325;&#26500;&#65292;&#36890;&#24120;&#23548;&#33268;&#26816;&#27979;&#25928;&#26524;&#19981;&#20339;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#36866;&#24212;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#21028;&#21035;&#20449;&#24687;&#65292;&#36890;&#36807;&#20462;&#25913;&#30340;&#37325;&#26500;&#35823;&#24046;&#38598;&#20013;&#22312;&#25968;&#25454;&#27969;&#24418;&#19978;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#23616;&#37096;&#19968;&#33268;&#30340;&#37325;&#26500;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep convolutional autoencoders provide an effective tool for learning non-linear dimensionality reduction in an unsupervised way. Recently, they have been used for the task of anomaly detection in the visual domain. By optimising for the reconstruction error using anomaly-free examples, the common belief is that a corresponding network should fail to accurately reconstruct anomalous regions in the application phase. This goal is typically addressed by controlling the capacity of the network by either reducing the size of the bottleneck layer or enforcing sparsity constraints on its activations. However, neither of these techniques does explicitly penalize reconstruction of anomalous signals often resulting in poor detection. We tackle this problem by adapting a self-supervised learning regime, which allows to use discriminative information during training focusing on the data manifold by means of a modified reconstruction error. This regularizes the model to produce locally consistent
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#24471;&#21040;&#30340;&#34920;&#31034;&#36866;&#24212;&#21040;&#27979;&#35797;&#26102;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;&#36890;&#36807;&#21033;&#29992;&#36328;&#27169;&#24577;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25991;&#26723;&#29702;&#35299;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#22312;&#23454;&#20307;&#35782;&#21035;&#12289;&#38190;&#20540;&#25552;&#21462;&#21644;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#19978;&#20998;&#21035;&#25552;&#39640;&#20102;1.89%&#12289;3.43%&#21644;17.68%&#12290;</title><link>http://arxiv.org/abs/2206.07240</link><description>&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Test-Time Adaptation for Visual Document Understanding. (arXiv:2206.07240v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07240
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#24471;&#21040;&#30340;&#34920;&#31034;&#36866;&#24212;&#21040;&#27979;&#35797;&#26102;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;&#36890;&#36807;&#21033;&#29992;&#36328;&#27169;&#24577;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25991;&#26723;&#29702;&#35299;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#22312;&#23454;&#20307;&#35782;&#21035;&#12289;&#38190;&#20540;&#25552;&#21462;&#21644;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#19978;&#20998;&#21035;&#25552;&#39640;&#20102;1.89%&#12289;3.43%&#21644;17.68%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#65288;VDU&#65289;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#25104;&#21151;&#29983;&#25104;&#20855;&#26377;&#21487;&#20256;&#36882;&#24615;&#30340;&#34920;&#31034;&#65292;&#28982;&#32780;&#65292;&#22312;&#27979;&#35797;&#26102;&#38388;&#23545;&#36825;&#31181;&#34920;&#31034;&#36827;&#34892;&#26377;&#25928;&#30340;&#36866;&#24212;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DocTTA&#65292;&#19968;&#31181;&#29992;&#20110;&#25991;&#26723;&#30340;&#26032;&#22411;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#26080;&#26631;&#31614;&#30340;&#30446;&#26631;&#25991;&#26723;&#25968;&#25454;&#36827;&#34892;&#26080;&#28304;&#22495;&#36866;&#24212;&#12290;DocTTA&#21033;&#29992;&#36328;&#27169;&#24577;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#23558;&#22312;&#8220;&#28304;&#8221;&#22495;&#19978;&#23398;&#21040;&#30340;&#27169;&#22411;&#36866;&#24212;&#21040;&#26410;&#26631;&#35760;&#30340;&#8220;&#30446;&#26631;&#8221;&#22495;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#20026;&#21508;&#31181;VDU&#20219;&#21153;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#23454;&#20307;&#35782;&#21035;&#12289;&#38190;&#20540;&#25552;&#21462;&#21644;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#12290;&#30456;&#27604;&#20110;&#28304;&#27169;&#22411;&#24615;&#33021;&#65292;DocTTA&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20998;&#21035;&#25552;&#39640;&#20102;1.89%&#65288;F1&#20998;&#25968;&#65289;&#12289;3.43%&#65288;F1&#20998;&#25968;&#65289;&#21644;17.68%&#65288;ANLS&#20998;&#25968;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
For visual document understanding (VDU), self-supervised pretraining has been shown to successfully generate transferable representations, yet, effective adaptation of such representations to distribution shifts at test-time remains to be an unexplored area. We propose DocTTA, a novel test-time adaptation method for documents, that does source-free domain adaptation using unlabeled target document data. DocTTA leverages cross-modality self-supervised learning via masked visual language modeling, as well as pseudo labeling to adapt models learned on a \textit{source} domain to an unlabeled \textit{target} domain at test time. We introduce new benchmarks using existing public datasets for various VDU tasks, including entity recognition, key-value extraction, and document visual question answering. DocTTA shows significant improvements on these compared to the source model performance, up to 1.89\% in (F1 score), 3.43\% (F1 score), and 17.68\% (ANLS score), respectively. Our benchmark dat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#25928;-Adam&#30340;&#36890;&#20449;&#25928;&#29575;&#26356;&#39640;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#38750;&#20984;&#29615;&#22659;&#19979;&#36890;&#36807;&#21452;&#21521;&#37327;&#21270;&#21644;&#21452;&#21521;&#35823;&#24046;&#21453;&#39304;&#31574;&#30053;&#26469;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#65292;&#24182;&#23545;&#20854;&#36845;&#20195;&#22797;&#26434;&#24230;&#21644;&#36890;&#20449;&#22797;&#26434;&#24230;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2205.14473</link><description>&lt;p&gt;
&#39640;&#25928;Adam&#65306;&#39640;&#25928;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;Adam
&lt;/p&gt;
&lt;p&gt;
Efficient-Adam: Communication-Efficient Distributed Adam. (arXiv:2205.14473v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14473
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#25928;-Adam&#30340;&#36890;&#20449;&#25928;&#29575;&#26356;&#39640;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#38750;&#20984;&#29615;&#22659;&#19979;&#36890;&#36807;&#21452;&#21521;&#37327;&#21270;&#21644;&#21452;&#21521;&#35823;&#24046;&#21453;&#39304;&#31574;&#30053;&#26469;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#65292;&#24182;&#23545;&#20854;&#36845;&#20195;&#22797;&#26434;&#24230;&#21644;&#36890;&#20449;&#22797;&#26434;&#24230;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#33258;&#36866;&#24212;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#24191;&#27867;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#38750;&#20984;&#20248;&#21270;&#65292;&#22914;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#20984;&#29615;&#22659;&#20013;&#24456;&#23569;&#26377;&#20851;&#20110;&#25214;&#21040;&#949;-&#31283;&#23450;&#28857;&#30340;&#36890;&#20449;&#22797;&#26434;&#24615;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#25928;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;Adam&#65292;&#29992;&#20110;&#38543;&#26426;&#38750;&#20984;&#20248;&#21270;&#30340;&#21442;&#25968;&#26381;&#21153;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;"&#39640;&#25928;-Adam"&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#21452;&#21521;&#37327;&#21270;&#26041;&#26696;&#32435;&#20837;&#39640;&#25928;-Adam&#65292;&#20197;&#20943;&#23569;&#24037;&#20316;&#32773;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#37319;&#29992;&#21452;&#21521;&#35823;&#24046;&#21453;&#39304;&#31574;&#30053;&#65292;&#20998;&#21035;&#20943;&#23567;&#20102;&#21452;&#21521;&#37327;&#21270;&#23545;&#26381;&#21153;&#22120;&#21644;&#24037;&#20316;&#32773;&#30340;&#20559;&#24046;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#31867;&#37327;&#21270;&#31639;&#23376;&#30340;&#39640;&#25928;-Adam&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#24182;&#36827;&#19968;&#27493;&#21051;&#30011;&#20102;&#26381;&#21153;&#22120;&#21644;&#24037;&#20316;&#32773;&#20043;&#38388;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#65292;&#24403;&#949;-&#31283;&#23450;&#28857;&#28385;&#36275;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed adaptive stochastic gradient methods have been widely used for large-scale nonconvex optimization, such as training deep learning models. However, their communication complexity on finding $\varepsilon$-stationary points has rarely been analyzed in the nonconvex setting. In this work, we present a novel communication-efficient distributed Adam in the parameter-server model for stochastic nonconvex optimization, dubbed {\em Efficient-Adam}. Specifically, we incorporate a two-way quantization scheme into Efficient-Adam to reduce the communication cost between the workers and server. Simultaneously, we adopt a two-way error feedback strategy to reduce the biases caused by the two-way quantization on both the server and workers, respectively. In addition, we establish the iteration complexity for the proposed Efficient-Adam with a class of quantization operators, and further characterize its communication complexity between the server and workers when an $\varepsilon$-stationar
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#20840;&#23616;&#20108;&#36827;&#21046;&#25513;&#30721;&#26469;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20013;&#32467;&#26500;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22120;&#23448;&#30340;&#35299;&#21078;&#24418;&#29366;&#21644;&#20301;&#32622;&#20449;&#24687;&#30340;&#19968;&#33268;&#24615;&#26469;&#36827;&#34892;&#20998;&#21106;&#12290;&#30740;&#31350;&#20102;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#65292;&#24182;&#22312;&#33041;&#37096;&#21644;&#24515;&#33039;CT&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2205.09107</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#21033;&#29992;&#20840;&#23616;&#20108;&#36827;&#21046;&#25513;&#30721;&#36827;&#34892;&#32467;&#26500;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Leveraging Global Binary Masks for Structure Segmentation in Medical Images. (arXiv:2205.09107v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09107
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#20840;&#23616;&#20108;&#36827;&#21046;&#25513;&#30721;&#26469;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20013;&#32467;&#26500;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22120;&#23448;&#30340;&#35299;&#21078;&#24418;&#29366;&#21644;&#20301;&#32622;&#20449;&#24687;&#30340;&#19968;&#33268;&#24615;&#26469;&#36827;&#34892;&#20998;&#21106;&#12290;&#30740;&#31350;&#20102;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#65292;&#24182;&#22312;&#33041;&#37096;&#21644;&#24515;&#33039;CT&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21463;&#21040;&#36755;&#20837;&#22270;&#20687;&#24378;&#24230;&#21464;&#21270;&#30340;&#24433;&#21709;&#24456;&#22823;&#65292;&#24182;&#19988;&#30001;&#20110;&#20027;&#35201;&#21033;&#29992;&#20687;&#32032;&#24378;&#24230;&#20449;&#24687;&#36827;&#34892;&#25512;&#29702;&#65292;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#33719;&#21462;&#36275;&#22815;&#30340;&#35757;&#32451;&#25968;&#25454;&#26159;&#38480;&#21046;&#27169;&#22411;&#24212;&#29992;&#30340;&#21478;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#21307;&#23398;&#22270;&#20687;&#20013;&#22120;&#23448;&#30340;&#35299;&#21078;&#24418;&#29366;&#21644;&#20301;&#32622;&#20449;&#24687;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20840;&#23616;&#20108;&#36827;&#21046;&#25513;&#30721;&#26469;&#21033;&#29992;&#37325;&#22797;&#20986;&#29616;&#30340;&#35299;&#21078;&#27169;&#24335;&#36827;&#34892;&#22120;&#23448;&#20998;&#21106;&#12290;&#30740;&#31350;&#20102;&#20004;&#31181;&#24773;&#20917;&#65306;1&#65289;&#20840;&#23616;&#20108;&#36827;&#21046;&#25513;&#30721;&#26159;&#27169;&#22411;(U-Net)&#30340;&#21807;&#19968;&#36755;&#20837;&#65292;&#24378;&#21046;&#36827;&#34892;&#22120;&#23448;&#30340;&#23450;&#20301;&#21644;&#24418;&#29366;&#20449;&#24687;&#32534;&#30721;&#36827;&#34892;&#20998;&#21106;&#12290;2&#65289;&#23558;&#20840;&#23616;&#20108;&#36827;&#21046;&#25513;&#30721;&#20316;&#20026;&#38468;&#21152;&#36890;&#36947;&#65292;&#24182;&#29992;&#20316;&#20301;&#32622;&#21644;&#24418;&#29366;&#32447;&#32034;&#20197;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290;&#23558;&#20004;&#20010;&#21253;&#21547;&#33041;&#37096;&#21644;&#24515;&#33039;CT&#22270;&#20687;&#21450;&#20854;&#22320;&#38754;&#23454;&#20917;&#30340;&#25968;&#25454;&#38598;&#20998;&#20026;(26:10:10)&#21644;(12:3:5)&#36827;&#34892;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) models for medical image segmentation are highly influenced by intensity variations of input images and lack generalization due to primarily utilizing pixels' intensity information for inference. Acquiring sufficient training data is another challenge limiting models' applications. We proposed to leverage the consistency of organs' anatomical shape and position information in medical images. We introduced a framework leveraging recurring anatomical patterns through global binary masks for organ segmentation. Two scenarios were studied.1) Global binary masks were the only model's (i.e. U-Net) input, forcing exclusively encoding organs' position and shape information for segmentation/localization.2) Global binary masks were incorporated as an additional channel functioning as position/shape clues to mitigate training data scarcity. Two datasets of the brain and heart CT images with their ground-truth were split into (26:10:10) and (12:3:5) for training, validation, and
&lt;/p&gt;</description></item><item><title>StableDR&#26159;&#19968;&#31181;&#31283;&#23450;&#30340;&#21452;&#37325;&#31283;&#20581;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#25968;&#25454;&#32570;&#22833;&#38750;&#38543;&#26426;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20943;&#23569;&#23545;&#22806;&#25512;&#30340;&#20381;&#36182;&#65292;StableDR&#33021;&#22815;&#21516;&#26102;&#20855;&#26377;&#26377;&#30028;&#30340;&#20559;&#24046;&#12289;&#26041;&#24046;&#21644;&#27867;&#21270;&#35823;&#24046;&#30028;&#65292;&#22312;&#19981;&#20934;&#30830;&#30340;&#20272;&#35745;&#35823;&#24046;&#21644;&#20219;&#24847;&#23567;&#30340;&#20542;&#21521;&#24615;&#19979;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.04701</link><description>&lt;p&gt;
StableDR:&#31283;&#23450;&#30340;&#21452;&#37325;&#31283;&#20581;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#25968;&#25454;&#32570;&#22833;&#38750;&#38543;&#26426;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
StableDR: Stabilized Doubly Robust Learning for Recommendation on Data Missing Not at Random. (arXiv:2205.04701v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.04701
&lt;/p&gt;
&lt;p&gt;
StableDR&#26159;&#19968;&#31181;&#31283;&#23450;&#30340;&#21452;&#37325;&#31283;&#20581;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#25968;&#25454;&#32570;&#22833;&#38750;&#38543;&#26426;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20943;&#23569;&#23545;&#22806;&#25512;&#30340;&#20381;&#36182;&#65292;StableDR&#33021;&#22815;&#21516;&#26102;&#20855;&#26377;&#26377;&#30028;&#30340;&#20559;&#24046;&#12289;&#26041;&#24046;&#21644;&#27867;&#21270;&#35823;&#24046;&#30028;&#65292;&#22312;&#19981;&#20934;&#30830;&#30340;&#20272;&#35745;&#35823;&#24046;&#21644;&#20219;&#24847;&#23567;&#30340;&#20542;&#21521;&#24615;&#19979;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#29992;&#25143;&#20542;&#21521;&#20110;&#36873;&#25321;&#33258;&#24049;&#21916;&#27426;&#30340;&#29289;&#21697;&#36827;&#34892;&#35780;&#20215;&#65292;&#36825;&#23548;&#33268;&#20102;&#25968;&#25454;&#32570;&#22833;&#38750;&#38543;&#26426;&#30340;&#38382;&#39064;&#65292;&#22312;&#23545;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#26080;&#20559;&#35780;&#20272;&#21644;&#23398;&#20064;&#26102;&#24102;&#26469;&#20102;&#24456;&#22823;&#25361;&#25112;&#12290;&#30446;&#21069;&#65292;&#21452;&#37325;&#31283;&#20581;&#65288;DR&#65289;&#26041;&#27861;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DR&#26041;&#27861;&#30340;&#19981;&#31283;&#23450;&#24615;&#20197;&#21450;&#23545;&#26497;&#23567;&#30340;&#20542;&#21521;&#24615;&#20855;&#26377;&#26080;&#30028;&#20559;&#24046;&#12289;&#26041;&#24046;&#21644;&#27867;&#21270;&#30028;&#38480;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;DR&#26356;&#22810;&#22320;&#20381;&#36182;&#22806;&#25512;&#65292;&#36825;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#21452;&#37325;&#31283;&#20581;&#65288;StableDR&#65289;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#22806;&#25512;&#30340;&#20381;&#36182;&#36739;&#24369;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#19981;&#20934;&#30830;&#30340;&#20272;&#35745;&#35823;&#24046;&#21644;&#20219;&#24847;&#23567;&#30340;&#20542;&#21521;&#24615;&#19979;&#65292;StableDR&#21516;&#26102;&#20855;&#26377;&#26377;&#30028;&#30340;&#20559;&#24046;&#12289;&#26041;&#24046;&#21644;&#27867;&#21270;&#35823;&#24046;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;StableDR&#30340;&#26032;&#22411;&#23398;&#20064;&#26041;&#27861;&#26469;&#26356;&#26032;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recommender systems, users always choose the favorite items to rate, which leads to data missing not at random and poses a great challenge for unbiased evaluation and learning of prediction models. Currently, the doubly robust (DR) methods have been widely studied and demonstrate superior performance. However, in this paper, we show that DR methods are unstable and have unbounded bias, variance, and generalization bounds to extremely small propensities. Moreover, the fact that DR relies more on extrapolation will lead to suboptimal performance. To address the above limitations while retaining double robustness, we propose a stabilized doubly robust (StableDR) learning approach with a weaker reliance on extrapolation. Theoretical analysis shows that StableDR has bounded bias, variance, and generalization error bound simultaneously under inaccurate imputed errors and arbitrarily small propensities. In addition, we propose a novel learning approach for StableDR that updates the imputat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27969;&#24418;&#19978;&#30340;min-max&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;Riemannian Hamiltonian&#26041;&#27861;&#20316;&#20026;&#20854;&#20195;&#29702;&#26041;&#27861;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;Hamiltonian&#20989;&#25968;&#65292;&#21487;&#20197;&#24471;&#21040;&#25152;&#38656;&#30340;min-max&#38797;&#28857;&#12290;&#35813;&#26041;&#27861;&#22312;geodesic-bilinear&#20248;&#21270;&#38382;&#39064;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#36890;&#36807;&#35299;&#20915;&#20195;&#29702;&#38382;&#39064;&#21487;&#20197;&#24471;&#21040;&#20840;&#23616;&#26368;&#20248;&#25628;&#32034;&#26041;&#21521;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.11418</link><description>&lt;p&gt;
&#27969;&#24418;&#19978;&#30340;Riemannian Hamiltonian&#26041;&#27861;&#29992;&#20110;min-max&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Riemannian Hamiltonian methods for min-max optimization on manifolds. (arXiv:2204.11418v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27969;&#24418;&#19978;&#30340;min-max&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;Riemannian Hamiltonian&#26041;&#27861;&#20316;&#20026;&#20854;&#20195;&#29702;&#26041;&#27861;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;Hamiltonian&#20989;&#25968;&#65292;&#21487;&#20197;&#24471;&#21040;&#25152;&#38656;&#30340;min-max&#38797;&#28857;&#12290;&#35813;&#26041;&#27861;&#22312;geodesic-bilinear&#20248;&#21270;&#38382;&#39064;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#36890;&#36807;&#35299;&#20915;&#20195;&#29702;&#38382;&#39064;&#21487;&#20197;&#24471;&#21040;&#20840;&#23616;&#26368;&#20248;&#25628;&#32034;&#26041;&#21521;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27969;&#24418;&#19978;&#30340;min-max&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;Riemannian Hamiltonian&#20989;&#25968;&#65292;&#20854;&#26368;&#23567;&#21270;&#20316;&#20026;&#35299;&#20915;&#21407;&#22987;min-max&#38382;&#39064;&#30340;&#20195;&#29702;&#12290;&#22312;Riemannian Polyak-{\L}ojasiewicz&#26465;&#20214;&#19979;&#65292;&#20854;&#26368;&#23567;&#20540;&#23545;&#24212;&#20110;&#25152;&#38656;&#30340;min-max&#38797;&#28857;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#28385;&#36275;&#27492;&#26465;&#20214;&#30340;&#24773;&#20917;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;geodesic-bilinear&#20248;&#21270;&#65292;&#22312;&#35299;&#20915;&#20195;&#29702;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#24471;&#21040;&#27491;&#30830;&#30340;&#20840;&#23616;&#26368;&#20248;&#25628;&#32034;&#26041;&#21521;&#65292;&#32780;&#22312;min-max&#24418;&#24335;&#21270;&#20013;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;Hamiltonian&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Riemannian Hamiltonian&#26041;&#27861;&#65288;RHM&#65289;&#24182;&#25552;&#20986;&#20102;&#23427;&#20204;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;RHM&#25193;&#23637;&#21040;&#21253;&#25324;&#20849;&#35782;&#27491;&#21017;&#21270;&#21644;&#38543;&#26426;&#35774;&#32622;&#12290;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#22914;&#23376;&#31354;&#38388;&#40065;&#26834;Wasserstein&#36317;&#31163;&#12289;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#35757;&#32451;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#31561;&#26469;&#35828;&#26126;&#25152;&#25552;&#20986;&#30340;RHM&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study min-max optimization problems on Riemannian manifolds. We introduce a Riemannian Hamiltonian function, minimization of which serves as a proxy for solving the original min-max problems. Under the Riemannian Polyak--{\L}ojasiewicz condition on the Hamiltonian function, its minimizer corresponds to the desired min-max saddle point. We also provide cases where this condition is satisfied. For geodesic-bilinear optimization in particular, solving the proxy problem leads to the correct search direction towards global optimality, which becomes challenging with the min-max formulation. To minimize the Hamiltonian function, we propose Riemannian Hamiltonian methods (RHM) and present their convergence analyses. We extend RHM to include consensus regularization and to the stochastic setting. We illustrate the efficacy of the proposed RHM in applications such as subspace robust Wasserstein distance, robust training of neural networks, and generative adversarial networks.
&lt;/p&gt;</description></item><item><title>FlexFringe&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#27010;&#29575;&#26377;&#38480;&#33258;&#21160;&#26426;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#24314;&#27169;&#36719;&#20214;&#34892;&#20026;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#36890;&#36807;&#23454;&#29616;&#25913;&#36827;&#30340;&#29366;&#24577;&#21512;&#24182;&#31574;&#30053;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#33021;&#22815;&#20174;&#36719;&#20214;&#26085;&#24535;&#20013;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#12290;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#23398;&#20064;&#26356;&#23567;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#33021;&#22815;&#25552;&#39640;FlexFringe&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.16331</link><description>&lt;p&gt;
FlexFringe:&#36890;&#36807;&#23398;&#20064;&#27010;&#29575;&#26377;&#38480;&#33258;&#21160;&#26426;&#26469;&#24314;&#27169;&#36719;&#20214;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
FlexFringe: Modeling Software Behavior by Learning Probabilistic Automata. (arXiv:2203.16331v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16331
&lt;/p&gt;
&lt;p&gt;
FlexFringe&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#27010;&#29575;&#26377;&#38480;&#33258;&#21160;&#26426;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#24314;&#27169;&#36719;&#20214;&#34892;&#20026;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#36890;&#36807;&#23454;&#29616;&#25913;&#36827;&#30340;&#29366;&#24577;&#21512;&#24182;&#31574;&#30053;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#33021;&#22815;&#20174;&#36719;&#20214;&#26085;&#24535;&#20013;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#12290;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#23398;&#20064;&#26356;&#23567;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#33021;&#22815;&#25552;&#39640;FlexFringe&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;FlexFringe&#20013;&#21487;&#29992;&#30340;&#27010;&#29575;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#23398;&#20064;&#26041;&#27861;&#30340;&#39640;&#25928;&#23454;&#29616;&#12290;&#36825;&#20123;&#23454;&#29616;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#29366;&#24577;&#21512;&#24182;&#31574;&#30053;&#65292;&#21253;&#25324;&#20960;&#31181;&#20462;&#25913;&#20197;&#25552;&#39640;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#33719;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#40664;&#35748;&#23454;&#29616;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;FlexFringe&#20174;&#36719;&#20214;&#26085;&#24535;&#20013;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#36739;&#38590;&#35299;&#37322;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#20064;&#26356;&#23567;&#12289;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#22914;&#20309;&#25552;&#39640;FlexFringe&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the efficient implementations of probabilistic deterministic finite automaton learning methods available in FlexFringe. These implement well-known strategies for state-merging including several modifications to improve their performance in practice. We show experimentally that these algorithms obtain competitive results and significant improvements over a default implementation. We also demonstrate how to use FlexFringe to learn interpretable models from software logs and use these for anomaly detection. Although less interpretable, we show that learning smaller more convoluted models improves the performance of FlexFringe on anomaly detection, outperforming an existing solution based on neural nets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BagPipe&#65292;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#28145;&#24230;&#25512;&#33616;&#27169;&#22411;&#35757;&#32451;&#30340;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#23884;&#20837;&#35775;&#38382;&#30340;&#29305;&#23450;&#32467;&#26500;&#65292;&#36890;&#36807;&#32531;&#23384;&#21644;&#39044;&#21462;&#30340;&#26041;&#24335;&#20248;&#21270;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#25512;&#33616;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2202.12429</link><description>&lt;p&gt;
BagPipe&#65306;&#21152;&#36895;&#28145;&#24230;&#25512;&#33616;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
BagPipe: Accelerating Deep Recommendation Model Training. (arXiv:2202.12429v3 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BagPipe&#65292;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#28145;&#24230;&#25512;&#33616;&#27169;&#22411;&#35757;&#32451;&#30340;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#23884;&#20837;&#35775;&#38382;&#30340;&#29305;&#23450;&#32467;&#26500;&#65292;&#36890;&#36807;&#32531;&#23384;&#21644;&#39044;&#21462;&#30340;&#26041;&#24335;&#20248;&#21270;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#25512;&#33616;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#27169;&#22411;&#65288;DLRM&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#20960;&#20010;&#20851;&#38190;&#30340;&#21830;&#19994;&#24212;&#29992;&#12290;&#39640;&#25928;&#22320;&#35757;&#32451;&#36825;&#31181;&#25512;&#33616;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21253;&#21547;&#25968;&#21313;&#20159;&#20010;&#22522;&#20110;&#23884;&#20837;&#30340;&#21442;&#25968;&#65292;&#20174;&#23884;&#20837;&#35775;&#38382;&#20013;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#24320;&#38144;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#30340;DLRM&#35757;&#32451;&#31995;&#32479;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#32422;75&#65285;&#30340;&#36845;&#20195;&#26102;&#38388;&#29992;&#20110;&#23884;&#20837;&#35775;&#38382;&#21644;&#27169;&#22411;&#21516;&#27493;&#12290;&#26412;&#25991;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#23884;&#20837;&#35775;&#38382;&#20855;&#26377;&#29305;&#23450;&#30340;&#32467;&#26500;&#65292;&#21487;&#20197;&#29992;&#20110;&#21152;&#36895;&#35757;&#32451;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#23884;&#20837;&#35775;&#38382;&#20855;&#26377;&#20005;&#37325;&#30340;&#20559;&#26012;&#24615;&#65292;&#32422;1&#65285;&#30340;&#23884;&#20837;&#34920;&#31034;&#20102;&#36229;&#36807;92&#65285;&#30340;&#24635;&#35775;&#38382;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#31163;&#32447;&#35757;&#32451;&#26399;&#38388;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#25209;&#27425;&#26469;&#30830;&#23450;&#23558;&#22312;&#23558;&#26469;&#30340;&#20309;&#26102;&#36845;&#20195;&#38656;&#35201;&#21738;&#20123;&#23884;&#20837;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Bagpipe&#65292;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#25512;&#33616;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#32531;&#23384;&#21644;&#39044;&#21462;&#26469;&#20248;&#21270;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning based recommendation models (DLRM) are widely used in several business critical applications. Training such recommendation models efficiently is challenging because they contain billions of embedding-based parameters, leading to significant overheads from embedding access. By profiling existing systems for DLRM training, we observe that around 75\% of the iteration time is spent on embedding access and model synchronization. Our key insight in this paper is that embedding access has a specific structure which can be used to accelerate training. We observe that embedding accesses are heavily skewed, with around 1\% of embeddings representing more than 92\% of total accesses. Further, we observe that during offline training we can lookahead at future batches to determine exactly which embeddings will be needed at what iteration in the future. Based on these insights, we develop Bagpipe, a system for training deep recommendation models that uses caching and prefetching to ov
&lt;/p&gt;</description></item><item><title>&#22810;&#39033;&#24335;&#26041;&#27861;&#22312;&#26080;&#20998;&#24067;&#30456;&#20851;SQ&#23398;&#20064;&#20013;&#26159;&#36890;&#29992;&#30340;&#65292;&#24182;&#19988;&#26159;&#26368;&#20339;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2010.11925</link><description>&lt;p&gt;
&#22810;&#39033;&#24335;&#26041;&#27861;&#22312;&#26080;&#20998;&#24067;&#30456;&#20851;SQ&#23398;&#20064;&#20013;&#26159;&#36890;&#29992;&#30340;
&lt;/p&gt;
&lt;p&gt;
The Polynomial Method is Universal for Distribution-Free Correlational SQ Learning. (arXiv:2010.11925v3 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.11925
&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#24335;&#26041;&#27861;&#22312;&#26080;&#20998;&#24067;&#30456;&#20851;SQ&#23398;&#20064;&#20013;&#26159;&#36890;&#29992;&#30340;&#65292;&#24182;&#19988;&#26159;&#26368;&#20339;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;PAC&#27169;&#22411;&#21644;agnostic&#27169;&#22411;&#20013;&#30340;&#24067;&#23572;&#20989;&#25968;&#31867;&#30340;&#26080;&#20998;&#24067;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#25512;&#24191;Malach&#21644;Shalev-Shwartz&#65288;2022&#24180;&#65289;&#30340;&#20248;&#31168;&#24037;&#20316;&#65292;&#32473;&#20986;&#20102;&#23398;&#20064;DNF&#20844;&#24335;&#30340;&#32039;&#20945;&#22411;SQ&#65288;CSQ&#65289;&#19979;&#30028;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#20309;&#20989;&#25968;&#31867;&#65292;&#38408;&#20540;&#25110;&#36817;&#20284;&#24230;&#30340;&#19979;&#30028;&#30452;&#25509;&#23548;&#33268;PAC&#25110;agnostic&#23398;&#20064;&#30340;CSQ&#19979;&#30028;&#12290;&#34429;&#28982;&#36890;&#36807;&#32467;&#21512;Feldman&#65288;2008&#12289;2012&#65289;&#21644;Sherstov&#65288;2008&#12289;2011&#65289;&#30340;&#20808;&#21069;&#32467;&#26524;&#65292;&#36825;&#20123;&#19979;&#30028;&#21487;&#20197;&#38544;&#21547;&#22320;&#24471;&#20986;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#32473;&#20986;&#30340;&#31934;&#30830;&#38472;&#36848;&#20197;&#27492;&#24418;&#24335;&#23578;&#26410;&#20986;&#29616;&#36807;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35777;&#26126;&#26041;&#27861;&#31616;&#21333;&#19988;&#22522;&#26412;&#29420;&#31435;&#12290;&#36825;&#20123;&#19979;&#30028;&#19982;PAC&#25110;agnostic&#23398;&#20064;&#20013;&#20351;&#29992;&#38408;&#20540;&#25110;&#36817;&#20284;&#24230;&#30340;SQ&#27169;&#22411;&#30340;&#30456;&#24212;&#27491;&#38754;&#32467;&#26524;&#30456;&#21305;&#37197;&#65292;&#20174;&#36825;&#20010;&#24847;&#20041;&#19978;&#35828;&#65292;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#22810;&#39033;&#24335;&#26041;&#27861;&#26159;&#19968;&#31181;&#26080;&#20998;&#24067;CSQ&#23398;&#20064;&#30340;&#36890;&#29992;&#19988;&#26368;&#20339;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of distribution-free learning for Boolean function classes in the PAC and agnostic models. Generalizing a beautiful work of Malach and Shalev-Shwartz (2022) that gave tight correlational SQ (CSQ) lower bounds for learning DNF formulas, we give new proofs that lower bounds on the threshold or approximate degree of any function class directly imply CSQ lower bounds for PAC or agnostic learning respectively. While such bounds implicitly follow by combining prior results by Feldman (2008, 2012) and Sherstov (2008, 2011), to our knowledge the precise statements we give had not appeared in this form before. Moreover, our proofs are simple and largely self-contained.  These lower bounds match corresponding positive results using upper bounds on the threshold or approximate degree in the SQ model for PAC or agnostic learning, and in this sense these results show that the polynomial method is a universal, best-possible approach for distribution-free CSQ learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;UCB&#26368;&#20248;&#25289;&#33218;&#31574;&#30053;&#65292;&#25104;&#26412;&#20026;$\sqrt{\log T}$&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#27492;&#25915;&#20987;&#31574;&#30053;&#36817;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2008.09312</link><description>&lt;p&gt;
UCB Bandits&#22312;&#23545;&#25239;&#25915;&#20987;&#20013;&#30340;&#36817;&#20046;&#26368;&#20248;&#25915;&#20987;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Near Optimal Adversarial Attack on UCB Bandits. (arXiv:2008.09312v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.09312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;UCB&#26368;&#20248;&#25289;&#33218;&#31574;&#30053;&#65292;&#25104;&#26412;&#20026;$\sqrt{\log T}$&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#27492;&#25915;&#20987;&#31574;&#30053;&#36817;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#20854;&#20013;&#22870;&#21169;&#21463;&#21040;&#23545;&#25239;&#24615;&#30772;&#22351;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#36890;&#36807;&#25805;&#20316;UCB&#21407;&#21017;&#26469;&#25289;&#21160;&#19968;&#20123;&#38750;&#26368;&#20248;&#30446;&#26631;&#33218;$T-o(T)$&#27425;&#65292;&#32047;&#31215;&#25104;&#26412;&#30340;&#26631;&#24230;&#20026;$\sqrt{\log T}$&#65292;&#20854;&#20013;$T$&#20026;&#22238;&#21512;&#25968;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#32047;&#31215;&#25915;&#20987;&#25104;&#26412;&#30340;&#31532;&#19968;&#20010;&#19979;&#30028;&#12290;&#25105;&#20204;&#30340;&#19979;&#30028;&#19982;&#25105;&#20204;&#30340;&#19978;&#30028;&#21305;&#37197;&#65292;&#38500;&#20102;$\log\log T$&#22240;&#23376;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#25915;&#20987;&#31574;&#30053;&#36817;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a stochastic multi-arm bandit problem where rewards are subject to adversarial corruption. We propose a novel attack strategy that manipulates a UCB principle into pulling some non-optimal target arm $T - o(T)$ times with a cumulative cost that scales as $\sqrt{\log T}$, where $T$ is the number of rounds. We also prove the first lower bound on the cumulative attack cost. Our lower bound matches our upper bound up to $\log \log T$ factors, showing our attack to be near optimal.
&lt;/p&gt;</description></item><item><title>SWAX&#22522;&#20934;&#27979;&#35797;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Sense Wax Attack (SWAX)&#30340;&#26032;&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#30495;&#23454;&#30340;&#20154;&#31867;&#21644;&#34593;&#20687;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#29992;&#20110;&#39564;&#35777;&#38754;&#37096;&#27450;&#39575;&#26816;&#27979;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#39640;&#36136;&#37327;&#30340;&#25915;&#20987;&#20173;&#28982;&#20351;&#39640;&#32423;&#27450;&#39575;&#26041;&#27861;&#26131;&#21463;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/1910.09642</link><description>&lt;p&gt;
SWAX&#22522;&#20934;&#27979;&#35797;&#65306;&#29992;&#34593;&#20687;&#25915;&#20987;&#29983;&#29289;&#35782;&#21035;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
The SWAX Benchmark: Attacking Biometric Systems with Wax Figures. (arXiv:1910.09642v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.09642
&lt;/p&gt;
&lt;p&gt;
SWAX&#22522;&#20934;&#27979;&#35797;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Sense Wax Attack (SWAX)&#30340;&#26032;&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#30495;&#23454;&#30340;&#20154;&#31867;&#21644;&#34593;&#20687;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#29992;&#20110;&#39564;&#35777;&#38754;&#37096;&#27450;&#39575;&#26816;&#27979;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#39640;&#36136;&#37327;&#30340;&#25915;&#20987;&#20173;&#28982;&#20351;&#39640;&#32423;&#27450;&#39575;&#26041;&#27861;&#26131;&#21463;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#27450;&#39575;&#25915;&#20987;&#26159;&#25351;&#20837;&#20405;&#32773;&#35797;&#22270;&#20882;&#20805;&#25658;&#24102;&#26377;&#30408;&#21033;&#35748;&#35777;&#20973;&#25454;&#30340;&#20154;&#12290;&#30001;&#20110;&#31227;&#21160;&#35774;&#22791;&#12289;&#39640;&#23433;&#20840;&#21306;&#22495;&#31561;&#23545;&#29983;&#29289;&#35782;&#21035;&#35748;&#35777;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#36825;&#19968;&#38382;&#39064;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#35805;&#39064;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Sense Wax Attack (SWAX)&#30340;&#26032;&#25968;&#25454;&#24211;&#65292;&#21253;&#21547;&#30495;&#23454;&#30340;&#20154;&#31867;&#21644;&#34593;&#20687;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#29992;&#20110;&#39564;&#35777;&#38754;&#37096;&#27450;&#39575;&#26816;&#27979;&#38382;&#39064;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;1800&#22810;&#24352;&#20154;&#33080;&#22270;&#20687;&#21644;110&#20010;55&#20154;/&#34593;&#20687;&#30340;&#35270;&#39057;&#65292;&#20998;&#20026;&#35757;&#32451;&#38598;&#12289;&#39564;&#35777;&#38598;&#21644;&#27979;&#35797;&#38598;&#65292;&#21253;&#21547;&#22823;&#37327;&#30340;&#34920;&#24773;&#12289;&#20809;&#29031;&#21644;&#23039;&#21183;&#21464;&#21270;&#12290;&#36890;&#36807;&#19982;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#39640;&#36136;&#37327;&#30340;&#25915;&#20987;&#20173;&#28982;&#20351;&#39640;&#32423;&#27450;&#39575;&#26041;&#27861;&#26131;&#21463;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
A face spoofing attack occurs when an intruder attempts to impersonate someone who carries a gainful authentication clearance. It is a trending topic due to the increasing demand for biometric authentication on mobile devices, high-security areas, among others. This work introduces a new database named Sense Wax Attack dataset (SWAX), comprised of real human and wax figure images and videos that endorse the problem of face spoofing detection. The dataset consists of more than 1800 face images and 110 videos of 55 people/waxworks, arranged in training, validation and test sets with a large range in expression, illumination and pose variations. Experiments performed with baseline methods show that despite the progress in recent years, advanced spoofing methods are still vulnerable to high-quality violation attempts.
&lt;/p&gt;</description></item></channel></rss>