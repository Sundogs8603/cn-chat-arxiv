<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#20855;&#26377;4D&#21160;&#24577;&#22330;&#26223;&#34920;&#31034;&#30340;HumanRF&#33021;&#22815;&#20174;&#22810;&#35270;&#35282;&#35270;&#39057;&#36755;&#20837;&#20013;&#25429;&#25417;&#20840;&#36523;&#22806;&#35980;&#65292;&#20197;&#39640;&#21387;&#32553;&#29575;&#25429;&#25417;&#31934;&#32454;&#32454;&#33410;&#24182;&#25903;&#25345;&#39640;&#20998;&#36776;&#29575;&#12290;ActorsHQ&#25552;&#20379;&#20102;12MP&#30340;&#38236;&#22836;&#65292;&#20026;&#38271;&#24207;&#21015;&#33719;&#24471;&#26102;&#38388;&#19978;&#36830;&#36143;&#30340;&#20154;&#29289;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2305.06356</link><description>&lt;p&gt;
HumanRF&#65306;&#29992;&#20110;&#36816;&#21160;&#20013;&#20154;&#30340;&#39640;&#20445;&#30495;&#31070;&#32463;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion. (arXiv:2305.06356v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06356
&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;4D&#21160;&#24577;&#22330;&#26223;&#34920;&#31034;&#30340;HumanRF&#33021;&#22815;&#20174;&#22810;&#35270;&#35282;&#35270;&#39057;&#36755;&#20837;&#20013;&#25429;&#25417;&#20840;&#36523;&#22806;&#35980;&#65292;&#20197;&#39640;&#21387;&#32553;&#29575;&#25429;&#25417;&#31934;&#32454;&#32454;&#33410;&#24182;&#25903;&#25345;&#39640;&#20998;&#36776;&#29575;&#12290;ActorsHQ&#25552;&#20379;&#20102;12MP&#30340;&#38236;&#22836;&#65292;&#20026;&#38271;&#24207;&#21015;&#33719;&#24471;&#26102;&#38388;&#19978;&#36830;&#36143;&#30340;&#20154;&#29289;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#22914;&#30005;&#24433;&#21046;&#20316;&#12289;&#30005;&#33041;&#28216;&#25103;&#25110;&#35270;&#39057;&#20250;&#35758;&#20013;&#65292;&#39640;&#20445;&#30495;&#22320;&#34920;&#29616;&#20154;&#31867;&#34920;&#29616;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#26500;&#24314;&#22359;&#12290;&#20026;&#20102;&#25509;&#36817;&#29983;&#20135;&#32423;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;HumanRF&#65292;&#36825;&#26159;&#19968;&#20010;4D&#21160;&#24577;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#65292;&#20174;&#22810;&#35270;&#35282;&#35270;&#39057;&#36755;&#20837;&#20013;&#25429;&#25417;&#36816;&#21160;&#20013;&#30340;&#20840;&#36523;&#22806;&#35980;&#65292;&#24182;&#20351;&#20854;&#21487;&#20197;&#22312;&#26032;&#30340;&#12289;&#30475;&#19981;&#35265;&#30340;&#35270;&#35282;&#19979;&#25773;&#25918;&#12290;&#25105;&#20204;&#30340;&#26032;&#22411;&#34920;&#31034;&#20316;&#20026;&#19968;&#20010;&#21160;&#24577;&#35270;&#39057;&#32534;&#30721;&#65292;&#36890;&#36807;&#23558;&#26102;&#31354;&#20998;&#35299;&#20026;&#19968;&#20010;&#26102;&#38388;&#30697;&#38453;&#21521;&#37327;&#20998;&#35299;&#65292;&#20197;&#39640;&#21387;&#32553;&#29575;&#25429;&#25417;&#31934;&#32454;&#32454;&#33410;&#12290;&#36825;&#20351;&#25105;&#20204;&#21487;&#20197;&#20026;&#38271;&#24207;&#21015;&#33719;&#24471;&#26102;&#38388;&#19978;&#36830;&#36143;&#30340;&#20154;&#29289;&#37325;&#24314;&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21160;&#20316;&#24773;&#20917;&#19979;&#34920;&#31034;&#39640;&#20998;&#36776;&#29575;&#30340;&#32454;&#33410;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#21512;&#25104;4MP&#25110;&#26356;&#20302;&#20998;&#36776;&#29575;&#65292;&#20294;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;12MP&#19978;&#25805;&#20316;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ActorsHQ&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#65292;&#20026;160&#20010;&#25668;&#20687;&#26426;&#25552;&#20379;&#20102;12MP&#30340;&#38236;&#22836;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representing human performance at high-fidelity is an essential building block in diverse applications, such as film production, computer games or videoconferencing. To close the gap to production-level quality, we introduce HumanRF, a 4D dynamic neural scene representation that captures full-body appearance in motion from multi-view video input, and enables playback from novel, unseen viewpoints. Our novel representation acts as a dynamic video encoding that captures fine details at high compression rates by factorizing space-time into a temporal matrix-vector decomposition. This allows us to obtain temporally coherent reconstructions of human actors for long sequences, while representing high-resolution details even in the context of challenging motion. While most research focuses on synthesizing at resolutions of 4MP or lower, we address the challenge of operating at 12MP. To this end, we introduce ActorsHQ, a novel multi-view dataset that provides 12MP footage from 160 cameras for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#30417;&#30563;&#23398;&#20064;&#20013;&#27491;&#30830;&#25439;&#22833;&#20989;&#25968;&#30340;&#27010;&#24565;&#65292;&#20854;&#36890;&#36807;&#27010;&#29575;&#27979;&#24230;&#30340;&#26465;&#20214;&#27491;&#21017;&#27010;&#29575;&#27979;&#24230;&#35299;&#20915;&#32447;&#24615;&#31639;&#23376;&#26041;&#31243;&#30340;&#38382;&#39064;&#24471;&#21040;&#23450;&#20041;&#65292;&#36866;&#29992;&#20110;&#21487;&#27979;&#31354;&#38388;&#30340;&#36755;&#20837;&#31354;&#38388;&#21644;&#26631;&#31614;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.06348</link><description>&lt;p&gt;
&#24102;&#27010;&#29575;&#24577;&#23556;&#21644;&#26680;&#24179;&#22343;&#23884;&#20837;&#30340;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Supervised learning with probabilistic morphisms and kernel mean embeddings. (arXiv:2305.06348v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#30417;&#30563;&#23398;&#20064;&#20013;&#27491;&#30830;&#25439;&#22833;&#20989;&#25968;&#30340;&#27010;&#24565;&#65292;&#20854;&#36890;&#36807;&#27010;&#29575;&#27979;&#24230;&#30340;&#26465;&#20214;&#27491;&#21017;&#27010;&#29575;&#27979;&#24230;&#35299;&#20915;&#32447;&#24615;&#31639;&#23376;&#26041;&#31243;&#30340;&#38382;&#39064;&#24471;&#21040;&#23450;&#20041;&#65292;&#36866;&#29992;&#20110;&#21487;&#27979;&#31354;&#38388;&#30340;&#36755;&#20837;&#31354;&#38388;&#21644;&#26631;&#31614;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30417;&#30563;&#23398;&#20064;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#27491;&#30830;&#25439;&#22833;&#20989;&#25968;&#30340;&#27010;&#24565;&#65292;&#36866;&#29992;&#20110;&#21487;&#27979;&#31354;&#38388;&#30340;&#36755;&#20837;&#31354;&#38388;X&#21644;&#26631;&#31614;&#31354;&#38388;Y&#12290; &#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27491;&#30830;&#25439;&#22833;&#20989;&#25968;&#24517;&#39035;&#27491;&#30830;&#22320;&#24230;&#37327;&#21487;&#33021;&#39044;&#27979;&#22120;&#30340;&#20551;&#35774;&#31354;&#38388;H&#20013;&#30340;&#20803;&#32032;&#19982;&#30417;&#31649;&#36816;&#31639;&#31526;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#32780;&#30417;&#31649;&#36816;&#31639;&#31526;&#21487;&#33021;&#19981;&#23646;&#20110;H&#12290; &#20026;&#20102;&#23450;&#20041;&#27491;&#30830;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#27010;&#29575;&#27979;&#24230;&#956;&#22312;&#25237;&#24433;&#928;X&#65306;X&#215;Y&#8594;X&#30456;&#23545;&#20110;&#27010;&#29575;&#27979;&#24230;&#956;&#119883;&#215;&#119884;&#30340;&#26465;&#20214;&#27491;&#21017;&#27010;&#29575;&#27979;&#24230;&#956;Y| X&#30340;&#29305;&#27530;&#24615;&#36136;&#30340;&#34920;&#24449;&#26041;&#27861;&#65292;&#20316;&#20026;&#32447;&#24615;&#31639;&#23376;&#26041;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290; &#22914;&#26524;Y&#26159;&#19968;&#20010;&#20855;&#26377;Borel &#963;-&#20195;&#25968; BY&#30340;&#21487;&#20998;&#30340;&#21487;&#24230;&#37327;&#21270;&#25299;&#25169;&#31354;&#38388;&#65292;&#21017;&#25552;&#20986;&#20102;&#20851;&#20110;&#27010;&#29575;&#27979;&#24230;&#956;&#30456;&#23545;&#20110;&#25237;&#24433;&#928;X&#30340;&#26465;&#20214;&#27491;&#21017;&#27010;&#29575;&#27979;&#24230;&#956;Y| X&#30340;&#21478;&#19968;&#31181;&#29305;&#27530;&#24615;&#36136;&#30340;&#34920;&#24449;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper I propose a concept of a correct loss function in a generative model of supervised learning for an input space $\mathcal{X}$ and a label space $\mathcal{Y}$, which are measurable spaces. A correct loss function in a generative model of supervised learning must correctly measure the discrepancy between elements of a hypothesis space $\mathcal{H}$ of possible predictors and the supervisor operator, which may not belong to $\mathcal{H}$. To define correct loss functions, I propose a characterization of a regular conditional probability measure $\mu_{\mathcal{Y}|\mathcal{X}}$ for a probability measure $\mu$ on $\mathcal{X} \times \mathcal{Y}$ relative to the projection $\Pi_{\mathcal{X}}: \mathcal{X}\times\mathcal{Y}\to \mathcal{X}$ as a solution of a linear operator equation. If $\mathcal{Y}$ is a separable metrizable topological space with the Borel $\sigma$-algebra $ \mathcal{B} (\mathcal{Y})$, I propose another characterization of a regular conditional probability measure
&lt;/p&gt;</description></item><item><title>CosmoPower-JAX&#20351;&#29992;&#21487;&#24494;&#30340;&#23431;&#23449;&#27169;&#25311;&#22120;&#36827;&#34892;&#39640;&#32500;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#20854;&#37319;&#29992;JAX&#30340;&#29305;&#24615;&#20197;&#21450;GPU&#25216;&#26415;&#21152;&#36895;&#21442;&#25968;&#20272;&#35745;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25506;&#32034;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#24182;&#22312;&#30701;&#26102;&#38388;&#20869;&#33719;&#24471;&#20934;&#30830;&#30340;&#21442;&#25968;&#21644;&#21518;&#39564;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2305.06347</link><description>&lt;p&gt;
CosmoPower-JAX:&#21033;&#29992;&#21487;&#24494;&#30340;&#23431;&#23449;&#27169;&#25311;&#22120;&#36827;&#34892;&#39640;&#32500;&#36125;&#21494;&#26031;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
CosmoPower-JAX: high-dimensional Bayesian inference with differentiable cosmological emulators. (arXiv:2305.06347v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06347
&lt;/p&gt;
&lt;p&gt;
CosmoPower-JAX&#20351;&#29992;&#21487;&#24494;&#30340;&#23431;&#23449;&#27169;&#25311;&#22120;&#36827;&#34892;&#39640;&#32500;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#20854;&#37319;&#29992;JAX&#30340;&#29305;&#24615;&#20197;&#21450;GPU&#25216;&#26415;&#21152;&#36895;&#21442;&#25968;&#20272;&#35745;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25506;&#32034;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#24182;&#22312;&#30701;&#26102;&#38388;&#20869;&#33719;&#24471;&#20934;&#30830;&#30340;&#21442;&#25968;&#21644;&#21518;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CosmoPower-JAX&#65292;&#36825;&#26159;CosmoPower&#26694;&#26550;&#30340;&#19968;&#20010;&#22522;&#20110;JAX&#30340;&#23454;&#29616;&#65292;&#36890;&#36807;&#26500;&#24314;&#23431;&#23449;&#21151;&#29575;&#35889;&#30340;&#31070;&#32463;&#20223;&#30495;&#22120;&#26469;&#21152;&#36895;&#23431;&#23449;&#23398;&#25512;&#26029;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;JAX&#30340;&#33258;&#21160;&#24494;&#20998;&#12289;&#25209;&#37327;&#35780;&#20272;&#21644;&#21363;&#26102;&#32534;&#35793;&#29305;&#24615;&#65292;&#24182;&#22312;&#22270;&#24418;&#22788;&#29702;&#22120;&#65288;GPU&#65289;&#19978;&#36816;&#34892;&#25512;&#26029;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#37319;&#26679;&#25216;&#26415;&#23558;&#21442;&#25968;&#20272;&#35745;&#21152;&#36895;&#25968;&#20493;&#12290;&#36825;&#20123;&#21487;&#20197;&#29992;&#20110;&#39640;&#25928;&#22320;&#25506;&#32034;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#65292;&#20363;&#22914;&#29992;&#20110;&#19979;&#19968;&#20195;&#23431;&#23449;&#23398;&#35843;&#26597;&#20998;&#26512;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CosmoPower-JAX&#22312;&#20004;&#20010;&#27169;&#25311;&#30340;&#31532;&#22235;&#38454;&#27573;&#37197;&#32622;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20102;&#19968;&#20010;&#36827;&#34892;37&#20010;&#27169;&#22411;&#21442;&#25968;&#30340;&#23431;&#23449;&#21098;&#20999;&#20998;&#26512;&#30340;&#21333;&#20010;&#35843;&#26597;&#12290;&#25105;&#20204;&#20351;&#29992;CosmoPower-JAX&#21644;&#21704;&#23494;&#39039;&#33945;&#29305;&#21345;&#32599;&#21462;&#26679;&#22120;&#27966;&#29983;&#30340;&#31561;&#39640;&#32447;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#19982;&#26410;&#20351;&#29992;&#20223;&#30495;&#20284;&#28982;&#30340;&#23884;&#22871;&#21462;&#26679;&#22120;&#27966;&#29983;&#30340;&#31561;&#39640;&#32447;&#30456;&#31526;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23431;&#23449;&#21098;&#20999;&#21644;&#26143;&#31995;&#32858;&#31867;&#30340;&#32852;&#21512;&#20998;&#26512;&#65292;&#23558;&#21442;&#25968;&#31354;&#38388;&#22686;&#21152;&#21040;167&#20010;&#32500;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#23454;&#29616;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#20960;&#20998;&#38047;&#32780;&#19981;&#26159;&#20960;&#22825;&#20869;&#20135;&#29983;&#21518;&#39564;&#21644;&#21442;&#25968;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present CosmoPower-JAX, a JAX-based implementation of the CosmoPower framework, which accelerates cosmological inference by building neural emulators of cosmological power spectra. We show how, using the automatic differentiation, batch evaluation and just-in-time compilation features of JAX, and running the inference pipeline on graphics processing units (GPUs), parameter estimation can be accelerated by orders of magnitude with advanced gradient-based sampling techniques. These can be used to efficiently explore high-dimensional parameter spaces, such as those needed for the analysis of next-generation cosmological surveys. We showcase the accuracy and computational efficiency of CosmoPower-JAX on two simulated Stage IV configurations. We first consider a single survey performing a cosmic shear analysis totalling 37 model parameters. We validate the contours derived with CosmoPower-JAX and a Hamiltonian Monte Carlo sampler against those derived with a nested sampler and without em
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#8212;&#8212;&#39057;&#29575;&#25903;&#25345;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#21152;&#20837;&#39057;&#29575;&#20449;&#24687;&#36866;&#24212;&#20110;&#38750;&#32447;&#24615;&#31995;&#32479;&#36776;&#35782;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06344</link><description>&lt;p&gt;
&#38754;&#21521;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#36776;&#35782;&#30340;&#39057;&#29575;&#25903;&#25345;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Frequency-Supported Neural Networks for Nonlinear Dynamical System Identification. (arXiv:2305.06344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#8212;&#8212;&#39057;&#29575;&#25903;&#25345;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#21152;&#20837;&#39057;&#29575;&#20449;&#24687;&#36866;&#24212;&#20110;&#38750;&#32447;&#24615;&#31995;&#32479;&#36776;&#35782;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#38750;&#24120;&#36890;&#29992;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#23398;&#20064;&#22810;&#20010;&#21464;&#37327;&#20043;&#38388;&#30340;&#21508;&#31181;&#20851;&#31995;&#12290;&#20854;&#20013;&#19968;&#31181;&#20851;&#31995;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#26377;&#36259;&#65292;&#21363;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#65292;&#20855;&#26377;&#22810;&#31181;&#24212;&#29992;&#12290;&#30740;&#31350;&#33021;&#22815;&#20272;&#35745;&#36825;&#31181;&#20851;&#31995;&#30340;&#27169;&#22411;&#26159;&#19968;&#38376;&#24191;&#27867;&#30340;&#23398;&#31185;&#65292;&#20855;&#26377;&#35768;&#22810;&#29702;&#35770;&#21644;&#23454;&#38469;&#32467;&#26524;&#12290;&#31070;&#32463;&#32593;&#32476;&#38750;&#24120;&#36890;&#29992;&#65292;&#20294;&#23384;&#22312;&#22810;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#20998;&#21035;&#29992;&#20110;&#29305;&#23450;&#30340;&#24212;&#29992;&#65292;&#21363;&#22270;&#20687;&#21644;&#24207;&#21015;&#22788;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#36890;&#36807;&#23558;&#39057;&#29575;&#20449;&#24687;&#32435;&#20837;&#21040;&#36890;&#29992;&#32593;&#32476;&#32467;&#26500;&#20013;&#26469;&#35843;&#25972;&#20854;&#32467;&#26500;&#65292;&#24212;&#35813;&#21487;&#20197;&#24471;&#21040;&#19968;&#31181;&#19987;&#38376;&#36866;&#29992;&#20110;&#38750;&#32447;&#24615;&#31995;&#32479;&#36776;&#35782;&#30340;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#21487;&#20197;&#28155;&#21152;&#36825;&#31181;&#39057;&#29575;&#20449;&#24687;&#32780;&#19981;&#20250;&#25439;&#22833;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26032;&#32467;&#26500;&#20026;&#39057;&#29575;&#25903;&#25345;&#31070;&#32463;&#32593;&#32476;&#65288;FSNN&#65289;&#65292;&#24182;&#22312;&#22810;&#20010;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#36776;&#35782;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are a very general type of model capable of learning various relationships between multiple variables. One example of such relationships, particularly interesting in practice, is the input-output relation of nonlinear systems, which has a multitude of applications. Studying models capable of estimating such relation is a broad discipline with numerous theoretical and practical results. Neural networks are very general, but multiple special cases exist, including convolutional neural networks and recurrent neural networks, which are adjusted for specific applications, which are image and sequence processing respectively. We formulate a hypothesis that adjusting general network structure by incorporating frequency information into it should result in a network specifically well suited to nonlinear system identification. Moreover, we show that it is possible to add this frequency information without the loss of generality from a theoretical perspective. We call this new st
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#29983;&#25104;AI&#24037;&#20316;&#27969;&#31243;&#65292;&#21487;&#20197;&#20811;&#26381;&#24403;&#21069;&#29983;&#25104;AI&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#21435;&#35774;&#35745;&#20986;&#20855;&#26377;&#39640;&#39044;&#27979;&#20146;&#21644;&#21147;&#30340;&#21487;&#34892;&#21270;&#23398;&#20998;&#23376;&#12290;</title><link>http://arxiv.org/abs/2305.06334</link><description>&lt;p&gt;
&#23558;&#29983;&#25104;AI&#19982;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#30456;&#32467;&#21512;&#26469;&#20248;&#21270;&#33647;&#29289;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Optimizing Drug Design by Merging Generative AI With Active Learning Frameworks. (arXiv:2305.06334v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06334
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#29983;&#25104;AI&#24037;&#20316;&#27969;&#31243;&#65292;&#21487;&#20197;&#20811;&#26381;&#24403;&#21069;&#29983;&#25104;AI&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#21435;&#35774;&#35745;&#20986;&#20855;&#26377;&#39640;&#39044;&#27979;&#20146;&#21644;&#21147;&#30340;&#21487;&#34892;&#21270;&#23398;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#33647;&#29289;&#30740;&#21457;&#39033;&#30446;&#27491;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#36716;&#22411;&#12290;&#20854;&#20013;&#65292;&#29983;&#25104;AI&#26041;&#27861;&#22240;&#20854;&#35774;&#35745;&#26032;&#20998;&#23376;&#21644;&#22686;&#24378;&#29616;&#26377;&#20998;&#23376;&#29305;&#24615;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#29983;&#25104;AI&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#23545;&#30446;&#26631;&#30340;&#20146;&#21644;&#21147;&#20302;&#65292;ADME/PK&#29305;&#24615;&#26410;&#30693;&#25110;&#32570;&#20047;&#21512;&#25104;&#21487;&#36861;&#28335;&#24615;&#31561;&#12290;&#20026;&#20102;&#25552;&#39640;&#29983;&#25104;AI&#26041;&#27861;&#30340;&#36866;&#29992;&#33539;&#22260;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#20027;&#21160;&#23398;&#20064;&#27493;&#39588;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#35774;&#35745;&#30340;&#29983;&#25104;AI&#24037;&#20316;&#27969;&#31243;&#20174;&#20998;&#23376;&#25351;&#26631;&#65292;&#21253;&#25324;&#33647;&#29289;&#30456;&#20284;&#24615;&#12289;&#21487;&#21512;&#25104;&#24615;&#12289;&#30456;&#20284;&#24615;&#21644;&#23545;&#25509;&#24471;&#20998;&#20013;&#36845;&#20195;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#26368;&#21518;&#30340;&#36873;&#25321;&#27493;&#39588;&#20013;&#21253;&#25324;&#20102;&#19968;&#32452;&#22522;&#20110;&#20808;&#36827;&#30340;&#20998;&#23376;&#24314;&#27169;&#27169;&#25311;&#30340;&#23618;&#27425;&#21270;&#26631;&#20934;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#27169;&#22411;&#31995;&#32479;CDK2&#21644;KRAS&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#29983;&#25104;AI&#24037;&#20316;&#27969;&#31243;&#12290;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#20102;&#20855;&#26377;&#39640;&#39044;&#27979;&#20146;&#21644;&#21147;&#30340;&#21487;&#34892;&#21270;&#23398;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional drug discovery programs are being transformed by the advent of machine learning methods. Among these, Generative AI methods (GM) have gained attention due to their ability to design new molecules and enhance specific properties of existing ones. However, current GM methods have limitations, such as low affinity towards the target, unknown ADME/PK properties, or the lack of synthetic tractability. To improve the applicability domain of GM methods, we have developed a workflow based on a variational autoencoder coupled with active learning steps. The designed GM workflow iteratively learns from molecular metrics, including drug likeliness, synthesizability, similarity, and docking scores. In addition, we also included a hierarchical set of criteria based on advanced molecular modeling simulations during a final selection step. We tested our GM workflow on two model systems, CDK2 and KRAS. In both cases, our model generated chemically viable molecules with a high predicted aff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30456;&#20284;&#24230;&#30340;&#20004;&#20010;&#35266;&#28857;&#65306;&#34920;&#31034;&#24615;&#30456;&#20284;&#21644;&#21151;&#33021;&#30456;&#20284;&#65292;&#25552;&#20379;&#20102;&#36825;&#20004;&#20010;&#23478;&#26063;&#30340;&#35814;&#32454;&#25551;&#36848;&#65292;&#24182;&#24635;&#32467;&#21644;&#35752;&#35770;&#20102;&#20854;&#23646;&#24615;&#21644;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#36341;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.06329</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#30456;&#20284;&#24230;&#65306;&#21151;&#33021;&#21644;&#34920;&#31034;&#24615;&#27979;&#37327;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Similarity of Neural Network Models: A Survey of Functional and Representational Measures. (arXiv:2305.06329v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30456;&#20284;&#24230;&#30340;&#20004;&#20010;&#35266;&#28857;&#65306;&#34920;&#31034;&#24615;&#30456;&#20284;&#21644;&#21151;&#33021;&#30456;&#20284;&#65292;&#25552;&#20379;&#20102;&#36825;&#20004;&#20010;&#23478;&#26063;&#30340;&#35814;&#32454;&#25551;&#36848;&#65292;&#24182;&#24635;&#32467;&#21644;&#35752;&#35770;&#20102;&#20854;&#23646;&#24615;&#21644;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#36341;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34913;&#37327;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#20284;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#19988;&#22791;&#21463;&#30740;&#31350;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#20197;&#20102;&#35299;&#21644;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#24046;&#24322;&#12290;&#34429;&#28982;&#26377;&#20960;&#31181;&#35266;&#28857;&#21487;&#20197;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#20284;&#24615;&#65292;&#20294;&#26159;&#26412;&#25991;&#29305;&#21035;&#20851;&#27880;&#20004;&#20010;&#20114;&#34917;&#30340;&#35266;&#28857;&#65292;&#21363;(i) &#34920;&#31034;&#24615;&#30456;&#20284;&#65292;&#32771;&#34385;&#20013;&#38388;&#31070;&#32463;&#23618;&#30340;&#28608;&#27963;&#24046;&#24322;&#65292;&#21644;(ii) &#21151;&#33021;&#30456;&#20284;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#27010;&#36848;&#20102;&#36825;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30456;&#20284;&#24615;&#27979;&#37327;&#30340;&#23478;&#26063;&#12290;&#38500;&#20102;&#25552;&#20379;&#29616;&#26377;&#27979;&#37327;&#30340;&#35814;&#32454;&#25551;&#36848;&#22806;&#65292;&#25105;&#20204;&#36824;&#24635;&#32467;&#21644;&#35752;&#35770;&#20102;&#36825;&#20123;&#27979;&#37327;&#30340;&#23646;&#24615;&#21644;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#20102;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#29992;&#24314;&#35758;&#65292;&#21487;&#20197;&#25351;&#23548;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#21033;&#29992;&#36825;&#20123;&#27979;&#37327;&#12290;&#25105;&#20204;&#24076;&#26395;&#26412;&#25991;&#20026;&#25105;&#20204;&#30340;&#31038;&#21306;&#21442;&#19982;&#26356;&#22810;&#26377;&#29992;&#30340;&#24037;&#20316;&#22880;&#23450;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring similarity of neural networks has become an issue of great importance and research interest to understand and utilize differences of neural networks. While there are several perspectives on how neural networks can be similar, we specifically focus on two complementing perspectives, i.e., (i) representational similarity, which considers how activations of intermediate neural layers differ, and (ii) functional similarity, which considers how models differ in their outputs. In this survey, we provide a comprehensive overview of these two families of similarity measures for neural network models. In addition to providing detailed descriptions of existing measures, we summarize and discuss results on the properties and relationships of these measures, and point to open research problems. Further, we provide practical recommendations that can guide researchers as well as practitioners in applying the measures. We hope our work lays a foundation for our community to engage in more s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38598;&#25104;&#22810;&#27169;&#24577;&#24863;&#30693;&#65288;IMP&#65289;&#26041;&#27861;&#65292;&#23558;&#22810;&#27169;&#24577;&#36755;&#20837;&#38598;&#25104;&#21040;&#21333;&#20010;&#32534;&#30721;&#22120;&#20013;&#65292;&#37319;&#29992;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;AGD&#65289;&#21644;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#23454;&#29616;&#39640;&#25928;&#30340;&#27169;&#22411;&#21644;&#20219;&#21153;&#25193;&#23637;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.06324</link><description>&lt;p&gt;
AGD&#21644;MoE&#29992;&#20110;&#38598;&#25104;&#22810;&#27169;&#24577;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception. (arXiv:2305.06324v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38598;&#25104;&#22810;&#27169;&#24577;&#24863;&#30693;&#65288;IMP&#65289;&#26041;&#27861;&#65292;&#23558;&#22810;&#27169;&#24577;&#36755;&#20837;&#38598;&#25104;&#21040;&#21333;&#20010;&#32534;&#30721;&#22120;&#20013;&#65292;&#37319;&#29992;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;AGD&#65289;&#21644;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#23454;&#29616;&#39640;&#25928;&#30340;&#27169;&#22411;&#21644;&#20219;&#21153;&#25193;&#23637;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#24314;&#27169;&#26041;&#27861;&#8212;&#8212;&#38598;&#25104;&#22810;&#27169;&#24577;&#24863;&#30693;&#65288;IMP&#65289;&#12290;IMP&#23558;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#25991;&#26412;&#21644;&#38899;&#39057;&#31561;&#22810;&#27169;&#24577;&#36755;&#20837;&#38598;&#25104;&#21040;&#21333;&#20010;Transformer&#32534;&#30721;&#22120;&#20013;&#65292;&#24182;&#20855;&#26377;&#26368;&#23567;&#30340;&#27169;&#24577;&#29305;&#23450;&#32452;&#20214;&#12290;IMP&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35774;&#35745;&#65292;&#23558;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;AGD&#65289;&#21644;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#27169;&#22411;&#21644;&#20219;&#21153;&#25193;&#23637;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20197;&#19979;&#20851;&#38190;&#35265;&#35299;&#65306;1&#65289;&#22312;&#22810;&#26679;&#21270;&#30340;&#24322;&#26500;&#27169;&#24577;&#12289;&#25439;&#22833;&#20989;&#25968;&#21644;&#20219;&#21153;&#19978;&#20132;&#26367;&#25191;&#34892;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#65292;&#24182;&#21516;&#26102;&#25913;&#21464;&#36755;&#20837;&#20998;&#36776;&#29575;&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#22810;&#27169;&#24577;&#29702;&#35299;&#12290;2&#65289;&#22312;&#21333;&#19968;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#32534;&#30721;&#22120;&#19978;&#20351;&#29992;MoE&#36827;&#34892;&#27169;&#22411;&#31232;&#30095;&#21270;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#32988;&#36807;&#20351;&#29992;&#27169;&#24577;&#29305;&#23450;&#32534;&#30721;&#22120;&#25110;&#39069;&#22806;&#34701;&#21512;&#23618;&#30340;&#31264;&#23494;&#27169;&#22411;&#65292;&#24182;&#22823;&#22823;&#32531;&#35299;&#27169;&#24577;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;IMP&#22312;&#19977;&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#32988;&#36807;&#20102;&#22823;&#37096;&#20998;&#24050;&#21457;&#34920;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Integrated Multimodal Perception (IMP), a simple and scalable multimodal multi-task training and modeling approach. IMP integrates multimodal inputs including image, video, text, and audio into a single Transformer encoder with minimal modality-specific components. IMP makes use of a novel design that combines Alternating Gradient Descent (AGD) and Mixture-of-Experts (MoE) for efficient model \&amp; task scaling. We conduct extensive empirical studies about IMP and reveal the following key insights: 1) performing gradient descent updates by alternating on diverse heterogeneous modalities, loss functions, and tasks, while also varying input resolutions, efficiently improves multimodal understanding. 2) model sparsification with MoE on a single modality-agnostic encoder substantially improves the performance, outperforming dense models that use modality-specific encoders or additional fusion layers and greatly mitigating the conflicts between modalities. IMP achieves competitive p
&lt;/p&gt;</description></item><item><title>&#21333;&#32431;&#22797;&#24418;&#27744;&#21270;&#23618;NervePool&#22312;&#27744;&#21270;&#22270;&#32467;&#26500;&#25968;&#25454;&#26102;&#65292;&#22522;&#20110;&#39030;&#28857;&#20998;&#21306;&#29983;&#25104;&#21333;&#32431;&#22797;&#24418;&#30340;&#20998;&#23618;&#34920;&#31034;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#24314;&#27169;&#26356;&#39640;&#38454;&#30340;&#20851;&#31995;&#65292;&#21516;&#26102;&#32553;&#23567;&#39640;&#32500;&#21333;&#32431;&#24418;&#65292;&#23454;&#29616;&#38477;&#37319;&#26679;&#65292;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.06315</link><description>&lt;p&gt;
NervePool: &#19968;&#20010;&#21333;&#32431;&#22797;&#24418;&#27744;&#21270;&#23618;
&lt;/p&gt;
&lt;p&gt;
NervePool: A Simplicial Pooling Layer. (arXiv:2305.06315v1 [cs.CG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06315
&lt;/p&gt;
&lt;p&gt;
&#21333;&#32431;&#22797;&#24418;&#27744;&#21270;&#23618;NervePool&#22312;&#27744;&#21270;&#22270;&#32467;&#26500;&#25968;&#25454;&#26102;&#65292;&#22522;&#20110;&#39030;&#28857;&#20998;&#21306;&#29983;&#25104;&#21333;&#32431;&#22797;&#24418;&#30340;&#20998;&#23618;&#34920;&#31034;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#24314;&#27169;&#26356;&#39640;&#38454;&#30340;&#20851;&#31995;&#65292;&#21516;&#26102;&#32553;&#23567;&#39640;&#32500;&#21333;&#32431;&#24418;&#65292;&#23454;&#29616;&#38477;&#37319;&#26679;&#65292;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#65292;&#27744;&#21270;&#23618;&#23545;&#20110;&#38477;&#37319;&#26679;&#12289;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#20943;&#23569;&#36807;&#25311;&#21512;&#37117;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27744;&#21270;&#23618;&#65292;NervePool&#65292;&#36866;&#29992;&#20110;&#21333;&#32431;&#22797;&#24418;&#32467;&#26500;&#30340;&#25968;&#25454;&#65292;&#36825;&#31181;&#32467;&#26500;&#26159;&#22270;&#30340;&#25512;&#24191;&#65292;&#21253;&#25324;&#27604;&#39030;&#28857;&#21644;&#36793;&#26356;&#39640;&#32500;&#24230;&#30340;&#21333;&#32431;&#24418;&#65307;&#36825;&#31181;&#32467;&#26500;&#21487;&#20197;&#26356;&#28789;&#27963;&#22320;&#24314;&#27169;&#26356;&#39640;&#38454;&#30340;&#20851;&#31995;&#12290;&#25152;&#25552;&#20986;&#30340;&#21333;&#32431;&#22797;&#21512;&#32553;&#23567;&#26041;&#26696;&#22522;&#20110;&#39030;&#28857;&#30340;&#20998;&#21306;&#26500;&#24314;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#21333;&#32431;&#22797;&#24418;&#30340;&#20998;&#23618;&#34920;&#31034;&#65292;&#20197;&#19968;&#31181;&#23398;&#20064;&#30340;&#26041;&#24335;&#25240;&#21472;&#20449;&#24687;&#12290;NervePool&#24314;&#31435;&#22312;&#23398;&#20064;&#30340;&#39030;&#28857;&#32676;&#38598;&#20998;&#37197;&#30340;&#22522;&#30784;&#19978;&#65292;&#24182;&#20197;&#19968;&#31181;&#30830;&#23450;&#24615;&#30340;&#26041;&#24335;&#25193;&#23637;&#21040;&#39640;&#32500;&#21333;&#32431;&#24418;&#30340;&#32553;&#23567;&#12290;&#34429;&#28982;&#22312;&#23454;&#36341;&#20013;&#65292;&#27744;&#21270;&#25805;&#20316;&#26159;&#36890;&#36807;&#19968;&#31995;&#21015;&#30697;&#38453;&#36816;&#31639;&#26469;&#35745;&#31639;&#30340;&#65292;&#20294;&#26159;&#20854;&#25299;&#25169;&#21160;&#26426;&#26159;&#19968;&#20010;&#22522;&#20110;&#21333;&#32431;&#24418;&#26143;&#26143;&#30340;&#24182;&#38598;&#21644;&#31070;&#32463;&#22797;&#21512;&#20307;&#30340;&#38598;&#21512;&#26500;&#36896;&#12290;
&lt;/p&gt;
&lt;p&gt;
For deep learning problems on graph-structured data, pooling layers are important for down sampling, reducing computational cost, and to minimize overfitting. We define a pooling layer, NervePool, for data structured as simplicial complexes, which are generalizations of graphs that include higher-dimensional simplices beyond vertices and edges; this structure allows for greater flexibility in modeling higher-order relationships. The proposed simplicial coarsening scheme is built upon partitions of vertices, which allow us to generate hierarchical representations of simplicial complexes, collapsing information in a learned fashion. NervePool builds on the learned vertex cluster assignments and extends to coarsening of higher dimensional simplices in a deterministic fashion. While in practice, the pooling operations are computed via a series of matrix operations, the topological motivation is a set-theoretic construction based on unions of stars of simplices and the nerve complex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;Scan2LoD3&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#22806;&#22681;&#23618;&#27425;&#30340;&#35821;&#20041;&#19977;&#32500;&#20998;&#21106;&#26469;&#31934;&#30830;&#37325;&#24314;&#20855;&#26377;&#35821;&#20041;&#20449;&#24687;&#30340;LoD3&#24314;&#31569;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.06314</link><description>&lt;p&gt;
Scan2LoD3: &#20351;&#29992;&#23556;&#32447;&#25237;&#23556;&#21644;&#36125;&#21494;&#26031;&#32593;&#32476;&#37325;&#24314;&#20855;&#26377;&#35821;&#20041;&#20449;&#24687;&#30340;LoD3&#19977;&#32500;&#24314;&#31569;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scan2LoD3: Reconstructing semantic 3D building models at LoD3 using ray casting and Bayesian networks. (arXiv:2305.06314v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;Scan2LoD3&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#22806;&#22681;&#23618;&#27425;&#30340;&#35821;&#20041;&#19977;&#32500;&#20998;&#21106;&#26469;&#31934;&#30830;&#37325;&#24314;&#20855;&#26377;&#35821;&#20041;&#20449;&#24687;&#30340;LoD3&#24314;&#31569;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31934;&#32454;&#31243;&#24230;&#20026;LoD3&#30340;&#32423;&#21035;&#19978;&#37325;&#24314;&#24102;&#26377;&#35821;&#20041;&#20449;&#24687;&#30340;&#19977;&#32500;&#24314;&#31569;&#27169;&#22411;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#19982;&#22522;&#20110;&#32593;&#26684;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#20855;&#26377;&#23436;&#20840;&#30340;&#20960;&#20309;&#24418;&#29366;&#21644;&#22806;&#22681;&#23618;&#27425;&#30340;&#29289;&#20307;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#31181;&#35201;&#27714;&#20005;&#26684;&#30340;&#35821;&#20041;&#19977;&#32500;&#37325;&#24314;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#21487;&#38752;&#22320;&#22312;&#19977;&#32500;&#36755;&#20837;&#25968;&#25454;&#30340;&#22806;&#22681;&#23618;&#27425;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Scan2LoD3&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#22806;&#22681;&#23618;&#27425;&#30340;&#35821;&#20041;&#19977;&#32500;&#20998;&#21106;&#26469;&#31934;&#30830;&#37325;&#24314;&#20855;&#26377;&#35821;&#20041;&#20449;&#24687;&#30340;LoD3&#24314;&#31569;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#28608;&#20809;&#29289;&#29702;&#23398;&#21644;&#19977;&#32500;&#24314;&#31569;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#26469;&#27010;&#29575;&#22320;&#35782;&#21035;&#27169;&#22411;&#20914;&#31361;&#12290;&#36825;&#20123;&#27010;&#29575;&#29289;&#29702;&#20914;&#31361;&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#21475;&#30340;&#20301;&#32622;&#65306;&#23427;&#20204;&#30340;&#26368;&#32456;&#35821;&#20041;&#21644;&#24418;&#29366;&#26159;&#36890;&#36807;&#34701;&#21512;&#20914;&#31361;&#12289;&#19977;&#32500;&#28857;&#20113;&#21644;&#20108;&#32500;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#27010;&#29575;&#22270;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#25512;&#26029;&#20986;&#26469;&#30340;&#12290;&#20026;&#20102;&#28385;&#36275;&#20005;&#26684;&#30340;LoD3&#35201;&#27714;&#65292;&#25105;&#20204;&#21033;&#29992;&#20272;&#35745;&#30340;&#24418;&#29366;&#22312;&#19977;&#32500;&#24314;&#31569;&#20808;&#39564;&#20013;&#20999;&#21106;&#20986;&#24320;&#21475;&#65292;&#24182;&#20174;&#24211;&#20013;&#36866;&#37197;&#35821;&#20041;&#19977;&#32500;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstructing semantic 3D building models at the level of detail (LoD) 3 is a long-standing challenge. Unlike mesh-based models, they require watertight geometry and object-wise semantics at the fa\c{c}ade level. The principal challenge of such demanding semantic 3D reconstruction is reliable fa\c{c}ade-level semantic segmentation of 3D input data. We present a novel method, called Scan2LoD3, that accurately reconstructs semantic LoD3 building models by improving fa\c{c}ade-level semantic 3D segmentation. To this end, we leverage laser physics and 3D building model priors to probabilistically identify model conflicts. These probabilistic physical conflicts propose locations of model openings: Their final semantics and shapes are inferred in a Bayesian network fusing multimodal probabilistic maps of conflicts, 3D point clouds, and 2D images. To fulfill demanding LoD3 requirements, we use the estimated shapes to cut openings in 3D building priors and fit semantic 3D objects from a libra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22522;&#20110;&#30005;&#23376;&#30149;&#21382;&#26469;&#23398;&#20064;&#33719;&#24471;&#27491;&#30830;&#35786;&#26029;&#25152;&#38656;&#30340;&#35266;&#23519;&#24207;&#21015;&#30340;&#26368;&#20248;&#39034;&#24207;&#12290;&#22240;&#20026;&#35786;&#26029;&#25351;&#21335;&#30340;&#32570;&#38519;&#65292;&#23588;&#20854;&#23545;&#32597;&#35265;&#30149;&#25110;&#24739;&#26377;&#22810;&#31181;&#30149;&#30340;&#24739;&#32773;&#65292;DRL&#31639;&#27861;&#20855;&#26377;&#37325;&#35201;&#29616;&#23454;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.06295</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20174;&#30005;&#23376;&#30149;&#21382;&#20013;&#25552;&#21462;&#35786;&#26029;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Extracting Diagnosis Pathways from Electronic Health Records Using Deep Reinforcement Learning. (arXiv:2305.06295v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22522;&#20110;&#30005;&#23376;&#30149;&#21382;&#26469;&#23398;&#20064;&#33719;&#24471;&#27491;&#30830;&#35786;&#26029;&#25152;&#38656;&#30340;&#35266;&#23519;&#24207;&#21015;&#30340;&#26368;&#20248;&#39034;&#24207;&#12290;&#22240;&#20026;&#35786;&#26029;&#25351;&#21335;&#30340;&#32570;&#38519;&#65292;&#23588;&#20854;&#23545;&#32597;&#35265;&#30149;&#25110;&#24739;&#26377;&#22810;&#31181;&#30149;&#30340;&#24739;&#32773;&#65292;DRL&#31639;&#27861;&#20855;&#26377;&#37325;&#35201;&#29616;&#23454;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35786;&#26029;&#25351;&#21335;&#26088;&#22312;&#35828;&#26126;&#21487;&#33021;&#23548;&#33268;&#35786;&#26029;&#30340;&#27493;&#39588;&#12290;&#25351;&#21335;&#33021;&#22815;&#29702;&#24615;&#22320;&#35268;&#33539;&#21270;&#20020;&#24202;&#20915;&#31574;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#30340;&#24314;&#31435;&#26159;&#20026;&#20102;&#35206;&#30422;&#22823;&#22810;&#25968;&#20154;&#32676;&#65292;&#22240;&#27492;&#22312;&#25351;&#23548;&#32597;&#35265;&#30149;&#25110;&#24739;&#26377;&#22810;&#31181;&#30149;&#30340;&#24739;&#32773;&#33719;&#24471;&#27491;&#30830;&#35786;&#26029;&#26041;&#38754;&#65292;&#23384;&#22312;&#32570;&#38519;&#12290;&#26412;&#25991;&#21463;&#25351;&#21335;&#21551;&#21457;&#65292;&#23558;&#35786;&#26029;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#20351;&#29992;&#30005;&#23376;&#30149;&#21382;(EHRs)&#35757;&#32451;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#33719;&#24471;&#27491;&#30830;&#35786;&#26029;&#25152;&#38656;&#30340;&#35266;&#23519;&#24207;&#21015;&#30340;&#26368;&#20248;&#39034;&#24207;&#12290;&#30001;&#20110;DRL&#31639;&#27861;&#30340;&#22810;&#26679;&#24615;&#21644;&#23545;&#19978;&#19979;&#25991;&#30340;&#25935;&#24863;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31181;&#26041;&#27861;&#21644;&#35774;&#32622;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#24444;&#27492;&#21644;&#32463;&#20856;&#20998;&#31867;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21512;&#25104;&#20294;&#36924;&#30495;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical diagnosis guidelines aim at specifying the steps that may lead to a diagnosis. Guidelines enable rationalizing and normalizing clinical decisions but suffer drawbacks as they are built to cover the majority of the population and may fail in guiding to the right diagnosis for patients with uncommon conditions or multiple pathologies. Moreover, their updates are long and expensive, making them unsuitable to emerging practices. Inspired by guidelines, we formulate the task of diagnosis as a sequential decision-making problem and study the use of Deep Reinforcement Learning (DRL) algorithms trained on Electronic Health Records (EHRs) to learn the optimal sequence of observations to perform in order to obtain a correct diagnosis. Because of the variety of DRL algorithms and of their sensitivity to the context, we considered several approaches and settings that we compared to each other, and to classical classifiers. We experimented on a synthetic but realistic dataset to differenti
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#36793;&#38469;&#24230;&#37327;&#26631;&#20934;&#19981;&#36275;&#20197;&#23436;&#20840;&#25429;&#25417;&#22810;&#20010;&#20114;&#21160;&#20195;&#29702;&#30340;&#32852;&#21512;&#34920;&#29616;&#65292;&#25552;&#20986;&#20351;&#29992;&#32852;&#21512;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06292</link><description>&lt;p&gt;
&#32852;&#21512;&#24230;&#37327;&#37325;&#35201;&#24615;&#65306;&#36712;&#36857;&#39044;&#27979;&#30340;&#26356;&#22909;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Joint Metrics Matter: A Better Standard for Trajectory Forecasting. (arXiv:2305.06292v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06292
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#36793;&#38469;&#24230;&#37327;&#26631;&#20934;&#19981;&#36275;&#20197;&#23436;&#20840;&#25429;&#25417;&#22810;&#20010;&#20114;&#21160;&#20195;&#29702;&#30340;&#32852;&#21512;&#34920;&#29616;&#65292;&#25552;&#20986;&#20351;&#29992;&#32852;&#21512;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#21333;&#20010;&#20195;&#29702;&#24230;&#37327;&#26631;&#20934;&#65288;&#36793;&#38469;&#24230;&#37327;&#26631;&#20934;&#65289;&#65292;&#20363;&#22914;&#26368;&#23567;&#24179;&#22343;&#20301;&#31227;&#35823;&#24046;&#65288;ADE&#65289;&#21644;&#26368;&#32456;&#20301;&#31227;&#35823;&#24046;&#65288;FDE&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#26080;&#27861;&#25429;&#25417;&#22810;&#20010;&#20114;&#21160;&#20195;&#29702;&#30340;&#32852;&#21512;&#34920;&#29616;&#12290;&#20165;&#20851;&#27880;&#36793;&#38469;&#24230;&#37327;&#26631;&#20934;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#33258;&#28982;&#30340;&#39044;&#27979;&#65292;&#22914;&#30896;&#25758;&#36712;&#36857;&#25110;&#26126;&#26174;&#19968;&#36215;&#34892;&#36208;&#30340;&#20154;&#30340;&#21457;&#25955;&#36712;&#36857;&#12290;&#22240;&#27492;&#65292;&#38024;&#23545;&#36793;&#38469;&#24230;&#37327;&#26631;&#20934;&#20248;&#21270;&#30340;&#26041;&#27861;&#20250;&#23548;&#33268;&#36807;&#20110;&#20048;&#35266;&#30340;&#24615;&#33021;&#20272;&#35745;&#65292;&#36825;&#23545;&#36712;&#36857;&#39044;&#27979;&#30740;&#31350;&#30340;&#36827;&#23637;&#26377;&#23475;&#12290;&#20026;&#20102;&#24212;&#23545;&#36793;&#38469;&#24230;&#37327;&#26631;&#20934;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#19982;&#22810;&#20195;&#29702;&#24230;&#37327;&#26631;&#20934;&#65288;&#32852;&#21512;&#24230;&#37327;&#26631;&#20934;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#65306;JADE&#12289;JFDE&#21644;&#30896;&#25758;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#37327;&#21270;&#35777;&#25454;&#21644;&#23450;&#24615;&#31034;&#20363;&#23637;&#31034;&#20102;&#32852;&#21512;&#24230;&#37327;&#26631;&#20934;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#19981;&#26159;&#36793;&#38469;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal trajectory forecasting methods commonly evaluate using single-agent metrics (marginal metrics), such as minimum Average Displacement Error (ADE) and Final Displacement Error (FDE), which fail to capture joint performance of multiple interacting agents. Only focusing on marginal metrics can lead to unnatural predictions, such as colliding trajectories or diverging trajectories for people who are clearly walking together as a group. Consequently, methods optimized for marginal metrics lead to overly-optimistic estimations of performance, which is detrimental to progress in trajectory forecasting research. In response to the limitations of marginal metrics, we present the first comprehensive evaluation of state-of-the-art (SOTA) trajectory forecasting methods with respect to multi-agent metrics (joint metrics): JADE, JFDE, and collision rate. We demonstrate the importance of joint metrics as opposed to marginal metrics with quantitative evidence and qualitative examples drawn 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; ViP &#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#20154;&#31867;&#30340;&#20219;&#21153;&#28436;&#31034;&#35270;&#39057;&#26144;&#23556;&#21040;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#20013;&#65292;&#23454;&#29616;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#25351;&#23450;&#20219;&#21153;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.06289</link><description>&lt;p&gt;
&#23398;&#20064;&#22522;&#20110;&#35270;&#39057;&#30340;&#26080;&#20154;&#20219;&#21153;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Video-Conditioned Policies for Unseen Manipulation Tasks. (arXiv:2305.06289v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06289
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; ViP &#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#20154;&#31867;&#30340;&#20219;&#21153;&#28436;&#31034;&#35270;&#39057;&#26144;&#23556;&#21040;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#20013;&#65292;&#23454;&#29616;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#25351;&#23450;&#20219;&#21153;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#38750;&#19987;&#19994;&#29992;&#25143;&#25351;&#23450;&#26426;&#22120;&#20154;&#21629;&#20196;&#26159;&#26500;&#24314;&#36890;&#29992;&#26234;&#33021;&#20307;&#33021;&#22815;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#30340;&#20851;&#38190;&#12290;&#36890;&#36807;&#20154;&#31034;&#33539;&#30446;&#26631;&#20219;&#21153;&#30340;&#35270;&#39057;&#26469;&#25351;&#23450;&#30446;&#26631;&#26426;&#22120;&#20154;&#30446;&#26631;&#26159;&#19968;&#31181;&#26041;&#20415;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35270;&#39057;&#30340;&#31574;&#30053;&#23398;&#20064;&#65288;ViP&#65289;&#65292;&#23558;&#24050;&#32463;&#30475;&#19981;&#35265;&#30340;&#20219;&#21153;&#30340;&#20154;&#31034;&#33539;&#26144;&#23556;&#21040;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to specify robot commands by a non-expert user is critical for building generalist agents capable of solving a large variety of tasks. One convenient way to specify the intended robot goal is by a video of a person demonstrating the target task. While prior work typically aims to imitate human demonstrations performed in robot environments, here we focus on a more realistic and challenging setup with demonstrations recorded in natural and diverse human environments. We propose Video-conditioned Policy learning (ViP), a data-driven approach that maps human demonstrations of previously unseen tasks to robot manipulation skills. To this end, we learn our policy to generate appropriate actions given current scene observations and a video of the target task. To encourage generalization to new tasks, we avoid particular tasks during training and learn our policy from unlabelled robot trajectories and corresponding robot videos. Both robot and human videos in our framework are rep
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20113;&#26080;&#32447;&#25509;&#20837;&#32593;&#30340;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#31354;&#20013;&#35745;&#31639;&#21644;&#21327;&#20316;&#27169;&#22411;&#32858;&#21512;&#26469;&#23454;&#29616;&#24555;&#36895;&#32780;&#31934;&#30830;&#30340;&#27169;&#22411;&#32858;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#31354;&#20013;&#35745;&#31639;&#21644;&#21069;&#31243;&#20256;&#36755;&#23481;&#37327;&#38480;&#21046;&#24341;&#36215;&#30340;&#35823;&#24046;&#38382;&#39064;&#65292;&#23545;&#35813;&#31995;&#32479;&#36827;&#34892;&#20102;&#25910;&#25947;&#20998;&#26512;&#21644;&#31995;&#32479;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.06279</link><description>&lt;p&gt;
&#20113;&#26080;&#32447;&#25509;&#20837;&#32593;&#20013;&#30340;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65306;&#25910;&#25947;&#20998;&#26512;&#19982;&#31995;&#32479;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning over Cloud-RAN: Convergence Analysis and System Optimization. (arXiv:2305.06279v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06279
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20113;&#26080;&#32447;&#25509;&#20837;&#32593;&#30340;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#31354;&#20013;&#35745;&#31639;&#21644;&#21327;&#20316;&#27169;&#22411;&#32858;&#21512;&#26469;&#23454;&#29616;&#24555;&#36895;&#32780;&#31934;&#30830;&#30340;&#27169;&#22411;&#32858;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#31354;&#20013;&#35745;&#31639;&#21644;&#21069;&#31243;&#20256;&#36755;&#23481;&#37327;&#38480;&#21046;&#24341;&#36215;&#30340;&#35823;&#24046;&#38382;&#39064;&#65292;&#23545;&#35813;&#31995;&#32479;&#36827;&#34892;&#20102;&#25910;&#25947;&#20998;&#26512;&#21644;&#31995;&#32479;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20351;&#24471;&#35774;&#22791;&#21487;&#20197;&#20174;&#29305;&#24449;&#20998;&#21306;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#26412;&#22320;&#21407;&#22987;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20113;&#26080;&#32447;&#25509;&#20837;&#32593;&#30340;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#31354;&#20013;&#35745;&#31639;&#21644;&#36890;&#36807;&#21327;&#20316;&#27169;&#22411;&#32858;&#21512;&#35299;&#20915;&#22320;&#29702;&#20998;&#24067;&#24335;&#36793;&#32536;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#24310;&#36831;&#38382;&#39064;&#65292;&#23454;&#29616;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#27169;&#22411;&#32858;&#21512;&#12290;&#28982;&#32780;&#65292;&#31354;&#20013;&#35745;&#31639;&#24341;&#36215;&#30340;&#27169;&#22411;&#32858;&#21512;&#35823;&#24046;&#20197;&#21450;&#30001;&#20110;&#38480;&#21046;&#30340;&#21069;&#31243;&#20256;&#36755;&#23481;&#37327;&#24341;&#36215;&#30340;&#37327;&#21270;&#35823;&#24046;&#20250;&#38477;&#20302;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#25910;&#25947;&#20998;&#26512;&#21644;&#31995;&#32479;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical federated learning (FL) is a collaborative machine learning framework that enables devices to learn a global model from the feature-partition datasets without sharing local raw data. However, as the number of the local intermediate outputs is proportional to the training samples, it is critical to develop communication-efficient techniques for wireless vertical FL to support high-dimensional model aggregation with full device participation. In this paper, we propose a novel cloud radio access network (Cloud-RAN) based vertical FL system to enable fast and accurate model aggregation by leveraging over-the-air computation (AirComp) and alleviating communication straggler issue with cooperative model aggregation among geographically distributed edge servers. However, the model aggregation error caused by AirComp and quantization errors caused by the limited fronthaul capacity degrade the learning performance for vertical FL. To address these issues, we characterize the convergenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FedPDD&#30340;&#38544;&#31169;&#20445;&#25252;&#21452;&#37325;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#24179;&#21488;&#32852;&#37030;&#25512;&#33616;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#25945;&#24072;&#33976;&#39311;&#21644;&#23398;&#29983;&#33976;&#39311;&#20004;&#20010;&#38454;&#27573;&#65292;&#22312;&#19981;&#20256;&#36755;&#27169;&#22411;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#36716;&#31227;&#30693;&#35782;&#21644;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#33976;&#39311;&#25439;&#22833;&#20989;&#25968;&#26469;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06272</link><description>&lt;p&gt;
FedPDD&#65306;&#29992;&#20110;&#36328;&#24179;&#21488;&#32852;&#37030;&#25512;&#33616;&#30340;&#38544;&#31169;&#20445;&#25252;&#21452;&#37325;&#33976;&#39311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedPDD: A Privacy-preserving Double Distillation Framework for Cross-silo Federated Recommendation. (arXiv:2305.06272v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FedPDD&#30340;&#38544;&#31169;&#20445;&#25252;&#21452;&#37325;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#24179;&#21488;&#32852;&#37030;&#25512;&#33616;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#25945;&#24072;&#33976;&#39311;&#21644;&#23398;&#29983;&#33976;&#39311;&#20004;&#20010;&#38454;&#27573;&#65292;&#22312;&#19981;&#20256;&#36755;&#27169;&#22411;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#36716;&#31227;&#30693;&#35782;&#21644;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#33976;&#39311;&#25439;&#22833;&#20989;&#25968;&#26469;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#24179;&#21488;&#25512;&#33616;&#26088;&#22312;&#36890;&#36807;&#20174;&#19981;&#21516;&#24179;&#21488;&#25910;&#38598;&#24322;&#26500;&#29305;&#24449;&#26469;&#25552;&#39640;&#25512;&#33616;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#25252;&#27861;&#35268;&#38480;&#21046;&#20102;&#36825;&#31181;&#24179;&#21488;&#38388;&#30340;&#36328;&#30028;&#21327;&#20316;&#65292;&#22240;&#27492;&#19981;&#33021;&#32858;&#21512;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#25512;&#33616;&#22330;&#26223;&#20013;&#22788;&#29702;&#25968;&#25454;&#23396;&#23707;&#38382;&#39064;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#36328;&#24179;&#21488;FL&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#37325;&#21472;&#29992;&#25143;&#30340;&#25968;&#25454;&#21327;&#21516;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#65292;&#20256;&#36755;&#27169;&#22411;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#37325;&#21472;&#29992;&#25143;&#30340;&#25968;&#37327;&#24448;&#24448;&#38750;&#24120;&#23567;&#65292;&#20174;&#32780;&#22823;&#22823;&#38480;&#21046;&#20102;&#27492;&#31867;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#26399;&#38388;&#20256;&#36755;&#27169;&#22411;&#20449;&#24687;&#38656;&#35201;&#39640;&#36890;&#20449;&#25104;&#26412;&#65292;&#21487;&#33021;&#20250;&#36896;&#25104;&#20005;&#37325;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;&#21452;&#37325;&#33976;&#39311;&#26694;&#26550; FedPDD &#29992;&#20110;&#36328;&#24179;&#21488;&#32852;&#37030;&#25512;&#33616;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#26377;&#25928;&#22320;&#36716;&#31227;&#30693;&#35782;&#26469;&#20445;&#25252;&#38544;&#31169;&#12290;FedPDD &#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#25945;&#24072;&#33976;&#39311;&#21644;&#23398;&#29983;&#33976;&#39311;&#12290;&#22312;&#25945;&#24072;&#33976;&#39311;&#38454;&#27573;&#65292;&#27599;&#20010;&#24179;&#21488;&#22312;&#33258;&#24049;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#65292;&#24182;&#23558;&#26469;&#33258;&#36825;&#20123;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#19968;&#20010;&#23567;&#30340;&#12289;&#24102;&#26377;&#22122;&#22768;&#30340;&#25945;&#24072;&#27169;&#22411;&#20013;&#12290;&#28982;&#21518;&#65292;&#22312;&#23398;&#29983;&#33976;&#39311;&#38454;&#27573;&#65292;&#27599;&#20010;&#24179;&#21488;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#33976;&#39311;&#25439;&#22833;&#20989;&#25968;&#65292;&#21516;&#26102;&#20174;&#25945;&#24072;&#27169;&#22411;&#21644;&#26412;&#22320;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#35757;&#32451;&#33258;&#24049;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;FedPDD &#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#36328;&#24179;&#21488;&#32852;&#37030;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-platform recommendation aims to improve recommendation accuracy by gathering heterogeneous features from different platforms. However, such cross-silo collaborations between platforms are restricted by increasingly stringent privacy protection regulations, thus data cannot be aggregated for training. Federated learning (FL) is a practical solution to deal with the data silo problem in recommendation scenarios. Existing cross-silo FL methods transmit model information to collaboratively build a global model by leveraging the data of overlapped users. However, in reality, the number of overlapped users is often very small, thus largely limiting the performance of such approaches. Moreover, transmitting model information during training requires high communication costs and may cause serious privacy leakage. In this paper, we propose a novel privacy-preserving double distillation framework named FedPDD for cross-silo federated recommendation, which efficiently transfers knowledge wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#20113;&#21407;&#29983;&#26080;&#32447;&#26550;&#26500;&#30340;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;&#65292;&#38024;&#23545;&#32593;&#32476;&#20999;&#29255;&#21644;&#22810;&#25509;&#20837;&#36793;&#32536;&#35745;&#31639;&#31561;&#22330;&#26223;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#25552;&#20986;&#20102;&#20004;&#20010;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#33021;&#22815;&#21160;&#24577;&#35757;&#32451;&#20998;&#37197;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#32593;&#32476;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.06249</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20113;&#21407;&#29983;&#26080;&#32447;&#32593;&#32476;&#36164;&#28304;&#20998;&#37197;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning Based Resource Allocation for Cloud Native Wireless Network. (arXiv:2305.06249v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#20113;&#21407;&#29983;&#26080;&#32447;&#26550;&#26500;&#30340;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;&#65292;&#38024;&#23545;&#32593;&#32476;&#20999;&#29255;&#21644;&#22810;&#25509;&#20837;&#36793;&#32536;&#35745;&#31639;&#31561;&#22330;&#26223;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#25552;&#20986;&#20102;&#20004;&#20010;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#33021;&#22815;&#21160;&#24577;&#35757;&#32451;&#20998;&#37197;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#32593;&#32476;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#21407;&#29983;&#25216;&#26415;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20102;5G&#21450;6G&#36890;&#20449;&#32593;&#32476;&#65292;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36816;&#33829;&#33258;&#21160;&#21270;&#12289;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#28982;&#32780;&#65292;&#20113;&#21407;&#29983;&#26381;&#21153;&#21644;&#24212;&#29992;&#30340;&#24191;&#27867;&#23454;&#29616;&#32473;&#21160;&#24577;&#20113;&#35745;&#31639;&#29615;&#22659;&#19979;&#30340;&#36164;&#28304;&#20998;&#37197;&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#23481;&#22120;&#30340;&#34394;&#25311;&#21270;&#23454;&#29616;&#28789;&#27963;&#26381;&#21153;&#37096;&#32626;&#30340;&#20113;&#21407;&#29983;&#26080;&#32447;&#26550;&#26500;&#65292;&#24182;&#30740;&#31350;&#20102;&#20004;&#20010;&#20195;&#34920;&#24615;&#24212;&#29992;&#22330;&#26223;&#65306;&#32593;&#32476;&#20999;&#29255;&#21644;&#22810;&#25509;&#20837;&#36793;&#32536;&#35745;&#31639;&#12290;&#20026;&#20102;&#20248;&#21270;&#36825;&#20123;&#22330;&#26223;&#19979;&#30340;&#36164;&#28304;&#20998;&#37197;&#65292;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#24341;&#20837;&#20102;&#20004;&#20010;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#33021;&#22815;&#30417;&#25511;&#32593;&#32476;&#29366;&#24577;&#24182;&#21160;&#24577;&#35757;&#32451;&#20998;&#37197;&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;Free5gc&#24320;&#21457;&#30340;&#27979;&#35797;&#24179;&#21488;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25552;&#39640;&#32593;&#32476;&#25928;&#29575;&#26041;&#38754;&#65292;&#20113;&#21407;&#29983;&#25216;&#26415;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud native technology has revolutionized 5G beyond and 6G communication networks, offering unprecedented levels of operational automation, flexibility, and adaptability. However, the vast array of cloud native services and applications presents a new challenge in resource allocation for dynamic cloud computing environments. To tackle this challenge, we investigate a cloud native wireless architecture that employs container-based virtualization to enable flexible service deployment. We then study two representative use cases: network slicing and Multi-Access Edge Computing. To optimize resource allocation in these scenarios, we leverage deep reinforcement learning techniques and introduce two model-free algorithms capable of monitoring the network state and dynamically training allocation policies. We validate the effectiveness of our algorithms in a testbed developed using Free5gc. Our findings demonstrate significant improvements in network efficiency, underscoring the potential of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21644;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#22788;&#29702;&#23454;&#20363;&#30456;&#20851;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#24456;&#22909;&#22320;&#35782;&#21035;&#39640;&#23618;&#27425;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#28508;&#22312;&#22240;&#32032;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06247</link><description>&lt;p&gt;
&#20851;&#20110;&#23454;&#20363;&#30456;&#20851;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#20215;&#20540;&#30340;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Value of Labels for Instance-Dependent Label Noise Learning. (arXiv:2305.06247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21644;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#22788;&#29702;&#23454;&#20363;&#30456;&#20851;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#24456;&#22909;&#22320;&#35782;&#21035;&#39640;&#23618;&#27425;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#28508;&#22312;&#22240;&#32032;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#65292;&#26631;&#31614;&#22122;&#22768;&#26222;&#36941;&#23384;&#22312;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#23454;&#20363;&#30456;&#20851;&#22122;&#22768;&#36716;&#31227;&#30697;&#38453;&#30340;&#19981;&#21487;&#35782;&#21035;&#24615;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#31639;&#27861;&#36890;&#36807;&#20551;&#35774;&#22122;&#22768;&#26631;&#31614;&#29983;&#25104;&#36807;&#31243;&#19982;&#23454;&#20363;&#29305;&#24449;&#26080;&#20851;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#22122;&#22768;&#26631;&#31614;&#36890;&#24120;&#21462;&#20915;&#20110;&#30495;&#23454;&#26631;&#31614;&#21644;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#22788;&#29702;&#23454;&#20363;&#30456;&#20851;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#36991;&#20813;&#20102;&#26126;&#30830;&#24314;&#27169;&#22122;&#22768;&#36716;&#31227;&#30697;&#38453;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21033;&#29992;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#65292;&#21516;&#26102;&#20174;&#25968;&#25454;&#20013;&#35782;&#21035;&#39640;&#23618;&#27425;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#28508;&#22312;&#22240;&#32032;&#12290;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#30340;&#22122;&#22768;&#26631;&#31614;&#30417;&#30563;&#20449;&#24687;&#65292;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#20363;&#30456;&#20851;&#26631;&#31614;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26174;&#33879;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label noise widely exists in large-scale datasets and significantly degenerates the performances of deep learning algorithms. Due to the non-identifiability of the instance-dependent noise transition matrix, most existing algorithms address the problem by assuming the noisy label generation process to be independent of the instance features. Unfortunately, noisy labels in real-world applications often depend on both the true label and the features. In this work, we tackle instance-dependent label noise with a novel deep generative model that avoids explicitly modeling the noise transition matrix. Our algorithm leverages casual representation learning and simultaneously identifies the high-level content and style latent factors from the data. By exploiting the supervision information of noisy labels with structural causal models, our empirical evaluations on a wide range of synthetic and real-world instance-dependent label noise datasets demonstrate that the proposed algorithm significa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#21644;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#31574;&#30053;&#65292;&#21019;&#24314;&#29992;&#20110;&#35774;&#22791;&#20869;&#22810;&#26631;&#31614;CXr&#22270;&#20687;&#20998;&#31867;&#30340;&#32039;&#20945;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;CXr&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06244</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#35774;&#22791;&#20869;&#33016;&#37096;X&#20809;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Explainable Knowledge Distillation for On-device Chest X-Ray Classification. (arXiv:2305.06244v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06244
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#21644;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#31574;&#30053;&#65292;&#21019;&#24314;&#29992;&#20110;&#35774;&#22791;&#20869;&#22810;&#26631;&#31614;CXr&#22270;&#20687;&#20998;&#31867;&#30340;&#32039;&#20945;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;CXr&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#33258;&#21160;&#22810;&#26631;&#31614;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#20998;&#31867;&#24050;&#32463;&#22312;&#20020;&#24202;&#35786;&#26029;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#28145;&#24230;&#27169;&#22411;&#20855;&#26377;&#39640;&#35745;&#31639;&#38656;&#27714;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#19981;&#22826;&#36866;&#21512;&#20302;&#35745;&#31639;&#38656;&#27714;&#30340;&#23567;&#22411;&#35774;&#22791;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#31574;&#30053;&#65292;&#29992;&#20110;&#21019;&#24314;&#29992;&#20110;&#23454;&#26102;&#22810;&#26631;&#31614;CXR&#22270;&#20687;&#20998;&#31867;&#30340;&#32039;&#20945;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;CNN&#21644;Transforms&#20316;&#20026;teacher&#26469;&#21521;&#36739;&#23567;&#30340;student&#33976;&#39311;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26469;&#25552;&#20379;&#27169;&#22411;&#20915;&#31574;&#30340;&#21487;&#35270;&#21270;&#35299;&#37322;&#65292;&#20174;&#32780;&#25913;&#21892;KD&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;CXR&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;KD&#31574;&#30053;&#25552;&#20379;&#20102;&#32039;&#20945;&#23398;&#29983;&#27169;&#22411;&#30340;&#25913;&#36827;&#24615;&#33021;&#65292;&#22240;&#27492;&#26159;&#35768;&#22810;&#26377;&#38480;&#30828;&#20214;&#24179;&#21488;&#30340;&#21487;&#34892;&#36873;&#25321;&#12290;&#20363;&#22914;&#65292;&#24403;&#20197;DenseNet161&#20316;&#20026;teacher&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27169;&#22411;&#22823;&#23567;&#20174;115 MB&#20943;&#23567;&#21040;5 MB&#65292;&#35780;&#20272;&#26102;&#38388;&#21152;&#36895;&#20102;4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated multi-label chest X-rays (CXR) image classification has achieved substantial progress in clinical diagnosis via utilizing sophisticated deep learning approaches. However, most deep models have high computational demands, which makes them less feasible for compact devices with low computational requirements. To overcome this problem, we propose a knowledge distillation (KD) strategy to create the compact deep learning model for the real-time multi-label CXR image classification. We study different alternatives of CNNs and Transforms as the teacher to distill the knowledge to a smaller student. Then, we employed explainable artificial intelligence (XAI) to provide the visual explanation for the model decision improved by the KD. Our results on three benchmark CXR datasets show that our KD strategy provides the improved performance on the compact student model, thus being the feasible choice for many limited hardware platforms. For instance, when using DenseNet161 as the teacher
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#24809;&#32602;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#23398;&#20064;&#24369;&#30456;&#20851;&#24615;&#36807;&#31243;&#65292;&#24182;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#20351;&#29992;$\theta_\infty$&#31995;&#25968;&#12290;&#25991;&#20013;&#36824;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#31070;&#35861;&#19981;&#31561;&#24335;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#23545;&#20110;&#30446;&#26631;&#20989;&#25968;&#36275;&#22815;&#20809;&#28369;&#30340;&#24773;&#20917;&#65292;&#36229;&#39069;&#39118;&#38505;&#30340;&#25910;&#25947;&#36895;&#24230;&#25509;&#36817;&#20110;$\mathcal{O}(n^{-1/3})$&#12290;&#20854;&#20013;&#25552;&#20379;&#20102;&#27169;&#25311;&#32467;&#26524;&#21644;&#24212;&#29992;&#20110;&#39044;&#27979;Vit&#243;ria&#39063;&#31890;&#29289;&#30340;&#26696;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.06230</link><description>&lt;p&gt;
&#24102;&#26377;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#24809;&#32602;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22120;&#22312;&#24369;&#30456;&#20851;&#24615;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Penalized deep neural networks estimator with general loss functions under weak dependence. (arXiv:2305.06230v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#24809;&#32602;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#23398;&#20064;&#24369;&#30456;&#20851;&#24615;&#36807;&#31243;&#65292;&#24182;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#20351;&#29992;$\theta_\infty$&#31995;&#25968;&#12290;&#25991;&#20013;&#36824;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#31070;&#35861;&#19981;&#31561;&#24335;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#23545;&#20110;&#30446;&#26631;&#20989;&#25968;&#36275;&#22815;&#20809;&#28369;&#30340;&#24773;&#20917;&#65292;&#36229;&#39069;&#39118;&#38505;&#30340;&#25910;&#25947;&#36895;&#24230;&#25509;&#36817;&#20110;$\mathcal{O}(n^{-1/3})$&#12290;&#20854;&#20013;&#25552;&#20379;&#20102;&#27169;&#25311;&#32467;&#26524;&#21644;&#24212;&#29992;&#20110;&#39044;&#27979;Vit&#243;ria&#39063;&#31890;&#29289;&#30340;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;&#20110;&#23398;&#20064;&#24369;&#30456;&#20851;&#24615;&#36807;&#31243;&#36827;&#34892;&#20102;&#31232;&#30095;&#24809;&#32602;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#12290;&#25105;&#20204;&#22788;&#29702;&#21253;&#25324;&#22238;&#24402;&#20272;&#35745;&#12289;&#20998;&#31867;&#12289;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#20869;&#30340;&#19968;&#33324;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;$\psi$&#24369;&#30456;&#20851;&#32467;&#26500;&#65292;&#24182;&#38024;&#23545;&#26377;&#30028;&#35266;&#27979;&#30340;&#29305;&#23450;&#24773;&#20917;&#65292;&#20063;&#20351;&#29992;&#20102;$\theta_\infty$&#31995;&#25968;&#12290;&#22312;&#36825;&#31181;$\theta_\infty$&#24369;&#30456;&#20381;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#22120;&#31867;&#20013;&#25552;&#20379;&#38750;&#28176;&#36817;&#27867;&#21270;&#30028;&#38480;&#12290;&#23545;&#20110;&#23398;&#20064;$\psi$&#21644;$\theta_\infty$&#24369;&#30456;&#20851;&#30340;&#36807;&#31243;&#65292;&#30830;&#23450;&#20102;&#31232;&#30095;&#24809;&#32602;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22120;&#36229;&#39069;&#39118;&#38505;&#30340;&#31070;&#35861;&#19981;&#31561;&#24335;&#12290;&#24403;&#30446;&#26631;&#20989;&#25968;&#36275;&#22815;&#20809;&#28369;&#26102;&#65292;&#36825;&#20123;&#36229;&#39069;&#39118;&#38505;&#30340;&#25910;&#25947;&#36895;&#24230;&#25509;&#36817;&#20110;$\mathcal{O}(n^{-1/3})$&#12290;&#25552;&#20379;&#20102;&#19968;&#20123;&#27169;&#25311;&#32467;&#26524;&#65292;&#24182;&#24212;&#29992;&#20110;Vit&#243;ria&#39063;&#31890;&#29289;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper carries out sparse-penalized deep neural networks predictors for learning weakly dependent processes, with a broad class of loss functions. We deal with a general framework that includes, regression estimation, classification, times series prediction, $\cdots$ The $\psi$-weak dependence structure is considered, and for the specific case of bounded observations, $\theta_\infty$-coefficients are also used. In this case of $\theta_\infty$-weakly dependent, a non asymptotic generalization bound within the class of deep neural networks predictors is provided. For learning both $\psi$ and $\theta_\infty$-weakly dependent processes, oracle inequalities for the excess risk of the sparse-penalized deep neural networks estimators are established. When the target function is sufficiently smooth, the convergence rate of these excess risk is close to $\mathcal{O}(n^{-1/3})$. Some simulation results are provided, and application to the forecast of the particulate matter in the Vit\'{o}ria
&lt;/p&gt;</description></item><item><title>&#34917;&#19969;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#38544;&#31169;&#12289;&#24322;&#26500;&#25968;&#25454;&#26469;&#28304;&#21644;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#30340;&#25361;&#25112;&#12290;&#23427;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#20114;&#34917;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#20174;&#32780;&#23454;&#29616;&#24320;&#21457;&#26356;&#20840;&#38754;&#21644;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;ML&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.06217</link><description>&lt;p&gt;
&#34917;&#19969;&#23398;&#20064;&#65306;&#23454;&#29616;&#36328;&#19981;&#21516;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#28304;&#30340;&#32508;&#21512;&#20998;&#26512;&#30340;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patchwork Learning: A Paradigm Towards Integrative Analysis across Diverse Biomedical Data Sources. (arXiv:2305.06217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06217
&lt;/p&gt;
&lt;p&gt;
&#34917;&#19969;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#38544;&#31169;&#12289;&#24322;&#26500;&#25968;&#25454;&#26469;&#28304;&#21644;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#30340;&#25361;&#25112;&#12290;&#23427;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#20114;&#34917;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#20174;&#32780;&#23454;&#29616;&#24320;&#21457;&#26356;&#20840;&#38754;&#21644;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;ML&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25552;&#20379;&#20102;&#35768;&#22810;&#22686;&#24378;&#24739;&#32773;&#25252;&#29702;&#12289;&#20154;&#21475;&#20581;&#24247;&#21644;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#24037;&#20316;&#27969;&#31243;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#12289;&#24322;&#26500;&#25968;&#25454;&#26469;&#28304;&#21644;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#30340;&#25361;&#25112;&#65292;&#29616;&#23454;&#20013;&#30340;&#20020;&#24202;&#21644;&#25104;&#26412;&#25928;&#30410;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#36825;&#31687;&#35266;&#28857;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;&#34917;&#19969;&#23398;&#20064;&#8221;&#65288;PL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#38598;&#25104;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#26469;&#28304;&#65288;&#20363;&#22914;&#65292;&#20020;&#24202;&#20813;&#36153;&#25991;&#26412;&#12289;&#21307;&#23398;&#22270;&#20687;&#12289;&#32452;&#23398;&#65289;&#21644;&#20998;&#24067;&#22312;&#19981;&#21516;&#23433;&#20840;&#31449;&#28857;&#19978;&#30340;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;PL&#20801;&#35768;&#21516;&#26102;&#21033;&#29992;&#20114;&#34917;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#20174;&#32780;&#23454;&#29616;&#24320;&#21457;&#26356;&#20840;&#38754;&#21644;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;ML&#27169;&#22411;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#34917;&#19969;&#23398;&#20064;&#30340;&#27010;&#24565;&#20197;&#21450;&#20854;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24403;&#21069;&#23454;&#29616;&#65292;&#25506;&#35752;&#20102;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#30340;&#28508;&#22312;&#26426;&#20250;&#21644;&#36866;&#29992;&#25968;&#25454;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) in healthcare presents numerous opportunities for enhancing patient care, population health, and healthcare providers' workflows. However, the real-world clinical and cost benefits remain limited due to challenges in data privacy, heterogeneous data sources, and the inability to fully leverage multiple data modalities. In this perspective paper, we introduce "patchwork learning" (PL), a novel paradigm that addresses these limitations by integrating information from disparate datasets composed of different data modalities (e.g., clinical free-text, medical images, omics) and distributed across separate and secure sites. PL allows the simultaneous utilization of complementary data sources while preserving data privacy, enabling the development of more holistic and generalizable ML models. We present the concept of patchwork learning and its current implementations in healthcare, exploring the potential opportunities and applicable data sources for addressing various
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24207;&#22810;&#29289;&#20307;&#23548;&#33322;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#22870;&#21169;&#35268;&#33539;&#65292;&#22870;&#21169;&#21333;&#20010;&#21644;&#22810;&#20010;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#30340;&#36827;&#23637;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#20934;&#30830;&#23450;&#20301;&#65292;&#36866;&#29992;&#20110;&#21160;&#24577;&#21464;&#21270;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.06178</link><description>&lt;p&gt;
&#26080;&#24207;&#22810;&#29289;&#20307;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Sequence-Agnostic Multi-Object Navigation. (arXiv:2305.06178v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06178
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24207;&#22810;&#29289;&#20307;&#23548;&#33322;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#22870;&#21169;&#35268;&#33539;&#65292;&#22870;&#21169;&#21333;&#20010;&#21644;&#22810;&#20010;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#30340;&#36827;&#23637;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#20934;&#30830;&#23450;&#20301;&#65292;&#36866;&#29992;&#20110;&#21160;&#24577;&#21464;&#21270;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#29289;&#20307;&#23548;&#33322; (MultiON) &#20219;&#21153;&#35201;&#27714;&#26426;&#22120;&#20154;&#23450;&#20301;&#22810;&#31181;&#29289;&#20307;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;&#36825;&#26159;&#23478;&#24237;&#25110;&#24037;&#21378;&#36741;&#21161;&#26426;&#22120;&#20154;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#24050;&#26377;&#30340; MultiON &#26041;&#27861;&#23558;&#27492;&#35270;&#20026;&#23545;&#35937;&#23548;&#33322; (ON) &#30340;&#30452;&#25509;&#25193;&#23637;&#65292;&#21363;&#26412;&#25991;&#25152;&#36848;&#30340; ON &#20219;&#21153;&#38656;&#35201;&#25552;&#21069;&#25351;&#23450;&#25506;&#32034;&#23545;&#35937;&#31867;&#21035;&#30340;&#39034;&#24207;&#12290;&#36825;&#22312;&#21160;&#24577;&#21464;&#21270;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#24456;&#22823;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20307;&#31995;&#32467;&#26500;&#21644;&#36866;&#24403;&#30340;&#22870;&#21169;&#35268;&#33539;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#24207; MultiON&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#36807;&#21435;&#30340;&#32463;&#39564;&#65292;&#24182;&#35797;&#22270;&#22870;&#21169;&#21333;&#20010;&#21644;&#22810;&#20010;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#22312; AI Habitat 3D &#27169;&#25311;&#29615;&#22659;&#20013;&#20351;&#29992; Gibson &#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#29031;&#29255;&#36924;&#30495;&#22330;&#26223;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Multi-Object Navigation (MultiON) task requires a robot to localize an instance (each) of multiple object classes. It is a fundamental task for an assistive robot in a home or a factory. Existing methods for MultiON have viewed this as a direct extension of Object Navigation (ON), the task of localising an instance of one object class, and are pre-sequenced, i.e., the sequence in which the object classes are to be explored is provided in advance. This is a strong limitation in practical applications characterized by dynamic changes. This paper describes a deep reinforcement learning framework for sequence-agnostic MultiON based on an actor-critic architecture and a suitable reward specification. Our framework leverages past experiences and seeks to reward progress toward individual as well as multiple target object classes. We use photo-realistic scenes from the Gibson benchmark dataset in the AI Habitat 3D simulation environment to experimentally show that our method performs bett
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21462;&#20195;&#20165;&#21463;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#65292;&#20174;&#32780;&#28040;&#38500;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#38480;&#21046;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06176</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Language Models with Generative Adversarial Feedback. (arXiv:2305.06176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21462;&#20195;&#20165;&#21463;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#65292;&#20174;&#32780;&#28040;&#38500;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#38480;&#21046;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#36755;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#30340;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;RLHF&#21463;&#21040;&#20154;&#31867;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#29983;&#20135;&#21147;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;: &#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#20195;&#26367;RLHF&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#21457;&#29616;&#34920;&#26126;&#65292;RLGAF&#21487;&#20197;&#24110;&#21161;&#23545;&#40784;LLM&#30340;&#36755;&#20986;&#65292;&#21516;&#26102;&#19981;&#20250;&#21463;&#21040;RLHF&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#20026;&#36827;&#19968;&#27493;&#33258;&#21160;&#21270;AI&#23545;&#40784;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to significantly enhance the performance of large language models (LLMs) by aligning their outputs with desired human values. However, RLHF is constrained by the expertise and productivity limitations of human evaluators. In this study, we investigate an alternative approach: Reinforcement Learning with Generative Adversarial Feedback (RLGAF) to RLHF. Our preliminary findings indicate that RLGAF can help align LLMs outputs while not suffering from the inherent restrictions of RLHF, suggesting promising avenues for further research on automating AI alignment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#24037;&#19994;&#12289;&#20513;&#23548;&#32452;&#32455;&#21644;&#27668;&#20505;&#20513;&#23548;&#32452;&#32455;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#22914;&#20309;&#24433;&#21709;&#27668;&#20505;&#21464;&#21270;&#30340;&#21465;&#20107;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#30417;&#30563;&#27169;&#22411;&#32452;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;Facebook&#19978;&#27668;&#20505;&#24191;&#21578;&#30340;&#31435;&#22330;&#12290;</title><link>http://arxiv.org/abs/2305.06174</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#27668;&#20505;&#23459;&#20256;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
Analysis of Climate Campaigns on Social Media using Bayesian Model Averaging. (arXiv:2305.06174v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#24037;&#19994;&#12289;&#20513;&#23548;&#32452;&#32455;&#21644;&#27668;&#20505;&#20513;&#23548;&#32452;&#32455;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#22914;&#20309;&#24433;&#21709;&#27668;&#20505;&#21464;&#21270;&#30340;&#21465;&#20107;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#30417;&#30563;&#27169;&#22411;&#32452;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;Facebook&#19978;&#27668;&#20505;&#24191;&#21578;&#30340;&#31435;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#21464;&#21270;&#26159;&#25105;&#20204;&#26102;&#20195;&#30340;&#26680;&#24515;&#38382;&#39064;&#65292;&#25105;&#20204;&#27491;&#22788;&#20110;&#19968;&#20010;&#20851;&#38190;&#26102;&#21051;&#12290;&#21508;&#31181;&#21033;&#30410;&#38598;&#22242;&#12289;&#31038;&#20250;&#36816;&#21160;&#32452;&#32455;&#21644;&#20010;&#20154;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#24320;&#23637;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#38598;&#20307;&#34892;&#21160;&#12290;&#27492;&#22806;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#38382;&#39064;&#20513;&#23548;&#27963;&#21160;&#24448;&#24448;&#26159;&#38024;&#23545;&#24403;&#21069;&#31038;&#20250;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#33021;&#28304;&#34892;&#19994;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#20998;&#26512;&#24037;&#19994;&#12289;&#20513;&#23548;&#32452;&#32455;&#21644;&#27668;&#20505;&#20513;&#23548;&#32452;&#32455;&#22914;&#20309;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#27668;&#20505;&#21464;&#21270;&#30340;&#21465;&#20107;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#30417;&#30563;&#27169;&#22411;&#32452;&#21512;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#28040;&#24687;&#20027;&#39064;&#26469;&#35782;&#21035;Facebook&#19978;&#27668;&#20505;&#24191;&#21578;&#30340;&#31435;&#22330;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#31435;&#22330;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#19982;&#27668;&#20505;&#23459;&#20256;&#27963;&#21160;&#30456;&#20851;&#30340;&#20027;&#39064;&#65292;&#20379;&#26410;&#26469;&#30340;&#33286;&#24773;&#25366;&#25496;&#21644;&#33258;&#21160;&#26816;&#27979;&#27668;&#20505;&#21464;&#21270;&#31435;&#22330;&#30340;&#30740;&#31350;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate change is the defining issue of our time, and we are at a defining moment. Various interest groups, social movement organizations, and individuals engage in collective action on this issue on social media. In addition, issue advocacy campaigns on social media often arise in response to ongoing societal concerns, especially those faced by energy industries. Our goal in this paper is to analyze how those industries, their advocacy group, and climate advocacy group use social media to influence the narrative on climate change. In this work, we propose a minimally supervised model soup [56] approach combined with messaging themes to identify the stances of climate ads on Facebook. Finally, we release our stance dataset, model, and set of themes related to climate campaigns for future work on opinion mining and the automatic detection of climate change stances.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;K-SpecPart&#30340;&#36229;&#22270;&#21010;&#20998;&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#65292;&#25429;&#25417;&#24179;&#34913;&#20998;&#21306;&#30446;&#26631;&#21644;&#20840;&#23616;&#36229;&#22270;&#32467;&#26500;&#65292;&#22312;&#22810;&#20803;&#21010;&#20998;&#20013;&#25552;&#39640;&#31639;&#27861;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.06167</link><description>&lt;p&gt;
K-SpecPart: &#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#36229;&#22270;&#21010;&#20998;&#35299;&#20915;&#26041;&#26696;&#25913;&#36827;&#30340;&#30417;&#30563;&#35889;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
K-SpecPart: A Supervised Spectral Framework for Multi-Way Hypergraph Partitioning Solution Improvement. (arXiv:2305.06167v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;K-SpecPart&#30340;&#36229;&#22270;&#21010;&#20998;&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#65292;&#25429;&#25417;&#24179;&#34913;&#20998;&#21306;&#30446;&#26631;&#21644;&#20840;&#23616;&#36229;&#22270;&#32467;&#26500;&#65292;&#22312;&#22810;&#20803;&#21010;&#20998;&#20013;&#25552;&#39640;&#31639;&#27861;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#36229;&#22270;&#21010;&#20998;&#22120;&#37319;&#29992;&#22810;&#23618;&#27425;&#31574;&#30053;&#65292;&#26500;&#24314;&#22810;&#20010;&#26356;&#31895;&#31961;&#30340;&#36229;&#22270;&#26469;&#36827;&#34892;&#20999;&#21106;&#23610;&#23544;&#30340;&#20248;&#21270;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#65306;&#65288;&#19968;&#65289;&#31895;&#21270;&#36807;&#31243;&#20381;&#36182;&#20110;&#23616;&#37096;&#37051;&#22495;&#32467;&#26500;&#65292;&#24573;&#30053;&#20102;&#20840;&#23616;&#36229;&#22270;&#32467;&#26500;&#65307;&#65288;&#20108;&#65289;&#20248;&#21270;&#21551;&#21457;&#24335;&#31639;&#27861;&#23384;&#22312;&#38519;&#20837;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#35889;&#26694;&#26550;&#8212;&#8212;K-SpecPart&#65292;&#36890;&#36807;&#35299;&#20915;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#25429;&#25417;&#20102;&#24179;&#34913;&#20998;&#21306;&#30446;&#26631;&#21644;&#20840;&#23616;&#36229;&#22270;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#22810;&#23618;&#27425;&#21010;&#20998;&#26041;&#26696;&#20316;&#20026;&#25552;&#31034;&#12290;&#22312;&#22810;&#20803;&#21010;&#20998;&#30340;&#24773;&#20917;&#19979;&#65292;K-SpecPart&#20174;&#22810;&#20803;&#25552;&#31034;&#21010;&#20998;&#26041;&#26696;&#20013;&#33719;&#24471;&#22810;&#20010;&#21452;&#21521;&#21010;&#20998;&#26041;&#26696;&#12290;&#23558;&#36825;&#20123;&#26041;&#26696;&#25972;&#21512;&#21040;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#20013;&#20197;&#35745;&#31639;&#29305;&#24449;&#21521;&#37327;&#65292;&#20174;&#32780;&#21019;&#24314;&#22823;&#32500;&#24230;&#23884;&#20837;&#12290;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#29992;&#20110;&#23558;&#20854;&#36716;&#25442;&#20026;&#20302;&#32500;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art hypergraph partitioners follow the multilevel paradigm, constructing multiple levels of coarser hypergraphs to drive cutsize refinement. These partitioners face limitations: (i) coarsening processes depend on local neighborhood structure, ignoring global hypergraph structure; (ii) refinement heuristics risk entrapment in local minima. We introduce K-SpecPart, a supervised spectral framework addressing these limitations by solving a generalized eigenvalue problem, capturing balanced partitioning objectives and global hypergraph structure in a low-dimensional vertex embedding while leveraging high-quality multilevel partitioning solutions as hints. In multi-way partitioning, K-SpecPart derives multiple bipartitioning solutions from a multi-way hint partitioning solution. It integrates these solutions into the generalized eigenvalue problem to compute eigenvectors, creating a large-dimensional embedding. Linear Discriminant Analysis (LDA) is used to transform this into a 
&lt;/p&gt;</description></item><item><title>EdgeNet&#26159;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#29983;&#25104;&#32593;&#32476;&#65292;&#21487;&#29992;&#20110;&#22312;&#32447;&#30005;&#21830;&#24191;&#21578;&#25293;&#21334;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#31454;&#20215;&#35774;&#35745;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;EdgeNet &#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#24191;&#21578;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#65292;&#24182;&#21033;&#29992;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#24191;&#21578;&#31454;&#25293;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.06158</link><description>&lt;p&gt;
EdgeNet&#65306;&#30005;&#23376;&#21830;&#21153;&#22312;&#32447;&#24191;&#21578;&#31454;&#20215;&#35774;&#35745;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#29983;&#25104;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
EdgeNet : Encoder-decoder generative Network for Auction Design in E-commerce Online Advertising. (arXiv:2305.06158v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06158
&lt;/p&gt;
&lt;p&gt;
EdgeNet&#26159;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#29983;&#25104;&#32593;&#32476;&#65292;&#21487;&#29992;&#20110;&#22312;&#32447;&#30005;&#21830;&#24191;&#21578;&#25293;&#21334;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#31454;&#20215;&#35774;&#35745;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;EdgeNet &#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#24191;&#21578;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#65292;&#24182;&#21033;&#29992;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#24191;&#21578;&#31454;&#25293;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#29983;&#25104;&#32593;&#32476;EdgeNet&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32447;&#30005;&#23376;&#21830;&#21153;&#24191;&#21578;&#20013;&#25968;&#25454;&#39537;&#21160;&#30340;&#25293;&#21334;&#35774;&#35745;&#12290;&#25105;&#20204;&#25171;&#30772;&#20102;&#24191;&#20041;&#27425;&#39640;&#20215;&#65288;GSP&#65289;&#30340;&#31070;&#32463;&#25293;&#21334;&#33539;&#24335;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#21033;&#29992;&#25928;&#29575;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#25293;&#21334;&#26426;&#21046;&#30340;&#32463;&#27982;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;EdgeNet&#24341;&#20837;&#20102;&#22522;&#20110;transformer&#30340;&#32534;&#30721;&#22120;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#19981;&#21516;&#24191;&#21578;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#19982;&#22522;&#20110;GSP&#30340;&#31070;&#32463;&#25293;&#21334;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#22312;&#32447;&#24191;&#21578;&#31454;&#25293;&#20013;&#30340;&#20016;&#23500;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;EdgeNet&#22312;&#27010;&#24565;&#19978;&#31616;&#21333;&#26131;&#25026;&#65292;&#24182;&#26131;&#20110;&#25193;&#23637;&#21040;&#29616;&#26377;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#25293;&#21334;&#26694;&#26550;&#20013;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#30005;&#23376;&#21830;&#21153;&#24191;&#21578;&#31454;&#25293;&#20013;&#39564;&#35777;&#20102;EdgeNet&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#25552;&#39640;&#29992;&#25143;&#20307;&#39564;&#21644;&#24179;&#21488;&#25910;&#20837;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new encoder-decoder generative network dubbed EdgeNet, which introduces a novel encoder-decoder framework for data-driven auction design in online e-commerce advertising. We break the neural auction paradigm of Generalized-Second-Price(GSP), and improve the utilization efficiency of data while ensuring the economic characteristics of the auction mechanism. Specifically, EdgeNet introduces a transformer-based encoder to better capture the mutual influence among different candidate advertisements. In contrast to GSP based neural auction model, we design an autoregressive decoder to better utilize the rich context information in online advertising auctions. EdgeNet is conceptually simple and easy to extend to the existing end-to-end neural auction framework. We validate the efficiency of EdgeNet on a wide range of e-commercial advertising auction, demonstrating its potential in improving user experience and platform revenue.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#30446;&#26631;&#20174;&#32780;&#25552;&#39640;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#19981;&#21516;&#27979;&#35797;&#22522;&#20934;&#19979;&#30340;&#34920;&#29616;&#20248;&#20110;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#35757;&#32451;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#26377;&#38480;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#23588;&#20854;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.06155</link><description>&lt;p&gt;
&#21033;&#29992;&#21512;&#25104;&#30446;&#26631;&#36827;&#34892;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Leveraging Synthetic Targets for Machine Translation. (arXiv:2305.06155v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#30446;&#26631;&#20174;&#32780;&#25552;&#39640;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#19981;&#21516;&#27979;&#35797;&#22522;&#20934;&#19979;&#30340;&#34920;&#29616;&#20248;&#20110;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#35757;&#32451;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#26377;&#38480;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#23588;&#20854;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#36164;&#28304;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#36890;&#36807;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#30446;&#26631;&#25968;&#25454;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#21452;&#35821;&#12289;&#22810;&#35821;&#35328;&#21644;&#35821;&#38899;&#32763;&#35793;&#35774;&#32622;&#30340;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20351;&#29992;&#21512;&#25104;&#30446;&#26631;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#20248;&#20110;&#20351;&#29992;&#23454;&#38469;&#30340;&#27491;&#30830;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#31181;&#24615;&#33021;&#24046;&#36317;&#38543;&#30528;&#21487;&#29992;&#36164;&#28304;&#30340;&#38480;&#21046;&#65288;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#65289;&#30340;&#22686;&#21152;&#32780;&#21464;&#24471;&#26356;&#22823;&#12290;&#25105;&#20204;&#36824;&#23545;&#24615;&#33021;&#25552;&#21319;&#26159;&#21542;&#19982;&#20248;&#21270;&#30340;&#20415;&#21033;&#24615;&#25110;&#39044;&#27979;&#30340;&#26356;&#30830;&#23450;&#24615;&#30456;&#20851;&#36827;&#34892;&#20102;&#21021;&#27493;&#20998;&#26512;&#65292;&#20197;&#21450;&#36825;&#31181;&#33539;&#20363;&#26159;&#21542;&#33021;&#22815;&#25552;&#39640;&#22312;&#19981;&#21516;&#27979;&#35797;&#39046;&#22495;&#30340;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we provide a recipe for training machine translation models in a limited resource setting by leveraging synthetic target data generated using a large pre-trained model. We show that consistently across different benchmarks in bilingual, multilingual, and speech translation setups, training models on synthetic targets outperforms training on the actual ground-truth data. This performance gap grows bigger with increasing limits on the amount of available resources in the form of the size of the dataset and the number of parameters in the model. We also provide preliminary analysis into whether this boost in performance is linked to ease of optimization or more deterministic nature of the predictions, and whether this paradigm leads to better out-of-distribution performance across different testing domains.
&lt;/p&gt;</description></item><item><title>&#36816;&#36755;&#34892;&#19994;&#20351;&#29992;OCR&#25216;&#26415;&#23545;&#25991;&#20214;&#36827;&#34892;&#20998;&#31867;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#21322;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#20851;&#38190;&#35789;&#39057;&#29575;&#26500;&#24314;&#19968;&#20010;&#31283;&#20581;&#30340;&#25991;&#26723;&#20998;&#31867;&#31995;&#32479;&#65292;&#20854;&#22312;&#25910;&#38598;&#30340;85&#20010;&#36829;&#32422;&#26696;&#20363;&#21644;555&#20010;&#38750;&#36829;&#32422;&#26696;&#20363;&#19978;&#23454;&#29616;&#20102;93.31%&#30340;&#39640;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.06148</link><description>&lt;p&gt;
&#19968;&#31181;&#36816;&#36755;&#34892;&#19994;&#25991;&#26723;&#20998;&#31867;&#30340;&#21322;&#33258;&#21160;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A semi-automatic method for document classification in the shipping industry. (arXiv:2305.06148v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06148
&lt;/p&gt;
&lt;p&gt;
&#36816;&#36755;&#34892;&#19994;&#20351;&#29992;OCR&#25216;&#26415;&#23545;&#25991;&#20214;&#36827;&#34892;&#20998;&#31867;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#21322;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#20851;&#38190;&#35789;&#39057;&#29575;&#26500;&#24314;&#19968;&#20010;&#31283;&#20581;&#30340;&#25991;&#26723;&#20998;&#31867;&#31995;&#32479;&#65292;&#20854;&#22312;&#25910;&#38598;&#30340;85&#20010;&#36829;&#32422;&#26696;&#20363;&#21644;555&#20010;&#38750;&#36829;&#32422;&#26696;&#20363;&#19978;&#23454;&#29616;&#20102;93.31%&#30340;&#39640;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36816;&#36755;&#34892;&#19994;&#20013;&#65292;&#25991;&#26723;&#20998;&#31867;&#23545;&#20110;&#30830;&#20445;&#24517;&#35201;&#30340;&#25991;&#20214;&#34987;&#27491;&#30830;&#35782;&#21035;&#24182;&#22788;&#29702;&#20197;&#36890;&#36807;&#28023;&#20851;&#28165;&#20851;&#27969;&#31243;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;OCR&#25216;&#26415;&#34987;&#29992;&#20110;&#33258;&#21160;&#21270;&#25991;&#26723;&#20998;&#31867;&#36807;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#35782;&#21035;&#21830;&#19994;&#21457;&#31080;&#12289;&#35013;&#31665;&#28165;&#21333;&#12289;&#36827;&#20986;&#21475;&#25253;&#20851;&#21333;&#12289;&#25552;&#21333;&#12289;&#28023;&#36816;&#25552;&#21333;&#12289;&#35777;&#20070;&#12289;&#33322;&#31354;&#25110;&#38081;&#36335;&#36135;&#36816;&#25552;&#21333;&#12289;&#21040;&#36798;&#36890;&#30693;&#20070;&#12289;&#21407;&#20135;&#22320;&#35777;&#26126;&#12289;&#36827;&#21475;&#21830;&#23433;&#20840;&#30003;&#25253;&#21644;&#20449;&#29992;&#35777;&#31561;&#37325;&#35201;&#25991;&#20214;&#12290;&#36890;&#36807;&#20351;&#29992;OCR&#25216;&#26415;&#65292;&#36816;&#36755;&#34892;&#19994;&#21487;&#20197;&#25552;&#39640;&#25991;&#26723;&#20998;&#31867;&#21644;&#28023;&#20851;&#28165;&#20851;&#27969;&#31243;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22522;&#20110;&#20851;&#38190;&#35789;&#39057;&#29575;&#26500;&#24314;&#19968;&#20010;&#31283;&#20581;&#30340;&#25991;&#26723;&#20998;&#31867;&#31995;&#32479;&#12290;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;IN-D&#20844;&#21496;&#25552;&#20379;&#30340;&#36829;&#32422;&#35785;&#35772;&#25991;&#26723;&#24471;&#20986;&#65292;&#35813;&#25991;&#26723;&#25910;&#38598;&#33258;&#26032;&#21152;&#22369;&#25919;&#24220;&#21496;&#27861;&#32593;&#31449;&#12290;&#25968;&#25454;&#24211;&#20013;&#21253;&#21547;85&#20010;&#36829;&#32422;&#26696;&#20363;&#21644;555&#20010;&#38750;&#36829;&#32422;&#26696;&#20363;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;93.31%&#30340;&#39640;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the shipping industry, document classification plays a crucial role in ensuring that the necessary documents are properly identified and processed for customs clearance. OCR technology is being used to automate the process of document classification, which involves identifying important documents such as Commercial Invoices, Packing Lists, Export/Import Customs Declarations, Bills of Lading, Sea Waybills, Certificates, Air or Rail Waybills, Arrival Notices, Certificate of Origin, Importer Security Filings, and Letters of Credit. By using OCR technology, the shipping industry can improve accuracy and efficiency in document classification and streamline the customs clearance process. The aim of this study is to build a robust document classification system based on keyword frequencies. The research is carried out by analyzing Contract-Breach law documents available with IN-D. The documents were collected by scraping the Singapore Government Judiciary website. The database developed ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#29305;&#24449;&#23376;&#31354;&#38388;&#23637;&#24320;&#21644;&#32467;&#26500;&#20027;&#25104;&#20998;&#20004;&#31181;&#26041;&#27861;&#26469;&#25193;&#23637;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.06142</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Feature Expansion for Graph Neural Networks. (arXiv:2305.06142v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#29305;&#24449;&#23376;&#31354;&#38388;&#23637;&#24320;&#21644;&#32467;&#26500;&#20027;&#25104;&#20998;&#20004;&#31181;&#26041;&#27861;&#26469;&#25193;&#23637;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26088;&#22312;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#34920;&#31034;&#65292;&#24182;&#23637;&#29616;&#20986;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#33410;&#28857;&#20998;&#31867;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#25903;&#37197;&#34920;&#31034;&#23398;&#20064;&#30340;&#29305;&#24449;&#31354;&#38388;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23578;&#26410;&#34987;&#31995;&#32479;&#22320;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20998;&#26512;&#31354;&#38388;&#27169;&#22411;&#21644;&#35889;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#35299;&#20026;&#30830;&#23450;&#30340;&#29305;&#24449;&#31354;&#38388;&#21644;&#21487;&#35757;&#32451;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#36890;&#36807;&#30697;&#38453;&#31354;&#38388;&#20998;&#26512;&#26126;&#30830;&#22320;&#30740;&#31350;&#29305;&#24449;&#31354;&#38388;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21457;&#29616;&#65292;&#30001;&#20110;&#37325;&#22797;&#32858;&#21512;&#65292;&#29305;&#24449;&#31354;&#38388;&#20542;&#21521;&#20110;&#32447;&#24615;&#30456;&#20851;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;1&#65289;&#29305;&#24449;&#23376;&#31354;&#38388;&#23637;&#24320;&#21644;2&#65289;&#32467;&#26500;&#20027;&#25104;&#20998;&#26469;&#25193;&#23637;&#29305;&#24449;&#31354;&#38388;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks aim to learn representations for graph-structured data and show impressive performance, particularly in node classification. Recently, many methods have studied the representations of GNNs from the perspective of optimization goals and spectral graph theory. However, the feature space that dominates representation learning has not been systematically studied in graph neural networks. In this paper, we propose to fill this gap by analyzing the feature space of both spatial and spectral models. We decompose graph neural networks into determined feature spaces and trainable weights, providing the convenience of studying the feature space explicitly using matrix space analysis. In particular, we theoretically find that the feature space tends to be linearly correlated due to repeated aggregations. Motivated by these findings, we propose 1) feature subspaces flattening and 2) structural principal components to expand the feature space. Extensive experiments verify the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35782;&#21035;&#21450;&#20998;&#31867;&#24433;&#21709;&#21407;&#27833;&#24066;&#22330;&#20379;&#38656;&#30340;&#26032;&#38395;&#20107;&#20214;&#30340;&#26041;&#27861;&#65292;&#24182;&#25454;&#27492;&#35774;&#35745;&#20102;CrudeBERT&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#20854;&#20182;&#19987;&#26377;&#21644;&#24320;&#28304;&#26041;&#26696;&#65292;&#20026;&#21407;&#27833;&#26399;&#36135;&#24066;&#22330;&#30456;&#20851;&#26631;&#39064;&#25552;&#20379;&#24773;&#24863;&#20998;&#31867;&#30340;&#25913;&#36827;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.06140</link><description>&lt;p&gt;
CrudeBERT&#65306;&#24212;&#29992;&#32463;&#27982;&#23398;&#29702;&#35770;&#25913;&#36827;&#22522;&#20110;Transformer&#30340;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#22312;&#21407;&#27833;&#24066;&#22330;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
CrudeBERT: Applying Economic Theory towards fine-tuning Transformer-based Sentiment Analysis Models to the Crude Oil Market. (arXiv:2305.06140v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35782;&#21035;&#21450;&#20998;&#31867;&#24433;&#21709;&#21407;&#27833;&#24066;&#22330;&#20379;&#38656;&#30340;&#26032;&#38395;&#20107;&#20214;&#30340;&#26041;&#27861;&#65292;&#24182;&#25454;&#27492;&#35774;&#35745;&#20102;CrudeBERT&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#20854;&#20182;&#19987;&#26377;&#21644;&#24320;&#28304;&#26041;&#26696;&#65292;&#20026;&#21407;&#27833;&#26399;&#36135;&#24066;&#22330;&#30456;&#20851;&#26631;&#39064;&#25552;&#20379;&#24773;&#24863;&#20998;&#31867;&#30340;&#25913;&#36827;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26032;&#38395;&#23186;&#20307;&#24773;&#24863;&#26469;&#39044;&#27979;&#24066;&#22330;&#36208;&#21521;&#22312;&#25968;&#25454;&#20998;&#26512;&#20013;&#24050;&#26377;&#24736;&#20037;&#20256;&#32479;&#12290;&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#27493;&#65292;&#20986;&#29616;&#20102;&#21487;&#36827;&#34892;&#19978;&#19979;&#25991;&#24863;&#30693;&#24773;&#24863;&#20998;&#31867;&#30340;Transformer&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20026;&#37329;&#34701;&#24066;&#22330;&#21046;&#23450;&#30340;&#26041;&#27861;&#65292;&#22914;FinBERT&#65292;&#26080;&#27861;&#21306;&#20998;&#24433;&#21709;&#29305;&#23450;&#36164;&#20135;&#20215;&#20540;&#39537;&#21160;&#22240;&#32032;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#22823;&#37327;&#30456;&#20851;&#26032;&#38395;&#22836;&#26465;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#24433;&#21709;&#21407;&#27833;&#24066;&#22330;&#20379;&#38656;&#30340;&#20107;&#20214;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#38543;&#21518;&#25552;&#20986;&#20102;CrudeBERT&#65292;&#36825;&#26159;&#19968;&#20010;&#20381;&#25454;&#36825;&#20123;&#20107;&#20214;&#26469;&#19978;&#19979;&#25991;&#21270;&#24182;&#20248;&#21270;FinBERT&#30340;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#20379;&#19982;&#21407;&#27833;&#26399;&#36135;&#24066;&#22330;&#30456;&#20851;&#30340;&#26631;&#39064;&#30340;&#24773;&#24863;&#20998;&#31867;&#30340;&#25913;&#36827;&#34920;&#29616;&#12290;&#19968;&#39033;&#24191;&#27867;&#30340;&#35780;&#20272;&#23637;&#31034;&#20102;CrudeBERT&#22312;&#21407;&#27833;&#39046;&#22495;&#32988;&#36807;&#19987;&#26377;&#21644;&#24320;&#28304;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting market movements based on the sentiment of news media has a long tradition in data analysis. With advances in natural language processing, transformer architectures have emerged that enable contextually aware sentiment classification. Nevertheless, current methods built for the general financial market such as FinBERT cannot distinguish asset-specific value-driving factors. This paper addresses this shortcoming by presenting a method that identifies and classifies events that impact supply and demand in the crude oil markets within a large corpus of relevant news headlines. We then introduce CrudeBERT, a new sentiment analysis model that draws upon these events to contextualize and fine-tune FinBERT, thereby yielding improved sentiment classifications for headlines related to the crude oil futures market. An extensive evaluation demonstrates that CrudeBERT outperforms proprietary and open-source solutions in the domain of crude oil.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#30452;&#25509;&#20272;&#35745;&#32473;&#23450;&#36755;&#20837;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#30340;&#28779;&#28798;&#20256;&#25773;&#27010;&#29575;&#65292;&#30465;&#21435;&#20102;&#32321;&#37325;&#30340;&#27169;&#25311;&#38598;&#21512;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#23545;&#28779;&#28798;&#20256;&#25773;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.06139</link><description>&lt;p&gt;
&#28779;&#28798;&#20256;&#25773;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#31070;&#32463;&#20223;&#30495;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Neural Emulator for Uncertainty Estimation of Fire Propagation. (arXiv:2305.06139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#30452;&#25509;&#20272;&#35745;&#32473;&#23450;&#36755;&#20837;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#30340;&#28779;&#28798;&#20256;&#25773;&#27010;&#29575;&#65292;&#30465;&#21435;&#20102;&#32321;&#37325;&#30340;&#27169;&#25311;&#38598;&#21512;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#23545;&#28779;&#28798;&#20256;&#25773;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26862;&#26519;&#28779;&#28798;&#30340;&#20256;&#25773;&#26159;&#19968;&#20010;&#39640;&#24230;&#38543;&#26426;&#30340;&#36807;&#31243;&#65292;&#29615;&#22659;&#26465;&#20214;&#30340;&#24494;&#23567;&#21464;&#21270;&#65288;&#20363;&#22914;&#39118;&#36895;&#21644;&#26041;&#21521;&#65289;&#20250;&#23548;&#33268;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#21457;&#29983;&#24040;&#22823;&#21464;&#21270;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#36807;&#27169;&#25311;&#38598;&#21512;&#29983;&#25104;&#27010;&#29575;&#22270;&#26469;&#37327;&#21270;&#28779;&#32447;&#20256;&#25773;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20294;&#20351;&#29992;&#38598;&#21512;&#36890;&#24120;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#36825;&#20250;&#38480;&#21046;&#19981;&#30830;&#23450;&#24615;&#20998;&#26512;&#30340;&#33539;&#22260;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#26041;&#27861;&#65292;&#30452;&#25509;&#20272;&#35745;&#32473;&#23450;&#36755;&#20837;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#30340;&#28779;&#28798;&#20256;&#25773;&#27010;&#29575;&#12290;&#36890;&#36807;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#26377;&#24847;&#35797;&#22270;&#25200;&#21160;&#36755;&#20837;&#30340;&#22825;&#27668;&#39044;&#25253;&#26469;&#34920;&#31034;&#19981;&#30830;&#23450;&#24615;&#12290;&#35745;&#31639;&#36127;&#36733;&#38598;&#20013;&#20110;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36825;&#20801;&#35768;&#22312;&#37096;&#32626;&#26399;&#38388;&#25506;&#32034;&#26356;&#22823;&#30340;&#27010;&#29575;&#31354;&#38388;&#12290;&#32463;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#19982;&#20256;&#32479; S*t&#27169;&#25311;&#25152;&#20135;&#29983;&#30340;&#30456;&#24403;&#28779;&#28798;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wildfire propagation is a highly stochastic process where small changes in environmental conditions (such as wind speed and direction) can lead to large changes in observed behaviour. A traditional approach to quantify uncertainty in fire-front progression is to generate probability maps via ensembles of simulations. However, use of ensembles is typically computationally expensive, which can limit the scope of uncertainty analysis. To address this, we explore the use of a spatio-temporal neural-based modelling approach to directly estimate the likelihood of fire propagation given uncertainty in input parameters. The uncertainty is represented by deliberately perturbing the input weather forecast during model training. The computational load is concentrated in the model training process, which allows larger probability spaces to be explored during deployment. Empirical evaluations indicate that the proposed model achieves comparable fire boundaries to those produced by the traditional S
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#29702;&#35770;&#23618;&#38754;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;Wasserstein&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#24120;&#35268;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.06137</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#35777;&#26126;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A proof of convergence of inverse reinforcement learning for multi-objective optimization. (arXiv:2305.06137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#29702;&#35770;&#23618;&#38754;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;Wasserstein&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#24120;&#35268;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#31561;&#25928;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;WIRL&#38382;&#39064;&#30340;&#36870;&#38382;&#39064;&#19982;&#25237;&#24433;&#27425;&#26799;&#24230;&#27861;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;Wasserstein&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;WIRL&#65289;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;&#26368;&#22823;&#29109;&#36870;&#24378;&#21270;&#23398;&#20064;&#65292;&#23548;&#24341;&#25104;&#26412;&#23398;&#20064;&#65289;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show the convergence of Wasserstein inverse reinforcement learning (WIRL) for multi-objective optimizations with the projective subgradient method by formulating an inverse problem of the optimization problem that is equivalent to WIRL for multi-objective optimizations.  In addition, we prove convergence of inverse reinforcement learning (maximum entropy inverse reinforcement learning, guid cost learning) for multi-objective optimization with the projective subgradient method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#21517;&#20026;FedDWA&#65292;&#37319;&#29992;&#21160;&#24577;&#26435;&#37325;&#35843;&#25972;&#26469;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#20197;&#26356;&#23569;&#30340;&#36890;&#20449;&#24320;&#38144;&#25429;&#25417;&#23458;&#25143;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#33021;&#22815;&#35757;&#32451;&#39640;&#31934;&#24230;&#21644;&#39640;&#25928;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.06124</link><description>&lt;p&gt;
FedDWA: &#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#19982;&#21160;&#24577;&#26435;&#37325;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
FedDWA: Personalized Federated Learning with Online Weight Adjustment. (arXiv:2305.06124v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#21517;&#20026;FedDWA&#65292;&#37319;&#29992;&#21160;&#24577;&#26435;&#37325;&#35843;&#25972;&#26469;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#20197;&#26356;&#23569;&#30340;&#36890;&#20449;&#24320;&#38144;&#25429;&#25417;&#23458;&#25143;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#33021;&#22815;&#35757;&#32451;&#39640;&#31934;&#24230;&#21644;&#39640;&#25928;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#19981;&#21516;&#65292;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#33021;&#22815;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#29420;&#29305;&#38656;&#27714;&#26469;&#35757;&#32451;&#23450;&#21046;&#21270;&#27169;&#22411;&#12290;&#20027;&#27969;&#26041;&#27861;&#26159;&#37319;&#29992;&#19968;&#31181;&#21152;&#26435;&#32858;&#21512;&#26041;&#27861;&#26469;&#29983;&#25104;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#20854;&#20013;&#26435;&#37325;&#26159;&#30001;&#19981;&#21516;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25439;&#22833;&#20540;&#25110;&#27169;&#22411;&#21442;&#25968;&#30830;&#23450;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#35201;&#27714;&#23458;&#25143;&#31471;&#19979;&#36733;&#20854;&#20182;&#27169;&#22411;&#65292;&#19981;&#20165;&#22686;&#21152;&#20102;&#36890;&#20449;&#27969;&#37327;&#65292;&#32780;&#19988;&#21487;&#33021;&#20405;&#29359;&#25968;&#25454;&#38544;&#31169;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PFL&#31639;&#27861;&#65292;&#31216;&#20026;FedDWA&#65288;&#24102;&#21160;&#24577;&#26435;&#37325;&#35843;&#25972;&#30340;&#32852;&#37030;&#23398;&#20064;&#65289;&#65292;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#21442;&#25968;&#26381;&#21153;&#22120;&#65288;PS&#65289;&#26681;&#25454;&#20174;&#23458;&#25143;&#31471;&#25910;&#38598;&#30340;&#27169;&#22411;&#35745;&#31639;&#20010;&#24615;&#21270;&#32858;&#21512;&#26435;&#37325;&#12290;&#36825;&#26679;&#65292;FedDWA&#21487;&#20197;&#20197;&#26356;&#23569;&#30340;&#36890;&#20449;&#24320;&#38144;&#25429;&#25417;&#23458;&#25143;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#23558;PFL&#38382;&#39064;&#21046;&#23450;&#20026;&#19968;&#31181;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;&#26435;&#37325;&#35843;&#25972;&#26426;&#21046;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#12290;FedDWA&#33021;&#22815;&#23398;&#20064;&#39640;&#31934;&#24230;&#21644;&#39640;&#25928;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#25105;&#20204;&#22312;&#32508;&#21512;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;FedDWA&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different from conventional federated learning, personalized federated learning (PFL) is able to train a customized model for each individual client according to its unique requirement. The mainstream approach is to adopt a kind of weighted aggregation method to generate personalized models, in which weights are determined by the loss value or model parameters among different clients. However, such kinds of methods require clients to download others' models. It not only sheer increases communication traffic but also potentially infringes data privacy. In this paper, we propose a new PFL algorithm called \emph{FedDWA (Federated Learning with Dynamic Weight Adjustment)} to address the above problem, which leverages the parameter server (PS) to compute personalized aggregation weights based on collected models from clients. In this way, FedDWA can capture similarities between clients with much less communication overhead. More specifically, we formulate the PFL problem as an optimization 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;VIM&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#33258;&#36866;&#24212;&#31354;&#38388;-&#26102;&#38388;&#35270;&#39057;&#37319;&#26679;&#22120;&#21644;&#26102;&#31354;&#21160;&#20316;&#22686;&#24378;&#22120;&#65292;&#29992;&#20110;&#23567;&#26679;&#26412;&#21160;&#20316;&#35782;&#21035;&#65292;&#20805;&#20998;&#21033;&#29992;&#35270;&#39057;&#20869;&#37096;&#21644;&#35270;&#39057;&#38388;&#20449;&#24687;&#20197;&#25552;&#39640;&#35782;&#21035;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.06114</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#39057;&#20869;&#37096;&#21644;&#35270;&#39057;&#38388;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#23567;&#26679;&#26412;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Few-shot Action Recognition via Intra- and Inter-Video Information Maximization. (arXiv:2305.06114v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;VIM&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#33258;&#36866;&#24212;&#31354;&#38388;-&#26102;&#38388;&#35270;&#39057;&#37319;&#26679;&#22120;&#21644;&#26102;&#31354;&#21160;&#20316;&#22686;&#24378;&#22120;&#65292;&#29992;&#20110;&#23567;&#26679;&#26412;&#21160;&#20316;&#35782;&#21035;&#65292;&#20805;&#20998;&#21033;&#29992;&#35270;&#39057;&#20869;&#37096;&#21644;&#35270;&#39057;&#38388;&#20449;&#24687;&#20197;&#25552;&#39640;&#35782;&#21035;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#23567;&#26679;&#26412;&#21160;&#20316;&#35782;&#21035;&#28041;&#21450;&#20004;&#20010;&#20027;&#35201;&#30340;&#20449;&#24687;&#28304;&#29992;&#20110;&#20998;&#31867;: (1) &#26469;&#33258;&#35270;&#39057;&#29255;&#27573;&#20869;&#37096;&#30340;&#35270;&#39057;&#20869;&#37096;&#20449;&#24687;&#65292;&#30001;&#21333;&#20010;&#35270;&#39057;&#21098;&#36753;&#20013;&#30340;&#24103;&#20869;&#23481;&#30830;&#23450;&#65292;&#20197;&#21450; (2) &#36890;&#36807;&#35270;&#39057;&#20043;&#38388;&#30340;&#20851;&#31995;(&#20363;&#22914;&#65292;&#29305;&#24449;&#30456;&#20284;&#24615;)&#27979;&#37327;&#30340;&#35270;&#39057;&#38388;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#20805;&#20998;&#21033;&#29992;&#36825;&#20004;&#20010;&#20449;&#24687;&#28304;&#12290;&#20851;&#20110;&#35270;&#39057;&#20869;&#37096;&#20449;&#24687;&#65292;&#24403;&#21069;&#30340;&#36755;&#20837;&#35270;&#39057;&#37319;&#26679;&#25805;&#20316;&#21487;&#33021;&#20250;&#36951;&#28431;&#20851;&#38190;&#30340;&#21160;&#20316;&#20449;&#24687;&#65292;&#38477;&#20302;&#35270;&#39057;&#25968;&#25454;&#30340;&#21033;&#29992;&#25928;&#29575;&#12290;&#23545;&#20110;&#35270;&#39057;&#38388;&#20449;&#24687;&#65292;&#35270;&#39057;&#20043;&#38388;&#30340;&#21160;&#20316;&#19981;&#23545;&#40784;&#20351;&#24471;&#35745;&#31639;&#31934;&#30830;&#20851;&#31995;&#21464;&#24471;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#22914;&#20309;&#20849;&#21516;&#32771;&#34385;&#35270;&#39057;&#20869;&#22806;&#20449;&#24687;&#22312;&#23567;&#26679;&#26412;&#21160;&#20316;&#35782;&#21035;&#20013;&#20173;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#35270;&#39057;&#20449;&#24687;&#26368;&#22823;&#21270; (VIM)&#65292;&#29992;&#20110;&#23567;&#26679;&#26412;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#12290;VIM&#37197;&#22791;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;&#26102;&#31354;&#35270;&#39057;&#37319;&#26679;&#22120;&#21644;&#19968;&#20010;&#26102;&#31354;&#21160;&#20316;&#22686;&#24378;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current few-shot action recognition involves two primary sources of information for classification:(1) intra-video information, determined by frame content within a single video clip, and (2) inter-video information, measured by relationships (e.g., feature similarity) among videos. However, existing methods inadequately exploit these two information sources. In terms of intra-video information, current sampling operations for input videos may omit critical action information, reducing the utilization efficiency of video data. For the inter-video information, the action misalignment among videos makes it challenging to calculate precise relationships. Moreover, how to jointly consider both inter- and intra-video information remains under-explored for few-shot action recognition. To this end, we propose a novel framework, Video Information Maximization (VIM), for few-shot video action recognition. VIM is equipped with an adaptive spatial-temporal video sampler and a spatiotemporal actio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20266;&#21160;&#24577;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#24515;&#33039;&#30149;&#24739;&#32773;&#22312; ICU &#20013;&#30340;&#27515;&#20129;&#29575;&#39044;&#27979;&#65292;&#24182;&#25552;&#20379;&#20102;&#35786;&#26029;&#21487;&#35299;&#37322;&#24615;&#21644;&#20020;&#24202;&#39118;&#38505;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#27515;&#20129;&#29575;&#26102;&#25552;&#20379;&#20102;&#39640;&#31934;&#24230;&#30340;&#39044;&#27979;&#32467;&#26524;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#21487;&#35299;&#37322;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.06109</link><description>&lt;p&gt;
XMI-ICU: &#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#24515;&#33039;&#30149;&#24739;&#32773;&#22312; ICU &#30340; &#20266;&#21160;&#24577;&#27515;&#20129;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
XMI-ICU: Explainable Machine Learning Model for Pseudo-Dynamic Prediction of Mortality in the ICU for Heart Attack Patients. (arXiv:2305.06109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20266;&#21160;&#24577;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#24515;&#33039;&#30149;&#24739;&#32773;&#22312; ICU &#20013;&#30340;&#27515;&#20129;&#29575;&#39044;&#27979;&#65292;&#24182;&#25552;&#20379;&#20102;&#35786;&#26029;&#21487;&#35299;&#37322;&#24615;&#21644;&#20020;&#24202;&#39118;&#38505;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#27515;&#20129;&#29575;&#26102;&#25552;&#20379;&#20102;&#39640;&#31934;&#24230;&#30340;&#39044;&#27979;&#32467;&#26524;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#21487;&#35299;&#37322;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#33039;&#30149;&#26159;&#32654;&#22269;&#21644;&#20840;&#29699;&#27515;&#20129;&#29575;&#30340;&#20027;&#35201;&#36129;&#29486;&#22240;&#32032;&#20043;&#19968;&#12290;&#36827;&#20837;&#37325;&#30151;&#30417;&#25252;&#23460;(ICU)&#30340;&#35786;&#26029;&#20026;&#24515;&#32908;&#26775;&#27515;&#25110; MI &#30340;&#24739;&#32773;&#26377;&#26356;&#39640;&#30340;&#27515;&#20129;&#39118;&#38505;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20174; eICU &#21644; MIMIC-IV &#25968;&#25454;&#24211;&#25552;&#21462;&#30340;&#20004;&#20010;&#22238;&#39038;&#24615;&#38431;&#21015;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#20266;&#21160;&#24577;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312; ICU &#20013;&#36827;&#34892;&#27515;&#20129;&#29575;&#39044;&#27979;&#65292;&#24182;&#25552;&#20379;&#35786;&#26029;&#21487;&#35299;&#37322;&#24615;&#21644;&#20020;&#24202;&#39118;&#38505;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#39640;&#31934;&#24230;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#22312;&#20107;&#20214;&#21457;&#29983;&#21069; 24 &#23567;&#26102;&#20026; ICU &#24739;&#32773;&#25552;&#20379;&#26102;&#38388;&#20998;&#36776;&#29575;&#21487;&#35299;&#37322;&#24615;&#32467;&#26524;&#12290;&#35813;&#26694;&#26550;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#26497;&#31471;&#26799;&#24230;&#25552;&#21319;(Extreme Gradient Boosting)&#65292;&#22312;&#26469;&#33258; eICU &#30340;&#26657;&#39564;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#20351;&#29992;&#26102;&#38388;&#20998;&#36776;&#29575; Shapley &#20540;&#30340;&#26368;&#37325;&#35201;&#29305;&#24449;&#36890;&#36807;&#22806;&#37096;&#39564;&#35777;&#22312; MIMIC-IV &#38431;&#21015;&#19978;&#33719;&#24471;&#20102; AUC &#20026; 91.0&#65292;&#20998;&#21035;&#39044;&#27979;&#20102; 6 &#23567;&#26102;&#27515;&#20129;&#29575;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26412;&#30740;&#31350;&#26694;&#26550;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heart attack remain one of the greatest contributors to mortality in the United States and globally. Patients admitted to the intensive care unit (ICU) with diagnosed heart attack (myocardial infarction or MI) are at higher risk of death. In this study, we use two retrospective cohorts extracted from the eICU and MIMIC-IV databases, to develop a novel pseudo-dynamic machine learning framework for mortality prediction in the ICU with interpretability and clinical risk analysis. The method provides accurate prediction for ICU patients up to 24 hours before the event and provide time-resolved interpretability results. The performance of the framework relying on extreme gradient boosting was evaluated on a held-out test set from eICU, and externally validated on the MIMIC-IV cohort using the most important features identified by time-resolved Shapley values achieving AUCs of 91.0 (balanced accuracy of 82.3) for 6-hour prediction of mortality respectively. We show that our framework success
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23569;&#26679;&#26412;N-&#20803;&#20107;&#23454;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLEN&#30340;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;FLEN&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#26631;&#35760;&#23454;&#20363;&#20013;&#39044;&#27979;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#32570;&#22833;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2305.06104</link><description>&lt;p&gt;
N-&#20803;&#20107;&#23454;&#30340;&#23569;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-shot Link Prediction on N-ary Facts. (arXiv:2305.06104v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23569;&#26679;&#26412;N-&#20803;&#20107;&#23454;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLEN&#30340;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;FLEN&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#26631;&#35760;&#23454;&#20363;&#20013;&#39044;&#27979;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#32570;&#22833;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
N-&#20803;&#20107;&#23454;&#30001;&#20027;&#35201;&#19977;&#20803;&#32452;&#65288;&#22836;&#23454;&#20307;&#12289;&#20851;&#31995;&#12289;&#23614;&#23454;&#20307;&#65289;&#21644;&#20219;&#24847;&#25968;&#37327;&#30340;&#36741;&#21161;&#23646;&#24615;&#20540;&#23545;&#32452;&#25104;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#24456;&#24120;&#35265;&#12290;&#23545;&#20110;N-&#20803;&#20107;&#23454;&#30340;&#38142;&#25509;&#39044;&#27979;&#26159;&#39044;&#27979;&#20854;&#20013;&#19968;&#20010;&#20803;&#32032;&#30340;&#32570;&#22833;&#65292;&#22635;&#34917;&#32570;&#22833;&#20803;&#32032;&#26377;&#21161;&#20110;&#20016;&#23500;&#30693;&#35782;&#22270;&#35889;&#24182;&#20419;&#36827;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26469;&#29702;&#35299;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#20803;&#32032;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#24573;&#35270;&#20102;&#23569;&#26679;&#26412;&#20851;&#31995;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#21364;&#24456;&#24120;&#35265;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23569;&#26679;&#26412;N-&#20803;&#20107;&#23454;&#38142;&#25509;&#39044;&#27979;&#65292;&#26088;&#22312;&#20351;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#23454;&#20363;&#26469;&#39044;&#27979;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#32570;&#22833;&#23454;&#20307;&#12290;&#25105;&#20204;&#20063;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;N-&#20803;&#20107;&#23454;&#30340;&#23569;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;FLEN&#65292;&#23427;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#20851;&#31995;&#23398;&#20064;&#27169;&#22359;&#12289;&#25903;&#25345;&#29305;&#23450;&#35843;&#25972;&#27169;&#22359;&#21644;&#26597;&#35810;&#25512;&#29702;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
N-ary facts composed of a primary triple (head entity, relation, tail entity) and an arbitrary number of auxiliary attribute-value pairs, are prevalent in real-world knowledge graphs (KGs). Link prediction on n-ary facts is to predict a missing element in an n-ary fact. This helps populate and enrich KGs and further promotes numerous downstream applications. Previous studies usually require a substantial amount of high-quality data to understand the elements in n-ary facts. However, these studies overlook few-shot relations, which have limited labeled instances, yet are common in real-world scenarios. Thus, this paper introduces a new task, few-shot link prediction on n-ary facts. It aims to predict a missing entity in an n-ary fact with limited labeled instances. We further propose a model for Few-shot Link prEdict on N-ary facts, thus called FLEN, which consists of three modules: the relation learning, support-specific adjusting, and query inference modules. FLEN captures relation me
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#37319;&#29992;&#21442;&#25968;&#21270;&#20998;&#35299;&#21644;&#28388;&#27874;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;GNN&#30340;&#28789;&#27963;&#24615;&#65292;&#32531;&#35299;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#24179;&#28369;&#21644;&#25918;&#22823;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.06102</link><description>&lt;p&gt;
&#37319;&#29992;&#21442;&#25968;&#20998;&#35299;&#21644;&#28388;&#27874;&#25216;&#26415;&#23454;&#29616;&#26356;&#22909;&#30340;&#22270;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Better Graph Representation Learning with Parameterized Decomposition &amp; Filtering. (arXiv:2305.06102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#37319;&#29992;&#21442;&#25968;&#21270;&#20998;&#35299;&#21644;&#28388;&#27874;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;GNN&#30340;&#28789;&#27963;&#24615;&#65292;&#32531;&#35299;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#24179;&#28369;&#21644;&#25918;&#22823;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#32780;&#28789;&#27963;&#30340;&#30697;&#38453;&#26469;&#34920;&#31034;&#22270;&#24418;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#25361;&#25112;&#65292;&#20854;&#24050;&#20174;&#22810;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#25506;&#32034;&#65292;&#20363;&#22914;&#65292;&#20351;&#29992;&#22270;&#20613;&#37324;&#21494;&#21464;&#25442;&#20013;&#30340;&#28388;&#27874;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#21442;&#25968;&#21270;&#20998;&#35299;&#21644;&#28388;&#27874;&#30340;&#35282;&#24230;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#32479;&#19968;&#20102;&#35768;&#22810;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#25552;&#39640;GNN&#30340;&#28789;&#27963;&#24615;&#65292;&#21516;&#26102;&#20943;&#36731;&#29616;&#26377;&#27169;&#22411;&#30340;&#24179;&#28369;&#21644;&#25918;&#22823;&#38382;&#39064;&#12290;&#26412;&#36136;&#19978;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#20855;&#26377;&#21487;&#23398;&#20064;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#30340;&#35889;&#22270;&#21367;&#31215;&#26159;&#36825;&#31181;&#34920;&#36848;&#30340;&#32422;&#26463;&#21464;&#20307;&#65292;&#25918;&#24323;&#36825;&#20123;&#32422;&#26463;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#34920;&#36798;&#25152;&#38656;&#30340;&#20998;&#35299;&#21644;&#28388;&#27874;&#12290;&#22522;&#20110;&#36825;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#25105;&#20204;&#24320;&#21457;&#30340;&#27169;&#22411;&#22312;&#23454;&#29616;&#19978;&#31616;&#21333;&#65292;&#20294;&#22312;&#21508;&#31181;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290; &#20195;&#30721;&#21487;&#22312; https://github.com/qslim/PDF &#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proposing an effective and flexible matrix to represent a graph is a fundamental challenge that has been explored from multiple perspectives, e.g., filtering in Graph Fourier Transforms. In this work, we develop a novel and general framework which unifies many existing GNN models from the view of parameterized decomposition and filtering, and show how it helps to enhance the flexibility of GNNs while alleviating the smoothness and amplification issues of existing models. Essentially, we show that the extensively studied spectral graph convolutions with learnable polynomial filters are constrained variants of this formulation, and releasing these constraints enables our model to express the desired decomposition and filtering simultaneously. Based on this generalized framework, we develop models that are simple in implementation but achieve significant improvements and computational efficiency on a variety of graph learning tasks. Code is available at https://github.com/qslim/PDF.
&lt;/p&gt;</description></item><item><title>XTab &#26159;&#19968;&#20010;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#30340;&#36328;&#34920;&#26684;&#39044;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29420;&#31435;&#30340;&#29305;&#24449;&#24037;&#31243;&#24072;&#21644;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#34920;&#26684;&#20043;&#38388;&#19981;&#19968;&#33268;&#30340;&#21015;&#31867;&#22411;&#21644;&#25968;&#37327;&#30340;&#25361;&#25112;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292; XTab &#21487;&#20197;&#25552;&#39640;&#22810;&#20010;&#34920;&#26684;&#21464;&#25442;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#23398;&#20064;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06090</link><description>&lt;p&gt;
XTab: &#36328;&#34920;&#26684;&#34920;&#26684;&#21464;&#25442;&#22120;&#30340;&#20132;&#21449;&#34920;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
XTab: Cross-table Pretraining for Tabular Transformers. (arXiv:2305.06090v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06090
&lt;/p&gt;
&lt;p&gt;
XTab &#26159;&#19968;&#20010;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#30340;&#36328;&#34920;&#26684;&#39044;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29420;&#31435;&#30340;&#29305;&#24449;&#24037;&#31243;&#24072;&#21644;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#34920;&#26684;&#20043;&#38388;&#19981;&#19968;&#33268;&#30340;&#21015;&#31867;&#22411;&#21644;&#25968;&#37327;&#30340;&#25361;&#25112;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292; XTab &#21487;&#20197;&#25552;&#39640;&#22810;&#20010;&#34920;&#26684;&#21464;&#25442;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#23398;&#20064;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25104;&#21151;&#20419;&#20351;&#20102;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#24320;&#21457;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#34920;&#26684;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#26410;&#33021;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#34920;&#20043;&#38388;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#34920;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;XTab&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#26469;&#33258;&#21508;&#20010;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20132;&#21449;&#34920;&#39044;&#35757;&#32451;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#29420;&#31435;&#30340;&#29305;&#24449;&#24037;&#31243;&#24072;&#21644;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#26469;&#39044;&#20808;&#35757;&#32451;&#20849;&#20139;&#32452;&#20214;&#65292;&#35299;&#20915;&#20102;&#34920;&#26684;&#20043;&#38388;&#19981;&#19968;&#33268;&#30340;&#21015;&#31867;&#22411;&#21644;&#25968;&#37327;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of self-supervised learning in computer vision and natural language processing has motivated pretraining methods on tabular data. However, most existing tabular self-supervised learning models fail to leverage information across multiple data tables and cannot generalize to new tables. In this work, we introduce XTab, a framework for cross-table pretraining of tabular transformers on datasets from various domains. We address the challenge of inconsistent column types and quantities among tables by utilizing independent featurizers and using federated learning to pretrain the shared component. Tested on 84 tabular prediction tasks from the OpenML-AutoML Benchmark (AMLB), we show that (1) XTab consistently boosts the generalizability, learning speed, and performance of multiple tabular transformers, (2) by pretraining FT-Transformer via XTab, we achieve superior performance than other state-of-the-art tabular deep learning models on various tasks such as regression, binary, a
&lt;/p&gt;</description></item><item><title>ChatGPT&#23637;&#31034;&#20102;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#36816;&#34892;&#38656;&#35201;&#24040;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#39640;&#26114;&#30340;&#25104;&#26412;&#65292;&#39044;&#35745;&#20250;&#32473;AI&#30740;&#31350;&#24102;&#26469;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.06087</link><description>&lt;p&gt;
ChatGPT&#33021;&#21147;&#23637;&#31034;&#21450;&#20854;&#23545;AI&#30740;&#31350;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
A Glimpse in ChatGPT Capabilities and its impact for AI research. (arXiv:2305.06087v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06087
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#23637;&#31034;&#20102;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#36816;&#34892;&#38656;&#35201;&#24040;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#39640;&#26114;&#30340;&#25104;&#26412;&#65292;&#39044;&#35745;&#20250;&#32473;AI&#30740;&#31350;&#24102;&#26469;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30740;&#31350;&#39046;&#22495;&#25104;&#20026;&#28909;&#38376;&#35805;&#39064;&#12290;&#20687;Google&#12289;&#20122;&#39532;&#36874;&#12289;Facebook&#12289;&#29305;&#26031;&#25289;&#21644;&#33529;&#26524;&#65288;GAFA&#65289;&#36825;&#26679;&#30340;&#20844;&#21496;&#27491;&#22312;&#22823;&#21147;&#21457;&#23637;&#36825;&#20123;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20250;&#20351;&#29992;&#28023;&#37327;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#35328;&#32763;&#35793;&#12289;&#25991;&#31456;&#29983;&#25104;&#21644;&#38382;&#31572;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#21644;&#36816;&#34892;&#36825;&#20123;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#38750;&#24120;&#24040;&#22823;&#65292;&#32780;&#30828;&#20214;&#21644;&#30005;&#21147;&#30340;&#25104;&#26412;&#21487;&#33021;&#38480;&#21046;&#20102;&#37027;&#20123;&#27809;&#26377;GAFA&#36164;&#37329;&#21644;&#36164;&#28304;&#30340;&#30740;&#31350;&#23454;&#39564;&#23460;&#12290;&#26412;&#25991;&#23558;&#30740;&#31350;LLMs&#23545;AI&#30740;&#31350;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;GPT3.5/ChatGPT3.4&#65292;&#24182;&#32473;&#20986;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#30740;&#31350;&#30340;&#19968;&#20123;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently become a popular topic in the field of Artificial Intelligence (AI) research, with companies such as Google, Amazon, Facebook, Amazon, Tesla, and Apple (GAFA) investing heavily in their development. These models are trained on massive amounts of data and can be used for a wide range of tasks, including language translation, text generation, and question answering. However, the computational resources required to train and run these models are substantial, and the cost of hardware and electricity can be prohibitive for research labs that do not have the funding and resources of the GAFA. In this paper, we will examine the impact of LLMs on AI research. The pace at which such models are generated as well as the range of domains covered is an indication of the trend which not only the public but also the scientific community is currently experiencing. We give some examples on how to use such models in research by focusing on GPT3.5/ChatGPT3.4 and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#31934;&#24230;&#19979;&#37319;&#26679;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#36319;&#36394;&#31639;&#27861;&#26469;&#22788;&#29702;&#26368;&#20248;&#20998;&#37197;&#30340;&#38750;&#21807;&#19968;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#28176;&#36827;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06082</link><description>&lt;p&gt;
&#26377;&#38480;&#31934;&#24230;&#37319;&#26679;&#19979;&#30340;&#36172;&#21338;&#26426;&#26368;&#20248;&#33218;&#35782;&#21035;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Best Arm Identification in Bandits with Limited Precision Sampling. (arXiv:2305.06082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#31934;&#24230;&#19979;&#37319;&#26679;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#36319;&#36394;&#31639;&#27861;&#26469;&#22788;&#29702;&#26368;&#20248;&#20998;&#37197;&#30340;&#38750;&#21807;&#19968;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#28176;&#36827;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#23398;&#20064;&#32773;&#22312;&#36873;&#25321;&#33218;&#26102;&#26377;&#38480;&#30340;&#31934;&#24230;&#12290;&#23398;&#20064;&#32773;&#21482;&#33021;&#36890;&#36807;&#29305;&#23450;&#30340;&#25506;&#32034;&#32452;&#21512;&#65288;&#21363;&#25152;&#35859;&#30340;&#31665;&#23376;&#65289;&#37319;&#26679;&#33218;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#27599;&#20010;&#37319;&#26679;&#26102;&#21051;&#65292;&#23398;&#20064;&#32773;&#36873;&#25321;&#19968;&#20010;&#31665;&#23376;&#65292;&#28982;&#21518;&#26681;&#25454;&#31665;&#23376;&#29305;&#23450;&#30340;&#27010;&#29575;&#20998;&#24067;&#25289;&#21160;&#33218;&#65292;&#25581;&#31034;&#34987;&#25289;&#21160;&#30340;&#33218;&#21450;&#20854;&#30636;&#26102;&#25910;&#30410;&#65292;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#26368;&#23567;&#21270;&#26399;&#26395;&#20572;&#27490;&#26102;&#38388;&#26469;&#25214;&#21040;&#26368;&#20339;&#33218;&#65292;&#32780;&#35823;&#24046;&#27010;&#29575;&#21463;&#21040;&#19978;&#38480;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study best arm identification in a variant of the multi-armed bandit problem where the learner has limited precision in arm selection. The learner can only sample arms via certain exploration bundles, which we refer to as boxes. In particular, at each sampling epoch, the learner selects a box, which in turn causes an arm to get pulled as per a box-specific probability distribution. The pulled arm and its instantaneous reward are revealed to the learner, whose goal is to find the best arm by minimising the expected stopping time, subject to an upper bound on the error probability. We present an asymptotic lower bound on the expected stopping time, which holds as the error probability vanishes. We show that the optimal allocation suggested by the lower bound is, in general, non-unique and therefore challenging to track. We propose a modified tracking-based algorithm to handle non-unique optimal allocations, and demonstrate that it is asymptotically optimal. We also present non-asympto
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;PaPi&#65292;&#29992;&#20110;&#35299;&#20915;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#65288;PLL&#65289;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#30001;&#32447;&#24615;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#21407;&#22411;&#20998;&#31867;&#22120;&#20248;&#21270;&#26469;&#20316;&#20026;&#27491;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#20419;&#36827;&#20102;&#26631;&#31614;&#28040;&#27495;&#12290;</title><link>http://arxiv.org/abs/2305.06080</link><description>&lt;p&gt;
&#38754;&#21521;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#30340;&#26377;&#25928;&#35270;&#35273;&#34920;&#31034;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Effective Visual Representations for Partial-Label Learning. (arXiv:2305.06080v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;PaPi&#65292;&#29992;&#20110;&#35299;&#20915;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#65288;PLL&#65289;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#30001;&#32447;&#24615;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#21407;&#22411;&#20998;&#31867;&#22120;&#20248;&#21270;&#26469;&#20316;&#20026;&#27491;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#20419;&#36827;&#20102;&#26631;&#31614;&#28040;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#65288;PLL&#65289;&#30340;&#24773;&#20917;&#65292;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#20165;&#21487;&#33719;&#24471;&#19968;&#32452;&#21253;&#21547;&#26410;&#30693;&#30495;&#23454;&#26631;&#31614;&#30340;&#27169;&#26865;&#20004;&#21487;&#30340;&#20505;&#36873;&#26631;&#31614;&#65292;&#23545;&#20110;&#35270;&#35273;&#20219;&#21153;&#65292;&#23545;&#27604;&#23398;&#20064;&#26368;&#36817;&#25552;&#39640;&#20102;PLL&#30340;&#24615;&#33021;&#65292;&#24402;&#22240;&#20110;&#23545;&#30456;&#21516;/&#19981;&#21516;&#23454;&#20307;&#31867;&#21035;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#24471;&#21040;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;PaPi&#30340;&#31616;&#21333;&#26694;&#26550;&#65292;&#37325;&#26032;&#24605;&#32771;&#26368;&#20808;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;PLL&#26041;&#27861;PiCO[24]&#65292;&#23427;&#36890;&#36807;&#30001;&#32447;&#24615;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#21407;&#22411;&#20998;&#31867;&#22120;&#20248;&#21270;&#26469;&#20316;&#20026;&#27491;&#26679;&#26412;&#21644;&#20266;&#26631;&#31614;&#39044;&#27979;&#36127;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#20419;&#36827;&#20102;&#26631;&#31614;&#28040;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Under partial-label learning (PLL) where, for each training instance, only a set of ambiguous candidate labels containing the unknown true label is accessible, contrastive learning has recently boosted the performance of PLL on vision tasks, attributed to representations learned by contrasting the same/different classes of entities. Without access to true labels, positive points are predicted using pseudo-labels that are inherently noisy, and negative points often require large batches or momentum encoders, resulting in unreliable similarity information and a high computational overhead. In this paper, we rethink a state-of-the-art contrastive PLL method PiCO[24], inspiring the design of a simple framework termed PaPi (Partial-label learning with a guided Prototypical classifier), which demonstrates significant scope for improvement in representation learning, thus contributing to label disambiguation. PaPi guides the optimization of a prototypical classifier by a linear classifier wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#24314;&#27169;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#30340;&#20004;&#31181;&#26041;&#27861;&#65306;&#20998;&#24067;&#24335;&#36719;&#26631;&#35760;&#26041;&#27861;&#21644;&#24314;&#27169;&#20010;&#20307;&#27880;&#37322;&#32773;&#25110;&#20854;&#32452;&#30340;&#35266;&#28857;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#30340;&#26041;&#24335;&#26469;&#27169;&#25311;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#12290;&#23613;&#31649;&#22810;&#20219;&#21153;&#26041;&#27861;&#20043;&#21069;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#20854;&#22312;&#21253;&#21547;&#19981;&#21516;&#27880;&#37322;&#32773;&#35266;&#28857;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#32454;&#33268;&#22320;&#29702;&#35299;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#65292;&#20026;&#26356;&#20934;&#30830;&#30340;&#35266;&#28857;&#24314;&#27169;&#25552;&#20379;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06074</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;11&#20013;&#30340;iLab Le-Wi-Di&#65306;&#27169;&#25311;&#19981;&#19968;&#33268;&#24615;&#36824;&#26159;&#27169;&#25311;&#35266;&#28857;&#65311;
&lt;/p&gt;
&lt;p&gt;
iLab at SemEval-2023 Task 11 Le-Wi-Di: Modelling Disagreement or Modelling Perspectives?. (arXiv:2305.06074v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#24314;&#27169;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#30340;&#20004;&#31181;&#26041;&#27861;&#65306;&#20998;&#24067;&#24335;&#36719;&#26631;&#35760;&#26041;&#27861;&#21644;&#24314;&#27169;&#20010;&#20307;&#27880;&#37322;&#32773;&#25110;&#20854;&#32452;&#30340;&#35266;&#28857;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#30340;&#26041;&#24335;&#26469;&#27169;&#25311;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#12290;&#23613;&#31649;&#22810;&#20219;&#21153;&#26041;&#27861;&#20043;&#21069;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#20854;&#22312;&#21253;&#21547;&#19981;&#21516;&#27880;&#37322;&#32773;&#35266;&#28857;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#32454;&#33268;&#22320;&#29702;&#35299;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#65292;&#20026;&#26356;&#20934;&#30830;&#30340;&#35266;&#28857;&#24314;&#27169;&#25552;&#20379;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24314;&#27169;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#65292;&#26377;&#20004;&#31181;&#31454;&#20105;&#30340;&#26041;&#27861;&#65306;&#20998;&#24067;&#24335;&#36719;&#26631;&#35760;&#26041;&#27861;&#65288;&#26088;&#22312;&#25429;&#25417;&#19981;&#19968;&#33268;&#31243;&#24230;&#65289;&#25110;&#24314;&#27169;&#20010;&#20307;&#27880;&#37322;&#32773;&#25110;&#20854;&#32452;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20808;&#21069;&#22312;&#24314;&#27169;&#35266;&#28857;&#26041;&#38754;&#34920;&#29616;&#25104;&#21151;&#30340;&#22810;&#20219;&#21153;&#26550;&#26500;&#65292;&#23545;SEMEVAL&#20219;&#21153;11&#30340;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#23558;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#21363;&#39044;&#27979;&#20010;&#21035;&#27880;&#37322;&#32773;&#30340;&#35266;&#28857;&#20316;&#20026;&#39044;&#27979;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#30340;&#36807;&#28193;&#27493;&#39588;&#12290;&#23613;&#31649;&#27492;&#26041;&#27861;&#20043;&#21069;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#22312;&#21253;&#21547;&#19981;&#21516;&#27880;&#37322;&#32773;&#35266;&#28857;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#22810;&#20219;&#21153;&#26041;&#27861;&#30340;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#24314;&#27169;&#35266;&#28857;&#26102;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#36866;&#29992;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#24378;&#28872;&#30340;&#35266;&#28857;&#20027;&#20041;&#26041;&#27861;&#21487;&#33021;&#19981;&#20250;&#26681;&#25454;&#20998;&#24067;&#24335;&#26041;&#27861;&#20351;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#26356;&#32454;&#33268;&#30340;&#29702;&#35299;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#21487;&#20197;&#23548;&#33268;&#26356;&#20934;&#30830;&#30340;&#35266;&#28857;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are two competing approaches for modelling annotator disagreement: distributional soft-labelling approaches (which aim to capture the level of disagreement) or modelling perspectives of individual annotators or groups thereof. We adapt a multi-task architecture -- which has previously shown success in modelling perspectives -- to evaluate its performance on the SEMEVAL Task 11. We do so by combining both approaches, i.e. predicting individual annotator perspectives as an interim step towards predicting annotator disagreement. Despite its previous success, we found that a multi-task approach performed poorly on datasets which contained distinct annotator opinions, suggesting that this approach may not always be suitable when modelling perspectives. Furthermore, our results explain that while strongly perspectivist approaches might not achieve state-of-the-art performance according to evaluation metrics used by distributional approaches, our approach allows for a more nuanced under
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65306;&#37327;&#23376;&#21464;&#20998;&#26680;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;QVK-SVM&#65289;&#65292;&#33021;&#22815;&#22312;&#20934;&#30830;&#24615;&#12289;&#25439;&#22833;&#21644;&#28151;&#28102;&#30697;&#38453;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;&#23427;&#24212;&#35813;&#25104;&#20026;&#26410;&#26469;QML&#30740;&#31350;&#20013;&#30340;&#21487;&#38752;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.06063</link><description>&lt;p&gt;
&#36890;&#36807;&#35823;&#24046;&#20989;&#25968;&#26680;&#35757;&#32451;&#22686;&#24378;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
Enhancing Quantum Support Vector Machines through Variational Kernel Training. (arXiv:2305.06063v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65306;&#37327;&#23376;&#21464;&#20998;&#26680;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;QVK-SVM&#65289;&#65292;&#33021;&#22815;&#22312;&#20934;&#30830;&#24615;&#12289;&#25439;&#22833;&#21644;&#28151;&#28102;&#30697;&#38453;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;&#23427;&#24212;&#35813;&#25104;&#20026;&#26410;&#26469;QML&#30740;&#31350;&#20013;&#30340;&#21487;&#38752;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20854;&#20013;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;QSVM&#65289;&#25104;&#20026;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#20004;&#31181;&#29616;&#26377;&#30340;QSVM&#26041;&#27861;&#65306;&#37327;&#23376;&#26680;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;QK-SVM&#65289;&#21644;&#37327;&#23376;&#21464;&#20998;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;QV-SVM&#65289;&#12290;&#34429;&#28982;&#20004;&#31181;&#26041;&#27861;&#22343;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#37327;&#23376;&#21464;&#20998;&#26680;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;QVK-SVM&#65289;&#65292;&#20197;&#22686;&#24378;&#20934;&#30830;&#24615;&#65292;&#34701;&#21512;&#20102;QK-SVM&#21644;QV-SVM&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#22312;&#40482;&#23614;&#33457;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;QVK-SVM&#22312;&#20934;&#30830;&#24615;&#12289;&#25439;&#22833;&#21644;&#28151;&#28102;&#30697;&#38453;&#25351;&#26631;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;QVK-SVM&#20316;&#20026;&#19968;&#31181;&#21487;&#38752;&#30340;&#12289;&#20855;&#26377;&#21464;&#38761;&#24615;&#30340;QML&#24212;&#29992;&#24037;&#20855;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#26410;&#26469;&#30340;QML&#30740;&#31350;&#20013;&#37319;&#29992;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning (QML) has witnessed immense progress recently, with quantum support vector machines (QSVMs) emerging as a promising model. This paper focuses on the two existing QSVM methods: quantum kernel SVM (QK-SVM) and quantum variational SVM (QV-SVM). While both have yielded impressive results, we present a novel approach that synergizes the strengths of QK-SVM and QV-SVM to enhance accuracy. Our proposed model, quantum variational kernel SVM (QVK-SVM), leverages the quantum kernel and quantum variational algorithm. We conducted extensive experiments on the Iris dataset and observed that QVK-SVM outperforms both existing models in terms of accuracy, loss, and confusion matrix indicators. Our results demonstrate that QVK-SVM holds tremendous potential as a reliable and transformative tool for QML applications. Hence, we recommend its adoption in future QML research endeavors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#35270;&#35273;&#35843;&#25972;&#30340;&#21457;&#23637;&#19982;&#29616;&#29366;&#65292;&#23558;&#36817;&#26399;&#30340;&#35270;&#35273;&#35843;&#25972;&#25216;&#26415;&#20998;&#20026;&#20116;&#31867;&#65292;&#21253;&#25324;&#25552;&#31034;&#35843;&#25972;&#12289;&#36866;&#37197;&#22120;&#35843;&#25972;&#12289;&#21442;&#25968;&#32763;&#35793;&#12289;&#32039;&#20945;&#35843;&#25972;&#21644;&#27169;&#22359;&#35843;&#25972;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.06061</link><description>&lt;p&gt;
&#35270;&#35273;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Visual Tuning. (arXiv:2305.06061v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#35270;&#35273;&#35843;&#25972;&#30340;&#21457;&#23637;&#19982;&#29616;&#29366;&#65292;&#23558;&#36817;&#26399;&#30340;&#35270;&#35273;&#35843;&#25972;&#25216;&#26415;&#20998;&#20026;&#20116;&#31867;&#65292;&#21253;&#25324;&#25552;&#31034;&#35843;&#25972;&#12289;&#36866;&#37197;&#22120;&#35843;&#25972;&#12289;&#21442;&#25968;&#32763;&#35793;&#12289;&#32039;&#20945;&#35843;&#25972;&#21644;&#27169;&#22359;&#35843;&#25972;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#24050;&#34987;&#24191;&#27867;&#35777;&#26126;&#22312;&#35768;&#22810;&#19979;&#28216;&#35270;&#35273;&#20219;&#21153;&#20013;&#20855;&#26377;&#26377;&#21069;&#36884;&#30340;&#34920;&#29616;&#12290;&#38543;&#30528;&#39044;&#35757;&#32451;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#24778;&#20154;&#21457;&#23637;&#65292;&#35270;&#35273;&#35843;&#25972;&#36339;&#20986;&#20102;&#26631;&#20934;&#30340;&#27169;&#24335;&#25805;&#20316;&#65292;&#21363;&#24494;&#35843;&#25972;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#25110;&#20165;&#24494;&#35843;&#23436;&#20840;&#36830;&#25509;&#23618;&#12290;&#30456;&#21453;&#65292;&#36817;&#26399;&#30340;&#36827;&#23637;&#21487;&#20197;&#36890;&#36807;&#26356;&#26032;&#26356;&#23569;&#30340;&#21442;&#25968;&#23454;&#29616;&#27604;&#20840;&#38754;&#24494;&#35843;&#25972;&#20010;&#39044;&#35757;&#32451;&#21442;&#25968;&#26356;&#20248;&#24322;&#30340;&#34920;&#29616;&#65292;&#20351;&#36793;&#32536;&#35774;&#22791;&#21644;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#37096;&#32626;&#22312;&#20113;&#31471;&#30340;&#26085;&#30410;&#24222;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#20840;&#38754;&#20102;&#35299;&#35270;&#35273;&#35843;&#25972;&#30340;&#20840;&#35980;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#26412;&#32508;&#36848;&#25551;&#32472;&#20102;&#22823;&#37327;&#30340;&#36817;&#26399;&#30740;&#31350;&#20316;&#21697;&#65292;&#25552;&#20379;&#20102;&#29616;&#26377;&#24037;&#20316;&#21644;&#27169;&#22411;&#31995;&#32479;&#21644;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#25552;&#20379;&#20102;&#35270;&#35273;&#35843;&#25972;&#30340;&#35814;&#32454;&#32972;&#26223;&#65292;&#24182;&#23558;&#26368;&#36817;&#30340;&#35270;&#35273;&#35843;&#25972;&#25216;&#26415;&#20998;&#20026;&#20116;&#32452;&#65306;&#25552;&#31034;&#35843;&#25972;&#12289;&#36866;&#37197;&#22120;&#35843;&#25972;&#12289;&#21442;&#25968;&#32763;&#35793;&#12289;&#32039;&#20945;&#35843;&#25972;&#21644;&#27169;&#22359;&#35843;&#25972;&#12290;&#26412;&#25991;&#36824;&#24378;&#35843;&#20102;&#24403;&#21069;&#35270;&#35273;&#35843;&#25972;&#25216;&#26415;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning visual models has been widely shown promising performance on many downstream visual tasks. With the surprising development of pre-trained visual foundation models, visual tuning jumped out of the standard modus operandi that fine-tunes the whole pre-trained model or just the fully connected layer. Instead, recent advances can achieve superior performance than full-tuning the whole pre-trained parameters by updating far fewer parameters, enabling edge devices and downstream applications to reuse the increasingly large foundation models deployed on the cloud. With the aim of helping researchers get the full picture and future directions of visual tuning, this survey characterizes a large and thoughtful selection of recent works, providing a systematic and comprehensive overview of existing work and models. Specifically, it provides a detailed background of visual tuning and categorizes recent visual tuning techniques into five groups: prompt tuning, adapter tuning, parameter 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#21464;&#21442;&#25968;&#32534;&#30721;&#20026;&#22810;&#23618;&#24352;&#37327;&#32593;&#32476;&#65292;&#26126;&#26174;&#20943;&#23569;&#20102;&#21487;&#21464;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#22312;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#21387;&#32553;&#24615;&#33021;&#65292;&#20197;VGG-16&#30340;&#27979;&#35797;&#31934;&#24230;&#25552;&#39640;&#20026;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.06058</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#25968;&#32423;&#21035;&#30340;&#23569;&#37327;&#21464;&#20998;&#21442;&#25968;&#30340;&#24352;&#37327;&#32593;&#32476;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Compressing neural network by tensor network with exponentially fewer variational parameters. (arXiv:2305.06058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#21464;&#21442;&#25968;&#32534;&#30721;&#20026;&#22810;&#23618;&#24352;&#37327;&#32593;&#32476;&#65292;&#26126;&#26174;&#20943;&#23569;&#20102;&#21487;&#21464;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#22312;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#21387;&#32553;&#24615;&#33021;&#65292;&#20197;VGG-16&#30340;&#27979;&#35797;&#31934;&#24230;&#25552;&#39640;&#20026;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#25152;&#21253;&#21547;&#30340;&#24040;&#22823;&#21487;&#21464;&#30340;&#21442;&#25968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36825;&#20123;&#21442;&#25968; encoding &#20026;&#22810;&#23618;&#24352;&#37327;&#32593;&#32476;&#65288;TN&#65289;&#30340;&#21387;&#32553;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#26696;&#28436;&#31034;&#20102;&#20986;&#33394;&#30340;&#21387;&#32553;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20197;&#27973;&#23618;&#24352;&#37327;&#32593;&#32476;&#20026;&#22522;&#30784;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;VGG-16&#20013;&#30340;3&#20010;&#21367;&#31215;&#23618;&#30340;&#22823;&#32422;1000&#19975;&#21442;&#25968;&#34987;&#21387;&#32553;&#21040;&#20855;&#26377;&#20165;632&#20010;&#21442;&#25968;&#30340;TN&#20013;&#65292;&#32780;&#22312;CIFAR-10&#19978;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#20196;&#20154;&#24778;&#21916;&#22320;&#25552;&#39640;&#20102;81.14&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network (NN) designed for challenging machine learning tasks is in general a highly nonlinear mapping that contains massive variational parameters. High complexity of NN, if unbounded or unconstrained, might unpredictably cause severe issues including over-fitting, loss of generalization power, and unbearable cost of hardware. In this work, we propose a general compression scheme that significantly reduces the variational parameters of NN by encoding them to multi-layer tensor networks (TN's) that contain exponentially-fewer free parameters. Superior compression performance of our scheme is demonstrated on several widely-recognized NN's (FC-2, LeNet-5, and VGG-16) and datasets (MNIST and CIFAR-10), surpassing the state-of-the-art method based on shallow tensor networks. For instance, about 10 million parameters in the three convolutional layers of VGG-16 are compressed in TN's with just $632$ parameters, while the testing accuracy on CIFAR-10 is surprisingly improved from $81.14
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#20381;&#36182;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#29992;&#20197;&#24110;&#21161;&#26356;&#22909;&#22320;&#20102;&#35299;&#33258;&#21160;&#21270;&#20915;&#31574;&#31995;&#32479;&#20013;&#30340;&#21453;&#39304;&#24490;&#29615;&#65292;&#20854;&#20013;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#20250;&#23548;&#33268;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#24471;&#21040;&#21152;&#21095;&#12290;</title><link>http://arxiv.org/abs/2305.06055</link><description>&lt;p&gt;
&#21453;&#39304;&#24490;&#29615;&#20998;&#31867;&#21450;&#20854;&#19982;&#33258;&#21160;&#21270;&#20915;&#31574;&#31995;&#32479;&#20013;&#20559;&#35265;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
A Classification of Feedback Loops and Their Relation to Biases in Automated Decision-Making Systems. (arXiv:2305.06055v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#20381;&#36182;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#29992;&#20197;&#24110;&#21161;&#26356;&#22909;&#22320;&#20102;&#35299;&#33258;&#21160;&#21270;&#20915;&#31574;&#31995;&#32479;&#20013;&#30340;&#21453;&#39304;&#24490;&#29615;&#65292;&#20854;&#20013;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#20250;&#23548;&#33268;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#24471;&#21040;&#21152;&#21095;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39044;&#27979;&#30340;&#20915;&#31574;&#31995;&#32479;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#36825;&#31867;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#22833;&#25511;&#30340;&#21453;&#39304;&#24490;&#29615;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#35686;&#23519;&#19981;&#32771;&#34385;&#23454;&#38469;&#29359;&#32618;&#29575;&#65292;&#19968;&#20877;&#22320;&#21069;&#24448;&#21516;&#19968;&#20010;&#31038;&#21306;&#65292;&#36825;&#21152;&#21095;&#20102;&#29616;&#26377;&#30340;&#20559;&#35265;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#33258;&#21160;&#21270;&#20915;&#31574;&#23545;&#31995;&#32479;&#26412;&#36523;&#20135;&#29983;&#20102;&#21160;&#24577;&#21453;&#39304;&#25928;&#24212;&#65292;&#36825;&#21487;&#33021;&#20250;&#25345;&#32493;&#24456;&#38271;&#26102;&#38388;&#65292;&#20351;&#24471;&#30701;&#35270;&#30340;&#35774;&#35745;&#36873;&#25321;&#38590;&#20197;&#25511;&#21046;&#31995;&#32479;&#30340;&#28436;&#21464;&#12290;&#23613;&#31649;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25552;&#20986;&#38271;&#26399;&#35299;&#20915;&#26041;&#26696;&#20197;&#38450;&#27490;&#36127;&#38754;&#32467;&#26524;&#65288;&#22914;&#23545;&#26576;&#20123;&#32676;&#20307;&#30340;&#20559;&#35265;&#65289;&#65292;&#20294;&#36825;&#20123;&#24178;&#39044;&#22823;&#37096;&#20998;&#21462;&#20915;&#20110;&#20020;&#26102;&#24314;&#27169;&#20551;&#35774;&#65292;&#30446;&#21069;&#32570;&#20047;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20915;&#31574;&#31995;&#32479;&#20013;&#21453;&#39304;&#21160;&#24577;&#30340;&#20005;&#26684;&#29702;&#35770;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21160;&#21147;&#31995;&#32479;&#29702;&#35770;&#30340;&#35821;&#35328;&#65292;&#36825;&#26159;&#24212;&#29992;&#25968;&#23398;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#29992;&#20110;&#20998;&#26512;&#33258;&#28982;&#21644;&#20154;&#24037;&#30340;&#31995;&#32479;&#21644;&#23427;&#20204;&#22914;&#20309;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#20381;&#36182;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#20102;&#35299;&#33258;&#21160;&#21270;&#20915;&#31574;&#30340;&#21453;&#39304;&#24490;&#29615;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prediction-based decision-making systems are becoming increasingly prevalent in various domains. Previous studies have demonstrated that such systems are vulnerable to runaway feedback loops, e.g., when police are repeatedly sent back to the same neighborhoods regardless of the actual rate of criminal activity, which exacerbate existing biases. In practice, the automated decisions have dynamic feedback effects on the system itself that can perpetuate over time, making it difficult for short-sighted design choices to control the system's evolution. While researchers started proposing longer-term solutions to prevent adverse outcomes (such as bias towards certain groups), these interventions largely depend on ad hoc modeling assumptions and a rigorous theoretical understanding of the feedback dynamics in ML-based decision-making systems is currently missing. In this paper, we use the language of dynamical systems theory, a branch of applied mathematics that deals with the analysis of the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#32570;&#22833;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#23545;&#30456;&#20851;&#22270;&#30340;&#24433;&#21709;&#65292;&#24314;&#35758;&#20351;&#29992;&#30452;&#25509;&#21442;&#25968;&#20272;&#35745;&#27861;(DPER)&#26469;&#32472;&#21046;&#30456;&#20851;&#22270;</title><link>http://arxiv.org/abs/2305.06044</link><description>&lt;p&gt;
&#32570;&#22833;&#20540;&#19979;&#30340;&#30456;&#20851;&#24615;&#21487;&#35270;&#21270;&#65306;&#22635;&#20805;&#27861;&#21644;&#30452;&#25509;&#21442;&#25968;&#20272;&#35745;&#27861;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Correlation visualization under missing values: a comparison between imputation and direct parameter estimation methods. (arXiv:2305.06044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#32570;&#22833;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#23545;&#30456;&#20851;&#22270;&#30340;&#24433;&#21709;&#65292;&#24314;&#35758;&#20351;&#29992;&#30452;&#25509;&#21442;&#25968;&#20272;&#35745;&#27861;(DPER)&#26469;&#32472;&#21046;&#30456;&#20851;&#22270;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#30697;&#38453;&#21487;&#35270;&#21270;&#23545;&#20110;&#29702;&#35299;&#25968;&#25454;&#38598;&#20013;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26159;&#32570;&#22833;&#25968;&#25454;&#20250;&#23545;&#30456;&#20851;&#31995;&#25968;&#30340;&#20272;&#35745;&#20135;&#29983;&#26174;&#33879;&#25361;&#25112;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#32570;&#22833;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#23545;&#30456;&#20851;&#22270;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;&#20004;&#31181;&#24120;&#35265;&#30340;&#32570;&#22833;&#27169;&#24335;&#65306;&#38543;&#26426;&#21644;&#21333;&#35843;&#12290;&#25105;&#20204;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#25552;&#20379;&#23454;&#29992;&#30340;&#31574;&#30053;&#21644;&#24314;&#35758;&#65292;&#20197;&#21019;&#24314;&#21644;&#20998;&#26512;&#30456;&#20851;&#22270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#22635;&#20805;&#27861;&#36890;&#24120;&#29992;&#20110;&#32570;&#22833;&#25968;&#25454;&#65292;&#20294;&#20351;&#29992;&#22635;&#20805;&#30340;&#25968;&#25454;&#26469;&#29983;&#25104;&#30456;&#20851;&#30697;&#38453;&#22270;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;&#29305;&#24449;&#20043;&#38388;&#20851;&#31995;&#30340;&#35823;&#23548;&#24615;&#25512;&#26029;&#12290;&#25105;&#20204;&#24314;&#35758;&#22522;&#20110;&#20854;&#22312;&#23454;&#39564;&#20013;&#30340;&#34920;&#29616;&#65292;&#20351;&#29992;DPER&#65292;&#19968;&#31181;&#30452;&#25509;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#32472;&#21046;&#30456;&#20851;&#30697;&#38453;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Correlation matrix visualization is essential for understanding the relationships between variables in a dataset, but missing data can pose a significant challenge in estimating correlation coefficients. In this paper, we compare the effects of various missing data methods on the correlation plot, focusing on two common missing patterns: random and monotone. We aim to provide practical strategies and recommendations for researchers and practitioners in creating and analyzing the correlation plot. Our experimental results suggest that while imputation is commonly used for missing data, using imputed data for plotting the correlation matrix may lead to a significantly misleading inference of the relation between the features. We recommend using DPER, a direct parameter estimation approach, for plotting the correlation matrix based on its performance in the experiments.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22359;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#22788;&#29702;&#21333;&#35843;&#32570;&#22833;&#25968;&#25454;&#30340;&#25554;&#20540;&#19982;&#38477;&#32500;&#26694;&#26550;&#65292;&#21487;&#20197;&#26174;&#33879;&#22320;&#20943;&#23569;&#25554;&#34917;&#26102;&#38388;&#65292;&#36866;&#29992;&#20110;&#22823;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.06042</link><description>&lt;p&gt;
&#22522;&#20110;&#22359;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#22788;&#29702;&#21333;&#35843;&#32570;&#22833;&#25968;&#25454;&#30340;&#25554;&#20540;&#19982;&#38477;&#32500;
&lt;/p&gt;
&lt;p&gt;
Blockwise Principal Component Analysis for monotone missing data imputation and dimensionality reduction. (arXiv:2305.06042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06042
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22359;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#22788;&#29702;&#21333;&#35843;&#32570;&#22833;&#25968;&#25454;&#30340;&#25554;&#20540;&#19982;&#38477;&#32500;&#26694;&#26550;&#65292;&#21487;&#20197;&#26174;&#33879;&#22320;&#20943;&#23569;&#25554;&#34917;&#26102;&#38388;&#65292;&#36866;&#29992;&#20110;&#22823;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35843;&#32570;&#22833;&#25968;&#25454;&#26159;&#25968;&#25454;&#20998;&#26512;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#25554;&#20540;&#21644;&#38477;&#32500;&#30340;&#32452;&#21512;&#21487;&#33021;&#20250;&#38754;&#20020;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;BPI&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#20102;&#22359;&#20869;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#26041;&#27861;&#22788;&#29702;&#21333;&#35843;&#32570;&#22833;&#25968;&#25454;&#30340;&#25554;&#20540;&#20197;&#21450;&#38477;&#32500;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#19982;&#21508;&#31181;&#25554;&#34917;&#25216;&#26415;&#19968;&#36215;&#20351;&#29992;&#65292;&#24182;&#19988;&#19982;&#25554;&#34917;&#21518;&#30340;&#38477;&#32500;&#30456;&#27604;&#65292;&#20854;&#21487;&#20197;&#26174;&#33879;&#22320;&#20943;&#23569;&#25554;&#34917;&#26102;&#38388;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26694;&#26550;&#20855;&#26377;&#36739;&#39640;&#25928;&#29575;&#24182;&#19988;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#23558;MICE&#30452;&#25509;&#24212;&#29992;&#20110;&#32570;&#22833;&#25968;&#25454;&#21487;&#33021;&#19981;&#26159;&#26368;&#20339;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monotone missing data is a common problem in data analysis. However, imputation combined with dimensionality reduction can be computationally expensive, especially with the increasing size of datasets. To address this issue, we propose a Blockwise principal component analysis Imputation (BPI) framework for dimensionality reduction and imputation of monotone missing data. The framework conducts Principal Component Analysis (PCA) on the observed part of each monotone block of the data and then imputes on merging the obtained principal components using a chosen imputation technique. BPI can work with various imputation techniques and can significantly reduce imputation time compared to conducting dimensionality reduction after imputation. This makes it a practical and efficient approach for large datasets with monotone missing data. Our experiments validate the improvement in speed. In addition, our experiments also show that while applying MICE imputation directly on missing data may not
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GNN&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#26694;&#26550;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06026</link><description>&lt;p&gt;
&#25628;&#32034;UGLE&#30495;&#30456;&#65306;&#26080;&#30417;&#30563;GNN&#23398;&#20064;&#29615;&#22659;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Search for the UGLE Truth: An Investigation into Unsupervised GNN Learning Environments. (arXiv:2305.06026v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GNN&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#26694;&#26550;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#26159;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#22270;&#32467;&#26500;&#19978;&#30340;&#20989;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#21644;&#34920;&#36798;&#24615;&#24378;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#31038;&#21306;&#26816;&#27979;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;GNN&#36827;&#34892;&#12290;&#21033;&#29992;&#33410;&#28857;&#29305;&#24449;&#30340;&#22810;&#32500;&#24230;&#19982;&#22270;&#30340;&#36830;&#25509;&#24615;&#23545;&#22270;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#32858;&#31867;&#65292;&#23545;&#20174;&#31038;&#20132;&#32593;&#32476;&#21040;&#22522;&#22240;&#32452;&#23398;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#25991;&#29486;&#20013;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#30340;&#20805;&#20998;&#22522;&#20934;&#29615;&#22659;&#65292;&#20174;&#32780;&#21487;&#33021;&#38459;&#30861;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#22256;&#38590;&#26159;&#27169;&#31946;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#29615;&#22659;&#19982;&#24615;&#33021;&#21644;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#20914;&#31361;&#25351;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21644;&#35780;&#20272;&#20102;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;GNN&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#19968;&#33268;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#25351;&#26631;&#65292;&#21453;&#26144;&#20102;&#26816;&#27979;&#21040;&#30340;&#31038;&#21306;&#30340;&#20869;&#22312;&#36136;&#37327;&#20197;&#21450;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a pertinent tool for any machine learning task due to their ability to learn functions over graph structures, a powerful and expressive data representation. The detection of communities, an unsupervised task has increasingly been performed with GNNs. Clustering nodes in a graph using the multi-dimensionality of node features with the connectivity of the graph has many applications to real world tasks from social networks to genomics. Unfortunately, there is currently a gap in the literature with no established sufficient benchmarking environment for fairly and rigorously evaluating GNN based community detection, thereby potentially impeding progress in this nascent field. We observe the particular difficulties in this setting is the ambiguous hyperparameter tuning environments combined with conflicting metrics of performance and evaluation datasets. In this work, we propose and evaluate frameworks for the consistent comparisons of community detection al
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102; Deep Galerkin Method&#65288;DGM&#65289;&#31639;&#27861;&#22312;&#20840;&#23616;&#25910;&#25947;&#26102;&#65292;&#35757;&#32451;&#24471;&#21040;&#30340;&#31070;&#32463;&#32593;&#32476;&#20250;&#25910;&#25947;&#20110;&#19968;&#20010;&#26080;&#38480;&#32500;&#30340;&#32447;&#24615;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65292;&#20174;&#32780;&#36924;&#36817;&#27714;&#35299;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#65292;PINNs &#26041;&#27861;&#20250;&#26356;&#31283;&#23450;&#12290;</title><link>http://arxiv.org/abs/2305.06000</link><description>&lt;p&gt;
&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340; Deep Galerkin &#19982; PINNs &#26041;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global Convergence of Deep Galerkin and PINNs Methods for Solving Partial Differential Equations. (arXiv:2305.06000v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06000
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102; Deep Galerkin Method&#65288;DGM&#65289;&#31639;&#27861;&#22312;&#20840;&#23616;&#25910;&#25947;&#26102;&#65292;&#35757;&#32451;&#24471;&#21040;&#30340;&#31070;&#32463;&#32593;&#32476;&#20250;&#25910;&#25947;&#20110;&#19968;&#20010;&#26080;&#38480;&#32500;&#30340;&#32447;&#24615;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65292;&#20174;&#32780;&#36924;&#36817;&#27714;&#35299;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#65292;PINNs &#26041;&#27861;&#20250;&#26356;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#25968;&#20540;&#27714;&#35299;&#20013;&#65292;&#30001;&#20110;&#32500;&#24230;&#35781;&#21650;&#65292;&#20256;&#32479;&#26041;&#27861;&#22914;&#26377;&#38480;&#24046;&#20998;&#26080;&#27861;&#27714;&#35299;&#12290;&#36817;&#26399;&#65292;&#19968;&#31995;&#21015;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#24335;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#26469;&#36924;&#36817;&#27714;&#35299;&#32467;&#26524;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30446;&#21069;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861; Deep Galerkin Method (DGM) &#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;DGM &#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#27714;&#35299;&#32467;&#26524;&#12290;&#24403;&#21333;&#23618;&#32593;&#32476;&#20013;&#38544;&#23618;&#21333;&#20803;&#30340;&#25968;&#37327;&#36235;&#21521;&#20110;&#26080;&#31351;&#22823;&#26102;&#65288;&#21363;"&#23485;&#24230;&#32593;&#32476;&#26497;&#38480;"&#65289;&#65292;&#35757;&#32451;&#21518;&#30340;&#31070;&#32463;&#32593;&#32476;&#36880;&#28176;&#25910;&#25947;&#20110;&#19968;&#20010;&#26080;&#38480;&#32500;&#30340;&#32447;&#24615;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#12290;&#26497;&#38480;&#36924;&#36817;&#22120;&#30340; PDE &#27531;&#24046;&#22312;&#35757;&#32451;&#26102;&#38388;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#36235;&#36817;&#20110;&#38646;&#12290;&#22312;&#21512;&#29702;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#23558; Deep Galerkin Method &#30340;&#32467;&#26524;&#19982;&#21478;&#19968;&#31181;&#24120;&#29992;&#26041;&#27861; PINNs &#30340;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#65292; PINNs &#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#26356;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerically solving high-dimensional partial differential equations (PDEs) is a major challenge. Conventional methods, such as finite difference methods, are unable to solve high-dimensional PDEs due to the curse-of-dimensionality. A variety of deep learning methods have been recently developed to try and solve high-dimensional PDEs by approximating the solution using a neural network. In this paper, we prove global convergence for one of the commonly-used deep learning algorithms for solving PDEs, the Deep Galerkin Method (DGM). DGM trains a neural network approximator to solve the PDE using stochastic gradient descent. We prove that, as the number of hidden units in the single-layer network goes to infinity (i.e., in the ``wide network limit"), the trained neural network converges to the solution of an infinite-dimensional linear ordinary differential equation (ODE). The PDE residual of the limiting approximator converges to zero as the training time $\rightarrow \infty$. Under mild 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#32467;&#26500;Hawkes&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30636;&#26102;&#25928;&#24212;&#23398;&#20064;&#31163;&#25955;&#26102;&#38388;&#20107;&#20214;&#24207;&#21015;&#20013;&#20107;&#20214;&#31867;&#22411;&#20043;&#38388;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.05986</link><description>&lt;p&gt;
&#20174;&#31163;&#25955;&#21270;&#26102;&#38388;&#20107;&#20214;&#24207;&#21015;&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#30340;&#32467;&#26500;Hawkes&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Structural Hawkes Processes for Learning Causal Structure from Discrete-Time Event Sequences. (arXiv:2305.05986v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#32467;&#26500;Hawkes&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30636;&#26102;&#25928;&#24212;&#23398;&#20064;&#31163;&#25955;&#26102;&#38388;&#20107;&#20214;&#24207;&#21015;&#20013;&#20107;&#20214;&#31867;&#22411;&#20043;&#38388;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31163;&#25955;&#26102;&#38388;&#20107;&#20214;&#24207;&#21015;&#20013;&#23398;&#20064;&#20107;&#20214;&#31867;&#22411;&#20043;&#38388;&#30340;&#22240;&#26524;&#32467;&#26500;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;&#22810;&#20803;Hawkes&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#22823;&#22810;&#37117;&#24402;&#32467;&#20026;&#23398;&#20064;&#25152;&#35859;&#30340;Granger&#22240;&#26524;&#20851;&#31995;&#65292;&#23427;&#20551;&#23450;&#22240;&#26524;&#20107;&#20214;&#22312;&#25928;&#24212;&#20107;&#20214;&#20043;&#21069;&#20005;&#26684;&#21457;&#29983;&#12290;&#36825;&#31181;&#20551;&#35774;&#22312;&#20302;&#20998;&#36776;&#29575;&#31163;&#25955;&#26102;&#38388;&#20107;&#20214;&#24207;&#21015;&#20013;&#24448;&#24448;&#26159;&#19981;&#21487;&#34892;&#30340;&#65307;&#32780;&#20856;&#22411;&#30340;&#31163;&#25955;Hawkes&#36807;&#31243;&#20027;&#35201;&#21463;&#21040;&#30636;&#26102;&#25928;&#24212;&#24341;&#36215;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#21363;&#30001;&#20110;&#20302;&#20998;&#36776;&#29575;&#25968;&#25454;&#32780;&#21516;&#26102;&#21457;&#29983;&#30340;&#22240;&#26524;&#20851;&#31995;&#19981;&#20250;&#34987;Granger&#22240;&#26524;&#24615;&#25152;&#25429;&#25417;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;Hawkes&#36807;&#31243;&#65288;SHPs&#65289;&#65292;&#21033;&#29992;&#30636;&#26102;&#25928;&#24212;&#26469;&#23398;&#20064;&#31163;&#25955;&#20107;&#20214;&#24207;&#21015;&#20013;&#20107;&#20214;&#31867;&#22411;&#20043;&#38388;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26368;&#23567;&#21270;-&#26368;&#22823;&#21270;&#20284;&#28982;&#20989;&#25968;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning causal structure among event types from discrete-time event sequences is a particularly important but challenging task. Existing methods, such as the multivariate Hawkes processes based methods, mostly boil down to learning the so-called Granger causality which assumes that the cause event happens strictly prior to its effect event. Such an assumption is often untenable beyond applications, especially when dealing with discrete-time event sequences in low-resolution; and typical discrete Hawkes processes mainly suffer from identifiability issues raised by the instantaneous effect, i.e., the causal relationship that occurred simultaneously due to the low-resolution data will not be captured by Granger causality. In this work, we propose Structure Hawkes Processes (SHPs) that leverage the instantaneous effect for learning the causal structure among events type in discrete-time event sequence. The proposed method is featured with the minorization-maximization of the likelihood fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38656;&#35201;&#35299;&#20915;&#21307;&#30103;&#23545;&#35805;&#25688;&#35201;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21010;&#20998;&#20026;&#20960;&#20010;&#23567;&#30340;&#23545;&#35805;&#20219;&#21153;&#20381;&#27425;&#26500;&#24314;&#65292;&#20351;&#29992;GPT-3&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21160;&#24577;&#26500;&#24314;&#25552;&#31034;&#20219;&#21153;&#20197;&#21450;&#30830;&#23450;&#21307;&#23398;&#23454;&#20307;&#21450;&#20854;&#30830;&#35748;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2305.05982</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#26041;&#27861;&#29983;&#25104;&#21307;&#23398;&#31934;&#30830;&#30340;&#24739;&#32773;-&#21307;&#29983;&#23545;&#35805;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Generating medically-accurate summaries of patient-provider dialogue: A multi-stage approach using large language models. (arXiv:2305.05982v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38656;&#35201;&#35299;&#20915;&#21307;&#30103;&#23545;&#35805;&#25688;&#35201;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21010;&#20998;&#20026;&#20960;&#20010;&#23567;&#30340;&#23545;&#35805;&#20219;&#21153;&#20381;&#27425;&#26500;&#24314;&#65292;&#20351;&#29992;GPT-3&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21160;&#24577;&#26500;&#24314;&#25552;&#31034;&#20219;&#21153;&#20197;&#21450;&#30830;&#23450;&#21307;&#23398;&#23454;&#20307;&#21450;&#20854;&#30830;&#35748;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#25552;&#20379;&#32773;&#23545;&#24739;&#32773;&#35775;&#38382;&#30340;&#25688;&#35201;&#20855;&#26377;&#20851;&#38190;&#20316;&#29992;&#65292;&#21253;&#25324;&#20020;&#24202;&#20915;&#31574;&#12289;&#21327;&#35843;&#21307;&#30103;&#22242;&#38431;&#21644;&#24739;&#32773;&#30340;&#21442;&#32771;&#36164;&#26009;&#12290;&#26377;&#25928;&#30340;&#25688;&#35201;&#38656;&#35201;&#27969;&#30021;&#65292;&#24182;&#20934;&#30830;&#25429;&#25417;&#23545;&#35805;&#20013;&#30340;&#25152;&#26377;&#21307;&#23398;&#30456;&#20851;&#20449;&#24687;&#65292;&#23613;&#31649;&#24739;&#32773;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#12290;&#21363;&#20351;&#22312;&#35775;&#38382;&#25688;&#35201;&#20013;&#20986;&#29616;&#36731;&#24494;&#30340;&#19981;&#20934;&#30830; (&#20363;&#22914;&#65292;&#22312;&#21457;&#28909;&#26102;&#24635;&#32467;&#20026;&#8220;&#24739;&#32773;&#27809;&#26377;&#21457;&#28903;&#8221;) &#20063;&#20250;&#23545;&#24739;&#32773;&#30340;&#25252;&#29702;&#32467;&#26524;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#26412;&#25991;&#23558;&#21307;&#23398;&#20250;&#35805;&#25688;&#35201;&#38382;&#39064;&#21010;&#20998;&#20026;&#25968;&#20010;&#23567;&#22411;&#23545;&#35805;&#29702;&#35299;&#20219;&#21153;&#65292;&#20854;&#20250;&#20381;&#27425;&#26500;&#24314;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#23545;&#35805;&#20013;&#30340;&#21307;&#23398;&#23454;&#20307;&#21450;&#20854;&#30830;&#35748;&#29366;&#24577;&#65292;&#20316;&#20026;&#26500;&#24314;&#27169;&#22359;&#65292;&#36890;&#36807;&#22312;&#20851;&#32852;&#24739;&#32773;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21160;&#24577;&#26500;&#24314;&#23569;&#37327;&#25552;&#31034;&#20219;&#21153;&#12290;&#20351;&#29992;GPT-3&#20316;&#20026;&#25105;&#20204;&#30340;&#25903;&#25745;&#12290;
&lt;/p&gt;
&lt;p&gt;
A medical provider's summary of a patient visit serves several critical purposes, including clinical decision-making, facilitating hand-offs between providers, and as a reference for the patient. An effective summary is required to be coherent and accurately capture all the medically relevant information in the dialogue, despite the complexity of patient-generated language. Even minor inaccuracies in visit summaries (for example, summarizing "patient does not have a fever" when a fever is present) can be detrimental to the outcome of care for the patient.  This paper tackles the problem of medical conversation summarization by discretizing the task into several smaller dialogue-understanding tasks that are sequentially built upon. First, we identify medical entities and their affirmations within the conversation to serve as building blocks. We study dynamically constructing few-shot prompts for tasks by conditioning on relevant patient information and use GPT-3 as the backbone for our 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#20102;&#19968;&#31867;&#30001;&#31649;&#36947;&#22270;&#25551;&#36848;&#30340;&#19977;&#32500;&#27969;&#24418;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#39640;&#20934;&#30830;&#24615;&#30340;GNN&#27169;&#22411;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;GNN&#22312;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.05966</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#19977;&#32500;&#25299;&#25169;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks and 3-Dimensional Topology. (arXiv:2305.05966v1 [math.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#20102;&#19968;&#31867;&#30001;&#31649;&#36947;&#22270;&#25551;&#36848;&#30340;&#19977;&#32500;&#27969;&#24418;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#39640;&#20934;&#30830;&#24615;&#30340;GNN&#27169;&#22411;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;GNN&#22312;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#26576;&#20123;&#31616;&#21333;&#24773;&#22659;&#20013;&#27979;&#35797;&#20102;&#23558;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#21040;&#20302;&#32500;&#25299;&#25169;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#30001;&#31649;&#36947;&#22270;&#25551;&#36848;&#30340;&#19977;&#32500;&#27969;&#24418;&#31867;&#21035;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#35299;&#20915;&#21028;&#26029;&#19968;&#23545;&#22270;&#26159;&#21542;&#25552;&#20379;&#21516;&#32986;&#19977;&#32500;&#27969;&#24418;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;GNN&#65292;&#20197;&#39640;&#20934;&#30830;&#24615;&#25552;&#20379;&#36825;&#31181;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#31572;&#26696;&#20026;&#32943;&#23450;&#65292;&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;GNN&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#65292;&#25214;&#21040;&#23558;&#19968;&#23545;&#22270;&#30456;&#20851;&#32852;&#30340;Neumann&#31227;&#21160;&#24207;&#21015;&#12290;&#36825;&#20010;&#24773;&#22659;&#21487;&#20197;&#20316;&#20026;&#35299;&#20915;&#19968;&#23545;Kirby&#22270;&#26159;&#21542;&#25552;&#20379;&#21516;&#26500;&#19977;&#32500;&#25110;&#22235;&#32500;&#27969;&#24418;&#38382;&#39064;&#30340;&#29609;&#20855;&#27169;&#22411;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We test the efficiency of applying Geometric Deep Learning to the problems in low-dimensional topology in a certain simple setting. Specifically, we consider the class of 3-manifolds described by plumbing graphs and use Graph Neural Networks (GNN) for the problem of deciding whether a pair of graphs give homeomorphic 3-manifolds. We use supervised learning to train a GNN that provides the answer to such a question with high accuracy. Moreover, we consider reinforcement learning by a GNN to find a sequence of Neumann moves that relates the pair of graphs if the answer is positive. The setting can be understood as a toy model of the problem of deciding whether a pair of Kirby diagrams give diffeomorphic 3- or 4-manifolds.
&lt;/p&gt;</description></item><item><title>Spectrum Breathing&#26159;&#19968;&#31181;&#20445;&#25252;&#31354;&#20013;&#32852;&#21512;&#23398;&#20064;&#20813;&#21463;&#24178;&#25200;&#30340;&#23454;&#38469;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#26799;&#24230;&#21098;&#26525;&#21644;&#25193;&#39057;&#32423;&#32852;&#36215;&#26469;&#65292;&#20197;&#21387;&#21046;&#24178;&#25200;&#32780;&#26080;&#38656;&#25193;&#23637;&#24102;&#23485;&#12290;&#20195;&#20215;&#26159;&#22686;&#21152;&#30340;&#23398;&#20064;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2305.05933</link><description>&lt;p&gt;
Spectrum Breathing&#65306;&#20445;&#25252;&#31354;&#20013;&#32852;&#21512;&#23398;&#20064;&#20813;&#21463;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Spectrum Breathing: Protecting Over-the-Air Federated Learning Against Interference. (arXiv:2305.05933v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05933
&lt;/p&gt;
&lt;p&gt;
Spectrum Breathing&#26159;&#19968;&#31181;&#20445;&#25252;&#31354;&#20013;&#32852;&#21512;&#23398;&#20064;&#20813;&#21463;&#24178;&#25200;&#30340;&#23454;&#38469;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#26799;&#24230;&#21098;&#26525;&#21644;&#25193;&#39057;&#32423;&#32852;&#36215;&#26469;&#65292;&#20197;&#21387;&#21046;&#24178;&#25200;&#32780;&#26080;&#38656;&#25193;&#23637;&#24102;&#23485;&#12290;&#20195;&#20215;&#26159;&#22686;&#21152;&#30340;&#23398;&#20064;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#26159;&#19968;&#31181;&#20174;&#20998;&#24067;&#24335;&#31227;&#21160;&#25968;&#25454;&#20013;&#33976;&#39311;&#20154;&#24037;&#26234;&#33021;&#30340;&#24191;&#27867;&#24212;&#29992;&#33539;&#20363;&#12290;&#20294;&#32852;&#21512;&#23398;&#20064;&#22312;&#31227;&#21160;&#32593;&#32476;&#20013;&#30340;&#37096;&#32626;&#21487;&#33021;&#20250;&#21463;&#21040;&#37051;&#36817;&#21333;&#20803;&#25110;&#24178;&#25200;&#28304;&#30340;&#24178;&#25200;&#32780;&#21463;&#25439;&#12290;&#29616;&#26377;&#30340;&#24178;&#25200;&#25233;&#21046;&#25216;&#26415;&#38656;&#35201;&#22810;&#21333;&#20803;&#21512;&#20316;&#25110;&#33267;&#23569;&#38656;&#35201;&#26114;&#36149;&#30340;&#24178;&#25200;&#36890;&#36947;&#29366;&#24577;&#20449;&#24687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23558;&#24178;&#25200;&#35270;&#20026;&#22122;&#22768;&#36827;&#34892;&#21151;&#29575;&#25511;&#21046;&#21487;&#33021;&#24182;&#19981;&#26377;&#25928;&#65292;&#30001;&#20110;&#39044;&#31639;&#38480;&#21046;&#65292;&#20063;&#30001;&#20110;&#36825;&#31181;&#26426;&#21046;&#21487;&#33021;&#20250;&#35302;&#21457;&#24178;&#25200;&#28304;&#30340;&#21453;&#21046;&#25514;&#26045;&#12290;&#20316;&#20026;&#20445;&#25252;&#31354;&#20013;&#32852;&#21512;&#23398;&#20064;&#20813;&#21463;&#24178;&#25200;&#30340;&#23454;&#38469;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Spectrum Breathing&#65292;&#23427;&#23558;&#38543;&#26426;&#26799;&#24230;&#21098;&#26525;&#21644;&#25193;&#39057;&#32423;&#32852;&#36215;&#26469;&#65292;&#20197;&#21387;&#21046;&#24178;&#25200;&#32780;&#26080;&#38656;&#25193;&#23637;&#24102;&#23485;&#12290;&#20195;&#20215;&#26159;&#36890;&#36807;&#21033;&#29992;&#21098;&#26525;&#23548;&#33268;&#23398;&#20064;&#36895;&#24230;&#20248;&#38597;&#38477;&#20302;&#32780;&#22686;&#21152;&#30340;&#23398;&#20064;&#24310;&#36831;&#12290;&#25105;&#20204;&#23558;&#20004;&#20010;&#25805;&#20316;&#21516;&#27493;&#65292;&#20197;&#20445;&#35777;&#23427;&#20204;&#30340;&#32423;&#21035;&#26159;&#30456;&#20114;&#23545;&#24212;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a widely embraced paradigm for distilling artificial intelligence from distributed mobile data. However, the deployment of FL in mobile networks can be compromised by exposure to interference from neighboring cells or jammers. Existing interference mitigation techniques require multi-cell cooperation or at least interference channel state information, which is expensive in practice. On the other hand, power control that treats interference as noise may not be effective due to limited power budgets, and also that this mechanism can trigger countermeasures by interference sources. As a practical approach for protecting FL against interference, we propose Spectrum Breathing, which cascades stochastic-gradient pruning and spread spectrum to suppress interference without bandwidth expansion. The cost is higher learning latency by exploiting the graceful degradation of learning speed due to pruning. We synchronize the two operations such that their levels are contr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#39044;&#27979;&#33181;&#20851;&#33410;&#39592;&#20851;&#33410;&#28814;&#36827;&#23637;&#65292;&#21457;&#29616;&#25104;&#20687;&#25968;&#25454;&#22312;&#39044;&#27979;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.05927</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#33181;&#20851;&#33410;X&#32447;&#29031;&#29255;&#39044;&#27979;&#39628;&#32929;&#20851;&#33410;&#39592;&#20851;&#33410;&#28814;&#36827;&#23637;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Predicting Progression of Patellofemoral Osteoarthritis Based on Lateral Knee Radiographs, Demographic Data and Symptomatic Assessments. (arXiv:2305.05927v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#39044;&#27979;&#33181;&#20851;&#33410;&#39592;&#20851;&#33410;&#28814;&#36827;&#23637;&#65292;&#21457;&#29616;&#25104;&#20687;&#25968;&#25454;&#22312;&#39044;&#27979;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#39044;&#27979;&#39628;&#32929;&#20851;&#33410;&#39592;&#20851;&#33410;&#28814;&#65288;PFOA&#65289;&#22312;&#19971;&#24180;&#20869;&#25918;&#23556;&#24615;&#36827;&#23637;&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#30740;&#31350;&#21253;&#25324;&#26469;&#33258;MOST&#30740;&#31350;&#22522;&#32447;&#30340;&#20027;&#20307;&#65288;1832&#20010;&#20027;&#20307;&#65292;3276&#20010;&#33181;&#30422;&#65289;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#21270;&#30340;&#26631;&#24535;&#26816;&#27979;&#24037;&#20855;&#65288;BoneFinder&#65289;&#22312;&#33181;&#20851;&#33410;X&#32447;&#30340;&#20391;&#38754;&#35782;&#21035;PF&#20851;&#33410;&#24863;&#20852;&#36259;&#21306;&#22495;&#12290;&#22312;5&#20493;&#20132;&#21449;&#39564;&#35777;&#35774;&#32622;&#20013;&#65292;&#22522;&#20110;&#25104;&#20687;&#25968;&#25454;&#24320;&#21457;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;DL&#26041;&#27861;&#26469;&#39044;&#27979;PFOA&#36827;&#23637;&#12290;&#24320;&#21457;&#20102;&#19968;&#32452;&#22522;&#20110;&#24050;&#30693;&#39118;&#38505;&#22240;&#32032;&#30340;&#22522;&#32447;&#65292;&#24182;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#26426;&#65288;GBM&#65289;&#36827;&#34892;&#20998;&#26512;&#12290;&#39118;&#38505;&#22240;&#32032;&#21253;&#25324;&#24180;&#40836;&#65292;&#24615;&#21035;&#65292;BMI&#21644;WOMAC&#35780;&#20998;&#65292;&#20197;&#21450;&#33003;&#39592;-&#32929;&#39592;&#20851;&#33410;&#30340;&#25918;&#23556;&#24615;&#39592;&#20851;&#33410;&#28814;&#20998;&#26399;&#65288;KL&#20998;&#25968;&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25104;&#20687;&#21644;&#20020;&#24202;&#25968;&#25454;&#35757;&#32451;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#12290;&#22312;&#20010;&#20307;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27880;&#24847;&#27169;&#22411;&#30340;&#24615;&#33021;&#33719;&#24471;&#20102;&#26368;&#20339;&#32508;&#21512;&#24615;&#33021;&#65292;AUC-ROC&#20026;0.83&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#21033;&#29992;DL&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#39044;&#27979;PFOA&#36827;&#23637;&#30340;&#28508;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#38500;&#20102;&#20020;&#24202;&#25968;&#25454;&#22806;&#36824;&#38656;&#35201;&#32435;&#20837;&#25104;&#20687;&#25968;&#25454;&#20197;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose a novel framework that utilizes deep learning (DL) and attention mechanisms to predict the radiographic progression of patellofemoral osteoarthritis (PFOA) over a period of seven years. This study included subjects (1832 subjects, 3276 knees) from the baseline of the MOST study. PF joint regions-of-interest were identified using an automated landmark detection tool (BoneFinder) on lateral knee X-rays. An end-to-end DL method was developed for predicting PFOA progression based on imaging data in a 5-fold cross-validation setting. A set of baselines based on known risk factors were developed and analyzed using gradient boosting machine (GBM). Risk factors included age, sex, BMI and WOMAC score, and the radiographic osteoarthritis stage of the tibiofemoral joint (KL score). Finally, we trained an ensemble model using both imaging and clinical data. Among the individual models, the performance of our deep convolutional neural network attention model achieved the b
&lt;/p&gt;</description></item><item><title>FastServe&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#25512;&#29702;&#26381;&#21153;&#31995;&#32479;&#65292;&#21033;&#29992;&#25250;&#21344;&#24335;&#35843;&#24230;&#21644;&#36339;&#36807;-&#36830;&#25509;&#22810;&#32423;&#21453;&#39304;&#38431;&#21015;&#65292;&#26368;&#23567;&#21270;&#27169;&#22411;&#25512;&#26029;&#30340;&#20316;&#19994;&#23436;&#25104;&#26102;&#38388;(JCT)&#12290;</title><link>http://arxiv.org/abs/2305.05920</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24555;&#36895;&#20998;&#24067;&#24335;&#25512;&#26029;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Fast Distributed Inference Serving for Large Language Models. (arXiv:2305.05920v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05920
&lt;/p&gt;
&lt;p&gt;
FastServe&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#25512;&#29702;&#26381;&#21153;&#31995;&#32479;&#65292;&#21033;&#29992;&#25250;&#21344;&#24335;&#35843;&#24230;&#21644;&#36339;&#36807;-&#36830;&#25509;&#22810;&#32423;&#21453;&#39304;&#38431;&#21015;&#65292;&#26368;&#23567;&#21270;&#27169;&#22411;&#25512;&#26029;&#30340;&#20316;&#19994;&#23436;&#25104;&#26102;&#38388;(JCT)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#25512;&#21160;&#20102;&#20197;ChatGPT&#20026;&#20195;&#34920;&#30340;&#26032;&#19968;&#20195;&#20114;&#21160;AI&#24212;&#29992;&#31243;&#24207;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#30340;&#20132;&#20114;&#24615;&#35201;&#27714;&#27169;&#22411;&#25512;&#26029;&#30340;&#20302;&#20316;&#19994;&#23436;&#25104;&#26102;&#38388;(JCT)&#12290;&#29616;&#26377;&#30340;LLM&#26381;&#21153;&#31995;&#32479;&#20351;&#29992;&#30340;&#26159;&#36816;&#34892;&#21040;&#23436;&#25104;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#23384;&#22312;&#22836;&#37096;&#38459;&#22622;&#21644;&#38271;JCT&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FastServe&#65292;&#19968;&#31181;&#38024;&#23545;LLMs&#30340;&#20998;&#24067;&#24335;&#25512;&#29702;&#26381;&#21153;&#31995;&#32479;&#12290;FastServe&#21033;&#29992;LLM&#25512;&#29702;&#30340;&#33258;&#22238;&#24402;&#27169;&#24335;&#65292;&#20197;&#27599;&#20010;&#36755;&#20986;&#26631;&#35760;&#30340;&#31890;&#24230;&#23454;&#29616;&#25250;&#21344;&#24335;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#36339;&#36807;-&#36830;&#25509;&#22810;&#32423;&#21453;&#39304;&#38431;&#21015;&#35843;&#24230;&#22120;&#26368;&#23567;&#21270;JCT&#12290;&#22522;&#20110;LLM&#25512;&#29702;&#30340;&#26032;&#21322;&#20449;&#24687;&#19981;&#21487;&#30693;&#35774;&#32622;&#65292;&#35843;&#24230;&#31243;&#24207;&#21033;&#29992;&#36755;&#20837;&#38271;&#24230;&#20449;&#24687;&#26469;&#20026;&#27599;&#20010;&#21040;&#36798;&#20316;&#19994;&#20998;&#37197;&#36866;&#24403;&#30340;&#21021;&#22987;&#38431;&#21015;&#26469;&#36830;&#25509;&#12290;&#39640;&#20110;&#25152;&#36830;&#25509;&#38431;&#21015;&#30340;&#20248;&#20808;&#32423;&#38431;&#21015;&#34987;&#36339;&#36807;&#20197;&#20943;&#23569;&#38477;&#32423;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;GPU&#20869;&#23384;&#31649;&#29702;&#26426;&#21046;&#65292;&#20197;&#25552;&#21069;&#28165;&#38500;&#19981;&#20877;&#20351;&#29992;&#30340;GPU&#32531;&#23384;&#65292;&#24182;&#23545;&#24120;&#29992;&#27169;&#22411;&#36827;&#34892;&#32531;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) power a new generation of interactive AI applications exemplified by ChatGPT. The interactive nature of these applications demand low job completion time (JCT) for model inference. Existing LLM serving systems use run-to-completion processing for inference jobs, which suffers from head-of-line blocking and long JCT. We present FastServe, a distributed inference serving system for LLMs. FastServe exploits the autoregressive pattern of LLM inference to enable preemption at the granularity of each output token. FastServe uses preemptive scheduling to minimize JCT with a novel skip-join Multi-Level Feedback Queue scheduler. Based on the new semi information-agnostic setting of LLM inference, the scheduler leverages the input length information to assign an appropriate initial queue for each arrival job to join. The higher priority queues than the joined queue are skipped to reduce demotions. We design an efficient GPU memory management mechanism that proactivel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#21028;&#21035;&#27169;&#22411;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#39640;&#26031;&#32806;&#21512;softmax&#23618;&#20801;&#35768;&#20998;&#31867;&#22120;&#20272;&#35745;&#31867;&#21518;&#39564;&#20998;&#24067;&#21644;&#31867;&#26465;&#20214;&#25968;&#25454;&#20998;&#24067;&#65292;&#20855;&#26377;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#32622;&#20449;&#24230;&#26657;&#20934;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.05912</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;&#32806;&#21512;softmax&#23618;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#21028;&#21035;&#27169;&#22411;&#30340;&#28151;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Hybrid of Generative and Discriminative Models Based on the Gaussian-coupled Softmax Layer. (arXiv:2305.05912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#21028;&#21035;&#27169;&#22411;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#39640;&#26031;&#32806;&#21512;softmax&#23618;&#20801;&#35768;&#20998;&#31867;&#22120;&#20272;&#35745;&#31867;&#21518;&#39564;&#20998;&#24067;&#21644;&#31867;&#26465;&#20214;&#25968;&#25454;&#20998;&#24067;&#65292;&#20855;&#26377;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#32622;&#20449;&#24230;&#26657;&#20934;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#26080;&#30417;&#30563;&#25968;&#25454;&#21644;&#21487;&#26657;&#20934;&#30340;&#32622;&#20449;&#24230;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#21028;&#21035;&#27169;&#22411;&#21017;&#22312;&#27169;&#22411;&#32467;&#26500;&#21644;&#23398;&#20064;&#31639;&#27861;&#30340;&#31616;&#21333;&#24615;&#20197;&#21450;&#22312;&#24615;&#33021;&#26041;&#38754;&#36229;&#36234;&#20854;&#29983;&#25104;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#20013;&#35757;&#32451;&#21028;&#21035;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23637;&#29616;&#20102;&#20004;&#31181;&#27169;&#22411;&#30340;&#29305;&#28857;&#12290;&#20851;&#38190;&#30340;&#24605;&#24819;&#26159;&#39640;&#26031;&#32806;&#21512;softmax&#23618;&#65292;&#35813;&#23618;&#26159;&#19968;&#20010;&#20840;&#36830;&#25509;&#23618;&#65292;&#20855;&#26377;softmax&#28608;&#27963;&#20989;&#25968;&#21644;&#39640;&#26031;&#20998;&#24067;&#32806;&#21512;&#12290;&#35813;&#23618;&#21487;&#20197;&#23884;&#20837;&#21040;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#22120;&#20013;&#65292;&#24182;&#20801;&#35768;&#20998;&#31867;&#22120;&#20272;&#35745;&#31867;&#21518;&#39564;&#20998;&#24067;&#21644;&#31867;&#26465;&#20214;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#32622;&#20449;&#24230;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models have advantageous characteristics for classification tasks such as the availability of unsupervised data and calibrated confidence, whereas discriminative models have advantages in terms of the simplicity of their model structures and learning algorithms and their ability to outperform their generative counterparts. In this paper, we propose a method to train a hybrid of discriminative and generative models in a single neural network (NN), which exhibits the characteristics of both models. The key idea is the Gaussian-coupled softmax layer, which is a fully connected layer with a softmax activation function coupled with Gaussian distributions. This layer can be embedded into an NN-based classifier and allows the classifier to estimate both the class posterior distribution and the class-conditional data distribution. We demonstrate that the proposed hybrid model can be applied to semi-supervised learning and confidence calibration.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21270;&#29983;&#25104;&#36741;&#21161;&#25932;&#23545;&#25915;&#20987;&#26469;&#25552;&#39640;&#35757;&#32451;&#31574;&#30053;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#27979;&#35797;&#26102;&#31574;&#30053;&#25200;&#21160;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;CMARL&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05909</link><description>&lt;p&gt;
&#22522;&#20110;&#28436;&#21270;&#36741;&#21161;&#25932;&#23545;&#25915;&#20987;&#30340;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
Robust multi-agent coordination via evolutionary generation of auxiliary adversarial attackers. (arXiv:2305.05909v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05909
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21270;&#29983;&#25104;&#36741;&#21161;&#25932;&#23545;&#25915;&#20987;&#26469;&#25552;&#39640;&#35757;&#32451;&#31574;&#30053;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#27979;&#35797;&#26102;&#31574;&#30053;&#25200;&#21160;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;CMARL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(CMARL)&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#21069;&#26223;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#35299;&#20915;&#38024;&#23545;MARL&#30340;&#25361;&#25112;&#65288;&#22914;&#38750;&#31283;&#24577;&#12289;&#20449;&#29992;&#20998;&#37197;&#12289;&#21487;&#25193;&#23637;&#24615;&#65289;&#26469;&#25552;&#39640;&#21327;&#35843;&#33021;&#21147;&#65292;&#20294;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#27979;&#35797;&#26102;&#24573;&#30053;&#20102;&#31574;&#30053;&#25200;&#21160;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#38382;&#39064;&#23450;&#20041;&#25110;&#31639;&#27861;&#35774;&#35745;&#26041;&#38754;&#36824;&#27809;&#26377;&#24471;&#21040;&#32771;&#34385;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#26377;&#38480;&#31574;&#30053;&#23545;&#25239;Dec-POMDP&#38382;&#39064;(LPA-Dec-POMDP)&#65292;&#20854;&#20013;&#19968;&#20123;&#26469;&#33258;&#22242;&#38431;&#30340;&#21327;&#35843;&#21592;&#21487;&#33021;&#20250;&#24847;&#22806;&#32780;&#19981;&#21487;&#39044;&#27979;&#22320;&#36973;&#36935;&#21040;&#26377;&#38480;&#25968;&#37327;&#30340;&#24694;&#24847;&#34892;&#20026;&#25915;&#20987;&#65292;&#20294;&#24120;&#35268;&#30340;&#21327;&#35843;&#21592;&#20173;&#28982;&#20250;&#20026;&#26082;&#23450;&#30446;&#26631;&#32780;&#21162;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#28436;&#21270;&#29983;&#25104;&#36741;&#21161;&#25932;&#23545;&#25915;&#20987;&#26469;&#23454;&#29616;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;(ROMANCE)&#30340;&#26041;&#27861;&#65292;&#36825;&#26679;&#35757;&#32451;&#30340;&#31574;&#30053;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36973;&#36935;&#22810;&#26679;&#21270;&#21644;&#24378;&#22823;&#30340;&#36741;&#21161;&#25932;&#23545;&#25915;&#20987;&#65292;&#20174;&#32780;&#22312;&#27979;&#35797;&#22330;&#26223;&#20013;&#23454;&#29616;&#39640;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ROMANCE&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;CMARL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cooperative multi-agent reinforcement learning (CMARL) has shown to be promising for many real-world applications. Previous works mainly focus on improving coordination ability via solving MARL-specific challenges (e.g., non-stationarity, credit assignment, scalability), but ignore the policy perturbation issue when testing in a different environment. This issue hasn't been considered in problem formulation or efficient algorithm design. To address this issue, we firstly model the problem as a limited policy adversary Dec-POMDP (LPA-Dec-POMDP), where some coordinators from a team might accidentally and unpredictably encounter a limited number of malicious action attacks, but the regular coordinators still strive for the intended goal. Then, we propose Robust Multi-Agent Coordination via Evolutionary Generation of Auxiliary Adversarial Attackers (ROMANCE), which enables the trained policy to encounter diversified and strong auxiliary adversarial attacks during training, thus achieving h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DPMLBench&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#32508;&#21512;&#34913;&#37327;&#21152;&#24378;DP-SGD&#30340;DPML&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#38450;&#24481;&#33021;&#21147;&#65292;&#22635;&#34917;&#20102;&#27604;&#36739;DPML&#31639;&#27861;&#25913;&#36827;&#34920;&#29616;&#30340;&#31354;&#30333;&#65292;&#25552;&#39640;&#20102;DPML&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05900</link><description>&lt;p&gt;
DPMLBench&#65306;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#30340;&#25972;&#20307;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DPMLBench: Holistic Evaluation of Differentially Private Machine Learning. (arXiv:2305.05900v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DPMLBench&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#32508;&#21512;&#34913;&#37327;&#21152;&#24378;DP-SGD&#30340;DPML&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#38450;&#24481;&#33021;&#21147;&#65292;&#22635;&#34917;&#20102;&#27604;&#36739;DPML&#31639;&#27861;&#25913;&#36827;&#34920;&#29616;&#30340;&#31354;&#30333;&#65292;&#25552;&#39640;&#20102;DPML&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20316;&#20026;&#19968;&#31181;&#20005;&#26684;&#30340;&#25968;&#23398;&#23450;&#20041;&#65292;&#37327;&#21270;&#20102;&#38544;&#31169;&#27844;&#38706;&#65292;&#24050;&#25104;&#20026;&#38544;&#31169;&#20445;&#25252;&#30340;&#19968;&#20010;&#24191;&#20026;&#25509;&#21463;&#30340;&#26631;&#20934;&#12290;&#32467;&#21512;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#65288;DPML&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#26368;&#32463;&#20856;&#30340;DPML&#31639;&#27861;&#20043;&#19968;&#65292;DP-SGD&#20250;&#36896;&#25104;&#26174;&#33879;&#30340;&#25928;&#29992;&#25439;&#22833;&#65292;&#36825;&#38459;&#30861;&#20102;DPML&#22312;&#23454;&#36341;&#20013;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#35768;&#22810;&#30740;&#31350;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;DP-SGD&#30340;&#25913;&#36827;&#31639;&#27861;&#65292;&#20294;&#26159;&#36825;&#20123;&#30740;&#31350;&#26159;&#23396;&#31435;&#30340;&#65292;&#26080;&#27861;&#20840;&#38754;&#34913;&#37327;&#31639;&#27861;&#20013;&#25552;&#20986;&#30340;&#25913;&#36827;&#30340;&#34920;&#29616;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36824;&#32570;&#20047;&#20840;&#38754;&#30740;&#31350;&#26469;&#27604;&#36739;&#36825;&#20123;DPML&#31639;&#27861;&#30340;&#25913;&#36827;&#22312;&#25928;&#29992;&#12289;&#38450;&#24481;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#23545;&#25913;&#36827;&#30340;DPML&#31639;&#27861;&#36827;&#34892;&#32508;&#21512;&#27979;&#37327;&#65292;&#23545;&#23454;&#29992;&#24615;&#21644;&#38450;&#24481;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#24314;&#35774;&#24615;&#21644;&#20840;&#38754;&#24615;&#30340;&#26694;&#26550;DPMLBench&#26469;&#35780;&#20272;DPML&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#24230;&#37327;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#19981;&#21516;&#30340;&#38544;&#31169;&#39044;&#31639;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19979;&#30340;&#23454;&#29992;&#24615;&#21644;&#38450;&#24481;&#33021;&#21147;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;DPMLBench&#26694;&#26550;&#20248;&#20110;&#29616;&#26377;&#26694;&#26550;&#65292;&#21516;&#26102;&#36824;&#26174;&#31034;&#20986;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;DPML&#31639;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential privacy (DP), as a rigorous mathematical definition quantifying privacy leakage, has become a well-accepted standard for privacy protection. Combined with powerful machine learning techniques, differentially private machine learning (DPML) is increasingly important. As the most classic DPML algorithm, DP-SGD incurs a significant loss of utility, which hinders DPML's deployment in practice. Many studies have recently proposed improved algorithms based on DP-SGD to mitigate utility loss. However, these studies are isolated and cannot comprehensively measure the performance of improvements proposed in algorithms. More importantly, there is a lack of comprehensive research to compare improvements in these DPML algorithms across utility, defensive capabilities, and generalizability.  We fill this gap by performing a holistic measurement of improved DPML algorithms on utility and defense capability against membership inference attacks (MIAs) on image classification tasks. We fir
&lt;/p&gt;</description></item><item><title>CUTS+&#26159;&#19968;&#31181;&#22522;&#20110;Granger&#22240;&#26524;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;(MPGNN)&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31895;&#21040;&#32454;&#21457;&#29616;&#65288;C2FD&#65289;&#25216;&#26415;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CUTS+&#22312;&#38750;&#35268;&#21017;&#39640;&#32500;&#25968;&#25454;&#19978;&#30340;&#22240;&#26524;&#21457;&#29616;&#24615;&#33021;&#22823;&#24133;&#24230;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.05890</link><description>&lt;p&gt;
CUTS+&#65306;&#22522;&#20110;&#38750;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#30340;&#39640;&#32500;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
CUTS+: High-dimensional Causal Discovery from Irregular Time-series. (arXiv:2305.05890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05890
&lt;/p&gt;
&lt;p&gt;
CUTS+&#26159;&#19968;&#31181;&#22522;&#20110;Granger&#22240;&#26524;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;(MPGNN)&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31895;&#21040;&#32454;&#21457;&#29616;&#65288;C2FD&#65289;&#25216;&#26415;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CUTS+&#22312;&#38750;&#35268;&#21017;&#39640;&#32500;&#25968;&#25454;&#19978;&#30340;&#22240;&#26524;&#21457;&#29616;&#24615;&#33021;&#22823;&#24133;&#24230;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#26159;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#19968;&#20010;&#26681;&#26412;&#24615;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#24773;&#22659;&#19979;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#21644;&#20915;&#31574;&#21046;&#23450;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#25104;&#21151;&#22320;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982; Granger &#22240;&#26524;&#30456;&#32467;&#21512;&#26469;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#65292;&#20294;&#26159;&#24403;&#36935;&#21040;&#39640;&#32500;&#25968;&#25454;&#26102;&#65292;&#30001;&#20110;&#39640;&#24230;&#20887;&#20313;&#30340;&#32593;&#32476;&#35774;&#35745;&#21644;&#24222;&#22823;&#30340;&#22240;&#26524;&#22270;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#20005;&#37325;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#35266;&#23519;&#20540;&#20013;&#30340;&#32570;&#22833;&#26465;&#30446;&#36827;&#19968;&#27493;&#38459;&#30861;&#20102;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986; CUTS+&#65292;&#23427;&#24314;&#31435;&#22312;&#22522;&#20110; Granger &#22240;&#26524;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861; CUTS &#19978;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#31216;&#20026;&#31895;&#21040;&#32454;&#21457;&#29616;&#65288;C2FD&#65289;&#30340;&#25216;&#26415;&#21644;&#21033;&#29992;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MPGNN&#65289;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;&#27169;&#25311;&#12289;&#20934;&#23454;&#38469;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; CUTS+ &#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#38750;&#35268;&#21017;&#39640;&#32500;&#25968;&#25454;&#19978;&#22823;&#22823;&#25552;&#39640;&#20102;&#22240;&#26524;&#21457;&#29616;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery in time-series is a fundamental problem in the machine learning community, enabling causal reasoning and decision-making in complex scenarios. Recently, researchers successfully discover causality by combining neural networks with Granger causality, but their performances degrade largely when encountering high-dimensional data because of the highly redundant network design and huge causal graphs. Moreover, the missing entries in the observations further hamper the causal structural learning. To overcome these limitations, We propose CUTS+, which is built on the Granger-causality-based causal discovery method CUTS and raises the scalability by introducing a technique called Coarse-to-fine-discovery (C2FD) and leveraging a message-passing-based graph neural network (MPGNN). Compared to previous methods on simulated, quasi-real, and real datasets, we show that CUTS+ largely improves the causal discovery performance on high-dimensional data with different types of irregula
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#24418;&#28040;&#27495;&#30340;&#28145;&#24230;&#37096;&#20998;&#22810;&#26631;&#31614;&#27169;&#22411;&#65288;PLAIN&#65289;&#65292;&#23427;&#21487;&#20197;&#20174;&#20505;&#36873;&#26631;&#31614;&#20013;&#24674;&#22797;&#26631;&#31614;&#32622;&#20449;&#24230;&#65292;&#24182;&#21033;&#29992;&#26631;&#31614;&#20381;&#36182;&#24615;&#65292;&#25552;&#39640;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05882</link><description>&lt;p&gt;
&#24102;&#26377;&#22270;&#24418;&#28040;&#27495;&#30340;&#28145;&#24230;&#37096;&#20998;&#22810;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Partial Multi-Label Learning with Graph Disambiguation. (arXiv:2305.05882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#24418;&#28040;&#27495;&#30340;&#28145;&#24230;&#37096;&#20998;&#22810;&#26631;&#31614;&#27169;&#22411;&#65288;PLAIN&#65289;&#65292;&#23427;&#21487;&#20197;&#20174;&#20505;&#36873;&#26631;&#31614;&#20013;&#24674;&#22797;&#26631;&#31614;&#32622;&#20449;&#24230;&#65292;&#24182;&#21033;&#29992;&#26631;&#31614;&#20381;&#36182;&#24615;&#65292;&#25552;&#39640;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#20998;&#22810;&#26631;&#31614;&#23398;&#20064;&#65288;PML&#65289;&#20013;&#65292;&#27599;&#20010;&#25968;&#25454;&#31034;&#20363;&#37117;&#37197;&#22791;&#26377;&#20505;&#36873;&#26631;&#31614;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#22810;&#20010;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#21644;&#20854;&#20182;&#20551;&#38451;&#24615;&#26631;&#31614;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;PML&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#33021;&#22815;&#20174;&#20505;&#36873;&#26631;&#31614;&#20013;&#20272;&#35745;&#20934;&#30830;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;PML&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#32447;&#24615;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#65292;&#22240;&#27492;&#26080;&#27861;&#36798;&#21040;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#28040;&#38500;&#20960;&#20010;&#38556;&#30861;&#65292;&#23558;&#23427;&#20204;&#25193;&#23637;&#20026;&#28145;&#24230;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24102;&#26377;&#22270;&#24418;- &#28040;&#27495;&#30340;&#28145;&#24230;&#37096;&#20998;&#22810;&#26631;&#31614;&#27169;&#22411;&#65288;PLAIN&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23454;&#20363;&#32423;&#21644;&#26631;&#31614;&#32423;&#30456;&#20284;&#24615;&#20197;&#24674;&#22797;&#26631;&#31614;&#32622;&#20449;&#24230;&#65292;&#24182;&#21033;&#29992;&#26631;&#31614;&#20381;&#36182;&#24615;&#12290;&#22312;&#27599;&#20010;&#35757;&#32451;&#26102;&#26399;&#65292;&#26631;&#31614;&#22312;&#23454;&#20363;&#21644;&#26631;&#31614;&#22270;&#19978;&#20256;&#25773;&#20197;&#20135;&#29983;&#30456;&#23545;&#20934;&#30830;&#30340;&#20266;&#26631;&#31614;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;&#28145;&#24230;&#27169;&#22411;&#20197;&#36866;&#24212;&#25968;&#20540;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
In partial multi-label learning (PML), each data example is equipped with a candidate label set, which consists of multiple ground-truth labels and other false-positive labels. Recently, graph-based methods, which demonstrate a good ability to estimate accurate confidence scores from candidate labels, have been prevalent to deal with PML problems. However, we observe that existing graph-based PML methods typically adopt linear multi-label classifiers and thus fail to achieve superior performance. In this work, we attempt to remove several obstacles for extending them to deep models and propose a novel deep Partial multi-Label model with grAph-disambIguatioN (PLAIN). Specifically, we introduce the instance-level and label-level similarities to recover label confidences as well as exploit label dependencies. At each training epoch, labels are propagated on the instance and label graphs to produce relatively accurate pseudo-labels; then, we train the deep model to fit the numerical labels
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#20013;&#22810;&#26679;&#24615;&#30340;&#27010;&#24565;&#21450;&#20854;&#23450;&#20041;&#65292;&#25552;&#20986;&#20102;&#29983;&#25104;&#22810;&#31181;&#22240;&#26524;&#20851;&#31995;&#20363;&#23376;&#20197;&#35299;&#37322;&#19968;&#20010;&#39044;&#27979;&#32467;&#26524;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05840</link><description>&lt;p&gt;
&#23454;&#29616;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#30340;&#22810;&#26679;&#24615;&#65306;&#19968;&#39033;&#32508;&#36848;&#21644;&#35752;&#35770;
&lt;/p&gt;
&lt;p&gt;
Achieving Diversity in Counterfactual Explanations: a Review and Discussion. (arXiv:2305.05840v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#20013;&#22810;&#26679;&#24615;&#30340;&#27010;&#24565;&#21450;&#20854;&#23450;&#20041;&#65292;&#25552;&#20986;&#20102;&#29983;&#25104;&#22810;&#31181;&#22240;&#26524;&#20851;&#31995;&#20363;&#23376;&#20197;&#35299;&#37322;&#19968;&#20010;&#39044;&#27979;&#32467;&#26524;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#22240;&#26524;&#20851;&#31995;&#20363;&#23376;&#36890;&#36807;&#25351;&#20986;&#26356;&#25913;&#23454;&#20363;&#20197;&#26356;&#25913;&#20854;&#39044;&#27979;&#30340;&#20462;&#25913;&#26469;&#35299;&#37322;&#21463;&#36807;&#35757;&#32451;&#30340;&#20915;&#31574;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#36825;&#20123;&#22240;&#26524;&#20851;&#31995;&#20363;&#23376;&#36890;&#24120;&#23450;&#20041;&#20026;&#35299;&#20915;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#20195;&#20215;&#20989;&#25968;&#32467;&#21512;&#20102;&#20960;&#20010;&#34913;&#37327;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#30340;&#33391;&#22909;&#35299;&#37322;&#30340;&#26631;&#20934;&#12290;&#21487;&#20197;&#32771;&#34385;&#22810;&#31181;&#36825;&#26679;&#36866;&#24403;&#30340;&#24615;&#36136;&#65292;&#22240;&#20026;&#29992;&#25143;&#38656;&#27714;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#32780;&#19988;&#19981;&#21516;&#29992;&#25143;&#20043;&#38388;&#20063;&#26377;&#25152;&#19981;&#21516;&#65307;&#23427;&#20204;&#30340;&#36873;&#25321;&#21644;&#35268;&#33539;&#21270;&#26159;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#20123;&#26041;&#27861;&#25552;&#20986;&#29983;&#25104;&#19968;&#32452;&#19981;&#21516;&#30340;&#22240;&#26524;&#20851;&#31995;&#20363;&#23376;&#26469;&#35299;&#37322;&#19968;&#20010;&#39044;&#27979;&#32467;&#26524;&#65292;&#32780;&#19981;&#26159;&#21333;&#20010;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#36825;&#20010;&#22810;&#26679;&#24615;&#27010;&#24565;&#30340;&#35768;&#22810;&#12289;&#26377;&#26102;&#30456;&#20114;&#30683;&#30462;&#30340;&#23450;&#20041;&#30340;&#32508;&#36848;&#12290;&#23427;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#22522;&#26412;&#21407;&#21017;&#12289;&#20551;&#35774;&#12289;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#20171;&#32461;&#20102;&#19968;&#20123;&#20851;&#20110;&#36825;&#20010;&#29305;&#23450;&#30740;&#31350;&#20027;&#39064;&#30340;&#26368;&#26032;&#24037;&#20316;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of Explainable Artificial Intelligence (XAI), counterfactual examples explain to a user the predictions of a trained decision model by indicating the modifications to be made to the instance so as to change its associated prediction. These counterfactual examples are generally defined as solutions to an optimization problem whose cost function combines several criteria that quantify desiderata for a good explanation meeting user needs. A large variety of such appropriate properties can be considered, as the user needs are generally unknown and differ from one user to another; their selection and formalization is difficult. To circumvent this issue, several approaches propose to generate, rather than a single one, a set of diverse counterfactual examples to explain a prediction. This paper proposes a review of the numerous, sometimes conflicting, definitions that have been proposed for this notion of diversity. It discusses their underlying principles as well as the hypothe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#32771;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#29983;&#25104;&#26694;&#26550;&#65292;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#32441;&#29702;&#29983;&#25104;&#22120;&#20135;&#29983;&#32441;&#29702;&#20174;&#32780;&#20445;&#25345;OCTA&#22270;&#20687;&#30340;&#20998;&#36776;&#29575;&#65292;&#35299;&#20915;&#20102;&#25195;&#25551;&#21306;&#22495;&#22686;&#22823;&#23548;&#33268;&#20998;&#36776;&#29575;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.05835</link><description>&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#30340;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#34880;&#31649;&#36896;&#24433;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#24102;&#26377;&#21487;&#23398;&#20064;&#30340;&#32441;&#29702;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reference-based OCT Angiogram Super-resolution with Learnable Texture Generation. (arXiv:2305.05835v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#32771;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#29983;&#25104;&#26694;&#26550;&#65292;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#32441;&#29702;&#29983;&#25104;&#22120;&#20135;&#29983;&#32441;&#29702;&#20174;&#32780;&#20445;&#25345;OCTA&#22270;&#20687;&#30340;&#20998;&#36776;&#29575;&#65292;&#35299;&#20915;&#20102;&#25195;&#25551;&#21306;&#22495;&#22686;&#22823;&#23548;&#33268;&#20998;&#36776;&#29575;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#34880;&#31649;&#36896;&#24433;&#65288;OCTA&#65289;&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#35270;&#32593;&#33180;&#24494;&#34880;&#31649;&#30340;&#26032;&#22411;&#25104;&#20687;&#25216;&#26415;&#65292;&#24182;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#20020;&#24202;&#12290;&#39640;&#20998;&#36776;&#29575;OCTA&#23545;&#20110;&#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#20934;&#30830;&#35782;&#21035;&#19981;&#21516;&#35270;&#32593;&#33180;&#30142;&#30149;&#30340;&#28508;&#22312;&#29983;&#29289;&#26631;&#24535;&#29289;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;OCTA&#30340;&#19968;&#20010;&#26174;&#33879;&#38382;&#39064;&#26159;&#22312;&#22266;&#23450;&#37319;&#38598;&#26102;&#38388;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#35270;&#37326;&#26102;&#65292;&#20998;&#36776;&#29575;&#19981;&#21487;&#36991;&#20813;&#22320;&#38477;&#20302;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#36229;&#20998;&#36776;&#29575;&#65288;RefSR&#65289;&#26694;&#26550;&#65292;&#20197;&#20445;&#25345;OCTA&#22270;&#20687;&#30340;&#20998;&#36776;&#29575;&#21516;&#26102;&#22686;&#21152;&#25195;&#25551;&#21306;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20351;&#29992;&#27491;&#24120;&#30340;RefSR&#27969;&#31243;&#20013;&#30340;&#32441;&#29702;&#26469;&#35757;&#32451;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#32441;&#29702;&#29983;&#25104;&#22120;&#65288;LTG&#65289;&#65292;&#35813;&#29983;&#25104;&#22120;&#34987;&#35774;&#35745;&#29992;&#20110;&#26681;&#25454;&#36755;&#20837;&#29983;&#25104;&#32441;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical coherence tomography angiography (OCTA) is a new imaging modality to visualize retinal microvasculature and has been readily adopted in clinics. High-resolution OCT angiograms are important to qualitatively and quantitatively identify potential biomarkers for different retinal diseases accurately. However, one significant problem of OCTA is the inevitable decrease in resolution when increasing the field-of-view given a fixed acquisition time. To address this issue, we propose a novel reference-based super-resolution (RefSR) framework to preserve the resolution of the OCT angiograms while increasing the scanning area. Specifically, textures from the normal RefSR pipeline are used to train a learnable texture generator (LTG), which is designed to generate textures according to the input. The key difference between the proposed method and traditional RefSR models is that the textures used during inference are generated by the LTG instead of being searched from a single reference i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#22240;&#26524;&#26426;&#21046;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20445;&#25345;&#19981;&#21464;&#30340;&#30452;&#35273;&#26469;&#20027;&#21160;&#20934;&#22791;&#30340;&#20195;&#29702;&#29305;&#24449;&#36873;&#25321;&#21644;&#24037;&#31243;&#25216;&#26415;&#65292;&#29992;&#20197;&#24212;&#23545;&#32479;&#35745;&#39044;&#27979;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.05832</link><description>&lt;p&gt;
&#22240;&#26524;&#20449;&#24687;&#20998;&#31163;&#65306;&#20026;&#25239;&#20998;&#24067;&#36716;&#31227;&#35774;&#35745;&#20195;&#29702;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Causal Information Splitting: Engineering Proxy Features for Robustness to Distribution Shifts. (arXiv:2305.05832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#22240;&#26524;&#26426;&#21046;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20445;&#25345;&#19981;&#21464;&#30340;&#30452;&#35273;&#26469;&#20027;&#21160;&#20934;&#22791;&#30340;&#20195;&#29702;&#29305;&#24449;&#36873;&#25321;&#21644;&#24037;&#31243;&#25216;&#26415;&#65292;&#29992;&#20197;&#24212;&#23545;&#32479;&#35745;&#39044;&#27979;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#19982;&#26368;&#32456;&#20351;&#29992;&#24773;&#20917;&#19981;&#21516;&#30340;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#20026;&#20102;&#39044;&#27979;&#20998;&#24067;&#36716;&#31227;&#65292;&#26377;&#19968;&#31181;&#26041;&#27861;&#26159;&#21033;&#29992;&#22240;&#26524;&#26426;&#21046;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20445;&#25345;&#19981;&#21464;&#30340;&#30452;&#35273;&#26469;&#20027;&#21160;&#20934;&#22791;&#12290;&#26412;&#25991;&#38024;&#23545;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#65292;&#20854;&#20013;&#30446;&#26631;&#30340;&#22240;&#26524;&#21644;&#21453;&#22240;&#26524;&#21464;&#37327;&#37117;&#26159;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#12290;&#21033;&#29992;&#20449;&#24687;&#35770;&#65292;&#25105;&#20204;&#20026;&#19979;&#28216;&#35266;&#27979;&#21464;&#37327;&#24320;&#21457;&#20102;&#29305;&#24449;&#36873;&#25321;&#21644;&#24037;&#31243;&#25216;&#26415;&#65292;&#36825;&#20123;&#21464;&#37327;&#20805;&#24403;&#20195;&#29702;&#12290;&#25105;&#20204;&#36873;&#25321;&#26377;&#21161;&#20110;&#24314;&#31435;&#31283;&#23450;&#27169;&#22411;&#30340;&#20195;&#29702;&#65292;&#24182;&#20351;&#29992;&#36741;&#21161;&#35757;&#32451;&#20219;&#21153;&#20174;&#20195;&#29702;&#20013;&#25552;&#21462;&#22686;&#24378;&#31283;&#23450;&#24615;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical prediction models are often trained on data that is drawn from different probability distributions than their eventual use cases. One approach to proactively prepare for these shifts harnesses the intuition that causal mechanisms should remain invariant between environments. Here we focus on a challenging setting in which the causal and anticausal variables of the target are unobserved. Leaning on information theory, we develop feature selection and engineering techniques for the observed downstream variables that act as proxies. We identify proxies that help to build stable models and moreover utilize auxiliary training tasks to extract stability-enhancing information from proxies. We demonstrate the effectiveness of our techniques on synthetic and real data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#27491;&#24577;&#26144;&#23556;&#31639;&#27861;&#29992;&#20110;&#38750;&#20984;&#22797;&#21512;&#22411;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20854;&#25910;&#25947;&#24615;&#36136;&#12290;&#35813;&#26041;&#27861;&#25193;&#23637;&#20102;&#22522;&#26412;Proximal&#38543;&#26426;&#26799;&#24230;&#27861;&#30340;&#26356;&#26377;&#38480;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.05828</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#24577;&#26144;&#23556;&#30340;Prox-SGD&#26041;&#27861;&#22312;KL&#19981;&#31561;&#24335;&#19979;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Convergence of a Normal Map-based Prox-SGD Method under the KL Inequality. (arXiv:2305.05828v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#27491;&#24577;&#26144;&#23556;&#31639;&#27861;&#29992;&#20110;&#38750;&#20984;&#22797;&#21512;&#22411;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20854;&#25910;&#25947;&#24615;&#36136;&#12290;&#35813;&#26041;&#27861;&#25193;&#23637;&#20102;&#22522;&#26412;Proximal&#38543;&#26426;&#26799;&#24230;&#27861;&#30340;&#26356;&#26377;&#38480;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#27491;&#24577;&#26144;&#23556;&#31639;&#27861;&#65288;$\mathsf{norM}\text{-}\mathsf{SGD}$&#65289;&#29992;&#20110;&#38750;&#20984;&#22797;&#21512;&#22411;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#25910;&#25947;&#24615;&#36136;&#12290;&#20351;&#29992;&#22522;&#20110;&#26102;&#38388;&#31383;&#21475;&#30340;&#31574;&#30053;&#65292;&#39318;&#20808;&#20998;&#26512;&#20102;$\mathsf{norM}\text{-}\mathsf{SGD}$&#30340;&#20840;&#23616;&#25910;&#25947;&#34892;&#20026;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#29983;&#25104;&#30340;&#36845;&#20195;&#24207;&#21015;$\{\boldsymbol{x}^k\}_k$&#30340;&#27599;&#20010;&#32047;&#31215;&#28857;&#20960;&#20046;&#30830;&#23450;&#22320;&#21644;&#26399;&#26395;&#19978;&#37117;&#23545;&#24212;&#20110;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#25152;&#24471;&#32467;&#26524;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#25104;&#31435;&#65292;&#24182;&#25193;&#23637;&#20102;&#22522;&#26412;Proximal&#38543;&#26426;&#26799;&#24230;&#27861;&#30340;&#26356;&#26377;&#38480;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#33879;&#21517;&#30340;Kurdyka-{\L}ojasiewicz&#65288;KL&#65289;&#20998;&#26512;&#26694;&#26550;&#65292;&#25105;&#20204;&#20026;&#36845;&#20195;&#24207;&#21015;$\{\boldsymbol{x}^k\}_k$&#25552;&#20379;&#20102;&#26032;&#30340;&#36880;&#28857;&#25910;&#25947;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#21462;&#20915;&#20110;&#22522;&#30784;KL&#25351;&#25968;$\boldsymbol{\theta}$&#21644;&#27493;&#38271;&#21160;&#24577;$\{\alpha_k\}_k$&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel stochastic normal map-based algorithm ($\mathsf{norM}\text{-}\mathsf{SGD}$) for nonconvex composite-type optimization problems and discuss its convergence properties. Using a time window-based strategy, we first analyze the global convergence behavior of $\mathsf{norM}\text{-}\mathsf{SGD}$ and it is shown that every accumulation point of the generated sequence of iterates $\{\boldsymbol{x}^k\}_k$ corresponds to a stationary point almost surely and in an expectation sense. The obtained results hold under standard assumptions and extend the more limited convergence guarantees of the basic proximal stochastic gradient method. In addition, based on the well-known Kurdyka-{\L}ojasiewicz (KL) analysis framework, we provide novel point-wise convergence results for the iterates $\{\boldsymbol{x}^k\}_k$ and derive convergence rates that depend on the underlying KL exponent $\boldsymbol{\theta}$ and the step size dynamics $\{\alpha_k\}_k$. Specifically, for the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36151;&#27454;&#31579;&#36873;&#20013;&#30340;&#34920;&#24449;&#20559;&#24046;&#65292;&#25552;&#39640;&#20102;&#36151;&#27454;&#31579;&#36873;&#25152;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#23545;&#20110;&#20302;&#31038;&#20250;&#32463;&#27982;&#32972;&#26223;&#20511;&#27454;&#20154;&#30340;&#34920;&#29616;&#26377;&#24456;&#22823;&#30340;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2305.05827</link><description>&lt;p&gt;
&#21253;&#23481;&#24615;FinTech&#36151;&#27454;&#65306;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inclusive FinTech Lending via Contrastive Learning and Domain Adaptation. (arXiv:2305.05827v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36151;&#27454;&#31579;&#36873;&#20013;&#30340;&#34920;&#24449;&#20559;&#24046;&#65292;&#25552;&#39640;&#20102;&#36151;&#27454;&#31579;&#36873;&#25152;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#23545;&#20110;&#20302;&#31038;&#20250;&#32463;&#27982;&#32972;&#26223;&#20511;&#27454;&#20154;&#30340;&#34920;&#29616;&#26377;&#24456;&#22823;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
FinTech&#36151;&#27454;&#22312;&#20419;&#36827;&#37329;&#34701;&#26222;&#24800;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#32553;&#30701;&#20102;&#22788;&#29702;&#26102;&#38388;&#21644;&#25104;&#26412;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#20307;&#39564;&#65292;&#24182;&#20351;&#24471;&#37027;&#20123;&#21487;&#33021;&#22312;&#20256;&#32479;&#38134;&#34892;&#36151;&#27454;&#20013;&#26080;&#27861;&#33719;&#24471;&#20449;&#29992;&#35748;&#35777;&#30340;&#20154;&#20204;&#21487;&#20197;&#33719;&#24471;&#36151;&#27454;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#36151;&#27454;&#31579;&#36873;&#20013;&#28508;&#22312;&#30340;&#31639;&#27861;&#20915;&#31574;&#20559;&#35265;&#25345;&#26377;&#25285;&#24551;&#12290;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35780;&#20272;&#20449;&#29992;&#36136;&#37327;&#26102;&#21487;&#33021;&#20250;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#24449;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#25105;&#20204;&#21482;&#33021;&#35775;&#38382;&#24050;&#33719;&#25209;&#20934;&#36151;&#27454;&#30003;&#35831;&#30340;&#36829;&#32422;&#32467;&#26524;&#26631;&#31614;&#65292;&#23545;&#20110;&#27492;&#31867;&#30003;&#35831;&#65292;&#20511;&#27454;&#20154;&#30340;&#31038;&#20250;&#32463;&#27982;&#29305;&#24449;&#20248;&#20110;&#34987;&#25298;&#32477;&#30340;&#36151;&#27454;&#30003;&#35831;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#21382;&#21490;&#19978;&#34987;&#25209;&#20934;&#30340;&#20154;&#32676;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#20302;&#31038;&#20250;&#32463;&#27982;&#32972;&#26223;&#30340;&#20511;&#27454;&#20154;&#20013;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#30495;&#23454;&#30340;FinTech&#36151;&#27454;&#24179;&#21488;&#36151;&#27454;&#31579;&#36873;&#20013;&#30340;&#34920;&#24449;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#26469;&#20943;&#23569;&#34920;&#24449;&#20559;&#24046;&#65292;&#25552;&#39640;&#36151;&#27454;&#31579;&#36873;&#25152;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#26469;&#33258;&#24050;&#25209;&#20934;&#21644;&#34987;&#25298;&#32477;&#30340;&#36151;&#27454;&#30003;&#35831;&#30340;&#26631;&#35760;&#25968;&#25454;&#28151;&#21512;&#35757;&#32451;&#23545;&#27604;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#23398;&#20064;&#21040;&#30340;&#34920;&#24449;&#36866;&#24212;&#20110;&#20855;&#26377;&#36739;&#20302;&#31038;&#20250;&#32463;&#27982;&#29305;&#24449;&#30340;&#36151;&#27454;&#30003;&#35831;&#20154;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20943;&#36731;&#34920;&#24449;&#20559;&#24046;&#65292;&#25552;&#39640;&#20302;&#31038;&#20250;&#32463;&#27982;&#32972;&#26223;&#20511;&#27454;&#20154;&#30340;&#36151;&#27454;&#31579;&#36873;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
FinTech lending (e.g., micro-lending) has played a significant role in facilitating financial inclusion. It has reduced processing times and costs, enhanced the user experience, and made it possible for people to obtain loans who may not have qualified for credit from traditional lenders. However, there are concerns about the potentially biased algorithmic decision-making during loan screening. Machine learning algorithms used to evaluate credit quality can be influenced by representation bias in the training data, as we only have access to the default outcome labels of approved loan applications, for which the borrowers' socioeconomic characteristics are better than those of rejected ones. In this case, the model trained on the labeled data performs well on the historically approved population, but does not generalize well to borrowers of low socioeconomic background. In this paper, we investigate the problem of representation bias in loan screening for a real-world FinTech lending pl
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#26368;&#20339;&#21162;&#21147;&#36866;&#24212;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24046;&#24322;&#30340;&#29702;&#35770;&#20998;&#26512;&#26041;&#27861;&#20197;&#21450;&#29992;&#20110;&#26631;&#20934;&#22495;&#36866;&#24212;&#24615;&#38382;&#39064;&#30340;&#25913;&#36827;&#23398;&#20064;&#31639;&#27861;&#65292;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.05816</link><description>&lt;p&gt;
&#26368;&#20339;&#21162;&#21147;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Best-Effort Adaptation. (arXiv:2305.05816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05816
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#26368;&#20339;&#21162;&#21147;&#36866;&#24212;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24046;&#24322;&#30340;&#29702;&#35770;&#20998;&#26512;&#26041;&#27861;&#20197;&#21450;&#29992;&#20110;&#26631;&#20934;&#22495;&#36866;&#24212;&#24615;&#38382;&#39064;&#30340;&#25913;&#36827;&#23398;&#20064;&#31639;&#27861;&#65292;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#30001;&#22810;&#20010;&#24212;&#29992;&#21644;&#32771;&#34385;&#22240;&#32032;&#28608;&#21457;&#20986;&#30340;&#26368;&#20339;&#21162;&#21147;&#36866;&#24212;&#24615;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#30830;&#23450;&#19968;&#20010;&#31934;&#30830;&#30340;&#39044;&#27979;&#22120;&#20197;&#29992;&#20110;&#30446;&#26631;&#22495;&#65292;&#34429;&#28982;&#21482;&#26377;&#36866;&#37327;&#30340;&#24050;&#26631;&#35760;&#26679;&#26412;&#21487;&#29992;&#65292;&#20294;&#21033;&#29992;&#26469;&#33258;&#21478;&#19968;&#20010;&#25317;&#26377;&#22823;&#37327;&#24050;&#26631;&#35760;&#26679;&#26412;&#30340;&#22495;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21644;&#36890;&#29992;&#30340;&#22522;&#20110;&#24046;&#24322;&#30340;&#29702;&#35770;&#20998;&#26512;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#65292;&#21253;&#25324;&#22312;&#26435;&#37325;&#19978;&#22343;&#21248;&#20445;&#25345;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#36793;&#30028;&#22914;&#20309;&#25351;&#23548;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#35774;&#35745;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#23398;&#20064;&#20445;&#35777;&#21644;&#31639;&#27861;&#20026;&#26631;&#20934;&#22495;&#36866;&#24212;&#24615;&#38382;&#39064;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#30446;&#26631;&#22495;&#21482;&#26377;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#25110;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#30340;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26368;&#20339;&#21162;&#21147;&#36866;&#24212;&#24615;&#21644;&#22495;&#36866;&#24212;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#19982;&#20960;&#20010;&#22522;&#32447;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a problem of best-effort adaptation motivated by several applications and considerations, which consists of determining an accurate predictor for a target domain, for which a moderate amount of labeled samples are available, while leveraging information from another domain for which substantially more labeled samples are at one's disposal. We present a new and general discrepancy-based theoretical analysis of sample reweighting methods, including bounds holding uniformly over the weights. We show how these bounds can guide the design of learning algorithms that we discuss in detail. We further show that our learning guarantees and algorithms provide improved solutions for standard domain adaptation problems, for which few labeled data or none are available from the target domain. We finally report the results of a series of experiments demonstrating the effectiveness of our best-effort adaptation and domain adaptation algorithms, as well as comparisons with several baselines. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;&#36965;&#24863;&#24433;&#20687;&#21464;&#21270;&#26816;&#27979;&#26041;&#38754;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#20171;&#32461;&#20102;&#38382;&#39064;&#23450;&#20041;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#21464;&#21387;&#22120;&#31561;&#21021;&#27493;&#30693;&#35782;&#65292;&#28145;&#24230;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#34987;&#24191;&#27867;&#36816;&#29992;&#20110;&#35299;&#20915;&#26816;&#27979;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.05813</link><description>&lt;p&gt;
&#36965;&#24863;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;&#22312;&#19978;&#20010;&#21313;&#24180;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Change Detection Methods for Remote Sensing in the Last Decade: A Comprehensive Review. (arXiv:2305.05813v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;&#36965;&#24863;&#24433;&#20687;&#21464;&#21270;&#26816;&#27979;&#26041;&#38754;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#20171;&#32461;&#20102;&#38382;&#39064;&#23450;&#20041;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#21464;&#21387;&#22120;&#31561;&#21021;&#27493;&#30693;&#35782;&#65292;&#28145;&#24230;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#34987;&#24191;&#27867;&#36816;&#29992;&#20110;&#35299;&#20915;&#26816;&#27979;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21270;&#26816;&#27979;&#26159;&#36965;&#24863;&#20013;&#19968;&#39033;&#22522;&#26412;&#32780;&#24191;&#27867;&#24212;&#29992;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#26816;&#27979;&#21644;&#20998;&#26512;&#21516;&#19968;&#22320;&#29702;&#21306;&#22495;&#38543;&#26102;&#38388;&#21457;&#29983;&#30340;&#21464;&#21270;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#22478;&#24066;&#21457;&#23637;&#12289;&#20892;&#19994;&#35843;&#26597;&#21644;&#22303;&#22320;&#35206;&#30422;&#30417;&#27979;&#24212;&#29992;&#12290;&#30001;&#20110;&#21508;&#31181;&#22240;&#32032;&#65292;&#21253;&#25324;&#22270;&#20687;&#36136;&#37327;&#21464;&#21270;&#12289;&#22122;&#22768;&#12289;&#27880;&#20876;&#35823;&#24046;&#12289;&#20809;&#29031;&#21464;&#21270;&#12289;&#22797;&#26434;&#26223;&#35266;&#21644;&#31354;&#38388;&#24322;&#36136;&#24615;&#65292;&#26816;&#27979;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#24212;&#36816;&#32780;&#29983;&#65292;&#24182;&#19988;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#30340;&#28789;&#27963;&#24615;&#65292;&#22240;&#27492;&#34987;&#24191;&#27867;&#29992;&#20110;&#35768;&#22810;&#22270;&#20687;&#22788;&#29702;&#20219;&#21153;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;&#36965;&#24863;&#24433;&#20687;&#21464;&#21270;&#26816;&#27979;&#26041;&#38754;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#30340;&#19968;&#20123;&#21021;&#27493;&#30693;&#35782;&#65292;&#22914;&#38382;&#39064;&#23450;&#20041;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#21464;&#21387;&#22120;&#22522;&#30784;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Change detection is an essential and widely utilized task in remote sensing that aims to detect and analyze changes occurring in the same geographical area over time, which has broad applications in urban development, agricultural surveys, and land cover monitoring. Detecting changes in remote sensing images is a complex challenge due to various factors, including variations in image quality, noise, registration errors, illumination changes, complex landscapes, and spatial heterogeneity. In recent years, deep learning has emerged as a powerful tool for feature extraction and addressing these challenges. Its versatility has resulted in its widespread adoption for numerous image-processing tasks. This paper presents a comprehensive survey of significant advancements in change detection for remote sensing images over the past decade. We first introduce some preliminary knowledge for the change detection task, such as problem definition, datasets, evaluation metrics, and transformer basics
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#26680;&#30005;&#31449;&#29123;&#26009;&#20248;&#21270;&#38382;&#39064;&#65292;&#33021;&#22815;&#25552;&#39640;&#26680;&#30005;&#31449;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2305.05812</link><description>&lt;p&gt;
&#26680;&#30005;&#31449;&#29123;&#26009;&#20248;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Assessment of Reinforcement Learning Algorithms for Nuclear Power Plant Fuel Optimization. (arXiv:2305.05812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#26680;&#30005;&#31449;&#29123;&#26009;&#20248;&#21270;&#38382;&#39064;&#65292;&#33021;&#22815;&#25552;&#39640;&#26680;&#30005;&#31449;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21830;&#19994;&#26680;&#33021;&#20135;&#19994;&#35806;&#29983;&#20197;&#26469;&#65292;&#20154;&#20204;&#19968;&#30452;&#22312;&#30740;&#31350;&#26680;&#29123;&#26009;&#35013;&#36733;&#27169;&#24335;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#23427;&#20855;&#26377;&#22810;&#20010;&#30446;&#26631;&#21644;&#32422;&#26463;&#26465;&#20214;&#65292;&#20505;&#36873;&#27169;&#24335;&#25968;&#37327;&#38750;&#24120;&#39640;&#65292;&#22240;&#27492;&#26080;&#27861;&#26126;&#30830;&#35299;&#20915;&#12290;&#19981;&#21516;&#30340;&#26680;&#33021;&#20844;&#29992;&#20107;&#19994;&#21644;&#20379;&#24212;&#21830;&#20351;&#29992;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#26469;&#25191;&#34892;&#29123;&#26009;&#24490;&#29615;&#37325;&#35013;&#35774;&#35745;&#65292;&#20294;&#25163;&#21160;&#35774;&#35745;&#30340;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#26159;&#20027;&#27969;&#12290;&#20026;&#20102;&#25913;&#36827;&#29616;&#26377;&#29366;&#24577;&#30340;&#29123;&#26009;&#24490;&#29615;&#37325;&#35013;&#27169;&#24335;&#65292;&#25105;&#20204;&#26088;&#22312;&#21019;&#24314;&#19968;&#31181;&#23613;&#21487;&#33021;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#31526;&#21512;&#35774;&#35745;&#24072;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#30446;&#26631;&#12290;&#20026;&#20102;&#24110;&#21161;&#23436;&#25104;&#27492;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#29305;&#21035;&#26159;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#12290;&#26368;&#36817;&#65292;RL&#22312;&#28216;&#25103;&#20013;&#30340;&#25104;&#21151;&#32473;&#23427;&#24102;&#26469;&#20102;&#24378;&#21170;&#30340;&#25512;&#21160;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#22522;&#30784;&#65292;&#24182;&#25552;&#35758;&#30740;&#31350;&#24433;&#21709;RL&#31639;&#27861;&#24615;&#33021;&#30340;&#20960;&#20010;&#36229;&#21442;&#25968;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
The nuclear fuel loading pattern optimization problem has been studied since the dawn of the commercial nuclear energy industry. It is characterized by multiple objectives and constraints, with a very high number of candidate patterns, which makes it impossible to solve explicitly. Stochastic optimization methodologies are used by different nuclear utilities and vendors to perform fuel cycle reload design. Nevertheless, hand-designed solutions continue to be the prevalent method in the industry. To improve the state-of-the-art core reload patterns, we aim to create a method as scalable as possible, that agrees with the designer's goal of performance and safety. To help in this task Deep Reinforcement Learning (RL), in particular, Proximal Policy Optimization is leveraged. RL has recently experienced a strong impetus from its successes applied to games. This paper lays out the foundation of this method and proposes to study the behavior of several hyper-parameters that influence the RL 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#32852;&#24819;&#35745;&#31639;&#27169;&#22411;&#65292;&#20854;&#20013;&#35745;&#31639;&#19982;&#35760;&#24518;&#19981;&#21487;&#20998;&#65292;&#36890;&#36807;&#20174;&#36755;&#20837;&#21521;&#37327;&#21040;&#26368;&#36817;&#37051;&#38170;&#28857;&#30340;&#25910;&#25947;&#23436;&#25104;&#35745;&#31639;&#12290;&#25991;&#31456;&#20851;&#27880;&#20110;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#34920;&#31034;&#24067;&#23572;&#20989;&#25968;&#30340;&#20449;&#24687;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.05808</link><description>&lt;p&gt;
&#20851;&#20110;&#26368;&#36817;&#37051;&#34920;&#31034;&#30340;&#20449;&#24687;&#23481;&#37327;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Information Capacity of Nearest Neighbor Representations. (arXiv:2305.05808v1 [cs.CC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#32852;&#24819;&#35745;&#31639;&#27169;&#22411;&#65292;&#20854;&#20013;&#35745;&#31639;&#19982;&#35760;&#24518;&#19981;&#21487;&#20998;&#65292;&#36890;&#36807;&#20174;&#36755;&#20837;&#21521;&#37327;&#21040;&#26368;&#36817;&#37051;&#38170;&#28857;&#30340;&#25910;&#25947;&#23436;&#25104;&#35745;&#31639;&#12290;&#25991;&#31456;&#20851;&#27880;&#20110;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#34920;&#31034;&#24067;&#23572;&#20989;&#25968;&#30340;&#20449;&#24687;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#21644;&#20154;&#31867;&#22823;&#33041;&#22312;&#35745;&#31639;&#21644;&#23384;&#20648;&#26041;&#38754;&#26377;&#30528;&#19981;&#21516;&#30340;&#26550;&#26500;&#12290;&#21463;&#21040;&#22823;&#33041;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#24819;&#35745;&#31639;&#27169;&#22411;&#65292;&#20854;&#20013;&#20869;&#23384;&#30001;&#19968;&#32452;&#22312;$\mathbb{R}^n$&#31354;&#38388;&#20013;&#30340;&#21521;&#37327;&#65288;&#31216;&#20026;&#8220;&#38170;&#28857;&#8221;&#65289;&#23450;&#20041;&#65292;&#35745;&#31639;&#26159;&#36890;&#36807;&#20174;&#36755;&#20837;&#21521;&#37327;&#21040;&#26368;&#36817;&#37051;&#38170;&#28857;&#30340;&#25910;&#25947;&#23436;&#25104;&#30340;&#65292;&#36755;&#20986;&#26159;&#19982;&#19968;&#20010;&#38170;&#28857;&#30456;&#20851;&#32852;&#30340;&#26631;&#31614;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#24067;&#23572;&#20989;&#25968;&#22312;&#32852;&#24819;&#35745;&#31639;&#27169;&#22411;&#20013;&#30340;&#34920;&#31034;&#65292;&#20854;&#20013;&#36755;&#20837;&#20026;&#20108;&#36827;&#21046;&#21521;&#37327;&#65292;&#30456;&#24212;&#30340;&#36755;&#20986;&#20026;&#26368;&#36817;&#37051;&#38170;&#28857;&#30340;&#19968;&#20010;&#26631;&#31614;&#65288;$0$&#25110;$1$&#65289;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#24067;&#23572;&#20989;&#25968;&#30340;&#20449;&#24687;&#23481;&#37327;&#19982;&#20004;&#20010;&#25968;&#37327;&#30456;&#20851;&#32852;&#65306;(i) &#38170;&#28857;&#30340;&#25968;&#37327;&#65288;&#31216;&#20026;&#8220;&#26368;&#36817;&#37051;&#22797;&#26434;&#24230;&#8221;&#65289;&#21644;(ii)
&lt;/p&gt;
&lt;p&gt;
The $\textit{von Neumann Computer Architecture}$ has a distinction between computation and memory. In contrast, the brain has an integrated architecture where computation and memory are indistinguishable. Motivated by the architecture of the brain, we propose a model of $\textit{associative computation}$ where memory is defined by a set of vectors in $\mathbb{R}^n$ (that we call $\textit{anchors}$), computation is performed by convergence from an input vector to a nearest neighbor anchor, and the output is a label associated with an anchor. Specifically, in this paper, we study the representation of Boolean functions in the associative computation model, where the inputs are binary vectors and the corresponding outputs are the labels ($0$ or $1$) of the nearest neighbor anchors. The information capacity of a Boolean function in this model is associated with two quantities: $\textit{(i)}$ the number of anchors (called $\textit{Nearest Neighbor (NN) Complexity}$) and $\textit{(ii)}$ the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#24067;&#21464;&#21270;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#21327;&#35758;&#26469;&#20998;&#26512;&#22810;&#26679;&#24615;&#21464;&#21270;&#21644;&#30456;&#20851;&#24615;&#21464;&#21270;&#12290;&#20351;&#29992;&#30382;&#32932;&#30284;&#20998;&#26512;&#20998;&#31867;&#38382;&#39064;&#30340;&#23454;&#20363;&#65292;&#21457;&#29616;&#27169;&#22411;&#19981;&#20165;&#20250;&#23398;&#20064;&#21644;&#20256;&#25773;&#30456;&#20851;&#24615;&#21464;&#21270;&#65292;&#32780;&#19988;&#21487;&#33021;&#20250;&#20351;&#29992;&#38169;&#35823;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.05807</link><description>&lt;p&gt;
&#21363;&#20351;&#24456;&#23567;&#30340;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#21464;&#21270;&#20063;&#20250;&#23548;&#33268;&#25968;&#25454;&#38598;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Even Small Correlation and Diversity Shifts Pose Dataset-Bias Issues. (arXiv:2305.05807v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#24067;&#21464;&#21270;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#21327;&#35758;&#26469;&#20998;&#26512;&#22810;&#26679;&#24615;&#21464;&#21270;&#21644;&#30456;&#20851;&#24615;&#21464;&#21270;&#12290;&#20351;&#29992;&#30382;&#32932;&#30284;&#20998;&#26512;&#20998;&#31867;&#38382;&#39064;&#30340;&#23454;&#20363;&#65292;&#21457;&#29616;&#27169;&#22411;&#19981;&#20165;&#20250;&#23398;&#20064;&#21644;&#20256;&#25773;&#30456;&#20851;&#24615;&#21464;&#21270;&#65292;&#32780;&#19988;&#21487;&#33021;&#20250;&#20351;&#29992;&#38169;&#35823;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#21464;&#21270;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#24456;&#24120;&#35265;&#65292;&#20250;&#24433;&#21709;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#65306;&#22810;&#26679;&#24615;&#21464;&#21270;&#21644;&#30456;&#20851;&#24615;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#21327;&#35758;&#65292;&#20351;&#29992;&#21516;&#26102;&#23384;&#22312;&#36825;&#20004;&#31181;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#26469;&#20998;&#26512;&#23427;&#20204;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#20010;&#30495;&#23454;&#30340;&#30382;&#32932;&#30284;&#20998;&#26512;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;&#20102;&#36229;&#20986;&#25968;&#25454;&#38598;&#21644;&#19987;&#38376;&#30340;&#20559;&#24046;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#21327;&#35758;&#25581;&#31034;&#20102;&#19977;&#20010;&#21457;&#29616;&#65306;1&#65289;&#27169;&#22411;&#21363;&#20351;&#36827;&#34892;&#20102;&#20302;&#20559;&#24046;&#35757;&#32451;&#20063;&#20250;&#23398;&#20064;&#24182;&#20256;&#25773;&#30456;&#20851;&#24615;&#21464;&#21270;&#65292;&#36825;&#21487;&#33021;&#20250;&#32047;&#31215;&#21644;&#32467;&#21512;&#38590;&#20197;&#35299;&#37322;&#30340;&#24369;&#20559;&#24046;&#30340;&#39118;&#38505;&#65307;2&#65289;&#27169;&#22411;&#22312;&#39640;&#12289;&#20302;&#20559;&#24046;&#24773;&#20917;&#19979;&#21487;&#20197;&#23398;&#20064;&#21040;&#31283;&#20581;&#30340;&#29305;&#24449;&#65292;&#20294;&#26159;&#22914;&#26524;&#27979;&#35797;&#26679;&#26412;&#26377;&#38169;&#35823;&#30340;&#29305;&#24449;&#23427;&#20204;&#21487;&#33021;&#20250;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distribution shifts are common in real-world datasets and can affect the performance and reliability of deep learning models. In this paper, we study two types of distribution shifts: diversity shifts, which occur when test samples exhibit patterns unseen during training, and correlation shifts, which occur when test data present a different correlation between seen invariant and spurious features. We propose an integrated protocol to analyze both types of shifts using datasets where they co-exist in a controllable manner. Finally, we apply our approach to a real-world classification problem of skin cancer analysis, using out-of-distribution datasets and specialized bias annotations. Our protocol reveals three findings: 1) Models learn and propagate correlation shifts even with low-bias training; this poses a risk of accumulating and combining unaccountable weak biases; 2) Models learn robust features in highand low-bias scenarios but use spurious ones if test samples have them; this
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;Segment Anything Model (SAM)&#22686;&#24378;Class Activation Maps (CAM)&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;CAM&#30340;&#23616;&#37096;&#28608;&#27963;&#21644;&#34394;&#20551;&#28608;&#27963;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.05803</link><description>&lt;p&gt;
&#22522;&#20110;Segment Anything Model (SAM)&#22686;&#24378;&#20266;&#26631;&#31614;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation. (arXiv:2305.05803v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;Segment Anything Model (SAM)&#22686;&#24378;Class Activation Maps (CAM)&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;CAM&#30340;&#23616;&#37096;&#28608;&#27963;&#21644;&#34394;&#20551;&#28608;&#27963;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#22270;&#20687;&#32423;&#21035;&#30340;&#30417;&#30563;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;(WSSS)&#30001;&#20110;&#20854;&#19982;&#20687;&#32032;&#32423;&#27880;&#37322;&#30456;&#27604;&#30340;&#20302;&#25104;&#26412;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#31867;&#28608;&#27963;&#22270;(CAM)&#29983;&#25104;&#20687;&#32032;&#32423;&#30340;&#20266;&#26631;&#31614;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;CAM&#32463;&#24120;&#36973;&#21463;&#23616;&#37096;&#28608;&#27963;&#30340;&#38480;&#21046;-&#21482;&#28608;&#27963;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#37096;&#20998;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;&#23545;&#35937;&#21306;&#22495;&#21644;&#34394;&#20551;&#30340;&#28608;&#27963;-&#19981;&#24517;&#35201;&#22320;&#28608;&#27963;&#29289;&#20307;&#21608;&#22260;&#30340;&#32972;&#26223;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#21363;&#21033;&#29992;&#26368;&#36817;&#21457;&#24067;&#30340;Segment Anything Model (SAM)&#22686;&#24378;CAM&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#12290;SAM&#26159;&#19968;&#20010;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#23558;&#22270;&#20687;&#20998;&#21106;&#25104;&#27573;&#33853;&#30340;&#24378;&#38646;-shot&#33021;&#21147;&#65292;&#20294;&#32570;&#20047;&#36825;&#20123;&#21306;&#22495;&#30340;&#35821;&#20041;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#29305;&#23450;&#31867;&#21035;&#30340;&#20266;&#26631;&#31614;&#20316;&#20026;&#20449;&#21495;&#26469;&#36873;&#25321;m&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Semantic Segmentation (WSSS) with only image-level supervision has garnered increasing attention due to its low annotation cost compared to pixel-level annotation. Most existing methods rely on Class Activation Maps (CAM) to generate pixel-level pseudo labels for supervised training. However, it is well known that CAM often suffers from partial activation -- activating the most discriminative part instead of the entire object area, and false activation -- unnecessarily activating the background around the object. In this study, we introduce a simple yet effective approach to address these limitations by harnessing the recently released Segment Anything Model (SAM) to generate higher-quality pseudo labels with CAM. SAM is a segmentation foundation model that demonstrates strong zero-shot ability in partitioning images into segments but lacks semantic labels for these regions. To circumvent this, we employ pseudo labels for a specific class as the signal to select the m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20648;&#22791;&#35745;&#31639;&#26426;&#30340;&#22810;&#21151;&#33021;&#24615;&#33021;&#21147;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#21560;&#24341;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#22914;&#20309;&#24433;&#21709;&#20854;&#23454;&#29616;&#22810;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35201;&#23454;&#29616;&#22810;&#21151;&#33021;&#24615;&#65292;&#38656;&#35201;RC&#20869;&#37096;&#32593;&#32476;c&#30340;&#35889;&#21322;&#24452;&#21512;&#36866;&#36873;&#25321;&#30340;&#20020;&#30028;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05799</link><description>&lt;p&gt;
&#22810;&#21151;&#33021;&#20648;&#22791;&#35745;&#31639;&#26426;&#30340;&#21452;&#37325;&#35270;&#35273;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Seeing double with a multifunctional reservoir computer. (arXiv:2305.05799v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20648;&#22791;&#35745;&#31639;&#26426;&#30340;&#22810;&#21151;&#33021;&#24615;&#33021;&#21147;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#21560;&#24341;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#22914;&#20309;&#24433;&#21709;&#20854;&#23454;&#29616;&#22810;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35201;&#23454;&#29616;&#22810;&#21151;&#33021;&#24615;&#65292;&#38656;&#35201;RC&#20869;&#37096;&#32593;&#32476;c&#30340;&#35889;&#21322;&#24452;&#21512;&#36866;&#36873;&#25321;&#30340;&#20020;&#30028;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21151;&#33021;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#22810;&#37325;&#31283;&#23450;&#24615;&#20197;&#25191;&#34892;&#22810;&#31181;&#20219;&#21153;&#32780;&#19981;&#25913;&#21464;&#20219;&#20309;&#32593;&#32476;&#23646;&#24615;&#12290;&#20351;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#33719;&#24471;&#26576;&#20123;&#22810;&#31283;&#23450;&#24615;&#20197;&#25191;&#34892;&#22810;&#20010;&#20219;&#21153;&#65292;&#20854;&#20013;&#27599;&#20010;&#20219;&#21153;&#19982;&#32593;&#32476;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#29305;&#23450;&#21560;&#24341;&#23376;&#30456;&#20851;&#32852;&#65292;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#33258;&#28982;&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#12290;&#22240;&#20026;&#19982;&#22810;&#31283;&#23450;&#24615;&#26377;&#20851;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#19981;&#21516;&#21560;&#24341;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#22914;&#20309;&#24433;&#21709;&#20648;&#22791;&#35745;&#31639;&#26426;&#65288;RC&#65289;&#30340;&#22810;&#21151;&#33021;&#24615;&#33021;&#21147;&#65292;&#20648;&#22791;&#35745;&#31639;&#26426;&#26159;&#19968;&#31181;&#20197;ANN&#24418;&#24335;&#21576;&#29616;&#30340;&#21160;&#24577;&#31995;&#32479;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#8220;&#21452;&#37325;&#35270;&#35273;&#25928;&#24212;&#8221;&#38382;&#39064;&#26469;&#31995;&#32479;&#22320;&#30740;&#31350;&#24403;&#20004;&#20010;&#21560;&#24341;&#23376;&#20043;&#38388;&#23384;&#22312;&#37325;&#21472;&#26102;RC&#22914;&#20309;&#37325;&#26500;&#21560;&#24341;&#23376;&#30340;&#20849;&#23384;&#12290;&#38543;&#30528;&#37325;&#21472;&#37327;&#30340;&#22686;&#21152;&#65292;&#25105;&#20204;&#21457;&#29616;&#35201;&#23454;&#29616;&#22810;&#21151;&#33021;&#24615;&#65292;&#38656;&#35201;RC&#20869;&#37096;&#32593;&#32476;c&#30340;&#35889;&#21322;&#24452;&#21512;&#36866;&#36873;&#25321;&#30340;&#20020;&#30028;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multifunctional biological neural networks exploit multistability in order to perform multiple tasks without changing any network properties. Enabling artificial neural networks (ANNs) to obtain certain multistabilities in order to perform several tasks, where each task is related to a particular attractor in the network's state space, naturally has many benefits from a machine learning perspective. Given the association to multistability, in this paper we explore how the relationship between different attractors influences the ability of a reservoir computer (RC), which is a dynamical system in the form of an ANN, to achieve multifunctionality. We construct the `seeing double' problem to systematically study how a RC reconstructs a coexistence of attractors when there is an overlap between them. As the amount of overlap increases, we discover that for multifunctionality to occur, there is a critical dependence on a suitable choice of the spectral radius for the RC's internal network c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#30340;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#23450;&#20041;&#21644;&#26816;&#27979;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.05792</link><description>&lt;p&gt;
&#36807;&#25311;&#21512;&#30340;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Testing for Overfitting. (arXiv:2305.05792v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#30340;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#23450;&#20041;&#21644;&#26816;&#27979;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#39640;&#22797;&#26434;&#24230;&#30340;&#27169;&#22411;&#24120;&#35265;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#20195;&#34920;&#25968;&#25454;&#65292;&#20294;&#26080;&#27861;&#25512;&#24191;&#21040;&#22522;&#30784;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12290;&#35299;&#20915;&#36807;&#25311;&#21512;&#30340;&#20856;&#22411;&#26041;&#27861;&#26159;&#22312;&#30041;&#32622;&#38598;&#19978;&#35745;&#31639;&#32463;&#39564;&#39118;&#38505;&#65292;&#19968;&#26086;&#39118;&#38505;&#24320;&#22987;&#22686;&#21152;&#65292;&#23601;&#20572;&#27490;&#65288;&#25110;&#26631;&#35760;&#20309;&#26102;&#20572;&#27490;&#65289;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#36755;&#20986;&#20102;&#33391;&#22909;&#27867;&#21270;&#30340;&#27169;&#22411;&#65292;&#20294;&#20854;&#23454;&#29616;&#21407;&#29702;&#20027;&#35201;&#26159;&#21551;&#21457;&#24335;&#30340;&#12290;&#26412;&#25991;&#35752;&#35770;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#26631;&#20934;&#28176;&#36817;&#21644;&#27987;&#24230;&#32467;&#26524;&#19981;&#25104;&#31435;&#12290;&#25105;&#20204;&#38543;&#21518;&#25552;&#20986;&#24182;&#38416;&#36848;&#20102;&#19968;&#20010;&#20551;&#35774;&#26816;&#39564;&#65292;&#36890;&#36807;&#35813;&#26816;&#39564;&#21487;&#20197;&#23545;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#37327;&#21270;&#22320;&#23450;&#20041;&#21644;&#26816;&#27979;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#20381;&#38752;&#30830;&#20445;&#32463;&#39564;&#22343;&#20540;&#24212;&#35813;&#39640;&#27010;&#29575;&#22320;&#36817;&#20284;&#20854;&#30495;&#23454;&#22343;&#20540;&#30340;&#27987;&#24230;&#30028;&#38480;&#65292;&#20197;&#24471;&#20986;&#20182;&#20204;&#24212;&#35813;&#30456;&#20114;&#25509;&#36817;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
High complexity models are notorious in machine learning for overfitting, a phenomenon in which models well represent data but fail to generalize an underlying data generating process. A typical procedure for circumventing overfitting computes empirical risk on a holdout set and halts once (or flags that/when) it begins to increase. Such practice often helps in outputting a well-generalizing model, but justification for why it works is primarily heuristic.  We discuss the overfitting problem and explain why standard asymptotic and concentration results do not hold for evaluation with training data. We then proceed to introduce and argue for a hypothesis test by means of which both model performance may be evaluated using training data, and overfitting quantitatively defined and detected. We rely on said concentration bounds which guarantee that empirical means should, with high probability, approximate their true mean to conclude that they should approximate each other. We stipulate co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35821;&#38899;&#36716;&#25442;&#20026;Mel&#39057;&#35889;&#22270;&#24182;&#20351;&#29992;&#22270;&#20687;&#20462;&#22797;&#25216;&#26415;&#36827;&#34892;&#38388;&#38548;&#24674;&#22797;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#23454;&#26102;&#30340;&#38899;&#39057;&#38388;&#38548;&#37325;&#24314;&#65292;&#22635;&#20805;&#21518;&#30340;&#36136;&#37327;&#19982;&#21407;&#22987;&#25968;&#25454;&#30456;&#36817;&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#35821;&#38899;&#20449;&#21495;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.05780</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22686;&#24378;&#26377;&#22122;&#38899;&#30340;&#35821;&#38899;&#20449;&#21495;&#30340;&#38388;&#38548;
&lt;/p&gt;
&lt;p&gt;
Enhancing Gappy Speech Audio Signals with Generative Adversarial Networks. (arXiv:2305.05780v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35821;&#38899;&#36716;&#25442;&#20026;Mel&#39057;&#35889;&#22270;&#24182;&#20351;&#29992;&#22270;&#20687;&#20462;&#22797;&#25216;&#26415;&#36827;&#34892;&#38388;&#38548;&#24674;&#22797;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#23454;&#26102;&#30340;&#38899;&#39057;&#38388;&#38548;&#37325;&#24314;&#65292;&#22635;&#20805;&#21518;&#30340;&#36136;&#37327;&#19982;&#21407;&#22987;&#25968;&#25454;&#30456;&#36817;&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#35821;&#38899;&#20449;&#21495;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#20013;&#30340;&#22122;&#22768;&#12289;&#28431;&#25253;&#21644;&#29255;&#27573;&#20002;&#22833;&#26159;&#24120;&#35265;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#35821;&#38899;&#35782;&#21035;&#31561;&#24212;&#29992;&#20013;&#24433;&#21709;&#24456;&#22823;&#12290;&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22686;&#24378;320ms&#20869;&#30340;&#35821;&#38899;&#20449;&#21495;&#20013;&#30340;&#38388;&#38548;&#12290;&#36890;&#36807;&#23558;&#38899;&#39057;&#36716;&#25442;&#20026;Mel-&#39057;&#35889;&#22270;&#24182;&#20351;&#29992;&#22270;&#20687;&#20462;&#22797;&#25216;&#26415;&#36827;&#34892;&#38388;&#38548;&#24674;&#22797;&#12290;&#26368;&#32456;&#23558;&#23436;&#25972;&#30340;Mel-&#39057;&#35889;&#22270;&#36716;&#25442;&#22238;&#38899;&#39057;&#65292;&#24182;&#20351;&#29992;Parallel-WaveGAN&#22768;&#30721;&#22120;&#38598;&#25104;&#21040;&#38899;&#39057;&#27969;&#20013;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#20855;&#26377;GPU&#30340;GAN&#31995;&#32479;&#23454;&#29616;&#25509;&#36817;&#23454;&#26102;&#30340;&#38899;&#39057;&#38388;&#38548;&#37325;&#24314;&#12290;&#19982;&#39044;&#26399;&#30456;&#31526;&#65292;&#38899;&#39057;&#20013;&#38388;&#38548;&#36234;&#23567;&#65292;&#22635;&#20805;&#21518;&#30340;&#36136;&#37327;&#23601;&#36234;&#22909;&#12290;&#22312;240ms&#30340;&#38388;&#38548;&#19978;&#65292;&#26368;&#20339;&#30340;&#27169;&#22411;&#30340;&#24179;&#22343;&#24847;&#35265;&#20998;&#25968;&#65288;MOS&#65289;&#20026;3.737&#65292;&#36825;&#36275;&#20197;&#34987;&#20154;&#31867;&#24863;&#30693;&#20026;&#22909;&#30340;&#38899;&#39057;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaps, dropouts and short clips of corrupted audio are a common problem and particularly annoying when they occur in speech. This paper uses machine learning to regenerate gaps of up to 320ms in an audio speech signal. Audio regeneration is translated into image regeneration by transforming audio into a Mel-spectrogram and using image in-painting to regenerate the gaps. The full Mel-spectrogram is then transferred back to audio using the Parallel-WaveGAN vocoder and integrated into the audio stream. Using a sample of 1300 spoken audio clips of between 1 and 10 seconds taken from the publicly-available LJSpeech dataset our results show regeneration of audio gaps in close to real time using GANs with a GPU equipped system. As expected, the smaller the gap in the audio, the better the quality of the filled gaps. On a gap of 240ms the average mean opinion score (MOS) for the best performing models was 3.737, on a scale of 1 (worst) to 5 (best) which is sufficient for a human to perceive as 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph2Par&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#21483;&#20570;Augmented-AST&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;&#34920;&#31034;&#27861;&#26469;&#26816;&#27979;&#24182;&#34892;&#21270;&#20195;&#30721;&#21306;&#22495;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#21487;&#24182;&#34892;&#21270;&#24490;&#29615;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.05779</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#24322;&#26500;AST&#34920;&#31034;&#27861;&#23398;&#20064;&#20351;&#29992;OpenMP&#24182;&#34892;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning to Parallelize with OpenMP by Augmented Heterogeneous AST Representation. (arXiv:2305.05779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05779
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph2Par&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#21483;&#20570;Augmented-AST&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;&#34920;&#31034;&#27861;&#26469;&#26816;&#27979;&#24182;&#34892;&#21270;&#20195;&#30721;&#21306;&#22495;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#21487;&#24182;&#34892;&#21270;&#24490;&#29615;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#21487;&#24182;&#34892;&#21270;&#30340;&#20195;&#30721;&#21306;&#22495;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21363;&#20351;&#23545;&#20110;&#32463;&#39564;&#20016;&#23500;&#30340;&#24320;&#21457;&#20154;&#21592;&#20063;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;Graph2Par&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;Augmented-AST&#30340;&#24322;&#26500;&#22686;&#24378;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;&#34920;&#31034;&#26469;&#34920;&#31034;&#20195;&#30721;&#12290;&#35813;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#20351;&#29992;OpenMP&#36827;&#34892;&#24490;&#29615;&#32423;&#24182;&#34892;&#21270;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;18598&#20010;&#21487;&#24182;&#34892;&#21270;&#21644;13972&#20010;&#19981;&#21487;&#24182;&#34892;&#21270;&#24490;&#29615;&#30340;OMP\_Serial&#25968;&#25454;&#38598;&#65292;&#23558;&#20195;&#30721;&#20998;&#26512;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#36827;&#21046;&#20998;&#31867;&#38382;&#39064;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26816;&#27979;&#21487;&#24182;&#34892;&#21270;&#24490;&#29615;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#21462;&#24471;&#20102;&#39640;&#36798;88.74%&#30340;F1&#24471;&#20998;&#21644;81.94%&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting parallelizable code regions is a challenging task, even for experienced developers. Numerous recent studies have explored the use of machine learning for code analysis and program synthesis, including parallelization, in light of the success of machine learning in natural language processing. However, applying machine learning techniques to parallelism detection presents several challenges, such as the lack of an adequate dataset for training, an effective code representation with rich information, and a suitable machine learning model to learn the latent features of code for diverse analyses. To address these challenges, we propose a novel graph-based learning approach called Graph2Par that utilizes a heterogeneous augmented abstract syntax tree (Augmented-AST) representation for code. The proposed approach primarily focused on loop-level parallelization with OpenMP. Moreover, we create an OMP\_Serial dataset with 18598 parallelizable and 13972 non-parallelizable loops to tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#22810;&#30446;&#26631;&#28145;&#24230;&#21435;&#22122;&#27969;&#31243;&#65292;&#20351;&#29992;&#26469;&#33258;&#39640;&#36136;&#37327;&#20256;&#24863;&#22120;&#30340;&#28145;&#24230;&#22270;&#36827;&#34892;&#30417;&#30563;&#65292;&#21435;&#22122;&#20302;&#36136;&#37327;&#28145;&#24230;&#25668;&#20687;&#26426;&#30340;&#28145;&#24230;&#22270;&#12290;</title><link>http://arxiv.org/abs/2305.05778</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#33258;&#30417;&#30563;&#28145;&#24230;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Multi-Object Self-Supervised Depth Denoising. (arXiv:2305.05778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#22810;&#30446;&#26631;&#28145;&#24230;&#21435;&#22122;&#27969;&#31243;&#65292;&#20351;&#29992;&#26469;&#33258;&#39640;&#36136;&#37327;&#20256;&#24863;&#22120;&#30340;&#28145;&#24230;&#22270;&#36827;&#34892;&#30417;&#30563;&#65292;&#21435;&#22122;&#20302;&#36136;&#37327;&#28145;&#24230;&#25668;&#20687;&#26426;&#30340;&#28145;&#24230;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#25668;&#20687;&#26426;&#32463;&#24120;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340;&#35270;&#35273;&#20282;&#26381;&#65292;&#20294;&#26159;&#23567;&#22411;&#32039;&#20945;&#30340;&#28145;&#24230;&#25668;&#20687;&#26426;&#30340;&#36136;&#37327;&#36890;&#24120;&#19981;&#36275;&#20197;&#36827;&#34892;&#28145;&#24230;&#37325;&#24314;&#65292;&#36825;&#23545;&#20110;&#26426;&#22120;&#20154;&#24037;&#20316;&#31354;&#38388;&#30340;&#31934;&#30830;&#36319;&#36394;&#21644;&#24863;&#30693;&#26159;&#24517;&#39035;&#30340;&#12290;&#22312;Shabanov&#31561;&#20154;&#65288;2021&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#22810;&#30446;&#26631;&#28145;&#24230;&#21435;&#22122;&#27969;&#31243;&#65292;&#20351;&#29992;&#26469;&#33258;&#39640;&#36136;&#37327;&#20256;&#24863;&#22120;&#30340;&#28145;&#24230;&#22270;&#20316;&#20026;&#36817;&#20284;&#20110;&#23454;&#38469;&#30417;&#30563;&#20449;&#21495;&#65292;&#23545;&#26469;&#33258;&#20302;&#36136;&#37327;&#20256;&#24863;&#22120;&#30340;&#28145;&#24230;&#22270;&#36827;&#34892;&#21435;&#22122;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#65292;&#22312;&#31354;&#38388;&#20013;&#23545;&#20004;&#20010;&#24103;&#23545;&#36827;&#34892;&#23545;&#40784;&#24182;&#26816;&#32034;&#22522;&#20110;&#24103;&#30340;&#22810;&#30446;&#26631;&#25513;&#30721;&#65292;&#20197;&#33719;&#21462;&#19968;&#20010;&#24178;&#20928;&#30340;&#24102;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#21435;&#22122;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#24037;&#20316;&#30340;&#23454;&#29616;&#21487;&#20197;&#22312;https://github.com/alr-internship/self-supervised-depth-denoising&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depth cameras are frequently used in robotic manipulation, e.g. for visual servoing. The quality of small and compact depth cameras is though often not sufficient for depth reconstruction, which is required for precise tracking in and perception of the robot's working space. Based on the work of Shabanov et al. (2021), in this work, we present a self-supervised multi-object depth denoising pipeline, that uses depth maps of higher-quality sensors as close-to-ground-truth supervisory signals to denoise depth maps coming from a lower-quality sensor. We display a computationally efficient way to align sets of two frame pairs in space and retrieve a frame-based multi-object mask, in order to receive a clean labeled dataset to train a denoising neural network on. The implementation of our presented work can be found at https://github.com/alr-internship/self-supervised-depth-denoising.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;DeepTextMark&#65292;&#21487;&#29992;&#20110;&#26816;&#27979;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#35813;&#25216;&#26415;&#23454;&#29616;&#20102;&#30450;&#30446;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#38544;&#34109;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#22312;&#27700;&#21360;&#26816;&#27979;&#31934;&#24230;&#21644;&#25269;&#25239;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05773</link><description>&lt;p&gt;
DeepTextMark&#65306;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#29992;&#20110;&#26816;&#27979;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
DeepTextMark: Deep Learning based Text Watermarking for Detection of Large Language Model Generated Text. (arXiv:2305.05773v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;DeepTextMark&#65292;&#21487;&#29992;&#20110;&#26816;&#27979;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#35813;&#25216;&#26415;&#23454;&#29616;&#20102;&#30450;&#30446;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#38544;&#34109;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#22312;&#27700;&#21360;&#26816;&#27979;&#31934;&#24230;&#21644;&#25269;&#25239;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#25991;&#26412;&#29983;&#25104;&#22120;&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;&#20026;&#20102;&#38450;&#27490;&#28508;&#22312;&#30340;&#28389;&#29992;&#65292;&#26816;&#27979;&#25991;&#26412;&#26159;&#21542;&#30001;LLM&#29983;&#25104;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19968;&#20123;&#30456;&#20851;&#30340;&#24037;&#20316;&#35797;&#22270;&#20351;&#29992;&#23558;&#36755;&#20837;&#25991;&#26412;&#20998;&#31867;&#20026;&#20154;&#31867;&#32534;&#20889;&#30340;&#25110;LLM&#29983;&#25104;&#30340;&#20108;&#20803;&#20998;&#31867;&#22120;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20998;&#31867;&#22120;&#24050;&#34987;&#35777;&#26126;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#30001;&#20110;&#20998;&#31867;&#32467;&#26524;&#21487;&#33021;&#23545;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#20915;&#31574;&#20135;&#29983;&#24433;&#21709;&#65292;&#25991;&#26412;&#28304;&#30340;&#26816;&#27979;&#38656;&#35201;&#20855;&#26377;&#39640;&#36136;&#37327;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DeepTextMark&#65292;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#26412;&#27700;&#21360;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#28304;&#26816;&#27979;&#12290;DeepTextMark&#36890;&#36807;&#24212;&#29992;Word2Vec&#21644;&#21477;&#23376;&#32534;&#30721;&#36827;&#34892;&#27700;&#21360;&#25554;&#20837;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#20998;&#31867;&#22120;&#36827;&#34892;&#27700;&#21360;&#26816;&#27979;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#30450;&#30446;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#38544;&#34109;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#27491;&#22914;&#26412;&#25991;&#25152;&#36827;&#19968;&#27493;&#35752;&#35770;&#30340;&#37027;&#26679;&#65292;&#36825;&#20123;&#29305;&#24615;&#23545;&#20110;&#36890;&#29992;&#25991;&#26412;&#28304;&#26816;&#27979;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#65292;&#24182;&#19988;DeepTextMark&#22312;&#27700;&#21360;&#26816;&#27979;&#31934;&#24230;&#21644;&#25269;&#25239;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capabilities of text generators have grown with the rapid development of Large Language Models (LLM). To prevent potential misuse, the ability to detect whether texts are produced by LLM has become increasingly important. Several related works have attempted to solve this problem using binary classifiers that categorize input text as human-written or LLM-generated. However, these classifiers have been shown to be unreliable. As impactful decisions could be made based on the result of the classification, the text source detection needs to be high-quality. To this end, this paper presents DeepTextMark, a deep learning-based text watermarking method for text source detection. Applying Word2Vec and Sentence Encoding for watermark insertion and a transformer-based classifier for watermark detection, DeepTextMark achieves blindness, robustness, imperceptibility, and reliability simultaneously. As discussed further in the paper, these traits are indispensable for generic text source detec
&lt;/p&gt;</description></item><item><title>DifFIQA&#26159;&#19968;&#20010;&#26032;&#30340;&#20154;&#33080;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#26469;&#36827;&#34892;&#38754;&#37096;&#22270;&#20687;&#25200;&#21160;&#65292;&#24182;&#36890;&#36807;&#37327;&#21270;&#36825;&#20123;&#25200;&#21160;&#23545;&#24212;&#30340;&#22270;&#20687;&#23884;&#20837;&#30340;&#24433;&#21709;&#26469;&#36827;&#34892;&#36136;&#37327;&#35780;&#20272;&#65292;&#20197;&#25552;&#20379;&#21487;&#38752;&#30340;&#26679;&#26412;&#36136;&#37327;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.05768</link><description>&lt;p&gt;
DifFIQA: &#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#20154;&#33080;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DifFIQA: Face Image Quality Assessment Using Denoising Diffusion Probabilistic Models. (arXiv:2305.05768v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05768
&lt;/p&gt;
&lt;p&gt;
DifFIQA&#26159;&#19968;&#20010;&#26032;&#30340;&#20154;&#33080;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#26469;&#36827;&#34892;&#38754;&#37096;&#22270;&#20687;&#25200;&#21160;&#65292;&#24182;&#36890;&#36807;&#37327;&#21270;&#36825;&#20123;&#25200;&#21160;&#23545;&#24212;&#30340;&#22270;&#20687;&#23884;&#20837;&#30340;&#24433;&#21709;&#26469;&#36827;&#34892;&#36136;&#37327;&#35780;&#20272;&#65292;&#20197;&#25552;&#20379;&#21487;&#38752;&#30340;&#26679;&#26412;&#36136;&#37327;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20154;&#33080;&#35782;&#21035;&#65288;FR&#65289;&#27169;&#22411;&#22312;&#21463;&#25511;&#24773;&#22659;&#19979;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#37096;&#32626;&#21040;&#26410;&#21463;&#25511;&#21046;&#30340;&#65288;&#29616;&#23454;&#19990;&#30028;&#30340;&#65289;&#29615;&#22659;&#20013;&#26102;&#65292;&#24448;&#24448;&#30001;&#20110;&#20851;&#20110;&#25429;&#33719;&#38754;&#37096;&#25968;&#25454;&#36136;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#32780;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20154;&#33080;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#65288;FIQA&#65289;&#25216;&#26415;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#26679;&#26412;&#36136;&#37327;&#39044;&#27979;&#26469;&#32531;&#35299;&#36825;&#20123;&#24615;&#33021;&#38477;&#32423;&#65292;&#36825;&#20123;&#39044;&#27979;&#21487;&#29992;&#20110;&#25298;&#32477;&#20302;&#36136;&#37327;&#26679;&#26412;&#24182;&#20943;&#23569;&#35823;&#21305;&#37197;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#19981;&#26029;&#21462;&#24471;&#25913;&#36827;&#65292;&#20294;&#30830;&#20445;&#22312;&#20855;&#26377;&#22810;&#26679;&#21270;&#29305;&#24449;&#30340;&#38754;&#37096;&#22270;&#20687;&#20013;&#21487;&#38752;&#22320;&#20272;&#35745;&#36136;&#37327;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DifFIQA&#30340;&#26032;&#22411;FIQA&#26041;&#27861;&#65292;&#23427;&#20381;&#36182;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#24182;&#30830;&#20445;&#39640;&#24230;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#21033;&#29992;DDPM&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#36807;&#31243;&#26469;&#25200;&#21160;&#38754;&#37096;&#22270;&#20687;&#65292;&#24182;&#37327;&#21270;&#36825;&#20123;&#25200;&#21160;&#23545;&#24212;&#30340;&#22270;&#20687;&#23884;&#20837;&#30340;&#24433;&#21709;&#20197;&#36827;&#34892;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern face recognition (FR) models excel in constrained scenarios, but often suffer from decreased performance when deployed in unconstrained (real-world) environments due to uncertainties surrounding the quality of the captured facial data. Face image quality assessment (FIQA) techniques aim to mitigate these performance degradations by providing FR models with sample-quality predictions that can be used to reject low-quality samples and reduce false match errors. However, despite steady improvements, ensuring reliable quality estimates across facial images with diverse characteristics remains challenging. In this paper, we present a powerful new FIQA approach, named DifFIQA, which relies on denoising diffusion probabilistic models (DDPM) and ensures highly competitive results. The main idea behind the approach is to utilize the forward and backward processes of DDPMs to perturb facial images and quantify the impact of these perturbations on the corresponding image embeddings for qua
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#22522;&#20110;&#21608;&#26399;&#26102;&#38388;&#35774;&#32622;&#36229;&#21442;&#25968;&#65292;&#20351;&#24471;PPO&#21644;SAC&#22312;&#24191;&#27867;&#30340;&#24490;&#29615;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#23398;&#20064;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#25509;&#36817;&#32791;&#26102;&#30340;&#22312;&#32447;&#36229;&#21442;&#25968;&#35843;&#25972;&#33719;&#24471;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05760</link><description>&lt;p&gt;
&#38477;&#20302;&#29616;&#23454;&#31574;&#30053;&#20248;&#21270;&#20013;&#24490;&#29615;&#26102;&#38388;&#35843;&#25972;&#30340;&#20195;&#20215;
&lt;/p&gt;
&lt;p&gt;
Reducing the Cost of Cycle-Time Tuning for Real-World Policy Optimization. (arXiv:2305.05760v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#22522;&#20110;&#21608;&#26399;&#26102;&#38388;&#35774;&#32622;&#36229;&#21442;&#25968;&#65292;&#20351;&#24471;PPO&#21644;SAC&#22312;&#24191;&#27867;&#30340;&#24490;&#29615;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#23398;&#20064;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#25509;&#36817;&#32791;&#26102;&#30340;&#22312;&#32447;&#36229;&#21442;&#25968;&#35843;&#25972;&#33719;&#24471;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#36890;&#24120;&#20351;&#29992;&#22266;&#23450;&#21608;&#26399;&#26102;&#38388;&#30340;&#31163;&#25955;&#27493;&#39588;&#36827;&#34892;&#25805;&#20316;&#12290;&#23454;&#36341;&#20013;&#38656;&#35201;&#20026;&#32473;&#23450;&#20219;&#21153;&#36873;&#25321;&#25805;&#20316;&#21608;&#26399;&#26102;&#38388;&#65292;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#23398;&#20064;&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#26159;&#21542;&#38656;&#35201;&#20026;&#27599;&#20010;&#21608;&#26399;&#26102;&#38388;&#37325;&#26032;&#35843;&#25972;&#65292;&#36825;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#26469;&#35828;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;--PPO&#21644;SAC--&#22312;&#19981;&#21516;&#30340;&#21608;&#26399;&#26102;&#38388;&#19979;&#20351;&#29992;&#22522;&#32447;&#36229;&#21442;&#25968;&#20540;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#32447;&#36229;&#21442;&#25968;&#22312;&#19968;&#20010;&#22522;&#20934;&#20219;&#21153;&#20013;&#23637;&#31034;&#36825;&#20004;&#31181;&#31639;&#27861;&#34920;&#29616;&#33391;&#22909;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#36873;&#25321;&#19981;&#21516;&#20110;&#20219;&#21153;&#40664;&#35748;&#20540;&#30340;&#21608;&#26399;&#26102;&#38388;&#26102;&#65292;&#20351;&#29992;&#22522;&#32447;&#36229;&#21442;&#25968;&#30340;PPO&#26080;&#27861;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#24403;&#36229;&#21442;&#25968;&#29992;&#20110;&#27599;&#20010;&#21608;&#26399;&#26102;&#38388;&#26102;&#65292;&#22522;&#20110;&#22522;&#32447;&#30340;PPO&#21644;SAC&#34920;&#29616;&#22343;&#26126;&#26174;&#21155;&#20110;&#23427;&#20204;&#30340;&#35843;&#25972;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21608;&#26399;&#26102;&#38388;&#35774;&#32622;&#36825;&#20123;&#36229;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#36816;&#21160;&#20219;&#21153;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;PPO&#21644;SAC&#22312;&#26497;&#20854;&#24191;&#27867;&#30340;&#21608;&#26399;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#23398;&#20064;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#32791;&#26102;&#30340;&#22312;&#32447;&#36229;&#21442;&#25968;&#35843;&#25972;&#33719;&#24471;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continuous-time reinforcement learning tasks commonly use discrete steps of fixed cycle times for actions. As practitioners need to choose the action-cycle time for a given task, a significant concern is whether the hyper-parameters of the learning algorithm need to be re-tuned for each choice of the cycle time, which is prohibitive for real-world robotics. In this work, we investigate the widely-used baseline hyper-parameter values of two policy gradient algorithms -- PPO and SAC -- across different cycle times. Using a benchmark task where the baseline hyper-parameters of both algorithms were shown to work well, we reveal that when a cycle time different than the task default is chosen, PPO with baseline hyper-parameters fails to learn. Moreover, both PPO and SAC with their baseline hyper-parameters perform substantially worse than their tuned values for each cycle time. We propose novel approaches for setting these hyper-parameters based on the cycle time. In our experiments on simu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25240;&#25187;&#32047;&#31215;&#22686;&#30410;&#65288;DCG&#65289;&#25490;&#24207;&#24182;&#21152;&#26435;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#20302;&#20195;&#34920;&#24615;&#32452;&#30340;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05759</link><description>&lt;p&gt;
&#25490;&#21517;&#21644;&#37325;&#26032;&#21152;&#26435;&#25552;&#39640;&#20102;&#32452;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Ranking &amp; Reweighting Improves Group Distributional Robustness. (arXiv:2305.05759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25240;&#25187;&#32047;&#31215;&#22686;&#30410;&#65288;DCG&#65289;&#25490;&#24207;&#24182;&#21152;&#26435;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#20302;&#20195;&#34920;&#24615;&#32452;&#30340;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#36827;&#34892;&#26631;&#20934;&#35757;&#32451;&#21487;&#33021;&#20250;&#20135;&#29983;&#22312;&#24179;&#22343;&#31934;&#24230;&#19978;&#34920;&#29616;&#20986;&#33394;&#20294;&#22312;&#20302;&#20195;&#34920;&#24615;&#32452;&#19978;&#20934;&#30830;&#24615;&#36739;&#20302;&#30340;&#27169;&#22411;&#65292;&#36825;&#26159;&#30001;&#20110;&#34920;&#24449;&#20013;&#34394;&#20551;&#29305;&#24449;&#30340;&#26222;&#36941;&#23384;&#22312;&#25152;&#33268;&#12290;&#35299;&#20915;&#36825;&#20010;&#32452;&#40065;&#26834;&#24615;&#38382;&#39064;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#26368;&#23567;&#21270;&#26368;&#22351;&#30340;&#32452;&#35823;&#24046;&#65288;&#31867;&#20284;&#20110;&#26497;&#23567;&#20540;&#31574;&#30053;&#65289;&#65292;&#24076;&#26395;&#23427;&#20250;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#26159;&#27425;&#20248;&#30340;&#65292;&#23588;&#20854;&#26159;&#24403;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#32452;&#26102;&#12290;&#26412;&#25991;&#21463;&#20449;&#24687;&#26816;&#32034;&#21644;Learning-to-Rank&#25991;&#29486;&#30340;&#21551;&#21457;&#65292;&#39318;&#20808;&#25552;&#20986;&#20351;&#29992;&#25240;&#25187;&#32047;&#31215;&#22686;&#30410;&#65288;DCG&#65289;&#20316;&#20026;&#27169;&#22411;&#36136;&#37327;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#20419;&#36827;&#26356;&#22909;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#27169;&#22411;&#36873;&#25321;&#12290;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#25490;&#24207;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;DCG&#21152;&#26435;&#22810;&#20010;&#24615;&#33021;&#36739;&#24046;&#30340;&#32452;&#65288;&#32780;&#19981;&#20165;&#20165;&#26159;&#32771;&#34385;&#24615;&#33021;&#26368;&#24046;&#30340;&#32452;&#65289;&#12290;&#20316;&#20026;&#33258;&#28982;&#30340;&#19979;&#19968;&#27493;&#65292;&#25105;&#20204;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#40723;&#21169;&#27169;&#22411;&#38598;&#20013;&#20110;&#20302;&#20195;&#34920;&#24615;&#30340;&#32452;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#32452;&#19981;&#24179;&#34913;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that standard training via empirical risk minimization (ERM) can produce models that achieve high accuracy on average but low accuracy on underrepresented groups due to the prevalence of spurious features. A predominant approach to tackle this group robustness problem minimizes the worst group error (akin to a minimax strategy) on the training data, hoping it will generalize well on the testing data. However, this is often suboptimal, especially when the out-of-distribution (OOD) test data contains previously unseen groups. Inspired by ideas from the information retrieval and learning-to-rank literature, this paper first proposes to use Discounted Cumulative Gain (DCG) as a metric of model quality for facilitating better hyperparameter tuning and model selection. Being a ranking-based metric, DCG weights multiple poorly-performing groups (instead of considering just the group with the worst performance). As a natural next step, we build on our results to propose a
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35299;&#20915;&#20102;&#22312;&#21327;&#20316;&#24314;&#31569;&#20219;&#21153;&#20013;&#22914;&#20309;&#35299;&#20915;&#27169;&#26865;&#20004;&#21487;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#20309;&#26102;&#23547;&#27714;&#28548;&#28165;&#20197;&#21450;&#24212;&#35813;&#35810;&#38382;&#20160;&#20040;&#28548;&#28165;&#38382;&#39064;&#36825;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#30340;&#35299;&#31572;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05754</link><description>&lt;p&gt;
&#36890;&#36807;&#19990;&#30028;&#29366;&#24577;&#21644;&#25991;&#26412;&#25351;&#20196;&#25552;&#38382;&#30340;&#26102;&#38388;&#21644;&#38382;&#39064;&#65306;IGLU NLP&#25361;&#25112;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
When and What to Ask Through World States and Text Instructions: IGLU NLP Challenge Solution. (arXiv:2305.05754v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05754
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35299;&#20915;&#20102;&#22312;&#21327;&#20316;&#24314;&#31569;&#20219;&#21153;&#20013;&#22914;&#20309;&#35299;&#20915;&#27169;&#26865;&#20004;&#21487;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#20309;&#26102;&#23547;&#27714;&#28548;&#28165;&#20197;&#21450;&#24212;&#35813;&#35810;&#38382;&#20160;&#20040;&#28548;&#28165;&#38382;&#39064;&#36825;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#30340;&#35299;&#31572;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#65292;&#26377;&#25928;&#30340;&#20132;&#27969;&#23545;&#20110;&#23454;&#29616;&#20849;&#21516;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#20010;&#20219;&#21153;&#26159;&#21327;&#20316;&#24314;&#31569;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#24314;&#31569;&#32773;&#24517;&#39035;&#30456;&#20114;&#36890;&#20449;&#65292;&#22312;&#35832;&#22914;Minecraft&#20043;&#31867;&#30340;&#27169;&#25311;&#29615;&#22659;&#20013;&#26500;&#24314;&#25152;&#38656;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#26234;&#33021;&#24314;&#31569;&#20195;&#29702;&#65292;&#26681;&#25454;&#29992;&#25143;&#23545;&#35805;&#24314;&#36896;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#22312;&#21327;&#20316;&#24314;&#31569;&#20013;&#65292;&#24314;&#31569;&#32773;&#21487;&#33021;&#20250;&#36935;&#21040;&#38590;&#20197;&#35299;&#35835;&#30340;&#24773;&#20917;&#65292;&#22240;&#20026;&#20449;&#24687;&#21644;&#25351;&#20196;&#26377;&#38480;&#65292;&#23548;&#33268;&#27169;&#26865;&#20004;&#21487;&#12290;&#22312;NeurIPS 2022&#31454;&#36187;&#30340;NLP&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#22238;&#31572;&#20102;&#20004;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65306;&#20195;&#29702;&#20309;&#26102;&#24212;&#35813;&#23547;&#27714;&#28548;&#28165;&#65292;&#24212;&#35813;&#35810;&#38382;&#20160;&#20040;&#28548;&#28165;&#38382;&#39064;&#65311;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#23376;&#20219;&#21153;&#26397;&#30528;&#36825;&#20010;&#30446;&#26631;&#36808;&#36827;&#65292;&#19968;&#20010;&#26159;&#20998;&#31867;&#20219;&#21153;&#65292;&#19968;&#20010;&#26159;&#25490;&#24207;&#20219;&#21153;&#12290;&#23545;&#20110;&#20998;&#31867;&#20219;&#21153;&#65292;&#30446;&#26631;&#26159;&#26681;&#25454;&#24403;&#21069;&#30340;&#19990;&#30028;&#29366;&#24577;&#21644;&#23545;&#35805;&#21382;&#21490;&#30830;&#23450;&#20195;&#29702;&#26159;&#21542;&#24212;&#35813;&#23547;&#27714;&#28548;&#28165;&#12290;&#23545;&#20110;&#25490;&#24207;&#20219;&#21153;&#65292;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20221;&#21487;&#33021;&#30340;&#28548;&#28165;&#38382;&#39064;&#30340;&#25490;&#21517;&#21015;&#34920;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#22312;IGLU NLP&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In collaborative tasks, effective communication is crucial for achieving joint goals. One such task is collaborative building where builders must communicate with each other to construct desired structures in a simulated environment such as Minecraft. We aim to develop an intelligent builder agent to build structures based on user input through dialogue. However, in collaborative building, builders may encounter situations that are difficult to interpret based on the available information and instructions, leading to ambiguity. In the NeurIPS 2022 Competition NLP Task, we address two key research questions, with the goal of filling this gap: when should the agent ask for clarification, and what clarification questions should it ask? We move towards this target with two sub-tasks, a classification task and a ranking task. For the classification task, the goal is to determine whether the agent should ask for clarification based on the current world state and dialogue history. For the ran
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30828;&#20214;&#21487;&#38752;&#24615;&#36827;&#34892;&#20102;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#21487;&#38752;&#24615;&#35780;&#20272;&#26041;&#27861;&#20197;&#21450;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#32570;&#12290;</title><link>http://arxiv.org/abs/2305.05750</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30828;&#20214;&#21487;&#38752;&#24615;&#35780;&#20272;&#26041;&#27861;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Systematic Literature Review on Hardware Reliability Assessment Methods for Deep Neural Networks. (arXiv:2305.05750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30828;&#20214;&#21487;&#38752;&#24615;&#36827;&#34892;&#20102;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#21487;&#38752;&#24615;&#35780;&#20272;&#26041;&#27861;&#20197;&#21450;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#32570;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#29305;&#21035;&#26159;&#30001;&#20110;&#20854;&#23398;&#20064;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#33021;&#21147;&#32780;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#12290;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#24555;&#36895;&#36827;&#23637;&#25552;&#20986;&#20102;&#30001;&#22823;&#37327;&#31070;&#32463;&#20803;&#21644;&#23618;&#32452;&#25104;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30828;&#20214;&#21152;&#36895;&#22120;&#65288;DHA&#65289;&#29992;&#20110;&#23558;DNNs&#37096;&#32626;&#21040;&#30446;&#26631;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#21516;&#26102;&#22312;&#30828;&#20214;&#25925;&#38556;/&#38169;&#35823;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#21518;&#26524;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#20013;&#20063;&#21463;&#30410;&#20110;DHA&#12290;&#22240;&#27492;&#65292;DNN&#30340;&#21487;&#38752;&#24615;&#26159;&#30740;&#31350;&#30340;&#37325;&#35201;&#20027;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#21457;&#34920;&#20102;&#22810;&#39033;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;DNNs&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#21487;&#38752;&#24615;&#35780;&#20272;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#24179;&#21488;&#21644;&#24212;&#29992;&#31243;&#24207;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#24635;&#32467;&#29616;&#26377;&#25216;&#26415;&#20197;&#30830;&#23450;&#30740;&#31350;DNN&#30340;&#21487;&#38752;&#24615;&#30340;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65288;SLR&#65289;&#65292;&#20197;&#30830;&#23450;&#29616;&#26377;&#30340;DNN&#30828;&#20214;&#21487;&#38752;&#24615;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#21487;&#29992;&#20110;DNN&#21487;&#38752;&#24615;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#23558;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#20102;&#35299;&#24403;&#21069;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30828;&#20214;&#21487;&#38752;&#24615;&#35780;&#20272;&#26041;&#27861;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#36951;&#28431;&#31354;&#32570;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) and, in particular, Machine Learning (ML) have emerged to be utilized in various applications due to their capability to learn how to solve complex problems. Over the last decade, rapid advances in ML have presented Deep Neural Networks (DNNs) consisting of a large number of neurons and layers. DNN Hardware Accelerators (DHAs) are leveraged to deploy DNNs in the target applications. Safety-critical applications, where hardware faults/errors would result in catastrophic consequences, also benefit from DHAs. Therefore, the reliability of DNNs is an essential subject of research. In recent years, several studies have been published accordingly to assess the reliability of DNNs. In this regard, various reliability assessment methods have been proposed on a variety of platforms and applications. Hence, there is a need to summarize the state of the art to identify the gaps in the study of the reliability of DNNs. In this work, we conduct a Systematic Literature R
&lt;/p&gt;</description></item><item><title>DOCTOR&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#31359;&#25140;&#21307;&#30103;&#20256;&#24863;&#22120;&#30340;&#22810;&#30142;&#30149;&#26816;&#27979;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#22810;&#22836;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;Exemplar-replay&#39118;&#26684;&#30340;CL&#31639;&#27861;&#12290;&#23427;&#21487;&#20197;&#19981;&#26029;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#24182;&#22312;&#20869;&#23384;&#20351;&#29992;&#12289;&#30005;&#27744;&#28040;&#32791;&#21644;&#26816;&#27979;&#22797;&#26434;&#24230;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;ML&#39537;&#21160;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05738</link><description>&lt;p&gt;
DOCTOR&#65306;&#22522;&#20110;&#21487;&#31359;&#25140;&#21307;&#30103;&#20256;&#24863;&#22120;&#30340;&#22810;&#30142;&#30149;&#26816;&#27979;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DOCTOR: A Multi-Disease Detection Continual Learning Framework Based on Wearable Medical Sensors. (arXiv:2305.05738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05738
&lt;/p&gt;
&lt;p&gt;
DOCTOR&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#31359;&#25140;&#21307;&#30103;&#20256;&#24863;&#22120;&#30340;&#22810;&#30142;&#30149;&#26816;&#27979;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#22810;&#22836;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;Exemplar-replay&#39118;&#26684;&#30340;CL&#31639;&#27861;&#12290;&#23427;&#21487;&#20197;&#19981;&#26029;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#24182;&#22312;&#20869;&#23384;&#20351;&#29992;&#12289;&#30005;&#27744;&#28040;&#32791;&#21644;&#26816;&#27979;&#22797;&#26434;&#24230;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;ML&#39537;&#21160;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#36793;&#32536;&#35774;&#22791;&#20013;&#30340;&#21487;&#31359;&#25140;&#21307;&#30103;&#20256;&#24863;&#22120;&#65288;WMS&#65289;&#30340;&#36827;&#27493;&#20351;&#24471;&#26234;&#33021;&#21307;&#30103;&#30340;ML&#39537;&#21160;&#30142;&#30149;&#26816;&#27979;&#25104;&#20026;&#21487;&#33021;&#12290;&#20256;&#32479;&#30340;ML&#39537;&#21160;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#20026;&#27599;&#31181;&#30142;&#30149;&#21644;&#30456;&#24212;&#30340;WMS&#25968;&#25454;&#23450;&#21046;&#20010;&#21035;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26032;&#20219;&#21153;&#20998;&#31867;&#30340;&#36866;&#24212;&#24615;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#26816;&#27979;&#27599;&#20010;&#26032;&#30142;&#30149;&#65292;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#26500;&#24314;&#21644;&#35757;&#32451;&#27169;&#22411;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;WMS&#30340;&#22810;&#30142;&#30149;&#26816;&#27979;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;DOCTOR&#12290;&#23427;&#37319;&#29992;&#20102;&#22810;&#22836;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21644;&#19968;&#31181;Exemplar-replay&#39118;&#26684;&#30340;CL&#31639;&#27861;&#12290;CL&#31639;&#27861;&#20351;&#24471;&#26694;&#26550;&#33021;&#22815;&#19981;&#26029;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#20854;&#20013;&#28041;&#21450;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12289;&#20998;&#31867;&#31867;&#21035;&#21644;&#30142;&#30149;&#26816;&#27979;&#20219;&#21153;&#12290;DOCTOR&#22312;&#20351;&#29992;&#26469;&#33258;&#23454;&#38469;WMS&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#22235;&#31181;&#24120;&#35265;&#30142;&#30149;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#22312;&#20869;&#23384;&#20351;&#29992;&#12289;&#30005;&#27744;&#28040;&#32791;&#21644;&#26816;&#27979;&#22797;&#26434;&#24230;&#26041;&#38754;&#65292;DOCTOR&#20063;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern advances in machine learning (ML) and wearable medical sensors (WMSs) in edge devices have enabled ML-driven disease detection for smart healthcare. Conventional ML-driven disease detection methods rely on customizing individual models for each disease and its corresponding WMS data. However, such methods lack adaptability to distribution shifts and new task classification classes. Also, they need to be rearchitected and retrained from scratch for each new disease. Moreover, installing multiple ML models in an edge device consumes excessive memory, drains the battery faster, and complicates the detection process. To address these challenges, we propose DOCTOR, a multi-disease detection continual learning (CL) framework based on WMSs. It employs a multi-headed deep neural network (DNN) and an exemplar-replay-style CL algorithm. The CL algorithm enables the framework to continually learn new missions where different data distributions, classification classes, and disease detection
&lt;/p&gt;</description></item><item><title>&#26460;&#20811;&#33086;&#33039;&#25968;&#25454;&#38598;&#65288;DSDS&#65289;&#26159;&#19968;&#31181;&#20844;&#24320;&#30340;&#33086;&#33039;MRI&#21644;CT&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#33086;&#33039;&#20998;&#21106;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;109&#20010;&#26469;&#33258;&#24739;&#26377;&#24930;&#24615;&#32925;&#30149;&#21644;&#38376;&#38745;&#33033;&#39640;&#21387;&#30340;&#24739;&#32773;&#30340;&#22270;&#20687;&#65292;&#24182;&#21253;&#25324;&#22810;&#31181;&#33086;&#33039;&#24418;&#29366;&#21644;&#22823;&#23567;&#30340;&#28151;&#21512;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.05732</link><description>&lt;p&gt;
&#26460;&#20811;&#33086;&#33039;&#25968;&#25454;&#38598;&#65306;&#29992;&#20110;&#35757;&#32451;&#20998;&#21106;&#31639;&#27861;&#30340;&#20844;&#24320;&#30340;MRI&#21644;CT&#33086;&#33039;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Duke Spleen Data Set: A Publicly Available Spleen MRI and CT dataset for Training Segmentation. (arXiv:2305.05732v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05732
&lt;/p&gt;
&lt;p&gt;
&#26460;&#20811;&#33086;&#33039;&#25968;&#25454;&#38598;&#65288;DSDS&#65289;&#26159;&#19968;&#31181;&#20844;&#24320;&#30340;&#33086;&#33039;MRI&#21644;CT&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#33086;&#33039;&#20998;&#21106;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;109&#20010;&#26469;&#33258;&#24739;&#26377;&#24930;&#24615;&#32925;&#30149;&#21644;&#38376;&#38745;&#33033;&#39640;&#21387;&#30340;&#24739;&#32773;&#30340;&#22270;&#20687;&#65292;&#24182;&#21253;&#25324;&#22810;&#31181;&#33086;&#33039;&#24418;&#29366;&#21644;&#22823;&#23567;&#30340;&#28151;&#21512;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33086;&#33039;&#20307;&#31215;&#30340;&#27979;&#37327;&#20027;&#35201;&#19982;&#24739;&#26377;&#24930;&#24615;&#32925;&#30149;&#21644;&#38376;&#38745;&#33033;&#39640;&#21387;&#30340;&#24739;&#32773;&#26377;&#20851;&#65292;&#22240;&#20026;&#20182;&#20204;&#30340;&#33086;&#33039;&#24418;&#29366;&#21644;&#22823;&#23567;&#36890;&#24120;&#24322;&#24120;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#20998;&#21106;&#33086;&#33039;&#20197;&#33719;&#24471;&#20854;&#20307;&#31215;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#24050;&#34987;&#35777;&#26126;&#26159;&#33258;&#21160;&#21270;&#33086;&#33039;&#20998;&#21106;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#20294;&#35757;&#32451;&#36825;&#20123;&#31639;&#27861;&#38656;&#35201;&#19968;&#20010;&#36866;&#24403;&#30340;&#25968;&#25454;&#38598;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#29616;&#26377;&#30340;&#20960;&#20010;&#20844;&#24320;&#30340;&#33086;&#33039;&#20998;&#21106;&#25968;&#25454;&#38598;&#32570;&#20047;&#28151;&#28102;&#29305;&#24449;&#65292;&#22914;&#33145;&#27700;&#21644;&#33145;&#22721;&#38745;&#33033;&#26354;&#24352;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#26460;&#20811;&#33086;&#33039;&#25968;&#25454;&#38598;&#65288;DSDS&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#24930;&#24615;&#32925;&#30149;&#21644;&#38376;&#38745;&#33033;&#39640;&#21387;&#24739;&#32773;&#30340;109&#20010;CT&#21644;MRI&#20307;&#31215;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#21508;&#31181;&#22270;&#20687;&#31867;&#22411;&#12289;&#20379;&#24212;&#21830;&#12289;&#24179;&#38754;&#21644;&#23545;&#27604;&#24230;&#65292;&#20197;&#21450;&#30001;&#20110;&#22522;&#30784;&#30142;&#30149;&#29366;&#24577;&#23548;&#33268;&#30340;&#19981;&#21516;&#30340;&#33086;&#33039;&#24418;&#29366;&#21644;&#22823;&#23567;&#12290;DSDS&#26088;&#22312;&#20419;&#36827;&#21019;&#24314;&#33021;&#32771;&#34385;&#30142;&#30149;&#33086;&#33039;&#22797;&#26434;&#24615;&#30340;&#24378;&#22823;&#33086;&#33039;&#20998;&#21106;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spleen volumetry is primarily associated with patients suffering from chronic liver disease and portal hypertension, as they often have spleens with abnormal shapes and sizes. However, manually segmenting the spleen to obtain its volume is a time-consuming process. Deep learning algorithms have proven to be effective in automating spleen segmentation, but a suitable dataset is necessary for training such algorithms. To our knowledge, the few publicly available datasets for spleen segmentation lack confounding features such as ascites and abdominal varices. To address this issue, the Duke Spleen Data Set (DSDS) has been developed, which includes 109 CT and MRI volumes from patients with chronic liver disease and portal hypertension. The dataset includes a diverse range of image types, vendors, planes, and contrasts, as well as varying spleen shapes and sizes due to underlying disease states. The DSDS aims to facilitate the creation of robust spleen segmentation models that can take into
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#26368;&#20339;&#31867;&#21035;&#27604;&#20363;&#19982;&#27169;&#22411;&#22797;&#26434;&#24230;&#32852;&#31995;&#36215;&#26469;&#65292;&#27599;&#31181;&#27169;&#22411;&#37117;&#33021;&#22815;&#24471;&#21040;&#27491;&#30830;&#30340;&#31867;&#21035;&#27604;&#20363;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#22312;&#38463;&#29255;&#31867;&#33647;&#29289;&#36807;&#37327;&#39044;&#27979;&#38382;&#39064;&#19978;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05722</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#31867;&#21035;&#27604;&#20363;&#35843;&#25972;&#22686;&#24378;&#20020;&#24202;&#39044;&#27979;&#27169;&#22411;&#65306;&#20197;&#38463;&#29255;&#31867;&#33647;&#29289;&#36807;&#37327;&#39044;&#27979;&#20026;&#20363;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing Clinical Predictive Modeling through Model Complexity-Driven Class Proportion Tuning for Class Imbalanced Data: An Empirical Study on Opioid Overdose Prediction. (arXiv:2305.05722v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05722
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#26368;&#20339;&#31867;&#21035;&#27604;&#20363;&#19982;&#27169;&#22411;&#22797;&#26434;&#24230;&#32852;&#31995;&#36215;&#26469;&#65292;&#27599;&#31181;&#27169;&#22411;&#37117;&#33021;&#22815;&#24471;&#21040;&#27491;&#30830;&#30340;&#31867;&#21035;&#27604;&#20363;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#22312;&#38463;&#29255;&#31867;&#33647;&#29289;&#36807;&#37327;&#39044;&#27979;&#38382;&#39064;&#19978;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#22312;&#21307;&#30103;&#39046;&#22495;&#24191;&#27867;&#23384;&#22312;&#65292;&#20005;&#37325;&#30772;&#22351;&#20102;&#20020;&#24202;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22823;&#22810;&#25968;&#32531;&#35299;&#38382;&#39064;&#30340;&#26041;&#27861;&#36890;&#36807;&#37325;&#26032;&#24179;&#34913;&#31867;&#21035;&#27604;&#20363;&#26469;&#23454;&#29616;&#65292;&#23427;&#20204;&#20027;&#35201;&#20551;&#35774;&#37325;&#26032;&#24179;&#34913;&#30340;&#27604;&#20363;&#24212;&#35813;&#26159;&#21407;&#22987;&#25968;&#25454;&#30340;&#20989;&#25968;&#65292;&#19982;&#25152;&#20351;&#29992;&#30340;&#27169;&#22411;&#26080;&#20851;&#12290;&#26412;&#25991;&#23545;&#36825;&#19968;&#27969;&#34892;&#20551;&#35774;&#25552;&#20986;&#20102;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#23558;&#26368;&#20339;&#31867;&#21035;&#27604;&#20363;&#19982;&#27169;&#22411;&#22797;&#26434;&#24230;&#32852;&#31995;&#36215;&#26469;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20174;&#32780;&#38024;&#23545;&#27599;&#31181;&#27169;&#22411;&#35843;&#25972;&#31867;&#21035;&#27604;&#20363;&#12290;&#25105;&#20204;&#22312;&#38463;&#29255;&#31867;&#33647;&#29289;&#36807;&#37327;&#39044;&#27979;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#31361;&#20986;&#20102;&#35843;&#25972;&#31867;&#21035;&#27604;&#20363;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20005;&#26684;&#30340;&#22238;&#24402;&#20998;&#26512;&#20063;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#30340;&#29702;&#35770;&#26694;&#26550;&#30340;&#20248;&#28857;&#65292;&#20197;&#21450;&#25511;&#21046;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#26368;&#20339;&#31867;&#21035;&#27604;&#20363;&#20043;&#38388;&#30340;&#36229;&#21442;&#25968;&#20043;&#38388;&#30340;&#32479;&#35745;&#26174;&#30528;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance problems widely exist in the medical field and heavily deteriorates performance of clinical predictive models. Most techniques to alleviate the problem rebalance class proportions and they predominantly assume the rebalanced proportions should be a function of the original data and oblivious to the model one uses. This work challenges this prevailing assumption and proposes that links the optimal class proportions to the model complexity, thereby tuning the class proportions per model. Our experiments on the opioid overdose prediction problem highlight the performance gain of tuning class proportions. Rigorous regression analysis also confirms the advantages of the theoretical framework proposed and the statistically significant correlation between the hyperparameters controlling the model complexity and the optimal class proportions.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#19977;&#32500;&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#20998;&#23376;&#12289;&#26448;&#26009;&#21644;&#34507;&#30333;&#36136;&#32467;&#21512;&#20301;&#28857;&#30340;&#32467;&#26500;&#65292;&#32780;&#19981;&#38656;&#36716;&#25442;&#25104;&#32447;&#24615;&#23383;&#31526;&#20018;&#34920;&#31034;&#24418;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.05708</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#20197;XYZ&#12289;CIF&#21644;PDB&#25991;&#20214;&#30340;&#24418;&#24335;&#29983;&#25104;&#20998;&#23376;&#12289;&#26448;&#26009;&#21644;&#34507;&#30333;&#36136;&#32467;&#21512;&#20301;&#28857;&#30340;&#19977;&#32500;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files. (arXiv:2305.05708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05708
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#19977;&#32500;&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#20998;&#23376;&#12289;&#26448;&#26009;&#21644;&#34507;&#30333;&#36136;&#32467;&#21512;&#20301;&#28857;&#30340;&#32467;&#26500;&#65292;&#32780;&#19981;&#38656;&#36716;&#25442;&#25104;&#32447;&#24615;&#23383;&#31526;&#20018;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#20998;&#23376;&#35774;&#35745;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#24403;&#21069;&#20027;&#27969;&#30340;&#24605;&#36335;&#26159;&#23558;&#20998;&#23376;&#22270;&#24418;&#35299;&#26512;&#25104;&#32447;&#24615;&#23383;&#31526;&#20018;&#34920;&#31034;&#65292;&#20174;&#32780;&#26131;&#20110;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#38750;&#24120;&#25104;&#21151;&#65292;&#20294;&#23427;&#20165;&#38480;&#20110;&#21487;&#20197;&#23436;&#20840;&#36890;&#36807;&#22270;&#24418;&#34920;&#31034;&#30340;&#21270;&#23398;&#32467;&#26500;&#65292;&#22914;&#26377;&#26426;&#20998;&#23376;&#65292;&#32780;&#26448;&#26009;&#21644;&#29983;&#29289;&#20998;&#23376;&#32467;&#26500;&#22914;&#34507;&#30333;&#36136;&#32467;&#21512;&#20301;&#28857;&#38656;&#35201;&#26356;&#23436;&#25972;&#30340;&#34920;&#31034;&#26041;&#24335;&#65292;&#20854;&#20013;&#21253;&#25324;&#23427;&#20204;&#30340;&#21407;&#23376;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#23545;&#23450;&#20301;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#36827;&#34892;&#35757;&#32451;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#26550;&#26500;&#20462;&#25913;&#65292;&#23601;&#33021;&#22815;&#20174;&#21508;&#31181;&#22522;&#20110;&#21270;&#23398;&#32467;&#26500;&#30340;&#38750;&#24120;&#19981;&#21516;&#30340;&#20998;&#24067;&#20013;&#29983;&#25104;&#19977;&#32500;&#30340;&#26032;&#39062;&#19988;&#26377;&#25928;&#30340;&#32467;&#26500;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30452;&#25509;&#35757;&#32451;&#20110;&#20174;&#21270;&#23398;&#25991;&#20214;&#26684;&#24335;&#22914;XYZ&#25991;&#20214;&#12289;&#26230;&#20307;&#23398;&#20449;&#24687;&#25991;&#20214;&#65288;CIF&#65289;&#25110;&#34507;&#30333;&#36136;&#25968;&#25454;&#24211;&#25991;&#20214;&#65288;PDB&#65289;&#27966;&#29983;&#30340;&#24207;&#21015;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#19977;&#32500;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models are powerful tools for molecular design. Currently, the dominant paradigm is to parse molecular graphs into linear string representations that can easily be trained on. This approach has been very successful, however, it is limited to chemical structures that can be completely represented by a graph -- like organic molecules -- while materials and biomolecular structures like protein binding sites require a more complete representation that includes the relative positioning of their atoms in space. In this work, we show how language models, without any architecture modifications, trained using next-token prediction -- can generate novel and valid structures in three dimensions from various substantially different distributions of chemical structures. In particular, we demonstrate that language models trained directly on sequences derived directly from chemical file formats like XYZ files, Crystallographic Information files (CIFs), or Protein Data Bank files (PDBs) can d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; DexArt &#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#29289;&#29702;&#27169;&#25311;&#22120;&#65292;&#35780;&#20272;&#26426;&#22120;&#20154;&#25163;&#22312;&#26410;&#35265;&#36807;&#30340;&#20851;&#33410;&#29289;&#20307;&#19978;&#36827;&#34892;&#28789;&#24039;&#25805;&#20316;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05706</link><description>&lt;p&gt;
DexArt&#65306;&#22522;&#20110;&#20851;&#33410;&#29289;&#20307;&#30340;&#36890;&#29992;&#28789;&#24039;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
DexArt: Benchmarking Generalizable Dexterous Manipulation with Articulated Objects. (arXiv:2305.05706v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05706
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; DexArt &#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#29289;&#29702;&#27169;&#25311;&#22120;&#65292;&#35780;&#20272;&#26426;&#22120;&#20154;&#25163;&#22312;&#26410;&#35265;&#36807;&#30340;&#20851;&#33410;&#29289;&#20307;&#19978;&#36827;&#34892;&#28789;&#24039;&#25805;&#20316;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#36890;&#29992;&#65292;&#25105;&#20204;&#38656;&#35201;&#35753;&#26426;&#22120;&#20154;&#20687;&#20154;&#31867;&#19968;&#26679;&#25805;&#20316;&#26085;&#24120;&#20351;&#29992;&#30340;&#20851;&#33410;&#29289;&#21697;&#12290;&#30446;&#21069;&#65292;&#26426;&#22120;&#20154;&#25805;&#32437;&#22823;&#37327;&#20381;&#36182;&#20110;&#20351;&#29992;&#24179;&#34892;&#22841;&#20855;&#65292;&#36825;&#20351;&#24471;&#26426;&#22120;&#20154;&#23616;&#38480;&#20110;&#19968;&#20123;&#29289;&#20307;&#30340;&#25805;&#20316;&#12290;&#32780;&#20351;&#29992;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#23558;&#26356;&#22909;&#22320;&#36817;&#20284;&#20154;&#31867;&#34892;&#20026;&#65292;&#24182;&#20351;&#26426;&#22120;&#20154;&#22312;&#22810;&#26679;&#21270;&#30340;&#20851;&#33410;&#29289;&#20307;&#19978;&#36827;&#34892;&#25805;&#20316;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#31216;&#20026; DexArt&#65292;&#23427;&#28041;&#21450;&#20351;&#29992;&#29289;&#29702;&#27169;&#25311;&#22120;&#36827;&#34892;&#20851;&#33410;&#29289;&#20307;&#30340;&#28789;&#24039;&#25805;&#20316;&#12290;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#22810;&#20010;&#22797;&#26434;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#26426;&#22120;&#20154;&#25163;&#38656;&#35201;&#22312;&#27599;&#20010;&#20219;&#21153;&#20013;&#25805;&#20316;&#22810;&#26679;&#21270;&#30340;&#20851;&#33410;&#29289;&#20307;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#37325;&#28857;&#26159;&#35780;&#20272;&#22312;&#26410;&#35265;&#36807;&#30340;&#20851;&#33410;&#29289;&#20307;&#19978;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#32771;&#34385;&#21040;&#25163;&#21644;&#29289;&#20307;&#30340;&#33258;&#30001;&#24230;&#24456;&#39640;&#65292;&#36825;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#19977;&#32500;&#34920;&#31034;&#23398;&#20064;&#26469;&#23454;&#29616;&#27867;&#21270;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26410;&#35265;&#36807;&#30340;&#20851;&#33410;&#29289;&#20307;&#19978;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enable general-purpose robots, we will require the robot to operate daily articulated objects as humans do. Current robot manipulation has heavily relied on using a parallel gripper, which restricts the robot to a limited set of objects. On the other hand, operating with a multi-finger robot hand will allow better approximation to human behavior and enable the robot to operate on diverse articulated objects. To this end, we propose a new benchmark called DexArt, which involves Dexterous manipulation with Articulated objects in a physical simulator. In our benchmark, we define multiple complex manipulation tasks, and the robot hand will need to manipulate diverse articulated objects within each task. Our main focus is to evaluate the generalizability of the learned policy on unseen articulated objects. This is very challenging given the high degrees of freedom of both hands and objects. We use Reinforcement Learning with 3D representation learning to achieve generalization. Through e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#37319;&#38598;&#24310;&#36831;&#23545;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#22312;&#29482;&#32905;&#20215;&#26684;&#39044;&#27979;&#20013;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#21516;&#26085;&#33719;&#21462;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#25968;&#25454;&#26102;&#38388;&#28382;&#21518;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.05677</link><description>&lt;p&gt;
&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#20915;&#31574;&#31995;&#32479;&#20013;&#65292;&#25968;&#25454;&#26102;&#38388;&#28382;&#21518;&#23545;&#29482;&#32905;&#20215;&#26684;&#39044;&#27979;&#30340;&#24433;&#21709;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Effects of data time lag in a decision-making system using machine learning for pork price prediction. (arXiv:2305.05677v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#37319;&#38598;&#24310;&#36831;&#23545;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#22312;&#29482;&#32905;&#20215;&#26684;&#39044;&#27979;&#20013;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#21516;&#26085;&#33719;&#21462;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#25968;&#25454;&#26102;&#38388;&#28382;&#21518;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35199;&#29677;&#29273;&#26159;&#19990;&#30028;&#19978;&#31532;&#19977;&#22823;&#29482;&#32905;&#29983;&#20135;&#22269;&#65292;&#35768;&#22810;&#20892;&#22330;&#20381;&#36182;&#20110;&#36825;&#20010;&#24066;&#22330;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#23450;&#20215;&#20307;&#31995;&#26159;&#19981;&#20844;&#24179;&#30340;&#65292;&#22240;&#20026;&#19968;&#20123;&#20154;&#27604;&#20854;&#20182;&#20154;&#26377;&#26356;&#22909;&#30340;&#24066;&#22330;&#20449;&#24687;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21382;&#21490;&#20215;&#26684;&#26159;&#26131;&#20110;&#33719;&#21462;&#21644;&#32463;&#27982;&#23454;&#24800;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#21487;&#20197;&#24110;&#21161;&#25152;&#26377;&#20195;&#29702;&#21830;&#26356;&#22909;&#22320;&#20102;&#35299;&#24066;&#22330;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#37319;&#38598;&#24310;&#36831;&#21487;&#33021;&#20250;&#24433;&#21709;&#20182;&#20204;&#30340;&#23450;&#20215;&#20915;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;&#39044;&#27979;&#31639;&#27861;&#22312;&#20215;&#26684;&#39044;&#27979;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#37319;&#38598;&#24310;&#36831;&#23545;&#20854;&#24433;&#21709;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#26368;&#20339;&#25552;&#26696;&#30340;&#38598;&#25104;&#21040;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#21407;&#22411;&#20013;&#65292;&#24182;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#35199;&#29677;&#29273;&#26368;&#37325;&#35201;&#30340;&#22320;&#21306;&#29482;&#32905;&#24066;&#22330;&#30340;&#20844;&#20849;&#25968;&#25454;&#65292;&#30001;&#20892;&#19994;&#37096;&#21457;&#24067;&#65292;&#24310;&#36831;&#20004;&#21608;&#65292;&#21644;&#35746;&#38405;&#26041;&#24335;&#33719;&#21462;&#30340;&#21516;&#19968;&#26085;&#30340;&#21516;&#19968;&#24066;&#22330;&#25968;&#25454;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#20339;&#39044;&#27979;&#31639;&#27861;&#21644;&#20004;&#21608;&#24310;&#36831;&#30340;&#25968;&#25454;&#30456;&#27604;&#65292;&#35746;&#38405;&#25968;&#25454;&#21487;&#20197;&#20943;&#23569;&#38169;&#35823;&#24046;&#24322;&#65292;&#36825;&#35777;&#26126;&#20102;&#38598;&#25104;&#22810;&#20010;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#25968;&#25454;&#26102;&#38388;&#28382;&#21518;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spain is the third-largest producer of pork meat in the world, and many farms in several regions depend on the evolution of this market. However, the current pricing system is unfair, as some actors have better market information than others. In this context, historical pricing is an easy-to-find and affordable data source that can help all agents to be better informed. However, the time lag in data acquisition can affect their pricing decisions. In this paper, we study the effect that data acquisition delay has on a price prediction system using multiple prediction algorithms. We describe the integration of the best proposal into a decision support system prototype and test it in a real-case scenario. Specifically, we use public data from the most important regional pork meat markets in Spain published by the Ministry of Agriculture with a two-week delay and subscription-based data of the same markets obtained on the same day. The results show that the error difference between the bes
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;Adam&#22411;&#31639;&#27861;&#26694;&#26550;UAdam&#65292;&#35813;&#26694;&#26550;&#26159;&#21253;&#25324;Adam&#21450;&#20854;&#21464;&#20307;&#22312;&#20869;&#30340;&#29305;&#20363;&#65292;&#24182;&#19988;&#22312;&#38750;&#20984;&#38543;&#26426;&#35774;&#32622;&#19979;&#20855;&#26377;&#25910;&#25947;&#24615;&#65292;&#21487;&#20197;&#20197;$\mathcal{O}(1/T)$&#30340;&#36895;&#29575;&#25910;&#25947;&#21040;&#38745;&#27490;&#28857;&#30340;&#37051;&#22495;&#12290;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#36229;&#21442;&#25968;&#65292;&#39321;&#33609;Adam&#20063;&#21487;&#20197;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2305.05675</link><description>&lt;p&gt;
UAdam&#65306;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#30340;&#32479;&#19968;Adam&#22411;&#31639;&#27861;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UAdam: Unified Adam-Type Algorithmic Framework for Non-Convex Stochastic Optimization. (arXiv:2305.05675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;Adam&#22411;&#31639;&#27861;&#26694;&#26550;UAdam&#65292;&#35813;&#26694;&#26550;&#26159;&#21253;&#25324;Adam&#21450;&#20854;&#21464;&#20307;&#22312;&#20869;&#30340;&#29305;&#20363;&#65292;&#24182;&#19988;&#22312;&#38750;&#20984;&#38543;&#26426;&#35774;&#32622;&#19979;&#20855;&#26377;&#25910;&#25947;&#24615;&#65292;&#21487;&#20197;&#20197;$\mathcal{O}(1/T)$&#30340;&#36895;&#29575;&#25910;&#25947;&#21040;&#38745;&#27490;&#28857;&#30340;&#37051;&#22495;&#12290;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#36229;&#21442;&#25968;&#65292;&#39321;&#33609;Adam&#20063;&#21487;&#20197;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Adam&#22411;&#31639;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#24050;&#25104;&#20026;&#20248;&#21270;&#30340;&#39318;&#36873;&#65292;&#28982;&#32780;&#65292;&#23613;&#31649;&#25104;&#21151;&#65292;&#20854;&#25910;&#25947;&#24615;&#20173;&#19981;&#22815;&#29702;&#35299;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;Adam&#22411;&#31639;&#27861;&#26694;&#26550;&#65288;&#31216;&#20026;UAdam&#65289;&#12290;&#23427;&#37197;&#22791;&#20102;&#31532;&#20108;&#38454;&#30697;&#30340;&#19968;&#33324;&#24418;&#24335;&#65292;&#21487;&#20197;&#21253;&#25324;Adam&#21450;&#20854;&#21464;&#20307;&#20316;&#20026;&#29305;&#20363;&#65292;&#22914;NAdam&#12289;AMSGrad&#12289;AdaBound&#12289;AdaFom&#21644;Adan&#12290;&#22312;&#38750;&#20984;&#38543;&#26426;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#23545;UAdam&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#34920;&#26126;UAdam&#20197;$\mathcal{O}(1/T)$&#30340;&#36895;&#29575;&#25910;&#25947;&#21040;&#38745;&#27490;&#28857;&#30340;&#37051;&#22495;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;$\beta$&#30340;&#22686;&#21152;&#65292;&#37051;&#22495;&#30340;&#22823;&#23567;&#20943;&#23567;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#20165;&#35201;&#27714;&#31532;&#19968;&#38454;&#21160;&#37327;&#22240;&#23376;&#36275;&#22815;&#25509;&#36817;1&#65292;&#23545;&#20110;&#31532;&#20108;&#38454;&#21160;&#37327;&#22240;&#23376;&#27809;&#26377;&#20219;&#20309;&#38480;&#21046;&#12290;&#29702;&#35770;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#36229;&#21442;&#25968;&#65292;&#39321;&#33609;Adam&#21487;&#20197;&#25910;&#25947;&#65292;&#36825;&#20026;&#23454;&#36341;&#32773;&#36873;&#25321;Adam&#26063;&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adam-type algorithms have become a preferred choice for optimisation in the deep learning setting, however, despite success, their convergence is still not well understood. To this end, we introduce a unified framework for Adam-type algorithms (called UAdam). This is equipped with a general form of the second-order moment, which makes it possible to include Adam and its variants as special cases, such as NAdam, AMSGrad, AdaBound, AdaFom, and Adan. This is supported by a rigorous convergence analysis of UAdam in the non-convex stochastic setting, showing that UAdam converges to the neighborhood of stationary points with the rate of $\mathcal{O}(1/T)$. Furthermore, the size of neighborhood decreases as $\beta$ increases. Importantly, our analysis only requires the first-order momentum factor to be close enough to 1, without any restrictions on the second-order momentum factor. Theoretical results also show that vanilla Adam can converge by selecting appropriate hyperparameters, which pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#24490;&#29615;&#32593;&#32476;&#30340;&#21487;&#38752;DBD&#31995;&#32479;&#65292;&#21033;&#29992;&#20844;&#20849;&#20256;&#24863;&#22120;&#25552;&#39640;&#20102;&#39550;&#39542;&#34892;&#20026;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05670</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#21367;&#31215;&#24490;&#29615;&#32593;&#32476;&#30340;&#21361;&#38505;&#39550;&#39542;&#34892;&#20026;&#31934;&#20934;&#26816;&#27979;&#25552;&#39640;&#36947;&#36335;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Road Safety through Accurate Detection of Hazardous Driving Behaviors with Graph Convolutional Recurrent Networks. (arXiv:2305.05670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#24490;&#29615;&#32593;&#32476;&#30340;&#21487;&#38752;DBD&#31995;&#32479;&#65292;&#21033;&#29992;&#20844;&#20849;&#20256;&#24863;&#22120;&#25552;&#39640;&#20102;&#39550;&#39542;&#34892;&#20026;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#31096;&#20173;&#28982;&#26159;&#20840;&#29699;&#37325;&#22823;&#30340;&#20844;&#20849;&#23433;&#20840;&#38382;&#39064;&#65292;&#20854;&#20013;&#30340;&#22823;&#22810;&#25968;&#24402;&#22240;&#20110;&#39550;&#39542;&#21592;&#38169;&#35823;&#65292;&#21253;&#25324;&#19981;&#36275;&#30340;&#39550;&#39542;&#30693;&#35782;&#12289;&#19981;&#21512;&#35268;&#23450;&#20197;&#21450;&#19981;&#33391;&#39550;&#39542;&#20064;&#24815;&#12290;&#20026;&#20102;&#25552;&#39640;&#36947;&#36335;&#23433;&#20840;&#24615;&#65292;&#22810;&#39033;&#39550;&#39542;&#34892;&#20026;&#26816;&#27979;&#65288;DBD&#65289;&#31995;&#32479;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#20197;&#35782;&#21035;&#23433;&#20840;&#21644;&#19981;&#23433;&#20840;&#30340;&#39550;&#39542;&#34892;&#20026;&#12290;&#36825;&#20123;&#30740;&#31350;&#20013;&#30340;&#35768;&#22810;&#21033;&#29992;&#20102;&#20174;&#25511;&#21046;&#22120;&#21306;&#22495;&#32593;&#32476;&#65288;CAN&#65289;&#24635;&#32447;&#33719;&#21462;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#26469;&#26500;&#24314;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20844;&#20849;&#21487;&#29992;&#20256;&#24863;&#22120;&#24050;&#30693;&#20250;&#38477;&#20302;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#23558;&#20379;&#24212;&#21830;&#29305;&#23450;&#30340;&#20256;&#24863;&#22120;&#32435;&#20837;&#25968;&#25454;&#38598;&#20013;&#21017;&#21487;&#20197;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;GConvLSTM&#65289;&#30340;&#21487;&#38752;DBD&#31995;&#32479;&#65292;&#21033;&#29992;&#20844;&#20849;&#20256;&#24863;&#22120;&#25552;&#39640;&#20102;DBD&#27169;&#22411;&#30340;&#31934;&#24230;&#21644;&#23454;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21152;&#20837;&#20102;&#38750;&#20844;&#20849;&#20256;&#24863;&#22120;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Car accidents remain a significant public safety issue worldwide, with the majority of them attributed to driver errors stemming from inadequate driving knowledge, non-compliance with regulations, and poor driving habits. To improve road safety, Driving Behavior Detection (DBD) systems have been proposed in several studies to identify safe and unsafe driving behavior. Many of these studies have utilized sensor data obtained from the Controller Area Network (CAN) bus to construct their models. However, the use of publicly available sensors is known to reduce the accuracy of detection models, while incorporating vendor-specific sensors into the dataset increases accuracy. To address the limitations of existing approaches, we present a reliable DBD system based on Graph Convolutional Long Short-Term Memory Networks (GConvLSTM) that enhances the precision and practicality of DBD models using public sensors. Additionally, we incorporate non-public sensors to evaluate the model's effectivene
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#39044;&#27979;&#22686;&#26448;&#21046;&#36896;&#32858;&#20083;&#37240;&#65288;PLA&#65289;&#26679;&#21697;&#30340;&#20914;&#20987;&#24378;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.05668</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#30340;&#39044;&#27979;&#22686;&#26448;&#21046;&#36896;&#32858;&#20083;&#37240;&#65288;PLA&#65289;&#26679;&#21697;&#20914;&#20987;&#24378;&#24230;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic Artificial Intelligence (NSAI) based Algorithm for predicting the Impact Strength of Additive Manufactured Polylactic Acid (PLA) Specimens. (arXiv:2305.05668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#39044;&#27979;&#22686;&#26448;&#21046;&#36896;&#32858;&#20083;&#37240;&#65288;PLA&#65289;&#26679;&#21697;&#30340;&#20914;&#20987;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65288;NSAI&#65289;&#22312;&#39044;&#27979;&#22686;&#26448;&#21046;&#36896;&#32858;&#20083;&#37240;&#65288;PLA&#65289;&#32452;&#20214;&#20914;&#20987;&#24378;&#24230;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#26159;NSAI&#22312;&#22686;&#26448;&#21046;&#36896;&#39046;&#22495;&#39318;&#27425;&#24212;&#29992;&#12290;NSAI&#27169;&#22411;&#34701;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#20248;&#21183;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#25552;&#20379;&#26356;&#31934;&#20934;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#37319;&#38598;&#20102;&#23454;&#39564;&#25968;&#25454;&#24182;&#36827;&#34892;&#20102;&#21512;&#25104;&#22686;&#24378;&#65292;&#20351;&#27169;&#22411;&#26356;&#21152;&#31934;&#30830;&#12290;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#20351;&#29992;&#21253;&#25324;&#36755;&#20837;&#12289;&#20004;&#20010;&#38544;&#34255;&#23618;&#12289;&#21644;&#19968;&#20010;&#36755;&#20986;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#24320;&#21457;&#65292;&#25509;&#30528;&#26159;&#19968;&#20010;&#20195;&#34920;&#31526;&#21495;&#32452;&#20214;&#30340;&#20915;&#31574;&#26641;&#22238;&#24402;&#22120;&#12290;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#35757;&#32451;&#38598;&#21644;&#39564;&#35777;&#38598;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#21644;R2&#20540;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#34987;&#19982;&#31616;&#21333;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#20248;&#20110;&#31616;&#21333;&#30340;ANN&#27169;&#22411;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;MSE&#26356;&#20302;&#65292;R2&#20540;&#26356;&#39640;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;NSAI&#31639;&#27861;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#22686;&#26448;&#21046;&#36896;PLA&#32452;&#20214;&#30340;&#20914;&#20987;&#24378;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#26497;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce application of Neurosymbolic Artificial Intelligence (NSAI) for predicting the impact strength of additive manufactured polylactic acid (PLA) components, representing the first-ever use of NSAI in the domain of additive manufacturing. The NSAI model amalgamates the advantages of neural networks and symbolic AI, offering a more robust and accurate prediction than traditional machine learning techniques. Experimental data was collected and synthetically augmented to 1000 data points, enhancing the model's precision. The Neurosymbolic model was developed using a neural network architecture comprising input, two hidden layers, and an output layer, followed by a decision tree regressor representing the symbolic component. The model's performance was benchmarked against a Simple Artificial Neural Network (ANN) model by assessing mean squared error (MSE) and R-squared (R2) values for both training and validation datasets. The results reveal that the Neurosymbolic m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861; HEER&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.05640</link><description>&lt;p&gt;
&#38754;&#21521;&#20010;&#20154;&#25110;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65306;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24212;&#29992;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Representation Learning for Person or Entity-centric Knowledge Graphs: an application in Healthcare. (arXiv:2305.05640v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861; HEER&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26159;&#19968;&#31181;&#25353;&#26412;&#20307;&#25110;&#27169;&#24335;&#32452;&#32455;&#20449;&#24687;&#30340;&#27969;&#34892;&#26041;&#24335;&#65292;&#24050;&#32463;&#22312;&#20174;&#25628;&#32034;&#21040;&#25512;&#33616;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;&#30693;&#35782;&#22270;&#35889;&#26041;&#38754;&#26377;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#30693;&#35782;&#34920;&#31034;&#20173;&#28982;&#26159;&#36328;&#34892;&#19994;&#30340;&#19968;&#20010;&#38750;&#24120;&#26840;&#25163;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#30001;&#20110;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20851;&#31995;&#12289;&#24322;&#36136;&#24615;&#12289;&#32570;&#20047;&#26631;&#20934;&#21270;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#31561;&#22240;&#32032;&#65292;&#36825;&#19968;&#20219;&#21153;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#25429;&#25417;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21517;&#20026;HEER&#65288;Healthcare Entity-Entity Representation learning&#65289;&#65292;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#12290;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;HEER&#22312;&#25913;&#21892;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) are a popular way to organise information based on ontologies or schemas and have been used across a variety of scenarios from search to recommendation. Despite advances in KGs, representing knowledge remains a non-trivial task across industries and it is especially challenging in the biomedical and healthcare domains due to complex interdependent relations between entities, heterogeneity, lack of standardization, and sparseness of data. KGs are used to discover diagnoses or prioritize genes relevant to disease, but they often rely on schemas that are not centred around a node or entity of interest, such as a person. Entity-centric KGs are relatively unexplored but hold promise in representing important facets connected to a central node and unlocking downstream tasks beyond graph traversal and reasoning, such as generating graph embeddings and training graph neural networks for a wide range of predictive tasks. This paper presents an end-to-end representation le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#20195;&#30721;&#29255;&#27573;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#20197;&#22635;&#34917;&#20174;&#20195;&#30721;&#20013;&#20998;&#31867;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24615;&#30340;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#20195;&#30721;&#30340;&#22810;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.05379</link><description>&lt;p&gt;
TASTY&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26102;&#31354;&#22797;&#26434;&#24230;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TASTY: A Transformer based Approach to Space and Time complexitY. (arXiv:2305.05379v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#20195;&#30721;&#29255;&#27573;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#20197;&#22635;&#34917;&#20174;&#20195;&#30721;&#20013;&#20998;&#31867;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24615;&#30340;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#20195;&#30721;&#30340;&#22810;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#38750;&#24120;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#22914;&#20195;&#30721;&#30340;&#23436;&#21892;&#12289;&#20195;&#30721;&#30340;&#34917;&#20840;&#21644;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#25968;&#25454;&#38598;&#65292;&#20174;&#20195;&#30721;&#20013;&#20998;&#31867;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24615;&#30340;&#20219;&#21153;&#36824;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#65292;&#20808;&#21069;&#30340;&#21162;&#21147;&#20165;&#38480;&#20110;Java&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#20195;&#30721;&#29255;&#27573;&#26631;&#35760;&#25968;&#25454;&#38598;&#26469;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65288;&#30446;&#21069;&#26159;Python&#21644;C ++&#25968;&#25454;&#38598;&#65292;&#19981;&#20037;&#23558;&#21457;&#24067;C&#65292;C&#65283;&#21644;JavaScript&#25968;&#25454;&#38598;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#35745;&#31639;&#24211;&#21644;&#24037;&#20855;&#20165;&#36866;&#29992;&#20110;&#23569;&#25968;&#29992;&#20363;&#12290;&#32570;&#20047;&#26126;&#30830;&#23450;&#20041;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#20419;&#20351;&#36816;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#20195;&#30721;&#30340;&#22810;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27515;&#20195;&#30721;&#28040;&#38500;&#21644;&#22686;&#21152;LM&#30340;&#26368;&#22823;&#24207;&#21015;&#38271;&#24230;&#30340;&#26377;&#25928;&#24615;&#12290;&#38500;&#20102;&#26102;&#38388;&#22797;&#26434;&#24615;&#22806;&#65292;&#25105;&#20204;&#36824;&#24314;&#35758;&#20351;&#29992;LM&#26469;&#23547;&#25214;&#31354;&#38388;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code based Language Models (LMs) have shown very promising results in the field of software engineering with applications such as code refinement, code completion and generation. However, the task of time and space complexity classification from code has not been extensively explored due to a lack of datasets, with prior endeavors being limited to Java. In this project, we aim to address these gaps by creating a labelled dataset of code snippets spanning multiple languages (Python and C++ datasets currently, with C, C#, and JavaScript datasets being released shortly). We find that existing time complexity calculation libraries and tools only apply to a limited number of use-cases. The lack of a well-defined rule based system motivates the application of several recently proposed code-based LMs. We demonstrate the effectiveness of dead code elimination and increasing the maximum sequence length of LMs. In addition to time complexity, we propose to use LMs to find space complexities from
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#30340; URL &#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#32593;&#32476;&#20869;&#23481;&#36807;&#28388;&#65292;&#20854;&#23398;&#29983;&#27169;&#22411;&#22312;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569; 175 &#20493;&#30340;&#24773;&#20917;&#19979;&#65292;&#31934;&#24230;&#25552;&#21319;&#20102; 9%&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05027</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#30340;&#32593;&#32476;&#20869;&#23481;&#36807;&#28388;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Web Content Filtering through knowledge distillation of Large Language Models. (arXiv:2305.05027v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#30340; URL &#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#32593;&#32476;&#20869;&#23481;&#36807;&#28388;&#65292;&#20854;&#23398;&#29983;&#27169;&#22411;&#22312;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569; 175 &#20493;&#30340;&#24773;&#20917;&#19979;&#65292;&#31934;&#24230;&#25552;&#21319;&#20102; 9%&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340; URL &#20998;&#31867;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#32593;&#32476;&#20869;&#23481;&#36807;&#28388;&#30340;&#20027;&#35201;&#30446;&#26631;&#65306;&#20445;&#38556;&#32452;&#32455;&#20813;&#21463;&#27861;&#24459;&#21644;&#20262;&#29702;&#39118;&#38505;&#65292;&#38480;&#21046;&#35775;&#38382;&#39640;&#39118;&#38505;&#25110;&#21487;&#30097;&#32593;&#31449;&#65292;&#20197;&#21450;&#20419;&#36827;&#23433;&#20840;&#30340;&#19987;&#19994;&#24037;&#20316;&#29615;&#22659;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20934;&#30830;&#30340;&#20998;&#31867;&#65292;&#24182;&#21033;&#29992;&#24050;&#26377;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#21019;&#24314;&#26356;&#23567;&#12289;&#26356;&#19987;&#19994;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#20197;&#29992;&#20110;&#32593;&#32476;&#20869;&#23481;&#36807;&#28388;&#12290;&#22312;&#23558;&#36890;&#36807;&#22823;&#22411;&#23433;&#20840;&#20379;&#24212;&#21830;&#25910;&#38598;&#30340;&#23458;&#25143;&#36965;&#27979;&#25968;&#25454;&#30340; 30 &#20010;&#19981;&#21516;&#20869;&#23481;&#31867;&#21035;&#30340;&#32593;&#31449;&#36827;&#34892;&#20998;&#31867;&#30340;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#23398;&#29983;&#27169;&#22411;&#36890;&#36807;&#33976;&#39311;&#32467;&#26524;&#23454;&#29616;&#20102; 9% &#30340;&#20998;&#31867;&#31934;&#24230;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23398;&#29983;&#27169;&#22411;&#22312;&#21442;&#25968;&#25968;&#37327;&#19978;&#19982;&#21407;&#22987;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#20943;&#23569;&#20102; 175 &#20493;&#65292;&#20174;&#32780;&#36798;&#21040;&#20102;&#19982;&#32769;&#24072;&#27169;&#22411;&#30456;&#21305;&#37197;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#29992;&#20110;&#22823;&#35268;&#27169;&#30340;&#22312;&#32447;&#25195;&#25551;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a state-of-the-art approach for URL categorization that leverages the power of Large Language Models (LLMs) to address the primary objectives of web content filtering: safeguarding organizations from legal and ethical risks, limiting access to high-risk or suspicious websites, and fostering a secure and professional work environment. Our method utilizes LLMs to generate accurate classifications and then employs established knowledge distillation techniques to create smaller, more specialized student models tailored for web content filtering. Distillation results in a student model with a 9\% accuracy rate improvement in classifying websites, sourced from customer telemetry data collected by a large security vendor, into 30 distinct content categories based on their URLs, surpassing the current state-of-the-art approach. Our student model matches the performance of the teacher LLM with 175 times less parameters, allowing the model to be used for in-line scanning of large vo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#34920;&#31034;AST&#65292;&#24182;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22270;&#24418;&#36827;&#34892;&#20195;&#30721;&#20998;&#31867;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.04228</link><description>&lt;p&gt;
&#22522;&#20110;&#25277;&#35937;&#35821;&#27861;&#26641;&#30340;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20195;&#30721;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Directed Hypergraph Neural Network over abstract syntax tree (AST) for Code Classification. (arXiv:2305.04228v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#34920;&#31034;AST&#65292;&#24182;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22270;&#24418;&#36827;&#34892;&#20195;&#30721;&#20998;&#31867;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#20998;&#31867;&#26159;&#31243;&#24207;&#29702;&#35299;&#21644;&#33258;&#21160;&#32534;&#30721;&#20013;&#30340;&#19968;&#20010;&#38590;&#39064;&#12290;&#30001;&#20110;&#31243;&#24207;&#30340;&#27169;&#31946;&#35821;&#27861;&#21644;&#22797;&#26434;&#35821;&#20041;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#25216;&#26415;&#21019;&#24314;&#20195;&#30721;&#34920;&#31034;&#29992;&#20110;&#20195;&#30721;&#20998;&#31867;&#12290;&#36825;&#20123;&#25216;&#26415;&#21033;&#29992;&#20195;&#30721;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#20294;&#21482;&#32771;&#34385;&#33410;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;AST&#20013;&#33410;&#28857;&#20043;&#38388;&#24050;&#32463;&#23384;&#22312;&#30340;&#39640;&#38454;&#30456;&#20851;&#24615;&#65292;&#21487;&#33021;&#23548;&#33268;&#20195;&#30721;&#32467;&#26500;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#65288;HDHG&#65289;&#34920;&#31034;AST&#65292;&#24182;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HDHGN&#65289;&#22788;&#29702;&#22270;&#24418;&#12290;HDHG&#20445;&#30041;&#20102;&#33410;&#28857;&#20043;&#38388;&#30340;&#39640;&#38454;&#30456;&#20851;&#24615;&#65292;&#24182;&#26356;&#20840;&#38754;&#22320;&#32534;&#30721;&#20102;AST&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#12290;HDHGN&#36890;&#36807;&#32858;&#21512;&#19981;&#21516;&#33410;&#28857;&#30340;&#29305;&#24449;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#22788;&#29702;&#26469;&#23545;AST&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;HDHG&#21644;HDHGN&#22312;&#20195;&#30721;&#20998;&#31867;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code classification is a difficult issue in program understanding and automatic coding. Due to the elusive syntax and complicated semantics in programs, most existing studies use techniques based on abstract syntax tree (AST) and graph neural network (GNN) to create code representations for code classification. These techniques utilize the structure and semantic information of the code, but they only take into account pairwise associations and neglect the high-order correlations that already exist between nodes in the AST, which may result in the loss of code structural information. On the other hand, while a general hypergraph can encode high-order data correlations, it is homogeneous and undirected which will result in a lack of semantic and structural information such as node types, edge types, and directions between child nodes and parent nodes when modeling AST. In this study, we propose to represent AST as a heterogeneous directed hypergraph (HDHG) and process the graph by hetero
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#23454;&#38469;&#39046;&#22495;&#20013;&#31163;&#32447;&#35757;&#32451;&#25110;&#22686;&#38271;&#25209;&#37327;&#35757;&#32451;&#30340;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;&#21521;&#23398;&#20064;&#32773;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#25968;&#25454;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.03870</link><description>&lt;p&gt;
&#22312;&#22686;&#38271;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#20013;&#25945;&#24072;&#21521;&#23398;&#20064;&#32773;&#30340;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Knowledge Transfer from Teachers to Learners in Growing-Batch Reinforcement Learning. (arXiv:2305.03870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#23454;&#38469;&#39046;&#22495;&#20013;&#31163;&#32447;&#35757;&#32451;&#25110;&#22686;&#38271;&#25209;&#37327;&#35757;&#32451;&#30340;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;&#21521;&#23398;&#20064;&#32773;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#25968;&#25454;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#36143;&#20915;&#31574;&#21046;&#23450;&#30340;&#26631;&#20934;&#26041;&#27861;&#21033;&#29992;&#20102;&#26234;&#33021;&#20307;&#19981;&#26029;&#19982;&#20854;&#29615;&#22659;&#20132;&#20114;&#21644;&#25913;&#21892;&#20854;&#25511;&#21046;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23433;&#20840;&#12289;&#20262;&#29702;&#21644;&#23454;&#29992;&#24615;&#30340;&#38480;&#21046;&#65292;&#36825;&#31181;&#35797;&#38169;&#23454;&#39564;&#26041;&#27861;&#22312;&#35768;&#22810;&#23454;&#38469;&#39046;&#22495;&#65288;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#65289;&#20013;&#24448;&#24448;&#19981;&#21487;&#34892;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#39046;&#22495;&#20013;&#30340;&#25511;&#21046;&#31574;&#30053;&#36890;&#24120;&#26159;&#36890;&#36807;&#20197;&#20808;&#21069;&#35760;&#24405;&#30340;&#25968;&#25454;&#20026;&#22522;&#30784;&#36827;&#34892;&#31163;&#32447;&#35757;&#32451;&#25110;&#36880;&#27493;&#25193;&#23637;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#22266;&#23450;&#30340;&#31574;&#30053;&#34987;&#37096;&#32626;&#21040;&#29615;&#22659;&#20013;&#65292;&#24182;&#29992;&#20110;&#25910;&#38598;&#19968;&#25972;&#20010;&#25209;&#27425;&#30340;&#26032;&#25968;&#25454;&#65292;&#28982;&#21518;&#19982;&#36807;&#21435;&#30340;&#25209;&#27425;&#27719;&#24635;&#24182;&#29992;&#20110;&#26356;&#26032;&#31574;&#30053;&#12290;&#21487;&#20197;&#22810;&#27425;&#37325;&#22797;&#36825;&#20010;&#25913;&#36827;&#21608;&#26399;&#12290;&#34429;&#28982;&#22312;&#23454;&#38469;&#39046;&#22495;&#20013;&#26377;&#38480;&#25968;&#37327;&#30340;&#36825;&#26679;&#30340;&#21608;&#26399;&#26159;&#21487;&#34892;&#30340;&#65292;&#20294;&#20135;&#29983;&#30340;&#25968;&#25454;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#36828;&#36828;&#20302;&#20110;&#26631;&#20934;&#30340;&#19981;&#26029;&#20132;&#20114;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#36890;&#24120;&#26159;&#19982;&#20154;&#31867;&#19987;&#23478;&#21327;&#20316;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard approaches to sequential decision-making exploit an agent's ability to continually interact with its environment and improve its control policy. However, due to safety, ethical, and practicality constraints, this type of trial-and-error experimentation is often infeasible in many real-world domains such as healthcare and robotics. Instead, control policies in these domains are typically trained offline from previously logged data or in a growing-batch manner. In this setting a fixed policy is deployed to the environment and used to gather an entire batch of new data before being aggregated with past batches and used to update the policy. This improvement cycle can then be repeated multiple times. While a limited number of such cycles is feasible in real-world domains, the quantity and diversity of the resulting data are much lower than in the standard continually-interacting approach. However, data collection in these domains is often performed in conjunction with human expert
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#19977;&#20010;&#28145;&#24230;&#23398;&#20064;&#24211;&#20013;&#23454;&#29616;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#24211;&#23545;&#22312;&#32447;&#33258;&#36866;&#24212;&#36731;&#37327;&#32423;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.00595</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24211;&#23545;&#22312;&#32447;&#33258;&#36866;&#24212;&#36731;&#37327;&#32423;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Deep Learning Libraries on Online Adaptive Lightweight Time Series Anomaly Detection. (arXiv:2305.00595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#19977;&#20010;&#28145;&#24230;&#23398;&#20064;&#24211;&#20013;&#23454;&#29616;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#24211;&#23545;&#22312;&#32447;&#33258;&#36866;&#24212;&#36731;&#37327;&#32423;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#20154;&#20026;&#24178;&#39044;&#21644;&#39046;&#22495;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#22312;&#32447;&#33258;&#36866;&#24212;&#36731;&#37327;&#32423;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#36807;&#21435;&#20960;&#24180;&#20013;&#24341;&#20837;&#20102;&#20960;&#31181;&#36825;&#26679;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#37117;&#20165;&#22312;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#24211;&#20013;&#23454;&#29616;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#24211;&#30340;&#21457;&#23637;&#65292;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#22914;&#20309;&#24433;&#21709;&#36825;&#20123;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#23578;&#19981;&#28165;&#26970;&#65292;&#22240;&#20026;&#27809;&#26377;&#36825;&#26679;&#30340;&#35780;&#20272;&#12290;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#24211;&#26469;&#23454;&#29616;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21487;&#33021;&#19981;&#33021;&#23637;&#29616;&#20986;&#35813;&#26041;&#27861;&#30340;&#30495;&#23454;&#24615;&#33021;&#65292;&#36825;&#21487;&#33021;&#20250;&#35823;&#23548;&#29992;&#25143;&#30456;&#20449;&#26576;&#31181;&#26041;&#27861;&#27604;&#21478;&#19968;&#31181;&#26356;&#22909;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#19977;&#20010;&#30693;&#21517;&#28145;&#24230;&#23398;&#20064;&#24211;&#20013;&#23454;&#29616;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#32467;&#26524;&#65292;&#26469;&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#24211;&#23545;&#22312;&#32447;&#33258;&#36866;&#24212;&#36731;&#37327;&#32423;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing online adaptive lightweight time series anomaly detection without human intervention and domain knowledge is highly valuable. Several such anomaly detection approaches have been introduced in the past years, but all of them were only implemented in one deep learning library. With the development of deep learning libraries, it is unclear how different deep learning libraries impact these anomaly detection approaches since there is no such evaluation available. Randomly choosing a deep learning library to implement an anomaly detection approach might not be able to show the true performance of the approach. It might also mislead users in believing one approach is better than another. Therefore, in this paper, we investigate the impact of deep learning libraries on online adaptive lightweight time series anomaly detection by implementing two state-of-the-art anomaly detection approaches in three well-known deep learning libraries and evaluating how these two approaches are indiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;AdaNPC&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#21442;&#25968;&#20998;&#31867;&#22120;&#36827;&#34892;&#24314;&#27169;&#20174;&#32780;&#36991;&#20813;&#20102;&#31163;&#32447;&#30446;&#26631;&#25968;&#25454;&#25110;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#39069;&#22806;&#30340;&#22797;&#26434;&#20248;&#21270;&#36807;&#31243;&#12290;AdaNPC&#20174;&#23384;&#20648;&#22120;&#20013;&#22238;&#39038;&#26368;&#30456;&#20284;&#30340; K &#20010;&#26679;&#26412;&#36827;&#34892;&#25237;&#31080;&#39044;&#27979;&#65292;&#36880;&#28176;&#25913;&#21464;&#23384;&#20648;&#22120;&#20013;&#30340;&#26679;&#26412;&#20998;&#24067;&#20197;&#25552;&#39640;&#27979;&#35797;&#22495;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12566</link><description>&lt;p&gt;
AdaNPC&#65306;&#25506;&#32034;&#38750;&#21442;&#25968;&#20998;&#31867;&#22120;&#36827;&#34892;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
AdaNPC: Exploring Non-Parametric Classifier for Test-Time Adaptation. (arXiv:2304.12566v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;AdaNPC&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#21442;&#25968;&#20998;&#31867;&#22120;&#36827;&#34892;&#24314;&#27169;&#20174;&#32780;&#36991;&#20813;&#20102;&#31163;&#32447;&#30446;&#26631;&#25968;&#25454;&#25110;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#39069;&#22806;&#30340;&#22797;&#26434;&#20248;&#21270;&#36807;&#31243;&#12290;AdaNPC&#20174;&#23384;&#20648;&#22120;&#20013;&#22238;&#39038;&#26368;&#30456;&#20284;&#30340; K &#20010;&#26679;&#26412;&#36827;&#34892;&#25237;&#31080;&#39044;&#27979;&#65292;&#36880;&#28176;&#25913;&#21464;&#23384;&#20648;&#22120;&#20013;&#30340;&#26679;&#26412;&#20998;&#24067;&#20197;&#25552;&#39640;&#27979;&#35797;&#22495;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#37117;&#38598;&#20013;&#22312;&#24320;&#21457;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#20998;&#24067;&#30340;&#27169;&#22411;&#19978;&#12290;&#22495;&#36890;&#29992;&#24615;&#65288;DG&#65289;&#24050;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#35838;&#39064;&#20043;&#19968;&#12290;&#20960;&#31687;&#25991;&#29486;&#34920;&#26126;&#65292;&#22914;&#26524;&#19981;&#21033;&#29992;&#30446;&#26631;&#22495;&#30340;&#20449;&#24687;&#65292;&#22495;&#36890;&#29992;&#24615;&#21487;&#33021;&#20250;&#21464;&#24471;&#26497;&#20854;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;TTA&#26041;&#27861;&#38656;&#35201;&#31163;&#32447;&#30446;&#26631;&#25968;&#25454;&#25110;&#22312;&#25512;&#29702;&#38454;&#27573;&#20351;&#29992;&#39069;&#22806;&#30340;&#22797;&#26434;&#20248;&#21270;&#36807;&#31243;&#12290;&#26412;&#25991;&#37319;&#29992;&#38750;&#21442;&#25968;&#20998;&#31867;&#22120;&#36827;&#34892;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;AdaNPC&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#29305;&#24449;&#21644;&#26631;&#31614;&#23545;&#30340;&#23384;&#20648;&#22120;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#32473;&#23450;&#19968;&#20010;&#27979;&#35797;&#23454;&#20363;&#65292;AdaNPC&#39318;&#20808;&#20174;&#23384;&#20648;&#22120;&#20013;&#22238;&#39038;K&#20010;&#26368;&#30456;&#20284;&#30340;&#26679;&#26412;&#36827;&#34892;&#25237;&#31080;&#39044;&#27979;&#65292;&#28982;&#21518;&#23558;&#27979;&#35797;&#29305;&#24449;&#21644;&#39044;&#27979;&#26631;&#31614;&#28155;&#21152;&#21040;&#23384;&#20648;&#22120;&#20013;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#23384;&#20648;&#22120;&#20013;&#30340;&#26679;&#26412;&#20998;&#24067;&#21487;&#20197;&#36880;&#28176;&#20174;&#35757;&#32451;&#20998;&#24067;&#21521;&#27979;&#35797;&#20998;&#24067;&#21464;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#27979;&#35797;&#22495;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;TTA&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#19982;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent machine learning tasks focus to develop models that can generalize to unseen distributions. Domain generalization (DG) has become one of the key topics in various fields. Several literatures show that DG can be arbitrarily hard without exploiting target domain information. To address this issue, test-time adaptive (TTA) methods are proposed. Existing TTA methods require offline target data or extra sophisticated optimization procedures during the inference stage. In this work, we adopt Non-Parametric Classifier to perform the test-time Adaptation (AdaNPC). In particular, we construct a memory that contains the feature and label pairs from training domains. During inference, given a test instance, AdaNPC first recalls K closed samples from the memory to vote for the prediction, and then the test feature and predicted label are added to the memory. In this way, the sample distribution in the memory can be gradually changed from the training distribution towards the test distr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24182;&#34892;bootstrap&#30340;on-policy&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37096;&#20998;&#36712;&#36857;&#32531;&#20914;&#21306;&#21644;&#36820;&#22238;bootstrapping&#27493;&#39588;&#26469;&#23454;&#29616;&#28789;&#27963;&#20351;&#29992;&#24182;&#34892;&#29615;&#22659;&#65292;&#21516;&#26102;&#20445;&#25345;&#26356;&#26032;&#30340;on-policy&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#36830;&#32493;&#27969;&#25511;&#21046;&#38382;&#39064;&#19978;&#26377;&#24456;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.12330</link><description>&lt;p&gt;
&#22522;&#20110;&#24182;&#34892;bootstrap&#30340;&#36830;&#32493;&#27969;&#25511;&#21046;&#24212;&#29992;&#30340;on-policy&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Parallel bootstrap-based on-policy deep reinforcement learning for continuous flow control applications. (arXiv:2304.12330v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24182;&#34892;bootstrap&#30340;on-policy&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37096;&#20998;&#36712;&#36857;&#32531;&#20914;&#21306;&#21644;&#36820;&#22238;bootstrapping&#27493;&#39588;&#26469;&#23454;&#29616;&#28789;&#27963;&#20351;&#29992;&#24182;&#34892;&#29615;&#22659;&#65292;&#21516;&#26102;&#20445;&#25345;&#26356;&#26032;&#30340;on-policy&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#36830;&#32493;&#27969;&#25511;&#21046;&#38382;&#39064;&#19978;&#26377;&#24456;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19982;&#25968;&#20540;&#27969;&#25511;&#38382;&#39064;&#30340;&#32806;&#21512;&#36817;&#26399;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#25104;&#26524;&#24182;&#20026;&#35813;&#39046;&#22495;&#24320;&#36767;&#20102;&#26032;&#30340;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27969;&#20307;&#21160;&#21147;&#23398;&#27714;&#35299;&#22120;&#30340;&#35745;&#31639;&#25104;&#26412;&#36890;&#24120;&#24456;&#39640;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20351;&#29992;&#24182;&#34892;&#29615;&#22659;&#26159;&#23454;&#29616;&#26377;&#25928;&#25511;&#21046;&#30340;&#24517;&#35201;&#25163;&#27573;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#27969;&#25511;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25991;&#29486;&#20173;&#20381;&#36182;&#20110;on-policy&#31639;&#27861;&#65292;&#32780;&#36825;&#31181;&#31639;&#27861;&#30340;&#39640;&#24182;&#34892;&#36716;&#31227;&#25910;&#38598;&#21487;&#33021;&#20250;&#30772;&#22351;&#29702;&#35770;&#20551;&#35774;&#24182;&#23548;&#33268;&#27425;&#20248;&#30340;&#25511;&#21046;&#27169;&#22411;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#37096;&#20998;&#36712;&#36857;&#32531;&#20914;&#21306;&#30340;&#24182;&#34892;&#27169;&#24335;&#65292;&#36890;&#36807;&#19968;&#20010;&#36820;&#22238;bootstrapping&#27493;&#39588;&#65292;&#20801;&#35768;&#28789;&#27963;&#22320;&#20351;&#29992;&#24182;&#34892;&#29615;&#22659;&#65292;&#21516;&#26102;&#20445;&#25345;&#26356;&#26032;&#30340;on-policy&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#25991;&#29486;&#20013;&#19968;&#20010;&#32791;&#36153;&#22823;&#37327;&#35745;&#31639;&#30340;&#36830;&#32493;&#27969;&#25511;&#21046;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
The coupling of deep reinforcement learning to numerical flow control problems has recently received a considerable attention, leading to groundbreaking results and opening new perspectives for the domain. Due to the usually high computational cost of fluid dynamics solvers, the use of parallel environments during the learning process represents an essential ingredient to attain efficient control in a reasonable time. Yet, most of the deep reinforcement learning literature for flow control relies on on-policy algorithms, for which the massively parallel transition collection may break theoretical assumptions and lead to suboptimal control models. To overcome this issue, we propose a parallelism pattern relying on partial-trajectory buffers terminated by a return bootstrapping step, allowing a flexible use of parallel environments while preserving the on-policiness of the updates. This approach is illustrated on a CPU-intensive continuous flow control problem from the literature.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;Python&#26694;&#26550;&#19979;&#30340;&#24211;PyLogik&#26469;&#24110;&#21161;&#36229;&#22768;&#22270;&#20687;&#21435;&#26631;&#35782;&#21270;&#21644;&#28165;&#27927;&#21387;&#32553;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#25454;&#20849;&#20139;&#24212;&#29992;&#25552;&#20379;&#22270;&#20687;&#25968;&#25454;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2304.12322</link><description>&lt;p&gt;
&#20351;&#29992;Pylogik&#36827;&#34892;&#21307;&#23398;&#24433;&#20687;&#21435;&#26631;&#35782;&#21270;&#21644;&#28165;&#27927;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Medical Image Deidentification, Cleaning and Compression Using Pylogik. (arXiv:2304.12322v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12322
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;Python&#26694;&#26550;&#19979;&#30340;&#24211;PyLogik&#26469;&#24110;&#21161;&#36229;&#22768;&#22270;&#20687;&#21435;&#26631;&#35782;&#21270;&#21644;&#28165;&#27927;&#21387;&#32553;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#25454;&#20849;&#20139;&#24212;&#29992;&#25552;&#20379;&#22270;&#20687;&#25968;&#25454;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#22823;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#35760;&#24405;&#20449;&#24687;&#26041;&#38754;&#39035;&#27880;&#24847;&#65292;&#24517;&#39035;&#28165;&#27927;&#21644;&#21435;&#26631;&#35782;&#21270;&#25968;&#25454;&#12290;&#24403;&#21463;&#20445;&#25252;&#30340;&#20581;&#24247;&#20449;&#24687;&#23884;&#20837;&#22312;&#24433;&#20687;&#20803;&#25968;&#25454;&#20013;&#26102;&#65292;&#20419;&#36827;&#22810;&#20013;&#24515;&#21512;&#20316;&#20013;&#25968;&#25454;&#20849;&#20139;&#21644;&#21327;&#35843;&#21464;&#24471;&#23588;&#20854;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Python&#26694;&#26550;&#19979;&#30340;&#24211;&#65292;&#31216;&#20026;PyLogik&#65292;&#24110;&#21161;&#35299;&#20915;&#36229;&#22768;&#22270;&#20687;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#28165;&#27927;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20123;&#22270;&#20687;&#30452;&#25509;&#21253;&#21547;&#24456;&#22810;PHI&#12290;PyLogik&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#25991;&#26412;&#26816;&#27979;/&#25552;&#21462;&#12289;&#36807;&#28388;&#12289;&#38408;&#20540;&#21270;&#12289;&#24418;&#24577;&#23398;&#21644;&#36718;&#24275;&#27604;&#36739;&#22788;&#29702;&#22270;&#20687;&#20307;&#31215;&#12290;&#36825;&#31181;&#26041;&#27861;&#21435;&#26631;&#35782;&#21270;&#22270;&#20687;&#65292;&#20943;&#23567;&#25991;&#20214;&#22823;&#23567;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#25454;&#20849;&#20139;&#24212;&#29992;&#20934;&#22791;&#22909;&#20102;&#22270;&#20687;&#25968;&#25454;&#12290;&#20026;&#20102;&#35780;&#20272;PyLogik&#22312;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#30340;&#35782;&#21035;&#26377;&#25928;&#24615;&#65292;&#38543;&#26426;&#25277;&#21462;&#20102;50&#24352;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#65288;&#36229;&#22768;&#24515;&#21160;&#22270;&#65289;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging medical record information in the era of big data and machine learning comes with the caveat that data must be cleaned and deidentified. Facilitating data sharing and harmonization for multi-center collaborations are particularly difficult when protected health information (PHI) is contained or embedded in image meta-data. We propose a novel library in the Python framework, called PyLogik, to help alleviate this issue for ultrasound images, which are particularly challenging because of the frequent inclusion of PHI directly on the images. PyLogik processes the image volumes through a series of text detection/extraction, filtering, thresholding, morphological and contour comparisons. This methodology deidentifies the images, reduces file sizes, and prepares image volumes for applications in deep learning and data sharing. To evaluate its effectiveness in the identification of regions of interest (ROI), a random sample of 50 cardiac ultrasounds (echocardiograms) were processed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;(TSDiff)&#65292;&#29992;&#20110;&#20174;&#20108;&#32500;&#20998;&#23376;&#22270;&#20013;&#39044;&#27979;&#36807;&#28193;&#24577;&#20960;&#20309;&#32467;&#26500;&#12290;&#19982;&#29616;&#26377;&#20855;&#26377; 3D &#20960;&#20309;&#32467;&#26500;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;TSdiff &#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#37117;&#26356;&#39640;&#12290;TSDiff &#33021;&#22815;&#25214;&#21040;&#27604;&#21442;&#32771;&#25968;&#25454;&#24211;&#26356;&#20248;&#21270;&#30340;&#21453;&#24212;&#36884;&#24452;&#65292;&#22312;&#21453;&#24212;&#21183;&#22418;&#26356;&#20302;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#26356;&#20026;&#20248;&#21270;&#30340;&#21453;&#24212;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2304.12233</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#25506;&#32034;&#20108;&#32500;&#20998;&#23376;&#22270;&#30340;&#36807;&#28193;&#24577;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based Generative AI for Exploring Transition States from 2D Molecular Graphs. (arXiv:2304.12233v2 [physics.chem-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;(TSDiff)&#65292;&#29992;&#20110;&#20174;&#20108;&#32500;&#20998;&#23376;&#22270;&#20013;&#39044;&#27979;&#36807;&#28193;&#24577;&#20960;&#20309;&#32467;&#26500;&#12290;&#19982;&#29616;&#26377;&#20855;&#26377; 3D &#20960;&#20309;&#32467;&#26500;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;TSdiff &#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#37117;&#26356;&#39640;&#12290;TSDiff &#33021;&#22815;&#25214;&#21040;&#27604;&#21442;&#32771;&#25968;&#25454;&#24211;&#26356;&#20248;&#21270;&#30340;&#21453;&#24212;&#36884;&#24452;&#65292;&#22312;&#21453;&#24212;&#21183;&#22418;&#26356;&#20302;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#26356;&#20026;&#20248;&#21270;&#30340;&#21453;&#24212;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#31350;&#36807;&#28193;&#24577;&#20960;&#20309;&#32467;&#26500;&#23545;&#20110;&#38416;&#26126;&#21270;&#23398;&#21453;&#24212;&#26426;&#29702;&#21644;&#27169;&#25311;&#21453;&#24212;&#21160;&#21147;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39044;&#27979;&#36807;&#28193;&#24577;&#20960;&#20309;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#21453;&#24212;&#29289;&#21644;&#20135;&#29289;&#30340; 3D &#24418;&#24577;&#21644;&#26041;&#21521;&#20316;&#20026;&#36755;&#20837;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#21162;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25193;&#25955;&#26041;&#27861;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;(TSDiff)&#65292;&#29992;&#20110;&#20174;&#20108;&#32500;&#20998;&#23376;&#22270;&#20013;&#39044;&#27979;&#36807;&#28193;&#24577;&#20960;&#20309;&#32467;&#26500;&#12290;TSDiff&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#20855;&#26377; 3D &#20960;&#20309;&#32467;&#26500;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#20174;&#35757;&#32451;&#20013;&#20102;&#35299;&#21040;&#21508;&#31181;&#21453;&#24212;&#30340;&#36807;&#28193;&#24577;&#20960;&#20309;&#20998;&#24067;&#65292;&#20174;&#32780;&#33021;&#22815;&#37319;&#26679;&#21508;&#31181;&#36807;&#28193;&#24577;&#26500;&#35937;&#12290;&#22240;&#27492;&#65292;TSDiff &#33021;&#22815;&#25214;&#21040;&#27604;&#21442;&#32771;&#25968;&#25454;&#24211;&#20013;&#26356;&#20026;&#26377;&#21033;&#30340;&#21453;&#24212;&#36884;&#24452;&#65292;&#22312;&#21453;&#24212;&#21183;&#22418;&#26356;&#20302;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#26356;&#20026;&#20248;&#21270;&#30340;&#21453;&#24212;&#36335;&#24452;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;TSDiff &#22312;&#21152;&#36895;&#21270;&#23398;&#21453;&#24212;&#21644;&#21453;&#24212;&#36884;&#24452;&#30340;&#21457;&#29616;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exploration of transition state (TS) geometries is crucial for elucidating chemical reaction mechanisms and modeling their kinetics. Recently, machine learning (ML) models have shown remarkable performance for prediction of TS geometries. However, they require 3D conformations of reactants and products often with their appropriate orientations as input, which demands substantial efforts and computational cost. Here, we propose a generative approach based on the stochastic diffusion method, namely TSDiff, for prediction of TS geometries just from 2D molecular graphs. TSDiff outperformed the existing ML models with 3D geometries in terms of both accuracy and efficiency. Moreover, it enables to sample various TS conformations, because it learned the distribution of TS geometries for diverse reactions in training. Thus, TSDiff was able to find more favorable reaction pathways with lower barrier heights than those in the reference database. These results demonstrate that TSDiff shows pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#21516;&#24577;&#21152;&#23494;&#25216;&#26415;&#19979;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#36716;&#25442;&#24605;&#24819;&#21644;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#20307;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03807</link><description>&lt;p&gt;
&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving CNN Training with Transfer Learning. (arXiv:2304.03807v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#21516;&#24577;&#21152;&#23494;&#25216;&#26415;&#19979;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#36716;&#25442;&#24605;&#24819;&#21644;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#20307;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#24050;&#32463;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#21516;&#26102;&#20445;&#25345;&#21516;&#24577;CNN&#35757;&#32451;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#23454;&#29616;&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#25216;&#26415;&#30340;&#38544;&#31169;&#20445;&#25252;CNN&#35757;&#32451;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#25104;&#21151;&#31361;&#30772;&#36825;&#20010;&#38590;&#39064;&#65292;&#20197;&#21069;&#27809;&#26377;&#20219;&#20309;&#24037;&#20316;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#12290;&#37319;&#29992;&#20102;&#20960;&#31181;&#25216;&#26415;&#65306;&#65288;1&#65289;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#65292;&#21487;&#20197;&#23558;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;&#31616;&#21270;&#20026;&#21516;&#24577;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#29978;&#33267;&#26159;&#22810;&#31867;&#36923;&#36753;&#22238;&#24402;&#65288;MLR&#65289;&#35757;&#32451;&#65307;&#65288;2&#65289;&#36890;&#36807;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#20307;$\texttt{Quadratic Gradient}$&#65292;&#24212;&#29992;&#20110;MLR&#30340;&#22686;&#24378;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65307;&#65288;3&#65289;&#25105;&#20204;&#37319;&#29992;&#25968;&#23398;&#20013;&#30340;&#21464;&#25442;&#24605;&#24819;&#65292;&#23558;&#21152;&#23494;&#22495;&#20013;&#30340;&#36817;&#20284;Softmax&#20989;&#25968;&#36716;&#25442;&#25104;&#24050;&#32463;&#30740;&#31350;&#36807;&#30340;&#36924;&#36817;&#26041;&#27861;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving nerual network inference has been well studied while homomorphic CNN training still remains an open challenging task. In this paper, we present a practical solution to implement privacy-preserving CNN training based on mere Homomorphic Encryption (HE) technique. To our best knowledge, this is the first attempt successfully to crack this nut and no work ever before has achieved this goal. Several techniques combine to make it done: (1) with transfer learning, privacy-preserving CNN training can be reduced to homomorphic neural network training, or even multiclass logistic regression (MLR) training; (2) via a faster gradient variant called $\texttt{Quadratic Gradient}$, an enhanced gradient method for MLR with a state-of-the-art performance in converge speed is applied in this work to achieve high performance; (3) we employ the thought of transformation in mathematics to transform approximating Softmax function in encryption domain to the well-studied approximation of 
&lt;/p&gt;</description></item><item><title>&#24314;&#35758;&#23558;&#25512;&#26029;&#32593;&#32476;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#25972;&#21512;&#21040;&#27491;&#21017;&#21270;&#24378;&#24230;&#30340;&#20132;&#21449;&#39564;&#35777;&#20013;&#65292;&#20197;&#25913;&#21892;&#39640;&#26031;&#22270;&#27169;&#22411;&#22312;&#35266;&#23519;&#21547;&#22122;&#25968;&#25454;&#26102;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.16796</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22359;&#21270;&#27491;&#21017;&#21270;&#21487;&#25913;&#21892;&#35266;&#23519;&#21547;&#22122;&#25968;&#25454;&#30340;&#39640;&#26031;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Module-based regularization improves Gaussian graphical models when observing noisy data. (arXiv:2303.16796v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16796
&lt;/p&gt;
&lt;p&gt;
&#24314;&#35758;&#23558;&#25512;&#26029;&#32593;&#32476;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#25972;&#21512;&#21040;&#27491;&#21017;&#21270;&#24378;&#24230;&#30340;&#20132;&#21449;&#39564;&#35777;&#20013;&#65292;&#20197;&#25913;&#21892;&#39640;&#26031;&#22270;&#27169;&#22411;&#22312;&#35266;&#23519;&#21547;&#22122;&#25968;&#25454;&#26102;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#20351;&#29992;&#39640;&#26031;&#22270;&#27169;&#22411;&#34920;&#31034;&#22810;&#21464;&#37327;&#30456;&#20851;&#25968;&#25454;&#20013;&#30340;&#20851;&#31995;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#27491;&#21017;&#21270;&#26469;&#31232;&#30095;&#27169;&#22411;&#12290;&#25105;&#20204;&#24314;&#35758;&#22312;&#27491;&#21017;&#21270;&#24378;&#24230;&#30340;&#20132;&#21449;&#39564;&#35777;&#20013;&#65292;&#23558;&#25512;&#26029;&#32593;&#32476;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#25972;&#21512;&#36215;&#26469;&#20197;&#24179;&#34913;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#12290;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#20351;&#29992;&#39640;&#26031;&#23545;&#25968;&#20284;&#28982;&#36827;&#34892;&#20132;&#21449;&#39564;&#35777;&#30340;&#26631;&#20934;&#26041;&#27861;&#65288;&#22270;&#24418;&#22871;&#32034;&#27861;&#65289;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#24674;&#22797;&#21644;&#25512;&#26029;&#21547;&#22122;&#22768;&#25968;&#25454;&#20013;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers often represent relations in multi-variate correlational data using Gaussian graphical models, which require regularization to sparsify the models. Acknowledging that they often study the modular structure of the inferred network, we suggest integrating it in the cross-validation of the regularization strength to balance under- and overfitting. Using synthetic and real data, we show that this approach allows us to better recover and infer modular structure in noisy data compared with the graphical lasso, a standard approach using the Gaussian log-likelihood when cross-validating the regularization strength.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#24179;&#31283;&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#65292;GD&#21644;SGD&#30340;&#27867;&#21270;&#19979;&#30028;&#21487;&#20197;&#38477;&#20302;&#65292;&#24182;&#19988;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#21487;&#33021;&#23548;&#33268;&#26356;&#24046;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36825;&#19982;&#20854;&#20182;&#30740;&#31350;&#25104;&#26524;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2303.10758</link><description>&lt;p&gt;
&#24179;&#31283;&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;GD&#21644;SGD&#30340;&#27867;&#21270;&#19979;&#30028;&#38477;&#20302;
&lt;/p&gt;
&lt;p&gt;
Lower Generalization Bounds for GD and SGD in Smooth Stochastic Convex Optimization. (arXiv:2303.10758v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#24179;&#31283;&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#65292;GD&#21644;SGD&#30340;&#27867;&#21270;&#19979;&#30028;&#21487;&#20197;&#38477;&#20302;&#65292;&#24182;&#19988;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#21487;&#33021;&#23548;&#33268;&#26356;&#24046;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36825;&#19982;&#20854;&#20182;&#30740;&#31350;&#25104;&#26524;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23398;&#20064;&#29702;&#35770;&#30028;&#22312;&#21051;&#30011;&#19968;&#33324;&#20984;&#25439;&#22833;&#26799;&#24230;&#26041;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#35752;&#35770;&#22312;&#27867;&#21270;&#20809;&#28369;&#38543;&#26426;&#20984;&#20248;&#21270;&#65288;SCO&#65289;&#38382;&#39064;&#20013;&#35757;&#32451;&#26102;&#38388;&#22914;&#20309;&#24433;&#21709;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;&#19968;&#33324;&#30340;&#19981;&#21487;&#23454;&#29616;SCO&#38382;&#39064;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#19978;&#30028;&#32467;&#26524;&#34920;&#26126;&#65292;&#20551;&#35774;&#25439;&#22833;&#21487;&#23454;&#29616;&#65288;&#21363;&#26368;&#20248;&#35299;&#21516;&#26102;&#26368;&#23567;&#21270;&#25152;&#26377;&#25968;&#25454;&#28857;&#65289;&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#20294;&#26159;&#65292;&#24403;&#35757;&#32451;&#26102;&#38388;&#38271;&#19988;&#32570;&#20047;&#19979;&#30028;&#26102;&#65292;&#36825;&#31181;&#25913;&#36827;&#20250;&#21463;&#21040;&#25439;&#23475;&#12290;&#25105;&#20204;&#23545;&#27492;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#23545;&#20110;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22312;&#20004;&#31181;&#21487;&#23454;&#29616;&#24773;&#20917;&#19979;&#30340;&#36807;&#37327;&#39118;&#38505;&#19979;&#30028;&#65306;1&#65289;&#23454;&#29616;&#38656;$T = O(n)$&#65292;&#21644;&#65288;2&#65289;&#23454;&#29616;&#38656;$T = \Omega(n)$&#65292;&#20854;&#20013;$T$&#34920;&#31034;&#35757;&#32451;&#36845;&#20195;&#27425;&#25968;&#65292;$n$&#20026;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#12290;&#36825;&#20123;&#19979;&#30028;&#30340;&#35777;&#26126;&#20351;&#29992;&#20102;&#26469;&#33258;&#20248;&#21270;&#30340;&#29616;&#20195;&#24037;&#20855;&#65292;&#21253;&#25324;&#23545;&#20598;&#29702;&#35770;&#21644;&#38236;&#20687;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21487;&#23454;&#29616;&#30340;SCO&#20013;&#65292;&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#24046;&#30340;&#27867;&#21270;&#65292;&#36825;&#19982;&#25991;&#29486;&#20013;&#30340;&#20808;&#21069;&#21457;&#29616;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#25903;&#25345;&#25105;&#20204;&#32467;&#26524;&#30340;&#25968;&#20540;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress was made in characterizing the generalization error of gradient methods for general convex loss by the learning theory community. In this work, we focus on how training longer might affect generalization in smooth stochastic convex optimization (SCO) problems. We first provide tight lower bounds for general non-realizable SCO problems. Furthermore, existing upper bound results suggest that sample complexity can be improved by assuming the loss is realizable, i.e. an optimal solution simultaneously minimizes all the data points. However, this improvement is compromised when training time is long and lower bounds are lacking. Our paper examines this observation by providing excess risk lower bounds for gradient descent (GD) and stochastic gradient descent (SGD) in two realizable settings: 1) realizable with $T = O(n)$, and (2) realizable with $T = \Omega(n)$, where $T$ denotes the number of training iterations and $n$ is the size of the training dataset. These bounds are 
&lt;/p&gt;</description></item><item><title>HiT-DVAE&#26159;&#19968;&#31181;&#23618;&#27425;Transformer&#21160;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#20248;&#20110;&#20854;&#20182;DVAEs&#22312;&#35821;&#38899;&#39057;&#35889;&#24314;&#27169;&#26041;&#38754;&#12290;&#23427;&#20855;&#26377;&#20004;&#20010;&#28508;&#21464;&#37327;&#27700;&#24179;&#21644;&#31616;&#21333;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#39640;&#30340;&#28508;&#21147;&#22312;&#20302;&#32423;&#21035;&#35821;&#38899;&#22788;&#29702;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2303.09404</link><description>&lt;p&gt;
&#19968;&#31181;&#23618;&#27425;Transformer&#21160;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#35821;&#38899;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Speech Modeling with a Hierarchical Transformer Dynamical VAE. (arXiv:2303.09404v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09404
&lt;/p&gt;
&lt;p&gt;
HiT-DVAE&#26159;&#19968;&#31181;&#23618;&#27425;Transformer&#21160;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#20248;&#20110;&#20854;&#20182;DVAEs&#22312;&#35821;&#38899;&#39057;&#35889;&#24314;&#27169;&#26041;&#38754;&#12290;&#23427;&#20855;&#26377;&#20004;&#20010;&#28508;&#21464;&#37327;&#27700;&#24179;&#21644;&#31616;&#21333;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#39640;&#30340;&#28508;&#21147;&#22312;&#20302;&#32423;&#21035;&#35821;&#38899;&#22788;&#29702;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;DVAEs&#65289;&#26159;&#19968;&#31867;&#28508;&#21464;&#37327;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#25193;&#23637;&#20102;VAE&#20197;&#23545;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#24207;&#21015;&#21644;&#30456;&#24212;&#30340;&#28508;&#21521;&#37327;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#25991;&#29486;&#20013;&#30340;&#20960;&#20046;&#25152;&#26377;DVAEs&#20013;&#65292;&#27599;&#20010;&#24207;&#21015;&#20869;&#37096;&#21644;&#20004;&#20010;&#24207;&#21015;&#20043;&#38388;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#37117;&#30001;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;Hierarchical Transformer DVAE&#65288;HiT-DVAE&#65289;&#27169;&#25311;&#35821;&#38899;&#20449;&#21495;&#65292;&#23427;&#26159;&#19968;&#31181;&#20855;&#26377;&#20004;&#20010;&#28508;&#21464;&#37327;&#27700;&#24179;&#65288;&#24207;&#21015;&#32423;&#21644;&#24103;&#32423;&#65289;&#30340;DVAE&#65292;&#24182;&#19988;&#20854;&#20013;&#26102;&#38388;&#20381;&#36182;&#24615;&#26159;&#20351;&#29992;Transformer&#26550;&#26500;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;HiT-DVAE&#22312;&#35821;&#38899;&#39057;&#35889;&#24314;&#27169;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#33509;&#24178;DVAEs&#65292;&#21516;&#26102;&#36824;&#33021;&#22815;&#23454;&#29616;&#26356;&#31616;&#21333;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#26174;&#31034;&#20986;&#20854;&#22312;&#19979;&#28216;&#20302;&#32423;&#21035;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#65288;&#22914;&#35821;&#38899;&#22686;&#24378;&#65289;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamical variational autoencoders (DVAEs) are a family of latent-variable deep generative models that extends the VAE to model a sequence of observed data and a corresponding sequence of latent vectors. In almost all the DVAEs of the literature, the temporal dependencies within each sequence and across the two sequences are modeled with recurrent neural networks. In this paper, we propose to model speech signals with the Hierarchical Transformer DVAE (HiT-DVAE), which is a DVAE with two levels of latent variable (sequence-wise and frame-wise) and in which the temporal dependencies are implemented with the Transformer architecture. We show that HiT-DVAE outperforms several other DVAEs for speech spectrogram modeling, while enabling a simpler training procedure, revealing its high potential for downstream low-level speech processing tasks such as speech enhancement.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#65292;CNN&#22312;&#20581;&#24247;&#35760;&#24405;&#25991;&#26412;&#32534;&#30721;&#26041;&#38754;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#38544;&#21547;&#23618;&#27425;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#32534;&#30721;&#22120;&#26469;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;EHR&#29305;&#24449;&#65292;&#24182;&#22312;&#20020;&#24202;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08290</link><description>&lt;p&gt;
&#37325;&#26032;&#21457;&#29616;CNN&#22312;&#21407;&#22987;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25991;&#26412;&#32534;&#30721;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rediscovery of CNN's Versatility for Text-based Encoding of Raw Electronic Health Records. (arXiv:2303.08290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#65292;CNN&#22312;&#20581;&#24247;&#35760;&#24405;&#25991;&#26412;&#32534;&#30721;&#26041;&#38754;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#38544;&#21547;&#23618;&#27425;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#32534;&#30721;&#22120;&#26469;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;EHR&#29305;&#24449;&#65292;&#24182;&#22312;&#20020;&#24202;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20805;&#20998;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#20016;&#23500;&#30340;&#20449;&#24687;&#27491;&#36880;&#28176;&#25104;&#20026;&#21307;&#23398;&#39046;&#22495;&#30340;&#37325;&#35201;&#35805;&#39064;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#20854;&#26684;&#24335;&#21644;&#21307;&#23398;&#32534;&#30721;&#26631;&#20934;&#30340;&#24773;&#20917;&#19979;&#23884;&#20837;&#21407;&#22987;EHR&#25968;&#25454;&#30340;&#25972;&#20010;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#35813;&#26694;&#26550;&#20165;&#20391;&#37325;&#20110;&#23545;EHR&#36827;&#34892;&#26368;&#23567;&#30340;&#39044;&#22788;&#29702;&#65292;&#26410;&#32771;&#34385;&#22914;&#20309;&#23398;&#20064;&#39640;&#25928;&#30340;EHR&#34920;&#31034;&#65292;&#21253;&#25324;&#35745;&#31639;&#21644;&#20869;&#23384;&#20351;&#29992;&#31561;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23547;&#25214;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#32534;&#30721;&#22120;&#65292;&#19981;&#20165;&#23558;&#22823;&#37327;&#25968;&#25454;&#32553;&#23567;&#21040;&#21487;&#31649;&#29702;&#30340;&#22823;&#23567;&#65292;&#36824;&#33021;&#24456;&#22909;&#22320;&#20445;&#30041;&#24739;&#32773;&#30340;&#26680;&#24515;&#20449;&#24687;&#65292;&#20197;&#25191;&#34892;&#21508;&#31181;&#20020;&#24202;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;&#20998;&#23618;&#32467;&#26500;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#65288;&#22914;&#37325;&#24314;&#65292;&#39044;&#27979;&#21644;&#29983;&#25104;&#65289;&#20013;&#32463;&#24120;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#21363;&#20351;&#21442;&#25968;&#36739;&#23569;&#19988;&#35757;&#32451;&#26102;&#38388;&#36739;&#30701;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;EHR&#25968;&#25454;&#30340;&#22266;&#26377;&#23618;&#27425;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;CNN&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#20123;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;EHR&#29305;&#24449;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20960;&#31181;&#20020;&#24202;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Making the most use of abundant information in electronic health records (EHR) is rapidly becoming an important topic in the medical domain. Recent work presented a promising framework that embeds entire features in raw EHR data regardless of its form and medical code standards. The framework, however, only focuses on encoding EHR with minimal preprocessing and fails to consider how to learn efficient EHR representation in terms of computation and memory usage. In this paper, we search for a versatile encoder not only reducing the large data into a manageable size but also well preserving the core information of patients to perform diverse clinical tasks. We found that hierarchically structured Convolutional Neural Network (CNN) often outperforms the state-of-the-art model on diverse tasks such as reconstruction, prediction, and generation, even with fewer parameters and less training time. Moreover, it turns out that making use of the inherent hierarchy of EHR data can boost the perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#23610;&#24230;&#31354;&#38388;&#30740;&#31350;&#30340;&#35282;&#24230;&#30740;&#31350;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#22312;&#28436;&#21270;&#27010;&#29575;&#20998;&#24067;&#19978;&#28385;&#36275;&#24191;&#20041;&#23610;&#24230;&#31354;&#38388;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.07900</link><description>&lt;p&gt;
&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#30340;&#24191;&#20041;&#23610;&#24230;&#31354;&#38388;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generalised Scale-Space Properties for Probabilistic Diffusion Models. (arXiv:2303.07900v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#23610;&#24230;&#31354;&#38388;&#30740;&#31350;&#30340;&#35282;&#24230;&#30740;&#31350;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#22312;&#28436;&#21270;&#27010;&#29575;&#20998;&#24067;&#19978;&#28385;&#36275;&#24191;&#20041;&#23610;&#24230;&#31354;&#38388;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#22312;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23427;&#20204;&#29983;&#25104;&#20174;&#23398;&#20064;&#22270;&#20687;&#20998;&#24067;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#26679;&#26412;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#36825;&#20123;&#26041;&#27861;&#26368;&#21021;&#26159;&#21463;&#28418;&#31227;-&#25193;&#25955;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#20294;&#22312;&#36817;&#26399;&#30340;&#23454;&#36341;&#23548;&#21521;&#30340;&#20986;&#29256;&#29289;&#20013;&#65292;&#36825;&#20123;&#36215;&#28304;&#24471;&#21040;&#20102;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20174;&#23610;&#24230;&#31354;&#38388;&#30740;&#31350;&#30340;&#35282;&#24230;&#30740;&#31350;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#22312;&#28436;&#21270;&#27010;&#29575;&#20998;&#24067;&#19978;&#28385;&#36275;&#24191;&#20041;&#23610;&#24230;&#31354;&#38388;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#19990;&#30028;&#20013;&#28418;&#31227;-&#25193;&#25955;&#29289;&#29702;&#26680;&#24515;&#27010;&#24565;&#35299;&#37322;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#27010;&#29575;&#25193;&#25955;&#19982;&#28183;&#36879;&#28388;&#27874;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic diffusion models enjoy increasing popularity in the deep learning community. They generate convincing samples from a learned distribution of input images with a wide field of practical applications. Originally, these approaches were motivated from drift-diffusion processes, but these origins find less attention in recent, practice-oriented publications.  We investigate probabilistic diffusion models from the viewpoint of scale-space research and show that they fulfil generalised scale-space properties on evolving probability distributions. Moreover, we discuss similarities and differences between interpretations of the physical core concept of drift-diffusion in the deep learning and model-based world. To this end, we examine relations of probabilistic diffusion to osmosis filters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#31215;&#20851;&#27880;&#35745;&#31639;&#30340;&#24555;&#36895;&#31639;&#27861;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#22312;B = O(sqrt(log n))&#26102;&#23384;&#22312;&#19968;&#31181;n ^&#65288;1 + O&#65288;1&#65289;&#65289;&#26102;&#38388;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.13214</link><description>&lt;p&gt;
&#24555;&#36895;&#27880;&#24847;&#21147;&#38656;&#35201;&#26377;&#30028;&#26465;&#30446;
&lt;/p&gt;
&lt;p&gt;
Fast Attention Requires Bounded Entries. (arXiv:2302.13214v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#31215;&#20851;&#27880;&#35745;&#31639;&#30340;&#24555;&#36895;&#31639;&#27861;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#22312;B = O(sqrt(log n))&#26102;&#23384;&#22312;&#19968;&#31181;n ^&#65288;1 + O&#65288;1&#65289;&#65289;&#26102;&#38388;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20869;&#31215;&#20851;&#27880;&#35745;&#31639;&#26159;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;Transformer&#65292;GPT-1&#65292;BERT&#65292;GPT-2&#65292;GPT-3&#21644;ChatGPT&#65289;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#24418;&#24335;&#19978;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#36755;&#20837;&#19977;&#20010;&#30697;&#38453;$Q&#65292;K&#65292;V \in [-B&#65292;B]^{n \times d}$&#65292;&#30446;&#26631;&#26159;&#26500;&#36896;&#30697;&#38453;$\mathrm{Att}(Q,K,V) := \mathrm{diag}(A {\bf 1}_n)^{-1} A V \in \mathbb{R}^{n \times d}$&#65292;&#20854;&#20013; $A = \exp(QK^\top/d)$ &#26159;&#8220;&#27880;&#24847;&#21147;&#30697;&#38453;&#8221;&#65292;$\exp$&#26159;&#20998;&#37327;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#38544;&#24335;&#21033;&#29992;&#30697;&#38453; $A$ &#26469;&#23454;&#29616;&#26356;&#24555;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#22312; $B = \Theta(\sqrt{\log n})$&#22788;&#23384;&#22312;&#19968;&#20010;&#23574;&#38160;&#30340;&#36716;&#25442;&#12290;$\bullet$ &#22914;&#26524; $d = O(\log n)$&#65292;$B = o(\sqrt{\log n})$&#65292;&#21017;&#23384;&#22312;&#19968;&#20010;$n^{1+o(1)}$&#26102;&#38388;&#31639;&#27861;&#26469;&#36817;&#20284;$\mathbb{R}^{n \times d}$&#20013;&#30340; $\mathrm{Att}(Q,K,V)$&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern machine learning, inner product attention computation is a fundamental task for training large language models such as Transformer, GPT-1, BERT, GPT-2, GPT-3 and ChatGPT. Formally, in this problem, one is given as input three matrices $Q, K, V \in [-B,B]^{n \times d}$, and the goal is to construct the matrix $\mathrm{Att}(Q,K,V) := \mathrm{diag}(A {\bf 1}_n)^{-1} A V \in \mathbb{R}^{n \times d}$, where $A = \exp(QK^\top/d)$ is the `attention matrix', and $\exp$ is applied entry-wise. Straightforward methods for this problem explicitly compute the $n \times n$ attention matrix $A$, and hence require time $\Omega(n^2)$ even when $d = n^{o(1)}$ is small.  In this paper, we investigate whether faster algorithms are possible by implicitly making use of the matrix $A$. We present two results, showing that there is a sharp transition at $B = \Theta(\sqrt{\log n})$.  $\bullet$ If $d = O(\log n)$ and $B = o(\sqrt{\log n})$, there is an $n^{1+o(1)}$ time algorithm to approximate $\math
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#30340;&#28145;&#24230;&#29983;&#25104;&#24335;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#31070;&#32463;&#26041;&#27861;&#21644;&#36951;&#20256;&#32534;&#31243;&#31639;&#27861;&#30340;&#20248;&#28857;&#65292;&#36816;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#31070;&#32463;&#31361;&#21464;&#27169;&#22411;&#65292;&#24182;&#22312;&#30693;&#21517;&#25968;&#25454;&#38598;&#20013;&#23637;&#31034;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.11223</link><description>&lt;p&gt;
&#22522;&#20110;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#30340;&#28145;&#24230;&#29983;&#25104;&#24335;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Symbolic Regression with Monte-Carlo-Tree-Search. (arXiv:2302.11223v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#30340;&#28145;&#24230;&#29983;&#25104;&#24335;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#31070;&#32463;&#26041;&#27861;&#21644;&#36951;&#20256;&#32534;&#31243;&#31639;&#27861;&#30340;&#20248;&#28857;&#65292;&#36816;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#31070;&#32463;&#31361;&#21464;&#27169;&#22411;&#65292;&#24182;&#22312;&#30693;&#21517;&#25968;&#25454;&#38598;&#20013;&#23637;&#31034;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#26159;&#20174;&#25968;&#20540;&#25968;&#25454;&#20013;&#23398;&#20064;&#31526;&#21495;&#34920;&#36798;&#24335;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#22312;&#27169;&#25311;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#34920;&#29616;&#20986;&#19982;&#26356;&#32463;&#20856;&#30340;&#36951;&#20256;&#32534;&#31243;&#65288;GP&#65289;&#31639;&#27861;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;&#19982;&#20854;GP&#23545;&#24212;&#29289;&#19981;&#21516;&#65292;&#36825;&#20123;&#31070;&#32463;&#26041;&#27861;&#26159;&#38024;&#23545;&#32473;&#23450;&#19978;&#19979;&#25991;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#34920;&#36798;&#24335;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#22522;&#20110;&#20351;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#31070;&#32463;&#31361;&#21464;&#27169;&#22411;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#36807;&#31243;&#65292;&#35813;&#27169;&#22411;&#26368;&#21021;&#39044;&#20808;&#35757;&#32451;&#20197;&#23398;&#20064;&#26377;&#21069;&#36884;&#30340;&#31361;&#21464;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#32447;&#26041;&#24335;&#20174;&#25104;&#21151;&#30340;&#32463;&#39564;&#20013;&#36827;&#19968;&#27493;&#23436;&#21892;&#12290;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#30693;&#21517;&#30340;&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression (SR) is the problem of learning a symbolic expression from numerical data. Recently, deep neural models trained on procedurally-generated synthetic datasets showed competitive performance compared to more classical Genetic Programming (GP) algorithms. Unlike their GP counterparts, these neural approaches are trained to generate expressions from datasets given as context. This allows them to produce accurate expressions in a single forward pass at test time. However, they usually do not benefit from search abilities, which result in low performance compared to GP on out-of-distribution datasets. In this paper, we propose a novel method which provides the best of both worlds, based on a Monte-Carlo Tree Search procedure using a context-aware neural mutation model, which is initially pre-trained to learn promising mutations, and further refined from successful experiences in an online fashion. The approach demonstrates state-of-the-art performance on the well-known \te
&lt;/p&gt;</description></item><item><title>AttentionMixer&#26159;&#19968;&#20010;&#26088;&#22312;&#20026;&#33021;&#37327;&#21464;&#25442;&#21378;&#24314;&#31435;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#36752;&#23556;&#30417;&#27979;&#26694;&#26550;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20854;&#25216;&#26415;&#21019;&#26032;&#28857;&#20026;&#31354;&#38388;&#21644;&#26102;&#38388;&#33258;&#36866;&#24212;&#28040;&#24687;&#20256;&#36882;&#22359;&#21644;&#27880;&#24847;&#21147;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2302.10426</link><description>&lt;p&gt;
AttentionMixer&#65306;&#19968;&#20010;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#36807;&#31243;&#30417;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AttentionMixer: An Accurate and Interpretable Framework for Process Monitoring. (arXiv:2302.10426v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10426
&lt;/p&gt;
&lt;p&gt;
AttentionMixer&#26159;&#19968;&#20010;&#26088;&#22312;&#20026;&#33021;&#37327;&#21464;&#25442;&#21378;&#24314;&#31435;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#36752;&#23556;&#30417;&#27979;&#26694;&#26550;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20854;&#25216;&#26415;&#21019;&#26032;&#28857;&#20026;&#31354;&#38388;&#21644;&#26102;&#38388;&#33258;&#36866;&#24212;&#28040;&#24687;&#20256;&#36882;&#22359;&#21644;&#27880;&#24847;&#21147;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26497;&#31471;&#24037;&#20316;&#26465;&#20214;&#19979;&#36816;&#34892;&#30340;&#39640;&#25928;&#33021;&#36716;&#25442;&#24037;&#21378;&#30340;&#23433;&#20840;&#24615;&#39640;&#24230;&#20381;&#36182;&#20110;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#33258;&#21160;&#30417;&#27979;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21487;&#29992;&#30340;&#25968;&#25454;&#39537;&#21160;&#30417;&#27979;&#31995;&#32479;&#22312;&#39640;&#20934;&#30830;&#24615;&#25110;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#24448;&#24448;&#26080;&#27861;&#28385;&#36275;&#35201;&#27714;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#8212;&#8212;AttentionMixer&#65292;&#26088;&#22312;&#20026;&#33021;&#37327;&#21464;&#25442;&#21378;&#24314;&#31435;&#19968;&#20010;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#36752;&#23556;&#30417;&#27979;&#26694;&#26550;&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#31532;&#19968;&#39033;&#25216;&#26415;&#36129;&#29486;&#21253;&#25324;&#24320;&#21457;&#31354;&#38388;&#21644;&#26102;&#38388;&#33258;&#36866;&#24212;&#28040;&#24687;&#20256;&#36882;&#22359;&#65292;&#20998;&#21035;&#29992;&#20110;&#25429;&#33719;&#31354;&#38388;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#65307;&#36825;&#20004;&#20010;&#22359;&#36890;&#36807;&#28151;&#21512;&#31639;&#23376;&#32423;&#32852;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#65292;&#31532;&#20108;&#39033;&#25216;&#26415;&#36129;&#29486;&#28041;&#21450;&#23454;&#29616;&#27880;&#24847;&#21147;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#20351;&#24471;&#21487;&#20197;&#23545;&#27169;&#22411;&#30340;&#39044;&#27979;&#20570;&#20986;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
An accurate and explainable automatic monitoring system is critical for the safety of high efficiency energy conversion plants that operate under extreme working condition. Nonetheless, currently available data-driven monitoring systems often fall short in meeting the requirements for either high-accuracy or interpretability, which hinders their application in practice. To overcome this limitation, a data-driven approach, AttentionMixer, is proposed under a generalized message passing framework, with the goal of establishing an accurate and interpretable radiation monitoring framework for energy conversion plants. To improve the model accuracy, the first technical contribution involves the development of spatial and temporal adaptive message passing blocks, which enable the capture of spatial and temporal correlations, respectively; the two blocks are cascaded through a mixing operator. To enhance the model interpretability, the second technical contribution involves the implementation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#40657;&#30418;&#27169;&#22411;&#20013;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#40657;&#30418;&#27169;&#22411;&#20998;&#25104;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#21644;&#27531;&#24046;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#36827;&#34892;&#22522;&#26412;&#25512;&#29702;&#12290;&#27492;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#19988;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.10289</link><description>&lt;p&gt;
&#23558;&#40657;&#21283;&#23376;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#65306;&#36335;&#32447;&#35268;&#21010;&#65292;&#35299;&#37322;&#65292;&#37325;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat. (arXiv:2302.10289v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#40657;&#30418;&#27169;&#22411;&#20013;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#40657;&#30418;&#27169;&#22411;&#20998;&#25104;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#21644;&#27531;&#24046;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#36827;&#34892;&#22522;&#26412;&#25512;&#29702;&#12290;&#27492;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#19988;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#35201;&#20040;&#20174;&#35299;&#37322;&#24615;&#27169;&#22411;&#24320;&#22987;&#65292;&#35201;&#20040;&#20174;&#40657;&#30418;&#24320;&#22987;&#24182;&#20107;&#21518;&#35299;&#37322;&#12290;&#40657;&#30418;&#27169;&#22411;&#28789;&#27963;&#20294;&#38590;&#20197;&#35299;&#37322;&#65292;&#32780;&#35299;&#37322;&#24615;&#27169;&#22411;&#26412;&#36136;&#19978;&#26159;&#21487;&#35299;&#37322;&#30340;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#24615;&#27169;&#22411;&#38656;&#35201;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#30693;&#35782;&#65292;&#24182;&#19988;&#24448;&#24448;&#27604;&#23427;&#20204;&#30340;&#40657;&#30418;&#21464;&#20307;&#19981;&#22815;&#28789;&#27963;&#21644;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#26088;&#22312;&#27169;&#31946;&#40657;&#30418;&#30340;&#20107;&#21518;&#35299;&#37322;&#21644;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#20043;&#38388;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#20174;&#40657;&#30418;&#24320;&#22987;&#65292;&#36845;&#20195;&#22320;Carve&#20986;&#19968;&#31181;&#28151;&#21512;&#35299;&#37322;&#27169;&#22411;&#65288;MoIE&#65289;&#21644;&#19968;&#20010;&#27531;&#20313;&#32593;&#32476;&#12290;&#27599;&#20010;&#21487;&#35299;&#37322;&#27169;&#22411;&#19987;&#38376;&#22788;&#29702;&#19968;&#20010;&#26679;&#26412;&#23376;&#38598;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;(FOL)&#23545;&#20854;&#36827;&#34892;&#35299;&#37322;&#65292;&#20174;&#40657;&#30418;&#20013;&#25552;&#20379;&#22522;&#26412;&#25512;&#29702;&#27010;&#24565;&#12290;&#25105;&#20204;&#36890;&#36807;&#28789;&#27963;&#30340;&#27531;&#24046;&#36335;&#30001;&#20854;&#20313;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#27531;&#36716;&#32593;&#32476;&#19978;&#37325;&#22797;&#35813;&#26041;&#27861;&#65292;&#30452;&#21040;&#25152;&#26377;&#21487;&#35299;&#37322;&#27169;&#22411;&#35299;&#37322;&#25152;&#38656;&#27604;&#20363;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#36335;&#32447;&#35268;&#21010;&#65292;&#35299;&#37322;&#21644;&#37325;&#22797;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#30446;&#21069;&#20960;&#31181;&#40657;&#21283;&#23376;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
ML model design either starts with an interpretable model or a Blackbox and explains it post hoc. Blackbox models are flexible but difficult to explain, while interpretable models are inherently explainable. Yet, interpretable models require extensive ML knowledge and tend to be less flexible and underperforming than their Blackbox variants. This paper aims to blur the distinction between a post hoc explanation of a Blackbox and constructing interpretable models. Beginning with a Blackbox, we iteratively carve out a mixture of interpretable experts (MoIE) and a residual network. Each interpretable model specializes in a subset of samples and explains them using First Order Logic (FOL), providing basic reasoning on concepts from the Blackbox. We route the remaining samples through a flexible residual. We repeat the method on the residual network until all the interpretable models explain the desired proportion of data. Our extensive experiments show that our route, interpret, and repeat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;BPLS&#65292;&#19968;&#31181;&#29992;&#20110;PLS&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#26512;&#36924;&#36817;&#36873;&#25321;&#26631;&#31614;&#23454;&#20363;&#30340;&#26631;&#20934;&#65292;&#20197;&#36991;&#20813;&#30001;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#39044;&#27979;&#30340;&#23454;&#20363;&#36873;&#25321;&#32780;&#23548;&#33268;&#30340;&#30830;&#35748;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.08883</link><description>&lt;p&gt;
&#36817;&#20046;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#20266;&#26631;&#31614;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Approximately Bayes-Optimal Pseudo Label Selection. (arXiv:2302.08883v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BPLS&#65292;&#19968;&#31181;&#29992;&#20110;PLS&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#26512;&#36924;&#36817;&#36873;&#25321;&#26631;&#31614;&#23454;&#20363;&#30340;&#26631;&#20934;&#65292;&#20197;&#36991;&#20813;&#30001;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#39044;&#27979;&#30340;&#23454;&#20363;&#36873;&#25321;&#32780;&#23548;&#33268;&#30340;&#30830;&#35748;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#35757;&#32451;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20005;&#37325;&#20381;&#36182;&#20110;&#20266;&#26631;&#31614;&#36873;&#25321;&#65288;PLS&#65289;&#12290;&#36873;&#25321;&#36890;&#24120;&#21462;&#20915;&#20110;&#21021;&#22987;&#27169;&#22411;&#25311;&#21512;&#26631;&#35760;&#25968;&#25454;&#30340;&#31243;&#24230;&#12290;&#36807;&#26089;&#30340;&#36807;&#25311;&#21512;&#21487;&#33021;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#30340;&#39044;&#27979;&#30340;&#23454;&#20363;&#65288;&#36890;&#24120;&#31216;&#20026;&#30830;&#35748;&#20559;&#24046;&#65289;&#32780;&#20256;&#25773;&#21040;&#26368;&#32456;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BPLS&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;PLS&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#26088;&#22312;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#12290;&#20854;&#26680;&#24515;&#26159;&#36873;&#25321;&#26631;&#31614;&#23454;&#20363;&#30340;&#26631;&#20934;&#65306;&#20266;&#26679;&#26412;&#30340;&#21518;&#39564;&#39044;&#27979;&#30340;&#20998;&#26512;&#36817;&#20284;&#12290;&#25105;&#20204;&#36890;&#36807;&#35777;&#26126;&#20266;&#26679;&#26412;&#30340;&#21518;&#39564;&#39044;&#27979;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#24615;&#33719;&#24471;&#20102;&#36825;&#31181;&#36873;&#25321;&#26631;&#20934;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#35299;&#26512;&#36924;&#36817;&#20811;&#26381;&#35745;&#31639;&#38590;&#39064;&#12290;&#23427;&#19982;&#36793;&#38469;&#20284;&#28982;&#30340;&#20851;&#31995;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20986;&#22522;&#20110;&#25289;&#26222;&#25289;&#26031;&#26041;&#27861;&#21644;&#39640;&#26031;&#31215;&#20998;&#30340;&#36924;&#36817;&#12290;&#25105;&#20204;&#38024;&#23545;&#21442;&#25968;&#24191;&#20041;&#32447;&#24615;&#21644;&#38750;&#21442;&#25968;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#23545;BPLS&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning by self-training heavily relies on pseudo-label selection (PLS). The selection often depends on the initial model fit on labeled data. Early overfitting might thus be propagated to the final model by selecting instances with overconfident but erroneous predictions, often referred to as confirmation bias. This paper introduces BPLS, a Bayesian framework for PLS that aims to mitigate this issue. At its core lies a criterion for selecting instances to label: an analytical approximation of the posterior predictive of pseudo-samples. We derive this selection criterion by proving Bayes optimality of the posterior predictive of pseudo-samples. We further overcome computational hurdles by approximating the criterion analytically. Its relation to the marginal likelihood allows us to come up with an approximation based on Laplace's method and the Gaussian integral. We empirically assess BPLS for parametric generalized linear and non-parametric generalized additive models
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SNeRL&#30340;&#35821;&#20041;&#24863;&#30693;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;3D-aware&#30340;&#38544;&#24335;&#34920;&#31034;&#26469;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#22312;&#22522;&#20110;&#20687;&#32032;&#30340;&#20197;&#21450;&#26368;&#26032;&#30340;3D&#24863;&#30693;&#34920;&#31034;&#26041;&#27861;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11520</link><description>&lt;p&gt;
SNeRL: &#35821;&#20041;&#24863;&#30693;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning. (arXiv:2301.11520v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SNeRL&#30340;&#35821;&#20041;&#24863;&#30693;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;3D-aware&#30340;&#38544;&#24335;&#34920;&#31034;&#26469;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#22312;&#22522;&#20110;&#20687;&#32032;&#30340;&#20197;&#21450;&#26368;&#26032;&#30340;3D&#24863;&#30693;&#34920;&#31034;&#26041;&#27861;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#34920;&#31034;&#26041;&#27861;&#24456;&#38590;&#26377;&#25928;&#22320;&#34701;&#21512;&#20154;&#31867;&#30452;&#35266;&#30340;3D&#29615;&#22659;&#29702;&#35299;&#65292;&#22240;&#27492;&#32463;&#24120;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SNeRL&#30340;&#35821;&#20041;&#24863;&#30693;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#23427;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#21367;&#31215;&#32534;&#30721;&#22120;&#21644;&#35821;&#20041;&#24863;&#30693;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#26469;&#20174;&#22810;&#35270;&#35282;&#22270;&#20687;&#20013;&#23398;&#20064;3D&#24863;&#30693;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;NeRF&#20013;&#24341;&#20837;&#20102;3D&#35821;&#20041;&#21644;&#33976;&#39311;&#29305;&#24449;&#22330;&#65292;&#24182;&#19982;RGB&#36752;&#23556;&#22330;&#24182;&#34892;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35821;&#20041;&#21644;&#23545;&#35937;&#20013;&#24515;&#34920;&#31034;&#23398;&#20064;&#12290;SNeRL&#22312;&#26080;&#27169;&#22411;&#21644;&#26377;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20013;&#19981;&#20165;&#20248;&#20110;&#20197;&#24448;&#30340;&#22522;&#20110;&#20687;&#32032;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#36824;&#20248;&#20110;&#26368;&#36817;&#30340;3D&#24863;&#30693;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As previous representations for reinforcement learning cannot effectively incorporate a human-intuitive understanding of the 3D environment, they usually suffer from sub-optimal performances. In this paper, we present Semantic-aware Neural Radiance Fields for Reinforcement Learning (SNeRL), which jointly optimizes semantic-aware neural radiance fields (NeRF) with a convolutional encoder to learn 3D-aware neural implicit representation from multi-view images. We introduce 3D semantic and distilled feature fields in parallel to the RGB radiance fields in NeRF to learn semantic and object-centric representation for reinforcement learning. SNeRL outperforms not only previous pixel-based representations but also recent 3D-aware representations both in model-free and model-based reinforcement learning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26412;&#22320;&#26032;&#38395;&#26816;&#27979;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#26412;&#22320;&#26032;&#38395;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#26694;&#26550;&#21644;&#33258;&#21160;&#21270;&#25968;&#25454;&#22788;&#29702;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.08146</link><description>&lt;p&gt;
&#20320;&#25152;&#22312;&#31038;&#21306;&#21457;&#29983;&#20102;&#20160;&#20040;&#65311;&#19968;&#31181;&#24369;&#30417;&#30563;&#26041;&#27861;&#29992;&#20110;&#21457;&#29616;&#26412;&#22320;&#26032;&#38395;&#12290;
&lt;/p&gt;
&lt;p&gt;
What's happening in your neighborhood? A Weakly Supervised Approach to Detect Local News. (arXiv:2301.08146v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08146
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26412;&#22320;&#26032;&#38395;&#26816;&#27979;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#26412;&#22320;&#26032;&#38395;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#26694;&#26550;&#21644;&#33258;&#21160;&#21270;&#25968;&#25454;&#22788;&#29702;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#22320;&#26032;&#38395;&#26159;&#24433;&#21709;&#29305;&#23450;&#22320;&#29702;&#21306;&#22495;&#65288;&#22914;&#22478;&#24066;&#12289;&#21439;&#21644;&#24030;&#65289;&#29992;&#25143;&#30340;&#26032;&#38395;&#23376;&#38598;&#12290;&#26816;&#27979;&#26412;&#22320;&#26032;&#38395;&#26159;&#20934;&#30830;&#22320;&#25512;&#33616;&#26412;&#22320;&#26032;&#38395;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#22522;&#20110;&#26368;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38598;&#25104;&#21270;&#30340;&#27969;&#31243;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#26412;&#22320;&#26032;&#38395;&#26816;&#27979;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#26412;&#22320;&#26032;&#38395;&#25512;&#33616;&#12290;&#26412;&#25991;&#30528;&#37325;&#20171;&#32461;&#20102;&#31649;&#36947;&#30340;&#31532;&#19968;&#27493;&#39588;&#65306;&#65288;1&#65289;&#32467;&#21512;&#39046;&#22495;&#30693;&#35782;&#21644;&#33258;&#21160;&#25968;&#25454;&#22788;&#29702;&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#65288;2&#65289;&#21487;&#25193;&#23637;&#21040;&#22810;&#35821;&#35328;&#35774;&#32622;&#12290;&#19982;&#26031;&#22374;&#31119;CoreNLP NER&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27969;&#31243;&#22312;&#32463;&#36807;&#30495;&#23454;&#19990;&#30028;&#21644;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#30340;&#35780;&#20272;&#26102;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local news articles are a subset of news that impact users in a geographical area, such as a city, county, or state. Detecting local news (Step 1) and subsequently deciding its geographical location as well as radius of impact (Step 2) are two important steps towards accurate local news recommendation. Naive rule-based methods, such as detecting city names from the news title, tend to give erroneous results due to lack of understanding of the news content. Empowered by the latest development in natural language processing, we develop an integrated pipeline that enables automatic local news detection and content-based local news recommendations. In this paper, we focus on Step 1 of the pipeline, which highlights: (1) a weakly supervised framework incorporated with domain knowledge and auto data processing, and (2) scalability to multi-lingual settings. Compared with Stanford CoreNLP NER model, our pipeline has higher precision and recall evaluated on a real-world and human-labeled datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#39640;&#22122;&#22768;&#35889;&#23398;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#34920;&#26126;&#27492;&#26041;&#27861;&#21487;&#20197;&#27604;&#26631;&#20934;&#25216;&#26415;&#26356;&#21152;&#20934;&#30830;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2301.05079</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22686;&#24378;&#33258;&#26059;&#37327;&#23376;&#27604;&#29305;&#29615;&#22659;&#30340;&#22122;&#22768;&#35889;&#23398;
&lt;/p&gt;
&lt;p&gt;
Deep learning enhanced noise spectroscopy of a spin qubit environment. (arXiv:2301.05079v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#39640;&#22122;&#22768;&#35889;&#23398;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#34920;&#26126;&#27492;&#26041;&#27861;&#21487;&#20197;&#27604;&#26631;&#20934;&#25216;&#26415;&#26356;&#21152;&#20934;&#30830;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31995;&#32479;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#19981;&#33391;&#30456;&#20114;&#20316;&#29992;&#36890;&#24120;&#23548;&#33268;&#36229;&#24577;&#22312;&#26102;&#38388;&#19978;&#30340;&#30456;&#24178;&#34928;&#20943;&#12290;&#31934;&#30830;&#20102;&#35299;&#29615;&#22659;&#24341;&#36215;&#30340;&#22122;&#22768;&#30340;&#39057;&#35889;&#20869;&#23481;&#23545;&#20110;&#20445;&#25252;&#37327;&#23376;&#27604;&#29305;&#30456;&#24178;&#24615;&#24182;&#20248;&#21270;&#20854;&#22312;&#37327;&#23376;&#22120;&#20214;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#21152;&#22122;&#22768;&#35889;&#23398;&#30340;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#37325;&#26500;&#30899;&#26434;&#36136;&#32452;&#21512;&#22260;&#32469;&#38075;&#30707;&#27694;-&#31354;&#20301;&#65288;NV&#65289;&#20013;&#24515;&#25152;&#29305;&#24449;&#21270;&#30340;&#21151;&#29575;&#35889;&#23494;&#24230;&#12290;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20110;NV&#20013;&#24515;&#30340;&#33258;&#26059;&#30456;&#24178;&#20989;&#25968;&#20043;&#19978;&#65292;NV&#20013;&#24515;&#21463;&#19981;&#21516;&#30340;Carr-Purcell&#24207;&#21015;&#25511;&#21046;&#65292;&#36890;&#24120;&#29992;&#20110;&#21160;&#21147;&#23398;&#35299;&#20598;&#65288;DD&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#27604;&#26631;&#20934;DD&#22122;&#22768;&#35889;&#23398;&#25216;&#26415;&#26356;&#21152;&#20934;&#30830;&#65292;&#24182;&#19988;&#21516;&#26102;&#38656;&#35201;&#26356;&#23569;&#30340;DD&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
The undesired interaction of a quantum system with its environment generally leads to a coherence decay of superposition states in time. A precise knowledge of the spectral content of the noise induced by the environment is crucial to protect qubit coherence and optimize its employment in quantum device applications. We experimentally show that the use of neural networks can highly increase the accuracy of noise spectroscopy, by reconstructing the power spectral density that characterizes an ensemble of carbon impurities around a nitrogen-vacancy (NV) center in diamond. Neural networks are trained over spin coherence functions of the NV center subjected to different Carr-Purcell sequences, typically used for dynamical decoupling (DD). As a result, we determine that deep learning models can be more accurate than standard DD noise-spectroscopy techniques, by requiring at the same time a much smaller number of DD sequences.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22238;&#24402;&#20219;&#21153;&#30340;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#36890;&#36807;&#21407;&#24072;&#29983;&#32593;&#32476;&#30340;&#26631;&#31614;&#35757;&#32451;&#23398;&#29983;&#32593;&#32476;&#12290;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#29983;&#25104;&#31574;&#30053;&#65292;&#21487;&#20197;&#30452;&#25509;&#20248;&#21270;&#23398;&#29983;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.04338</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#20013;&#25968;&#25454;&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Synthetic data generation method for data-free knowledge distillation in regression neural networks. (arXiv:2301.04338v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22238;&#24402;&#20219;&#21153;&#30340;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#36890;&#36807;&#21407;&#24072;&#29983;&#32593;&#32476;&#30340;&#26631;&#31614;&#35757;&#32451;&#23398;&#29983;&#32593;&#32476;&#12290;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#29983;&#25104;&#31574;&#30053;&#65292;&#21487;&#20197;&#30452;&#25509;&#20248;&#21270;&#23398;&#29983;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#23558;&#22823;&#23567;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#33267;&#25928;&#26524;&#30456;&#36817;&#30340;&#25216;&#26415;&#12290;&#29616;&#26377;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#20027;&#35201;&#36866;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#38656;&#35201;&#20351;&#29992;&#29992;&#20110;&#35757;&#32451;&#21407;&#27169;&#22411;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#22238;&#24402;&#20219;&#21153;&#19979;&#26080;&#21407;&#22987;&#25968;&#25454;&#30340;&#30693;&#35782;&#33976;&#39311;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#29992;&#21407;&#27169;&#22411;&#39044;&#27979;&#30340;&#26631;&#31614;&#26469;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#30340;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#30452;&#25509;&#20248;&#21270;&#20102;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is the technique of compressing a larger neural network, known as the teacher, into a smaller neural network, known as the student, while still trying to maintain the performance of the larger neural network as much as possible. Existing methods of knowledge distillation are mostly applicable for classification tasks. Many of them also require access to the data used to train the teacher model. To address the problem of knowledge distillation for regression tasks under the absence of original training data, previous work has proposed a data-free knowledge distillation method where synthetic data are generated using a generator model trained adversarially against the student model. These synthetic data and their labels predicted by the teacher model are then used to train the student model. In this study, we investigate the behavior of various synthetic data generation methods and propose a new synthetic data generation strategy that directly optimizes for a large
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TarViS&#30340;&#26032;&#39062;&#12289;&#32479;&#19968;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#29992;&#20110;&#20219;&#20309;&#38656;&#35201;&#22312;&#35270;&#39057;&#20013;&#20998;&#27573;&#20219;&#24847;&#23450;&#20041;&#30340;&#8220;&#30446;&#26631;&#8221;&#38598;&#30340;&#20219;&#21153;&#12290;&#21487;&#20197;&#32852;&#21512;&#35757;&#32451;&#19981;&#21516;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#38598;&#21512;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#25512;&#26029;&#26399;&#38388;&#22312;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#28909;&#20999;&#25442;&#12290;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#24212;&#29992;&#65292;&#22343;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.02657</link><description>&lt;p&gt;
TarViS&#65306;&#19968;&#31181;&#38754;&#21521;&#30446;&#26631;&#30340;&#35270;&#39057;&#20998;&#21106;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TarViS: A Unified Approach for Target-based Video Segmentation. (arXiv:2301.02657v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02657
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TarViS&#30340;&#26032;&#39062;&#12289;&#32479;&#19968;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#29992;&#20110;&#20219;&#20309;&#38656;&#35201;&#22312;&#35270;&#39057;&#20013;&#20998;&#27573;&#20219;&#24847;&#23450;&#20041;&#30340;&#8220;&#30446;&#26631;&#8221;&#38598;&#30340;&#20219;&#21153;&#12290;&#21487;&#20197;&#32852;&#21512;&#35757;&#32451;&#19981;&#21516;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#38598;&#21512;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#25512;&#26029;&#26399;&#38388;&#22312;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#28909;&#20999;&#25442;&#12290;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#24212;&#29992;&#65292;&#22343;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20998;&#21106;&#39046;&#22495;&#30446;&#21069;&#34987;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#22522;&#20934;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#25216;&#26415;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#65292;&#20294;&#26159;&#24403;&#21069;&#26041;&#27861;&#20027;&#35201;&#26159;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#33021;&#27010;&#24565;&#19978;&#25512;&#24191;&#21040;&#20854;&#20182;&#20219;&#21153;&#12290;&#21463;&#21040;&#20855;&#26377;&#22810;&#20219;&#21153;&#33021;&#21147;&#30340;&#26368;&#36817;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TarViS&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#24212;&#29992;&#20110;&#38656;&#35201;&#22312;&#35270;&#39057;&#20013;&#20998;&#27573;&#20219;&#24847;&#23450;&#20041;&#30340;&#8220;&#30446;&#26631;&#8221;&#38598;&#30340;&#20219;&#20309;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20219;&#21153;&#23450;&#20041;&#36825;&#20123;&#30446;&#26631;&#30340;&#26041;&#24335;&#26041;&#38754;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#22240;&#20026;&#23427;&#23558;&#21518;&#32773;&#24314;&#27169;&#20026;&#25277;&#35937;&#30340;&#8220;&#26597;&#35810;&#8221;&#65292;&#28982;&#21518;&#29992;&#20110;&#39044;&#27979;&#20687;&#32032;&#31934;&#30830;&#30340;&#30446;&#26631;&#25513;&#30721;&#12290;&#21333;&#20010;TarViS&#27169;&#22411;&#21487;&#20197;&#32852;&#21512;&#35757;&#32451;&#36328;&#36234;&#19981;&#21516;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#38598;&#21512;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#25512;&#26029;&#26399;&#38388;&#22312;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#28909;&#20999;&#25442;&#65292;&#26080;&#38656;&#20219;&#20309;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;TarViS&#24212;&#29992;&#20110;&#22235;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#21363;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;(VIS)&#12289;&#35270;&#39057;&#20840;&#26223;&#20998;&#21106;(VPS)&#12289;&#35270;&#39057;&#35821;&#20041;&#20998;&#21106;&#21644;&#35270;&#39057;&#30446;&#26631;&#20998;&#21106;(VOS)&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TarViS&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The general domain of video segmentation is currently fragmented into different tasks spanning multiple benchmarks. Despite rapid progress in the state-of-the-art, current methods are overwhelmingly task-specific and cannot conceptually generalize to other tasks. Inspired by recent approaches with multi-task capability, we propose TarViS: a novel, unified network architecture that can be applied to any task that requires segmenting a set of arbitrarily defined 'targets' in video. Our approach is flexible with respect to how tasks define these targets, since it models the latter as abstract 'queries' which are then used to predict pixel-precise target masks. A single TarViS model can be trained jointly on a collection of datasets spanning different tasks, and can hot-swap between tasks during inference without any task-specific retraining. To demonstrate its effectiveness, we apply TarViS to four different tasks, namely Video Instance Segmentation (VIS), Video Panoptic Segmentation (VPS
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#28151;&#21512;&#22810;&#26234;&#33021;&#20307;&#33258;&#20027;&#20986;&#34892;&#31995;&#32479;&#65292;&#37319;&#29992;&#22810;&#26234;&#33021;&#20307;Soft Actor-Critic&#21644;&#21152;&#26435;&#20108;&#20998;&#22270;&#21305;&#37197;&#65292;&#33021;&#22815;&#35299;&#20915;&#33258;&#20027;&#31227;&#21160;&#20986;&#34892;&#31995;&#32479;&#20013;&#30340;&#20027;&#21160;&#35831;&#27714;&#20998;&#37197;&#21644;&#25298;&#32477;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#22312;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#35745;&#31639;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2212.07313</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#28151;&#21512;&#22810;&#26234;&#33021;&#20307;&#33258;&#20027;&#20986;&#34892;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Hybrid Multi-agent Deep Reinforcement Learning for Autonomous Mobility on Demand Systems. (arXiv:2212.07313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#28151;&#21512;&#22810;&#26234;&#33021;&#20307;&#33258;&#20027;&#20986;&#34892;&#31995;&#32479;&#65292;&#37319;&#29992;&#22810;&#26234;&#33021;&#20307;Soft Actor-Critic&#21644;&#21152;&#26435;&#20108;&#20998;&#22270;&#21305;&#37197;&#65292;&#33021;&#22815;&#35299;&#20915;&#33258;&#20027;&#31227;&#21160;&#20986;&#34892;&#31995;&#32479;&#20013;&#30340;&#20027;&#21160;&#35831;&#27714;&#20998;&#37197;&#21644;&#25298;&#32477;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#22312;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#35745;&#31639;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#38754;&#21521;&#33258;&#20027;&#31227;&#21160;&#20986;&#34892;&#31995;&#32479;&#20013;&#30340;&#21033;&#28070;&#26368;&#22823;&#21270;&#36816;&#33829;&#21830;&#30340;&#20027;&#21160;&#35831;&#27714;&#20998;&#37197;&#21644;&#25298;&#32477;&#20915;&#31574;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#26234;&#33021;&#20307;Soft Actor-Critic&#21644;&#21152;&#26435;&#20108;&#20998;&#22270;&#21305;&#37197;&#30340;&#26032;&#39062;&#32452;&#21512;&#65292;&#20197;&#33719;&#24471;&#19968;&#31181;&#39044;&#26399;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#36816;&#33829;&#21830;&#30340;&#34892;&#21160;&#31354;&#38388;&#20998;&#35299;&#24320;&#26469;&#65292;&#20294;&#20173;&#28982;&#33719;&#24471;&#20102;&#20840;&#23616;&#21327;&#35843;&#30340;&#20915;&#31574;&#12290;&#22522;&#20110;&#30495;&#23454;&#20986;&#31199;&#36710;&#25968;&#25454;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#35745;&#31639;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the sequential decision-making problem of making proactive request assignment and rejection decisions for a profit-maximizing operator of an autonomous mobility on demand system. We formalize this problem as a Markov decision process and propose a novel combination of multi-agent Soft Actor-Critic and weighted bipartite matching to obtain an anticipative control policy. Thereby, we factorize the operator's otherwise intractable action space, but still obtain a globally coordinated decision. Experiments based on real-world taxi data show that our method outperforms state of the art benchmarks with respect to performance, stability, and computational tractability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20154;&#31867;&#31934;&#23376;&#36319;&#36394;&#25968;&#25454;&#38598;VISEM-Tracking&#65292;&#21253;&#21547;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#22260;&#26694;&#22352;&#26631;&#21644;&#30001;&#19987;&#23478;&#20998;&#26512;&#30340;&#31934;&#23376;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#20197;&#20379;&#26131;&#20110;&#35775;&#38382;&#21644;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#35757;&#32451;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#39640;&#22312;&#35780;&#20272;&#31934;&#23376;&#36816;&#21160;&#21644;&#36816;&#21160;&#23398;&#26041;&#38754;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.02842</link><description>&lt;p&gt;
VISEM-Tracking&#65292;&#19968;&#20221;&#20154;&#31867;&#31934;&#23376;&#36319;&#36394;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VISEM-Tracking, a human spermatozoa tracking dataset. (arXiv:2212.02842v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20154;&#31867;&#31934;&#23376;&#36319;&#36394;&#25968;&#25454;&#38598;VISEM-Tracking&#65292;&#21253;&#21547;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#22260;&#26694;&#22352;&#26631;&#21644;&#30001;&#19987;&#23478;&#20998;&#26512;&#30340;&#31934;&#23376;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#20197;&#20379;&#26131;&#20110;&#35775;&#38382;&#21644;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#35757;&#32451;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#39640;&#22312;&#35780;&#20272;&#31934;&#23376;&#36816;&#21160;&#21644;&#36816;&#21160;&#23398;&#26041;&#38754;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#23376;&#36816;&#21160;&#30340;&#25163;&#21160;&#35780;&#20272;&#38656;&#35201;&#26174;&#24494;&#38236;&#35266;&#23519;&#65292;&#30001;&#20110;&#25152;&#35266;&#23519;&#30340;&#31934;&#23376;&#22312;&#35270;&#37326;&#20013;&#30340;&#24555;&#36895;&#31227;&#21160;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#33719;&#24471;&#27491;&#30830;&#30340;&#32467;&#26524;&#65292;&#25163;&#21160;&#35780;&#20272;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#22521;&#35757;&#12290;&#22240;&#27492;&#65292;&#22312;&#35786;&#25152;&#20013;&#65292;&#35745;&#31639;&#26426;&#36741;&#21161;&#31934;&#23376;&#20998;&#26512;&#65288;CASA&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#24120;&#29992;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#26469;&#35757;&#32451;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22312;&#35780;&#20272;&#31934;&#23376;&#36816;&#21160;&#21644;&#36816;&#21160;&#23398;&#26041;&#38754;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;VISEM-Tracking&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;20&#20010;30&#31186;&#30340;&#35270;&#39057;&#35760;&#24405;&#65288;&#21253;&#25324;29,196&#24103;&#65289;&#30340;&#28287;&#24615;&#31934;&#23376;&#21046;&#22791;&#29289;&#65292;&#20855;&#22791;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#22260;&#26694;&#22352;&#26631;&#21644;&#30001;&#35813;&#39046;&#22495;&#30340;&#19987;&#23478;&#20998;&#26512;&#30340;&#19968;&#32452;&#31934;&#23376;&#29305;&#24449;&#12290;&#38500;&#20102;&#24050;&#27880;&#37322;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#21098;&#36753;&#65292;&#20197;&#20415;&#36890;&#36807;&#33258;&#30417;&#30563;&#25110;&#26080;&#30417;&#30563;&#23398;&#20064;&#31561;&#26041;&#27861;&#36731;&#26494;&#35775;&#38382;&#21644;&#20998;&#26512;&#25968;&#25454;&#12290;&#20316;&#20026;&#26412;&#25991;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#32447;&#31934;&#23376;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A manual assessment of sperm motility requires microscopy observation, which is challenging due to the fast-moving spermatozoa in the field of view. To obtain correct results, manual evaluation requires extensive training. Therefore, computer-assisted sperm analysis (CASA) has become increasingly used in clinics. Despite this, more data is needed to train supervised machine learning approaches in order to improve accuracy and reliability in the assessment of sperm motility and kinematics. In this regard, we provide a dataset called VISEM-Tracking with 20 video recordings of 30 seconds (comprising 29,196 frames) of wet sperm preparations with manually annotated bounding-box coordinates and a set of sperm characteristics analyzed by experts in the domain. In addition to the annotated data, we provide unlabeled video clips for easy-to-use access and analysis of the data via methods such as selfor unsupervised learning. As part of this paper, we present baseline sperm detection performan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21435;&#20013;&#24515;&#21270;&#36882;&#24402;&#26799;&#24230;&#19978;&#21319;&#27861;&#65288;DREAM&#65289;&#30340;&#31616;&#21333;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21435;&#20013;&#24515;&#21270;&#38750;&#20984;&#26497;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#23547;&#25214;&#21407;&#20989;&#25968;&#30340; $\epsilon$-&#31283;&#23450;&#28857;&#30340;&#26368;&#20339;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2212.02387</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#38750;&#20984;&#26497;&#23567;&#21270;&#38382;&#39064;&#38543;&#26426;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simple and Efficient Stochastic Algorithm for Decentralized Nonconvex-Strongly-Concave Minimax Optimization. (arXiv:2212.02387v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21435;&#20013;&#24515;&#21270;&#36882;&#24402;&#26799;&#24230;&#19978;&#21319;&#27861;&#65288;DREAM&#65289;&#30340;&#31616;&#21333;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21435;&#20013;&#24515;&#21270;&#38750;&#20984;&#26497;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#23547;&#25214;&#21407;&#20989;&#25968;&#30340; $\epsilon$-&#31283;&#23450;&#28857;&#30340;&#26368;&#20339;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21435;&#20013;&#24515;&#21270;&#38750;&#20984;&#26497;&#23567;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#21435;&#20013;&#24515;&#21270;&#36882;&#24402;&#26799;&#24230;&#19978;&#21319;&#27861;&#65288;\texttt{DREAM}&#65289;&#65292;&#23427;&#23454;&#29616;&#20102;&#23547;&#25214;&#21407;&#20989;&#25968;&#30340;$\epsilon$-&#31283;&#23450;&#28857;&#30340;&#26368;&#20339;&#24050;&#30693;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#22312;&#32447;&#35774;&#32622;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#38656;&#35201;$\mathcal{O}(\kappa^3\epsilon^{-3})$&#38543;&#26426;&#19968;&#38454;&#39044;&#35328;&#26426;&#65288;SFO&#65289;&#35843;&#29992;&#20197;&#21450;$\mathcal{O}\big(\kappa^2\epsilon^{-2}/\sqrt{1-\lambda_2(W)}\,\big)$&#36890;&#20449;&#36718;&#27425;&#26469;&#25214;&#21040;$\epsilon$-&#31283;&#23450;&#28857;&#65292;&#20854;&#20013;$\kappa$&#26159;&#26465;&#20214;&#25968;&#65292;$\lambda_2(W)$&#26159;&#20843;&#21350;&#30697;&#38453;$W$&#30340;&#27425;&#22823;&#29305;&#24449;&#20540;&#12290;&#23545;&#20110;&#23436;&#20840;&#30001;$N$&#20010;&#20998;&#37327;&#20989;&#25968;&#32452;&#25104;&#30340;&#31163;&#32447;&#35774;&#32622;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#38656;&#35201;$\mathcal{O}\big(\kappa^2 \sqrt{N} \epsilon^{-2}\big)$ SFO &#35843;&#29992;&#21644;&#19982;&#22312;&#32447;&#35774;&#32622;&#30456;&#21516;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the stochastic optimization for decentralized nonconvex-strongly-concave minimax problem. We propose a simple and efficient algorithm, called Decentralized Recursive-gradient descEnt Ascent Method (\texttt{DREAM}), which achieves the best-known theoretical guarantee for finding the $\epsilon$-stationary point of the primal function. For the online setting, the proposed method requires $\mathcal{O}(\kappa^3\epsilon^{-3})$ stochastic first-order oracle (SFO) calls and $\mathcal{O}\big(\kappa^2\epsilon^{-2}/\sqrt{1-\lambda_2(W)}\,\big)$ communication rounds to find an $\epsilon$-stationary point, where $\kappa$ is the condition number and $\lambda_2(W)$ is the second-largest eigenvalue of the gossip matrix~$W$. For the offline setting with totally $N$ component functions, the proposed method requires $\mathcal{O}\big(\kappa^2 \sqrt{N} \epsilon^{-2}\big)$ SFO calls and the same communication complexity as the online setting.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#26597;&#35810;&#30340;&#22810;&#27169;&#24577;&#36335;&#24452;&#34701;&#21512;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#30693;&#35782;&#24211;&#36827;&#34892;&#34917;&#20840;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#22522;&#20110;&#26597;&#35810;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.01923</link><description>&lt;p&gt;
&#22522;&#20110;&#26597;&#35810;&#30340;&#22810;&#27169;&#24577;&#36335;&#24452;&#34701;&#21512;&#30693;&#35782;&#24211;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Query-Driven Knowledge Base Completion using Multimodal Path Fusion over Multimodal Knowledge Graph. (arXiv:2212.01923v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01923
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26597;&#35810;&#30340;&#22810;&#27169;&#24577;&#36335;&#24452;&#34701;&#21512;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#30693;&#35782;&#24211;&#36827;&#34892;&#34917;&#20840;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#22522;&#20110;&#26597;&#35810;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#22823;&#22411;&#30693;&#35782;&#24211;&#24050;&#32463;&#34987;&#26500;&#24314;&#26469;&#23384;&#20648;&#22823;&#37327;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30693;&#35782;&#24211;&#39640;&#24230;&#19981;&#23436;&#25972;&#65292;&#20363;&#22914;Freebase&#20013;&#26377;70%&#30340;&#20154;&#27809;&#26377;&#20986;&#29983;&#22320;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#12289;&#22522;&#20110;&#26597;&#35810;&#39537;&#21160;&#30340;&#30693;&#35782;&#24211;&#34917;&#20840;&#31995;&#32479;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#34701;&#21512;&#26469;&#33258;Web&#30340;&#38750;&#32467;&#26500;&#21270;&#20449;&#24687;&#21644;&#30693;&#35782;&#24211;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22522;&#20110;&#38382;&#31572;&#21644;&#35268;&#21017;&#25512;&#29702;&#26500;&#24314;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#36335;&#24452;&#34701;&#21512;&#31639;&#27861;&#65292;&#22312;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#20013;&#22522;&#20110;&#19981;&#21516;&#30340;&#36335;&#24452;&#23545;&#20505;&#36873;&#31572;&#26696;&#36827;&#34892;&#25490;&#21517;&#65292;&#21462;&#24471;&#20102;&#27604;&#38382;&#31572;&#12289;&#35268;&#21017;&#25512;&#29702;&#21644;&#22522;&#32447;&#34701;&#21512;&#31639;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#39640;&#31995;&#32479;&#25928;&#29575;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22522;&#20110;&#26597;&#35810;&#30340;&#25216;&#26415;&#26469;&#20943;&#23569;&#31995;&#32479;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#20026;&#29992;&#25143;&#26597;&#35810;&#25552;&#20379;&#24555;&#36895;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, large knowledge bases have been constructed to store massive amounts of knowledge. However, these knowledge bases are highly incomplete, for example, over 70% of people in Freebase have no known place of birth. To solve this problem, we propose a query-driven knowledge base completion system with multimodal fusion of unstructured and structured information. To effectively fuse unstructured information from the Web and structured information in knowledge bases to achieve good performance, our system builds multimodal knowledge graphs based on question answering and rule inference. We propose a multimodal path fusion algorithm to rank candidate answers based on different paths in the multimodal knowledge graphs, achieving much better performance than question answering, rule inference and a baseline fusion algorithm. To improve system efficiency, query-driven techniques are utilized to reduce the runtime of our system, providing fast responses to user queries. Ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#25215;&#35834;&#21644;&#22122;&#22768;&#35266;&#27979;&#30340;$2\times 2$&#38646;&#21644;&#21338;&#24328;&#65292;&#21457;&#29616;&#24179;&#34913;&#28857;&#24635;&#26159;&#23384;&#22312;&#30340;&#65307;&#39046;&#23548;&#32773;&#30340;&#21160;&#20316;&#35266;&#27979;&#32467;&#26524;&#23545;&#20110;&#36861;&#38543;&#32773;&#26469;&#35828;&#35201;&#20040;&#26159;&#26377;&#30410;&#30340;&#65292;&#35201;&#20040;&#26159;&#26080;&#20851;&#32039;&#35201;&#30340;&#65307;&#35813;&#21338;&#24328;&#30340;&#25910;&#30410;&#22312;&#22343;&#34913;&#28857;&#19978;&#34987;&#19978;&#30028;&#38480;&#21046;&#20026;&#32431;&#31574;&#30053;&#19979;&#30340;SE&#30340;&#25910;&#30410;&#65292;&#19979;&#30028;&#20026;&#28151;&#21512;&#31574;&#30053;&#19979;&#30340;&#32435;&#20160;&#22343;&#34913;&#30340;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2211.01703</link><description>&lt;p&gt;
&#24102;&#25215;&#35834;&#21644;&#22122;&#22768;&#35266;&#27979;&#30340;$2\times 2$&#38646;&#21644;&#21338;&#24328;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
$2 \times 2$ Zero-Sum Games with Commitments and Noisy Observations. (arXiv:2211.01703v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#25215;&#35834;&#21644;&#22122;&#22768;&#35266;&#27979;&#30340;$2\times 2$&#38646;&#21644;&#21338;&#24328;&#65292;&#21457;&#29616;&#24179;&#34913;&#28857;&#24635;&#26159;&#23384;&#22312;&#30340;&#65307;&#39046;&#23548;&#32773;&#30340;&#21160;&#20316;&#35266;&#27979;&#32467;&#26524;&#23545;&#20110;&#36861;&#38543;&#32773;&#26469;&#35828;&#35201;&#20040;&#26159;&#26377;&#30410;&#30340;&#65292;&#35201;&#20040;&#26159;&#26080;&#20851;&#32039;&#35201;&#30340;&#65307;&#35813;&#21338;&#24328;&#30340;&#25910;&#30410;&#22312;&#22343;&#34913;&#28857;&#19978;&#34987;&#19978;&#30028;&#38480;&#21046;&#20026;&#32431;&#31574;&#30053;&#19979;&#30340;SE&#30340;&#25910;&#30410;&#65292;&#19979;&#30028;&#20026;&#28151;&#21512;&#31574;&#30053;&#19979;&#30340;&#32435;&#20160;&#22343;&#34913;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20197;&#19979;&#20551;&#35774;&#19979;&#30340;$2\times 2$&#38646;&#21644;&#21338;&#24328;&#65306;$(1)$&#20854;&#20013;&#19968;&#20301;&#29609;&#23478;&#65288;&#39046;&#23548;&#32773;&#65289;&#25215;&#35834;&#36890;&#36807;&#37319;&#26679;&#32473;&#23450;&#30340;&#27010;&#29575;&#20998;&#24067;&#65288;&#31574;&#30053;&#65289;&#26469;&#36873;&#25321;&#20182;&#30340;&#21160;&#20316;;$(2)$&#39046;&#23548;&#32773;&#23459;&#24067;&#20182;&#30340;&#21160;&#20316;&#65292;&#36825;&#20010;&#21160;&#20316;&#36890;&#36807;&#20108;&#36827;&#21046;&#20449;&#36947;&#34987;&#23545;&#25163;&#65288;&#36861;&#38543;&#32773;&#65289;&#35266;&#23519;&#21040;;$(3)$&#36861;&#38543;&#32773;&#22522;&#20110;&#39046;&#23548;&#32773;&#30340;&#31574;&#30053;&#21644;&#39046;&#23548;&#32773;&#21160;&#20316;&#30340;&#22122;&#22768;&#35266;&#27979;&#26469;&#36873;&#25321;&#22905;&#30340;&#31574;&#30053;&#12290;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#24179;&#34913;&#28857;&#34987;&#35777;&#26126;&#24635;&#26159;&#23384;&#22312;&#30340;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#21363;&#20351;&#21463;&#21040;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#35266;&#23519;&#39046;&#23548;&#32773;&#30340;&#34892;&#21160;&#23545;&#36861;&#38543;&#32773;&#26469;&#35828;&#23454;&#36136;&#19978;&#35201;&#20040;&#26159;&#26377;&#30410;&#30340;&#65292;&#35201;&#20040;&#26159;&#26080;&#20851;&#32039;&#35201;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#36825;&#20010;&#21338;&#24328;&#30340;&#22343;&#34913;&#28857;&#19978;&#65292;&#25910;&#30410;&#34987;&#19978;&#30028;&#38480;&#21046;&#20026;&#32431;&#31574;&#30053;&#19979;SE&#30340;&#25910;&#30410;&#65307;&#24182;&#19988;&#19979;&#30028;&#20026;&#32435;&#20160;&#22343;&#34913;&#30340;&#25910;&#30410;&#65292;&#36825;&#31561;&#20215;&#20110;&#28151;&#21512;&#31574;&#30053;&#19979;&#30340;SE&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#30340;&#26465;&#20214;&#26469;&#35266;&#23519;&#22343;&#34913;&#28857;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, $2\times2$ zero-sum games are studied under the following assumptions: $(1)$ One of the players (the leader) commits to choose its actions by sampling a given probability measure (strategy); $(2)$ The leader announces its action, which is observed by its opponent (the follower) through a binary channel; and $(3)$ the follower chooses its strategy based on the knowledge of the leader's strategy and the noisy observation of the leader's action. Under these conditions, the equilibrium is shown to always exist. Interestingly, even subject to noise, observing the actions of the leader is shown to be either beneficial or immaterial for the follower. More specifically, the payoff at the equilibrium of this game is upper bounded by the payoff at the Stackelberg equilibrium (SE) in pure strategies; and lower bounded by the payoff at the Nash equilibrium, which is equivalent to the SE in mixed strategies.Finally, necessary and sufficient conditions for observing the payoff at equi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21033;&#29992;&#26102;&#21464;&#29305;&#24449;&#35843;&#21046;&#26469;&#27169;&#25311;&#40657;&#30418;&#38899;&#39057;&#25928;&#24212;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#33719;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#36866;&#29992;&#20110;fuzz&#21644;&#21387;&#32553;&#31561;&#38899;&#39057;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2211.00497</link><description>&lt;p&gt;
&#21033;&#29992;&#26102;&#21464;&#29305;&#24449;&#35843;&#21046;&#27169;&#25311;&#40657;&#30418;&#38899;&#39057;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Modelling black-box audio effects with time-varying feature modulation. (arXiv:2211.00497v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00497
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21033;&#29992;&#26102;&#21464;&#29305;&#24449;&#35843;&#21046;&#26469;&#27169;&#25311;&#40657;&#30418;&#38899;&#39057;&#25928;&#24212;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#33719;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#36866;&#29992;&#20110;fuzz&#21644;&#21387;&#32553;&#31561;&#38899;&#39057;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#40657;&#30418;&#24314;&#27169;&#38899;&#25928;&#24050;&#26174;&#31034;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#20855;&#26377;&#30456;&#23545;&#36739;&#30701;&#26102;&#38388;&#23610;&#24230;&#34892;&#20026;&#30340;&#38750;&#32447;&#24615;&#25928;&#24212;&#65292;&#20363;&#22914;&#21513;&#20182;&#25918;&#22823;&#22120;&#21644;&#22833;&#30495;&#12290;&#34429;&#28982;&#36882;&#24402;&#21644;&#21367;&#31215;&#32467;&#26500;&#22312;&#29702;&#35770;&#19978;&#21487;&#20197;&#25193;&#23637;&#21040;&#38271;&#26102;&#38388;&#23610;&#24230;&#26469;&#25429;&#33719;&#34892;&#20026;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#22320;&#25193;&#23637;&#29616;&#26377;&#32467;&#26500;&#30340;&#23485;&#24230;&#12289;&#28145;&#24230;&#25110;&#33192;&#32960;&#22240;&#23376;&#19981;&#33021;&#20196;&#20854;&#22312;&#27169;&#25311;fuzz&#21644;&#21160;&#24577;&#33539;&#22260;&#21387;&#32553;&#31561;&#38899;&#39057;&#25928;&#24212;&#26102;&#34920;&#29616;&#24471;&#21313;&#20998;&#20196;&#20154;&#28385;&#24847;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#29616;&#26377;&#30340;&#26102;&#38388;&#21367;&#31215;&#39592;&#24178;&#20013;&#25972;&#21512;&#26102;&#21464;&#29305;&#24449;&#30340;&#32447;&#24615;&#35843;&#21046;&#30340;&#26041;&#27861;&#65292;&#20351;&#20013;&#38388;&#28608;&#27963;&#21487;&#20197;&#36827;&#34892;&#21487;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#20934;&#30830;&#25429;&#33719;&#20102;&#19968;&#31995;&#21015;fuzz&#21644;&#21387;&#32553;&#23454;&#29616;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#21253;&#25324;&#26102;&#38388;&#21644;&#39057;&#29575;&#39046;&#22495;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22768;&#38899;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning approaches for black-box modelling of audio effects have shown promise, however, the majority of existing work focuses on nonlinear effects with behaviour on relatively short time-scales, such as guitar amplifiers and distortion. While recurrent and convolutional architectures can theoretically be extended to capture behaviour at longer time scales, we show that simply scaling the width, depth, or dilation factor of existing architectures does not result in satisfactory performance when modelling audio effects such as fuzz and dynamic range compression. To address this, we propose the integration of time-varying feature-wise linear modulation into existing temporal convolutional backbones, an approach that enables learnable adaptation of the intermediate activations. We demonstrate that our approach more accurately captures long-range dependencies for a range of fuzz and compressor implementations across both time and frequency domain metrics. We provide sound examples, s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#20445;&#25252;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21487;&#20197;&#36924;&#36817;&#20960;&#20046;&#21608;&#26399;&#36763;&#26144;&#23556;&#65292;&#24182;&#24341;&#20986;&#31163;&#25955;&#26102;&#38388;&#32477;&#28909;&#19981;&#21464;&#37327;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.05087</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#20445;&#25252;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#20960;&#20046;&#21608;&#26399;&#36763;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Approximation of nearly-periodic symplectic maps via structure-preserving neural networks. (arXiv:2210.05087v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#20445;&#25252;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21487;&#20197;&#36924;&#36817;&#20960;&#20046;&#21608;&#26399;&#36763;&#26144;&#23556;&#65292;&#24182;&#24341;&#20986;&#31163;&#25955;&#26102;&#38388;&#32477;&#28909;&#19981;&#21464;&#37327;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21442;&#25968; $\varepsilon$ &#36235;&#36817;&#20110; 0 &#26102;&#65292;&#20855;&#26377;&#21442;&#25968; $\varepsilon$ &#30340;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#23398;&#31995;&#32479;&#28385;&#36275;&#25152;&#26377;&#36712;&#36857;&#22343;&#20026;&#21608;&#26399;&#24615;&#65292;&#19988;&#35282;&#39057;&#29575;&#19981;&#20250;&#28040;&#22833;&#12290;&#20960;&#20046;&#21608;&#26399;&#26144;&#23556;&#26159;&#20960;&#20046;&#21608;&#26399;&#31995;&#32479;&#30340;&#31163;&#25955;&#26102;&#38388;&#27169;&#25311;&#65292;&#23450;&#20041;&#20026;&#21442;&#25968;&#20381;&#36182;&#30340;&#24494;&#20998;&#21516;&#32986;&#65292;&#24403;&#26497;&#38480;&#26059;&#36716;&#38750;&#20849;&#25391;&#26102;&#65292;&#23427;&#20204;&#20801;&#35768;&#24418;&#24335;&#19978;&#30340;&#25152;&#26377;&#38454; $U(1)$ &#23545;&#31216;&#24615;&#12290;&#23545;&#20110;&#31934;&#30830;&#36763;&#39044;&#36777;&#31354;&#38388;&#19978;&#30340;&#21704;&#23494;&#39039;&#20960;&#20046;&#21608;&#26399;&#26144;&#23556;&#65292;&#24418;&#24335;&#19978;&#30340; $U(1)$ &#23545;&#31216;&#24615;&#24341;&#20986;&#20102;&#19968;&#20010;&#31163;&#25955;&#26102;&#38388;&#32477;&#28909;&#19981;&#21464;&#37327;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#20445;&#25252;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#36924;&#36817;&#20960;&#20046;&#21608;&#26399;&#36763;&#26144;&#23556;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#36763;&#38464;&#34746;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20445;&#35777;&#20102;&#25152;&#24471;&#21040;&#30340;&#27169;&#25311;&#26144;&#23556;&#26159;&#20960;&#20046;&#21608;&#26399;&#21644;&#36763;&#12289;&#24182;&#24341;&#20986;&#20102;&#31163;&#25955;&#26102;&#38388;&#32477;&#28909;&#19981;&#21464;&#37327;&#21644;&#38271;&#26102;&#38388;&#31283;&#23450;&#24615;&#12290;&#36825;&#19968;&#32467;&#26500;&#20445;&#25252;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#20026;&#20960;&#20046;&#21608;&#26399;&#36763;&#26144;&#23556;&#30340;&#25968;&#20540;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
A continuous-time dynamical system with parameter $\varepsilon$ is nearly-periodic if all its trajectories are periodic with nowhere-vanishing angular frequency as $\varepsilon$ approaches 0. Nearly-periodic maps are discrete-time analogues of nearly-periodic systems, defined as parameter-dependent diffeomorphisms that limit to rotations along a circle action, and they admit formal $U(1)$ symmetries to all orders when the limiting rotation is non-resonant. For Hamiltonian nearly-periodic maps on exact presymplectic manifolds, the formal $U(1)$ symmetry gives rise to a discrete-time adiabatic invariant. In this paper, we construct a novel structure-preserving neural network to approximate nearly-periodic symplectic maps. This neural network architecture, which we call symplectic gyroceptron, ensures that the resulting surrogate map is nearly-periodic and symplectic, and that it gives rise to a discrete-time adiabatic invariant and a long-time stability. This new structure-preserving neu
&lt;/p&gt;</description></item><item><title>Learnware&#33539;&#24335;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#20805;&#20998;&#21033;&#29992;&#23567;&#22411;&#27169;&#22411;&#65292;&#23454;&#29616;&#36229;&#36234;&#21407;&#22987;&#30446;&#30340;&#30340;&#20107;&#24773;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#38754;&#20020;&#30340;&#25968;&#25454;&#37327;&#22823;&#12289;&#35757;&#32451;&#38590;&#24230;&#39640;&#12289;&#25968;&#25454;&#23433;&#20840;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.03647</link><description>&lt;p&gt;
Learnware: &#23567;&#27169;&#22411;&#23454;&#29616;&#22823;&#20316;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learnware: Small Models Do Big. (arXiv:2210.03647v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03647
&lt;/p&gt;
&lt;p&gt;
Learnware&#33539;&#24335;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#20805;&#20998;&#21033;&#29992;&#23567;&#22411;&#27169;&#22411;&#65292;&#23454;&#29616;&#36229;&#36234;&#21407;&#22987;&#30446;&#30340;&#30340;&#20107;&#24773;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#38754;&#20020;&#30340;&#25968;&#25454;&#37327;&#22823;&#12289;&#35757;&#32451;&#38590;&#24230;&#39640;&#12289;&#25968;&#25454;&#23433;&#20840;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23384;&#22312;&#35757;&#32451;&#25968;&#25454;&#37327;&#22823;&#12289;&#35757;&#32451;&#25216;&#33021;&#39640;&#12289;&#36830;&#32493;&#23398;&#20064;&#38590;&#12289;&#36951;&#24536;&#39118;&#38505;&#22823;&#12289;&#25968;&#25454;&#38544;&#31169;&#21644;&#19987;&#26377;&#20449;&#24687;&#27844;&#38706;&#31561;&#38382;&#39064;&#65292;&#32780;&#36807;&#21435;&#30340;&#22823;&#27169;&#22411;&#33539;&#24335;&#34429;&#28982;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#32467;&#26524;&#65292;&#20294;&#24182;&#26410;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#21453;&#32780;&#25104;&#20026;&#20005;&#37325;&#30340;&#30899;&#25490;&#25918;&#28304;&#12290;&#35813;&#25991;&#27010;&#36848;&#20102;Learnware&#33539;&#24335;&#65292;&#35753;&#29992;&#25143;&#19981;&#38656;&#35201;&#20174;&#22836;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24076;&#26395;&#21033;&#29992;&#23567;&#22411;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#36229;&#36234;&#21407;&#22987;&#30446;&#30340;&#30340;&#20107;&#24773;&#65292;&#20854;&#20013;&#20851;&#38190;&#26159;&#35268;&#33539;&#65292;&#21487;&#20197;&#20351;&#35757;&#32451;&#30340;&#27169;&#22411;&#24471;&#21040;&#20805;&#20998;&#37492;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are complaints about current machine learning techniques such as the requirement of a huge amount of training data and proficient training skills, the difficulty of continual learning, the risk of catastrophic forgetting, the leaking of data privacy/proprietary, etc. Most research efforts have been focusing on one of those concerned issues separately, paying less attention to the fact that most issues are entangled in practice. The prevailing big model paradigm, which has achieved impressive results in natural language processing and computer vision applications, has not yet addressed those issues, whereas becoming a serious source of carbon emissions. This article offers an overview of the learnware paradigm, which attempts to enable users not need to build machine learning models from scratch, with the hope of reusing small models to do things even beyond their original purposes, where the key ingredient is the specification which enables a trained model to be adequately identi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26080;&#20154;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#25506;&#32034;&#26080;&#32447;&#30005;&#39057;&#35889;&#27963;&#21160;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26080;&#20154;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21644;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#31934;&#20934;&#30340;&#39057;&#35889;&#27963;&#21160;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2210.02899</link><description>&lt;p&gt;
&#26080;&#20154;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#26080;&#32447;&#30005;&#39057;&#35889;&#27963;&#21160;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Learning for Clustering of Wireless Spectrum Activity. (arXiv:2210.02899v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26080;&#20154;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#25506;&#32034;&#26080;&#32447;&#30005;&#39057;&#35889;&#27963;&#21160;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26080;&#20154;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21644;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#31934;&#20934;&#30340;&#39057;&#35889;&#27963;&#21160;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22788;&#29702;&#26080;&#32447;&#30005;&#39057;&#35889;&#25968;&#25454;&#20197;&#35299;&#20915;&#35748;&#30693;&#26080;&#32447;&#30005;&#32593;&#32476;&#30456;&#20851;&#38382;&#39064;&#65292;&#22914;&#24322;&#24120;&#26816;&#27979;&#12289;&#35843;&#21046;&#20998;&#31867;&#12289;&#25216;&#26415;&#20998;&#31867;&#21644;&#35774;&#22791;&#25351;&#32441;&#31561;&#39046;&#22495;&#65292;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#12290;&#22823;&#22810;&#25968;&#35299;&#20915;&#26041;&#26696;&#37117;&#26159;&#22522;&#20110;&#21463;&#25511;&#21046;&#30340;&#12289;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#30417;&#30563;&#24335;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#19979;&#27979;&#37327;&#30340;&#39057;&#35889;&#25968;&#25454;&#39640;&#24230;&#19981;&#30830;&#23450;&#65292;&#20854;&#26631;&#35760;&#26159;&#19968;&#39033;&#36153;&#26102;&#19988;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#22240;&#27492;&#25104;&#20026;&#22312;&#35813;&#39046;&#22495;&#20351;&#29992;&#30417;&#30563;&#24335;&#23398;&#20064;&#26041;&#27861;&#30340;&#20027;&#35201;&#32570;&#28857;&#20043;&#19968;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#21033;&#29992;&#26080;&#20154;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26469;&#25506;&#32034;&#39057;&#35889;&#27963;&#21160;&#30340;&#20351;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181; SSL &#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#19968;&#31181;&#22522;&#20110; DeepCluster &#21442;&#32771;&#20307;&#31995;&#32467;&#26500;&#65292;&#21478;&#19968;&#31181;&#36866;&#29992;&#20110;&#39057;&#35889;&#27963;&#21160;&#35782;&#21035;&#21644;&#32858;&#31867;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512; SSL &#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#20004;&#31181;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#36825;&#19977;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#39057;&#35889;&#27963;&#21160;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#65292;&#28151;&#21512;&#26041;&#27861;&#30340;&#24615;&#33021;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, much work has been done on processing of wireless spectrum data involving machine learning techniques in domain-related problems for cognitive radio networks, such as anomaly detection, modulation classification, technology classification and device fingerprinting. Most of the solutions are based on labeled data, created in a controlled manner and processed with supervised learning approaches. However, spectrum data measured in real-world environment is highly nondeterministic, making its labeling a laborious and expensive process, requiring domain expertise, thus being one of the main drawbacks of using supervised learning approaches in this domain. In this paper, we investigate the use of self-supervised learning (SSL) for exploring spectrum activities in a real-world unlabeled data. In particular, we compare the performance of two SSL models, one based on a reference DeepCluster architecture and one adapted for spectrum activity identification and clustering, and a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#38543;&#26426;&#31995;&#32479;&#20013;&#31232;&#26377;&#36712;&#36857;&#30340;&#25216;&#26415;&#23450;&#20041;&#21644;&#35757;&#32451;NNE&#65292;&#36890;&#36807;&#23545;&#36712;&#36857;&#36827;&#34892;&#20559;&#32622;&#26469;&#35757;&#32451;NNE&#65292;&#30456;&#36739;&#20110;&#26356;&#20256;&#32479;&#30340;&#26799;&#24230;&#26041;&#27861;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2209.11116</link><description>&lt;p&gt;
&#36890;&#36807;&#36712;&#36857;&#25277;&#26679;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Training neural network ensembles via trajectory sampling. (arXiv:2209.11116v2 [cond-mat.stat-mech] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#38543;&#26426;&#31995;&#32479;&#20013;&#31232;&#26377;&#36712;&#36857;&#30340;&#25216;&#26415;&#23450;&#20041;&#21644;&#35757;&#32451;NNE&#65292;&#36890;&#36807;&#23545;&#36712;&#36857;&#36827;&#34892;&#20559;&#32622;&#26469;&#35757;&#32451;NNE&#65292;&#30456;&#36739;&#20110;&#26356;&#20256;&#32479;&#30340;&#26799;&#24230;&#26041;&#27861;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;(NNEs)&#36817;&#24180;&#26469;&#20877;&#27425;&#24341;&#36215;&#20851;&#27880;&#12290;&#23427;&#36890;&#36807;&#20174;&#22810;&#20010;&#36739;&#23567;&#30340;&#27169;&#22411;&#20013;&#33719;&#24471;&#39044;&#27979;&#32467;&#26524;&#65292;&#32780;&#19981;&#26159;&#20174;&#21333;&#20010;&#26356;&#22823;&#30340;&#27169;&#22411;&#20013;&#33719;&#24471;&#32467;&#26524;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#38543;&#26426;&#31995;&#32479;&#20013;&#31232;&#26377;&#36712;&#36857;&#30340;&#25216;&#26415;&#26469;&#23450;&#20041;&#21644;&#35757;&#32451;NNE&#12290;&#25105;&#20204;&#26681;&#25454;&#27169;&#22411;&#21442;&#25968;&#22312;&#31616;&#21333;&#19988;&#26102;&#38388;&#31163;&#25955;&#30340;&#25193;&#25955;&#21160;&#21147;&#23398;&#19979;&#30340;&#36712;&#36857;&#26469;&#23450;&#20041;NNE&#65292;&#24182;&#36890;&#36807;&#20559;&#32622;&#36825;&#20123;&#36712;&#36857;&#26469;&#35757;&#32451;NNE&#20197;&#36798;&#21040;&#23567;&#30340;&#26102;&#38388;&#31215;&#20998;&#35823;&#24046;&#65292;&#36825;&#30001;&#36866;&#24403;&#30340;&#35745;&#25968;&#22330;&#20316;&#20026;&#36229;&#21442;&#25968;&#26469;&#25511;&#21046;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#31616;&#21333;&#30340;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#36825;&#31181;&#25216;&#26415;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#36712;&#36857;&#25277;&#26679;&#26041;&#27861;&#19982;&#26356;&#20256;&#32479;&#30340;&#26799;&#24230;&#26041;&#27861;&#30456;&#27604;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning, there is renewed interest in neural network ensembles (NNEs), whereby predictions are obtained as an aggregate from a diverse set of smaller models, rather than from a single larger model. Here, we show how to define and train a NNE using techniques from the study of rare trajectories in stochastic systems. We define an NNE in terms of the trajectory of the model parameters under a simple, and discrete in time, diffusive dynamics, and train the NNE by biasing these trajectories towards a small time-integrated loss, as controlled by appropriate counting fields which act as hyperparameters. We demonstrate the viability of this technique on a range of simple supervised learning tasks. We discuss potential advantages of our trajectory sampling approach compared with more conventional gradient based methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#38750;&#36845;&#20195;&#29983;&#25104;&#20219;&#24847;&#21494;&#29255;&#36890;&#36947;&#26368;&#20248;&#32593;&#26684;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#21644;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2209.05280</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38750;&#36845;&#20195;&#29983;&#25104;&#21494;&#29255;&#36890;&#36947;&#26368;&#20248;&#32593;&#26684;
&lt;/p&gt;
&lt;p&gt;
Non-iterative generation of an optimal mesh for a blade passage using deep reinforcement learning. (arXiv:2209.05280v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#38750;&#36845;&#20195;&#29983;&#25104;&#20219;&#24847;&#21494;&#29255;&#36890;&#36947;&#26368;&#20248;&#32593;&#26684;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#21644;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#26469;&#38750;&#36845;&#20195;&#29983;&#25104;&#20219;&#24847;&#21494;&#29255;&#36890;&#36947;&#26368;&#20248;&#32593;&#26684;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#20351;&#29992;&#32463;&#39564;&#27861;&#25110;&#20248;&#21270;&#31639;&#27861;&#33258;&#21160;&#21270;&#32593;&#26684;&#29983;&#25104;&#65292;&#20294;&#23545;&#20110;&#26032;&#20960;&#20309;&#32467;&#26500;&#20173;&#38656;&#35201;&#21453;&#22797;&#35843;&#25972;&#32593;&#26684;&#21442;&#25968;&#12290;&#26412;&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;DRL&#30340;&#22810;&#26465;&#20214;&#20248;&#21270;&#25216;&#26415;&#65292;&#20197;&#21494;&#29255;&#20960;&#20309;&#24418;&#29366;&#20026;&#20989;&#25968;&#23450;&#20041;&#26368;&#20248;&#32593;&#26684;&#21442;&#25968;&#65292;&#23454;&#29616;&#33258;&#21160;&#21270;&#12289;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#21644;&#35745;&#31639;&#25928;&#29575;&#26368;&#23567;&#21270;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#26925;&#22278;&#32593;&#26684;&#29983;&#25104;&#22120;&#23545;&#32593;&#26684;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#35813;&#29983;&#25104;&#22120;&#20026;&#20219;&#24847;&#21494;&#29255;&#20960;&#20309;&#24418;&#29366;&#29983;&#25104;&#32467;&#26500;&#21270;&#32593;&#26684;&#12290;&#22312;DRL&#36807;&#31243;&#30340;&#27599;&#20010;&#22238;&#21512;&#20013;&#65292;&#36890;&#36807;&#26356;&#26032;&#32593;&#26684;&#21442;&#25968;&#65292;&#35757;&#32451;&#29983;&#25104;&#22120;&#29983;&#25104;&#38543;&#26426;&#36873;&#25321;&#30340;&#21494;&#29255;&#36890;&#36947;&#30340;&#26368;&#20248;&#32593;&#26684;&#65292;&#21516;&#26102;&#35780;&#20272;&#32593;&#26684;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
A method using deep reinforcement learning (DRL) to non-iteratively generate an optimal mesh for an arbitrary blade passage is developed. Despite automation in mesh generation using either an empirical approach or an optimization algorithm, repeated tuning of meshing parameters is still required for a new geometry. The method developed herein employs a DRL-based multi-condition optimization technique to define optimal meshing parameters as a function of the blade geometry, attaining automation, minimization of human intervention, and computational efficiency. The meshing parameters are optimized by training an elliptic mesh generator which generates a structured mesh for a blade passage with an arbitrary blade geometry. During each episode of the DRL process, the mesh generator is trained to produce an optimal mesh for a randomly selected blade passage by updating the meshing parameters until the mesh quality, as measured by the ratio of determinants of the Jacobian matrices and the sk
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#27493;&#39588;&#30340;&#36827;&#21270;&#24335;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;FreeREA&#65292;&#21487;&#30452;&#25509;&#22312;&#30446;&#26631;&#30828;&#20214;&#19978;&#20248;&#21270;&#25628;&#32034;&#65292;&#19988;&#33021;&#22815;&#22312;&#24615;&#33021;&#26368;&#22823;&#21270;&#30340;&#21516;&#26102;&#65292;&#26497;&#22823;&#22320;&#20943;&#23567;&#20869;&#23384;&#21344;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#27604;&#25163;&#21160;&#35774;&#35745;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2207.05135</link><description>&lt;p&gt;
FreeREA: &#26080;&#38656;&#35757;&#32451;&#30340;&#36827;&#21270;&#24335;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FreeREA: Training-Free Evolution-based Architecture Search. (arXiv:2207.05135v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#27493;&#39588;&#30340;&#36827;&#21270;&#24335;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;FreeREA&#65292;&#21487;&#30452;&#25509;&#22312;&#30446;&#26631;&#30828;&#20214;&#19978;&#20248;&#21270;&#25628;&#32034;&#65292;&#19988;&#33021;&#22815;&#22312;&#24615;&#33021;&#26368;&#22823;&#21270;&#30340;&#21516;&#26102;&#65292;&#26497;&#22823;&#22320;&#20943;&#23567;&#20869;&#23384;&#21344;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#27604;&#25163;&#21160;&#35774;&#35745;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#26159;&#20026;&#20102;&#25913;&#36827;&#29616;&#26377;&#27169;&#22411;&#65292;&#20197;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#19981;&#21516;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#36827;&#27493;&#24448;&#24448;&#20197;&#22686;&#21152;&#27169;&#22411;&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#20195;&#20215;&#20026;&#20195;&#20215;&#12290;&#36825;&#23545;&#20110;&#30740;&#31350;&#25104;&#26524;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#37096;&#32626;&#20855;&#26377;&#30456;&#24403;&#22823;&#30340;&#38480;&#21046;&#24615;&#65292;&#20854;&#20013;&#25104;&#26412;&#12289;&#33021;&#28304;&#28040;&#32791;&#21644;&#26694;&#26550;&#30340;&#22797;&#26434;&#24615;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36827;&#21270;&#24335;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;FreeREA&#65292;&#36890;&#36807;&#35813;&#31639;&#27861;&#21487;&#20197;&#24555;&#36895;&#35782;&#21035;&#24615;&#33021;&#26368;&#22823;&#21270;&#19988;&#20869;&#23384;&#21344;&#29992;&#26368;&#23567;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#27493;&#39588;&#65292;&#24182;&#22312;&#30446;&#26631;&#30828;&#20214;&#19978;&#30452;&#25509;&#20248;&#21270;&#26550;&#26500;&#25628;&#32034;&#65292;&#26080;&#38656;&#20195;&#29702;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;FreeREA&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#33021;&#22815;&#22312;&#38656;&#35201;&#23569;&#36798;487&#20493;&#35745;&#31639;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#65292;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#25163;&#21160;&#35774;&#35745;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last decade, most research in Machine Learning contributed to the improvement of existing models, with the aim of increasing the performance of neural networks for the solution of a variety of different tasks. However, such advancements often come at the cost of an increase of model memory and computational requirements. This represents a significant limitation for the deployability of research output in realistic settings, where the cost, the energy consumption, and the complexity of the framework play a crucial role. To solve this issue, the designer should search for models that maximise the performance while limiting its footprint. Typical approaches to reach this goal rely either on manual procedures, which cannot guarantee the optimality of the final design, or upon Neural Architecture Search algorithms to automatise the process, at the expenses of extremely high computational time. This paper provides a solution for the fast identification of a neural network that maximis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#21644;&#32467;&#26500;&#31283;&#23450;&#24615;&#20026;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#25351;&#23450;&#20102;&#26368;&#22823;&#31283;&#23450;&#20998;&#27495;&#35299;&#25968;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;</title><link>http://arxiv.org/abs/2207.04876</link><description>&lt;p&gt;
&#22522;&#20110;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21644;&#32467;&#26500;&#31283;&#23450;&#24615;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Generalization of Spiking Neural Networks via Minimum Description Length and Structural Stability. (arXiv:2207.04876v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#21644;&#32467;&#26500;&#31283;&#23450;&#24615;&#20026;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#25351;&#23450;&#20102;&#26368;&#22823;&#31283;&#23450;&#20998;&#27495;&#35299;&#25968;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#30001;&#20110;&#20854;&#23545;&#20110;&#24314;&#27169;&#26102;&#38388;&#30456;&#20851;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#35768;&#22810;&#32463;&#39564;&#31639;&#27861;&#21644;&#25216;&#26415;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#12290;&#28982;&#32780;&#65292;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#35757;&#32451;&#21518;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#20173;&#28982;&#26159;&#26410;&#30693;&#30340;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#65292;&#20026;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#19968;&#20010;&#26126;&#30830;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#26500;&#31283;&#23450;&#24615;&#23454;&#26045;&#20102;SNN&#30340;&#25551;&#36848;&#38271;&#24230;&#65292;&#24182;&#25351;&#23450;&#20102;&#26368;&#22823;&#31283;&#23450;&#20998;&#27495;&#35299;&#25968;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#65292;&#23558;&#22312;SNN&#20013;&#30830;&#23450;&#32467;&#26500;&#31283;&#23450;&#24615;&#30340;&#25361;&#25112;&#36716;&#21270;&#20026;&#19968;&#20010;&#20855;&#26377;&#23450;&#37327;&#29305;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decades have witnessed an increasing interest in spiking neural networks due to their great potential of modeling time-dependent data. Many empirical algorithms and techniques have been developed. However, theoretically, it remains unknown whether and to what extent a trained spiking neural network performs well on unseen data. This work takes one step in this direction by exploiting the minimum description length principle and thus, presents an explicit generalization bound for spiking neural networks. Further, we implement the description length of SNNs through structural stability and specify the lower and upper bounds of the maximum number of stable bifurcation solutions, which convert the challenge of qualifying structural stability in SNNs into a mathematical problem with quantitative properties.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#31354;&#38388;&#21306;&#22495;&#21010;&#20998;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25104;&#26524;&#12290;&#20854;&#20013;&#65292;&#20004;&#38454;&#27573;K&#27169;&#22411;&#31639;&#27861;&#34920;&#29616;&#26368;&#20339;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#27169;&#22411;&#25311;&#21512;&#12289;&#21306;&#22495;&#37325;&#26500;&#21644;&#31995;&#25968;&#20272;&#35745;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.09429</link><description>&lt;p&gt;
&#23558;&#21306;&#22495;&#21270;&#31639;&#27861;&#25193;&#23637;&#21040;&#25506;&#32034;&#31354;&#38388;&#36827;&#31243;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Extending regionalization algorithms to explore spatial process heterogeneity. (arXiv:2206.09429v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09429
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#31354;&#38388;&#21306;&#22495;&#21010;&#20998;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25104;&#26524;&#12290;&#20854;&#20013;&#65292;&#20004;&#38454;&#27573;K&#27169;&#22411;&#31639;&#27861;&#34920;&#29616;&#26368;&#20339;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#27169;&#22411;&#25311;&#21512;&#12289;&#21306;&#22495;&#37325;&#26500;&#21644;&#31995;&#25968;&#20272;&#35745;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31354;&#38388;&#22238;&#24402;&#27169;&#22411;&#20013;&#65292;&#31354;&#38388;&#24322;&#36136;&#24615;&#21487;&#20197;&#36890;&#36807;&#36830;&#32493;&#25110;&#31163;&#25955;&#35268;&#33539;&#36827;&#34892;&#32771;&#34385;&#12290;&#21518;&#32773;&#28041;&#21450;&#21040;&#30830;&#23450;&#20855;&#26377;&#21516;&#36136;&#21464;&#37327;&#20043;&#38388;&#30340;&#22343;&#36136;&#20851;&#31995;&#30340;&#36830;&#32493;&#31354;&#38388;&#21306;&#22495;&#65288;&#31354;&#38388;&#21306;&#22495;&#65289;&#12290;&#23613;&#31649;&#22312;&#31354;&#38388;&#20998;&#26512;&#39046;&#22495;&#20013;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#21508;&#31181;&#21306;&#22495;&#21270;&#31639;&#27861;&#65292;&#20294;&#20248;&#21270;&#31354;&#38388;&#21306;&#22495;&#30340;&#26041;&#27861;&#22522;&#26412;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#31354;&#38388;&#21306;&#22495;&#21010;&#20998;&#31639;&#27861;&#65292;&#21363;&#20004;&#38454;&#27573;K&#27169;&#22411;&#21644;&#21306;&#22495;K&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#23558;&#32463;&#20856;&#30340;&#33258;&#21160;&#20998;&#21306;&#31243;&#24207;&#25193;&#23637;&#21040;&#31354;&#38388;&#22238;&#24402;&#32972;&#26223;&#19979;&#12290;&#36825;&#20123;&#31639;&#27861;&#34987;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#26377;&#19977;&#31181;&#31639;&#27861;&#22312;&#27169;&#22411;&#25311;&#21512;&#12289;&#21306;&#22495;&#37325;&#26500;&#21644;&#31995;&#25968;&#20272;&#35745;&#26041;&#38754;&#22343;&#36798;&#21040;&#20102;&#36229;&#36234;&#25110;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#32780;&#20004;&#38454;&#27573;K&#27169;&#22411;&#31639;&#27861;&#22312;&#27169;&#22411;&#25311;&#21512;&#12289;&#21306;&#22495;&#37325;&#26500;&#21644;&#31995;&#25968;&#20272;&#35745;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In spatial regression models, spatial heterogeneity may be considered with either continuous or discrete specifications. The latter is related to delineation of spatially connected regions with homogeneous relationships between variables (spatial regimes). Although various regionalization algorithms have been proposed and studied in the field of spatial analytics, methods to optimize spatial regimes have been largely unexplored. In this paper, we propose two new algorithms for spatial regime delineation, two-stage K-Models and Regional-K-Models. We also extend the classic Automatic Zoning Procedure to spatial regression context. The proposed algorithms are applied to a series of synthetic datasets and two real-world datasets. Results indicate that all three algorithms achieve superior or comparable performance to existing approaches, while the two-stage K-Models algorithm largely outperforms existing approaches on model fitting, region reconstruction, and coefficient estimation. Our wo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#25216;&#26415;&#65292;&#21253;&#25324;Vanilla Transformer&#12289;Vision Transformer&#21644;&#22810;&#27169;&#24577;Transformer&#65292;&#22312;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#21644;&#29305;&#23450;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#20849;&#20139;&#30340;&#24120;&#35265;&#25361;&#25112;&#21644;&#35774;&#35745;&#12290;&#24182;&#25506;&#35752;&#20102;&#24320;&#25918;&#30340;&#38382;&#39064;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2206.06488</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Multimodal Learning with Transformers: A Survey. (arXiv:2206.06488v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#25216;&#26415;&#65292;&#21253;&#25324;Vanilla Transformer&#12289;Vision Transformer&#21644;&#22810;&#27169;&#24577;Transformer&#65292;&#22312;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#21644;&#29305;&#23450;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#20849;&#20139;&#30340;&#24120;&#35265;&#25361;&#25112;&#21644;&#35774;&#35745;&#12290;&#24182;&#25506;&#35752;&#20102;&#24320;&#25918;&#30340;&#38382;&#39064;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#22120;&#65292;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#30001;&#20110;&#22810;&#27169;&#24577;&#24212;&#29992;&#21644;&#22823;&#25968;&#25454;&#30340;&#26222;&#21450;&#65292;&#22522;&#20110;Transformer&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#28909;&#38376;&#35805;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#38024;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;Transformer&#25216;&#26415;&#20840;&#38754;&#32508;&#36848;&#12290;&#35813;&#32508;&#36848;&#30340;&#20027;&#35201;&#20869;&#23481;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22810;&#27169;&#24577;&#23398;&#20064;&#19982;Transformer&#29983;&#24577;&#31995;&#32479;&#21450;&#22810;&#27169;&#24577;&#22823;&#25968;&#25454;&#26102;&#20195;&#30340;&#32972;&#26223;&#65292;&#65288;2&#65289;&#20174;&#20960;&#20309;&#25299;&#25169;&#23398;&#30340;&#35282;&#24230;&#23545;Vanilla Transformer&#12289;Vision Transformer&#21644;&#22810;&#27169;&#24577;Transformer&#36827;&#34892;&#29702;&#35770;&#32508;&#36848;&#65292;&#65288;3&#65289;&#36890;&#36807;&#20004;&#31181;&#37325;&#35201;&#33539;&#20363;&#65292;&#21363;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#21644;&#29305;&#23450;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#32508;&#36848;&#22810;&#27169;&#24577;Transformer&#24212;&#29992;&#65292;&#65288;4&#65289;&#24635;&#32467;&#22810;&#27169;&#24577;Transformer&#27169;&#22411;&#21644;&#24212;&#29992;&#25152;&#20849;&#20139;&#30340;&#24120;&#35265;&#25361;&#25112;&#21644;&#35774;&#35745;&#65292;&#65288;5&#65289;&#35752;&#35770;&#31038;&#21306;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer is a promising neural network learner, and has achieved great success in various machine learning tasks. Thanks to the recent prevalence of multimodal applications and big data, Transformer-based multimodal learning has become a hot topic in AI research. This paper presents a comprehensive survey of Transformer techniques oriented at multimodal data. The main contents of this survey include: (1) a background of multimodal learning, Transformer ecosystem, and the multimodal big data era, (2) a theoretical review of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective, (3) a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks, (4) a summary of the common challenges and designs shared by the multimodal Transformer models and applications, and (5) a discussion of open problems and potential research directions for the community
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#22312;&#20195;&#29702;&#21644;&#29615;&#22659;&#30340;&#26080;&#35823;&#24046;&#39640;&#25928;&#37327;&#23376;&#23454;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#27604;&#22522;&#20110;&#32463;&#20856;&#33945;&#29305;&#21345;&#32599;&#30340;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2206.04741</link><description>&lt;p&gt;
&#37327;&#23376;&#31574;&#30053;&#36845;&#20195;&#65306;&#22522;&#20110;&#25391;&#24133;&#20272;&#35745;&#21644;Grover&#25628;&#32034;&#8212;&#8212;&#36208;&#21521;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Quantum Policy Iteration via Amplitude Estimation and Grover Search -- Towards Quantum Advantage for Reinforcement Learning. (arXiv:2206.04741v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04741
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#22312;&#20195;&#29702;&#21644;&#29615;&#22659;&#30340;&#26080;&#35823;&#24046;&#39640;&#25928;&#37327;&#23376;&#23454;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#27604;&#22522;&#20110;&#32463;&#20856;&#33945;&#29305;&#21345;&#32599;&#30340;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23454;&#29616;&#21644;&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35777;&#26126;&#37327;&#23376;&#31639;&#27861;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#22312;&#20195;&#29702;&#21644;&#29615;&#22659;&#30340;&#26080;&#35823;&#24046;&#39640;&#25928;&#37327;&#23376;&#23454;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#37327;&#23376;&#26041;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#21487;&#20197;&#35777;&#26126;&#27604;&#22522;&#20110;&#32463;&#20856;&#33945;&#29305;&#21345;&#32599;&#30340;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35814;&#32454;&#22320;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#25391;&#24133;&#20272;&#35745;&#21644;Grover&#25628;&#32034;&#32452;&#21512;&#25104;&#31574;&#30053;&#35780;&#20272;&#21644;&#25913;&#36827;&#26041;&#26696;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#37327;&#23376;&#31574;&#30053;&#35780;&#20272;&#65288;QPE&#65289;&#65292;&#20854;&#27604;&#19968;&#20010;&#31867;&#20284;&#30340;&#32463;&#20856;&#33945;&#29305;&#21345;&#32599;&#20272;&#35745;&#39640;&#25928;&#24179;&#26041;&#65292;&#22522;&#20110;&#37327;&#23376;&#21147;&#23398;&#23454;&#29616;&#26377;&#38480;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#12290;&#22312;QPE&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#31181;&#37327;&#23376;&#31574;&#30053;&#36845;&#20195;&#65292;&#23427;&#20351;&#29992;Grover&#25628;&#32034;&#37325;&#22797;&#25913;&#36827;&#21021;&#22987;&#31574;&#30053;&#65292;&#30452;&#21040;&#36798;&#21040;&#26368;&#20339;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29616;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a full implementation and simulation of a novel quantum reinforcement learning method. Our work is a detailed and formal proof of concept for how quantum algorithms can be used to solve reinforcement learning problems and shows that, given access to error-free, efficient quantum realizations of the agent and environment, quantum methods can yield provable improvements over classical Monte-Carlo based methods in terms of sample complexity. Our approach shows in detail how to combine amplitude estimation and Grover search into a policy evaluation and improvement scheme. We first develop quantum policy evaluation (QPE) which is quadratically more efficient compared to an analogous classical Monte Carlo estimation and is based on a quantum mechanical realization of a finite Markov decision process (MDP). Building on QPE, we derive a quantum policy iteration that repeatedly improves an initial policy using Grover search until the optimum is reached. Finally, we present an impleme
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;POP&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#23454;&#20363;&#30456;&#20851;&#30340;&#23616;&#37096;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#65292;POP&#22312;&#27599;&#20010;&#26102;&#20195;&#26356;&#26032;&#23398;&#20064;&#27169;&#22411;&#24182;&#36880;&#27493;&#20928;&#21270;&#27599;&#20010;&#20505;&#36873;&#26631;&#31614;&#38598;&#65292;&#33021;&#20197;&#29305;&#23450;&#30340;&#24555;&#36895;&#36895;&#24230;&#25193;&#22823;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2206.00830</link><description>&lt;p&gt;
&#23454;&#20363;&#30456;&#20851;&#30340;&#23616;&#37096;&#26631;&#31614;&#23398;&#20064;&#30340;&#28176;&#36827;&#24335;&#32431;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Progressive Purification for Instance-Dependent Partial Label Learning. (arXiv:2206.00830v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;POP&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#23454;&#20363;&#30456;&#20851;&#30340;&#23616;&#37096;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#65292;POP&#22312;&#27599;&#20010;&#26102;&#20195;&#26356;&#26032;&#23398;&#20064;&#27169;&#22411;&#24182;&#36880;&#27493;&#20928;&#21270;&#27599;&#20010;&#20505;&#36873;&#26631;&#31614;&#38598;&#65292;&#33021;&#20197;&#29305;&#23450;&#30340;&#24555;&#36895;&#36895;&#24230;&#25193;&#22823;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23616;&#37096;&#26631;&#31614;&#23398;&#20064;&#65288;PLL&#65289;&#26088;&#22312;&#20174;&#27599;&#20010;&#27880;&#37322;&#26377;&#20505;&#36873;&#26631;&#31614;&#38598;&#30340;&#31034;&#20363;&#20013;&#35757;&#32451;&#22810;&#31867;&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#19968;&#20010;&#22266;&#23450;&#20294;&#26410;&#30693;&#30340;&#20505;&#36873;&#26631;&#31614;&#26159;&#27491;&#30830;&#30340;&#12290;&#36817;&#20960;&#24180;&#26469;&#65292;&#20505;&#36873;&#26631;&#31614;&#30340;&#26080;&#23454;&#20363;&#29420;&#31435;&#29983;&#25104;&#36807;&#31243;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#22522;&#20110;&#27492;&#22312;PLL&#26041;&#38754;&#21462;&#24471;&#20102;&#35768;&#22810;&#29702;&#35770;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#19978;&#20505;&#36873;&#26631;&#31614;&#24635;&#26159;&#19982;&#23454;&#20363;&#30456;&#20851;&#30340;&#65292;&#27809;&#26377;&#29702;&#35770;&#20445;&#35777;&#22312;&#23454;&#20363;&#30456;&#20851;&#30340;PLL&#31034;&#20363;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#25910;&#25947;&#21040;&#29702;&#24819;&#20540;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;POP&#30340;&#29702;&#35770;&#19978;&#26377;&#26681;&#25454;&#21644;&#23454;&#38469;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21363;&#36880;&#27493;&#32431;&#21270;&#23454;&#20363;&#30456;&#20851;&#30340;&#23616;&#37096;&#26631;&#31614;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;POP&#22312;&#27599;&#20010;&#26102;&#20195;&#26356;&#26032;&#23398;&#20064;&#27169;&#22411;&#24182;&#36880;&#27493;&#20928;&#21270;&#27599;&#20010;&#20505;&#36873;&#26631;&#31614;&#38598;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;POP&#22312;&#21512;&#36866;&#30340;&#24555;&#36895;&#36895;&#24230;&#19979;&#25193;&#22823;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#33539;&#22260;&#65292;&#24182;&#26368;&#32456;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Partial label learning (PLL) aims to train multiclass classifiers from the examples each annotated with a set of candidate labels where a fixed but unknown candidate label is correct. In the last few years, the instance-independent generation process of candidate labels has been extensively studied, on the basis of which many theoretical advances have been made in PLL. Nevertheless, the candidate labels are always instance-dependent in practice and there is no theoretical guarantee that the model trained on the instance-dependent PLL examples can converge to an ideal one. In this paper, a theoretically grounded and practically effective approach named POP, i.e. PrOgressive Purification for instance-dependent partial label learning, is proposed. Specifically, POP updates the learning model and purifies each candidate label set progressively in every epoch. Theoretically, we prove that POP enlarges the region appropriately fast where the model is reliable, and eventually approximates the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;&#20302;&#25104;&#26412;&#20351;&#20219;&#20309;&#20998;&#23618;&#32858;&#31867;&#20844;&#24179;&#24179;&#34913;&#65292;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#25104;&#26412;&#20844;&#24179;&#26435;&#34913;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2205.14198</link><description>&lt;p&gt;
&#36890;&#29992;&#21270;&#38477;&#32500;&#65306;&#20302;&#25104;&#26412;&#20351;&#20219;&#20309;&#20998;&#23618;&#32858;&#31867;&#20844;&#24179;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Generalized Reductions: Making any Hierarchical Clustering Fair and Balanced with Low Cost. (arXiv:2205.14198v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;&#20302;&#25104;&#26412;&#20351;&#20219;&#20309;&#20998;&#23618;&#32858;&#31867;&#20844;&#24179;&#24179;&#34913;&#65292;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#25104;&#26412;&#20844;&#24179;&#26435;&#34913;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26159;&#29616;&#20195;&#32479;&#35745;&#20998;&#26512;&#27969;&#31243;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#12290;&#36817;&#24180;&#26469;&#65292;&#20844;&#24179;&#32858;&#31867;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#25209;&#22312;Ahmadian&#31561;&#20154;&#22312;2020&#24180;&#30340;NeurIPS&#32467;&#26524;&#20043;&#21518;&#30740;&#31350;&#20998;&#23618;&#32858;&#31867;&#20844;&#24179;&#24615;&#30340;&#20154;&#12290;&#25105;&#20204;&#20351;&#29992;Dasgupta&#30340;&#25104;&#26412;&#20989;&#25968;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#21487;&#33021;&#26159;&#20998;&#23618;&#32858;&#31867;&#35780;&#20272;&#20013;&#26368;&#24120;&#35265;&#30340;&#29702;&#35770;&#25351;&#26631;&#20043;&#19968;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#20808;&#21069;&#30340;$ O(n^{5/6}poly \ log&#65288;n))$&#25104;&#26412;&#20844;&#24179;&#36817;&#20284;&#22823;&#22823;&#25913;&#36827;&#20026;&#25509;&#36817;&#22810;&#23545;&#25968;$ O(n^\delta poly \ log&#65288;n))$&#30340;&#25104;&#26412;&#20844;&#24179;&#36817;&#20284;&#65292;&#20854;&#20013;&#20219;&#20309;&#24120;&#25968;$\delta \ in&#65288;0,1&#65289;$&#12290;&#36825;&#20010;&#32467;&#26524;&#24314;&#31435;&#20102;&#25104;&#26412;&#20844;&#24179;&#26435;&#34913;&#65292;&#24182;&#25193;&#23637;&#21040;&#27604;&#20808;&#21069;&#30340;&#24037;&#20316;&#26356;&#24191;&#27867;&#30340;&#20844;&#24179;&#32422;&#26463;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#25913;&#21464;&#29616;&#26377;&#30340;&#20998;&#23618;&#32858;&#31867;&#20197;&#20445;&#35777;&#22312;&#23618;&#27425;&#32467;&#26500;&#30340;&#20219;&#20309;&#32423;&#21035;&#19978;&#30340;&#20844;&#24179;&#24615;&#21644;&#31751;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering is a fundamental building block of modern statistical analysis pipelines. Fair clustering has seen much attention from the machine learning community in recent years. We are some of the first to study fairness in the context of hierarchical clustering, after the results of Ahmadian et al. from NeurIPS in 2020. We evaluate our results using Dasgupta's cost function, perhaps one of the most prevalent theoretical metrics for hierarchical clustering evaluation. Our work vastly improves the previous $O(n^{5/6}poly\log(n))$ fair approximation for cost to a near polylogarithmic $O(n^\delta poly\log(n))$ fair approximation for any constant $\delta\in(0,1)$. This result establishes a cost-fairness tradeoff and extends to broader fairness constraints than the previous work. We also show how to alter existing hierarchical clusterings to guarantee fairness and cluster balance across any level in the hierarchy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#29616;&#20195;CNN&#21644;&#35270;&#35273;Transformer&#27169;&#22411;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#30340;&#24615;&#33021;&#12289;&#40065;&#26834;&#24615;&#21644;&#20998;&#31867;&#31574;&#30053;&#12290;&#22312;&#20083;&#33146;&#30284;&#12289;&#32963;&#30284;&#21644;&#32467;&#30452;&#32928;&#30284;&#20840;&#20999;&#29255;&#22270;&#20687;&#31561;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;ViT&#27169;&#22411;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;CNN&#65292;&#32780;Swin Transformer&#27169;&#22411;&#22312;&#23545;&#25239;&#26579;&#33394;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2204.05044</link><description>&lt;p&gt;
&#20174;&#29616;&#20195;CNN&#21040;&#35270;&#35273;Transformer&#65306;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#30340;&#24615;&#33021;&#12289;&#40065;&#26834;&#24615;&#21644;&#20998;&#31867;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
From Modern CNNs to Vision Transformers: Assessing the Performance, Robustness, and Classification Strategies of Deep Learning Models in Histopathology. (arXiv:2204.05044v2 [eess.IV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#29616;&#20195;CNN&#21644;&#35270;&#35273;Transformer&#27169;&#22411;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#30340;&#24615;&#33021;&#12289;&#40065;&#26834;&#24615;&#21644;&#20998;&#31867;&#31574;&#30053;&#12290;&#22312;&#20083;&#33146;&#30284;&#12289;&#32963;&#30284;&#21644;&#32467;&#30452;&#32928;&#30284;&#20840;&#20999;&#29255;&#22270;&#20687;&#31561;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;ViT&#27169;&#22411;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;CNN&#65292;&#32780;Swin Transformer&#27169;&#22411;&#22312;&#23545;&#25239;&#26579;&#33394;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27491;&#22312;&#25913;&#21464;&#32452;&#32455;&#30149;&#29702;&#23398;&#39046;&#22495;&#65292;&#20294;&#35813;&#39046;&#22495;&#32570;&#20047;&#20840;&#38754;&#35780;&#20272;&#26368;&#26032;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#35201;&#32771;&#34385;&#31616;&#21333;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#36824;&#35201;&#32771;&#34385;&#20854;&#20182;&#36136;&#37327;&#35201;&#27714;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23545;&#19968;&#31995;&#21015;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#21253;&#25324;&#26368;&#26032;&#30340;&#35270;&#35273;Transformer&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;ConvNeXt&#12289;ResNet&#65288;BiT&#65289;&#12289;Inception&#12289;ViT&#21644;Swin Transformer&#65292;&#24182;&#22312;&#26377;&#30417;&#30563;&#25110;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#23545;&#21253;&#21547;&#20083;&#33146;&#30284;&#12289;&#32963;&#30284;&#21644;&#32467;&#30452;&#32928;&#30284;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#20116;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#32452;&#32455;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#27979;&#35797;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22270;&#20687;&#36716;&#25442;&#27169;&#22411;&#26469;&#35780;&#20272;&#30284;&#30151;&#20998;&#31867;&#27169;&#22411;&#23545;&#26579;&#33394;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#31995;&#32479;&#22320;&#25581;&#31034;&#20102;&#23427;&#20204;&#23398;&#21040;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;ViT&#27169;&#22411;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;CNN&#65292;&#32780;Swin Transformer&#27169;&#22411;&#22312;&#23545;&#25239;&#26579;&#33394;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#22823;&#22810;&#25968;&#27169;&#22411;&#30340;&#20998;&#31867;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#23398;&#21040;&#30340;&#29305;&#24449;&#65292;&#21253;&#25324;&#31354;&#38388;&#39057;&#29575;&#20449;&#24687;&#21644;&#32959;&#30244;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
While machine learning is currently transforming the field of histopathology, the domain lacks a comprehensive evaluation of state-of-the-art models based on essential but complementary quality requirements beyond a mere classification accuracy. In order to fill this gap, we developed a new methodology to extensively evaluate a wide range of classification models, including recent vision transformers, and convolutional neural networks such as: ConvNeXt, ResNet (BiT), Inception, ViT and Swin transformer, with and without supervised or self-supervised pretraining. We thoroughly tested the models on five widely used histopathology datasets containing whole slide images of breast, gastric, and colorectal cancer and developed a novel approach using an image-to-image translation model to assess the robustness of a cancer classification model against stain variations. Further, we extended existing interpretability methods to previously unstudied models and systematically reveal insights of th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21452;&#23618;&#20248;&#21270;&#20013;&#30340;&#38797;&#28857;&#36867;&#36920;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#19968;&#31181;&#26159;&#29992;&#20110;&#30830;&#23450;&#24615;&#38382;&#39064;&#30340;&#25200;&#21160;&#36817;&#20284;&#38544;&#24335;&#24494;&#20998;&#31639;&#27861;&#65292;&#19968;&#31181;&#26159;&#29992;&#20110;&#38543;&#26426;&#38382;&#39064;&#30340;&#19981;&#31934;&#30830;&#38750;&#26354;&#29575;&#28304;&#33258;&#22122;&#22768;&#31639;&#27861;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#25200;&#21160;&#22810;&#27493;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#38750;&#28176;&#36817;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2202.03684</link><description>&lt;p&gt;
&#22522;&#20110;&#36817;&#20284;&#38544;&#24335;&#24494;&#20998;&#30340;&#21452;&#23618;&#20248;&#21270;&#20013;&#30340;&#38797;&#28857;&#36867;&#36920;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficiently Escaping Saddle Points in Bilevel Optimization. (arXiv:2202.03684v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21452;&#23618;&#20248;&#21270;&#20013;&#30340;&#38797;&#28857;&#36867;&#36920;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#19968;&#31181;&#26159;&#29992;&#20110;&#30830;&#23450;&#24615;&#38382;&#39064;&#30340;&#25200;&#21160;&#36817;&#20284;&#38544;&#24335;&#24494;&#20998;&#31639;&#27861;&#65292;&#19968;&#31181;&#26159;&#29992;&#20110;&#38543;&#26426;&#38382;&#39064;&#30340;&#19981;&#31934;&#30830;&#38750;&#26354;&#29575;&#28304;&#33258;&#22122;&#22768;&#31639;&#27861;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#25200;&#21160;&#22810;&#27493;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#38750;&#28176;&#36817;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#20248;&#21270;&#20013;&#30340;&#22522;&#26412;&#38382;&#39064;&#20043;&#19968;&#12290;&#26368;&#36817;&#30340;&#29702;&#35770;&#21457;&#23637;&#38598;&#20013;&#20110;&#23547;&#25214;&#38750;&#20984;&#24378;&#20984;&#24773;&#20917;&#19979;&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#20123;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#22312;&#38750;&#20984;&#24378;&#20984;&#21452;&#23618;&#20248;&#21270;&#20013;&#36867;&#31163;&#38797;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#24102;&#26377;&#28909;&#21551;&#21160;&#31574;&#30053;&#30340;&#25200;&#21160;&#36817;&#20284;&#38544;&#24335;&#24494;&#20998;&#65288;AID&#65289;&#21487;&#20197;&#22312;$\tilde{O}(\epsilon^{-2})$&#36845;&#20195;&#20013;&#39640;&#27010;&#29575;&#22320;&#25214;&#21040;$\epsilon$-&#36817;&#20284;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#21452;&#23618;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#31934;&#30830;&#38750;&#26354;&#29575;&#28304;&#33258;&#22122;&#22768;&#31639;&#27861;&#65288;iNEON&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#32431;&#19968;&#38454;&#31639;&#27861;&#65292;&#21487;&#20197;&#36867;&#31163;&#38797;&#28857;&#24182;&#25214;&#21040;&#38543;&#26426;&#21452;&#23618;&#20248;&#21270;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#20316;&#20026;&#19968;&#20010;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25200;&#21160;&#22810;&#27493;&#26799;&#24230;&#19979;&#38477;&#65288;GDmax&#65289;&#31639;&#27861;&#30340;&#31532;&#19968;&#20010;&#38750;&#28176;&#36817;&#20998;&#26512;&#65292;&#35813;&#31639;&#27861;&#25910;&#25947;&#21040;&#26368;&#23567;&#21270;&#26368;&#22823;&#38382;&#39064;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization is one of the fundamental problems in machine learning and optimization. Recent theoretical developments in bilevel optimization focus on finding the first-order stationary points for nonconvex-strongly-convex cases. In this paper, we analyze algorithms that can escape saddle points in nonconvex-strongly-convex bilevel optimization. Specifically, we show that the perturbed approximate implicit differentiation (AID) with a warm start strategy finds $\epsilon$-approximate local minimum of bilevel optimization in $\tilde{O}(\epsilon^{-2})$ iterations with high probability. Moreover, we propose an inexact NEgative-curvature-Originated-from-Noise Algorithm (iNEON), a pure first-order algorithm that can escape saddle point and find local minimum of stochastic bilevel optimization. As a by-product, we provide the first nonasymptotic analysis of perturbed multi-step gradient descent ascent (GDmax) algorithm that converges to local minimax point for minimax problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#31181;&#8212;&#8212;&#20108;&#27425;&#26799;&#24230;&#65292;&#29992;&#20110;&#22312;&#21516;&#24577;&#21152;&#23494;&#39046;&#22495;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#65292;&#24182;&#25104;&#21151;&#25552;&#21319;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#29616;&#20102;&#21516;&#24577;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#20165;&#38656;&#36739;&#23569;&#30340;&#36845;&#20195;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2201.10838</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#21450;&#20854;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#31181;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Logistic Regression Training with A Faster Gradient Variant. (arXiv:2201.10838v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.10838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#31181;&#8212;&#8212;&#20108;&#27425;&#26799;&#24230;&#65292;&#29992;&#20110;&#22312;&#21516;&#24577;&#21152;&#23494;&#39046;&#22495;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#65292;&#24182;&#25104;&#21151;&#25552;&#21319;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#29616;&#20102;&#21516;&#24577;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#20165;&#38656;&#36739;&#23569;&#30340;&#36845;&#20195;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#21152;&#23494;&#25968;&#25454;&#19978;&#30340;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#19968;&#30452;&#26159;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#31181;&#8212;&#8212;&#20108;&#27425;&#26799;&#24230;&#65292;&#29992;&#20110;&#22312;&#21516;&#24577;&#21152;&#23494;&#39046;&#22495;&#23454;&#29616;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#65292;&#20854;&#26680;&#24515;&#21487;&#20197;&#30475;&#20316;&#26159;&#31616;&#21270;&#30340;&#22266;&#23450;Hessian&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#20998;&#21035;&#21521;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#65288;NAG&#65289;&#21644;&#33258;&#36866;&#24212;&#26799;&#24230;&#31639;&#27861;&#65288;Adagrad&#65289;&#22686;&#24378;&#20102;&#35813;&#26799;&#24230;&#21464;&#31181;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#22686;&#24378;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#24378;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#19978;&#27604;&#26420;&#32032;&#30340;&#19968;&#38454;&#26799;&#24230;&#26041;&#27861;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#22686;&#24378;&#30340;NAG&#26041;&#27861;&#26469;&#23454;&#29616;&#21516;&#24577;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#65292;&#24182;&#22312;&#20165;3&#27425;&#36845;&#20195;&#20013;&#33719;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logistic regression training over encrypted data has been an attractive idea to security concerns for years. In this paper, we propose a faster gradient variant called $\texttt{quadratic gradient}$ to implement logistic regression training in a homomorphic encryption domain, the core of which can be seen as an extension of the simplified fixed Hessian.  We enhance Nesterov's accelerated gradient (NAG) and Adaptive Gradient Algorithm (Adagrad) respectively with this gradient variant and evaluate the enhanced algorithms on several datasets. Experimental results show that the enhanced methods have a state-of-the-art performance in convergence speed compared to the naive first-order gradient methods. We then adopt the enhanced NAG method to implement homomorphic logistic regression training and obtain a comparable result by only $3$ iterations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110; Python &#35821;&#27861;&#32422;&#26463;&#21644;&#35821;&#20041;&#32422;&#26463;&#30340;&#24341;&#23548;&#33258;&#21160;&#29983;&#25104; Python &#20195;&#30721;&#30340;&#26041;&#27861; GAP-Gen&#65292;&#36890;&#36807;&#24494;&#35843; T5 &#21644; CodeT5 &#36825;&#20004;&#31181;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#33258;&#21160;&#29983;&#25104; Python &#20195;&#30721;&#20219;&#21153;&#19978;&#20445;&#25345;&#20102;&#39640;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2201.08810</link><description>&lt;p&gt;
GAP-Gen: &#24341;&#23548;&#33258;&#21160;&#29983;&#25104; Python &#20195;&#30721;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GAP-Gen: Guided Automatic Python Code Generation. (arXiv:2201.08810v2 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.08810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110; Python &#35821;&#27861;&#32422;&#26463;&#21644;&#35821;&#20041;&#32422;&#26463;&#30340;&#24341;&#23548;&#33258;&#21160;&#29983;&#25104; Python &#20195;&#30721;&#30340;&#26041;&#27861; GAP-Gen&#65292;&#36890;&#36807;&#24494;&#35843; T5 &#21644; CodeT5 &#36825;&#20004;&#31181;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#33258;&#21160;&#29983;&#25104; Python &#20195;&#30721;&#20219;&#21153;&#19978;&#20445;&#25345;&#20102;&#39640;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#22312;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#38750;&#24120;&#26377;&#21161;&#20110;&#25552;&#39640;&#25928;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Python &#35821;&#27861;&#32422;&#26463;&#21644;&#35821;&#20041;&#32422;&#26463;&#30340;&#24341;&#23548;&#33258;&#21160;&#29983;&#25104; Python &#20195;&#30721;&#30340;&#26041;&#27861; GAP-Gen&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102; Syntax-Flow&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21270;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#24418;&#24335;&#30340; Python &#35821;&#27861;&#32422;&#26463;&#65292;&#24182;&#19988;&#22312;&#20195;&#30721;&#20013;&#20445;&#30041;&#20102;&#20851;&#38190;&#30340;&#35821;&#27861;&#20449;&#24687;&#65292;&#20174;&#32780;&#20943;&#23567;&#20102;&#25277;&#35937;&#35821;&#27861;&#26641;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#12290;&#38500;&#20102; Syntax-Flow&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102; Variable-Flow&#65292;&#23427;&#33021;&#22815;&#22312;&#25972;&#20010;&#20195;&#30721;&#20013;&#19968;&#33268;&#22320;&#25277;&#35937;&#21464;&#37327;&#21644;&#20989;&#25968;&#21517;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#37325;&#28857;&#19981;&#26159;&#22312;&#39044;&#35757;&#32451;&#19978;&#65292;&#32780;&#26159;&#22312;&#20462;&#25913;&#24494;&#35843;&#36807;&#31243;&#65292;&#20174;&#32780;&#20943;&#23569;&#35745;&#31639;&#38656;&#27714;&#65292;&#20294;&#22312;&#33258;&#21160;&#29983;&#25104; Python &#20195;&#30721;&#20219;&#21153;&#19978;&#20445;&#25345;&#39640;&#29983;&#25104;&#24615;&#33021;&#12290; GAP-Gen &#20351;&#29992; T5 &#21644; CodeT5 &#36825;&#20004;&#31181;&#22522;&#20110; transformer &#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807; CodeSearchNet &#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic code generation from natural language descriptions can be highly beneficial during the process of software development. In this work, we propose GAP-Gen, a Guided Automatic Python Code Generation method based on Python syntactic constraints and semantic constraints. We first introduce Python syntactic constraints in the form of Syntax-Flow, which is a simplified version of Abstract Syntax Tree (AST) reducing the size and high complexity of Abstract Syntax Tree but maintaining crucial syntactic information of Python code. In addition to Syntax-Flow, we introduce Variable-Flow which abstracts variable and function names consistently through out the code. In our work, rather than pretraining, we focus on modifying the finetuning process which reduces computational requirements but retains high generation performance on automatic Python code generation task. GAP-Gen fine-tunes the transformer based language models T5 and CodeT5 using the Code-to-Docstring datasets CodeSearchNet, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#31232;&#30095;&#24674;&#22797;&#65288;&#21387;&#32553;&#24863;&#30693;&#65289;&#20013;&#65292;&#37319;&#29992;&#36807;&#24230;&#21442;&#25968;&#30340;&#24179;&#26041;&#25439;&#22833;&#20989;&#25968;&#65292;&#25512;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#31639;&#27861;&#35299;&#20915;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#23545;&#31232;&#30095;&#38382;&#39064;&#30340;&#33391;&#22909;&#36866;&#24212;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#65292;&#20197;&#21450;&#31454;&#20105;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2112.11027</link><description>&lt;p&gt;
&#36890;&#36807;&#36807;&#24230;&#21442;&#25968;&#26469;&#24341;&#23548;&#31232;&#30095;&#24615;: More is Less
&lt;/p&gt;
&lt;p&gt;
More is Less: Inducing Sparsity via Overparameterization. (arXiv:2112.11027v5 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.11027
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#31232;&#30095;&#24674;&#22797;&#65288;&#21387;&#32553;&#24863;&#30693;&#65289;&#20013;&#65292;&#37319;&#29992;&#36807;&#24230;&#21442;&#25968;&#30340;&#24179;&#26041;&#25439;&#22833;&#20989;&#25968;&#65292;&#25512;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#31639;&#27861;&#35299;&#20915;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#23545;&#31232;&#30095;&#38382;&#39064;&#30340;&#33391;&#22909;&#36866;&#24212;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#65292;&#20197;&#21450;&#31454;&#20105;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#24120;&#24120;&#20250;&#36807;&#24230;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21363;&#20351;&#29992;&#27604;&#35757;&#32451;&#26679;&#26412;&#26356;&#22810;&#30340;&#21442;&#25968;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36890;&#36807;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20250;&#23548;&#33268;&#33391;&#22909;&#30340;&#27867;&#21270;&#27169;&#22411;&#65292;&#32780;&#20256;&#32479;&#30340;&#32479;&#35745;&#23398;&#21017;&#20250;&#35748;&#20026;&#20250;&#36807;&#25311;&#21512;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#31181;&#38544;&#24335;&#20559;&#24046;&#29616;&#35937;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31232;&#30095;&#24674;&#22797;&#65288;&#21387;&#32553;&#24863;&#30693;&#65289;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#23427;&#26412;&#36523;&#23601;&#24456;&#26377;&#36259;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#20026;&#20102;&#20174;&#27424;&#23450;&#32447;&#24615;&#27979;&#37327;&#20013;&#37325;&#26500;&#21521;&#37327;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30456;&#24212;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#24179;&#26041;&#25439;&#22833;&#20989;&#25968;&#65292;&#20854;&#20013;&#35201;&#37325;&#26500;&#30340;&#21521;&#37327;&#34987;&#28145;&#24230;&#20998;&#35299;&#25104;&#22810;&#20010;&#21521;&#37327;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;&#23384;&#22312;&#19968;&#20010;&#31934;&#30830;&#30340;&#35299;&#65292;&#21017;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#20002;&#22833;&#20989;&#25968;&#30340;&#39321;&#33609;&#26799;&#24230;&#27969;&#20250;&#25910;&#25947;&#21040;&#26368;&#23567;$\ell_1$&#33539;&#25968;&#30340;&#31934;&#30830;&#35299;&#30340;&#33391;&#22909;&#36924;&#36817;&#12290;&#21518;&#32773;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#33021;&#20419;&#36827;&#31232;&#30095;&#35299;&#30340;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#33879;&#25552;&#39640;&#20102;&#31232;&#30095;&#27169;&#24335;&#20043;&#38388;&#30340;&#36716;&#31227;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31232;&#30095;&#24674;&#22797;&#30340;&#26032;&#30340;&#31454;&#20105;&#24615;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In deep learning it is common to overparameterize neural networks, that is, to use more parameters than training samples. Quite surprisingly training the neural network via (stochastic) gradient descent leads to models that generalize very well, while classical statistics would suggest overfitting. In order to gain understanding of this implicit bias phenomenon we study the special case of sparse recovery (compressed sensing) which is of interest on its own. More precisely, in order to reconstruct a vector from underdetermined linear measurements, we introduce a corresponding overparameterized square loss functional, where the vector to be reconstructed is deeply factorized into several vectors. We show that, if there exists an exact solution, vanilla gradient flow for the overparameterized loss functional converges to a good approximation of the solution of minimal $\ell_1$-norm. The latter is well-known to promote sparse solutions. As a by-product, our results significantly improve t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21151;&#33021;&#25968;&#25454;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#36830;&#32493;&#38544;&#34255;&#23618;&#23454;&#29616;&#23545;&#21151;&#33021;&#21709;&#24212;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#31181;&#27169;&#22411;&#25311;&#21512;&#31574;&#30053;&#65288;FDNN&#21644;FBNN&#65289;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#25216;&#26415;&#24471;&#21040;&#26356;&#21152;&#31616;&#26126;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2107.14151</link><description>&lt;p&gt;
&#29616;&#20195;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#65306;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21151;&#33021;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Modern Non-Linear Function-on-Function Regression. (arXiv:2107.14151v1 [stat.ME] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.14151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21151;&#33021;&#25968;&#25454;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#36830;&#32493;&#38544;&#34255;&#23618;&#23454;&#29616;&#23545;&#21151;&#33021;&#21709;&#24212;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#31181;&#27169;&#22411;&#25311;&#21512;&#31574;&#30053;&#65288;FDNN&#21644;FBNN&#65289;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#25216;&#26415;&#24471;&#21040;&#26356;&#21152;&#31616;&#26126;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#31867;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21151;&#33021;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#29992;&#30001;&#36830;&#32493;&#31070;&#32463;&#20803;&#32452;&#25104;&#30340;&#38544;&#34255;&#23618;&#65292;&#31216;&#20026;&#36830;&#32493;&#38544;&#34255;&#23618;&#65292;&#29992;&#20110;&#21151;&#33021;&#21709;&#24212;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#31181;&#27169;&#22411;&#25311;&#21512;&#31574;&#30053;&#65306;&#21151;&#33021;&#30452;&#25509;&#31070;&#32463;&#32593;&#32476;&#65288;FDNN&#65289;&#21644;&#21151;&#33021;&#22522;&#30784;&#31070;&#32463;&#32593;&#32476;&#65288;FBNN&#65289;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#26159;&#19987;&#38376;&#35774;&#35745;&#26469;&#21033;&#29992;&#21151;&#33021;&#25968;&#25454;&#22266;&#26377;&#30340;&#32467;&#26500;&#65292;&#24182;&#25429;&#25417;&#21151;&#33021;&#39044;&#27979;&#21464;&#37327;&#21644;&#21151;&#33021;&#21709;&#24212;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#27714;&#35299;&#20989;&#25968;&#26799;&#24230;&#24182;&#23454;&#26045;&#27491;&#21017;&#21270;&#25216;&#26415;&#36827;&#34892;&#27169;&#22411;&#25311;&#21512;&#65292;&#24471;&#21040;&#26356;&#31616;&#26126;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#31034;&#20363;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#21151;&#33021;&#27169;&#22411;&#26041;&#38754;&#30340;&#24378;&#22823;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new class of non-linear function-on-function regression models for functional data using neural networks. We propose a framework using a hidden layer consisting of continuous neurons, called a continuous hidden layer, for functional response modeling and give two model fitting strategies, Functional Direct Neural Network (FDNN) and Functional Basis Neural Network (FBNN). Both are designed explicitly to exploit the structure inherent in functional data and capture the complex relations existing between the functional predictors and the functional response. We fit these models by deriving functional gradients and implement regularization techniques for more parsimonious results. We demonstrate the power and flexibility of our proposed method in handling complex functional models through extensive simulation studies as well as real data examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#12289;&#36866;&#29992;&#20110;&#20989;&#25968;&#25968;&#25454;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21464;&#20307;&#65292;&#26088;&#22312;&#26174;&#24335;&#21033;&#29992;&#20989;&#25968;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#20840;&#38754;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#31034;&#20363;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2104.09371</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Non-linear Functional Modeling using Neural Networks. (arXiv:2104.09371v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.09371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#12289;&#36866;&#29992;&#20110;&#20989;&#25968;&#25968;&#25454;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21464;&#20307;&#65292;&#26088;&#22312;&#26174;&#24335;&#21033;&#29992;&#20989;&#25968;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#20840;&#38754;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#31034;&#20363;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#20989;&#25968;&#25968;&#25454;&#27169;&#22411;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#38750;&#32447;&#24615;&#24314;&#27169;&#26041;&#38754;&#38750;&#24120;&#25104;&#21151;&#65292;&#20294;&#22312;&#20989;&#25968;&#25968;&#25454;&#35774;&#32622;&#26041;&#38754;&#21364;&#24456;&#23569;&#26377;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21464;&#20307;&#65306;&#19968;&#31181;&#26159;&#20855;&#26377;&#36830;&#32493;&#38544;&#34255;&#23618;&#30340;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;&#20989;&#25968;&#30452;&#25509;&#31070;&#32463;&#32593;&#32476;&#65288;FDNN&#65289;&#65292;&#21478;&#19968;&#31181;&#21017;&#21033;&#29992;&#22522;&#25193;&#23637;&#21644;&#36830;&#32493;&#38544;&#34255;&#23618;&#65292;&#31216;&#20026;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#65288;FBNN&#65289;&#12290;&#20004;&#31181;&#21464;&#20307;&#37117;&#26159;&#35774;&#35745;&#29992;&#26469;&#26174;&#24335;&#21033;&#29992;&#20989;&#25968;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#32467;&#26500;&#12290;&#20026;&#20102;&#25311;&#21512;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#26799;&#24230;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20840;&#38754;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#31034;&#20363;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#20989;&#25968;&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new class of non-linear models for functional data based on neural networks. Deep learning has been very successful in non-linear modeling, but there has been little work done in the functional data setting. We propose two variations of our framework: a functional neural network with continuous hidden layers, called the Functional Direct Neural Network (FDNN), and a second version that utilizes basis expansions and continuous hidden layers, called the Functional Basis Neural Network (FBNN). Both are designed explicitly to exploit the structure inherent in functional data. To fit these models we derive a functional gradient based optimization algorithm. The effectiveness of the proposed methods in handling complex functional models is demonstrated by comprehensive simulation studies and real data examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21551;&#21457;&#24335;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;&#29305;&#21035;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#28431;&#27934;&#20844;&#21578;&#19982;&#20854;&#20462;&#22797;&#25552;&#20132;&#26144;&#23556;&#21040;&#24320;&#28304;&#20195;&#30721;&#24211;&#20013;&#65292;&#20197;&#35299;&#20915;&#32570;&#20047;&#20840;&#38754;&#20934;&#30830;&#30340;&#28431;&#27934;&#25968;&#25454;&#26469;&#28304;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2103.13375</link><description>&lt;p&gt;
&#33258;&#21160;&#23558;&#28431;&#27934;&#20844;&#21578;&#26144;&#23556;&#21040;&#24320;&#28304;&#20195;&#30721;&#24211;&#20013;&#30340;&#20462;&#22797;&#25552;&#20132;
&lt;/p&gt;
&lt;p&gt;
Automated Mapping of Vulnerability Advisories onto their Fix Commits in Open Source Repositories. (arXiv:2103.13375v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.13375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21551;&#21457;&#24335;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;&#29305;&#21035;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#28431;&#27934;&#20844;&#21578;&#19982;&#20854;&#20462;&#22797;&#25552;&#20132;&#26144;&#23556;&#21040;&#24320;&#28304;&#20195;&#30721;&#24211;&#20013;&#65292;&#20197;&#35299;&#20915;&#32570;&#20047;&#20840;&#38754;&#20934;&#30830;&#30340;&#28431;&#27934;&#25968;&#25454;&#26469;&#28304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#20840;&#38754;&#20934;&#30830;&#30340;&#28431;&#27934;&#25968;&#25454;&#26469;&#28304;&#26159;&#30740;&#31350;&#21644;&#29702;&#35299;&#36719;&#20214;&#28431;&#27934;&#65288;&#21450;&#20854;&#20462;&#27491;&#65289;&#30340;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#20174;&#23454;&#38469;&#32463;&#39564;&#20013;&#20135;&#29983;&#30340;&#21551;&#21457;&#24335;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#29305;&#21035;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#65292;&#20197;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#26041;&#27861;&#21253;&#21547;&#19977;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#20174;&#20844;&#21578;&#20013;&#25552;&#21462;&#21253;&#21547;&#28431;&#27934;&#20851;&#38190;&#20449;&#24687;&#30340;&#35760;&#24405;&#65288;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#65289;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#36807;&#28388;&#24050;&#30693;&#19982;&#20219;&#21153;&#19981;&#30456;&#20851;&#30340;&#25552;&#20132;&#26469;&#20174;&#21463;&#24433;&#21709;&#39033;&#30446;&#30340;&#28304;&#20195;&#30721;&#24211;&#20013;&#33719;&#21462;&#19968;&#32452;&#20505;&#36873;&#20462;&#22797;&#25552;&#20132;&#12290;&#26368;&#21518;&#65292;&#23545;&#20110;&#27599;&#20010;&#36825;&#26679;&#30340;&#20505;&#36873;&#25552;&#20132;&#65292;&#35813;&#26041;&#27861;&#26500;&#24314;&#19968;&#20010;&#25968;&#20540;&#29305;&#24449;&#21521;&#37327;&#65292;&#21453;&#26144;&#25552;&#20132;&#30340;&#29305;&#24449;&#19982;&#25163;&#22836;&#30340;&#28431;&#27934;&#21305;&#37197;&#30456;&#20851;&#12290;&#28982;&#21518;&#21033;&#29992;&#29305;&#24449;&#21521;&#37327;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lack of comprehensive sources of accurate vulnerability data represents a critical obstacle to studying and understanding software vulnerabilities (and their corrections). In this paper, we present an approach that combines heuristics stemming from practical experience and machine-learning (ML) specifically, natural language processing (NLP) - to address this problem. Our method consists of three phases. First, an advisory record containing key information about a vulnerability is extracted from an advisory (expressed in natural language). Second, using heuristics, a subset of candidate fix commits is obtained from the source code repository of the affected project by filtering out commits that are known to be irrelevant for the task at hand. Finally, for each such candidate commit, our method builds a numerical feature vector reflecting the characteristics of the commit that are relevant to predicting its match with the advisory at hand. The feature vectors are then exploited fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#38480;&#21046;&#30340;&#26356;&#22909;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#38480;&#21046;&#22270;&#20687;&#25200;&#21160;&#12290;&#20316;&#32773;&#25351;&#20986;&#24182;&#32416;&#27491;&#20102;&#20808;&#21069;Wasserstein&#23041;&#32961;&#27169;&#22411;&#23450;&#20041;&#20013;&#30340;&#32570;&#38519;&#65292;&#24182;&#23637;&#31034;&#20102;&#26356;&#24378;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;Wasserstein-&#40065;&#26834;&#27169;&#22411;&#22312;&#38450;&#24481;&#29616;&#23454;&#19990;&#30028;&#20013;&#20986;&#29616;&#30340;&#25200;&#21160;&#26041;&#38754;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2004.12478</link><description>&lt;p&gt;
&#25552;&#39640;&#22270;&#20687;Wasserstein&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Improved Image Wasserstein Attacks and Defenses. (arXiv:2004.12478v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.12478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#38480;&#21046;&#30340;&#26356;&#22909;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#38480;&#21046;&#22270;&#20687;&#25200;&#21160;&#12290;&#20316;&#32773;&#25351;&#20986;&#24182;&#32416;&#27491;&#20102;&#20808;&#21069;Wasserstein&#23041;&#32961;&#27169;&#22411;&#23450;&#20041;&#20013;&#30340;&#32570;&#38519;&#65292;&#24182;&#23637;&#31034;&#20102;&#26356;&#24378;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;Wasserstein-&#40065;&#26834;&#27169;&#22411;&#22312;&#38450;&#24481;&#29616;&#23454;&#19990;&#30028;&#20013;&#20986;&#29616;&#30340;&#25200;&#21160;&#26041;&#38754;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#25991;&#29486;&#20013;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#23545;&#20110;$\ell_p$&#38480;&#21046;&#19979;&#21463;&#21040;&#22270;&#20687;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#20294;&#26159;&#65292;&#29616;&#23454;&#20013;&#30340;&#25200;&#21160;&#24456;&#23569;&#34920;&#29616;&#20986;$\ell_p$&#23041;&#32961;&#27169;&#22411;&#25152;&#20551;&#23450;&#30340;&#20687;&#32032;&#29420;&#31435;&#24615;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Wasserstein&#36317;&#31163;&#38480;&#21046;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#23427;&#38480;&#21046;&#25200;&#21160;&#20026;&#20687;&#32032;&#36136;&#37327;&#31227;&#21160;&#12290;&#25105;&#20204;&#25351;&#20986;&#24182;&#32416;&#27491;&#20102;&#20808;&#21069;Wasserstein&#23041;&#32961;&#27169;&#22411;&#23450;&#20041;&#20013;&#30340;&#32570;&#38519;&#65292;&#24182;&#22312;&#25105;&#20204;&#26356;&#22909;&#22320;&#23450;&#20041;&#30340;&#26694;&#26550;&#19979;&#25506;&#32034;&#20102;&#26356;&#24378;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;Wasserstein-&#40065;&#26834;&#27169;&#22411;&#22312;&#38450;&#24481;&#29616;&#23454;&#19990;&#30028;&#20013;&#20986;&#29616;&#30340;&#25200;&#21160;&#26041;&#38754;&#30340;&#26080;&#33021;&#20026;&#21147;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/edwardjhu/improved_wasserstein&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness against image perturbations bounded by a $\ell_p$ ball have been well-studied in recent literature. Perturbations in the real-world, however, rarely exhibit the pixel independence that $\ell_p$ threat models assume. A recently proposed Wasserstein distance-bounded threat model is a promising alternative that limits the perturbation to pixel mass movements. We point out and rectify flaws in previous definition of the Wasserstein threat model and explore stronger attacks and defenses under our better-defined framework. Lastly, we discuss the inability of current Wasserstein-robust models in defending against perturbations seen in the real world. Our code and trained models are available at https://github.com/edwardjhu/improved_wasserstein .
&lt;/p&gt;</description></item></channel></rss>